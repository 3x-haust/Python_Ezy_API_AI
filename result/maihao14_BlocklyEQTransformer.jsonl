{"repo_info": {"repo_name": "BlocklyEQTransformer", "repo_owner": "maihao14", "repo_url": "https://github.com/maihao14/BlocklyEQTransformer"}}
{"type": "test_file", "path": "tests/test_0_imports.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\n# V1.59 created by @author: mostafamousavi\n# Ref Repo: https://github.com/smousavi05/EQTransformer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\ndef test_import_bocklyeqtransformer():\n    \"\"\"Test imports.\"\"\"\n    import BlocklyEQTransformer\n    from BlocklyEQTransformer.core.trainer import trainer\n    from BlocklyEQTransformer.core.predictor import predictor\n    from BlocklyEQTransformer.core.tester import tester\n    print(\"Imports are working!\")\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/core/__init__.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\n# V1.59 created by @author: mostafamousavi\n# Ref Repo: https://github.com/smousavi05/EQTransformer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nname='core'\n\nfrom .EqT_utils import *\nfrom .trainer import trainer\nfrom .tester import tester\nfrom .predictor import predictor\nfrom .mseed_predictor import mseed_predictor\n\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/__init__.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\n# V1.59 created by @author: mostafamousavi\n# Ref Repo: https://github.com/smousavi05/EQTransformer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nname='BlocklyEQTransformer'\n\nimport importlib\nimport sys\nimport warnings\n\nfrom BlocklyEQTransformer.core.trainer import trainer\nfrom BlocklyEQTransformer.core.tester import tester\nfrom BlocklyEQTransformer.core.predictor import predictor\nfrom BlocklyEQTransformer.core.mseed_predictor import mseed_predictor\nfrom BlocklyEQTransformer.core.EqT_utils import *\nfrom BlocklyEQTransformer.utils.associator import run_associator\nfrom BlocklyEQTransformer.utils.downloader import downloadMseeds, makeStationList, downloadSacs\nfrom BlocklyEQTransformer.utils.hdf5_maker import preprocessor\nfrom BlocklyEQTransformer.utils.plot import plot_detections, plot_data_chart\n\n__all__ = ['core', 'utils'] #'test'\n__version__ = '0.0.1'\n_import_map = {}\n\nclass EqtDeprecationWarning(UserWarning):\n    \"\"\"\n    Force pop-up of warnings.\n    \"\"\"\n    pass\n\nif sys.version_info.major < 3:\n    raise NotImplementedError(\n        \"EqT no longer supports Python 2.x.\")\n\n\nclass EqtRestructureAndLoad(object):\n    \"\"\"\n    Path finder and module loader for transitioning\n    \"\"\"\n\n    def find_module(self, fullname, path=None):\n        # Compatibility with namespace paths.\n        if hasattr(path, \"_path\"):\n            path = path._path\n\n        if not path or not path[0].startswith(__path__[0]):\n            return None\n\n        for key in _import_map.keys():\n            if fullname.startswith(key):\n                break\n        else:\n            return None\n        return self\n\n    def load_module(self, name):\n        # Use cached modules.\n        if name in sys.modules:\n            return sys.modules[name]\n        # Otherwise check if the name is part of the import map.\n        elif name in _import_map:\n            new_name = _import_map[name]\n        else:\n            new_name = name\n            for old, new in _import_map.items():\n                if not new_name.startswith(old):\n                    continue\n                new_name = new_name.replace(old, new)\n                break\n            else:\n                return None\n\n        # Don't load again if already loaded.\n        if new_name in sys.modules:\n            module = sys.modules[new_name]\n        else:\n            module = importlib.import_module(new_name)\n\n        # Warn here as at this point the module has already been imported.\n        warnings.warn(\"Module '%s' is deprecated and will stop working \"\n                      \"with the next delphi version. Please import module \"\n                      \"'%s' instead.\" % (name, new_name),\n                      EqtDeprecationWarning)\n        sys.modules[new_name] = module\n        sys.modules[name] = module\n        return module\n\n\nsys.meta_path.append(EqtRestructureAndLoad())\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/core/EqT_utils.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\n# V1.59 created by @author: mostafamousavi\n# Ref Repo: https://github.com/smousavi05/EQTransformer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nfrom __future__ import division, print_function\nimport numpy as np\nimport h5py\nimport matplotlib\nmatplotlib.use('agg')\nfrom tqdm import tqdm\nimport keras\nfrom keras import backend as K\nfrom keras.layers import add, Activation, LSTM, Conv1D\nfrom keras.layers import MaxPooling1D, UpSampling1D, Cropping1D, SpatialDropout1D, Bidirectional, BatchNormalization\nfrom keras.models import Model\nfrom keras.utils import multi_gpu_model\nfrom keras.optimizers import Adam\nfrom obspy.signal.trigger import trigger_onset,recursive_sta_lta\nimport matplotlib\nfrom tensorflow.python.util import deprecation\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    \"\"\"\n\n    Keras generator with preprocessing\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Name of hdf5 file containing waveforms data.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32\n        Batch size.\n\n    n_channels: int, default=3\n        Number of channels.\n\n    phase_window: int, fixed=40\n        The number of samples (window) around each phaset.\n\n    shuffle: bool, default=True\n        Shuffeling the list.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'.\n\n    label_type: str, default=gaussian\n        Labeling type: 'gaussian', 'triangle', or 'box'.\n\n    augmentation: bool, default=True\n        If True, half of each batch will be augmented version of the other half.\n\n    add_event_r: {float, None}, default=None\n        Chance for randomly adding a second event into the waveform.\n\n    add_gap_r: {float, None}, default=None\n        Add an interval with zeros into the waveform representing filled gaps.\n\n    shift_event_r: {float, None}, default=0.9\n        Rate of augmentation for randomly shifting the event within a trace.\n\n    add_noise_r: {float, None}, default=None\n        Chance for randomly adding Gaussian noise into the waveform.\n\n    drop_channe_r: {float, None}, default=None\n        Chance for randomly dropping some of the channels.\n\n    scale_amplitude_r: {float, None}, default=None\n        Chance for randomly amplifying the waveform amplitude.\n\n    pre_emphasis: bool, default=False\n        If True, waveforms will be pre emphasized.\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n    \"\"\"\n\n    def __init__(self,\n                 list_IDs,\n                 file_name,\n                 dim,\n                 batch_size=32,\n                 n_channels=3,\n                 phase_window = 40,\n                 shuffle=True,\n                 norm_mode = 'max',\n                 label_type = 'gaussian',\n                 augmentation = False,\n                 add_event_r = None,\n                 add_gap_r = None,\n                 shift_event_r = None,\n                 add_noise_r = None,\n                 drop_channe_r = None,\n                 scale_amplitude_r = None,\n                 pre_emphasis = True,\n                 phase_type = ['d','P','S']):\n\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.phase_window = phase_window\n        self.list_IDs = list_IDs\n        self.file_name = file_name\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n        self.label_type = label_type\n        self.augmentation = augmentation\n        self.add_event_r = add_event_r\n        self.add_gap_r = add_gap_r\n        self.shift_event_r = shift_event_r\n        self.add_noise_r = add_noise_r\n        self.drop_channe_r = drop_channe_r\n        self.scale_amplitude_r = scale_amplitude_r\n        self.pre_emphasis = pre_emphasis\n        self.phase_type = phase_type\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        if self.augmentation:\n            return 2*int(np.floor(len(self.list_IDs) / self.batch_size))\n        else:\n            return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        if self.augmentation:\n            indexes = self.indexes[index*self.batch_size//2:(index+1)*self.batch_size//2]\n            indexes = np.append(indexes, indexes)\n        else:\n            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        #Old\n        #X, y1, y2, y3 = self.__data_generation(list_IDs_temp)\n        #New(Hao)\n        X, Y = self.__data_generation(list_IDs_temp)\n        # Hao added Aug 2022\n        return (X, Y)\n        #return ({'input': X}, {'detector': y1, 'picker_P': y2, 'picker_S': y3})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def _normalize(self, data, mode = 'max'):\n        'Normalize waveforms in each batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def _scale_amplitude(self, data, rate):\n        'Scale amplitude or waveforms'\n\n        tmp = np.random.uniform(0, 1)\n        if tmp < rate:\n            data *= np.random.uniform(1, 3)\n        elif tmp < 2*rate:\n            data /= np.random.uniform(1, 3)\n        return data\n\n    def _drop_channel(self, data, snr, rate):\n        'Randomly replace values of one or two components to zeros in earthquake data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate and all(snr >= 10.0):\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _drop_channel_noise(self, data, rate):\n        'Randomly replace values of one or two components to zeros in noise data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate:\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _add_gaps(self, data, rate):\n        'Randomly add gaps (zeros) of different sizes into waveforms'\n\n        data = np.copy(data)\n        gap_start = np.random.randint(0, 4000)\n        gap_end = np.random.randint(gap_start, 5500)\n        if np.random.uniform(0, 1) < rate:\n            data[gap_start:gap_end,:] = 0\n        return data\n\n    def _add_noise(self, data, snr, rate):\n        'Randomly add Gaussian noie with a random SNR into waveforms'\n\n        data_noisy = np.empty((data.shape))\n        if np.random.uniform(0, 1) < rate and all(snr >= 10.0):\n            data_noisy = np.empty((data.shape))\n            data_noisy[:, 0] = data[:,0] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,0]), data.shape[0])\n            data_noisy[:, 1] = data[:,1] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,1]), data.shape[0])\n            data_noisy[:, 2] = data[:,2] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,2]), data.shape[0])\n        else:\n            data_noisy = data\n        return data_noisy\n\n    def _adjust_amplitude_for_multichannels(self, data):\n        'Adjust the amplitude of multichaneel data'\n\n        tmp = np.max(np.abs(data), axis=0, keepdims=True)\n        assert(tmp.shape[-1] == data.shape[-1])\n        if np.count_nonzero(tmp) > 0:\n          data *= data.shape[-1] / np.count_nonzero(tmp)\n        return data\n\n    def _label(self, a=0, b=20, c=40):\n        'Used for triangolar labeling'\n\n        z = np.linspace(a, c, num = 2*(b-a)+1)\n        y = np.zeros(z.shape)\n        y[z <= a] = 0\n        y[z >= c] = 0\n        first_half = np.logical_and(a < z, z <= b)\n        y[first_half] = (z[first_half]-a) / (b-a)\n        second_half = np.logical_and(b < z, z < c)\n        y[second_half] = (c-z[second_half]) / (c-b)\n        return y\n\n    def _gaussian_label(self, spt, dim, half_window=20):\n        'Used for Gaussian labeling'\n        y = np.zeros((dim, 1))\n        if spt and (spt - half_window >= 0) : #and (spt + half_window < dim)\n            y[spt - half_window:spt + half_window, 0] = np.exp(-(np.arange(spt - half_window, spt + half_window) - spt) ** 2 / (2 * (10) ** 2))[:dim - (spt - half_window)]\n        elif spt and (spt - half_window < dim):\n            y[0:spt + half_window, 0] = np.exp(-(np.arange(0, spt + half_window) - spt) ** 2 / (2 * (10) ** 2))[:dim - (spt - half_window)]\n        return y\n    def _add_event(self, data, addp, adds, coda_end, snr, rate):\n        'Add a scaled version of the event into the empty part of the trace'\n\n        added = np.copy(data)\n        additions = None\n        spt_secondEV = None\n        sst_secondEV = None\n        if addp and adds:\n            s_p = adds - addp\n            if np.random.uniform(0, 1) < rate and all(snr>=10.0) and (data.shape[0]-s_p-21-coda_end) > 20:\n                secondEV_strt = np.random.randint(coda_end, data.shape[0]-s_p-21)\n                scaleAM = 1/np.random.randint(1, 10)\n                space = data.shape[0]-secondEV_strt\n                added[secondEV_strt:secondEV_strt+space, 0] += data[addp:addp+space, 0]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 1] += data[addp:addp+space, 1]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 2] += data[addp:addp+space, 2]*scaleAM\n                spt_secondEV = secondEV_strt\n                if  spt_secondEV + s_p + 21 <= data.shape[0]:\n                    sst_secondEV = spt_secondEV + s_p\n                if spt_secondEV and sst_secondEV:\n                    additions = [spt_secondEV, sst_secondEV]\n                    data = added\n\n        return data, additions\n\n    def _new_shift_event(self, data, arrivals, rate=0.45):\n        'Randomly rotate the array to shift the event location'\n        org_len = len(data)\n        data2 = np.copy(data)\n        if np.random.uniform(0, 1) < rate:\n            nrotate = int(np.random.uniform(1, int(org_len)))\n            data2 = np.roll(data, nrotate, axis=0)\n            for idx in range(len(arrivals)):\n                if ~np.isnan(arrivals[idx]):\n                    arrivals[idx] = (arrivals[idx] + nrotate) % org_len\n        return data2, arrivals\n\n    def _shift_event(self, data, addp, adds, coda_end, snr, rate):\n        'Randomly rotate the array to shift the event location'\n\n        org_len = len(data)\n        data2 = np.copy(data)\n        addp2 = adds2 = coda_end2 = None;\n        if np.random.uniform(0, 1) < rate:\n            nrotate = int(np.random.uniform(1, int(org_len - coda_end)))\n            data2[:, 0] = list(data[:, 0])[-nrotate:] + list(data[:, 0])[:-nrotate]\n            data2[:, 1] = list(data[:, 1])[-nrotate:] + list(data[:, 1])[:-nrotate]\n            data2[:, 2] = list(data[:, 2])[-nrotate:] + list(data[:, 2])[:-nrotate]\n\n            if addp+nrotate >= 0 and addp+nrotate < org_len:\n                addp2 = addp+nrotate;\n            else:\n                addp2 = None;\n    def _shift_event(self, data, addp, adds, coda_end, snr, rate):\n        'Randomly rotate the array to shift the event location'\n\n        org_len = len(data)\n        data2 = np.copy(data)\n        addp2 = adds2 = coda_end2 = None;\n        if np.random.uniform(0, 1) < rate:\n            nrotate = int(np.random.uniform(1, int(org_len - coda_end)))\n            data2[:, 0] = list(data[:, 0])[-nrotate:] + list(data[:, 0])[:-nrotate]\n            data2[:, 1] = list(data[:, 1])[-nrotate:] + list(data[:, 1])[:-nrotate]\n            data2[:, 2] = list(data[:, 2])[-nrotate:] + list(data[:, 2])[:-nrotate]\n\n            if addp+nrotate >= 0 and addp+nrotate < org_len:\n                addp2 = addp+nrotate;\n            else:\n                addp2 = None;\n            if adds+nrotate >= 0 and adds+nrotate < org_len:\n                adds2 = adds+nrotate;\n            else:\n                adds2 = None;\n            if coda_end+nrotate < org_len:\n                coda_end2 = coda_end+nrotate\n            else:\n                coda_end2 = org_len\n            if addp2 and adds2:\n                data = data2;\n                addp = addp2;\n                adds = adds2;\n                coda_end= coda_end2;\n        return data, addp, adds, coda_end\n\n    def _pre_emphasis(self, data, pre_emphasis=0.97):\n        'apply the pre_emphasis'\n\n        for ch in range(self.n_channels):\n            bpf = data[:, ch]\n            data[:, ch] = np.append(bpf[0], bpf[1:] - pre_emphasis * bpf[:-1])\n        return data\n\n    def __data_generation(self, list_IDs_temp):\n        '''read the waveforms and generate the samples\n        return:\n            X: waveforms\n            y: labels\n        '''\n        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n        y1 = np.zeros((self.batch_size, self.dim, 1))  # label for the detector of the event\n        y2 = np.zeros((self.batch_size, self.dim, 1))  # label for the P arrival\n        y3 = np.zeros((self.batch_size, self.dim, 1))  # label for the S arrival\n        y4 = np.zeros((self.batch_size, self.dim, 1))  # label for the Pn arrival\n        y5 = np.zeros((self.batch_size, self.dim, 1))  # label for the Sn arrival\n        y6 = np.zeros((self.batch_size, self.dim, 1))  # label for the Pg arrival\n        y7 = np.zeros((self.batch_size, self.dim, 1))  # label for the Sg arrival\n        fl = h5py.File(self.file_name, 'r')\n\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            additions = None\n            dataset = fl.get('/data/'+str(ID))\n            # Hao Apr 6 2022\n            # if ID.split('_')[-1] == 'NO':\n            #     data = np.array(dataset)\n            # else:\n            #     # Hao revised\n            #     data = np.array(dataset)\n            #     spt = int(dataset.attrs['p_arrival_sample'])\n            #     sst = dataset.attrs['s_arrival_sample']\n            #     sst = None\n            #     coda_end = spt + 100\n            #     snr = dataset.attrs['snr_db']\n            # self.augmentation = False\n            if ID.split('_')[-1] == 'EV':\n                data = np.array(dataset)\n                # dat_channel: int, numbers of channel\n                # dat_dim: int, numbers of dimention\n                if data.ndim == 1:\n                    # original trace could be 1-component, i.e., (6000,) or (6000)\n                    dat_channel = 1\n                    dat_dim = len(data)\n                else:\n                    # convert data format to the one that is used in the prediction\n                    if data.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\n                        data = np.transpose(data)\n                    # more than 1 component trace\n                    dat_channel = data.shape[1]\n                    dat_dim = data.shape[0]\n                if dat_channel > self.n_channels:\n                    dat_channel = self.n_channels\n                # Load P, S arrival time from new STEAD format\n                try:\n                    arrivals = dataset.attrs['p_pn_pg_s_sn_sg']\n                except:\n                    arrivals = np.array([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n                # Load P, S arrival time from original STEAD format\n                try:\n                    arrivals[0] = int(dataset.attrs['p_arrival_sample'])\n                    arrivals[3] = int(dataset.attrs['s_arrival_sample'])\n                except:\n                    pass\n                # load SNR from the dataset; Some dataset does not have SNR\n                try:\n                    snr = dataset.attrs['snr_db']\n                except:\n                    snr = 0\n                # check data shape\n                temp = data\n                data = np.zeros((self.dim, self.n_channels))\n                attr_value = dataset.attrs.get('component', None)\n                if attr_value is not None:\n                    # label contains component information\n                    if attr_value == 'Z':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 2] = temp\n                        else:\n                            data[:, 2] = temp[:self.dim]\n                    if attr_value == 'N' or attr_value == '2':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 1] = temp\n                        else:\n                            data[:, 1] = temp[:self.dim]\n                    if attr_value == 'E' or attr_value == '0':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 0] = temp\n                        else:\n                            data[:, 0] = temp[:self.dim]\n                else:\n                    if dat_channel == 1:\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0]] = temp\n                        else:\n                            data[:, 0] = temp[:self.dim]\n                    else:\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 0:dat_channel] = temp[:, 0:dat_channel]\n                        else:\n                            data[:, 0:dat_channel] = temp[:self.dim, 0:dat_channel]\n                # if dat_dim < self.dim:\n                #     # option 1: duplicate a original clip\n                #     # duplicate_len = int(self.dim - data.shape[0])\n                #     # data = np.concatenate((data, data[0:duplicate_len, :]))\n                #     # option 2: padding with zero (recommended)\n                #     temp = data\n                #     data = np.zeros((self.dim, self.n_channels))\n                #     data[:dat_dim,:dat_channel] = temp\n                # else:\n                #     if dat_dim > self.dim:\n                #         data = data[0:self.dim, :]\n                #         # delete arrivals after the end of the data\n                #         for i in range(len(arrivals)):\n                #             if arrivals[i] > self.dim:\n                #\n                #                 arrivals[i] = np.nan\n                for i in range(len(arrivals)):\n                    if arrivals[i] > self.dim:\n                        arrivals[i] = np.nan\n                    if arrivals[i] <= 0:\n                        arrivals[i] = np.nan\n            elif ID.split('_')[-1] == 'NO':\n                data = np.array(dataset)\n                # dat_channel: int, numbers of channel\n                # dat_dim: int, numbers of dimention\n                # if data.ndim == 1:\n                #     # original trace could be 1-component, i.e., (6000,) or (6000)\n                #     dat_channel = 1\n                #     dat_dim = len(data)\n                # else:\n                #     # convert data format to the one that is used in the prediction\n                #     if data.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\n                #         data = np.transpose(data)\n                #     # more than 1 component trace\n                #     dat_channel = data.shape[1]\n                #     dat_dim = data.shape[0]\n                # # check data shape\n                # if dat_dim < self.dim:\n                #     # option 1: duplicate a original clip\n                #     # duplicate_len = int(self.dim - data.shape[0])\n                #     # data = np.concatenate((data, data[0:duplicate_len, :]))\n                #     # option 2: padding with zero\n                #     temp = data\n                #     data = np.zeros((self.dim, self.n_channels))\n                #     data[:dat_dim, :dat_channel] = temp\n                # if dat_dim > self.dim:\n                #         data = data[0:self.dim, :]\n                if data.ndim == 1:\n                    # original trace could be 1-component, i.e., (6000,) or (6000)\n                    dat_channel = 1\n                    dat_dim = len(data)\n                else:\n                    # convert data format to the one that is used in the prediction\n                    if data.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\n                        data = np.transpose(data)\n                    # more than 1 component trace\n                    dat_channel = data.shape[1]\n                    dat_dim = data.shape[0]\n                if dat_channel > self.n_channels:\n                    dat_channel = self.n_channels\n                # check data shape\n                temp = data\n                data = np.zeros((self.dim, self.n_channels))\n                attr_value = dataset.attrs.get('component', None)\n                if attr_value is not None:\n                    # label contains component information\n                    if attr_value == 'Z':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 2] = temp\n                        else:\n                            data[:, 2] = temp[:self.dim]\n                    if attr_value == 'N' or attr_value == '2':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 1] = temp\n                        else:\n                            data[:, 1] = temp[:self.dim]\n                    if attr_value == 'E' or attr_value == '0':\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 0] = temp\n                        else:\n                            data[:, 0] = temp[:self.dim]\n                else:\n                    if dat_channel == 1:\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0]] = temp\n                        else:\n                            data[:, 0] = temp[:self.dim]\n                    else:\n                        if temp.shape[0] < self.dim:\n                            data[:temp.shape[0], 0:dat_channel] = temp[:, 0:dat_channel]\n                        else:\n                            data[:, 0:dat_channel] = temp[:self.dim, 0:dat_channel]\n            # # enforce duplicate the data to the required n_channels\n            # if dat_channel < self.n_channels:\n            #     temp = data\n            #     data = np.zeros((self.dim, self.n_channels))\n            #     if dat_channel == 1:\n            #         data[:, 0] = temp.flatten()\n            #     else:\n            #         data[:, 0:dat_channel] = temp\n            #     # option 1 : duplicate channels\n            #     # if dat_channel < self.n_channels:\n            #     #    for i in range(dat_channel, temp.shape[1]):\n            #     #        data[:, i] = data[:, 0]\n            #     # option 2: padding with zeros\n            # if dat_channel > self.n_channels:\n            #      # trim channel to required size\n            #     data = data[:self.dim,:self.n_channels]\n            ## augmentation\n            if self.augmentation == True:\n                if i <= self.batch_size//2:\n                    if self.shift_event_r and 'earthquake' in dataset.attrs['trace_category']:\n                        data, spt, sst, coda_end = self._shift_event(data, spt, sst, coda_end, snr, self.shift_event_r/2);\n                    if self.norm_mode:\n                        data = self._normalize(data, self.norm_mode)\n                else:\n                    if 'earthquake' in dataset.attrs['trace_category']:\n                        if self.shift_event_r:\n                            data, spt, sst, coda_end = self._shift_event(data, spt, sst, coda_end, snr, self.shift_event_r);\n\n                        if self.add_event_r:\n                            data, additions = self._add_event(data, spt, sst, coda_end, snr, self.add_event_r);\n\n                        if self.add_noise_r:\n                            data = self._add_noise(data, snr, self.add_noise_r);\n\n                        if self.drop_channe_r:\n                            data = self._drop_channel(data, snr, self.drop_channe_r);\n                            data = self._adjust_amplitude_for_multichannels(data)\n\n                        if self.scale_amplitude_r:\n                            data = self._scale_amplitude(data, self.scale_amplitude_r);\n\n                        if self.pre_emphasis:\n                            data = self._pre_emphasis(data)\n\n                        if self.norm_mode:\n                            data = self._normalize(data, self.norm_mode)\n\n                    elif dataset.attrs['trace_category'] == 'noise':\n                        if self.drop_channe_r:\n                            data = self._drop_channel_noise(data, self.drop_channe_r);\n\n                        if self.add_gap_r:\n                            data = self._add_gaps(data, self.add_gap_r)\n\n                        if self.norm_mode:\n                            data = self._normalize(data, self.norm_mode)\n\n            elif self.augmentation == False:\n                if self.shift_event_r and ID.split('_')[-1] == 'EV':\n                    #data = self._shift_event(data, self.shift_event_r/2);\n                    # Hao Simplify random_shift function Aug 23 2022\n                    data, arrivals = self._new_shift_event(data, arrivals, self.shift_event_r/2);\n                if self.norm_mode:\n                    data = self._normalize(data, self.norm_mode)\n\n            X[i, :, :] = data[:,:self.n_channels]\n\n            ## labeling\n            #if 'earthquake' in dataset.attrs['trace_category']:\n            if ID.split('_')[-1] == 'EV':\n                if self.label_type == 'gaussian':\n                    for phase in self.phase_type:\n                        if phase == 'P':\n                            if ~np.isnan(arrivals[0]):\n                                y2[i] = self._gaussian_label(int(arrivals[0]), self.dim, self.phase_window //2)\n                        elif phase == 'S':\n                            if ~np.isnan(arrivals[3]):\n                                y3[i] = self._gaussian_label(int(arrivals[3]), self.dim, self.phase_window // 2)\n                        elif phase == 'Pn':\n                            if ~np.isnan(arrivals[1]):\n                                y4[i] = self._gaussian_label(int(arrivals[1]), self.dim, self.phase_window //2)\n                        elif phase == 'Sn':\n                            if ~np.isnan(arrivals[4]):\n                                y5[i] = self._gaussian_label(int(arrivals[4]), self.dim, self.phase_window //2)\n                        elif phase == 'Pg':\n                            if ~np.isnan(arrivals[2]):\n                                y6[i] = self._gaussian_label(int(arrivals[2]), self.dim, self.phase_window //2)\n                        elif phase == 'Sg':\n                            if ~np.isnan(arrivals[5]):\n                                y7[i] = self._gaussian_label(int(arrivals[5]), self.dim, self.phase_window //2)\n                    if 'd' in self.phase_type:\n                        y1[i] = y2[i] + y3[i] + y4[i] + y5[i] + y6[i] + y7[i]\n                elif self.label_type  == 'triangle':\n                    sd = None\n                    if spt and sst:\n                        sd = sst - spt\n\n                    if sd and sst:\n                        if sst+int(0.4*sd) <= self.dim:\n                            y1[i, spt:int(sst+(0.4*sd)), 0] = 1\n                        else:\n                            y1[i, spt:self.dim, 0] = 1\n\n                    if spt and (spt-20 >= 0) and (spt+21 < self.dim):\n                        y2[i, spt-20:spt+21, 0] = self._label()\n                    elif spt and (spt+21 < self.dim):\n                        y2[i, 0:spt+spt+1, 0] = self._label(a=0, b=spt, c=2*spt)\n                    elif spt and (spt-20 >= 0):\n                        pdif = self.dim - spt\n                        y2[i, spt-pdif-1:self.dim, 0] = self._label(a=spt-pdif, b=spt, c=2*pdif)\n\n                    if sst and (sst-20 >= 0) and (sst+21 < self.dim):\n                        y3[i, sst-20:sst+21, 0] = self._label()\n                    elif sst and (sst+21 < self.dim):\n                        y3[i, 0:sst+sst+1, 0] = self._label(a=0, b=sst, c=2*sst)\n                    elif sst and (sst-20 >= 0):\n                        sdif = self.dim - sst\n                        y3[i, sst-sdif-1:self.dim, 0] = self._label(a=sst-sdif, b=sst, c=2*sdif)\n\n                    if additions:\n                        add_spt = additions[0];\n                        add_sst = additions[1];\n                        add_sd = None\n                        if add_spt and add_sst:\n                            add_sd = add_sst - add_spt\n\n                        if add_sd and add_sst+int(0.4*add_sd) <= self.dim:\n                            y1[i, add_spt:int(add_sst+(0.4*add_sd)), 0] = 1\n                        else:\n                            y1[i, add_spt:self.dim, 0] = 1\n\n                        if add_spt and (add_spt-20 >= 0) and (add_spt+21 < self.dim):\n                            y2[i, add_spt-20:add_spt+21, 0] = self._label()\n                        elif add_spt and (add_spt+21 < self.dim):\n                            y2[i, 0:add_spt+add_spt+1, 0] = self._label(a=0, b=add_spt, c=2*add_spt)\n                        elif add_spt and (add_spt-20 >= 0):\n                            pdif = self.dim - add_spt\n                            y2[i, add_spt-pdif-1:self.dim, 0] = self._label(a=add_spt-pdif, b=add_spt, c=2*pdif)\n\n                        if add_sst and (add_sst-20 >= 0) and (add_sst+21 < self.dim):\n                            y3[i, add_sst-20:add_sst+21, 0] = self._label()\n                        elif add_sst and (add_sst+21 < self.dim):\n                            y3[i, 0:add_sst+add_sst+1, 0] = self._label(a=0, b=add_sst, c=2*add_sst)\n                        elif add_sst and (add_sst-20 >= 0):\n                            sdif = self.dim - add_sst\n                            y3[i, add_sst-sdif-1:self.dim, 0] = self._label(a=add_sst-sdif, b=add_sst, c=2*sdif)\n\n\n                elif self.label_type  == 'box':\n                    sd = None\n                    if sst and spt:\n                        sd = sst - spt\n\n                    if sd and sst+int(0.4*sd) <= self.dim:\n                        y1[i, spt:int(sst+(0.4*sd)), 0] = 1\n                    else:\n                        y1[i, spt:self.dim, 0] = 1\n                    if spt:\n                        y2[i, spt-20:spt+20, 0] = 1\n                    if sst:\n                        y3[i, sst-20:sst+20, 0] = 1\n\n                    if additions:\n                        add_sd = None\n                        add_spt = additions[0];\n                        add_sst = additions[1];\n                        if add_spt and add_sst:\n                            add_sd = add_sst - add_spt\n\n                        if add_sd and add_sst+int(0.4*add_sd) <= self.dim:\n                            y1[i, add_spt:int(add_sst+(0.4*add_sd)), 0] = 1\n                        else:\n                            y1[i, add_spt:self.dim, 0] = 1\n                        if add_spt:\n                            y2[i, add_spt-20:add_spt+20, 0] = 1\n                        if add_sst:\n                            y3[i, add_sst-20:add_sst+20, 0] = 1\n\n        fl.close()\n        input_dict = {'input': X}\n        output_dict = {}\n        # {'detector': y1, 'picker_P': y2, 'picker_S': y3}\n        if 'd' in self.phase_type or 'D' in self.phase_type:\n            # add detector channel\n            output_name = 'detector'\n            output_dict[output_name] = y1.astype('float32')\n        for picker_name in self.phase_type:\n            if picker_name == 'P':\n                # add P-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y2.astype('float32')\n            if picker_name == 'S':\n                # add S-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y3.astype('float32')\n            if picker_name == 'Pn':\n                # add Pn-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y4.astype('float32')\n            if picker_name == 'Sn':\n                # add Sn-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y5.astype('float32')\n            if picker_name == 'Pg':\n                # add Pg-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y6.astype('float32')\n            if picker_name == 'Sg':\n                # add Sg-type output channel\n                output_name = 'picker_' + picker_name\n                output_dict[output_name] = y7.astype('float32')\n        return input_dict, output_dict\n\n\n\n\n\nclass PreLoadGenerator(keras.utils.Sequence):\n\n    \"\"\"\n    Keras generator with preprocessing. Pre-load version.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    inp_data: dic\n        A dictionary of input hdf5 datasets.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32\n        Batch size.\n\n    n_channels: int, default=3\n        Number of channels.\n\n    phase_window: int, fixed=40\n        The number of samples (window) around each phaset.\n\n    shuffle: bool, default=True\n        Shuffeling the list.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'.\n\n    label_type: str, default=gaussian\n        Labeling type. 'gaussian', 'triangle', or 'box'.\n\n    augmentation: bool, default=True\n        If True, half of each batch will be augmented version of the other half.\n\n    add_event_r: {float, None}, default=None\n        Chance for randomly adding a second event into the waveform.\n\n    add_gap_r: {float, None}, default=None\n        Add an interval with zeros into the waveform representing filled gaps.\n\n    shift_event_r: {float, None}, default=0.9\n        Rate of augmentation for randomly shifting the event within a trace.\n\n    add_noise_r: {float, None}, default=None\n        Chance for randomly adding Gaussian noise into the waveform.\n\n    drop_channe_r: {float, None}, default=None\n        Chance for randomly dropping some of the channels.\n\n    scale_amplitude_r: {float, None}, default=None\n        Chance for randomly amplifying the waveform amplitude.\n\n    pre_emphasis: bool, default=False\n        If True, waveforms will be pre emphasized.\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n    \"\"\"\n\n    def __init__(self,\n                 inp_data,\n                 list_IDs,\n                 file_name,\n                 dim,\n                 batch_size=32,\n                 n_channels=3,\n                 phase_window= 40,\n                 shuffle=True,\n                 norm_mode = 'max',\n                 label_type = 'gaussian',\n                 augmentation = False,\n                 add_event_r = None,\n                 add_gap_r = None,\n                 shift_event_r = None,\n                 add_noise_r = None,\n                 drop_channe_r = None,\n                 scale_amplitude_r = None,\n                 pre_emphasis = True):\n\n        'Initialization'\n        self.inp_data =inp_data\n        self.dim = dim\n        self.batch_size = batch_size\n        self.phase_window = phase_window\n        self.list_IDs = list_IDs\n        self.file_name = file_name\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n        self.label_type = label_type\n        self.augmentation = augmentation\n        self.add_event_r = add_event_r\n        self.add_gap_r = add_gap_r\n        self.shift_event_r = shift_event_r\n        self.add_noise_r = add_noise_r\n        self.drop_channe_r = drop_channe_r\n        self.scale_amplitude_r = scale_amplitude_r\n        self.pre_emphasis = pre_emphasis\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n\n        if self.augmentation:\n            return 2*int(np.floor(len(self.list_IDs) / self.batch_size))\n        else:\n            return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n\n        if self.augmentation:\n            indexes = self.indexes[index*self.batch_size//2:(index+1)*self.batch_size//2]\n            indexes = np.append(indexes, indexes)\n        else:\n            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X, y1, y2, y3 = self.__data_generation(list_IDs_temp)\n        return ({'input': X}, {'detector': y1, 'picker_P': y2, 'picker_S': y3})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def _normalize(self, data, mode = 'max'):\n        'Normalize waveforms in each batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def _scale_amplitude(self, data, rate):\n        'Scale amplitude or waveforms'\n\n        tmp = np.random.uniform(0, 1)\n        if tmp < rate:\n            data *= np.random.uniform(1, 3)\n        elif tmp < 2*rate:\n            data /= np.random.uniform(1, 3)\n        return data\n\n    def _drop_channel(self, data, snr, rate):\n        'Randomly replace values of one or two components to zeros in earthquake data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate and all(snr >= 10):\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _drop_channel_noise(self, data, rate):\n        'Randomly replace values of one or two components to zeros in noise data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate:\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _add_gaps(self, data, rate):\n        'Randomly add gaps (zeros) of different sizes into waveforms'\n\n        data = np.copy(data)\n        gap_start = np.random.randint(0, 4000)\n        gap_end = np.random.randint(gap_start, 5500)\n        if np.random.uniform(0, 1) < rate:\n            data[gap_start:gap_end,:] = 0\n        return data\n\n    def _add_noise(self, data, snr, rate):\n        'Randomly add Gaussian noie with a random SNR into waveforms'\n\n        data_noisy = np.empty((data.shape))\n        if np.random.uniform(0, 1) < rate and all(snr >= 10.0):\n            data_noisy = np.empty((data.shape))\n            data_noisy[:, 0] = data[:,0] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,0]), data.shape[0])\n            data_noisy[:, 1] = data[:,1] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,1]), data.shape[0])\n            data_noisy[:, 2] = data[:,2] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,2]), data.shape[0])\n        else:\n            data_noisy = data\n        return data_noisy\n\n    def _adjust_amplitude_for_multichannels(self, data):\n        'Adjust the amplitude of multichaneel data'\n\n        tmp = np.max(np.abs(data), axis=0, keepdims=True)\n        assert(tmp.shape[-1] == data.shape[-1])\n        if np.count_nonzero(tmp) > 0:\n          data *= data.shape[-1] / np.count_nonzero(tmp)\n        return data\n\n    def _label(self, a=0, b=20, c=40):\n        'Used for triangolar labeling'\n\n        z = np.linspace(a, c, num = 2*(b-a)+1)\n        y = np.zeros(z.shape)\n        y[z <= a] = 0\n        y[z >= c] = 0\n        first_half = np.logical_and(a < z, z <= b)\n        y[first_half] = (z[first_half]-a) / (b-a)\n        second_half = np.logical_and(b < z, z < c)\n        y[second_half] = (c-z[second_half]) / (c-b)\n        return y\n\n    def _add_event(self, data, addp, adds, coda_end, snr, rate):\n        'Add a scaled version of the event into the empty part of the trace'\n\n        added = np.copy(data)\n        additions = None\n        spt_secondEV = None\n        sst_secondEV = None\n        if addp and adds:\n            s_p = adds - addp\n            if np.random.uniform(0, 1) < rate and all(snr >= 10.0) and (data.shape[0]-s_p-21-coda_end) > 20:\n                secondEV_strt = np.random.randint(coda_end, data.shape[0]-s_p-21)\n                scaleAM = 1/np.random.randint(1, 10)\n                space = data.shape[0]-secondEV_strt\n                added[secondEV_strt:secondEV_strt+space, 0] += data[addp:addp+space, 0]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 1] += data[addp:addp+space, 1]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 2] += data[addp:addp+space, 2]*scaleAM\n                spt_secondEV = secondEV_strt\n                if  spt_secondEV + s_p + 21 <= data.shape[0]:\n                    sst_secondEV = spt_secondEV + s_p\n                if spt_secondEV and sst_secondEV:\n                    additions = [spt_secondEV, sst_secondEV]\n                    data = added\n\n        return data, additions\n\n\n    def _shift_event(self, data, addp, adds, coda_end, snr, rate):\n        'Randomly rotate the array to shift the event location'\n\n        org_len = len(data)\n        data2 = np.copy(data)\n        addp2 = adds2 = coda_end2 = None;\n        if np.random.uniform(0, 1) < rate:\n            nrotate = int(np.random.uniform(1, int(org_len - coda_end)))\n            data2[:, 0] = list(data[:, 0])[-nrotate:] + list(data[:, 0])[:-nrotate]\n            data2[:, 1] = list(data[:, 1])[-nrotate:] + list(data[:, 1])[:-nrotate]\n            data2[:, 2] = list(data[:, 2])[-nrotate:] + list(data[:, 2])[:-nrotate]\n\n            if addp+nrotate >= 0 and addp+nrotate < org_len:\n                addp2 = addp+nrotate;\n            else:\n                addp2 = None;\n            if adds+nrotate >= 0 and adds+nrotate < org_len:\n                adds2 = adds+nrotate;\n            else:\n                adds2 = None;\n            if coda_end+nrotate < org_len:\n                coda_end2 = coda_end+nrotate\n            else:\n                coda_end2 = org_len\n            if addp2 and adds2:\n                data = data2;\n                addp = addp2;\n                adds = adds2;\n                coda_end= coda_end2;\n        return data, addp, adds, coda_end\n\n\n\n    def _pre_emphasis(self, data, pre_emphasis=0.97):\n        'apply the pre_emphasis'\n\n        for ch in range(self.n_channels):\n            bpf = data[:, ch]\n            data[:, ch] = np.append(bpf[0], bpf[1:] - pre_emphasis * bpf[:-1])\n        return data\n\n    def __data_generation(self, list_IDs_temp):\n        'readint the waveforms'\n        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n        y1 = np.zeros((self.batch_size, self.dim, 1))\n        y2 = np.zeros((self.batch_size, self.dim, 1))\n        y3 = np.zeros((self.batch_size, self.dim, 1))\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            additions = None\n            dataset = self.inp_data[ID]\n            data = np.array(dataset)\n            if dataset.attrs['trace_category'] == 'earthquake_local':\n                spt = int(dataset.attrs['p_arrival_sample']);\n                sst = int(dataset.attrs['s_arrival_sample']);\n                coda_end = int(dataset.attrs['coda_end_sample']);\n                snr = dataset.attrs['snr_db'];\n\n            if self.augmentation == True:\n                if i <= self.batch_size//2:\n                    if self.shift_event_r and dataset.attrs['trace_category'] == 'earthquake_local':\n                        data, spt, sst, coda_end = self._shift_event(data, spt, sst, coda_end, snr, self.shift_event_r/2);\n                    if self.norm_mode:\n                        data = self._normalize(data, self.norm_mode)\n                else:\n                    if dataset.attrs['trace_category'] == 'earthquake_local':\n                        if self.shift_event_r and spt:\n                            data, spt, sst, coda_end = self._shift_event(data, spt, sst, coda_end, snr, self.shift_event_r);\n\n                        if self.add_event_r and spt and sst:\n                            data, additions = self._add_event(data, spt, sst, coda_end, snr, self.add_event_r);\n\n                        if self.add_noise_r:\n                            data = self._add_noise(data, snr, self.add_noise_r);\n\n                        if self.drop_channe_r:\n                            data = self._drop_channel(data, snr, self.drop_channe_r);\n                            data = self._adjust_amplitude_for_multichannels(data)\n\n                        if self.scale_amplitude_r:\n                            data = self._scale_amplitude(data, self.scale_amplitude_r);\n\n                        if self.pre_emphasis:\n                            data = self._pre_emphasis(data)\n\n                        if self.norm_mode:\n                            data = self._normalize(data, self.norm_mode)\n\n                    elif dataset.attrs['trace_category'] == 'noise':\n                        if self.drop_channe_r:\n                            data = self._drop_channel_noise(data, self.drop_channe_r);\n\n                        if self.add_gap_r:\n                            data = self._add_gaps(data, self.add_gap_r)\n\n                        if self.norm_mode:\n                            data = self._normalize(data, self.norm_mode)\n\n            elif self.augmentation == False:\n                if self.shift_event_r and dataset.attrs['trace_category'] == 'earthquake_local':\n                    data, spt, sst, coda_end = self._shift_event(data, spt, sst, coda_end, snr, self.shift_event_r/2);\n                if self.norm_mode:\n                    data = self._normalize(data, self.norm_mode)\n\n            X[i, :, :] = data\n            ## labeling\n            if dataset.attrs['trace_category'] == 'earthquake_local':\n\n                if self.label_type  == 'gaussian':\n                    sd = None\n                    if spt and sst:\n                        sd = sst - spt\n\n                    if sd and sst:\n                        if sst+int(1.4*sd) <= self.dim:\n                            y1[i, spt:int(sst+(1.4*sd)), 0] = 1\n                        else:\n                            y1[i, spt:self.dim, 0] = 1\n\n                    if spt and (spt-20 >= 0) and (spt+20 < self.dim):\n                        y2[i, spt-20:spt+20, 0] = np.exp(-(np.arange(spt-20,spt+20)-spt)**2/(2*(10)**2))[:self.dim-(spt-20)]\n                    elif spt and (spt-20 < self.dim):\n                        y2[i, 0:spt+20, 0] = np.exp(-(np.arange(0,spt+20)-spt)**2/(2*(10)**2))[:self.dim-(spt-20)]\n\n                    if sst and (sst-20 >= 0) and (sst-20 < self.dim):\n                        y3[i, sst-20:sst+20, 0] = np.exp(-(np.arange(sst-20,sst+20)-sst)**2/(2*(10)**2))[:self.dim-(sst-20)]\n                    elif sst and (sst-20 < self.dim):\n                        y3[i, 0:sst+20, 0] = np.exp(-(np.arange(0,sst+20)-sst)**2/(2*(10)**2))[:self.dim-(sst-20)]\n\n                    if additions:\n                        add_spt = additions[0];\n                        add_sst = additions[1];\n                        add_sd = None\n\n                        if add_spt and add_sst:\n                            add_sd = add_sst - add_spt\n\n                        if add_sd and add_sst+int(1.4*add_sd) <= self.dim:\n                            y1[i, add_spt:int(add_sst+(1.4*add_sd)), 0] = 1\n                        else:\n                            y1[i, add_spt:self.dim, 0] = 1\n\n                        if add_spt and (add_spt-20 >= 0) and (add_spt+20 < self.dim):\n                            y2[i, add_spt-20:add_spt+20, 0] = np.exp(-(np.arange(add_spt-20,add_spt+20)-add_spt)**2/(2*(10)**2))[:self.dim-(add_spt-20)]\n                        elif add_spt and (add_spt+20 < self.dim):\n                            y2[i, 0:add_spt+20, 0] = np.exp(-(np.arange(0,add_spt+20)-add_spt)**2/(2*(10)**2))[:self.dim-(add_spt-20)]\n\n                        if add_sst and (add_sst-20 >= 0) and (add_sst+20 < self.dim):\n                            y3[i, add_sst-20:add_sst+20, 0] = np.exp(-(np.arange(add_sst-20,add_sst+20)-add_sst)**2/(2*(10)**2))[:self.dim-(add_sst-20)]\n                        elif add_sst and (add_sst+20 < self.dim):\n                            y3[i, 0:add_sst+20, 0] = np.exp(-(np.arange(0,add_sst+20)-add_sst)**2/(2*(10)**2))[:self.dim-(add_sst-20)]\n\n                elif self.label_type  == 'triangle':\n                    sd = None\n                    if spt and sst:\n                        sd = sst - spt\n\n                    if sd and sst:\n                        if sst+int(1.4*sd) <= self.dim:\n                            y1[i, spt:int(sst+(1.4*sd)), 0] = 1\n                        else:\n                            y1[i, spt:self.dim, 0] = 1\n\n                    if spt and (spt-20 >= 0) and (spt+21 < self.dim):\n                        y2[i, spt-20:spt+21, 0] = self._label()\n                    elif spt and (spt+21 < self.dim):\n                        y2[i, 0:spt+spt+1, 0] = self._label(a=0, b=spt, c=2*spt)\n                    elif spt and (spt-20 >= 0):\n                        pdif = self.dim - spt\n                        y2[i, spt-pdif-1:self.dim, 0] = self._label(a=spt-pdif, b=spt, c=2*pdif)\n\n                    if sst and (sst-20 >= 0) and (sst+21 < self.dim):\n                        y3[i, sst-20:sst+21, 0] = self._label()\n                    elif sst and (sst+21 < self.dim):\n                        y3[i, 0:sst+sst+1, 0] = self._label(a=0, b=sst, c=2*sst)\n                    elif sst and (sst-20 >= 0):\n                        sdif = self.dim - sst\n                        y3[i, sst-sdif-1:self.dim, 0] = self._label(a=sst-sdif, b=sst, c=2*sdif)\n\n                    if additions:\n                        add_spt = additions[0];\n                        add_sst = additions[1];\n                        add_sd = None\n\n                        if add_spt and add_sst:\n                            add_sd = add_sst - add_spt\n\n                        if add_sd and add_sst+int(1.4*add_sd) <= self.dim:\n                            y1[i, add_spt:int(add_sst+(1.4*add_sd)), 0] = 1\n                        else:\n                            y1[i, add_spt:self.dim, 0] = 1\n\n                        if add_spt and (add_spt-20 >= 0) and (add_spt+21 < self.dim):\n                            y2[i, add_spt-20:add_spt+21, 0] = self.label()\n                        elif add_spt and (add_spt+21 < self.dim):\n                            y2[i, 0:add_spt+add_spt+1, 0] = self.label(a=0, b=add_spt, c=2*add_spt)\n                        elif add_spt and (add_spt-20 >= 0):\n                            pdif = self.dim - add_spt\n                            y2[i, add_spt-pdif-1:self.dim, 0] = self.label(a=add_spt-pdif, b=add_spt, c=2*pdif)\n\n                        if add_sst and (add_sst-20 >= 0) and (add_sst+21 < self.dim):\n                            y3[i, add_sst-20:add_sst+21, 0] = self.label()\n                        elif add_sst and (add_sst+21 < self.dim):\n                            y3[i, 0:add_sst+add_sst+1, 0] = self.label(a=0, b=add_sst, c=2*add_sst)\n                        elif add_sst and (add_sst-20 >= 0):\n                            sdif = self.dim - add_sst\n                            y3[i, add_sst-sdif-1:self.dim, 0] = self.label(a=add_sst-sdif, b=add_sst, c=2*sdif)\n\n                elif self.label_type  == 'box':\n                    sd = None\n                    if sst and spt:\n                        sd = sst - spt\n\n                    if sd and sst+int(1.4*sd) <= self.dim:\n                        y1[i, spt:int(sst+(1.4*sd)), 0] = 1\n                    else:\n                        y1[i, spt:self.dim, 0] = 1\n\n                    if spt:\n                        y2[i, spt-20:spt+20, 0] = 1\n                    if sst:\n                        y3[i, sst-20:sst+20, 0] = 1\n\n                    if additions:\n                        add_sd = None\n                        add_spt = additions[0];\n                        add_sst = additions[1];\n                        if add_spt and add_sst:\n                            add_sd = add_sst - add_spt\n\n                        if add_sd and add_sst+int(1.4*add_sd) <= self.dim:\n                            y1[i, add_spt:int(add_sst+(1.4*add_sd)), 0] = 1\n                        else:\n                            y1[i, add_spt:self.dim, 0] = 1\n                        if add_spt:\n                            y2[i, add_spt-20:add_spt+20, 0] = 1\n                        if add_sst:\n                            y3[i, add_sst-20:add_sst+20, 0] = 1\n\n        return X.astype('float32'), y1.astype('float32'), y2.astype('float32'), y3.astype('float32')\n\n\n\ndef data_reader( list_IDs,\n                 file_name,\n                 dim=6000,\n                 n_channels=3,\n                 norm_mode='max',\n                 augmentation=False,\n                 add_event_r=None,\n                 add_gap_r=None,\n                 shift_event_r=None,\n                 add_noise_r=None,\n                 drop_channe_r=None,\n                 scale_amplitude_r=None,\n                 pre_emphasis=True):\n\n    \"\"\"\n\n    For pre-processing and loading of data into memory.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Path to the input hdf5 datasets.\n\n    dim: int, default=6000\n        Dimension of input traces, in sample.\n\n    n_channels: int, default=3\n        Number of channels.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'.\n\n    augmentation: bool, default=True\n        If True, half of each batch will be augmented version of the other half.\n\n    add_event_r: {float, None}, default=None\n        Chance for randomly adding a second event into the waveform.\n\n    shift_event_r: {float, None}, default=0.9\n        Rate of augmentation for randomly shifting the event within a trace.\n\n    add_noise_r: {float, None}, default=None\n        Chance for randomly adding Gaussian noise into the waveform.\n\n    drop_channe_r: {float, None}, default=None\n        Chance for randomly dropping some of the channels.\n\n    scale_amplitude_r: {float, None}, default=None\n        Chance for randomly amplifying the waveform amplitude.\n\n    pre_emphasis: bool, default=False\n        If True, waveforms will be pre emphasized.\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n    Note\n    -----\n    Label type is fixed to box.\n\n\n    \"\"\"\n\n    def _normalize( data, mode = 'max'):\n        'Normalize waveforms in each batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def _scale_amplitude( data, rate):\n        'Scale amplitude or waveforms'\n\n        tmp = np.random.uniform(0, 1)\n        if tmp < rate:\n            data *= np.random.uniform(1, 3)\n        elif tmp < 2*rate:\n            data /= np.random.uniform(1, 3)\n        return data\n\n    def _drop_channel( data, snr, rate):\n        'Randomly replace values of one or two components to zeros in earthquake data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate and all(snr >= 10):\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _drop_channel_noise(data, rate):\n        'Randomly replace values of one or two components to zeros in noise data'\n\n        data = np.copy(data)\n        if np.random.uniform(0, 1) < rate:\n            c1 = np.random.choice([0, 1])\n            c2 = np.random.choice([0, 1])\n            c3 = np.random.choice([0, 1])\n            if c1 + c2 + c3 > 0:\n                data[..., np.array([c1, c2, c3]) == 0] = 0\n        return data\n\n    def _add_gaps(data, rate):\n        'Randomly add gaps (zeros) of different sizes into waveforms'\n\n        data = np.copy(data)\n        gap_start = np.random.randint(0, 4000)\n        gap_end = np.random.randint(gap_start, 5500)\n        if np.random.uniform(0, 1) < rate:\n            data[gap_start:gap_end,:] = 0\n        return data\n\n    def _add_noise(data, snr, rate):\n        'Randomly add Gaussian noie with a random SNR into waveforms'\n\n        data_noisy = np.empty((data.shape))\n        if np.random.uniform(0, 1) < rate and all(snr >= 10.0):\n            data_noisy = np.empty((data.shape))\n            data_noisy[:, 0] = data[:,0] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,0]), data.shape[0])\n            data_noisy[:, 1] = data[:,1] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,1]), data.shape[0])\n            data_noisy[:, 2] = data[:,2] + np.random.normal(0, np.random.uniform(0.01, 0.15)*max(data[:,2]), data.shape[0])\n        else:\n            data_noisy = data\n        return data_noisy\n\n    def _adjust_amplitude_for_multichannels(data):\n        'Adjust the amplitude of multichaneel data'\n\n        tmp = np.max(np.abs(data), axis=0, keepdims=True)\n        assert(tmp.shape[-1] == data.shape[-1])\n        if np.count_nonzero(tmp) > 0:\n          data *= data.shape[-1] / np.count_nonzero(tmp)\n        return data\n\n    def _label(a=0, b=20, c=40):\n        'Used for triangolar labeling'\n\n        z = np.linspace(a, c, num = 2*(b-a)+1)\n        y = np.zeros(z.shape)\n        y[z <= a] = 0\n        y[z >= c] = 0\n        first_half = np.logical_and(a < z, z <= b)\n        y[first_half] = (z[first_half]-a) / (b-a)\n        second_half = np.logical_and(b < z, z < c)\n        y[second_half] = (c-z[second_half]) / (c-b)\n        return y\n\n    def _add_event(data, addp, adds, coda_end, snr, rate):\n        'Add a scaled version of the event into the empty part of the trace'\n\n        added = np.copy(data)\n        additions = spt_secondEV = sst_secondEV = None\n        if addp and adds:\n            s_p = adds - addp\n            if np.random.uniform(0, 1) < rate and all(snr >= 10.0) and (data.shape[0]-s_p-21-coda_end) > 20:\n                secondEV_strt = np.random.randint(coda_end, data.shape[0]-s_p-21)\n                scaleAM = 1/np.random.randint(1, 10)\n                space = data.shape[0]-secondEV_strt\n                added[secondEV_strt:secondEV_strt+space, 0] += data[addp:addp+space, 0]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 1] += data[addp:addp+space, 1]*scaleAM\n                added[secondEV_strt:secondEV_strt+space, 2] += data[addp:addp+space, 2]*scaleAM\n                spt_secondEV = secondEV_strt\n                if  spt_secondEV + s_p + 21 <= data.shape[0]:\n                    sst_secondEV = spt_secondEV + s_p\n                if spt_secondEV and sst_secondEV:\n                    additions = [spt_secondEV, sst_secondEV]\n                    data = added\n        return data, additions\n\n\n\n    def _shift_event(data, addp, adds, coda_end, snr, rate):\n        'Randomly rotate the array to shift the event location'\n\n        org_len = len(data)\n        data2 = np.copy(data)\n        addp2 = adds2 = coda_end2 = None;\n        if np.random.uniform(0, 1) < rate:\n            nrotate = int(np.random.uniform(1, int(org_len - coda_end)))\n            data2[:, 0] = list(data[:, 0])[-nrotate:] + list(data[:, 0])[:-nrotate]\n            data2[:, 1] = list(data[:, 1])[-nrotate:] + list(data[:, 1])[:-nrotate]\n            data2[:, 2] = list(data[:, 2])[-nrotate:] + list(data[:, 2])[:-nrotate]\n\n            if addp+nrotate >= 0 and addp+nrotate < org_len:\n                addp2 = addp+nrotate;\n            else:\n                addp2 = None;\n            if adds+nrotate >= 0 and adds+nrotate < org_len:\n                adds2 = adds+nrotate;\n            else:\n                adds2 = None;\n            if coda_end+nrotate < org_len:\n                coda_end2 = coda_end+nrotate\n            else:\n                coda_end2 = org_len\n            if addp2 and adds2:\n                data = data2;\n                addp = addp2;\n                adds = adds2;\n                coda_end= coda_end2;\n        return data, addp, adds, coda_end\n\n    def _pre_emphasis( data, pre_emphasis=0.97):\n        'apply the pre_emphasis'\n\n        for ch in range(n_channels):\n            bpf = data[:, ch]\n            data[:, ch] = np.append(bpf[0], bpf[1:] - pre_emphasis * bpf[:-1])\n        return data\n\n    fl = h5py.File(file_name, 'r')\n\n    if augmentation:\n        X = np.zeros((2*len(list_IDs), dim, n_channels))\n        y1 = np.zeros((2*len(list_IDs), dim, 1))\n        y2 = np.zeros((2*len(list_IDs), dim, 1))\n        y3 = np.zeros((2*len(list_IDs), dim, 1))\n    else:\n        X = np.zeros((len(list_IDs), dim, n_channels))\n        y1 = np.zeros((len(list_IDs), dim, 1))\n        y2 = np.zeros((len(list_IDs), dim, 1))\n        y3 = np.zeros((len(list_IDs), dim, 1))\n\n    # Generate data\n    pbar = tqdm(total=len(list_IDs))\n    for i, ID in enumerate(list_IDs):\n        pbar.update()\n\n        additions = None\n        dataset = fl.get('data/'+str(ID))\n\n        if ID.split('_')[-1] != 'NO':\n            data = np.array(dataset)\n            spt = int(dataset.attrs['p_arrival_sample']);\n            sst = int(dataset.attrs['s_arrival_sample']);\n            coda_end = int(dataset.attrs['coda_end_sample']);\n            snr = dataset.attrs['snr_db'];\n\n        elif ID.split('_')[-1] == 'NO':\n            data = np.array(dataset)\n\n        if augmentation:\n            if dataset.attrs['trace_category'] == 'earthquake_local':\n                data, spt, sst, coda_end = _shift_event(data, spt, sst, coda_end, snr, shift_event_r/2);\n            if norm_mode:\n                data1 = _normalize(data, norm_mode)\n\n            if dataset.attrs['trace_category'] == 'earthquake_local':\n                if shift_event_r and spt:\n                    data, spt, sst, coda_end = _shift_event(data, spt, sst, coda_end, snr, shift_event_r);\n\n                if add_event_r:\n                    data, additions = _add_event(data, spt, sst, coda_end, snr, add_event_r);\n\n                if drop_channe_r:\n                    data = _drop_channel(data, snr, drop_channe_r);\n                  #  data = _adjust_amplitude_for_multichannels(data);\n\n                if scale_amplitude_r:\n                    data = _scale_amplitude(data, scale_amplitude_r);\n\n                if pre_emphasis:\n                    data = _pre_emphasis(data);\n\n                if add_noise_r:\n                    data = _add_noise(data, snr, add_noise_r);\n\n                if norm_mode:\n                    data2 = _normalize(data, norm_mode);\n\n\n            if dataset.attrs['trace_category'] == 'noise':\n                if drop_channe_r:\n                    data = _drop_channel_noise(data, drop_channe_r);\n                if add_gap_r:\n                    data = _add_gaps(data, add_gap_r)\n                if norm_mode:\n                    data2 = _normalize(data, norm_mode)\n\n            X[i, :, :] = data1\n            X[len(list_IDs)+i, :, :] = data2\n\n            if dataset.attrs['trace_category'] == 'earthquake_local':\n\n                if spt and (spt-20 >= 0) and (spt+21 < dim):\n                    y2[i, spt-20:spt+21, 0] = _label()\n                    y2[len(list_IDs)+i, spt-20:spt+21, 0] = _label()\n                elif spt and (spt+21 < dim):\n                    y2[i, 0:spt+spt+1, 0] = _label(a=0, b=spt, c=2*spt)\n                    y2[len(list_IDs)+i, 0:spt+spt+1, 0] = _label(a=0, b=spt, c=2*spt)\n                elif spt and (spt-20 >= 0):\n                    pdif = dim - spt\n                    y2[i, spt-pdif-1:dim, 0] = _label(a=spt-pdif, b=spt, c=2*pdif)\n                    y2[len(list_IDs)+i, spt-pdif-1:dim, 0] = _label(a=spt-pdif, b=spt, c=2*pdif)\n\n                if sst and (sst-20 >= 0) and (sst+21 < dim):\n                    y3[i, sst-20:sst+21, 0] = _label()\n                    y3[len(list_IDs)+i, sst-20:sst+21, 0] = _label()\n                elif sst and (sst+21 < dim):\n                    y3[i, 0:sst+sst+1, 0] = _label(a=0, b=sst, c=2*sst)\n                    y3[len(list_IDs)+i, 0:sst+sst+1, 0] = _label(a=0, b=sst, c=2*sst)\n                elif sst and (sst-20 >= 0):\n                    sdif = dim - sst\n                    y3[i, sst-sdif-1:dim, 0] = _label(a=sst-sdif, b=sst, c=2*sdif)\n                    y3[len(list_IDs)+i, sst-sdif-1:dim, 0] = _label(a=sst-sdif, b=sst, c=2*sdif)\n\n                sd = sst - spt\n                if sst+int(0.4*sd) <= dim:\n                    y1[i, spt:int(sst+(0.4*sd)), 0] = 1\n                    y1[len(list_IDs)+i, spt:int(sst+(0.4*sd)), 0] = 1\n                else:\n                    y1[i, spt:dim, 0] = 1\n                    y1[len(list_IDs)+i, spt:dim, 0] = 1\n\n                if additions:\n                    add_spt = additions[0];\n                    print(add_spt)\n                    add_sst = additions[1];\n                    add_sd = add_sst - add_spt\n\n                    if add_spt and (add_spt-20 >= 0) and (add_spt+21 < dim):\n                        y2[len(list_IDs)+i, add_spt-20:add_spt+21, 0] = _label()\n                    elif add_spt and (add_spt+21 < dim):\n                        y2[len(list_IDs)+i, 0:add_spt+add_spt+1, 0] = _label(a=0, b=add_spt, c=2*add_spt)\n                    elif add_spt and (add_spt-20 >= 0):\n                        pdif = dim - add_spt\n                        y2[len(list_IDs)+i, add_spt-pdif-1:dim, 0] = _label(a=add_spt-pdif, b=add_spt, c=2*pdif)\n\n                    if add_sst and (add_sst-20 >= 0) and (add_sst+21 < dim):\n                        y3[len(list_IDs)+i, add_sst-20:add_sst+21, 0] = _label()\n                    elif add_sst and (add_sst+21 < dim):\n                        y3[len(list_IDs)+i, 0:add_sst+add_sst+1, 0] = _label(a=0, b=add_sst, c=2*add_sst)\n                    elif add_sst and (add_sst-20 >= 0):\n                        sdif = dim - add_sst\n                        y3[len(list_IDs)+i, add_sst-sdif-1:dim, 0] = _label(a=add_sst-sdif, b=add_sst, c=2*sdif)\n\n                    if add_sst+int(0.4*add_sd) <= dim:\n                        y1[len(list_IDs)+i, add_spt:int(add_sst+(0.4*add_sd)), 0] = 1\n                    else:\n                        y1[len(list_IDs)+i, add_spt:dim, 0] = 1\n\n    fl.close()\n    return X.astype('float32'), y1.astype('float32'), y2.astype('float32'), y3.astype('float32')\n\n\n\n\nclass PreLoadGeneratorTest(keras.utils.Sequence):\n\n    \"\"\"\n\n    Keras generator with preprocessing. For testing. Pre-load version.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Path to the input hdf5 file.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32.\n        Batch size.\n\n    n_channels: int, default=3.\n        Number of channels.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n\n    \"\"\"\n\n    def __init__(self,\n                 list_IDs,\n                 inp_data,\n                 dim,\n                 batch_size=32,\n                 n_channels=3,\n                 norm_mode = 'std'):\n\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.inp_data = inp_data\n        self.n_channels = n_channels\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X = self.__data_generation(list_IDs_temp)\n        return ({'input': X})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def _normalize(self, data, mode='max'):\n        'Normalize waveforms in each batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def __data_generation(self, list_IDs_temp):\n        'readint the waveforms'\n        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            dataset = self.inp_data[ID]\n            data = np.array(dataset)\n            data = self._normalize(data, self.norm_mode)\n            X[i, :, :] = data\n\n        return X\n\n\n\nclass DataGeneratorTest(keras.utils.Sequence):\n\n    \"\"\"\n\n    Keras generator with preprocessing. For testing.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Path to the input hdf5 file.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32\n        Batch size.\n\n    n_channels: int, default=3\n        Number of channels.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'.\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n    \"\"\"\n\n    def __init__(self,\n                 list_IDs,\n                 file_name,\n                 dim,\n                 batch_size=32,\n                 n_channels=3,\n                 norm_mode = 'max'):\n\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.file_name = file_name\n        self.n_channels = n_channels\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X = self.__data_generation(list_IDs_temp)\n        return ({'input': X})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def normalize(self, data, mode = 'max'):\n        'Normalize waveforms in each batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n\n    def __data_generation(self, list_IDs_temp):\n        'readint the waveforms'\n\n        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n        fl = h5py.File(self.file_name, 'r')\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n\n            dataset = fl.get('data/'+str(ID))\n            data = np.array(dataset)\n            if data.ndim == 1:\n                # original trace could be 1-component, i.e., (6000,) or (6000)\n                dat_channel = 1\n                dat_dim = len(data)\n            else:\n                # convert data format to the one that is used in the prediction\n                if data.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\n                    data = np.transpose(data)\n                # more than 1 component trace\n                dat_channel = data.shape[1]\n                dat_dim = data.shape[0]\n            if dat_channel > self.n_channels:\n                dat_channel = self.n_channels\n            temp = data\n            data = np.zeros((self.dim, self.n_channels))\n            attr_value = dataset.attrs.get('component', None)\n            if attr_value is not None:\n                # label contains component information\n                if attr_value == 'Z':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 2] = temp\n                    else:\n                        data[:, 2] = temp[:self.dim]\n                if attr_value == 'N' or attr_value == '2':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 1] = temp\n                    else:\n                        data[:, 1] = temp[:self.dim]\n                if attr_value == 'E' or attr_value == '0':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 0] = temp\n                    else:\n                        data[:, 0] = temp[:self.dim]\n            else:\n                if dat_channel == 1:\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0]] = temp\n                    else:\n                        data[:, 0] = temp[:self.dim]\n                else:\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 0:dat_channel] = temp[:, 0:dat_channel]\n                    else:\n                        data[:, 0:dat_channel] = temp[:self.dim, 0:dat_channel]\n            if self.norm_mode:\n                data = self.normalize(data, self.norm_mode)\n            X[i, :, :] = data[:, :self.n_channels]\n\n        fl.close()\n\n        return X\n\n\n\nclass DataGeneratorPrediction(keras.utils.Sequence):\n\n    \"\"\"\n    Keras generator with preprocessing. For prediction.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Path to the input hdf5 file.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32\n        Batch size.\n\n    n_channels: int, default=3\n        Number of channels.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'.\n\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n\n    \"\"\"\n\n    def __init__(self,\n                 list_IDs,\n                 file_name,\n                 dim,\n                 batch_size=32,\n                 n_channels=3,\n                 norm_mode = 'max'):\n\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.file_name = file_name\n        self.n_channels = n_channels\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X = self.__data_generation(list_IDs_temp)\n        return ({'input': X})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def normalize(self, data, mode = 'max'):\n        'Normalize waveforms in a batch'\n\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def __data_generation(self, list_IDs_temp):\n        'read the waveforms'\n        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n        fl = h5py.File(self.file_name, 'r')\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            dataset = fl.get('data/'+str(ID))\n            data = np.array(dataset)\n            if data.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\n                data = np.transpose(data)\n            # Hao update Nov 6 2022\n            # augment when trace channel is less than required n_channels\n            if data.ndim == 1:\n                dat_channel = 1\n            else:\n                dat_channel = data.shape[1]\n            if dat_channel > self.n_channels:\n                dat_channel = self.n_channels\n            temp = data\n            data = np.zeros((self.dim, self.n_channels))\n            attr_value = dataset.attrs.get('component', None)\n            if attr_value is not None:\n                # label contains component information\n                if attr_value == 'Z':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 2] = temp\n                    else:\n                        data[:, 2] = temp[:self.dim]\n                if attr_value == 'N' or attr_value == '2':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 1] = temp\n                    else:\n                        data[:, 1] = temp[:self.dim]\n                if attr_value == 'E' or attr_value == '0':\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 0] = temp\n                    else:\n                        data[:, 0] = temp[:self.dim]\n            else:\n                if dat_channel == 1:\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0]] = temp\n                    else:\n                        data[:, 0] = temp[:self.dim]\n                else:\n                    if temp.shape[0] < self.dim:\n                        data[:temp.shape[0], 0:dat_channel] = temp[:, 0:dat_channel]\n                    else:\n                        data[:, 0:dat_channel] = temp[:self.dim, 0:dat_channel]\n\n            # # check data shape e.g, sample length < required n_samples, start duplicating\n            # if data.shape[0] < self.dim:\n            #     duplicate_len = int(self.dim - data.shape[0])\n            #     data = np.concatenate((data, data[0:duplicate_len, :]))\n            # else:\n            #     # check data shape e.g, sample length > required n_samples, start trimming\n            #     if data.shape[0] > self.dim:\n            #         data = data[0:self.dim, :]\n            if self.norm_mode:\n                data = self.normalize(data, self.norm_mode)\n            X[i, :, :] = data[:,:self.n_channels]\n\n        fl.close()\n\n        return X\n\n\n\n\ndef _detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False):\n\n    \"\"\"\n\n    Detect peaks in data based on their amplitude and other features.\n\n    Parameters\n    ----------\n    x : 1D array_like\n        data.\n\n    mph : {None, number}, default=None\n        detect peaks that are greater than minimum peak height.\n\n    mpd : int, default=1\n        detect peaks that are at least separated by minimum peak distance (in number of data).\n\n    threshold : int, default=0\n        detect peaks (valleys) that are greater (smaller) than `threshold in relation to their immediate neighbors.\n\n    edge : str, default=rising\n        for a flat peak, keep only the rising edge ('rising'), only the falling edge ('falling'), both edges ('both'), or don't detect a flat peak (None).\n\n    kpsh : bool, default=False\n        keep peaks with same height even if they are closer than `mpd`.\n\n    valley : bool, default=False\n        if True (1), detect valleys (local minima) instead of peaks.\n\n    Returns\n    ---------\n    ind : 1D array_like\n        indeces of the peaks in `x`.\n\n    Modified from\n   ----------------\n    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n\n\n    \"\"\"\n\n    x = np.atleast_1d(x).astype('float64')\n    if x.size < 3:\n        return np.array([], dtype=int)\n    if valley:\n        x = -x\n    # find indices of all peaks\n    dx = x[1:] - x[:-1]\n    # handle NaN's\n    indnan = np.where(np.isnan(x))[0]\n    if indnan.size:\n        x[indnan] = np.inf\n        dx[np.where(np.isnan(dx))[0]] = np.inf\n    ine, ire, ife = np.array([[], [], []], dtype=int)\n    if not edge:\n        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n    else:\n        if edge.lower() in ['rising', 'both']:\n            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n        if edge.lower() in ['falling', 'both']:\n            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n    ind = np.unique(np.hstack((ine, ire, ife)))\n    # handle NaN's\n    if ind.size and indnan.size:\n        # NaN's and values close to NaN's cannot be peaks\n        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n    # first and last values of x cannot be peaks\n    if ind.size and ind[0] == 0:\n        ind = ind[1:]\n    if ind.size and ind[-1] == x.size-1:\n        ind = ind[:-1]\n    # remove peaks < minimum peak height\n    if ind.size and mph is not None:\n        ind = ind[x[ind] >= mph]\n    # remove peaks - neighbors < threshold\n    if ind.size and threshold > 0:\n        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n        ind = np.delete(ind, np.where(dx < threshold)[0])\n    # detect small peaks closer than minimum peak distance\n    if ind.size and mpd > 1:\n        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n        idel = np.zeros(ind.size, dtype=bool)\n        for i in range(ind.size):\n            if not idel[i]:\n                # keep peaks with the same height if kpsh is True\n                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n                    & (x[ind[i]] > x[ind] if kpsh else True)\n                idel[i] = 0  # Keep current peak\n        # remove the small peaks and sort back the indices by their occurrence\n        ind = np.sort(ind[~idel])\n\n    return ind\n\n\n\n\ndef picker(args, yh1, yh2, yh3, yh1_std, yh2_std, yh3_std, spt=None, sst=None):\n\n    \"\"\"\n\n    Performs detection and picking.\n\n    Parameters\n    ----------\n    args : dic\n        A dictionary containing all of the input parameters.\n\n    yh1 : 1D array\n        Detection probabilities.\n\n    yh2 : 1D array\n        P arrival probabilities.\n\n    yh3 : 1D array\n        S arrival probabilities.\n\n    yh1_std : 1D array\n        Detection standard deviations.\n\n    yh2_std : 1D array\n        P arrival standard deviations.\n\n    yh3_std : 1D array\n        S arrival standard deviations.\n\n    spt : {int, None}, default=None\n        P arrival time in sample.\n\n    sst : {int, None}, default=None\n        S arrival time in sample.\n\n\n    Returns\n    --------\n    matches: dic\n        Contains the information for the detected and picked event.\n\n    matches: dic\n        {detection statr-time:[ detection end-time, detection probability, detectin uncertainty, P arrival, P probabiliy, P uncertainty, S arrival,  S probability, S uncertainty]}\n\n    pick_errors : dic\n        {detection statr-time:[ P_ground_truth - P_pick, S_ground_truth - S_pick]}\n\n    yh3: 1D array\n        normalized S_probability\n\n    \"\"\"\n\n #   yh3[yh3>0.04] = ((yh1+yh3)/2)[yh3>0.04]\n #   yh2[yh2>0.10] = ((yh1+yh2)/2)[yh2>0.10]\n\n    detection = trigger_onset(yh1, args['detection_threshold'], args['detection_threshold'])\n    pp_arr = _detect_peaks(yh2, mph=args['P_threshold'], mpd=1)\n    ss_arr = _detect_peaks(yh3, mph=args['S_threshold'], mpd=1)\n\n    P_PICKS = {}\n    S_PICKS = {}\n    EVENTS = {}\n    matches = {}\n    pick_errors = {}\n    if len(pp_arr) > 0:\n        P_uncertainty = None\n\n        for pick in range(len(pp_arr)):\n            pauto = pp_arr[pick]\n\n            if args['estimate_uncertainty'] and pauto:\n                P_uncertainty = np.round(yh2_std[int(pauto)], 3)\n\n            if pauto:\n                P_prob = np.round(yh2[int(pauto)], 3)\n                P_PICKS.update({pauto : [P_prob, P_uncertainty]})\n\n    if len(ss_arr) > 0:\n        S_uncertainty = None\n\n        for pick in range(len(ss_arr)):\n            sauto = ss_arr[pick]\n\n            if args['estimate_uncertainty'] and sauto:\n                S_uncertainty = np.round(yh3_std[int(sauto)], 3)\n\n            if sauto:\n                S_prob = np.round(yh3[int(sauto)], 3)\n                S_PICKS.update({sauto : [S_prob, S_uncertainty]})\n\n    if len(detection) > 0:\n        D_uncertainty = None\n\n        for ev in range(len(detection)):\n            if args['estimate_uncertainty']:\n                D_uncertainty = np.mean(yh1_std[detection[ev][0]:detection[ev][1]])\n                D_uncertainty = np.round(D_uncertainty, 3)\n\n            D_prob = np.mean(yh1[detection[ev][0]:detection[ev][1]])\n            D_prob = np.round(D_prob, 3)\n\n            EVENTS.update({ detection[ev][0] : [D_prob, D_uncertainty, detection[ev][1]]})\n\n    for ev in EVENTS:\n        bg = ev\n        ed = EVENTS[ev][2]\n        S_error = None\n        P_error = None\n        if int(ed-bg) >= 10:\n\n            candidate_Ss = {}\n            for Ss, S_val in S_PICKS.items():\n                if Ss > bg and Ss < ed:\n                    candidate_Ss.update({Ss : S_val})\n\n            if len(candidate_Ss) > 1:\n# =============================================================================\n#                 Sr_st = 0\n#                 buffer = {}\n#                 for SsCan, S_valCan in candidate_Ss.items():\n#                     if S_valCan[0] > Sr_st:\n#                         buffer = {SsCan : S_valCan}\n#                         Sr_st = S_valCan[0]\n#                 candidate_Ss = buffer\n# =============================================================================\n                candidate_Ss = {list(candidate_Ss.keys())[0] : candidate_Ss[list(candidate_Ss.keys())[0]]}\n\n\n            if len(candidate_Ss) == 0:\n                    candidate_Ss = {None:[None, None]}\n\n            candidate_Ps = {}\n            for Ps, P_val in P_PICKS.items():\n                if list(candidate_Ss)[0]:\n                    if Ps > bg-100 and Ps < list(candidate_Ss)[0]-10:\n                        candidate_Ps.update({Ps : P_val})\n                else:\n                    if Ps > bg-100 and Ps < ed:\n                        candidate_Ps.update({Ps : P_val})\n\n            if len(candidate_Ps) > 1:\n                Pr_st = 0\n                buffer = {}\n                for PsCan, P_valCan in candidate_Ps.items():\n                    if P_valCan[0] > Pr_st:\n                        buffer = {PsCan : P_valCan}\n                        Pr_st = P_valCan[0]\n                candidate_Ps = buffer\n\n            if len(candidate_Ps) == 0:\n                    candidate_Ps = {None:[None, None]}\n\n\n# =============================================================================\n#             Ses =[]; Pes=[]\n#             if len(candidate_Ss) >= 1:\n#                 for SsCan, S_valCan in candidate_Ss.items():\n#                     Ses.append(SsCan)\n#\n#             if len(candidate_Ps) >= 1:\n#                 for PsCan, P_valCan in candidate_Ps.items():\n#                     Pes.append(PsCan)\n#\n#             if len(Ses) >=1 and len(Pes) >= 1:\n#                 PS = pair_PS(Pes, Ses, ed-bg)\n#                 if PS:\n#                     candidate_Ps = {PS[0] : candidate_Ps.get(PS[0])}\n#                     candidate_Ss = {PS[1] : candidate_Ss.get(PS[1])}\n# =============================================================================\n            # matching the detection and picks\n            if list(candidate_Ss)[0] or list(candidate_Ps)[0]:\n                matches.update({\n                                bg:[ed,\n                                    EVENTS[ev][0],\n                                    EVENTS[ev][1],\n\n                                    list(candidate_Ps)[0],\n                                    candidate_Ps[list(candidate_Ps)[0]][0],\n                                    candidate_Ps[list(candidate_Ps)[0]][1],\n\n                                    list(candidate_Ss)[0],\n                                    candidate_Ss[list(candidate_Ss)[0]][0],\n                                    candidate_Ss[list(candidate_Ss)[0]][1],\n                                                ] })\n                # error calculation\n                if sst and sst > bg and sst < EVENTS[ev][2]:\n                    if list(candidate_Ss)[0]:\n                        S_error = sst -list(candidate_Ss)[0]\n                    else:\n                        S_error = None\n\n                if spt and spt > bg-100 and spt < EVENTS[ev][2]:\n                    if list(candidate_Ps)[0]:\n                        P_error = spt - list(candidate_Ps)[0]\n                    else:\n                        P_error = None\n\n                pick_errors.update({bg:[P_error, S_error]})\n\n    return matches, pick_errors, yh3\n\n\n\n\n\ndef generate_arrays_from_file(file_list, step):\n\n    \"\"\"\n\n    Make a generator to generate list of trace names.\n\n    Parameters\n    ----------\n    file_list : str\n        A list of trace names.\n\n    step : int\n        Batch size.\n\n    Returns\n    --------\n    chunck : str\n        A batch of trace names.\n\n    \"\"\"\n\n    n_loops = int(np.ceil(len(file_list) / step))\n    b = 0\n    while True:\n        for i in range(n_loops):\n            e = i*step + step\n            if e > len(file_list):\n                e = len(file_list)\n            chunck = file_list[b:e]\n            b=e\n            yield chunck\n\n\n\n\ndef f1(y_true, y_pred):\n\n    \"\"\"\n\n    Calculate F1-score.\n\n    Parameters\n    ----------\n    y_true : 1D array\n        Ground truth labels.\n\n    y_pred : 1D array\n        Predicted labels.\n\n    Returns\n    -------\n    f1 : float\n        Calculated F1-score.\n\n    \"\"\"\n\n    def recall(y_true, y_pred):\n        'Recall metric. Only computes a batch-wise average of recall. Computes the recall, a metric for multi-label classification of how many relevant items are selected.'\n\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        'Precision metric. Only computes a batch-wise average of precision. Computes the precision, a metric for multi-label classification of how many selected items are relevant.'\n\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n\n\ndef normalize(data, mode='std'):\n\n    \"\"\"\n\n    Normalize 3D arrays.\n\n    Parameters\n    ----------\n    data : 3D numpy array\n        3 component traces.\n\n    mode : str, default='std'\n        Mode of normalization. 'max' or 'std'\n\n    Returns\n    -------\n    data : 3D numpy array\n        normalized data.\n\n    \"\"\"\n\n    data -= np.mean(data, axis=0, keepdims=True)\n    if mode == 'max':\n        max_data = np.max(data, axis=0, keepdims=True)\n        assert(max_data.shape[-1] == data.shape[-1])\n        max_data[max_data == 0] = 1\n        data /= max_data\n    elif mode == 'std':\n        std_data = np.std(data, axis=0, keepdims=True)\n        assert(std_data.shape[-1] == data.shape[-1])\n        std_data[std_data == 0] = 1\n        data /= std_data\n    return data\n\n\n\n\nclass LayerNormalization(keras.layers.Layer):\n\n    \"\"\"\n\n    Layer normalization layer modified from https://github.com/CyberZHG based on [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n\n    Parameters\n    ----------\n    center: bool\n        Add an offset parameter if it is True.\n\n    scale: bool\n        Add a scale parameter if it is True.\n\n    epsilon: bool\n        Epsilon for calculating variance.\n\n    gamma_initializer: str\n        Initializer for the gamma weight.\n\n    beta_initializer: str\n        Initializer for the beta weight.\n\n    Returns\n    -------\n    data: 3D tensor\n        with shape: (batch_size, , input_dim)\n\n    \"\"\"\n\n    def __init__(self,\n                 center=True,\n                 scale=True,\n                 epsilon=None,\n                 gamma_initializer='ones',\n                 beta_initializer='zeros',\n                 **kwargs):\n\n        super(LayerNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.center = center\n        self.scale = scale\n        if epsilon is None:\n            epsilon = K.epsilon() * K.epsilon()\n        self.epsilon = epsilon\n        self.gamma_initializer = keras.initializers.get(gamma_initializer)\n        self.beta_initializer = keras.initializers.get(beta_initializer)\n\n\n    def get_config(self):\n        config = {\n            'center': self.center,\n            'scale': self.scale,\n            'epsilon': self.epsilon,\n            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),\n            'beta_initializer': keras.initializers.serialize(self.beta_initializer),\n        }\n        base_config = super(LayerNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        return input_mask\n\n    def build(self, input_shape):\n        self.input_spec = keras.engine.InputSpec(shape=input_shape)\n        shape = input_shape[-1:]\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                initializer=self.gamma_initializer,\n                name='gamma',\n            )\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                initializer=self.beta_initializer,\n                name='beta',\n            )\n        super(LayerNormalization, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        mean = K.mean(inputs, axis=-1, keepdims=True)\n        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)\n        std = K.sqrt(variance + self.epsilon)\n        outputs = (inputs - mean) / std\n        if self.scale:\n            outputs *= self.gamma\n        if self.center:\n            outputs += self.beta\n        return outputs\n\n\n\nclass FeedForward(keras.layers.Layer):\n    \"\"\"Position-wise feed-forward layer. modified from https://github.com/CyberZHG\n    # Arguments\n        units: int >= 0. Dimension of hidden units.\n        activation: Activation function to use\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        dropout_rate: 0.0 <= float <= 1.0. Dropout rate for hidden units.\n    # Input shape\n        3D tensor with shape: `(batch_size, ..., input_dim)`.\n    # Output shape\n        3D tensor with shape: `(batch_size, ..., input_dim)`.\n    # References\n        - [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n    \"\"\"\n\n    def __init__(self,\n                 units,\n                 activation='relu',\n                 use_bias=True,\n                 kernel_initializer='glorot_normal',\n                 bias_initializer='zeros',\n                 dropout_rate=0.0,\n                 **kwargs):\n        self.supports_masking = True\n        self.units = units\n        self.activation = keras.activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.dropout_rate = dropout_rate\n        self.W1, self.b1 = None, None\n        self.W2, self.b2 = None, None\n        super(FeedForward, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'activation': keras.activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n            'dropout_rate': self.dropout_rate,\n        }\n        base_config = super(FeedForward, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        return input_mask\n\n    def build(self, input_shape):\n        feature_dim = int(input_shape[-1])\n        self.W1 = self.add_weight(\n            shape=(feature_dim, self.units),\n            initializer=self.kernel_initializer,\n            name='{}_W1'.format(self.name),\n        )\n        if self.use_bias:\n            self.b1 = self.add_weight(\n                shape=(self.units,),\n                initializer=self.bias_initializer,\n                name='{}_b1'.format(self.name),\n            )\n        self.W2 = self.add_weight(\n            shape=(self.units, feature_dim),\n            initializer=self.kernel_initializer,\n            name='{}_W2'.format(self.name),\n        )\n        if self.use_bias:\n            self.b2 = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                name='{}_b2'.format(self.name),\n            )\n        super(FeedForward, self).build(input_shape)\n\n    def call(self, x, mask=None, training=None):\n        h = K.dot(x, self.W1)\n        if self.use_bias:\n            h = K.bias_add(h, self.b1)\n        if self.activation is not None:\n            h = self.activation(h)\n        if 0.0 < self.dropout_rate < 1.0:\n            def dropped_inputs():\n                return K.dropout(h, self.dropout_rate, K.shape(h))\n            h = K.in_train_phase(dropped_inputs, h, training=training)\n        y = K.dot(h, self.W2)\n        if self.use_bias:\n            y = K.bias_add(y, self.b2)\n        return y\n\n\nclass SeqSelfAttention(keras.layers.Layer):\n    \"\"\"Layer initialization. modified from https://github.com/CyberZHG\n    For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n    :param units: The dimension of the vectors that used to calculate the attention weights.\n    :param attention_width: The width of local attention.\n    :param attention_type: 'additive' or 'multiplicative'.\n    :param return_attention: Whether to return the attention weights for visualization.\n    :param history_only: Only use historical pieces of data.\n    :param kernel_initializer: The initializer for weight matrices.\n    :param bias_initializer: The initializer for biases.\n    :param kernel_regularizer: The regularization for weight matrices.\n    :param bias_regularizer: The regularization for biases.\n    :param kernel_constraint: The constraint for weight matrices.\n    :param bias_constraint: The constraint for biases.\n    :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n                              in additive mode.\n    :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n    :param attention_activation: The activation used for calculating the weights of attention.\n    :param attention_regularizer_weight: The weights of attention regularizer.\n    :param kwargs: Parameters for parent class.\n    \"\"\"\n\n    ATTENTION_TYPE_ADD = 'additive'\n    ATTENTION_TYPE_MUL = 'multiplicative'\n\n    def __init__(self,\n                 units=32,\n                 attention_width=None,\n                 attention_type=ATTENTION_TYPE_ADD,\n                 return_attention=False,\n                 history_only=False,\n                 kernel_initializer='glorot_normal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 use_additive_bias=True,\n                 use_attention_bias=True,\n                 attention_activation=None,\n                 attention_regularizer_weight=0.0,\n                 **kwargs):\n\n        super(SeqSelfAttention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.attention_width = attention_width\n        self.attention_type = attention_type\n        self.return_attention = return_attention\n        self.history_only = history_only\n        if history_only and attention_width is None:\n            self.attention_width = int(1e9)\n\n        self.use_additive_bias = use_additive_bias\n        self.use_attention_bias = use_attention_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n        self.bias_constraint = keras.constraints.get(bias_constraint)\n        self.attention_activation = keras.activations.get(attention_activation)\n        self.attention_regularizer_weight = attention_regularizer_weight\n        self._backend = keras.backend.backend()\n\n        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self.Wx, self.Wt, self.bh = None, None, None\n            self.Wa, self.ba = None, None\n        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self.Wa, self.ba = None, None\n        else:\n            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'attention_width': self.attention_width,\n            'attention_type': self.attention_type,\n            'return_attention': self.return_attention,\n            'history_only': self.history_only,\n            'use_additive_bias': self.use_additive_bias,\n            'use_attention_bias': self.use_attention_bias,\n            'kernel_initializer': keras.regularizers.serialize(self.kernel_initializer),\n            'bias_initializer': keras.regularizers.serialize(self.bias_initializer),\n            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n            'attention_activation': keras.activations.serialize(self.attention_activation),\n            'attention_regularizer_weight': self.attention_regularizer_weight,\n        }\n        base_config = super(SeqSelfAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self._build_additive_attention(input_shape)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self._build_multiplicative_attention(input_shape)\n        super(SeqSelfAttention, self).build(input_shape)\n\n    def _build_additive_attention(self, input_shape):\n        feature_dim = int(input_shape[2])\n\n        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n                                  name='{}_Add_Wt'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n                                  name='{}_Add_Wx'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_additive_bias:\n            self.bh = self.add_weight(shape=(self.units,),\n                                      name='{}_Add_bh'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n        self.Wa = self.add_weight(shape=(self.units, 1),\n                                  name='{}_Add_Wa'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name='{}_Add_ba'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def _build_multiplicative_attention(self, input_shape):\n        feature_dim = int(input_shape[2])\n\n        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n                                  name='{}_Mul_Wa'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name='{}_Mul_ba'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def call(self, inputs, mask=None, **kwargs):\n        input_len = K.shape(inputs)[1]\n\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            e = self._call_additive_emission(inputs)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            e = self._call_multiplicative_emission(inputs)\n\n        if self.attention_activation is not None:\n            e = self.attention_activation(e)\n        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n        if self.attention_width is not None:\n            if self.history_only:\n                lower = K.arange(0, input_len) - (self.attention_width - 1)\n            else:\n                lower = K.arange(0, input_len) - self.attention_width // 2\n            lower = K.expand_dims(lower, axis=-1)\n            upper = lower + self.attention_width\n            indices = K.expand_dims(K.arange(0, input_len), axis=0)\n            e = e * K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx())\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            mask = K.expand_dims(mask)\n            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n\n        # a_{t} = \\text{softmax}(e_t)\n        s = K.sum(e, axis=-1, keepdims=True)\n        a = e / (s + K.epsilon())\n\n        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n        v = K.batch_dot(a, inputs)\n        if self.attention_regularizer_weight > 0.0:\n            self.add_loss(self._attention_regularizer(a))\n\n        if self.return_attention:\n            return [v, a]\n        return v\n\n    def _call_additive_emission(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size, input_len = input_shape[0], input_shape[1]\n\n        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n        q = K.expand_dims(K.dot(inputs, self.Wt), 2)\n        k = K.expand_dims(K.dot(inputs, self.Wx), 1)\n        if self.use_additive_bias:\n            h = K.tanh(q + k + self.bh)\n        else:\n            h = K.tanh(q + k)\n\n        # e_{t, t'} = W_a h_{t, t'} + b_a\n        if self.use_attention_bias:\n            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n        else:\n            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n        return e\n\n    def _call_multiplicative_emission(self, inputs):\n        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n        if self.use_attention_bias:\n            e += self.ba[0]\n        return e\n\n    def compute_output_shape(self, input_shape):\n        output_shape = input_shape\n        if self.return_attention:\n            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n            return [output_shape, attention_shape]\n        return output_shape\n\n    def compute_mask(self, inputs, mask=None):\n        if self.return_attention:\n            return [mask, None]\n        return mask\n\n    def _attention_regularizer(self, attention):\n        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n        input_len = K.shape(attention)[-1]\n        indices = K.expand_dims(K.arange(0, input_len), axis=0)\n        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)\n        eye = K.cast(K.equal(indices, diagonal), K.floatx())\n        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n            attention,\n            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size\n\n    @staticmethod\n    def get_custom_objects():\n        return {'SeqSelfAttention': SeqSelfAttention}\n\n\n\ndef _block_BiLSTM(filters, drop_rate, padding, inpR):\n    'Returns LSTM residual block'\n    prev = inpR\n    x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout=drop_rate, recurrent_dropout=drop_rate))(prev)\n    NiN = Conv1D(filters, 1, padding = padding)(x_rnn)\n    res_out = BatchNormalization()(NiN)\n    return res_out\n\n\ndef _block_CNN_1(filters, ker, drop_rate, activation, padding, inpC):\n    ' Returns CNN residual blocks '\n    prev = inpC\n    layer_1 = BatchNormalization()(prev)\n    act_1 = Activation(activation)(layer_1)\n    act_1 = SpatialDropout1D(drop_rate)(act_1, training=True)\n    conv_1 = Conv1D(filters, ker, padding = padding)(act_1)\n\n    layer_2 = BatchNormalization()(conv_1)\n    act_2 = Activation(activation)(layer_2)\n    act_2 = SpatialDropout1D(drop_rate)(act_2, training=True)\n    conv_2 = Conv1D(filters, ker, padding = padding)(act_2)\n\n    res_out = add([prev, conv_2])\n\n    return res_out\n\n\ndef _transformer(drop_rate, width, name, inpC):\n    ' Returns a transformer block containing one addetive attention and one feed  forward layer with residual connections '\n    x = inpC\n\n    att_layer, weight = SeqSelfAttention(return_attention =True,\n                                         attention_width = width,\n                                         name=name)(x)\n\n#  att_layer = Dropout(drop_rate)(att_layer, training=True)\n    att_layer2 = add([x, att_layer])\n    norm_layer = LayerNormalization()(att_layer2)\n\n    FF = FeedForward(units=128, dropout_rate=drop_rate)(norm_layer)\n\n    FF_add = add([norm_layer, FF])\n    norm_out = LayerNormalization()(FF_add)\n\n    return norm_out, weight\n\n\n\ndef _encoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):\n    ' Returns the encoder that is a combination of residual blocks and maxpooling.'\n    e = inpC\n    for dp in range(depth):\n        e = Conv1D(filter_number[dp],\n                   filter_size[dp],\n                   padding = padding,\n                   activation = activation,\n                   kernel_regularizer = ker_regul,\n                   bias_regularizer = bias_regul,\n                   )(e)\n        e = MaxPooling1D(2, padding = padding)(e)\n    return(e)\n\n\ndef _decoder(filter_number, filter_size, depth, drop_rate, ker_regul, bias_regul, activation, padding, inpC):\n    ' Returns the dencoder that is a combination of residual blocks and upsampling. '\n    d = inpC\n    for dp in range(depth):\n        d = UpSampling1D(2)(d)\n        if dp == 3:\n            d = Cropping1D(cropping=(1, 1))(d)\n        d = Conv1D(filter_number[dp],\n                   filter_size[dp],\n                   padding = padding,\n                   activation = activation,\n                   kernel_regularizer = ker_regul,\n                   bias_regularizer = bias_regul,\n                   )(d)\n    return(d)\n\n\n\ndef _lr_schedule(epoch):\n    ' Learning rate is scheduled to be reduced after 40, 60, 80, 90 epochs.'\n\n    lr = 1e-3\n    if epoch > 90:\n        lr *= 0.5e-3\n    elif epoch > 60:\n        lr *= 1e-3\n    elif epoch > 40:\n        lr *= 1e-2\n    elif epoch > 20:\n        lr *= 1e-1\n    print('Learning rate: ', lr)\n    return lr\n\n\n\nclass cred2():\n\n    \"\"\"\n\n    Creates the model\n\n    Parameters\n    ----------\n    nb_filters: list\n        The list of filter numbers.\n\n    kernel_size: list\n        The size of the kernel to use in each convolutional layer.\n\n    padding: str\n        The padding to use in the convolutional layers.\n\n    activationf: str\n        Activation funciton type.\n\n    endcoder_depth: int\n        The number of layers in the encoder.\n\n    decoder_depth: int\n        The number of layers in the decoder.\n\n    cnn_blocks: int\n        The number of residual CNN blocks.\n\n    BiLSTM_blocks: int=\n        The number of Bidirectional LSTM blocks.\n\n    drop_rate: float\n        Dropout rate.\n\n    loss_weights: list\n        Weights of the loss function for the detection, P picking, and S picking.\n\n    loss_types: list\n        Types of the loss function for the detection, P picking, and S picking.\n\n    kernel_regularizer: str\n        l1 norm regularizer.\n\n    bias_regularizer: str\n        l1 norm regularizer.\n\n    multi_gpu: bool\n        If use multiple GPUs for the training.\n\n    gpu_number: int\n        The number of GPUs for the muli-GPU training.\n\n    Returns\n    ----------\n        The complied model: keras model\n\n    \"\"\"\n\n    def __init__(self,\n                 nb_filters=[8, 16, 16, 32, 32, 96, 96, 128],\n                 kernel_size=[11, 9, 7, 7, 5, 5, 3, 3],\n                 padding='same',\n                 activationf='relu',\n                 endcoder_depth=7,\n                 decoder_depth=7,\n                 cnn_blocks=5,\n                 BiLSTM_blocks=3,\n                 drop_rate=0.1,\n                 loss_weights=[0.2, 0.3, 0.5],\n                 loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\n                 kernel_regularizer=keras.regularizers.l1(1e-4),\n                 bias_regularizer=keras.regularizers.l1(1e-4),\n                 multi_gpu=False,\n                 gpu_number=4,\n                 ):\n\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.padding = padding\n        self.activationf = activationf\n        self.endcoder_depth= endcoder_depth\n        self.decoder_depth= decoder_depth\n        self.cnn_blocks= cnn_blocks\n        self.BiLSTM_blocks= BiLSTM_blocks\n        self.drop_rate= drop_rate\n        self.loss_weights= loss_weights\n        self.loss_types = loss_types\n        self.kernel_regularizer = kernel_regularizer\n        self.bias_regularizer = bias_regularizer\n        self.multi_gpu = multi_gpu\n        self.gpu_number = gpu_number\n\n\n    def __call__(self, inp):\n\n        x = inp\n        x = _encoder(self.nb_filters,\n                    self.kernel_size,\n                    self.endcoder_depth,\n                    self.drop_rate,\n                    self.kernel_regularizer,\n                    self.bias_regularizer,\n                    self.activationf,\n                    self.padding,\n                    x)\n\n        for cb in range(self.cnn_blocks):\n            x = _block_CNN_1(self.nb_filters[6], 3, self.drop_rate, self.activationf, self.padding, x)\n            if cb > 2:\n                x = _block_CNN_1(self.nb_filters[6], 2, self.drop_rate, self.activationf, self.padding, x)\n\n        for bb in range(self.BiLSTM_blocks):\n            x = _block_BiLSTM(self.nb_filters[1], self.drop_rate, self.padding, x)\n\n\n        x, weightdD0 = _transformer(self.drop_rate, None, 'attentionD0', x)\n        encoded, weightdD = _transformer(self.drop_rate, None, 'attentionD', x)\n\n        decoder_D = _decoder([i for i in reversed(self.nb_filters)],\n                             [i for i in reversed(self.kernel_size)],\n                             self.decoder_depth,\n                             self.drop_rate,\n                             self.kernel_regularizer,\n                             self.bias_regularizer,\n                             self.activationf,\n                             self.padding,\n                             encoded)\n        d = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='detector')(decoder_D)\n\n\n        PLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)\n        norm_layerP, weightdP = SeqSelfAttention(return_attention=True,\n                                                 attention_width= 3,\n                                                 name='attentionP')(PLSTM)\n\n        decoder_P = _decoder([i for i in reversed(self.nb_filters)],\n                            [i for i in reversed(self.kernel_size)],\n                            self.decoder_depth,\n                            self.drop_rate,\n                            self.kernel_regularizer,\n                            self.bias_regularizer,\n                            self.activationf,\n                            self.padding,\n                            norm_layerP)\n        P = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_P')(decoder_P)\n\n        SLSTM = LSTM(self.nb_filters[1], return_sequences=True, dropout=self.drop_rate, recurrent_dropout=self.drop_rate)(encoded)\n        norm_layerS, weightdS = SeqSelfAttention(return_attention=True,\n                                                 attention_width= 3,\n                                                 name='attentionS')(SLSTM)\n\n\n        decoder_S = _decoder([i for i in reversed(self.nb_filters)],\n                            [i for i in reversed(self.kernel_size)],\n                            self.decoder_depth,\n                            self.drop_rate,\n                            self.kernel_regularizer,\n                            self.bias_regularizer,\n                            self.activationf,\n                            self.padding,\n                            norm_layerS)\n\n        S = Conv1D(1, 11, padding = self.padding, activation='sigmoid', name='picker_S')(decoder_S)\n\n\n        if self.multi_gpu == True:\n            parallel_model = Model(inputs=inp, outputs=[d, P, S])\n            model = multi_gpu_model(parallel_model, gpus=self.gpu_number)\n        else:\n            model = Model(inputs=inp, outputs=[d, P, S])\n\n        model.compile(loss=self.loss_types, loss_weights=self.loss_weights,\n            optimizer=Adam(lr=_lr_schedule(0)), metrics=[f1])\n\n        return model\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/core/mseed_predictor.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\n# V1.59 created by @author: mostafamousavi\n# Ref Repo: https://github.com/smousavi05/EQTransformer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n# import os\n# os.environ['KERAS_BACKEND']='tensorflow'\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import load_model\n# from tensorflow.keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport csv\n#from tensorflow import keras\nimport keras\nimport time\nfrom os import listdir\nimport os\nimport platform\nimport shutil\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta\nimport contextlib\nimport sys\nimport warnings\nfrom scipy import signal\nfrom matplotlib.lines import Line2D\nfrom obspy import read\nfrom os.path import join\nimport json\nimport pickle\nimport faulthandler; faulthandler.enable()\nimport obspy\nimport logging\nfrom obspy.signal.trigger import trigger_onset\nfrom .EqT_utils import f1, SeqSelfAttention, FeedForward, LayerNormalization\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.python.util import deprecation\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\n\ntry:\n    f = open('setup.py')\n    for li, l in enumerate(f):\n        if li == 8:\n            EQT_VERSION = l.split('\"')[1]\nexcept Exception:\n    EQT_VERSION = \"0.1.59\"\n\n\ndef mseed_predictor(input_dir='downloads_mseeds',\n              input_model=\"sampleData&Model/EqT1D8pre_048.h5\",\n              stations_json= \"station_list.json\",\n              output_dir=\"detections\",\n              detection_threshold=0.3,\n              P_threshold=0.1,\n              S_threshold=0.1,\n              number_of_plots=10,\n              plot_mode='time',\n              loss_weights=[0.03, 0.40, 0.58],\n              loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\n              normalization_mode='std',\n              batch_size=500,\n              overlap = 0.3,\n              gpuid=None,\n              gpu_limit=None,\n              overwrite=False):\n\n    \"\"\"\n\n    To perform fast detection directly on mseed data.\n\n    Parameters\n    ----------\n    input_dir: str\n        Directory name containing hdf5 and csv files-preprocessed data.\n\n    input_model: str\n        Path to a trained model.\n\n    stations_json: str\n        Path to a JSON file containing station information.\n\n    output_dir: str\n        Output directory that will be generated.\n\n    detection_threshold: float, default=0.3\n        A value in which the detection probabilities above it will be considered as an event.\n\n    P_threshold: float, default=0.1\n        A value which the P probabilities above it will be considered as P arrival.\n\n    S_threshold: float, default=0.1\n        A value which the S probabilities above it will be considered as S arrival.\n\n    number_of_plots: float, default=10\n        The number of plots for detected events outputed for each station data.\n\n    plot_mode: str, default=time\n        The type of plots: time only time series or time_frequency time and spectrograms.\n\n    loss_weights: list, default=[0.03, 0.40, 0.58]\n        Loss weights for detection P picking and S picking respectively.\n\n    loss_types: list, default=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy']\n        Loss types for detection P picking and S picking respectively.\n\n    normalization_mode: str, default=std\n        Mode of normalization for data preprocessing max maximum amplitude among three components std standard deviation.\n\n    batch_size: int, default=500\n        Batch size. This wont affect the speed much but can affect the performance. A value beteen 200 to 1000 is recommanded.\n\n    overlap: float, default=0.3\n        If set the detection and picking are performed in overlapping windows.\n\n    gpuid: int\n        Id of GPU used for the prediction. If using CPU set to None.\n\n    gpu_limit: int\n       Set the maximum percentage of memory usage for the GPU.\n\n    overwrite: Bolean, default=False\n        Overwrite your results automatically.\n\n    Returns\n    --------\n    output_dir/STATION_OUTPUT/X_prediction_results.csv: A table containing all the detection, and picking results. Duplicated events are already removed.\n    output_dir/STATION_OUTPUT/X_report.txt: A summary of the parameters used for prediction and performance.\n    output_dir/STATION_OUTPUT/figures: A folder containing plots detected events and picked arrival times.\n    time_tracks.pkl: A file containing the time track of the continous data and its type.\n\n    Note\n    --------\n    This does not allow uncertainty estimation or writing the probabilities out.\n\n\n    \"\"\"\n\n\n    args = {\n    \"input_dir\": input_dir,\n    \"input_model\": input_model,\n    \"stations_json\": stations_json,\n    \"output_dir\": output_dir,\n    \"detection_threshold\": detection_threshold,\n    \"P_threshold\": P_threshold,\n    \"S_threshold\": S_threshold,\n    \"number_of_plots\": number_of_plots,\n    \"plot_mode\": plot_mode,\n    \"loss_weights\": loss_weights,\n    \"loss_types\": loss_types,\n    \"normalization_mode\": normalization_mode,\n    \"overlap\": overlap,\n    \"batch_size\": batch_size,\n    \"gpuid\": gpuid,\n    \"gpu_limit\": gpu_limit\n    }\n\n    if args['gpuid']:\n        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\n        tf.Session(config=tf.ConfigProto(log_device_placement=True))\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit'])\n        K.tensorflow_backend.set_session(tf.Session(config=config))\n\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',\n                        datefmt='%m-%d %H:%M')\n\n    class DummyFile(object):\n        file = None\n        def __init__(self, file):\n            self.file = file\n\n        def write(self, x):\n            # Avoid print() second call (useless \\n)\n            if len(x.rstrip()) > 0:\n                tqdm.write(x, file=self.file)\n\n    @contextlib.contextmanager\n    def nostdout():\n        save_stdout = sys.stdout\n        sys.stdout = DummyFile(sys.stdout)\n        yield\n        sys.stdout = save_stdout\n\n\n    # print('============================================================================')\n    # print('Running EqTransformer ', str(EQT_VERSION))\n    eqt_logger = logging.getLogger(\"EQTransformer\")\n    eqt_logger.info(f\"Running EqTransformer  {EQT_VERSION}\")\n\n    # print(' *** Loading the model ...', flush=True)\n    eqt_logger.info(f\"*** Loading the model ...\")\n    model = load_model(args['input_model'],\n                       custom_objects={'SeqSelfAttention': SeqSelfAttention,\n                                       'FeedForward': FeedForward,\n                                       'LayerNormalization': LayerNormalization,\n                                       'f1': f1\n                                        })\n    model.compile(loss = args['loss_types'],\n                  loss_weights = args['loss_weights'],\n                  optimizer = Adam(lr = 0.001),\n                  metrics = [f1])\n    # print('*** Loading is complete!', flush=True)\n    eqt_logger.info(f\"*** Loading is complete!\")\n\n\n    out_dir = os.path.join(os.getcwd(), str(args['output_dir']))\n    if os.path.isdir(out_dir):\n        # print('============================================================================')\n        # print(f' *** {out_dir} already exists!')\n        eqt_logger.info(f\"*** {out_dir} already exists!\")\n        if overwrite == True:\n            inp = \"y\"\n            eqt_logger.info(f\"Overwriting your previous results\")\n            # print(\"Overwriting your previous results\")\n        else:\n            inp = input(\" --> Type (Yes or y) to create a new empty directory! This will erase your previous results so make a copy if you want them.\")\n        if inp.lower() == \"yes\" or inp.lower() == \"y\":\n            shutil.rmtree(out_dir)\n            os.makedirs(out_dir)\n        else:\n            print(\"Okay.\")\n            return\n\n    if platform.system() == 'Windows':\n        station_list = [ev.split(\".\")[0] for ev in listdir(args['input_dir']) if ev.split(\"\\\\\")[-1] != \".DS_Store\"];\n    else:\n        station_list = [ev.split(\".\")[0] for ev in listdir(args['input_dir']) if ev.split(\"/\")[-1] != \".DS_Store\"];\n\n\n    station_list = sorted(set(station_list))\n\n    data_track = dict()\n\n    # print(f\"######### There are files for {len(station_list)} stations in {args['input_dir']} directory. #########\", flush=True)\n    eqt_logger.info(f\"There are files for {len(station_list)} stations in {args['input_dir']} directory.\")\n    for ct, st in enumerate(station_list):\n\n        save_dir = os.path.join(out_dir, str(st)+'_outputs')\n        save_figs = os.path.join(save_dir, 'figures')\n        if os.path.isdir(save_dir):\n            shutil.rmtree(save_dir)\n        os.makedirs(save_dir)\n        if args['number_of_plots']:\n            os.makedirs(save_figs)\n\n        plt_n = 0\n        csvPr_gen = open(os.path.join(save_dir,'X_prediction_results.csv'), 'w')\n        predict_writer = csv.writer(csvPr_gen, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        predict_writer.writerow(['file_name',\n                                 'network',\n                                 'station',\n                                 'instrument_type',\n                                 'station_lat',\n                                 'station_lon',\n                                 'station_elv',\n                                 'event_start_time',\n                                 'event_end_time',\n                                 'detection_probability',\n                                 'detection_uncertainty',\n                                 'p_arrival_time',\n                                 'p_probability',\n                                 'p_uncertainty',\n                                 'p_snr',\n                                 's_arrival_time',\n                                 's_probability',\n                                 's_uncertainty',\n                                 's_snr'\n                                     ])\n        csvPr_gen.flush()\n        # print(f'========= Started working on {st}, {ct+1} out of {len(station_list)} ...', flush=True)\n        eqt_logger.info(f\"Started working on {st}, {ct+1} out of {len(station_list)} ...\")\n\n\n        start_Predicting = time.time()\n        if platform.system() == 'Windows':\n            file_list = [join(st, ev) for ev in listdir(args[\"input_dir\"]+\"\\\\\"+st) if ev.split(\"\\\\\")[-1].split(\".\")[-1].lower() == \"mseed\"];\n        else:\n            file_list = [join(st, ev) for ev in listdir(args[\"input_dir\"]+\"/\"+st) if ev.split(\"/\")[-1].split(\".\")[-1].lower() == \"mseed\"];\n\n        mon = [ev.split('__')[1]+'__'+ev.split('__')[2] for ev in file_list ];\n        uni_list = list(set(mon))\n        uni_list.sort()\n\n        time_slots, comp_types = [], []\n\n        # print('============ Station {} has {} chunks of data.'.format(st, len(uni_list)), flush=True)\n        for _, month in enumerate(uni_list):\n            eqt_logger.info(f\"{month}\")\n            matching = [s for s in file_list if month in s]\n            meta, time_slots, comp_types, data_set = _mseed2nparry(args, matching, time_slots, comp_types, st)\n\n            params_pred = {'batch_size': args['batch_size'],\n                           'norm_mode': args['normalization_mode']}\n\n            pred_generator = PreLoadGeneratorTest(meta[\"trace_start_time\"], data_set, **params_pred)\n\n            predD, predP, predS = model.predict_generator(pred_generator)\n\n            detection_memory = []\n            for ix in range(len(predD)):\n                matches, pick_errors, yh3 =  _picker(args, predD[ix][:, 0], predP[ix][:, 0], predS[ix][:, 0])\n                if (len(matches) >= 1) and ((matches[list(matches)[0]][3] or matches[list(matches)[0]][6])):\n                    snr = [_get_snr(data_set[meta[\"trace_start_time\"][ix]], matches[list(matches)[0]][3], window = 100), _get_snr(data_set[meta[\"trace_start_time\"][ix]], matches[list(matches)[0]][6], window = 100)]\n                    pre_write = len(detection_memory)\n                    detection_memory=_output_writter_prediction(meta, predict_writer, csvPr_gen, matches, snr, detection_memory, ix)\n                    post_write = len(detection_memory)\n                    if plt_n < args['number_of_plots'] and post_write > pre_write:\n                        _plotter_prediction(data_set[meta[\"trace_start_time\"][ix]], args, save_figs, predD[ix][:, 0], predP[ix][:, 0], predS[ix][:, 0], meta[\"trace_start_time\"][ix], matches)\n                        plt_n += 1\n\n        end_Predicting = time.time()\n        data_track[st]=[time_slots, comp_types]\n        delta = (end_Predicting - start_Predicting)\n        hour = int(delta / 3600)\n        delta -= hour * 3600\n        minute = int(delta / 60)\n        delta -= minute * 60\n        seconds = delta\n\n        dd = pd.read_csv(os.path.join(save_dir,'X_prediction_results.csv'))\n        print(f'\\n', flush=True)\n        eqt_logger.info(f\"Finished the prediction in: {hour} hours and {minute} minutes and {round(seconds, 2)} seconds.\")\n        eqt_logger.info(f'*** Detected: '+str(len(dd))+' events.')\n        eqt_logger.info(f' *** Wrote the results into --> \" ' + str(save_dir)+' \"')\n        # print(' *** Finished the prediction in: {} hours and {} minutes and {} seconds.'.format(hour, minute, round(seconds, 2)), flush=True)\n        # print(' *** Detected: '+str(len(dd))+' events.', flush=True)\n        # print(' *** Wrote the results into --> \" ' + str(save_dir)+' \"', flush=True)\n\n        with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file:\n            the_file.write('================== PREDICTION FROM MSEED ===================='+'\\n')\n            the_file.write('================== Overal Info =============================='+'\\n')\n            the_file.write('date of report: '+str(datetime.now())+'\\n')\n            the_file.write('input_model: '+str(args['input_model'])+'\\n')\n            the_file.write('input_dir: '+str(args['input_dir'])+'\\n')\n            the_file.write('output_dir: '+str(save_dir)+'\\n')\n            the_file.write('================== Prediction Parameters ====================='+'\\n')\n            the_file.write('finished the prediction in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 2)))\n            the_file.write('detected: '+str(len(dd))+' events.'+'\\n')\n            the_file.write('loss_types: '+str(args['loss_types'])+'\\n')\n            the_file.write('loss_weights: '+str(args['loss_weights'])+'\\n')\n            the_file.write('================== Other Parameters =========================='+'\\n')\n            the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\n            the_file.write('overlap: '+str(args['overlap'])+'\\n')\n            the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\n            the_file.write('detection_threshold: '+str(args['detection_threshold'])+'\\n')\n            the_file.write('P_threshold: '+str(args['P_threshold'])+'\\n')\n            the_file.write('S_threshold: '+str(args['S_threshold'])+'\\n')\n            the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\\n')\n            the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\n            the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')\n\n    with open('time_tracks.pkl', 'wb') as f:\n        pickle.dump(data_track, f, pickle.HIGHEST_PROTOCOL)\n\n\n\n\ndef _mseed2nparry(args, matching, time_slots, comp_types, st_name):\n    ' read miniseed files and from a list of string names and returns 3 dictionaries of numpy arrays, meta data, and time slice info'\n\n    json_file = open(args['stations_json'])\n    stations_ = json.load(json_file)\n\n    st = obspy.core.Stream()\n    tsw = False\n    for m in matching:\n        temp_st = read(os.path.join(str(args['input_dir']), m),debug_headers=True)\n        if tsw == False and temp_st:\n            tsw = True\n            for tr in temp_st:\n                time_slots.append((tr.stats.starttime, tr.stats.endtime))\n        try:\n            temp_st.merge(fill_value=0)\n        except Exception:\n            temp_st =_resampling(temp_st)\n            temp_st.merge(fill_value=0)\n        temp_st.detrend('demean')\n        st += temp_st\n\n    st.filter(type='bandpass', freqmin = 1.0, freqmax = 45, corners=2, zerophase=True)\n    st.taper(max_percentage=0.001, type='cosine', max_length=2)\n    if len([tr for tr in st if tr.stats.sampling_rate != 100.0]) != 0:\n        try:\n            st.interpolate(100, method=\"linear\")\n        except Exception:\n            st=_resampling(st)\n\n    st.trim(min([tr.stats.starttime for tr in st]), max([tr.stats.endtime for tr in st]), pad=True, fill_value=0)\n\n    start_time = st[0].stats.starttime\n    end_time = st[0].stats.endtime\n\n    meta = {\"start_time\":start_time,\n            \"end_time\": end_time,\n            \"trace_name\":m\n             }\n\n    chanL = [tr.stats.channel[-1] for tr in st]\n    comp_types.append(len(chanL))\n    tim_shift = int(60-(args['overlap']*60))\n    next_slice = start_time+60\n\n    data_set={}\n\n    sl = 0; st_times = []\n    while next_slice <= end_time:\n        npz_data = np.zeros([6000, 3])\n        st_times.append(str(start_time).replace('T', ' ').replace('Z', ''))\n        w = st.slice(start_time, next_slice)\n        if 'Z' in chanL:\n            npz_data[:,2] = w[chanL.index('Z')].data[:6000]\n        if ('E' in chanL) or ('1' in chanL):\n            try:\n                npz_data[:,0] = w[chanL.index('E')].data[:6000]\n            except Exception:\n                npz_data[:,0] = w[chanL.index('1')].data[:6000]\n        if ('N' in chanL) or ('2' in chanL):\n            try:\n                npz_data[:,1] = w[chanL.index('N')].data[:6000]\n            except Exception:\n                npz_data[:,1] = w[chanL.index('2')].data[:6000]\n\n        data_set.update( {str(start_time).replace('T', ' ').replace('Z', '') : npz_data})\n\n        start_time = start_time+tim_shift\n        next_slice = next_slice+tim_shift\n        sl += 1\n\n    meta[\"trace_start_time\"] = st_times\n\n    try:\n        meta[\"receiver_code\"]=st[0].stats.station\n        meta[\"instrument_type\"]=st[0].stats.channel[:2]\n        meta[\"network_code\"]=stations_[st[0].stats.station]['network']\n        meta[\"receiver_latitude\"]=stations_[st[0].stats.station]['coords'][0]\n        meta[\"receiver_longitude\"]=stations_[st[0].stats.station]['coords'][1]\n        meta[\"receiver_elevation_m\"]=stations_[st[0].stats.station]['coords'][2]\n    except Exception:\n        meta[\"receiver_code\"]=st_name\n        meta[\"instrument_type\"]=stations_[st_name]['channels'][0][:2]\n        meta[\"network_code\"]=stations_[st_name]['network']\n        meta[\"receiver_latitude\"]=stations_[st_name]['coords'][0]\n        meta[\"receiver_longitude\"]=stations_[st_name]['coords'][1]\n        meta[\"receiver_elevation_m\"]=stations_[st_name]['coords'][2]\n\n    return meta, time_slots, comp_types, data_set\n\n\n\nclass PreLoadGeneratorTest(keras.utils.Sequence):\n\n    \"\"\"\n\n    Keras generator with preprocessing. For testing. Pre-load version.\n\n    Parameters\n    ----------\n    list_IDsx: str\n        List of trace names.\n\n    file_name: str\n        Path to the input hdf5 file.\n\n    dim: tuple\n        Dimension of input traces.\n\n    batch_size: int, default=32.\n        Batch size.\n\n    n_channels: int, default=3.\n        Number of channels.\n\n    norm_mode: str, default=max\n        The mode of normalization, 'max' or 'std'\n\n    Returns\n    --------\n    Batches of two dictionaries: {'input': X}: pre-processed waveform as input {'detector': y1, 'picker_P': y2, 'picker_S': y3}: outputs including three separate numpy arrays as labels for detection, P, and S respectively.\n\n\n    \"\"\"\n\n    def __init__(self,\n                 list_IDs,\n                 inp_data,\n                 batch_size=32,\n                 norm_mode = 'std'):\n\n        'Initialization'\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.inp_data = inp_data\n        self.on_epoch_end()\n        self.norm_mode = norm_mode\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        try:\n            return int(np.floor(len(self.list_IDs) / self.batch_size))\n        except ZeroDivisionError:\n            print(\"Your data duration in mseed file is too short! Try either longer files or reducing batch_size. \")\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        X = self.__data_generation(list_IDs_temp)\n        return ({'input': X})\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def _normalize(self, data, mode = 'max'):\n        data -= np.mean(data, axis=0, keepdims=True)\n        if mode == 'max':\n            max_data = np.max(data, axis=0, keepdims=True)\n            assert(max_data.shape[-1] == data.shape[-1])\n            max_data[max_data == 0] = 1\n            data /= max_data\n\n        elif mode == 'std':\n            std_data = np.std(data, axis=0, keepdims=True)\n            assert(std_data.shape[-1] == data.shape[-1])\n            std_data[std_data == 0] = 1\n            data /= std_data\n        return data\n\n    def __data_generation(self, list_IDs_temp):\n        'readint the waveforms'\n        X = np.zeros((self.batch_size, 6000, 3))\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            data = self.inp_data[ID]\n            data = self._normalize(data, self.norm_mode)\n            X[i, :, :] = data\n\n        return X\n\n\ndef _output_writter_prediction(meta, predict_writer, csvPr, matches, snr, detection_memory, idx):\n\n    \"\"\"\n\n    Writes the detection & picking results into a CSV file.\n\n    Parameters\n    ----------\n    dataset: hdf5 obj\n        Dataset object of the trace.\n\n    predict_writer: obj\n        For writing out the detection/picking results in the CSV file.\n\n    csvPr: obj\n        For writing out the detection/picking results in the CSV file.\n\n    matches: dic\n        It contains the information for the detected and picked event.\n\n    snr: list of two floats\n        Estimated signal to noise ratios for picked P and S phases.\n\n    detection_memory : list\n        Keep the track of detected events.\n\n    Returns\n    -------\n    detection_memory : list\n        Keep the track of detected events.\n\n\n    \"\"\"\n\n    station_name = meta[\"receiver_code\"]\n    station_lat = meta[\"receiver_latitude\"]\n    station_lon = meta[\"receiver_longitude\"]\n    station_elv = meta[\"receiver_elevation_m\"]\n    start_time = meta[\"trace_start_time\"][idx]\n    station_name = \"{:<4}\".format(station_name)\n    network_name = meta[\"network_code\"]\n    network_name = \"{:<2}\".format(network_name)\n    instrument_type = meta[\"instrument_type\"]\n    instrument_type = \"{:<2}\".format(instrument_type)\n\n    try:\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f')\n    except Exception:\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n\n    def _date_convertor(r):\n        if isinstance(r, str):\n            mls = r.split('.')\n            if len(mls) == 1:\n                new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S')\n            else:\n                new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S.%f')\n        else:\n            new_t = r\n\n        return new_t\n\n    for match, match_value in matches.items():\n        ev_strt = start_time+timedelta(seconds= match/100)\n        ev_end = start_time+timedelta(seconds= match_value[0]/100)\n\n        doublet = [ st for st in detection_memory if abs((st-ev_strt).total_seconds()) < 2]\n\n        if len(doublet) == 0:\n            det_prob = round(match_value[1], 2)\n\n            if match_value[3]:\n                p_time = start_time+timedelta(seconds= match_value[3]/100)\n            else:\n                p_time = None\n            p_prob = match_value[4]\n\n            if p_prob:\n                p_prob = round(p_prob, 2)\n\n            if match_value[6]:\n                s_time = start_time+timedelta(seconds= match_value[6]/100)\n            else:\n                s_time = None\n            s_prob = match_value[7]\n            if s_prob:\n                s_prob = round(s_prob, 2)\n\n            predict_writer.writerow([meta[\"trace_name\"],\n                                         network_name,\n                                         station_name,\n                                         instrument_type,\n                                         station_lat,\n                                         station_lon,\n                                         station_elv,\n                                         _date_convertor(ev_strt),\n                                         _date_convertor(ev_end),\n                                         det_prob,\n                                         None,\n                                         _date_convertor(p_time),\n                                         p_prob,\n                                         None,\n                                         snr[0],\n                                         _date_convertor(s_time),\n                                         s_prob,\n                                         None,\n                                         snr[1]\n                                         ])\n\n            csvPr.flush()\n            detection_memory.append(ev_strt)\n\n    return detection_memory\n\n\n\ndef _get_snr(data, pat, window=200):\n\n    \"\"\"\n\n    Estimates SNR.\n\n    Parameters\n    ----------\n    data : numpy array\n        3 component data.\n\n    pat: positive integer\n        Sample point where a specific phase arrives.\n\n    window: positive integer, default=200\n        The length of the window for calculating the SNR (in the sample).\n\n    Returns\n   --------\n    snr : {float, None}\n       Estimated SNR in db.\n\n\n    \"\"\"\n\n    snr = None\n    if pat:\n        try:\n            if int(pat) >= window and (int(pat)+window) < len(data):\n                nw1 = data[int(pat)-window : int(pat)];\n                sw1 = data[int(pat) : int(pat)+window];\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\n            elif int(pat) < window and (int(pat)+window) < len(data):\n                window = int(pat)\n                nw1 = data[int(pat)-window : int(pat)];\n                sw1 = data[int(pat) : int(pat)+window];\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\n            elif (int(pat)+window) > len(data):\n                window = len(data)-int(pat)\n                nw1 = data[int(pat)-window : int(pat)];\n                sw1 = data[int(pat) : int(pat)+window];\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\n        except Exception:\n            pass\n    return snr\n\n\n\ndef _detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False):\n\n    \"\"\"\n\n    Detect peaks in data based on their amplitude and other features.\n\n    Parameters\n    ----------\n    x : 1D array_like\n        data.\n\n    mph : {None, number}, default=None\n        detect peaks that are greater than minimum peak height.\n\n    mpd : int, default=1\n        detect peaks that are at least separated by minimum peak distance (in number of data).\n\n    threshold : int, default=0\n        detect peaks (valleys) that are greater (smaller) than `threshold in relation to their immediate neighbors.\n\n    edge : str, default=rising\n        for a flat peak, keep only the rising edge ('rising'), only the falling edge ('falling'), both edges ('both'), or don't detect a flat peak (None).\n\n    kpsh : bool, default=False\n        keep peaks with same height even if they are closer than `mpd`.\n\n    valley : bool, default=False\n        if True (1), detect valleys (local minima) instead of peaks.\n\n    Returns\n    -------\n    ind : 1D array_like\n        indeces of the peaks in `x`.\n\n    Modified from\n    ----------\n    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n\n\n    \"\"\"\n\n    x = np.atleast_1d(x).astype('float64')\n    if x.size < 3:\n        return np.array([], dtype=int)\n    if valley:\n        x = -x\n    # find indices of all peaks\n    dx = x[1:] - x[:-1]\n    # handle NaN's\n    indnan = np.where(np.isnan(x))[0]\n    if indnan.size:\n        x[indnan] = np.inf\n        dx[np.where(np.isnan(dx))[0]] = np.inf\n    ine, ire, ife = np.array([[], [], []], dtype=int)\n    if not edge:\n        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n    else:\n        if edge.lower() in ['rising', 'both']:\n            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n        if edge.lower() in ['falling', 'both']:\n            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n    ind = np.unique(np.hstack((ine, ire, ife)))\n    # handle NaN's\n    if ind.size and indnan.size:\n        # NaN's and values close to NaN's cannot be peaks\n        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n    # first and last values of x cannot be peaks\n    if ind.size and ind[0] == 0:\n        ind = ind[1:]\n    if ind.size and ind[-1] == x.size-1:\n        ind = ind[:-1]\n    # remove peaks < minimum peak height\n    if ind.size and mph is not None:\n        ind = ind[x[ind] >= mph]\n    # remove peaks - neighbors < threshold\n    if ind.size and threshold > 0:\n        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n        ind = np.delete(ind, np.where(dx < threshold)[0])\n    # detect small peaks closer than minimum peak distance\n    if ind.size and mpd > 1:\n        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n        idel = np.zeros(ind.size, dtype=bool)\n        for i in range(ind.size):\n            if not idel[i]:\n                # keep peaks with the same height if kpsh is True\n                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n                    & (x[ind[i]] > x[ind] if kpsh else True)\n                idel[i] = 0  # Keep current peak\n        # remove the small peaks and sort back the indices by their occurrence\n        ind = np.sort(ind[~idel])\n\n    return ind\n\n\n\n\n\ndef _picker(args, yh1, yh2, yh3):\n\n    \"\"\"\n\n    Performs detection and picking.\n\n    Parameters\n    ----------\n    args : dic\n        A dictionary containing all of the input parameters.\n\n    yh1 : 1D array\n        Detection probabilities.\n\n    yh2 : 1D array\n        P arrival probabilities.\n\n    yh3 : 1D array\n        S arrival probabilities.\n\n\n    Returns\n    -------\n    matches : dic\n        Contains the information for the detected and picked event.\n\n    matches : dic\n        {detection statr-time:[ detection end-time, detection probability, detectin uncertainty, P arrival, P probabiliy, P uncertainty, S arrival,  S probability, S uncertainty]}\n\n    yh3 : 1D array\n        normalized S_probability\n\n    \"\"\"\n\n    detection = trigger_onset(yh1, args['detection_threshold'], args['detection_threshold'])\n    pp_arr = _detect_peaks(yh2, mph=args['P_threshold'], mpd=1)\n    ss_arr = _detect_peaks(yh3, mph=args['S_threshold'], mpd=1)\n\n    P_PICKS = {}\n    S_PICKS = {}\n    EVENTS = {}\n    matches = {}\n    pick_errors = {}\n    if len(pp_arr) > 0:\n        P_uncertainty = None\n\n        for pick in range(len(pp_arr)):\n            pauto = pp_arr[pick]\n\n            if pauto:\n                P_prob = np.round(yh2[int(pauto)], 3)\n                P_PICKS.update({pauto : [P_prob, P_uncertainty]})\n\n    if len(ss_arr) > 0:\n        S_uncertainty = None\n\n        for pick in range(len(ss_arr)):\n            sauto = ss_arr[pick]\n\n            if sauto:\n                S_prob = np.round(yh3[int(sauto)], 3)\n                S_PICKS.update({sauto : [S_prob, S_uncertainty]})\n\n    if len(detection) > 0:\n        D_uncertainty = None\n\n        for ev in range(len(detection)):\n\n            D_prob = np.mean(yh1[detection[ev][0]:detection[ev][1]])\n            D_prob = np.round(D_prob, 3)\n\n            EVENTS.update({ detection[ev][0] : [D_prob, D_uncertainty, detection[ev][1]]})\n\n    # matching the detection and picks\n    def pair_PS(l1, l2, dist):\n        l1.sort()\n        l2.sort()\n        b = 0\n        e = 0\n        ans = []\n\n        for a in l1:\n            while l2[b] and b < len(l2) and a - l2[b] > dist:\n                b += 1\n            while l2[e] and e < len(l2) and l2[e] - a <= dist:\n                e += 1\n            ans.extend([[a,x] for x in l2[b:e]])\n\n        best_pair = None\n        for pr in ans:\n            ds = pr[1]-pr[0]\n            if abs(ds) < dist:\n                best_pair = pr\n                dist = ds\n        return best_pair\n\n\n    for ev in EVENTS:\n        bg = ev\n        ed = EVENTS[ev][2]\n        if int(ed-bg) >= 10:\n\n            candidate_Ss = {}\n            for Ss, S_val in S_PICKS.items():\n                if Ss > bg and Ss < ed:\n                    candidate_Ss.update({Ss : S_val})\n\n            if len(candidate_Ss) > 1:\n                candidate_Ss = {list(candidate_Ss.keys())[0] : candidate_Ss[list(candidate_Ss.keys())[0]]}\n\n\n            if len(candidate_Ss) == 0:\n                    candidate_Ss = {None:[None, None]}\n\n            candidate_Ps = {}\n            for Ps, P_val in P_PICKS.items():\n                if list(candidate_Ss)[0]:\n                    if Ps > bg-100 and Ps < list(candidate_Ss)[0]-10:\n                        candidate_Ps.update({Ps : P_val})\n                else:\n                    if Ps > bg-100 and Ps < ed:\n                        candidate_Ps.update({Ps : P_val})\n\n            if len(candidate_Ps) > 1:\n                Pr_st = 0\n                buffer = {}\n                for PsCan, P_valCan in candidate_Ps.items():\n                    if P_valCan[0] > Pr_st:\n                        buffer = {PsCan : P_valCan}\n                        Pr_st = P_valCan[0]\n                candidate_Ps = buffer\n\n            if len(candidate_Ps) == 0:\n                    candidate_Ps = {None:[None, None]}\n\n            if list(candidate_Ss)[0] or list(candidate_Ps)[0]:\n                matches.update({\n                                bg:[ed,\n                                    EVENTS[ev][0],\n                                    EVENTS[ev][1],\n\n                                    list(candidate_Ps)[0],\n                                    candidate_Ps[list(candidate_Ps)[0]][0],\n                                    candidate_Ps[list(candidate_Ps)[0]][1],\n\n                                    list(candidate_Ss)[0],\n                                    candidate_Ss[list(candidate_Ss)[0]][0],\n                                    candidate_Ss[list(candidate_Ss)[0]][1],\n                                                ] })\n\n    return matches, pick_errors, yh3\n\n\n\ndef _resampling(st):\n    'perform resampling on Obspy stream objects'\n\n    need_resampling = [tr for tr in st if tr.stats.sampling_rate != 100.0]\n    if len(need_resampling) > 0:\n       # print('resampling ...', flush=True)\n        for indx, tr in enumerate(need_resampling):\n            if tr.stats.delta < 0.01:\n                tr.filter('lowpass',freq=45,zerophase=True)\n            tr.resample(100)\n            tr.stats.sampling_rate = 100\n            tr.stats.delta = 0.01\n            tr.data.dtype = 'int32'\n            st.remove(tr)\n            st.append(tr)\n    return st\n\n\n\ndef _normalize(data, mode = 'max'):\n    \"\"\"\n\n    Normalize 3D arrays.\n\n    Parameters\n    ----------\n    data : 3D numpy array\n        3 component traces.\n\n    mode : str, default='std'\n        Mode of normalization. 'max' or 'std'\n\n    Returns\n    -------\n    data : 3D numpy array\n        normalized data.\n\n    \"\"\"\n\n    data -= np.mean(data, axis=0, keepdims=True)\n    if mode == 'max':\n        max_data = np.max(data, axis=0, keepdims=True)\n        assert(max_data.shape[-1] == data.shape[-1])\n        max_data[max_data == 0] = 1\n        data /= max_data\n\n    elif mode == 'std':\n        std_data = np.std(data, axis=0, keepdims=True)\n        assert(std_data.shape[-1] == data.shape[-1])\n        std_data[std_data == 0] = 1\n        data /= std_data\n    return data\n\n\n\n\ndef _plotter_prediction(data, args, save_figs, yh1, yh2, yh3, evi, matches):\n\n    \"\"\"\n\n    Generates plots of detected events with the prediction probabilities and arrival picks.\n\n    Parameters\n    ----------\n    data: NumPy array\n        3 component raw waveform.\n\n    evi: str\n        Trace name.\n\n    args: dic\n        A dictionary containing all of the input parameters.\n\n    save_figs: str\n        Path to the folder for saving the plots.\n\n    yh1: 1D array\n        Detection probabilities.\n\n    yh2: 1D array\n        P arrival probabilities.\n\n    yh3: 1D array\n        S arrival probabilities.\n\n    matches: dic\n        Contains the information for the detected and picked event.\n\n\n    \"\"\"\n\n    font0 = {'family': 'serif',\n            'color': 'white',\n            'stretch': 'condensed',\n            'weight': 'normal',\n            'size': 12,\n            }\n\n    spt, sst, detected_events = [], [], []\n    for match, match_value in matches.items():\n        detected_events.append([match, match_value[0]])\n        if match_value[3]:\n            spt.append(match_value[3])\n        else:\n            spt.append(None)\n\n        if match_value[6]:\n            sst.append(match_value[6])\n        else:\n            sst.append(None)\n\n    if args['plot_mode'] == 'time_frequency':\n\n        fig = plt.figure(constrained_layout=False)\n        widths = [6, 1]\n        heights = [1, 1, 1, 1, 1, 1, 1.8]\n        spec5 = fig.add_gridspec(ncols=2, nrows=7, width_ratios=widths,\n                              height_ratios=heights, left=0.1, right=0.9, hspace=0.1)\n\n\n        ax = fig.add_subplot(spec5[0, 0])\n        plt.plot(data[:, 0], 'k')\n        plt.xlim(0, 6000)\n        x = np.arange(6000)\n        if platform.system() == 'Windows':\n            plt.title(save_figs.split(\"\\\\\")[-2].split(\"_\")[0]+\":\"+str(evi))\n        else:\n            plt.title(save_figs.split(\"/\")[-2].split(\"_\")[0]+\":\"+str(evi))\n\n        ax.set_xticks([])\n        plt.rcParams[\"figure.figsize\"] = (10, 10)\n        legend_properties = {'weight':'bold'}\n\n        pl = None\n        sl = None\n\n        if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n\n        ax = fig.add_subplot(spec5[0, 1])\n        if pl or sl:\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['E', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\n            plt.axis('off')\n\n        ax = fig.add_subplot(spec5[1, 0])\n        f, t, Pxx = signal.stft(data[:, 0], fs=100, nperseg=80)\n        Pxx = np.abs(Pxx)\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\n        plt.ylim(0, 40)\n        plt.text(1, 1, 'STFT', fontdict=font0)\n        plt.ylabel('Hz', fontsize=12)\n        ax.set_xticks([])\n\n        ax = fig.add_subplot(spec5[2, 0])\n        plt.plot(data[:, 1] , 'k')\n        plt.xlim(0, 6000)\n\n        ax.set_xticks([])\n        if len(spt) > 0 and np.count_nonzero(data[:, 1]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 1]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n        ax = fig.add_subplot(spec5[2, 1])\n        if pl or sl:\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['N', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\n            plt.axis('off')\n\n\n        ax = fig.add_subplot(spec5[3, 0])\n        f, t, Pxx = signal.stft(data[:, 1], fs=100, nperseg=80)\n        Pxx = np.abs(Pxx)\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\n        plt.ylim(0, 40)\n        plt.text(1, 1, 'STFT', fontdict=font0)\n        plt.ylabel('Hz', fontsize=12)\n        ax.set_xticks([])\n\n\n        ax = fig.add_subplot(spec5[4, 0])\n        plt.plot(data[:, 2], 'k')\n        plt.xlim(0, 6000)\n\n        ax.set_xticks([])\n        if len(spt) > 0 and np.count_nonzero(data[:, 2]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 2]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n        ax = fig.add_subplot(spec5[4, 1])\n        if pl or sl:\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['Z', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\n            plt.axis('off')\n\n        ax = fig.add_subplot(spec5[5, 0])\n        f, t, Pxx = signal.stft(data[:, 2], fs=100, nperseg=80)\n        Pxx = np.abs(Pxx)\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\n        plt.ylim(0, 40)\n        plt.text(1, 1, 'STFT', fontdict=font0)\n        plt.ylabel('Hz', fontsize=12)\n        ax.set_xticks([])\n\n        ax = fig.add_subplot(spec5[6, 0])\n        x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)\n\n        plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=2, label='Earthquake')\n        plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=2, label='P_arrival')\n        plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=2, label='S_arrival')\n        plt.tight_layout()\n        plt.ylim((-0.1, 1.1))\n        plt.xlim(0, 6000)\n        plt.ylabel('Probability', fontsize=12)\n        plt.xlabel('Sample', fontsize=12)\n        plt.yticks(np.arange(0, 1.1, step=0.2))\n        axes = plt.gca()\n        axes.yaxis.grid(color='lightgray')\n\n        ax = fig.add_subplot(spec5[6, 1])\n        custom_lines = [Line2D([0], [0], linestyle='--', color='mediumblue', lw=2),\n                        Line2D([0], [0], linestyle='--', color='c', lw=2),\n                        Line2D([0], [0], linestyle='--', color='m', lw=2)]\n        plt.legend(custom_lines, ['Earthquake', 'P_arrival', 'S_arrival'], fancybox=True, shadow=True)\n        plt.axis('off')\n\n        font = {'family': 'serif',\n                    'color': 'dimgrey',\n                    'style': 'italic',\n                    'stretch': 'condensed',\n                    'weight': 'normal',\n                    'size': 12,\n                    }\n\n        plt.text(1, 0.2, 'EQTransformer', fontdict=font)\n        if EQT_VERSION:\n            plt.text(2000, 0.05, str(EQT_VERSION), fontdict=font)\n\n        plt.xlim(0, 6000)\n        fig.tight_layout()\n        fig.savefig(os.path.join(save_figs, str(evi)+'.png'))\n        plt.close(fig)\n        plt.clf()\n\n\n    else:\n\n        ########################################## ploting only in time domain\n        fig = plt.figure(constrained_layout=True)\n        widths = [1]\n        heights = [1.6, 1.6, 1.6, 2.5]\n        spec5 = fig.add_gridspec(ncols=1, nrows=4, width_ratios=widths,\n                              height_ratios=heights)\n\n        ax = fig.add_subplot(spec5[0, 0])\n        plt.plot(data[:, 0], 'k')\n        x = np.arange(6000)\n        plt.xlim(0, 6000)\n\n        if platform.system() == 'Windows':\n            plt.title(save_figs.split(\"\\\\\")[-2].split(\"_\")[0]+\":\"+str(evi))\n        else:\n            plt.title(save_figs.split(\"/\")[-2].split(\"_\")[0]+\":\"+str(evi))\n\n        plt.ylabel('Amplitude\\nCounts')\n\n        plt.rcParams[\"figure.figsize\"] = (8,6)\n        legend_properties = {'weight':'bold'}\n\n        pl = sl = None\n        if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n        if pl or sl:\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['E', 'Picked P', 'Picked S'],\n                       loc='center left', bbox_to_anchor=(1, 0.5),\n                       fancybox=True, shadow=True)\n\n        ax = fig.add_subplot(spec5[1, 0])\n        plt.plot(data[:, 1] , 'k')\n        plt.xlim(0, 6000)\n        plt.ylabel('Amplitude\\nCounts')\n\n        if len(spt) > 0 and np.count_nonzero(data[:, 1]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 1]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n        if pl or sl:\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['N', 'Picked P', 'Picked S'],\n                       loc='center left', bbox_to_anchor=(1, 0.5),\n                       fancybox=True, shadow=True)\n\n        ax = fig.add_subplot(spec5[2, 0])\n        plt.plot(data[:, 2], 'k')\n        plt.xlim(0, 6000)\n        plt.ylabel('Amplitude\\nCounts')\n\n        ax.set_xticks([])\n\n        if len(spt) > 0 and np.count_nonzero(data[:, 2]) > 10:\n            ymin, ymax = ax.get_ylim()\n            for ipt, pt in enumerate(spt):\n                if pt and ipt == 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\n                elif pt and ipt > 0:\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\n\n        if len(sst) > 0 and np.count_nonzero(data[:, 2]) > 10:\n            for ist, st in enumerate(sst):\n                if st and ist == 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\n                elif st and ist > 0:\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\n\n        if pl or sl:\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\n                            Line2D([0], [0], color='c', lw=2),\n                            Line2D([0], [0], color='m', lw=2)]\n            plt.legend(custom_lines, ['Z', 'Picked P', 'Picked S'],\n                       loc='center left', bbox_to_anchor=(1, 0.5),\n                       fancybox=True, shadow=True)\n\n        ax = fig.add_subplot(spec5[3, 0])\n        x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)\n\n        plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=1.5, label='Earthquake')\n        plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=1.5, label='P_arrival')\n        plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=1.5, label='S_arrival')\n\n        plt.tight_layout()\n        plt.ylim((-0.1, 1.1))\n        plt.xlim(0, 6000)\n        plt.ylabel('Probability')\n        plt.xlabel('Sample')\n        plt.legend(loc='lower center', bbox_to_anchor=(0., 1.17, 1., .102), ncol=3, mode=\"expand\",\n                       prop=legend_properties,  borderaxespad=0., fancybox=True, shadow=True)\n        plt.yticks(np.arange(0, 1.1, step=0.2))\n        axes = plt.gca()\n        axes.yaxis.grid(color='lightgray')\n\n        font = {'family': 'serif',\n                    'color': 'dimgrey',\n                    'style': 'italic',\n                    'stretch': 'condensed',\n                    'weight': 'normal',\n                    'size': 12,\n                    }\n\n        plt.text(6500, 0.5, 'EQTransformer', fontdict=font)\n        if EQT_VERSION:\n            plt.text(7000, 0.1, str(EQT_VERSION), fontdict=font)\n\n        fig.tight_layout()\n        fig.savefig(os.path.join(save_figs, str(evi)+'.png'))\n        plt.close(fig)\n        plt.clf()\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/associator.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Dec 27 18:52:42 2019\n\n@author: mostafamousavi\n\nlast update: 01/29/2021\n\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm\nimport numpy as np\nimport json\nimport os\nimport platform\nimport sqlite3 \nimport pandas as pd\nimport csv\nfrom os import listdir\nimport h5py\n#import matplotlib.pyplot as plt\nfrom obspy import UTCDateTime\nfrom obspy.signal.trigger import ar_pick\nfrom obspy.signal.trigger import recursive_sta_lta, trigger_onset\nfrom itertools import combinations\nfrom obspy.core.event import Catalog, Event, Origin, Arrival, Pick, WaveformStreamID\n\n\ndef run_associator(input_dir,\n                   start_time, \n                   end_time, \n                   moving_window=15, \n                   pair_n=3,\n                   output_dir='.',\n                   consider_combination=False):\n    \n    \"\"\"\n    \n    It performs a very simple association based on detection times on multiple stations. It works fine when you have a small and local network of seismic stations. \n\n    Parameters\n    ----------\n    input_dir: str, default=None\n        Directory name containing hdf5 and csv files-preprocessed data.\n        \n    start_time: str, default=None\n        Start of a time period of interest in 'YYYY-MM-DD hh:mm:ss.f' format.\n        \n    end_time: str, default=None\n        End of a timeperiod of interest in 'YYYY-MM-DD hh:mm:ss.f' format. \n        \n    moving_window: int, default=15\n        The length of time window used for association in second. \n        \n    pair_n: int, default=2\n        The minimum number of stations used for the association. \n        \n    output_dir: str, default='.'\n        Path to the directory to write the output file.\n        \n    consider_combination: bool, default=False\n        If True, it will write down all possible combinations of picked arrival times for each event. This will generate multiple events with the same ID, and you will need to remove those with poor solutions after location. This helps to remove the false positives from the associated event. \n\n\n    Returns\n    ----------        \n    output_dir/Y2000.phs: Phase information for the associated events in hypoInverse format.     \n\n    output_dir/associations.xml: quakeml output (containing origin and pick objects - using ObsPy functions). QuakeML is useful so that the user can then easily use ObsPy to generate input for other relocator methods (e.g. NonLinLoc). Contributed by Stephen Hicks  \n\n    output_dir/traceNmae_dic.json: A dictionary where the trace name for all the detections associated to an event are listed. This can be used later to access the traces for calculating the cross-correlations during the relocation process. \n        \n        \n    Warning\n    ----------        \n    Unlike the other modules, this function does not create the ouput directory. So if the given path does not exist will give an error. \n        \n    \"\"\"   \n   \n    \n    if os.path.exists(\"phase_dataset\"):\n        os.remove(\"phase_dataset\")\n    conn = sqlite3.connect(\"phase_dataset\")\n    cur = conn.cursor()\n    \n    cur.execute('''\n        CREATE TABLE phase_dataset (traceID TEXT, \n                                    network TEXT,\n                                    station TEXT,\n                                    instrument_type TEXT,\n                                    stlat NUMERIC, \n                                    stlon NUMERIC, \n                                    stelv NUMERIC,                        \n                                    event_start_time DateTime, \n                                    event_end_time DateTime, \n                                    detection_prob NUMERIC, \n                                    detection_unc NUMERIC, \n                                    p_arrival_time DateTime, \n                                    p_prob NUMERIC, \n                                    p_unc NUMERIC, \n                                    p_snr NUMERIC,\n                                    s_arrival_time DateTime, \n                                    s_prob NUMERIC, \n                                    s_unc NUMERIC, \n                                    s_snr NUMERIC,\n                                    amp NUMERIC\n                                    )''')\n    if platform.system() == 'Windows':\n        station_list = [ev for ev in listdir(input_dir) if ev.split(\"\\\\\")[-1] != \".DS_Store\"];\n    else:\n        station_list = [ev for ev in listdir(input_dir) if ev.split(\"/\")[-1] != \".DS_Store\"];\n        \n    station_list = sorted(set(station_list))\n\n    for st in station_list:       \n        print(f'reading {st} ...')\n        if platform.system() == 'Windows':\n            _pick_database_maker(conn, cur, input_dir+\"\\\\\"+st+'\"\\\\\"X_prediction_results.csv')\n        else:\n            _pick_database_maker(conn, cur, input_dir+\"/\"+st+'/X_prediction_results.csv')\n\n    #  read the database as dataframe \n    conn = sqlite3.connect(\"phase_dataset\")\n    tbl = pd.read_sql_query(\"SELECT * FROM phase_dataset\", conn); \n    #tbl = tbl[tbl.p_prob > 0.3]\n    #tbl = tbl[tbl.s_prob > 0.3]\n\n    tbl['event_start_time'] = tbl['event_start_time'].apply(lambda row : _date_convertor(row)) \n    tbl['event_end_time'] = tbl['event_end_time'].apply(lambda row : _date_convertor(row)) \n    tbl['p_arrival_time'] = tbl['p_arrival_time'].apply(lambda row : _date_convertor(row)) \n    tbl['s_arrival_time'] = tbl['s_arrival_time'].apply(lambda row : _date_convertor(row)) \n\n    _dbs_associator(start_time,\n                    end_time,\n                    moving_window,\n                    tbl, \n                    pair_n,\n                    output_dir,\n                    station_list,\n                    consider_combination)\n    \n    os.remove(\"phase_dataset\")\n\n\n\n\n\n\ndef _pick_database_maker(conn, cur, input_file):\n    csv_file = open(input_file)\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    line_count = 0\n    for row in csv_reader:\n        if line_count == 0:\n            #  print(f'Column names are {\", \".join(row)}')\n            line_count += 1\n        else: \n            line_count += 1\n            \n            traceID = row[0]\n                \n            network = row[1]\n            station = row[2]\n            instrument_type = row[3]\n            stlat = float(row[4])\n            stlon = float(row[5]) \n            stelv = float(row[6])                        \n            \n            mls = row[7].split('.')\n            if len(mls) == 1:\n                event_start_time = datetime.strptime(row[7], '%Y-%m-%d %H:%M:%S')\n            else:\n                event_start_time = datetime.strptime(row[7], '%Y-%m-%d %H:%M:%S.%f')\n\n            mls = row[8].split('.')\n            if len(mls) == 1:\n                event_end_time = datetime.strptime(row[8], '%Y-%m-%d %H:%M:%S')\n            else:\n                event_end_time = datetime.strptime(row[8], '%Y-%m-%d %H:%M:%S.%f')\n                            \n            detection_prob = float(row[9]) \n            try:\n                detection_unc = float(row[10]) \n            except Exception:\n                detection_unc = None          \n\n            if len(row[11]) > 10:\n           #     p_arrival_time = UTCDateTime(row[11].replace(' ', 'T')+'Z')\n                mls = row[11].split('.')\n                if len(mls) == 1:\n                    p_arrival_time = datetime.strptime(row[11], '%Y-%m-%d %H:%M:%S')\n                else:\n                    p_arrival_time = datetime.strptime(row[11], '%Y-%m-%d %H:%M:%S.%f')\n                                    \n                p_prob = float(row[12])\n                try:\n                    p_unc = float(row[13]) \n                except Exception:\n                    p_unc = None\n            else:\n                p_arrival_time = None\n                p_prob = None\n                p_unc = None \n                \n            try:\n                p_snr = float(row[14]) \n            except Exception:\n                p_snr = None \n                                \n            if len(row[15]) > 10:\n                mls = row[15].split('.')\n                if len(mls) == 1:\n                    s_arrival_time = datetime.strptime(row[15], '%Y-%m-%d %H:%M:%S')\n                else:\n                    s_arrival_time = datetime.strptime(row[15], '%Y-%m-%d %H:%M:%S.%f')                \n                                \n                s_prob = float(row[16])\n                try:\n                    s_unc = float(row[17]) \n                except Exception:\n                    s_unc = None\n            else:\n                s_arrival_time = None\n                s_prob = None\n                s_unc = None   \n\n            try:\n                s_snr = float(row[18]) \n            except Exception:\n                s_snr = None\n                \n            amp = None\n\n            cur.execute('''INSERT INTO phase_dataset VALUES \n                        (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?, ?)''', \n                        (traceID, network, station, instrument_type, stlat, stlon, stelv,\n                         event_start_time, event_end_time, detection_prob, detection_unc, \n                         p_arrival_time, p_prob, p_unc, p_snr, s_arrival_time, s_prob, s_unc, s_snr,\n                         amp))\n            \n            conn.commit()\n\n\n\n\ndef _decimalDegrees2DMS(value,type):\n    \n    'Converts a Decimal Degree Value into Degrees Minute Seconds Notation. Pass value as double type = {Latitude or Longitude} as string returns a string as D:M:S:Direction created by: anothergisblog.blogspot.com' \n\n    degrees = int(value)\n    submin = abs( (value - int(value) ) * 60)\n    direction = \"\"\n    if type == \"Longitude\":\n        if degrees < 0:\n            direction = \"W\"\n        elif degrees > 0:\n            direction = \" \"\n        else:\n            direction = \"\"\n        notation = [\"{:>3}\".format(str(abs(degrees))), direction, \"{:>5}\".format(str(round(submin, 2)))] \n\n    elif type == \"Latitude\":\n        if degrees < 0:\n            direction = \"S\"\n        elif degrees > 0:\n            direction = \" \"\n        else:\n            direction = \"\" \n        notation =[\"{:>2}\".format(str(abs(degrees))), direction, \"{:>5}\".format(str(round(submin, 2)))] \n        \n    return notation\n\n\n\ndef _weighcalculator_prob(pr):\n    'calculate the picks weights'\n    weight = 4\n    if pr > 0.6:\n        weight = 0\n    elif pr <= 0.6 and pr > 0.5:\n        weight = 1\n    elif pr <= 0.5 and pr > 0.2:\n        weight = 2\n    elif pr <= 0.2 and pr > 0.1:\n        weight = 3  \n    elif pr <= 0.1:\n        weight = 4 \n         \n    return weight\n\n\n\ndef _date_convertor(r): \n    'convert datatime form string'\n    if r and len(r)>5:\n        mls = r.split('.')\n        if len(mls) == 1:\n            new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S')\n        else:\n            new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S.%f')\n        return new_t\n            \n\n\ndef _doubleChecking(station_list, detections, preprocessed_dir, moving_window, thr_on=3.7, thr_of=0.5):\n    'this function perform traditional detection (STA/LTA) and picker (AIC) to double check for events on the remaining stations when an event has been detected on more than two stations'\n    for stt in station_list:\n        sttt = stt.split('_')[0]\n      #  print(sttt)\n        if sttt not in detections['station'].to_list():\n            new_picks = {}                    \n            if platform.system() == 'Windows':\n                file_name = preprocessed_dir+\"\\\\\"+sttt+\".hdf5\"\n                file_csv = preprocessed_dir+\"\\\\\"+sttt+\".csv\"\n            else:\n                file_name = preprocessed_dir+\"/\"+sttt+\".hdf5\"\n                file_csv = preprocessed_dir+\"/\"+sttt+\".csv\"\n            \n            df = pd.read_csv(file_csv)\n            df['start_time'] = pd.to_datetime(df['start_time'])  \n            \n            mask = (df['start_time'] > detections.iloc[0]['event_start_time']-timedelta(seconds = moving_window)) & (df['start_time'] < detections.iloc[0]['event_start_time']+timedelta(seconds = moving_window))\n            df = df.loc[mask]\n            dtfl = h5py.File(file_name, 'r')\n            dataset = dtfl.get('data/'+df['trace_name'].to_list()[0]) \n            data = np.array(dataset)\n                \n            cft = recursive_sta_lta(data[:,2], int(2.5 * 100), int(10. * 100))\n            on_of = trigger_onset(cft, thr_on, thr_of)\n            if len(on_of) >= 1:                    \n                p_pick, s_pick = ar_pick(data[:,2], data[:,1], data[:,0], 100, 1.0, 20.0, 1.0, 0.1, 4.0, 1.0, 2, 8, 0.1, 0.2)\n                if (on_of[0][1]+100)/100 > p_pick > (on_of[0][0]-100)/100: \n                   # print('got one')\n                    new_picks['traceID'] = df['trace_name'].to_list()[0]\n                    new_picks['network'] = dataset.attrs[\"network_code\"]\n                    new_picks['station'] = sttt\n                    new_picks['instrument_type'] = df['trace_name'].to_list()[0].split('_')[2]\n                    new_picks['stlat'] = round(dataset.attrs[\"receiver_latitude\"], 4)\n                    new_picks['stlon'] = round(dataset.attrs[\"receiver_longitude\"], 4)\n                    new_picks['stelv'] = round(dataset.attrs[\"receiver_elevation_m\"], 2)\n                    new_picks['event_start_time'] = datetime.strptime(str(UTCDateTime(dataset.attrs['trace_start_time'].replace(' ', 'T')+'Z')+(on_of[0][0]/100)).replace('T', ' ').replace('Z', ''), '%Y-%m-%d %H:%M:%S.%f')\n                    new_picks['event_end_time'] = datetime.strptime(str(UTCDateTime(dataset.attrs['trace_start_time'].replace(' ', 'T')+'Z')+(on_of[0][1]/100)).replace('T', ' ').replace('Z', ''), '%Y-%m-%d %H:%M:%S.%f')\n                    new_picks['detection_prob'] = 0.3\n                    new_picks['detection_unc'] = 0.6\n                    new_picks['p_arrival_time'] = datetime.strptime(str(UTCDateTime(dataset.attrs['trace_start_time'].replace(' ', 'T')+'Z')+p_pick).replace('T', ' ').replace('Z', ''), '%Y-%m-%d %H:%M:%S.%f')\n                    new_picks['p_prob'] = 0.3\n                    new_picks['p_unc'] = 0.6\n                    new_picks['p_snr'] = None\n                    new_picks['s_arrival_time'] = None\n                    new_picks['s_prob'] = 0.0\n                    new_picks['s_unc'] = None\n                    new_picks['s_snr'] = None\n                    new_picks['amp'] = None\n                    detections = detections.append(new_picks , ignore_index=True)      \n    return detections                    \n                            \n                            \n\ndef _dbs_associator(start_time, end_time, moving_window, \n                    tbl, pair_n, save_dir, station_list,\n                    consider_combination=False):  \n    \n    if consider_combination==True: \n        if platform.system() == 'Windows':\n            Y2000_writer = open(save_dir+\"\\\\\"+\"Y2000.phs\", \"w\")\n        else:\n            Y2000_writer = open(save_dir+\"/\"+\"Y2000.phs\", \"w\")\n            \n        traceNmae_dic = dict()   \n        st = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f')\n        et = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S.%f')\n        total_t = et-st;\n        evid = 0; \n        tt = st\n        pbar = tqdm(total= int(np.ceil(total_t.total_seconds()/moving_window)), ncols=100) \n        while tt < et:\n            \n            detections = tbl[(tbl.event_start_time >= tt) & (tbl.event_start_time < tt+timedelta(seconds = moving_window))]        \n            pbar.update()\n            if len(detections) >= pair_n:  \n                evid += 1\n    \n                yr = \"{:>4}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[0])\n                mo = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[1]) \n                dy = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[2]) \n                hr = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[0]) \n                mi = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[1]) \n                sec = \"{:>4}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[2]) \n                st_lat_DMS = _decimalDegrees2DMS(float(detections.iloc[0]['stlat']),  \"Latitude\")\n                st_lon_DMS = _decimalDegrees2DMS(float(detections.iloc[0]['stlon']),  \"Longitude\")\n                depth = 5.0\n                mag = 0.0\n\n                # QuakeML\n                print(detections.iloc[0]['event_start_time'])\n                      \n                if len(detections)/pair_n <= 2:\n                    ch = pair_n\n                else:\n                    ch = int(len(detections)-pair_n)\n                  \n                picks = []            \n                for ns in range(ch, len(detections)+1):\n                    comb = 0\n                    for ind in list(combinations(detections.index, ns)):\n                        comb+=1\n                        selected_detections = detections.loc[ind,:]\n                        sorted_detections = selected_detections.sort_values('p_arrival_time')\n    \n                        Y2000_writer.write(\"%4d%2d%2d%2d%2d%4.2f%2.0f%1s%4.2f%3.0f%1s%4.2f%5.2f%3.2f\\n\"%\n                                           (int(yr),int(mo),int(dy), int(hr),int(mi),float(sec),float(st_lat_DMS[0]), \n                                            str(st_lat_DMS[1]), float(st_lat_DMS[2]),float(st_lon_DMS[0]), str(st_lon_DMS[1]), \n                                            float(st_lon_DMS[2]),float(depth), float(mag))); \n                                \n                        station_buffer=[]; row_buffer=[]; tr_names=[]; tr_names2=[]\n                        for _, row in sorted_detections.iterrows():\n                            \n                            trace_name = row['traceID']+'*'+row['station']+'*'+str(row['event_start_time'])\n                            p_unc = row['p_unc']\n                            p_prob = row['p_prob']\n                            s_unc = row['s_unc']\n                            s_prob = row['s_prob']\n            \n                            if p_unc:                    \n                                Pweihgt = _weighcalculator_prob(p_prob*(1-p_unc))\n                            else:\n                                Pweihgt = _weighcalculator_prob(p_prob)\n                            try:\n                                Pweihgt = int(Pweihgt)\n                            except Exception:\n                                Pweihgt = 4\n            \n                            if s_unc: \n                                Sweihgt = _weighcalculator_prob(s_prob*(1-s_unc))\n                            else:\n                                Sweihgt = _weighcalculator_prob(s_prob)  \n                            try:\n                                Sweihgt = int(Sweihgt)\n                            except Exception:\n                                Sweihgt = 4\n                                \n                            station = \"{:<5}\".format(row['station'])\n                            network = \"{:<2}\".format(row['network']) \n                            try:\n                                yrp = \"{:>4}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[0])\n                                mop = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[1]) \n                                dyp = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[2]) \n                                hrp = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[0]) \n                                mip = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[1]) \n                                sec_p = \"{:>4}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[2]) \n                                p = Pick(time=UTCDateTime(row['p_arrival_time']), \n                                         waveform_id=WaveformStreamID(network_code=network,\n                                         station_code=station.rstrip()),\n                                         phase_hint=\"P\")\n                                picks.append(p)\n                            except Exception:\n                                sec_p = None\n\n                            try:\n                                yrs = \"{:>4}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[0])\n                                mos = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[1]) \n                                dys = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[2]) \n                                hrs = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[0]) \n                                mis = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[1])                                 \n                                sec_s = \"{:>4}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[2]) \n                                p = Pick(time=UTCDateTime(row['p_arrival_time']), \n                                         waveform_id=WaveformStreamID(network_code=network, station_code=station.rstrip()),\n                                         phase_hint=\"S\")\n                                picks.append(p)\n                            except Exception:\n                                sec_s = None\n                            \n                            if row['station'] not in station_buffer:\n                                tr_names.append(trace_name)\n                                station_buffer.append(row['station'])                      \n                                if sec_s:\n                                    Y2000_writer.write(\"%5s%2s  HHE     %4d%2d%2d%2d%2d%5.2f       %5.2fES %1d\\n\"%\n                                                       (station,network,int(yrs),int(mos),int(dys),int(hrs),int(mis),\n                                                        float(0.0),float(sec_s), Sweihgt))\n                                if sec_p:\n                                    Y2000_writer.write(\"%5s%2s  HHZ IP %1d%4d%2d%2d%2d%2d%5.2f       %5.2f   0\\n\"%\n                                                       (station,network,Pweihgt,int(yrp),int(mop),int(dyp),int(hrp),\n                                                        int(mip),float(sec_p),float(0.0)))                        \n                            else :\n                                tr_names2.append(trace_name)\n                                if sec_s:\n                                    row_buffer.append(\"%5s%2s  HHE     %4d%2d%2d%2d%2d%5.2f       %5.2fES %1d\\n\"%(station,network,\n                                                                                                                 int(yrs),int(mos),int(dys),\n                                                                                                                 int(hrs),int(mis),0.0,\n                                                                                                                 float(sec_s), Sweihgt)); \n                                if sec_p:\n                                    row_buffer.append(\"%5s%2s  HHZ IP %1d%4d%2d%2d%2d%2d%5.2f       %5.2f   0\\n\"%(station,network,\n                                                                                                                 Pweihgt,   \n                                                                                                                 int(yrp),int(mop),int(dyp),\n                                                                                                                 int(hrp),int(mip),float(sec_p),\n                                                                                                                 float(0.0)));                                 \n                        Y2000_writer.write(\"{:<62}\".format(' ')+\"%10d\"%(evid)+'\\n');\n    \n                traceNmae_dic[str(evid)] = tr_names\n    \n                if len(row_buffer) >= 2*pair_n: \n                    Y2000_writer.write(\"%4d%2d%2d%2d%2d%4.2f%2.0f%1s%4.2f%3.0f%1s%4.2f%5.2f%3.2f\\n\"%\n                                       (int(yr),int(mo),int(dy),int(hr),int(mi),float(sec), \n                                        float(st_lat_DMS[0]), str(st_lat_DMS[1]), float(st_lat_DMS[2]),\n                                        float(st_lon_DMS[0]), str(st_lon_DMS[1]), float(st_lon_DMS[2]),\n                                        float(depth), float(mag)));   \n                    for rr in row_buffer:\n                        Y2000_writer.write(rr);\n                 \n                    Y2000_writer.write(\"{:<62}\".format(' ')+\"%10d\"%(evid)+'\\n');\n                    traceNmae_dic[str(evid)] = tr_names2\n                \n    \n            tt += timedelta(seconds= moving_window)\n        \n     #   plt.scatter(LTTP, TTP, s=10, marker='o', c='b', alpha=0.4, label='P')\n     #   plt.scatter(LTTS, TTS, s=10, marker='o', c='r', alpha=0.4, label='S')\n     #   plt.legend('upper right')\n     #   plt.show()\n    \n        print('The Number of Realizations: '+str(evid)+'\\n', flush=True)\n            \n        jj = json.dumps(traceNmae_dic) \n        if platform.system() == 'Windows':\n            f = open(save_dir+\"\\\\\"+\"traceNmae_dic.json\",\"w\")\n        else:\n            f = open(save_dir+\"/\"+\"traceNmae_dic.json\",\"w\")\n        f.write(jj)\n        f.close()\n\n    else:  \n        if platform.system() == 'Windows':\n            Y2000_writer = open(save_dir+\"\\\\\"+\"Y2000.phs\", \"w\")\n        else:\n            Y2000_writer = open(save_dir+\"/\"+\"Y2000.phs\", \"w\")\n            \n        cat = Catalog()\n        traceNmae_dic = dict()    \n        st = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f')\n        et = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S.%f')\n        total_t = et-st;\n        evid = 200000;  evidd = 100000\n        tt = st\n        pbar = tqdm(total= int(np.ceil(total_t.total_seconds()/moving_window))) \n        while tt < et:\n            \n            detections = tbl[(tbl.event_start_time >= tt) & (tbl.event_start_time < tt+timedelta(seconds = moving_window))]        \n            pbar.update()\n            if len(detections) >= pair_n:   \n    \n                yr = \"{:>4}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[0])\n                mo = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[1]) \n                dy = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[0].split('-')[2]) \n                hr = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[0]) \n                mi = \"{:>2}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[1]) \n                sec = \"{:>4}\".format(str(detections.iloc[0]['event_start_time']).split(' ')[1].split(':')[2]) \n                st_lat_DMS = _decimalDegrees2DMS(float(detections.iloc[0]['stlat']),  \"Latitude\")\n                st_lon_DMS = _decimalDegrees2DMS(float(detections.iloc[0]['stlon']),  \"Longitude\")\n                depth = 5.0\n                mag = 0.0\n    \n                Y2000_writer.write(\"%4d%2d%2d%2d%2d%4.2f%2.0f%1s%4.2f%3.0f%1s%4.2f%5.2f%3.2f\\n\"%(int(yr),int(mo),int(dy),\n                                                                                             int(hr),int(mi),float(sec), \n                                                                                             float(st_lat_DMS[0]), str(st_lat_DMS[1]), float(st_lat_DMS[2]),\n                                                                                             float(st_lon_DMS[0]), str(st_lon_DMS[1]), float(st_lon_DMS[2]),\n                                                                                             float(depth), float(mag)));          \n                event = Event()\n                origin = Origin(time=UTCDateTime(detections.iloc[0]['event_start_time']),\n                                longitude=detections.iloc[0]['stlon'],\n                                latitude=detections.iloc[0]['stlat'],\n                                method=\"EqTransformer\")\n                event.origins.append(origin)\n                \n                station_buffer = []\n                row_buffer = []\n                sorted_detections = detections.sort_values('p_arrival_time')\n                tr_names = []\n                tr_names2 = []\n                picks = []\n                for _, row in sorted_detections.iterrows():\n                    trace_name = row['traceID']+'*'+row['station']+'*'+str(row['event_start_time'])\n                    p_unc = row['p_unc']\n                    p_prob = row['p_prob']\n                    s_unc = row['s_unc']\n                    s_prob = row['s_prob']\n    \n                    if p_unc:                    \n                        Pweihgt = _weighcalculator_prob(p_prob*(1-p_unc))\n                    else:\n                        Pweihgt =_weighcalculator_prob(p_prob)\n                    try:\n                        Pweihgt = int(Pweihgt)\n                    except Exception:\n                        Pweihgt = 4\n    \n                    if s_unc: \n                        Sweihgt = _weighcalculator_prob(s_prob*(1-s_unc))\n                    else:\n                        Sweihgt = _weighcalculator_prob(s_prob)  \n                    try:\n                        Sweihgt = int(Sweihgt)\n                    except Exception:\n                        Sweihgt = 4\n                        \n                    station = \"{:<5}\".format(row['station'])\n                    network = \"{:<2}\".format(row['network'])\n                    \n                    try:\n                        yrp = \"{:>4}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[0])\n                        mop = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[1]) \n                        dyp = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[0].split('-')[2]) \n                        hrp = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[0]) \n                        mip = \"{:>2}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[1]) \n                        sec_p = \"{:>4}\".format(str(row['p_arrival_time']).split(' ')[1].split(':')[2]) \n                        p = Pick(time=UTCDateTime(row['p_arrival_time']), \n                                     waveform_id=WaveformStreamID(network_code=network, station_code=station.rstrip()),\n                                     phase_hint=\"P\", method_id=\"EqTransformer\")\n                        picks.append(p)\n                    except Exception:\n                        sec_p = None\n\n                    try:\n                        yrs = \"{:>4}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[0])\n                        mos = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[1]) \n                        dys = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[0].split('-')[2]) \n                        hrs = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[0]) \n                        mis = \"{:>2}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[1])                                 \n                        sec_s = \"{:>4}\".format(str(row['s_arrival_time']).split(' ')[1].split(':')[2]) \n                        p = Pick(time=UTCDateTime(row['s_arrival_time']), \n                                     waveform_id=WaveformStreamID(network_code=network, station_code=station.rstrip()),\n                                     phase_hint=\"S\", method_id=\"EqTransformer\")\n                        picks.append(p)\n                    except Exception:\n                        sec_s = None                              \n    \n                    if row['station'] not in station_buffer:\n                        tr_names.append(trace_name)\n                        station_buffer.append(row['station'])  \n                        if sec_s:\n                            Y2000_writer.write(\"%5s%2s  HHE     %4d%2d%2d%2d%2d%5.2f       %5.2fES %1d\\n\"%(station,network,\n                                                                                                         int(yrs),int(mos),int(dys),\n                                                                                                         int(hrs),int(mis),float(0.0),\n                                                                                                         float(sec_s), Sweihgt))\n                        if sec_p:\n                            Y2000_writer.write(\"%5s%2s  HHZ IP %1d%4d%2d%2d%2d%2d%5.2f       %5.2f   0\\n\"%(station,network,\n                                                                                                         Pweihgt,   \n                                                                                                         int(yrp),int(mop),int(dyp),\n                                                                                                         int(hrp),int(mip),float(sec_p),\n                                                                                                         float(0.0)))                        \n                    else :\n                        tr_names2.append(trace_name)\n                        if sec_s:\n                            row_buffer.append(\"%5s%2s  HHE     %4d%2d%2d%2d%2d%5.2f       %5.2fES %1d\\n\"%(station,network,\n                                                                                                         int(yrs),int(mos),int(dys),\n                                                                                                         int(hrs),int(mis),0.0,\n                                                                                                         float(sec_s), Sweihgt)); \n                        if sec_p:\n                            row_buffer.append(\"%5s%2s  HHZ IP %1d%4d%2d%2d%2d%2d%5.2f       %5.2f   0\\n\"%(station,network,\n                                                                                                         Pweihgt,   \n                                                                                                         int(yrp),int(mop),int(dyp),\n                                                                                                         int(hrp),int(mip),float(sec_p),\n                                                                                                         float(0.0))); \n                event.picks = picks\n                event.preferred_origin_id = event.origins[0].resource_id\n                cat.append(event)\n    \n                evid += 1\n                Y2000_writer.write(\"{:<62}\".format(' ')+\"%10d\"%(evid)+'\\n');\n                traceNmae_dic[str(evid)] = tr_names\n    \n                if len(row_buffer) >= 2*pair_n: \n                    Y2000_writer.write(\"%4d%2d%2d%2d%2d%4.2f%2.0f%1s%4.2f%3.0f%1s%4.2f%5.2f%3.2f\\n\"%\n                                       (int(yr),int(mo),int(dy),int(hr),int(mi),float(sec), \n                                        float(st_lat_DMS[0]), str(st_lat_DMS[1]), float(st_lat_DMS[2]),\n                                        float(st_lon_DMS[0]), str(st_lon_DMS[1]), float(st_lon_DMS[2]),\n                                        float(depth), float(mag)));   \n                    for rr in row_buffer:\n                        Y2000_writer.write(rr);\n                 \n                    evid += 1\n                    Y2000_writer.write(\"{:<62}\".format(' ')+\"%10d\"%(evid)+'\\n');\n                    traceNmae_dic[str(evid)] = tr_names2\n                    \n                elif len(row_buffer) < pair_n and len(row_buffer) != 0:\n                    evidd += 1\n                    traceNmae_dic[str(evidd)] = tr_names2\n                    \n            elif len(detections) < pair_n and len(detections) != 0:\n                tr_names = []\n                for _, row in detections.iterrows():\n                    trace_name = row['traceID']\n                    tr_names.append(trace_name)\n                evidd += 1\n                traceNmae_dic[str(evidd)] = tr_names                \n    \n            tt += timedelta(seconds= moving_window)\n            \n        print('The Number of Associated Events: '+str(evid-200000)+'\\n', flush=True)\n            \n        jj = json.dumps(traceNmae_dic)  \n        if platform.system() == 'Windows':\n            f = open(save_dir+\"\\\\\"+\"traceNmae_dic.json\",\"w\")\n        else:    \n            f = open(save_dir+\"/\"+\"traceNmae_dic.json\",\"w\")\n            \n        f.write(jj)\n        f.close()\n        print(cat.__str__(print_all=True))\n        cat.write(save_dir+\"/associations.xml\", format=\"QUAKEML\")\n        \n"}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/downloader.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Aug 31 21:21:31 2019\n\n@author: mostafamousavi\n\nlast update: 01/29/2021 \n\"\"\"\nimport json\nimport time\nfrom obspy.clients.fdsn.mass_downloader import RectangularDomain, Restrictions, MassDownloader\nfrom obspy import UTCDateTime\nimport datetime\nimport os\nimport platform\nfrom obspy.clients.fdsn.client import Client\nimport shutil\nfrom multiprocessing.pool import ThreadPool\nimport multiprocessing\nimport numpy as np\n\n\ndef makeStationList(json_path,client_list, min_lat, max_lat, min_lon, max_lon, start_time, end_time, channel_list=[], filter_network=[], filter_station=[],**kwargs):\n\n\n     \"\"\"\n    \n    Uses fdsn to find available stations in a specific geographical location and time period.  \n\n    Parameters\n    ----------\n    json_path: str\n        Path of the json file that will be returned\n\n    client_list: list\n        List of client names e.g. [\"IRIS\", \"SCEDC\", \"USGGS\"].\n                                \n    min_lat: float\n        Min latitude of the region.\n        \n    max_lat: float\n        Max latitude of the region.\n        \n    min_lon: float\n        Min longitude of the region.\n        \n    max_lon: float\n        Max longitude of the region.\n        \n    start_time: str\n        Start DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n        \n    end_time: str\n        End DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n        \n    channel_list: str, default=[]\n        A list containing the desired channel codes. Downloads will be limited to these channels based on priority. Defaults to [] --> all channels\n        \n    filter_network: str, default=[]\n        A list containing the network codes that need to be avoided. \n        \n    filter_station: str, default=[]\n        A list containing the station names that need to be avoided.\n\n    kwargs: \n        special symbol for passing Client.get_stations arguments\n\n    Returns\n    ----------\n    stations_list.json: A dictionary containing information for the available stations.      \n        \n     \"\"\"  \n \n     station_list = {}\n     for cl in client_list:\n         inventory = Client(cl).get_stations(minlatitude=min_lat,\n                                     maxlatitude=max_lat, \n                                     minlongitude=min_lon, \n                                     maxlongitude=max_lon, \n                                     starttime=UTCDateTime(start_time), \n                                     endtime=UTCDateTime(end_time), \n                                     level='channel',**kwargs)    \n\n         for ev in inventory:\n             net = ev.code\n             if net not in filter_network:\n                 for st in ev:\n                     station = st.code\n                     print(str(net)+\"--\"+str(station))\n    \n                     if station not in filter_station:\n\n                         elv = st.elevation\n                         lat = st.latitude\n                         lon = st.longitude\n                         new_chan = [ch.code for ch in st.channels]\n                         if len(channel_list) > 0:\n                             chan_priority=[ch[:2] for ch in channel_list]\n        \n                             for chnn in chan_priority:\n                                 if chnn in [ch[:2] for ch in new_chan]:\n                                     new_chan = [ch for ch in new_chan if ch[:2] == chnn]                     \n    # =============================================================================\n    #                      if (\"BHZ\" in new_chan) and (\"HHZ\" in new_chan):\n    #                          new_chan = [ch for ch in new_chan if ch[:2] != \"BH\"]\n    #                      if (\"HHZ\" in new_chan) and (\"HNZ\" in new_chan):\n    #                          new_chan = [ch for ch in new_chan if ch[:2] != \"HH\"]\n    #                          \n    #                          if len(new_chan)>3 and len(new_chan)%3 != 0:\n    #                              chan_type = [ch for ch in new_chan if ch[2] == 'Z']\n    #                              chan_groups = []\n    #                              for i, cht in enumerate(chan_type):\n    #                                  chan_groups.append([ch for ch in new_chan if ch[:2] == cht[:2]])\n    #                              new_chan2 = []\n    #                              for chg in chan_groups:\n    #                                  if len(chg) == 3:\n    #                                      new_chan2.append(chg)\n    #                              new_chan = new_chan2 \n    # ============================================================================= \n                        \n                         if len(new_chan) > 0 and (station not in station_list):\n                             station_list[str(station)] ={\"network\": net,\n                                                      \"channels\": list(set(new_chan)),\n                                                      \"coords\": [lat, lon, elv]\n                                                      }\n     json_dir = os.path.dirname(json_path)\n     if not os.path.exists(json_dir):\n         os.makedirs(json_dir)\n     with open(json_path, 'w') as fp:\n         json.dump(station_list, fp)\n         \n         \ndef downloadMseeds(client_list, stations_json, output_dir, start_time, end_time, min_lat, max_lat, min_lon, max_lon, chunk_size, channel_list=[], n_processor=None):\n    \n    \n    \"\"\"\n    \n    Uses obspy mass downloader to get continuous waveforms from a specific client in miniseed format in variable chunk sizes. The minimum chunk size is 1 day. \n \n    Parameters\n    ----------\n    client_list: list\n        List of client names e.g. [\"IRIS\", \"SCEDC\", \"USGGS\"].\n\n    stations_json: dic,\n        Station informations.\n        \n    output_dir: str\n        Output directory.\n                                \n    min_lat: float\n        Min latitude of the region.\n        \n    max_lat: float\n        Max latitude of the region.\n        \n    min_lon: float\n        Min longitude of the region.\n        \n    max_lon: float\n        Max longitude of the region.\n        \n    start_time: str\n        Start DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n        \n    end_time: str\n        End DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n        \n    channel_list: str, default=[]\n        A list containing the desired channel codes. Downloads will be limited to these channels based on priority. Defaults to [] --> all channels\n\n    chunk_size: int\n        Chunck size in day.\n        \n    n_processor: int, default=None\n        Number of CPU processors for parallel downloading.\n\n    Returns\n    ----------\n\n    output_name/station_name/*.mseed: Miniseed fiels for each station.      \n \n    Warning\n    ----------\n    usage of multiprocessing and parallel downloads heavily depends on the client. It might cause missing some data for some network. Please test first for some short period and if It did miss some chunks of data for some channels then set n_processor=None to avoid parallel downloading.        \n        \n    \"\"\"\n     \n         \n    json_file = open(stations_json)\n    station_dic = json.load(json_file)\n    print(f\"####### There are {len(station_dic)} stations in the list. #######\")\n\n    start_t = UTCDateTime(start_time)\n    end_t = UTCDateTime(end_time)\n    \n    domain = RectangularDomain(minlatitude=min_lat, maxlatitude=max_lat, minlongitude=min_lon, maxlongitude=max_lon)\n    mdl = MassDownloader(providers=client_list)\n\n    bg=start_t \n \n    if n_processor==None:\n        for st in station_dic:\n            print(f'======= Working on {st} station.')\n            _get_w(bg, st, station_dic, end_t, mdl, domain, output_dir, chunk_size, channel_list)\n    else:        \n        def process(st):\n            print(f'======= Working on {st} station.')\n            _get_w(bg, st, station_dic, end_t, mdl, domain, output_dir, chunk_size, channel_list)\n            \n        with ThreadPool(n_processor) as p:\n            p.map(process, [ st for st in station_dic])      \n        \n       \n    \n    \ndef downloadSacs(client, stations_json, output_dir, start_time, end_time, patience, n_processor=None):\n    \n    \"\"\"\n    \n    Uses obspy to get continuous waveforms from IRIS in sac format after preprocessing and in daily chunks. The difference to the mseed downloader is that this function removes the instrument response as it gets the data. \n \n    Parameters\n    ----------\n    client_list: list\n        List of client names e.g. [\"IRIS\", \"SCEDC\", \"USGGS\"].\n\n    stations_json: dic,\n        Station informations.\n \n    output_dir: str\n        Output directory.\n        \n    start_time: str\n        Start DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n        \n    end_time: str\n        End DateTime for the beginning of the period in \"YYYY-MM-DDThh:mm:ss.f\" format.\n                    \n    patience: int\n        The maximum number of days without data that the program allows continuing the downloading.\n        \n    chunk_size: int\n        Chunck size in day.\n        \n      n_processor: int, default=None\n        Number of CPU processors for parallel downloading. \n        \n\n    Returns\n    ----------\n     \n    output_name/station_name/*.SAC: SAC fiels for each station.      \n \n    Warning\n    ----------\n    This function was not tested so you should be careful using it and make sure it gets the data.     \n    \n        \n    \"\"\"    \n    \n\n\n    if not n_processor:\n        n_processor = multiprocessing.cpu_count()\n        \n    t_step = 86400   \n    fr = open(stations_json, 'r'); \n    new_list = json.load(fr)\n    print(f\"####### There are {len(new_list)} stations in the list. #######\")\n\n    if platform.system() == 'Windows':\n        if not os.path.exists(output_dir+\"\\\\\"):\n            os.makedirs(output_dir+\"\\\\\")  \n    else:        \n        if not os.path.exists(output_dir+\"/\"):\n            os.makedirs(output_dir+\"/\")  \n\n    def process(station):          \n        net = new_list[station]['network']\n        \n        if platform.system() == 'Windows':\n            dirname = str(station)+\"\\\\\"\n            if not os.path.exists(dirname):\n                os.makedirs(dirname) \n        else:   \n            dirname = str(station)+\"/\"\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)  \n                \n        chans = new_list[station]['channels']  \n        \n        for chan in chans:\n            print(f'======= Working on {station} station, {chan} channel.')\n            unsucessful_downloads = []\n            tstr = UTCDateTime(start_time)\n            tend = UTCDateTime(start_time) + t_step   \n            while tend <= UTCDateTime(end_time):  \n                oo = _get_data(cel=client,\n                              dirn=dirname,\n                              net=net, \n                              station=station, \n                              chan=chan, \n                              starttime=tstr,\n                              tend=tend,\n                              count=0)\n                unsucessful_downloads.append(oo)\n\n                if sum(unsucessful_downloads) >= patience:\n                    break\n                    \n                tstr = tend       \n                tend = tend+t_step   \n                  \n        if len(os.listdir(dirname)) == 0: \n            os.rmdir(dirname)  \n        else: \n            shutil.move(dirname, output_dir+\"/\"+dirname)\n            \n    with ThreadPool(n_processor) as p:\n        p.map(process, new_list)             \n\n\n\ndef _get_w(bg, st, station_dic, end_t, mdl, domain, output_dir, n_days, channel_list):\n    \n    next_month=bg + datetime.timedelta(n_days)\n    nt = station_dic[str(st)]['network'] \n    save_dir = os.path.join(output_dir, st)\n    save_dir2 = os.path.join(output_dir+\"xml\", st)\n\n    while next_month <= end_t:\n        if len(channel_list) == 0:\n            restrictions = Restrictions(starttime=bg,\n                                        endtime=next_month,\n                                        network=nt,\n                                        station=st,\n                                        reject_channels_with_gaps=False,\n                                        minimum_length=0.0)\n        else:\n            restrictions = Restrictions(starttime=bg,\n                                        endtime=next_month,\n                                        network=nt,\n                                        station=st,\n                                        reject_channels_with_gaps=False,\n                                        channel_priorities=channel_list,\n                                        minimum_length=0.0)\n        try:\n            mdl.download(domain, \n                         restrictions,\n                         mseed_storage = save_dir,\n                         stationxml_storage = save_dir2)\n            print(f\"** done with --> {st} -- {nt} -- {str(bg).split('T')[0]}\")                 \n\n        except Exception:\n            print(f\"!! failed downloading --> {st} -- {nt} !\")\n            pass\n        time.sleep(np.random.randint(25, 30))\n        bg = next_month\n        next_month = bg + datetime.timedelta(n_days)     \n\n\n\n\ndef _get_data(**kwargs):\n    \n    global out\n    stio = kwargs['station']; cha = kwargs['chan']\n    try:\n        st = kwargs['cel'].get_waveforms(network=kwargs['net'],\n                                   station=kwargs['station'],\n                                   channel=kwargs['chan'],\n                                   starttime=kwargs['starttime'],\n                                   endtime=kwargs['tend'],\n                                   location=False,\n                                   attach_response=True)\n        tt = str(kwargs['starttime']).split('T')[0]\n        print(f\"** --> got {stio} -- {cha} -- {tt}\")                 \n        st.merge(method=1, fill_value='interpolate')\n      #  st.interpolate(sampling_rate=100)\n        st[0].resample(100)\n        st[0].data.dtype = 'int32'            \n        st.detrend(\"demean\")\n        pre_filt = [0.8, 9.5, 40, 45] \n        st.remove_response(pre_filt=pre_filt,water_level=10,taper=True,taper_fraction=0.05)\n      #  st.filter('bandpass',freqmin = 1.0, freqmax = 45, corners=2, zerophase=True) \n        st.write(filename=kwargs['dirn']+kwargs['net']+'.'+kwargs['station']+'..'+kwargs['chan']+\"__\"+str(kwargs['starttime']).split('T')[0].replace(\"-\", \"\")+\"T000000Z__\"+str(kwargs['tend']).split('T')[0].replace(\"-\", \"\")+\"T000000Z.SAC\",format=\"SAC\")\n        out = 0\n\n    except:\n        c = kwargs['count']\n        print(f're-try downloading for {c} time!')\n        kwargs['count'] += 1\n        if kwargs['count'] <= 5:\n            time.sleep(50)\n            out = _get_data(cel=kwargs['cel'],\n                     dirn=kwargs['dirn'],\n                     net=kwargs['net'], \n                     station=kwargs['station'], \n                     chan=kwargs['chan'], \n                     starttime=kwargs['starttime'],\n                     tend=kwargs['tend'],\n                     count=kwargs['count'])\n        else:\n            print(f\"!! didnt get ---> {stio} --- {cha}\")\n            out = 1            \n    return out\n\n    "}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/__init__.py", "content": "name='utils'\n\nfrom .downloader import downloadMseeds, makeStationList, downloadSacs\nfrom .hdf5_maker import preprocessor\nfrom .associator import run_associator\nfrom .plot import plot_detections, plot_data_chart\n\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/hdf5_maker.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Aug 31 21:21:31 2019\n\n@author: mostafamousavi\n\nlast update: 01/29/2021\n\n- downsampling using the interpolation function can cause false segmentaiton error. \n    This depend on your data and its sampling rate. If you kept getting this error when \n    using multiprocessors, try using only a single cpu. \n    \n\"\"\"\n\nfrom obspy import read\nimport os\nimport platform\nfrom os import listdir\nfrom os.path import join\nimport h5py\nimport numpy as np\nimport csv\n#from tqdm import tqdm\nimport shutil\nimport json\nimport pandas as pd\nfrom multiprocessing.pool import ThreadPool\nimport multiprocessing\nimport pickle\nimport faulthandler; faulthandler.enable()\nfrom obspy import read\n\n\n\ndef preprocessor(preproc_dir, mseed_dir, stations_json, overlap=0.3, n_processor=None):\n    \n    \n    \"\"\"\n    \n    Performs preprocessing and partitions the continuous waveforms into 1-minute slices. \n\n    Parameters\n    ----------\n    preproc_dir: str\n        Path of the directory where will be located the summary files generated by preprocessor step.\n\n    mseed_dir: str\n        Path of the directory where the mseed files are located. \n\n    stations_json: str\n        Path to a JSON file containing station information.        \n        \n    overlap: float, default=0.3\n        If set, detection, and picking are performed in overlapping windows.\n           \n    n_processor: int, default=None \n        The number of CPU processors for parallel downloading.         \n\n    Returns\n    ----------\n    mseed_dir_processed_hdfs/station.csv: Phase information for the associated events in hypoInverse format. \n    \n    mseed_dir_processed_hdfs/station.hdf5: Containes all slices and preprocessed traces. \n    \n    preproc_dir/X_preprocessor_report.txt: A summary of processing performance. \n    \n    preproc_dir/time_tracks.pkl: Contain the time track of the continous data and its type.\n       \n    \"\"\"  \n \n    \n    if not n_processor:\n        n_processor = multiprocessing.cpu_count()\n    \n    json_file = open(stations_json)\n    stations_ = json.load(json_file)\n    \n    save_dir = os.path.join(os.getcwd(), str(mseed_dir)+'_processed_hdfs')\n    if os.path.isdir(save_dir):\n        print(f' *** \" {save_dir} \" directory already exists!')\n        inp = input(\" * --> Do you want to creat a new empty folder? Type (Yes or y) \")\n        if inp.lower() == \"yes\" or inp.lower() == \"y\":        \n            shutil.rmtree(save_dir)  \n    os.makedirs(save_dir)\n              \n    if not os.path.exists(preproc_dir):\n            os.makedirs(preproc_dir)\n    repfile = open(os.path.join(preproc_dir,\"X_preprocessor_report.txt\"), 'w');\n    \n    if platform.system() == 'Windows':\n        station_list = [join(mseed_dir, ev) for ev in listdir(mseed_dir) if ev.split(\"\\\\\")[-1] != \".DS_Store\"];\n    else:   \n        station_list = [join(mseed_dir, ev) for ev in listdir(mseed_dir) if ev.split(\"/\")[-1] != \".DS_Store\"];\n    \n    data_track = dict()\n    \n    def process(station):\n    # for station in station_list:\n        if platform.system() == 'Windows':\n            output_name = station.split(\"\\\\\")[-1];\n        else:\n            output_name = station.split(\"/\")[-1]\n        \n        try:\n            os.remove(output_name+'.hdf5')\n            os.remove(output_name+\".csv\")\n        except Exception:\n            pass\n        \n        HDF = h5py.File(os.path.join(save_dir, output_name+'.hdf5'), 'a')\n        HDF.create_group(\"data\")\n    \n        csvfile = open(os.path.join(save_dir, output_name+\".csv\"), 'w')\n        output_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        output_writer.writerow(['trace_name', 'start_time'])\n        csvfile.flush()   \n    \n        if platform.system() == 'Windows':\n            file_list = [join(station, ev) for ev in listdir(station) if ev.split(\"\\\\\")[-1] != \".DS_Store\"];\n        else:\n            file_list = [join(station, ev) for ev in listdir(station) if ev.split(\"/\")[-1] != \".DS_Store\"];\n            \n        mon = [ev.split('__')[1]+'__'+ev.split('__')[2] for ev in file_list ];\n        uni_list = list(set(mon))\n        uni_list.sort()        \n        tim_shift = int(60-(overlap*60))\n        \n        time_slots, comp_types = [], []\n        \n        if platform.system() == 'Windows':\n            print('============ Station {} has {} chunks of data.'.format(station.split(\"\\\\\")[1], len(uni_list)), flush=True)   \n        else:\n            print('============ Station {} has {} chunks of data.'.format(station.split(\"/\")[1], len(uni_list)), flush=True)  \n            \n        count_chuncks=0; fln=0; c1=0; c2=0; c3=0; fl_counts=1; slide_estimates=[];\n        \n        for ct, month in enumerate(uni_list):\n            matching = [s for s in file_list if month in s]\n            \n            if len(matching) == 3:  \n                \n                st1 = read(matching[0], debug_headers=True)\n                org_samplingRate = st1[0].stats.sampling_rate\n                \n                for tr in st1:                   \n                    time_slots.append((tr.stats.starttime, tr.stats.endtime))\n                    comp_types.append(3)\n\n                try:\n                    st1.merge(fill_value=0) \n                except Exception:\n                    st1=_resampling(st1)\n                    st1.merge(fill_value=0)                     \n                st1.detrend('demean') \n                count_chuncks += 1; c3 += 1\n                if platform.system() == 'Windows':\n                    print('  * '+station.split(\"\\\\\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 3 components .. sampling rate: '+str(org_samplingRate))  \n                else:\n                    print('  * '+station.split(\"/\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 3 components .. sampling rate: '+str(org_samplingRate))  \n                 \n                st2 = read(matching[1], debug_headers=True) \n                try:\n                    st2.merge(fill_value=0)                    \n                except Exception:\n                    st2=_resampling(st2)\n                    st2.merge(fill_value=0)                    \n                st2.detrend('demean')\n    \n                st3 = read(matching[2], debug_headers=True) \n                try:\n                    st3.merge(fill_value=0)                     \n                except Exception:\n                    st3=_resampling(st3)\n                    st3.merge(fill_value=0) \n                st3.detrend('demean')\n                \n                st1.append(st2[0])\n                st1.append(st3[0])\n                st1.filter('bandpass',freqmin = 1.0, freqmax = 45, corners=2, zerophase=True)\n                st1.taper(max_percentage=0.001, type='cosine', max_length=2)\n                if len([tr for tr in st1 if tr.stats.sampling_rate != 100.0]) != 0:\n                    try:\n                        st1.interpolate(100, method=\"linear\")\n                    except Exception:\n                        st1=_resampling(st1)\n                        \n                                     \n                longest = st1[0].stats.npts\n                start_time = st1[0].stats.starttime\n                end_time = st1[0].stats.endtime\n                \n                for tt in st1:\n                    if tt.stats.npts > longest:\n                        longest = tt.stats.npts\n                        start_time = tt.stats.starttime\n                        end_time = tt.stats.endtime\n                    \n                st1.trim(start_time, end_time, pad=True, fill_value=0)\n\n                start_time = st1[0].stats.starttime\n                end_time = st1[0].stats.endtime  \n                slide_estimates.append((end_time - start_time)//tim_shift)                \n                fl_counts += 1 \n                \n                chanL = [st1[0].stats.channel[-1], st1[1].stats.channel[-1], st1[2].stats.channel[-1]]\n                next_slice = start_time+60               \n                while next_slice <= end_time:\n                    w = st1.slice(start_time, next_slice) \n                    npz_data = np.zeros([6000,3])\n                                        \n                    npz_data[:,2] = w[chanL.index('Z')].data[:6000]\n                    try: \n                        npz_data[:,0] = w[chanL.index('E')].data[:6000]\n                    except Exception:\n                        npz_data[:,0] = w[chanL.index('1')].data[:6000]\n                    try: \n                        npz_data[:,1] = w[chanL.index('N')].data[:6000]\n                    except Exception:\n                        npz_data[:,1] = w[chanL.index('2')].data[:6000]                        \n                                     \n                    tr_name = st1[0].stats.station+'_'+st1[0].stats.network+'_'+st1[0].stats.channel[:2]+'_'+str(start_time)\n                    HDF = h5py.File(os.path.join(save_dir,output_name+'.hdf5'), 'r')\n                    dsF = HDF.create_dataset('data/'+tr_name, npz_data.shape, data = npz_data, dtype= np.float32)        \n                       \n                    dsF.attrs[\"trace_name\"] = tr_name \n                    if platform.system() == 'Windows':\n                        dsF.attrs[\"receiver_code\"] = station.split(\"\\\\\")[-1]\n                        dsF.attrs[\"network_code\"] = stations_[station.split(\"\\\\\")[-1]]['network']\n                        dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][0]\n                        dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][1]\n                        dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][2] \n                    else:\n                        dsF.attrs[\"receiver_code\"] = station.split(\"/\")[-1]\n                        dsF.attrs[\"network_code\"] = stations_[station.split(\"/\")[-1]]['network']\n                        dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"/\")[-1]]['coords'][0]\n                        dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"/\")[-1]]['coords'][1]\n                        dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"/\")[-1]]['coords'][2] \n                    \n                    start_time_str = str(start_time)   \n                    start_time_str = start_time_str.replace('T', ' ')                 \n                    start_time_str = start_time_str.replace('Z', '')          \n                    dsF.attrs['trace_start_time'] = start_time_str\n                    HDF.flush()\n                    output_writer.writerow([str(tr_name), start_time_str])  \n                    csvfile.flush()\n                    fln += 1            \n            \n                    start_time = start_time+tim_shift\n                    next_slice = next_slice+tim_shift \n  \n            if len(matching) == 1:  \n                 count_chuncks += 1; c1 += 1\n                \n                 st1 = read(matching[0], debug_headers=True)\n                 org_samplingRate = st1[0].stats.sampling_rate\n\n                 for tr in st1:                   \n                     time_slots.append((tr.stats.starttime, tr.stats.endtime))\n                     comp_types.append(1)\n                 try:\n                     st1.merge(fill_value=0) \n                 except Exception:\n                     st1=_resampling(st1)\n                     st1.merge(fill_value=0)                 \n                 st1.detrend('demean')\n                 \n                 if platform.system() == 'Windows':\n                     print('  * '+station.split(\"\\\\\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 1 components .. sampling rate: '+str(org_samplingRate)) \n                 else:\n                     print('  * '+station.split(\"/\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 1 components .. sampling rate: '+str(org_samplingRate)) \n                 \n                 st1.filter('bandpass',freqmin = 1.0, freqmax = 45, corners=2, zerophase=True)\n                 st1.taper(max_percentage=0.001, type='cosine', max_length=2)\n                 if len([tr for tr in st1 if tr.stats.sampling_rate != 100.0]) != 0:\n                     try:\n                         st1.interpolate(100, method=\"linear\")\n                     except Exception:\n                         st1=_resampling(st1) \n                         \n                 chan = st1[0].stats.channel\n                 start_time = st1[0].stats.starttime\n                 end_time = st1[0].stats.endtime\n                 slide_estimates.append((end_time - start_time)//tim_shift)\n                 fl_counts += 1    \n\n                 next_slice = start_time+60\n\n                 while next_slice <= end_time:\n                     w = st1.slice(start_time, next_slice)                    \n                     npz_data = np.zeros([6000,3])\n                     if chan[-1] == 'Z':\n                         npz_data[:,2] = w[0].data[:6000]\n                     if chan[-1] == 'E' or  chan[-1] == '1':\n                         npz_data[:,0] = w[0].data[:6000]\n                     if chan[-1] == 'N' or  chan[-1] == '2':\n                         npz_data[:,1] = w[0].data[:6000]\n                    \n                     tr_name = st1[0].stats.station+'_'+st1[0].stats.network+'_'+st1[0].stats.channel[:2]+'_'+str(start_time)\n                     HDF = h5py.File(os.path.join(save_dir,output_name+'.hdf5'), 'r')\n                     dsF = HDF.create_dataset('data/'+tr_name, npz_data.shape, data = npz_data, dtype= np.float32)        \n                     dsF.attrs[\"trace_name\"] = tr_name \n                     \n                     if platform.system() == 'Windows':\n                         dsF.attrs[\"receiver_code\"] = station.split(\"\\\\\")[-1]\n                         dsF.attrs[\"network_code\"] = stations_[station.split(\"\\\\\")[-1]]['network']\n                         dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][0]\n                         dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][1]\n                         dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][2]  \n                     else:                         \n                         dsF.attrs[\"receiver_code\"] = station.split(\"/\")[-1]\n                         dsF.attrs[\"network_code\"] = stations_[station.split(\"/\")[-1]]['network']\n                         dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"/\")[-1]]['coords'][0]\n                         dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"/\")[-1]]['coords'][1]\n                         dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"/\")[-1]]['coords'][2] \n                         \n                     start_time_str = str(start_time)   \n                     start_time_str = start_time_str.replace('T', ' ')                 \n                     start_time_str = start_time_str.replace('Z', '')          \n                     dsF.attrs['trace_start_time'] = start_time_str\n                     HDF.flush()\n                     output_writer.writerow([str(tr_name), start_time_str])  \n                     csvfile.flush()\n                     fln += 1            \n\n                     start_time = start_time+tim_shift\n                     next_slice = next_slice+tim_shift                \n                \n            if len(matching) == 2:  \n                count_chuncks += 1; c2 += 1                \n                st1 = read(matching[0], debug_headers=True)\n                org_samplingRate = st1[0].stats.sampling_rate\n\n                for tr in st1:                   \n                    time_slots.append((tr.stats.starttime, tr.stats.endtime))\n                    comp_types.append(2)\n\n                try:\n                    st1.merge(fill_value=0) \n                except Exception:\n                    st1=_resampling(st1)\n                    st1.merge(fill_value=0)  \n                st1.detrend('demean')  \n                \n                org_samplingRate = st1[0].stats.sampling_rate\n                \n                if platform.system() == 'Windows':\n                    print('  * '+station.split(\"\\\\\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 2 components .. sampling rate: '+str(org_samplingRate)) \n                else:    \n                    print('  * '+station.split(\"/\")[1]+' ('+str(count_chuncks)+') .. '+month.split('T')[0]+' --> '+month.split('__')[1].split('T')[0]+' .. 2 components .. sampling rate: '+str(org_samplingRate)) \n                 \n                st2 = read(matching[1], debug_headers=True)  \n                try:\n                    st2.merge(fill_value=0) \n                except Exception:\n                    st2=_resampling(st1)\n                    st2.merge(fill_value=0)                 \n                st2.detrend('demean')\n    \n                st1.append(st2[0])\n                st1.filter('bandpass',freqmin = 1.0, freqmax = 45, corners=2, zerophase=True)\n                st1.taper(max_percentage=0.001, type='cosine', max_length=2)\n                if len([tr for tr in st1 if tr.stats.sampling_rate != 100.0]) != 0:\n                    try:\n                        st1.interpolate(100, method=\"linear\")\n                    except Exception:\n                        st1=_resampling(st1)   \n                        \n                longest = st1[0].stats.npts\n                start_time = st1[0].stats.starttime\n                end_time = st1[0].stats.endtime\n                \n                for tt in st1:\n                    if tt.stats.npts > longest:\n                        longest = tt.stats.npts\n                        start_time = tt.stats.starttime\n                        end_time = tt.stats.endtime               \n                \n                st1.trim(start_time, end_time, pad=True, fill_value=0)\n\n                start_time = st1[0].stats.starttime\n                end_time = st1[0].stats.endtime\n                slide_estimates.append((end_time - start_time)//tim_shift)\n                \n                chan1 = st1[0].stats.channel\n                chan2 = st1[1].stats.channel\n                fl_counts += 1  \n                \n                next_slice = start_time+60\n\n                while next_slice <= end_time:\n                    w = st1.slice(start_time, next_slice)                     \n                    npz_data = np.zeros([6000,3])\n                    if chan1[-1] == 'Z':\n                        npz_data[:,2] = w[0].data[:6000]\n                    elif chan1[-1] == 'E' or  chan1[-1] == '1':\n                        npz_data[:,0] = w[0].data[:6000]\n                    elif chan1[-1] == 'N' or  chan1[-1] == '2':\n                        npz_data[:,1] = w[0].data[:6000]\n\n                    if chan2[-1] == 'Z':\n                        npz_data[:,2] = w[1].data[:6000]\n                    elif chan2[-1] == 'E' or  chan2[-1] == '1':\n                        npz_data[:,0] = w[1].data[:6000]\n                    elif chan2[-1] == 'N' or  chan2[-1] == '2':\n                        npz_data[:,1] = w[1].data[:6000]\n                    \n                    tr_name = st1[0].stats.station+'_'+st1[0].stats.network+'_'+st1[0].stats.channel[:2]+'_'+str(start_time)\n                    HDF = h5py.File(os.path.join(save_dir,output_name+'.hdf5'), 'r')\n                    dsF = HDF.create_dataset('data/'+tr_name, npz_data.shape, data = npz_data, dtype= np.float32)        \n                       \n                    dsF.attrs[\"trace_name\"] = tr_name \n                    \n                    if platform.system() == 'Windows':\n                        dsF.attrs[\"receiver_code\"] = station.split(\"\\\\\")[-1]\n                        dsF.attrs[\"network_code\"] = stations_[station.split(\"\\\\\")[-1]]['network']\n                        dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][0]\n                        dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][1]\n                        dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"\\\\\")[-1]]['coords'][2] \n                    else:    \n                        dsF.attrs[\"receiver_code\"] = station.split(\"/\")[-1]\n                        dsF.attrs[\"network_code\"] = stations_[station.split(\"/\")[-1]]['network']\n                        dsF.attrs[\"receiver_latitude\"] = stations_[station.split(\"/\")[-1]]['coords'][0]\n                        dsF.attrs[\"receiver_longitude\"] = stations_[station.split(\"/\")[-1]]['coords'][1]\n                        dsF.attrs[\"receiver_elevation_m\"] = stations_[station.split(\"/\")[-1]]['coords'][2] \n                    \n                    start_time_str = str(start_time)   \n                    start_time_str = start_time_str.replace('T', ' ')                 \n                    start_time_str = start_time_str.replace('Z', '')          \n                    dsF.attrs['trace_start_time'] = start_time_str\n                    HDF.flush()\n                    output_writer.writerow([str(tr_name), start_time_str])  \n                    csvfile.flush()\n                    fln += 1            \n            \n                    start_time = start_time+tim_shift\n                    next_slice = next_slice+tim_shift \n                    \n            st1, st2, st3 = None, None, None\n                \n        HDF.close() \n        \n        dd = pd.read_csv(os.path.join(save_dir, output_name+\".csv\"))\n                \n        \n        assert count_chuncks == len(uni_list)  \n        assert sum(slide_estimates)-(fln/100) <= len(dd) <= sum(slide_estimates)+10\n        data_track[output_name]=[time_slots, comp_types]\n        print(f\" Station {output_name} had {len(uni_list)} chuncks of data\") \n        print(f\"{len(dd)} slices were written, {sum(slide_estimates)} were expected.\")\n        print(f\"Number of 1-components: {c1}. Number of 2-components: {c2}. Number of 3-components: {c3}.\")\n        try:\n            print(f\"Original samplieng rate: {org_samplingRate}.\") \n            repfile.write(f' Station {output_name} had {len(uni_list)} chuncks of data, {len(dd)} slices were written, {int(sum(slide_estimates))} were expected. Number of 1-components: {c1}, Number of 2-components: {c2}, number of 3-components: {c3}, original samplieng rate: {org_samplingRate}\\n')\n        except Exception:\n            pass\n    with ThreadPool(n_processor) as p:\n        p.map(process, station_list) \n    with open(os.path.join(preproc_dir,'time_tracks.pkl'), 'wb') as f:\n        pickle.dump(data_track, f, pickle.HIGHEST_PROTOCOL)\n\n\n\ndef stationListFromMseed(mseed_directory, station_locations):\n    \"\"\"\n    Contributed by: Tyler Newton\n        \n    Reads all miniseed files contained within subdirectories in the specified directory and generates a station_list.json file that describes the miniseed files in the correct format for EQTransformer.\n    \n    Parameters\n    ----------\n    mseed_directory: str\n        String specifying the absolute path to the directory containing miniseed files. Directory must contain subdirectories of station names, which contain miniseed files in the EQTransformer format. \n        Each component must be a seperate miniseed file, and the naming\n        convention is GS.CA06.00.HH1__20190901T000000Z__20190902T000000Z\n        .mseed, or more generally \n        NETWORK.STATION.LOCATION.CHANNEL__STARTTIMESTAMP__ENDTIMESTAMP.mseed\n        where LOCATION is optional.\n    station_locations: dict\n        Dictonary with station names as keys and lists of latitude,\n        longitude, and elevation as items. For example: {\"CA06\": [35.59962,\n        -117.49268, 796.4], \"CA10\": [35.56736, -117.667427, 835.9]}\n   \n    Returns\n    -------\n    stations_list.json: A dictionary containing information for the available stations.\n    \n    Example\n    -------\n    directory = '/Users/human/Downloads/eqt/examples/downloads_mseeds'\n    locations = {\"CA06\": [35.59962, -117.49268, 796.4], \"CA10\": [35.56736, -117.667427, 835.9]}\n    stationListFromMseed(directoy, locations)\n    \"\"\"\n\n    station_list = {}\n\n    # loop through subdirectories of specified directory\n    for subdirectory in os.scandir(mseed_directory):\n        if subdirectory.is_dir():\n            channels = []\n            # build channel list from miniseed files\n            for mseed_file in os.scandir(subdirectory.path):\n                temp_stream = read(mseed_file.path, debug_headers=True)\n                channels.append(temp_stream[0].stats.channel)\n            # add entry to station list for the current station\n            station_list[str(temp_stream[0].stats.station)] = {\"network\":\n                        temp_stream[0].stats.network, \"channels\": list(set(\n                        channels)), \"coords\": station_locations[str(\n                        temp_stream[0].stats.station)]}\n\n    with open('station_list.json', 'w') as fp:\n        json.dump(station_list, fp)             \n        \ndef _resampling(st):\n    need_resampling = [tr for tr in st if tr.stats.sampling_rate != 100.0]\n    if len(need_resampling) > 0:\n       # print('resampling ...', flush=True)    \n        for indx, tr in enumerate(need_resampling):\n            if tr.stats.delta < 0.01:\n                tr.filter('lowpass',freq=45,zerophase=True)\n            tr.resample(100)\n            tr.stats.sampling_rate = 100\n            tr.stats.delta = 0.01\n            tr.data.dtype = 'int32'\n            st.remove(tr)                    \n            st.append(tr)    \n             \n    return st"}
{"type": "source_file", "path": "BlocklyEQTransformer/core/trainer.py", "content": "# -*- coding: utf-8 -*-\r\n# MIT License\r\n#\r\n# Copyright (c) 2022 Hao Mai & Pascal Audet\r\n#\r\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\r\n# V1.59 created by @author: mostafamousavi\r\n# Ref Repo: https://github.com/smousavi05/EQTransformer\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the \"Software\"), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nfrom __future__ import print_function\r\nimport keras\r\nfrom keras import backend as K\r\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\r\nfrom keras.layers import Input\r\n# import os\r\n# os.environ['KERAS_BACKEND']='tensorflow'\r\n# from tensorflow import keras\r\n# from tensorflow.keras import backend as K\r\n# from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\r\n# from tensorflow.keras.layers import Input\r\nimport tensorflow as tf\r\nimport matplotlib\r\nmatplotlib.use('agg')\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport h5py\r\nimport time\r\nimport os\r\nimport shutil\r\nimport multiprocessing\r\nfrom .EqT_utils import DataGenerator, _lr_schedule, cred2, PreLoadGenerator, data_reader\r\nimport datetime\r\nfrom tqdm import tqdm\r\nfrom tensorflow.python.util import deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n\r\n#Transfer learning\r\n# import dependecies\r\nfrom keras.layers import add, Activation, LSTM, Conv1D\r\nfrom keras.models import load_model\r\nfrom .EqT_utils import f1, SeqSelfAttention, FeedForward, LayerNormalization\r\nfrom keras.optimizers import Adam\r\n\r\n\r\ndef trainer(input_model=None,\r\n            retrain=0,\r\n            input_hdf5=None,\r\n            input_csv=None,\r\n            output_name=None,\r\n            input_dimention=(6000, 3),\r\n            cnn_blocks=5,\r\n            lstm_blocks=2,\r\n            padding='same',\r\n            activation = 'relu',\r\n            drop_rate=0.1,\r\n            shuffle=True,\r\n            label_type='gaussian',\r\n            normalization_mode='std',\r\n            augmentation=False,\r\n            add_event_r=0.6,\r\n            shift_event_r=0.99,\r\n            add_noise_r=0.3,\r\n            drop_channel_r=0.5,\r\n            add_gap_r=0.2,\r\n            scale_amplitude_r=None,\r\n            pre_emphasis=False,\r\n            loss_weights=[ 0.40, 0.55], #0.05,\r\n            loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\r\n            train_valid_test_split=[0.85, 0.05, 0.10],\r\n            mode='generator',\r\n            batch_size=200,\r\n            epochs=200,\r\n            monitor='val_loss',\r\n            patience=12,\r\n            multi_gpu=False,\r\n            number_of_gpus=4,\r\n            gpuid=None,\r\n            gpu_limit=None,\r\n            use_multiprocessing=True,\r\n            phase_types=['d','P','S'],\r\n            nb_filters=[8, 16, 16, 32, 32, 64, 64],\r\n            kernel_size=[11, 9, 7, 7, 5, 5, 3]):\r\n\r\n    \"\"\"\r\n\r\n    Generate a model and train it.\r\n\r\n    Parameters\r\n    ----------\r\n    input_model: str, default=None\r\n        Path to an hdf5 file containing pretrained mode for transfer learning and fine tuning use.\r\n    retrain: int, default = 0\r\n        Flag for loading retrained model. If 0, not use retrained model;\r\n        if 1, use retrained model for transfer learning; if 2, use retrained\r\n        model for fine tuning.\r\n    input_hdf5: str, default=None\r\n        Path to an hdf5 file containing only one class of data with NumPy arrays containing 3 component waveforms each 1 min long.\r\n\r\n    input_csv: str, default=None\r\n        Path to a CSV file with one column (trace_name) listing the name of all datasets in the hdf5 file.\r\n\r\n    output_name: str, default=None\r\n        Output directory.\r\n\r\n    input_dimention: tuple, default=(6000, 3)\r\n        OLoss types for detection, P picking, and S picking respectively.\r\n\r\n    cnn_blocks: int, default=5\r\n        The number of residual blocks of convolutional layers.\r\n\r\n    lstm_blocks: int, default=2\r\n        The number of residual blocks of BiLSTM layers.\r\n\r\n    padding: str, default='same'\r\n        Padding type.\r\n\r\n    activation: str, default='relu'\r\n        Activation function used in the hidden layers.\r\n\r\n    drop_rate: float, default=0.1\r\n        Dropout value.\r\n\r\n    shuffle: bool, default=True\r\n        To shuffle the list prior to the training.\r\n\r\n    label_type: str, default='triangle'\r\n        Labeling type. 'gaussian', 'triangle', or 'box'.\r\n\r\n    normalization_mode: str, default='std'\r\n        Mode of normalization for data preprocessing, 'max': maximum amplitude among three components, 'std', standard deviation.\r\n\r\n    augmentation: bool, default=True\r\n        If True, data will be augmented simultaneously during the training.\r\n\r\n    add_event_r: float, default=0.6\r\n        Rate of augmentation for adding a secondary event randomly into the empty part of a trace.\r\n\r\n    shift_event_r: float, default=0.99\r\n        Rate of augmentation for randomly shifting the event within a trace.\r\n\r\n    add_noise_r: float, defaults=0.3\r\n        Rate of augmentation for adding Gaussian noise with different SNR into a trace.\r\n\r\n    drop_channel_r: float, defaults=0.4\r\n        Rate of augmentation for randomly dropping one of the channels.\r\n\r\n    add_gap_r: float, defaults=0.2\r\n        Add an interval with zeros into the waveform representing filled gaps.\r\n\r\n    scale_amplitude_r: float, defaults=None\r\n        Rate of augmentation for randomly scaling the trace.\r\n\r\n    pre_emphasis: bool, defaults=False\r\n        If True, waveforms will be pre-emphasized. Defaults to False.\r\n\r\n    loss_weights: list, defaults=[0.03, 0.40, 0.58]\r\n        Loss weights for detection, P picking, and S picking respectively.\r\n\r\n    loss_types: list, defaults=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy']\r\n        Loss types for detection, P picking, and S picking respectively.\r\n\r\n    train_valid_test_split: list, defaults=[0.85, 0.05, 0.10]\r\n        Precentage of data split into the training, validation, and test sets respectively.\r\n\r\n    mode: str, defaults='generator'\r\n        Mode of running. 'generator', or 'preload'.\r\n\r\n    batch_size: int, default=200\r\n        Batch size.\r\n\r\n    epochs: int, default=200\r\n        The number of epochs.\r\n\r\n    monitor: int, default='val_loss'\r\n        The measure used for monitoring.\r\n\r\n    patience: int, default=12\r\n        The number of epochs without any improvement in the monitoring measure to automatically stop the training.\r\n\r\n    multi_gpu: bool, default=False\r\n        If True, multiple GPUs will be used for the training.\r\n\r\n    number_of_gpus: int, default=4\r\n        Number of GPUs uses for multi-GPU training.\r\n\r\n    gpuid: int, default=None\r\n        Id of GPU used for the prediction. If using CPU set to None.\r\n\r\n    gpu_limit: float, default=None\r\n        Set the maximum percentage of memory usage for the GPU.\r\n\r\n    use_multiprocessing: bool, default=True\r\n        If True, multiple CPUs will be used for the preprocessing of data even when GPU is used for the prediction.\r\n\r\n    phase_types: list, default = ['d', 'P', 'S']\r\n        Define detect channels. 'd': signal detector, 'P' : P-phase, 'S' : S-phase\r\n\r\n    Returns\r\n    --------\r\n    output_name/models/output_name_.h5: This is where all good models will be saved.\r\n\r\n    output_name/final_model.h5: This is the full model for the last epoch.\r\n\r\n    output_name/model_weights.h5: These are the weights for the last model.\r\n\r\n    output_name/history.npy: Training history.\r\n\r\n    output_name/X_report.txt: A summary of the parameters used for prediction and performance.\r\n\r\n    output_name/test.npy: A number list containing the trace names for the test set.\r\n\r\n    output_name/X_learning_curve_f1.png: The learning curve of Fi-scores.\r\n\r\n    output_name/X_learning_curve_loss.png: The learning curve of loss.\r\n\r\n    Notes\r\n    --------\r\n    'generator' mode is memory efficient and more suitable for machines with fast disks.\r\n    'pre_load' mode is faster but requires more memory and it comes with only box labeling.\r\n\r\n    \"\"\"\r\n\r\n\r\n    args = {\r\n    \"input_model\":input_model,\r\n    \"retrain\":retrain,\r\n    \"input_hdf5\": input_hdf5,\r\n    \"input_csv\": input_csv,\r\n    \"output_name\": output_name,\r\n    \"input_dimention\": input_dimention,\r\n    \"cnn_blocks\": cnn_blocks,\r\n    \"lstm_blocks\": lstm_blocks,\r\n    \"padding\": padding,\r\n    \"activation\": activation,\r\n    \"drop_rate\": drop_rate,\r\n    \"shuffle\": shuffle,\r\n    \"label_type\": label_type,\r\n    \"normalization_mode\": normalization_mode,\r\n    \"augmentation\": augmentation,\r\n    \"add_event_r\": add_event_r,\r\n    \"shift_event_r\": shift_event_r,\r\n    \"add_noise_r\": add_noise_r,\r\n    \"add_gap_r\": add_gap_r,\r\n    \"drop_channel_r\": drop_channel_r,\r\n    \"scale_amplitude_r\": scale_amplitude_r,\r\n    \"pre_emphasis\": pre_emphasis,\r\n    \"loss_weights\": loss_weights,\r\n    \"loss_types\": loss_types,\r\n    \"train_valid_test_split\": train_valid_test_split,\r\n    \"mode\": mode,\r\n    \"batch_size\": batch_size,\r\n    \"epochs\": epochs,\r\n    \"monitor\": monitor,\r\n    \"patience\": patience,\r\n    \"multi_gpu\": multi_gpu,\r\n    \"number_of_gpus\": number_of_gpus,\r\n    \"gpuid\": gpuid,\r\n    \"gpu_limit\": gpu_limit,\r\n    \"use_multiprocessing\": use_multiprocessing,\r\n    \"phase_types\":phase_types,\r\n    \"nb_filters\":nb_filters,\r\n    \"kernel_size\":kernel_size\r\n    }\r\n\r\n    def train(args):\r\n        \"\"\"\r\n\r\n        Performs the training.\r\n\r\n        Parameters\r\n        ----------\r\n        args : dic\r\n            A dictionary object containing all of the input parameters.\r\n\r\n        Returns\r\n        -------\r\n        history: dic\r\n            Training history.\r\n\r\n        model:\r\n            Trained model.\r\n\r\n        start_training: datetime\r\n            Training start time.\r\n\r\n        end_training: datetime\r\n            Training end time.\r\n\r\n        save_dir: str\r\n            Path to the output directory.\r\n\r\n        save_models: str\r\n            Path to the folder for saveing the models.\r\n\r\n        training size: int\r\n            Number of training samples.\r\n\r\n        validation size: int\r\n            Number of validation samples.\r\n\r\n        \"\"\"\r\n\r\n\r\n        save_dir, save_models=_make_dir(args['output_name'])\r\n        training, validation=_split(args, save_dir)\r\n        callbacks=_make_callback(args, save_models)\r\n        # build model\r\n        model=_build_model(args)\r\n\r\n        if args['gpuid']:\r\n            os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(gpuid)\r\n            tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n            config = tf.ConfigProto()\r\n            config.gpu_options.allow_growth = True\r\n            config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit'])\r\n            K.tensorflow_backend.set_session(tf.Session(config=config))\r\n\r\n        start_training = time.time()\r\n\r\n        if args['mode'] == 'generator':\r\n\r\n            params_training = {'file_name': str(args['input_hdf5']),\r\n                              'dim': args['input_dimention'][0],\r\n                              'batch_size': args['batch_size'],\r\n                              'n_channels': args['input_dimention'][-1],\r\n                              'shuffle': args['shuffle'],\r\n                              'norm_mode': args['normalization_mode'],\r\n                              'label_type': args['label_type'],\r\n                              'augmentation': args['augmentation'],\r\n                              'add_event_r': args['add_event_r'],\r\n                              'add_gap_r': args['add_gap_r'],\r\n                              'shift_event_r': args['shift_event_r'],\r\n                              'add_noise_r': args['add_noise_r'],\r\n                              'drop_channe_r': args['drop_channel_r'],\r\n                              'scale_amplitude_r': args['scale_amplitude_r'],\r\n                              'pre_emphasis': args['pre_emphasis'],\r\n                              'phase_type': args['phase_types']}\r\n\r\n            params_validation = {'file_name': str(args['input_hdf5']),\r\n                                 'dim': args['input_dimention'][0],\r\n                                 'batch_size': args['batch_size'],\r\n                                 'n_channels': args['input_dimention'][-1],\r\n                                 'shuffle': False,\r\n                                 'norm_mode': args['normalization_mode'],\r\n                                 'augmentation': False,\r\n                                 'phase_type': args['phase_types']}\r\n\r\n            training_generator = DataGenerator(training, **params_training)\r\n            validation_generator = DataGenerator(validation, **params_validation)\r\n\r\n            print('Started training in generator mode ...')\r\n            history = model.fit_generator(generator=training_generator,\r\n                                          validation_data=validation_generator,\r\n                                          use_multiprocessing=args['use_multiprocessing'],\r\n                                          workers=multiprocessing.cpu_count(),\r\n                                          callbacks=callbacks,\r\n                                          epochs=args['epochs'],\r\n                                          class_weight={0: 0.11, 1: 0.89})\r\n\r\n        elif args['mode'] == 'preload':\r\n            X, y1, y2, y3 = data_reader(list_IDs=training+validation,\r\n                                       file_name=str(args['input_hdf5']),\r\n                                       dim=args['input_dimention'][0],\r\n                                       n_channels=args['input_dimention'][-1],\r\n                                       norm_mode=args['normalization_mode'],\r\n                                       augmentation=args['augmentation'],\r\n                                       add_event_r=args['add_event_r'],\r\n                                       add_gap_r=args['add_gap_r'],\r\n                                       shift_event_r=args['shift_event_r'],\r\n                                       add_noise_r=args['add_noise_r'],\r\n                                       drop_channe_r=args['drop_channel_r'],\r\n                                       scale_amplitude_r=args['scale_amplitude_r'],\r\n                                       pre_emphasis=args['pre_emphasis'])\r\n\r\n            print('Started training in preload mode ...', flush=True)\r\n            history = model.fit({'input': X},\r\n                                {'detector': y1, 'picker_P': y2, 'picker_S': y3},\r\n                                epochs=args['epochs'],\r\n                                validation_split=args['train_valid_test_split'][1],\r\n                                batch_size=args['batch_size'],\r\n                                callbacks=callbacks,\r\n                                class_weight={0: 0.11, 1: 0.89})\r\n        else:\r\n            print('Please specify training_mode !', flush=True)\r\n        end_training = time.time()\r\n\r\n        return history, model, start_training, end_training, save_dir, save_models, len(training), len(validation)\r\n\r\n    history, model, start_training, end_training, save_dir, save_models, training_size, validation_size=train(args)\r\n    _document_training(history, model, start_training, end_training, save_dir, save_models, training_size, validation_size, args)\r\n\r\n\r\n\r\n\r\n\r\ndef _make_dir(output_name):\r\n\r\n    \"\"\"\r\n\r\n    Make the output directories.\r\n\r\n    Parameters\r\n    ----------\r\n    output_name: str\r\n        Name of the output directory.\r\n\r\n    Returns\r\n    -------\r\n    save_dir: str\r\n        Full path to the output directory.\r\n\r\n    save_models: str\r\n        Full path to the model directory.\r\n\r\n    \"\"\"\r\n\r\n    if output_name == None:\r\n        print('Please specify output_name!')\r\n        return\r\n    else:\r\n        save_dir = os.path.join(os.getcwd(), str(output_name)+'_outputs')\r\n        save_models = os.path.join(save_dir, 'models')\r\n        if os.path.isdir(save_dir):\r\n            shutil.rmtree(save_dir)\r\n        os.makedirs(save_models)\r\n    return save_dir, save_models\r\n\r\ndef _build_output(model, output_list = ['d','P','S']):\r\n\r\n    \"\"\"\r\n    Hao created this function to amplify the output channels of the model.\r\n    Transfer learning for building the output layer.\r\n\r\n    Parameters\r\n    ----------\r\n    model: loaded model\r\n        A pre-built model with all trained parameters.\r\n    output_list: list\r\n        A string list which stores all names of desired output channels.\r\n        default: output_list = ['d','P','S']\r\n    Returns\r\n    -------\r\n    model:\r\n        Compiled model.\r\n\r\n    \"\"\"\r\n    base_model = model\r\n\r\n    #print(base_model.summary())\r\n    decoder_D = base_model.layers[-6].output\r\n    decoder_P = base_model.layers[-5].output\r\n    decoder_S = base_model.layers[-4].output\r\n\r\n    new_outputs = []\r\n    if 'd' in output_list or 'D' in output_list or 'Detector' in output_list:\r\n        # add detector channel\r\n        output_name = 'detector'\r\n        d = Conv1D(1, 11, padding = 'same', activation='sigmoid', name=output_name)(decoder_D)\r\n        new_outputs.append(d)\r\n    for picker_name in output_list:\r\n        if picker_name[0].upper() == 'P':\r\n            # add P-type output channel\r\n            output_name = 'picker_' + picker_name\r\n            P = Conv1D(1, 11, padding = 'same', activation='sigmoid', name=output_name)(decoder_P)\r\n            new_outputs.append(P)\r\n        if picker_name[0].upper() == 'S':\r\n            # add S-type output channel\r\n            output_name = 'picker_' + picker_name\r\n            S = Conv1D(1, 11, padding = 'same', activation='sigmoid', name=output_name)(decoder_S)\r\n            new_outputs.append(S)\r\n    model = keras.Model(inputs=base_model.inputs, outputs=new_outputs)\r\n    return model\r\n\r\ndef _build_model(args):\r\n\r\n    \"\"\"\r\n\r\n    Build and compile the model.\r\n\r\n    Transfer Learning:\r\n    Try to frozen all layers except for the top one.\r\n    Revise model here.\r\n    - Hao\r\n    Parameters\r\n    ----------\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    Returns\r\n    -------\r\n    model:\r\n        Compiled model.\r\n\r\n    \"\"\"\r\n    # build new model (not transfer learning or fine tuning)\r\n    if args['retrain'] == -1:\r\n        inp = Input(shape=args['input_dimention'], name='input')\r\n        model = cred2(nb_filters=args['nb_filters'], #[8, 16, 16, 32, 32, 64, 64],\r\n                  kernel_size=args['kernel_size'], #[11, 9, 7, 7, 5, 5, 3],\r\n                  padding=args['padding'],\r\n                  activationf =args['activation'],\r\n                  cnn_blocks=args['cnn_blocks'],\r\n                  BiLSTM_blocks=args['lstm_blocks'],\r\n                  drop_rate=args['drop_rate'],\r\n                  loss_weights=args['loss_weights'],\r\n                  loss_types=args['loss_types'],\r\n                  kernel_regularizer=keras.regularizers.l2(1e-6),\r\n                  bias_regularizer=keras.regularizers.l1(1e-4),\r\n                  multi_gpu=args['multi_gpu'],\r\n                  gpu_number=args['number_of_gpus'],\r\n                    )(inp)\r\n        model = _build_output(model, args['phase_types'])\r\n        model.summary()\r\n        print('Loading is complete!', flush=True)\r\n        return model\r\n    if args['retrain'] == 0:\r\n    # ================================================ #\r\n    #                  Original-Model                #\r\n    # ================================================ #\r\n        inp = Input(shape=args['input_dimention'], name='input')\r\n        model = cred2(nb_filters=[8, 16, 16, 32, 32, 64, 64],\r\n                  kernel_size=[11, 9, 7, 7, 5, 5, 3],\r\n                  padding=args['padding'],\r\n                  activationf =args['activation'],\r\n                  cnn_blocks=args['cnn_blocks'],\r\n                  BiLSTM_blocks=args['lstm_blocks'],\r\n                  drop_rate=args['drop_rate'],\r\n                  loss_weights=args['loss_weights'],\r\n                  loss_types=args['loss_types'],\r\n                  kernel_regularizer=keras.regularizers.l2(1e-6),\r\n                  bias_regularizer=keras.regularizers.l1(1e-4),\r\n                  multi_gpu=args['multi_gpu'],\r\n                  gpu_number=args['number_of_gpus'],\r\n                    )(inp)\r\n\r\n        model.summary()\r\n        print('Loading is complete!', flush=True)\r\n        return model\r\n    if args['retrain'] > 0:\r\n    # ================================================ #\r\n    #                  Transfer Learning               #\r\n    # ================================================ #\r\n        # args['input_model'] = \"/Users/hao/opt/anaconda3/envs/eqtdev/lib/python3.7/site-packages/EQTransformer/pretrained/EqT_model.h5\"\r\n        new_model = load_model(args['input_model'], custom_objects={'SeqSelfAttention': SeqSelfAttention,\r\n                                                             'FeedForward': FeedForward,\r\n                                                             'LayerNormalization': LayerNormalization,\r\n                                                             'f1': f1\r\n                                                             })\r\n        #Hao\r\n        #frozon layers except the output channels\r\n        if args['retrain'] == 1:\r\n            for layer in new_model.layers[:-6]:\r\n                layer.trainable = False\r\n\r\n        #revise output channels\r\n        tl_model = _build_output(new_model, args['phase_types'])\r\n        #compile\r\n        tl_model.compile(loss=args['loss_types'],\r\n                         loss_weights=args['loss_weights'],\r\n                         optimizer=Adam(lr=0.001),\r\n                         metrics=[f1])\r\n        tl_model.summary()\r\n        print('Loading is complete!', flush=True)\r\n        return tl_model\r\n\r\n\r\n\r\n\r\n\r\ndef _split(args, save_dir):\r\n\r\n    \"\"\"\r\n\r\n    Split the list of input data into training, validation, and test set.\r\n\r\n    Parameters\r\n    ----------\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    save_dir: str\r\n       Path to the output directory.\r\n\r\n    Returns\r\n    -------\r\n    training: str\r\n        List of trace names for the training set.\r\n    validation : str\r\n        List of trace names for the validation set.\r\n\r\n    \"\"\"\r\n\r\n    df = pd.read_csv(args['input_csv'])\r\n    ev_list = df.trace_name.tolist()\r\n    np.random.shuffle(ev_list)\r\n    training = ev_list[:int(args['train_valid_test_split'][0]*len(ev_list))]\r\n    validation =  ev_list[int(args['train_valid_test_split'][0]*len(ev_list)):\r\n                            int(args['train_valid_test_split'][0]*len(ev_list) + args['train_valid_test_split'][1]*len(ev_list))]\r\n    test =  ev_list[ int(args['train_valid_test_split'][0]*len(ev_list) + args['train_valid_test_split'][1]*len(ev_list)):]\r\n    np.save(save_dir+'/test', test)\r\n    return training, validation\r\n\r\n\r\n\r\ndef _make_callback(args, save_models):\r\n\r\n    \"\"\"\r\n\r\n    Generate the callback.\r\n\r\n    Parameters\r\n    ----------\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    save_models: str\r\n       Path to the output directory for the models.\r\n\r\n    Returns\r\n    -------\r\n    callbacks: obj\r\n        List of callback objects.\r\n\r\n\r\n    \"\"\"\r\n\r\n    m_name=str(args['output_name'])+'_{epoch:03d}.h5'\r\n    filepath=os.path.join(save_models, m_name)\r\n    early_stopping_monitor=EarlyStopping(monitor=args['monitor'],\r\n                                           patience=args['patience'])\r\n    checkpoint=ModelCheckpoint(filepath=filepath,\r\n                                 monitor=args['monitor'],\r\n                                 mode='auto',\r\n                                 verbose=1,\r\n                                 save_best_only=True)\r\n    lr_scheduler=LearningRateScheduler(_lr_schedule)\r\n\r\n    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\r\n                                   cooldown=0,\r\n                                   patience=args['patience']-2,\r\n                                   min_lr=0.5e-6)\r\n\r\n    callbacks = [checkpoint, lr_reducer, lr_scheduler, early_stopping_monitor]\r\n    return callbacks\r\n\r\n\r\n\r\n\r\ndef _pre_loading(args, training, validation):\r\n\r\n    \"\"\"\r\n\r\n    Load data into memory.\r\n\r\n    Parameters\r\n    ----------\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    training: str\r\n        List of trace names for the training set.\r\n\r\n    validation: str\r\n        List of trace names for the validation set.\r\n\r\n    Returns\r\n    -------\r\n    training_generator: obj\r\n        Keras generator for the training set.\r\n\r\n    validation_generator: obj\r\n        Keras generator for the validation set.\r\n\r\n\r\n    \"\"\"\r\n\r\n    training_set={}\r\n    fl = h5py.File(args['input_hdf5'], 'r')\r\n\r\n    print('Loading the training data into the memory ...')\r\n    pbar = tqdm(total=len(training))\r\n    for ID in training:\r\n        pbar.update()\r\n        if ID.split('_')[-1] != 'NO':\r\n            dataset = fl.get('earthquake/local/'+str(ID))\r\n        elif ID.split('_')[-1] == 'NO':\r\n            dataset = fl.get('non_earthquake/noise/'+str(ID))\r\n        training_set.update( {str(ID) : dataset})\r\n\r\n    print('Loading the validation data into the memory ...', flush=True)\r\n    validation_set={}\r\n    pbar = tqdm(total=len(validation))\r\n    for ID in validation:\r\n        pbar.update()\r\n        if ID.split('_')[-1] != 'NO':\r\n            dataset = fl.get('earthquake/local/'+str(ID))\r\n        elif ID.split('_')[-1] == 'NO':\r\n            dataset = fl.get('non_earthquake/noise/'+str(ID))\r\n        validation_set.update( {str(ID) : dataset})\r\n\r\n    params_training = {'dim':args['input_dimention'][0],\r\n                       'batch_size': args['batch_size'],\r\n                       'n_channels': args['input_dimention'][-1],\r\n                       'shuffle': args['shuffle'],\r\n                       'norm_mode': args['normalization_mode'],\r\n                       'label_type': args['label_type'],\r\n                       'augmentation': args['augmentation'],\r\n                       'add_event_r': args['add_event_r'],\r\n                       'add_gap_r': args['add_gap_r'],\r\n                       'shift_event_r': args['shift_event_r'],\r\n                       'add_noise_r': args['add_noise_r'],\r\n                       'drop_channe_r': args['drop_channel_r'],\r\n                       'scale_amplitude_r': args['scale_amplitude_r'],\r\n                       'pre_emphasis': args['pre_emphasis']}\r\n\r\n    params_validation = {'dim': args['input_dimention'][0],\r\n                         'batch_size': args['batch_size'],\r\n                         'n_channels': args['input_dimention'][-1],\r\n                         'shuffle': False,\r\n                         'norm_mode': args['normalization_mode'],\r\n                         'augmentation': False}\r\n\r\n    training_generator = PreLoadGenerator(training, training_set, **params_training)\r\n    validation_generator = PreLoadGenerator(validation, validation_set, **params_validation)\r\n\r\n    return training_generator, validation_generator\r\n\r\n\r\n\r\n\r\ndef _document_training(history, model, start_training, end_training, save_dir, save_models, training_size, validation_size, args):\r\n\r\n    \"\"\"\r\n\r\n    Write down the training results.\r\n\r\n    Parameters\r\n    ----------\r\n    history: dic\r\n        Training history.\r\n\r\n    model:\r\n        Trained model.\r\n\r\n    start_training: datetime\r\n        Training start time.\r\n\r\n    end_training: datetime\r\n        Training end time.\r\n\r\n    save_dir: str\r\n        Path to the output directory.\r\n\r\n    save_models: str\r\n        Path to the folder for saveing the models.\r\n\r\n    training_size: int\r\n        Number of training samples.\r\n\r\n    validation_size: int\r\n        Number of validation samples.\r\n\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    Returns\r\n    --------\r\n    ./output_name/history.npy: Training history.\r\n\r\n    ./output_name/X_report.txt: A summary of parameters used for the prediction and perfomance.\r\n\r\n    ./output_name/X_learning_curve_f1.png: The learning curve of Fi-scores.\r\n\r\n    ./output_name/X_learning_curve_loss.png: The learning curve of loss.\r\n\r\n\r\n    \"\"\"\r\n\r\n    np.save(save_dir+'/history',history)\r\n    model.save(save_dir+'/final_model.h5')\r\n    model.to_json()\r\n    model.save_weights(save_dir+'/model_weights.h5')\r\n\r\n    fig = plt.figure()\r\n    ax = fig.add_subplot(111)\r\n    ax.plot(history.history['loss'])\r\n    # adaptive plot\r\n    legend_list = []\r\n    legend_list.append('loss')\r\n    for picker_name in args[\"phase_types\"]:\r\n        if 'd' in picker_name or 'D' in picker_name:\r\n            # detector loss\r\n            ax.plot(history.history['detector_loss'])\r\n            legend_list.append('detector_loss')\r\n        else:\r\n            output_name = 'picker_'+picker_name + '_loss'\r\n            ax.plot(history.history[output_name])\r\n            legend_list.append(output_name)\r\n# =============================================================================\r\n#     ax.plot(history.history['detector_loss'])\r\n#     ax.plot(history.history['picker_P_loss'])\r\n#     ax.plot(history.history['picker_S_loss'])\r\n# =============================================================================\r\n    try:\r\n        ax.plot(history.history['val_loss'], '--')\r\n        # adaptive plot\r\n\r\n        for picker_name in args[\"phase_types\"]:\r\n            if 'd' in picker_name or 'D' in picker_name:\r\n                # detector loss\r\n                ax.plot(history.history['val_detector_loss'], '--')\r\n                legend_list.append('val_detector_loss')\r\n            else:\r\n                output_name = 'val_picker_' + picker_name + '_loss'\r\n                ax.plot(history.history[output_name], '--')\r\n                legend_list.append(output_name)\r\n        ax.legend(legend_list, loc='upper right')\r\n# =============================================================================\r\n#         ax.plot(history.history['val_detector_loss'], '--')\r\n#         ax.plot(history.history['val_picker_P_loss'], '--')\r\n#         ax.plot(history.history['val_picker_S_loss'], '--')\r\n#         ax.legend(['loss', 'detector_loss', 'picker_P_loss', 'picker_S_loss',\r\n#                'val_loss', 'val_detector_loss', 'val_picker_P_loss', 'val_picker_S_loss'], loc='upper right')\r\n# =============================================================================\r\n    except Exception:\r\n        ax.legend(legend_list, loc='upper right')\r\n\r\n    plt.ylabel('Loss')\r\n    plt.xlabel('Epoch')\r\n    plt.grid(b=True, which='major', color='#666666', linestyle='-')\r\n    fig.savefig(os.path.join(save_dir,str('X_learning_curve_loss.png')))\r\n\r\n    fig = plt.figure()\r\n    ax = fig.add_subplot(111)\r\n    # adaptive figure\r\n    legend_list = []\r\n    for picker_name in args[\"phase_types\"]:\r\n        if 'd' in picker_name or 'D' in picker_name:\r\n            # detector loss\r\n            ax.plot(history.history['detector_f1'])\r\n            legend_list.append('detector_f1')\r\n        else:\r\n            output_name = 'picker_'+ picker_name + '_f1'\r\n            ax.plot(history.history[output_name])\r\n            legend_list.append(output_name)\r\n\r\n# =============================================================================\r\n#     ax.plot(history.history['detector_f1'])\r\n#     ax.plot(history.history['picker_P_f1'])\r\n#     ax.plot(history.history['picker_S_f1'])\r\n# =============================================================================\r\n    try:\r\n        ax.plot(history.history['val_loss'], '--')\r\n        # adaptive plot\r\n\r\n        for picker_name in args[\"phase_types\"]:\r\n            if 'd' in picker_name or 'D' in picker_name:\r\n                # detector loss\r\n                ax.plot(history.history['val_detector_f1'], '--')\r\n                legend_list.append('val_detector_f1')\r\n            else:\r\n                output_name = 'val_picker_' + picker_name + '_f1'\r\n                ax.plot(history.history[output_name], '--')\r\n                legend_list.append(output_name)\r\n        ax.legend(legend_list, loc='upper right')\r\n\r\n# =============================================================================\r\n#         ax.plot(history.history['val_detector_f1'], '--')\r\n#         ax.plot(history.history['val_picker_P_f1'], '--')\r\n#         ax.plot(history.history['val_picker_S_f1'], '--')\r\n#         ax.legend(['detector_f1', 'picker_P_f1', 'picker_S_f1', 'val_detector_f1', 'val_picker_P_f1', 'val_picker_S_f1'], loc='lower right')\r\n# =============================================================================\r\n    except Exception:\r\n        ax.legend(legend_list, loc='upper right')\r\n        #ax.legend(['detector_f1', 'picker_P_f1', 'picker_S_f1'], loc='lower right')\r\n    plt.ylabel('F1')\r\n    plt.xlabel('Epoch')\r\n    plt.grid(b=True, which='major', color='#666666', linestyle='-')\r\n    fig.savefig(os.path.join(save_dir,str('X_learning_curve_f1.png')))\r\n\r\n    delta = end_training - start_training\r\n    hour = int(delta / 3600)\r\n    delta -= hour * 3600\r\n    minute = int(delta / 60)\r\n    delta -= minute * 60\r\n    seconds = delta\r\n\r\n    trainable_count = int(np.sum([K.count_params(p) for p in model.trainable_weights]))\r\n    non_trainable_count = int(np.sum([K.count_params(p) for p in model.non_trainable_weights]))\r\n\r\n    with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file:\r\n        the_file.write('================== Overal Info =============================='+'\\n')\r\n        the_file.write('date of report: '+str(datetime.datetime.now())+'\\n')\r\n        the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')\r\n        the_file.write('input_csv: '+str(args['input_csv'])+'\\n')\r\n        the_file.write('output_name: '+str(args['output_name']+'_outputs')+'\\n')\r\n        the_file.write('================== Model Parameters ========================='+'\\n')\r\n        the_file.write('input_dimention: '+str(args['input_dimention'])+'\\n')\r\n        the_file.write('cnn_blocks: '+str(args['cnn_blocks'])+'\\n')\r\n        the_file.write('lstm_blocks: '+str(args['lstm_blocks'])+'\\n')\r\n        the_file.write('padding_type: '+str(args['padding'])+'\\n')\r\n        the_file.write('activation_type: '+str(args['activation'])+'\\n')\r\n        the_file.write('drop_rate: '+str(args['drop_rate'])+'\\n')\r\n        the_file.write(str('total params: {:,}'.format(trainable_count + non_trainable_count))+'\\n')\r\n        the_file.write(str('trainable params: {:,}'.format(trainable_count))+'\\n')\r\n        the_file.write(str('non-trainable params: {:,}'.format(non_trainable_count))+'\\n')\r\n        the_file.write('================== Training Parameters ======================'+'\\n')\r\n        the_file.write('mode of training: '+str(args['mode'])+'\\n')\r\n        the_file.write('loss_types: '+str(args['loss_types'])+'\\n')\r\n        the_file.write('loss_weights: '+str(args['loss_weights'])+'\\n')\r\n        the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\r\n        the_file.write('epochs: '+str(args['epochs'])+'\\n')\r\n        the_file.write('train_valid_test_split: '+str(args['train_valid_test_split'])+'\\n')\r\n        the_file.write('total number of training: '+str(training_size)+'\\n')\r\n        the_file.write('total number of validation: '+str(validation_size)+'\\n')\r\n        the_file.write('monitor: '+str(args['monitor'])+'\\n')\r\n        the_file.write('patience: '+str(args['patience'])+'\\n')\r\n        the_file.write('multi_gpu: '+str(args['multi_gpu'])+'\\n')\r\n        the_file.write('number_of_gpus: '+str(args['number_of_gpus'])+'\\n')\r\n        the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\r\n        the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')\r\n        the_file.write('use_multiprocessing: '+str(args['use_multiprocessing'])+'\\n')\r\n        the_file.write('================== Training Performance ====================='+'\\n')\r\n        the_file.write('finished the training in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds,2)))\r\n        the_file.write('stoped after epoche: '+str(len(history.history['loss']))+'\\n')\r\n        the_file.write('last loss: '+str(history.history['loss'][-1])+'\\n')\r\n        # adaptive write\r\n        for picker_name in args[\"phase_types\"]:\r\n            if 'd' in picker_name or 'D' in picker_name:\r\n                # detector loss\r\n                the_file.write('last detector_loss: '+str(history.history['detector_loss'][-1])+'\\n')\r\n            else:\r\n                output_name = 'picker_'+picker_name + '_loss'\r\n                the_file.write('last '+picker_name+'_loss: '+str(history.history[output_name][-1])+'\\n')\r\n        for picker_name in args[\"phase_types\"]:\r\n            if 'd' in picker_name or 'D' in picker_name:\r\n                # detector loss\r\n                the_file.write('last detector_f1: '+str(history.history['detector_f1'][-1])+'\\n')\r\n            else:\r\n                output_name = 'picker_'+picker_name + '_f1'\r\n                the_file.write('last '+picker_name+'_f1: '+str(history.history[output_name][-1])+'\\n')\r\n# =============================================================================\r\n#         the_file.write('last detector_loss: '+str(history.history['detector_loss'][-1])+'\\n')\r\n#         the_file.write('last picker_P_loss: '+str(history.history['picker_P_loss'][-1])+'\\n')\r\n#         the_file.write('last picker_S_loss: '+str(history.history['picker_S_loss'][-1])+'\\n')\r\n#         the_file.write('last detector_f1: '+str(history.history['detector_f1'][-1])+'\\n')\r\n#         the_file.write('last picker_P_f1: '+str(history.history['picker_P_f1'][-1])+'\\n')\r\n#         the_file.write('last picker_S_f1: '+str(history.history['picker_S_f1'][-1])+'\\n')\r\n# =============================================================================\r\n        the_file.write('================== Other Parameters ========================='+'\\n')\r\n        the_file.write('label_type: '+str(args['label_type'])+'\\n')\r\n        the_file.write('augmentation: '+str(args['augmentation'])+'\\n')\r\n        the_file.write('shuffle: '+str(args['shuffle'])+'\\n')\r\n        the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\r\n        the_file.write('add_event_r: '+str(args['add_event_r'])+'\\n')\r\n        the_file.write('add_noise_r: '+str(args['add_noise_r'])+'\\n')\r\n        the_file.write('shift_event_r: '+str(args['shift_event_r'])+'\\n')\r\n        the_file.write('drop_channel_r: '+str(args['drop_channel_r'])+'\\n')\r\n        the_file.write('scale_amplitude_r: '+str(args['scale_amplitude_r'])+'\\n')\r\n        the_file.write('pre_emphasis: '+str(args['pre_emphasis'])+'\\n')\r\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/plot.py", "content": "\"\"\"\nCreated on Wed Jul 24 19:16:51 2019\n\n@author: mostafamousavi\nlast update: 06/05/2020\n\n\"\"\"\nimport pandas as pd\nfrom os import listdir\nimport platform\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport matplotlib\nimport matplotlib.cm\nimport pickle\nimport datetime as dt\nfrom matplotlib.dates import  HOURLY, DateFormatter, rrulewrapper, RRuleLocator\nimport matplotlib.font_manager as font_manager\nfrom obspy import read\nfrom obspy import UTCDateTime\n\n\n\n\ndef plot_helicorder(input_mseed, input_csv=None, save_plot=False):\n    \n    \"\"\"\n    \n    Plots an stream object overlied by detection times. \n\n    Parameters\n    ----------       \n    input_mseed: str\n        Path to the miniseed files for day long data.  \n        \n    input_csv: str, default=None\n        Path to the \"X_prediction_results.csv\" file associated with the miniseed file.                   \n        \n    save_plot: str, default=False\n        If set to True the generated plot will be saved with the name of miniseed file. \n                          \n\n    Returns\n    ----------       \n    miniseed_name.png: fig\n       \n    \"\"\"  \n          \n    event_list=[]\n    def _date_convertor(r):            \n        r.replace(' ', 'T')+'Z'\n        new_t = UTCDateTime(r)\n        return new_t\n         \n    st = read(input_mseed)\n    st.filter(\"highpass\", freq=0.1, corners=2)\n\n    if input_csv:\n        detlist = pd.read_csv(input_csv)       \n        detlist['event_start_time'] = detlist['event_start_time'].apply(lambda row : _date_convertor(row)) \n        detlist = detlist[(detlist.event_start_time > st[0].stats['starttime']) & (detlist.event_start_time < st[0].stats['endtime'])]\n        ev_list = detlist['event_end_time'].to_list()\n        for ev in ev_list:\n            event_list.append({\"time\": UTCDateTime(ev)})\n    if save_plot:    \n        if platform.system() == 'Windows':\n            st[0].plot(type='dayplot', color=['k'],interval=60, events=event_list, outfile=input_mseed.split(\"\\\\\")[-1].split('.mseed')[0]+'.png')\n            print('saved the plot as '+input_mseed.split(\"\\\\\")[-1].split('.mseed')[0]+'.png')\n        else:    \n            st[0].plot(type='dayplot', color=['k'],interval=60, events=event_list, outfile=input_mseed.split(\"/\")[-1].split('.mseed')[0]+'.png')\n            print('saved the plot as '+input_mseed.split(\"/\")[-1].split('.mseed')[0]+'.png')\n    else:    \n        st[0].plot(type='dayplot', color=['k'],interval=60, events=event_list)\n\n\n\n\n\ndef plot_detections(input_dir, input_json, plot_type=None, time_window=60, marker_size=6):\n    \n    \n     \"\"\"\n    \n     Uses fdsn to find availave stations in a specific geographical location and time period. \n\n    Parameters\n    ----------       \n    input_dir: str\n        Path to the directory containing detection results.\n         \n    input_json: str\n        Json file containing station information.\n         \n    plot_type: str, default=None\n        Type of plot, 'station_map', 'hist'. \n           \n    time_window: int, default=60 \n        Time window for histogram plot in minutes. \n        \n\n    Returns\n    ----------   \n    station_output.png: fig\n        \n    station_map.png: fig\n       \n     \"\"\"  \n\n     if platform.system() == 'Windows':\n         station_list = [ev for ev in listdir(input_dir) if ev.split(\"\\\\\")[-1] != '.DS_Store'];\n     else:\n         station_list = [ev for ev in listdir(input_dir) if ev.split(\"/\")[-1] != '.DS_Store'];\n\n     station_list = sorted(set(station_list))\n    \n    \n     json_file = open(input_json)\n     stations_ = json.load(json_file)\n    \n     detection_list = {}\n     for st in station_list: \n         if platform.system() == 'Windows':\n             df_mulistaition = pd.read_csv(input_dir+\"\\\\\"+st+'\"\\\\\"X_prediction_results.csv') \n         else:\n             df_mulistaition = pd.read_csv(input_dir+\"/\"+st+\"/X_prediction_results.csv\") \n             \n         detection_list[st.split(\"_\")[0]]=[stations_[st.split(\"_\")[0]]['coords'][1],stations_[st.split(\"_\")[0]]['coords'][0],len(df_mulistaition)]\n    \n     ln2=[]; lt2=[]; detections=[]\n     for stations, L in detection_list.items():\n         ln2.append(L[0])\n         lt2.append(L[1])\n         detections.append(L[2])\n        \n     if plot_type == 'station_map':\n         \n         plt.figure(constrained_layout=True) \n         plt.scatter(ln2, lt2, s=marker_size, marker=\"^\", c=detections)\n         plt.xticks(rotation=45)\n         c = plt.colorbar(orientation='vertical')\n         c.set_label(\"Number of Detections\")\n        \n         for stations, L in detection_list.items(): \n             plt.text(L[0], L[1], stations, fontsize=marker_size//4)\n            \n         plt.title(str(len(detection_list))+' Stations')\n         plt.savefig('station_map.png', dpi=300)  \n         plt.tight_layout()   \n         plt.show()\n         \n     elif plot_type == 'hist':\n         for st in station_list: \n             if platform.system() == 'Windows':\n                 df = pd.read_csv(input_dir+\"\\\\\"+st+'\"\\\\\"X_prediction_results.csv')     \n             else:    \n                 df = pd.read_csv(input_dir+\"/\"+st+\"/X_prediction_results.csv\")\n                 \n             df['event_start_time'] = df['event_start_time'].apply(lambda row : _date_convertor(row)) \n\n             plt.figure(constrained_layout=True)\n             df.set_index('event_start_time', drop=False, inplace=True)\n             df = df['event_start_time'].groupby(pd.Grouper(freq=str(time_window)+'Min')).count()\n             ax = df.plot(kind='bar', color='slateblue')\n             ticklabels = df.index.strftime('%D:%H')\n             ax.xaxis.set_major_formatter(matplotlib.ticker.FixedFormatter(ticklabels))\n             ax.set_ylabel('Event Counts')\n             plt.title(st)\n             plt.savefig(st+'.png', dpi=300)\n             plt.show()\n     else:\n         print('Please define the plot type!')\n          \n         \n         \n\n\ndef plot_data_chart(time_tracks, time_interval):\n    \n    \"\"\"\n    \n    Uses fdsn to find availave stations in a specific geographical location and time period. \n\n    Parameters\n    ----------      \n    time_tracks: pkl\n        Pickel file outputed by preprocessor or mseed_predictor.     \n    \n    time_interval: int \n        Time interval in hours for tick spaces in xaxes. \n               \n\n    Returns\n    ----------      \n    data_chart.png: fig\n       \n    \"\"\"      \n     \n     \n    with open(time_tracks, 'rb') as f:\n        time_tracks = pickle.load(f)\n        \n    def create_date(date_time):\n     \n        date = dt.datetime(int(str(date_time).split('T')[0].split('-')[0]),\n                           int(str(date_time).split('T')[0].split('-')[1]), \n                           int(str(date_time).split('T')[0].split('-')[2]),\n                           int(str(date_time).split('T')[1].split(':')[0]),\n                           int(str(date_time).split('T')[1].split(':')[1])\n                           )   \n        mdate = matplotlib.dates.date2num(date)\n         \n        return mdate\n\n    ylabels, customDates, task_dates = [], [], {}\n    for st, tracks in time_tracks.items():\n        ylabels.append(st)\n        time_slots, comp_types = [], []\n        for times in tracks[0]:\n            time_slots.append((create_date(times[0]),create_date(times[1])-create_date(times[0])))\n        for comps in tracks[1]:\n            comp_types.append(comps)  \n        task_dates[st]=[time_slots, comp_types]\n        customDates.append(time_slots)\n    \n    fig, ax = plt.subplots(figsize=(8,10))\n    ax.patch.set_facecolor('lavender')\n    \n    # use a colormap\n    cmap = plt.cm.Blues\n    barHeight = len(ylabels)/3\n    ticklist = []; \n    def drawLoadDuration(period, starty, compt, c1, c2, c3):\n        if len(compt) >= 1:\n            if compt[0] == 3:\n                if c3 == 0:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='crimson', lw=0, zorder=2, alpha = 0.9, label='3-component'); c3 += 1\n                else:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='crimson', lw=0, zorder=2, alpha = 0.9)\n            \n            if compt[0] == 1:\n                if c1 == 0:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='mediumslateblue', lw=0, zorder=2, alpha = 0.9, label='1-component'); c1 += 1\n                else:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='mediumslateblue', lw=0, zorder=2, alpha = 0.9)                          \n                \n            if compt[0] == 2 : \n                if c2 == 0:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='darkorange', lw=0, zorder=2, alpha = 0.9, label='2-component'); c2 += 1\n                else:\n                    ax.broken_barh((period), (starty, barHeight), facecolors='darkorange', lw=0, zorder=2, alpha = 0.9)\n    \n        ticklist.append(starty+barHeight/2.0)\n        return c1, c2, c3\n    \n    \n    h0 = 3; c1 = 0; c2=0; c3=0\n    for st in ylabels:\n        c1, c2, c3= drawLoadDuration(task_dates[st][0], h0, task_dates[st][1], c1, c2, c3)\n        h0 += int(len(ylabels)*2)\n        \n    legend_properties = {'weight':'bold'}  \n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    ax.set_ylim(0, h0)\n    ax.set_xlabel('Time', fontsize=12)\n    ax.set_ylabel('Stations', fontsize=12)\n    ax.set_yticks(ticklist)\n    ax.tick_params('x', colors=cmap(1.))\n    ax.set_yticklabels(ylabels, fontsize=10)\n    ax.grid(True)\n    \n    ax.xaxis_date() #Tell matplotlib that these are dates...\n    rule = rrulewrapper(HOURLY, interval=time_interval)\n    loc = RRuleLocator(rule)\n    formatter = DateFormatter('%Y-%m-%d %H')\n     \n    ax.xaxis.set_major_locator(loc)\n    ax.xaxis.set_major_formatter(formatter)\n    labelsx = ax.get_xticklabels()\n    plt.setp(labelsx, rotation=30, fontsize=10)\n     \n    # Format the legend\n    font = font_manager.FontProperties(size='small')\n    ax.legend(loc=1, prop=font)\n    plt.locator_params(axis='x', nbins=10)\n    fig.autofmt_xdate()\n    fig.tight_layout()\n    plt.savefig('data_chart.png', dpi=300)\n    plt.show()\n\n\n\ndef _date_convertor(r):  \n          \n    mls = r.split('.')\n    if len(mls) == 1:\n        new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S')\n    else:\n        new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S.%f')\n    return new_t"}
{"type": "source_file", "path": "setup.py", "content": "# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2022 Hao Mai & Pascal Audet\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nNote that Blockly Earthquake Transformer (BET) is driven by Earthquake\nTransformer V1.59 created by @author: mostafamousavi\nRef Repo: https://github.com/smousavi05/EQTransformer\n\"\"\"\n\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"BlocklyEQTransformer\",\n    author=\"Hao Mai & Pascal Audet\",\n    version=\"0.0.1\",\n    author_email=\"hmai090@uottawa.ca\",\n    description=\"A Python Toolbox for Customizing Seismic Phase Pickers\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/maihao14/BlocklyEQTransformer\",\n    license=\"MIT\",\n    packages=find_packages(),\n    keywords='Seismology, Earthquakes Detection, Phase Picking, Deep Learning',\n    install_requires=[\n                'matplotlib',\n                'h5py==2.10.0',\n                'tqdm',\n                'obspy',\n                'pandas',\n                'keras==2.3.1',\n                'tensorflow==2.0.0',\n                'pytest',\n                'jupyter==1.0.0',\n                'voila==0.3.5',\n                'ipywidgets~=7.6.5',\n                'protobuf<=3.20.1'\n\n    ],\n\n    python_requires='>=3.7',\n)\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/utils/train_external_data.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 16 15:49:08 2022\n\n@author: hao\n\"\"\"\n#%% Train Example\n\nhdf5_file_path = './ModelsAndSampleData/100samples.hdf5'\ncsv_file_path = './ModelsAndSampleData/100samples.csv'\nfrom EQTransformer.core.trainer import trainer\ntrainer(input_hdf5=hdf5_file_path,\n        input_csv=csv_file_path,\n        output_name='ps_samples_trainer',                \n        cnn_blocks=2,\n        lstm_blocks=1,\n        padding='same',\n        activation='relu',\n        drop_rate=0.2,\n        label_type='gaussian',\n        add_event_r=0.6,\n        add_gap_r=0.2,\n        shift_event_r=0.9,\n        add_noise_r=0.5, \n        mode='generator',\n        train_valid_test_split=[0.80, 0.10, 0.10],\n        loss_types=['binary_crossentropy', 'binary_crossentropy'], # if only 2 output\n        batch_size=20,\n        epochs=10, \n        patience=20,\n        gpuid=None,\n        gpu_limit=None,\n        phase_types=['P','S'])\n#%% Train to Test\nfrom EQTransformer.core.tester import tester\ntester(input_hdf5='./ModelsAndSampleData/100samples.hdf5',\n       input_testset='./ps_samples_trainer_outputs/test.npy',\n       input_model='./ps_samples_trainer_outputs/final_model.h5',\n       output_name='tl_noaug_samples_tester',\n       detection_threshold=0.20,                \n       P_threshold=0.1,\n       S_threshold=0.1, \n       number_of_plots=3,\n       estimate_uncertainty=True, \n       number_of_sampling=2,\n       input_dimention=(6000, 3),\n       normalization_mode='std',\n       mode='generator',\n       loss_weights=[ 0.40, 0.55], #0.05,\n       loss_types=['binary_crossentropy', 'binary_crossentropy'],# if only 2 output\n       batch_size=50,\n       gpuid=None,\n       gpu_limit=None)\n#%% test\nfrom EQTransformer.core.tester import tester\ntester(input_hdf5='./ModelsAndSampleData/100samples.hdf5',\n       input_testset='./samples_trainer_outputs/test.npy',\n       input_model='./samples_trainer_outputs/final_model.h5',\n       output_name='samples100_tester',\n       detection_threshold=0.20,                \n       P_threshold=0.1,\n       S_threshold=0.1, \n       number_of_plots=3,\n       estimate_uncertainty=True, \n       number_of_sampling=2,\n       input_dimention=(6000, 3),\n       normalization_mode='std',\n       mode='generator',\n       batch_size=10,\n       gpuid=None,\n       gpu_limit=None)\n\n\n#%% Detection\nfrom EQTransformer.core.predictor import predictor\npredictor(input_dir='./ModelsAndSampleData',   \n         input_model='./samples_trainer_outputs/final_model.h5',\n         output_dir='tl_detections_p',\n         estimate_uncertainty=False, \n         output_probabilities=False,\n         number_of_sampling=5,\n         loss_weights=[0.02, 0.40, 0.58],          \n         detection_threshold=0.3,                \n         P_threshold=0.1,\n         S_threshold=0.1, \n         number_of_plots=10,\n         plot_mode='time',\n         batch_size=500,\n         number_of_cpus=4,\n         keepPS=False,\n         spLimit=60) \n\n\n"}
{"type": "source_file", "path": "BlocklyEQTransformer/core/predictor.py", "content": "# -*- coding: utf-8 -*-\r\n# MIT License\r\n#\r\n# Copyright (c) 2022 Hao Mai & Pascal Audet\r\n#\r\n# Note that Blockly Earthquake Transformer (BET) is driven by Earthquake Transformer\r\n# V1.59 created by @author: mostafamousavi\r\n# Ref Repo: https://github.com/smousavi05/EQTransformer\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the \"Software\"), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nfrom keras import backend as K\r\nfrom keras.models import load_model\r\nfrom keras.optimizers import Adam\r\n# from tensorflow.keras import backend as K\r\n# from tensorflow.keras.models import load_model\r\n# from tensorflow.keras.optimizers import Adam\r\nimport tensorflow as tf\r\nimport matplotlib\r\nmatplotlib.use('agg')\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport math\r\nimport csv\r\nimport h5py\r\nimport time\r\nfrom os import listdir\r\nimport os\r\nimport platform\r\nimport shutil\r\nfrom .EqT_utils import DataGeneratorPrediction, picker, generate_arrays_from_file\r\nfrom .EqT_utils import f1, SeqSelfAttention, FeedForward, LayerNormalization\r\nfrom tqdm import tqdm\r\nfrom datetime import datetime, timedelta\r\nfrom obspy.core import UTCDateTime\r\nimport multiprocessing\r\nimport contextlib\r\nimport sys\r\nimport warnings\r\nfrom scipy import signal\r\nfrom matplotlib.lines import Line2D\r\nwarnings.filterwarnings(\"ignore\")\r\nfrom tensorflow.python.util import deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n\r\ntry:\r\n    f = open('setup.py')\r\n    for li, l in enumerate(f):\r\n        if li == 8:\r\n            EQT_VERSION = l.split('\"')[1]\r\nexcept Exception:\r\n    EQT_VERSION = \"0.1.59\"\r\n\r\ndef predictor(input_dir=None,\r\n              input_model=None,\r\n              output_dir=None,\r\n              output_probabilities=False,\r\n              detection_threshold=0.3,\r\n              P_threshold=0.1,\r\n              S_threshold=0.1,\r\n              number_of_plots=20,\r\n              plot_mode='time',\r\n              estimate_uncertainty=False,\r\n              number_of_sampling=5,\r\n              loss_weights=[0.03, 0.40, 0.58],\r\n              loss_types=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'],\r\n              phase_types = ['d','P', 'S'], #default phase types\r\n              input_dimention=(6000, 3),\r\n              normalization_mode='std',\r\n              batch_size=500,\r\n              gpuid=None,\r\n              gpu_limit=None,\r\n              number_of_cpus=5,\r\n              use_multiprocessing=True,\r\n              keepPS=False,\r\n              spLimit=60):\r\n\r\n\r\n    \"\"\"\r\n\r\n    Applies a trained model to a windowed waveform to perform both detection and picking at the same time.\r\n\r\n\r\n    Parameters\r\n    ----------\r\n    input_dir: str, default=None\r\n        Directory name containing hdf5 and csv files-preprocessed data.\r\n\r\n    input_model: str, default=None\r\n        Path to a trained model.\r\n\r\n    output_dir: str, default=None\r\n        Output directory that will be generated.\r\n\r\n    output_probabilities: bool, default=False\r\n        If True, it will output probabilities and estimated uncertainties for each trace into an HDF file.\r\n\r\n    detection_threshold : float, default=0.3\r\n        A value in which the detection probabilities above it will be considered as an event.\r\n\r\n    P_threshold: float, default=0.1\r\n        A value which the P probabilities above it will be considered as P arrival.\r\n\r\n    S_threshold: float, default=0.1\r\n        A value which the S probabilities above it will be considered as S arrival.\r\n\r\n    number_of_plots: float, default=10\r\n        The number of plots for detected events outputed for each station data.\r\n\r\n    plot_mode: str, default='time'\r\n        The type of plots: 'time': only time series or 'time_frequency', time and spectrograms.\r\n\r\n    estimate_uncertainty: bool, default=False\r\n        If True uncertainties in the output probabilities will be estimated.\r\n\r\n    number_of_sampling: int, default=5\r\n        Number of sampling for the uncertainty estimation.\r\n\r\n    loss_weights: list, default=[0.03, 0.40, 0.58]\r\n        Loss weights for detection, P picking, and S picking respectively.\r\n\r\n    loss_types: list, default=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy']\r\n        Loss types for detection, P picking, and S picking respectively.\r\n\r\n    input_dimention: tuple, default=(6000, 3)\r\n        Loss types for detection, P picking, and S picking respectively.\r\n\r\n    normalization_mode: str, default='std'\r\n        Mode of normalization for data preprocessing, 'max', maximum amplitude among three components, 'std', standard deviation.\r\n\r\n    batch_size: int, default=500\r\n        Batch size. This wont affect the speed much but can affect the performance. A value beteen 200 to 1000 is recommanded.\r\n\r\n    gpuid: int, default=None\r\n        Id of GPU used for the prediction. If using CPU set to None.\r\n\r\n    gpu_limit: int, default=None\r\n        Set the maximum percentage of memory usage for the GPU.\r\n\r\n    number_of_cpus: int, default=5\r\n        Number of CPUs used for the parallel preprocessing and feeding of data for prediction.\r\n\r\n    use_multiprocessing: bool, default=True\r\n        If True, multiple CPUs will be used for the preprocessing of data even when GPU is used for the prediction.\r\n\r\n    keepPS: bool, default=False\r\n        If True, only detected events that have both P and S picks will be written otherwise those events with either P or S pick.\r\n\r\n    spLimit: int, default=60\r\n        S - P time in seconds. It will limit the results to those detections with events that have a specific S-P time limit.\r\n\r\n    Returns\r\n    --------\r\n    ./output_dir/STATION_OUTPUT/X_prediction_results.csv: A table containing all the detection, and picking results. Duplicated events are already removed.\r\n\r\n    ./output_dir/STATION_OUTPUT/X_report.txt: A summary of the parameters used for prediction and performance.\r\n\r\n    ./output_dir/STATION_OUTPUT/figures: A folder containing plots detected events and picked arrival times.\r\n\r\n    ./time_tracks.pkl: A file containing the time track of the continous data and its type.\r\n\r\n\r\n    Notes\r\n    --------\r\n    Estimating the uncertainties requires multiple predictions and will increase the computational time.\r\n\r\n\r\n    \"\"\"\r\n\r\n\r\n    args = {\r\n    \"input_dir\": input_dir,\r\n    \"input_hdf5\": None,\r\n    \"input_csv\": None,\r\n    \"input_model\": input_model,\r\n    \"output_dir\": output_dir,\r\n    \"output_probabilities\": output_probabilities,\r\n    \"detection_threshold\": detection_threshold,\r\n    \"P_threshold\": P_threshold,\r\n    \"S_threshold\": S_threshold,\r\n    \"number_of_plots\": number_of_plots,\r\n    \"plot_mode\": plot_mode,\r\n    \"estimate_uncertainty\": estimate_uncertainty,\r\n    \"number_of_sampling\": number_of_sampling,\r\n    \"loss_weights\": loss_weights,\r\n    \"loss_types\": loss_types,\r\n    \"phase_types\": phase_types,  # create a list of phases to be predicted\r\n    \"input_dimention\": input_dimention,\r\n    \"normalization_mode\": normalization_mode,\r\n    \"batch_size\": batch_size,\r\n    \"gpuid\": gpuid,\r\n    \"gpu_limit\": gpu_limit,\r\n    \"number_of_cpus\": number_of_cpus,\r\n    \"use_multiprocessing\": use_multiprocessing,\r\n    \"keepPS\": keepPS,\r\n    \"spLimit\": spLimit\r\n    }\r\n\r\n    availble_cpus = multiprocessing.cpu_count()\r\n    if args['number_of_cpus'] > availble_cpus:\r\n        args['number_of_cpus'] = availble_cpus\r\n\r\n    if args['gpuid']:\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(args['gpuid'])\r\n        tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n        config = tf.ConfigProto()\r\n        config.gpu_options.allow_growth = True\r\n        config.gpu_options.per_process_gpu_memory_fraction = float(args['gpu_limit'])\r\n        K.tensorflow_backend.set_session(tf.Session(config=config))\r\n\r\n    class DummyFile(object):\r\n        file = None\r\n        def __init__(self, file):\r\n            self.file = file\r\n\r\n        def write(self, x):\r\n            # Avoid print() second call (useless \\n)\r\n            if len(x.rstrip()) > 0:\r\n                tqdm.write(x, file=self.file)\r\n\r\n    @contextlib.contextmanager\r\n    def nostdout():\r\n        save_stdout = sys.stdout\r\n        sys.stdout = DummyFile(sys.stdout)\r\n        yield\r\n        sys.stdout = save_stdout\r\n\r\n\r\n    print('============================================================================')\r\n    print('Running EqTransformer ', str(EQT_VERSION))\r\n\r\n    print(' *** Loading the model ...', flush=True)\r\n    model = load_model(args['input_model'],\r\n                       custom_objects={'SeqSelfAttention': SeqSelfAttention,\r\n                                       'FeedForward': FeedForward,\r\n                                       'LayerNormalization': LayerNormalization,\r\n                                       'f1': f1\r\n                                        })\r\n    model.compile(loss = args['loss_types'],\r\n                  loss_weights =  args['loss_weights'],\r\n                  optimizer = Adam(lr = 0.001),\r\n                  metrics = [f1])\r\n    print('*** Loading is complete!', flush=True)\r\n\r\n\r\n    out_dir = os.path.join(os.getcwd(), str(args['output_dir']))\r\n    if os.path.isdir(out_dir):\r\n        print('============================================================================')\r\n        print(f' *** {out_dir} already exists!')\r\n        inp = input(\" --> Type (Yes or y) to create a new empty directory! otherwise it will overwrite!   \")\r\n        if inp.lower() == \"yes\" or inp.lower() == \"y\":\r\n            shutil.rmtree(out_dir)\r\n            os.makedirs(out_dir)\r\n    if platform.system() == 'Windows':\r\n        station_list = [ev.split(\".\")[0] for ev in listdir(args[\"input_dir\"]) if ev.split(\"\\\\\")[-1] != \".DS_Store\"];\r\n    else:\r\n        station_list = [ev.split(\".\")[0] for ev in listdir(args['input_dir']) if ev.split(\"/\")[-1] != \".DS_Store\"];\r\n    station_list = sorted(set(station_list))\r\n\r\n    print(f\"######### There are files for {len(station_list)} stations in {args['input_dir']} directory. #########\", flush=True)\r\n    for ct, st in enumerate(station_list):\r\n        if platform.system() == 'Windows':\r\n            args[\"input_hdf5\"] = args[\"input_dir\"]+\"\\\\\"+st+\".hdf5\"\r\n            args[\"input_csv\"] = args[\"input_dir\"]+\"\\\\\"+st+\".csv\"\r\n        else:\r\n            args[\"input_hdf5\"] = args[\"input_dir\"]+\"/\"+st+\".hdf5\"\r\n            args[\"input_csv\"] = args[\"input_dir\"]+\"/\"+st+\".csv\"\r\n\r\n        save_dir = os.path.join(out_dir, str(st)+'_outputs')\r\n        out_probs = os.path.join(save_dir, 'prediction_probabilities.hdf5')\r\n        save_figs = os.path.join(save_dir, 'figures')\r\n        if os.path.isdir(save_dir):\r\n            shutil.rmtree(save_dir)\r\n        os.makedirs(save_dir)\r\n        if args['number_of_plots']:\r\n            os.makedirs(save_figs)\r\n        try:\r\n            os.remove(out_probs)\r\n        except Exception:\r\n             pass\r\n\r\n        if args['output_probabilities']:\r\n            # create a hdf5 file to save the probabilities and uncertainties\r\n            HDF_PROB = h5py.File(out_probs, 'a')\r\n            HDF_PROB.create_group(\"probabilities\")\r\n            HDF_PROB.create_group(\"uncertainties\")\r\n        else:\r\n            HDF_PROB = None\r\n\r\n        csvPr_gen = open(os.path.join(save_dir,'X_prediction_results.csv'), 'w')\r\n        predict_writer = csv.writer(csvPr_gen, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\r\n        predict_writer.writerow(['file_name',\r\n                                 'network',\r\n                                 'station',\r\n                                 'instrument_type',\r\n                                 'station_lat',\r\n                                 'station_lon',\r\n                                 'station_elv',\r\n                                 'event_start_time',\r\n                                 'event_end_time',\r\n                                 'detection_probability',\r\n                                 'detection_uncertainty',\r\n                                 'p_arrival_time',\r\n                                 'p_probability',\r\n                                 'p_uncertainty',\r\n                                 'p_snr',\r\n                                 's_arrival_time',\r\n                                 's_probability',\r\n                                 's_uncertainty',\r\n                                 's_snr'\r\n                                     ])\r\n        csvPr_gen.flush()\r\n        print(f'========= Started working on {st}, {ct+1} out of {len(station_list)} ...', flush=True)\r\n\r\n        start_Predicting = time.time()\r\n        detection_memory = []\r\n        plt_n = 0\r\n\r\n        df = pd.read_csv(args['input_csv'])\r\n        prediction_list = df.trace_name.tolist()\r\n        fl = h5py.File(args['input_hdf5'], 'r')\r\n        list_generator=generate_arrays_from_file(prediction_list, args['batch_size'])\r\n\r\n        pbar_test = tqdm(total= int(np.ceil(len(prediction_list)/args['batch_size'])), ncols=100, file=sys.stdout)\r\n        for bn in range(int(np.ceil(len(prediction_list) / args['batch_size']))):\r\n            with nostdout():\r\n                pbar_test.update()\r\n\r\n            new_list = next(list_generator)\r\n            # generate probabilities and uncertainties\r\n            prob_dic=_gen_predictor(new_list, args, model,phase_types)\r\n\r\n            pred_set={}\r\n            for ID in new_list:\r\n                dataset = fl.get('data/'+str(ID))\r\n\r\n                pred_set.update( {str(ID) : dataset})\r\n            # generate the plots hdf5 and csv files\r\n            plt_n, detection_memory= _gen_writer(new_list, args, prob_dic, pred_set, HDF_PROB, predict_writer, save_figs, csvPr_gen, plt_n, detection_memory, keepPS, spLimit)\r\n\r\n        end_Predicting = time.time()\r\n        delta = (end_Predicting - start_Predicting)\r\n        hour = int(delta / 3600)\r\n        delta -= hour * 3600\r\n        minute = int(delta / 60)\r\n        delta -= minute * 60\r\n        seconds = delta\r\n\r\n\r\n        dd = pd.read_csv(os.path.join(save_dir,'X_prediction_results.csv'))\r\n        print(f'\\n', flush=True)\r\n        print(' *** Finished the prediction in: {} hours and {} minutes and {} seconds.'.format(hour, minute, round(seconds, 2)), flush=True)\r\n        print(' *** Detected: '+str(len(dd))+' events.', flush=True)\r\n        print(' *** Wrote the results into --> \" ' + str(save_dir)+' \"', flush=True)\r\n\r\n        with open(os.path.join(save_dir,'X_report.txt'), 'a') as the_file:\r\n            the_file.write('================== Overal Info =============================='+'\\n')\r\n            the_file.write('date of report: '+str(datetime.now())+'\\n')\r\n            the_file.write('input_hdf5: '+str(args['input_hdf5'])+'\\n')\r\n            the_file.write('input_csv: '+str(args['input_csv'])+'\\n')\r\n            the_file.write('input_model: '+str(args['input_model'])+'\\n')\r\n            the_file.write('output_dir: '+str(save_dir)+'\\n')\r\n            the_file.write('================== Prediction Parameters ======================='+'\\n')\r\n            the_file.write('finished the prediction in:  {} hours and {} minutes and {} seconds \\n'.format(hour, minute, round(seconds, 2)))\r\n            the_file.write('detected: '+str(len(dd))+' events.'+'\\n')\r\n            the_file.write('writting_probability_outputs: '+str(args['output_probabilities'])+'\\n')\r\n            the_file.write('loss_types: '+str(args['loss_types'])+'\\n')\r\n            the_file.write('loss_weights: '+str(args['loss_weights'])+'\\n')\r\n            the_file.write('batch_size: '+str(args['batch_size'])+'\\n')\r\n            the_file.write('================== Other Parameters ========================='+'\\n')\r\n            the_file.write('normalization_mode: '+str(args['normalization_mode'])+'\\n')\r\n            the_file.write('estimate uncertainty: '+str(args['estimate_uncertainty'])+'\\n')\r\n            the_file.write('number of Monte Carlo sampling: '+str(args['number_of_sampling'])+'\\n')\r\n            the_file.write('detection_threshold: '+str(args['detection_threshold'])+'\\n')\r\n            the_file.write('P_threshold: '+str(args['P_threshold'])+'\\n')\r\n            the_file.write('S_threshold: '+str(args['S_threshold'])+'\\n')\r\n            the_file.write('number_of_plots: '+str(args['number_of_plots'])+'\\n')\r\n            the_file.write('use_multiprocessing: '+str(args['use_multiprocessing'])+'\\n')\r\n            the_file.write('gpuid: '+str(args['gpuid'])+'\\n')\r\n            the_file.write('gpu_limit: '+str(args['gpu_limit'])+'\\n')\r\n            the_file.write('keepPS: '+str(args['keepPS'])+'\\n')\r\n            the_file.write('spLimit: '+str(args['spLimit'])+' seconds\\n')\r\n\r\n\r\n\r\ndef _gen_predictor(new_list, args, model, phase_type):\r\n\r\n\r\n    \"\"\"\r\n\r\n    Performs the predictions for the current batch.\r\n\r\n    Parameters\r\n    ----------\r\n    new_list: list of str\r\n        A list of trace names in the batch.\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    model:\r\n        The compiled model used for the prediction.\r\n\r\n    Returns\r\n    -------\r\n    prob_dic: dic\r\n        A dictionary containing output probabilities and their estimated standard deviations.\r\n\r\n    \"\"\"\r\n\r\n    prob_dic = dict()\r\n    params_prediction = {'file_name': str(args['input_hdf5']),\r\n                         'dim': args['input_dimention'][0],\r\n                         'batch_size': len(new_list),\r\n                         'n_channels': args['input_dimention'][-1],\r\n                         'norm_mode': args['normalization_mode']}\r\n\r\n    prediction_generator = DataGeneratorPrediction(new_list, **params_prediction)\r\n    if args['estimate_uncertainty']:\r\n        if not args['number_of_sampling'] or args['number_of_sampling'] <= 0:\r\n            print('please define the number of Monte Carlo sampling!')\r\n\r\n        pred_DD = []\r\n        pred_PP = []\r\n        pred_SS = []\r\n        for mc in range(args['number_of_sampling']):\r\n            predD, predP, predS = model.predict_generator(generator = prediction_generator,\r\n                                                          use_multiprocessing = args['use_multiprocessing'],\r\n                                                          workers = args['number_of_cpus'])\r\n            pred_DD.append(predD)\r\n            pred_PP.append(predP)\r\n            pred_SS.append(predS)\r\n\r\n        pred_DD = np.array(pred_DD).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])\r\n        pred_DD_mean = pred_DD.mean(axis=0)\r\n        pred_DD_std = pred_DD.std(axis=0)\r\n\r\n        pred_PP = np.array(pred_PP).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])\r\n        pred_PP_mean = pred_PP.mean(axis=0)\r\n        pred_PP_std = pred_PP.std(axis=0)\r\n\r\n        pred_SS = np.array(pred_SS).reshape(args['number_of_sampling'], len(new_list), params_prediction['dim'])\r\n        pred_SS_mean = pred_SS.mean(axis=0)\r\n        pred_SS_std = pred_SS.std(axis=0)\r\n        # add Sept 2022 Hao\r\n        prob_dic['DD_mean'] = pred_DD_mean\r\n        prob_dic['PP_mean'] = pred_PP_mean\r\n        prob_dic['SS_mean'] = pred_SS_mean\r\n        prob_dic['DD_std'] = pred_DD_std\r\n        prob_dic['PP_std'] = pred_PP_std\r\n        prob_dic['SS_std'] = pred_SS_std\r\n    else:\r\n        # Only output probabilities\r\n        # pred_DD_mean, pred_PP_mean, pred_SS_mean = model.predict_generator(generator = prediction_generator,\r\n        #                                                                    use_multiprocessing = args['use_multiprocessing'],\r\n        #                                                                    workers = args['number_of_cpus'])\r\n        # rewrite adaptive prediction array\r\n        pred_DD = model.predict_generator(generator = prediction_generator,\r\n                                                                           use_multiprocessing = args['use_multiprocessing'],\r\n                                                                           workers = args['number_of_cpus'])\r\n        index = 0\r\n        if 'd' in phase_type or 'D' in phase_type or 'Detector' in phase_type:\r\n            # add detector prediction\r\n            pred_DD_mean = pred_DD[index]\r\n            index = index + 1\r\n            pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])\r\n            pred_DD_std = np.zeros((pred_DD_mean.shape))\r\n            prob_dic['DD_mean'] = pred_DD_mean\r\n            prob_dic['DD_std'] = pred_DD_std\r\n        for picker_name in phase_type:\r\n            if picker_name == 'P':\r\n                # add P-type output prediction\r\n                pred_PP_mean = pred_DD[index]\r\n                pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1])\r\n                pred_PP_std = np.zeros((pred_PP_mean.shape))\r\n                prob_dic['PP_mean'] = pred_PP_mean\r\n                prob_dic['PP_std'] = pred_PP_std\r\n                index = index + 1\r\n            if picker_name == 'S':\r\n                # add S-type output prediction\r\n                pred_SS_mean = pred_DD[index]\r\n                pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1])\r\n                pred_SS_std = np.zeros((pred_SS_mean.shape))\r\n                prob_dic['SS_mean'] = pred_SS_mean\r\n                prob_dic['SS_std'] = pred_SS_std\r\n                index = index + 1\r\n            if picker_name == 'Pn':\r\n                # add Pn-type output prediction\r\n                pred_PN_mean = pred_DD[index]\r\n                pred_PN_mean = pred_PN_mean.reshape(pred_PN_mean.shape[0], pred_PN_mean.shape[1])\r\n                pred_PN_std = np.zeros((pred_PN_mean.shape))\r\n                prob_dic['PN_mean'] = pred_PN_mean\r\n                prob_dic['PP_std'] = pred_PN_std\r\n                index = index + 1\r\n            if picker_name == 'Sn':\r\n                # add Sn-type output channel\r\n                pred_SN_mean = pred_DD[index]\r\n                pred_SN_mean = pred_SN_mean.reshape(pred_SN_mean.shape[0], pred_SN_mean.shape[1])\r\n                pred_SN_std = np.zeros((pred_SN_mean.shape))\r\n                prob_dic['SN_mean'] = pred_SN_mean\r\n                prob_dic['SS_std'] = pred_SN_std\r\n                index = index + 1\r\n            if picker_name == 'Pg':\r\n                # add Pg-type output channel\r\n                pred_PG_mean = pred_DD[index]\r\n                pred_PG_mean = pred_PG_mean.reshape(pred_PG_mean.shape[0], pred_PG_mean.shape[1])\r\n                pred_PG_std = np.zeros((pred_PG_mean.shape))\r\n                prob_dic['PG_mean'] = pred_PG_mean\r\n                prob_dic['PG_std'] = pred_PG_std\r\n                index = index + 1\r\n            if picker_name == 'Sg':\r\n                # add Sg-type output channel\r\n                pred_SG_mean = pred_DD[index]\r\n                pred_SG_mean = pred_SG_mean.reshape(pred_SG_mean.shape[0], pred_SG_mean.shape[1])\r\n                pred_SG_std = np.zeros((pred_SG_mean.shape))\r\n                prob_dic['SG_mean'] = pred_SG_mean\r\n                prob_dic['SG_std'] = pred_SG_std\r\n                index = index + 1\r\n        # old version\r\n        # pred_DD_mean = pred_DD_mean.reshape(pred_DD_mean.shape[0], pred_DD_mean.shape[1])\r\n        # pred_PP_mean = pred_PP_mean.reshape(pred_PP_mean.shape[0], pred_PP_mean.shape[1])\r\n        # pred_SS_mean = pred_SS_mean.reshape(pred_SS_mean.shape[0], pred_SS_mean.shape[1])\r\n\r\n        # pred_DD_std = np.zeros((pred_DD_mean.shape))\r\n        # pred_PP_std = np.zeros((pred_PP_mean.shape))\r\n        # pred_SS_std = np.zeros((pred_SS_mean.shape))\r\n    # old version Hao blocked on 2021-09-02\r\n    # prob_dic['DD_mean']=pred_DD_mean\r\n    # prob_dic['PP_mean']=pred_PP_mean\r\n    # prob_dic['SS_mean']=pred_SS_mean\r\n    # prob_dic['DD_std']=pred_DD_std\r\n    # prob_dic['PP_std']=pred_PP_std\r\n    # prob_dic['SS_std']=pred_SS_std\r\n\r\n    return prob_dic\r\n\r\n\r\n\r\ndef _gen_writer(new_list, args, prob_dic, pred_set, HDF_PROB, predict_writer, save_figs, csvPr_gen, plt_n, detection_memory, keepPS, spLimit):\r\n\r\n    \"\"\"\r\n\r\n    Applies the detection and picking on the output predicted probabilities and if it founds any, write them out in the CSV file,\r\n    makes the plots, and save the probabilities and uncertainties.\r\n\r\n    Parameters\r\n    ----------\r\n    new_list: list of str\r\n        A list of trace names in the batch.\r\n\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    prob_dic: dic\r\n        A dictionary containing output probabilities and their estimated standard deviations.\r\n\r\n    pred_set: dic\r\n        A dictionary containing HDF datasets for the current batch.\r\n\r\n    HDF_PROB: obj\r\n        For writing out the probabilities and uncertainties.\r\n\r\n    predict_writer: obj\r\n        For writing out the detection/picking results in the CSV file.\r\n\r\n    save_figs: str\r\n        Path to the folder for saving the plots.\r\n\r\n    csvPr_gen : obj\r\n        For writing out the detection/picking results in the CSV file.\r\n\r\n    plt_n: positive integer\r\n        Keep the track of plotted figures.\r\n\r\n    detection_memory: list\r\n        Keep the track of detected events.\r\n\r\n    spLimit: int, default : 60\r\n        S - P time in seconds. It will limit the results to those detections with events that have a specific S-P time limit.\r\n\r\n    Returns\r\n    -------\r\n    plt_n: positive integer\r\n        Keep the track of plotted figures.\r\n\r\n    detection_memory: list\r\n        Keep the track of detected events.\r\n\r\n\r\n    \"\"\"\r\n    # find first key in the prob_dic dictionary\r\n    keys = []\r\n    for key in prob_dic.keys():\r\n        keys.append(key)\r\n    if  'DD_mean' not in keys:\r\n        prob_dic['DD_mean'] = {}\r\n    if 'PP_mean' not in keys:\r\n        prob_dic['PP_mean'] = {}\r\n    if 'SS_mean' not in keys:\r\n        prob_dic['SS_mean'] = {}\r\n    if 'PN_mean' not in keys:\r\n        prob_dic['PN_mean'] = {}\r\n    if 'SN_mean' not in keys:\r\n        prob_dic['SN_mean'] = {}\r\n    if 'PG_mean' not in keys:\r\n        prob_dic['PG_mean'] = {}\r\n    if 'SG_mean' not in keys:\r\n        prob_dic['SG_mean'] = {}\r\n\r\n    for ts in range(prob_dic[keys[0]].shape[0]):\r\n        evi =  new_list[ts]\r\n        dataset = pred_set[evi]\r\n        dat = np.array(dataset)\r\n        if dat.ndim == 1:\r\n            # original trace could be 1-component, i.e., (6000,) or (6000)\r\n            dat_channel = 1\r\n            dat_dim = len(dat)\r\n        else:\r\n            # convert data format to the one that is used in the prediction\r\n            if dat.shape[0] <= 10:  # assume the original shape is (n_channels, n_samples )\r\n                dat = np.transpose(dat)\r\n            # more than 1 component trace\r\n            dat_channel = dat.shape[1]\r\n            dat_dim = dat.shape[0]\r\n        if dat_channel > args[\"input_dimention\"][1]:\r\n            dat_channel = args[\"input_dimention\"][1]\r\n        # check data shape\r\n        temp = dat\r\n        dat = np.zeros((args[\"input_dimention\"][0], args[\"input_dimention\"][1]))\r\n        attr_value = dataset.attrs.get('component', None)\r\n        if attr_value is not None:\r\n            # label contains component information\r\n            if attr_value == 'Z':\r\n                if temp.shape[0] < args[\"input_dimention\"][0]:\r\n                    dat[:temp.shape[0], 2] = temp\r\n                else:\r\n                    dat[:, 2] = temp[:args[\"input_dimention\"][0]]\r\n            if attr_value == 'N' or attr_value == '2':\r\n                if temp.shape[0] < args[\"input_dimention\"][0]:\r\n                    dat[:temp.shape[0], 1] = temp\r\n                else:\r\n                    dat[:, 1] = temp[:args[\"input_dimention\"][0]]\r\n            if attr_value == 'E' or attr_value == '0':\r\n                if temp.shape[0] < args[\"input_dimention\"][0]:\r\n                    dat[:temp.shape[0], 0] = temp\r\n                else:\r\n                    dat[:, 0] = temp[:args[\"input_dimention\"][0]]\r\n        else:\r\n            if dat_channel == 1:\r\n                if temp.shape[0] < args[\"input_dimention\"][0]:\r\n                    dat[:temp.shape[0]] = temp\r\n                else:\r\n                    dat[:, 0] = temp[:args[\"input_dimention\"][0]]\r\n            else:\r\n                if temp.shape[0] < args[\"input_dimention\"][0]:\r\n                    dat[:temp.shape[0], 0:dat_channel] = temp[:, 0:dat_channel]\r\n                else:\r\n                    dat[:, 0:dat_channel] = temp[:args[\"input_dimention\"][0], 0:dat_channel]\r\n\r\n        if args['output_probabilities']:\r\n\r\n            probs = np.zeros((prob_dic['DD_mean'].shape[1], 3))\r\n            probs[:, 0] = prob_dic['DD_mean'][ts]\r\n            probs[:, 1] = prob_dic['PP_mean'][ts]\r\n            probs[:, 2] = prob_dic['SS_mean'][ts]\r\n\r\n            uncs = np.zeros((prob_dic['DD_mean'].shape[1], 3))\r\n            uncs[:, 0] = prob_dic['DD_std'][ts]\r\n            uncs[:, 1] = prob_dic['PP_std'][ts]\r\n            uncs[:, 2] = prob_dic['SS_std'][ts]\r\n\r\n            HDF_PROB.create_dataset('probabilities/'+str(evi), probs.shape, data=probs, dtype= np.float32)\r\n            HDF_PROB.create_dataset('uncertainties/'+str(evi), uncs.shape, data=uncs, dtype= np.float32)\r\n            HDF_PROB.flush()\r\n        global matches\r\n        global matches2\r\n        global matches3\r\n        matches ={}\r\n        matches2 ={}\r\n        matches3 ={}\r\n\r\n        if 'DD_mean' in keys and 'PP_mean' in keys and 'SS_mean' in keys:\r\n            matches, pick_errors, yh3 =  picker(args, prob_dic['DD_mean'][ts], prob_dic['PP_mean'][ts], prob_dic['SS_mean'][ts],\r\n                                            prob_dic['DD_std'][ts], prob_dic['PP_std'][ts], prob_dic['SS_std'][ts])\r\n        else:\r\n            prob_dic['PP_mean'][ts] = None\r\n            prob_dic['SS_mean'][ts] = None\r\n\r\n        if keepPS:\r\n            if (len(matches) >= 1) and (matches[list(matches)[0]][3] and matches[list(matches)[0]][6]):\r\n                if (matches[list(matches)[0]][6] - matches[list(matches)[0]][3]) < spLimit*100:\r\n                    snr = [_get_snr(dat, matches[list(matches)[0]][3], window = 100), _get_snr(dat, matches[list(matches)[0]][6], window = 100)]\r\n                    pre_write = len(detection_memory)\r\n                    detection_memory=_output_writter_prediction(dataset, predict_writer, csvPr_gen, matches, snr, detection_memory)\r\n                    post_write = len(detection_memory)\r\n                    if plt_n < args['number_of_plots'] and post_write > pre_write:\r\n                        _plotter_prediction(dat, evi, args, save_figs,\r\n                                              prob_dic['DD_mean'][ts],\r\n                                              prob_dic['PP_mean'][ts],\r\n                                              prob_dic['SS_mean'][ts],\r\n                                              prob_dic['DD_std'][ts],\r\n                                              prob_dic['PP_std'][ts],\r\n                                              prob_dic['SS_std'][ts],\r\n                                              matches)\r\n                        plt_n += 1 ;\r\n        else:\r\n            if (len(matches) >= 1) and ((matches[list(matches)[0]][3] or matches[list(matches)[0]][6])):\r\n                snr = [_get_snr(dat, matches[list(matches)[0]][3], window = 100), _get_snr(dat, matches[list(matches)[0]][6], window = 100)]\r\n                pre_write = len(detection_memory)\r\n                detection_memory=_output_writter_prediction(dataset, predict_writer, csvPr_gen, matches, snr, detection_memory)\r\n                post_write = len(detection_memory)\r\n                # if plt_n < args['number_of_plots'] and post_write > pre_write:\r\n                #     _plotter_prediction(dat, evi, args, save_figs,\r\n                #                           prob_dic['DD_mean'][ts],\r\n                #                           prob_dic['PP_mean'][ts],\r\n                #                           prob_dic['SS_mean'][ts],\r\n                #                           prob_dic['DD_std'][ts],\r\n                #                           prob_dic['PP_std'][ts],\r\n                #                           prob_dic['SS_std'][ts],\r\n                #                           matches, keys)\r\n                #     plt_n += 1 ;\r\n        # Pn and Sn Hao Sept.2022\r\n        # bug: cannot display other phases's name on plots, e.g., Pg, Sg and Pn, Sn\r\n        if 'DD_mean' in keys and 'PN_mean' in keys and 'SN_mean' in keys:\r\n            matches2, pick_errors, yh3 =  picker(args, prob_dic['DD_mean'][ts], prob_dic['PN_mean'][ts], prob_dic['SN_mean'][ts],\r\n                                            prob_dic['DD_std'][ts], prob_dic['PN_std'][ts], prob_dic['SN_std'][ts])\r\n            if (len(matches2) >= 1) and ((matches2[list(matches2)[0]][3] or matches2[list(matches2)[0]][6])):\r\n                snr = [_get_snr(dat, matches2[list(matches2)[0]][3], window = 100), _get_snr(dat, matches2[list(matches2)[0]][6], window = 100)]\r\n                pre_write = len(detection_memory)\r\n                detection_memory=_output_writter_prediction(dataset, predict_writer, csvPr_gen, matches2, snr, detection_memory)\r\n                post_write = len(detection_memory)\r\n                # if plt_n < args['number_of_plots'] and post_write > pre_write:\r\n                #     _plotter_prediction(dat, evi, args, save_figs,\r\n                #                           prob_dic['DD_mean'][ts],\r\n                #                           prob_dic['PN_mean'][ts],\r\n                #                           prob_dic['SN_mean'][ts],\r\n                #                           prob_dic['DD_std'][ts],\r\n                #                           prob_dic['PN_std'][ts],\r\n                #                           prob_dic['SN_std'][ts],\r\n                #                           matches, keys)\r\n                #     plt_n += 1 ;\r\n        else:\r\n            prob_dic['PN_mean'][ts] = None\r\n            prob_dic['SN_mean'][ts] = None\r\n        # Pg and Sg Hao Sept.2022\r\n        # bug: cannot display other phases's name on plots, e.g., Pg, Sg and Pn, Sn\r\n        if 'DD_mean' in keys and 'PG_mean' in keys and 'SG_mean' in keys:\r\n            matches3, pick_errors, yh3 =  picker(args, prob_dic['DD_mean'][ts], prob_dic['PG_mean'][ts], prob_dic['SG_mean'][ts],\r\n                                            prob_dic['DD_std'][ts], prob_dic['PG_std'][ts], prob_dic['SG_std'][ts])\r\n            if (len(matches3) >= 1) and ((matches3[list(matches3)[0]][3] or matches3[list(matches3)[0]][6])):\r\n                snr = [_get_snr(dat, matches3[list(matches3)[0]][3], window = 100), _get_snr(dat, matches3[list(matches3)[0]][6], window = 100)]\r\n                pre_write = len(detection_memory)\r\n                detection_memory=_output_writter_prediction(dataset, predict_writer, csvPr_gen, matches3, snr, detection_memory)\r\n                post_write = len(detection_memory)\r\n                # if plt_n < args['number_of_plots'] and post_write > pre_write:\r\n                #     _plotter_prediction(dat, evi, args, save_figs,\r\n                #                           prob_dic['DD_mean'][ts],\r\n                #                           prob_dic['PG_mean'][ts],\r\n                #                           prob_dic['SG_mean'][ts],\r\n                #                           prob_dic['DD_std'][ts],\r\n                #                           prob_dic['PG_std'][ts],\r\n                #                           prob_dic['SG_std'][ts],\r\n                #                           matches, keys)\r\n                #     plt_n += 1 ;\r\n        else:\r\n            prob_dic['PG_mean'][ts] = None\r\n            prob_dic['SG_mean'][ts] = None\r\n        if plt_n < args['number_of_plots'] and (matches or matches2 or matches3):\r\n            _plotter_mul_prediction(dat, evi, args, save_figs, matches, keys, matches2, matches3,\r\n                                    prob_dic['DD_mean'][ts],\r\n                                    prob_dic['PP_mean'][ts],\r\n                                    prob_dic['SS_mean'][ts],\r\n                                    prob_dic['PN_mean'][ts],\r\n                                    prob_dic['SN_mean'][ts],\r\n                                    prob_dic['PG_mean'][ts],\r\n                                    prob_dic['SG_mean'][ts])\r\n            plt_n += 1;\r\n\r\n    return plt_n, detection_memory\r\n\r\n\r\n\r\ndef _output_writter_prediction(dataset, predict_writer, csvPr, matches, snr, detection_memory):\r\n\r\n    \"\"\"\r\n\r\n    Writes the detection & picking results into a CSV file.\r\n\r\n    Parameters\r\n    ----------\r\n    dataset: hdf5 obj\r\n        Dataset object of the trace.\r\n\r\n    predict_writer: obj\r\n        For writing out the detection/picking results in the CSV file.\r\n\r\n    csvPr: obj\r\n        For writing out the detection/picking results in the CSV file.\r\n\r\n    matches: dic\r\n        It contains the information for the detected and picked event.\r\n\r\n    snr: list of two floats\r\n        Estimated signal to noise ratios for picked P and S phases.\r\n\r\n    detection_memory : list\r\n        Keep the track of detected events.\r\n\r\n    Returns\r\n    -------\r\n    detection_memory : list\r\n        Keep the track of detected events.\r\n\r\n\r\n    \"\"\"\r\n    try:\r\n        trace_name = dataset.attrs[\"trace_name\"]\r\n    except KeyError:\r\n        trace_name = dataset.name\r\n    try:\r\n        station_name = dataset.attrs[\"receiver_code\"]\r\n        station_lat = dataset.attrs[\"receiver_latitude\"]\r\n        station_lon = dataset.attrs[\"receiver_longitude\"]\r\n        station_elv = dataset.attrs[\"receiver_elevation_m\"]\r\n        start_time = dataset.attrs[\"trace_start_time\"]\r\n        station_name = \"{:<4}\".format(station_name)\r\n        network_name = dataset.attrs[\"network_code\"]\r\n        network_name = \"{:<2}\".format(network_name)\r\n    except:\r\n        station_lat = 0\r\n        station_lon = 0\r\n        station_elv = 0\r\n        start_time = 0\r\n        stainfo = trace_name.split('_')[0]\r\n        station_name = stainfo.split('.')[1]\r\n        network_name = stainfo.split('.')[2]\r\n    instrument_type = trace_name.split('_')[2]\r\n    instrument_type = \"{:<2}\".format(instrument_type)\r\n    # try:\r\n    #     start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f')\r\n    # except Exception:\r\n    #     if not start_time == 0:\r\n    #         start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\r\n    #     else:\r\n    #         start_time = trace_name.split('_')[1]\r\n    #         start_time = start_time[:14]\r\n    #         start_time = datetime.strptime(start_time, '%Y%m%d%H%M%S')\r\n    try:\r\n        start_time = UTCDateTime(start_time)\r\n        start_time = start_time.datetime\r\n    except Exception:\r\n        if not start_time == 0:\r\n            start_time = UTCDateTime(start_time)\r\n        else:\r\n            start_time = trace_name.split\r\n\r\n    def _date_convertor(r):\r\n        if isinstance(r, str):\r\n            mls = r.split('.')\r\n            if len(mls) == 1:\r\n                new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S')\r\n            else:\r\n                new_t = datetime.strptime(r, '%Y-%m-%d %H:%M:%S.%f')\r\n        else:\r\n            new_t = r\r\n\r\n        return new_t\r\n\r\n    for match, match_value in matches.items():\r\n        ev_strt = start_time+timedelta(seconds= match/100)\r\n        ev_end = start_time+timedelta(seconds= match_value[0]/100)\r\n\r\n        doublet = [ st for st in detection_memory if abs((st-ev_strt).total_seconds()) < 2]\r\n\r\n        if len(doublet) == 0:\r\n            det_prob = round(match_value[1], 2)\r\n            if match_value[2]:\r\n                det_unc = round(match_value[2], 2)\r\n            else:\r\n                det_unc = match_value[2]\r\n\r\n            if match_value[3]:\r\n                p_time = start_time+timedelta(seconds= match_value[3]/100)\r\n            else:\r\n                p_time = None\r\n            p_prob = match_value[4]\r\n            p_unc = match_value[5]\r\n\r\n            if p_unc:\r\n                p_unc = round(p_unc, 2)\r\n            if p_prob:\r\n                p_prob = round(p_prob, 2)\r\n\r\n            if match_value[6]:\r\n                s_time = start_time+timedelta(seconds= match_value[6]/100)\r\n            else:\r\n                s_time = None\r\n            s_prob = match_value[7]\r\n            s_unc = match_value[8]\r\n\r\n            if s_unc:\r\n                s_unc = round(s_unc, 2)\r\n            if s_prob:\r\n                s_prob = round(s_prob, 2)\r\n\r\n            predict_writer.writerow([trace_name,\r\n                                         network_name,\r\n                                         station_name,\r\n                                         instrument_type,\r\n                                         station_lat,\r\n                                         station_lon,\r\n                                         station_elv,\r\n                                         _date_convertor(ev_strt),\r\n                                         _date_convertor(ev_end),\r\n                                         det_prob,\r\n                                         det_unc,\r\n                                         _date_convertor(p_time),\r\n                                         p_prob,\r\n                                         p_unc,\r\n                                         snr[0],\r\n                                         _date_convertor(s_time),\r\n                                         s_prob,\r\n                                         s_unc,\r\n                                         snr[1]\r\n                                         ])\r\n\r\n            csvPr.flush()\r\n            detection_memory.append(ev_strt)\r\n\r\n    return detection_memory\r\n\r\n\r\n\r\n\r\ndef _plotter_mul_prediction(data, evi, args, save_figs, matches, keys, matches2= None, matches3= None, yh1 = None,\r\n                            yh2 = None, yh3=None, yh4 = None, yh5=None,yh6=None, yh7=None):\r\n    \"\"\"\r\n    Adaptively generates plots of detected events waveforms, output predictions, and picked arrival times.\r\n\r\n    Parameters\r\n    ----------\r\n    data: NumPy array\r\n        N component raw waveform.\r\n\r\n    evi : str\r\n        Trace name.\r\n\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    save_figs: str\r\n        Path to the folder for saving the plots.\r\n\r\n    matches: dic\r\n        Contains the information for the P and S detected and picked event.\r\n\r\n    matches2: dic\r\n        Contains the information for the P and S detected and picked event.\r\n\r\n    matches3: dic\r\n        Contains the information for the P and S detected and picked event.\r\n\r\n    yh1: 1D array\r\n        Detection probabilities.\r\n\r\n    yh2: 1D array\r\n        P arrival probabilities.\r\n\r\n    yh3: 1D array\r\n        S arrival probabilities.\r\n\r\n    yh4: 1D array\r\n        Pn arrival probabilities.\r\n\r\n    yh5: 1D array\r\n        Sn arrival probabilities.\r\n\r\n    yh6: 1D array\r\n        Pg arrival probabilities.\r\n\r\n    yh7: 1D array\r\n        Sg arrival probabilities.\r\n\r\n\r\n    \"\"\"\r\n    #fetching detector and P and S picker\r\n    if matches:\r\n        spt, sst, detected_events = [], [], []\r\n        for match, match_value in matches.items():\r\n            detected_events.append([match, match_value[0]])\r\n            if match_value[3]:\r\n                spt.append(match_value[3])\r\n            else:\r\n                spt.append(None)\r\n\r\n            if match_value[6]:\r\n                sst.append(match_value[6])\r\n            else:\r\n                sst.append(None)\r\n    if matches2:\r\n        spt2, sst2, detected_events2 = [], [], []\r\n        for match, match_value in matches2.items():\r\n            detected_events2.append([match, match_value[0]])\r\n            if match_value[3]:\r\n                spt2.append(match_value[3])\r\n            else:\r\n                spt2.append(None)\r\n\r\n            if match_value[6]:\r\n                sst2.append(match_value[6])\r\n            else:\r\n                sst2.append(None)\r\n    if matches3:\r\n        spt3, sst3, detected_events3 = [], [], []\r\n        for match, match_value in matches3.items():\r\n            detected_events3.append([match, match_value[0]])\r\n            if match_value[3]:\r\n                spt3.append(match_value[3])\r\n            else:\r\n                spt3.append(None)\r\n\r\n            if match_value[6]:\r\n                sst3.append(match_value[6])\r\n            else:\r\n                sst3.append(None)\r\n    if data.ndim == 1:\r\n        dat_channel = 1\r\n    else:\r\n        dat_channel = data.shape[1]\r\n    fig = plt.figure()\r\n    fig_num = dat_channel + 1\r\n    for i in range(fig_num-1):\r\n        # plot the n-component raw data\r\n        ax = fig.add_subplot(fig_num, 1, i+1)\r\n        plt.plot(data[:, i], 'k')\r\n        ymin, ymax = ax.get_ylim()\r\n        # plotting the detected P and S events\r\n        if matches:\r\n            pl = sl = None\r\n            if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                ymin, ymax = ax.get_ylim()\r\n                for ipt, pt in enumerate(spt):\r\n                    if pt and ipt == 0:\r\n                        pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=1.5, label='Picked P')\r\n                    elif pt and ipt > 0:\r\n                        pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=1.5)\r\n\r\n            if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                for ist, st in enumerate(sst):\r\n                    if st and ist == 0:\r\n                        sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5, label='Picked S')\r\n                    elif st and ist > 0:\r\n                        sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5)\r\n        if matches2:\r\n            pl2 = sl2 = None\r\n            if len(spt2) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                ymin, ymax = ax.get_ylim()\r\n                for ipt, pt in enumerate(spt2):\r\n                    if pt and ipt == 0:\r\n                        pl2 = plt.vlines(int(pt), ymin, ymax, color='cyan', linewidth=1.5, label='Picked Pn')\r\n                    elif pt and ipt > 0:\r\n                        pl2 = plt.vlines(int(pt), ymin, ymax, color='cyan', linewidth=1.5)\r\n\r\n            if len(sst2) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                for ist, st in enumerate(sst2):\r\n                    if st and ist == 0:\r\n                        sl2 = plt.vlines(int(st), ymin, ymax, color='m', linewidth=1.5, label='Picked Sn')\r\n                    elif st and ist > 0:\r\n                        sl2 = plt.vlines(int(st), ymin, ymax, color='fuchsia', linewidth=1.5)\r\n        if matches3:\r\n            pl3 = sl3 = None\r\n            if len(spt3) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                ymin, ymax = ax.get_ylim()\r\n                for ipt, pt in enumerate(spt3):\r\n                    if pt and ipt == 0:\r\n                        pl3 = plt.vlines(int(pt), ymin, ymax, color='dodgerblue', linewidth=1.5, label='Picked Pg')\r\n                    elif pt and ipt > 0:\r\n                        pl3 = plt.vlines(int(pt), ymin, ymax, color='dodgerblue', linewidth=1.5)\r\n\r\n            if len(sst3) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n                for ist, st in enumerate(sst3):\r\n                    if st and ist == 0:\r\n                        sl3 = plt.vlines(int(st), ymin, ymax, color='crimson', linewidth=1.5, label='Picked Sg')\r\n                    elif st and ist > 0:\r\n                        sl3 = plt.vlines(int(st), ymin, ymax, color='crimson', linewidth=1.5)\r\n        plt.rcParams[\"figure.figsize\"] = (16, 9)\r\n        #plt.text(0, 10000, \"E\", fontsize=16)\r\n        if i ==0:\r\n            plt.title('Trace Name: ' +str(evi), fontsize=16)\r\n        plt.tight_layout()\r\n        plt.legend(loc='upper right', borderaxespad=0., fontsize=16)\r\n        plt.ylabel('Amplitude\\nCounts', fontsize=16)\r\n    # plot the detection results\r\n    i = i+1\r\n    ax = fig.add_subplot(fig_num, 1, i + 1)\r\n    # plotting the detected P and S events\r\n    if matches:\r\n        plt.plot(yh2, '--', color='deepskyblue', alpha=0.5, linewidth=1.5, label='P  Prediction')\r\n        plt.plot(yh3, '--', color='deeppink', alpha=0.5, linewidth=1.5, label='S  Prediction')\r\n    if matches2:\r\n        plt.plot(yh4, '--', color='cyan', alpha=0.5, linewidth=1.5, label='Pn Prediction')\r\n        plt.plot(yh5, '--', color='fuchsia', alpha=0.5, linewidth=1.5, label='Sn Prediction')\r\n    if matches3:\r\n        plt.plot(yh6, '--', color='dodgerblue', alpha=0.5, linewidth=1.5, label='Pg Prediction')\r\n        plt.plot(yh7, '--', color='crimson', alpha=0.5, linewidth=1.5, label='Sg Prediction')\r\n    plt.rcParams[\"figure.figsize\"] = (16, 9)\r\n    plt.tight_layout()\r\n    plt.legend(loc = 'upper right', borderaxespad=0., fontsize = 16)\r\n    plt.ylabel('Probability\\n', fontsize=16)\r\n    plt.xlabel('Sample', fontsize=16)\r\n    fig.savefig(os.path.join(save_figs, str(evi) + '.jpg'), dpi=300)\r\n    plt.close(fig)\r\n    plt.clf()\r\n\r\n\r\ndef _plotter_prediction(data, evi, args, save_figs, yh1, yh2, yh3, yh1_std, yh2_std, yh3_std, matches, keys):\r\n\r\n    \"\"\"\r\n\r\n    Generates plots of detected events waveforms, output predictions, and picked arrival times.\r\n\r\n    Parameters\r\n    ----------\r\n    data: NumPy array\r\n        3 component raw waveform.\r\n\r\n    evi : str\r\n        Trace name.\r\n\r\n    args: dic\r\n        A dictionary containing all of the input parameters.\r\n\r\n    save_figs: str\r\n        Path to the folder for saving the plots.\r\n\r\n    yh1: 1D array\r\n        Detection probabilities.\r\n\r\n    yh2: 1D array\r\n        P arrival probabilities.\r\n\r\n    yh3: 1D array\r\n        S arrival probabilities.\r\n\r\n    yh1_std: 1D array\r\n        Detection standard deviations.\r\n\r\n    yh2_std: 1D array\r\n        P arrival standard deviations.\r\n\r\n    yh3_std: 1D array\r\n        S arrival standard deviations.\r\n\r\n    matches: dic\r\n        Contains the information for the detected and picked event.\r\n\r\n\r\n    \"\"\"\r\n\r\n    font0 = {'family': 'serif',\r\n            'color': 'white',\r\n            'stretch': 'condensed',\r\n            'weight': 'normal',\r\n            'size': 12,\r\n            }\r\n\r\n    spt, sst, detected_events = [], [], []\r\n    for match, match_value in matches.items():\r\n        detected_events.append([match, match_value[0]])\r\n        if match_value[3]:\r\n            spt.append(match_value[3])\r\n        else:\r\n            spt.append(None)\r\n\r\n        if match_value[6]:\r\n            sst.append(match_value[6])\r\n        else:\r\n            sst.append(None)\r\n\r\n    if args['plot_mode'] == 'time_frequency':\r\n\r\n        fig = plt.figure(constrained_layout=False)\r\n        widths = [6, 1]\r\n        heights = [1, 1, 1, 1, 1, 1, 1.8]\r\n        spec5 = fig.add_gridspec(ncols=2, nrows=7, width_ratios=widths,\r\n                              height_ratios=heights, left=0.1, right=0.9, hspace=0.1)\r\n\r\n\r\n        ax = fig.add_subplot(spec5[0, 0])\r\n        plt.plot(data[:, 0], 'k')\r\n        plt.xlim(0, 6000)\r\n        x = np.arange(6000)\r\n     #   for ev in detected_events:\r\n     #       l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n        ax.set_xticks([])\r\n        plt.rcParams[\"figure.figsize\"] = (10, 10)\r\n        legend_properties = {'weight':'bold'}\r\n        plt.title('Trace Name: '+str(evi))\r\n\r\n        pl = None\r\n        sl = None\r\n\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n\r\n        ax = fig.add_subplot(spec5[0, 1])\r\n        if pl or sl:\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['E', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\r\n            plt.axis('off')\r\n\r\n\r\n        ax = fig.add_subplot(spec5[1, 0])\r\n        f, t, Pxx = signal.stft(data[:, 0], fs=100, nperseg=80)\r\n        Pxx = np.abs(Pxx)\r\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\r\n        plt.ylim(0, 40)\r\n        plt.text(1, 1, 'STFT', fontdict=font0)\r\n        plt.ylabel('Hz', fontsize=12)\r\n        ax.set_xticks([])\r\n\r\n\r\n        ax = fig.add_subplot(spec5[2, 0])\r\n        plt.plot(data[:, 1] , 'k')\r\n        plt.xlim(0, 6000)\r\n    #    for ev in detected_events:\r\n    #        l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n        ax.set_xticks([])\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 1]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 1]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n        ax = fig.add_subplot(spec5[2, 1])\r\n        if pl or sl:\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['N', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\r\n            plt.axis('off')\r\n\r\n\r\n        ax = fig.add_subplot(spec5[3, 0])\r\n        f, t, Pxx = signal.stft(data[:, 1], fs=100, nperseg=80)\r\n        Pxx = np.abs(Pxx)\r\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\r\n        plt.ylim(0, 40)\r\n        plt.text(1, 1, 'STFT', fontdict=font0)\r\n        plt.ylabel('Hz', fontsize=12)\r\n        ax.set_xticks([])\r\n\r\n\r\n        ax = fig.add_subplot(spec5[4, 0])\r\n        plt.plot(data[:, 2], 'k')\r\n        plt.xlim(0, 6000)\r\n    #    for ev in detected_events:\r\n     #       l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n        ax.set_xticks([])\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 2]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 2]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n        ax = fig.add_subplot(spec5[4, 1])\r\n        if pl or sl:\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['Z', 'Picked P', 'Picked S'], fancybox=True, shadow=True)\r\n            plt.axis('off')\r\n\r\n        ax = fig.add_subplot(spec5[5, 0])\r\n        f, t, Pxx = signal.stft(data[:, 2], fs=100, nperseg=80)\r\n        Pxx = np.abs(Pxx)\r\n        plt.pcolormesh(t, f, Pxx, alpha=None, cmap='hot', shading='flat', antialiased=True)\r\n        plt.ylim(0, 40)\r\n        plt.text(1, 1, 'STFT', fontdict=font0)\r\n        plt.ylabel('Hz', fontsize=12)\r\n        ax.set_xticks([])\r\n\r\n        ax = fig.add_subplot(spec5[6, 0])\r\n        x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)\r\n        if args['estimate_uncertainty']:\r\n            plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=2, label='Earthquake')\r\n            lowerD = yh1-yh1_std\r\n            upperD = yh1+yh1_std\r\n            plt.fill_between(x, lowerD, upperD, alpha=0.5, edgecolor='#3F7F4C', facecolor='#7EFF99')\r\n\r\n            plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=2, label='P_arrival')\r\n            lowerP = yh2-yh2_std\r\n            upperP = yh2+yh2_std\r\n            plt.fill_between(x, lowerP, upperP, alpha=0.5, edgecolor='#1B2ACC', facecolor='#089FFF')\r\n\r\n            plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=2, label='S_arrival')\r\n            lowerS = yh3-yh3_std\r\n            upperS = yh3+yh3_std\r\n            plt.fill_between(x, lowerS, upperS, edgecolor='#CC4F1B', facecolor='#FF9848')\r\n\r\n            plt.tight_layout()\r\n            plt.ylim((-0.1, 1.1))\r\n            plt.xlim(0, 6000)\r\n            plt.ylabel('Probability', fontsize=12)\r\n            plt.xlabel('Sample', fontsize=12)\r\n            plt.yticks(np.arange(0, 1.1, step=0.2))\r\n            axes = plt.gca()\r\n            axes.yaxis.grid(color='lightgray')\r\n\r\n            font = {'family': 'serif',\r\n                    'color': 'dimgrey',\r\n                    'style': 'italic',\r\n                    'stretch': 'condensed',\r\n                    'weight': 'normal',\r\n                    'size': 12,\r\n                    }\r\n\r\n\r\n        else:\r\n            plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=2, label='Earthquake')\r\n            plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=2, label='P_arrival')\r\n            plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=2, label='S_arrival')\r\n            plt.tight_layout()\r\n            plt.ylim((-0.1, 1.1))\r\n            plt.xlim(0, 6000)\r\n            plt.ylabel('Probability', fontsize=12)\r\n            plt.xlabel('Sample', fontsize=12)\r\n            plt.yticks(np.arange(0, 1.1, step=0.2))\r\n            axes = plt.gca()\r\n            axes.yaxis.grid(color='lightgray')\r\n\r\n        ax = fig.add_subplot(spec5[6, 1])\r\n        custom_lines = [Line2D([0], [0], linestyle='--', color='mediumblue', lw=2),\r\n                        Line2D([0], [0], linestyle='--', color='c', lw=2),\r\n                        Line2D([0], [0], linestyle='--', color='m', lw=2)]\r\n        plt.legend(custom_lines, ['Earthquake', 'P_arrival', 'S_arrival'], fancybox=True, shadow=True)\r\n        plt.axis('off')\r\n\r\n        font = {'family': 'serif',\r\n                    'color': 'dimgrey',\r\n                    'style': 'italic',\r\n                    'stretch': 'condensed',\r\n                    'weight': 'normal',\r\n                    'size': 12,\r\n                    }\r\n\r\n        plt.text(1, 0.2, 'EQTransformer', fontdict=font)\r\n        if EQT_VERSION:\r\n            plt.text(2000, 0.05, str(EQT_VERSION), fontdict=font)\r\n\r\n        plt.xlim(0, 6000)\r\n        fig.tight_layout()\r\n        fig.savefig(os.path.join(save_figs, str(evi)+'.png'), dpi=200)\r\n        plt.close(fig)\r\n        plt.clf()\r\n\r\n\r\n    else:\r\n\r\n        ########################################## ploting only in time domain\r\n        fig = plt.figure(constrained_layout=True)\r\n        widths = [1]\r\n        heights = [1.6, 1.6, 1.6, 2.5]\r\n        spec5 = fig.add_gridspec(ncols=1, nrows=4, width_ratios=widths,\r\n                              height_ratios=heights)\r\n\r\n        ax = fig.add_subplot(spec5[0, 0])\r\n        plt.plot(data[:, 0], 'k')\r\n        x = np.arange(6000)\r\n        plt.xlim(0, 6000)\r\n\r\n        plt.ylabel('Amplitude\\nCounts')\r\n\r\n    #    for ev in detected_events:\r\n    #        l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n\r\n        plt.rcParams[\"figure.figsize\"] = (8,6)\r\n        legend_properties = {'weight':'bold'}\r\n        plt.title('Trace Name: '+str(evi))\r\n\r\n        pl = sl = None\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 0]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n        if pl or sl:\r\n            box = ax.get_position()\r\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['E', 'Picked P', 'Picked S'],\r\n                       loc='center left', bbox_to_anchor=(1, 0.5),\r\n                       fancybox=True, shadow=True)\r\n\r\n        ax = fig.add_subplot(spec5[1, 0])\r\n        plt.plot(data[:, 1] , 'k')\r\n        plt.xlim(0, 6000)\r\n        plt.ylabel('Amplitude\\nCounts')\r\n\r\n     #   for ev in detected_events:\r\n     #       l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 1]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 1]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n        if pl or sl:\r\n            box = ax.get_position()\r\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['N', 'Picked P', 'Picked S'],\r\n                       loc='center left', bbox_to_anchor=(1, 0.5),\r\n                       fancybox=True, shadow=True)\r\n\r\n        ax = fig.add_subplot(spec5[2, 0])\r\n        plt.plot(data[:, 2], 'k')\r\n        plt.xlim(0, 6000)\r\n        plt.ylabel('Amplitude\\nCounts')\r\n\r\n   #     for ev in detected_events:\r\n   #         l, = plt.gca().plot(x[ev[0]:ev[1]], data[ev[0]:ev[1], 0], 'mediumblue')\r\n        ax.set_xticks([])\r\n\r\n        if len(spt) > 0 and np.count_nonzero(data[:, 2]) > 10:\r\n            ymin, ymax = ax.get_ylim()\r\n            for ipt, pt in enumerate(spt):\r\n                if pt and ipt == 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2, label='Picked P')\r\n                elif pt and ipt > 0:\r\n                    pl = plt.vlines(int(pt), ymin, ymax, color='c', linewidth=2)\r\n\r\n        if len(sst) > 0 and np.count_nonzero(data[:, 2]) > 10:\r\n            for ist, st in enumerate(sst):\r\n                if st and ist == 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2, label='Picked S')\r\n                elif st and ist > 0:\r\n                    sl = plt.vlines(int(st), ymin, ymax, color='m', linewidth=2)\r\n\r\n        if pl or sl:\r\n            box = ax.get_position()\r\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\r\n            custom_lines = [Line2D([0], [0], color='k', lw=0),\r\n                            Line2D([0], [0], color='c', lw=2),\r\n                            Line2D([0], [0], color='m', lw=2)]\r\n            plt.legend(custom_lines, ['Z', 'Picked P', 'Picked S'],\r\n                       loc='center left', bbox_to_anchor=(1, 0.5),\r\n                       fancybox=True, shadow=True)\r\n\r\n        ax = fig.add_subplot(spec5[3, 0])\r\n        x = np.linspace(0, data.shape[0], data.shape[0], endpoint=True)\r\n\r\n        if args['estimate_uncertainty']:\r\n            plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=1.5, label='Earthquake')\r\n            lowerD = yh1-yh1_std\r\n            upperD = yh1+yh1_std\r\n            plt.fill_between(x, lowerD, upperD, alpha=0.5, edgecolor='#3F7F4C', facecolor='#7EFF99')\r\n\r\n            plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=1.5, label='P_arrival')\r\n            lowerP = yh2-yh2_std\r\n            upperP = yh2+yh2_std\r\n            plt.fill_between(x, lowerP, upperP, alpha=0.5, edgecolor='#1B2ACC', facecolor='#089FFF')\r\n\r\n            plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=1.5, label='S_arrival')\r\n            lowerS = yh3-yh3_std\r\n            upperS = yh3+yh3_std\r\n            plt.fill_between(x, lowerS, upperS, edgecolor='#CC4F1B', facecolor='#FF9848')\r\n\r\n            plt.tight_layout()\r\n            plt.ylim((-0.1, 1.1))\r\n            plt.xlim(0, 6000)\r\n            plt.ylabel('Probability')\r\n            plt.xlabel('Sample')\r\n            plt.legend(loc='lower center', bbox_to_anchor=(0., 1.17, 1., .102), ncol=3, mode=\"expand\",\r\n                       prop=legend_properties,  borderaxespad=0., fancybox=True, shadow=True)\r\n            plt.yticks(np.arange(0, 1.1, step=0.2))\r\n            axes = plt.gca()\r\n            axes.yaxis.grid(color='lightgray')\r\n\r\n            font = {'family': 'serif',\r\n                    'color': 'dimgrey',\r\n                    'style': 'italic',\r\n                    'stretch': 'condensed',\r\n                    'weight': 'normal',\r\n                    'size': 12,\r\n                    }\r\n\r\n            plt.text(6500, 0.5, 'EqTransformer', fontdict=font)\r\n            if EQT_VERSION:\r\n                plt.text(7000, 0.1, str(EQT_VERSION), fontdict=font)\r\n\r\n        else:\r\n            # Simple plot\r\n            # Hao Nov 3 2022\r\n            plt.plot(x, yh1, '--', color='g', alpha = 0.5, linewidth=1.5, label='Earthquake')\r\n            plt.plot(x, yh2, '--', color='b', alpha = 0.5, linewidth=1.5, label='P_arrival')\r\n            plt.plot(x, yh3, '--', color='r', alpha = 0.5, linewidth=1.5, label='S_arrival')\r\n\r\n            plt.tight_layout()\r\n            plt.ylim((-0.1, 1.1))\r\n            plt.xlim(0, 6000)\r\n            plt.ylabel('Probability')\r\n            plt.xlabel('Sample')\r\n            plt.legend(loc='lower center', bbox_to_anchor=(0., 1.17, 1., .102), ncol=3, mode=\"expand\",\r\n                       prop=legend_properties,  borderaxespad=0., fancybox=True, shadow=True)\r\n            plt.yticks(np.arange(0, 1.1, step=0.2))\r\n            axes = plt.gca()\r\n            axes.yaxis.grid(color='lightgray')\r\n\r\n            font = {'family': 'serif',\r\n                    'color': 'dimgrey',\r\n                    'style': 'italic',\r\n                    'stretch': 'condensed',\r\n                    'weight': 'normal',\r\n                    'size': 12,\r\n                    }\r\n\r\n            plt.text(6500, 0.5, 'EQTransformer', fontdict=font)\r\n            if EQT_VERSION:\r\n                plt.text(7000, 0.1, str(EQT_VERSION), fontdict=font)\r\n\r\n        fig.tight_layout()\r\n        fig.savefig(os.path.join(save_figs, str(evi)+'.png'))\r\n        plt.close(fig)\r\n        plt.clf()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef _get_snr(data, pat, window = 200):\r\n\r\n    \"\"\"\r\n\r\n    Estimates SNR.\r\n\r\n    Parameters\r\n    ----------\r\n    data: NumPy array\r\n        3 component data.\r\n\r\n    pat: positive integer\r\n        Sample point where a specific phase arrives.\r\n\r\n    window: positive integer\r\n        The length of the window for calculating the SNR (in the sample).\r\n\r\n    Returns\r\n    -------\r\n    snr : {float, None}\r\n       Estimated SNR in db.\r\n\r\n    \"\"\"\r\n\r\n    snr = None\r\n    if pat:\r\n        try:\r\n            if int(pat) >= window and (int(pat)+window) < len(data):\r\n                nw1 = data[int(pat)-window : int(pat)];\r\n                sw1 = data[int(pat) : int(pat)+window];\r\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\r\n            elif int(pat) < window and (int(pat)+window) < len(data):\r\n                window = int(pat)\r\n                nw1 = data[int(pat)-window : int(pat)];\r\n                sw1 = data[int(pat) : int(pat)+window];\r\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\r\n            elif (int(pat)+window) > len(data):\r\n                window = len(data)-int(pat)\r\n                nw1 = data[int(pat)-window : int(pat)];\r\n                sw1 = data[int(pat) : int(pat)+window];\r\n                snr = round(10*math.log10((np.percentile(sw1,95)/np.percentile(nw1,95))**2), 1)\r\n        except Exception:\r\n            pass\r\n    return snr\r\n"}
