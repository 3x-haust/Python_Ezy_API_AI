{"repo_info": {"repo_name": "supercharger", "repo_owner": "catid", "repo_url": "https://github.com/catid/supercharger"}}
{"type": "test_file", "path": "airate/test_airate.py", "content": ""}
{"type": "test_file", "path": "codegen/test_indentation_errors.py", "content": "import ast\nimport unittest\n\nfrom fix_ast_errors import fix_ast_errors\n\n\nclass TestFixIndentationErrors(unittest.TestCase):\n\n    test_cases = [\n        \"\"\"\ndef my_function():\n    message = '''\n        This is a multi-line string.\n        It can span multiple lines.\n        '''\n    print(message)\n        \"\"\",\n        \"\"\"\ndef my_function():\n    message = '''\n        This is a multi-line string.\n        It can span multiple lines.\n        '''\n    print(message)\n        \"\"\",\n        \"\"\"\n    def my_function():\n    print(\"Hello, world!\")\n        \"\"\",\n        \"\"\"\ndef my_function():\n    print(\"Hello, world!\")\n        \"\"\",\n        \"\"\"\n    if x > 0:\n    print(\"x is positive\")\n        \"\"\",\n        \"\"\"\nif x > 0:\n    print(\"x is positive\")\n        \"\"\",\n        \"\"\"\n    for i in range(10):\n    print(i)\n        \"\"\",\n        \"\"\"\nfor i in range(10):\n    print(i)\n        \"\"\",\n        \"\"\"\n    try:\n    result = 1 / 0\n    except ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n        \"\"\",\n        \"\"\"\ntry:\n    result = 1 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n        \"\"\",\n        \"\"\"\n    if x > 0:\n        print(\"x is positive\")\n    else:\n    print(\"x is non-positive\")\n        \"\"\",\n        \"\"\"\nif x > 0:\n    print(\"x is positive\")\nelse:\n    print(\"x is non-positive\")\n        \"\"\",\n        \"\"\"\n    def my_function(x, y):\n        if x > 0:\n        return x + y\n        else:\n        return y - x\n        \"\"\",\n        \"\"\"\ndef my_function(x, y):\n    if x > 0:\n        return x + y\n    else:\n        return y - x\n        \"\"\",\n        \"\"\"\n    def my_function(x, y):\n        if x > 0:\n        return x + y\n        else:\n        return y - x\n        \"\"\",\n        \"\"\"\ndef my_function(x, y):\n    if x > 0:\n        return x + y\n    else:\n        return y - x\n        \"\"\",\n        \"\"\"\n    def my_function(x, \\\\\n        y):\n        if x > 0:\n        return x + y\n        else:\n        return y - x\n        \"\"\",\n        \"\"\"\ndef my_function(x, \\\\\ny):\n    if x > 0:\n        return x + y\n    else:\n        return y - x\n        \"\"\",\n        \"\"\"\n    def my_function(x,\n                    y):\n        if x > 0:\n        return x + y\n        else:\n        return y - x\n            \"\"\",\n            \"\"\"\ndef my_function(x,\n    y):\n    if x > 0:\n        return x + y\n    else:\n        return y - x\n        \"\"\",\n        \"def main():\\n\"\n        \"\\tprint('Hello, World!')\\n\"\n        \"\\t\\tif True:\\n\"\n        \"\\t\\t\\tprint('This is indented with tabs.')\\n\"\n        \"main()\",\n        \"def main():\\n\"\n        \"    print('Hello, World!')\\n\"\n        \"    if True:\\n\"\n        \"        print('This is indented with tabs.')\\n\"\n        \"main()\",\n        \"def main():\\n\"\n        \"\\tprint('Hello, World!')\\n\"\n        \"\\t\\tif True:\\n\"\n        \"\\t\\t\\tprint('This is indented with tabs.')\\n\"\n        \"\\tprint('Hello, World!')\\n\"\n        \"main()\",\n        \"def main():\\n\"\n        \"    print('Hello, World!')\\n\"\n        \"    if True:\\n\"\n        \"        print('This is indented with tabs.')\\n\"\n        \"    print('Hello, World!')\\n\"\n        \"main()\",\n        \"def main():\\n\"\n        \"print('Hello, World!')\\n\"\n        \"if True:\\n\"\n        \"if True:\\n\"\n        \"print('This line should be indented.')\\n\"\n        \"main()\",\n        \"def main():\\n\"\n        \"    print('Hello, World!')\\n\"\n        \"if True:\\n\"\n        \"    if True:\\n\"\n        \"        print('This line should be indented.')\\n\"\n        \"main()\"\n    ]\n\n    def test_fix_indentation(self):\n        for index, code in enumerate(self.test_cases):\n            with self.subTest(f\"Test case {index}\"):\n                fixed_code = fix_ast_errors(code.strip())\n                try:\n                    ast.parse(fixed_code)\n                except Exception as e:\n                    self.fail(f\"Test case {index}: Syntax error in fixed code\\n\"\n                              f\"Input code:\\n---\\n{code}\\n---\\n\"\n                              f\"Fixed code:\\n---\\n{fixed_code}\\n---\\n\"\n                              f\"Error: {str(e)}\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "codegen/test_docker_execute.py", "content": "import os\nimport timeit\n\nfrom docker_execute import DockerExecute\n\n# Test script file name to execute in the Docker container\nsources_dirname = \"test_sources\"\ntest_script = \"test_script.py\"\nexpected_output = \"Hello, Docker!\"\n\nscript_path = os.path.join(os.getcwd(), sources_dirname, test_script)\nos.makedirs(os.path.join(os.getcwd(), sources_dirname), exist_ok=True)\n\n# Write a simple test script to execute in the container\nwith open(script_path, \"w\") as f:\n    f.write(f\"print('{expected_output}')\\n\")\n\n# Set up the benchmarking function\ndef benchmark_docker_execute():\n    print(\"Setting up...\")\n\n    docker_execute = DockerExecute(sources_dirname=sources_dirname)\n\n    print(\"Priming the pump...\")\n\n    # Prime the pump\n    exit_code, output = docker_execute.execute(test_script)\n    assert output == expected_output, f\"Unexpected output: {output}\"\n    assert exit_code == 0, f\"Unexpected exit code: {exit_code}\"\n\n    print(\"Benchmarking...\")\n\n    try:\n        # Measure the execution time of the method\n        exec_time = timeit.timeit(\n            lambda: docker_execute.execute(test_script), number=4\n        )\n    finally:\n        # Clean up the Docker container\n        docker_execute.shutdown()\n\n    return exec_time\n\n# Run the benchmark\nexec_time = benchmark_docker_execute()\nprint(f\"DockerExecute method: {exec_time:.6f} seconds\")\n\n# Remove the test script file\nos.remove(script_path)\n"}
{"type": "test_file", "path": "codegen/test_clean_code.py", "content": "import os\n\nfrom docker_execute import DockerExecute\nfrom clean_code import clean_code\n\n# Set up logging\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nbad_code = \"\"\"\nSure!  Here's the code you requested:\n\n```python\n#import math oops we forgot the math library\n\ndef add(a,     b):\n    temp = {\n        \"a\": a,\n        \"b\": b\n    }\n    return a + b\n\ndef subtract(a, b\nreturn a - b\n\ndef multiply(a, \\\\\n             b)\n    return a * b\n\ndef divide(a,\n           b):\n    return a / b\n\n// Oh no the dumb AI typed something weird in here\ndef dostuff():\n    x = 5\n    y = 2\n\n    print(f\"{x} + {y} = {add(x, y)}\")\n    print(f\"{x} - {y} = {subtract(x, y)}\")\n    print(f\"{x} * {y} = {multiply(x, y)}\")\n    print(f\"{x} / {y} = {divide(x, y)}\")\n\n    # Calculate the area of a circle\n    radius = 3\n    area = math.pi * (radius ** 2)\n    print(f\"The area of a circle with radius {radius} is {area:.2f}\")\n\n    # Calculate the square root of a number\n    number = 49\n    square_root = math.sqrt(number)\n    print(f\"The square root of {number} is {square_root}\")\n\n    # Find the maximum value in a list\n    numbers = [34, 56, 12, 89, 23, 7, 91]\n    max_number = max(numbers)\n    print(f\"The maximum value in the list is {max_number}\")\n\n    # Check if a number is prime\n        num = 7\n    is_prime = True\n    for i in range(2, num):\n        if num % i == 0:\n                is_prime = False\n            break\n\n    if is_prime:\n        print(f\"{num} is a prime number\")\n    else:\n        print(f\"{num} is not a prime number\")\n\ndef main():\n    dostuff()\n\nif __name__ == \"__main__\":\n    main()\n\nmain()  # This is a comment\n```\n\nThis code does a bunch of stuff enjoy!\n\n\"\"\"\n\n\n\n\ndef main():\n    code, success = clean_code(bad_code)\n\n    if not success:\n        print(\"Code cleaning failed!\")\n        return\n\n    print(f\"Cleaned code: {code}\")\n\n    # Add this call back into the code\n    code += \"\\ndostuff()\"\n\n    # Test script file name to execute in the Docker container\n    sources_dirname = \"test_sources\"\n    test_script = \"test_script.py\"\n    expected_output = \"\"\"5 + 2 = 7\n5 - 2 = 3\n5 * 2 = 10\n5 / 2 = 2.5\nThe area of a circle with radius 3 is 28.27\nThe square root of 49 is 7.0\nThe maximum value in the list is 91\n7 is a prime number\"\"\"\n\n    script_path = os.path.join(os.getcwd(), sources_dirname, test_script)\n    os.makedirs(os.path.join(os.getcwd(), sources_dirname), exist_ok=True)\n\n    # Write a simple test script to execute in the container\n    with open(script_path, \"w\") as f:\n        f.write(code)\n\n    docker_execute = DockerExecute(sources_dirname=sources_dirname)\n\n    # Prime the pump\n    exit_code, output = docker_execute.execute(test_script)\n    assert output == expected_output, f\"Unexpected output: {output}\"\n    assert exit_code == 0, f\"Unexpected exit code: {exit_code}\"\n\n    # Remove the test script file\n    os.remove(script_path)\n\n    print(\"Success!  Script now runs and prints the expected output.\")\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "codegen/test_find_first_number.py", "content": "import unittest\nfrom autopy import find_first_number_between_0_and_1\n\nclass TestFindFirstNumberBetween0And1(unittest.TestCase):\n\n    def test_corner_cases(self):\n        self.assertEqual(find_first_number_between_0_and_1(\"0\"), 0)\n        self.assertEqual(find_first_number_between_0_and_1(\"1\"), 1)\n        self.assertEqual(find_first_number_between_0_and_1(\"0.0\"), 0.0)\n        self.assertEqual(find_first_number_between_0_and_1(\"1.0\"), 1.0)\n        self.assertEqual(find_first_number_between_0_and_1(\"1.00\"), 1.0)\n        self.assertIsNone(find_first_number_between_0_and_1(\"1.1\"))\n\n    def test_whitespace_and_punctuation(self):\n        self.assertEqual(find_first_number_between_0_and_1(\" 0 \"), 0)\n        self.assertEqual(find_first_number_between_0_and_1(\"is 0.\"), 0)\n        self.assertEqual(find_first_number_between_0_and_1(\"is 1.\"), 1)\n        self.assertEqual(find_first_number_between_0_and_1(\"is 0.75.\"), 0.75)\n        self.assertEqual(find_first_number_between_0_and_1(\",0,\"), 0)\n        self.assertEqual(find_first_number_between_0_and_1(\"(1)\"), 1)\n\n    def test_embedded_numbers(self):\n        self.assertEqual(find_first_number_between_0_and_1(\"abc0.5def\"), 0.5)\n        self.assertEqual(find_first_number_between_0_and_1(\"abc1.0def\"), 1.0)\n        self.assertEqual(find_first_number_between_0_and_1(\"abc0.0def\"), 0.0)\n\n    def test_multiple_numbers(self):\n        self.assertEqual(find_first_number_between_0_and_1(\"0.4 0.7 1.0\"), 0.4)\n        self.assertEqual(find_first_number_between_0_and_1(\"0.45 0.7 0.1\"), 0.45)\n        self.assertEqual(find_first_number_between_0_and_1(\"1.0 0.7 0.4\"), 1.0)\n\n    def test_negative_numbers(self):\n        self.assertEqual(find_first_number_between_0_and_1(\"-0.5 0.6\"), 0.6)\n        self.assertEqual(find_first_number_between_0_and_1(\"-1.0 1.0\"), 1.0)\n        self.assertIsNone(find_first_number_between_0_and_1(\"-1.0 -0.1\"))\n\n    def test_no_match(self):\n        self.assertIsNone(find_first_number_between_0_and_1(\"2 3 4\"))\n        self.assertIsNone(find_first_number_between_0_and_1(\"abc\"))\n        self.assertIsNone(find_first_number_between_0_and_1(\"\"))\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "codegen/test_extract_code_from_md.py", "content": "import unittest\n\nfrom extract_code_from_md import extract_code_from_md\n\nclass TestExtractCodeFromMd(unittest.TestCase):\n    def test_extract_code_from_md(self):\n        input_text = \"\"\"\n        # Example Markdown Document\n\n        This is an example Markdown document.\n\n```python\ndef add(x, y):\n    return x + y\n```\n\n        Here is some text that follows the code block.\n\n```python\ndef multiply(x, y):\n    return x * y\n```\n\"\"\"\n\n        expected_output = \"\"\"\ndef add(x, y):\n    return x + y\n\"\"\"\n        self.assertEqual(extract_code_from_md(input_text), expected_output.strip(), msg=f\"Input:\\n{input_text}\\n\\nExpected:\\n{expected_output.strip()}\\n\\nActual:\\n{extract_code_from_md(input_text)}\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "airate/tests/fact.py", "content": "def recursive_factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * recursive_factorial(n)\n\n# Example usage\nn = 5\nresult = recursive_factorial(n)\nprint(f\"The factorial of {n} is {result}\")\n"}
{"type": "test_file", "path": "codegen/test_mismatched_delimiters.py", "content": "import unittest\nimport ast\nfrom fix_ast_errors import fix_ast_errors\n\nclass TestFixMismatchedDelimiters(unittest.TestCase):\n    def check_syntax(self, code_string):\n        try:\n            ast.parse(code_string)\n            return True\n        except SyntaxError:\n            return False\n\n    def run_test_cases(self, test_cases, test_type):\n        for i, (input_str, expected_output) in enumerate(test_cases):\n            with self.subTest(input=input_str, expected=expected_output, test_type=test_type):\n                fixed_code = fix_ast_errors(input_str)\n                self.assertEqual(fixed_code, expected_output, msg=f\"\\n\\nFailed test {i} in {test_type}. Expected output:\\n\\n{expected_output}\\n\\nGot:\\n\\n{fixed_code}\\n\\n\")\n                self.assertTrue(self.check_syntax(fixed_code), msg=f\"\\n\\nFailed test {i} in {test_type}. Syntax checker found a problem with the output: {fixed_code}\")\n\n    def test_basic_cases(self):\n        test_cases = [\n            (\"print(x[1, 2, 3]\", \"print(x[1, 2, 3])\"),\n            (\"if x == 2: {print(x)\", \"if x == 2: {print(x)}\"),\n            (\"print(x}\", \"print(x)\"),\n            (\"{print(x[1, 2, 3]\", \"{print(x[1, 2, 3])}\"),\n        ]\n\n        self.run_test_cases(test_cases, \"Basic\")\n\n    def test_nested_delimiters(self):\n        test_cases = [\n            (\"print(x[1, 2, [3, 4], 5]\", \"print(x[1, 2, [3, 4], 5])\"),\n            (\"print({1, 2, {3, 4}, 5]\", \"print({1, 2, {3, 4}, 5})\"),\n            (\"print(x(1, 2, (3, 4), 5)\", \"print(x(1, 2, (3, 4), 5))\"),\n            (\"{[(print(1))]}\", \"{[(print(1))]}\"),\n        ]\n\n        self.run_test_cases(test_cases, \"Nested Delimiters\")\n\n    def test_comments_and_strings(self):\n        test_cases = [\n            ('print(\"hello) world\")', 'print(\"hello) world\")'),\n            ('print(\"hello[ world\")', 'print(\"hello[ world\")'),\n            (\"# Some comment (unclosed\", \"# Some comment (unclosed\"),\n            (\"'''Triple quotes { } ( ) [ ]'''\", \"'''Triple quotes { } ( ) [ ]'''\"),\n        ]\n\n        self.run_test_cases(test_cases, \"Comments and strings\")\n\n    def test_multiline_cases(self):\n        test_cases = [\n            (\n                \"if x > 2:\\n    print(x[1, 2, 3]\\nelse:\\n    print('Error')\",\n                \"if x > 2:\\n    print(x[1, 2, 3])\\nelse:\\n    print('Error')\"\n            ),\n            (\n                \"def func(a, b):\\n    return a * (b - 1\\n\\nprint(func(2, 3)\",\n                \"def func(a, b):\\n    return a * (b - 1)\\n\\nprint(func(2, 3))\"\n            ),\n            (\n                \"try:\\n    x = {'key': 'value'}\\n    print(x['key'\\nexcept KeyError:\\n    print('Key not found')\",\n                \"try:\\n    x = {'key': 'value'}\\n    print(x['key'])\\nexcept KeyError:\\n    print('Key not found')\"\n            ),\n            (\n                \"for i in range(5):\\n    if i % 2 == 0:\\n        print('{Even}')\",\n                \"for i in range(5):\\n    if i % 2 == 0:\\n        print('{Even}')\"\n            ),\n            (\n                \"if x > 2:\\n    print(x[1, 2, 3]\\nelse:\\n    print('Error')\",\n                \"if x > 2:\\n    print(x[1, 2, 3])\\nelse:\\n    print('Error')\"\n            ),\n            (\n                \"if x == 2:\\n    print(\\\"Hello, World!\\\"\\nelse:\\n    print(\\\"Goodbye, World!\\\")\",\n                \"if x == 2:\\n    print(\\\"Hello, World!\\\")\\nelse:\\n    print(\\\"Goodbye, World!\\\")\"\n            ),\n            (\n                \"temp = {\\n    \\\"a\\\": a,\\n    \\\"b\\\": b\\n}\",\n                \"temp = {\\n    \\\"a\\\": a,\\n    \\\"b\\\": b\\n}\"\n            ),\n            (\n                \"def multiply(a, \\\\\\n             b)\\n    return a * b\",\n                \"def multiply(a, \\\\\\n             b):\\n    return a * b\"\n            ),\n            (\n                \"def divide(a,\\n           b):\\n    return a / b\",\n                \"def divide(a,\\n           b):\\n    return a / b\"\n            ),\n        ]\n\n        self.run_test_cases(test_cases, \"Multi-line Cases\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "server/test_client.py", "content": "# Use modules from parent folder\nimport os, sys\nparent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.append(parent_dir)\n\nimport argparse\n\nfrom prompts.ask_templates import ask_assistant, ask_python_coder\nfrom client import ask_server\n\ndef main(args):\n    messages = [\n        {\n            \"role\": \"Human\",\n            \"content\": \"Why is the sky blue?\"\n        }\n    ]\n\n    prompt, stop_strs = ask_assistant(messages)\n    print(f\"\\n\\nTesting ask_assistant: {prompt}\")\n    print(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    r = ask_server(\n        prompt=prompt,\n        stop_strs=stop_strs,\n        node=args.node,\n        port=args.port,\n        temperature=args.temperature,\n        max_tokens=args.max_tokens)\n    print(r)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Unit test client for LLM model requests\")\n    parser.add_argument(\"--node\", type=str, default=\"localhost\", help=\"Server port\")\n    parser.add_argument(\"--port\", type=int, default=5000, help=\"Server port\")\n    parser.add_argument(\"--temperature\", type=float, help=\"Temperature for text generation (default: 0.7)\", default=0.7)\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum number of tokens in generated text (default: 1024)\", default=1024)\n\n    args = parser.parse_args()\n\n    main(args)\n"}
{"type": "test_file", "path": "server/test_model.py", "content": "# Use modules from parent folder\nimport os, sys\nparent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.append(parent_dir)\n\nimport time\nimport logging\nimport argparse\nfrom language_model import LanguageModel\nfrom prompts.ask_templates import ask_assistant, ask_python_coder\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_model(m, prompt, stop_strs):\n    start_time = time.time()\n    r = m.ask(\n        prompt=prompt,\n        stop_strs=stop_strs,\n        temperature=args.temperature,\n        max_new_tokens=args.max_tokens)\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    response_length = len(r)\n    chars_per_second = response_length / elapsed_time\n\n    logging.info(f\"Response:\\n\\n{r}\")\n    logging.info(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n    logging.info(f\"Characters per second: {chars_per_second:.2f}\")\n\n    return r, elapsed_time\n\ndef main(args):\n    m = LanguageModel(args.model, load_in_8bit=args.load_in_8bit)\n\n    # Test ask_assistant\n\n    messages = [\n        {\n            \"role\": \"Human\",\n            \"content\": \"What is the best way to make money with a 100W laser cutter?\"\n        }\n    ]\n\n    prompt, stop_strs = ask_assistant(messages)\n    logging.info(f\"Testing ask_assistant:\\n\\n{prompt}\")\n    logging.info(f\"Early stop strings: {stop_strs}\")\n\n    r, elapsed_time = test_model(m, prompt, stop_strs)\n\n\n    # Test ask_python_coder\n\n    messages = [\n        {\n            \"role\": \"Human\",\n            \"content\": \"Write a Python function that returns the factorial of a number.\"\n        }\n    ]\n\n    prompt, stop_strs = ask_python_coder(messages)\n    logging.info(f\"\\n\\nTesting ask_python_coder: {prompt}\")\n    logging.info(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    r, elapsed_time = test_model(m, prompt, stop_strs)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Unit test for language model\")\n    parser.add_argument(\"--temperature\", type=float, help=\"Temperature for text generation (default: 0.7)\", default=0.7)\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum number of tokens in generated text (default: 1024)\", default=1024)\n    parser.add_argument(\"--model\", type=str, help=\"Select model to use (default: galpaca-30b). Available options: baize-30b, baize-13b, baize-7b, galpaca-30b, galpaca-7b, koala-13b, koala-7b, vicuna-13b, vicuna-7b, llama-65b-4bit\", default=\"llama-65b-4bit\")\n    parser.add_argument(\"--8bit\", action=\"store_true\", help=\"Use 8-bit precision (default: False)\")\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Use 16-bit precision (default: False)\")\n\n    args = parser.parse_args()\n\n    if getattr(args, \"8bit\"):\n        logging.info(\"8-bit precision selected.\")\n        args.load_in_8bit = True\n    elif args.fp16:\n        logging.info(\"16-bit precision selected.\")\n        args.load_in_8bit = False\n    else:\n        args.load_in_8bit = None\n\n    main(args)\n"}
{"type": "test_file", "path": "airate/tests/gcd.py", "content": "def gcd_lcm(a, b):\n    def euclidean_algorithm(x, y):\n        while y != 0:\n            x, y = y, x % y\n        return x\n\n    if a == 0 or b == 0:\n        raise ValueError(\"Both input numbers must be non-zero\")\n\n    gcd = euclidean_algorithm(abs(a), abs(b))\n    lcm = abs(a * b) // gcd\n\n    return gcd, lcm\n\n# Example usage\na = 56\nb = 48\nresult_gcd, result_lcm = gcd_lcm(a, b)\nprint(f\"The GCD of {a} and {b} is {result_gcd}\")\nprint(f\"The LCM of {a} and {b} is {result_lcm}\")\n"}
{"type": "test_file", "path": "codegen/test_missing_colons.py", "content": "import ast\nimport unittest\nfrom fix_ast_errors import fix_ast_errors\n\nclass TestAddMissingColons(unittest.TestCase):\n    def check_syntax(self, code_string):\n        try:\n            ast.parse(code_string)\n            return True\n        except SyntaxError:\n            return False\n\n    def run_test_cases(self, test_cases, test_type, syntax_check=True):\n        for i, (input_str, expected_output) in enumerate(test_cases):\n            with self.subTest(input=input_str, expected=expected_output, test_type=test_type):\n                fixed_code = fix_ast_errors(input_str)\n                self.assertEqual(fixed_code, expected_output, msg=f\"\\n\\nFailed test {i} in {test_type}. Expected output:\\n\\n{expected_output}\\n\\nGot:\\n\\n{fixed_code}\\n\\n\")\n                if syntax_check:\n                    self.assertTrue(self.check_syntax(fixed_code), msg=f\"\\n\\nFailed test {i} in {test_type}. Syntax checker found a problem with the output: {fixed_code}\")\n\n    def test_basic_cases(self):\n        test_cases = [\n            (\"def foo(x)\\n    pass\", \"def foo(x):\\n    pass\"),\n            (\"if x > 0\\n    pass\\nelif x < 0\\n    pass\", \"if x > 0:\\n    pass\\nelif x < 0:\\n    pass\"),\n            (\"if x > 0\\n    pass\\nelse\\n    pass\", \"if x > 0:\\n    pass\\nelse:\\n    pass\"),\n            (\"for i in range(10)\\n    pass\", \"for i in range(10):\\n    pass\"),\n            (\"while True\\n    pass\", \"while True:\\n    pass\"),\n            (\"with open('file.txt') as f\\n    pass\", \"with open('file.txt') as f:\\n    pass\"),\n            (\"class MyClass\\n    def f():\\n        pass\", \"class MyClass:\\n    def f():\\n        pass\"),\n            (\"print(f\\\"The area of a circle with radius {radius} is {area:.2f}\\\")\", \"print(f\\\"The area of a circle with radius {radius} is {area:.2f}\\\")\")\n        ]\n\n        self.run_test_cases(test_cases, \"Basic\", syntax_check=False)\n\n    def test_single_line(self):\n        test_cases = [\n            (\"x = 1\\n\", \"x = 1\\n\"),\n            (\"def func(x):\\n    pass\\n\", \"def func(x):\\n    pass\\n\"),\n            (\"if x == 2\\n    print(x)\\n\", \"if x == 2:\\n    print(x)\\n\"),\n            (\"for i in range(10)\\n    print(i)\\n\", \"for i in range(10):\\n    print(i)\\n\"),\n            (\"while x < 10\\n    x += 1\\n\", \"while x < 10:\\n    x += 1\\n\"),\n        ]\n\n        self.run_test_cases(test_cases, \"Single-Line\")\n\n    def test_multi_line(self):\n        test_cases = [\n            (\"if x == 2\\n    print(x)\\nelse:\\n    print('Error')\\n\", \"if x == 2:\\n    print(x)\\nelse:\\n    print('Error')\\n\"),\n            ('if x > 0: print(\"x is positive\")\\nif y < 0: print(\"y is negative\")\\n', 'if x > 0: print(\"x is positive\")\\nif y < 0: print(\"y is negative\")\\n'),\n            (\"for i in range(10)\\n    if i % 2 == 0:\\n        print(i)\\n\", \"for i in range(10):\\n    if i % 2 == 0:\\n        print(i)\\n\"),\n            (\"with open('file.txt') as f\\n    data = f.read()\\n    print(data)\\n\", \"with open('file.txt') as f:\\n    data = f.read()\\n    print(data)\\n\")\n        ]\n\n        self.run_test_cases(test_cases, \"Multi-Line\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "codegen/test_runner.py", "content": "import os\nimport unittest\n\nif __name__ == '__main__':\n    loader = unittest.TestLoader()\n    script_path = os.path.dirname(os.path.abspath(__file__))\n    suite = loader.discover(script_path)\n    runner = unittest.TextTestRunner(verbosity=2)\n    runner.run(suite)\n"}
{"type": "source_file", "path": "airate/airate_java.py", "content": "import re\nfrom oracle import java_oracle\n\ndef extract_methods(file_path):\n    \"\"\"\n    Extracts all methods in a given Java file.\n    Returns a dictionary with method names as keys and method code blocks as values.\n    \"\"\"\n    with open(file_path) as f:\n        code = f.read()\n\n    methods = {}\n    pattern = re.compile(r'((public|private|protected|static|final|synchronized|native|\\s)*[\\w\\<\\>\\[\\]]+\\s+(\\w+)\\s*\\(.*?\\)\\s*(throws\\s+\\w+\\s*)?\\{[^{}]*\\})', re.DOTALL)\n    for match in pattern.findall(code):\n        method = match[0].strip()\n        method_name = match[2]\n        methods[method_name] = method\n\n    return methods\n\ndef airate_java(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_methods(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = java_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```java\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "airate/airate_cpp.py", "content": "import sys\n\nimport clang.cindex\nimport ctypes.util\n\nfrom oracle import cpp_oracle\n\nalready_setup = False\n\ndef setup_clang():\n    # Only do it once\n    global already_setup\n    if already_setup:\n        return\n    already_setup = True\n\n    # Find the Clang library file using ctypes.util.find_library()\n    libclang_path = ctypes.util.find_library('clang')\n    if not libclang_path:\n        raise RuntimeError('Failed to find Clang library')\n\n    # Set the path to the Clang library\n    clang.cindex.Config.set_library_file(libclang_path)\n\ndef functions_in_file(cursor, file_path):\n    if cursor.kind == clang.cindex.CursorKind.CXX_METHOD:\n        if cursor.location.file and cursor.location.file.name == file_path:\n            has_body = any(child.kind == clang.cindex.CursorKind.COMPOUND_STMT for child in cursor.get_children())\n            if has_body:\n                yield cursor\n\n    if cursor.kind == clang.cindex.CursorKind.FUNCTION_DECL:\n        if cursor.location.file and cursor.location.file.name == file_path:\n            yield cursor\n\n    for child in cursor.get_children():\n        yield from functions_in_file(child, file_path)\n\ndef function_source(node, file_contents):\n    extent = node.extent\n    start, end = extent.start, extent.end\n    func_src = file_contents[start.offset:end.offset]\n\n    # Find comments leading up to the function\n    lines = file_contents[:start.offset].splitlines()\n    comments = []\n    for line in reversed(lines):\n        stripped = line.strip()\n        if stripped.startswith(\"//\") or stripped.startswith(\"/*\") or stripped.endswith(\"*/\"):\n            comments.append(line)\n        else:\n            break\n\n    comments.reverse()\n    comment_str = \"\\n\".join(comments)\n\n    return f\"{comment_str}\\n{func_src}\" if comments else func_src\n\ndef extract_functions(file_path):\n    setup_clang()\n\n    index = clang.cindex.Index.create()\n    tu = index.parse(file_path)\n\n    with open(file_path, 'r') as f:\n        file_contents = f.read()\n\n    code_blocks = []\n    for func in functions_in_file(tu.cursor, file_path):\n        code_blocks.append(function_source(func, file_contents))\n\n    return code_blocks\n\ndef airate_cpp(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = cpp_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```cpp\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "codegen/autopy.py", "content": "# Use modules from parent folder\nimport os, sys\nparent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.append(parent_dir)\n\nimport re\nimport logging\n\nfrom server.client import ask_server\nfrom prompts.ask_templates import ask_python_pytest_prototype, ask_python_function_prototype, ask_python_analyzer, ask_python_test_analyzer, ask_python_test_judge, ask_python_code_judge\nfrom clean_code import clean_code\n\ndef autopy_func(comments, prototype, node=\"localhost\", port=5000, temperature=1.0, max_tokens=1024):\n    #logging.info(f\"autopy_func: comments = `{comments}`\")\n    #logging.info(f\"autopy_func: prototype = `{prototype}`\")\n\n    prompt, stop_strs = ask_python_function_prototype(comments, prototype)\n\n    #logging.info(f\"autopy_func: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_func: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_func: result = \\n{result}\")\n\n    code, _ = clean_code(result, strip_leading_comments=True)\n\n    #logging.info(f\"autopy_func: code = \\n{code}\")\n\n    if len(code.strip()) > 0:\n        # Prepend the comments\n        code = '\\n'.join(comments.splitlines() + code.splitlines())\n\n    return code\n\ndef autopy_func_improve(comments, code, node=\"localhost\", port=5000, temperature=1.0, max_tokens=1024):\n    #logging.info(f\"autopy_func_improve: comments = `{comments}`, prototype = `{code}`\")\n\n    prompt, stop_strs = ask_python_analyzer(code)\n\n    #logging.info(f\"autopy_func_improve: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_func_improve: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_func_improve: result = \\n{result}\")\n\n    code, _ = clean_code(result, strip_leading_comments=True)\n\n    #logging.info(f\"autopy_func_improve: code = \\n{code}\")\n\n    if len(code.strip()) > 0:\n        # Prepend the comments\n        code = '\\n'.join(comments.splitlines() + code.splitlines())\n\n    return code\n\ndef autopy_test(comments, prototype, function_name, node=\"localhost\", port=5000, temperature=1.0, max_tokens=1024):\n    #logging.info(f\"autopy_test: comments = `{comments}`, prototype = `{prototype}`\")\n\n    prompt, stop_strs = ask_python_pytest_prototype(comments, prototype)\n\n    #logging.info(f\"autopy_test: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_test: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_test: result = \\n{result}\")\n\n    code, _ = clean_code(result, strip_import_mods=[\"pytest\"], strip_import_funcs=[function_name])\n\n    #logging.debug(f\"autopy_test: code = \\n{code}\")\n\n    if len(code.strip()) > 0:\n        # Prepend the required imports\n        required_imports = [\"import pytest\", f\"from {function_name} import {function_name}\"]\n        code = '\\n'.join(required_imports + code.splitlines())\n\n    return code\n\ndef autopy_test_improve(comments, prototype, function_name, test_code, node=\"localhost\", port=5000, temperature=1.0, max_tokens=1024):\n    #logging.info(f\"autopy_test_improve: comments = `{comments}`, prototype = `{prototype}`\")\n\n    prompt, stop_strs = ask_python_test_analyzer(comments, prototype, function_name, test_code)\n\n    #logging.info(f\"autopy_test_improve: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_test_improve: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_test_improve: result = \\n{result}\")\n\n    code, _ = clean_code(result, strip_import_mods=[\"pytest\"], strip_import_funcs=[function_name])\n\n    #logging.info(f\"autopy_test_improve: code = \\n{code}\")\n\n    if len(code.strip()) > 0:\n        # Prepend the required imports\n        required_imports = [\"import pytest\", f\"from {function_name} import {function_name}\"]\n        code = '\\n'.join(required_imports + code.splitlines())\n\n    return code\n\ndef find_first_number_between_0_and_1(s: str):\n    pattern = r'(?<![0-9.])0(\\.\\d+)?(?!\\d)|(?<![0-9.])1(\\.\\d+)?(?!\\d)'\n    matches = re.finditer(pattern, s)\n    \n    for match in matches:\n        start_index = match.start()\n        if start_index == 0 or s[start_index - 1] != '-':\n            f = float(match.group())\n            if f >= 0 and f <= 1:\n                return f\n\n    return None\n\ndef autopy_code_judge(commented_code, function_name, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    #logging.info(f\"autopy_code_judge: comments = `{comments}`, prototype = `{prototype}`\")\n\n    prompt, stop_strs = ask_python_code_judge(commented_code, function_name)\n\n    #logging.info(f\"autopy_code_judge: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_code_judge: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_code_judge: result = \\n{result}\")\n\n    return find_first_number_between_0_and_1(result)\n\ndef autopy_test_judge(commented_code, function_name, test_code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    #logging.info(f\"autopy_test_judge: comments = `{comments}`, prototype = `{prototype}`\")\n\n    prompt, stop_strs = ask_python_test_judge(commented_code, function_name, test_code)\n\n    #logging.info(f\"autopy_test_judge: prompt = \\n{prompt}\")\n    #logging.info(f\"autopy_test_judge: stop_strs = {stop_strs}\")\n\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n\n    #logging.info(f\"autopy_test_judge: result = \\n{result}\")\n\n    return find_first_number_between_0_and_1(result)\n"}
{"type": "source_file", "path": "airate/oracle.py", "content": "# Use modules from parent folder\nimport os, sys\nparent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.append(parent_dir)\n\nimport re\n\nfrom server.client import ask_server\nfrom prompts.ask_templates import ask_cpp_expert_score, ask_python_expert_score, ask_cs_expert_score, ask_js_expert_score, ask_java_expert_score, ask_ts_expert_score, ask_php_expert_score\n\ndef find_first_number_between_0_and_1(s: str):\n    pattern = r'(?<![0-9.])0(\\.\\d+)?(?!\\d)|(?<![0-9.])1(\\.\\d+)?(?!\\d)'\n    matches = re.finditer(pattern, s)\n    \n    for match in matches:\n        start_index = match.start()\n        if start_index == 0 or s[start_index - 1] != '-':\n            f = float(match.group())\n            if f >= 0 and f <= 1:\n                return f\n\n    return None\n\ndef cpp_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_cpp_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef py_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_python_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef cs_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_cs_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef js_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_js_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef java_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_java_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef ts_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_ts_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n\ndef php_oracle(code, node=\"localhost\", port=5000, temperature=0.0, max_tokens=4):\n    prompt, stop_strs = ask_php_expert_score(code)\n    result = ask_server(prompt, stop_strs, node, port, temperature, max_tokens)\n    return find_first_number_between_0_and_1(result)\n"}
{"type": "source_file", "path": "airate/airate_php.py", "content": "import re\nfrom oracle import php_oracle\n\ndef extract_functions(file_contents: str):\n    function_regex = r\"function\\s+[a-zA-Z_][a-zA-Z0-9_]*\\s*\\(\"\n    functions = re.findall(function_regex, file_contents)\n    return functions\n\ndef airate_php(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = php_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```php\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "airate/airate.py", "content": "import argparse\nimport os\nfrom pathlib import Path\n\n# Import the airate modules for each file type\nimport airate_cpp\nimport airate_py\nimport airate_js\nimport airate_ts\nimport airate_java\nimport airate_cs\nimport airate_php\n\nfile_handlers = {\n    \".cpp\": airate_cpp.airate_cpp,\n    #\".py\": airate_py.airate_py,\n    #\".js\": airate_js.airate_js,\n    #\".ts\": airate_ts.airate_ts,\n    #\".java\": airate_java.airate_java,\n    #\".cs\": airate_cs.airate_cs,\n    #\".php\": airate_php.airate_php,\n}\n\ndef rate_file(args, file_path, depth):\n    markdown_str = \"\"\n    file_ext = Path(file_path).suffix\n\n    if file_ext in file_handlers:\n        handler = file_handlers[file_ext]\n        markdown_str += f\"{'  ' * depth}* {file_path}\\n\"\n        markdown_str += handler(file_path, args.node, args.port)\n\n    return markdown_str\n\ndef process_directory(args, path, depth=0):\n    markdown_str = \"\"\n    for entry in os.scandir(path):\n        if entry.is_file():\n            markdown_str += rate_file(args, entry.path, depth)\n        elif entry.is_dir():\n            markdown_str += f\"{'  ' * depth}* {entry.name}/\\n\"\n            markdown_str += process_directory(entry.path, depth + 1)\n\n    return markdown_str\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Recursively process files in a directory or a single file\")\n    parser.add_argument(\"path\", help=\"Path to the directory or file\")\n    parser.add_argument(\"--node\", type=str, default=\"localhost\", help=\"Server port\")\n    parser.add_argument(\"--port\", type=int, default=5000, help=\"Server port\")\n\n    args = parser.parse_args()\n    path = args.path\n\n    if os.path.isfile(path):\n        print(rate_file(path, 0))\n    elif os.path.isdir(path):\n        print(process_directory(args, path))\n    else:\n        print(f\"Error: {path} is not a valid file or directory\")\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "airate/airate_cs.py", "content": "import re\nfrom oracle import cs_oracle\n\ndef extract_functions_and_classes(file_path):\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    # Match functions and short class definitions\n    pattern = re.compile(r'\\b(?:class|void|bool|int|float|double|char|short|long|string)\\s+\\w+\\s*'\n                         r'(?:<[^>]*>)?\\s*\\(.*?\\)\\s*\\{[^{}]*?\\}|'\n                         r'\\bclass\\s+\\w+\\s*(?:[:]\\s*[^{]*?)?\\s*\\{[^{}]*?\\}', re.MULTILINE | re.DOTALL)\n\n    return pattern.findall(content)\n\ndef airate_cs(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions_and_classes(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = cs_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```csharp\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "airate/airate_py.py", "content": "import ast\nfrom oracle import py_oracle\n\ndef extract_functions_and_classes(file_path):\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    try:\n        tree = ast.parse(content)\n    except SyntaxError:\n        print(f\"Error: Failed to parse {file_path}\")\n        return []\n\n    functions_and_classes = []\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            start_line = node.lineno - 1\n            end_line = max([item.lineno for item in ast.walk(node) if isinstance(item, ast.Expr)], default=start_line)\n            functions_and_classes.append('\\n'.join(content.splitlines()[start_line:end_line + 1]))\n\n    return functions_and_classes\n\ndef airate_py(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions_and_classes(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = py_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```python\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "airate/airate_ts.py", "content": "import re\nfrom oracle import ts_oracle\n\ndef extract_functions_and_classes(file_path: str) -> list[str]:\n    with open(file_path, \"r\") as file:\n        code = file.read()\n    pattern = r\"((async\\s+)?function\\s+\\w+\\s*\\(.*?\\)\\s*\\{(?:[^{}]*?\\{[^{}]*?\\}[^{}]*?)*?\\}|class\\s+\\w+\\s*\\{[\\s\\S]*?\\})\"\n    matches = re.findall(pattern, code, re.MULTILINE)\n    return matches\n\ndef airate_ts(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions_and_classes(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = ts_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```typescript\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n        print(f\"Code block {idx + 1} score: {score:.2f}\")\n\n    return markdown_str\n"}
{"type": "source_file", "path": "airate/airate_js.py", "content": "import re\nfrom oracle import js_oracle\n\ndef extract_functions_and_classes(file_path):\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    # Match functions and short class definitions\n    pattern = re.compile(r'(?:(?:async\\s+)?function\\s+\\w+\\s*\\(.*?\\)\\s*\\{(?:[^{}]*?\\{[^{}]*?\\}[^{}]*?)*?\\}|'\n                        r'(?:class\\s+\\w+\\s*(?:extends\\s+\\w+)?\\s*\\{(?:[^{}]*?\\{[^{}]*?\\}[^{}]*?)*?\\}))',\n                        re.MULTILINE | re.DOTALL)\n\n    return pattern.findall(content)\n\ndef airate_js(file_path, node=\"localhost\", port=5000):\n    markdown_str = \"\"\n    code_blocks = extract_functions_and_classes(file_path)\n\n    for idx, code_block in enumerate(code_blocks):\n        score = js_oracle(code_block, node=node, port=port)\n        markdown_str += f\"\\n  - Code block {idx + 1}:\\n\"\n        markdown_str += f\"    ```javascript\\n{code_block.strip()}\\n    ```\\n\"\n        markdown_str += f\"    Score: {score:.2f}\\n\"\n\n    return markdown_str\n"}
{"type": "source_file", "path": "codegen/docker_execute.py", "content": "import os\nimport signal\nimport logging\n\nimport docker\n\nclass DockerExecute:\n    def __init__(self, image=\"python:3.10\", sources_dirname=\"sources\"):\n        self.image = image\n        self.client = docker.from_env()\n        self.sources_dirname = sources_dirname\n        self.full_source_path = os.path.join(os.getcwd(), sources_dirname)\n        self.container = None\n\n    def recreate_container(self):\n        self.shutdown()\n\n        # Pull the Python image\n        self.client.images.pull(self.image)\n\n        # Create a container and run an infinite loop to keep it alive\n        self.container = self.client.containers.run(\n            self.image,\n            command=\"bash -c 'while true; do sleep 1; done'\",\n            volumes={\n                self.full_source_path: {\n                    'bind': f\"/{self.sources_dirname}\",\n                    'mode': 'rw'\n                    }\n                },\n            working_dir=f\"/{self.sources_dirname}\",\n            stderr=True,\n            stdout=True,\n            detach=True,\n        )\n\n    def shutdown(self):\n        if not self.container is None:\n            self.container.stop()\n            self.container.remove()\n            self.container = None\n\n    # Execute a command under `sources` that depends on the given script.  By default it runs the given script.\n    def execute(self, script_filename=None, command=None, timeout=10):\n        try:\n            if self.container is None:\n                self.recreate_container()\n\n            # Define a signal handler for the timeout\n            def handler(signum, frame):\n                raise TimeoutError(\"Execution timed out\")\n\n            # Set a signal alarm to raise a TimeoutError after 30 seconds\n            signal.signal(signal.SIGALRM, handler)\n            signal.alarm(timeout)\n\n            if command is None:\n                command = f\"python {script_filename}\"\n\n            # Run the test code in the existing container\n            exit_code, output = self.container.exec_run(\n                command,\n                workdir=f\"/{self.sources_dirname}\",\n            )\n\n            # Disable the signal alarm after the code has finished executing\n            signal.alarm(0)\n\n            logs = output.decode(\"utf-8\").strip()\n\n            return exit_code, logs\n\n        except Exception as e:\n            self.shutdown()\n            return -1, \"\"\n"}
{"type": "source_file", "path": "codegen/extract_code_from_md.py", "content": "# Remove everything outside the first ``` code block containing a function.\ndef extract_code_from_md(input_text):\n    output = []\n    code_block = False\n    found_def = False\n    for line in input_text.splitlines():\n        if line.strip().startswith(\"```\"):\n            if found_def:\n                break\n            code_block = not code_block\n            if code_block:\n                found_def = False\n            output.clear()\n        elif code_block:\n            output.append(line)\n            if 'def' in line:\n                found_def = True\n    return '\\n'.join(output)\n"}
{"type": "source_file", "path": "server/launch_cluster.py", "content": "import subprocess\nimport os\nimport sys\nimport threading\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef read_node_addresses(filename=\"load_balancer_nodes.txt\"):\n    with open(filename, 'r') as f:\n        lines = [line.strip() for line in f.readlines() if line.strip() and not line.startswith('#')]\n    return lines\n\ndef log_output(process, addr):\n    for line in iter(process.stdout.readline, \"\"):\n        logging.info(f\"[{addr}] {line.rstrip()}\")\n\ndef launch_servers(node_addresses, script_path):\n    processes = []\n    log_threads = []\n    for addr in node_addresses:\n        host, port = addr.split(':')\n        cmd = f\"pdsh -b -R ssh -w {host} {script_path} {port}\"\n        print(f\"Running command: {cmd}\")\n\n        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n        log_thread = threading.Thread(target=log_output, args=(process, addr))\n        log_thread.start()\n\n        processes.append(process)\n        log_threads.append(log_thread)\n\n    return processes, log_threads\n\ndef get_script_path():\n    script_path = os.path.abspath(sys.argv[0])\n    home_path = os.path.expanduser(\"~\")\n    \n    if script_path.startswith(home_path):\n        script_path = f\"~{script_path[len(home_path):]}\"\n        \n    return script_path\n\ndef replace_filename_with_run_server(path):\n    dir_path = os.path.dirname(path)\n    new_path = os.path.join(dir_path, \"_cluster_launch.sh\")\n    return new_path\n\ndef main():\n    node_addresses = read_node_addresses()\n    script_path = replace_filename_with_run_server(get_script_path())\n\n    try:\n        logging.info(\"Launching remote shells...\")\n        processes, log_threads = launch_servers(node_addresses, script_path)\n\n        logging.info(\"Waiting for termination...\")\n        for process in processes:\n            process.wait()\n        for log_thread in log_threads:\n            log_thread.join()\n\n        logging.info(\"Terminated...\")\n    except KeyboardInterrupt:\n        logging.info(\"\\nTerminating remote shells...\")\n        for process in processes:\n            process.terminate()\n\n    logging.info(\"Terminated.\")\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "server/client.py", "content": "import requests\nimport time\nimport logging\n\n# Use ask_templates.py to generate the prompt and stop_strs\ndef ask_server(prompt, stop_strs=None, node=\"localhost\", port=5000, temperature=1.0, max_tokens=100):\n    data = {\n        'prompt': prompt,\n        'stop_strs': stop_strs,\n        'temperature': temperature,\n        'max_tokens': max_tokens,\n    }\n\n    while True:\n        try:\n            response = requests.post(f\"http://{node}:{port}/ask\", json=data)\n            response_json = response.json()\n            break\n        except Exception as e:\n            logging.warn(f\"Server busy: {e}\")\n            time.sleep(1)\n\n    return response_json['response']\n"}
{"type": "source_file", "path": "codegen/codegen.py", "content": "import glob\nimport re\nimport os\nimport logging\nimport argparse\nimport shutil\nimport subprocess\nfrom queue import Empty\n\nfrom codegen_workers import JobManager\nfrom docker_execute import DockerExecute\n\nlogging.basicConfig(level=logging.INFO)\n\ndef comment_multiline_string(s: str) -> str:\n    lines = s.split(\"\\n\")\n    commented_lines = []\n    for line in lines:\n        stripped_line = line.lstrip()\n        whitespace = line[:len(line) - len(stripped_line)]\n        if stripped_line and not stripped_line.startswith(\"#\"):\n            commented_line = whitespace + \"# \" + stripped_line\n        else:\n            commented_line = line\n        commented_lines.append(commented_line)\n    return \"\\n\".join(commented_lines)\n\ndef ensure_colon_at_end(prototype: str) -> str:\n    if not prototype.endswith(':'):\n        prototype += ':'\n    return prototype\n\ndef extract_function_name(function_def):\n    # Split the function definition by whitespace\n    words = re.split(r'[^A-Za-z0-9_]|(?<![A-Za-z_])[0-9]+', function_def)\n\n    # Look for the \"def\" keyword and extract the function name\n    for i, word in enumerate(words):\n        if word == \"def\":\n            function_name = words[i+1]\n            # Strip any trailing parentheses or whitespace\n            function_name = function_name.rstrip(\"() \")\n            return function_name\n\n    raise Exception(\"No function name found\")\n\ndef get_script_name_from_function_name(function_name, is_test=False, variation=None):\n    if variation is None:\n        if is_test:\n            return f\"test_{function_name}.py\"\n        else:\n            return f\"{function_name}.py\"\n    else:\n        if is_test:\n            return f\"test_{function_name}_{variation}.py\"\n        else:\n            return f\"{function_name}_{variation}.py\"\n\ndef delete_old_scripts(sources_dirname, func_name):\n    dir_path = os.path.join(sources_dirname, func_name)\n    if os.path.exists(dir_path) and os.path.isdir(dir_path):\n        py_files = glob.glob(os.path.join(dir_path, \"*.py\"))\n\n        # Iterate through the list of .py files and delete them\n        for py_file in py_files:\n            try:\n                os.remove(py_file)\n                print(f\"Deleted {py_file}\")\n            except OSError as e:\n                print(f\"Error deleting {py_file}: {e}\")\n\ndef write_script_to_disk(code, sources_dirname, func_name, is_test=False, variation=0):\n    # Create the directory if it doesn't exist\n    dir_path = os.path.join(sources_dirname, func_name)\n    os.makedirs(dir_path, exist_ok=True)\n\n    # Write the script to the file\n    file_path = os.path.join(dir_path, get_script_name_from_function_name(func_name, is_test, variation))\n    with open(file_path, \"w\") as f:\n        f.write(code)\n\ndef copy_candidate_scripts(source_dir, function_name, code_id, test_id):\n    code_source_path = os.path.join(source_dir, function_name, get_script_name_from_function_name(function_name, is_test=False, variation=code_id))\n    code_dest_path = os.path.join(source_dir, function_name, get_script_name_from_function_name(function_name, is_test=False))\n\n    test_source_path = os.path.join(source_dir, function_name, get_script_name_from_function_name(function_name, is_test=True, variation=test_id))\n    test_dest_path = os.path.join(source_dir, function_name, get_script_name_from_function_name(function_name, is_test=True))\n\n    # Copy files\n    shutil.copyfile(code_source_path, code_dest_path)\n    shutil.copyfile(test_source_path, test_dest_path)\n\ndef copy_and_run_pytest(source_dir, function_name, code_id, test_id, executor):\n    copy_candidate_scripts(source_dir, function_name, code_id, test_id)\n\n    # Run pytest command\n    test_script_name = os.path.join(function_name, get_script_name_from_function_name(function_name, is_test=True))\n    command = f\"pytest {test_script_name}\"\n    exit_code, logs = executor.execute(script_filename=test_script_name, command=command, timeout=10)\n\n    return exit_code, logs\n\ndef generate_requirements(project_path, output_file='requirements.txt'):\n    result = subprocess.run(['pipreqs', project_path, '--force', '--savepath', output_file], capture_output=True, text=True)\n\n    if result.returncode == 0:\n        return True\n    else:\n        logging.info(f\"An error occurred while generating requirements.txt: {result}\")\n        return False\n\ndef install_container_requirements(sources_dirname, function_name, docker_execute):\n    #logging.info(\"Installing container requirements.txt...\")\n\n    project_path = os.path.join(sources_dirname, function_name)\n    success = generate_requirements(project_path, output_file=os.path.join(project_path, f\"requirements.txt\"))\n\n    if success:\n        exit_code, logs = docker_execute.execute(command=f\"pip install -r {function_name}/requirements.txt\")\n        if exit_code != 0:\n            logging.info(f\"An error occurred while installing requirements.txt: exit_code={exit_code} logs={logs}\")\n\ndef count_non_empty_strings(array):\n    count = 0\n    for item in array:\n        if item is not None and isinstance(item, str) and len(item) > 0:\n            count += 1\n    return count\n\nclass CodeGen:\n    def __init__(self, args):\n        self.args = args\n        self.contents = {} # dictionary: maps task_id to contents (mix of tests and codes)\n        self.code_scores = {} # dictionary: maps task_id to scores (mix of tests and codes)\n        self.tests = []\n        self.codes = []\n        self.pair_scores = {} # dictionary: maps task_id to (code_id, test_id, score)\n        self.total_codes_requested = 0\n        self.total_tests_requested = 0\n\n        logging.info(\"Setting up VM...\")\n\n        self.docker_execute = DockerExecute(sources_dirname=args.sources_dirname)\n\n        logging.info(\"Starting LLM workers...\")\n\n        self.manager = JobManager(args)\n\n    def add_code_or_test_job(self):\n        test_count = len(self.tests)\n        code_count = len(self.codes)\n\n        add_code = True\n\n        if test_count == 0 and code_count == 0:\n            add_code = self.total_tests_requested > self.total_codes_requested\n        else:\n            add_code = test_count > code_count\n\n        if add_code:\n            logging.info(f\"Adding a job to write more code (tests asked/completed={self.total_codes_requested}/{test_count}, codes asked/completed={self.total_tests_requested}/{code_count})\")\n            self.manager.add_code_job()\n            self.total_codes_requested += 1\n        else:\n            logging.info(f\"Adding a job to write more tests (tests asked/completed={self.total_codes_requested}/{test_count}, codes asked/completed={self.total_tests_requested}/{code_count})\")\n            self.manager.add_test_job()\n            self.total_tests_requested += 1\n\n    def test_pair(self, code_id, test_id):\n        exit_code, logs = copy_and_run_pytest(\n            args.sources_dirname,\n            args.function_name,\n            code_id,\n            test_id,\n            self.docker_execute)\n\n        if exit_code != 0:\n            logging.info(f\"Test failed: code {code_id} <-> test {test_id}: exit_code={exit_code} logs={logs}\")\n\n            if len(logs) == \"\":\n                logging.info(\"Test failed really badly somehow. Deleting {code_id} and {test_id} to avoid repeating this error.\")\n                self.codes.remove(code_id)\n                self.tests.remove(test_id)\n            return False\n\n        logging.info(f\"Test passed: code {code_id} <-> test {test_id} - Asking judge if we are done\")\n\n        code = self.contents[code_id]\n        test = self.contents[test_id]\n        judge_id = self.manager.add_judge_pair_job(code, test)\n        self.pair_scores[judge_id] = (code_id, test_id, None)\n\n        return True\n\n    def handle_code(self, code_id, code, score, improved=False):\n        print(f\"Task ID {code_id}: Generated code (improved={improved}) with score {score} and len={len(code)}\")\n        #print(\"Code:\", code)\n\n        self.codes.append(code_id)\n        self.contents[code_id] = code\n        self.code_scores[code_id] = score\n\n        write_script_to_disk(\n            code,\n            args.sources_dirname,\n            args.function_name,\n            is_test=False,\n            variation=code_id)\n\n        install_container_requirements(\n            args.sources_dirname,\n            args.function_name,\n            self.docker_execute)\n\n        for test_id in self.tests:\n            self.test_pair(code_id, test_id)\n\n        if not improved:\n            logging.info(\"Adding a job to improve the code with self-reflection\")\n            self.manager.add_improve_code_job(code)\n\n    def handle_test(self, test_id, test, improved=False):\n        print(f\"Task ID {test_id}: Generated test (improved={improved}) len={len(test)}\")\n        #print(\"Test:\", test)\n\n        self.tests.append(test_id)\n        self.contents[test_id] = test\n\n        write_script_to_disk(\n            test,\n            args.sources_dirname,\n            args.function_name,\n            is_test=True,\n            variation=test_id)\n\n        install_container_requirements(\n            args.sources_dirname,\n            args.function_name,\n            self.docker_execute)\n\n        for code_id in self.codes:\n            self.test_pair(code_id, test_id)\n\n        if not improved:\n            logging.info(\"Adding a job to improve the test with self-reflection\")\n            self.manager.add_improve_test_job(test)\n\n    def handle_judge_pair(self, task_id, score):\n        (code_id, test_id, _) = self.pair_scores.get(task_id)\n\n        print(f\"Task {task_id} complete: Judged pair code={code_id} test={test_id} with score={score}\")\n\n        self.pair_scores[task_id] = (code_id, test_id, score)\n\n    def handle_results(self):\n        results = self.manager.get_results(timeout=1.0)\n\n        for result in results:\n            task_op, task_id, score, data = result\n\n            # Process a result\n            if task_op == \"code\":\n                self.handle_code(code_id=task_id, code=data, score=score, improved=False)\n\n            elif task_op == \"test\":\n                self.handle_test(test_id=task_id, test=data, improved=False)\n\n            elif task_op == \"improve_code\":\n                self.handle_code(code_id=task_id, code=data, score=score, improved=True)\n\n            elif task_op == \"improve_test\":\n                self.handle_test(test_id=task_id, test=data, improved=True)\n\n            elif task_op == \"judge_pair\":\n                self.handle_judge_pair(task_id=task_id, score=score)\n\n    def write_code(self):\n        # Delete any existing code from a previous run\n        delete_old_scripts(args.sources_dirname, args.function_name)\n\n        try:\n            while True:\n                self.handle_results()\n\n                active_workers = self.manager.active_workers()\n                approx_queue_depth = self.manager.approx_queue_depth()\n                if approx_queue_depth == 0 and active_workers < args.workers:\n                    logging.info(f\"Work queue empty and detected only {active_workers}/{args.workers} workers active.  Adding job...\")\n                    self.add_code_or_test_job()\n                else:\n                    logging.info(f\"Work queue depth = {approx_queue_depth} active workers = {active_workers}/{args.workers}\")\n\n                for pair_score in self.pair_scores.values():\n                    (code_id, test_id, score) = pair_score\n                    if score is None:\n                        continue\n\n                    if score >= args.threshold:\n                        logging.info(f\"Found a good code/test pair: code={code_id} test={test_id} score={score}\")\n                        copy_candidate_scripts(args.sources_dirname, args.function_name, code_id, test_id)\n                        logging.info(\"Wrote final code and test to disk. Exiting...\")\n                        return\n        except KeyboardInterrupt:\n            logging.info(\"Terminating early on user request...\")\n        except Exception as e:\n            logging.error(f\"Exception in run_queue: {e}\")\n        finally:\n            self.manager.terminate()\n\ndef main(args):\n    logging.info(f\"Input comments: {args.comments}\")\n    logging.info(f\"Function prototype: {args.prototype}\")\n    logging.info(f\"Function name: {args.function_name}\")\n\n    codegen = CodeGen(args)\n    codegen.write_code()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Automatically generate Python functions and their test scripts.\")\n    parser.add_argument(\"--sources-dirname\", default=\"sources\", help=\"Directory where the generated source files are stored.\")\n    parser.add_argument(\"--comments\", default=\"# A function that calculates the factorial of a given non-negative integer\", help=\"Comments describing the desired function.\")\n    parser.add_argument(\"--prototype\", default=\"def factorial(n)\", help=\"Prototype of the desired function.\")\n    parser.add_argument(\"--node\", default=\"localhost\", help=\"Hostname or IP address of the OpenAI GPT server.\")\n    parser.add_argument(\"--port\", type=int, default=5000, help=\"Port number of the OpenAI GPT server.\")\n    parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Temperature parameter for the model: Lower values will make the model more conservative but will repeat itself more often, while higher values will make the model more creative and generate less likely results.\")\n    parser.add_argument(\"--max-tokens\", type=int, default=1024, help=\"Maximum number of tokens in the generated code.\")\n    parser.add_argument(\"--workers\", type=int, default=1, help=\"Number of worker machines when using a load balancer in front of a cluster of worker nodes.\")\n    parser.add_argument(\"--threshold\", type=float, default=0.8, help=\"Minimum threshold of code correctness before stopping.\")\n\n    args = parser.parse_args()\n\n    args.comments = comment_multiline_string(args.comments)\n    args.prototype = ensure_colon_at_end(args.prototype)\n    args.function_name = extract_function_name(args.prototype)\n\n    main(args)\n"}
{"type": "source_file", "path": "server/model_llama.py", "content": "import os\nimport logging\n\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM\n\nfrom gptq import load_quant\n\nimport accelerate\n\nfrom model_tools import disable_torch_init\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_model_llama(model_name=\"llama-65b-4bit\", load_in_8bit=None):\n    if model_name == \"llama-65b-4bit\":\n        model_path = \"catid/llama-65b-4bit\"\n        model_load = \"llama65b-4bit-128g.safetensors\"\n        wbits = 4\n        groupsize = 128\n    else:\n        raise Exception(\"Unknown model_name={model_name}\")\n\n    logging.info(f\"Loading base_model={model_path}...\")\n\n    disable_torch_init()\n\n    model = load_quant(model_path, model_load, wbits, groupsize, -1)\n    max_memory = accelerate.utils.get_balanced_memory(model)\n    device_map = accelerate.infer_auto_device_map(model, max_memory=max_memory, no_split_module_classes=[\"LlamaDecoderLayer\"])\n    logging.info(f\"Using the following device map for the quantized model: {device_map}\")\n    model = accelerate.dispatch_model(model, device_map=device_map, offload_buffers=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    logging.info(f\"Loaded base_model={model_path} load_in_8bit={load_in_8bit} with context length {context_len}\")\n\n    return tokenizer, model, context_len\n"}
{"type": "source_file", "path": "server/server.py", "content": "import argparse\nimport logging\nimport asyncio\nimport time\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom language_model import LanguageModel\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI()\nsemaphore = asyncio.Semaphore(1)\n\nclass AskData(BaseModel):\n    prompt: str\n    stop_strs: Optional[List[str]] = None\n    temperature: Optional[float] = 0.7\n    max_tokens: Optional[int] = 512\n\n@app.post(\"/ask\")\nasync def ask_endpoint(data: AskData):\n    try:\n        # Attempt to acquire the semaphore without waiting\n        await asyncio.wait_for(semaphore.acquire(), timeout=0.1)\n\n    except asyncio.TimeoutError:\n        raise HTTPException(status_code=503, detail=\"Server is busy, please try again later\")\n\n    # Process the input with semaphore held\n    try:\n        prompt = data.prompt\n        stop_strs = data.stop_strs\n        temperature = data.temperature\n        max_tokens = data.max_tokens\n\n        logger.info(f\"Asking: {prompt}\")\n        logger.info(f\"Stop strings: {stop_strs}\")\n\n        start_time = time.time()\n\n        response = m.ask(\n            prompt,\n            stop_strs=stop_strs,\n            temperature=temperature,\n            max_new_tokens=max_tokens)\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        logger.info(f\"Response in {duration} seconds: {response}\")\n\n        return {\n            'response': response,\n            'duration': duration,\n        }\n\n    except Exception as e:\n        return {'response': f\"Exception while processing request: {e}\"}\n\n    finally:\n        semaphore.release()\n\ndef main(args):\n    global m\n    m = LanguageModel(args.model, load_in_8bit=args.load_in_8bit)\n\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=args.listen)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Supercharged Vicuna-13B\")\n    parser.add_argument(\"--listen\", type=int, default=5000, help=\"Port to listen on (default: 5000)\")\n    parser.add_argument(\"--model\", type=str, help=\"Select model to use. Available options: baize-30b, baize-13b, baize-7b, galpaca-30b, galpaca-7b, koala-13b, koala-7b, vicuna-13b, vicuna-7b, llama-65b-4bit\", default=\"llama-65b-4bit\")\n    parser.add_argument(\"--8bit\", action=\"store_true\", help=\"Use 8-bit precision (default: False)\")\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Use 16-bit precision (default: False)\")\n\n    args = parser.parse_args()\n\n    if getattr(args, \"8bit\"):\n        logging.info(\"8-bit precision selected.\")\n        args.load_in_8bit = True\n    elif args.fp16:\n        logging.info(\"16-bit precision selected.\")\n        args.load_in_8bit = False\n    else:\n        args.load_in_8bit = None\n\n    main(args)\n"}
{"type": "source_file", "path": "server/model_tools.py", "content": "import torch\n\ndef disable_torch_init():\n    \"\"\"\n    Disable the redundant torch default initialization to accelerate model creation.\n    \"\"\"\n    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n"}
{"type": "source_file", "path": "codegen/clean_code.py", "content": "import re\nimport ast\nimport logging\n\nfrom autoimport import fix_code\n\nfrom yapf.yapflib.yapf_api import FormatCode\n\nfrom fix_ast_errors import fix_ast_errors\nfrom extract_code_from_md import extract_code_from_md\n\n# If you only expect a Python function and nothing else but imports this can clean the junk after syntax errors are cleaned up\ndef only_defs_and_imports(code_string, strip_import_mods=[], strip_import_funcs=[]):\n    # This will fail if the input code is not valid Python\n    tree = ast.parse(code_string)\n    filtered_nodes = []\n\n    for node in ast.iter_child_nodes(tree):\n        if isinstance(node, ast.FunctionDef):\n            filtered_nodes.append(node)\n        elif isinstance(node, ast.Import):\n            filtered_names = [name for name in node.names if not name.name in strip_import_mods]\n            if filtered_names:\n                node.names = filtered_names\n                filtered_nodes.append(node)\n        elif isinstance(node, ast.ImportFrom):\n            if node.module not in strip_import_mods:\n                filtered_names = [name for name in node.names if not name.name in strip_import_funcs]\n                if filtered_names:\n                    node.names = filtered_names\n                    filtered_nodes.append(node)\n\n    return ast.unparse(filtered_nodes)\n\ndef remove_comments_before_first_function(script):\n    # Match function definition pattern\n    function_pattern = re.compile(r'^def .*\\(', re.MULTILINE)\n    \n    # Find the index of the first function definition\n    match = function_pattern.search(script)\n    if match:\n        start_index = match.start()\n    else:\n        # If there's no function definition, return the original script\n        return script\n    \n    # Remove line comments\n    line_comment_pattern = re.compile(r'(?<=\\n)#.*\\n', re.MULTILINE)\n    clean_script = line_comment_pattern.sub('\\n', script[:start_index])\n\n    # Remove multi-line comments\n    multiline_comment_pattern = re.compile(r'(\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\')', re.MULTILINE)\n    clean_script = multiline_comment_pattern.sub('', clean_script)\n\n    # Add the remaining script after the first function definition\n    clean_script += script[start_index:]\n\n    return clean_script\n\ndef clean_code(code, strip_md=True, strip_globals=True, strip_leading_comments=False, strip_import_mods=[], strip_import_funcs=[], try_autoimport=True):\n\n    #print(f\"CODE:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    if strip_md:\n        try:\n            code = extract_code_from_md(code)\n        except Exception as e:\n            logging.info(f\"clean_code::extract_code_from_md failed due to exception: {e}\")\n\n    #print(f\"extract_code_from_md:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    try:\n        code = fix_ast_errors(code)\n    except Exception as e:\n        logging.info(f\"clean_code::fix_ast_errors failed due to exception: {e}\")\n\n    #print(f\"fix_ast_errors:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    if strip_globals:\n        try:\n            code = only_defs_and_imports(code, strip_import_mods=strip_import_mods, strip_import_funcs=strip_import_funcs)\n        except Exception as e:\n            logging.info(f\"clean_code::only_defs_and_imports failed due to exception: {e}\")\n\n    #print(f\"only_defs_and_imports:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    if strip_leading_comments:\n        try:\n            code = remove_comments_before_first_function(code)\n        except Exception as e:\n            logging.info(f\"clean_code::remove_comments_before_first_function failed due to exception: {e}\")\n\n    #print(f\"remove_comments_before_first_function:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    if try_autoimport:\n        try:\n            code = fix_code(code)\n        except Exception as e:\n            logging.info(f\"clean_code::try_autoimport failed due to exception: {e}\")\n\n    try:\n        code, _ = FormatCode(code)\n    except Exception as e:\n        logging.info(f\"clean_code::yapf failed due to exception: {e}\")\n        return code, False\n\n    #print(f\"FormatCode:\\n\\n----\\n{code}\\n----\\n\\n\")\n\n    return code, True\n"}
{"type": "source_file", "path": "server/model_baize.py", "content": "import logging\n\nimport torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom peft import PeftModel\n\nfrom model_tools import disable_torch_init\n\ndef load_model_baize(model_name=\"baize-30b\", load_in_8bit=None):\n    if model_name == \"baize-30b\":\n        base_model_name = \"decapoda-research/llama-30b-hf\"\n        peft_model_name = \"project-baize/baize-lora-30B\"\n        if load_in_8bit is None:\n            load_in_8bit = True # Should fit on two GPUs\n    elif model_name == \"baize-13b\":\n        base_model_name = \"decapoda-research/llama-13b-hf\"\n        peft_model_name = \"project-baize/baize-lora-13B\"\n        if load_in_8bit is None:\n            load_in_8bit = True # Prefer running on one GPU\n    elif model_name == \"baize-7b\":\n        base_model_name = \"decapoda-research/llama-7b-hf\"\n        peft_model_name = \"project-baize/baize-lora-7B\"\n        if load_in_8bit is None:\n            load_in_8bit = False # Should fit on one GPU\n    else:\n        raise Exception(\"Unknown model_name={model_name}\")\n\n    logging.info(f\"Loading base_model={base_model_name} load_in_8bit={load_in_8bit} enhanced with peft_model={peft_model_name}...\")\n\n    disable_torch_init()\n\n    tokenizer = LlamaTokenizer.from_pretrained(base_model_name)\n\n    model = LlamaForCausalLM.from_pretrained(\n        base_model_name,\n        load_in_8bit=load_in_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n\n    model = PeftModel.from_pretrained(\n        model,\n        peft_model_name,\n        torch_dtype=torch.float16,\n    )\n\n    if not load_in_8bit:\n        model = model.half()\n        is_half = True\n    else:\n        is_half = False\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    logging.info(f\"Loaded base_model={base_model_name} load_in_8bit={load_in_8bit} enhanced with peft_model={peft_model_name} with context length {context_len}\")\n\n    return tokenizer, model, context_len\n"}
{"type": "source_file", "path": "server/model_koala.py", "content": "import logging\n\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\nfrom model_tools import disable_torch_init\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_model_koala(model_name=\"koala-13b\", load_in_8bit=None):\n    if model_name == \"koala-13b\":\n        base_model_name = \"TheBloke/koala-13B-HF\"\n        if load_in_8bit is None:\n            load_in_8bit = True # Should fit on one GPU\n    elif model_name == \"koala-7b\":\n        base_model_name = \"TheBloke/koala-7B-HF\"\n        if load_in_8bit is None:\n            load_in_8bit = False # Should fit on one GPU\n        raise Exception(\"Koala-7B model appears to be broken\")\n    else:\n        raise Exception(\"Unknown model_name={model_name}\")\n\n    logging.info(f\"Loading base_model={base_model_name} load_in_8bit={load_in_8bit}...\")\n\n    disable_torch_init()\n\n    tokenizer = LlamaTokenizer.from_pretrained(base_model_name)\n    model = LlamaForCausalLM.from_pretrained(\n        base_model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        load_in_8bit=load_in_8bit,\n    )\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    logging.info(f\"Loaded base_model={base_model_name} load_in_8bit={load_in_8bit} with context length {context_len}\")\n\n    return tokenizer, model, context_len\n"}
{"type": "source_file", "path": "server/language_model.py", "content": "import torch\n\nfrom model_baize import load_model_baize\nfrom model_galpaca import load_model_galpaca\nfrom model_koala import load_model_koala\nfrom model_vicuna import load_model_vicuna\nfrom model_llama import load_model_llama\n\ndef is_array_of_strings(input_array):\n    if isinstance(input_array, list) and all(isinstance(element, str) for element in input_array):\n        return True\n    else:\n        return False\n\nclass LanguageModel:\n    def __init__(self, model_name=\"baize-30b\", load_in_8bit=True):\n        if \"baize\" in model_name:\n            self.tokenizer, self.model, self.context_len = load_model_baize(model_name, load_in_8bit)\n        elif \"galpaca\" in model_name:\n            self.tokenizer, self.model, self.context_len = load_model_galpaca(model_name, load_in_8bit)\n        elif \"koala\" in model_name:\n            self.tokenizer, self.model, self.context_len = load_model_koala(model_name, load_in_8bit)\n        elif \"vicuna\" in model_name:\n            self.tokenizer, self.model, self.context_len = load_model_vicuna(model_name, load_in_8bit)\n        elif \"llama\" in model_name:\n            self.tokenizer, self.model, self.context_len = load_model_llama(model_name, load_in_8bit)\n        else:\n            raise Exception(f\"Unknown model_name={model_name}\")\n\n    @torch.inference_mode()\n    def ask(self, prompt, stop_strs=None, temperature=0.7, max_new_tokens=512):\n        has_stop_strs = is_array_of_strings(stop_strs)\n\n        max_new_tokens = min(max_new_tokens, 1024)\n\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n        output_ids = []\n\n        max_src_len = self.context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n\n        output = \"\"\n\n        with torch.no_grad():\n            for i in range(max_new_tokens):\n                if i == 0:\n                    input_tensor = input_ids.cuda()\n                    out = self.model(input_tensor, use_cache=True)\n                    logits = out.logits\n                    past_key_values = out.past_key_values\n                else:\n                    input_tensor = torch.as_tensor([[token]], device=\"cuda\")\n                    attention_mask = torch.ones(\n                        1, past_key_values[0][0].shape[-2] + 1, device=\"cuda\")\n                    out = self.model(input_ids=input_tensor,\n                                use_cache=True,\n                                attention_mask=attention_mask,\n                                past_key_values=past_key_values)\n                    logits = out.logits\n                    past_key_values = out.past_key_values\n\n                last_token_logits = logits[0][-1]\n                if temperature < 1e-4:\n                    token = int(torch.argmax(last_token_logits))\n                else:\n                    probs = torch.softmax(last_token_logits / temperature, dim=-1)\n                    token = int(torch.multinomial(probs, num_samples=1))\n\n                output_ids.append(token)\n\n                if token == self.tokenizer.eos_token_id:\n                    stopped = True\n                else:\n                    stopped = False\n\n                if i%2 == 0 or i == max_new_tokens - 1 or stopped:\n                    output = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n\n                    if has_stop_strs:\n                        for stop_str in stop_strs:\n                            pos = output.rfind(stop_str, 0)\n                            if pos > 8:\n                                output = output[:pos]\n                                stopped = True\n                                break\n\n                if stopped:\n                    break\n\n        torch.cuda.empty_cache()\n\n        return output\n"}
{"type": "source_file", "path": "server/model_galpaca.py", "content": "import logging\n\nimport torch\nfrom transformers import AutoTokenizer, OPTForCausalLM\n\nfrom model_tools import disable_torch_init\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_model_galpaca(model_name=\"galpaca-30b\", load_in_8bit=None):\n    if model_name == \"galpaca-30b\":\n        base_model_name = \"GeorgiaTechResearchInstitute/galpaca-30b\"\n        if load_in_8bit is None:\n            load_in_8bit = True # Should fit on two GPUs\n    elif model_name == \"galpaca-7b\":\n        base_model_name = \"GeorgiaTechResearchInstitute/galpaca-6.7b\"\n        if load_in_8bit is None:\n            load_in_8bit = False # Should fit on one GPU\n    else:\n        raise Exception(\"Unknown model_name={model_name}\")\n\n    logging.info(f\"Loading base_model={base_model_name} load_in_8bit={load_in_8bit}...\")\n\n    disable_torch_init()\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    model = OPTForCausalLM.from_pretrained(\n        base_model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        load_in_8bit=load_in_8bit,\n    )\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    logging.info(f\"Loaded base_model={base_model_name} load_in_8bit={load_in_8bit} with context length {context_len}\")\n\n    return tokenizer, model, context_len\n"}
{"type": "source_file", "path": "prompts/ask_templates.py", "content": "# Prompt engineering to improve the odds that the LLM produces useful output\n\ndef has_system_role(messages):\n    for message in messages:\n        if message[\"role\"] == \"system\":\n            return True\n    return False\n\ndef normalize_role(role):\n    return f\"{role.capitalize()}\"\n\ndef decorate_role(role):\n    return f\"[|{normalize_role(role)}|]\"\n\ndef create_conversation_template(messages, default_template=None, custom_start=\"\", user_role=\"Human\", assistant_role=\"Assistant\"):\n    conversation = []\n\n    if not has_system_role(messages) and default_template is not None:\n        messages = default_template + messages\n\n    for message in messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n\n        if role.capitalize() == \"System\":\n            conversation.append(content)\n        else:\n            conversation.append(f\"{decorate_role(role)}: {content}\")\n\n    if len(custom_start) > 0:\n        conversation.append(f\"{decorate_role(assistant_role)}: {custom_start}\")\n    else:\n        conversation.append(f\"{decorate_role(assistant_role)}:\")\n\n    # Sometimes the smaller models forget the colon at the end of the prompt,\n    # So I didn't include it in the stop_strs\n    stop_strs = [\n        f\"{decorate_role(user_role)}\",\n        f\"{decorate_role(assistant_role)}\"\n    ]\n\n    return \"\\n\".join(conversation), stop_strs\n\n# This version has a history for the assistant role\ndef ask_assistant(messages, user_role=\"Human\", assistant_role=\"Assistant\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    template = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} is a helpful AI assistant. {assistant_role} always considers responses carefully and thinks step by step before answering.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"What is a microorganism?\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"A microorganism, or microbe, is a microscopic living organism that is too small to be seen with the naked eye. Microorganisms can be single-celled, such as bacteria and protozoa, or multi-cellular, like some fungi and algae. They can be found in various environments, including soil, water, air, and inside the bodies of plants and animals.\n\nMicroorganisms play vital roles in many ecosystems, such as decomposing organic matter, recycling nutrients, and participating in symbiotic relationships. Some microorganisms can also cause diseases, while others are beneficial to humans and are used in various applications, such as food production (e.g., yogurt, cheese, and bread) and biotechnology (e.g., producing antibiotics, enzymes, and biofuels).\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"What's a good way to use a fiber laser?\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"Fiber lasers are versatile tools with many applications across various industries. A good way to use a fiber laser depends on your specific needs and requirements. Some common applications include:\n\n* Material processing: Fiber lasers are widely used for cutting, welding, and drilling materials, especially metals. They offer high precision, speed, and quality, making them ideal for manufacturing processes in the aerospace, automotive, electronics, and medical industries.\n\n* Marking and engraving: Fiber lasers can create high-resolution marks and engravings on a variety of materials such as metals, plastics, ceramics, and glass. This makes them suitable for product identification, traceability, and branding purposes.\"\"\"\n        },\n    ]\n\n    return create_conversation_template(template + messages, assistant_role=assistant_role, user_role=user_role)\n\n# This version has a history for python code generation\ndef ask_python_coder(messages, user_role=\"Human\", assistant_role=\"Coder\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    template = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers responses carefully and thinks step by step before answering. {assistant_role} always writes syntactically correct Python code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"Write a function that adds two numbers.\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"```\ndef add_numbers(x, y):\n    return x + y\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"Write a Python function that checks if a number is even\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"```\ndef is_even(x):\n    return (x%2) == 0\n```\"\"\"\n        },\n    ]\n\n    return create_conversation_template(template + messages, assistant_role=assistant_role, user_role=user_role)\n\n# Generate a python function with a specific prototype\ndef ask_python_function_prototype(comments, prototype, user_role=\"Human\", assistant_role=\"Coder\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n\n    system_prompt = f\"The following is a conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always starts by summarizing the task before providing working Python code. {assistant_role} always writes syntactically correct Python code.\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"# Add two numbers and return their sum\\ndef add_nums(x, y)\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"The add_nums function takes two input arguments (numbers) and returns their sum. Here is the code for the function:\n```\ndef add_nums(x, y):\n    return x + y\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"# Write a function that multiplies two floats\\ndef mul_nums(a, b)\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"The mul_nums function takes two input arguments (floats) and returns their product. Here is the code for the function:\n```\ndef mul_nums(x: float, y: float) -> float:\n    return x * y\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"# A function that checks if an integer is prime\\n# Returns true or false\\ndef is_prime(n)\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"The is_prime function takes an integer input argument and checks if it is a prime number. It returns True if the number is prime, otherwise, it returns False. Here is the code for the function:\n```\ndef is_prime(n: int) -> bool:\n    if n < 2:\n        return False\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"{comments}\\n{prototype}\"\n        },\n    ]\n\n    return create_conversation_template(messages, assistant_role=assistant_role, user_role=user_role)\n\n# Generate a python unit test for a function with a specific prototype\ndef ask_python_pytest_prototype(comments, prototype, user_role=\"Human\", assistant_role=\"Coder\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"The following is a conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers responses carefully and thinks step by step before answering with a Python program using pytest to exercise the given function. {assistant_role} always writes syntactically correct Python code using pytest.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"Please write a pytest for this function:\\n# Add two numbers and return their sum\\ndef add_nums(x, y):\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"```\nimport pytest\nfrom add_nums import add_nums\n\ndef test_add_nums_positive():\n    # Test case 1: adding two positive numbers\n    result = add_nums(2, 3)\n    assert result == 5\n\ndef test_add_nums_negative():\n    # Test case 2: adding two negative numbers\n    result = add_nums(-2, -3)\n    assert result == -5\n\ndef test_add_nums_mixed():\n    # Test case 3: adding a positive and a negative number\n    result = add_nums(2, -3)\n    assert result == -1\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"Please write a pytest for this function:\\n# A function that checks if an integer is prime\\n# Returns true or false\\ndef is_prime(n):\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"```\nimport pytest\nfrom is_prime import is_prime\n\ndef test_is_prime():\n    assert is_prime(2) == True\n    assert is_prime(3) == True\n    assert is_prime(4) == False\n    assert is_prime(5) == True\n    assert is_prime(6) == False\n    assert is_prime(7) == True\n    assert is_prime(8) == False\n    assert is_prime(9) == False\n    assert is_prime(10) == False\n```\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"Please write a pytest for this function:\\n{comments}\\n{prototype}\"\n        },\n    ]\n\n    return create_conversation_template(messages, assistant_role=assistant_role, user_role=user_role)\n\n# This version has a history for python code analysis and improvement\ndef ask_python_analyzer(code, user_role=\"Human\", assistant_role=\"Analyst\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully and summarizes before responding. {assistant_role} always provides valid Python code with improvements over the original.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please analyze the following code and suggest a fixed or improved version of the code:\n```python\n# Add two numbers and return their sum\ndef add_numbers(x, y):\n    return x + y * 2\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"It seems like there's a mistake in the given code. The code defines a function add_numbers(x, y) that takes two arguments, x and y. The function is supposed to add the two numbers and return their sum. However, the current implementation returns x plus two times y (i.e., x + 2*y), which is incorrect for the intended purpose.\n\nHere's the corrected version of the code:\n```python\n# Add two numbers and return their sum\ndef add_numbers(x, y):\n    return x + y\n```\n\nNow, the function add_numbers(x, y) correctly adds x and y together and returns the sum.\n\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please analyze the following code and suggest a fixed or improved version of the code:\n```python\n# A function that checks if an integer is prime\n# Returns true or false\ndef is_prime(n: int) -> bool:\n    if n < 2:\n        return False\n    for i in range(2, int(n ** 0.4) + 1):\n        if n % i == 0:\n            return False\n    return True\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"The given code is intended to check if a number is prime. However, there's a mistake in the loop range. It uses n ** 0.4 instead of n ** 0.5 (square root of n) as the upper limit. Here's a fixed and improved version of the code:\n\nHere's the corrected version of the code:\n```python\ndef is_prime(n: int) -> bool:\n    if n < 2:\n        return False\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n```\n\nIn this version, the function correctly checks for prime numbers by iterating up to the square root of n. This is an optimized approach since any factor larger than the square root would have a corresponding factor smaller than the square root, so it's not necessary to check beyond that point.\n\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please analyze the following code and suggest a fixed or improved version of the code:\n```python\n{code}\n```\"\"\"\n        },\n    ]\n\n    return create_conversation_template(messages, assistant_role=assistant_role, user_role=user_role)\n\n# This version has a history for python test code analysis and improvement\ndef ask_python_test_analyzer(comments, prototype, function_name, test_code, user_role=\"Human\", assistant_role=\"Analyst\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully and summarizes before responding. {assistant_role} always provides valid pytest Python code with improvements over the original.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Given description and prototype of add_nums function to test:\n```\n# Add two numbers and return their sum\ndef add_nums(x, y):\n```\n\nPlease analyze the following pytest code and suggest a fixed version of the pytest code that covers more scenarios:\n```python\nimport pytest\nfrom add_nums import add_nums\n\ndef test_add_nums_positive():\n    # Test case 1: adding two positive numbers\n    result = add_nums(2, 3)\n    assert result == 6\n\ndef test_add_nums_negative():\n    # Test case 2: adding two negative numbers\n    result = add_nums(-2, -3)\n    assert result == -5\n\ndef test_add_nums_mixed():\n    # Test case 3: adding a positive and a negative number\n    result = add_nums(2, -3)\n    assert result == -1\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"The given pytest code has a minor issue in the first test case, where it asserts the result to be 6 instead of 5. Also, it's a good idea to add more test cases to cover additional scenarios. Here's a fixed and improved version of the pytest code:\n```python\nimport pytest\nfrom add_nums import add_nums\n\ndef test_add_nums_positive():\n    # Test case 1: adding two positive numbers\n    result = add_nums(2, 3)\n    assert result == 5\n\ndef test_add_nums_negative():\n    # Test case 2: adding two negative numbers\n    result = add_nums(-2, -3)\n    assert result == -5\n\ndef test_add_nums_mixed():\n    # Test case 3: adding a positive and a negative number\n    result = add_nums(2, -3)\n    assert result == -1\n\ndef test_add_nums_zero():\n    # Test case 4: adding zero to a positive number\n    result = add_nums(0, 3)\n    assert result == 3\n\n    # Test case 5: adding zero to a negative number\n    result = add_nums(0, -3)\n    assert result == -3\n\n    # Test case 6: adding two zeros\n    result = add_nums(0, 0)\n    assert result == 0\n\ndef test_add_nums_large():\n    # Test case 7: adding large numbers\n    result = add_nums(100000, 200000)\n    assert result == 300000\n\n```\n\nNow, the function add_numbers(x, y) correctly adds x and y together and returns the sum.\n\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Given description and prototype of {function_name} function to test:\n```python\n{comments}\n{prototype}\n```\n\nPlease analyze the following pytest code and suggest a fixed version of the pytest code that covers more scenarios:\n```python\n{test_code}\n```\"\"\"\n        },\n    ]\n\n    return create_conversation_template(messages, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of python code\ndef ask_python_code_judge(commented_code, function_name, user_role=\"Human\", assistant_role=\"Judge\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided Python code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following add_nums function from 0 to 1, where 0 means the code has a bug and 1 means the code is good:\n```\n# Add two numbers and return their sum\ndef add_nums(x, y):\n    return x + y\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given add_nums function as 1. The code is simple, clean, and easy to understand. It performs the expected operation of adding two numbers together and returning their sum.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following mul_nums function from 0 to 1, where 0 means the code has a bug and 1 means the code is good:\n```\n# Multiply two numbers and return their product\ndef mul_nums(x, y):\n    return x + y\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given add_nums function as 0. The given mul_nums function does not perform the multiplication operation correctly, as it returns the sum of the two input parameters rather than their product.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following {function_name} function from 0 to 1, where 0 means the code has a bug and 1 means the code is good:\n```python\n{commented_code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = f\"After careful consideration, I would rate the given {function_name} function as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of python test code\ndef ask_python_test_judge(commented_code, function_name, test_code, user_role=\"Human\", assistant_role=\"Judge\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided Python code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Given the add_nums function:\n```\n# Add two numbers and return their sum\ndef add_nums(x, y):\n    return x + y\n```\n\nPlease rate the following unit test code from 0 to 1, where 0 means the code has a bug and 1 means the code is good:\n```python\nimport pytest\nfrom add_nums import add_nums\n\ndef test_add_nums_positive():\n    # Test case 1: adding two positive numbers\n    result = add_nums(2, 3)\n    assert result == 6\n\ndef test_add_nums_negative():\n    # Test case 2: adding two negative numbers\n    result = add_nums(-2, -3)\n    assert result == -5\n\ndef test_add_nums_mixed():\n    # Test case 3: adding a positive and a negative number\n    result = add_nums(2, -3)\n    assert result == -1\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given unit test code as 0. The test cases cover different scenarios (adding two positive numbers, two negative numbers, and a positive and a negative number), which is good. However, there's an error in the first test case:\n```python\ndef test_add_nums_positive():\n    # Test case 1: adding two positive numbers\n    result = add_nums(2, 3)\n    assert result == 6\n```\nThe assertion should be assert result == 5 instead of assert result == 6.\n\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Given the {function_name} function:\n```python\n{commented_code}\n```\n\nPlease rate the following unit test code from 0 to 1, where 0 means the code is not good and 1 means the code is good:\n```python\n{test_code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given unit test code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a C++ function\ndef ask_cpp_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a C++ conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided C++ code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following C++ function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n// Function to calculate the factorial of a positive integer using recursion\nint factorial(int n) {\n    // Base case: If n is 0 or 1, the factorial is 1\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n    // Recursive case: Calculate the factorial of (n-1) and multiply it by n\n    else {\n        return n * factorial(n-1);\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1, meaning that it cannot be improved as it is a valid and functional implementation of the factorial function in C++.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following C++ function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n// Sort an array of integers using bubble sort\nvoid bubble_sort(int arr[], int n) {\n    bool swapped;\n    for (int i = 0; i < n - 1; i++) {\n        swapped = false;\n        for (int j = 0; j < n - i - 1; j += 2) {\n            if (arr[j] > arr[j + 2]) {\n                swap(arr[j], arr[j + 2]);\n                swapped = true;\n            }\n        }\n        if (swapped == false) {\n            break;\n        }\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0, meaning it has a bug that needs to be fixed.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following C++ function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a Python function\ndef ask_python_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Python conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided Python code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following Python function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\ndef gcd_lcm(a, b):\n    def euclidean_algorithm(x, y):\n        while y != 0:\n            x, y = y, x % y\n        return x\n\n    if a == 0 or b == 0:\n        raise ValueError(\"Both input numbers must be non-zero\")\n\n    gcd = euclidean_algorithm(abs(a), abs(b))\n    lcm = abs(a * b) // gcd\n\n    return gcd, lcm\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0.95. Although the code seems correct and efficient, it's important to acknowledge that there might be room for minor improvements or alternative solutions.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following Python function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\ndef recursive_factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * recursive_factorial(n)\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0.  The provided recursive_factorial function has a bug, as it will result in an infinite loop due to the recursive call not reducing the argument.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following Python function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a C# function\ndef ask_cs_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a C# conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided C# code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following C# function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n    public static int RecursiveRangeSum(int start, int end)\n    {\n        if (start == end)\n        {\n            return start;\n        }\n        else\n        {\n            return start + RecursiveRangeSum(start, end - 1);\n        }\n    }\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0. The provided RecursiveRangeSum function has a bug that results in an infinite loop.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following C# function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n    public static string ReverseWords(string input)\n    {\n        StringBuilder reversed = new StringBuilder();\n        StringBuilder word = new StringBuilder();\n\n        for (int i = 0; i < input.Length; i++)\n        {\n            char currentChar = input[i];\n\n            if (currentChar == ' ')\n            {\n                if (word.Length > 0)\n                {\n                    reversed.Insert(0, word.ToString());\n                    word.Clear();\n                }\n                reversed.Insert(0, currentChar);\n            }\n            else\n            {\n                word.Append(currentChar);\n            }\n        }\n\n        if (word.Length > 0)\n        {\n            reversed.Insert(0, word.ToString());\n        }\n\n        return reversed.ToString();\n    }\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1.  The provided ReverseWords function appears to be well-written and correctly reverses the order of words in a given string.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following C# function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a JavaScript function\ndef ask_js_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a JavaScript conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided JavaScript code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following JavaScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\nfunction recursiveFactorial(n) {\n    if (n === 0) {\n        return 1;\n    } else {\n        return n * recursiveFactorial(n - 1);\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1. The provided recursiveFactorial function appears to be well-written and correctly calculates the factorial of a non-negative integer using recursion.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following JavaScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\nfunction recursiveRangeSum(start, end) {\n    if (start === end) {\n        return start;\n    } else {\n        return start + recursiveRangeSum(start, end - 1);\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0.  The provided recursiveRangeSum function has a bug that results in an infinite loop. The issue is in the recursive call, which should be made with start + 1 as its first argument, but it is called with start instead.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following JavaScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a Java function\ndef ask_java_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a Java conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided Java code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following Java function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n    public static int recursiveFactorial(int n) {\n        /*\n         * Returns the factorial of n using recursion.\n         */\n        if (n <= 0) {\n            return 0;\n        } else {\n            return n * recursiveFactorial(n - 1);\n        }\n    }\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0. This Java function has a bug and can be improved.  The bug is in the conditional statement in the beginning of the function. If n is less than or equal to 0, the function returns 0, which is incorrect. The correct result for 0! (0 factorial) is 1, not 0.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following Java function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\npublic static int recursiveRangeSum(int start, int end) {\n    /*\n        * Returns the sum of integers from start to end (inclusive) using recursion.\n        */\n    if (start == end) {\n        return start;\n    } else {\n        return start + recursiveRangeSum(start + 1, end);\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1.  This Java function does not have a bug and can be improved only slightly.\n\nThe function correctly calculates the sum of integers from `start` to `end` (inclusive).\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following Java function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a TypeScript function\ndef ask_ts_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a TypeScript conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided TypeScript code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following TypeScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n    public static int recursiveFactorial(int n) {\n        /*\n         * Returns the factorial of n using recursion.\n         */\n        if (n <= 0) {\n            return 0;\n        } else {\n            return n * recursiveFactorial(n - 1);\n        }\n    }\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0. This TypeScript function has a bug and can be improved.  The bug is in the conditional statement in the beginning of the function. If n is less than or equal to 0, the function returns 0, which is incorrect. The correct result for 0! (0 factorial) is 1, not 0.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following TypeScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\npublic static int recursiveRangeSum(int start, int end) {\n    /*\n        * Returns the sum of integers from start to end (inclusive) using recursion.\n        */\n    if (start == end) {\n        return start;\n    } else {\n        return start + recursiveRangeSum(start + 1, end);\n    }\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1.  This TypeScript function does not have a bug and can be improved only slightly.\n\nThe function correctly calculates the sum of integers from `start` to `end` (inclusive).\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following TypeScript function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n\n# This version rates the quality of a PHP function\ndef ask_php_expert_score(code, user_role=\"Human\", assistant_role=\"Expert\"):\n    user_role, assistant_role = normalize_role(user_role), normalize_role(assistant_role)\n    messages = [\n        {\n            \"role\": \"System\",\n            \"content\": f\"The following is a PHP conversation between {user_role} and {assistant_role}. {user_role} and {assistant_role} take turns chatting. {assistant_role} always considers the previous query carefully. {assistant_role} always provides an expert rating from 0 to 1 of the provided PHP code.\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following PHP function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\nfunction removeDuplicates($arr) {\n  $result = [];\n  foreach ($arr as $value) {\n    if (!in_array($value, $result)) {\n      $result[] = $value;\n    }\n  }\n  return $result;\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 1, as it appears to be correct and efficient. It takes an array as input, removes any duplicates in the array, and returns the resulting array.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": \"\"\"Please rate the following PHP function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\nfunction findMax($arr) {\n  $max = $arr[0];\n  foreach ($arr as $value) {\n    if ($value > $max) {\n      $max = $value;\n    }\n  }\n  return $min;\n}\n```\"\"\"\n        },\n        {\n            \"role\": assistant_role,\n            \"content\": \"\"\"After careful consideration, I would rate the given code as 0, as it contains a bug. The function takes an array as input and returns the maximum value in the array. However, the function incorrectly returns $min instead of $max at the end of the function.\"\"\"\n        },\n        {\n            \"role\": user_role,\n            \"content\": f\"\"\"Please rate the following PHP function from 0 to 1, where 0 means the code has a bug and 1 means the code cannot be improved:\n```\n{code}\n```\"\"\"\n        },\n    ]\n\n    custom_start = \"After careful consideration, I would rate the given code as \"\n\n    return create_conversation_template(messages, custom_start=custom_start, assistant_role=assistant_role, user_role=user_role)\n"}
{"type": "source_file", "path": "codegen/codegen_workers.py", "content": "import multiprocessing\nimport logging\nimport time\nfrom queue import Empty\nfrom typing import List, Tuple, Any\n\nfrom autopy import autopy_func, autopy_func_improve, autopy_test, autopy_test_improve, autopy_code_judge, autopy_test_judge\n\nclass JobWorkers:\n    def __init__(self, args):\n        self.workers = []\n        self.args = args\n        self.active_workers = multiprocessing.Value('i', 0)\n\n    def process_next(self, args, task_op, task_id, code, test, result_queue):\n        if task_op == \"code\":\n            #logging.info(\"Generating code...\")\n            t0 = time.time()\n            code = autopy_func(args.comments, args.prototype, node=args.node, port=args.port, temperature=args.temperature, max_tokens=args.max_tokens)\n            t1 = time.time()\n\n            if len(code) > 0:\n                score = autopy_code_judge(code, args.function_name, node=args.node, port=args.port)\n            else:\n                score = 0\n            t2 = time.time()\n\n            logging.info(f\"Generated code len={len(code)} in {t1 - t0} seconds, with score {score} (scored in {t2 - t1} seconds)\")\n\n            result_queue.put((task_op, task_id, score, code))\n\n        elif task_op == \"test\":\n            #logging.info(\"Generating test...\")\n            t0 = time.time()\n            test = autopy_test(args.comments, args.prototype, args.function_name, node=args.node, port=args.port, temperature=args.temperature, max_tokens=args.max_tokens)\n            t1 = time.time()\n\n            logging.info(f\"Generated test len={len(test)} in {t1 - t0} seconds\")\n            result_queue.put((task_op, task_id, None, test))\n\n        elif task_op == \"improve_code\":\n            #logging.info(\"Improving code...\")\n            t0 = time.time()\n            improved_code = autopy_func_improve(args.comments, code, node=args.node, port=args.port, temperature=args.temperature, max_tokens=args.max_tokens)\n            t1 = time.time()\n\n            if len(improved_code) > 0:\n                score = autopy_code_judge(improved_code, args.function_name, node=args.node, port=args.port)\n            else:\n                score = 0\n            t2 = time.time()\n\n            logging.info(f\"Generated improved code input len={len(code)} output len={len(improved_code)} in {t1 - t0} seconds, with new score {score} (scored in {t2 - t1} seconds)\")\n            result_queue.put((\"improve_code\", task_id, score, improved_code))\n\n        elif task_op == \"improve_test\":\n            #logging.info(\"Improving test...\")\n            t0 = time.time()\n            improved_test = autopy_test_improve(args.comments, args.prototype, args.function_name, test, node=args.node, port=args.port, temperature=args.temperature, max_tokens=args.max_tokens)\n            t1 = time.time()\n\n            logging.info(f\"Generated improved test input len={len(test)} output len={len(improved_test)} in {t1 - t0} seconds\")\n            result_queue.put((task_op, task_id, None, improved_test))\n\n        elif task_op == \"judge_pair\":\n            #logging.info(\"Judging pair...\")\n            t0 = time.time()\n            score = autopy_test_judge(code, args.function_name, test, node=args.node, port=args.port)\n            t1 = time.time()\n\n            logging.info(f\"Judged code/test pair with score {score} in {t1 - t0} seconds\")\n            result_queue.put((task_op, task_id, score, None))\n\n    def worker(self, work_queue, result_queue, worker_id):\n        args = self.args\n\n        while True:\n            try:\n                (task_op, task_id, code, test) = work_queue.get(timeout=2.0)\n            except Empty:\n                logging.info(f\"Worker {worker_id} idle... (2 seconds)\")\n                continue\n\n            with self.active_workers.get_lock():\n                self.active_workers.value += 1\n\n            try:\n                self.process_next(args, task_op, task_id, code, test, result_queue)\n            except Exception as e:\n                logging.error(f\"Worker {worker_id} error: {e}\")\n\n            with self.active_workers.get_lock():\n                self.active_workers.value -= 1\n\n    def launch(self, work_queue, result_queue):\n        for worker_id in range(self.args.workers):\n            p = multiprocessing.Process(target=self.worker, args=(work_queue, result_queue, worker_id))\n            p.start()\n            self.workers.append(p)\n\n    def terminate(self):\n        for worker in self.workers:\n            worker.terminate()\n\nclass JobManager:\n    def __init__(self, args):\n        self.past_jobs = []\n        self.work_queue = multiprocessing.Queue()\n        self.result_queue = multiprocessing.Queue()\n        self.next_job_id = 0\n        self.args = args\n        self.workers = JobWorkers(args)\n        self.workers.launch(self.work_queue, self.result_queue)\n\n    def _add_job(self, task_op, code = None, test = None):\n        job_id = self.next_job_id\n        job = (task_op, job_id, code, test)\n        self.past_jobs.append(job)\n        self.next_job_id += 1\n\n        self.work_queue.put(job)\n\n        return job_id\n\n    def add_code_job(self):\n        return self._add_job(\"code\")\n\n    def add_test_job(self):\n        return self._add_job(\"test\")\n\n    def add_improve_code_job(self, code):\n        return self._add_job(\"improve_code\", code=code)\n\n    def add_improve_test_job(self, test):\n        return self._add_job(\"improve_test\", test=test)\n\n    def add_judge_pair_job(self, code, test):\n        return self._add_job(\"judge_pair\", code=code, test=test)\n\n    def get_results(self, timeout: float = 1.0) -> List[Tuple[str, int, Any, Any]]:\n        results = []\n        try:\n            results.append(self.result_queue.get(timeout=timeout))\n            while not self.result_queue.empty():\n                results.append(self.result_queue.get(timeout=timeout))\n        except Empty:\n            pass\n        return results\n\n    def active_workers(self):\n        return self.workers.active_workers.value\n\n    def approx_queue_depth(self):\n        return self.work_queue.qsize()\n\n    def terminate(self):\n        self.workers.terminate()\n"}
{"type": "source_file", "path": "server/model_vicuna.py", "content": "import logging\n\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\nfrom model_tools import disable_torch_init\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_model_vicuna(model_name=\"vicuna-13b\", load_in_8bit=None):\n    if model_name == \"vicuna-13b\":\n        base_model_name = \"eachadea/vicuna-13b\"\n        if load_in_8bit is None:\n            load_in_8bit = True # Should fit on one GPU\n    elif model_name == \"vicuna-7b\":\n        base_model_name = \"AlekseyKorshuk/vicuna-7b\"\n        if load_in_8bit is None:\n            load_in_8bit = False # Should fit on one GPU\n    else:\n        raise Exception(\"Unknown model_name={model_name}\")\n\n    logging.info(f\"Loading base_model={base_model_name} load_in_8bit={load_in_8bit}...\")\n\n    disable_torch_init()\n\n    tokenizer = LlamaTokenizer.from_pretrained(base_model_name)\n    model = LlamaForCausalLM.from_pretrained(\n        base_model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        load_in_8bit=load_in_8bit,\n    )\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    logging.info(f\"Loaded base_model={base_model_name} load_in_8bit={load_in_8bit} with context length {context_len}\")\n\n    return tokenizer, model, context_len\n"}
{"type": "source_file", "path": "codegen/fix_ast_errors.py", "content": "import ast\nimport re\nfrom statistics import median\n\ndef detect_median_indentation(code):\n    indentation_counts = []\n    in_comment = False\n    for line in code.split('\\n'):\n        # Check if line starts with comment\n        if re.match(r'^\\s*#', line):\n            continue\n        # Check if line contains multi-line comment start\n        elif re.match(r'^\\s*\"\"\".*\"\"\"\\s*$', line):\n            continue\n        elif re.match(r'^\\s*\"\"\".*', line):\n            in_comment = True\n        # Check if line contains multi-line comment end\n        elif re.match(r'.*\"\"\".*$', line):\n            in_comment = False\n        # If not in multi-line comment, count the indentation level\n        elif not in_comment:\n            indentation_counts.append(len(line) - len(line.lstrip()))\n\n    if len(indentation_counts) > 0:\n        median_indentation = int(median(indentation_counts))\n    else:\n        median_indentation = 0\n\n    return median_indentation\n\ndef check_error(code):\n    try:\n        ast.parse(code)\n    except Exception as e:\n        return e\n    return None\n\ndef indent_line(code, e, delta=1):\n    lines = code.split('\\n')\n    error_line = lines[e.lineno - 1]\n\n    current_indent_level = len(error_line) - len(error_line.lstrip())\n\n    new_indent_level = current_indent_level + delta\n\n    # Add correct indentation to the problematic line\n    fixed_line = ' ' * new_indent_level + error_line.lstrip()\n    lines[e.lineno - 1] = fixed_line\n\n    # Update the code\n    code = '\\n'.join(lines)\n\n    return code\n\ndef try_indenting(code, e, delta=1):\n    orig_code = code\n    for i in range(1, 8):\n        code = indent_line(code, e, delta=delta)\n\n        #print(f\"MODIFIED CODE: {code}\")\n\n        ne = check_error(code)\n\n        #print(f\"NE: {ne}\")\n\n        if ne is None or ne.lineno != e.lineno:\n            return True, code\n    return False, orig_code\n\ndef try_adding_colon(code, e):\n    orig_code = code\n\n    lines = code.split('\\n')\n    error_line = lines[e.lineno - 1]\n\n    error_line += \":\"\n\n    lines[e.lineno - 1] = error_line\n\n    code = '\\n'.join(lines)\n\n    #print(f\"MODIFIED CODE: {code}\")\n\n    ne = check_error(code)\n\n    if ne is None or ne.lineno != e.lineno:\n        return True, code\n    return False, orig_code\n\ndef close_line(code, lineno, delim=\")\", start=True):\n    lines = code.split('\\n')\n    error_line = lines[lineno - 1]\n\n    if start:\n        error_line = delim + error_line\n    else:\n        error_line += delim\n\n    lines[lineno - 1] = error_line\n\n    code = '\\n'.join(lines)\n\n    return code\n\ndef try_closing_delim(code, e, delim):\n    orig_code = code\n    try:\n        for i in range(100):\n            if i > 0:\n                code = close_line(code, e.lineno + i, delim=delim, start=True)\n                #print(\"try_closing_delim CODE: {}\".format(code))\n                ne = check_error(code)\n                #print(\"try_closing_delim NE: {}\".format(ne))\n                if ne is None or ne.lineno != e.lineno or str(ne) != str(e):\n                    return True, code\n\n            code = close_line(code, e.lineno + i, delim=delim, start=False)\n            #print(\"try_closing_delim CODE: {}\".format(code))\n            ne = check_error(code)\n            #print(\"try_closing_delim NE: {}\".format(ne))\n            if ne is None or ne.lineno != e.lineno or str(ne) != str(e):\n                return True, code\n    except Exception as e:\n        pass\n    return False, orig_code\n\ndef extract_mismatched_delimiters(error_message):\n    pattern = r\"closing parenthesis '(\\S)' does not match opening parenthesis '(\\S)'\"\n    match = re.search(pattern, error_message)\n\n    if match:\n        closing_char, opening_char = match.groups()\n        return closing_char, opening_char\n    else:\n        return None\n\ndef replace_final_closing_char(input_string, closing_char, opening_char):\n    last_closing_index = input_string.rfind(closing_char)\n    if last_closing_index != -1:\n        input_string = input_string[:last_closing_index] + opening_char + input_string[last_closing_index + 1:]\n    return input_string\n\ndef flip_opening_delimiters(input_string):\n    opening_chars = \"{[(\"\n    closing_chars = \"}])\"\n    delimiter_map = dict(zip(opening_chars, closing_chars))\n\n    for opening, closing in delimiter_map.items():\n        input_string = input_string.replace(opening, closing)\n\n    return input_string\n\ndef try_replacing_closing_with_opening(code, e, closing_char, opening_char):\n    orig_code = code\n\n    lines = code.split('\\n')\n    error_line = lines[e.lineno - 1]\n\n    error_line = replace_final_closing_char(error_line, closing_char, opening_char)\n\n    lines[e.lineno - 1] = error_line\n\n    code = '\\n'.join(lines)\n\n    ne = check_error(code)\n\n    if ne is None or ne.lineno != e.lineno or str(ne) != str(e):\n        return True, code\n    return False, orig_code\n\ndef parse_unbalanced_paren(error_message):\n    match = re.search(r\"'(\\(|\\)|\\[|\\]|\\{|\\})' was never closed\", error_message)\n    if match:\n        return match.group(1)\n    return None\n\ndef fix_ast_errors(code, max_attempts=100, expandtabs=True, delete_on_error=True):\n    median_tab_spaces = detect_median_indentation(code)\n    if median_tab_spaces <= 1:\n        median_tab_spaces = 4\n\n    if expandtabs:\n        code = code.expandtabs(median_tab_spaces)\n\n    attempts = 0\n\n    while attempts < max_attempts:\n        try_indent = False\n        try_unindent = False\n        close_delim = None\n        error = None\n\n        try:\n            ast.parse(code)\n            break\n        except Exception as e:\n            error = e\n            #print(\"ERROR: {}\".format(e))\n            #print(\"CODE:\\n---\\n{}\\n---\\n\".format(code))\n            if \"expected ':'\" in e.msg:\n                r, code = try_adding_colon(code, e)\n                if r: continue\n            elif \"expected an indented block\" in e.msg:\n                try_indent = True\n            elif \"unexpected indent\" in e.msg or \"unindent does not match\" in e.msg:\n                try_unindent = True\n            elif \"was never closed\" in e.msg:\n                close_delim = parse_unbalanced_paren(e.msg)\n                if close_delim:\n                    r, code = try_closing_delim(code, error, flip_opening_delimiters(close_delim))\n                    if r: continue\n            elif \"does not match opening parenthesis\" in e.msg:\n                closing, opening = extract_mismatched_delimiters(e.msg)\n                r, code = try_replacing_closing_with_opening(code, e, closing, flip_opening_delimiters(opening))\n                if r: continue\n            else:\n                try_indent = True\n                try_unindent = True\n\n        if error is None:\n            break\n\n        if try_indent:\n            r, code = try_indenting(code, error, delta=1)\n            if r: continue\n        if try_unindent:\n            r, code = try_indenting(code, error, delta=-1)\n            if r: continue\n\n        # Give up if we can't fix the error and don't want to delete the code\n        if not delete_on_error:\n            break\n\n        #print(\"FAILED TO FIX ERROR: {}\".format(error))\n\n        # Delete the line that caused the error\n        lines = code.split('\\n')\n        del lines[error.lineno - 1]\n        code = '\\n'.join(lines)\n\n        attempts += 1\n\n    return code\n"}
{"type": "source_file", "path": "server/gptq.py", "content": "# Copied from https://github.com/catid/GPTQ-for-LLaMa-65B-2GPU\n\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nimport builtins\nimport math\nimport time\nfrom transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb\nfrom transformers.utils.hub import cached_file\nimport transformers\nimport triton\nimport triton.language as tl\nfrom typing import Dict\n\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\nclass Autotuner(triton.KernelInterface):\n\tdef __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, nearest_power_of_two: bool = False):\n\t\t'''\n\t\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t\t'top_k': number of configs to bench\n\t\t\t'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n\t\t\t'nearest_power_of_two'(optional): whether to round key arguments to the nearest power of two when caching tuning results\n\t\t'''\n\t\tif not configs:\n\t\t\tself.configs = [triton.Config({}, num_warps=4, num_stages=2)]\n\t\telse:\n\t\t\tself.configs = configs\n\t\tself.key_idx = [arg_names.index(k) for k in key]\n\t\tself.nearest_power_of_two = nearest_power_of_two\n\t\tself.cache = {}\n\t\t# hook to reset all required tensor to zeros before relaunching a kernel\n\t\tself.hook = lambda args: 0\n\t\tif reset_to_zero is not None:\n\t\t\tself.reset_idx = [arg_names.index(k) for k in reset_to_zero]\n\n\t\t\tdef _hook(args):\n\t\t\t\tfor i in self.reset_idx:\n\t\t\t\t\targs[i].zero_()\n\t\t\tself.hook = _hook\n\t\tself.arg_names = arg_names\n\t\t# prune configs\n\t\tif prune_configs_by:\n\t\t\tperf_model, top_k = prune_configs_by['perf_model'], prune_configs_by['top_k']\n\t\t\tif 'early_config_prune' in prune_configs_by:\n\t\t\t\tearly_config_prune = prune_configs_by['early_config_prune']\n\t\telse:\n\t\t\tperf_model, top_k, early_config_prune = None, None, None\n\t\tself.perf_model, self.configs_top_k = perf_model, top_k\n\t\tself.early_config_prune = early_config_prune\n\t\tself.fn = fn\n\n\tdef _bench(self, *args, config, **meta):\n\t\t# check for conflicts, i.e. meta-parameters both provided\n\t\t# as kwargs and by the autotuner\n\t\tconflicts = meta.keys() & config.kwargs.keys()\n\t\tif conflicts:\n\t\t\traise ValueError(\n\t\t\t\tf\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\n\t\t\t\t\" Make sure that you don't re-define auto-tuned symbols.\"\n\t\t\t)\n\t\t# augment meta-parameters with tunable ones\n\t\tcurrent = dict(meta, **config.kwargs)\n\n\t\tdef kernel_call():\n\t\t\tif config.pre_hook:\n\t\t\t\tconfig.pre_hook(self.nargs)\n\t\t\tself.hook(args)\n\t\t\tself.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n\t\ttry:\n\t\t\t# In testings using only 40 reps seems to be close enough and it appears to be what PyTorch uses\n\t\t\t# PyTorch also sets fast_flush to True, but I didn't see any speedup so I'll leave the default\n\t\t\treturn triton.testing.do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8), rep=40)\n\t\texcept triton.compiler.OutOfResources:\n\t\t\treturn (float('inf'), float('inf'), float('inf'))\t\n\n\tdef run(self, *args, **kwargs):\n\t\tself.nargs = dict(zip(self.arg_names, args))\n\t\tif len(self.configs) > 1:\n\t\t\tkey = tuple(args[i] for i in self.key_idx)\n\n\t\t\t# This reduces the amount of autotuning by rounding the keys to the nearest power of two\n\t\t\t# In my testing this gives decent results, and greatly reduces the amount of tuning required\n\t\t\tif self.nearest_power_of_two:\n\t\t\t\tkey = tuple([2 ** int(math.log2(x) + 0.5) for x in key])\n\t\t\t\n\t\t\tif key not in self.cache:\n\t\t\t\t# prune configs\n\t\t\t\tpruned_configs = self.prune_configs(kwargs)\n\t\t\t\tbench_start = time.time()\n\t\t\t\ttimings = {config: self._bench(*args, config=config, **kwargs)\n\t\t\t\t\t\t\tfor config in pruned_configs}\n\t\t\t\tbench_end = time.time()\n\t\t\t\tself.bench_time = bench_end - bench_start\n\t\t\t\tself.cache[key] = builtins.min(timings, key=timings.get)\n\t\t\t\tself.hook(args)\n\t\t\t\tself.configs_timings = timings\n\t\t\tconfig = self.cache[key]\n\t\telse:\n\t\t\tconfig = self.configs[0]\n\t\tself.best_config = config\n\t\tif config.pre_hook is not None:\n\t\t\tconfig.pre_hook(self.nargs)\n\t\treturn self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n\n\tdef prune_configs(self, kwargs):\n\t\tpruned_configs = self.configs\n\t\tif self.early_config_prune:\n\t\t\tpruned_configs = self.early_config_prune(self.configs, self.nargs)\n\t\tif self.perf_model:\n\t\t\ttop_k = self.configs_top_k\n\t\t\tif isinstance(top_k, float) and top_k <= 1.0:\n\t\t\t\ttop_k = int(len(self.configs) * top_k)\n\t\t\tif len(pruned_configs) > top_k:\n\t\t\t\test_timing = {\n\t\t\t\t\tconfig: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages,\n\t\t\t\t\t\t\t\t\t\t\tnum_warps=config.num_warps)\n\t\t\t\t\tfor config in pruned_configs\n\t\t\t\t}\n\t\t\t\tpruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n\t\treturn pruned_configs\n\n\tdef warmup(self, *args, **kwargs):\n\t\tself.nargs = dict(zip(self.arg_names, args))\n\t\tfor config in self.prune_configs(kwargs):\n\t\t\tself.fn.warmup(\n\t\t\t\t*args,\n\t\t\t\tnum_warps=config.num_warps,\n\t\t\t\tnum_stages=config.num_stages,\n\t\t\t\t**kwargs,\n\t\t\t\t**config.kwargs,\n\t\t\t)\n\t\tself.nargs = None\n\ndef autotune(configs, key, prune_configs_by=None, reset_to_zero=None, nearest_power_of_two=False):\n\t\"\"\"\n\tDecorator for auto-tuning a :code:`triton.jit`'d function.\n\t.. highlight:: python\n\t.. code-block:: python\n\t\t@triton.autotune(configs=[\n\t\t\ttriton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n\t\t\ttriton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n\t\t\t],\n\t\t\tkey=['x_size'] # the two above configs will be evaluated anytime\n\t\t\t\t\t\t\t# the value of x_size changes\n\t\t)\n\t\t@triton.jit\n\t\tdef kernel(x_ptr, x_size, **META):\n\t\t\tBLOCK_SIZE = META['BLOCK_SIZE']\n\t:note: When all the configurations are evaluated, the kernel will run multiple time.\n\t\t\tThis means that whatever value the kernel updates will be updated multiple times.\n\t\t\tTo avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n\t\t\treset the value of the provided tensor to `zero` before running any configuration.\n\t:param configs: a list of :code:`triton.Config` objects\n\t:type configs: list[triton.Config]\n\t:param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n\t:type key: list[str]\n\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t'top_k': number of configs to bench\n\t\t'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n\t:param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n\t:type reset_to_zero: list[str]\n\t\"\"\"\n\tdef decorator(fn):\n\t\treturn Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, nearest_power_of_two)\n\n\treturn decorator\n\ndef matmul4_kernel_config_pruner(configs, nargs):\n    \"\"\"\n    The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.\n    \"\"\"\n    m = max(2 ** int(math.ceil(math.log2(nargs['M']))), 16)\n    n = max(2 ** int(math.ceil(math.log2(nargs['N']))), 16)\n    k = max(2 ** int(math.ceil(math.log2(nargs['K']))), 16)\n\n    used = set()\n    for config in configs:\n        block_size_m = min(m, config.kwargs['BLOCK_SIZE_M'])\n        block_size_n = min(n, config.kwargs['BLOCK_SIZE_N'])\n        block_size_k = min(k, config.kwargs['BLOCK_SIZE_K'])\n        group_size_m = config.kwargs['GROUP_SIZE_M']\n\n        if (block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps) in used:\n            continue\n\n        used.add((block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps))\n        yield triton.Config({'BLOCK_SIZE_M': block_size_m, 'BLOCK_SIZE_N': block_size_n, 'BLOCK_SIZE_K': block_size_k, 'GROUP_SIZE_M': group_size_m}, num_stages=config.num_stages, num_warps=config.num_warps)\n\n# code based https://github.com/fpgaminer/GPTQ-triton\n@autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n    nearest_power_of_two=True,\n    prune_configs_by={\n        'early_config_prune': matmul4_kernel_config_pruner,\n        'perf_model': None,\n        'top_k': None,\n    },\n)\n\n@triton.jit\ndef matmul_248_kernel(a_ptr, b_ptr, c_ptr,\n                        scales_ptr, zeros_ptr, g_ptr,\n                        M, N, K, bits, maxq,\n                        stride_am, stride_ak,\n                        stride_bk, stride_bn,\n                        stride_cm, stride_cn,\n                        stride_scales, stride_zeros,\n                        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n                        GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32 \n    \"\"\"\n    infearure_per_bits = 32 // bits\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    g_ptrs = g_ptr + offs_k\n    # shifter is used to extract the N bits of each element in the 32-bit word from B\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + (offs_bn[None, :]// infearure_per_bits) \n    \n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n            \n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n        \n        zeros = (zeros >> zeros_shifter[None, :]) & maxq\n        zeros = (zeros + 1)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)   # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\n        # Now we need to unpack b (which is N-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n        b = (b - zeros) * scales  # Scale and shift\n\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n# code based https://github.com/fpgaminer/GPTQ-triton\ndef transpose_matmul4_kernel_config_pruner(configs, nargs):\n    \"\"\"\n    The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.\n    \"\"\"\n    m = max(2 ** int(math.ceil(math.log2(nargs['M']))), 16)\n    n = max(2 ** int(math.ceil(math.log2(nargs['N']))), 16)\n    k = max(2 ** int(math.ceil(math.log2(nargs['K']))), 16)\n\n    used = set()\n    for config in configs:\n        block_size_m = min(m, config.kwargs['BLOCK_SIZE_M'])\n        block_size_n = min(n, config.kwargs['BLOCK_SIZE_N'])\n        block_size_k = min(k, config.kwargs['BLOCK_SIZE_K'])\n        group_size_m = config.kwargs['GROUP_SIZE_M']\n\n        if (block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps) in used:\n            continue\n\n        used.add((block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps))\n        yield triton.Config({'BLOCK_SIZE_M': block_size_m, 'BLOCK_SIZE_N': block_size_n, 'BLOCK_SIZE_K': block_size_k, 'GROUP_SIZE_M': group_size_m}, num_stages=config.num_stages, num_warps=config.num_warps)\n\n@autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n    nearest_power_of_two=True,\n    prune_configs_by={\n        'early_config_prune': transpose_matmul4_kernel_config_pruner,\n        'perf_model': None,\n        'top_k': None,\n    },\n)\n\n@triton.jit\ndef transpose_matmul_248_kernel(a_ptr, b_ptr, c_ptr,\n                                scales_ptr, zeros_ptr, g_ptr,\n                                M, N, K, bits, maxq,\n                                stride_am, stride_ak,\n                                stride_bk, stride_bn,\n                                stride_cm, stride_cn,\n                                stride_scales, stride_zeros,\n                                BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n                                GROUP_SIZE_M: tl.constexpr):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32 \n    \"\"\"\n    infearure_per_bits = 32 // bits\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_k = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_bk[:, None] // infearure_per_bits) * stride_bk + offs_n[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    \n    # shifter is used to extract the N bits of each element in the 32-bit word from B\n    scales_ptrs = scales_ptr + offs_n[None, :]  + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + (offs_n[None, :]// infearure_per_bits) + g_idx[:, None] * stride_zeros\n    \n    shifter = (offs_bk % infearure_per_bits) * bits\n    zeros_shifter = (offs_n % infearure_per_bits) * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    \n    for k in range(0, num_pid_n):\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n        \n        zeros = (zeros >> zeros_shifter[None, :]) & maxq\n        zeros = (zeros + 1)\n        \n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n        b = tl.load(b_ptrs)   # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\n        # Now we need to unpack b (which is N-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n        b = (b - zeros) * scales  # Scale and shift\n        b = tl.trans(b)\n\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += (BLOCK_SIZE_N // infearure_per_bits)\n        \n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):\n    with torch.cuda.device(input.device):\n        output = torch.empty((input.shape[0], qweight.shape[1]), device='cuda', dtype=torch.float16)\n        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']),)\n        matmul_248_kernel[grid](input, qweight, output,\n                                scales, qzeros, g_idx,\n                                input.shape[0], qweight.shape[1], input.shape[1], bits, maxq,\n                                input.stride(0), input.stride(1),\n                                qweight.stride(0), qweight.stride(1),\n                                output.stride(0), output.stride(1),\n                                scales.stride(0), qzeros.stride(0))\n        return output\n\ndef transpose_matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):\n    with torch.cuda.device(input.device):\n        output_dim = (qweight.shape[0] * 32) // bits\n        output = torch.empty((input.shape[0], output_dim), device='cuda', dtype=torch.float16)\n        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']),)\n        transpose_matmul_248_kernel[grid](input, qweight, output,\n                                        scales, qzeros, g_idx,\n                                        input.shape[0], qweight.shape[1], output_dim, bits, maxq,\n                                        input.stride(0), input.stride(1),\n                                        qweight.stride(0), qweight.stride(1),\n                                        output.stride(0), output.stride(1),\n                                        scales.stride(0), qzeros.stride(0))\n        return output\n\nclass QuantLinearFunction(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float16)\n    def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n        output = matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq)\n        ctx.save_for_backward(qweight, scales, qzeros, g_idx)\n        ctx.bits,ctx.maxq = bits, maxq\n        return output\n    \n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output):\n        qweight, scales, qzeros, g_idx = ctx.saved_tensors\n        bits, maxq = ctx.bits, ctx.maxq\n        grad_input = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = transpose_matmul248(grad_output, qweight, scales, qzeros, g_idx, bits, maxq)\n        return grad_input, None, None, None, None, None, None\n    \nclass QuantLinear(nn.Module): \n    def __init__(self, bits, groupsize, infeatures, outfeatures, bias):\n        super().__init__()\n        if bits not in [2,4,8]:\n            raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n        self.infeatures = infeatures\n        self.outfeatures = outfeatures\n        self.bits = bits\n        self.maxq = 2 ** self.bits - 1\n        self.groupsize = groupsize if groupsize != -1 else infeatures\n        \n        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\n        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))\n        self.register_buffer('scales', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))\n        self.register_buffer('g_idx', torch.tensor([i // self.groupsize  for i in range(infeatures)], dtype = torch.int32))\n        if bias:\n            self.register_buffer('bias', torch.zeros((outfeatures),dtype=torch.float16))\n        else:\n            self.bias = None\n        \n    def pack(self, linear, scales, zeros, g_idx = None):\n        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx\n        \n        scales = scales.t().contiguous()\n        zeros = zeros.t().contiguous()\n        scale_zeros = zeros * scales\n        self.scales = scales.clone().half()\n        if linear.bias is not None:\n            self.bias = linear.bias.clone().half()\n            \n        intweight = []\n        for idx in range(self.infeatures):\n            intweight.append(torch.round((linear.weight.data[:,idx] + scale_zeros[self.g_idx[idx]]) / self.scales[self.g_idx[idx]]).to(torch.int)[:,None])\n        intweight = torch.cat(intweight,dim=1)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.numpy().astype(np.uint32)\n        qweight = np.zeros((intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32)\n        i = 0\n        row = 0\n        while row < qweight.shape[0]:\n            if self.bits in [2,4,8]:\n                for j in range(i, i + (32//self.bits)):\n                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n                i += 32//self.bits\n                row += 1\n            else:\n                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n                \n        qweight = qweight.astype(np.int32)\n        self.qweight = torch.from_numpy(qweight) \n        \n        zeros -= 1;\n        zeros = zeros.numpy().astype(np.uint32)\n        qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32)\n        i = 0\n        col = 0\n        while col < qzeros.shape[1]:\n            if self.bits in [2,4,8]:\n                for j in range(i, i + (32//self.bits)):\n                    qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))\n                i += 32//self.bits\n                col += 1\n            else:\n                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n                \n        qzeros = qzeros.astype(np.int32)\n        self.qzeros = torch.from_numpy(qzeros) \n        \n    def forward(self, x):\n        out_shape = x.shape[:-1] + (self.outfeatures, )\n        out = QuantLinearFunction.apply(x.reshape(-1,x.shape[-1]), self.qweight, self.scales, \n                                        self.qzeros, self.g_idx, self.bits, self.maxq)\n        out = out + self.bias if self.bias is not None else out  \n        return out.reshape(out_shape)\n        \ndef make_quant_attn(model):\n    \"\"\"\n    Replace all LlamaAttention modules with QuantLlamaAttention modules, fusing the q, k, v projections.\n    \"\"\"\n    for name, m in model.named_modules():\n        if not isinstance(m, LlamaAttention):\n            continue\n\n        q_proj = m.q_proj\n        k_proj = m.k_proj\n        v_proj = m.v_proj\n\n        qweights = torch.cat([q_proj.qweight, k_proj.qweight, v_proj.qweight], dim=1)\n        qzeros = torch.cat([q_proj.qzeros, k_proj.qzeros, v_proj.qzeros], dim=1)\n        scales = torch.cat([q_proj.scales, k_proj.scales, v_proj.scales], dim=1)\n        g_idx = torch.cat([q_proj.g_idx, k_proj.g_idx, v_proj.g_idx], dim=0)\n        bias = torch.cat([q_proj.bias, k_proj.bias, v_proj.bias], dim=0) if q_proj.bias is not None else None\n\n        qkv_layer = QuantLinear(q_proj.bits, q_proj.groupsize, q_proj.infeatures, q_proj.outfeatures + k_proj.outfeatures + v_proj.outfeatures, True if q_proj.bias is not None else False)\n        qkv_layer.qweight = qweights\n        qkv_layer.qzeros = qzeros\n        qkv_layer.scales = scales\n        qkv_layer.g_idx = g_idx\n        qkv_layer.bias = bias\n\n        attn = QuantLlamaAttention(m.hidden_size, m.num_heads, qkv_layer, m.o_proj, m.rotary_emb)\n\n        if '.' in name:\n            parent_name = name.rsplit('.', 1)[0]\n            child_name = name[len(parent_name) + 1:]\n            parent = model.get_submodule(parent_name)\n        else:\n            parent_name = ''\n            parent = model\n            child_name = name\n\n        #print(f\"Replacing {name} with quant_attn; parent: {parent_name}, child's name: {child_name}\")\n\n        setattr(parent, child_name, attn)\n\n\nclass QuantLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self,hidden_size,num_heads,qkv_proj,o_proj,rotary_emb,):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        if (self.head_dim * num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"f\" and `num_heads`: {num_heads}).\")\n        self.qkv_proj = qkv_proj\n        self.o_proj = o_proj\n        self.rotary_emb = rotary_emb\n\n    def _shape(self, tensor, seq_len, bsz):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(self,hidden_states,past_key_value = None,attention_mask = None,position_ids = None, output_attentions = False,use_cache= False):\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, q_len, _ = hidden_states.size()\n\n        qkv_states = self.qkv_proj(hidden_states)\n        query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n        \ndef autotune_warmup(model, transpose = False):\n    \"\"\"\n    Pre-tunes the quantized kernel\n    \"\"\"\n    from tqdm import tqdm\n\n    kn_values  = {}\n\n    for _, m in model.named_modules():\n        if not isinstance(m, QuantLinear):\n            continue\n\n        k = m.infeatures\n        n = m.outfeatures\n        \n        if (k, n) not in kn_values:\n            kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq)\n\n    print(f'Found {len(kn_values)} unique KN values.')\n\n    print('Warming up autotune cache ...')\n    with torch.no_grad():\n        for m in tqdm(range(0, 12)):\n            m = 2 ** m   # [1, 2048]\n            for (k, n), (qweight, scales, qzeros, g_idx, bits, maxq) in kn_values.items():\n                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n                matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)\n                if transpose:\n                    a = torch.randn(m, n, dtype=torch.float16, device='cuda')\n                    transpose_matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)\n    del kn_values\n        \ndef make_quant(module, names, bits, groupsize, name=''):\n    if isinstance(module, QuantLinear):\n        return\n    for attr in dir(module):\n        tmp = getattr(module, attr)\n        name1 = name + '.' + attr if name != '' else attr\n        if name1 in names:\n            delattr(module, attr)\n            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))\n    for name1, child in module.named_children():\n        make_quant(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1)\n\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(\n            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n        ))\n    return res\n\ndef load_quant(model_url, checkpoint, wbits, groupsize, device, warmup_autotune = True):\n    from transformers import LlamaConfig, LlamaForCausalLM \n    config = LlamaConfig.from_pretrained(model_url)\n    def noop(*args, **kwargs):\n        pass\n    torch.nn.init.kaiming_uniform_ = noop \n    torch.nn.init.uniform_ = noop \n    torch.nn.init.normal_ = noop \n\n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.float)\n    model = model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name in layers:\n            del layers[name]\n    make_quant(model, layers, wbits, groupsize)\n\n    checkpoint = cached_file(model_url, filename=checkpoint)\n\n    print(f\"Loading model {checkpoint} ...\")\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import load_file as safe_load\n        if device == -1:\n            device = \"cpu\"\n        model.load_state_dict(safe_load(checkpoint, device))\n    else:\n        model.load_state_dict(torch.load(checkpoint))\n        \n    make_quant_attn(model)\n\n    if warmup_autotune:\n        autotune_warmup(model)\n    model.seqlen = 2048\n    print('Done.')\n\n    return model\n"}
{"type": "source_file", "path": "server/benchmark_model.py", "content": "# Use modules from parent folder\nimport os, sys\nparent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.append(parent_dir)\n\nimport time\nimport logging\nimport argparse\nfrom language_model import LanguageModel\nfrom prompts.ask_templates import ask_assistant, ask_python_coder, ask_python_function_prototype, ask_python_analyzer, ask_python_code_judge\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_model(m, prompt, stop_strs, max_tokens, temperature):\n    start_time = time.time()\n\n    r = m.ask(\n        prompt=prompt,\n        stop_strs=stop_strs,\n        temperature=temperature,\n        max_new_tokens=max_tokens)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    prompt_length = len(prompt)\n    response_length = len(r)\n    response_chars_per_second = response_length / duration\n\n    logging.info(f\"Response:\\n\\n{r}\")\n    logging.info(f\"Prompt length: {prompt_length} characters\")\n    logging.info(f\"Response length: {response_length} characters. Characters per second: {response_chars_per_second:.2f}. Time: {duration:.2f} seconds\")\n\ndef main(args):\n    start_time = time.time()\n    m = LanguageModel(args.model, load_in_8bit=args.load_in_8bit)\n    end_time = time.time()\n    logging.info(f\"Model loaded in {end_time - start_time:.2f} seconds\")\n\n    # Test ask_assistant\n\n    messages = [\n        {\n            \"role\": \"Human\",\n            \"content\": \"What is the best way to make money with a 100W laser cutter?\"\n        }\n    ]\n\n    prompt, stop_strs = ask_assistant(messages)\n    #logging.info(f\"Testing ask_assistant:\\n\\n{prompt}\")\n    #logging.info(f\"Early stop strings: {stop_strs}\")\n\n    test_model(m, prompt, stop_strs, max_tokens=args.max_tokens, temperature=args.temperature)\n\n    # Test ask_python_coder\n\n    messages = [\n        {\n            \"role\": \"Human\",\n            \"content\": \"Write a Python function that returns the factorial of a number.\"\n        }\n    ]\n\n    prompt, stop_strs = ask_python_coder(messages)\n    #logging.info(f\"\\n\\nTesting ask_python_coder: {prompt}\")\n    #logging.info(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    test_model(m, prompt, stop_strs, max_tokens=args.max_tokens, temperature=args.temperature)\n\n    # Test ask_python_function_prototype\n\n    prompt, stop_strs = ask_python_function_prototype(\"# Returns the factorial of n\", \"def factorial(n):\")\n    #logging.info(f\"\\n\\nTesting ask_python_function_prototype: {prompt}\")\n    #logging.info(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    test_model(m, prompt, stop_strs, max_tokens=args.max_tokens, temperature=args.temperature)\n\n    # Test ask_python_analyzer\n\n    prompt, stop_strs = ask_python_analyzer(\"# Returns the factorial of n\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\")\n    #logging.info(f\"\\n\\nTesting ask_python_analyzer: {prompt}\")\n    #logging.info(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    test_model(m, prompt, stop_strs, max_tokens=args.max_tokens, temperature=args.temperature)\n\n    # Test ask_python_code_judge\n\n    prompt, stop_strs = ask_python_code_judge(\"# Returns the factorial of n\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\", \"factorial\")\n    #logging.info(f\"\\n\\nTesting ask_python_code_judge: {prompt}\")\n    #logging.info(f\"Early stop strings: {stop_strs}\\n\\n\")\n\n    test_model(m, prompt, stop_strs, max_tokens=4, temperature=args.temperature)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Unit test for BaizeModel\")\n    parser.add_argument(\"--temperature\", type=float, help=\"Temperature for text generation (default: 0.0)\", default=0.0)\n    parser.add_argument(\"--max-tokens\", type=int, help=\"Maximum number of tokens in generated text (default: 1024)\", default=1024)\n    parser.add_argument(\"--model\", type=str, help=\"Select model to use (default: galpaca-30b). Available options: baize-30b, baize-13b, baize-7b, galpaca-30b, galpaca-7b, koala-13b, koala-7b, vicuna-13b, vicuna-7b, llama-65b-4bit\", default=\"llama-65b-4bit\")\n    parser.add_argument(\"--8bit\", action=\"store_true\", help=\"Use 8-bit precision (default: False)\")\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Use 16-bit precision (default: False)\")\n\n    args = parser.parse_args()\n\n    if getattr(args, \"8bit\"):\n        logging.info(\"8-bit precision selected.\")\n        args.load_in_8bit = True\n    elif args.fp16:\n        logging.info(\"16-bit precision selected.\")\n        args.load_in_8bit = False\n    else:\n        args.load_in_8bit = None\n\n    main(args)\n"}
{"type": "source_file", "path": "server/load_balancer.py", "content": "import argparse\nimport asyncio\nimport random\nimport time\nimport logging\n\nimport aiohttp\nfrom fastapi import FastAPI, HTTPException, Request\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI()\nnodes = []\nbusy_nodes = set()\n\nasync def fetch_node_data(session: aiohttp.ClientSession, url: str, data) -> dict:\n    async with session.post(url, json=data) as response:\n        response.raise_for_status()\n        return await response.json()\n\n@app.post(\"/ask\")\nasync def ask_endpoint(request: Request):\n    data = await request.json()\n\n    start_time = time.time()\n\n    global nodes, busy_nodes\n\n    for node in nodes:\n        if node in busy_nodes:\n            continue\n\n        node_url = f\"http://{node}/ask\"\n\n        try:\n            busy_nodes.add(node)\n\n            logging.info(f\"Trying on {node_url}\")\n            try_start_time = time.time()\n\n            async with aiohttp.ClientSession() as session:\n                response_data = await fetch_node_data(session, node_url, data)\n\n            logging.info(f\"Completed on {node_url} in {time.time() - try_start_time:.2f} seconds (overall delay: {time.time() - start_time:.2f} seconds)\")\n            return response_data\n        except (aiohttp.ClientError, aiohttp.ClientResponseError):\n            logging.info(f\"Node unreachable: {node_url}\")\n            pass\n        finally:\n            busy_nodes.remove(node)\n\n    raise HTTPException(status_code=503, detail=\"All nodes are currently unavailable\")\n\ndef read_node_addresses(filename=\"load_balancer_nodes.txt\"):\n    with open(filename, 'r') as f:\n        lines = [line.strip() for line in f.readlines() if line.strip() and not line.startswith('#')]\n    return lines\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Load balancer\")\n    parser.add_argument(\"--nodes\", type=str, nargs=\"+\", help=\"List of node addresses\")\n    parser.add_argument(\"--listen\", type=int, default=8000, help=\"Load balancer port\")\n    args = parser.parse_args()\n\n    global nodes\n\n    if args.nodes is None:\n        nodes = read_node_addresses()\n        if not nodes:\n            raise ValueError(\"No node addresses found in load_balancer_nodes.txt\")\n    else:\n        nodes = args.nodes\n\n    random.shuffle(nodes)\n\n    # Start the FastAPI server\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=args.listen)\n\nif __name__ == \"__main__\":\n    main()\n"}
