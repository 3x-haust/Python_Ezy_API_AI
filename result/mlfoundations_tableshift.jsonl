{"repo_info": {"repo_name": "tableshift", "repo_owner": "mlfoundations", "repo_url": "https://github.com/mlfoundations/tableshift"}}
{"type": "test_file", "path": "tableshift/tests/test_feature_list.py", "content": "\"\"\"\nTests for FeatureLists.\n\nTo run tests: python -m unittest tableshift/tests/test_feature_list.py -v\n\"\"\"\nimport unittest\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\n\nclass TestFeatureList(unittest.TestCase):\n    def test_add_features(self):\n        fl1 = FeatureList(features=[\n            Feature(\"intfeat1\", int),\n            Feature(\"floatfeat1\", float),\n            Feature(\"catfeat1\", cat_dtype)])\n        fl2 = FeatureList(features=[\n            Feature(\"intfeat2\", int),\n            Feature(\"floatfeat2\", float),\n            Feature(\"catfeat2\", cat_dtype)])\n        new_fl = fl1 + fl2\n\n        # Check output feature list\n        self.assertListEqual(\n            sorted(new_fl.names),\n            sorted([\"intfeat1\", \"floatfeat1\", \"catfeat1\",\n                    \"intfeat2\", \"floatfeat2\", \"catfeat2\"]))\n\n        # Check that original FeatureList objects are not modified.\n        self.assertListEqual(sorted(fl1.names),\n                             sorted([\"intfeat1\", \"floatfeat1\", \"catfeat1\"]))\n        self.assertListEqual(sorted(fl2.names),\n                             sorted([\"intfeat2\", \"floatfeat2\", \"catfeat2\"]))\n\n    def test_add_multi_target(self):\n        fl1 = FeatureList(features=[\n            Feature(\"intfeat1\", int),\n            Feature(\"floatfeat1\", float, is_target=True)])\n        fl2 = FeatureList(features=[\n            Feature(\"intfeat2\", int),\n            Feature(\"floatfeat2\", float, is_target=True)])\n        with self.assertRaises(ValueError):\n            fl1 + fl2\n"}
{"type": "test_file", "path": "tableshift/tests/test_tabular_dataset.py", "content": "\"\"\"\nTests for TabularDatasets.\n\nTo run tests: python -m unittest tableshift/tests/test_tabular_dataset.py -v\n\"\"\"\nimport logging\nimport tempfile\nimport unittest\n\nfrom tableshift.core import TabularDataset, RandomSplitter, \\\n    DatasetConfig, PreprocessorConfig, CachedDataset\n\nLOG_LEVEL = logging.DEBUG\n\nlogger = logging.getLogger()\nlogging.basicConfig(\n    format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',\n    level=LOG_LEVEL,\n    datefmt='%Y-%m-%d %H:%M:%S')\n\n\nclass TestTabularDataset(unittest.TestCase):\n    def test_init_dataset_german(self):\n        \"\"\"\n        Initialize the German dataset. Checks that an exception is not raised.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as td:\n            _ = TabularDataset(name=\"german\",\n                               config=DatasetConfig(cache_dir=td),\n                               grouper=None,\n                               splitter=RandomSplitter(val_size=0.25,\n                                                       random_state=68594,\n                                                       test_size=0.25),\n                               preprocessor_config=PreprocessorConfig(\n                                   passthrough_columns=\"all\"))\n        return\n\n    def test_get_pandas_adult(self):\n        \"\"\"\n        Initialize the Adult dataset and check for nonempty train/test/val DFs.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as td:\n            dset = TabularDataset(name=\"adult\",\n                                  config=DatasetConfig(cache_dir=td),\n                                  grouper=None,\n                                  splitter=RandomSplitter(val_size=0.25,\n                                                          random_state=68594,\n                                                          test_size=0.25),\n                                  preprocessor_config=PreprocessorConfig(\n                                      passthrough_columns=\"all\"))\n            for split in (\"train\", \"validation\", \"test\"):\n                X, y, _, _ = dset.get_pandas(split)\n                self.assertTrue(len(X) != 0)\n                self.assertTrue(len(X) == len(y))\n        return\n\n\nclass TestCachedDataset(unittest.TestCase):\n    def _test_cache_and_load(self, name: str):\n        \"\"\"Cache a dataset and reload it.\"\"\"\n        with tempfile.TemporaryDirectory() as td:\n            preprocessor_config = PreprocessorConfig(passthrough_columns=\"all\")\n            dataset_config = DatasetConfig(cache_dir=td)\n            dset = TabularDataset(name=name,\n                                  config=dataset_config,\n                                  grouper=None,\n                                  splitter=RandomSplitter(val_size=0.25,\n                                                          random_state=68594,\n                                                          test_size=0.25),\n                                  preprocessor_config=preprocessor_config)\n            dset.to_sharded(rows_per_shard=64)\n\n            cached_dset = CachedDataset(config=dataset_config,\n                                        name=name,\n                                        initialize_data=True,\n                                        preprocessor_config=preprocessor_config)\n            for split in (\"train\", \"validation\", \"test\"):\n                dset_split = dset.get_pandas(split)\n                cached_dset_split = cached_dset.get_pandas(split)\n                self.assertTrue(\n                    dset_split[0].shape == cached_dset_split[0].shape,\n                    msg=\"X shapes not identical for split %s\" % split)\n                self.assertListEqual(\n                    dset_split[0].columns.tolist(),\n                    cached_dset_split[0].columns.tolist(),\n                    msg='Feature names not identical for split %s' % split)\n                self.assertTrue(\n                    dset_split[1].shape == cached_dset_split[1].shape,\n                    msg=\"y shapes not identical for split %s\" % split)\n\n    def test_cache_and_load_german(self):\n        \"\"\"Cache the German dataset and reload it.\"\"\"\n        self._test_cache_and_load(\"german\")\n\n    def test_cache_and_load_adult(self):\n        \"\"\"Cache the Adult dataset and reload it.\"\"\"\n        self._test_cache_and_load(\"adult\")\n"}
{"type": "test_file", "path": "tableshift/tests/test_splitter.py", "content": "\"\"\"\nTests for splitters.\n\nTo run tests: python -m unittest tableshift/tests/test_splitter.py -v\n\"\"\"\nimport unittest\nimport pandas as pd\nimport numpy as np\n\nfrom tableshift.core.splitter import DomainSplitter\n\nnp.random.seed(54329)\n\n\nclass TestDomainSplitterSplits(unittest.TestCase):\n\n    def setUp(self) -> None:\n        n = 5000\n        self.data = pd.DataFrame({\n            \"values1\": np.arange(n),\n            \"values2\": np.random.choice([1, 2, 3], n),\n            \"domain\": ([\"a\"] * int(n / 2) + [\"b\"] * int(n / 2))\n        })\n\n        self.groups = pd.DataFrame({\n            \"group_var_a\": np.random.choice([0, 1], n),\n            \"group_var_b\": np.random.choice([0, 1], n),\n        })\n\n        self.labels = pd.Series(np.random.choice([0, 1], n))\n\n    def test_disjoint_splits(self):\n        data = self.data\n        groups = self.groups\n        labels = self.labels\n        ood_vals = [\"a\"]\n        splitter = DomainSplitter(id_test_size=0.5,\n                                  val_size=0.1,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_ood_values=ood_vals,\n                                  ood_val_size=0.25,\n                                  random_state=45378)\n\n        splits = splitter(data, labels, groups=groups,\n                          domain_labels=data[\"domain\"])\n\n        train_domains = data.iloc[splits[\"train\"]][\"domain\"]\n        val_domains = data.iloc[splits[\"validation\"]][\"domain\"]\n        ood_val_domains = data.iloc[splits[\"ood_validation\"]][\"domain\"]\n        id_test_domains = data.iloc[splits[\"id_test\"]][\"domain\"]\n        ood_test_domains = data.iloc[splits[\"ood_test\"]][\"domain\"]\n\n        # Check that OOD splits only contain OOD values\n        self.assertTrue(np.all(ood_test_domains.isin(ood_vals)))\n        self.assertTrue(np.all(ood_val_domains.isin(ood_vals)))\n\n        # Check that ID splits do not contain any OOD values\n        self.assertFalse(np.any(train_domains.isin(ood_vals)))\n        self.assertFalse(np.any(val_domains.isin(ood_vals)))\n        self.assertFalse(np.any(id_test_domains.isin(ood_vals)))\n\n        # Check for proper partitioning of id/ood\n        assert set(train_domains) == set(id_test_domains)\n        self.assertTrue(set(train_domains).isdisjoint(set(ood_test_domains)))\n        self.assertTrue(set(val_domains).isdisjoint(set(ood_val_domains)))\n        self.assertTrue(set(id_test_domains).isdisjoint(set(ood_test_domains)))\n\n        # Check that output size is same as input\n        self.assertEqual(sum(len(x) for x in splits.values()), len(data))\n\n        # Check that every index is somewhere in splits\n        all_idxs = set(idx for split_idxs in splits.values()\n                       for idx in split_idxs)\n        self.assertEqual(all_idxs, set(data.index.tolist()))\n\n    def test_no_grouper(self):\n        data = self.data\n        labels = self.labels\n        ood_vals = [\"a\"]\n        splitter = DomainSplitter(id_test_size=0.5,\n                                  val_size=0.1,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_ood_values=ood_vals,\n                                  ood_val_size=0.25,\n                                  random_state=45378)\n\n        splits = splitter(data, labels, groups=None,\n                          domain_labels=data[\"domain\"])\n\n        # Check that output size is same as input\n        self.assertEqual(sum(len(x) for x in splits.values()), len(data))\n\n        # Check that every index is somewhere in splits\n        all_idxs = set(idx for split_idxs in splits.values()\n                       for idx in split_idxs)\n        self.assertEqual(all_idxs, set(data.index.tolist()))\n\n\nclass TestDomainSplitterDtypes(unittest.TestCase):\n    \"\"\"Test DomainSplitter with different types of ood values.\"\"\"\n\n    def test_float_vs_int(self):\n        \"\"\"Test case where domain values are floats and OOD values are ints.\"\"\"\n        n = 5000\n        ood_vals = [2]\n        data = pd.DataFrame({\n            \"values1\": np.arange(n),\n            \"values2\": np.random.choice([1, 2, 3], n),\n            \"domain\": ([1.0] * int(n / 2) + [2.0] * int(n / 2))\n        })\n\n        labels = pd.Series(np.random.choice([0, 1], n))\n\n        splitter = DomainSplitter(id_test_size=0.5,\n                                  val_size=0.1,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_ood_values=ood_vals,\n                                  ood_val_size=0.25,\n                                  random_state=45478)\n\n        splits = splitter(data, labels, domain_labels=data[\"domain\"])\n\n        # Check that number of OOD observations over all splits matches\n        # number of OOD observations in the original data.\n        ood_elems_in_splits = sum(\n            len(v) for k, v in splits.items() if \"ood\" in k)\n        ood_elems_in_data = np.isin(data[\"domain\"].values, ood_vals).sum()\n        self.assertEqual(ood_elems_in_data, ood_elems_in_splits)\n\n    def test_int_vs_float(self):\n        \"\"\"Test case where domain values are ints and OOD values are floats.\"\"\"\n        n = 5000\n        ood_vals = [2.0]\n        data = pd.DataFrame({\n            \"values1\": np.arange(n),\n            \"values2\": np.random.choice([1, 2, 3], n),\n            \"domain\": ([1] * int(n / 2) + [2] * int(n / 2))\n        })\n\n        labels = pd.Series(np.random.choice([0, 1], n))\n\n        splitter = DomainSplitter(id_test_size=0.5,\n                                  val_size=0.1,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_ood_values=ood_vals,\n                                  ood_val_size=0.25,\n                                  random_state=41378)\n\n        splits = splitter(data, labels, domain_labels=data[\"domain\"])\n\n        # Check that number of OOD observations over all splits matches\n        # number of OOD observations in the original data.\n        ood_elems_in_splits = sum(\n            len(v) for k, v in splits.items() if \"ood\" in k)\n        ood_elems_in_data = np.isin(data[\"domain\"].values, ood_vals).sum()\n        self.assertEqual(ood_elems_in_data, ood_elems_in_splits)\n\n\nclass TestThresholdDomainSplitter(unittest.TestCase):\n    def test_float_split(self):\n\n        data = pd.DataFrame(\n            {\"values1\": np.arange(10),\n             \"values2\": np.arange(10),\n             \"domain\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n        )\n        labels = pd.Series(np.random.choice([0, 1], 10))\n\n        thresh = 0.55\n        splitter = DomainSplitter(id_test_size=0.2, val_size=0.2,\n                                  ood_val_size=0.2,\n                                  random_state=43890,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_gt_thresh=thresh)\n        splits = splitter(data, labels, domain_labels=data[\"domain\"])\n\n        # Check splits have expected size\n        self.assertEqual(len(splits[\"train\"]), 3)\n        self.assertEqual(len(splits[\"id_test\"]), 1)\n        self.assertEqual(len(splits[\"ood_test\"]), 4)\n        self.assertEqual(len(splits[\"validation\"]), 1)\n        self.assertEqual(len(splits[\"ood_validation\"]), 1)\n\n        # Check splits have expected domain values\n        for split in (\"train\", \"validation\", \"id_test\"):\n            vals = data.loc[splits[split]][\"domain\"]\n            self.assertTrue(np.all(vals <= thresh))\n\n        for split in (\"ood_validation\", \"ood_test\"):\n            vals = data.loc[splits[split]][\"domain\"]\n            self.assertTrue(np.all(vals > thresh))\n\n    def test_int_split(self):\n        # half ID, half OOD\n        domain_vals = ([10] * 50) + ([20] * 50)\n\n        data = pd.DataFrame(\n            {\"values1\": np.arange(100),\n             \"values2\": np.arange(100),\n             \"domain\": domain_vals}\n        )\n        labels = pd.Series(np.random.choice([0, 1], 100))\n\n        thresh = 15\n        splitter = DomainSplitter(id_test_size=0.1, val_size=0.1,\n                                  ood_val_size=0.2,\n                                  random_state=463890,\n                                  domain_split_varname=\"domain\",\n                                  domain_split_gt_thresh=thresh)\n        splits = splitter(data, labels, domain_labels=data[\"domain\"])\n\n        # Check splits have expected size\n        self.assertEqual(len(splits[\"train\"]), 40)\n        self.assertEqual(len(splits[\"id_test\"]), 5)\n        self.assertEqual(len(splits[\"validation\"]), 5)\n        self.assertEqual(len(splits[\"ood_validation\"]), 10)\n        self.assertEqual(len(splits[\"ood_test\"]), 40)\n\n        # Check splits have expected domain values\n        for split in (\"train\", \"validation\", \"id_test\"):\n            vals = data.loc[splits[split]][\"domain\"]\n            self.assertTrue(np.all(vals <= thresh))\n\n        for split in (\"ood_validation\", \"ood_test\"):\n            vals = data.loc[splits[split]][\"domain\"]\n            self.assertTrue(np.all(vals > thresh))\n"}
{"type": "test_file", "path": "tableshift/tests/test_preprocessor.py", "content": "\"\"\"\nTests for Preprocessor objects.\n\nTo run tests: python -m unittest tableshift/tests/test_preprocessor.py -v\n\nTo run a specific individual test:\npython -m unittest tableshift/tests/test_preprocessor.py \\\n    tableshift.tests.test_preprocessor.TestPreprocessor.test_map_name -v\n\"\"\"\n\nimport copy\nimport unittest\nimport numpy as np\nimport pandas as pd\nfrom tableshift.core.features import Preprocessor, PreprocessorConfig, \\\n    FeatureList, Feature, cat_dtype, remove_verbose_prefixes\n\n\nclass TestPreprocessor(unittest.TestCase):\n    def setUp(self) -> None:\n        n = 100\n        self.df = pd.DataFrame({\n            \"int_a\": np.arange(0, n, dtype=int),\n            \"int_b\": np.arange(-n, 0, dtype=int),\n            \"float_a\": np.random.uniform(size=n),\n            \"float_b\": np.random.uniform(-1., 1., size=n),\n            \"string_a\": np.random.choice([\"a\", \"b\", \"c\"], size=n),\n            \"cat_a\": pd.Categorical(\n                np.random.choice([\"typea\", \"typeb\"], size=n)),\n        })\n        return\n\n    def test_passthrough_target(self):\n        \"\"\"Test that the target feature is not transformed.\n\n        This tests mimics/tests the pattern used in tabular_dataset.py.\"\"\"\n        feature_list = FeatureList([\n            Feature(\"int_a\", int, is_target=True,\n                    value_mapping={x: -x for x in range(len(self.df))}),\n            Feature(\"int_b\", int),\n            Feature(\"float_a\", float),\n            Feature(\"float_b\", float),\n            Feature(\"string_a\", str, value_mapping={\n                \"a\": \"Diagnosis of disease A\",\n                \"b\": \"Diagnosis of disease B\",\n                \"c\": \"Diagnosis of disease C\"}),\n            Feature(\"cat_a\", cat_dtype),\n        ])\n        for num_feat_handling in (\"map_values\", \"normalize\", \"kbins\"):\n            preprocessor = Preprocessor(\n                config=PreprocessorConfig(numeric_features=num_feat_handling),\n                feature_list=feature_list)\n            data = copy.deepcopy(self.df)\n            train_idxs = list(range(50))\n            transformed = preprocessor.fit_transform(\n                data, train_idxs=train_idxs,\n                passthrough_columns=[feature_list.target])\n            self.assertListEqual(\n                self.df[feature_list.target].tolist(),\n                transformed[feature_list.target].tolist(),\n                msg=f'Target was transformed with handling {num_feat_handling}')\n\n    def test_passthrough_all(self):\n        \"\"\"Test case with no transformations (passthrough=\"all\").\"\"\"\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(passthrough_columns=\"all\"))\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n\n        # Check that values are unmodified\n        np.testing.assert_array_equal(data.values, transformed.values)\n        # Check that dtypes are the same\n        self.assertListEqual(data.dtypes.tolist(),\n                             transformed.dtypes.tolist())\n        # Check that feature names are the same\n        self.assertListEqual(sorted(transformed.columns.tolist()),\n                             sorted(self.df.columns.tolist()))\n        return\n\n    def test_passthrough_numeric(self):\n        \"\"\"Test case with no numeric transformations.\"\"\"\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(numeric_features=\"passthrough\"))\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n        numeric_cols = [\"int_a\", \"int_b\", \"float_a\", \"float_b\"]\n\n        # Check that values of numeric cols are unmodified\n        np.testing.assert_array_equal(data[numeric_cols].values,\n                                      transformed[numeric_cols].values)\n        # Check that dtypes of numeric cols are the same\n        self.assertListEqual(data.dtypes[numeric_cols].tolist(),\n                             transformed.dtypes[numeric_cols].tolist())\n        return\n\n    def test_passthrough_categorical(self):\n        \"\"\"Test case with no categorical transformations.\"\"\"\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(categorical_features=\"passthrough\"))\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n        categorical_cols = [\"string_a\", \"cat_a\"]\n\n        # Check that values of categorical cols are unmodified\n        np.testing.assert_array_equal(data[categorical_cols].values,\n                                      transformed[categorical_cols].values)\n        # Check that dtypes of numeric cols are the same\n        self.assertListEqual(data.dtypes[categorical_cols].tolist(),\n                             transformed.dtypes[categorical_cols].tolist())\n        return\n\n    def test_label_encoder(self):\n        preprocessor_config = PreprocessorConfig(\n            categorical_features=\"label_encode\")\n        preprocessor = Preprocessor(config=preprocessor_config)\n        data = copy.deepcopy(self.df)\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data=data,\n                                                 train_idxs=train_idxs)\n        for feat in (\"string_a\", \"cat_a\"):\n            self.assertTrue(\n                np.issubdtype(transformed[feat].dtype, float) or\n                np.issubdtype(transformed[feat].dtype, int))\n\n    def test_map_values(self):\n        \"\"\"Test mapping of values.\"\"\"\n\n        feature_list = FeatureList([\n            Feature(\"int_a\", int,\n                    value_mapping={x: -x for x in range(len(self.df))}),\n            Feature(\"int_b\", int),\n            Feature(\"float_a\", float),\n            Feature(\"float_b\", float),\n            Feature(\"string_a\", str, value_mapping={\n                \"a\": \"Diagnosis of disease A\",\n                \"b\": \"Diagnosis of disease B\",\n                \"c\": \"Diagnosis of disease C\"}),\n            Feature(\"cat_a\", cat_dtype),\n        ])\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(\n                categorical_features=\"map_values\",\n                numeric_features=\"map_values\"),\n            feature_list=feature_list)\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n\n        for f in feature_list.features:\n            if f.value_mapping is not None:\n                fname = f.name\n                self.assertTrue(fname in transformed.columns)\n\n                self.assertTrue(np.all(\n                    np.isin(transformed[fname].values,\n                            np.array(list(f.value_mapping.values())))\n                ),\n                    msg=f\"failed for feature {fname}\")\n\n    def test_map_values_all(self):\n        \"\"\"Test case where a value map is used for *all* features.\"\"\"\n\n        feature_list = FeatureList([\n            Feature(\"int_a\", int,\n                    value_mapping={x: -x for x in range(len(self.df))}),\n            Feature(\"int_b\", int,\n                    value_mapping={x: x % 2 for x in\n                                   self.df[\"int_b\"].unique()}),\n            Feature(\"float_a\", float,\n                    value_mapping={x: x + 1000 for x in\n                                   self.df[\"float_a\"].unique()}\n                    ),\n            Feature(\"float_b\", float,\n                    value_mapping={x: x > 0 for x in\n                                   self.df[\"float_b\"].unique()}),\n            Feature(\"string_a\", str, value_mapping={\n                \"a\": \"Diagnosis of disease A\",\n                \"b\": \"Diagnosis of disease B\",\n                \"c\": \"Diagnosis of disease C\"}),\n            Feature(\"cat_a\", cat_dtype,\n                    value_mapping={\"typea\": \"Patient of Type A\",\n                                   \"typeb\": \"Patient of Type B\"}),\n        ])\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(\n                categorical_features=\"map_values\",\n                numeric_features=\"map_values\"),\n            feature_list=feature_list)\n        train_idxs = list(range(50))\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n        for f in feature_list.features:\n            fname = f.name\n            self.assertTrue(fname in transformed.columns)\n\n            self.assertTrue(np.all(\n                np.isin(transformed[fname].values,\n                        np.array(list(f.value_mapping.values())))\n            ),\n                msg=f\"failed for feature {fname}\")\n\n    def test_map_name(self):\n        \"\"\"Test mapping of extended feature names.\"\"\"\n\n        feature_list = FeatureList([\n            Feature(\"int_a\", int,\n                    name_extended=\"Integer A value\"),\n            Feature(\"int_b\", int,\n                    name_extended=\"Integer B value\",\n                    is_target=True),\n            Feature(\"float_a\", float),\n            Feature(\"float_b\", float),\n            Feature(\"string_a\", str,\n                    name_extended=\"String A value\"),\n            Feature(\"cat_a\", cat_dtype),\n        ])\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(\n            config=PreprocessorConfig(use_extended_names=True,\n                                      numeric_features=\"passthrough\",\n                                      categorical_features=\"passthrough\"),\n            feature_list=feature_list)\n        train_idxs = list(range(50))\n\n        transformed = preprocessor.fit_transform(data, train_idxs=train_idxs)\n\n        expected_names = [\"Integer A value\", \"int_b\", \"float_a\", \"float_b\",\n                          \"String A value\", \"cat_a\"]\n\n        # Check that names are mapped\n        self.assertListEqual(transformed.columns.tolist(), expected_names)\n        # Check that data is unchanged\n        np.testing.assert_array_equal(transformed.values, self.df.values)\n\n    def test_remove_verbose_prefixes(self):\n        colnames = [\"onehot__x_feature\", \"scale__float0\",\n                    \"kbin__myfeature\", \"map__cat_feature\"]\n        expected = [\"x_feature\", \"float0\",\n                    \"myfeature\", \"cat_feature\"]\n        output = remove_verbose_prefixes(colnames)\n        self.assertListEqual(output, expected)\n\n    def test_remove_verbose_prefixes_in_pipeline_onehot_normalize(self):\n        \"\"\"End-to-end test that verbose prefixes are removed, with onehot/norm.\n        \"\"\"\n        data = copy.deepcopy(self.df)\n        preprocessor = Preprocessor(config=PreprocessorConfig(\n            categorical_features=\"one_hot\",\n            numeric_features=\"normalize\",\n        ))\n        train_idxs = list(range(50))\n\n        expected_output_columns = ['int_a', 'int_b', 'float_a', 'float_b',\n                                   'string_a_c', 'string_a_b',\n                                   'string_a_a', 'cat_a_typeb', 'cat_a_typea']\n        transformed = preprocessor.fit_transform(\n            data, train_idxs=train_idxs)\n\n        self.assertListEqual(sorted(transformed.columns.tolist()),\n                             sorted(expected_output_columns))\n\n    def test_remove_verbose_prefixes_in_pipeline_map_bin(self):\n        \"\"\"End-to-end test that verbose prefixes are removed, with mapping\n        and binning.\n        \"\"\"\n        data = copy.deepcopy(self.df)\n\n        feature_list = FeatureList([\n            Feature(\"int_a\", int,\n                    value_mapping={x: -x for x in range(len(self.df))}),\n            Feature(\"int_b\", int),\n            Feature(\"float_a\", float),\n            Feature(\"float_b\", float),\n            Feature(\"string_a\", str,\n                    value_mapping={\n                        \"a\": \"Diagnosis of disease A\",\n                        \"b\": \"Diagnosis of disease B\",\n                        \"c\": \"Diagnosis of disease C\"}),\n            Feature(\"cat_a\", cat_dtype),\n        ])\n\n        preprocessor = Preprocessor(config=PreprocessorConfig(\n            categorical_features=\"map_values\",\n            numeric_features=\"kbins\"),\n            feature_list=feature_list)\n        train_idxs = list(range(50))\n\n        expected_output_columns = self.df.columns.tolist()\n        transformed = preprocessor.fit_transform(\n            data, train_idxs=train_idxs)\n\n        self.assertListEqual(sorted(transformed.columns.tolist()),\n                             sorted(expected_output_columns))\n"}
{"type": "test_file", "path": "tableshift/tests/test_features.py", "content": "\"\"\"\nTests for Feature objects.\n\nTo run tests: python -m unittest tableshift/tests/test_features.py -v\n\"\"\"\nimport unittest\n\nimport pandas as pd\nimport numpy as np\n\nfrom tableshift.core import features\nfrom tableshift.core.features import Feature, cat_dtype\n\n\nclass TestFeatureFillNA(unittest.TestCase):\n    def test_fillna_int(self):\n        data = pd.Series(np.arange(10), dtype=int)\n        feature = Feature(\"my_feature\", int, na_values=(3, 4, 5))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 3)\n\n    def test_fillna_float(self):\n        data = pd.Series(np.arange(10), dtype=float)\n        feature = Feature(\"my_feature\", float, na_values=(2., 7.))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 2)\n\n    def test_fillna_int_with_float(self):\n        \"\"\"Tests case of int feature with float na_values.\"\"\"\n        data = pd.Series(np.arange(10), dtype=int)\n        feature = Feature(\"my_feature\", int, na_values=(3., 4., 5.))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 3)\n\n    def test_fillna_float_with_int(self):\n        \"\"\"Tests case of float feature with int na_values.\"\"\"\n        data = pd.Series(np.arange(10), dtype=float)\n        feature = Feature(\"my_feature\", float, na_values=(2, 4, 6, 8))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 4)\n\n    def test_fillna_categorical(self):\n        \"\"\"Tests case of categorical feature with string na_values.\"\"\"\n        letters = list(\"abcdefg\")\n        data = pd.Categorical(letters * 2, categories=letters)\n        feature = Feature(\"my_feature\", cat_dtype, na_values=(\"a\", \"b\"))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 4)\n\n    def test_fillna_int_categorical(self):\n        \"\"\"Tests case of categorical feature with int na_values.\"\"\"\n        numbers = list(range(5))\n        data = pd.Categorical(numbers * 2, categories=numbers)\n        feature = Feature(\"my_feature\", cat_dtype, na_values=(2, 4))\n        data_na = feature.fillna(data)\n        self.assertEqual(pd.isnull(data_na).sum(), 4)\n\n\nclass TestColumnIsOfType(unittest.TestCase):\n    def test_categorical_type_check(self):\n        letters = list(\"abcdefg\")\n        numbers = list(range(5))\n        numeric_cat_data = pd.Series(\n            pd.Categorical(numbers * 2, categories=numbers))\n\n        string_cat_data = pd.Series(\n            pd.Categorical(letters * 2, categories=letters))\n\n        self.assertTrue(features.column_is_of_type(string_cat_data, cat_dtype))\n        self.assertTrue(features.column_is_of_type(numeric_cat_data, cat_dtype))\n        self.assertFalse(features.column_is_of_type(numeric_cat_data, int))\n        self.assertFalse(features.column_is_of_type(numeric_cat_data, float))\n\n    def test_numeric_type_check(self):\n        int_data = pd.Series(np.arange(10, dtype=int))\n        self.assertTrue(features.column_is_of_type(int_data, int))\n        self.assertFalse(features.column_is_of_type(int_data, float))\n        float_data = pd.Series(np.arange(10, dtype=float))\n        self.assertTrue(features.column_is_of_type(float_data, float))\n        self.assertFalse(features.column_is_of_type(float_data, int))\n\n"}
{"type": "test_file", "path": "tableshift/tests/test_getters.py", "content": "\"\"\"\nTests for TabularDatasets.\n\nTo run tests: python -m unittest tableshift/tests/test_getters.py -v\n\"\"\"\nimport unittest\n\nfrom tableshift import get_dataset, get_iid_dataset\n\n\nclass TestGetters(unittest.TestCase):\n    \"\"\"Test getters for some (small, public) datasets.\"\"\"\n\n    def test_get_dataset_german(self):\n        _ = get_dataset(\"german\")\n\n    def test_get_iid_dataset_german(self):\n        _ = get_iid_dataset(\"german\")\n\n    def test_get_dataset_adult(self):\n        _ = get_dataset(\"adult\")\n\n    def test_get_iid_dataset_adult(self):\n        _ = get_iid_dataset(\"adult\")\n"}
{"type": "source_file", "path": "scripts/train_pytorch.py", "content": "\"\"\"\nA helper script to train a pytorch model without Ray. Useful for debugging.\n\nUsage:\n    python scripts/train_pytorch.py --model mlp --experiment adult\n\"\"\"\nimport argparse\n\nimport numpy as np\nimport torch\nimport torchinfo\nfrom sklearn.metrics import accuracy_score\n\nfrom tableshift.core import get_dataset\nfrom tableshift.models.default_hparams import get_default_config\nfrom tableshift.models.torchutils import get_predictions_and_labels\nfrom tableshift.models.training import train\nfrom tableshift.models.utils import get_estimator\n\n\ndef main(experiment, cache_dir, model, debug: bool, use_cached: bool):\n    if debug:\n        print(\"[INFO] running in debug mode.\")\n        experiment = \"_debug\"\n\n    dset = get_dataset(name=experiment, cache_dir=cache_dir,\n                       use_cached=use_cached)\n    config = get_default_config(model, dset)\n    estimator = get_estimator(model, **config)\n    print(torchinfo.summary(estimator))\n    device = f\"cuda:{torch.cuda.current_device()}\" \\\n        if torch.cuda.is_available() else \"cpu\"\n\n    print(f\"device is {device}\")\n    train(estimator, dset, device=device, config=config)\n\n    splits = (\"id_test\", \"ood_test\") if dset.is_domain_split else (\"test\",)\n    for split in splits:\n        loader = dset.get_dataloader(split)\n        preds, labels = get_predictions_and_labels(estimator, loader, device)\n        acc = accuracy_score(labels, np.round(preds))\n        print(f'accuracy on split {split}: {acc:.4f}')\n    return\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cache_dir\", default=\"tmp\",\n                        help=\"Directory to cache raw data files to.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", default=False,\n                        help=\"Whether to run in debug mode. If True, various \"\n                             \"truncations/simplifications are performed to \"\n                             \"speed up experiment.\")\n    parser.add_argument(\"--experiment\",\n                        help=\"Experiment to run. Overridden when debug=True.\")\n    parser.add_argument(\"--model\", default=\"mlp\",\n                        help=\"model to use.\")\n    parser.add_argument(\"--use_cached\", default=False, action=\"store_true\",\n                        help=\"whether to use cached data.\")\n    args = parser.parse_args()\n    main(**vars(args))\n"}
{"type": "source_file", "path": "tableshift/configs/ray_configs.py", "content": "import os\n\n_DEFAULT_RAY_TMP_DIRS = (\"/projects/grail/jpgard/ray-scratch\",\n                         \"/data1/home/jpgard/ray-scratch\",\n                         \"/gscratch/scrubbed/jpgard/ray-scratch\")\n_DEFAULT_RAY_LOCAL_DIRS = (\"/projects/grail/jpgard/ray-results\",\n                           \"/data1/home/jpgard/ray-results\",\n                           \"/gscratch/scrubbed/jpgard/ray-results\")\n\n\ndef get_default_ray_tmp_dir():\n    \"\"\"Check if any of the default ray tmp dirs exist; if they do, use them.\"\"\"\n    for dirpath in _DEFAULT_RAY_TMP_DIRS:\n        if os.path.exists(dirpath):\n            ray_tmp_dir = dirpath\n            print(\n                f\"[INFO] detected directory {dirpath}; \"\n                f\"setting this to ray temporary directory.\")\n            return ray_tmp_dir\n    return None\n\n\ndef get_default_ray_local_dir():\n    \"\"\"Check if any of the default ray local dirs exist; if they do, use them.\"\"\"\n\n    for dirpath in _DEFAULT_RAY_LOCAL_DIRS:\n        if os.path.exists(dirpath):\n            ray_local_dir = dirpath\n            print(\n                f\"[INFO] detected directory {dirpath}; \"\n                f\"setting this to ray local directory.\")\n            return ray_local_dir\n    return None\n"}
{"type": "source_file", "path": "scripts/train_catboost_optuna.py", "content": "\"\"\"\nUsage:\n\npython scripts/train_catboost_optuna \\\n    --experiment adult \\\n    --use_gpu \\\n    --use_cached\n\n\"\"\"\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport optuna\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, \\\n    average_precision_score\n\nfrom tableshift.core import get_dataset\nfrom tableshift.core.utils import timestamp_as_int\n\n\ndef evaluate(model: CatBoostClassifier, X: pd.DataFrame, y: pd.Series,\n             split: str) -> dict:\n    yhat_hard = model.predict(X)\n    yhat_soft = model.predict_proba(X)[:, 1]\n    metrics = {}\n    metrics[f\"{split}_accuracy\"] = accuracy_score(y, yhat_hard)\n    metrics[f\"{split}_auc\"] = roc_auc_score(y, yhat_soft)\n    metrics[f\"{split}_map\"] = average_precision_score(y, yhat_soft)\n\n    metrics[f\"{split}_num_samples\"] = len(y)\n    metrics[f\"{split}_ymean\"] = np.mean(y).item()\n    return metrics\n\n\ndef main(experiment: str, cache_dir: str, results_dir: str, num_samples: int,\n         random_seed:int,\n         use_gpu: bool, use_cached: bool):\n    start_time = timestamp_as_int()\n\n    dset = get_dataset(experiment, cache_dir, use_cached=use_cached)\n    uid = dset.uid\n\n    X_tr, y_tr, _, _ = dset.get_pandas(\"train\")\n    X_val, y_val, _, _ = dset.get_pandas(\"validation\")\n\n    def optimize_hp(trial: optuna.trial.Trial):\n        cb_params = {\n            # Same tuning grid as https://arxiv.org/abs/2106.11959,\n            # see supplementary section F.4.\n            'learning_rate': trial.suggest_float('learning_rate', 1e-3,\n                                                 1., log=True),\n            'depth': trial.suggest_int('depth', 3, 10),\n            'bagging_temperature': trial.suggest_float(\n                'bagging_temperature', 1e-6, 1., log=True),\n            'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 100, log=True),\n            'leaf_estimation_iterations': trial.suggest_int(\n                'leaf_estimation_iterations', 1, 10),\n\n            \"use_best_model\": True,\n            \"task_type\": \"GPU\" if use_gpu else \"CPU\",\n            'random_seed': random_seed,\n        }\n\n        model = CatBoostClassifier(**cb_params)\n        model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=False)\n        y_pred = model.predict(X_val)\n        return accuracy_score(y_val, y_pred)\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(optimize_hp, n_trials=num_samples)\n    print('Trials:', len(study.trials))\n    print('Best parameters:', study.best_trial.params)\n    print('Best score:', study.best_value)\n    print(\"training completed! retraining model with best params and \"\n          \"evaluating it.\")\n\n    clf_with_best_params = CatBoostClassifier(**study.best_trial.params)\n    clf_with_best_params = clf_with_best_params.fit(X_tr, y_tr)\n\n    expt_results_dir = os.path.join(results_dir, experiment, str(start_time))\n\n    metrics = {}\n    model_name = \"catboost\"\n    metrics[\"estimator\"] = model_name\n    metrics[\"domain_split_varname\"] = dset.domain_split_varname\n    metrics[\"domain_split_ood_values\"] = str(dset.get_domains(\"ood_test\"))\n    metrics[\"domain_split_id_values\"] = str(dset.get_domains(\"id_test\"))\n\n    splits = (\n        'id_test', 'ood_test', 'ood_validation', 'validation') if dset.is_domain_split else (\n        'test', 'validation')\n    for split in splits:\n        X, y, _, _ = dset.get_pandas(split)\n        _metrics = evaluate(clf_with_best_params, X, y, split)\n        print(_metrics)\n        metrics.update(_metrics)\n\n    iter_fp = os.path.join(\n        expt_results_dir,\n        f\"tune_results_{experiment}_{start_time}_{uid[:100]}_\"\n        f\"{model_name}.csv\")\n    if not os.path.exists(expt_results_dir):\n        os.makedirs(expt_results_dir)\n\n    logging.info(f\"writing results for {model_name} to {iter_fp}\")\n    pd.DataFrame(metrics, index=[1]).to_csv(iter_fp, index=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cache_dir\", default=\"tmp\",\n                        help=\"Directory to cache raw data files to.\")\n    parser.add_argument(\"--experiment\", default=\"adult\",\n                        help=\"Experiment to run. Overridden when debug=True.\")\n    parser.add_argument(\"--num_samples\", type=int, default=100,\n                        help=\"Number of hparam samples to take in tuning \"\n                             \"sweep.\")\n    parser.add_argument(\"--results_dir\", default=\"./optuna_results\",\n                        help=\"where to write results. CSVs will be written to \"\n                             \"experiment-specific subdirectories within this \"\n                             \"directory.\")\n    parser.add_argument(\"--random_seed\", default=42, type=int)\n    parser.add_argument(\"--use_cached\", default=False, action=\"store_true\",\n                        help=\"whether to use cached data.\")\n    parser.add_argument(\"--use_gpu\", action=\"store_true\", default=False,\n                        help=\"whether to use GPU (if available)\")\n    args = parser.parse_args()\n    main(**vars(args))\n"}
{"type": "source_file", "path": "examples/run_expt.py", "content": "import argparse\nimport logging\n\nimport torch\nfrom sklearn.metrics import accuracy_score\n\nfrom tableshift import get_dataset\nfrom tableshift.models.training import train\nfrom tableshift.models.utils import get_estimator\nfrom tableshift.models.default_hparams import get_default_config\n\n\nLOG_LEVEL = logging.DEBUG\n\nlogger = logging.getLogger()\nlogging.basicConfig(\n    format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',\n    level=LOG_LEVEL,\n    datefmt='%Y-%m-%d %H:%M:%S')\n\n\ndef main(experiment, cache_dir, model, debug: bool):\n    if debug:\n        print(\"[INFO] running in debug mode.\")\n        experiment = \"_debug\"\n\n    dset = get_dataset(experiment, cache_dir)\n    X, y, _, _ = dset.get_pandas(\"train\")\n    config = get_default_config(model, dset)\n    estimator = get_estimator(model, **config)\n    estimator = train(estimator, dset, config=config)\n\n    if not isinstance(estimator, torch.nn.Module):\n        # Case: non-pytorch estimator; perform test-split evaluation.\n        test_split = \"ood_test\" if dset.is_domain_split else \"test\"\n        # Fetch predictions and labels for a sklearn model.\n        X_te, y_te, _, _ = dset.get_pandas(test_split)\n        yhat_te = estimator.predict(X_te)\n    \n        acc = accuracy_score(y_true=y_te, y_pred=yhat_te)\n        print(f\"training completed! {test_split} accuracy: {acc:.4f}\")\n    \n    else:\n        # Case: pytorch estimator; eval is already performed + printed by train().\n        print(\"training completed!\")\n    return\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cache_dir\", default=\"tmp\",\n                        help=\"Directory to cache raw data files to.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", default=False,\n                        help=\"Whether to run in debug mode. If True, various \"\n                             \"truncations/simplifications are performed to \"\n                             \"speed up experiment.\")\n    parser.add_argument(\"--experiment\", default=\"diabetes_readmission\",\n                        help=\"Experiment to run. Overridden when debug=True.\")\n    parser.add_argument(\"--model\", default=\"histgbm\",\n                        help=\"model to use.\")\n    args = parser.parse_args()\n    main(**vars(args))\n"}
{"type": "source_file", "path": "tableshift/configs/__init__.py", "content": ""}
{"type": "source_file", "path": "tableshift/core/metrics.py", "content": "from collections.abc import Iterable\nimport functools\nimport logging\n\nimport fairlearn.metrics\nimport numpy as np\nimport pandas as pd\nfrom torch.nn.functional import binary_cross_entropy\nimport torch\nimport sklearn.metrics as skm\n\n\ndef extract_positive_class_predictions(y_pred):\n    if len(y_pred.shape) == 2:\n        # Case: predictions contain class preds for classes (0,1).\n        return y_pred[:, 1]\n    else:\n        # Case: predictions contain class preds only for class 1.\n        return y_pred\n\n\ndef clip_torch_outputs(t: torch.Tensor, eps=1e-8, clip_max=1.0, clip_min=0.0):\n    \"\"\"Helper function to safely clip tensors with values outside a range.\n\n    This is mostly used when casting numpy arrays to torch doubles, which can\n    result in values slightly outside the expected range in critical ways\n    (e.g. 1. can be cast as a double to 1.0000000000000002, which raises\n    errors in binary cross-entropy).\n    \"\"\"\n    if not (t.max() <= clip_max + eps) and (t.min() >= clip_max - eps):\n        logging.warning(\n            f\"tensor values outside clip range [{clip_min}-{eps},{clip_max}+{eps}]\")\n    return torch.clip(t, min=clip_min, max=clip_max)\n\n\ndef all_subgroups_contain_all_label_values(y_true, sens) -> bool:\n    \"\"\"Check whether all labels are represented in all sensitive subgroups.\"\"\"\n    if np.ndim(sens) == 2:\n        # Case: multiple sensitive attributes\n        sens_cols = [sens.iloc[:, i] for i in range(sens.shape[1])]\n        crosstab = pd.crosstab(y_true, sens_cols)\n    else:\n        # Case: single sensitive attribute\n        crosstab = pd.crosstab(y_true, sens)\n    return np.all(crosstab != 0)\n\n\ndef _intersectional_metrics_from_grouped_metrics(grouped_metrics, metrics,\n                                                 suffix,\n                                                 sensitive_features):\n    # Compute metrics by subgroup (intersectional); note that marginals can\n    # always be recovered using the per-group counts.\n    for sens_idx, metrics_dict in grouped_metrics.by_group.to_dict(\n            'index').items():\n        if not isinstance(sens_idx, Iterable):\n            # Case: only a single sensitive attribute.\n            sens_idx = [sens_idx]\n        # sens_Str is e.g. 'race0sex1'\n        sens_str = ''.join(f\"{col}{val}\" for col, val in\n                           zip(sensitive_features.columns, sens_idx))\n\n        for metric_name, metric_value in metrics_dict.items():\n            metrics[sens_str + metric_name + suffix] = metric_value\n    return metrics\n\n\n# DORO; adapted from\n# https://github.com/RuntianZ/doro/blob/master/wilds-exp/algorithms/doro.py\ndef cvar_doro_criterion(outputs, targets, eps, alpha):\n    batch_size = len(targets)\n    loss = binary_cross_entropy(outputs, targets, reduction=\"none\")\n    # CVaR-DORO\n    gamma = eps + alpha * (1 - eps)\n    n1 = int(gamma * batch_size)\n    n2 = int(eps * batch_size)\n    rk = torch.argsort(loss, descending=True)\n    loss = loss[rk[n2:n1]].sum() / alpha / (batch_size - n2)\n    return loss\n\n\ndef cvar_doro_metric(y_true, y_pred, eps=0.005, alpha=0.2) -> float:\n    \"\"\"Compute CVaR DORO metric with a fairlearn-compatible interface.\"\"\"\n    y_pred = extract_positive_class_predictions(y_pred)\n    outputs_clipped = clip_torch_outputs(torch.from_numpy(y_pred).double())\n    targets_clipped = clip_torch_outputs(torch.from_numpy(y_true).double())\n\n    return cvar_doro_criterion(outputs=outputs_clipped,\n                               targets=targets_clipped,\n                               eps=eps,\n                               alpha=alpha).detach().cpu().numpy().item()\n\n\ndef cvar_metric(y_true, y_pred, alpha=0.2) -> float:\n    \"\"\"Compute CVaR metric with a fairlearn-compatible interface.\"\"\"\n    y_pred = extract_positive_class_predictions(y_pred)\n    outputs_clipped = clip_torch_outputs(torch.from_numpy(y_pred).double())\n    targets_clipped = clip_torch_outputs(torch.from_numpy(y_true).double())\n    return cvar_doro_criterion(outputs=outputs_clipped,\n                               targets=targets_clipped,\n                               eps=0.,\n                               alpha=alpha).detach().cpu().numpy().item()\n\n\ndef loss_variance_metric(y_true, y_pred):\n    \"\"\"Compute loss variance metric with a fairlearn-compatible interface.\"\"\"\n    if len(y_pred.shape) == 2:\n        y_pred = y_pred[:, 1]\n    elementwise_loss = binary_cross_entropy(\n        input=torch.from_numpy(y_pred).float(),\n        target=torch.from_numpy(y_true).float(),\n        reduction=\"none\")\n    return torch.var(elementwise_loss).cpu().numpy().item()\n\n\ndef append_suffix_to_keys(d: dict, suffix: str) -> dict:\n    return {f\"{k}{suffix}\": v for k, v in d.items()}\n\n\ndef metrics_by_group(y_true: pd.Series, yhat_soft: pd.Series,\n                     sensitive_features: pd.DataFrame, suffix: str = '',\n                     threshold=0.5):\n    # Check inputs\n    assert isinstance(sensitive_features, pd.DataFrame)\n    assert len(y_true) == len(yhat_soft)\n\n    if len(y_true) <= 1: raise ValueError(\"Cannot compute metrics when n<=1.\")\n\n    if y_true.nunique() == 0:\n        raise ValueError(\"only one unique label in y_true\")\n\n    yhat_hard = (yhat_soft >= threshold)\n    if (suffix != '') and (not suffix.startswith('_')):\n        # Ensure suffix has proper leading sep token\n        suffix = '_' + suffix\n    metrics = {}\n    _log_loss = functools.partial(skm.log_loss, labels=[0., 1.])\n\n    metric_fns_continuous = {\n        'crossentropy': _log_loss,\n        'cvar_doro': cvar_doro_metric,\n        'cvar': cvar_metric,\n        'loss_variance': loss_variance_metric,\n    }\n\n    metric_fns_binary = {\n        'accuracy': skm.accuracy_score,\n        'selection_rate': fairlearn.metrics.selection_rate,\n        'count': fairlearn.metrics.count,\n        'tpr': fairlearn.metrics.true_positive_rate,\n        'fpr': fairlearn.metrics.false_positive_rate,\n    }\n\n    if all_subgroups_contain_all_label_values(y_true, sensitive_features):\n        # Only compute AUC if all labels exist in each sens group. This is\n        # due to a limitation in fairlearn.MetricFrame, which can't handle\n        # errors or nan values when computing group difference metrics.\n        metric_fns_binary['auc'] = skm.roc_auc_score\n    else:\n        logging.info(\"Not computing AUC for this split because one or more\"\n                     \" sensitive subgroups do not contain all classes.\")\n\n    grouped_metrics_binary = fairlearn.metrics.MetricFrame(\n        metrics=metric_fns_binary,\n        y_true=y_true,\n        y_pred=yhat_hard,\n        sensitive_features=sensitive_features)\n\n    grouped_metrics_continuous = fairlearn.metrics.MetricFrame(\n        metrics=metric_fns_continuous,\n        y_true=y_true,\n        y_pred=yhat_soft,\n        sensitive_features=sensitive_features)\n\n    metrics.update(append_suffix_to_keys(\n        grouped_metrics_binary.overall.to_dict(), suffix))\n\n    metrics.update(append_suffix_to_keys(\n        grouped_metrics_continuous.overall.to_dict(), suffix))\n\n    for metric, value in grouped_metrics_continuous.difference().iteritems():\n        metrics[f\"abs_{metric}_disparity{suffix}\"] = value\n\n    # Compute some specific metrics of interest from the results\n    metrics['abs_accuracy_disparity' + suffix] = \\\n        grouped_metrics_binary.difference()['accuracy']\n    metrics['demographic_parity_diff' + suffix] = \\\n        grouped_metrics_binary.difference()['selection_rate']\n    # EO diff is defined as  The greater of two metrics:\n    # `true_positive_rate_difference` and `false_positive_rate_difference`;\n    # see fairlearn.metrics.equalized_odds_difference\n    metrics['equalized_odds_diff' + suffix] = \\\n        max(grouped_metrics_binary.difference()[['tpr', 'fpr']])\n\n    metrics[\"accuracy_worstgroup\" + suffix] = \\\n        grouped_metrics_binary.group_min()['accuracy']\n    metrics[\"crossentropy_worstgroup\" + suffix] = \\\n        grouped_metrics_continuous.group_max()['crossentropy']\n    metrics['cvar_doro_worstgroup' + suffix] = \\\n        grouped_metrics_continuous.group_max()['cvar_doro']\n    metrics['cvar_worstgroup' + suffix] = \\\n        grouped_metrics_continuous.group_max()['cvar']\n\n    metrics = _intersectional_metrics_from_grouped_metrics(\n        grouped_metrics_binary, metrics,\n        suffix, sensitive_features)\n    metrics = _intersectional_metrics_from_grouped_metrics(\n        grouped_metrics_continuous, metrics,\n        suffix, sensitive_features)\n\n    return metrics\n"}
{"type": "source_file", "path": "setup.py", "content": "#!/usr/bin/env python\n\nfrom distutils.core import setup\nfrom setuptools import find_packages\n\nsetup(name='tableshift',\n      version='0.1',\n      url='https://tableshift.org',\n      description='A tabular data benchmarking toolkit.',\n      long_description='A benchmarking toolkit for tabular data under distirbution shift. '\n                       'For more details, see the paper '\n                       '\"Benchmarking Distribution Shift in Tabular Data with TableShift\", '\n                       'Gardner, Popovic, and Schmidt, 2023.',\n      author='Josh Gardner',\n      author_email='jpgard@cs.washington.edu',\n      packages=find_packages(),\n      include_package_data=True,\n      data_files=[('tableshift/datasets',\n                   ['tableshift/datasets/nhanes_data_sources.json',\n                    'tableshift/datasets/icd9-codes.json'])],\n      install_requires=[\n          'numpy==1.23.5',\n          'ray==2.2',\n          'torch',\n          'torchvision',\n          'scikit-learn',\n          'pandas',\n          'fairlearn',\n          'folktables',\n          'frozendict',\n          'rtdl',\n          'xport',\n          'tqdm',\n          'hyperopt',\n          'h5py',\n          'tables',\n          'category_encoders',\n          'einops',\n          'tab-transformer-pytorch',\n          'openpyxl',\n          'optuna',\n          'kaggle',\n          'datasets',\n          'torchinfo'\n      ]\n      )\n"}
{"type": "source_file", "path": "tableshift/datasets/acs_feature_mappings.py", "content": "\"\"\"\nVarious feature mappings for ACS data.\n\"\"\"\n\nfrom frozendict import frozendict\nfrom tableshift.datasets.acs_occp_mapping import ACS_OCCP_CODE_MAPPING\n\n# Maps place of birth to coarse categories for state\n# or global region (i.e. Africa, Middle East, South America).\nPOBP_MAPPING = {\n    001.: \"AL\",  # .Alabama/AL\n    002.: \"AK\",  # .Alaska/AK\n    004.: \"AZ\",  # .Arizona/AZ\n    005.: \"AR\",  # .Arkansas/AR\n    006.: \"CA\",  # .California/CA\n    008.: \"CO\",  # .Colorado/CO\n    009.: \"CT\",  # .Connecticut/CT\n    010.: \"DE\",  # .Delaware/DE\n    011.: \"DC\",  # .District of Columbia/DC\n    012.: \"FL\",  # .Florida/FL\n    013.: \"GA\",  # .Georgia/GA\n    015.: \"HI\",  # .Hawaii/HI\n    016.: \"ID\",  # .Idaho/ID\n    017.: \"IL\",  # .Illinois/IL\n    018.: \"IN\",  # .Indiana/IN\n    019.: \"IA\",  # .Iowa/IA\n    020.: \"KS\",  # .Kansas/KS\n    021.: \"KY\",  # .Kentucky/KY\n    022.: \"LA\",  # .Louisiana/LA\n    023.: \"ME\",  # .Maine/ME\n    024.: \"MD\",  # .Maryland/MD\n    025.: \"MA\",  # .Massachusetts/MA\n    026.: \"MI\",  # .Michigan/MI\n    027.: \"MN\",  # .Minnesota/MN\n    028.: \"MS\",  # .Mississippi/MS\n    029.: \"MO\",  # .Missouri/MO\n    030.: \"MT\",  # .Montana/MT\n    031.: \"NE\",  # .Nebraska/NE\n    032.: \"NV\",  # .Nevada/NV\n    033.: \"NH\",  # .New Hampshire/NH\n    034.: \"NJ\",  # .New Jersey/NJ\n    035.: \"NM\",  # .New Mexico/NM\n    036.: \"NY\",  # .New York/NY\n    037.: \"NC\",  # .North Carolina/NC\n    038.: \"ND\",  # .North Dakota/ND\n    039.: \"OH\",  # .Ohio/OH\n    040.: \"OK\",  # .Oklahoma/OK\n    041.: \"OR\",  # .Oregon/OR\n    042.: \"PA\",  # .Pennsylvania/PA\n    044.: \"RI\",  # .Rhode Island/RI\n    045.: \"SC\",  # .South Carolina/SC\n    046.: \"SD\",  # .South Dakota/SD\n    047.: \"TN\",  # .Tennessee/TN\n    048.: \"TX\",  # .Texas/TX\n    049.: \"UT\",  # .Utah/UT\n    050.: \"VT\",  # .Vermont/VT\n    051.: \"VA\",  # .Virginia/VA\n    053.: \"WA\",  # .Washington/WA\n    054.: \"WV\",  # .West Virginia/WV\n    055.: \"WI\",  # .Wisconsin/WI\n    056.: \"WY\",  # .Wyoming/WY\n    060.: \"USTERR\",  # .American Samoa\n    066.: \"USTERR\",  # .Guam\n    069.: \"USTERR\",  # .Commonwealth of the Northern Mariana Islands\n    072.: \"PR\",  # .Puerto Rico\n    078.: \"USTERR\",  # .US Virgin Islands\n    100.: \"EUROPE\",  # .Albania\n    102.: \"EUROPE\",  # .Austria\n    103.: \"EUROPE\",  # .Belgium\n    104.: \"EUROPE\",  # .Bulgaria\n    105.: \"EUROPE\",  # .Czechoslovakia\n    106.: \"EUROPE\",  # .Denmark\n    108.: \"EUROPE\",  # .Finland\n    109.: \"EUROPE\",  # .France\n    110.: \"EUROPE\",  # .Germany\n    116.: \"EUROPE\",  # .Greece\n    117.: \"EUROPE\",  # .Hungary\n    118.: \"EUROPE\",  # .Iceland\n    119.: \"EUROPE\",  # .Ireland\n    120.: \"EUROPE\",  # .Italy\n    126.: \"EUROPE\",  # .Netherlands\n    127.: \"EUROPE\",  # .Norway\n    128.: \"EUROPE\",  # .Poland\n    129.: \"EUROPE\",  # .Portugal\n    130.: \"EUROPE\",  # .Azores Islands\n    132.: \"EUROPE\",  # .Romania\n    134.: \"EUROPE\",  # .Spain\n    136.: \"EUROPE\",  # .Sweden\n    137.: \"EUROPE\",  # .Switzerland\n    138.: \"EUROPE\",  # .United Kingdom, Not Specified\n    139.: \"EUROPE\",  # .England\n    140.: \"EUROPE\",  # .Scotland\n    142.: \"EUROPE\",  # .Northern Ireland (201.: \"\", #7 or later)\n    147.: \"EUROPE\",  # .Yugoslavia\n    148.: \"EUROPE\",  # .Czech Republic\n    149.: \"EUROPE\",  # .Slovakia\n    150.: \"EUROPE\",  # .Bosnia and Herzegovina\n    151.: \"EUROPE\",  # .Croatia\n    152.: \"EUROPE\",  # .Macedonia\n    154.: \"EUROPE\",  # .Serbia\n    156.: \"EUROPE\",  # .Latvia\n    157.: \"EUROPE\",  # .Lithuania\n    158.: \"EUROPE\",  # .Armenia\n    159.: \"EUROPE\",  # .Azerbaijan\n    160.: \"EUROPE\",  # .Belarus\n    161.: \"EUROPE\",  # .Georgia\n    162.: \"EUROPE\",  # .Moldova\n    163.: \"ASIA\",  # .Russia\n    164.: \"ASIA\",  # .Ukraine\n    165.: \"ASIA\",  # .USSR\n    166.: \"EUROPE\",  # .Europe (201.: \"\", #7 or later)\n    167.: \"EUROPE\",  # .Kosovo (201.: \"\", #7 or later)\n    168.: \"EUROPE\",  # .Montenegro\n    169.: \"EUROPE\",  # .Other Europe, Not Specified\n    200.: \"MIDDLEEAST\",  # .Afghanistan\n    202.: \"ASIA\",  # .Bangladesh\n    203.: \"ASIA\",  # .Bhutan\n    205.: \"ASIA\",  # .Myanmar\n    206.: \"ASIA\",  # .Cambodia\n    207.: \"ASIA\",  # .China\n    208.: \"EUROPE\",  # .Cyprus (201.: \"\", #6 or earlier)\n    209.: \"ASIA\",  # .Hong Kong\n    210.: \"ASIA\",  # .India\n    211.: \"ASIA\",  # .Indonesia\n    212.: \"MIDDLEEAST\",  # .Iran\n    213.: \"MIDDLEEAST\",  # .Iraq\n    214.: \"MIDDLEEAST\",  # .Israel\n    215.: \"ASIA\",  # .Japan\n    216.: \"MIDDLEEAST\",  # .Jordan\n    217.: \"ASIA\",  # .Korea\n    218.: \"MIDDLEEAST\",  # .Kazakhstan\n    219.: \"MIDDLEEAST\",  # .Kyrgyzstan (201.: \"\", #7 or later)\n    222.: \"MIDDLEEAST\",  # .Kuwait\n    223.: \"ASIA\",  # .Laos\n    224.: \"MIDDLEEAST\",  # .Lebanon\n    226.: \"ASIA\",  # .Malaysia\n    228.: \"ASIA\",  # .Mongolia (201.: \"\", #7 or later)\n    229.: \"ASIA\",  # .Nepal\n    231.: \"ASIA\",  # .Pakistan\n    233.: \"ASIA\",  # .Philippines\n    235.: \"ASIA\",  # .Saudi Arabia\n    236.: \"ASIA\",  # .Singapore\n    238.: \"ASIA\",  # .Sri Lanka\n    239.: \"ASIA\",  # .Syria\n    240.: \"ASIA\",  # .Taiwan\n    242.: \"ASIA\",  # .Thailand\n    243.: \"EUROPE\",  # .Turkey\n    245.: \"MIDDLEEAST\",  # .United Arab Emirates\n    246.: \"MIDDLEEAST\",  # .Uzbekistan\n    247.: \"ASIA\",  # .Vietnam\n    248.: \"MIDDLEEAST\",  # .Yemen\n    249.: \"ASIA\",  # .Asia\n    253.: \"ASIA\",  # .South Central Asia, Not Specified\n    254.: \"ASIA\",  # .Other Asia, Not Specified\n    300.: \"CARIBBEAN\",  # .Bermuda\n    301.: \"CANADA\",  # .Canada\n    303.: \"MEXICO\",  # .Mexico\n    310.: \"CENTRALAMERICA\",  # .Belize\n    311.: \"CENTRALAMERICA\",  # .Costa Rica\n    312.: \"CENTRALAMERICA\",  # .El Salvador\n    313.: \"CENTRALAMERICA\",  # .Guatemala\n    314.: \"CENTRALAMERICA\",  # .Honduras\n    315.: \"CENTRALAMERICA\",  # .Nicaragua\n    316.: \"CENTRALAMERICA\",  # .Panama\n    321.: \"CARIBBEAN\",  # .Antigua and Barbuda\n    323.: \"CARIBBEAN\",  # .Bahamas\n    324.: \"CARIBBEAN\",  # .Barbados\n    327.: \"CARIBBEAN\",  # .Cuba\n    328.: \"CARIBBEAN\",  # .Dominica\n    329.: \"CARIBBEAN\",  # .Dominican Republic\n    330.: \"CARIBBEAN\",  # .Grenada\n    332.: \"CARIBBEAN\",  # .Haiti\n    333.: \"CARIBBEAN\",  # .Jamaica\n    338.: \"CARIBBEAN\",  # .St. Kitts-Nevis (201.: \"\", #7 or later)\n    339.: \"CARIBBEAN\",  # .St. Lucia\n    340.: \"CARIBBEAN\",  # .St. Vincent and the Grenadines\n    341.: \"CARIBBEAN\",  # .Trinidad and Tobago\n    343.: \"CARIBBEAN\",  # .West Indies\n    344.: \"CARIBBEAN\",  # .Caribbean, Not Specified\n    360.: \"SOUTHAMERICA\",  # .Argentina\n    361.: \"SOUTHAMERICA\",  # .Bolivia\n    362.: \"SOUTHAMERICA\",  # .Brazil\n    363.: \"SOUTHAMERICA\",  # .Chile\n    364.: \"SOUTHAMERICA\",  # .Colombia\n    365.: \"SOUTHAMERICA\",  # .Ecuador\n    368.: \"SOUTHAMERICA\",  # .Guyana\n    369.: \"SOUTHAMERICA\",  # .Paraguay\n    370.: \"SOUTHAMERICA\",  # .Peru\n    372.: \"SOUTHAMERICA\",  # .Uruguay\n    373.: \"SOUTHAMERICA\",  # .Venezuela\n    374.: \"SOUTHAMERICA\",  # .South America\n    399.: \"SOUTHAMERICA\",  # .Americas, Not Specified\n    400.: \"AFRICA\",  # .Algeria\n    407.: \"AFRICA\",  # .Cameroon\n    408.: \"AFRICA\",  # .Cabo Verde\n    412.: \"AFRICA\",  # .Congo\n    414.: \"AFRICA\",  # .Egypt\n    416.: \"AFRICA\",  # .Ethiopia\n    417.: \"AFRICA\",  # .Eritrea\n    420.: \"AFRICA\",  # .Gambia\n    421.: \"AFRICA\",  # .Ghana\n    423.: \"AFRICA\",  # .Guinea\n    425.: \"AFRICA\",  # .Ivory Coast (201.: \"\", #7 or later)\n    427.: \"AFRICA\",  # .Kenya\n    429.: \"AFRICA\",  # .Liberia\n    430.: \"AFRICA\",  # .Libya\n    436.: \"AFRICA\",  # .Morocco\n    440.: \"AFRICA\",  # .Nigeria\n    442.: \"AFRICA\",  # .Rwanda (201.: \"\", #7 or later)\n    444.: \"AFRICA\",  # .Senegal\n    447.: \"AFRICA\",  # .Sierra Leone\n    448.: \"AFRICA\",  # .Somalia\n    449.: \"AFRICA\",  # .South Africa\n    451.: \"AFRICA\",  # .Sudan\n    453.: \"AFRICA\",  # .Tanzania\n    454.: \"AFRICA\",  # .Togo\n    456.: \"AFRICA\",  # .Tunisia (201.: \"\", #7 or later)\n    457.: \"AFRICA\",  # .Uganda\n    459.: \"AFRICA\",  # .Democratic Republic of Congo (Zaire)\n    460.: \"AFRICA\",  # .Zambia\n    461.: \"AFRICA\",  # .Zimbabwe\n    462.: \"AFRICA\",  # .Africa\n    463.: \"AFRICA\",  # .South Sudan (201.: \"\", #7 or later)\n    464.: \"AFRICA\",  # .Northern Africa, Not Specified\n    467.: \"AFRICA\",  # .Western Africa, Not Specified\n    468.: \"AFRICA\",  # .Other Africa, Not Specified\n    469.: \"AFRICA\",  # .Eastern Africa, Not Specified\n    501.: \"OCEANIA\",  # .Australia\n    508.: \"OCEANIA\",  # .Fiji\n    511.: \"OCEANIA\",  # .Marshall Islands\n    512.: \"OCEANIA\",  # .Micronesia\n    515.: \"OCEANIA\",  # .New Zealand\n    523.: \"OCEANIA\",  # .Tonga\n    527.: \"OCEANIA\",  # .Samoa\n    554.: \"OCEANIA\",\n    # .Other US Island Areas, Oceania, Not Specified, or at Sea\n}\n\n# List of 630 unique codes that occur in the data\nALL_CODES = [10.0, 20.0, 40.0, 50.0, 51.0, 52.0, 60.0, 100.0, 101.0, 102.0,\n             110.0, 120.0, 135.0, 136.0, 137.0, 140.0, 150.0, 160.0, 205.0,\n             220.0, 230.0, 300.0, 310.0, 330.0, 335.0, 340.0, 350.0, 360.0,\n             410.0, 420.0, 425.0, 430.0, 440.0, 500.0, 510.0, 520.0, 530.0,\n             540.0, 565.0, 600.0, 630.0, 640.0, 650.0, 700.0, 705.0, 710.0,\n             725.0, 726.0, 735.0, 740.0, 750.0, 800.0, 810.0, 820.0, 830.0,\n             840.0, 845.0, 850.0, 860.0, 900.0, 910.0, 930.0, 940.0, 950.0,\n             960.0, 1005.0, 1006.0, 1007.0, 1010.0, 1020.0, 1021.0, 1022.0,\n             1030.0, 1031.0, 1032.0, 1050.0, 1060.0, 1065.0, 1105.0, 1106.0,\n             1107.0, 1108.0, 1200.0, 1220.0, 1240.0, 1300.0, 1305.0, 1306.0,\n             1310.0, 1320.0, 1340.0, 1350.0, 1360.0, 1400.0, 1410.0, 1420.0,\n             1430.0, 1440.0, 1450.0, 1460.0, 1520.0, 1530.0, 1540.0, 1541.0,\n             1545.0, 1550.0, 1551.0, 1555.0, 1560.0, 1600.0, 1610.0, 1640.0,\n             1650.0, 1700.0, 1710.0, 1720.0, 1740.0, 1745.0, 1750.0, 1760.0,\n             1800.0, 1820.0, 1821.0, 1822.0, 1825.0, 1840.0, 1860.0, 1900.0,\n             1910.0, 1920.0, 1930.0, 1935.0, 1965.0, 1970.0, 1980.0, 2000.0,\n             2001.0, 2002.0, 2003.0, 2004.0, 2005.0, 2006.0, 2010.0, 2011.0,\n             2012.0, 2013.0, 2014.0, 2015.0, 2016.0, 2025.0, 2040.0, 2050.0,\n             2060.0, 2100.0, 2105.0, 2145.0, 2160.0, 2170.0, 2180.0, 2200.0,\n             2205.0, 2300.0, 2310.0, 2320.0, 2330.0, 2340.0, 2350.0, 2360.0,\n             2400.0, 2430.0, 2435.0, 2440.0, 2540.0, 2545.0, 2550.0, 2555.0,\n             2600.0, 2630.0, 2631.0, 2632.0, 2633.0, 2634.0, 2635.0, 2636.0,\n             2640.0, 2700.0, 2710.0, 2720.0, 2721.0, 2722.0, 2723.0, 2740.0,\n             2750.0, 2751.0, 2752.0, 2755.0, 2760.0, 2770.0, 2800.0, 2805.0,\n             2810.0, 2825.0, 2830.0, 2840.0, 2850.0, 2860.0, 2861.0, 2862.0,\n             2865.0, 2900.0, 2905.0, 2910.0, 2920.0, 3000.0, 3010.0, 3030.0,\n             3040.0, 3050.0, 3060.0, 3090.0, 3100.0, 3110.0, 3120.0, 3140.0,\n             3150.0, 3160.0, 3200.0, 3210.0, 3220.0, 3230.0, 3245.0, 3250.0,\n             3255.0, 3256.0, 3258.0, 3260.0, 3261.0, 3270.0, 3300.0, 3310.0,\n             3320.0, 3321.0, 3322.0, 3323.0, 3324.0, 3330.0, 3400.0, 3401.0,\n             3402.0, 3420.0, 3421.0, 3422.0, 3423.0, 3424.0, 3430.0, 3500.0,\n             3510.0, 3515.0, 3520.0, 3535.0, 3540.0, 3545.0, 3550.0, 3600.0,\n             3601.0, 3602.0, 3603.0, 3605.0, 3610.0, 3620.0, 3630.0, 3640.0,\n             3645.0, 3646.0, 3647.0, 3648.0, 3649.0, 3655.0, 3700.0, 3710.0,\n             3720.0, 3725.0, 3730.0, 3740.0, 3750.0, 3800.0, 3801.0, 3802.0,\n             3820.0, 3840.0, 3850.0, 3870.0, 3900.0, 3910.0, 3930.0, 3940.0,\n             3945.0, 3946.0, 3955.0, 3960.0, 4000.0, 4010.0, 4020.0, 4030.0,\n             4040.0, 4050.0, 4055.0, 4060.0, 4110.0, 4120.0, 4130.0, 4140.0,\n             4150.0, 4160.0, 4200.0, 4210.0, 4220.0, 4230.0, 4240.0, 4250.0,\n             4251.0, 4252.0, 4255.0, 4300.0, 4320.0, 4330.0, 4340.0, 4350.0,\n             4400.0, 4410.0, 4420.0, 4430.0, 4435.0, 4460.0, 4461.0, 4465.0,\n             4500.0, 4510.0, 4520.0, 4521.0, 4522.0, 4525.0, 4530.0, 4540.0,\n             4600.0, 4610.0, 4620.0, 4621.0, 4622.0, 4640.0, 4650.0, 4655.0,\n             4700.0, 4710.0, 4720.0, 4740.0, 4750.0, 4760.0, 4800.0, 4810.0,\n             4820.0, 4830.0, 4840.0, 4850.0, 4900.0, 4920.0, 4930.0, 4940.0,\n             4950.0, 4965.0, 5000.0, 5010.0, 5020.0, 5030.0, 5040.0, 5100.0,\n             5110.0, 5120.0, 5130.0, 5140.0, 5150.0, 5160.0, 5165.0, 5200.0,\n             5220.0, 5230.0, 5240.0, 5250.0, 5260.0, 5300.0, 5310.0, 5320.0,\n             5330.0, 5340.0, 5350.0, 5360.0, 5400.0, 5410.0, 5420.0, 5500.0,\n             5510.0, 5520.0, 5521.0, 5522.0, 5530.0, 5540.0, 5550.0, 5560.0,\n             5600.0, 5610.0, 5620.0, 5630.0, 5700.0, 5710.0, 5720.0, 5730.0,\n             5740.0, 5800.0, 5810.0, 5820.0, 5840.0, 5850.0, 5860.0, 5900.0,\n             5910.0, 5920.0, 5940.0, 6005.0, 6010.0, 6040.0, 6050.0, 6100.0,\n             6115.0, 6120.0, 6130.0, 6200.0, 6210.0, 6220.0, 6230.0, 6240.0,\n             6250.0, 6260.0, 6300.0, 6305.0, 6320.0, 6330.0, 6355.0, 6360.0,\n             6400.0, 6410.0, 6420.0, 6440.0, 6441.0, 6442.0, 6460.0, 6515.0,\n             6520.0, 6530.0, 6540.0, 6600.0, 6660.0, 6700.0, 6710.0, 6720.0,\n             6730.0, 6740.0, 6765.0, 6800.0, 6820.0, 6825.0, 6830.0, 6835.0,\n             6840.0, 6850.0, 6940.0, 6950.0, 7000.0, 7010.0, 7020.0, 7030.0,\n             7040.0, 7100.0, 7110.0, 7120.0, 7130.0, 7140.0, 7150.0, 7160.0,\n             7200.0, 7210.0, 7220.0, 7240.0, 7260.0, 7300.0, 7315.0, 7320.0,\n             7330.0, 7340.0, 7350.0, 7360.0, 7410.0, 7420.0, 7430.0, 7510.0,\n             7540.0, 7560.0, 7610.0, 7630.0, 7640.0, 7700.0, 7710.0, 7720,\n             7730.0, 7740.0, 7750.0, 7800.0, 7810.0, 7830.0, 7840.0, 7850.0,\n             7855.0, 7900.0, 7905.0, 7920.0, 7925.0, 7930.0, 7940.0, 7950.0,\n             8000.0, 8025.0, 8030.0, 8040.0, 8100.0, 8130.0, 8140.0, 8220.0,\n             8225.0, 8250.0, 8255.0, 8256.0, 8300.0, 8310.0, 8320.0, 8330.0,\n             8335.0, 8350.0, 8365.0, 8400.0, 8410.0, 8420.0, 8450.0, 8460.0,\n             8465.0, 8500.0, 8510.0, 8530.0, 8540.0, 8550.0, 8555.0, 8600.0,\n             8610.0, 8620.0, 8630.0, 8640.0, 8650.0, 8710.0, 8720.0, 8730.0,\n             8740.0, 8750.0, 8760.0, 8800.0, 8810.0, 8830.0, 8850.0, 8910.0,\n             8920.0, 8930.0, 8940.0, 8950.0, 8965.0, 8990.0, 9000.0, 9005.0,\n             9030.0, 9040.0, 9050.0, 9110.0, 9120.0, 9121.0, 9122.0, 9130.0,\n             9140.0, 9141.0, 9142.0, 9150.0, 9200.0, 9210.0, 9240.0, 9260.0,\n             9265.0, 9300.0, 9310.0, 9350.0, 9360.0, 9365.0, 9410.0, 9415.0,\n             9420.0, 9430.0, 9510.0, 9520.0, 9560.0, 9570.0, 9600.0, 9610.0,\n             9620.0, 9630.0, 9640.0, 9645.0, 9650.0, 9720.0, 9750.0, 9760.0,\n             9800.0, 9810.0, 9820.0, 9825.0, 9830.0, 9920.0]\n\nOCCP_MAPPING_IDENTITY = {\n    k: str(int(k)) for k in ALL_CODES\n}\n\n\ndef _float_to_string_mapping(minval, maxval):\n    return {float(x): f\"{x:02d}\" for x in range(minval, maxval + 1)}\n\n\nDEFAULT_ACS_FEATURE_MAPPINGS = {\n    # Ancestry recode (included to match folktables feature sets)\n    'ANC': _float_to_string_mapping(1, 8),\n    # Citizenship\n    'CIT': _float_to_string_mapping(1, 5),\n    # \"Class of worker\"\n    'COW': _float_to_string_mapping(1, 9),\n    # Hearing difficulty\n    'DEAR': _float_to_string_mapping(1, 2),\n    # Vision difficulty\n    'DEYE': _float_to_string_mapping(1, 2),\n    # Cognitive difficulty\n    'DREM': _float_to_string_mapping(1, 2),\n    # Division code based on 2010 Census\n    'DIVISION': _float_to_string_mapping(0, 9),\n    # Ability to speak English\n    'ENG': _float_to_string_mapping(0, 4),\n    # Employment status of parents\n    'ESP': _float_to_string_mapping(0, 8),\n    # Employment Status Recode\n    'ESR': _float_to_string_mapping(0, 6),\n    # Gave birth to child within the past 12 months\n    'FER': _float_to_string_mapping(0, 2),\n    # Insurance through a current or former employer or union\n    'HINS1': _float_to_string_mapping(1, 2),\n    # Insurance purchased directly from an insurance company\n    'HINS2': _float_to_string_mapping(1, 2),\n    # Medicare, for people 65 and older, or people with certain disabilities\n    'HINS3': _float_to_string_mapping(1, 2),\n    # Medicaid, Medical Assistance, or any kind of government-assistance plan for those with low incomes or a disability\n    'HINS4': _float_to_string_mapping(1, 2),\n    # Marital status.\n    'MAR': _float_to_string_mapping(0, 5),\n    # Mobility status\n    'MIG': _float_to_string_mapping(1, 3),\n    # Military service\n    'MIL': _float_to_string_mapping(0, 4),\n    # Nativity\n    'NATIVITY': _float_to_string_mapping(1, 2),\n    # On layoff from work (Unedited-See \"Employment Status Recode\" (ESR))\n    'NWLA': _float_to_string_mapping(1, 3),\n    # Looking for work (Unedited-See \"Employment Status Recode\" (ESR))\n    'NWLK': _float_to_string_mapping(1, 3),\n    # # Occupation recode for 2018 and later based on 2018 OCC codes.\n    # 'OCCP': OCCP_MAPPING_FINE,\n    # Place of birth.\n    'POBP': POBP_MAPPING,\n    # Relationship\n    'RELP': _float_to_string_mapping(0, 17),\n    # Educational attainment\n    'SCHL': _float_to_string_mapping(1, 24),\n    'ST': _float_to_string_mapping(1, 72),\n    # Worked last week\n    'WRK': _float_to_string_mapping(0, 2)\n}\n\nOCCP_MAPPINGS = {\n    'identity': OCCP_MAPPING_IDENTITY,\n    'coarse': {k: v[0] for k, v in ACS_OCCP_CODE_MAPPING.items()},\n    'fine': {k: v[1] for k, v in ACS_OCCP_CODE_MAPPING.items()},\n}\n\n\ndef get_feature_mapping(occp_mapping='coarse') -> frozendict:\n    \"\"\"Helper function to fetch feature mapping dict.\n\n    Returns a nested dict mapping feature names to a mapping;\n    the mapping assigns each possible value of the feature\n    to a new set of values (in most cases this is either\n    a 1:1 mapping or a many:1 mapping to reduce cardinality).\n    \"\"\"\n    assert occp_mapping in ('identity', 'coarse', 'fine')\n    mapping = DEFAULT_ACS_FEATURE_MAPPINGS\n    mapping['OCCP'] = OCCP_MAPPINGS[occp_mapping]\n    return frozendict(mapping)\n"}
{"type": "source_file", "path": "scripts/ray_train.py", "content": "\"\"\"\nMain worker script to conduct an experiment.\n\nBasic usage:\n    python scripts/ray_train.py \\\n    --models mlp \\\n    --experiment adult \\\n    --use_cached\n\"\"\"\nimport argparse\nimport json\nimport logging\nimport os\nfrom typing import Optional, List\n\nimport pandas as pd\nimport torch\n\nfrom tableshift.configs.ray_configs import get_default_ray_tmp_dir, \\\n    get_default_ray_local_dir\nfrom tableshift.core import get_dataset, get_iid_dataset\nfrom tableshift.core.utils import timestamp_as_int\nfrom tableshift.models.compat import PYTORCH_MODEL_NAMES, \\\n    DOMAIN_GENERALIZATION_MODEL_NAMES\nfrom tableshift.models.ray_utils import RayExperimentConfig, \\\n    run_ray_tune_experiment, \\\n    accuracy_metric_name_and_mode_for_model, \\\n    fetch_postprocessed_results_df\n\nLOG_LEVEL = logging.DEBUG\n\nlogger = logging.getLogger()\nlogging.basicConfig(\n    format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',\n    level=LOG_LEVEL,\n    datefmt='%Y-%m-%d %H:%M:%S')\n\n\ndef main(experiment: str,\n         cache_dir: str,\n         ray_tmp_dir: str,\n         ray_local_dir: str,\n         debug: bool,\n         no_tune: bool,\n         num_samples: int,\n         search_alg: str,\n         use_cached: bool,\n         results_dir: str,\n         max_concurrent_trials: int,\n         models: Optional[List[str]] = None,\n         num_workers=1,\n         gpu_per_worker: float = 1.0,\n         cpu_per_worker: int = 1,\n         scheduler: str = None,\n         gpu_models_only: bool = False,\n         cpu_models_only: bool = False,\n         no_dg: bool = False,\n         use_iid: bool = False,\n         no_per_domain_metrics: bool = False,\n         exclude_models: Optional[List[str]] = None,\n         config_dict: Optional[str] = None,\n         ):\n    start_time = timestamp_as_int()\n    assert not (gpu_models_only and cpu_models_only)\n    if gpu_models_only:\n        models = PYTORCH_MODEL_NAMES\n        assert torch.cuda.is_available(), \\\n            \"gpu_models_only is True but GPU is not available.\"\n    elif cpu_models_only:\n        models = [\"xgb\", \"lightgbm\", \"catboost\"]\n    else:\n        assert models is not None\n\n    if no_dg:\n        logging.info(\n            f\"no_dg is {no_dg}; dropping domain generalization models\")\n        models = list(set(models) - set(DOMAIN_GENERALIZATION_MODEL_NAMES))\n\n    if exclude_models:\n        logging.info(f\"dropping models {exclude_models}\")\n        models = list(set(models) - set(exclude_models))\n\n    logging.info(f\"training models {models}\")\n\n    if not ray_tmp_dir:\n        ray_tmp_dir = get_default_ray_tmp_dir()\n    if not ray_local_dir:\n        ray_local_dir = get_default_ray_local_dir()\n\n    if debug:\n        logging.info(\"running in debug mode.\")\n        experiment = \"_debug\"\n\n    if config_dict:\n        config_dict = json.loads(config_dict)\n        logging.info(f\"parsed config_dict {config_dict}\")\n\n    if use_iid:\n        dset = get_iid_dataset(name=experiment, cache_dir=cache_dir,\n                               use_cached=use_cached)\n    else:\n        dset = get_dataset(name=experiment, cache_dir=cache_dir,\n                           use_cached=use_cached)\n    logging.debug(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n\n    expt_results_dir = os.path.join(results_dir, experiment, str(start_time))\n    logging.info(f\"results will be written to {expt_results_dir}\")\n    if not os.path.exists(expt_results_dir): os.makedirs(expt_results_dir)\n\n    iterates = []\n    for model_name in models:\n        logging.info(f\"training model {model_name}\")\n        metric_name, mode = accuracy_metric_name_and_mode_for_model(model_name)\n\n        tune_config = RayExperimentConfig(\n            max_concurrent_trials=max_concurrent_trials,\n            ray_tmp_dir=ray_tmp_dir,\n            ray_local_dir=ray_local_dir,\n            num_workers=num_workers,\n            num_samples=num_samples,\n            tune_metric_name=metric_name,\n            search_alg=search_alg,\n            scheduler=scheduler,\n            gpu_per_worker=gpu_per_worker,\n            cpu_per_worker=cpu_per_worker,\n            config_dict=config_dict,\n            mode=mode) if not no_tune else None\n\n        results = run_ray_tune_experiment(\n            dset=dset, model_name=model_name,\n            tune_config=tune_config,\n            debug=debug,\n            compute_per_domain_metrics=not no_per_domain_metrics)\n\n        df = fetch_postprocessed_results_df(results)\n\n        df[\"estimator\"] = model_name\n        df[\"domain_split_varname\"] = dset.domain_split_varname\n        df[\"domain_split_ood_values\"] = str(dset.get_domains(\"ood_test\"))\n        df[\"domain_split_id_values\"] = str(dset.get_domains(\"id_test\"))\n        if not debug:\n            iter_fp = os.path.join(\n                expt_results_dir,\n                f\"tune_results_{experiment}_{start_time}_\"\n                f\"{dset.uid.replace(' ', '')[:50]}_{model_name}.csv\")\n            logging.info(f\"writing results for {model_name} to {iter_fp}\")\n            df.to_csv(iter_fp, index=False)\n        iterates.append(df)\n\n        print(df)\n        logging.info(f\"finished training model {model_name}\")\n\n    fp = os.path.join(expt_results_dir,\n                      f\"tune_results_{experiment}_{start_time}_full.csv\")\n    logging.info(f\"writing results to {fp}\")\n    pd.concat(iterates).to_csv(fp, index=False)\n    logging.info(f\"completed domain shift experiment {experiment}!\")\n    return\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config_dict\", default=None,\n        help='json-formatted string of config overrides.'\n             'If provided, any parameters here will override'\n             'values provided in the config during training.'\n             'Use to e.g. specify a fixed batch size.'\n             'Example: --config_dict \"{\\\"batch_size\\\": 256, \\\"some_list_param\\\": [0., 0., 1]}')\n    parser.add_argument(\"--cache_dir\", default=\"tmp\",\n                        help=\"Directory to cache raw data files to.\")\n    parser.add_argument(\"--cpu_models_only\", default=False,\n                        action=\"store_true\",\n                        help=\"whether to only use models that use CPU.\"\n                             \"Mutually exclusive of --gpu_models_only.\")\n    parser.add_argument(\"--cpu_per_worker\", default=1, type=int,\n                        help=\"Number of CPUs to provide per worker.\"\n                             \"If not set, Ray defaults to 1.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", default=False,\n                        help=\"Whether to run in debug mode. If True, various \"\n                             \"truncations/simplifications are performed to \"\n                             \"speed up experiment.\")\n    parser.add_argument(\"--experiment\", default=\"diabetes_readmission\",\n                        help=\"Experiment to run. Overridden when debug=True.\")\n    parser.add_argument(\"--exclude_models\", nargs=\"+\", action=\"store\",\n                        default=[],\n                        help=\"models to exclude, by name. Can be specified\"\n                             \"multiple times, e.g. --exclude_models dro mlp xgb ...\")\n    parser.add_argument(\"--gpu_models_only\", default=False,\n                        action=\"store_true\",\n                        help=\"whether to only train models that use GPU.\"\n                             \"Mutually exclusive of cpu_models_only.\")\n    parser.add_argument(\"--gpu_per_worker\", default=1.0, type=float,\n                        help=\"GPUs per worker. Use fractional values < 1. \"\n                             \"(e.g. --gpu_per_worker=0.5) in order\"\n                             \"to allow multiple workers to share GPU.\")\n    parser.add_argument(\"--models\", nargs=\"+\", action=\"store\", default=[\"mlp\"],\n                        help=\"Model names to train. Not used if \"\n                             \"--cpu_models_only or --gpu_models_only is used.\"\n                             \"Can be specified multiple times, e.g.\"\n                             \"==model mlp xgb dro ...\")\n    parser.add_argument(\"--num_samples\", type=int, default=100,\n                        help=\"Number of hparam samples to take in tuning \"\n                             \"sweep.\")\n    parser.add_argument(\"--num_workers\", type=int, default=1,\n                        help=\"Number of workers to use.\")\n    parser.add_argument(\"--no_dg\", action=\"store_true\", default=False,\n                        help=\"If true, do NOT train domain generalization\"\n                             \"models. Set this flag when there is only a\"\n                             \"single training domain.\")\n    parser.add_argument(\"--no_tune\", action=\"store_true\", default=False,\n                        help=\"If set, suppresses hyperparameter tuning of the \"\n                             \"model (for faster testing).\")\n    parser.add_argument(\"--ray_local_dir\", default=None, type=str,\n                        help=\"\"\"Set the local_dir argument to ray RunConfig. \n                        This is a local  directory where training results are \n                        saved to. If not specified, the script will first \n                        look for any of the dirs specified in ray_configs.py, \n                        and if none of those exist, it will use the Ray \n                        default.\"\"\")\n    parser.add_argument(\"--ray_tmp_dir\", default=None, type=str,\n                        help=\"\"\"Set the the root temporary path for ray. This \n                        is a local  directory where training results are \n                        saved to. If not specified, the script will first \n                        look for any of the dirs specified in ray_configs.py, \n                        and if none of those exist, it will use the Ray \n                        default of /tmp/ray. See \n                        https://docs.ray.io/en/latest/ray-core \n                        /configure.html#logging-and-debugging for more \n                        info.\"\"\")\n    parser.add_argument(\"--results_dir\", default=\"./ray_train_results\",\n                        help=\"where to write results. CSVs will be written to \"\n                             \"experiment-specific subdirectories within this \"\n                             \"directory.\")\n    parser.add_argument(\"--scheduler\", choices=(None, \"asha\", \"median\"),\n                        default=\"asha\",\n                        help=\"Scheduler to use for hyperparameter optimization.\"\n                             \"See https://docs.ray.io/en/latest/tune/api_docs/schedulers.html .\")\n\n    parser.add_argument(\"--search_alg\", default=\"hyperopt\",\n                        choices=[\"hyperopt\", \"random\"],\n                        help=\"Ray search alg to use for hyperparameter tuning.\")\n\n    parser.add_argument(\"--use_cached\", default=False, action=\"store_true\",\n                        help=\"whether to use cached data.\")\n    parser.add_argument(\"--no_per_domain_metrics\", default=False,\n                        action=\"store_true\",\n                        help=\"Whether to suppress per-domain metrics. \"\n                             \"Recommended to set this flag if the number of \"\n                             \"subdomains is large. Sets the \"\n                             \"compute_per_domain_metrics flag in \"\n                             \"run_ray_tune_experiment().\")\n    parser.add_argument(\"--use_iid\", default=False, action=\"store_true\",\n                        help=\"if True, use the IID version of the dataset\"\n                             \"(NOT the domain split version). Do NOT set this \"\n                             \"flag if you intend to use the TableShift \"\n                             \"domain shift benchmark.\")\n    parser.add_argument(\"--max_concurrent_trials\", default=2, type=int,\n                        help=\"max number of concurrent trials in ray.\"\n                             \"Recommended to set to the number of available GPUs,\"\n                             \"but sometimes large numbers of concurrent trials\"\n                             \"can lead to system issues.\")\n    args = parser.parse_args()\n    main(**vars(args))\n"}
{"type": "source_file", "path": "tableshift/configs/domain_shift.py", "content": "from dataclasses import dataclass\nfrom typing import Sequence, Optional, Any, Iterator\n\nfrom tableshift.configs.benchmark_configs import ExperimentConfig\nfrom tableshift.configs.experiment_defaults import DEFAULT_ID_TEST_SIZE, \\\n    DEFAULT_OOD_VAL_SIZE, DEFAULT_ID_VAL_SIZE, DEFAULT_RANDOM_STATE\nfrom tableshift.core import Grouper, PreprocessorConfig, DomainSplitter\nfrom tableshift.datasets import ACS_REGIONS, ACS_YEARS, \\\n    BRFSS_YEARS, CANDC_STATE_LIST, NHANES_YEARS, ANES_YEARS, \\\n    ANES_REGIONS, MIMIC_EXTRACT_SHARED_FEATURES, MIMIC_EXTRACT_STATIC_FEATURES\nfrom tableshift.datasets.assistments import SCHOOL_IDS\n\n\ndef _to_nested(ary: Sequence[Any]) -> Sequence[Sequence[Any]]:\n    \"\"\"Create a nested tuple from a sequence.\n\n    This reformats lists e.g. where each element in the list is the only desired\n    out-of-domain value in an experiment.\n    \"\"\"\n    return tuple([x] for x in ary)\n\n\n@dataclass\nclass DomainShiftExperimentConfig:\n    \"\"\"Class to hold parameters for a domain shift experiment.\n\n    This class defines a *set* of experiments, where the distribution split changes\n    over experiments but all other factors (preprocessing, grouping, etc.) stay fixed.\n\n    This class is used e.g. to identify which of a set of candidate domain splits has the\n    biggest domain gap.\n    \"\"\"\n    tabular_dataset_kwargs: dict\n    domain_split_varname: str\n    domain_split_ood_values: Sequence[Any]\n    grouper: Optional[Grouper]\n    preprocessor_config: PreprocessorConfig\n    domain_split_id_values: Optional[Sequence[Any]] = None\n\n    def as_experiment_config_iterator(\n            self, val_size=DEFAULT_ID_VAL_SIZE,\n            ood_val_size=DEFAULT_OOD_VAL_SIZE,\n            id_test_size=DEFAULT_ID_TEST_SIZE,\n            random_state=DEFAULT_RANDOM_STATE\n    ) -> Iterator[ExperimentConfig]:\n        for i, tgt in enumerate(self.domain_split_ood_values):\n            if self.domain_split_id_values is not None:\n                src = self.domain_split_id_values[i]\n            else:\n                src = None\n            if not isinstance(tgt, tuple) and not isinstance(tgt, list):\n                tgt = (tgt,)\n            splitter = DomainSplitter(\n                val_size=val_size,\n                ood_val_size=ood_val_size,\n                id_test_size=id_test_size,\n                domain_split_varname=self.domain_split_varname,\n                domain_split_ood_values=tgt,\n                domain_split_id_values=src,\n                random_state=random_state)\n            yield ExperimentConfig(splitter=splitter, grouper=self.grouper,\n                                   preprocessor_config=self.preprocessor_config,\n                                   tabular_dataset_kwargs=self.tabular_dataset_kwargs)\n\n\n# Set of fixed domain shift experiments.\ndomain_shift_experiment_configs = {\n    \"acsfoodstamps_region\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"acsfoodstamps\",\n                                \"acs_task\": \"acsfoodstamps\"},\n        domain_split_varname=\"DIVISION\",\n        domain_split_ood_values=_to_nested(ACS_REGIONS),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig()),\n\n    \"acsincome_region\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"acsincome\",\n                                \"acs_task\": \"acsincome\"},\n        domain_split_varname=\"DIVISION\",\n        domain_split_ood_values=_to_nested(ACS_REGIONS),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig()),\n\n    \"acspubcov_disability\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"acspubcov\",\n                                \"acs_task\": \"acspubcov\",\n                                \"years\": ACS_YEARS},\n        domain_split_varname=\"DIS\",\n        domain_split_ood_values=_to_nested(['1.0']),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig()),\n\n    \"acsunemployment_edlvl\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"acsunemployment\",\n                                \"acs_task\": \"acsunemployment\"},\n        domain_split_varname='SCHL',\n        # No high school diploma vs. GED/diploma or higher.\n        domain_split_ood_values=[['01', '02', '03', '04',\n                                  '05', '06', '07', '08',\n                                  '09', '10', '11', '12',\n                                  '13', '14', '15'],\n                                 ['16', '17', '18', '19',\n                                  '20', '21', '22', '23', '24']],\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig()),\n\n    \"acsunemployment_mobility\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"acsunemployment\",\n                                \"acs_task\": \"acsunemployment\"},\n        domain_split_varname='MIG',\n        domain_split_ood_values=[['02', '03']],\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig()),\n\n    \"assistments\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"assistments\"},\n        domain_split_varname='school_id',\n        domain_split_ood_values=SCHOOL_IDS,\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"skill_id\", \"bottom_hint\", \"first_action\"],\n        ),\n    ),\n\n    \"college_scorecard_ccbasic\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"college_scorecard\"},\n        domain_split_varname='CCBASIC',\n        domain_split_ood_values=[\n            \"Associate's--Public Urban-serving Multicampus\",\n            \"Associate's--Public 2-year colleges under 4-year universities\",\n            \"Associate's--Public Suburban-serving Single Campus\",\n            'Special Focus Institutions--Schools of business and management',\n            \"Associate's--Public Rural-serving Small\",\n            \"Associate's--Public Rural-serving Medium\",\n            'Baccalaureate Colleges--Arts & Sciences',\n            'Special Focus Institutions--Theological seminaries, Bible colleges, and other faith-related institutions',\n            'Research Universities (high research activity)',\n            \"Associate's--Private For-profit\",\n            \"Associate's--Public Rural-serving Large\",\n            \"Master's Colleges and Universities (larger programs)\",\n            \"Baccalaureate/Associate's Colleges\",\n            \"Master's Colleges and Universities (smaller programs)\",\n            'Tribal Colleges',\n            'Special Focus Institutions--Schools of art, music, and design',\n            \"Associate's--Private Not-for-profit\",\n            \"Associate's--Public Suburban-serving Multicampus\",\n            \"Associate's--Private For-profit 4-year Primarily Associate's\",\n            \"Master's Colleges and Universities (medium programs)\",\n            \"Associate's--Public 4-year Primarily Associate's\",\n            \"Associate's--Public Urban-serving Single Campus\",\n            'Special Focus Institutions--Schools of law',\n            'Baccalaureate Colleges--Diverse Fields',\n            'Research Universities (very high research activity)',\n            'Special Focus Institutions--Other health professions schools',\n            'Doctoral/Research Universities',\n            'Special Focus Institutions--Other technology-related schools',\n            'Special Focus Institutions--Other special-focus institutions',\n            'Special Focus Institutions--Medical schools and medical centers',\n            'Special Focus Institutions--Schools of engineering',\n            \"Associate's--Private Not-for-profit 4-year Primarily Associate's\",\n            \"Associate's--Public Special Use\"],\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"label_encode\",\n            numeric_features=\"kbins\",\n            n_bins=100,\n            dropna=None,\n        ),\n\n    ),\n\n    \"college_scorecard_ccsizset\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"college_scorecard\"},\n        domain_split_varname='CCSIZSET',\n        domain_split_ood_values=[\n            'Small 2-year (confers associate’s degrees, FTE enrollment 500 to 1,999)',\n            'Medium 2-year (confers associate’s degrees, FTE enrollment 2000 to 4,999)',\n            'Large 2-year (confers associate’s degrees, FTE enrollment 5000 to 9,999)',\n            'Small 4-year, highly residential (confers bachelor’s degrees, FTE enrollment 1,000 to 2,999, at least 50 percent of degree-seeking undergraduates live on campus and at least 80 percent attend full time)',\n            'Medium 4-year, highly residential (confers bachelor’s degrees, FTE enrollment 3,000 to 9,999, at least 50 percent of degree-seeking undergraduates live on campus and at least 80 percent attend full time)',\n            'Very large 2-year (confers associate’s degrees, FTE enrollment 10,000 or more)',\n            'Very small 4-year, primarily residential (confers bachelor’s degrees, FTE enrollment less than 1,000, 25 to 49 percent of degree-seeking undergraduates live on campus and at least 50 percent attend full time)',\n            'Very small 2-year (confers associate’s degrees, FTE enrollment less than 500)',\n            'Small 4-year, primarily nonresidential (confers bachelor’s degrees, FTE enrollment 1,000 to 2,999, less than 25 percent of degree-seeking undergraduates live on campus and/or less than 50 percent attend full time)',\n            'Large 4-year, primarily residential (confers bachelor’s degrees, FTE enrollment over 9,999, 25 to 49 percent of degree-seeking undergraduates live on campus and at least 50 percent attend full time)',\n            'Medium 4-year, primarily residential (confers bachelor’s degrees, FTE enrollment 3,000 to 9,999, 25 to 49 percent of degree-seeking undergraduates live on campus and at least 50 percent attend full time)',\n            'Large 4-year, primarily nonresidential (confers bachelor’s degrees, FTE enrollment over 9,999, less than 25 percent of degree-seeking undergraduates live on campus and/or less than 50 percent attend full time)',\n            'Very small 4-year, highly residential (confers bachelor’s degrees, FTE enrollment less than 1,000, at least 50 percent of degree-seeking undergraduates live on campus and at least 80 percent attend full time)',\n            'Very small 4-year, primarily nonresidential (confers bachelor’s degrees, FTE enrollment less than 1,000, less than 25 percent of degree-seeking undergraduates live on campus and/or less than 50 percent attend full time)',\n            'Small 4-year, primarily residential (confers bachelor’s degrees, FTE enrollment 1,000 to 2,999, 25 to 49 percent of degree-seeking undergraduates live on campus and at least 50 percent attend full time)',\n            'Medium 4-year, primarily nonresidential (confers bachelor’s degrees, FTE enrollment 3,000 to 9,999, less than 25 percent of degree-seeking undergraduates live on campus and/or less than 50 percent attend full time)',\n            'Large 4-year, highly residential (confers bachelor’s degrees, FTE enrollment over 9,999, at least 50 percent of degree-seeking undergraduates live on campus and at least 80 percent attend full time)',\n            'Not applicable, special-focus institution'],\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"label_encode\",\n            numeric_features=\"kbins\",\n            n_bins=100,\n            dropna=None,\n        ),\n\n    ),\n\n    \"brfss_diabetes_race\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"brfss_diabetes\", \"task\": \"diabetes\",\n                                \"years\": BRFSS_YEARS},\n        domain_split_varname=\"PRACE1\",\n        # Train on white nonhispanic; test on all other race identities.\n        domain_split_ood_values=[[2, 3, 4, 5, 6]],\n        domain_split_id_values=_to_nested([1, ]),\n        grouper=Grouper({\"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"IYEAR\"]), ),\n\n    \"brfss_blood_pressure_income\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"brfss_blood_pressure\",\n                                \"task\": \"blood_pressure\",\n                                \"years\": (2019, 2021)},\n        domain_split_varname=\"POVERTY\",\n        # Train on non-poverty observations; test (OOD) on poverty observations\n        domain_split_ood_values=_to_nested([1, ]),\n        domain_split_id_values=_to_nested([0, ]),\n        grouper=Grouper({\"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"IYEAR\"]), ),\n\n    \"brfss_blood_pressure_bmi\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"brfss_blood_pressure\",\n                                \"task\": \"blood_pressure\",\n                                \"years\": BRFSS_YEARS},\n        domain_split_varname=\"BMI5CAT\",\n        # OOD values: [1 underweight, 2 normal weight], [3 overweight, 4 obese]\n        domain_split_ood_values=[['1.0', '2.0'], ['3.0', '4.0']],\n        grouper=Grouper({\"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"IYEAR\"]), ),\n\n    \"candc_st\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"communities_and_crime\"},\n        domain_split_varname=\"state\",\n        domain_split_ood_values=_to_nested(CANDC_STATE_LIST),\n        grouper=Grouper({\"Race\": [1, ], \"income_level_above_median\": [1, ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(),\n    ),\n\n    # Counts by domain are below. We hold out all of the smallest\n    # domains to avoid errors with very small domains during dev.\n    # A48       9\n    # A44      12\n    # A410     12\n    # A45      22\n    # A46      50\n    # A49      97\n    # A41     103\n    # A42     181\n    # A40     234\n    # A43     280\n    \"_debug\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"german\"},\n        domain_split_varname=\"purpose\",\n        domain_split_ood_values=[[\"A44\", \"A410\", \"A45\", \"A46\", \"A48\"]],\n        grouper=Grouper({\"sex\": ['1', ], \"age_geq_median\": ['1', ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(),\n    ),\n\n    # Integer identifier corresponding to 21 distinct values, for example, physician referral,\n    # emergency room, and transfer from a hospital,\n    # https://downloads.hindawi.com/journals/bmri/2014/781670.pdf\n    \"diabetes_admsrc\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"diabetes_readmission\"},\n        domain_split_varname='admission_source_id',\n        domain_split_ood_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 17,\n                                 20, 22, 25],\n        grouper=Grouper({\"race\": [\"Caucasian\", ], \"gender\": [\"Male\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(min_frequency=0.01),\n    ),\n\n    \"heloc_externalrisk\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"heloc\"},\n        domain_split_varname='ExternalRiskEstimateLow',\n        domain_split_ood_values=[[0], [1]],\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(),\n    ),\n\n    \"mooc_course\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"mooc\"},\n        domain_split_varname=\"course_id\",\n        domain_split_ood_values=['HarvardX/CB22x/2013_Spring',\n                                 'HarvardX/CS50x/2012',\n                                 'HarvardX/ER22x/2013_Spring',\n                                 'HarvardX/PH207x/2012_Fall',\n                                 'HarvardX/PH278x/2013_Spring'],\n        grouper=Grouper({\"gender\": [\"m\", ],\n                         \"LoE_DI\": [\"Bachelor's\", \"Master's\", \"Doctorate\"]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(),\n    ),\n\n    \"nhanes_cholesterol_race\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"nhanes_cholesterol\",\n                                \"nhanes_task\": \"cholesterol\",\n                                \"years\": NHANES_YEARS},\n        domain_split_varname='RIDRETH_merged',\n        domain_split_ood_values=[[1, 2, 4, 6, 7]],\n        domain_split_id_values=[[3]],\n        # Group by male vs. all others\n        grouper=Grouper({\"RIAGENDR\": [\"1.0\", ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"nhanes_year\"],\n            numeric_features=\"kbins\")\n    ),\n\n    \"nhanes_lead_poverty\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"nhanes_lead\",\n                                \"nhanes_task\": \"lead\", \"years\": NHANES_YEARS},\n        domain_split_varname='INDFMPIRBelowCutoff',\n        domain_split_ood_values=[[1.]],\n        # Race (non. hispanic white vs. all others; male vs. all others)\n        grouper=Grouper({\"RIDRETH_merged\": [3, ], \"RIAGENDR\": [\"1.0\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"nhanes_year\"],\n            numeric_features=\"kbins\"),\n    ),\n\n    \"physionet_set\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"physionet\"},\n        domain_split_varname=\"set\",\n        domain_split_ood_values=_to_nested([\"a\", \"b\"]),\n        grouper=Grouper({\"Age\": [x for x in range(40, 100)], \"Gender\": [1, ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None)\n    ),\n    \"physionet_los\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"physionet\"},\n        domain_split_varname=\"set\",\n        domain_split_ood_values=_to_nested([\"a\", \"b\"]),\n        grouper=Grouper({\"Age\": [x for x in range(40, 100)], \"Gender\": [1, ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None)\n    ),\n\n    \"anes_region\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"anes\", \"years\": [2020, ]},\n        domain_split_varname='VCF0112',\n        domain_split_ood_values=_to_nested(ANES_REGIONS),\n        grouper=Grouper({\"VCF0104\": [\"1\", ], \"VCF0105a\": [\"1.0\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None)),\n\n    # ANES: test on (2016) or (2020); train on all years prior.\n    \"anes_year\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={\"name\": \"anes\"},\n        domain_split_varname=\"VCF0004\",\n        domain_split_ood_values=[[ANES_YEARS[-2]], [ANES_YEARS[-1]]],\n        domain_split_id_values=[ANES_YEARS[:-2], ANES_YEARS[:-1]],\n        grouper=Grouper({\"VCF0104\": [\"1\", ], \"VCF0105a\": [\"1.0\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None)),\n\n    \"mimic_extract_los_3_ins\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={'task': 'los_3', 'name': 'mimic_extract_los_3'},\n        domain_split_varname=\"insurance\",\n        domain_split_ood_values=_to_nested(\n            [\"Medicare\", \"Medicaid\", \"Government\", \"Self Pay\"]),\n        grouper=Grouper({\"gender\": ['M'], }, drop=False),\n        # We passthrough all non-static columns because we use MIMIC-extract's default\n        # preprocessing/imputation and do not wish to modify it for these features\n        # (static features are not preprocessed by MIMIC-extract). See\n        # tableshift.datasets.mimic_extract.preprocess_mimic_extract().\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[f for f in MIMIC_EXTRACT_SHARED_FEATURES.names\n                                 if\n                                 f not in MIMIC_EXTRACT_STATIC_FEATURES.names])),\n\n    \"mimic_extract_mort_hosp_ins\": DomainShiftExperimentConfig(\n        tabular_dataset_kwargs={'task': 'mort_hosp',\n                                'name': 'mimic_extract_mort_hosp'},\n        domain_split_varname=\"insurance\",\n        domain_split_ood_values=_to_nested(\n            [\"Medicare\", \"Medicaid\", \"Government\", \"Self Pay\"]),\n        grouper=Grouper({\"gender\": ['M'], }, drop=False),\n        # We passthrough all non-static columns because we use MIMIC-extract's default\n        # preprocessing/imputation and do not wish to modify it for these features\n        # (static features are not preprocessed by MIMIC-extract). See\n        # tableshift.datasets.mimic_extract.preprocess_mimic_extract().\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[f for f in MIMIC_EXTRACT_SHARED_FEATURES.names\n                                 if\n                                 f not in MIMIC_EXTRACT_STATIC_FEATURES.names])),\n}\n"}
{"type": "source_file", "path": "scripts/cache_task.py", "content": "\"\"\"\nCache a tabular dataset.\n\nUsage:\n    python scripts/cache_task.py --experiment heloc\n\"\"\"\nimport argparse\nimport logging\n\nfrom tableshift import get_dataset\nfrom tableshift.core.utils import make_uid\n\nLOG_LEVEL = logging.DEBUG\n\nlogger = logging.getLogger()\nlogging.basicConfig(\n    format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',\n    level=LOG_LEVEL,\n    datefmt='%Y-%m-%d %H:%M:%S')\n\n\ndef _cache_experiment(experiment: str, cache_dir,\n                      overwrite: bool,\n                      no_domains_to_subdirectories: bool):\n    dset = get_dataset(experiment, cache_dir, initialize_data=False)\n    if dset.is_cached() and (not overwrite):\n        uid = make_uid(dset.name, dset.splitter)\n        logging.info(f\"dataset with uid {uid} is already cached; skipping\")\n\n    else:\n        domains_to_subdirectories = not no_domains_to_subdirectories\n        logging.info(\n            f\"domains_to_subdirectories is {domains_to_subdirectories}\")\n        dset._initialize_data()\n        dset.to_sharded(domains_to_subdirectories=domains_to_subdirectories)\n    return\n\n\ndef main(cache_dir,\n         experiment,\n         overwrite: bool,\n         no_domains_to_subdirectories: bool = False,\n         domain_shift_experiment=None):\n    assert (experiment or domain_shift_experiment) and \\\n           not (experiment and domain_shift_experiment), \\\n        \"specify either experiment or domain_shift_experiment, but not both.\"\n\n    cache_kwargs = {\n        \"cache_dir\": cache_dir,\n        \"overwrite\": overwrite,\n        \"no_domains_to_subdirectories\": no_domains_to_subdirectories,\n    }\n    logging.debug(f\"cache_kwargs is: {cache_kwargs}\")\n    if experiment:\n        _cache_experiment(experiment, **cache_kwargs)\n        print(\"caching tasks complete!\")\n        return\n\n    print(\"caching tasks complete!\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cache_dir\", default=\"tmp\",\n                        help=\"Directory to cache raw data files to.\")\n    parser.add_argument(\"--domain_shift_experiment\", \"-d\",\n                        help=\"Experiment to run. Overridden when debug=True.\"\n                             \"Example value: 'physionet_set'.\")\n    parser.add_argument(\"--no_domains_to_subdirectories\",\n                        action=\"store_true\",\n                        help=\"If set, domains will NOT be written to separate\"\n                             \"subdirectories. For example, instead of writing files to\"\n                             \"/train/1/train_1.csv where the second level is \"\n                             \"the domain value, they will be written to \"\n                             \"/train/train_1.csv and not split by the domain\"\n                             \"value. Useful when using thresholding.\")\n    parser.add_argument(\"--experiment\",\n                        help=\"Experiment to run. Overridden when debug=True.\"\n                             \"Example value: 'adult'.\")\n    parser.add_argument(\"--overwrite\", action=\"store_true\", default=False)\n\n    args = parser.parse_args()\n    main(**vars(args))\n"}
{"type": "source_file", "path": "tableshift/datasets/acs.py", "content": "\"\"\"\nFeature specs and preprocessing for data from the American Community Survey.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, Union\n\nimport folktables\nimport frozendict\nimport numpy as np\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\n################################################################################\n# Shared features used in more than one task\n################################################################################\n\nNWLK_FEATURE = Feature('NWLK', cat_dtype, \"Looking for work\",\n                       name_extended=\"Looking for work\",\n                       value_mapping={'01': 'Yes', '02': 'No',\n                                      '03': 'Did not report'})\n\nNWLA_FEATURE = Feature('NWLA', cat_dtype, \"On layoff from work\",\n                       name_extended=\"On layoff from work\",\n                       value_mapping={'01': 'Yes', '02': 'No',\n                                      '03': 'Did not report'})\n\nFER_FEATURE = Feature('FER', cat_dtype,\n                      \"Gave birth to child within the past 12 months\",\n                      name_extended=\"Gave birth to child within the past 12 months\",\n                      value_mapping={\n                          '00': \"less than 15 years old or greater than 50 years or male\",\n                          '01': \"Yes\", '02': \"No\", })\n\nDREM_FEATURE = Feature('DREM', cat_dtype, \"Cognitive difficulty\",\n                       name_extended=\"Cognitive difficulty\",\n                       value_mapping={'01': 'Yes', '02': 'No'})\n\nDEYE_FEATURE = Feature('DEYE', cat_dtype, \"Vision difficulty\",\n                       name_extended=\"Vision difficulty\",\n                       value_mapping={'01': 'Yes', '02': 'No'})\n\nANC_FEATURE = Feature('ANC', cat_dtype, \"Ancestry recode\",\n                      name_extended=\"Ancestry\",\n                      value_mapping={'1': 'Single', '2': 'Multiple',\n                                     '3': 'Unclassified',\n                                     '4': 'Not reported',\n                                     '8': 'Ancestry suppressed for this group'})\n\nMIL_FEATURE = Feature('MIL', cat_dtype, \"Military service\",\n                      name_extended=\"Military service\",\n                      value_mapping={'00': 'N/A (less than 17 years old)',\n                                     '01': 'Now on active duty',\n                                     '02': 'On active duty in the past, but not now',\n                                     '03': 'Only on active duty for training in Reserves/National Guard',\n                                     '04': 'Never served in the military'})\n\nESP_FEATURE = Feature('ESP', cat_dtype, \"Employment status of parents\",\n                      name_extended=\"Employment status of parents\",\n                      value_mapping={\n                          '00': 'Not own child of householder, and not child in subfamily',\n                          '01': 'Living with two parents: both parents in labor force',\n                          '02': 'Living with two parents: Father only in labor force',\n                          '03': 'Living with two parents: Mother only in labor force',\n                          '04': 'Living with two parents: Neither parent in labor force',\n                          '05': 'Living with father: Father in the labor force',\n                          '06': 'Living with father: Father not in labor force',\n                          '07': 'Living with mother: Mother in the labor force',\n                          '08': 'Living with mother: Mother not in labor force'})\n\nDIS_FEATURE = Feature('DIS', cat_dtype, \"Disability recode\",\n                      name_extended=\"Disability status\",\n                      value_mapping={'01': 'With a disability',\n                                     '02': 'Without a disability'})\n\nOCCP_FEATURE = Feature('OCCP', cat_dtype,\n                       \"Occupation recode for 2018 and later based on 2018 OCC codes\",\n                       name_extended=\"Occupation\")\n\nWKHP_FEATURE = Feature('WKHP', int,\n                       \"Usual hours worked per week past 12 months\",\n                       name_extended=\"Usual hours worked per week past 12 months\")\n\nRELP_FEATURE = Feature('RELP', cat_dtype, \"Relationship\",\n                       name_extended=\"Relationship to reference person\",\n                       value_mapping={'00': 'Reference person',\n                                      '01': 'Husband/wife',\n                                      '02': 'Biological son or daughter',\n                                      '03': 'Adopted son or daughter',\n                                      '04': 'Stepson or stepdaughter',\n                                      '05': 'Brother or sister',\n                                      '06': 'Father or mother',\n                                      '07': 'Grandchild', '08': 'Parent-in-law',\n                                      '09': 'Son-in-law or daughter-in-law',\n                                      '10': 'Other relative',\n                                      '11': 'Roomer or boarder',\n                                      '12': 'Housemate or roommate',\n                                      '13': 'Unmarried partner',\n                                      '14': 'Foster child',\n                                      '15': 'Other nonrelative',\n                                      '16': 'Institutionalized group quarters population',\n                                      '17': 'Noninstitutionalized group quarters population'})\n\nPOBP_FEATURE = Feature('POBP', cat_dtype, \"Place of birth (Recode)\",\n                       name_extended=\"Place of birth\")\n\nENG_FEATURE = Feature('ENG', cat_dtype, \"Ability to speak English\",\n                      name_extended=\"Ability to speak English\",\n                      value_mapping={\n                          '00': \"less than 5 years old/speaks only English\",\n                          '01': \"Very well\", '02': \"Well\", '03': \"Not well\",\n                          '04': \"Not at all\"})\n\nMIG_FEATURE = Feature('MIG', cat_dtype,\n                      \"Mobility status (lived here 1 year ago)\",\n                      name_extended=\"Mobility status (whether respondent lived here 1 year ago)\",\n                      value_mapping={'00': 'N/A (less than 1 year old)',\n                                     '01': 'Yes, same house(nonmovers)',\n                                     '02': 'No, outside US and Puerto Rico',\n                                     '03': 'No, different house in US or Puerto Rico'})\nNATIVITY_FEATURE = Feature('NATIVITY', cat_dtype, \"Nativity\",\n                           name_extended=\"Nativity\",\n                           value_mapping={'01': 'Native', '02': 'Foreign born'})\nDEAR_FEATURE = Feature('DEAR', cat_dtype, \"Hearing difficulty\",\n                       name_extended=\"Hearing difficulty\",\n                       value_mapping={'01': 'Yes', '02': 'No'})\n\n################################################################################\n# Various ACS-related constants\n################################################################################\n\n\nACS_YEARS = [2014, 2015, 2016, 2017, 2018]\n\n# Region codes; see 'DIVISION' feature below.\nACS_REGIONS = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09']\n\nACS_STATE_LIST = [\n    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n    'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n    'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n    'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n    'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'PR']\n\n# copied from (non-importable) location in folktables.load_acs\n_STATE_CODES = {'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06',\n                'CO': '08', 'CT': '09', 'DE': '10', 'FL': '12', 'GA': '13',\n                'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19',\n                'KS': '20', 'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24',\n                'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28', 'MO': '29',\n                'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34',\n                'NM': '35', 'NY': '36', 'NC': '37', 'ND': '38', 'OH': '39',\n                'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',\n                'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50',\n                'VA': '51', 'WA': '53', 'WV': '54', 'WI': '55', 'WY': '56',\n                'PR': '72'}\n\n# Maps 2-digit numeric strings ('01') to\n# human-readable 2-letter state names ('WA')\nST_CODE_TO_STATE = {v: k for k, v in _STATE_CODES.items()}\n\n################################################################################\n# Feature lists\n################################################################################\n\n# Set of features shared by all ACS tasks.\nACS_SHARED_FEATURES = FeatureList(features=[\n    Feature('AGEP', int, \"Age\", name_extended='age in years'),\n    Feature('SEX', int, \"Sex\",\n            name_extended='sex',\n            value_mapping={\n                1: \"Male\", 2: \"Female\",\n            }),\n    Feature('ST', cat_dtype, \"State Code based on 2010 Census definitions.\",\n            name_extended=\"State\"),\n    Feature('MAR', cat_dtype, \"Marital status\",\n            name_extended='marital status',\n            value_mapping={\n                1: 'Married',\n                2: 'Widowed',\n                3: 'Divorced',\n                4: 'Separated',\n                5: 'Never married or under 15 years old'\n            }),\n    Feature('CIT', cat_dtype, \"\"\"Citizenship status\"\"\",\n            name_extended='citizenship status',\n            value_mapping={\n                1: 'Born in the U.S.',\n                2: 'Born in Puerto Rico, Guam, the U.S. Virgin Islands, '\n                   'or the Northern Marianas',\n                3: 'Born abroad of American parent(s)',\n                4: 'U.S. citizen by naturalization',\n                5: 'Not a citizen of the U.S.',\n            }),\n    Feature('RAC1P', int, \"\"\"Recoded detailed race code\"\"\",\n            name_extended='race',\n            value_mapping={\n                1: 'White alone',\n                2: 'Black or African American alone',\n                3: 'American Indian alone',\n                4: 'Alaska Native alone',\n                5: 'American Indian and Alaska Native tribes specified; or'\n                   ' American Indian or Alaska Native, not specified and '\n                   'no other races',\n                6: 'Asian alone',\n                7: 'Native Hawaiian and Other Pacific Islander alone',\n                8: 'Some Other Race alone',\n                9: 'Two or More Races'}),\n    Feature('SCHL', cat_dtype, \"Educational attainment\",\n            name_extended=\"Educational attainment\",\n            value_mapping={\n                np.nan: 'NA (less than 3 years old)',\n                1: 'No schooling completed',\n                2: 'Nursery school, preschool',\n                3: 'Kindergarten',\n                4: 'Grade 1',\n                5: 'Grade 2',\n                6: 'Grade 3',\n                7: 'Grade 4',\n                8: 'Grade 5',\n                9: 'Grade 6',\n                10: 'Grade 7',\n                11: 'Grade 8',\n                12: 'Grade 9',\n                13: 'Grade 10',\n                14: 'Grade 11',\n                15: '12th grade - no diploma',\n                16: 'Regular high school diploma',\n                17: 'GED or alternative credential',\n                18: 'Some college, but less than 1 year',\n                19: '1 or more years of college credit, no degree',\n                20: \"Associate's degree\",\n                21: \"Bachelor's degree\",\n                22: \"Master's degree\",\n                23: \"Professional degree beyond a bachelor's degree\",\n                24: 'Doctorate degree',\n            }),\n    Feature('DIVISION', cat_dtype,\n            \"Division code based on 2010 Census definitions.\",\n            name_extended='geographic region',\n            value_mapping={\n                0: 'Puerto Rico',\n                1: 'New England (Northeast region)',\n                2: 'Middle Atlantic (Northeast region)',\n                3: 'East North Central (Midwest region)',\n                4: 'West North Central (Midwest region)',\n                5: 'South Atlantic (South region)',\n                6: 'East South Central (South region)',\n                7: 'West South Central (South Region)',\n                8: 'Mountain (West region)',\n                9: 'Pacific (West region)',\n            }),\n    Feature('ACS_YEAR', int, 'Derived feature for ACS year.',\n            name_extended='Year of survey')],\n    documentation=\"https://www2.census.gov/programs-surveys/acs/tech_docs\"\n                  \"/pums/data_dict/PUMS_Data_Dictionary_2019.pdf\"\n)\n\nACS_INCOME_FEATURES = FeatureList([\n    Feature('COW', cat_dtype, \"\"\"Class of worker.\"\"\",\n            name_extended='class of worker',\n            value_mapping={\n                '01': \"Employee of a private for-profit company or business, \"\n                      \"or of an individual, for wages, salary, or commissions\",\n                '02': \"Employee of a private not-for-profit, tax-exempt, or charitable organization\",\n                '03': \"Local government employee (city, county, etc.)\",\n                '04': \"State government employee\",\n                '05': \"Federal government employee\",\n                '06': \"Self-employed in own not incorporated business, professional practice, or farm\",\n                '07': \"Self-employed in own incorporated business, professional practice or farm\",\n                '08': \"Working without pay in family business or farm\"}),\n    ENG_FEATURE,\n    FER_FEATURE,\n    Feature('HINS1', cat_dtype,\n            \"Insurance through a current or former employer or union\",\n            name_extended=\"Has health insurance through a current or former \"\n                          \"employer or union\",\n            value_mapping={'01': 'Yes', '02': 'No'}),\n\n    Feature('HINS2', cat_dtype,\n            \"Insurance purchased directly from an insurance company\",\n            name_extended=\"Has insurance purchased directly from an insurance company\",\n            value_mapping={'01': 'Yes', '02': 'No'}),\n    Feature('HINS3', cat_dtype,\n            \"Medicare, for people 65 and older, or people with certain disabilities\",\n            name_extended=\"Has medicare\",\n            value_mapping={'01': 'Yes', '02': 'No'}),\n    Feature('HINS4', cat_dtype, \"\"\"Medicaid, Medical Assistance, or any kind \n    of government-assistance plan for those with low incomes or a disability\"\"\",\n            name_extended=\"Has Medicaid, medical assistance, or any kind of \"\n                          \"government-assistance plan for those with low \"\n                          \"incomes or a disability\",\n            value_mapping={'01': 'Yes', '02': 'No'}),\n    NWLA_FEATURE,\n    NWLK_FEATURE,\n    OCCP_FEATURE,\n    POBP_FEATURE,\n    RELP_FEATURE,\n    WKHP_FEATURE,\n    Feature('WKW', int, \"Weeks worked during past 12 months.\",\n            name_extended=\"Weeks worked during past 12 months\"),\n    Feature('WRK', cat_dtype, \"Worked last week\",\n            name_extended=\"Worked last week\",\n            value_mapping={'00': 'NA (not reported)',\n                           '01': 'Worked',\n                           '02': 'Did not work'}),\n    Feature('PINCP', float, \"\"\"Total person's income >= threshold.\"\"\",\n            is_target=True),\n],\n    documentation=\"https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2014-2018.pdf\")\n\nACS_PUBCOV_FEATURES = FeatureList(features=[\n    DIS_FEATURE,\n    ESP_FEATURE,\n    MIG_FEATURE,\n    ANC_FEATURE,\n    NATIVITY_FEATURE,\n    DEAR_FEATURE,\n    DEYE_FEATURE,\n    DREM_FEATURE,\n    Feature('PINCP', float, \"Total person's income\",\n            name_extended=\"Total person's income in dollars\"),\n    Feature('ESR', cat_dtype, \"\"\"Employment status recode b .N/A (less than \n    16 years old) 1 .Civilian employed, at work 2 .Civilian employed, with a \n    job but not at work 3 .Unemployed 4 .Armed forces, at work 5 .Armed \n    forces, with a job but not at work 6 .Not in labor force\"\"\",\n            name_extended=\"Employment status\",\n            value_mapping={\n                '00': 'N/A (less than 16 years old)',\n                '01': 'Civilian employed, at work',\n                '02': 'Civilian employed, with a job but not at work',\n                '03': 'Unemployed',\n                '04': 'Armed forces, at work',\n                '05': 'Armed forces, with a job but not at work',\n                '06': 'Not in labor force'}),\n    FER_FEATURE,\n    Feature('PUBCOV', int, \"\"\"Public health coverage recode =With public \n    health coverage 0=Without public health coverage\"\"\", is_target=True)],\n    documentation=\"https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2014-2018.pdf\")\n\nACS_UNEMPLOYMENT_FEATURES = FeatureList(features=[\n    Feature('ESR', int, \"Employment status (is unemployed)\", is_target=True),\n    ENG_FEATURE,\n    POBP_FEATURE,\n    RELP_FEATURE,\n    WKHP_FEATURE,\n    Feature('WKW', int, \"Weeks worked during past 12 months\",\n            name_extended=\"Weeks worked during past 12 months\"),\n    Feature('WRK', cat_dtype, \"Worked last week\",\n            name_extended=\"Worked last week\",\n            value_mapping={\n                '00': 'N/A (not reported',\n                '01': 'Worked',\n                '02': 'Did not work'}),\n    OCCP_FEATURE,\n    DIS_FEATURE,\n    ESP_FEATURE,\n    MIG_FEATURE,\n    MIL_FEATURE,\n    ANC_FEATURE,\n    NATIVITY_FEATURE,\n    DEAR_FEATURE,\n    DEYE_FEATURE,\n    DREM_FEATURE,\n    Feature('DPHY', cat_dtype, \"Ambulatory difficulty\",\n            name_extended=\"Ambulatory difficulty\",\n            value_mapping={'00': 'N/A (Less than 5 years old)',\n                           '01': 'Yes',\n                           '02': 'No', }),\n    FER_FEATURE,\n],\n    documentation=\"https://www2.census.gov/programs-surveys/acs/tech_docs\"\n                  \"/pums/data_dict/PUMS_Data_Dictionary_2019.pdf\"\n)\n\nACS_FOODSTAMPS_FEATURES = FeatureList(features=[\n    Feature('FS', int, \"\"\"Yearly food stamp/Supplemental Nutrition Assistance \n    Program (SNAP) recipiency (household) b .N/A (vacant) 5 1 .Yes 2 .No\"\"\",\n            is_target=True),\n    ENG_FEATURE,\n    FER_FEATURE,\n    Feature('HUPAC', int, \"Household presence and age of children\",\n            name_extended=\"Household presence and age of children\",\n            value_mapping={\n                '00': 'N/A (GQ/vacant)',\n                '01': 'With children under 6 years only',\n                '02': 'With children 6 to 17 years only',\n                '03': 'With children under 6 years and 6 to 17 years',\n                '04': 'No children'\n            }),\n    Feature('WIF', int, \"Workers in family during the past 12 months\",\n            name_extended=\"Workers in family during the past 12 months\",\n            value_mapping={\n                0: 'No workers',\n                1: '1 worker',\n                2: '2 workers',\n                3: '3 or more workers'}),\n    NWLA_FEATURE,\n    NWLK_FEATURE,\n    OCCP_FEATURE,\n    POBP_FEATURE,\n    RELP_FEATURE,\n    WKHP_FEATURE,\n    Feature('WKW', int,\n            \"Weeks worked during past 12 months.\",\n            name_extended=\"Weeks worked during past 12 months\",\n            ),\n    Feature('WRK', cat_dtype, \"Worked last week\",\n            name_extended=\"worked last week\",\n            value_mapping={'01': 'worked', '02': 'did not work'}),\n    DIS_FEATURE,\n    MIL_FEATURE,\n    ANC_FEATURE,\n    NATIVITY_FEATURE,\n    DEAR_FEATURE,\n    DEYE_FEATURE,\n    DREM_FEATURE,\n    Feature('PUBCOV', cat_dtype, \"Public health coverage recode\",\n            value_mapping={\n                '00': 'Without public health coverage',\n                '01': 'With public health coverage'})\n],\n    documentation=\"https://www2.census.gov/programs-surveys/acs/tech_docs\"\n                  \"/pums/data_dict/PUMS_Data_Dictionary_2019.pdf\"\n)\n\n\n################################################################################\n# Preprocessing functions\n################################################################################\n\n\ndef map_categorical_features(df, feature_mapping):\n    \"\"\"Convert a subset of features from numeric to categorical format.\n\n    Note that this only maps a known set of features used in this work;\n    there are likely many additional categorical features treated as numeric\n    that could be returned by folktables!\n    \"\"\"\n    for feature, mapping in feature_mapping.items():\n        if feature in df.columns:\n            assert pd.isnull(\n                df[feature]).values.sum() == 0, \"nan values in input\"\n\n            mapped_setdiff = set(df[feature].unique().tolist()) - set(\n                list(mapping.keys()))\n            assert not mapped_setdiff, \"missing keys {} from mapping {}\".format(\n                list(mapped_setdiff), list(mapping.keys()))\n            for x in df[feature].unique():\n                try:\n                    assert x in list(mapping.keys())\n                except AssertionError:\n                    raise ValueError(f\"features {feature} value {x} not in \"\n                                     f\"mapping keys {list(mapping.keys())}\")\n            if feature in df.columns:\n                df[feature] = pd.Categorical(\n                    df[feature].map(mapping),\n                    categories=list(set(mapping.values())))\n            assert pd.isnull(df[feature]).values.sum() == 0, \\\n                \"nan values in output; check for non-mapped input values.\"\n    return df\n\n\ndef acs_data_to_df(\n        features: np.ndarray, label: np.ndarray,\n        feature_list: FeatureList,\n        feature_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Build a DataFrame from the result of folktables.BasicProblem.df_to_numpy().\n    \"\"\"\n    ary = np.concatenate((features, label.reshape(-1, 1),), axis=1)\n    df = pd.DataFrame(ary,\n                      columns=feature_list.predictors + [feature_list.target])\n    df = map_categorical_features(df, feature_mapping=feature_mapping)\n    return df\n\n\ndef default_acs_group_transform(x):\n    # 'White alone' vs. all other categories (RAC1P) or\n    # 'Male' vs. Female (SEX)\n    # Note that *privileged* group is coded as 1, by default.\n    return x == 1\n\n\ndef default_acs_postprocess(x):\n    return np.nan_to_num(x, -1)\n\n\ndef income_cls_target_transform(y, threshold):\n    \"\"\"Binarization target transform for income.\"\"\"\n    return y > threshold\n\n\ndef pubcov_target_transform(y, threshold):\n    \"\"\"Default Public Coverage target transform from folktables.\"\"\"\n    del threshold\n    return y == 1\n\n\ndef unemployment_target_transform(y, threshold):\n    \"\"\"Default Public Coverage target transform from folktables.\"\"\"\n    del threshold\n    return y == 3\n\n\ndef unemployment_filter(data):\n    \"\"\"\n    Filters for the unemployment; focus on Americans of working age not eligible for Social Security.\n    \"\"\"\n    df = data\n    df = df[(df['AGEP'] < 62) & (df['AGEP'] >= 18)]\n    return df\n\n\ndef foodstamps_target_transform(y, threshold):\n    del threshold\n    return y == 1\n\n\ndef foodstamps_filter(data):\n    \"\"\"Filter for food stamp recipiency task; focus on low income Americans (\n    as in public coverage task) of working age (since these would be\n    individuals actually pplying for this benefit, as opposed to children\n    living in a household that receives food stamps). \"\"\"\n    df = data\n    df = df[df['HUPAC'] >= 1]  # at least one child in household\n    df = df[(df['AGEP'] < 62) & (df['AGEP'] >= 18)]\n    return df[df['PINCP'] <= 30000]\n\n\n@dataclass\nclass ACSTaskConfig:\n    \"\"\"A class to configure data loading/preprocessing for an ACS task.\"\"\"\n    features_to_use: FeatureList\n    group_transform: Callable\n    postprocess: Callable\n    preprocess: Callable\n    target: str\n    target_transform: Callable\n    threshold: Union[int, float]\n\n\nACS_TASK_CONFIGS = frozendict.frozendict({\n    'income': ACSTaskConfig(**{\n        'features_to_use': ACS_INCOME_FEATURES + ACS_SHARED_FEATURES,\n        'group_transform': default_acs_group_transform,\n        'postprocess': default_acs_postprocess,\n        'preprocess': folktables.acs.adult_filter,\n        'target': 'PINCP',\n        'target_transform': income_cls_target_transform,\n        'threshold': 56000,\n    }),\n    'pubcov': ACSTaskConfig(**{\n        'features_to_use': ACS_PUBCOV_FEATURES + ACS_SHARED_FEATURES,\n        'group_transform': default_acs_group_transform,\n        'postprocess': default_acs_postprocess,\n        'preprocess': folktables.acs.public_coverage_filter,\n        'target': 'PUBCOV',\n        'target_transform': pubcov_target_transform,\n        'threshold': None,\n    }),\n    'unemployment': ACSTaskConfig(**{\n        'features_to_use': ACS_UNEMPLOYMENT_FEATURES + ACS_SHARED_FEATURES,\n        'group_transform': default_acs_group_transform,\n        'postprocess': default_acs_postprocess,\n        'preprocess': unemployment_filter,\n        'target': 'ESR',\n        'target_transform': unemployment_target_transform,\n        'threshold': None,\n    }),\n    'foodstamps': ACSTaskConfig(**{\n        'features_to_use': ACS_FOODSTAMPS_FEATURES + ACS_SHARED_FEATURES,\n        'group_transform': default_acs_group_transform,\n        'postprocess': default_acs_postprocess,\n        'preprocess': foodstamps_filter,\n        'target': 'FS',\n        'target_transform': foodstamps_target_transform,\n        'threshold': None,\n    })\n})\n\n\ndef get_acs_data_source(year, root_dir='datasets/acs'):\n    return folktables.ACSDataSource(survey_year=str(year),\n                                    horizon='1-Year',\n                                    survey='person',\n                                    root_dir=root_dir)\n\n\ndef preprocess_acs(df: pd.DataFrame):\n    if 'ST' in df.columns:\n        # Map numeric state codes to human-readable values\n        df['ST'] = df['ST'].map(ST_CODE_TO_STATE)\n        assert pd.isnull(df['ST']).sum() == 0\n    return df\n"}
{"type": "source_file", "path": "tableshift/configs/benchmark_configs.py", "content": "\"\"\"\nExperiment configs for the 'official' TableShift benchmark tasks.\n\nAll other configs are in non_benchmark_configs.py.\n\"\"\"\n\nfrom tableshift.configs.experiment_config import ExperimentConfig\nfrom tableshift.configs.experiment_defaults import DEFAULT_ID_TEST_SIZE, \\\n    DEFAULT_OOD_VAL_SIZE, DEFAULT_ID_VAL_SIZE, DEFAULT_RANDOM_STATE\nfrom tableshift.core import Grouper, PreprocessorConfig, DomainSplitter\nfrom tableshift.datasets import BRFSS_YEARS, ACS_YEARS, NHANES_YEARS\nfrom tableshift.datasets.mimic_extract import MIMIC_EXTRACT_STATIC_FEATURES\nfrom tableshift.datasets.mimic_extract_feature_lists import \\\n    MIMIC_EXTRACT_SHARED_FEATURES\n\n# We passthrough all non-static columns because we use\n# MIMIC-extract's default preprocessing/imputation and do not\n# wish to modify it for these features\n# (static features are not preprocessed by MIMIC-extract). See\n# tableshift.datasets.mimic_extract.preprocess_mimic_extract().\n_MIMIC_EXTRACT_PASSTHROUGH_COLUMNS = [\n    f for f in MIMIC_EXTRACT_SHARED_FEATURES.names\n    if f not in MIMIC_EXTRACT_STATIC_FEATURES.names]\n\nBENCHMARK_CONFIGS = {\n    \"acsfoodstamps\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"DIVISION\",\n                                domain_split_ood_values=['06']),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"acs_task\": \"acsfoodstamps\"}),\n\n    \"acsincome\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"DIVISION\",\n                                domain_split_ood_values=['01']),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"acs_task\": \"acsincome\"}),\n\n    \"acspubcov\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"DIS\",\n                                domain_split_ood_values=['1.0']),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"acs_task\": \"acspubcov\", \"name\": \"acspubcov\",\n                                \"years\": ACS_YEARS}),\n\n    \"acsunemployment\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='SCHL',\n                                # No high school diploma vs. GED/diploma or higher.\n                                domain_split_ood_values=['01', '02', '03', '04',\n                                                         '05', '06', '07', '08',\n                                                         '09', '10', '11', '12',\n                                                         '13', '14', '15']),\n        grouper=Grouper({\"RAC1P\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"acs_task\": \"acsunemployment\"}),\n\n    # ANES, Split by region; OOD is south: (AL, AR, DE, D.C., FL, GA, KY, LA,\n    # MD, MS, NC, OK, SC,TN, TX, VA, WV)\n    \"anes\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='VCF0112',\n                                domain_split_ood_values=['3.0']),\n        # male vs. all others; white non-hispanic vs. others\n        grouper=Grouper({\"VCF0104\": [\"1\", ], \"VCF0105a\": [\"1.0\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None),\n        tabular_dataset_kwargs={}),\n\n    \"brfss_blood_pressure\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"BMI5CAT\",\n                                # OOD values: [1 underweight, 2 normal weight], [3 overweight, 4 obese]\n                                domain_split_ood_values=['3.0', '4.0']),\n        grouper=Grouper({\"PRACE1\": [1, ], \"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(passthrough_columns=[\"IYEAR\"]),\n        tabular_dataset_kwargs={\"name\": \"brfss_blood_pressure\",\n                                \"task\": \"blood_pressure\",\n                                \"years\": BRFSS_YEARS},\n    ),\n\n    # \"White nonhispanic\" (in-domain) vs. all other race/ethnicity codes (OOD)\n    \"brfss_diabetes\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"PRACE1\",\n                                domain_split_ood_values=[2, 3, 4, 5, 6],\n                                domain_split_id_values=[1, ]),\n        grouper=Grouper({\"SEX\": [1, ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(passthrough_columns=[\"IYEAR\"]),\n        tabular_dataset_kwargs={\"name\": \"brfss_diabetes\",\n                                \"task\": \"diabetes\", \"years\": BRFSS_YEARS},\n    ),\n\n    \"diabetes_readmission\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='admission_source_id',\n                                domain_split_ood_values=[7, ]),\n        # male vs. all others; white non-hispanic vs. others\n        grouper=Grouper({\"race\": [\"Caucasian\", ], \"gender\": [\"Male\", ]},\n                        drop=False),\n        # Note: using min_frequency=0.01 reduces data\n        # dimensionality from ~2400 -> 169 columns.\n        # This is due to high cardinality of 'diag_*' features.\n        preprocessor_config=PreprocessorConfig(min_frequency=0.01),\n        tabular_dataset_kwargs={}),\n\n    \"heloc\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='ExternalRiskEstimateLow',\n                                domain_split_ood_values=[0]),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"name\": \"heloc\"},\n    ),\n\n    \"mimic_extract_los_3\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"insurance\",\n                                domain_split_ood_values=[\"Medicare\"]),\n\n        grouper=Grouper({\"gender\": ['M'], }, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=_MIMIC_EXTRACT_PASSTHROUGH_COLUMNS),\n        tabular_dataset_kwargs={\"task\": \"los_3\",\n                                \"name\": \"mimic_extract_los_3\"}),\n\n    \"mimic_extract_mort_hosp\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"insurance\",\n                                domain_split_ood_values=[\"Medicare\",\n                                                         \"Medicaid\"]),\n        grouper=Grouper({\"gender\": ['M'], }, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=_MIMIC_EXTRACT_PASSTHROUGH_COLUMNS),\n        tabular_dataset_kwargs={\"task\": \"mort_hosp\",\n                                \"name\": \"mimic_extract_mort_hosp\"}),\n\n    \"nhanes_cholesterol\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='RIDRETH_merged',\n                                domain_split_ood_values=[1, 2, 4, 6, 7],\n                                domain_split_id_values=[3],\n                                ),\n        # Group by male vs. all others\n        grouper=Grouper({\"RIAGENDR\": [\"1.0\", ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"nhanes_year\"],\n            numeric_features=\"kbins\"),\n        tabular_dataset_kwargs={\"nhanes_task\": \"cholesterol\",\n                                \"years\": NHANES_YEARS}),\n\n    \"assistments\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='school_id',\n                                domain_split_ood_values=[5040.0,\n                                                         11502.0,\n                                                         11318.0,\n                                                         11976.0,\n                                                         12421.0,\n                                                         12379.0,\n                                                         11791.0,\n                                                         8359.0,\n                                                         12406.0,\n                                                         7594.0]),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"skill_id\", \"bottom_hint\", \"first_action\"],\n        ),\n        tabular_dataset_kwargs={},\n    ),\n\n    \"college_scorecard\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='CCBASIC',\n                                domain_split_ood_values=[\n                                    'Special Focus Institutions--Other special-focus institutions',\n                                    'Special Focus Institutions--Theological seminaries, Bible colleges, and other faith-related institutions',\n                                    \"Associate's--Private For-profit 4-year Primarily Associate's\",\n                                    'Baccalaureate Colleges--Diverse Fields',\n                                    'Special Focus Institutions--Schools of art, music, and design',\n                                    \"Associate's--Private Not-for-profit\",\n                                    \"Baccalaureate/Associate's Colleges\",\n                                    \"Master's Colleges and Universities (larger programs)\"]\n                                ),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            # Several categorical features in college scorecard have > 10k\n            # unique values; so we label-encode instead of one-hot encoding.\n            categorical_features=\"label_encode\",\n            # Some important numeric features are not reported by universities\n            # in a way that could be systematic (and we would like these included\n            # in the sample, not excluded), so we use kbins\n            numeric_features=\"kbins\",\n            n_bins=100,\n            dropna=None,\n        ),\n        tabular_dataset_kwargs={},\n    ),\n\n    \"nhanes_lead\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='INDFMPIRBelowCutoff',\n                                domain_split_ood_values=[1.]),\n        # Race (non. hispanic white vs. all others; male vs. all others)\n        grouper=Grouper({\"RIDRETH_merged\": [3, ], \"RIAGENDR\": [\"1.0\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=[\"nhanes_year\"],\n            numeric_features=\"kbins\"),\n        tabular_dataset_kwargs={\"nhanes_task\": \"lead\", \"years\": NHANES_YEARS}),\n\n    # LOS >= 47 is roughly the 80th %ile of data.\n    \"physionet\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname='ICULOS',\n                                domain_split_gt_thresh=47.0),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(numeric_features=\"kbins\",\n                                               dropna=None),\n        tabular_dataset_kwargs={\"name\": \"physionet\"}),\n}\n"}
{"type": "source_file", "path": "tableshift/__init__.py", "content": "from .core import get_dataset, get_iid_dataset\n"}
{"type": "source_file", "path": "tableshift/core/utils.py", "content": "import collections\nimport datetime\nimport logging\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nfrom itertools import islice\n\nimport pandas as pd\nimport requests\nimport xport.v56\n\nfrom .splitter import Splitter, DomainSplitter\n\n\ndef initialize_dir(dir: str):\n    \"\"\"Create a directory if it does not exist.\"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n\ndef basename_from_url(url: str) -> str:\n    parsed_url = urllib.parse.urlparse(url)\n    fname = os.path.basename(parsed_url.path)\n    return fname\n\n\ndef download_file(url: str, dirpath: str, if_not_exist=True,\n                  dest_file_name=None):\n    \"\"\"Download file to the specified directory.\"\"\"\n    if dest_file_name:\n        fname = dest_file_name\n    else:\n        fname = basename_from_url(url)\n    fpath = os.path.join(dirpath, fname)\n\n    if os.path.exists(fpath) and if_not_exist:\n        # Case: file already exists; skip it.\n        logging.debug(f\"not downloading {url}; exists at {fpath}\")\n\n    else:\n        initialize_dir(dirpath)\n        logging.debug(f\"downloading {url} to {fpath}\")\n        with open(fpath, \"wb\") as f:\n            f.write(requests.get(url).content)\n\n    return fpath\n\n\ndef read_xpt(fp) -> pd.DataFrame:\n    assert os.path.exists(fp), \"file does not exist %s\" % fp\n    with open(fp, \"rb\") as f:\n        obj = xport.v56.load(f)\n    # index into SAS structure, assuming there is only one dataframe\n    assert len(tuple(obj._members.keys())) == 1\n    key = tuple(obj._members.keys())[0]\n    ds = obj[key]\n    # convert xport.Dataset to pd.DataFrame\n    columns = [ds[c] for c in ds.columns]\n    df = pd.DataFrame(columns).T\n    del ds\n    return df\n\n\ndef run_in_subprocess(cmd):\n    logging.info(f\"running {cmd}\")\n    res = subprocess.run(cmd, shell=True)\n    logging.debug(f\"{cmd} returned {res}\")\n    return res\n\n\ndef sliding_window(iterable, n):\n    \"\"\" From https://docs.python.org/3/library/itertools.html\"\"\"\n    # sliding_window('ABCDEFG', 4) --> ABCD BCDE CDEF DEFG\n    it = iter(iterable)\n    window = collections.deque(islice(it, n), maxlen=n)\n    if len(window) == n:\n        yield tuple(window)\n    for x in it:\n        window.append(x)\n        yield tuple(window)\n\n\ndef make_uid(name: str, splitter: Splitter, replace_chars=\"*/:'$!\") -> str:\n    \"\"\"Make a unique identifier for an experiment.\"\"\"\n    uid = name\n    if isinstance(splitter, DomainSplitter) and splitter.is_explicit_split():\n        attrs = {'domain_split_varname': splitter.domain_split_varname,\n                 'domain_split_ood_value': ''.join(\n                     str(x) for x in splitter.domain_split_ood_values)}\n        if splitter.domain_split_id_values:\n            attrs['domain_split_id_values'] = ''.join(\n                str(x) for x in splitter.domain_split_id_values)\n        uid += ''.join(f'{k}_{v}' for k, v in attrs.items())\n    elif isinstance(splitter, DomainSplitter) and splitter.is_threshold_split():\n        uid += f'{splitter.domain_split_varname}gt{splitter.domain_split_gt_thresh}'\n\n    # if any slashes exist, replace with periods.\n    for char in replace_chars:\n        uid = uid.replace(char, '.')\n    # Max path length on some OSs is 255.\n    return uid[:240]\n\n\ndef timestamp_as_int() -> int:\n    \"\"\"Helper function to get the current timestamp as int.\"\"\"\n    dt = datetime.datetime.now()\n    return int(dt.strftime(\"%Y%m%d%H%M%S\"))\n\n\nILLEGAL_CHARS_REGEX = '[\\\\[\\\\]{}.:<>/,\"]'\n\n\ndef contains_illegal_chars(s: str) -> bool:\n    if re.search(ILLEGAL_CHARS_REGEX, s):\n        return True\n    else:\n        return False\n\n\ndef sub_illegal_chars(s: str) -> str:\n    return re.sub(ILLEGAL_CHARS_REGEX, \"\", s)\n\n\ndef convert_64bit_numeric_cols(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convert float64 or int64 columns to float/int32.\n\n    Not all numpy dtypes are compatible with 64-bit precision.\"\"\"\n    int64_float64_cols = list(df.select_dtypes(include=['float64', 'int64']))\n    df[int64_float64_cols] = df[int64_float64_cols].astype('float32')\n    return df\n"}
{"type": "source_file", "path": "tableshift/configs/experiment_config.py", "content": "from dataclasses import dataclass\nfrom typing import Union, Dict, Any\n\nfrom tableshift.core import Grouper, PreprocessorConfig, Splitter\n\n\n@dataclass\nclass ExperimentConfig:\n    splitter: Splitter\n    grouper: Union[Grouper, None]\n    preprocessor_config: PreprocessorConfig\n    tabular_dataset_kwargs: Dict[str, Any]\n"}
{"type": "source_file", "path": "tableshift/datasets/assistments.py", "content": "\"\"\"\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nimport numpy as np\nfrom pandas import DataFrame\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nASSISTMENTS_FEATURES = FeatureList(features=[\n    Feature('Average_confidence(BORED)', float),\n    Feature('Average_confidence(CONCENTRATING)', float),\n    Feature('Average_confidence(CONFUSED)', float),\n    Feature('Average_confidence(FRUSTRATED)', float),\n    Feature('attempt_count', int),\n    Feature('hint_count', int),\n    Feature('school_id', float),\n    Feature('skill_id', float,\n            value_mapping={\n                54.0: \"Rounding\",\n                279.0: \"Multiplication and Division Integers\",\n                79.0: \"Proportion\",\n                86.0: \"Exponents\",\n                311.0: \"Equation Solving Two or Fewer Steps\",\n                11.0: \"Venn Diagram\",\n                280.0: \"Addition and Subtraction Fractions\",\n                338.0: \"Combining Like Terms\",\n                81.0: \"Unit Rate\",\n                325.0: \"Write Linear Equation from Graph\",\n                322.0: \"Write Linear Equation from Ordered Pairs\",\n                13.0: \"Median\",\n                312.0: \"Equation Solving More Than Two Steps\",\n                75.0: \"Square Root\",\n                336.0: \"Finding y-intercept from Linear Equation\",\n                315.0: \"Parallel and Perpendicular Lines\",\n                340.0: \"Distributive Property\",\n                70.0: \"Percent Of\",\n                67.0: \"Multiplication Fractions\",\n                277.0: \"Addition and Subtraction Integers\",\n                24.0: \"Congruence\",\n                363.0: \"Multiplying non Monomial Polynomials\",\n                309.0:      \"Order of Operations +,-,/,* () positive reals\",\n                324.0: \"Recognize Linear Pattern\",\n                333.0: \"Finding Slope From Equation\",\n                25.0: \"Complementary and Supplementary Angles\",\n                378.0: \"Solving Systems of Linear Equations by Graphing\",\n                49.0: \"Ordering Positive Decimals\",\n                110.0: \"D.4.8-understanding-concept-of-probabilities\",\n                42.0: \"Perimeter of a Polygon\",\n                368.0: \"Solving for a variable\",\n                12.0: \"Mean\",\n                103.0: \"Point Plotting\",\n                34.0: \"Unit Conversion Within a System\",\n                47.0: \"Conversion of Fraction Decimals Percents\",\n                61.0: \"Division Fractions\",\n                296.0: \"Area Rectangle\",\n                82.0: \"Scientific Notation\",\n                15.0: \"Range\",\n                51.0: \"Ordering Integers\",\n                27.0: \"Pythagorean Theorem\",\n                307.0: \"Volume Rectangular Prism\",\n                65.0: \"Least Common Multiple\",\n                323.0: \"Write Linear Equation from Situation\",\n                326.0: \"Write Linear Equation from Slope and y-intercept\",\n                46.0: \"Calculations with Similar Figures\",\n                278.0: \"Addition and Subtraction Positive Decimals\",\n                299.0: \"Surface Area Cylinder\",\n                8.0: \"Scatter Plot\",\n                77.0: \"Finding Percents\",\n                163.0: \"Absolute Value\",\n                523.0: \"Picking Expressions From Choices\",\n                310.0: \"Order of Operations All\",\n                80.0: \"Scale Factor\",\n                297.0: \"Area Trapezoid\",\n                39.0: \"Area Circle\",\n                301.0: \"Surface Area Rectangular Prism\",\n                302.0: \"Surface Area Sphere\",\n                53.0: \"Ordering Real Numbers\",\n                166.0: \"Algebraic Solving\",\n                21.0: \"Interior Angles Figures with More than 3 Sides\",\n                92.0: \"Pattern Finding \",\n                95.0: \"Substitution\",\n                17.0: \"Probability of Two Distinct Events\",\n                288.0: \"Properties and Classification Rectangular Prisms\",\n                334.0: \"Finding Slope from Ordered Pairs\",\n                282.0: \"Properties and Classification Polygons with 5 or more sides\",\n                281.0: \"Properties and Classification Triangles\",\n                371.0: \"Simplifying Expressions positive exponents\",\n                84.0: \"Prime Number\",\n                580.0: \"Exponents\",\n                48.0: \"Equivalent Fractions\",\n                314.0: \"Angles - Obtuse, Acute, and Right\",\n                1.0: \"Box and Whisker\",\n                362.0: \"Parts of a Polyomial, Terms, Coefficient, Monomial, Exponent, Variable\",\n                350.0: \"Solving Systems of Linear Equations\",\n                4.0: \"Histogram as Table or Graph\",\n                306.0: \"Volume Pyramid\",\n                298.0: \"Area Triangle\",\n                83.0: \"Divisibility Rules\",\n                294.0: \"Area Irregular Figure\",\n                588.0: \"Equivalent Fractions\",\n                577.0: \"Nets of 3D Objects\",\n                331.0: \"Finding Slope From Situation\",\n                569.0: \"Surface Area Rectangular Prism\",\n                22.0: \"Interior Angles Triangle\",\n                36.0: \"Unit Conversion Standard to Metric\",\n                346.0: \"Polynomial Factors\",\n                317.0: \"Greatest Common Factor\",\n                332.0: \"Finding Slope from Graph\",\n                90.0: \"Picking Equation and Inequality from Choices\",\n                303.0: \"Volume Cylinder\",\n                173.0: \"Choose an Equation from Given Information\",\n                308.0: \"Volume Sphere\",\n                217.0: \"Rate\",\n                356.0: \"Quadratic Formula to Solve Quadratic Equation\",\n                40.0: \"Circumference \",\n                283.0: \"Properties and Classification Quadrilaterals\",\n                343.0: \"Midpoint\",\n                295.0: \"Area Parallelogram\",\n                392.0: \"Surface Area of Prism\",\n                50.0: \"Ordering Fractions\",\n                41.0: \"Definition Pi\",\n                5.0: \"Number Line\",\n                287.0: \"Properties and Classification Prism\",\n                18.0: \"Probability of a Single Event\",\n                276.0: \"Multiplication and Division Positive Decimals\",\n                63.0: \"Estimation\",\n                292.0: \"Rotations\",\n                26.0: \"Angles on Parallel Lines Cut by a Transversal\",\n                572.0: \"Circumference\",\n                575.0: \"Concept Volume\",\n                58.0: \"Addition Whole Numbers\",\n                85.0: \"Absolute Value\",\n                186.0: \"Histogram as Table or Graph\",\n                358.0: \"Making a Table from an Equation\",\n                284.0: \"Properties and Classification Circle\",\n                584.0: \"Multiplication Proper Fractions\",\n                574.0: \"Subtraction Mixed Fractions\",\n                576.0: \"Area Rectangle\",\n                366.0: \"Composition of Function Adding\",\n                305.0: \"Volume Cone\",\n                354.0: \"Factoring Polynomials Standard\",\n                304.0: \"Volume Prism\",\n                319.0: \"Prime Factor\",\n                591.0: \"Subtraction Proper Fractions\",\n                582.0: \"Multiplication Positive Decimals\",\n                587.0: \"Least Common Multiple\",\n                16.0: \"Counting Methods\",\n                32.0: \"Nets of 3D Figures\",\n                88.0: \"Inverse Relations\",\n                365.0: \"Interpreting Coordinate Graphs \",\n                106.0: \"Graph Shape\",\n                33.0: \"English and Metric Terminology\",\n                375.0: \"Solving Inequalities\",\n                1641.0: \"Finding Ratios\",\n                62.0: \"Division Whole Numbers\",\n                7.0: \"Sampling Techniques\",\n                290.0: \"Reflection\",\n                203.0: \"Percent Discount\",\n                339.0: \"Bar Graph\",\n                240.0: \"X-Y Graph Reading\",\n                69.0: \"Multiplication Whole Numbers\",\n                222.0: \"Solving Inequalities\",\n                316.0: \"Expanded, Standard and Word Notation\",\n                74.0: \"Subtraction Whole Numbers\",\n                321.0: \"Computation with Real Numbers\",\n                78.0: \"Percent Increase or Decrease\",\n                391.0: \"Graphing Linear Equations\",\n                230.0: \"Surface Area of 3D Objects\",\n                327.0: \"Comparing and Identifying Slope/Rate of Change\",\n                293.0: \"Translations\",\n                10.0: \"Table\",\n                300.0: \"Surface Area Pyramid\",\n                9.0: \"Stem and Leaf Plot\",\n                613.0: \"Addition Mixed Fractions\",\n                360.0: \"Writine Expression from Diagrams\",\n                387.0: \"Perimeter of an Irregular Figure\",\n                14.0: \"Mode\",\n                64.0: \"Fraction Of\",\n                202.0: \"Pattern Finding\",\n                318.0: \"Multiplication Division by Powers of 10\",\n                344.0: \"Distance Formula\",\n                231.0: \"Symbolization\",\n                223.0: \"Solving System of Equation\",\n                226.0: \"Substitution\",\n                204.0: \"Percents\",\n                6.0: \"Line Plot\",\n                214.0: \"Quadratic Equation Solving\",\n                585.0: \"Division Proper Fractions\",\n                578.0: \"Area Parallelogram\",\n                181.0: \"Exponents\",\n                581.0: \"Circle Concept\",\n                570.0: \"Area Trapezoid\",\n                233.0: \"Transformation\",\n                571.0: \"Area Circle\",\n                583.0: \"Multiplication Mixed Fractions\",\n                579.0: \"Area Triangle\",\n                589.0: \"Addition Proper Fractions\",\n                393.0: \"Associative Property\",\n                198.0: \"Median\",\n                370.0: \"Recognizing Equivalent Expressions\",\n                376.0: \"Graphing Inequalities on a number line\",\n                388.0: \"Common Multiple\",\n                595.0: \"Area Circle\",\n                599.0: \"Concept Volume\",\n                52.0: \"Ordering Whole Numbers\",\n                601.0: \"Area Parallelogram\",\n                596.0: \"Circumference\",\n                359.0: \"Commutative Property\",\n                172.0: \"Calculation with + - * /\",\n                2.0: \"Circle Graph\",\n                224.0: \"Square Roots\",\n                586.0: \"Greatest Common Factor\",\n                35.0: \"Effect of Changing Dimensions of a Shape Prportionally\",\n                568.0: \"Volume Rectangular Prism\",\n                573.0: \"Division Mixed Fractions\",\n                192.0: \"Line of Best-Fit\",\n                238.0: \"Volume of 3D Objects\",\n                345.0: \"Properties and Clasification of Pyramid\",\n                605.0: \"Multiplication Positive Decimals\",\n                313.0: \"Tree Diagrams, Lists for Counting\",\n                597.0: \"Division Mixed Fractions\",\n                216.0: \"Range\",\n                386.0: \"Odd and Even Number\",\n                213.0: \"Pythagorean Theorem\",\n                206.0: \"Factoring Trinomials\",\n                289.0: \"Line Symmetry\",\n                165.0: \"Algebraic Simplification\",\n                610.0: \"Least Common Multiple\",\n                390.0: \"Graph Shape\",\n                389.0: \"Common Factor\",\n                212.0: \"Proportion\",\n                593.0: \"Surface Area Rectangular Prism\",\n                609.0: \"Greatest Common Factor\",\n                606.0: \"Multiplication Mixed Fractions\",\n                598.0: \"Subtraction Mixed Fractions\",\n                385.0: \"Elapsed Time\",\n                590.0: \"Addition Mixed Fractions\",\n                594.0: \"Area Trapezoid\",\n                184.0: \"Geometric Definitions\",\n                218.0: \"Finding Ratios\",\n                197.0: \"Mean-Median-Mode-Range Differentiation\",\n                600.0: \"Area Rectangle\",\n                196.0: \"Mean\",\n                361.0: \"Recognizing Expressions or Equations from Diagrams\",\n                603.0: \"Exponents\",\n                614.0: \"Subtraction Proper Fractions\",\n                608.0: \"Division Proper Fractions\",\n                602.0: \"Area Triangle\",\n                220.0: \"Similar Figures\",\n                355.0: \"Solve Quadratic Equations Using Factoring\",\n                592.0: \"Volume Rectangular Prism\",\n                43.0: \"Reading a Ruler or Scale\",\n                320.0: \"Equal As Balance Concept\",\n                209.0: \"Properties of Numbers\",\n                607.0: \"Multiplication Proper Fractions\",\n                174.0: \"Circle Graph\",\n                604.0: \"Circle Concept\",\n                348.0: \"Recognize Quadratic Pattern\",\n                612.0: \"Addition Proper Fractions\",\n                178.0: \"Combinatorics\",\n                611.0: \"Equivalent Fractions\",\n                291.0: \"Rotational Symmetry\",\n                177.0: \"Co-ordinate Points\",\n            }),\n    Feature('problem_type', cat_dtype),\n    Feature('bottom_hint', float,\n            value_mapping={0.: 'False', 1: 'True'}),\n    Feature('ms_first_response', int),\n    Feature('tutor_mode', cat_dtype),\n    Feature('position', int),\n    Feature('type', cat_dtype),\n    Feature('overlap_time', int),\n    Feature('first_action', int,\n            value_mapping={\n                0: 'Answer',\n                1: 'Hint',\n                2: 'Scaffold'\n            }),\n    # There are other text features in this dataset that would be useful for\n    # text-only models; i.e. answer_text\n    Feature('correct', int, is_target=True)\n],\n    documentation=\"https://www.kaggle.com/datasets/nicolaswattiez/skillbuilder-data-2009-2010 ,\"\n                  \"https://sites.google.com/site/assistmentsdata/datasets/2012-13-school-data-with-affect\")\n\n\ndef preprocess_assistments(df: DataFrame) -> DataFrame:\n    # keep only binary correct/incorrect\n    df = df[np.isin(df.correct.values, (0, 1))]\n    return df\n\n\nSCHOOL_IDS = [1.0, 73.0, 76.0, 139.0, 397.0, 1357.0, 1404.0, 1411.0, 1645.0,\n              1689.0, 1862.0, 1998.0, 2268.0, 2770.0, 4720.0, 4724.0, 5006.0,\n              5018.0, 5040.0, 5092.0, 5126.0, 5159.0, 5459.0, 5479.0, 5555.0,\n              5843.0, 5872.0, 5887.0, 5897.0, 5922.0, 6090.0, 6144.0, 6257.0,\n              6987.0, 7288.0, 7301.0, 7335.0, 7495.0, 7572.0, 7591.0, 7603.0,\n              7703.0, 7740.0, 7760.0, 7796.0, 7804.0, 7806.0, 7836.0, 7839.0,\n              7884.0, 7905.0, 8483.0, 8653.0, 8784.0, 8889.0, 8905.0, 8936.0,\n              9180.0, 9208.0, 9214.0, 9270.0, 9362.0, 9409.0, 9481.0, 9495.0,\n              9537.0, 9587.0, 9627.0, 9713.0, 9793.0, 9941.0, 9948.0, 10134.0,\n              10152.0, 10725.0, 11154.0, 11195.0, 11230.0, 11234.0, 11247.0,\n              11249.0, 11252.0, 11274.0, 11281.0, 11296.0, 11313.0, 11334.0,\n              11338.0, 11357.0, 11378.0, 11390.0, 11404.0, 11446.0, 11475.0,\n              11482.0, 11484.0, 11513.0, 11539.0, 11553.0, 11562.0, 11576.0,\n              11594.0, 11720.0, 11772.0, 11791.0, 11823.0, 11857.0, 11876.0,\n              11889.0, 11894.0, 11904.0, 11906.0, 11915.0, 11918.0, 11924.0,\n              11930.0, 11931.0, 11943.0, 11944.0, 11950.0, 11954.0, 11955.0,\n              11967.0, 11975.0, 11976.0, 11977.0, 11986.0, 11987.0, 11989.0,\n              11996.0, 12011.0, 12012.0, 12021.0, 12026.0, 12038.0, 12056.0,\n              12068.0, 12069.0, 12076.0, 12084.0, 12085.0, 12089.0, 12091.0,\n              12097.0, 12116.0, 12138.0, 12141.0, 12148.0, 12154.0, 12164.0,\n              12175.0, 12197.0, 12200.0, 12205.0, 12208.0, 12215.0, 12217.0,\n              12221.0, 12223.0, 12225.0, 12238.0, 12240.0, 12246.0, 12248.0,\n              12252.0, 12256.0, 12268.0, 12272.0, 12273.0, 12279.0, 12334.0,\n              12364.0, 12367.0, 12388.0, 12406.0, 12408.0, 12412.0, 12419.0,\n              np.nan, 4732.0, 4780.0, 4812.0, 4829.0, 4838.0, 4986.0, 5046.0,\n              5048.0, 5049.0, 5056.0, 5062.0, 5068.0, 5098.0, 5104.0, 5106.0,\n              5109.0, 5116.0, 5117.0, 5125.0, 5177.0, 5197.0, 5255.0, 5260.0,\n              5307.0, 5308.0, 5309.0, 5366.0, 5399.0, 5405.0, 5406.0, 5441.0,\n              5444.0, 5445.0, 5446.0, 5449.0, 5450.0, 5451.0, 5497.0, 5536.0,\n              5545.0, 5550.0, 5558.0, 5559.0, 5561.0, 5689.0, 5692.0, 5721.0,\n              5750.0, 5751.0, 5754.0, 5757.0, 5758.0, 5759.0, 5775.0, 5790.0,\n              5797.0, 5809.0, 5874.0, 5905.0, 5906.0, 5909.0, 5910.0, 5913.0,\n              5917.0, 5978.0, 5994.0, 6004.0, 6012.0, 6023.0, 6042.0, 6063.0,\n              6089.0, 6123.0, 6168.0, 6177.0, 6197.0, 6205.0, 6232.0, 6246.0,\n              6260.0, 6351.0, 6380.0, 6390.0, 6443.0, 6504.0, 6546.0, 6550.0,\n              6553.0, 6842.0, 6905.0, 6963.0, 6977.0, 6979.0, 6992.0, 6995.0,\n              7007.0, 7148.0, 7184.0, 7200.0, 7206.0, 7212.0, 7213.0, 7227.0,\n              7265.0, 7304.0, 7313.0, 7340.0, 7349.0, 7359.0, 7367.0, 7389.0,\n              7445.0, 7450.0, 7466.0, 7526.0, 7561.0, 7594.0, 7596.0, 7604.0,\n              7630.0, 7654.0, 7674.0, 7777.0, 7778.0, 7782.0, 7787.0, 7801.0,\n              7840.0, 7856.0, 7857.0, 7900.0, 7929.0, 7930.0, 7992.0, 8003.0,\n              8040.0, 8041.0, 8051.0, 8086.0, 8096.0, 8207.0, 8253.0, 8264.0,\n              8265.0, 8272.0, 8332.0, 8359.0, 8411.0, 8423.0, 8430.0, 8471.0,\n              8478.0, 8534.0, 8554.0, 8574.0, 8646.0, 8687.0, 8744.0, 8747.0,\n              8771.0, 8772.0, 8781.0, 8806.0, 8828.0, 8900.0, 9100.0, 9112.0,\n              9155.0, 9161.0, 9178.0, 9186.0, 9193.0, 9203.0, 9221.0, 9241.0,\n              9252.0, 9253.0, 9313.0, 9314.0, 9343.0, 9354.0, 9376.0, 9377.0,\n              9394.0, 9446.0, 9450.0, 9456.0, 9486.0, 9518.0, 9532.0, 9534.0,\n              9548.0, 9596.0, 9603.0, 9605.0, 9609.0, 9620.0, 9625.0, 9638.0,\n              9660.0, 9667.0, 9668.0, 9684.0, 9703.0, 9727.0, 9735.0, 9775.0,\n              9840.0, 9880.0, 9884.0, 9903.0, 10120.0, 10180.0, 10241.0,\n              10284.0, 10323.0, 10399.0, 10410.0, 10499.0, 10735.0, 10736.0,\n              10766.0, 11073.0, 11079.0, 11142.0, 11169.0, 11171.0, 11178.0,\n              11180.0, 11220.0, 11228.0, 11229.0, 11231.0, 11233.0, 11236.0,\n              11248.0, 11254.0, 11255.0, 11261.0, 11262.0, 11263.0, 11264.0,\n              11266.0, 11267.0, 11277.0, 11280.0, 11283.0, 11294.0, 11297.0,\n              11302.0, 11312.0, 11315.0, 11317.0, 11318.0, 11320.0, 11321.0,\n              11322.0, 11330.0, 11335.0, 11350.0, 11362.0, 11364.0, 11368.0,\n              11371.0, 11372.0, 11384.0, 11385.0, 11393.0, 11394.0, 11397.0,\n              11398.0, 11403.0, 11409.0, 11416.0, 11418.0, 11420.0, 11422.0,\n              11443.0, 11449.0, 11451.0, 11456.0, 11463.0, 11469.0, 11479.0,\n              11481.0, 11486.0, 11502.0, 11505.0, 11509.0, 11514.0, 11518.0,\n              11519.0, 11529.0, 11531.0, 11540.0, 11542.0, 11546.0, 11555.0,\n              11564.0, 11572.0, 11575.0, 11580.0, 11589.0, 11590.0, 11600.0,\n              11604.0, 11609.0, 11633.0, 11679.0, 11696.0, 11742.0, 11746.0,\n              11749.0, 11757.0, 11765.0, 11770.0, 11773.0, 11779.0, 11782.0,\n              11788.0, 11803.0, 11808.0, 11811.0, 11813.0, 11864.0, 11868.0,\n              11875.0, 11878.0, 11883.0, 11884.0, 11885.0, 11887.0, 11888.0,\n              11890.0, 11891.0, 11895.0, 11896.0, 11901.0, 11903.0, 11905.0,\n              11909.0, 11910.0, 11912.0, 11913.0, 11916.0, 11917.0, 11920.0,\n              11925.0, 11929.0, 11933.0, 11935.0, 11936.0, 11938.0, 11939.0,\n              11941.0, 11942.0, 11945.0, 11946.0, 11947.0, 11948.0, 11949.0,\n              11951.0, 11956.0, 11959.0, 11960.0, 11961.0, 11962.0, 11963.0,\n              11964.0, 11966.0, 11971.0, 11973.0, 11983.0, 11991.0, 11992.0,\n              11993.0, 11995.0, 11998.0, 12003.0, 12005.0, 12007.0, 12008.0,\n              12010.0, 12013.0, 12014.0, 12016.0, 12017.0, 12018.0, 12022.0,\n              12024.0, 12029.0, 12033.0, 12041.0, 12044.0, 12055.0, 12057.0,\n              12058.0, 12059.0, 12063.0, 12064.0, 12070.0, 12071.0, 12075.0,\n              12077.0, 12079.0, 12083.0, 12086.0, 12087.0, 12096.0, 12098.0,\n              12099.0, 12103.0, 12104.0, 12108.0, 12112.0, 12114.0, 12118.0,\n              12121.0, 12122.0, 12126.0, 12131.0, 12134.0, 12135.0, 12140.0,\n              12142.0, 12144.0, 12149.0, 12153.0, 12155.0, 12157.0, 12158.0,\n              12159.0, 12161.0, 12162.0, 12163.0, 12169.0, 12174.0, 12176.0,\n              12177.0, 12181.0, 12182.0, 12184.0, 12185.0, 12198.0, 12199.0,\n              12201.0, 12202.0, 12203.0, 12204.0, 12207.0, 12209.0, 12211.0,\n              12218.0, 12219.0, 12230.0, 12232.0, 12244.0, 12245.0, 12253.0,\n              12270.0, 12293.0, 12294.0, 12302.0, 12303.0, 12311.0, 12320.0,\n              12326.0, 12344.0, 12346.0, 12350.0, 12353.0, 12356.0, 12363.0,\n              12373.0, 12379.0, 12383.0, 12384.0, 12391.0, 12403.0, 12416.0,\n              12420.0, 12421.0, 12428.0]\n"}
{"type": "source_file", "path": "tableshift/core/grouper.py", "content": "from dataclasses import dataclass\nimport logging\nfrom typing import Mapping, Sequence, Any, List\n\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\n\n\n@dataclass\nclass Grouper:\n    features_and_values: Mapping[str, Sequence[Any]]\n    drop: bool\n    transformer: ColumnTransformer = None\n\n    @property\n    def features(self) -> List[str]:\n        return list(self.features_and_values.keys())\n\n    def _check_inputs(self, data: pd.DataFrame):\n        \"\"\"Check inputs to the transform function.\"\"\"\n        for c in self.features:\n            assert c in data.columns, \\\n                f\"data does not contain grouping feature {c}\"\n            data_vals = set(data[c].unique().tolist())\n            group_vals = set(self.features_and_values[c])\n            intersection = data_vals.intersection(group_vals)\n            assert len(intersection), \\\n                f\"None of the specified grouping values {group_vals} \" \\\n                f\"are in column {c} values {data_vals}. Do the grouping \" \\\n                f\"values have the same type as the column type {data[c].dtype}?\"\n\n            missing_ood_vals = set(group_vals) - set(data_vals)\n            if len(missing_ood_vals):\n                logging.warning(f\"values {list(missing_ood_vals)} specified \"\n                      f\"in Grouper split but not  present in the data.\")\n            return\n\n    def _check_transformed(self, data: pd.DataFrame):\n        \"\"\"Check the outputs of the transform function.\"\"\"\n        for c in self.features:\n            vals = data[c].unique()\n            if len(vals) < 2:\n                raise ValueError(f\"[ERROR] column {c} contains only one \"\n                                 f\"unique value after transformation {vals}\")\n\n        # Print a summary of the counts after grouping.\n        logging.debug(\"overall counts after grouping:\")\n        if len(self.features) == 1:\n            logging.info(data[self.features[0]].value_counts())\n        else:\n            row_feat = self.features[0]\n            col_feats = self.features[1:]\n            xt = pd.crosstab(data[row_feat].squeeze(),\n                             data[col_feats].squeeze(),\n                             dropna=False)\n            logging.debug(xt)\n\n    def _group_column(self, x: pd.Series, vals: Sequence) -> pd.Series:\n        \"\"\"Apply a grouping to a column, retuning a binary numeric Series.\"\"\"\n        # Ensure types are the same by casting vals to the same type as x.\n        tmp = pd.Series(vals).astype(x.dtype)\n        return x.isin(tmp).astype(int)\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        self._check_inputs(data)\n        for c in self.features:\n            data[c] = self._group_column(data[c], self.features_and_values[c])\n        self._check_transformed(data)\n        return data\n"}
{"type": "source_file", "path": "tableshift/core/data_source.py", "content": "\"\"\"Data sources for TableBench.\"\"\"\nimport glob\nimport gzip\nimport logging\nimport os\nimport re\nimport zipfile\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom functools import partial\nfrom io import StringIO\nfrom typing import Sequence, Optional, Callable\n\nimport datasets\nimport folktables\nimport numpy as np\nimport pandas as pd\nimport requests\n\nimport tableshift.datasets\nfrom tableshift.core import utils\nfrom tableshift.datasets.acs import ACS_STATE_LIST, preprocess_acs, \\\n    get_acs_data_source, ACS_TASK_CONFIGS, acs_data_to_df\nfrom tableshift.datasets.acs_feature_mappings import get_feature_mapping\nfrom tableshift.datasets.adult import ADULT_RESOURCES, ADULT_FEATURE_NAMES, \\\n    preprocess_adult\nfrom tableshift.datasets.anes import preprocess_anes\nfrom tableshift.datasets.automl_multimodal_benchmark import preprocess_automl\nfrom tableshift.datasets.brfss import preprocess_brfss, align_brfss_features\nfrom tableshift.datasets.catboost_benchmarks import preprocess_appetency, \\\n    preprocess_click, preprocess_kick\nfrom tableshift.datasets.communities_and_crime import CANDC_RESOURCES, \\\n    preprocess_candc, CANDC_INPUT_FEATURES\nfrom tableshift.datasets.compas import COMPAS_RESOURCES, preprocess_compas\nfrom tableshift.datasets.diabetes_readmission import \\\n    DIABETES_READMISSION_RESOURCES, preprocess_diabetes_readmission\nfrom tableshift.datasets.german import GERMAN_RESOURCES, preprocess_german\nfrom tableshift.datasets.grinsztajn import preprocess_grinsztain_datataset\nfrom tableshift.datasets.heloc import preprocess_heloc\nfrom tableshift.datasets.kaggle import preprocess_otto, preprocess_walmart\nfrom tableshift.datasets.mimic_extract import preprocess_mimic_extract, \\\n    MIMIC_EXTRACT_STATIC_FEATURES\nfrom tableshift.datasets.mooc import preprocess_mooc\nfrom tableshift.datasets.nhanes import preprocess_nhanes_cholesterol, \\\n    get_nhanes_data_sources, preprocess_nhanes_lead, NHANES_YEARS\nfrom tableshift.datasets.physionet import preprocess_physionet\nfrom tableshift.datasets.uci import WINE_CULTIVARS_FEATURES, ABALONE_FEATURES, \\\n    preprocess_abalone\nfrom tableshift.datasets.utils import apply_column_missingness_threshold\n\n\nclass DataSource(ABC):\n    \"\"\"Abstract class to represent a generic data source.\"\"\"\n\n    def __init__(self, cache_dir: str,\n                 preprocess_fn: Callable[[pd.DataFrame], pd.DataFrame],\n                 resources: Sequence[str] = None,\n                 download: bool = True,\n                 ):\n        self.cache_dir = cache_dir\n        self.download = download\n\n        self.preprocess_fn = preprocess_fn\n        self.resources = resources\n        self._initialize_cache_dir()\n\n    def _initialize_cache_dir(self):\n        \"\"\"Create cache_dir if it does not exist.\"\"\"\n        utils.initialize_dir(self.cache_dir)\n\n    def get_data(self) -> pd.DataFrame:\n        \"\"\"Fetch data from local cache or download if necessary.\"\"\"\n        self._download_if_not_cached()\n        raw_data = self._load_data()\n        return self.preprocess_fn(raw_data)\n\n    def _download_if_not_cached(self):\n        \"\"\"Download files if they are not already cached.\"\"\"\n        for url in self.resources:\n            utils.download_file(url, self.cache_dir)\n\n    @abstractmethod\n    def _load_data(self) -> pd.DataFrame:\n        \"\"\"Load the raw data from disk and return it.\n\n        Any preprocessing should be performed in preprocess_fn, not here.\"\"\"\n        raise\n\n    @property\n    def is_cached(self) -> bool:\n        \"\"\"Check whether all resources exist in cache dir.\"\"\"\n        for url in self.resources:\n            basename = utils.basename_from_url(url)\n            fp = os.path.join(self.cache_dir, basename)\n            if not os.path.exists(fp):\n                return False\n        return True\n\n\nclass OfflineDataSource(DataSource):\n\n    def get_data(self) -> pd.DataFrame:\n        raw_data = self._load_data()\n        return self.preprocess_fn(raw_data)\n\n    def _load_data(self) -> pd.DataFrame:\n        raise\n\n\nclass ANESDataSource(OfflineDataSource):\n    def __init__(\n            self,\n            years: Optional[Sequence] = None,\n            preprocess_fn=preprocess_anes,\n            resources=(\"anes_timeseries_cdf_csv_20220916/\"\n                       \"anes_timeseries_cdf_csv_20220916.csv\",),\n            **kwargs):\n        if years is not None:\n            assert isinstance(years, list) or isinstance(years, tuple), \\\n                f\"years must be a list or tuple, not type {type(years)}.\"\n        self.years = years\n        super().__init__(resources=resources,\n                         preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        fp = os.path.join(self.cache_dir, self.resources[0])\n        df = pd.read_csv(fp, low_memory=False, na_values=(' '))\n        if self.years:\n            df = df[df[\"VCF0004\"].isin(self.years)]\n        return df\n\n\nclass MOOCDataSource(OfflineDataSource):\n    def __init__(\n            self,\n            preprocess_fn=preprocess_mooc,\n            resources=(os.path.join(\"dataverse_files\",\n                                    \"HXPC13_DI_v3_11-13-2019.csv\"),),\n            **kwargs):\n        super().__init__(resources=resources,\n                         preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        fp = os.path.join(self.cache_dir, self.resources[0])\n        if not os.path.exists(fp):\n            raise RuntimeError(\n                f\"\"\"Data files does not exist at {fp}. This dataset must be \n                manually downloaded. Visit https://doi.org/10.7910/DVN/26147,\n                click 'Access Dataset' > 'Original Format ZIP', download the ZIP\n                file to the cache directory at {self.cache_dir}, and \n                unzip it.\"\"\")\n        df = pd.read_csv(fp)\n        return df\n\n\nclass KaggleDataSource(DataSource):\n    def __init__(\n            self,\n            kaggle_dataset_name: str,\n            kaggle_creds_dir=\"~/.kaggle\",\n            **kwargs):\n        self.kaggle_creds_dir = kaggle_creds_dir\n        self.kaggle_dataset_name = kaggle_dataset_name\n        super().__init__(**kwargs)\n\n    @property\n    def kaggle_creds(self):\n        return os.path.expanduser(\n            os.path.join(self.kaggle_creds_dir, \"kaggle.json\"))\n\n    @property\n    def zip_file_name(self):\n        \"\"\"Name of the zip file downloaded by Kaggle API.\"\"\"\n        return os.path.basename(self.kaggle_dataset_name) + \".zip\"\n\n    @abstractmethod\n    def _load_data(self) -> pd.DataFrame:\n        # Should be implemented by subclasses.\n        raise\n\n    def _check_creds(self):\n        # Check Kaggle authentication.\n        assert os.path.exists(self.kaggle_creds), \\\n            f\"No kaggle credentials found at {self.kaggle_creds}.\"\n        \"Create an access token at https://www.kaggle.com/YOUR_USERNAME/account\"\n        f\"and store it at {self.kaggle_creds}.\"\n\n    def _download_kaggle_data(self):\n        \"\"\"Download the data from Kaggle.\"\"\"\n        self._check_creds()\n\n        # Download the dataset using Kaggle CLI.\n        cmd = \"kaggle datasets download \" \\\n              f\"-d {self.kaggle_dataset_name} \" \\\n              f\"-p {self.cache_dir}\"\n        utils.run_in_subprocess(cmd)\n        return\n\n    def _download_if_not_cached(self):\n        self._download_kaggle_data()\n        # location of the local zip file\n        zip_fp = os.path.join(self.cache_dir, self.zip_file_name)\n        # where to unzip the file to\n        unzip_dest = os.path.join(self.cache_dir, self.kaggle_dataset_name)\n        with zipfile.ZipFile(zip_fp, 'r') as zf:\n            zf.extractall(unzip_dest)\n\n\nclass KaggleDownloadError(ValueError):\n    pass\n\n\nclass KaggleCompetitionDataSource(KaggleDataSource):\n    def _download_kaggle_data(self):\n        \"\"\"Download the data from Kaggle.\"\"\"\n        self._check_creds()\n\n        # Download using Kaggle CLI.\n\n        cmd = \"kaggle competitions download \" \\\n              f\"{self.kaggle_dataset_name} \" \\\n              f\"-p {self.cache_dir}\"\n        res = utils.run_in_subprocess(cmd)\n\n        if res.returncode != 0:\n            raise KaggleDownloadError(\n                f\"exception when downloading data for competition \"\n                f\"{self.kaggle_dataset_name} you may\"\n                \"need to visit the competition page on kaggle at \"\n                f\"https://www.kaggle.com/competitions/{self.kaggle_dataset_name}/data\"\n                \" and agree to the terms of the competition.\")\n\n        return\n\n\nclass AmazonDataSource(KaggleCompetitionDataSource):\n    def __init__(self, preprocess_fn=lambda x: x, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        # only use the training data, since Kaggle set sets are unlabeled.\n        train_fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                                \"train.csv\")\n        return pd.read_csv(train_fp)\n\n\nclass BRFSSDataSource(DataSource):\n    \"\"\"BRFSS data source.\n\n    Note that the BRFSS is composed of three components: 'fixed core' questions,\n    asked every year, 'rotating core', asked every other year, and 'emerging\n    core'. Since some of our features come from the rotating core, we only\n    use every-other-year data sources; some features would be empty for the\n    intervening years.\n\n    See also https://www.cdc.gov/brfss/about/brfss_faq.htm , \"What are the\n    components of the BRFSS questionnaire?\"\n    \"\"\"\n\n    def __init__(self, task: str, preprocess_fn=preprocess_brfss,\n                 years=(2021,), **kwargs):\n        self.years = years\n        resources = tuple([\n            f\"https://www.cdc.gov/brfss/annual_data/{y}/files/LLCP{y}XPT.zip\"\n            for y in self.years])\n        _preprocess_fn = partial(preprocess_fn, task=task)\n        super().__init__(preprocess_fn=_preprocess_fn, resources=resources,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        dfs = {}\n        for url in self.resources:\n            zip_fname = utils.basename_from_url(url)\n            xpt_fname = zip_fname.replace(\"XPT.zip\", \".XPT\")\n            xpt_fp = os.path.join(self.cache_dir, xpt_fname)\n            # Unzip the file if needed\n            if not os.path.exists(xpt_fp):\n                zip_fp = os.path.join(self.cache_dir, zip_fname)\n                logging.debug(f\"unzipping {zip_fp}\")\n                with zipfile.ZipFile(zip_fp, 'r') as zf:\n                    zf.extractall(self.cache_dir)\n                # BRFSS data files have an awful space at the end; remove it.\n                os.rename(xpt_fp + \" \", xpt_fp)\n            # Read the XPT data\n            logging.debug(f\"reading {xpt_fp}\")\n            df = utils.read_xpt(xpt_fp)\n            df = align_brfss_features(df)\n            dfs[url] = df\n\n        return pd.concat(dfs.values(), axis=0)\n\n\nclass NHANESDataSource(DataSource):\n    def __init__(\n            self,\n            nhanes_task: str,\n            years: Sequence[int] = NHANES_YEARS,\n            **kwargs):\n        self.nhanes_task = nhanes_task\n        self.years = years\n\n        if self.nhanes_task == \"cholesterol\":\n            preprocess_fn = preprocess_nhanes_cholesterol\n        elif self.nhanes_task == \"lead\":\n            preprocess_fn = preprocess_nhanes_lead\n        else:\n            raise ValueError\n\n        super().__init__(preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _download_if_not_cached(self):\n\n        def _add_suffix_to_fname_from_url(url: str, suffix: str):\n            \"\"\"Helper function to add unique names to files by year.\"\"\"\n            fname = utils.basename_from_url(url)\n            f, extension = fname.rsplit(\".\")\n            new_fp = f + suffix + \".\" + extension\n            return new_fp\n\n        sources = get_nhanes_data_sources(self.nhanes_task, self.years)\n        for year, urls in sources.items():\n            for url in urls:\n                destfile = _add_suffix_to_fname_from_url(url, str(year))\n                utils.download_file(url, self.cache_dir,\n                                    dest_file_name=destfile)\n\n    def _load_data(self) -> pd.DataFrame:\n        files = glob.glob(os.path.join(self.cache_dir, \"*.XPT\"))\n\n        # Initialize a dict where keys are years, and values are lists\n        # containing the list of dataframes of data for that year; these\n        # can be joined on their index (not sure whether the index is\n        # unique across years).\n        year_dfs = defaultdict(list)\n\n        for f in files:\n            year = int(re.search(\".*([0-9]{4})\\\\.XPT\", f).group(1))\n            if year in self.years:\n                logging.debug(f\"reading {f}\")\n                df = utils.read_xpt(f)\n                try:\n                    df.set_index(\"SEQN\", inplace=True)\n                except KeyError:\n                    # Some LLCP files only contain 'SEQNO' feature.\n                    df.set_index(\"SEQNO\", inplace=True)\n                year_dfs[year].append(df)\n\n        df_list = []\n        for year in year_dfs.keys():\n            # Join the first dataframe with all others.\n            dfs = year_dfs[year]\n            src_df = dfs[0]\n            try:\n                logging.info(f\"joining {len(dfs)} dataframes for {year}\")\n                df = src_df.join(dfs[1:], how=\"outer\")\n                df[\"nhanes_year\"] = int(year)\n                logging.info(\"finished joins\")\n                df_list.append(df)\n            except Exception as e:\n                logging.error(e)\n\n        if len(df_list) > 1:\n            df = pd.concat(df_list, axis=0)\n        else:\n            df = df_list[0]\n\n        return df\n\n\nclass ACSDataSource(DataSource):\n    def __init__(self,\n                 acs_task: str,\n                 preprocess_fn=preprocess_acs,\n                 years: Sequence[int] = (2018,),\n                 states=ACS_STATE_LIST,\n                 feature_mapping=\"coarse\",\n                 **kwargs):\n        self.acs_task = acs_task.lower().replace(\"acs\", \"\")\n        self.feature_mapping = get_feature_mapping(feature_mapping)\n        self.states = states\n        self.years = years\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _get_acs_data(self):\n        year_dfs = []\n\n        for year in self.years:\n            logging.info(f\"fetching ACS data for year {year}...\")\n            data_source = get_acs_data_source(year, self.cache_dir)\n            year_data = data_source.get_data(states=self.states,\n                                             join_household=True,\n                                             download=True)\n            year_data[\"ACS_YEAR\"] = year\n            year_dfs.append(year_data)\n        logging.info(\"fetching ACS data complete.\")\n        return pd.concat(year_dfs, axis=0)\n\n    def _download_if_not_cached(self):\n        \"\"\"No-op for ACS data; folktables already downloads or uses cache as\n        needed at _load_data(). \"\"\"\n        return\n\n    def _load_data(self) -> pd.DataFrame:\n        acs_data = self._get_acs_data()\n        task_config = ACS_TASK_CONFIGS[self.acs_task]\n        target_transform = partial(task_config.target_transform,\n                                   threshold=task_config.threshold)\n        ACSProblem = folktables.BasicProblem(\n            features=task_config.features_to_use.predictors,\n            target=task_config.target,\n            target_transform=target_transform,\n            preprocess=task_config.preprocess,\n            postprocess=task_config.postprocess,\n        )\n        X, y, _ = ACSProblem.df_to_numpy(acs_data)\n        df = acs_data_to_df(X, y, task_config.features_to_use,\n                            feature_mapping=self.feature_mapping)\n        return df\n\n\nclass AdultDataSource(DataSource):\n    \"\"\"Data source for the Adult dataset.\"\"\"\n\n    def __init__(self, resources=ADULT_RESOURCES,\n                 preprocess_fn=preprocess_adult, **kwargs):\n        super().__init__(resources=resources,\n                         preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        train_fp = os.path.join(self.cache_dir, \"adult.data\")\n        train = pd.read_csv(\n            train_fp,\n            names=ADULT_FEATURE_NAMES,\n            sep=r'\\s*,\\s*',\n            engine='python', na_values=\"?\")\n        train[\"Split\"] = \"train\"\n\n        test_fp = os.path.join(self.cache_dir, \"adult.test\")\n\n        test = pd.read_csv(\n            test_fp,\n            names=ADULT_FEATURE_NAMES,\n            sep=r'\\s*,\\s*',\n            engine='python', na_values=\"?\", skiprows=1)\n        test[\"Split\"] = \"test\"\n\n        return pd.concat((train, test))\n\n\nclass COMPASDataSource(DataSource):\n    def __init__(self, resources=COMPAS_RESOURCES,\n                 preprocess_fn=preprocess_compas, **kwargs):\n        super().__init__(resources=resources,\n                         preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(\n            os.path.join(self.cache_dir, \"compas-scores-two-years.csv\"))\n        return df\n\n\nclass GermanDataSource(DataSource):\n    def __init__(self, resources=GERMAN_RESOURCES,\n                 preprocess_fn=preprocess_german, **kwargs):\n        super().__init__(resources=resources, preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir, \"german.data\"),\n                         sep=\" \", header=None)\n        return df\n\n\nclass DiabetesReadmissionDataSource(DataSource):\n    def __init__(self, resources=DIABETES_READMISSION_RESOURCES,\n                 preprocess_fn=preprocess_diabetes_readmission, **kwargs):\n        super().__init__(resources=resources, preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        # unzip the file\n        zip_fp = os.path.join(self.cache_dir, \"dataset_diabetes.zip\")\n        with zipfile.ZipFile(zip_fp, 'r') as zf:\n            zf.extractall(self.cache_dir)\n        # read the dataframe\n        df = pd.read_csv(os.path.join(self.cache_dir, \"dataset_diabetes\",\n                                      \"diabetic_data.csv\"),\n                         na_values=\"?\",\n                         low_memory=False)\n        return df\n\n\nclass CommunitiesAndCrimeDataSource(DataSource):\n    def __init__(self, resources=CANDC_RESOURCES,\n                 preprocess_fn=preprocess_candc, **kwargs):\n        super().__init__(resources=resources, preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir, 'communities.data'),\n                         names=CANDC_INPUT_FEATURES)\n        return df\n\n\nclass PhysioNetDataSource(DataSource):\n    def __init__(self, preprocess_fn=preprocess_physionet, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _download_if_not_cached(self):\n        # check if correct number of training files exist in cache dir\n        root = os.path.join(self.cache_dir, \"physionet.org\", \"files\",\n                            \"challenge-2019\", \"1.0.0\", \"training\")\n        n_train_a = len(glob.glob(os.path.join(root, \"training_setA\", \"*.psv\")))\n        n_train_b = len(glob.glob(os.path.join(root, \"training_setB\", \"*.psv\")))\n\n        if (not n_train_a == 20336) or (not n_train_b == 20000):\n            logging.info(\"downloading physionet training data. This could \"\n                         \"take several minutes.\")\n            # download the training data\n            cmd = \"wget -r -N -c -np https://physionet.org/files/challenge\" \\\n                  f\"-2019/1.0.0/training/ -P={self.cache_dir}\"\n            utils.run_in_subprocess(cmd)\n        else:\n            logging.info(f\"detected valid physionet training data at {root}; \"\n                         f\"skipping download\")\n        return\n\n    def _load_data(self) -> pd.DataFrame:\n        root = os.path.join(self.cache_dir, \"physionet.org\", \"files\",\n                            \"challenge-2019\", \"1.0.0\", \"training\")\n        logging.info(\"reading physionet data files.\")\n        train_a_files = glob.glob(os.path.join(root, \"training_setA\", \"*.psv\"))\n        df_a = pd.concat(pd.read_csv(x, delimiter=\"|\") for x in train_a_files)\n        train_b_files = glob.glob(os.path.join(root, \"training_setB\", \"*.psv\"))\n        df_b = pd.concat(pd.read_csv(x, delimiter=\"|\") for x in train_b_files)\n        logging.info(\"done reading physionet data files.\")\n        df_a[\"set\"] = \"a\"\n        df_b[\"set\"] = \"b\"\n        df = pd.concat((df_a, df_b))\n        df.reset_index(drop=True, inplace=True)\n        return df\n\n\nclass MIMICExtractDataSource(OfflineDataSource):\n\n    def __init__(self, task: str = \"los_3\",\n                 preprocess_fn=preprocess_mimic_extract,\n                 static_features=MIMIC_EXTRACT_STATIC_FEATURES.names, **kwargs):\n        # Note: mean label values in overall dataset:\n        # mort_hosp 0.106123\n        # mort_icu 0.071709\n        # los_3 0.430296\n        # los_7 0.077055\n\n        if task not in ('mort_hosp', 'mort_icu', 'los_3', 'los_7'):\n            raise NotImplementedError(f\"task {task} is not implemented.\")\n        self.task = task\n        self.static_features = static_features\n        _preprocess_fn = partial(preprocess_fn, task=task,\n                                 static_features=self.static_features)\n        super().__init__(**kwargs, preprocess_fn=_preprocess_fn)\n\n    def _load_data(self, gap_time_hrs=6, window_size_hrs=24) -> pd.DataFrame:\n        \"\"\"Load the data and apply any shared MIMIC-extract preprocessing\n        with default parameters.\"\"\"\n\n        filename = os.path.join(self.cache_dir, 'all_hourly_data.h5')\n        assert os.path.exists(\n            filename), \\\n            f\"\"\"file {filename} does not exist; see the TableShift \n            instructions for  accessing/placing the MIMIC-extract dataset at \n            the expected location. The data file can be accessed at \n            https://storage.googleapis.com/mimic_extract/all_hourly_data.h5 \n            after  obtaining access as described at \n            https://github.com/MLforHealth/MIMIC_Extract\"\"\"\n        data_full_lvl2 = pd.read_hdf(filename, 'vitals_labs')\n        statics = pd.read_hdf(filename, 'patients')\n\n        # Extract/compute labels, retaining only the labels for the current task.\n        Ys = statics[statics.max_hours > window_size_hrs + gap_time_hrs][\n            ['mort_hosp', 'mort_icu', 'los_icu']]\n        Ys['los_3'] = Ys['los_icu'] > 3\n        Ys['los_7'] = Ys['los_icu'] > 7\n        Ys = Ys[[self.task]]\n        Ys = Ys.astype(int)\n\n        # MIMIC-default filtering: keep only those observations where labels are known; keep\n        # only those observations within the window size.\n        lvl2 = data_full_lvl2[\n            (data_full_lvl2.index.get_level_values('icustay_id').isin(\n                set(Ys.index.get_level_values('icustay_id')))) &\n            (data_full_lvl2.index.get_level_values(\n                'hours_in') < window_size_hrs)]\n\n        # Join data with labels and static features.\n        df_out = lvl2.join(Ys, how=\"inner\")\n        df_out = df_out.join(statics[MIMIC_EXTRACT_STATIC_FEATURES.names])\n        assert len(df_out) == len(lvl2), \"Sanity check of labels join.\"\n        return df_out\n\n\nclass HELOCDataSource(OfflineDataSource):\n    \"\"\"FICO Home Equity Line of Credit data source.\n\n    To obtain data access, visit\n    https://community.fico.com/s/explainable-machine-learning-challenge\n    \"\"\"\n\n    def __init__(self, preprocess_fn=preprocess_heloc, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        filename = os.path.join(self.cache_dir, \"heloc_dataset_v1.csv\")\n        assert os.path.exists(\n            filename), \\\n            f\"\"\"file {filename} does not exist; see the TableShift \n            instructions for  accessing/placing the FICO HELOC dataset at the \n            expected location. The data file can be accessed by filling out the\n            data access agreement at\n            https://community.fico.com/s/explainable-machine-learning-challenge\n            \"\"\"\n        return pd.read_csv(filename)\n\n\nclass MetaMIMICDataSource(OfflineDataSource):\n    \"\"\"MetaMIMIC data source.\n\n    The dataset must be manually derived from MIMIC using the scripts\n    provided in https://github.com/ModelOriented/metaMIMIC .\n    \"\"\"\n\n    def __init__(self, preprocess_fn=lambda x: x, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        filename = os.path.join(self.cache_dir, \"metaMIMIC.csv\")\n        assert os.path.exists(filename), \\\n            f\"file {filename} does not exist. Ensure you have constructed the \" \\\n            f\"metaMIMIC dataset as described at \" \\\n            f\"https://github.com/ModelOriented/metaMIMIC and placed the \" \\\n            f\"resulting file at {filename} .\"\n        return pd.read_csv(filename)\n\n\nclass GrinstajnHFDataSource(DataSource):\n    \"\"\"Fetch a dataset from the Grinstajn benchmark from hugging face hub.\"\"\"\n\n    def __init__(self, dataset_name: str,\n                 preprocess_fn=preprocess_grinsztain_datataset, **kwargs):\n        self.dataset_name = dataset_name\n        _preprocess_fn = partial(preprocess_fn, name=dataset_name)\n        super().__init__(preprocess_fn=_preprocess_fn, **kwargs)\n\n    def _load_data(self, hf_split: str = datasets.Split.TRAIN) -> pd.DataFrame:\n        \"\"\"\n        Loading function with special logic to support Grinstajn benchmark.\n        \"\"\"\n\n        # The HF 'data_files' csv names. For datasets which are in both the\n        # 'numeric only' and 'numeric + categorical' benchmarks, we keep the\n        # version in the 'numeric + categorical' benchmark only, since it has\n        # a superset of the original features.\n        CLF_NUM_DSETS = ['bank-marketing', 'Bioresponse', 'california',\n                         'credit', 'Higgs', 'house_16H', 'jannis',\n                         'MagicTelescope', 'MiniBooNE', 'pol']\n        CLF_CAT_DSETS = ['albert', 'covertype',\n                         'default-of-credit-card-clients',\n                         'electricity', 'eye_movements', 'road-safety']\n\n        if self.dataset_name in CLF_NUM_DSETS:\n            # numeric-only datasets\n            data_file = f\"clf_num/{self.dataset_name}.csv\"\n        elif self.dataset_name in CLF_CAT_DSETS:\n            # numeric + categorical datasets\n            data_file = f\"clf_cat/{self.dataset_name}.csv\"\n        else:\n            raise ValueError(f\"dataset {self.dataset_name} not supported.\")\n        dataset = datasets.load_dataset(\"inria-soda/tabular-benchmark\",\n                                        data_files=data_file,\n                                        cache_dir=self.cache_dir)\n        return dataset[hf_split].to_pandas()\n\n    def _download_if_not_cached(self):\n        \"\"\"No-op, since datasets.load_dataset() implements this.\"\"\"\n        return\n\n\nclass ClickDataSource(KaggleCompetitionDataSource):\n    def __init__(self, preprocess_fn=preprocess_click, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        \"\"\"Implements data-loading logic from CatBoost benchmark for click.\"\"\"\n        dataset_dir = os.path.join(self.cache_dir, self.kaggle_dataset_name)\n        files = [os.path.join(dataset_dir, 'track2', f) for f in\n                 ('training.txt',)]\n\n        if any([not os.path.exists(f) for f in files]):\n            # Unzip the data file, which is zipped inside the main .zip file.\n            zip_fp = os.path.join(dataset_dir, 'track2.zip')\n            logging.info(f'unzipping {zip_fp}')\n            # where to unzip the file to\n            unzip_dest = os.path.join(self.cache_dir, self.kaggle_dataset_name)\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(unzip_dest)\n\n        utils.download_file(\n            \"https://raw.githubusercontent.com/catboost/benchmarks/master/quality_benchmarks/prepare_click/stratified_test_idx.txt\",\n            dataset_dir)\n        utils.download_file(\n            \"https://github.com/catboost/benchmarks/raw/master/quality_benchmarks/prepare_click/stratified_train_idx.txt\",\n            dataset_dir)\n        utils.download_file(\n            \"https://github.com/catboost/benchmarks/raw/master/quality_benchmarks/prepare_click/subsampling_idx.txt\",\n            dataset_dir)\n\n        # Data reading code from\n        logging.debug('parsing ids from file subsampling_idx.txt')\n        with open(os.path.join(dataset_dir, \"subsampling_idx.txt\")) as fin:\n            ids = list(map(int, fin.read().split()))\n\n        logging.debug('reading training data')\n        unique_ids = set(ids)\n        data_strings = {}\n        with open(os.path.join(dataset_dir, 'track2', 'training.txt')) as fin:\n            for i, string in enumerate(fin):\n                if i in unique_ids:\n                    data_strings[i] = string\n\n        data_rows = []\n        for i in ids:\n            data_rows.append(data_strings[i])\n\n        data = pd.read_table(StringIO(\"\".join(data_rows)), header=None).apply(\n            np.float64)\n        colnames = ['click',\n                    'impression',\n                    'url_hash',\n                    'ad_id',\n                    'advertiser_id',\n                    'depth',\n                    'position',\n                    'query_id',\n                    'keyword_id',\n                    'title_id',\n                    'description_id',\n                    'user_id']\n        data.columns = colnames\n\n        # train_idx = pd.read_csv(\n        #     os.path.join(dataset_dir, \"stratified_train_idx.txt\"), header=None)\n        test_idx = pd.read_csv(\n            os.path.join(dataset_dir, \"stratified_test_idx.txt\"), header=None)\n\n        data[\"Split\"] = \"train\"\n        data[\"Split\"].iloc[test_idx] = \"test\"\n\n        return data\n\n\nclass KddCup2009DataSource(DataSource):\n    def __init__(self, task_name: str, **kwargs):\n        self.task_name = task_name\n        self.name = task_name\n        _resources = [\n            \"https://kdd.org/cupfiles/KDDCupData/2009/orange_small_train.data.zip\",\n            f\"http://www.kdd.org/cupfiles/KDDCupData/2009/orange_small_train_{task_name}.labels\",\n        ]\n        super().__init__(resources=_resources,\n                         preprocess_fn=preprocess_appetency, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        data_fp = os.path.join(self.cache_dir, \"orange_small_train.data\")\n\n        if not os.path.exists(data_fp):\n            zip_fp = os.path.join(self.cache_dir, \"orange_small_train.data.zip\")\n\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(self.cache_dir)\n\n        data = pd.read_csv(data_fp, sep=\"\\t\")\n        labels = -pd.read_csv(\n            os.path.join(self.cache_dir,\n                         f\"orange_small_train_{self.task_name}.labels\"),\n            header=None)[0]\n        labels = labels.replace({-1: 0})\n        labels.name = \"label\"\n        data = pd.concat((data, labels), axis=1)\n        return data\n\n\nclass KickDataSource(KaggleCompetitionDataSource):\n    def __init__(self, preprocess_fn=preprocess_kick, **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                          \"training.csv\")\n        return pd.read_csv(fp)\n\n\nclass AutoMLBenchmarkDataSource(OfflineDataSource):\n    def __init__(self, automl_benchmark_dataset_name: str, **kwargs):\n        self.dataset_name = automl_benchmark_dataset_name\n        _preprocess_fn = partial(\n            preprocess_automl,\n            automl_benchmark_dataset_name=automl_benchmark_dataset_name)\n        super().__init__(preprocess_fn=_preprocess_fn, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        from auto_mm_bench.datasets import dataset_registry\n        train_dataset = dataset_registry.create(self.dataset_name, 'train')\n        train_df = train_dataset.data\n        train_df['Split'] = 'train'\n\n        test_dataset = dataset_registry.create(self.dataset_name, 'test')\n        test_df = test_dataset.data\n        test_df['Split'] = 'test'\n\n        return pd.concat((train_df, test_df))\n\n\nclass IrisDataSource(DataSource):\n    def __init__(self, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"]\n        super().__init__(resources=_resources,\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        fp = os.path.join(self.cache_dir, \"iris.data\")\n        df = pd.read_csv(fp, names=['Sepal_Length', 'Sepal_Width',\n                                    'Petal_Length', 'Petal_Width', 'Class'])\n        return df\n\n\nclass DryBeanDataSource(DataSource):\n    def __init__(self, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip\"]\n        super().__init__(resources=_resources,\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        excel_fp = os.path.join(self.cache_dir, \"DryBeanDataset\",\n                                \"Dry_Bean_Dataset.xlsx\")\n        if not os.path.exists(excel_fp):\n            zip_fp = os.path.join(self.cache_dir, \"DryBeanDataset.zip\")\n            # where to unzip the file to\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(self.cache_dir)\n\n        return pd.read_excel(excel_fp)\n\n\nclass HeartDiseaseDataSource(DataSource):\n    def __init__(self, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"]\n        super().__init__(resources=_resources,\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(\n            os.path.join(self.cache_dir, \"processed.cleveland.data\"),\n            names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                   'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal',\n                   'num'],\n            na_values='?')\n        return df\n\n\nclass WineCultivarsDataSource(DataSource):\n    def __init__(self, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"]\n        super().__init__(resources=_resources,\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(\n            os.path.join(self.cache_dir, \"wine.data\"),\n            names=WINE_CULTIVARS_FEATURES.names)\n        return df\n\n\nclass WineQualityDataSource(DataSource):\n    def __init__(self, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n        ]\n        super().__init__(resources=_resources,\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df_red = pd.read_csv(\n            os.path.join(self.cache_dir, \"winequality-red.csv\"), sep=\";\")\n        df_white = pd.read_csv(\n            os.path.join(self.cache_dir, \"winequality-white.csv\"), sep=\";\")\n        df_red[\"red_or_white\"] = \"red\"\n        df_white[\"red_or_white\"] = \"white\"\n        return pd.concat((df_red, df_white))\n\n\nclass RiceDataSource(DataSource):\n    \"\"\"Rice data source. Uses a non-UCI archive since the main data files\n    have been removed from UCI (even though it is still listed as a top\n    downloaded dataset). \"\"\"\n\n    def __init__(self,\n                 preprocess_fn=lambda x: x,\n                 **kwargs):\n        super().__init__(preprocess_fn=preprocess_fn,\n                         **kwargs)\n\n    def _download_if_not_cached(self):\n        # The file is no longer available on UCI, so we load it from another\n        # source. This source uses a PHP request, so we make and follow that\n        # to download the actual file.\n\n        ext = \".zip\"\n        zip_fp = os.path.join(self.cache_dir, \"rice\" + ext)\n\n        if not os.path.exists(zip_fp):\n            url = \"https://www.muratkoklu.com/datasets/vtdhnd01.php\"\n            response = requests.get(url)\n            content = response.content\n\n            with open(zip_fp, 'wb') as f:\n                f.write(content)\n                f.close()\n\n    def _load_data(self) -> pd.DataFrame:\n        excel_fp = os.path.join(self.cache_dir,\n                                \"Rice_Dataset_Commeo_and_Osmancik\",\n                                \"Rice_Cammeo_Osmancik.xlsx\")\n        if not os.path.exists(excel_fp):\n            zip_fp = os.path.join(self.cache_dir, \"rice.zip\")\n            # where to unzip the file to\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(self.cache_dir)\n\n        return pd.read_excel(excel_fp)\n\n\nclass BreastCancerDataSource(DataSource):\n    def __init__(self, preprocess_fn=lambda x: x, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\",\n        ]\n        super().__init__(preprocess_fn=preprocess_fn,\n                         resources=_resources,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(\n            os.path.join(self.cache_dir, \"wdbc.data\"),\n            names=[\"id\", \"diagnosis\", \"radius_mean\", \"texture_mean\",\n                   \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n                   \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\",\n                   \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_std\",\n                   \"texture_std\", \"perimeter_std\", \"area_std\", \"smoothness_std\",\n                   \"compactness_std\", \"concavity_std\", \"concave_points_std\",\n                   \"symmetry_std\", \"fractal_dimension_std\", \"radius_worst\",\n                   \"texture_worst\", \"perimeter_worst\", \"area_worst\",\n                   \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\",\n                   \"concave_points_worst\", \"symmetry_worst\",\n                   \"fractal_dimension_worst\"])\n        return df\n\n\nclass CarDataSource(DataSource):\n    def __init__(self, preprocess_fn=lambda x: x, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"]\n        super().__init__(preprocess_fn=preprocess_fn, resources=_resources,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir, \"car.data\"),\n                         names=['buying', 'maint', 'doors', 'persons',\n                                'lug_boot', 'safety', 'class'])\n        return df\n\n\nclass RaisinDataSource(DataSource):\n    def __init__(self, preprocess_fn=lambda x: x, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/00617/Raisin_Dataset.zip\"]\n        super().__init__(preprocess_fn=preprocess_fn,\n                         resources=_resources,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        excel_fp = os.path.join(self.cache_dir,\n                                \"Raisin_Dataset\",\n                                \"Raisin_Dataset.xlsx\")\n        if not os.path.exists(excel_fp):\n            zip_fp = os.path.join(self.cache_dir, \"Raisin_Dataset.zip\")\n            # where to unzip the file to\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(self.cache_dir)\n        return pd.read_excel(excel_fp)\n\n\nclass AbaloneDataSource(DataSource):\n    def __init__(self, preprocess_fn=preprocess_abalone, **kwargs):\n        _resources = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"]\n        super().__init__(preprocess_fn=preprocess_fn,\n                         resources=_resources,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir, \"abalone.data\"),\n                         names=ABALONE_FEATURES.names)\n        return df\n\n\nclass OttoProductsDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            **kwargs,\n            preprocess_fn=preprocess_otto,\n            kaggle_dataset_name=\"otto-group-product-classification-challenge\")\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(\n            self.cache_dir,\n            'otto-group-product-classification-challenge',\n            'train.csv'))\n        return df\n\n\nclass SfCrimeDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(kaggle_dataset_name='sf-crime',\n                         preprocess_fn=lambda x: x, **kwargs)\n\n    @property\n    def zip_file_name(self):\n        return \"train.csv.zip\"\n\n    def _load_data(self) -> pd.DataFrame:\n        csv_fp = os.path.join(self.cache_dir, \"sf-crime\", \"train.csv\")\n        df = pd.read_csv(csv_fp)\n        return df\n\n\nclass PlasticcDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, kaggle_dataset_name='PLAsTiCC-2018',\n                         preprocess_fn=lambda x: x)\n\n    @property\n    def zip_file_name(self):\n        return \"training_set.csv.zip\"\n\n    def _download_kaggle_data(self):\n        \"\"\"Download the data from Kaggle.\n\n        All data files are 40GB, but the training set is only 60MB, so we download\n        only the training set.\n        \"\"\"\n        self._check_creds()\n\n        # Download using Kaggle CLI.\n        try:\n            cmds = (\"kaggle competitions download \" \\\n                    f\"{self.kaggle_dataset_name} -f training_set.csv \" \\\n                    f\"-p {self.cache_dir}\",\n                    \"kaggle competitions download \" \\\n                    f\"{self.kaggle_dataset_name} -f training_set_metadata.csv \" \\\n                    f\"-p {self.cache_dir}\"\n                    )\n            for cmd in cmds:\n                utils.run_in_subprocess(cmd)\n        except Exception as e:\n            logging.warning(\"exception when downloading data; maybe you\"\n                            \"need to visit the competition page on kaggle\"\n                            \"and agree to the terms of the competition?\")\n            raise (e)\n        return\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"training_set.csv\"))\n        meta_df = pd.read_csv(os.path.join(self.cache_dir,\n                                           \"training_set_metadata.csv\"))\n        df = df.merge(meta_df, on=\"object_id\", how=\"inner\")\n        return df\n\n\nclass WalmartDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name=\"walmart-recruiting-trip-type-classification\",\n            preprocess_fn=preprocess_walmart, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        csv_fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                              \"train.csv\")\n        if not os.path.exists(csv_fp):\n            zip_fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                                  \"train.csv.zip\")\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(os.path.join(self.cache_dir,\n                                           self.kaggle_dataset_name))\n        df = pd.read_csv(csv_fp)\n        return df\n\n\nclass TradeShiftDataSource(KaggleCompetitionDataSource):\n    def __init__(self, label_colname: str = 'y33', **kwargs):\n        self.label_colname = label_colname\n        super().__init__(\n            kaggle_dataset_name=\"tradeshift-text-classification\",\n            preprocess_fn=lambda x: x, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        train_data_fp = os.path.join(self.cache_dir,\n                                     self.kaggle_dataset_name,\n                                     \"train.csv.gz\")\n        train_labels_fp = os.path.join(self.cache_dir,\n                                       self.kaggle_dataset_name,\n                                       \"trainLabels.csv.gz\")\n        logging.debug(f'reading training data from {train_data_fp}')\n        with gzip.open(train_data_fp) as f:\n            train_data_df = pd.read_csv(f)\n\n        logging.debug(f'reading train labels from {train_labels_fp}')\n        with gzip.open(train_labels_fp) as f:\n            train_labels = pd.read_csv(f)\n\n        train_data_df[self.label_colname] = train_labels[self.label_colname]\n\n        return train_data_df\n\n\nclass SchizophreniaDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(preprocess_fn=lambda x: x,\n                         kaggle_dataset_name=\"mlsp-2014-mri\",\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        train_data_dir = os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name)\n        train_fnc_fp = os.path.join(train_data_dir, \"train_FNC.csv\")\n        train_labels_fp = os.path.join(train_data_dir, \"train_labels.csv\")\n        train_sbm_fp = os.path.join(train_data_dir, \"train_SBM.csv\")\n        if any(not os.path.exists(f) for f in\n               (train_fnc_fp, train_labels_fp, train_sbm_fp)):\n            zip_fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                                  \"Train.zip\")\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(os.path.join(self.cache_dir,\n                                           self.kaggle_dataset_name))\n\n        train_fnc = pd.read_csv(train_fnc_fp)\n        train_labels = pd.read_csv(train_labels_fp)\n        train_sbm = pd.read_csv(train_sbm_fp)\n        df = train_fnc.merge(train_labels, on=\"Id\").merge(train_sbm, on=\"Id\")\n        return df\n\n\nclass TitanicDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(kaggle_dataset_name='titanic',\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train.csv\"))\n        return df\n\n\nclass SantanderTransactionDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name='santander-customer-transaction-prediction',\n            preprocess_fn=lambda x: x,\n            **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train.csv\"))\n        return df\n\n\nclass HomeCreditDefaultDataSource(KaggleCompetitionDataSource):\n\n    def __init__(self, **kwargs):\n        super().__init__(kaggle_dataset_name='home-credit-default-risk',\n                         preprocess_fn=lambda x: x,\n                         **kwargs)\n\n    @property\n    def zip_file_name(self):\n        return \"application_train.csv.zip\"\n\n    def _download_kaggle_data(self):\n        \"\"\"Download the data from Kaggle.\n\n        All data files are 40GB, but the training set is only 60MB, so we download\n        only the training set.\n        \"\"\"\n        self._check_creds()\n\n        # Download using Kaggle CLI.\n        try:\n            cmd = \"kaggle competitions download \" \\\n                  f\"{self.kaggle_dataset_name} -f application_train.csv \" \\\n                  f\"-p {self.cache_dir}\"\n\n            utils.run_in_subprocess(cmd)\n        except Exception as e:\n            logging.warning(\"exception when downloading data; maybe you\"\n                            \"need to visit the competition page on kaggle\"\n                            \"and agree to the terms of the competition?\")\n            raise e\n        return\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"application_train.csv\"))\n        return df\n\n\nclass IeeFraudDetectionDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(kaggle_dataset_name='ieee-fraud-detection',\n                         preprocess_fn=apply_column_missingness_threshold,\n                         **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        logging.debug(f'reading data for {self.kaggle_dataset_name}')\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train_transaction.csv\"))\n        identity = pd.read_csv(os.path.join(self.cache_dir,\n                                            self.kaggle_dataset_name,\n                                            \"train_identity.csv\"))\n        logging.debug(f\"merging data for {self.kaggle_dataset_name}\")\n        df = df.merge(identity, on=\"TransactionID\", how=\"left\",\n                      suffixes=(None, \"_y\"))\n        df.drop(columns=[c for c in df.columns if c.endswith(\"_y\")],\n                inplace=True)\n        return df\n\n\nclass SafeDriverPredictionDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name='porto-seguro-safe-driver-prediction',\n            preprocess_fn=lambda x: x,\n            **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train.csv\"))\n        return df\n\n\nclass SantanderCustomerSatisfactionDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name='santander-customer-satisfaction',\n            preprocess_fn=lambda x: x,\n            **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train.csv\"))\n        return df\n\n\nclass AmexDefaultDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name='amex-default-prediction',\n            preprocess_fn=apply_column_missingness_threshold,\n            **kwargs)\n\n    @property\n    def zip_file_name(self):\n        return \"train_data.csv.zip\"\n\n    def _download_kaggle_data(self):\n        self._check_creds()\n\n        # Download using Kaggle CLI.\n        cmds = (\"kaggle competitions download \" \\\n                f\"{self.kaggle_dataset_name} -f train_data.csv \" \\\n                f\"-p {self.cache_dir}\",\n                \"kaggle competitions download \" \\\n                f\"{self.kaggle_dataset_name} -f train_labels.csv \" \\\n                f\"-p {self.cache_dir}\"\n                )\n        for cmd in cmds:\n            res = utils.run_in_subprocess(cmd)\n            if res.returncode != 0:\n                raise KaggleDownloadError\n\n        return\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train_data.csv\"))\n        labels_fp = os.path.join(self.cache_dir, self.kaggle_dataset_name,\n                                 \"train_labels.csv\")\n        if not os.path.exists(labels_fp):\n            zip_fp = os.path.join(self.cache_dir, \"train_labels.csv.zip\")\n            with zipfile.ZipFile(zip_fp, 'r') as zf:\n                zf.extractall(os.path.join(self.cache_dir,\n                                           self.kaggle_dataset_name))\n\n        labels = pd.read_csv(labels_fp)\n        df = df.merge(labels, on='customer_ID', how='inner')\n        return df\n\n\nclass AdFraudDataSource(KaggleCompetitionDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name='talkingdata-adtracking-fraud-detection',\n            preprocess_fn=lambda x: x,\n            **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.cache_dir,\n                                      self.kaggle_dataset_name,\n                                      \"train.csv\"))\n\n        # Drop this column since it perfectly predicts the label.\n        df.drop(columns=['attributed_time'], inplace=True)\n        return df\n\n\nclass AssistmentsDataSource(KaggleDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name=\"nicolaswattiez/skillbuilder-data-2009-2010\",\n            preprocess_fn=tableshift.datasets.preprocess_assistments, **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        logging.info(\n            \"reading assistments data (can be slow due to large file size)\")\n        # TODO(jpgard): uncomment below to use full-width dataset after testing.\n        # df = pd.read_csv(os.path.join(\n        #     self.cache_dir,\n        #     self.kaggle_dataset_name,\n        #     \"2012-2013-data-with-predictions-4-final.csv\"))\n        # # # write out a tiny version of assistments datasets\n        # import ipdb;\n        # ipdb.set_trace()\n        # df[tableshift.datasets.ASSISTMENTS_FEATURES.names].to_feather(\n        #     os.path.join(self.cache_dir, \"assistments-subset.feather\"))\n        df = pd.read_feather(os.path.join(self.cache_dir,\n                                          \"assistments-subset.feather\"))\n        logging.info(\"finished reading data\")\n        return df\n\n\nclass CollegeScorecardDataSource(KaggleDataSource):\n    def __init__(self, **kwargs):\n        super().__init__(\n            kaggle_dataset_name=\"kaggle/college-scorecard\",\n            preprocess_fn=tableshift.datasets.preprocess_college_scorecard,\n            **kwargs)\n\n    def _load_data(self) -> pd.DataFrame:\n        return pd.read_csv(os.path.join(self.cache_dir,\n                                        self.kaggle_dataset_name,\n                                        \"Scorecard.csv\"))\n"}
{"type": "source_file", "path": "tableshift/core/splitter.py", "content": "import logging\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Sequence, Mapping, Any, List, Optional, Tuple, Union\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\n\ndef concat_columns(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Helper function to concatenate values over a set of columns.\n\n    This is useful, for example, as a preprocessing step for performing\n    stratified sampling over labels + sensitive attributes.\"\"\"\n    return data.agg(lambda x: ''.join(x.values.astype(str)), axis=1).T\n\n\ndef idx_where_in(x: pd.Series, vals: Sequence[Any]) -> np.ndarray:\n    \"\"\"Return a vector of the numeric indices i where X[i] in vals.\n\n    Note that this function does not differentiate between different numeric\n    types; i.e. if [1] (and integer) is in vals and x_j is 1.0 (a float),\n     the jth index will be included in the output.\n    \"\"\"\n    assert isinstance(vals, list) or isinstance(vals, tuple)\n    idxs_bool = x.isin(vals)\n    return np.nonzero(idxs_bool.values)[0]\n\n\ndef idx_where_not_in(x: pd.Series, vals: Sequence[Any]) -> np.ndarray:\n    \"\"\"Return a vector of the numeric indices i where X[i] not in vals.\n\n    See note in idx_where_in regarding numeric types.\n    \"\"\"\n    assert isinstance(vals, list) or isinstance(vals, tuple)\n    idxs_bool = ~x.isin(vals)\n    return np.nonzero(idxs_bool.values)[0]\n\n\n@dataclass\nclass Splitter:\n    \"\"\"Splitter for non-domain splits.\"\"\"\n    val_size: float\n    random_state: int\n\n    @abstractmethod\n    def __call__(self, data: pd.DataFrame, labels: pd.Series,\n                 groups: pd.DataFrame = None, *args, **kwargs) -> Mapping[\n        str, List[int]]:\n        \"\"\"Split a dataset.\n\n        Returns a dictionary mapping split names to indices of the data points\n        in that split.\"\"\"\n        raise\n\n\nclass FixedSplitter(Splitter):\n    \"\"\"A splitter for using fixed splits.\n\n    This occurs, for example, when a dataset has a fixed train-test\n    split (such as the Adult dataset).\n\n    The FixedSplitter assumes there is a column in the dataset, \"Split\",\n    which contains the values \"train\", \"test\".\n\n    Note that for the fixed splitter, val_size indicates what fraction\n    **of the training data** should be used for the validation set\n    (since we cannot control the fraction of the overall data dedicated\n    to validation, due to the prespecified train/test split).\n    \"\"\"\n\n    def __init__(self, split_colname: str=\"Split\", **kwargs):\n        self.split_colname = split_colname\n        super().__init__(**kwargs)\n\n    def __call__(self, data: pd.DataFrame, labels: pd.Series,\n                 groups: pd.DataFrame = None, *args, **kwargs) -> Mapping[\n        str, List[int]]:\n\n        assert self.split_colname in data.columns, \"data is missing 'Split' column.\"\n        assert all(np.isin(data[self.split_colname], [\"train\", \"test\"]))\n\n        test_idxs = np.nonzero((data[self.split_colname] == \"test\").values)[0]\n        train_val_idxs = \\\n            np.nonzero((data[self.split_colname] == \"train\").values)[0]\n\n        train_idxs, val_idxs = train_test_split(\n            train_val_idxs,\n            train_size=(1 - self.val_size),\n            random_state=self.random_state)\n\n        del train_val_idxs\n        return {\"train\": train_idxs, \"validation\": val_idxs, \"test\": test_idxs}\n\n\ndef _check_input_indices(data: pd.DataFrame):\n    \"\"\"Helper function to validate input indices.\n\n    If a DataFrame is not indexed from (0, n), which happens e.g. when the\n    DataFrame has been filtered without resetting the index, it can cause\n    major downstream issues with splitting. This is because splitting will\n    assume that all values (0,...n) are represented in the index.\n    \"\"\"\n    idxs = np.array(sorted(data.index.tolist()))\n    expected = np.arange(len(data))\n    assert np.all(idxs == expected), \"DataFrame is indexed non-sequentially;\" \\\n                                     \"try passing the dataframe after \"\n    return\n\n\n@dataclass\nclass RandomSplitter(Splitter):\n    test_size: float\n\n    @property\n    def train_size(self):\n        return 1. - (self.val_size + self.test_size)\n\n    def __call__(self, data: pd.DataFrame, labels: pd.Series,\n                 groups: pd.DataFrame = None, *args, **kwargs\n                 ) -> Mapping[str, List[int]]:\n        _check_input_indices(data)\n\n        idxs = data.index.tolist()\n        train_val_idxs, test_idxs = train_test_split(\n            idxs,\n            test_size=self.test_size,\n            random_state=self.random_state)\n        train_idxs, val_idxs = train_test_split(\n            train_val_idxs,\n            train_size=self.train_size / (self.train_size + self.val_size),\n            random_state=self.random_state)\n        del train_val_idxs\n        return {\"train\": train_idxs, \"validation\": val_idxs, \"test\": test_idxs}\n\n\n@dataclass\nclass DomainSplitter(Splitter):\n    \"\"\"Splitter for domain splits.\n\n    All observations with domain_split_varname values in domain_split_ood_values\n    are placed in the target (test) set; the remaining observations are split\n    between the train, validation, and eval set.\n    \"\"\"\n    id_test_size: float  # The in-domain test set.\n    domain_split_varname: str\n\n    domain_split_ood_values: Optional[Sequence[Any]] = None\n    domain_split_id_values: Optional[Sequence[Any]] = None\n\n    # If domain column is greater than this value, observation will be OOD.\n    # If less than or equal to this value, observation will be ID.\n    domain_split_gt_thresh: Optional[Union[int, float]] = None\n\n    drop_domain_split_col: bool = True  # If True, drop column after splitting.\n    ood_val_size: float = 0  # Fraction of OOD data to use for OOD validation set.\n\n    def _split_from_explicit_values(self, domain_vals: pd.Series\n                                    ) -> Tuple[np.ndarray, np.ndarray]:\n\n        # Check that either in- or out-of-domain values are specified.\n        assert self.is_explicit_split()\n\n        # Check that threshold is not specified, since we are using the\n        # explicit list of values to specify ID/OOD.\n        assert self.domain_split_gt_thresh is None\n\n        assert isinstance(self.domain_split_ood_values, tuple) \\\n               or isinstance(self.domain_split_ood_values, list), \\\n            \"domain_split_ood_values must be an iterable type; got type {}\".format(\n                type(self.domain_split_ood_values))\n\n        ood_vals = self.domain_split_ood_values\n\n        # Fetch the out-of-domain indices.\n        ood_idxs = idx_where_in(domain_vals, ood_vals)\n\n        # Fetch the in-domain indices; these are either the explicitly-specified\n        # in-domain values, or any values not in the OOD values.\n\n        if self.domain_split_id_values is not None:\n            # Check that there is no overlap between train/test domains.\n            assert not set(self.domain_split_id_values).intersection(\n                set(ood_vals))\n\n            id_idxs = idx_where_in(domain_vals, self.domain_split_id_values)\n            if not len(id_idxs):\n                raise ValueError(\n                    f\"No ID observations with {self.domain_split_varname} \"\n                    f\"values {self.domain_split_id_values}; are the values of \"\n                    f\"same type as the column type of {domain_vals.dtype}?\")\n        else:\n            id_idxs = idx_where_not_in(domain_vals, ood_vals)\n            if not len(id_idxs):\n                raise ValueError(\n                    f\"No ID observations with {self.domain_split_varname} \"\n                    f\"values not in {ood_vals}.\")\n\n        if not len(ood_idxs):\n            vals = domain_vals.unique()\n            raise ValueError(\n                f\"No OOD observations with {self.domain_split_varname} values \"\n                f\"{ood_vals}; are the values of same type\"\n                f\" as the column type of {domain_vals.dtype}? Examples of \"\n                f\"values in {self.domain_split_varname}: {vals[:10]}\")\n\n        return id_idxs, ood_idxs\n\n    def _split_from_threshold(self, domain_vals: pd.Series) -> Tuple[\n        np.ndarray, np.ndarray]:\n        \"\"\"Apply a threshold.\n\n        Values are OOD if > self.domain_split_gt_thresh, else ID.\"\"\"\n        assert self.is_threshold_split()\n        assert not self.is_explicit_split()\n\n        if np.any(np.isnan(domain_vals)):\n            logging.warning(\n                f\"detected missing values in domain column prior\"\n                \"to splitting; this can result in unexpected behavior\"\n                \"for threshold-based splits. Any nan values will\"\n                f\"have OOD value: {np.nan > self.domain_split_gt_thresh}\")\n\n        ood_idxs = \\\n            np.nonzero((domain_vals > self.domain_split_gt_thresh).values)[0]\n        id_idxs = \\\n            np.nonzero((domain_vals <= self.domain_split_gt_thresh).values)[0]\n        return id_idxs, ood_idxs\n\n    def is_explicit_split(self) -> bool:\n        \"\"\"Helper function to check whether an explicit split is used.\"\"\"\n        return (self.domain_split_ood_values is not None) or (\n                self.domain_split_id_values is not None)\n\n    def is_threshold_split(self) -> bool:\n        \"\"\"Helper function to check whether a threshold-based split is used.\"\"\"\n        return (self.domain_split_gt_thresh is not None)\n\n    def __call__(self, data: pd.DataFrame, labels: pd.Series,\n                 groups: pd.DataFrame = None, *args, **kwargs) -> Mapping[\n        str, List[int]]:\n        assert \"domain_labels\" in kwargs, \"domain labels are required.\"\n        domain_vals = kwargs.pop(\"domain_labels\")\n        assert isinstance(domain_vals, pd.Series)\n\n        if self.is_explicit_split():\n            id_idxs, ood_idxs = self._split_from_explicit_values(domain_vals)\n\n        elif self.is_threshold_split():\n            id_idxs, ood_idxs = self._split_from_threshold(domain_vals)\n\n        else:\n            raise NotImplementedError(\"Invalid domain split specified.\")\n\n        assert not set(id_idxs).intersection(ood_idxs), \"sanity check for \" \\\n                                                        \"nonoverlapping \" \\\n                                                        \"domain split\"\n        assert not set(domain_vals.iloc[id_idxs]) \\\n            .intersection(domain_vals.iloc[ood_idxs]), \"sanity check for no \" \\\n                                                       \"domain leakage\"\n\n        train_idxs, id_valid_eval_idxs = train_test_split(\n            id_idxs, test_size=(self.val_size + self.id_test_size),\n            random_state=self.random_state)\n\n        valid_idxs, id_test_idxs = train_test_split(\n            id_valid_eval_idxs,\n            test_size=self.id_test_size / (self.val_size + self.id_test_size),\n            random_state=self.random_state)\n\n        outputs = {\"train\": train_idxs, \"validation\": valid_idxs,\n                   \"id_test\": id_test_idxs}\n\n        # Out-of-distribution splits\n        if self.ood_val_size:\n            ood_test_idxs, ood_valid_idxs = train_test_split(\n                ood_idxs,\n                test_size=self.ood_val_size,\n                random_state=self.random_state)\n            outputs[\"ood_test\"] = ood_test_idxs\n            outputs[\"ood_validation\"] = ood_valid_idxs\n\n        else:\n            outputs[\"ood_test\"] = ood_idxs\n\n        return outputs\n"}
{"type": "source_file", "path": "tableshift/configs/hparams.py", "content": "from ray import tune\n\nfrom tableshift.models.compat import OPTIMIZER_ARGS\n\n# Superset of https://arxiv.org/pdf/2106.11959.pdf, Table 15,\n# in order to cover hparams for other searches that derive from this space.\n_DEFAULT_NN_SEARCH_SPACE = {\n    \"d_hidden\": tune.choice([64, 128, 256, 512, 1024]),\n    \"lr\": tune.loguniform(1e-5, 1e-1),\n    \"n_epochs\": tune.qrandint(5, 100, 5),\n    \"num_layers\": tune.randint(1, 8),\n    \"dropouts\": tune.uniform(0., 0.5),\n    \"weight_decay\": tune.loguniform(1e-6, 1.)\n}\n\n_aldro_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    \"eta_pi\": tune.loguniform(1e-5, 1e-1),\n    'r': tune.uniform(1e-5, 0.5),\n    'clip_max': tune.loguniform(1e-1, 10),\n    'eps': tune.loguniform(1e-4, 1e-1),\n}\n\n_dann_search_space = {\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items()\n       if k not in OPTIMIZER_ARGS},\n    # Below parameters all use the specified grid from DomainBed.\n    # G (classifier) hyperparameters\n    \"lr_g\": _DEFAULT_NN_SEARCH_SPACE[\"lr\"],\n    \"weight_decay_g\": _DEFAULT_NN_SEARCH_SPACE[\"weight_decay\"],\n    # D (discriminator) hyperparameters\n    \"lr_d\": _DEFAULT_NN_SEARCH_SPACE[\"lr\"],\n    \"weight_decay_d\": _DEFAULT_NN_SEARCH_SPACE[\"weight_decay\"],\n    # Adversarial training parameters\n    \"d_steps_per_g_step\": tune.loguniform(1, 2 ** 3, base=2),\n    \"grad_penalty\": tune.loguniform(1e-2, 1e1),\n    \"loss_lambda\": tune.loguniform(1e-2, 1e2),\n}\n\n_deepcoral_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    # Same range as DomainBed, see\n    # https://github.com/facebookresearch/DomainBed/blob/main/domainbed\n    # /hparams_registry.py#L72\n    \"mmd_gamma\": tune.loguniform(1e-1, 1e1),\n}\n\n_dro_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    \"geometry\": tune.choice([\"cvar\", \"chi-square\"]),\n    # Note: training is very slow for large values of uncertainty\n    # set size (larger than ~0.5) for chi-square geometry, particularly\n    # when the learning rate is small.\n    \"size\": tune.loguniform(1e-4, 1.),\n\n}\n\n_irm_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    # Same tuning space as  for IRM parameters as DomainBed; see\n    # https://github.com/facebookresearch/DomainBed/blob\n    # /2ed9edf781fe4b336c2fb6ffe7ca8a7c6f994422/domainbed/hparams_registry.py\n    # #L61\n    \"irm_lambda\": tune.loguniform(1e-1, 1e5),\n    \"irm_penalty_anneal_iters\": tune.loguniform(1, 1e4)\n}\n\n# Similar to XGBoost search space; however, note that LightGBM is not\n# use in the study from which the XGBoost space is derived.\n_lightgbm_search_space = {\n    \"learning_rate\": tune.loguniform(1e-5, 1.),\n    \"min_child_samples\": tune.choice([1, 2, 4, 8, 16, 32, 64]),\n    \"min_child_weight\": tune.loguniform(1e-8, 1e5),\n    \"subsample\": tune.uniform(0.5, 1),\n    \"max_depth\": tune.choice([-1] + list(range(1, 31))),\n    \"colsample_bytree\": tune.uniform(0.5, 1),\n    \"colsample_bylevel\": tune.uniform(0.5, 1),\n    \"reg_alpha\": tune.loguniform(1e-8, 1e2),\n    \"reg_lambda\": tune.loguniform(1e-8, 1e2),\n}\n\n_mixup_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    # Same range as DomainBed, see\n    # https://github.com/facebookresearch/DomainBed/blob\n    # /2ed9edf781fe4b336c2fb6ffe7ca8a7c6f994422/domainbed/hparams_registry.py\n    # #L66\n    \"mixup_alpha\": tune.uniform(10 ** -1, 10 ** 1)\n}\n\n_mmd_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    # Same range as DomainBed, see\n    # https://github.com/facebookresearch/DomainBed/blob/main/domainbed\n    # /hparams_registry.py#L72\n    \"mmd_gamma\": tune.loguniform(1e-1, 1e1),\n}\n\n_wcs_search_space = {\n    \"C_domain\": tune.choice([0.001, 0.01, 0.1, 1., 10., 100., 1000.]),\n    \"C_discrim\": tune.choice([0.001, 0.01, 0.1, 1., 10., 100., 1000.]),\n\n}\n# Matches https://arxiv.org/pdf/2106.11959.pdf; see Table 16\n_xgb_search_space = {\n    \"learning_rate\": tune.loguniform(1e-5, 1.),\n    \"max_depth\": tune.randint(3, 10),\n    \"min_child_weight\": tune.loguniform(1e-8, 1e5),\n    \"subsample\": tune.uniform(0.5, 1),\n    \"colsample_bytree\": tune.uniform(0.5, 1),\n    \"colsample_bylevel\": tune.uniform(0.5, 1),\n    \"gamma\": tune.loguniform(1e-8, 1e2),\n    \"lambda\": tune.loguniform(1e-8, 1e2),\n    \"alpha\": tune.loguniform(1e-8, 1e2),\n    \"max_bin\": tune.choice([128, 256, 512])\n}\n\n_expgrad_search_space = {\n    **_xgb_search_space,\n    \"eps\": tune.loguniform(1e-4, 1e0),\n    \"eta0\": tune.choice([0.1, 0.2, 1.0, 2.0]),\n}\n\n_group_dro_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    \"group_weights_step_size\": tune.loguniform(1e-4, 1e0),\n}\n\n# Superset of https://arxiv.org/pdf/2106.11959.pdf, Table 14.\n_resnet_search_space = {\n    # Drop the key for d_hidden;\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items() if k != \"d_hidden\"},\n    \"n_blocks\": tune.randint(1, 16),\n    \"d_main\": tune.randint(64, 1024),\n    \"hidden_factor\": tune.randint(1, 4),\n    \"dropout_first\": tune.uniform(0., 0.5),  # after first linear layer\n    \"dropout_second\": tune.uniform(0., 0.5),  # after second/hidden linear layer\n}\n\n# Superset of https://arxiv.org/pdf/2106.11959.pdf, Table 13\n_ft_transformer_search_space = {\n    # Drop the key for d_hidden;\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items() if k != \"d_hidden\"},\n    \"n_blocks\": tune.randint(1, 4),\n    \"residual_dropout\": tune.uniform(0, 0.2),\n    \"attention_dropout\": tune.uniform(0, 0.5),\n    \"ffn_dropout\": tune.uniform(0, 0.5),\n    \"ffn_factor\": tune.uniform(2 / 3, 8 / 3),\n    # This is feature embedding size in Table 13 above. Note that Table 13\n    # reports this as a uniform (64, 512) parameter, but d_token *must* be a\n    # multiple of n_heads, which is fixed at 8, so we use this\n    # simpler/equivalent range instead.\n    \"d_token\": tune.choice([64, 128, 256, 512])\n}\n\n_vrex_search_space = {\n    **_DEFAULT_NN_SEARCH_SPACE,\n    \"vrex_lambda\": tune.loguniform(1e-1, 1e5),\n    \"vrex_penalty_anneal_iters\": tune.loguniform(1, 1e4),\n}\n\n_node_search_space = {\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items() if\n       k in (\"lr\", \"weight_decay\")},\n    # NODE uses much smaller batch size (256x smaller: 4096 vs. 16), so we also\n    # train for fewer epochs to keep the number of gradient updates more closely\n    # matched to other methods.\n    \"n_epochs\": tune.qrandint(1, 5, 1),\n    # Below is identical search space to NODE paper (Appendix A.2.3). Note\n    # that they use a fixed learning rate of 10e-3, but we tune learning\n    # rate, and they use \"the maximal batch size that fits in GPU memory\" but\n    # we instead keep the batch size consistent at a size that fits in memory\n    # regardless of model (and tune LR) instead of manually tuning the batch\n    # size.\n    \"num_layers\": tune.choice([2, 4, 8]),\n    \"total_tree_count\": tune.choice([1024, 2048]),\n    \"depth\": tune.choice([6, 8]),  # \"tree depth\" in A.2.3\n    \"tree_dim\": tune.choice([2, 3])  # \"tree output dim\" in A.2.3\n}\n\n_saint_search_space = {\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items() if\n       k in (\"lr\", \"weight_decay\")},\n    # SAINT uses much smaller batch size (256x smaller: 4096 vs. 16) than\n    # other models in our suite, so we also train for fewer epochs to keep\n    # the number of gradient updates more closely matched to other methods.\n    \"n_epochs\": tune.qrandint(1, 5, 1),\n    \"dim\": tune.choice([4, 8, 12, 16, 32]),\n\n    # NOTE: this parameter is ignored when attentiontype is set to 'row' or\n    # 'colrow', as in original SAINT paper/code we use depth==1 for those\n    # attention types. See utils.get_estimator().\n    \"depth\": tune.choice([4, 6]),\n    \"ff_dropout\": tune.uniform(0.1, 0.8),\n    \"heads\": tune.choice([4, 8]),\n    \"attentiontype\": tune.choice(['row', 'col', 'colrow']),\n}\n\n_tabtransformer_search_space = {\n    **{k: v for k, v in _DEFAULT_NN_SEARCH_SPACE.items() if\n       k in (\"lr\", \"n_epochs\", \"weight_decay\")},\n    \"ff_dropout\": tune.uniform(0., 0.5),\n    \"attn_dropout\": tune.uniform(0., 0.5, ),\n    \"dim\": tune.choice([32, 64, 128, 256]),\n    \"depth\": tune.choice([3, 4, 5, 6]),\n    \"heads\": tune.choice([2, 4, 8]),\n}\n\nsearch_space = {\n    \"aldro\": _aldro_search_space,\n    \"dann\": _dann_search_space,\n    \"deepcoral\": _deepcoral_search_space,\n    \"dro\": _dro_search_space,\n    \"expgrad\": _expgrad_search_space,\n    \"ft_transformer\": _ft_transformer_search_space,\n    \"group_dro\": _group_dro_search_space,\n    \"irm\": _irm_search_space,\n    \"label_group_dro\": _group_dro_search_space,\n    \"lightgbm\": _lightgbm_search_space,\n    \"mixup\": _mixup_search_space,\n    \"mlp\": _DEFAULT_NN_SEARCH_SPACE,\n    \"mmd\": _mmd_search_space,\n    \"node\": _node_search_space,\n    \"resnet\": _resnet_search_space,\n    \"saint\": _saint_search_space,\n    \"tabtransformer\": _tabtransformer_search_space,\n    \"vrex\": _vrex_search_space,\n    \"wcs\": _wcs_search_space,\n    \"xgb\": _xgb_search_space,\n}\n"}
{"type": "source_file", "path": "tableshift/configs/non_benchmark_configs.py", "content": "\"\"\"\nExperiment configs for the non-TableShift benchmark tasks.\n\"\"\"\n\nfrom tableshift.configs.benchmark_configs import \\\n    _MIMIC_EXTRACT_PASSTHROUGH_COLUMNS\nfrom tableshift.configs.experiment_config import ExperimentConfig\nfrom tableshift.configs.experiment_defaults import DEFAULT_ID_TEST_SIZE, \\\n    DEFAULT_OOD_VAL_SIZE, DEFAULT_ID_VAL_SIZE, DEFAULT_RANDOM_STATE\nfrom tableshift.core import RandomSplitter, Grouper, PreprocessorConfig, \\\n    DomainSplitter, FixedSplitter\n\nGRINSTAJN_TEST_SIZE = 0.21\n\nGRINSZTAJN_VAL_SIZE = 0.09\nNON_BENCHMARK_CONFIGS = {\n    \"adult\": ExperimentConfig(\n        splitter=FixedSplitter(val_size=0.25, random_state=29746),\n        grouper=Grouper({\"Race\": [\"White\", ], \"Sex\": [\"Male\", ]}, drop=False),\n        preprocessor_config=PreprocessorConfig(), tabular_dataset_kwargs={}),\n\n    \"_debug\": ExperimentConfig(\n        splitter=DomainSplitter(\n            val_size=0.01,\n            id_test_size=0.2,\n            ood_val_size=0.25,\n            random_state=43406,\n            domain_split_varname=\"purpose\",\n            # Counts by domain are below. We hold out all of the smallest\n            # domains to avoid errors with very small domains during dev.\n            # A48       9\n            # A44      12\n            # A410     12\n            # A45      22\n            # A46      50\n            # A49      97\n            # A41     103\n            # A42     181\n            # A40     234\n            # A43     280\n            domain_split_ood_values=[\"A44\", \"A410\", \"A45\", \"A46\", \"A48\"]\n        ),\n        grouper=Grouper({\"sex\": ['1.0', ], \"age_geq_median\": ['1.0', ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"name\": \"german\"}),\n\n    \"german\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=0.01, test_size=0.2, random_state=832),\n        grouper=Grouper({\"sex\": ['1.0', ], \"age_geq_median\": ['1.0', ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(), tabular_dataset_kwargs={}),\n\n    \"mooc\": ExperimentConfig(\n        splitter=DomainSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                ood_val_size=DEFAULT_OOD_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                id_test_size=DEFAULT_ID_TEST_SIZE,\n                                domain_split_varname=\"course_id\",\n                                domain_split_ood_values=[\n                                    \"HarvardX/CB22x/2013_Spring\"]),\n        grouper=Grouper({\"gender\": [\"m\", ],\n                         \"LoE_DI\": [\"Bachelor's\", \"Master's\", \"Doctorate\"]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(), tabular_dataset_kwargs={}),\n\n    ################### Grinsztajn et al. benchmark datasets ###################\n\n    **{x: ExperimentConfig(\n        splitter=RandomSplitter(val_size=GRINSZTAJN_VAL_SIZE,\n                                test_size=GRINSTAJN_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\"dataset_name\": x}\n    ) for x in (\"electricity\", \"bank-marketing\", \"california\",\n                \"covertype\", \"credit\", 'default-of-credit-card-clients',\n                'eye_movements', 'Higgs', 'MagicTelescope', 'MiniBooNE',\n                'road-safety', 'pol', 'jannis', 'house_16H')},\n\n    ################### MetaMIMIC datasets #####################################\n\n    \"metamimic_alcohol\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_alcohol'}),\n\n    'metamimic_anemia': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_anemia'}),\n\n    'metamimic_atrial': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_atrial'}),\n\n    'metamimic_diabetes': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_diabetes'}),\n\n    'metamimic_heart': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_heart'}),\n\n    'metamimic_hypertension': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_hypertension'}),\n\n    'metamimic_hypotension': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_hypotension'}),\n\n    'metamimic_ischematic': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_ischematic'}),\n\n    'metamimic_lipoid': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_lipoid'}),\n\n    'metamimic_overweight': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_overweight'}),\n\n    'metamimic_purpura': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_purpura'}),\n\n    'metamimic_respiratory': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE,\n                                test_size=DEFAULT_ID_TEST_SIZE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            numeric_features=\"kbins\",\n            passthrough_columns=[\"age\"]\n        ),\n        tabular_dataset_kwargs={'name': 'metamimic_respiratory'}),\n\n    ################### CatBoost benchmark datasets ########################\n\n    \"amazon\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={\n            \"kaggle_dataset_name\": \"amazon-employee-access-challenge\"}),\n\n    **{k: ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        # categorical features in this dataset have *extremely* high cardinality\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"passthrough\"),\n        tabular_dataset_kwargs={\"task_name\": k}) for k in\n        (\"appetency\", \"churn\", \"upselling\")},\n\n    \"click\": ExperimentConfig(\n        splitter=FixedSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                               random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        # categorical features in this dataset have *extremely* high cardinality\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"passthrough\"),\n        tabular_dataset_kwargs={\n            \"kaggle_dataset_name\": \"kddcup2012-track2\"}),\n\n    'kick': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"passthrough\"),\n        tabular_dataset_kwargs={\"kaggle_dataset_name\": \"DontGetKicked\"},\n    ),\n\n    ############# AutoML benchmark datasets (classification only) ##############\n    **{x: ExperimentConfig(\n        splitter=FixedSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                               random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"passthrough\",\n            dropna=None),\n        tabular_dataset_kwargs={\n            \"automl_benchmark_dataset_name\": x}) for x in (\n        'product_sentiment_machine_hack', 'data_scientist_salary',\n        'melbourne_airbnb', 'news_channel', 'wine_reviews',\n        'imdb_genre_prediction', 'fake_job_postings2', 'kick_starter_funding',\n        'jigsaw_unintended_bias100K',)},\n\n    ######################## UCI datasets ######################################\n    **{x: ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(),\n        tabular_dataset_kwargs={}\n    ) for x in ('iris', 'dry-bean', 'heart-disease', 'wine', 'wine-quality',\n                'rice', 'cars', 'raisin', 'abalone')},\n\n    # For breast cancer, mean/stc/worst values are already computed as features,\n    # so we passthrough by default.\n    'breast-cancer': ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(numeric_features=\"passthrough\"),\n        tabular_dataset_kwargs={}),\n    ######################## Kaggle datasets ###################################\n    **{x: ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            categorical_features=\"passthrough\", dropna=None),\n        tabular_dataset_kwargs={}\n    ) for x in ('otto-products', 'sf-crime', 'plasticc', 'walmart',\n                'tradeshift', 'schizophrenia', 'titanic',\n                'santander-transactions', 'home-credit-default-risk',\n                'ieee-fraud-detection', 'safe-driver-prediction',\n                'santander-customer-satisfaction', 'amex-default',\n                'ad-fraud')},\n\n    ############################################################################\n\n    \"mimic_extract_los_3_selected\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=_MIMIC_EXTRACT_PASSTHROUGH_COLUMNS),\n        tabular_dataset_kwargs={\"task\": \"los_3\",\n                                \"name\": \"mimic_extract_los_3_selected\"}),\n\n    \"mimic_extract_mort_hosp_selected\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(\n            passthrough_columns=_MIMIC_EXTRACT_PASSTHROUGH_COLUMNS),\n        tabular_dataset_kwargs={\"task\": \"mort_hosp\",\n                                \"name\": \"mimic_extract_mort_hosp_selected\"}),\n\n    \"communities_and_crime\": ExperimentConfig(\n        splitter=RandomSplitter(val_size=DEFAULT_ID_VAL_SIZE,\n                                test_size=DEFAULT_ID_TEST_SIZE,\n                                random_state=DEFAULT_RANDOM_STATE),\n        grouper=None,\n        preprocessor_config=PreprocessorConfig(), tabular_dataset_kwargs={}),\n\n    \"compas\": ExperimentConfig(\n        splitter=RandomSplitter(test_size=0.2, val_size=0.01,\n                                random_state=90127),\n        grouper=Grouper({\"race\": [\"Caucasian\", ], \"sex\": [\"Male\", ]},\n                        drop=False),\n        preprocessor_config=PreprocessorConfig(), tabular_dataset_kwargs={}),\n\n}\n"}
{"type": "source_file", "path": "tableshift/datasets/brfss.py", "content": "\"\"\"\nUtilities for working with BRFSS dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\nAccessed via https://www.kaggle.com/datasets/cdc/behavioral-risk-factor-surveillance-system.\nRaw Data: https://www.cdc.gov/brfss/annual_data/annual_data.htm\nData Dictionary: https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf\n\"\"\"\n\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\nfrom tableshift.core.splitter import idx_where_not_in\n\n# Features present in every year of BRFSS\nBRFSS_GLOBAL_FEATURES = [\n    'CHCOCNCR', 'CHCSCNCR', 'CHECKUP1', 'CVDSTRK3', 'EDUCA', 'EMPLOY1',\n    'HIGH_BLOOD_PRESS', 'IYEAR', 'MARITAL', 'MEDCOST', 'MENTHLTH',\n    'PHYSHLTH', 'SEX', 'SMOKDAY2', 'SMOKE100', '_AGEG5YR',\n    '_BMI5', '_BMI5CAT', '_MICHD', '_PRACE1', '_RFBING5', '_STATE', '_TOTINDA']\n\n# While BRFSS exists for every year back several decades, feature alignment\n# is only implemented for these years due to \"rotating core\" features occurring\n# only every other year and other changes prior to 2015; see comments below.\nBRFSS_YEARS = (2015, 2017, 2019, 2021)\n\nBRFSS_STATE_LIST = [\n    '1.0', '10.0', '11.0', '12.0', '13.0', '15.0', '16.0', '17.0', '18.0',\n    '19.0', '2.0', '20.0', '21.0', '22.0', '23.0', '24.0', '25.0', '26.0',\n    '27.0', '28.0', '29.0', '30.0', '31.0', '32.0', '33.0', '34.0', '35.0',\n    '36.0', '37.0', '38.0', '39.0', '4.0', '40.0', '41.0', '42.0', '44.0',\n    '45.0', '46.0', '47.0', '48.0', '49.0', '5.0', '50.0', '51.0', '53.0',\n    '54.0', '55.0', '56.0', '6.0', '66.0', '72.0', '8.0', '9.0'\n]\n# Features shared across BRFSS prediction tasks.\nBRFSS_SHARED_FEATURES = FeatureList(features=[\n    # Derived feature for year.\n    Feature(\"IYEAR\", float, \"Year of BRFSS dataset.\",\n            name_extended=\"Survey year\"),\n    # ################ Demographics/sensitive attributes. ################\n    # Also see \"INCOME2\", \"MARITAL\", \"EDUCA\" features below.\n    Feature(\"STATE\", cat_dtype, \"\"\"State FIPS Code.\"\"\",\n            name_extended=\"State\",\n            value_mapping={\n                \"1.0\": 'Alabama',\n                \"4.0\": 'Arizona',\n                \"5.0\": 'Arkansas',\n                \"6.0\": 'California',\n                \"8.0\": 'Colorado',\n                \"9.0\": 'Connecticut',\n                \"10.0\": 'Delaware',\n                \"11.0\": 'District of Columbia',\n                \"12.0\": 'Florida',\n                \"13.0\": 'Georgia',\n                \"15.0\": 'Hawaii',\n                \"16.0\": 'Idaho',\n                \"17.0\": 'Illinois ',\n                \"18.0\": 'Indiana',\n                \"19.0\": 'Iowa',\n                \"20.0\": 'Kansas',\n                \"21.0\": 'Kentucky',\n                \"22.0\": 'Louisiana ',\n                \"23.0\": 'Maine',\n                \"24.0\": 'Maryland',\n                \"25.0\": 'Massachusetts',\n                \"26.0\": 'Michigan',\n                \"27.0\": 'Minnesota',\n                \"28.0\": 'Mississippi',\n                \"29.0\": 'Missouri',\n                \"30.0\": 'Montana',\n                \"31.0\": 'Nebraska',\n                \"32.0\": 'Nevada',\n                \"33.0\": 'New Hampshire',\n                \"34.0\": 'New Jersey',\n                \"35.0\": 'New Mexico',\n                \"36.0\": 'New York',\n                \"37.0\": 'North Carolina',\n                \"38.0\": 'North Dakota',\n                \"39.0\": 'Ohio',\n                \"40.0\": 'Oklahoma',\n                \"41.0\": 'Oregon',\n                \"42.0\": 'Pennsylvania',\n                \"44.0\": 'Rhode Island',\n                \"45.0\": 'South Carolina',\n                \"46.0\": 'South Dakota',\n                \"47.0\": 'Tennessee',\n                \"48.0\": 'Texas',\n                \"49.0\": 'Utah',\n                \"50.0\": 'Vermont',\n                \"51.0\": 'Virginia',\n                \"53.0\": 'Washington',\n                \"54.0\": 'West Virginia',\n                \"55.0\": 'Wisconsin',\n                \"56.0\": 'Wyoming',\n                \"66.0\": 'Guam',\n                \"72.0\": 'Puerto Rico'\n            }),\n    Feature(\"MEDCOST\", cat_dtype, \"\"\"Was there a time in the past 12 months \n    when you needed to see a doctor but could not because of cost?\"\"\",\n            name_extended=\"Answer to the question 'Was there a time in the \"\n                          \"past 12 months when you needed to see a doctor but \"\n                          \"could not because of cost?'\",\n            na_values=(7, 9),\n            value_mapping={\n                \"1.0\": \"Yes\", \"2.0\": \"No\", \"7.0\": \"Don't know/not sure\", \"9.0\": \"Refused\",\n            }),\n    # Preferred race category; note that ==1 is equivalent to\n    # \"White non-Hispanic race group\" variable _RACEG21\n    Feature(\"PRACE1\", float, \"\"\"Preferred race category.\"\"\",\n            name_extended=\"Preferred race category\",\n            na_values=(7., 8., 77., 99.),\n            value_mapping={\n                \"1.0\": 'White',\n                \"2.0\": 'Black or African American',\n                \"3.0\": 'American Indian or Alaskan Native',\n                \"4.0\": 'Asian', \"5.0\": 'Native Hawaiian or other Pacific Islander',\n                \"6.0\": 'Other race',\n                \"7.0\": 'No preferred race',\n                \"8.0\": 'Multiracial but preferred race not answered',\n                \"77.0\": 'Don’t know/Not sure', \"9.0\": 'refused', }),\n    Feature(\"SEX\", float, \"\"\"Indicate sex of respondent.\"\"\",\n            name_extended=\"Sex of respondent\",\n            value_mapping={1: \"Male\", 2: \"Female\"}),\n])\n\nBRFSS_DIET_FEATURES = [\n    Feature(\"FRUIT_ONCE_PER_DAY\", cat_dtype,\n            \"\"\"Consume Fruit 1 or more times per day\"\"\",\n            name_extended=\"Fruit consumption\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Consumed fruit one or more times per day',\n                \"2.0\": 'Consumed fruit less than one  time per day',\n                \"9.0\": \"Don't know, refused or missing values\"\n            }),\n    Feature(\"VEG_ONCE_PER_DAY\", cat_dtype,\n            \"\"\"Consume vegetables 1 or more times per day\"\"\",\n            name_extended=\"Vegetable consumption\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Consumed vegetables one or more times per day',\n                \"2.0\": 'Consumed vegetables less  than one time per day',\n                \"9.0\": \"Don't know, refused or missing values\",\n            }),\n]\n\nBRFSS_ALCOHOL_FEATURES = [\n    # Calculated total number of alcoholic beverages consumed per week\n    Feature(\"DRNK_PER_WEEK\", float,\n            \"\"\"Calculated total number of alcoholic beverages consumed per \n            week\"\"\",\n            name_extended=\"Total number of alcoholic beverages consumed per week\",\n            na_values=(99900,)),\n    Feature(\"RFBING5\", cat_dtype,\n            \"Binge drinkers (males having five or more drinks on one \"\n            \"occasion, females having four or more drinks on one occasion)\",\n            na_values=(9,),\n            value_mapping={\"1.0\": \"No\", \"2.0\": \"Yes\", \"9.0\": \"Don't know/Refused/Missing\"\n                           },\n            name_extended=\"Respondent is binge drinker\"),\n]\n\nBRFSS_SMOKE_FEATURES = [\n    # Have you smoked at least 100 cigarettes in your entire life?\n    Feature(\"SMOKE100\", cat_dtype,\n            \"Have you smoked at least 100 cigarettes in your entire life?\",\n            na_values=(7, 9),\n            value_mapping={\"1.0\": 'Yes', \"2.0\": 'No'},\n            name_extended=\"Answer to the question 'Have you smoked at least \"\n                          \"100 cigarettes in your entire life?'\"),\n\n    Feature(\"SMOKDAY2\", cat_dtype, \"Do you now smoke cigarettes every day, \"\n                                   \"some days, or not at all?\",\n            na_values=(7, 9),\n            value_mapping={\n                \"1.0\": 'Every day', \"2.0\": 'Some days', \"3.0\": 'Not at all',\n                \"7.0\": 'Don´t Know/Not Sure',\n                \"9.0\": 'Refused'\n            },\n            name_extended=\"Answer to the question 'Do you now smoke \"\n                          \"cigarettes every day, some days, or not at all?'\"),\n]\n\n# Brief feature descriptions below; for the full question/description\n# see the data dictionary linked above. Note that in the original data,\n# some of the feature names are preceded by underscores (these are\n# \"calculated variables\"; see data dictionary). These leading\n# underscores, where present, are removed in the preprocess_brfss() function\n# due to limitations on naming in the sklearn transformers module.\n\nBMI5CAT_FEATURE = Feature(\"BMI5CAT\", cat_dtype,\n                          \"Body Mass Index (BMI) category\",\n                          name_extended=\"Body Mass Index (BMI) category\",\n                          value_mapping={\n                              \"1.0\": 'Underweight (BMI < 1850)',\n                              \"2.0\": 'Normal Weight (1850 <= BMI < 2500)',\n                              \"3.0\": 'Overweight (2500 <= BMI < 3000)',\n                              \"4.0\": 'Obese (3000 <= BMI < 9999)'\n                          })\nPHYSICAL_ACTIVITY_FEATURE = Feature(\n    \"TOTINDA\", cat_dtype,\n    \"Adults who reported doing physical activity or exercise during \" \\\n    \"the past 30 days other than their regular job.\",\n    na_values=(9,), value_mapping={\n        \"1.0\": 'Had physical activity or exercise in last 30 days',\n        \"2.0\": 'No physical activity or exercise in last 30 days'},\n    name_extended=\"Physical activity or exercise during the past 30 \" \\\n                  \"days other than their regular job\")\nBRFSS_DIABETES_FEATURES = FeatureList([\n    ################ Target ################\n    Feature(\"DIABETES\", float,\n            '(Ever told) you have diabetes',\n            name_extended='(Ever told) you have diabetes',\n            is_target=True,\n            na_values=(7, 9),\n            # value_mapping={\n            #     1: 'Yes',\n            #     2: 'Yes but female told only during pregnancy',\n            #     3: 'No',\n            #     4: 'No, prediabetes or borderline diabetes',\n            #     7: 'Don’t know / Not Sure',\n            #     9: 'Refused'\n            # }\n            ),\n\n    # Below are a set of indicators for known risk factors for diabetes.\n    ################ General health ################\n    Feature(\"PHYSHLTH\", float,\n            \"For how many days during the past 30 days\"\n            \" was your physical health not good?\",\n            name_extended=\"Number of days during the past 30 days where \"\n                          \"physical health was not good\",\n            na_values=(77, 99),\n            note=\"\"\"Values: 1 - 30 Number of \n            days, 88 None, 77 Don’t know/Not sure, 99 Refused, BLANK Not \n            asked or Missing\"\"\"),\n    ################ High blood pressure ################\n\n    Feature(\"HIGH_BLOOD_PRESS\", cat_dtype, na_values=(9,),\n            description=\"Adults who have been told they have high blood \"\n                        \"pressure by a doctor, nurse, or other health \"\n                        \"professional.\",\n            name_extended=\"(Ever told) you have high blood pressure\",\n            value_mapping={\"1.0\": 'No', \"2.0\": 'Yes',\n                           \"9.0\": \" Don’t know/Not Sure/Refused/Missing\"}),\n    ################ High cholesterol ################\n    # Cholesterol check within past five years\n    Feature(\"CHOL_CHK_PAST_5_YEARS\", cat_dtype,\n            \"About how long has it been since you last\"\n            \" had your blood cholesterol checked?\",\n            name_extended=\"Time since last blooc cholesterol check\",\n            note=\"\"\"Aligned version of 'CHOLCHK*' features from 2015-2021; see \n            _align_chol_chk() below..\"\"\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Never',\n                \"2.0\": 'Within the past year (anytime less than 12 months ago)',\n                \"3.0\": 'Within the past 2 years (more than 1 year but less than 2 years ago)',\n                \"4.0\": 'Within the past 5 years (more than 2 years but less than 5 years ago)',\n                \"5.0\": '5 or more years ago', \"7.0\": \"Don’t know/Not Sure\", \"9.0\": 'Refused'\n            }),\n\n    Feature(\"TOLDHI\", cat_dtype,\n            \"\"\"Have you ever been told by a doctor, nurse or other health \n            professional that your blood cholesterol is high?\"\"\",\n            name_extended=\"Ever been told you have high blood cholesterol\",\n            na_values=(7, 9),\n            value_mapping={\n                \"1.0\": 'Yes', \"2.0\": 'No', \"7.0\": \"Don’t know/Not Sure\", \"9.0\": 'Refused',\n            }),\n    ################ BMI/Obesity ################\n    # Calculated Body Mass Index (BMI)\n    Feature(\"BMI5\", float, \"\"\"Computed Body Mass Index (BMI)\"\"\",\n            name_extended='Body Mass Index (BMI)',\n            note=\"\"\"Values: 1 - 9999 1 or greater - Notes: WTKG3/(HTM4*HTM4) \n            (Has 2 implied decimal places); BLANK: Don’t \n            know/Refused/Missing.\"\"\"),\n    # Four-categories of Body Mass Index (BMI)\n    BMI5CAT_FEATURE,\n    ################ Smoking ################\n    *BRFSS_SMOKE_FEATURES,\n    ################ Other chronic health conditions ################\n    Feature(\"CVDSTRK3\", cat_dtype,\n            \"\"\"Ever had a stroke, or been told you had a stroke\"\"\",\n            name_extended=\"Ever had a stroke, or been told you had a stroke\",\n            na_values=(7, 9),\n            value_mapping={\"1.0\": 'Yes', \"2.0\": 'No', \"7.0\": \"Don’t know/Not Sure\",\n                           \"9.0\": 'Refused', }),\n    Feature(\"MICHD\", cat_dtype, \"\"\"Question: Respondents that have ever \n    reported having coronary heart disease (CHD) or myocardial infarction ( \n    MI).\"\"\",\n            name_extended=\"Reports of coronary heart disease (CHD) or \"\n                          \"myocardial infarction (MI)\",\n            value_mapping={\n                \"1.0\": 'Reported having myocardial infarction or coronary heart '\n                   'disease',\n                \"2.0\": 'Did not report having myocardial infarction or coronary '\n                   'heart disease',\n            }),\n    ################ Diet ################\n    *BRFSS_DIET_FEATURES,\n    ################ Alcohol Consumption ################\n    *BRFSS_ALCOHOL_FEATURES,\n    ################ Exercise ################\n    PHYSICAL_ACTIVITY_FEATURE,\n    ################ Household income ################\n    Feature(\"INCOME\", cat_dtype,\n            \"\"\"Annual household income from all sources\"\"\",\n            name_extended=\"Annual household income from all sources\",\n            na_values=(77, 99),\n            value_mapping={\n                \"1.0\": 'Less than $10,000',\n                \"2.0\": 'Less than $15,000 ($10,000 to less than $15,000)',\n                \"3.0\": 'Less than $20,000 ($15,000 to less than $20,000)',\n                \"4.0\": 'Less than $25,000 ($20,000 to less than $25,000)',\n                \"5.0\": 'Less than $35,000 ($25,000 to less than $35,000)',\n                \"6.0\": 'Less than $50,000 ($35,000 to less than $50,000)',\n                \"7.0\": 'Less than $75, 000 ($50,000 to less than $75,000)',\n                \"8.0\": '$75,000 or more (BRFSS 2015-2019) or less than $100,'\n                   '000 ($75,000 to < $100,000) (BRFSS 2021)',\n                \"9.0\": 'Less than $150,000 ($100,000 to < $150,000)',\n                \"10.0\": 'Less than $200,000 ($150,000 to < $200,000)',\n                \"11.0\": '$200,000  or more',\n                \"77.0\": 'Don’t know/Not sure', \"99.0\": 'Refused',\n            }),\n    ################ Marital status ################\n    Feature(\"MARITAL\", cat_dtype,\n            \"Marital status\",\n            name_extended=\"Marital status\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Married', \"2.0\": 'Divorced',\n                \"3.0\": 'Widowed', \"4.0\": 'Separated', \"5.0\": 'Never married',\n                \"6.0\": 'A member of an unmarried couple', \"9.0\": 'Refused'\n            }),\n    ################ Time since last checkup\n    # About how long has it been since you last visited a\n    # doctor for a routine checkup?\n    Feature(\"CHECKUP1\", cat_dtype,\n            \"\"\"Time since last visit to the doctor for a checkup\"\"\",\n            name_extended=\"Time since last visit to the doctor for a checkup\",\n            na_values=(7, 9),\n            value_mapping={\n                \"1.0\": 'Within past year (anytime < 12 months ago)',\n                \"2.0\": 'Within past 2 years (1 year but < 2 years ago)',\n                \"3.0\": 'Within past 5 years (2 years but < 5 years ago)',\n                \"4.0\": '5 or more years ago',\n                \"7.0\": \"Don’t know/Not sure\", \"8.0\": 'Never', \"9.0\": 'Refuse'},\n            note=\"\"\"Question: About how long has it been since you last \n            visited a doctor for a routine checkup? [A routine checkup is a \n            general physical exam, not an exam for a specific injury, \n            illness, or condition.] \"\"\"\n            ),\n    ################ Education ################\n    # highest grade or year of school completed\n    Feature(\"EDUCA\", cat_dtype,\n            \"Highest grade or year of school completed\",\n            name_extended=\"Highest grade or year of school completed\",\n            note=\"\"\"Question: What is the highest grade or year of school you \n            completed?\"\"\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Never attended school or only kindergarten',\n                \"2.0\": 'Grades 1 through 8 (Elementary)',\n                \"3.0\": 'Grades 9 through 11 (Some high school)',\n                \"4.0\": 'Grade 12 or GED (High school graduate)',\n                \"5.0\": 'College 1 year to 3 years (Some college or technical school)',\n                \"6.0\": 'College 4 years or more (College graduate)', \"9.0\": 'Refused'\n            }),\n    ################ Health care coverage ################\n    # Note: we keep missing values (=9) for this column since they are grouped\n    # with respondents aged over 64; otherwise dropping the observations\n    # with this value would exclude all respondents over 64.\n    Feature(\"HEALTH_COV\", cat_dtype,\n            \"Respondents aged 18-64 who have any form of health care coverage\",\n            name_extended='Current health care coverage',\n            value_mapping={\n                \"1.0\": 'Have health care coverage',\n                \"2.0\": 'Do not have health care coverage',\n                \"9.0\": \"Not aged 18-64, Don’t know/Not Sure, Refused or Missing\"\n            }),\n    ################ Mental health ################\n    # for how many days during the past 30\n    # days was your mental health not good?\n    Feature(\"MENTHLTH\", float,\n            \"\"\"Now thinking about your mental health, which includes stress, \n            depression, and problems with emotions, for how many days during \n            the past 30 days was your mental health not good?\"\"\",\n            name_extended=\"Answer to the question 'for how many days during \"\n                          \"the past 30 days was your mental health not good?'\",\n            na_values=(77, 99),\n            note=\"\"\"Values: 1 - 30: Number of days, 88: None, 77: Don’t \n            know/Not sure, 99: Refused.\"\"\"),\n]) + BRFSS_SHARED_FEATURES\n\nBRFSS_BLOOD_PRESSURE_FEATURES = FeatureList(features=[\n    Feature(\"HIGH_BLOOD_PRESS\", int,\n            \"\"\"Have you ever been told by a doctor, nurse or other health \n            professional that you have high blood pressure? 0: No. 1: Yes. 8: \n            Don’t know/Not Sure/Refused/Missing (note: we subtract 1 from \n            original codebook values at preprocessing to create a binary \n            target variable).\"\"\",\n            is_target=True),\n\n    # Indicators for high blood pressure; see\n    # https://www.nhlbi.nih.gov/health/high-blood-pressure/causes\n    ################ BMI/Obesity ################\n    # Four-categories of Body Mass Index (BMI)\n    BMI5CAT_FEATURE,\n    ################ Age ################\n    Feature(\"AGEG5YR\", float, \"\"\"Fourteen-level age category\"\"\",\n            na_values=(14,),\n            name_extended=\"Age group\",\n            value_mapping={\n                \"1.0\": 'Age 18 to 24', \"2.0\": 'Age 25 to 29', \"3.0\": ' Age 30 to 34',\n                \"4.0\": 'Age 35 to 39',\n                \"5.0\": 'Age 40 to 44', \"6.0\": 'Age 45 to 49', \"7.0\": 'Age 50 to 54',\n                \"8.0\": 'Age 55 to 59', \"9.0\": 'Age 60 to 64',\n                \"10.0\": 'Age 65 to 69', \"11.0\": 'Age 70 to 74',\n                \"12.0\": 'Age 75 to 79', \"13.0\": 'Age 80 or older',\n                \"14.0\": 'Don’t know/Refused/Missing'}),\n    ################ Family history and genetics ################\n    # No questions related to this risk factor.\n    ################ Lifestyle habits ################\n    *BRFSS_DIET_FEATURES,\n    *BRFSS_ALCOHOL_FEATURES,\n    PHYSICAL_ACTIVITY_FEATURE,\n    *BRFSS_SMOKE_FEATURES,\n    ################ Medicines ################\n    # No questions related to this risk factor.\n    ################ Other medical conditions ################\n    Feature(\"CHCSCNCR\", cat_dtype,\n            \"Have skin cancer or ever told you have skin cancer\",\n            name_extended=\"Have skin cancer or ever told you have skin cancer\",\n            na_values=(7, 9),\n            value_mapping={\"1.0\": 'Yes', \"2.0\": 'No', \"7.0\": \"Don’t know/Not Sure\",\n                           \"9.0\": 'Refused'}),\n    Feature(\"CHCOCNCR\", cat_dtype,\n            \"Have any other types of cancer or ever told you have any other \"\n            \"types of cancer\",\n            name_extended=\"Have any other types of cancer or ever told you \"\n                          \"have any other types of cancer\",\n            na_values=(7, 9),\n            value_mapping={\"1.0\": 'Yes', \"2.0\": 'No', \"7.0\": \"Don’t know/Not Sure\",\n                           \"9.0\": 'Refused', }),\n    # 6 in 10 people suffering from diabetes also have high BP\n    # source: https://www.cdc.gov/bloodpressure/risk_factors.htm\n    Feature(\"DIABETES\", float,\n            \"Have diabetes or ever been told you have diabetes\",\n            name_extended=\"Have diabetes or ever been told you have diabetes\",\n            na_values=(7, 9),\n            value_mapping={\n                \"1.0\": 'Yes', \"2.0\": 'Yes, but female told only during pregnancy',\n                \"3.0\": 'No',\n                \"4.0\": 'No, pre-diabetes or borderline diabetes',\n                \"7.0\": \"Don’t know/Not Sure\",\n                \"9.0\": \"Refused, BLANK Not asked or Missing\"\n            }),\n\n    ################ Race/ethnicity ################\n    # Covered in BRFSS_SHARED_FEATURES.\n    ################ Sex ################\n    # Covered in BRFSS_SHARED_FEATURES.\n    ################ Social and economic factors ################\n    # Income\n    Feature(\"POVERTY\", int,\n            description=\"Binary indicator for whether an individuals' income \"\n                        \"falls below the 2021 poverty guideline for family of \"\n                        \"four.\",\n            name_extended=\"Binary indicator for whether an individuals' income \"\n                          \"falls below the 2021 poverty guideline for family of\"\n                          \" four\",\n            value_mapping={\"1.0\": \"Yes\", \"0.0\": \"No\"}),\n    # Type job status; related to early/late shifts which is a risk factor.\n    Feature(\"EMPLOY1\", cat_dtype, \"\"\"Current employment\"\"\",\n            name_extended=\"Current employment status\",\n            na_values=(9,),\n            value_mapping={\n                \"1.0\": 'Employed for wages', \"2.0\": 'Self-employed',\n                \"3.0\": 'Out of work for 1 year or more',\n                \"4.0\": 'Out of work for less than 1 year', \"5.0\": 'A homemaker',\n                \"6.0\": 'A student',\n                \"7.0\": 'Retired', \"8.0\": 'Unable to work', \"9.0\": 'Refused'\n            }),\n    # Additional relevant features in BRFSS_SHARED_FEATURES.\n]) + BRFSS_SHARED_FEATURES\n\n# Some features have different names over years, due to changes in prompts or\n# interviewer instructions. Here we map these different names to a single shared\n# name that is consistent across years.\nBRFSS_CROSS_YEAR_FEATURE_MAPPING = {\n    # Question: Consume Fruit 1 or more times per day\n    \"FRUIT_ONCE_PER_DAY\": (\n        \"_FRTLT1\",  # 2013, 2015\n        \"_FRTLT1A\",  # 2017, 2019, 2021\n    ),\n    # Question: Consume Vegetables 1 or more times per day\n    \"VEG_ONCE_PER_DAY\": (\n        \"_VEGLT1\",  # 2013, 2015\n        \"_VEGLT1A\",  # 2017, 2019, 2021\n    ),\n    # Question: Cholesterol check within past five years (calculated)\n    \"CHOL_CHK_PAST_5_YEARS\": (\n        \"_CHOLCHK\",  # 2013, 2015\n        \"_CHOLCH1\",  # 2017\n        \"_CHOLCH2\",  # 2019\n        \"_CHOLCH3\",  # 2021\n    ),\n    # Question: (Ever told) you have diabetes (If ´Yes´ and respondent is\n    # female, ask ´Was this only when you were pregnant?´. If Respondent\n    # says pre-diabetes or borderline diabetes, use response code 4.)\n    \"DIABETES\": (\n        \"DIABETE3\",  # 2013, 2015, 2017\n        \"DIABETE4\",  # 2019, 2021\n    ),\n    # Question:  Calculated total number of alcoholic beverages consumed\n    # per week\n    \"DRNK_PER_WEEK\": (\n        \"_DRNKWEK\",  # 2015, 2017\n        \"_DRNKWK1\",  # 2019, 2021\n    ),\n    # Question: Indicate sex of respondent.\n    \"SEX\": (\n        \"SEX\",  # 2015, 2017\n        \"SEXVAR\",  # 2019\n    ),\n    # Question: Was there a time in the past 12 months when you needed to\n    # see a doctor but could not because {2015-2019: of cost/ 2021: you\n    # could not afford it}?\n    \"MEDCOST\": (\n        \"MEDCOST\",  # 2015, 2017, 2019\n        \"MEDCOST1\",  # 2021\n    ),\n    # Question: Is your annual household income from all sources: (If\n    # respondent refuses at any income level, code ´Refused.´) Note:\n    # higher levels/new codes added in 2021.\n    \"INCOME\": (\n        \"INCOME2\",  # 2015, 2017, 2019\n        \"INCOME3\",  # 2021\n    ),\n    # Question: Adults who have been told they have high blood pressure by a\n    # doctor, nurse, or other health professional\n    \"HIGH_BLOOD_PRESS\": (\n        \"_RFHYPE5\",  # 2015, 2017, 2019\n        \"_RFHYPE6\",  # 2021\n    ),\n    # Question: Respondents aged 18-64 who have any form of health insurance\n    \"HEALTH_COV\": (\n        \"_HCVU651\",  # 2015, 2017, 2019\n        \"_HCVU652\",  # 2021\n    ),\n    # Question: Have you ever been told by a doctor, nurse or other\n    # health professional that your (TOLDHI2: blood) cholesterol is high?\n    \"TOLDHI\": (\n        \"TOLDHI2\",\n        \"TOLDHI3\"  # 2021\n    )\n}\n\n# Raw names of the input features used in BRFSS. Useful to\n# subset before preprocessing, since some features contain near-duplicate\n# versions (i.e. calculated and not-calculated versions, differing only by a\n# precending underscore).\n_BRFSS_INPUT_FEATURES = list(\n    set(BRFSS_GLOBAL_FEATURES +\n        list(BRFSS_CROSS_YEAR_FEATURE_MAPPING.keys())))\n\n\ndef align_brfss_features(df: pd.DataFrame):\n    \"\"\"Map BRFSS column names to a consistent format over years.\n\n    Some questions are asked over years, but while the options are identical,\n    the value labels change (specifically, the interviewing instructions change,\n    e.g. \"Refused—Go to Section 06.01 CHOLCHK3\" for BPHIGH6 in 2021 survey\n    https://www.cdc.gov/brfss/annual_data/2021/pdf/codebook21_llcp-v2-508.pdf\n    vs. \"Refused—Go to Section 05.01 CHOLCHK1\" in 2017 survey\n    https://www.cdc.gov/brfss/annual_data/2017/pdf/codebook17_llcp-v2-508.pdf\n    despite identical questions, and values.\n\n    This function addresses these different names by mapping a set of possible\n    variable names for the same question, over survey years, to a single shared\n    name.\n    \"\"\"\n\n    for outname, input_names in BRFSS_CROSS_YEAR_FEATURE_MAPPING.items():\n        assert len(set(df.columns).intersection(set(input_names))), \\\n            f\"none of {input_names} detected in dataframe with \" \\\n            f\"columns {sorted(df.columns)}\"\n\n        df.rename(columns={old: outname for old in input_names}, inplace=True)\n        assert outname in df.columns\n\n    # IYEAR is poorly coded, as e.g. \"b'2015'\"; here we parse it back to int.\n    df[\"IYEAR\"] = df[\"IYEAR\"].apply(\n        lambda x: re.search(\"\\d+\", x).group()).astype(int)\n\n    # CHOLCHK values are coded inconsistently in 2015 vs. post-2015 surveys;\n    # we align them here.\n    # (See https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf)\n    def _align_chol_chk(row):\n        \"\"\"Utility function to code 2015 BRFSS CHOLCHK to match post-2015.\"\"\"\n        if row[\"IYEAR\"] == 2015 and \\\n                row[\"CHOL_CHK_PAST_5_YEARS\"] in (1, 2, 3, 4):\n            return row[\"CHOL_CHK_PAST_5_YEARS\"] + 1\n        else:\n            return row[\"CHOL_CHK_PAST_5_YEARS\"]\n\n    df[\"CHOL_CHK_PAST_5_YEARS\"] = df.apply(_align_chol_chk, axis=1)\n    return df\n\n\ndef brfss_shared_preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Shared preprocessing function for BRFSS data tasks.\"\"\"\n    df = df[_BRFSS_INPUT_FEATURES]\n\n    # Sensitive columns\n    # df[\"_PRACE1\"] = (df[\"_PRACE1\"] == 1).astype(int)\n    df[\"SEX\"] = (df[\"SEX\"] - 1).astype(int)  # Map sex to male=0, female=1\n\n    # PHYSHLTH, POORHLTH, MENTHLTH are measured in days, but need to\n    # map 88 to 0 because it means zero (i.e. zero bad health days)\n    df[\"PHYSHLTH\"] = df[\"PHYSHLTH\"].replace({88: 0})\n    df[\"MENTHLTH\"] = df[\"MENTHLTH\"].replace({88: 0})\n\n    # Drop rows where drinks per week is unknown/refused/missing;\n    # this uses a different missingness code from other variables.\n    df = df[~(df[\"DRNK_PER_WEEK\"] == 99900)]\n\n    # Some questions are not asked for various reasons\n    # (see notes under \"BLANK\" for that question in data dictionary);\n    # create an indicator for these due to large fraction of missingness.\n    df[\"SMOKDAY2\"] = df[\"SMOKDAY2\"].fillna(\"NOTASKED_MISSING\").astype(str)\n    df[\"TOLDHI\"] = df[\"TOLDHI\"].fillna(\"NOTASKED_MISSING\").astype(str)\n\n    # Remove leading underscores from column names\n    renames = {c: re.sub(\"^_\", \"\", c) for c in df.columns if c.startswith(\"_\")}\n    df.rename(columns=renames, inplace=True)\n\n    return df\n\n\ndef preprocess_brfss_diabetes(df: pd.DataFrame):\n    df = brfss_shared_preprocessing(df)\n\n    df[\"DIABETES\"].replace({2: 0, 3: 0, 4: 0}, inplace=True)\n    # na_values have not been mapped yet; need to access and use these\n    df = df[~df[\"DIABETES\"].isin(\n        BRFSS_DIABETES_FEATURES.target_feature.na_values)]\n    df.dropna(subset=[\"DIABETES\"], inplace=True)\n\n    # Reset the index after preprocessing to ensure splitting happens\n    # correctly (splitting assumes sequential indexing).\n    return df.reset_index(drop=True)\n\n\ndef preprocess_brfss_blood_pressure(df: pd.DataFrame) -> pd.DataFrame:\n    df = brfss_shared_preprocessing(df)\n\n    df[\"HIGH_BLOOD_PRESS\"] = df[\"HIGH_BLOOD_PRESS\"].replace(9, np.nan) - 1\n    df.dropna(subset=[\"HIGH_BLOOD_PRESS\"], inplace=True)\n\n    # Retain samples only 50+ years of age (to focus on highest-risk groups\n    # for high BP; see:\n    # * Vasan RS, Beiser A, Seshadri S, Larson MG, Kannel WB, D’ Agostino RB,\n    # et al. Residual lifetime risk for developing hypertension in middle-aged\n    # women and men: the Framingham Heart Studyexternal icon. JAMA. 2002;287(\n    # 8):1003–1010. (\"the risk for developing hypertension increases markedly\n    # during and after the sixth decade of life\") and also:\n    # * Dannenberg AL,  Garrison RJ, Kannel WB. Incidence of hypertension in the\n    # Framingham Study. Am J Public Health.1988;78:676-679.\n\n    df = df[df[\"AGEG5YR\"] >= 7]\n\n    # Create a binary indicator for poverty. This is based on the 2021 US\n    # poverty income guideline for a family of 4, which was $26,\n    # 500. https://aspe.hhs.gov/topics/poverty-economic-mobility/poverty\n    # -guidelines/prior-hhs-poverty\n    # -guidelines-federal-register-references/2021-poverty-guidelines\n    # #threshholds Note that we actually use a slightly lower threshold of\n    # 25,000 due to the response coding in BRFSS.\n\n    # Drop unknown/not responded income levels; otherwise comparison with nan\n    # values always returns False.\n    idxs = idx_where_not_in(df[\"INCOME\"], (77, 99))\n    df = df.iloc[idxs]\n    df[\"POVERTY\"] = (df[\"INCOME\"] <= 4).astype(int)\n    df.drop(columns=[\"INCOME\"], inplace=True)\n\n    # Reset the index after preprocessing to ensure splitting happens\n    # correctly (splitting assumes sequential indexing).\n    return df.reset_index(drop=True)\n\n\ndef preprocess_brfss(df, task: str):\n    assert task in (\"diabetes\", \"blood_pressure\")\n    if task == \"diabetes\":\n        return preprocess_brfss_diabetes(df)\n    elif task == \"blood_pressure\":\n        return preprocess_brfss_blood_pressure(df)\n    else:\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "tableshift/configs/experiment_defaults.py", "content": "DEFAULT_ID_TEST_SIZE = 0.1\nDEFAULT_OOD_VAL_SIZE = 0.1\nDEFAULT_ID_VAL_SIZE = 0.1\nDEFAULT_RANDOM_STATE = 264738\n"}
{"type": "source_file", "path": "tableshift/datasets/acs_occp_mapping.py", "content": "# Each OCCP code is mapped to a tuple. The first element of the tuple is the\n# 'coarse' mapping corresponding to that key, and the second is the 'fine'\n# mapping corresponding to the key. These values are obtained by parsing the\n# values in the coding manual for the 'OCCP' feature from the\n# codebook at https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2014-2018.pdf\nACS_OCCP_CODE_MAPPING = {\n    0000.0: ('N/A', 'N/A (less than 16 years old/NILF who last worked more than .5 years ago or never worked'),\n    0010.0: ('MGR', 'Chief Executives And Legislators'),\n    0020.0: ('MGR', 'General And Operations Managers'),\n    0040.0: ('MGR', 'Advertising And Promotions Managers'),\n    0051.0: ('MGR', 'Marketing Managers'),\n    0052.0: ('MGR', 'Sales Managers'),\n    0060.0: ('MGR', 'Public Relations And Fundraising Managers'),\n    0101.0: ('MGR', 'Administrative Services Managers'),\n    0102.0: ('MGR', 'Facilities Managers'),\n    0110.0: ('MGR', 'Computer And Information Systems Managers'),\n    0120.0: ('MGR', 'Financial Managers'),\n    0135.0: ('MGR', 'Compensation And Benefits Managers'),\n    0136.0: ('MGR', 'Human Resources Managers'),\n    0137.0: ('MGR', 'Training And Development Managers'),\n    0140.0: ('MGR', 'Industrial Production Managers'),\n    0150.0: ('MGR', 'Purchasing Managers'),\n    0160.0: ('MGR', 'Transportation, Storage, And Distribution Managers'),\n    0205.0: ('MGR', 'Farmers, Ranchers, And Other Agricultural Managers'),\n    0220.0: ('MGR', 'Construction Managers'),\n    0230.0: ('MGR', 'Education And Childcare Administrators'),\n    0300.0: ('MGR', 'Architectural And Engineering Managers'),\n    0310.0: ('MGR', 'Food Service Managers'),\n    0335.0: ('MGR', 'Entertainment and Recreation Managers'),\n    0340.0: ('MGR', 'Lodging Managers'),\n    0350.0: ('MGR', 'Medical And Health Services Managers'),\n    0360.0: ('MGR', 'Natural Sciences Managers'),\n    0410.0: ('MGR',\n             'Property, Real Estate, And Community Association Managers'),\n    0420.0: ('MGR', 'Social And Community Service Managers'),\n    0425.0: ('MGR', 'Emergency Management Directors'),\n    0440.0: ('MGR', 'Other Managers'),\n    0500.0: ('BUS',\n             'Agents And Business Managers Of Artists, Performers, And Athletes'),\n    0510.0: ('BUS', 'Buyers And Purchasing Agents, Farm Products'),\n    0520.0: ('BUS', 'Wholesale And Retail Buyers, Except Farm Products'),\n    0530.0: ('BUS',\n             'Purchasing Agents, Except Wholesale, Retail, And Farm Products'),\n    0540.0: ('BUS',\n             'Claims Adjusters, Appraisers, Examiners, And Investigators'),\n    0565.0: ('BUS', 'Compliance Officers'),\n    0600.0: ('BUS', 'Cost Estimators'),\n    0630.0: ('BUS', 'Human Resources Workers'),\n    0640.0: ('BUS', 'Compensation, Benefits, And Job Analysis Specialists'),\n    0650.0: ('BUS', 'Training And Development Specialists'),\n    0700.0: ('BUS', 'Logisticians'),\n    0705.0: ('BUS', 'Project Management Specialists'),\n    0710.0: ('BUS', 'Management Analysts'),\n    0725.0: ('BUS', 'Meeting, Convention, And Event Planners'),\n    0726.0: ('BUS', 'Fundraisers'),\n    0735.0: ('BUS', 'Market Research Analysts And Marketing Specialists'),\n    0750.0: ('BUS', 'Business Operations Specialists, All Other'),\n    0800.0: ('FIN', 'Accountants And Auditors'),\n    0810.0: ('FIN', 'Property Appraisers and Assessors'),\n    0820.0: ('FIN', 'Budget Analysts'),\n    0830.0: ('FIN', 'Credit Analysts'),\n    0845.0: ('FIN', 'Financial And Investment Analysts'),\n    0850.0: ('FIN', 'Personal Financial Advisors'),\n    0860.0: ('FIN', 'Insurance Underwriters'),\n    0900.0: ('FIN', 'Financial Examiners'),\n    0910.0: ('FIN', 'Credit Counselors And Loan Officers'),\n    0930.0: ('FIN', 'Tax Examiners And Collectors, And Revenue Agents'),\n    0940.0: ('FIN', 'Tax Preparers'),\n    0960.0: ('FIN', 'Other Financial Specialists'),\n    1005.0: ('CMM', 'Computer And Information Research Scientists'),\n    1006.0: ('CMM', 'Computer Systems Analysts'),\n    1007.0: ('CMM', 'Information Security Analysts'),\n    1010.0: ('CMM', 'Computer Programmers'),\n    1021.0: ('CMM', 'Software Developers'),\n    1022.0: ('CMM', 'Software Quality Assurance Analysts and Testers'),\n    1031.0: ('CMM', 'Web Developers'),\n    1032.0: ('CMM', 'Web And Digital Interface Designers'),\n    1050.0: ('CMM', 'Computer Support Specialists'),\n    1065.0: ('CMM', 'Database Administrators and Architects'),\n    1105.0: ('CMM', 'Network And Computer Systems Administrators'),\n    1106.0: ('CMM', 'Computer Network Architects'),\n    1108.0: ('CMM', 'Computer Occupations, All Other'),\n    1200.0: ('CMM', 'Actuaries'),\n    1220.0: ('CMM', 'Operations Research Analysts'),\n    1240.0: ('CMM', 'Other Mathematical Science Occupations'),\n    1305.0: ('ENG', 'Architects, Except Landscape And Naval'),\n    1306.0: ('ENG', 'Landscape Architects'),\n    1310.0: ('ENG', 'Surveyors, Cartographers, And Photogrammetrists'),\n    1320.0: ('ENG', 'Aerospace Engineers'),\n    1340.0: ('ENG', 'Biomedical And Agricultural Engineers'),\n    1350.0: ('ENG', 'Chemical Engineers'),\n    1360.0: ('ENG', 'Civil Engineers'),\n    1400.0: ('ENG', 'Computer Hardware Engineers'),\n    1410.0: ('ENG', 'Electrical And Electronics Engineers'),\n    1420.0: ('ENG', 'Environmental Engineers'),\n    1430.0: ('ENG', 'Industrial Engineers, Including Health And Safety'),\n    1440.0: ('ENG', 'Marine Engineers And Naval Architects'),\n    1450.0: ('ENG', 'Materials Engineers'),\n    1460.0: ('ENG', 'Mechanical Engineers'),\n    1520.0: ('ENG',\n             'Petroleum, Mining And Geological Engineers, Including Mining Safety Engineers'),\n    1530.0: ('ENG', 'Other Engineers'),\n    1541.0: ('ENG', 'Architectural And Civil Drafters'),\n    1545.0: ('ENG', 'Other Drafters'),\n    1551.0: ('ENG',\n             'Electrical And Electronic Engineering Technologists and Technicians'),\n    1555.0: (\n        'Other Engineering Technologists And Technicians, Except Drafters'),\n    1560.0: ('ENG', 'Surveying And Mapping Technicians'),\n    1600.0: ('SCI', 'Agricultural And Food Scientists'),\n    1610.0: ('SCI', 'Biological Scientists'),\n    1640.0: ('SCI', 'Conservation Scientists And Foresters'),\n    1650.0: ('SCI', 'Other Life Scientists'),\n    1700.0: ('SCI', 'Astronomers And Physicists'),\n    1710.0: ('SCI', 'Atmospheric And Space Scientists'),\n    1720.0: ('SCI', 'Chemists And Materials Scientists'),\n    1745.0: ('SCI',\n             'Environmental Scientists And Specialists, Including Health'),\n    1750.0: ('SCI', 'Geoscientists And Hydrologists, Except Geographers'),\n    1760.0: ('SCI', 'Physical Scientists, All Other'),\n    1800.0: ('SCI', 'Economists'),\n    1821.0: ('SCI', 'Clinical And Counseling Psychologists'),\n    1822.0: ('SCI', 'School Psychologists'),\n    1825.0: ('SCI', 'Other Psychologists'),\n    1840.0: ('SCI', 'Urban And Regional Planners'),\n    1860.0: ('SCI', 'Other Social Scientists'),\n    1900.0: ('SCI', 'Agricultural And Food Science Technicians'),\n    1910.0: ('SCI', 'Biological Technicians'),\n    1920.0: ('SCI', 'Chemical Technicians'),\n    1935.0: ('SCI',\n             'Environmental Science and Geoscience Technicians, And Nuclear Technicians'),\n    1970.0: ('SCI', 'Other Life, Physical, And Social Science Technicians'),\n    1980.0: ('SCI',\n             'Occupational Health And Safety Specialists and Technicians'),\n    2001.0: ('CMS', 'Substance Abuse And Behavioral Disorder Counselors'),\n    2002.0: ('CMS',\n             'Educational, Guidance, And Career Counselors And Advisors'),\n    2003.0: ('CMS', 'Marriage And Family Therapists'),\n    2004.0: ('CMS', 'Mental Health Counselors'),\n    2005.0: ('CMS', 'Rehabilitation Counselors'),\n    2006.0: ('CMS', 'Counselors, All Other'),\n    2011.0: ('CMS', 'Child, Family, And School Social Workers'),\n    2012.0: ('CMS', 'Healthcare Social Workers'),\n    2013.0: ('CMS', 'Mental Health And Substance Abuse Social Workers'),\n    2014.0: ('CMS', 'Social Workers, All Other'),\n    2015.0: ('CMS',\n             'Probation Officers And Correctional Treatment Specialists'),\n    2016.0: ('CMS', 'Social And Human Service Assistants'),\n    2025.0: ('CMS', 'Other Community and Social Service Specialists'),\n    2040.0: ('CMS', 'Clergy'),\n    2050.0: ('CMS', 'Directors, Religious Activities And Education'),\n    2060.0: ('CMS', 'Religious Workers, All Other'),\n    2100.0: ('LGL',\n             'Lawyers, And Judges, Magistrates, And Other Judicial Workers'),\n    2105.0: ('LGL', 'Judicial Law Clerks'),\n    2145.0: ('LGL', 'Paralegals And Legal Assistants'),\n    2170.0: ('LGL', 'Title Examiners, Abstractors, and Searchers'),\n    2180.0: ('LGL', 'Legal Support Workers, All Other'),\n    2205.0: ('EDU', 'Postsecondary Teachers'),\n    2300.0: ('EDU', 'Preschool And Kindergarten Teachers'),\n    2310.0: ('EDU', 'Elementary And Middle School Teachers'),\n    2320.0: ('EDU', 'Secondary School Teachers'),\n    2330.0: ('EDU', 'Special Education Teachers'),\n    2350.0: ('EDU', 'Tutors'),\n    2360.0: ('EDU', 'Other Teachers and Instructors'),\n    2400.0: ('EDU', 'Archivists, Curators, And Museum Technicians'),\n    2435.0: ('EDU', 'Librarians And Media Collections Specialists'),\n    2440.0: ('EDU', 'Library Technicians'),\n    2545.0: ('EDU', 'Teaching Assistants'),\n    2555.0: ('EDU', 'Other Educational Instruction And Library Workers'),\n    2600.0: ('ENT', 'Artists And Related Workers'),\n    2631.0: ('ENT', 'Commercial And Industrial Designers'),\n    2632.0: ('ENT', 'Fashion Designers'),\n    2633.0: ('ENT', 'Floral Designers'),\n    2634.0: ('ENT', 'Graphic Designers'),\n    2635.0: ('ENT', 'Interior Designers'),\n    2636.0: ('ENT', 'Merchandise Displayers And Windows Trimmers'),\n    2640.0: ('ENT', 'Other Designers'),\n    2700.0: ('ENT', 'Actors'),\n    2710.0: ('ENT', 'Producers And Directors'),\n    2721.0: ('ENT', 'Athletes and Sports Competitors'),\n    2722.0: ('ENT', 'Coaches and Scouts'),\n    2723.0: ('ENT', 'Umpires, Referees, And Other Sports Officials'),\n    2740.0: ('ENT', 'Dancers And Choreographers'),\n    2751.0: ('ENT', 'Music Directors and Composers'),\n    2752.0: ('ENT', 'Musicians and Singers'),\n    2755.0: ('ENT', 'Disc Jockeys, Except Radio'),\n    2770.0: ('ENT',\n             'Entertainers And Performers, Sports and Related Workers, All Other'),\n    2805.0: ('ENT', 'Broadcast Announcers And Radio Disc Jockeys'),\n    2810.0: ('ENT', 'News Analysts, Reporters And Correspondents'),\n    2825.0: ('ENT', 'Public Relations Specialists'),\n    2830.0: ('ENT', 'Editors'),\n    2840.0: ('ENT', 'Technical Writers'),\n    2850.0: ('ENT', 'Writers And Authors'),\n    2861.0: ('ENT', 'Interpreters and Translators'),\n    2862.0: ('ENT', 'Court Reporters and Simultaneous Captioners'),\n    2865.0: ('ENT', 'Media And Communication Workers, All Other'),\n    2905.0: ('ENT', 'Other Media And Communication Equipment Workers'),\n    2910.0: ('ENT', 'Photographers'),\n    2920.0: ('ENT',\n             'Television, Video, And Motion Picture Camera Operators And Editors'),\n    3000.0: ('MED', 'Chiropractors'),\n    3010.0: ('MED', 'Dentists'),\n    3030.0: ('MED', 'Dietitians And Nutritionists'),\n    3040.0: ('MED', 'Optometrists'),\n    3050.0: ('MED', 'Pharmacists'),\n    3090.0: ('MED', 'Physicians'),\n    3100.0: ('MED', 'Surgeons'),\n    3110.0: ('MED', 'Physician Assistants'),\n    3120.0: ('MED', 'Podiatrists'),\n    3140.0: ('MED', 'Audiologists'),\n    3150.0: ('MED', 'Occupational Therapists'),\n    3160.0: ('MED', 'Physical Therapists'),\n    3200.0: ('MED', 'Radiation Therapists'),\n    3210.0: ('MED', 'Recreational Therapists'),\n    3220.0: ('MED', 'Respiratory Therapists'),\n    3230.0: ('MED', 'Speech-Language Pathologists'),\n    3245.0: ('MED', 'Other Therapists'),\n    3250.0: ('MED', 'Veterinarians'),\n    3255.0: ('MED', 'Registered Nurses'),\n    3256.0: ('MED', 'Nurse Anesthetists'),\n    3258.0: ('MED', 'Nurse Practitioners, And Nurse Midwives'),\n    3261.0: ('MED', 'Acupuncturists'),\n    3270.0: ('MED',\n             'Healthcare Diagnosing Or Treating Practitioners, All Other'),\n    3300.0: ('MED', 'Clinical Laboratory Technologists And Technicians'),\n    3310.0: ('MED', 'Dental Hygienists'),\n    3321.0: ('MED', 'Cardiovascular Technologists and Technicians'),\n    3322.0: ('MED', 'Diagnostic Medical Sonographers'),\n    3323.0: ('MED', 'Radiologic Technologists And Technicians'),\n    3324.0: ('MED', 'Magnetic Resonance Imaging Technologists'),\n    3330.0: ('MED', 'Nuclear Medicine Technologists and Medical Dosimetrists'),\n    3401.0: ('MED', 'Emergency Medical Technicians'),\n    3402.0: ('MED', 'Paramedics'),\n    3421.0: ('MED', 'Pharmacy Technicians'),\n    3422.0: ('MED', 'Psychiatric Technicians'),\n    3423.0: ('MED', 'Surgical Technologists'),\n    3424.0: ('MED', 'Veterinary Technologists and Technicians'),\n    3430.0: ('MED', 'Dietetic Technicians And Ophthalmic Medical Technicians'),\n    3500.0: ('MED', 'Licensed Practical And Licensed Vocational Nurses'),\n    3515.0: ('MED', 'Medical Records Specialists'),\n    3520.0: ('MED', 'Opticians, Dispensing'),\n    3545.0: ('MED', 'Miscellaneous Health Technologists and Technicians'),\n    3550.0: ('MED', 'Other Healthcare Practitioners and Technical Occupations'),\n    3601.0: ('HLS', 'Home Health Aides'),\n    3602.0: ('HLS', 'Personal Care Aides'),\n    3603.0: ('HLS', 'Nursing Assistants'),\n    3605.0: ('HLS', 'Orderlies and Psychiatric Aides'),\n    3610.0: ('HLS', 'Occupational Therapy Assistants And Aides'),\n    3620.0: ('HLS', 'Physical Therapist Assistants And Aides'),\n    3630.0: ('HLS', 'Massage Therapists'),\n    3640.0: ('HLS', 'Dental Assistants'),\n    3645.0: ('HLS', 'Medical Assistants'),\n    3646.0: ('HLS', 'Medical Transcriptionists'),\n    3647.0: ('HLS', 'Pharmacy Aides'),\n    3648.0: ('HLS', 'Veterinary Assistants And Laboratory Animal Caretakers'),\n    3649.0: ('HLS', 'Phlebotomists'),\n    3655.0: ('HLS', 'Other Healthcare Support Workers'),\n    3700.0: ('PRT', 'First-Line Supervisors Of Correctional Officers'),\n    3710.0: ('PRT', 'First-Line Supervisors Of Police And Detectives'),\n    3720.0: ('PRT',\n             'First-Line Supervisors Of Fire Fighting And Prevention Workers'),\n    3725.0: ('PRT',\n             'First-Line Supervisors of Security And Protective Service Workers, All Other'),\n    3740.0: ('PRT', 'Firefighters'),\n    3750.0: ('PRT', 'Fire Inspectors'),\n    3801.0: ('PRT', 'Bailiffs'),\n    3802.0: ('PRT', 'Correctional Officers and Jailers'),\n    3820.0: ('PRT', 'Detectives And Criminal Investigators'),\n    3840.0: ('PRT', 'Fish And Game Wardens And Parking Enforcement Officers'),\n    3870.0: ('PRT', 'Police Officers'),\n    3900.0: ('PRT', 'Animal Control Workers'),\n    3910.0: ('PRT', 'Private Detectives And Investigators'),\n    3930.0: ('PRT', 'Security Guards And Gaming Surveillance Officers'),\n    3940.0: ('PRT', 'Crossing Guards And Flaggers'),\n    3945.0: ('PRT', 'Transportation Security Screeners'),\n    3946.0: ('PRT', 'School Bus Monitors'),\n    3960.0: ('PRT', 'Other Protective Service Workers'),\n    4000.0: ('EAT', 'Chefs And Head Cooks'),\n    4010.0: ('EAT',\n             'First-Line Supervisors Of Food Preparation And Serving Workers'),\n    4020.0: ('EAT', 'Cooks'),\n    4030.0: ('EAT', 'Food Preparation Workers'),\n    4040.0: ('EAT', 'Bartenders'),\n    4055.0: ('EAT', 'Fast Food And Counter Workers'),\n    4110.0: ('EAT', 'Waiters And Waitresses'),\n    4120.0: ('EAT', 'Food Servers, Nonrestaurant'),\n    4130.0: ('EAT',\n             'Dining Room And Cafeteria Attendants And Bartender Helpers'),\n    4140.0: ('EAT', 'Dishwashers'),\n    4150.0: ('EAT', 'Hosts And Hostesses, Restaurant, Lounge, And Coffee Shop'),\n    4160.0: ('EAT', 'Food Preparation and Serving Related Workers, All Other'),\n    4200.0: ('CLN',\n             'First-Line Supervisors Of Housekeeping And Janitorial Workers'),\n    4210.0: ('CLN',\n             'First-Line Supervisors Of Landscaping, Lawn Service, And Groundskeeping Workers'),\n    4220.0: ('CLN', 'Janitors And Building Cleaners'),\n    4230.0: ('CLN', 'Maids And Housekeeping Cleaners'),\n    4240.0: ('CLN', 'Pest Control Workers'),\n    4251.0: ('CLN', 'Landscaping And Groundskeeping Workers'),\n    4252.0: ('CLN', 'Tree Trimmers and Pruners'),\n    4255.0: ('CLN', 'Other Grounds Maintenance Workers'),\n    4330.0: ('PRS', 'Supervisors Of Personal Care And Service Workers'),\n    4340.0: ('PRS', 'Animal Trainers'),\n    4350.0: ('PRS', 'Animal Caretakers'),\n    4400.0: ('PRS', 'Gambling Services Workers'),\n    4420.0: ('PRS', 'Ushers, Lobby Attendants, And Ticket Takers'),\n    4435.0: ('PRS', 'Other Entertainment Attendants And Related Workers'),\n    4461.0: ('PRS', 'Embalmers, Crematory Operators, And Funeral Attendants'),\n    4465.0: ('PRS', 'Morticians, Undertakers, And Funeral Arrangers'),\n    4500.0: ('PRS', 'Barbers'),\n    4510.0: ('PRS', 'Hairdressers, Hairstylists, And Cosmetologists'),\n    4521.0: ('PRS', 'Manicurists And Pedicurists'),\n    4522.0: ('PRS', 'Skincare Specialists'),\n    4525.0: ('PRS', 'Other Personal Appearance Workers'),\n    4530.0: ('PRS', 'Baggage Porters, Bellhops, And Concierges'),\n    4540.0: ('PRS', 'Tour And Travel Guides'),\n    4600.0: ('PRS', 'Childcare Workers'),\n    4621.0: ('PRS', 'Exercise Trainers And Group Fitness Instructors'),\n    4622.0: ('PRS', 'Recreation Workers'),\n    4640.0: ('PRS', 'Residential Advisors'),\n    4655.0: ('PRS', 'Personal Care and Service Workers, All Other'),\n    4700.0: ('SAL', 'First-Line Supervisors Of Retail Sales Workers'),\n    4710.0: ('SAL', 'First-Line Supervisors Of Non-Retail Sales Workers'),\n    4720.0: ('SAL', 'Cashiers'),\n    4740.0: ('SAL', 'Counter And Rental Clerks'),\n    4750.0: ('SAL', 'Parts Salespersons'),\n    4760.0: ('SAL', 'Retail Salespersons'),\n    4800.0: ('SAL', 'Advertising Sales Agents'),\n    4810.0: ('SAL', 'Insurance Sales Agents'),\n    4820.0: ('SAL',\n             'Securities, Commodities, And Financial Services Sales Agents'),\n    4830.0: ('SAL', 'Travel Agents'),\n    4840.0: ('SAL',\n             'Sales Representatives Of Services, Except Advertising, Insurance, Financial Services, And Travel'),\n    4850.0: ('SAL', 'Sales Representatives, Wholesale And Manufacturing'),\n    4900.0: ('SAL', 'Models, Demonstrators, And Product Promoters'),\n    4920.0: ('SAL', 'Real Estate Brokers And Sales Agents'),\n    4930.0: ('SAL', 'Sales Engineers'),\n    4940.0: ('SAL', 'Telemarketers'),\n    4950.0: ('SAL',\n             'Door-To-Door Sales Workers, News And Street Vendors, And Related Workers'),\n    4965.0: ('SAL', 'Sales And Related Workers, All Other'),\n    5000.0: ('OFF',\n             'First-Line Supervisors Of Office And Administrative Support Workers'),\n    5010.0: ('OFF', 'Switchboard Operators, Including Answering Service'),\n    5020.0: ('OFF', 'Telephone Operators'),\n    5040.0: ('OFF', 'Communications Equipment Operators, All Other'),\n    5100.0: ('OFF', 'Bill And Account Collectors'),\n    5110.0: ('OFF', 'Billing And Posting Clerks'),\n    5120.0: ('OFF', 'Bookkeeping, Accounting, And Auditing Clerks'),\n    5140.0: ('OFF', 'Payroll And Timekeeping Clerks'),\n    5150.0: ('OFF', 'Procurement Clerks'),\n    5160.0: ('OFF', 'Tellers'),\n    5165.0: ('OFF', 'Other Financial Clerks'),\n    5220.0: ('OFF', 'Court, Municipal, And License Clerks'),\n    5230.0: ('OFF', 'Credit Authorizers, Checkers, And Clerks'),\n    5240.0: ('OFF', 'Customer Service Representatives'),\n    5250.0: ('OFF', 'Eligibility Interviewers, Government Programs'),\n    5260.0: ('OFF', 'File Clerks'),\n    5300.0: ('OFF', 'Hotel, Motel, And Resort Desk Clerks'),\n    5310.0: ('OFF', 'Interviewers, Except Eligibility And Loan'),\n    5320.0: ('OFF', 'Library Assistants, Clerical'),\n    5330.0: ('OFF', 'Loan Interviewers And Clerks'),\n    5340.0: ('OFF', 'New Accounts Clerks'),\n    5350.0: ('OFF', 'Correspondence Clerks And Order Clerks'),\n    5360.0: ('OFF',\n             'Human Resources Assistants, Except Payroll And Timekeeping'),\n    5400.0: ('OFF', 'Receptionists And Information Clerks'),\n    5410.0: ('OFF',\n             'Reservation And Transportation Ticket Agents And Travel Clerks'),\n    5420.0: ('OFF', 'Other Information And Records Clerks'),\n    5500.0: ('OFF', 'Cargo And Freight Agents'),\n    5510.0: ('OFF', 'Couriers And Messengers'),\n    5521.0: ('OFF', 'Public Safety Telecommunicators'),\n    5522.0: ('OFF', 'Dispatchers, Except Police, Fire, And Ambulance'),\n    5530.0: ('OFF', 'Meter Readers, Utilities'),\n    5540.0: ('OFF', 'Postal Service Clerks'),\n    5550.0: ('OFF', 'Postal Service Mail Carriers'),\n    5560.0: ('OFF',\n             'Postal Service Mail Sorters, Processors, And Processing Machine Operators'),\n    5600.0: ('OFF', 'Production, Planning, And Expediting Clerks'),\n    5610.0: ('OFF', 'Shipping, Receiving, And Inventory Clerks'),\n    5630.0: ('OFF',\n             'Weighers, Measurers, Checkers, And Samplers, Recordkeeping'),\n    5710.0: ('OFF',\n             'Executive Secretaries And Executive Administrative Assistants'),\n    5720.0: ('OFF', 'Legal Secretaries and Administrative Assistants'),\n    5730.0: ('OFF', 'Medical Secretaries and Administrative Assistants'),\n    5740.0: ('OFF',\n             'Secretaries And Administrative Assistants, Except Legal, Medial, And Executive'),\n    5810.0: ('OFF', 'Data Entry Keyers'),\n    5820.0: ('OFF', 'Word Processors And Typists'),\n    5840.0: ('OFF', 'Insurance Claims And Policy Processing Clerks'),\n    5850.0: ('OFF',\n             'Mail Clerks And Mail Machine Operators, Except Postal Service'),\n    5860.0: ('OFF', 'Office Clerks, General'),\n    5900.0: ('OFF', 'Office Machine Operators, Except Computer'),\n    5910.0: ('OFF', 'Proofreaders And Copy Markers'),\n    5920.0: ('OFF', 'Statistical Assistants'),\n    5940.0: ('OFF', 'Other Office And Administrative Support Workers'),\n    6005.0: ('FFF',\n             'First-Line Supervisors Of Farming, Fishing, And Forestry Workers'),\n    6010.0: ('FFF', 'Agricultural Inspectors'),\n    6040.0: ('FFF', 'Graders And Sorters, Agricultural Products'),\n    6050.0: ('FFF', 'Other Agricultural Workers'),\n    6115.0: ('FFF', 'Fishing And Hunting Workers'),\n    6120.0: ('FFF', 'Forest And Conservation Workers'),\n    6130.0: ('FFF', 'Logging Workers'),\n    6200.0: ('CON',\n             'First-Line Supervisors Of Construction Trades And Extraction Workers'),\n    6210.0: ('CON', 'Boilermakers'),\n    6220.0: ('CON',\n             'Brickmasons, Blockmasons, Stonemasons, And Reinforcing Iron And Rebar Workers'),\n    6230.0: ('CON', 'Carpenters'),\n    6240.0: ('CON', 'Carpet, Floor, And Tile Installers And Finishers'),\n    6250.0: ('CON', 'Cement Masons, Concrete Finishers, And Terrazzo Workers'),\n    6260.0: ('CON', 'Construction Laborers'),\n    6305.0: ('CON', 'Construction Equipment Operators'),\n    6330.0: ('CON', 'Drywall Installers, Ceiling Tile Installers, And Tapers'),\n    6355.0: ('CON', 'Electricians'),\n    6360.0: ('CON', 'Glaziers'),\n    6400.0: ('CON', 'Insulation Workers'),\n    6410.0: ('CON', 'Painters and Paperhangers'),\n    6441.0: ('CON', 'Pipelayers'),\n    6442.0: ('CON', 'Plumbers, Pipefitters, And Steamfitters'),\n    6460.0: ('CON', 'Plasterers And Stucco Masons'),\n    6515.0: ('CON', 'Roofers'),\n    6520.0: ('CON', 'Sheet Metal Workers'),\n    6530.0: ('CON', 'Structural Iron And Steel Workers'),\n    6540.0: ('CON', 'Solar Photovoltaic Installers'),\n    6600.0: ('CON', 'Helpers, Construction Trades'),\n    6660.0: ('CON', 'Construction And Building Inspectors'),\n    6700.0: ('CON', 'Elevator Installers And Repairers'),\n    6710.0: ('CON', 'Fence Erectors'),\n    6720.0: ('CON', 'Hazardous Materials Removal Workers'),\n    6730.0: ('CON', 'Highway Maintenance Workers'),\n    6740.0: ('CON', 'Rail-Track Laying And Maintenance Equipment Operators'),\n    6765.0: ('CON', 'Other Construction And Related Workers'),\n    6800.0: ('EXT',\n             'Derrick, Rotary Drill, And Service Unit Operators, And Roustabouts, Oil, Gas, And Mining'),\n    6825.0: ('EXT', 'Surface Mining Machine Operators And Earth Drillers'),\n    6835.0: ('EXT',\n             'Explosives Workers, Ordnance Handling Experts, and Blasters'),\n    6850.0: ('EXT', 'Underground Mining Machine Operators'),\n    6950.0: ('EXT', 'Other Extraction Workers'),\n    7000.0: ('RPR',\n             'First-Line Supervisors Of Mechanics, Installers, And Repairers'),\n    7010.0: ('RPR', 'Computer, Automated Teller, And Office Machine Repairers'),\n    7020.0: ('RPR',\n             'Radio And Telecommunications Equipment Installers And Repairers'),\n    7030.0: ('RPR', 'Avionics Technicians'),\n    7040.0: ('RPR', 'Electric Motor, Power Tool, And Related Repairers'),\n    7100.0: ('RPR',\n             'Other Electrical And Electronic Equipment Mechanics, Installers, And Repairers'),\n    7120.0: ('RPR',\n             'Electronic Home Entertainment Equipment Installers And Repairers'),\n    7130.0: ('RPR', 'Security And Fire Alarm Systems Installers'),\n    7140.0: ('RPR', 'Aircraft Mechanics And Service Technicians'),\n    7150.0: ('RPR', 'Automotive Body And Related Repairers'),\n    7160.0: ('RPR', 'Automotive Glass Installers And Repairers'),\n    7200.0: ('RPR', 'Automotive Service Technicians And Mechanics'),\n    7210.0: ('RPR', 'Bus And Truck Mechanics And Diesel Engine Specialists'),\n    7220.0: ('RPR',\n             'Heavy Vehicle And Mobile Equipment Service Technicians And Mechanics'),\n    7240.0: ('RPR', 'Small Engine Mechanics'),\n    7260.0: ('RPR',\n             'Miscellaneous Vehicle And Mobile Equipment Mechanics, Installers, And Repairers'),\n    7300.0: ('RPR', 'Control And Valve Installers And Repairers'),\n    7315.0: ('RPR',\n             'Heating, Air Conditioning, And Refrigeration Mechanics And Installers'),\n    7320.0: ('RPR', 'Home Appliance Repairers'),\n    7330.0: ('RPR', 'Industrial And Refractory Machinery Mechanics'),\n    7340.0: ('RPR', 'Maintenance And Repair Workers, General'),\n    7350.0: ('RPR', 'Maintenance Workers, Machinery'),\n    7360.0: ('RPR', 'Millwrights'),\n    7410.0: ('RPR', 'Electrical Power-Line Installers And Repairers'),\n    7420.0: ('RPR', 'Telecommunications Line Installers And Repairers'),\n    7430.0: ('RPR', 'Precision Instrument And Equipment Repairers'),\n    7510.0: ('RPR',\n             'Coin, Vending, And Amusement Machine Servicers And Repairers'),\n    7540.0: ('RPR', 'Locksmiths And Safe Repairers'),\n    7560.0: ('RPR', 'Riggers'),\n    7610.0: ('RPR', 'Helpers--Installation, Maintenance, And Repair Workers'),\n    7640.0: ('RPR', 'Other Installation, Maintenance, And Repair Workers'),\n    7700.0: ('PRD',\n             'First-Line Supervisors Of Production And Operating Workers'),\n    7720.0: ('PRD',\n             'Electrical, Electronics, And Electromechanical Assemblers'),\n    7730.0: ('PRD', 'Engine And Other Machine Assemblers'),\n    7740.0: ('PRD', 'Structural Metal Fabricators And Fitters'),\n    7750.0: ('PRD', 'Other Assemblers And Fabricators'),\n    7800.0: ('PRD', 'Bakers'),\n    7810.0: ('PRD',\n             'Butchers And Other Meat, Poultry, And Fish Processing Workers'),\n    7830.0: ('PRD',\n             'Food And Tobacco Roasting, Baking, And Drying Machine Operators And Tenders'),\n    7840.0: ('PRD', 'Food Batchmakers'),\n    7850.0: ('PRD', 'Food Cooking Machine Operators And Tenders'),\n    7855.0: ('PRD', 'Food Processing Workers, All Other'),\n    7905.0: ('PRD',\n             'Computer Numerically Controlled Tool Operators And Programmers'),\n    7925.0: ('PRD',\n             'Forming Machine Setters, Operators, And Tenders, Metal And Plastic'),\n    7950.0: ('PRD',\n             'Cutting, Punching, And Press Machine Setters, Operators, And Tenders, Metal And Plastic'),\n    8000.0: ('Grinding, Lapping, Polishing, And Buffing Machine Tool'),\n    8025.0: ('PRD',\n             'Other Machine Tool Setters, Operators, And Tenders, Metal and Plastic'),\n    8030.0: ('PRD', 'Machinists'),\n    8040.0: ('PRD', 'Metal Furnace Operators, Tenders, Pourers, And Casters'),\n    8100.0: ('PRD',\n             'Model Makers, Patternmakers, And Molding Machine Setters, Metal And Plastic'),\n    8130.0: ('PRD', 'Tool And Die Makers'),\n    8140.0: ('PRD', 'Welding, Soldering, And Brazing Workers'),\n    8225.0: ('PRD', 'Other Metal Workers And Plastic Workers'),\n    8250.0: ('PRD', 'Prepress Technicians And Workers'),\n    8255.0: ('PRD', 'Printing Press Operators'),\n    8256.0: ('PRD', 'Print Binding And Finishing Workers'),\n    8300.0: ('PRD', 'Laundry And Dry-Cleaning Workers'),\n    8310.0: ('PRD', 'Pressers, Textile, Garment, And Related Materials'),\n    8320.0: ('PRD', 'Sewing Machine Operators'),\n    8335.0: ('PRD', 'Shoe And Leather Workers'),\n    8350.0: ('PRD', 'Tailors, Dressmakers, And Sewers'),\n    8365.0: ('PRD', 'Textile Machine Setters, Operators, And Tenders'),\n    8450.0: ('PRD', 'Upholsterers'),\n    8465.0: ('PRD', 'Other Textile, Apparel, And Furnishings Workers'),\n    8500.0: ('PRD', 'Cabinetmakers And Bench Carpenters'),\n    8510.0: ('PRD', 'Furniture Finishers'),\n    8530.0: ('PRD', 'Sawing Machine Setters, Operators, And Tenders, Wood'),\n    8540.0: ('PRD',\n             'Woodworking Machine Setters, Operators, And Tenders, Except Sawing'),\n    8555.0: ('PRD', 'Other Woodworkers'),\n    8600.0: ('PRD', 'Power Plant Operators, Distributors, And Dispatchers'),\n    8610.0: ('PRD', 'Stationary Engineers And Boiler Operators'),\n    8620.0: ('PRD',\n             'Water And Wastewater Treatment Plant And System Operators'),\n    8630.0: ('PRD', 'Miscellaneous Plant And System Operators'),\n    8640.0: ('PRD',\n             'Chemical Processing Machine Setters, Operators, And Tenders'),\n    8650.0: ('PRD',\n             'Crushing, Grinding, Polishing, Mixing, And Blending Workers'),\n    8710.0: ('PRD', 'Cutting Workers'),\n    8720.0: ('PRD',\n             'Extruding, Forming, Pressing, And Compacting Machine Setters, Operators, And Tenders'),\n    8730.0: ('PRD',\n             'Furnace, Kiln, Oven, Drier, And Kettle Operators And Tenders'),\n    8740.0: ('PRD', 'Inspectors, Testers, Sorters, Samplers, And Weighers'),\n    8750.0: ('PRD', 'Jewelers And Precious Stone And Metal Workers'),\n    8760.0: ('PRD',\n             'Dental And Ophthalmic Laboratory Technicians And Medical Appliance Technicians'),\n    8800.0: ('PRD', 'Packaging And Filling Machine Operators And Tenders'),\n    8810.0: ('PRD', 'Painting Workers'),\n    8830.0: ('PRD',\n             'Photographic Process Workers And Processing Machine Operators'),\n    8850.0: ('PRD', 'Adhesive Bonding Machine Operators And Tenders'),\n    8910.0: ('PRD', 'Etchers And Engravers'),\n    8920.0: ('PRD', 'Molders, Shapers, And Casters, Except Metal And Plastic'),\n    8930.0: ('PRD', 'Paper Goods Machine Setters, Operators, And Tenders'),\n    8940.0: ('PRD', 'Tire Builders'),\n    8950.0: ('PRD', 'Helpers-Production Workers'),\n    8990.0: ('PRD',\n             'Miscellaneous Production Workers, Including Equipment Operators And Tenders'),\n    9005.0: ('TRN',\n             'Supervisors Of Transportation And Material Moving Workers'),\n    9030.0: ('TRN', 'Aircraft Pilots And Flight Engineers'),\n    9040.0: ('TRN',\n             'Air Traffic Controllers And Airfield Operations Specialists'),\n    9050.0: ('TRN', 'Flight Attendants'),\n    9110.0: ('TRN',\n             'Ambulance Drivers And Attendants, Except Emergency Medical Technicians'),\n    9121.0: ('TRN', 'Bus Drivers, School'),\n    9122.0: ('TRN', 'Bus Drivers, Transit And Intercity'),\n    9130.0: ('TRN', 'Driver/Sales Workers And Truck Drivers'),\n    9141.0: ('TRN', 'Shuttle Drivers And Chauffeurs'),\n    9142.0: ('TRN', 'Taxi Drivers'),\n    9150.0: ('TRN', 'Motor Vehicle Operators, All Other'),\n    9210.0: ('TRN', 'Locomotive Engineers And Operators'),\n    9240.0: ('TRN', 'Railroad Conductors And Yardmasters'),\n    9265.0: ('TRN', 'Other Rail Transportation Workers'),\n    9300.0: ('TRN', 'Sailors And Marine Oilers, And Ship Engineers'),\n    9310.0: ('TRN', 'Ship And Boat Captains And Operators'),\n    9350.0: ('TRN', 'Parking Lot Attendants'),\n    9365.0: ('TRN', 'Transportation Service Attendants'),\n    9410.0: ('TRN', 'Transportation Inspectors'),\n    9415.0: ('TRN', 'Passenger Attendants'),\n    9430.0: ('TRN', 'Other Transportation Workers'),\n    9510.0: ('TRN', 'Crane And Tower Operators'),\n    9570.0: ('TRN', 'Conveyor, Dredge, And Hoist and Winch Operators'),\n    9600.0: ('TRN', 'Industrial Truck And Tractor Operators'),\n    9610.0: ('TRN', 'Cleaners Of Vehicles And Equipment'),\n    9620.0: ('TRN', 'Laborers And Freight, Stock, And Material Movers, Hand'),\n    9630.0: ('TRN', 'Machine Feeders And Offbearers'),\n    9640.0: ('TRN', 'Packers And Packagers, Hand'),\n    9645.0: ('TRN', 'Stockers And Order Fillers'),\n    9650.0: ('TRN', 'Pumping Station Operators'),\n    9720.0: ('TRN', 'Refuse And Recyclable Material Collectors'),\n    9760.0: ('TRN', 'Other Material Moving Workers'),\n    9800.0: ('MIL', 'Military Officer Special And Tactical Operations Leaders'),\n    9810.0: ('MIL', 'First-Line Enlisted Military Supervisors'),\n    9825.0: ('MIL',\n             'Military Enlisted Tactical Operations And Air/Weapons Specialists And Crew Members'),\n    9830.0: ('MIL', 'Military, Rank Not Specified'),\n    9920.0: ('Une',\n             'Unemployed And Last Worked 5 Years Ago Or Earlier Or Never Worked'),\n}\n"}
{"type": "source_file", "path": "tableshift/datasets/automl_multimodal_benchmark.py", "content": "\"\"\"\nDatasets from the AutoML benchmark at https://github.com/sxjscience/automl_multimodal_benchmark .\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nfrom typing import Sequence\nfrom pandas import DataFrame\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype, \\\n    column_is_of_type\n\nPROD_FEATURES = FeatureList(features=[\n    Feature('Text_ID', int),\n    Feature('Product_Description', cat_dtype,\n            name_extended=\"product description\"),\n    Feature('Product_Type', cat_dtype, name_extended='product type'),\n    Feature('Sentiment', int, is_target=True,\n            value_mapping={\n                0: \"Cannot Say\", 1: \"Negative\", 2: \"Positive\",\n                3: \"No Sentiment\"})\n],\n    documentation='https://machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/overview')\n\nAIRBNB_FEATURES = FeatureList(features=[\n    Feature('id', int),\n    Feature('listing_url', cat_dtype),\n    Feature('scrape_id', float),\n    Feature('last_scraped', cat_dtype,\n            name_extended='date and time of last scrape'),\n    Feature('name', cat_dtype, name_extended='listing name'),\n    Feature('summary', cat_dtype),\n    Feature('space', cat_dtype),\n    Feature('description', cat_dtype,\n            name_extended='Detailed description of the listing'),\n    Feature('neighborhood_overview', cat_dtype,\n            name_extended=\"Host's description of the neighbourhood\"),\n    Feature('notes', cat_dtype),\n    Feature('transit', cat_dtype),\n    Feature('access', cat_dtype),\n    Feature('interaction', cat_dtype),\n    Feature('house_rules', cat_dtype, name_extended='house rules'),\n    Feature('picture_url', cat_dtype,\n            'URL to the Airbnb hosted regular sized image for the listing',\n            name_extended='listing image URL'),\n    Feature('host_id', int, \"Airbnb's unique identifier for the host/user\",\n            name_extended='host ID'),\n    Feature('host_url', cat_dtype, \"The Airbnb page for the host\",\n            name_extended='host URL'),\n    Feature('host_name', cat_dtype, name_extended='host name'),\n    Feature('host_since', cat_dtype,\n            \"The date the host/user was created. For hosts that are Airbnb guests this could be the date they registered as a guest.\",\n            name_extended='date of host registration'),\n    Feature('host_location', cat_dtype, \"The host's self reported location\",\n            name_extended='host location'),\n    Feature('host_about', cat_dtype, \"Description about the host\",\n            name_extended=\"description of host\"),\n    Feature('host_response_time', cat_dtype,\n            name_extended='host response time'),\n    Feature('host_response_rate', cat_dtype,\n            name_extended='host response rate'),\n    Feature('host_is_superhost', cat_dtype, name_extended='host is superhost'),\n    Feature('host_thumbnail_url', cat_dtype,\n            name_extended='host thumbnail URL'),\n    Feature('host_picture_url', cat_dtype, name_extended='host picture URL'),\n    Feature('host_neighborhood', cat_dtype, name_extended='host neighborhood'),\n    Feature('host_verifications', cat_dtype,\n            name_extended='host verifications'),\n    Feature('host_has_profile_pic', cat_dtype,\n            name_extended='host has profile pic'),\n    Feature('host_identity_verified', cat_dtype,\n            name_extended='host identitity verified'),\n    Feature('street', cat_dtype),\n    Feature('neighborhood', cat_dtype),\n    Feature('city', cat_dtype),\n    Feature('suburb', cat_dtype),\n    Feature('state', cat_dtype),\n    Feature('zipcode', cat_dtype),\n    Feature('smart_location', cat_dtype, name_extended='smart location'),\n    Feature('country_code', cat_dtype, name_extended='country code'),\n    Feature('country', cat_dtype),\n    Feature('latitude', float),\n    Feature('longitude', float),\n    Feature('is_location_exact', cat_dtype, name_extended='location is exact'),\n    Feature('property_type', cat_dtype, name_extended='property type'),\n    Feature('room_type', cat_dtype, name_extended='room type'),\n    Feature('accommodates', int),\n    Feature('bathrooms', float),\n    Feature('bedrooms', float),\n    Feature('beds', float),\n    Feature('bed_type', cat_dtype, name_extended='bed type'),\n    Feature('amenities', cat_dtype),\n    Feature('price', int),\n    Feature('weekly_price', float, name_extended='weekly price'),\n    Feature('monthly_price', float, name_extended='monthly price'),\n    Feature('security_deposit', float, name_extended='security deposit'),\n    Feature('cleaning_fee', float, name_extended='cleaning fee'),\n    Feature('guests_included', int, name_extended='guests included'),\n    Feature('extra_people', int, name_extended='extra people'),\n    Feature('minimum_nights', int, name_extended='minimum nights'),\n    Feature('maximum_nights', int, name_extended='maximum nights'),\n    Feature('calendar_updated', cat_dtype, name_extended='calendar updated'),\n    Feature('has_availability', cat_dtype, name_extended='has availability'),\n    Feature('availability_30', int,\n            \"avaliability_x. The availability of the listing x days in the \"\n            \"future as determined by the calendar. Note a listing may not be \"\n            \"available because it has been booked by a guest or blocked by \"\n            \"the host.\",\n            name_extended='availability in next 30 days'),\n    Feature('availability_60', int,\n            \"avaliability_x. The availability of the listing x days in the \"\n            \"future as determined by the calendar. Note a listing may not be \"\n            \"available because it has been booked by a guest or blocked by \"\n            \"the host.\",\n            name_extended='availability in next 60 days'),\n    Feature('availability_90', int,\n            \"avaliability_x. The availability of the listing x days in the \"\n            \"future as determined by the calendar. Note a listing may not be \"\n            \"available because it has been booked by a guest or blocked by \"\n            \"the host.\",\n            name_extended='availability in next 90 days'),\n    Feature('availability_365', int,\n            \"avaliability_x. The availability of the listing x days in the \"\n            \"future as determined by the calendar. Note a listing may not be \"\n            \"available because it has been booked by a guest or blocked by \"\n            \"the host.\",\n            name_extended='availability in next 365 days'),\n    Feature('calendar_last_scraped', cat_dtype,\n            name_extended='last calendar scrape date'),\n    Feature('number_of_reviews', int, name_extended='number of reviews'),\n    Feature('first_review', cat_dtype,\n            name_extended='date of the first/oldest review'),\n    Feature('last_review', cat_dtype,\n            name_extended='date of the last/newest review'),\n    Feature('review_scores_rating', float,\n            name_extended='average reviewer overall rating'),\n    Feature('review_scores_accuracy', float,\n            name_extended='average reviewer rating for accuracy'),\n    Feature('review_scores_cleanliness', float,\n            name_extended='average reviewer rating for cleanliness'),\n    Feature('review_scores_checkin', float,\n            name_extended='average reviewer rating for check-in'),\n    Feature('review_scores_communication', float,\n            name_extended='average reviewer rating for communication'),\n    Feature('review_scores_location', float,\n            name_extended='average reviewer rating for location'),\n    Feature('review_scores_value', float,\n            name_extended='average reviewer rating for value'),\n    Feature('requires_license', cat_dtype, name_extended='requires license'),\n    Feature('license', cat_dtype),\n    Feature('instant_bookable', cat_dtype, name_extended='instant bookable'),\n    Feature('cancellation_policy', cat_dtype,\n            name_extended='cancellation policy'),\n    Feature('require_guest_profile_picture', cat_dtype,\n            name_extended='requires guest profile picture'),\n    Feature('require_guest_phone_verification', cat_dtype,\n            name_extended='requires guest phone verification'),\n    Feature('calculated_host_listings_count', int,\n            \"The number of listings the host has in the current scrape, in the city/region geography.\",\n            name_extended='number of current listings for this host in the city/region'),\n    Feature('reviews_per_month', float,\n            \"The number of reviews the listing has over the lifetime of the listing\",\n            name_extended='reviews per month'),\n    Feature('price_label', int, is_target=True,\n            name_extended='price per night',\n            value_mapping={\n                0: '[0,46)',\n                1: \"[46,60)\",\n                2: \"[60,75)\",\n                3: \"[75,90)\",\n                4: \"[90,100)\",\n                5: \"[100,120)\",\n                6: \"[120,140)\",\n                7: \"[140,159)\",\n                8: \"[159,192)\",\n                9: \"[192,inf)\"}),\n    Feature('host_verifications_jumio', bool),\n    Feature('host_verifications_government_id', bool),\n    Feature('host_verifications_kba', bool),\n    Feature('host_verifications_zhima_selfie', bool),\n    Feature('host_verifications_facebook', bool),\n    Feature('host_verifications_work_email', bool),\n    Feature('host_verifications_google', bool),\n    Feature('host_verifications_sesame', bool),\n    Feature('host_verifications_manual_online', bool),\n    Feature('host_verifications_manual_offline', bool),\n    Feature('host_verifications_offline_government_id', bool),\n    Feature('host_verifications_selfie', bool),\n    Feature('host_verifications_reviews', bool),\n    Feature('host_verifications_identity_manual', bool),\n    Feature('host_verifications_sesame_offline', bool),\n    Feature('host_verifications_weibo', bool),\n    Feature('host_verifications_email', bool),\n    Feature('host_verifications_sent_id', bool),\n    Feature('host_verifications_phone', bool),\n\n], documentation=\"https://www.kaggle.com/tylerx/melbourne-airbnb-open-data ,\"\n                 \"https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=1322284596\")\n\nWINE_REVIEWS_FEATURES = FeatureList([\n    Feature('country', cat_dtype),\n    Feature('description', cat_dtype),\n    Feature('points', int, name_extended='wine enthusiast rating'),\n    Feature('price', float),\n    Feature('province', cat_dtype),\n    Feature('variety', cat_dtype, is_target=True),\n],\n    documentation=\"https://www.kaggle.com/zynicide/wine-reviews\")\n\nIMDB_FEATURES = FeatureList([\n    Feature('Rank', int),\n    Feature('Title', cat_dtype, name_extended='title of the job ad'),\n    Feature('Description', cat_dtype),\n    Feature('Director', cat_dtype),\n    Feature('Actors', cat_dtype, name_extended='List of actors'),\n    Feature('Year', int),\n    Feature('Runtime (Minutes)', int, name_extended='runtime in minutes'),\n    Feature('Rating', float),\n    Feature('Votes', int),\n    Feature('Revenue (Millions)', float),\n    Feature('Metascore', float),\n    Feature('Genre_is_Drama', int, name_extended='genre is drama',\n            is_target=True),\n], documentation=\"https://www.kaggle.com/PromptCloudHQ/imdb-data\")\n\nJIGSAW_FEATURES = FeatureList([\n    Feature('id', int),\n    Feature('target', int, name_extended='is toxic', is_target=True),\n    Feature('comment_text', cat_dtype, name_extended='comment text'),\n    Feature('severe_toxicity', float, name_extended='severe toxicity score'),\n    Feature('obscene', float, name_extended='obscene score'),\n    Feature('identity_attack', float, name_extended='identity attack score'),\n    Feature('insult', float, name_extended='insult score'),\n    Feature('threat', float, name_extended='threat score'),\n    Feature('asian', float, name_extended='asian score'),\n    Feature('atheist', float, name_extended='atheist score'),\n    Feature('bisexual', float, name_extended='bisexual score'),\n    Feature('black', float, name_extended='black score'),\n    Feature('buddhist', float, name_extended='buddhist score'),\n    Feature('christian', float, name_extended='christian score'),\n    Feature('female', float, name_extended='female score'),\n    Feature('heterosexual', float, name_extended='heterosexual score'),\n    Feature('hindu', float, name_extended='hindu score'),\n    Feature('homosexual_gay_or_lesbian', float,\n            name_extended='homosexual gay or lesbian score'),\n    Feature('intellectual_or_learning_disability', float,\n            name_extended='intellectual or learning disability score'),\n    Feature('jewish', float, name_extended='jewish score'),\n    Feature('latino', float, name_extended='latino score'),\n    Feature('male', float, name_extended='male score'),\n    Feature('muslim', float, name_extended='muslim score'),\n    Feature('other_disability', float, name_extended='other disability score'),\n    Feature('other_gender', float, name_extended='other gender score'),\n    Feature('other_race_or_ethnicity', float,\n            name_extended='other race or ethnicity score'),\n    Feature('other_religion', float, name_extended='other religion score'),\n    Feature('other_sexual_orientation', float,\n            name_extended='other sexual orientation score'),\n    Feature('physical_disability', float,\n            name_extended='physical disability score'),\n    Feature('psychiatric_or_mental_illness', float,\n            name_extended='psychiatric or mental illness score'),\n    Feature('transgender', float, name_extended='transgender score'),\n    Feature('white', float, name_extended='white score'),\n    Feature('created_date', cat_dtype),\n    Feature('publication_id', int),\n    Feature('parent_id', float),\n    Feature('article_id', int),\n    Feature('rating', cat_dtype),\n    Feature('funny', int, name_extended='user votes for funny'),\n    Feature('wow', int, name_extended='user votes for wow'),\n    Feature('sad', int, name_extended='user votes for sad'),\n    Feature('likes', int, name_extended='user likes'),\n    Feature('disagree', int, name_extended='user votes for disagree'),\n    Feature('sexual_explicit', float, name_extended='sexual explicit score'),\n    Feature('identity_annotator_count', int,\n            name_extended='count of identity annotators'),\n    Feature('toxicity_annotator_count', int,\n            name_extended='count of toxicity annotators'),\n],\n    documentation='https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification')\n\nFAKE_JOBS_FEATURES = FeatureList([\n    Feature('title', cat_dtype, name_extended='title of the job ad entry'),\n    # Feature('location', cat_dtype, name_extended='geographical location of the job ad'),\n    Feature('salary_range', cat_dtype, name_extended='salary range'),\n    Feature('description', cat_dtype),\n    Feature('required_experience', cat_dtype,\n            name_extended='required experience'),\n    Feature('required_education', cat_dtype,\n            name_extended='required education'),\n    Feature('fraudulent', int, is_target=True)\n\n],\n    documentation='https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction')\n\nKICKSTARTER_FEATURES = FeatureList([\n    Feature('name', cat_dtype, name_extended='name of the project'),\n    Feature('desc', cat_dtype, name_extended='description of project'),\n    Feature('goal', float, 'the goal (amount) required for the project'),\n    Feature('keywords', cat_dtype),\n    Feature('disable_communication', bool,\n            name_extended='project authors have disabled communication'),\n    Feature('country', cat_dtype),\n    Feature('currency', cat_dtype),\n    Feature('deadline', int,\n            name_extended='unix timestamp of project deadline'),\n    Feature('created_at', int,\n            name_extended='unix timestamp of when the project was posted'),\n    Feature('final_status', int, is_target=True,\n            name_extended='funded successfully'),\n],\n    documentation='https://www.kaggle.com/codename007/funding-successful-projects')\n\nNEWS_CHANNEL_FEATURES = FeatureList([\n    Feature('n_tokens_content', float, name_extended='Number of words in the content'),\n    Feature('n_unique_tokens', float, name_extended='Rate of unique words in the content'),\n    Feature('n_non_stop_words', float, name_extended='Rate of non-stop words in the content'),\n    Feature('n_non_stop_unique_tokens', float, name_extended='Rate of unique non-stop words in the content'),\n    Feature('num_hrefs', float, name_extended='Number of links'),\n    Feature('num_self_hrefs', float, name_extended='Number of links to other articles published by Mashable'),\n    Feature('num_imgs', float, name_extended='Number of images'),\n    Feature('num_videos', float, name_extended='Number of videos'),\n    Feature('average_token_length', float, name_extended='Average length of the words in the content '),\n    Feature('num_keywords', float, name_extended='Number of keywords in the metadata'),\n    Feature('global_subjectivity', float, name_extended='Text subjectivity'),\n    Feature('global_sentiment_polarity', float, name_extended='Text sentiment polarity'),\n    Feature('global_rate_positive_words', float, name_extended='Rate of positive words in the content'),\n    Feature('global_rate_negative_words', float, name_extended='Rate of negative words in the content'),\n    Feature('rate_positive_words', float, name_extended='Rate of positive words among non-neutral tokens'),\n    Feature('rate_negative_words', float, name_extended='Rate of negative words among non-neutral tokens'),\n    Feature('article_title', cat_dtype, name_extended='article title'),\n    Feature('channel', cat_dtype, is_target=True,\n            value_mapping={\n                ' data_channel_is_tech': 'tech',\n                ' data_channel_is_entertainment': 'entertainment',\n                ' data_channel_is_lifestyle': 'lifestyle',\n                ' data_channel_is_bus': 'business',\n                ' data_channel_is_world': 'world',\n                ' data_channel_is_socmed': 'social media',\n            }),\n],\n    documentation='https://archive.ics.uci.edu/ml/datasets/online+news+popularity')\n\nSALARY_FEATURES = FeatureList([\n    Feature('experience', cat_dtype),\n    Feature('job_description', cat_dtype, name_extended='job description'),\n    Feature('job_desig', cat_dtype, name_extended='job designation'),\n    Feature('job_type', cat_dtype, name_extended=\"job type\"),\n    Feature('key_skills', cat_dtype, name_extended='key skills'),\n    Feature('location', cat_dtype),\n    Feature('salary', cat_dtype, is_target=True,\n            name_extended='Salary in Rupees Lakhs',\n            value_mapping={\n                '25to50': '[25,50)', '3to6': '[3,6)', '15to25': '[15,25)',\n                '10to15': '[10,15)', '6to10': '[6,10)', '0to3': '[0,3)', }),\n],\n    documentation='https://machinehack.com/hackathons/predict_the_data_scientists_salary_in_india_hackathon/overview')\n\n\n\ndef fill_missing(df: DataFrame, cols: Sequence[str]) -> DataFrame:\n    for col in cols:\n        df[col] = df[col].fillna('MISSING')\n    return df\n\n\ndef preprocess_automl(df: DataFrame,\n                      automl_benchmark_dataset_name: str) -> DataFrame:\n    # For any string columns, we insert a 'missing' dummy value.\n    object_cols = [c for c in df.columns if column_is_of_type(df[c], 'object')]\n    df = fill_missing(df, object_cols)\n\n    # remove leading spaces from column names\n    for c in df.columns:\n        if c.startswith(' '):\n            df.rename(columns={c: c.lstrip()}, inplace=True)\n\n    if automl_benchmark_dataset_name == \"imdb_genre_prediction\":\n        df[IMDB_FEATURES.target] = df[IMDB_FEATURES.target].astype(int)\n    elif automl_benchmark_dataset_name == \"jigsaw_unintended_bias100K\":\n        df[JIGSAW_FEATURES.target] = df[JIGSAW_FEATURES.target].astype(int)\n\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/adult.py", "content": "\"\"\"\nFeature specs and preprocessing for data from the Adult dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nADULT_RESOURCES = [\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\",\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n]\n\n# Names to use for the features in the Adult dataset. These correspond to\n# (human-readable) column names in the order of the columns in adult.data file.\nADULT_FEATURE_NAMES = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\",\n                       \"Education-Num\",\n                       \"Marital Status\",\n                       \"Occupation\", \"Relationship\", \"Race\", \"Sex\",\n                       \"Capital Gain\",\n                       \"Capital Loss\",\n                       \"Hours per week\", \"Country\", \"Target\"]\n\nADULT_FEATURES = FeatureList(features=[\n    Feature(\"Age\", float, \"Age\"),\n    Feature(\"Workclass\", cat_dtype, \"\"\"Private, Self-emp-not-inc, \n    Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, \n    Never-worked.\"\"\",\n            name_extended=\"Worker class\"),\n    Feature(\"Education-Num\", cat_dtype,\n            name_extended=\"Max education level achieved\",\n            value_mapping={\n                \"1.0\": \"Preschool\",\n                \"2.0\": \"1st-4th\",\n                \"3.0\": \"5th-6th\",\n                \"4.0\": \"7th-8th\",\n                \"5.0\": \"9th\",\n                \"6.0\": \"10th\",\n                \"7.0\": \"11th\",\n                \"8.0\": \"12th\",\n                \"9.0\": \"high school graduate\",\n                \"10.0\": \"some college\",\n                \"11.0\": \"associates degree - vocational\",\n                \"12.0\": \"associates degree - academic\",\n                \"13.0\": \"bachelor's degree\",\n                \"14.0\": \"master's degree\",\n                \"15.0\": \"professional school\",\n                \"16.0\": \"doctorate\"}),\n\n    Feature(\"Marital Status\", cat_dtype, \"\"\"Married-civ-spouse, Divorced, \n    Never-married, Separated, Widowed, Married-spouse-absent, \n    Married-AF-spouse.\"\"\",\n            value_mapping={\n                \"Married-civ-spouse\": \"married with civilian spouse\",\n                \"Divorced\": \"divorced\",\n                \"Never-married\": \"never married\",\n                \"Separated\": \"separated\",\n                \"Widowed\": \"widowed\",\n                \"Married-spouse-absent\": \"married, spouse is absent\",\n                \"Married-AF-spouse\": \"married, spouse in armed forces\"\n            }),\n    Feature(\"Occupation\", cat_dtype, \"\"\"Tech-support, Craft-repair, \n    Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, \n    Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, \n    Priv-house-serv, Protective-serv, Armed-Forces.\"\"\"),\n    Feature(\"Relationship\", cat_dtype, \"\"\"Wife, Own-child, Husband, \n    Not-in-family, Other-relative, Unmarried.\"\"\"),\n    Feature(\"Race\", cat_dtype, \"\"\"White, Asian-Pac-Islander, \n    Amer-Indian-Eskimo, Other, Black.\"\"\"),\n    Feature(\"Sex\", cat_dtype, \"Female, Male.\"),\n    Feature(\"Capital Gain\", float, \"No documentation provided.\"),\n    Feature(\"Capital Loss\", float, \"No documentation provided.\"),\n    Feature(\"Hours per week\", float, \"No documentation provided.\",\n            name_extended=\"Hours worked per week\"),\n    Feature(\"Country\", cat_dtype, \"\"\"United-States, Cambodia, England, \n    Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, \n    Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, \n    Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, \n    Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, \n    Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, \n    Holand-Netherlands.\"\"\",\n            name_extended=\"Native country\"),\n    Feature(\"Target\", int,\n            description=\"Binary indicator for whether income >= 50k. See \"\n                        \"preprocess_adult() below.\",\n            is_target=True),\n], documentation=\"https://archive.ics.uci.edu/ml/datasets/Adult\")\n\n\ndef preprocess_adult(df: pd.DataFrame):\n    \"\"\"Process a raw adult dataset.\"\"\"\n    df['Target'] = df['Target'].replace(\n        {'<=50K': 0,\n         '<=50K.': 0,\n         '>50K': 1,\n         '>50K.': 1})\n    del df['Education']\n    return df\n"}
{"type": "source_file", "path": "tableshift/core/features.py", "content": "import copy\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import List, Any, Sequence, Optional, Mapping, Tuple, Union, Dict\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype as cat_dtype\nfrom pandas.api.types import is_object_dtype, is_float_dtype, is_integer_dtype, is_categorical_dtype, is_bool_dtype\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, \\\n    FunctionTransformer, OrdinalEncoder\nfrom tableshift.core.discretization import KBinsDiscretizer\nfrom tableshift.core.utils import sub_illegal_chars\n\n\ndef _contains_missing_values(df: pd.DataFrame) -> bool:\n    return np.any(pd.isnull(df).values)\n\n\ndef safe_cast(x: pd.Series, dtype):\n    logging.debug(f\"casting feature {x.name} from dtype \"\n                  f\"{x.dtype.name} to dtype {dtype.__name__}\")\n    if dtype == cat_dtype:\n        return x.apply(str).astype(\"category\")\n    else:\n        try:\n            return x.astype(dtype)\n        except pd.errors.IntCastingNaNError as e:\n            if 'int' in dtype.__name__.lower():\n                # Case: integer with nan values; cast to float instead. Integers\n                # are not nullable in Pandas (but \"Int64\" type is).\n                logging.warning(\n                    f\"cannot cast feature {x.name} to \"\n                    f\"dtype {dtype.__name__} due to missing values; \"\n                    f\"attempting cast to float instead. Recommend changing\"\n                    f\"the feature spec for this feature to type float.\")\n                return x.astype(float)\n\n\ndef is_categorical(x: pd.Series):\n    return isinstance(x.dtype, cat_dtype) or isinstance(x, pd.Categorical)\n\n\ndef column_is_of_type(x: pd.Series, dtype) -> bool:\n    \"\"\"Helper function to check whether column has specified dtype.\"\"\"\n    if hasattr(dtype, \"name\"):\n        # Case: target dtype is of categorical dtype.\n        return (hasattr(x.dtype, \"name\")) and (x.dtype.name == dtype.name)\n    elif is_categorical(x):\n        # Case: input data is of categorical dtype.\n        return dtype == cat_dtype\n    else:\n        # Check if x is a subdtype of the more general type specified in\n        # dtype; this will not perform casting of identical subtypes (i.e.\n        # does not cast int64 to int).\n        return np.issubdtype(x.dtype, dtype)\n\n\ndef get_dtype(dtype):\n    if is_categorical_dtype(dtype) or is_object_dtype(dtype):\n        return cat_dtype\n    elif is_integer_dtype(dtype):\n        return int\n    elif is_float_dtype(dtype):\n        return float\n    elif is_bool_dtype(dtype):\n        return bool\n    else:\n        raise ValueError(f\"unknown dtype: {dtype}\")\n\n\ndef cast_number(number_str: str):\n    try:\n        # Try to convert to integer\n        return int(number_str)\n    except ValueError:\n        try:\n            # If integer conversion fails, try to convert to float\n            return float(number_str)\n        except ValueError:\n            # If both conversions fail, return the original string\n            return number_str\n\n\n@dataclass(frozen=True)\nclass Feature:\n    name: str\n    kind: Any  # a data type to which the feature should be castable.\n    description: str = None\n    is_target: bool = False\n    # Values, besides np.nan, to count as null/missing. These should be\n    # values in the original feature encoding (that is, the values that would\n    # occur for this column in the output of the preprocess_fn, not after\n    # casting to `kind`), because values after casting may be unpredictable.\n    na_values: Tuple = field(default_factory=tuple)\n    # Mapping of the set of values in the data to a more descriptive set of\n    # values. Used e.g. for categorical features that are coded with numeric\n    # values that map to known/named categories.\n    value_mapping: Dict[Any, Any] = None\n    # Optional longer name of feature (can contain arbitrary text, including\n    # illegal JSON characters).\n    name_extended: str = None\n    # Additional notes regarding feature or values.\n    note: str = None\n\n    def fillna(self, data: pd.Series) -> pd.Series:\n        \"\"\"Apply the list of na_values, filling these values in data with np.nan.\"\"\"\n        logging.debug(f\"replacing missing values of {self.na_values} \"\n                      f\"for feature {self.name}\")\n        # Handles case where na values have misspecified type (i.e. float vs. int);\n        # we would like the filling to be robust to these kinds of misspecification.\n        if not isinstance(data.dtype, cat_dtype):\n            try:\n                na_values = np.array(self.na_values).astype(data.dtype)\n            except ValueError:\n                # Raised when some na_values cannot be cast to the target type\n                na_values = np.array(self.na_values)\n        else:\n            na_values = np.array(self.na_values)\n        return data.replace(na_values, np.nan)\n\n    def apply_dtype(self, data: pd.Series) -> pd.Series:\n        \"\"\"Apply the specified dtype, casting if needed (and otherwise returning data).\"\"\"\n        return safe_cast(data, self.kind)\n\n\n@dataclass\nclass FeatureList:\n    features: List[Feature]\n    documentation: str = None  # optional link to docs\n\n    @classmethod\n    def from_dataframe(cls, df: pd.DataFrame, target_colname: str,\n                       names_extended: Dict[Any, str] = None,\n                       value_mapping: Dict[Any, Dict] = None,\n                       **kwargs):\n        if names_extended is None:\n            names_extended = {}\n        if value_mapping is None:\n            value_mapping = {}\n        assert target_colname in df.columns, \\\n            f\"target colname {target_colname} not in columns {df.columns}\"\n\n        dtypes = df.dtypes.to_dict()\n        features = [Feature(name,\n                            get_dtype(dtype),\n                            is_target=name == target_colname,\n                            name_extended=names_extended.get(name),\n                            value_mapping=value_mapping.get(name))\n                    for name, dtype in dtypes.items()]\n\n        return cls(features=features, **kwargs)\n\n    def to_jsonl(self, file):\n        \"\"\"Write to a jsonl file. File can either be a sting path to a file, or a handle-like object.\"\"\"\n        if isinstance(file, str):\n            # Case: file is a string; open handle to a file object at this path.\n            with open(file, \"w\") as f:\n                for feature in self.features:\n                    output_dict = copy.deepcopy(feature.__dict__)\n                    kind = output_dict.pop('kind')\n                    kind_str = getattr(kind, \"name\", kind.__name__)\n                    output_dict['kind'] = kind_str\n                    f.write(json.dumps(output_dict) + \"\\n\")\n        else:\n            # Case: file is a handle; try to write to it directly.\n            try:\n                for feature in self.features:\n                    output_dict = copy.deepcopy(feature.__dict__)\n                    kind = output_dict.pop('kind')\n                    kind_str = getattr(kind, \"name\", kind.__name__)\n                    output_dict['kind'] = kind_str\n                    line = json.dumps(output_dict) + \"\\n\"\n                    file.write(line.encode())\n            except Exception as e:\n                logging.error(f\"error writing to file {file} of type {type(file)}\")\n                raise e\n\n    @classmethod\n    def from_jsonl(cls, file: str, auto_cast_value_mappings: bool = False):\n        assert os.path.exists(file), f\"file {file} does not exist.\"\n        with open(file, \"r\") as f:\n            lines = f.readlines()\n        feature_dicts = [json.loads(l) for l in lines]\n\n        # JSON parsing automatically casts any int/float keys to string; if auto_cast_value_mapping\n        # is True, we attempt to recover these.\n        if auto_cast_value_mappings:\n            for feature_dict in feature_dicts:\n                if feature_dict['value_mapping']:\n                    feature_dict['value_mapping'] = {cast_number(k): cast_number(v) for k, v in\n                                                     feature_dict['value_mapping'].items()}\n\n        # For each element in feature_dicts, create the actual class object\n        # corresponding to the feature kind, from its string representation.\n\n        for i in range(len(feature_dicts)):\n            kind = feature_dicts[i]['kind']\n            if kind in ('float', 'int', 'bool'):\n                feature_dicts[i]['kind'] = eval(kind)\n            elif kind == \"category\":\n                feature_dicts[i]['kind'] = cat_dtype\n            else:\n                raise ValueError(f\"unexpected kind: {kind}\")\n\n        features = [Feature(**feature_dict) for feature_dict in feature_dicts]\n        return cls(features=features)\n\n    @property\n    def predictors(self) -> List[str]:\n        \"\"\"Fetch the names of non-target features.\"\"\"\n        return [x.name for x in self.features if not x.is_target]\n\n    @property\n    def names(self) -> List[str]:\n        return [f.name for f in self.features]\n\n    @property\n    def target_feature(self) -> Feature:\n        \"\"\"Return the target feature (if it exists).\"\"\"\n        for f in self.features:\n            if f.is_target:\n                return f\n        raise ValueError(f\"no target in features {self.names}\")\n\n    @property\n    def target(self) -> Union[str, None]:\n        \"\"\"Return the name of the target feature (if it exists).\"\"\"\n        for f in self.features:\n            if f.is_target:\n                return f.name\n        return None\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, item: str) -> Feature:\n        \"\"\"Get a feature by name.\"\"\"\n        for f in self.features:\n            if f.name == item:\n                return f\n        raise ValueError(f\"feature {item} not in features {self.names}.\")\n\n    def __add__(self, other):\n        if (self.target and other.target):\n            raise ValueError(\"cannot add two lists which both contain targets.\")\n        overlap = set(self.names).intersection(set(other.names))\n        assert not overlap, \\\n            \"cannot add lists with overlapping feature names; remove the \" \\\n            \"following features from one of the FeatureLists: {}\".format(list(\n                overlap))\n        return FeatureList(self.features + other.features,\n                           documentation=self.documentation)\n\n    def __iter__(self):\n        yield from self.features\n\n    def apply_schema(self, df: pd.DataFrame,\n                     passthrough_columns: Sequence[str] = None) -> pd.DataFrame:\n        \"\"\"Apply the schema defined in the FeatureList to the DataFrame.\n\n        Subsets to only the columns corresponding to features in FeatureList,\n        and then transforms each column by casting it to the type specified\n        in each Feature.\n        \"\"\"\n        if not passthrough_columns:\n            passthrough_columns = []\n\n        drop_cols = list(set(x for x in df.columns\n                             if x not in self.names\n                             and x not in passthrough_columns))\n        if drop_cols:\n            logging.info(f\"dropping columns not in FeatureList: {drop_cols}\")\n            df.drop(columns=drop_cols, inplace=True)\n        for f in self.features:\n            logging.debug(f\"checking feature {f.name}\")\n            if f.name not in df.columns:\n                # Case: expected this feature, and it is missing.\n                raise ValueError(f\"feature {f.name} not present in data with \"\n                                 f\"columns {df.columns}.\")\n\n            # Fill na values (before casting)\n            if f.na_values:\n                df[f.name] = f.fillna(df[f.name])\n\n            # Cast to desired type\n            if not column_is_of_type(df[f.name], f.kind):\n                df[f.name] = f.apply_dtype(df[f.name])\n\n        if _contains_missing_values(df):\n            logging.debug(\"missing values detected in data; counts by column:\")\n            logging.debug(pd.isnull(df).sum())\n\n        return df\n\n\ndef _transformed_columns_to_numeric(df, prefix: str,\n                                    to_type=float) -> pd.DataFrame:\n    \"\"\"Postprocess the results of a ColumnTransformer.\n\n    ColumnTransformers convert their output to 'object' dtype, even when the\n    outputs are properly numeric.\n\n    Using pattern-matching from the verbose feature names of a\n    ColumnTransformer, cast any transformed columns to the specified dtype.\n\n    This provides maximum compatibility with downstream models by eliminating\n    categorical dtypes where they are no longer needed (and no longer properly\n    describe the data type of a column).\n\n    Valid prefixes include \"scale_\" for scaled columns, and \"onehot_\" for\n    one-hot-encoded columns.\n    \"\"\"\n    cols_to_transform = [c for c in df.columns if c.startswith(prefix)]\n    logging.debug(f\"casting {len(cols_to_transform)} columns to type {to_type}\"\n                  \"; this can be slow for large datasets.\")\n    for i, c in enumerate(cols_to_transform):\n        logging.debug(f\"casting feature {i} of {len(cols_to_transform)} {c}\")\n        df[c] = df[c].astype(to_type)\n    return df\n\n\n@dataclass\nclass PreprocessorConfig:\n    # Preprocessing for categorical features (also applies to boolean features).\n    # Options are: one_hot, map_values, label_encode, passthrough.\n    categorical_features: str = \"one_hot\"\n    # Preprocessing for float and int features.\n    # Options: normalize, passthrough, map_values.\n    numeric_features: str = \"normalize\"\n    domain_labels: str = \"label_encode\"\n    passthrough_columns: Union[\n        str, List[str]] = None  # Feature names to passthrough, or \"all\".\n    # If \"rows\", drop rows containing na values, if \"columns\", drop columns\n    # containing na values; if None do not do anything for missing values.\n    dropna: Union[str, None] = \"rows\"\n\n    # If true, map feature name -> name_extended to features when name_extended\n    # is specified.\n    use_extended_names: bool = False\n\n    # By defaults, target names/values are not mapped even when map_values is\n    # used. However, if this is set to true, targets will be mapped when\n    # map_values is specified.\n    map_targets: bool = False\n\n    default_targets_dtype = int\n    cast_targets_to_default_type: bool = False\n\n    min_frequency: float = None  # see OneHotEncoder.min_frequency\n    max_categories: int = None  # see OneHotEncoder.max_categories\n    n_bins: int = 5  # see KBinsDiscretizer.num_bins\n    sub_illegal_chars: bool=True  # whether to replace illegal characters in column names\n\n\ndef map_values(df: pd.DataFrame, mapping: dict, strict=True) -> pd.DataFrame:\n    \"\"\"Apply mapping to a column.\"\"\"\n    column = df.stack()\n    unmapped_values = list(set(column.unique()) - set(mapping.keys()))\n\n    if unmapped_values and strict:\n        # Case: there are unmapped values; raise an error in 'strict' mode.\n        raise ValueError(\n            f\"Got the following unmapped values for column with first rows {column.head()}: {unmapped_values}.\"\n            \"If this is intended (and you want these values mapped to None), set strict=False. If you\"\n            \"want these values mapped to themselves (identity mapping), explicitly specify this\"\n            \"in your mapping, or set map_values=False.\")\n    elif unmapped_values:\n        # Case: there are unmapped values but not in 'strict' mode; raise a warning.\n        logging.warning(\n            f'got value(s) in column {df.columns[0]} with no'\n            f'mapping: {unmapped_values}; will pass these values through '\n            f'instead. If this is intended, then no action is required. '\n            f'For guaranteed behavior (to ensure the value is not mapped), '\n            f'it is best to define this explicitly in your mapping.')\n        mapping.update({x: x for x in unmapped_values})\n\n    return column.map(mapping).unstack()\n\n\ndef get_numeric_columns(data: pd.DataFrame) -> List[str]:\n    \"\"\"Helper function to extract numeric colnames from a dataset.\"\"\"\n    numeric_columns = make_column_selector(\n        pattern=\"^(?![Tt]arget)\",\n        dtype_include=np.number)(data)\n    return numeric_columns\n\n\ndef get_categorical_columns(data: pd.DataFrame) -> List[str]:\n    \"\"\"Helper function to extract categorical colnames from a dataset.\"\"\"\n    categorical_columns = make_column_selector(\n        pattern=\"^(?![Tt]arget)\",\n        dtype_include=[object, bool, cat_dtype])(data)\n    return categorical_columns\n\n\ndef make_value_map_transforms(features_to_map: List[Feature]) -> List[\n    Tuple[str, FunctionTransformer, List[str]]]:\n    \"\"\"Helper function to build the mapping transforms for a set of features.\n\n    The output of this function can be used as input to a ColumnTransformer.\n    \"\"\"\n    transforms = [\n        (f'map_{f.name}',\n         FunctionTransformer(partial(map_values,\n                                     mapping=f.value_mapping),\n                             check_inverse=False,\n                             feature_names_out=\"one-to-one\"),\n         [f.name]) for f in features_to_map]\n    return transforms\n\n\ndef remove_verbose_prefixes(colnames: List[str], sep='__') -> List[str]:\n    \"\"\"Helper function to remove the prefixes added by ColumnTransformer.\n\n    Example output: 'map__col1' -> col1, 'scale_featurez' -> featurez, etc.\n\n    This allows us to retain the verbose names internally during the\n    preprocessing phase, but not actually expose those verbose names in the\n    data that is returned to the user.\n    \"\"\"\n    return [c if sep not in c else c.split(sep)[1] for c in colnames]\n\n\n@dataclass\nclass Preprocessor:\n    config: PreprocessorConfig\n    feature_transformer: ColumnTransformer = None\n    domain_label_transformer: LabelEncoder = None\n    feature_list: Optional[FeatureList] = None\n\n    def _get_categorical_transforms(self, data: pd.DataFrame,\n                                    passthrough_columns: List[str]) -> List:\n\n        cols = [c for c in get_categorical_columns(data) if\n                c not in passthrough_columns]\n\n        if self.config.categorical_features == \"passthrough\":\n            transforms = []\n\n        elif self.config.categorical_features == \"one_hot\":\n            transforms = [\n                (f'onehot_{c}',\n                 OneHotEncoder(dtype=np.int8, categories=[data[c].unique()],\n                               min_frequency=self.config.min_frequency,\n                               max_categories=self.config.max_categories), [c])\n                for c in cols]\n\n        elif self.config.categorical_features == \"map_values\":\n            assert self.feature_list is not None, \\\n                \"Feature list is required to use value mapping.\"\n            features_to_map = [f for f in self.feature_list\n                               if f.value_mapping is not None\n                               and f.name in cols]\n            if not len(features_to_map):\n                # Case: map_values is specified, but there are no features\n                # with mappable values.\n                logging.warning(\n                    \"No categorical columns with  mappings provided. Either \"\n                    \"provide mappings for one or more columns or set \"\n                    \"categorical_columns='passthrough' in the feature config.\")\n                transforms = []\n            else:\n                # Case: there are categorical features to map.\n                transforms = make_value_map_transforms(features_to_map)\n\n        elif self.config.categorical_features == \"label_encode\":\n            transforms = [(f'le_{c}',\n                           OrdinalEncoder(handle_unknown='use_encoded_value',\n                                          unknown_value=-2,\n                                          encoded_missing_value=-1),\n                           [c])\n                          for c in cols]\n\n        else:\n            raise ValueError(f\"{self.config.categorical_features} is not \"\n                             \"a valid categorical preprocessor type.\")\n        return transforms\n\n    def _get_numeric_transforms(self, data: pd.DataFrame,\n                                passthrough_columns: List[str] = None) -> List:\n\n        cols = [c for c in get_numeric_columns(data)\n                if c not in passthrough_columns]\n\n        if self.config.numeric_features == \"passthrough\":\n            transforms = []\n\n        elif self.config.numeric_features == \"map_values\":\n            assert self.feature_list is not None, \\\n                \"Feature list is required to use value mapping.\"\n            features_to_map = [f for f in self.feature_list\n                               if f.value_mapping is not None\n                               and f.name in cols]\n            if not len(features_to_map):\n                # Case: map_values is specified, but there are no features\n                # with mappable values.\n                logging.warning(\n                    \"No numeric columns with mappings provided. Either provide \"\n                    \"mappings for one or more columns or set \"\n                    \"numeric_columns='passthrough' in the feature config.\")\n                transforms = []\n            else:\n                # Case: there are features with mappable values.\n                transforms = make_value_map_transforms(features_to_map)\n\n        elif self.config.numeric_features == \"normalize\":\n            transforms = [(f'scale_{c}', StandardScaler(), [c]) for c in cols]\n\n        elif self.config.numeric_features == \"kbins\":\n            transforms = [(\"kbin\", KBinsDiscretizer(encode=\"ordinal\",\n                                                    n_bins=self.config.n_bins),\n                           cols)]\n\n        else:\n            raise ValueError(f\"{self.config.numeric_features} is not \"\n                             f\"a valid numeric preprocessor type.\")\n        return transforms\n\n    def _post_transform_summary(self, data: pd.DataFrame):\n        logging.debug(\"printing post-transform feature summary\")\n        if self.config.numeric_features == \"kbins\":\n            for c in data.columns:\n                if \"kbin\" in c: logging.info(f\"{c}:{data[c].unique().tolist()}\")\n        elif self.config.numeric_features == \"normalize\":\n            for c in data.columns:\n                if \"scale\" in c: logging.info(f\"{c}: mean {data[c].mean()}, \"\n                                              f\"std {data[c].std()}\")\n\n    def fit_feature_transformer(self, data, train_idxs: List[int],\n                                passthrough_columns: List[str] = None):\n        \"\"\"Fits the feature_transformer defined by this Preprocessor.\"\"\"\n        transforms = []\n        transforms += self._get_numeric_transforms(data,\n                                                   passthrough_columns)\n\n        transforms += self._get_categorical_transforms(data,\n                                                       passthrough_columns)\n\n        # Note that we use the verbose feature names to track which columns are\n        # the result of various preprocessing transformations downstream;\n        # ultimately the verbose names are cleaned up.\n\n        self.feature_transformer = ColumnTransformer(\n            transforms,\n            remainder='passthrough',\n            sparse_threshold=0,\n            verbose_feature_names_out=True)\n\n        self.feature_transformer.fit(data.loc[train_idxs, :])\n        return\n\n    def transform_features(self, data) -> pd.DataFrame:\n        transformed = self.feature_transformer.transform(data)\n        transformed = pd.DataFrame(\n            transformed,\n            columns=self.feature_transformer.get_feature_names_out())\n\n        return transformed\n\n    def _post_transform(self, transformed: pd.DataFrame,\n                        cast_dtypes: Optional[Mapping] = None) -> pd.DataFrame:\n        \"\"\"Postprocess the result of a ColumnTransformer.\"\"\"\n        transformed.columns = [c.replace(\"remainder__\", \"\")\n                               for c in transformed.columns]\n\n        # By default transformed columns will be cast to 'object' dtype; we\n        # cast them back to a numeric type.\n        transformed = _transformed_columns_to_numeric(transformed, \"onehot_\",\n                                                      np.int8)\n        transformed = _transformed_columns_to_numeric(transformed, \"scale_\")\n        # Cast the specified columns back to their original types\n        if cast_dtypes:\n            for colname, dtype in cast_dtypes.items():\n                transformed[colname] = transformed[colname].astype(dtype)\n        transformed.columns = remove_verbose_prefixes(transformed.columns)\n\n        if self.config.use_extended_names:\n            transformed.columns = self.map_names_extended(\n                transformed.columns.tolist())\n        elif self.config.sub_illegal_chars:\n            transformed.columns = [sub_illegal_chars(c) for c in\n                                   transformed.columns]\n\n        return transformed\n\n    def _dropna(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply the specified handling of NA values.\n\n        If \"rows\", drop any rows containing NA values. If \"columns\", drop\n        any columns containing NA values. If None, do not alter data.\n\n        This function should be called *before* splitting data.\n        \"\"\"\n        if self.config.dropna is None:\n            return data\n\n        start_len = len(data)\n        if self.config.dropna == \"rows\":\n            data.dropna(inplace=True)\n        elif self.config.dropna == \"columns\":\n            data.dropna(axis=1, inplace=True)\n        logging.debug(\n            f\"dropped {start_len - len(data)} rows \"\n            f\"containing missing values \"\n            f\"({(start_len - len(data)) * 100 / start_len}% of data).\")\n        data.reset_index(drop=True, inplace=True)\n        if not len(data):\n            raise ValueError(f\"Data is empty after applying dropna=\"\n                             f\"{self.config.dropna}\")\n        return data\n\n    def _check_inputs(self, data):\n        prohibited_chars = \"[].<>\"\n        for char in prohibited_chars:\n            for colname in data.columns:\n                if char in colname:\n                    raise ValueError(\n                        f\"[ERROR] illegal character {char} in column name \"\n                        f\"{colname}; this will likely lead to an error.\")\n                if \"__\" in colname:\n                    raise ValueError(f\"Double underscore '__' prohibited in \"\n                                     f\"column names; got {colname}\")\n\n    def fit_transform_domain_labels(self, x: pd.Series):\n        if self.config.domain_labels == \"label_encode\":\n            self.domain_label_transformer = LabelEncoder()\n            return self.domain_label_transformer.fit_transform(x)\n        else:\n            raise NotImplementedError(f\"Method {self.config.domain_labels} not \"\n                                      f\"implemented.\")\n\n    def get_passthrough_columns(self, data: pd.DataFrame,\n                                passthrough_columns: List[str] = None,\n                                domain_label_colname: Optional[str] = None,\n                                target_colname: Optional[str] = None):\n        if passthrough_columns is None:\n            passthrough_columns = []\n\n        if self.config.passthrough_columns:\n            passthrough_columns += self.config.passthrough_columns\n\n        if self.config.numeric_features == \"passthrough\":\n            passthrough_columns += get_numeric_columns(data)\n\n        elif self.config.numeric_features == \"map_values\":\n            # add any unmapped numeric columns\n            unmapped_numerics = [f for f in get_numeric_columns(data) if self.feature_list[f].value_mapping is None]\n            passthrough_columns += unmapped_numerics\n\n        if self.config.categorical_features == \"passthrough\":\n            passthrough_columns += get_categorical_columns(data)\n\n        if domain_label_colname and (\n                domain_label_colname not in passthrough_columns):\n            logging.debug(f\"adding domain label column {domain_label_colname} \"\n                          f\"to passthrough columns\")\n            passthrough_columns.append(domain_label_colname)\n        if not self.config.map_targets:\n            passthrough_columns.append(target_colname)\n\n        return list(set(passthrough_columns))\n\n    def map_names_extended(self, colnames: List[str]) -> List[str]:\n        \"\"\"Map the original feature names to any extended feature names.\"\"\"\n        assert self.feature_list is not None, \\\n            \"Feature list is required to map extended feature names.\"\n        names_out = []\n        for c in colnames:\n            if (self.feature_list[c].name_extended is not None) and \\\n                    (self.config.map_targets\n                     or not self.feature_list[c].is_target):\n                names_out.append(self.feature_list[c].name_extended)\n            else:\n                names_out.append(c)\n        return names_out\n\n    def fit_transform(self, data: pd.DataFrame, train_idxs: List[int],\n                      domain_label_colname: Optional[str] = None,\n                      target_colname: Optional[str] = None,\n                      passthrough_columns: List[str] = None) -> pd.DataFrame:\n        \"\"\"Fit a feature_transformer and apply it to the input features.\"\"\"\n        logging.info(f\"transforming columns\")\n        if self.config.passthrough_columns == \"all\":\n            logging.info(\"passthrough is 'all'; data will not be preprocessed \"\n                         \"by tableshift.\")\n            if self.config.use_extended_names:\n                logging.warning(\n                    \"passthrough is 'all' but \"\n                    \"config.use_extended_names is True; extended \"\n                    \"names are not applied when passthrough is 'all'. Try \"\n                    \"setting numeric_columns='passthough', \"\n                    \"categorical_columns='passthrough' instead.\")\n            return data\n\n        passthrough_columns = self.get_passthrough_columns(\n            data,\n            passthrough_columns,\n            target_colname=target_colname)\n\n        # All non-domain label passthrough columns will be cast to their\n        # original type post-transformation (ColumnTransformer actually casts\n        # all columns to object type).\n        dtypes_in = data.dtypes.to_dict()\n\n        post_transform_cast_dtypes = (\n            {c: dtypes_in[c] for c in passthrough_columns if\n             c != domain_label_colname}\n            if passthrough_columns else None)\n\n        self._check_inputs(data)\n\n        # Fit the feature transformer and apply it.\n        self.fit_feature_transformer(data, train_idxs, passthrough_columns)\n        transformed = self.transform_features(data)\n\n        transformed = self._post_transform(\n            transformed, cast_dtypes=post_transform_cast_dtypes)\n\n        if domain_label_colname:\n            # Case: fit the domain label transformer and apply it.\n            transformed.loc[:, domain_label_colname] = \\\n                self.fit_transform_domain_labels(\n                    transformed.loc[:, domain_label_colname])\n        self._post_transform_summary(transformed)\n        logging.info(\"transforming columns complete.\")\n        return transformed\n"}
{"type": "source_file", "path": "tableshift/datasets/anes.py", "content": "\"\"\"\nUtilities for ANES Time Series Cumulative Data File.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\nList of variables: https://electionstudies.org/wp-content/uploads/2019/09/anes_timeseries_cdf_codebook_Varlist.pdf\nCodebook: https://electionstudies.org/wp-content/uploads/2022/09/anes_timeseries_cdf_codebook_var_20220916.pdf\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, cat_dtype, FeatureList\n\n# Note that \"state\" feature is named as VCF0901b; see below. Note that '99' is\n# also a valid value, but it contains all missing targets .\nANES_STATES = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL',\n               'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD',\n               'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ',\n               'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN',\n               'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n\n# Note that \"year\" feature is named as VCF0004; see below.\nANES_YEARS = [1948, 1952, 1954, 1956, 1958, 1960, 1962, 1964, 1966, 1968, 1970,\n              1972, 1974, 1976, 1978, 1980, 1982, 1984, 1986, 1988, 1990, 1992,\n              1994, 1996, 1998, 2000, 2002, 2004, 2008, 2012, 2016, 2020]\n\n# U.S. Census Regions; see 'VCF0112' feature below.\nANES_REGIONS = ['MISSING', '1.0', '2.0', '4.0', '3.0']\n\n# This is a very preliminary feature list. We should probably\n# try to find a good reference/principled heuristics for selecting these.\n# I generally tried to select a few from each category, with an emphasis on\n# questions that would be asked/relevant every year (i.e. not\n# questions about Kennedy, Vietnam, Cold War, etc.).\n# Only pre-election questions. Also dropped questions that were\n# asked in only 3 or fewer years.\n\n# We give the actual coding values for potential sensitive variables; for others\n# we mostly give the question title; see the documentation linked above for\n# details.\n\nANES_FEATURES = FeatureList(features=[\n    Feature('VCF0702', int, \"DID RESPONDENT VOTE IN THE NATIONAL \"\n                            \"ELECTIONS 1. No, did not vote 2. Yes, \"\n                            \"voted 0. DK; NA; no Post IW; refused to \"\n                            \"say if voted; Washington D.C. (\"\n                            \"presidential years only)\",\n            is_target=True,\n            name_extended='voted in national election'),\n    Feature(\"VCF0004\", int, name_extended=\"election year\"),\n    Feature(\"VCF0901b\", cat_dtype, \"\"\"State of interview - state postal \n    abbreviation, 99. NA; wrong district identified (2000) INAP. question not \n    used\"\"\",\n            name_extended='state'),\n\n    # PARTISANSHIP AND ATTITUDES TOWARDS PARTIES\n    Feature('VCF0218', float,\n            name_extended=\"Democratic Party feeling thermometer\"),\n    Feature('VCF0224', float,\n            name_extended=\"Republican Party feeling thermometer\"),\n    Feature('VCF0301', cat_dtype,\n            \"\"\"Generally speaking, do you usually think of yourself as a \n            Republican, a Democrat, an Independent, or what? (IF REPUBLICAN \n            OR DEMOCRAT) you call yourself a strong (REP/DEM) or a not very \n            strong (REP/DEM)? (IF INDEPENDENT, OTHER [1966 AND LATER:] OR NO \n            PREFERENCE; 2008: OR DK) Do you think of yourself as closer to \n            the Republican or Democratic party?\"\"\",\n            name_extended=\"think of yourself as closer to the Republican or Democratic party\",\n            value_mapping={\n                '0.0': \"no answer\",\n                '1.0': \"Strong Democrat\",\n                '2.0': \"Weak Democrat\",\n                '3.0': \"Independent - Democrat\",\n                '4.0': \"Independent - Independent\",\n                '5.0': \"Independent - Republican\",\n                '6.0': \"Weak Republican\",\n                '7.0': \"Strong Republican\",\n            }),\n    Feature('VCF0302', cat_dtype,\n            \"Generally speaking, do you usually think of yourself as a \"\n            \"Republican, a Democrat, an Independent, or what?\",\n            name_extended='party identification',\n            value_mapping={\n                '1.0': \"Republican\",\n                '2.0': \"Independent\",\n                '3.0': \"No preference; none; neither\",\n                '4.0': \"Other\",\n                '5.0': \"Democrat\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\", }),\n    Feature('VCF9008', cat_dtype,\n            \"\"\"Which party do you think would do a better job of handling the \n            problem of pollution and (1990,1994: protection of) the \n            environment?\"\"\",\n            name_extended='party preference on pollution and environment',\n            value_mapping={\n                '1.0': \"Better by Democrats\",\n                '3.0': \"Same by both\",\n                '5.0': \"Better by Republicans\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF9010', cat_dtype,\n            \"\"\"Do you think inflation would be handled better by the \n            Democrats, by the Republicans, or about the same by both?\"\"\",\n            name_extended='party preference on inflation',\n            value_mapping={\n                '1.0': \"Better by Democrats\",\n                '3.0': \"Same by both\",\n                '5.0': \"Better by Republicans\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF9011', cat_dtype,\n            \"\"\"Do you think the problems of unemployment would be handled \n            better by the Democrats, by the Republicans, or about the same by \n            both?\"\"\",\n            name_extended='party preference on unemployment',\n            value_mapping={\n                '1.0': \"Better by Democrats\",\n                '3.0': \"Same by both\",\n                '5.0': \"Better by Republicans\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF9201', cat_dtype,\n            \"\"\"(I’d like to know what you think about each of our political \n            parties. After I read the name of a political party, please rate \n            it on a scale from 0 to 10, where 0 means you strongly dislike \n            that party and 10 means that you strongly like that party. If I \n            come to a party you haven’t heard of or you feel you do not know \n            enough about, just say so.) [The first party is: / Using the same \n            scale where would you place:] the Democratic party {INTERVIEWER: \n            DO NOT PROBE DON’T KNOW}\"\"\",\n            name_extended='like-dislike scale placement for democratic party (0-10)',\n            na_values=(-7., -8., -9.)),\n    Feature('VCF9202', cat_dtype,\n            \"\"\"(I’d like to know what you think about each of our political \n            parties. After I read the name of a political party, please rate \n            it on a scale from 0 to 10, where 0 means you strongly dislike \n            that party and 10 means that you strongly like that party. If I \n            come to a party you haven’t heard of or you feel you do not know \n            enough about, just say so.) [The first party is: / Using the same \n            scale where would you place:] the Republican party {INTERVIEWER: \n            DO NOT PROBE DON’T KNOW}\"\"\",\n            name_extended='like-dislike scale placement for republican party (0-10)',\n            na_values=(-7., -8., -9.)\n            ),\n    Feature('VCF9203', cat_dtype,\n            \"\"\"Would you say that any of the parties in the United States \n            represents your views reasonably well? {INTERVIEWER: DO NOT PROBE \n            DON’T KNOW}\"\"\",\n            name_extended='do any of the parties in the U.S. represent views reasonably well',\n            value_mapping={\n                '1.0': \"Yes\",\n                '2.0': \"No\",\n                '-8.0': \"no answer\",\n                '-9.0': \"no answer\"}),\n    Feature('VCF9204', cat_dtype,\n            \"\"\"(Would you say that any of the parties in the United States \n            represents your views reasonably well?) Which party represents \n            your views best? {INTERVIEWER: DO NOT PROBE DON’T KNOW}\"\"\",\n            name_extended='party in US that represents views best',\n            value_mapping={\n                '1.0': \"Democratic\",\n                '2.0': \"Republican\",\n                '7.0': \"Other\",\n                '-8.0': \"no answer\",\n                '-9.0': \"no answer\", }),\n    Feature('VCF9205', cat_dtype,\n            \"\"\"Which party do you think would do a better job of handling the \n            nation’s economy, the Democrats, the Republicans, or wouldn’t \n            there be much difference between them? {1996: IF ‘NO DIFFERENCE’ \n            AND ‘NEITHER PARTY’ ARE VOLUNTEERED, DO NOT PROBE RESPONSES. \n            2000-later: IF ‘DK’ OR ‘NEITHER PARTY’ IS VOLUNTEERED, DO NOT \n            PROBE]}\"\"\",\n            name_extended='which political party represents views best',\n            value_mapping={\n                '1.0': 'Democrats',\n                '2.0': 'No difference',\n                '3.0': 'Republican',\n                '7.0': 'Neither party',\n                '-8.0': \"no answer\",\n                '-9.0': \"no answer\"}),\n    Feature('VCF9206', cat_dtype,\n            \"\"\"Do you think it is better when one party controls both the \n            presidency and Congress; better when control is split between the \n            Democrats and Republicans, or doesn’t it matter?\"\"\",\n            name_extended='better when one party controls both presidency and congress or when control is split',\n            value_mapping={\n                '1.0': \"One party control both \",\n                '2.0': \"Control is split\",\n                '3.0': \"It doesn’t matter\",\n                '-8.0': \"no answer\",\n                '-9.0': \"no answer\"}),\n\n    # PERCEIVED POSITIONS OF PARTIES\n    Feature('VCF0521', cat_dtype,\n            \"\"\"Which party do you think is more likely to favor a stronger [\n            1978,1980, 1984: more powerful; 1988,1992: a powerful] government \n            in Washington – the Democrats, the Republicans, or wouldn’t there \n            be any difference between them on this?\"\"\",\n            name_extended='which party favors stronger government',\n            value_mapping={\n                '0.0': \"no answer\",\n                '1.0': \"Democrats\",\n                '2.0': \"No difference\",\n                '3.0': \"Republicans\",\n                '8.0': \"DK which party\",\n            }),\n    Feature('VCF0523', cat_dtype,\n            \"\"\"Which political party do you think is more in favor of cutting \n            military spending - the Democrats, the Republicans, or wouldn’t \n            there be much difference between them?\"\"\",\n            name_extended='which party favors military spending cut',\n            value_mapping={\n                '1.0': \"Democrats\",\n                '2.0': \"Not much difference\",\n                '3.0': \"Republicans\",\n                '8.0': \"no answer\",\n                '0.0': \"no answer\"}),\n\n    # CANDIDATE AND INCUMBENT EVALUATIONS\n    Feature('VCF0428', float, name_extended=\"President thermometer\",\n            na_values=(98., 99.)),\n    Feature('VCF0429', float, name_extended=\"Vice-president thermometer\",\n            na_values=(98., 99.)),\n\n    # CANDIDATE/INCUMBENT PERFORMANCE EVALUATIONS\n    Feature('VCF0875', cat_dtype, \"MENTION 1: WHAT IS THE MOST IMPORTANT \"\n                                  \"NATIONAL PROBLEM\",\n            name_extended='most important national problem',\n            value_mapping={\n                '1.0': \"AGRICULTURAL\",\n                '2.0': \"ECONOMICS; BUSINESS; CONSUMER ISSUES\",\n                '3.0': \"FOREIGN AFFAIRS AND NATIONAL DEFENSE\",\n                '4.0': \"GOVERNMENT FUNCTIONING\",\n                '5.0': \"LABOR ISSUES\",\n                '6.0': \"NATURAL RESOURCES\",\n                '7.0': \"PUBLIC ORDER\",\n                '8.0': \"RACIAL PROBLEMS\",\n                '9.0': \"SOCIAL WELFARE\",\n                '97.0': \"Other problems\",\n                '98.0': \"no answer\",\n                '99.0': \"no answer\",\n                '0.0': \"no answer\"}),\n    Feature('VCF9052', cat_dtype,\n            \"\"\"Let’s talk about the country as a whole. Would you say that \n            things in the country are generally going very well, fairly well, \n            not too well or not well at all?\"\"\",\n            name_extended='are things in U.S. going well or not',\n            value_mapping={\n                '1.0': \"Very well\",\n                '2.0': \"Fairly well\",\n                '4.0': \"Not too well\",\n                '5.0': \"Not well at all\",\n                '8.0': \"no answer\",\n                '9.0': \"no answer\"}),\n    # ISSUES\n    Feature('VCF0809', cat_dtype, \"GUARANTEED JOBS AND INCOME SCALE.\",\n            name_extended=\"guaranteed jobs and income scale (support/don't support)\"),\n    Feature('VCF0839', cat_dtype, \"GOVERNMENT SERVICES-SPENDING SCALE\",\n            name_extended='government services and spending scale (fewer/more services)'),\n    Feature('VCF0822', cat_dtype,\n            \"\"\"As to the economic policy of the government – I mean steps \n            taken to fight inflation or unemployment – would you say the \n            government is doing a good job, only fair, or a poor job?\"\"\",\n            name_extended='rating of government economic policy',\n            value_mapping={\n                '1.0': \"Poor job\",\n                '2.0': \"Only fair\",\n                '3.0': \"Good job\",\n                '0.0': \"no answer\",\n                '9.0': \"no answer\"}),\n    Feature('VCF0870', cat_dtype, \"BETTER OR WORSE ECONOMY IN PAST YEAR\",\n            name_extended='better or worse economy in past year',\n            value_mapping={\n                '1.0': \"Better\",\n                '3.0': \"Stayed same\",\n                '5.0': \"Worse\",\n                '0.0': \"no answer\",\n                '8.0': \"no answer\"}),\n    Feature('VCF0843', cat_dtype, \"DEFENSE SPENDING SCALE\",\n            name_extended='defense spending scale (decrease/increase)',\n            na_values=(0.,)\n            ),\n    Feature('VCF9045', cat_dtype,\n            \"POSITION OF THE U.S. WEAKER/STRONGER IN THE PAST YEAR\",\n            name_extended='position of the U.S. in past year',\n            value_mapping={\n                '1.0': \"Weaker\",\n                '3.0': \"Same\",\n                '5.0': \"Stronger\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF0838', cat_dtype, \"BY LAW, WHEN SHOULD ABORTION BE ALLOWED\",\n            name_extended='when should abortion be allowed by law',\n            value_mapping={\n                '1.0': \"By law, abortion should never be permitted.\",\n                '2.0': \"The law should permit abortion only in case of rape, incest, or when the woman’s life is in danger.\",\n                '3.0': \"The law should permit abortion for reasons other than rape, incest, or danger to the woman’s life, but only after the need for the abortion has been clearly established.\",\n                '4.0': \"By law, a woman should always be able to obtain an abortion as a matter of personal choice.\",\n                '9.0': \"Don't know or other\",\n                '0.0': \"no answer\"}),\n    Feature('VCF9239', cat_dtype, \"HOW IMPORTANT IS GUN CONTROL ISSUE TO R\",\n            name_extended='importance of gun control',\n            value_mapping={\n                '1.0': \"Extremely important \",\n                '2.0': \"Very important\",\n                '3.0': \"Somewhat important \",\n                '4.0': \"Not too important\",\n                '5.0': \"Not at all important\",\n                '-8.0': \"no answer\",\n                '-9.0': \"no answer\"}),\n    # IDEOLOGY AND VALUES\n    Feature('VCF0803', cat_dtype, \"LIBERAL-CONSERVATIVE SCALE\",\n            name_extended='liberal-conservative scale',\n            value_mapping={\n                '1.0': \"Extremely liberal\",\n                '2.0': \"Liberal\",\n                '3.0': \"Slightly liberal\",\n                '4.0': \"Moderate, middle of the road \",\n                '5.0': \"Slightly conservative\",\n                '6.0': \"Conservative\",\n                '7.0': \"Extremely conservative\",\n                '9.0': \"Don't know; haven’t thought much about it\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0846', cat_dtype, \"IS RELIGION IMPORTANT TO RESPONDENT\",\n            name_extended='importance of religion',\n            value_mapping={\n                '1.0': \"Yes, important\",\n                '2.0': \"Little to no importance\",\n                '0.0': \"no answer\",\n                '8.0': \"no answer\"}),\n    # SYSTEM SUPPORT\n    Feature('VCF0601', cat_dtype, \"APPROVE PARTICIPATION IN PROTESTS\",\n            name_extended='approve participation in protests',\n            value_mapping={\n                '1.0': \"Disapprove\",\n                '2.0': \"Pro-con, depends, don’t know\",\n                '3.0': \"Approve\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0606', cat_dtype, \"HOW MUCH DOES THE FEDERAL GOVERNMENT WASTE \"\n                                  \"TAX MONEY\",\n            name_extended='how much does federal government waste tax money',\n            value_mapping={\n                '1.0': \"A lot\",\n                '2.0': \"Some\",\n                '3.0': \"Not very much\",\n                '9.0': \"Don't know\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0612', cat_dtype, \"VOTING IS THE ONLY WAY TO HAVE A SAY IN \"\n                                  \"GOVERNMENT\",\n            name_extended='voting is the only way to have a say in government',\n            value_mapping=\n            {\n                '1.0': \"Agree\",\n                '2.0': \"Disagree\",\n                '9.0': \"Don't know or not sure\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0615', cat_dtype, \"MATTER WHETHER RESPONDENT VOTES OR NOT\",\n            name_extended='it matters whether I vote',\n            value_mapping={\n                '1.0': \"Agree\",\n                '2.0': \"Disagree\",\n                '3.0': \"Neither agree nor disagree\",\n                '9.0': \"Don't know or not sure\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0616', cat_dtype, \"SHOULD THOSE WHO DON’T CARE ABOUT ELECTION \"\n                                  \"OUTCOME VOTE\",\n            name_extended=\"those who don't care about election outcome should vote\",\n            value_mapping={\n                '1.0': \"Agree\",\n                '2.0': \"Disagree\",\n                '3.0': \"Neither agree nor disagree\",\n                '9.0': \"Don't know or not sure\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0617', cat_dtype, \"SHOULD SOMEONE VOTE IF THEIR PARTY CAN’T \"\n                                  \"WIN\",\n            name_extended=\"someone should vote if their party can't win\",\n            value_mapping={\n                '1.0': \"Agree\",\n                '2.0': \"Disagree\",\n                '9.0': \"Don't know or not sure\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0310', cat_dtype, \"INTEREST IN THE ELECTIONS\",\n            name_extended='interest in the elections',\n            value_mapping={\n                '1.0': \"Not much interested \",\n                '2.0': \"Somewhat interested \",\n                '3.0': \"Very much interested \",\n                '9.0': \"Don't know\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0743', cat_dtype, \"DOES R BELONG TO POLITICAL ORGANIZATION OR \"\n                                  \"CLUB\",\n            name_extended='belongs to political organization or club',\n            value_mapping={'1.0': \"Yes\", '5.0': \"No\", '9.0': \"no answer\"}),\n    Feature('VCF0717', cat_dtype, \"RESPONDENT TRY TO INFLUENCE THE VOTE OF \"\n                                  \"OTHERS DURING THE CAMPAIGN\",\n            name_extended='tried to influence others during campaign',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0718', cat_dtype, \"RESPONDENT ATTEND POLITICAL \"\n                                  \"MEETINGS/RALLIES DURING THE CAMPAIGN\",\n            name_extended='attended political meetings/rallies during campaign',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0720', cat_dtype, \"RESPONDENT DISPLAY CANDIDATE \"\n                                  \"BUTTON/STICKER DURING THE CAMPAIGN\",\n            name_extended='displayed candidate button/sticker during campaign',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0721', cat_dtype, \"RESPONDENT DONATE MONEY TO PARTY OR \"\n                                  \"CANDIDATE DURING THE CAMPAIGN\",\n            name_extended='donated money to party or candidate during campaign',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n\n    # REGISTRATION, TURNOUT, AND VOTE CHOICE\n    Feature('VCF0701', cat_dtype, \"REGISTERED TO VOTE PRE-ELECTION\",\n            name_extended='registered to vote pre-election',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n\n    # MEDIA\n    Feature('VCF0675', cat_dtype,\n            \"HOW MUCH OF THE TIME DOES RESPONDENT TRUST THE \"\n            \"MEDIA TO REPORT FAIRLY\",\n            name_extended='how much of the time can you trust the media to report the news fairly',\n            value_mapping={\n                '1.0': \"Just about always\",\n                '2.0': \"Most of the time\",\n                '3.0': \"Only some of the time\",\n                '4.0': \"Almost never\",\n                '5.0': \"Never\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF0724', cat_dtype, \"WATCH TV PROGRAMS ABOUT THE ELECTION \"\n                                  \"CAMPAIGNS\",\n            name_extended='watched TV programs about the election campaigns',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0725', cat_dtype, \"HEAR PROGRAMS ABOUT CAMPAIGNS ON THE RADIO \"\n                                  \"2- CATEGORY\",\n            name_extended='heard radio programs about the election campaigns',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0726', cat_dtype, \"ARTICLES ABOUT ELECTION CAMPAIGNS IN \"\n                                  \"MAGAZINES\",\n            name_extended='read about the election campaigns in magazines',\n            value_mapping={\n                '1.0': \"No \",\n                '2.0': \"Yes\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0745', cat_dtype, \"SAW ELECTION CAMPAIGN INFORMATION ON THE \"\n                                  \"INTERNET\",\n            name_extended='saw election campaign information on the internet',\n            value_mapping={\n                '1.0': \"Yes\",\n                '5.0': \"No \",\n                '9.0': \"no answer\"}),\n\n    # PERSONAL AND DEMOGRAPHIC\n    Feature('VCF0101', int, \"RESPONDENT - AGE\",\n            name_extended='age'),\n    Feature('VCF0104', cat_dtype, name_extended='gender',\n            value_mapping={\n                '1': \"Male\",\n                '2': \"Female\",\n                '3': \"Other\",\n                '0': \"no answer\"}),\n    Feature('VCF0105a', cat_dtype, \"\"\"RACE-ETHNICITY SUMMARY, 7 CATEGORIES\"\"\",\n            name_extended='race/ethnicity',\n            value_mapping={\n                '1.0': \"White non-Hispanic\",\n                '2.0': \"Black non-Hispanic\",\n                '3.0': \"Asian or Pacific Islander, non-Hispanic\",\n                '4.0': \"American Indian or Alaska Native non-Hispanic\",\n                '5.0': \"Hispanic\",\n                '6.0': \"Other or multiple races, non-Hispanic\",\n                '7.0': \"Non-white and non-black\",\n                '9.0': \"no answer\"}),\n    Feature('VCF0115', cat_dtype,\n            \"\"\"RESPONDENT - OCCUPATION GROUP 6-CATEGORY\"\"\",\n            name_extended='occupation group',\n            value_mapping={\n                '1.0': \"Professional and managerial\",\n                '2.0': \"Clerical and sales workers\",\n                '3.0': \"Skilled, semi-skilled and service workers\",\n                '4.0': \"Laborers, except farm\",\n                '5.0': \"Farmers, farm managers, farm laborers and foremen; forestry and fishermen\",\n                '6.0': \"Homemakers\",\n                '0.0': \"no answer\"}),\n    Feature('VCF0140a', cat_dtype, \"\"\"RESPONDENT - EDUCATION 7-CATEGORY\"\"\",\n            name_extended='education level',\n            value_mapping={\n                '1.0': \"8 grades or less (‘grade school’)\",\n                '2.0': \"9-12 grades (‘high school’), no diploma/equivalency; less than high school credential (2020)\",\n                '3.0': \"12 grades, diploma or equivalency\",\n                '4.0': \"12 grades, diploma or equivalency plus non-academic training\",\n                '5.0': \"Some college, no degree; junior/community college level degree (AA degree)\",\n                '6.0': \"BA level degrees\",\n                '7.0': \"Advanced degrees incl. LLB\",\n                '8.0': \"Don't know\",\n                '9.0': \"no answer\"}),\n    Feature('VCF0112', cat_dtype, \"\"\"Region - U.S. Census 1. Northeast (CT, \n    ME, MA, NH, NJ, NY, PA, RI, VT) 2. North Central (IL, IN, IA, KS, MI, MN, \n    MO, NE, ND, OH, SD, WI) 3. South (AL, AR, DE, D.C., FL, GA, KY, LA, MD, \n    MS, NC, OK, SC,TN, TX, VA, WV) 4. West (AK, AZ, CA, CO, HI, ID, MT, NV, \n    NM, OR, UT, WA, WY)\"\"\",\n            name_extended='US census region',\n            value_mapping={\n                # (CT, ME, MA, NH, NJ, NY, PA, RI, VT)\n                '1.0': \"Northeast\",\n                # (IL, IN, IA, KS, MI, MN, MO, NE, ND, OH, SD, WI)\n                '2.0': \"North Central\",\n                # (AL, AR, DE, D.C., FL, GA, KY, LA, MD, MS, NC, OK, SC,TN, TX, VA, WV) 4. West (AK, AZ, CA, CO, HI, ID, MT, NV, NM, OR, UT, WA, WY)\n                '3.0': \"South\",\n                # (AK, AZ, CA, CO, HI, ID, MT, NV, NM, OR, UT, WA, WY)\n                '4.0': 'West', }),\n],\n    documentation=\"https://electionstudies.org/data-center/anes-time-series-cumulative-data-file/\")\n\n\ndef preprocess_anes(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.dropna(subset=[ANES_FEATURES.target])\n    df[ANES_FEATURES.target] = (\n            df[ANES_FEATURES.target].astype(float) == 2.0).astype(int)\n    for f in ANES_FEATURES.features:\n        if f.kind == cat_dtype:\n            df[f.name] = df[f.name].fillna(\"no answer\").apply(str) \\\n                .astype(\"category\")\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/college_scorecard.py", "content": "\"\"\"\nUtilities for the College Scorecard dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nimport pandas as pd\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nCOLLEGE_SCORECARD_FEATURES = FeatureList(features=[\n        Feature('C150_4', int, is_target=True,\n                name_extended=\"Completion rate for first-time, full-time \"\n                              \"students at four-year institutions (150% of \"\n                              \"expected time to completion/6 years)\"),\n        Feature('STABBR', cat_dtype, name_extended='State postcode'),\n        Feature('AccredAgency', cat_dtype, name_extended='Accreditor for institution'),\n        Feature('sch_deg', float, name_extended='Predominant degree awarded (recoded 0s and 4s)'),\n        Feature('main', cat_dtype, name_extended='Flag for main campus'),\n        Feature('NUMBRANCH', int, name_extended='Number of branch campuses'),\n        Feature('HIGHDEG', cat_dtype, name_extended='Highest degree awarded',\n                value_mapping={\n                        0: \"Non-degree-granting\",\n                        1: \"Certificate degree\",\n                        2: \"Associate degree\",\n                        3: \"Bachelor's degree\",\n                        4: \"Graduate degree\"}),\n        Feature('CONTROL', cat_dtype, name_extended='Control of institution'),\n        Feature('region', cat_dtype, name_extended='Region (IPEDS)'),\n        Feature('LOCALE', cat_dtype, name_extended='Locale of institution'),\n        Feature('locale2', float, name_extended='Degree of urbanization of institution'),\n        Feature('CCBASIC', cat_dtype, name_extended='Carnegie Classification -- basic'),\n        Feature('CCSIZSET', cat_dtype, name_extended='Carnegie Classification -- size and setting'),\n        Feature('HBCU', cat_dtype, name_extended='Flag for Historically Black College and University'),\n        Feature('ADM_RATE', float, name_extended='Admission rate'),\n        Feature('ADM_RATE_ALL', float, name_extended='Admission rate for all campuses rolled up to the 6-digit OPE ID'),\n        Feature('SATVRMID', float, name_extended='Midpoint of SAT scores at the institution (critical reading)'),\n        Feature('SATMTMID', float, name_extended='Midpoint of SAT scores at the institution (math)'),\n        Feature('SATWRMID', float, name_extended='Midpoint of SAT scores at the institution (writing)'),\n        Feature('ACTCMMID', float, name_extended='Midpoint of the ACT cumulative score'),\n        Feature('ACTENMID', float, name_extended='Midpoint of the ACT English score'),\n        Feature('ACTMTMID', float, name_extended='Midpoint of the ACT math score'),\n        Feature('ACTWRMID', float, name_extended='Midpoint of the ACT writing score'),\n        Feature('PCIP01', float, name_extended='Percentage of degrees awarded in Agriculture, Agriculture Operations, And Related Sciences.'),\n        Feature('PCIP03', float, name_extended='Percentage of degrees awarded in Natural Resources And Conservation.'),\n        Feature('PCIP04', float, name_extended='Percentage of degrees awarded in Architecture And Related Services.'),\n        Feature('PCIP05', float, name_extended='Percentage of degrees awarded in Area, Ethnic, Cultural, Gender, And Group Studies.'),\n        Feature('PCIP09', float, name_extended='Percentage of degrees awarded in Communication, Journalism, And Related Programs.'),\n        Feature('PCIP10', float, name_extended='Percentage of degrees awarded in Communications Technologies/Technicians And Support Services.'),\n        Feature('PCIP11', float, name_extended='Percentage of degrees awarded in Computer And Information Sciences And Support Services.'),\n        Feature('PCIP12', float, name_extended='Percentage of degrees awarded in Personal And Culinary Services.'),\n        Feature('PCIP13', float, name_extended='Percentage of degrees awarded in Education.'),\n        Feature('PCIP14', float, name_extended='Percentage of degrees awarded in Engineering.'),\n        Feature('PCIP15', float, name_extended='Percentage of degrees awarded in Engineering Technologies And Engineering-Related Fields.'),\n        Feature('PCIP16', float, name_extended='Percentage of degrees awarded in Foreign Languages, Literatures, And Linguistics.'),\n        Feature('PCIP19', float, name_extended='Percentage of degrees awarded in Family And Consumer Sciences/Human Sciences.'),\n        Feature('PCIP22', float, name_extended='Percentage of degrees awarded in Legal Professions And Studies.'),\n        Feature('PCIP23', float, name_extended='Percentage of degrees awarded in English Language And Literature/Letters.'),\n        Feature('PCIP24', float, name_extended='Percentage of degrees awarded in Liberal Arts And Sciences, General Studies And Humanities.'),\n        Feature('PCIP25', float, name_extended='Percentage of degrees awarded in Library Science.'),\n        Feature('PCIP26', float, name_extended='Percentage of degrees awarded in Biological And Biomedical Sciences.'),\n        Feature('PCIP27', float, name_extended='Percentage of degrees awarded in Mathematics And Statistics.'),\n        Feature('PCIP29', float, name_extended='Percentage of degrees awarded in Military Technologies And Applied Sciences.'),\n        Feature('PCIP30', float, name_extended='Percentage of degrees awarded in Multi/Interdisciplinary Studies.'),\n        Feature('PCIP31', float, name_extended='Percentage of degrees awarded in Parks, Recreation, Leisure, And Fitness Studies.'),\n        Feature('PCIP38', float, name_extended='Percentage of degrees awarded in Philosophy And Religious Studies.'),\n        Feature('PCIP39', float, name_extended='Percentage of degrees awarded in Theology And Religious Vocations.'),\n        Feature('PCIP40', float, name_extended='Percentage of degrees awarded in Physical Sciences.'),\n        Feature('PCIP41', float, name_extended='Percentage of degrees awarded in Science Technologies/Technicians.'),\n        Feature('PCIP42', float, name_extended='Percentage of degrees awarded in Psychology.'),\n        Feature('PCIP43', float, name_extended='Percentage of degrees awarded in Homeland Security, Law Enforcement, Firefighting And Related Protective Services.'),\n        Feature('PCIP44', float, name_extended='Percentage of degrees awarded in Public Administration And Social Service Professions.'),\n        Feature('PCIP45', float, name_extended='Percentage of degrees awarded in Social Sciences.'),\n        Feature('PCIP46', float, name_extended='Percentage of degrees awarded in Construction Trades.'),\n        Feature('PCIP47', float, name_extended='Percentage of degrees awarded in Mechanic And Repair Technologies/Technicians.'),\n        Feature('PCIP48', float, name_extended='Percentage of degrees awarded in Precision Production.'),\n        Feature('PCIP49', float, name_extended='Percentage of degrees awarded in Transportation And Materials Moving.'),\n        Feature('PCIP50', float, name_extended='Percentage of degrees awarded in Visual And Performing Arts.'),\n        Feature('PCIP51', float, name_extended='Percentage of degrees awarded in Health Professions And Related Programs.'),\n        Feature('PCIP52', float, name_extended='Percentage of degrees awarded in Business, Management, Marketing, And Related Support Services.'),\n        Feature('PCIP54', float, name_extended='Percentage of degrees awarded in History.'),\n        Feature('DISTANCEONLY', cat_dtype, name_extended='Flag for distance-education-only education'),\n        Feature('UGDS', float, name_extended='Enrollment of undergraduate degree-seeking students'),\n        Feature('UG', float, name_extended='Enrollment of all undergraduate students'),\n        Feature('UGDS_WHITE', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are white'),\n        Feature('UGDS_BLACK', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are black'),\n        Feature('UGDS_HISP', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are Hispanic'),\n        Feature('UGDS_ASIAN', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are Asian'),\n        Feature('UGDS_AIAN', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are American Indian/Alaska Native'),\n        Feature('UGDS_NHPI', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are Native Hawaiian/Pacific Islander'),\n        Feature('UGDS_2MOR', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are two or more races'),\n        Feature('UGDS_NRA', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are non-resident aliens'),\n        Feature('UGDS_UNKN', float, name_extended='Total share of enrollment of undergraduate degree-seeking students whose race is unknown'),\n        Feature('UGDS_WHITENH', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are white non-Hispanic'),\n        Feature('UGDS_BLACKNH', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are black non-Hispanic'),\n        Feature('UGDS_API', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are Asian/Pacific Islander'),\n        Feature('UGDS_AIANOld', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are American Indian/Alaska Native'),\n        Feature('UGDS_HISPOld', float, name_extended='Total share of enrollment of undergraduate degree-seeking students who are Hispanic'),\n        Feature('UG_NRA', float, name_extended='Total share of enrollment of undergraduate students who are non-resident aliens'),\n        Feature('UG_UNKN', float, name_extended='Total share of enrollment of undergraduate students whose race is unknown'),\n        Feature('UG_WHITENH', float, name_extended='Total share of enrollment of undergraduate students who are white non-Hispanic'),\n        Feature('UG_BLACKNH', float, name_extended='Total share of enrollment of undergraduate students who are black non-Hispanic'),\n        Feature('UG_API', float, name_extended='Total share of enrollment of undergraduate students who are Asian/Pacific Islander'),\n        Feature('UG_AIANOld', float, name_extended='Total share of enrollment of undergraduate students who are American Indian/Alaska Native'),\n        Feature('UG_HISPOld', float, name_extended='Total share of enrollment of undergraduate students who are Hispanic'),\n        Feature('PPTUG_EF', float, name_extended='Share of undergraduate, degree-/certificate-seeking students who are part-time'),\n        Feature('PPTUG_EF2', float, name_extended='Share of undergraduate, degree-/certificate-seeking students who are part-time'),\n        Feature('NPT4_PROG', float, name_extended='Average net price for the largest program at the institution for program-year institutions'),\n        Feature('COSTT4_A', float, name_extended='Average cost of attendance (academic year institutions)'),\n        Feature('COSTT4_P', float, name_extended='Average cost of attendance (program-year institutions)'),\n        Feature('TUITIONFEE_IN', float, name_extended='In-state tuition and fees'),\n        Feature('TUITIONFEE_OUT', float, name_extended='Out-of-state tuition and fees'),\n        Feature('TUITIONFEE_PROG', float, name_extended='Tuition and fees for program-year institutions'),\n        Feature('TUITFTE', float, name_extended='Net tuition revenue per full-time equivalent student'),\n        Feature('INEXPFTE', float, name_extended='Instructional expenditures per full-time equivalent student'),\n        Feature('AVGFACSAL', float, name_extended='Average faculty salary'),\n        Feature('PFTFAC', float, name_extended='Proportion of faculty that is full-time'),\n        Feature('PCTPELL', float, name_extended='Percentage of undergraduates who receive a Pell Grant'),\n        Feature('loan_ever', float, name_extended='Share of students who received a federal loan while in school',\n                na_values=('PrivacySuppressed',)),\n        Feature('pell_ever', float, name_extended='Share of students who received a Pell Grant while in school',\n                na_values=('PrivacySuppressed',)),\n        Feature('age_entry', float, name_extended='Average age of entry, via SSA data',\n                na_values=('PrivacySuppressed',)),\n        Feature('age_entry_sq', float, name_extended='Average of the age of entry squared',\n                na_values=('PrivacySuppressed',)),\n        Feature('agege24', float, name_extended='Percent of students over 23 at entry',\n                na_values=('PrivacySuppressed',)),\n        Feature('female', float, name_extended='Share of female students, via SSA data',\n                na_values=('PrivacySuppressed',)),\n        Feature('married', float, name_extended='Share of married students',\n                na_values=('PrivacySuppressed',)),\n        Feature('dependent', float, name_extended='Share of dependent students',\n                na_values=('PrivacySuppressed',)),\n        Feature('veteran', float, name_extended='Share of veteran students',\n                na_values=('PrivacySuppressed',)),\n        Feature('first_gen', float, name_extended='Share of first-generation students',\n                na_values=('PrivacySuppressed',)),\n        Feature('faminc', float, name_extended='Average family income',\n                na_values=('PrivacySuppressed',)),\n        Feature('md_faminc', float, name_extended='Median family income',\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_white', float, name_extended=\"Percent of the population from students' zip codes that is White, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_black', float, name_extended=\"Percent of the population from students' zip codes that is Black, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_asian', float, name_extended=\"Percent of the population from students' zip codes that is Asian, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_hispanic', float, name_extended=\"Percent of the population from students' zip codes that is Hispanic, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_ba', float, name_extended=\"Percent of the population from students' zip codes with a bachelor's degree over the age 25, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_grad_prof', float, name_extended=\"Percent of the population from students' zip codes over 25 with a professional degree, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('pct_born_us', float, name_extended=\"Percent of the population from students' zip codes that was born in the US, via Census data\",\n                na_values=('PrivacySuppressed',)),\n        Feature('median_hh_inc', float, name_extended='Median household income',\n                na_values=('PrivacySuppressed',)),\n        Feature('poverty_rate', float, name_extended='Poverty rate, via Census data',\n                na_values=('PrivacySuppressed',)),\n        Feature('unemp_rate', float, name_extended='Unemployment rate, via Census data',\n                na_values=('PrivacySuppressed',)),\n])\n\n\ndef preprocess_college_scorecard(df:pd.DataFrame)->pd.DataFrame:\n        df[COLLEGE_SCORECARD_FEATURES.target] = (df[COLLEGE_SCORECARD_FEATURES.target] > 0.5).astype(int)\n        return df"}
{"type": "source_file", "path": "tableshift/core/tasks.py", "content": "\"\"\"\nContains task configurations.\n\nA task is a set of features (including both predictors and a target variable)\nalong with a DataSource. Tasks are the fundamental benchmarks that comprise\nthe tableshift benchmark.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\nfrom .data_source import *\nfrom .features import FeatureList\n\nfrom tableshift.datasets import *\n\n\n@dataclass\nclass TaskConfig:\n    # The data_source_cls instantiates the DataSource,\n    # which fetches data and preprocesses it using a preprocess_fn.\n    data_source_cls: Any\n    # The feature_list describes the schema of the data *after* the\n    # preprocess_fn is applied. It is used to check the output of the\n    # preprocess_fn, and features are dropped or type-cast as necessary.\n    feature_list: FeatureList\n\n\n\n# Mapping of task names to their configs. An arbitrary number of tasks\n# can be created from a single data source, by specifying different\n# preprocess_fn and features.\n_TASK_REGISTRY: Dict[str, TaskConfig] = {\n    \"acsincome\":\n        TaskConfig(ACSDataSource,\n                   ACS_INCOME_FEATURES + ACS_SHARED_FEATURES),\n    \"acsfoodstamps\":\n        TaskConfig(ACSDataSource,\n                   ACS_FOODSTAMPS_FEATURES + ACS_SHARED_FEATURES),\n    \"acspubcov\":\n        TaskConfig(ACSDataSource,\n                   ACS_PUBCOV_FEATURES + ACS_SHARED_FEATURES),\n    \"acsunemployment\":\n        TaskConfig(ACSDataSource,\n                   ACS_UNEMPLOYMENT_FEATURES + ACS_SHARED_FEATURES),\n    \"adult\":\n        TaskConfig(AdultDataSource, ADULT_FEATURES),\n    \"anes\":\n        TaskConfig(ANESDataSource, ANES_FEATURES),\n    \"assistments\":\n        TaskConfig(AssistmentsDataSource, ASSISTMENTS_FEATURES),\n    \"brfss_diabetes\":\n        TaskConfig(BRFSSDataSource, BRFSS_DIABETES_FEATURES),\n    \"brfss_blood_pressure\":\n        TaskConfig(BRFSSDataSource, BRFSS_BLOOD_PRESSURE_FEATURES),\n    \"communities_and_crime\":\n        TaskConfig(CommunitiesAndCrimeDataSource, CANDC_FEATURES),\n    \"compas\":\n        TaskConfig(COMPASDataSource, COMPAS_FEATURES),\n    \"diabetes_readmission\":\n        TaskConfig(DiabetesReadmissionDataSource,\n                   DIABETES_READMISSION_FEATURES),\n    \"german\":\n        TaskConfig(GermanDataSource, GERMAN_FEATURES),\n    \"heloc\":\n        TaskConfig(HELOCDataSource, HELOC_FEATURES),\n    \"metamimic_alcohol\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_ALCOHOL_FEATURES),\n    \"metamimic_anemia\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_ANEMIA_FEATURES),\n    \"metamimic_atrial\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_ATRIAL_FEATURES),\n    \"metamimic_diabetes\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_DIABETES_FEATURES),\n    \"metamimic_heart\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_HEART_FEATURES),\n    \"metamimic_hypertension\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_HYPERTENSIVE_FEATURES),\n    \"metamimic_hypotension\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_HYPOTENSION_FEATURES),\n    \"metamimic_ischematic\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_ISCHEMATIC_FEATURES),\n    \"metamimic_lipoid\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_LIPOID_FEATURES),\n    \"metamimic_overweight\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_OVERWEIGHT_FEATURES),\n    \"metamimic_purpura\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_PURPURA_FEATURES),\n    \"metamimic_respiratory\":\n        TaskConfig(MetaMIMICDataSource, METAMIMIC_RESPIRATORY_FEATURES),\n    \"mimic_extract_los_3\":\n        TaskConfig(MIMICExtractDataSource, MIMIC_EXTRACT_LOS_3_FEATURES),\n    \"mimic_extract_los_3_selected\":\n        TaskConfig(MIMICExtractDataSource, MIMIC_EXTRACT_LOS_3_SELECTED_FEATURES),\n    \"mimic_extract_mort_hosp\":\n        TaskConfig(MIMICExtractDataSource, MIMIC_EXTRACT_MORT_HOSP_FEATURES),\n    \"mimic_extract_mort_hosp_selected\":\n        TaskConfig(MIMICExtractDataSource, MIMIC_EXTRACT_MORT_HOSP_SELECTED_FEATURES),\n    \"mooc\":\n        TaskConfig(MOOCDataSource, MOOC_FEATURES),\n    \"nhanes_cholesterol\":\n        TaskConfig(NHANESDataSource,\n                   NHANES_CHOLESTEROL_FEATURES + \\\n                   NHANES_SHARED_FEATURES),\n    \"nhanes_lead\":\n        TaskConfig(NHANESDataSource,\n                   NHANES_SHARED_FEATURES +\n                   NHANES_LEAD_FEATURES),\n    \"physionet\":\n        TaskConfig(PhysioNetDataSource, PHYSIONET_FEATURES),\n    \"electricity\":\n        TaskConfig(GrinstajnHFDataSource, ELECTRICITY_FEATURES),\n    \"bank-marketing\":\n        TaskConfig(GrinstajnHFDataSource, BANK_MARKETING_FEATURES),\n    \"california\":\n        TaskConfig(GrinstajnHFDataSource, CALIFORNIA_FEATURES),\n    'covertype':\n        TaskConfig(GrinstajnHFDataSource, COVERTYPE_FEATURES),\n    'credit':\n        TaskConfig(GrinstajnHFDataSource, CREDIT_FEATURES),\n    'default-of-credit-card-clients':\n        TaskConfig(GrinstajnHFDataSource, DEFAULT_OF_CREDIT_CLIENTS_FEATURES),\n    'eye_movements':\n        TaskConfig(GrinstajnHFDataSource, EYE_MOVEMENTS_FEATURES),\n    'Higgs':\n        TaskConfig(GrinstajnHFDataSource, HIGGS_FEATURES),\n    'MagicTelescope':\n        TaskConfig(GrinstajnHFDataSource, MAGIC_TELESCOPE_FEATURES),\n    'MiniBooNE':\n        TaskConfig(GrinstajnHFDataSource, MINI_BOONE_FEATURES),\n    'road-safety':\n        TaskConfig(GrinstajnHFDataSource, ROAD_SAFETY_FEATURES),\n    'pol':\n        TaskConfig(GrinstajnHFDataSource, POL_FEATURES),\n    'jannis':\n        TaskConfig(GrinstajnHFDataSource, JANNIS_FEATURES),\n    'house_16H':\n        TaskConfig(GrinstajnHFDataSource, HOUSE_16H_FEATURES),\n    'amazon':\n        TaskConfig(AmazonDataSource, AMAZON_FEATURES),\n    'appetency':\n        TaskConfig(KddCup2009DataSource, APPETENCY_FEATURES),\n    'churn':\n        TaskConfig(KddCup2009DataSource, CHURN_FEATURES),\n    'upselling':\n        TaskConfig(KddCup2009DataSource, UPSELLING_FEATURES),\n    'click':\n        TaskConfig(ClickDataSource, CLICK_FEATURES),\n    'kick':\n        TaskConfig(KickDataSource, KICK_FEATURES),\n    'product_sentiment_machine_hack':\n        TaskConfig(AutoMLBenchmarkDataSource, PROD_FEATURES),\n    'data_scientist_salary':\n        TaskConfig(AutoMLBenchmarkDataSource, SALARY_FEATURES),\n    'melbourne_airbnb':\n        TaskConfig(AutoMLBenchmarkDataSource, AIRBNB_FEATURES),\n    'news_channel':\n        TaskConfig(AutoMLBenchmarkDataSource, NEWS_CHANNEL_FEATURES),\n    'wine_reviews':\n        TaskConfig(AutoMLBenchmarkDataSource, WINE_REVIEWS_FEATURES),\n    'imdb_genre_prediction':\n        TaskConfig(AutoMLBenchmarkDataSource, IMDB_FEATURES),\n    'fake_job_postings2':\n        TaskConfig(AutoMLBenchmarkDataSource, FAKE_JOBS_FEATURES),\n    'kick_starter_funding':\n        TaskConfig(AutoMLBenchmarkDataSource, KICKSTARTER_FEATURES),\n    'jigsaw_unintended_bias100K':\n        TaskConfig(AutoMLBenchmarkDataSource, JIGSAW_FEATURES),\n    'iris':\n        TaskConfig(IrisDataSource, IRIS_FEATURES),\n    'dry-bean':\n        TaskConfig(DryBeanDataSource, DRY_BEAN_FEATURES),\n    'heart-disease':\n        TaskConfig(HeartDiseaseDataSource, HEART_DISEASE_FEATURES),\n    'wine':\n        TaskConfig(WineCultivarsDataSource, WINE_CULTIVARS_FEATURES),\n    'wine-quality':\n        TaskConfig(WineQualityDataSource, WINE_QUALITY_FEATURES),\n    'rice':\n        TaskConfig(RiceDataSource, RICE_FEATURES),\n    'breast-cancer':\n        TaskConfig(BreastCancerDataSource, BREAST_CANCER_FEATURES),\n    'cars':\n        TaskConfig(CarDataSource, CAR_FEATURES),\n    'raisin':\n        TaskConfig(RaisinDataSource, RAISIN_FEATURES),\n    'abalone':\n        TaskConfig(AbaloneDataSource, ABALONE_FEATURES),\n    'otto-products':\n        TaskConfig(OttoProductsDataSource, OTTO_FEATURES),\n    'sf-crime':\n        TaskConfig(SfCrimeDataSource, SF_CRIME_FEATURES),\n    'plasticc':\n        TaskConfig(PlasticcDataSource, PLASTICC_FEATURES),\n    'walmart':\n        TaskConfig(WalmartDataSource, WALMART_FEATURES),\n    'tradeshift':\n        TaskConfig(TradeShiftDataSource, TRADESHIFT_FEATURES),\n    'schizophrenia':\n        TaskConfig(SchizophreniaDataSource, SCHIZOPHRENIA_FEATURES),\n    'titanic':\n        TaskConfig(TitanicDataSource, TITANIC_FEATURES),\n    'santander-transactions':\n        TaskConfig(SantanderTransactionDataSource,\n                   SANTANDER_TRANSACTION_FEATURES),\n    'home-credit-default-risk':\n        TaskConfig(HomeCreditDefaultDataSource, HOME_CREDIT_DEFAULT_FEATURES),\n    'ieee-fraud-detection':\n        TaskConfig(IeeFraudDetectionDataSource, IEEE_FRAUD_DETECTION_FEATURES),\n    'safe-driver-prediction':\n        TaskConfig(SafeDriverPredictionDataSource,\n                   SAFE_DRIVER_PREDICTION_FEATURES),\n    'santander-customer-satisfaction':\n        TaskConfig(SantanderCustomerSatisfactionDataSource,\n                   SANTANDER_CUSTOMER_SATISFACTION_FEATURES),\n    'amex-default':\n        TaskConfig(AmexDefaultDataSource, AMEX_DEFAULT_FEATURES),\n    'ad-fraud':\n        TaskConfig(AdFraudDataSource, AD_FRAUD_FEATURES),\n    'college_scorecard':\n        TaskConfig(CollegeScorecardDataSource, COLLEGE_SCORECARD_FEATURES),\n}\n\n\ndef get_task_config(name: str) -> TaskConfig:\n    if name in _TASK_REGISTRY:\n        return _TASK_REGISTRY[name]\n    else:\n        raise NotImplementedError(f\"task {name} not implemented.\")\n"}
{"type": "source_file", "path": "tableshift/datasets/communities_and_crime.py", "content": "\"\"\"\nUtilities for the Communities and Crime dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nNA_VALUES = (\"?\",)\n\nCANDC_RESOURCES = [\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n    \"/communities/communities.data\"]\n\n# Column names of the raw input features from UCI.\nCANDC_INPUT_FEATURES = [\n    'state', 'county', 'community', 'communityname', 'fold', 'population',\n    'householdsize', 'racepctblack', 'racePctWhite', 'racePctAsian',\n    'racePctHisp', 'agePct12t21', 'agePct12t29', 'agePct16t24', 'agePct65up',\n    'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf',\n    'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc',\n    'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap',\n    'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov',\n    'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed',\n    'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu',\n    'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv',\n    'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par',\n    'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom',\n    'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5',\n    'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5',\n    'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell',\n    'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous',\n    'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup',\n    'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant',\n    'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos',\n    'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart',\n    'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ',\n    'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc',\n    'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn',\n    'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', 'PctSameState85',\n    'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n    'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop',\n    'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite',\n    'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor',\n    'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked', 'LandArea',\n    'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg',\n    'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn',\n    'PolicBudgPerPop', 'ViolentCrimesPerPop'\n]\n\nCANDC_STATE_LIST = ['1', '10', '11', '12', '13', '16', '18', '19', '2', '20',\n                    '21', '22', '23', '24', '25', '27', '28', '29', '32', '33',\n                    '34', '35', '36', '37', '38', '39', '4', '40', '41', '42',\n                    '44', '45', '46', '47', '48', '49', '5', '50', '51', '53',\n                    '54', '55', '56', '6', '8', '9']\n\nCANDC_FEATURES = FeatureList([\n    Feature('Target', int,\n            name_extended=\"Binary indicator for whether total number of violent crimes per 100K population exceeds threshold\",\n            is_target=True),\n    Feature('PctKids2Par', float,\n            name_extended='percentage of kids in family housing with two parents',\n            na_values=NA_VALUES),  # importance: 0.3411\n    Feature('pctUrban', float,\n            name_extended='percentage of people living in areas classified as urban',\n            na_values=NA_VALUES),  # importance: 0.1829\n    Feature('NumIlleg', float,\n            name_extended='number of kids born to never married',\n            na_values=NA_VALUES),  # importance: 0.1176\n    Feature('PctSameCity85', float,\n            name_extended='percent of people living in the same city as in 1985 (5 years before)',\n            na_values=NA_VALUES),  # importance: 0.1145\n    Feature('numbUrban', float,\n            name_extended='number of people living in areas classified as urban',\n            na_values=NA_VALUES),  # importance: 0.0863\n    Feature('PctNotHSGrad', float,\n            name_extended='percentage of people 25 and over that are not high school graduates',\n            na_values=NA_VALUES),  # importance: 0.0823\n    Feature('MedRentPctHousInc', float,\n            name_extended='median gross rent as a percentage of household income',\n            na_values=NA_VALUES),  # importance: 0.0291\n    Feature('HousVacant', float, name_extended='number of vacant households',\n            na_values=NA_VALUES),  # importance: 0.0183\n    Feature('agePct16t24', float,\n            name_extended='percentage of population that is 16-24 in age',\n            na_values=NA_VALUES),  # importance: 0.0147\n    Feature('LemasPctOfficDrugUn', float,\n            name_extended='percent of officers assigned to drug units',\n            na_values=NA_VALUES),  # importance: 0.0133\n    Feature('RentHighQ', float,\n            name_extended='rental housing - upper quartile rent',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctRecImmig8', float,\n            name_extended='percent of population who have immigrated within the last 8 years',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('indianPerCap', float,\n            name_extended='per capita income for native americans',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('pctWInvInc', float,\n            name_extended='percentage of households with investment/rent income in 1989',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctBornSameState', float,\n            name_extended='percent of people born in the same state as currently living',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('medFamInc', float, name_extended='median family income',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctSameHouse85', float,\n            name_extended='percent of people living in the same house as in 1985 (5 years before)',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctTeen2Par', float,\n            name_extended='percent of kids age 12-17 in two parent households',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PersPerOccupHous', float,\n            name_extended='mean persons per household', na_values=NA_VALUES),\n    # importance: 0.0\n    Feature('OtherPerCap', float,\n            name_extended='per capita income for people with other heritage',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctOccupMgmtProf', float,\n            name_extended='percentage of people 16 and over who are employed in management or professional occupations',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PolicAveOTWorked', float,\n            name_extended='police average overtime worked', na_values=NA_VALUES),\n    # importance: 0.0\n    Feature('OwnOccLowQuart', float,\n            name_extended='owner occupied housing - lower quartile value',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctLargHouseOccup', float,\n            name_extended='percent of all occupied households that are large (6 or more)',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('MedNumBR', float, name_extended='median number of bedrooms',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PolicPerPop', float,\n            name_extended='police officers per 100K population',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('agePct65up', float,\n            name_extended='percentage of population that is 65 and over in age',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctRecentImmig', float,\n            name_extended='percent of population who have immigrated within the last 3 years',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('LemasGangUnitDeploy', float, name_extended='gang unit deployed',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PolicCars', float, name_extended='number of police cars',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('PctForeignBorn', float,\n            name_extended='percent of people foreign born', na_values=NA_VALUES),\n    # importance: 0.0\n    Feature('PctImmigRec8', float,\n            name_extended='percentage of immigrants who immigated within last 8 years',\n            na_values=NA_VALUES),  # importance: 0.0\n    Feature('agePct12t21', float,\n            name_extended='percentage of population that is 12-21 in age',\n            na_values=NA_VALUES),  # importance: 0.0\n    ##################################################\n    ##################################################\n    # Feature('PctLargHouseFam', float,\n    #         name_extended='percent of family households that are large (6 or more)',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('racePctWhite', float,\n    #         name_extended='percentage of population that is caucasian',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('state', int, name_extended='US state (by number)',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('racePctHisp', float,\n    #         name_extended='percentage of population that is of hispanic heritage',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPopUnderPov', float,\n    #         name_extended='percentage of people under the poverty level',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctSpeakEnglOnly', float,\n    #         name_extended='percent of people who speak only English',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PopDens', float,\n    #         name_extended='population density in persons per square mile',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctEmploy', float,\n    #         name_extended='percentage of people 16 and over who are employed',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PersPerFam', float,\n    #         name_extended='mean number of people per family', na_values=NA_VALUES),\n    # # importance: 0.0\n    # Feature('PolicReqPerOffic', float,\n    #         name_extended='total requests for police per police officer',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctImmigRec5', float,\n    #         name_extended='percentage of immigrants who immigated within last 5 years',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctVacantBoarded', float,\n    #         name_extended='percent of vacant housing that is boarded up',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('population', float, name_extended='population', na_values=NA_VALUES),\n    # # importance: 0.0\n    # Feature('NumImmig', float,\n    #         name_extended='total number of people known to be foreign born',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('OwnOccMedVal', float,\n    #         name_extended='owner occupied housing - median value',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MalePctNevMarr', float,\n    #         name_extended='percentage of males who have never married',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasTotReqPerPop', float,\n    #         name_extended='total requests for police per 100K popuation',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('NumUnderPov', float,\n    #         name_extended='number of people under the poverty level',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctUnemployed', float,\n    #         name_extended='percentage of people 16 and over, in the labor force, and unemployed',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPolicWhite', float,\n    #         name_extended='percent of police that are caucasian',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasSwFTFieldOps', float,\n    #         name_extended='number of sworn full time police officers in field operations',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PersPerOwnOccHous', float,\n    #         name_extended='mean persons per owner occupied household',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('RentMedian', float, name_extended='rental housing - median rent',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPersDenseHous', float,\n    #         name_extended='percent of persons in dense housing (more than 1 person per room)',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctWorkMom', float,\n    #         name_extended='percentage of moms of kids under 18 in labor force',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('blackPerCap', float,\n    #         name_extended='per capita income for african americans',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('OfficAssgnDrugUnits', float,\n    #         name_extended='number of officers assigned to special drug units',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('RacialMatchCommPol', float,\n    #         name_extended='measure of the racial match between the community and the police force',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctImmigRec10', float,\n    #         name_extended='percentage of immigrants who immigated within last 10 years',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('racePctAsian', float,\n    #         name_extended='percentage of population that is of asian heritage',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('county', float, name_extended='numeric code for county',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('OwnOccHiQuart', float,\n    #         name_extended='owner occupied housing - upper quartile value',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctHousOwnOcc', float,\n    #         name_extended='percent of households owner occupied',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctHousNoPhone', float,\n    #         name_extended='percent of occupied housing units without phone',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('racepctblack', float,\n    #         name_extended='percentage of population that is african american',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctSameState85', float,\n    #         name_extended='percent of people living in the same state as in 1985 (5 years before)',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasTotalReq', float, name_extended='total requests for police',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('TotalPctDiv', float,\n    #         name_extended='percentage of population who are divorced',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctVacMore6Mos', float,\n    #         name_extended='percent of vacant housing that has been vacant more than 6 months',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('pctWRetire', float,\n    #         name_extended='percentage of households with retirement income in 1989',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('pctWFarmSelf', float,\n    #         name_extended='percentage of households with farm or self employment income in 1989',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('AsianPerCap', float,\n    #         name_extended='per capita income for people with asian heritage',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('pctWWage', float,\n    #         name_extended='percentage of households with wage or salary income in 1989',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctWorkMomYoungKids', float,\n    #         name_extended='percentage of moms of kids 6 and under in labor force',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LandArea', float, name_extended='land area in square miles',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctNotSpeakEnglWell', float,\n    #         name_extended='percent of people who do not speak English well',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPolicBlack', float,\n    #         name_extended='percent of police that are african american',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasSwFTFieldPerPop', float,\n    #         name_extended='sworn full time police officers in field operations per 100K population',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('pctWSocSec', float,\n    #         name_extended='percentage of households with social security income in 1989',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('agePct12t29', float,\n    #         name_extended='percentage of population that is 12-29 in age',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('HispPerCap', float,\n    #         name_extended='per capita income for people with hispanic heritage',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('whitePerCap', float,\n    #         name_extended='per capita income for caucasians', na_values=NA_VALUES),\n    # # importance: 0.0\n    # Feature('PctPolicHisp', float,\n    #         name_extended='percent of police that are hispanic',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('communityname', cat_dtype, name_extended='community name',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasSwornFT', float,\n    #         name_extended='number of sworn full time police officers',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MedOwnCostPctIncNoMtg', float,\n    #         name_extended='median owners cost as a percentage of household income',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PolicBudgPerPop', float,\n    #         name_extended='police operating budget per population',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPolicMinor', float,\n    #         name_extended='percent of police that are minority of any kind',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctYoungKids2Par', float,\n    #         name_extended='percent of kids 4 and under in two parent households',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPolicAsian', float,\n    #         name_extended='percent of police that are asian', na_values=NA_VALUES),\n    # # importance: 0.0\n    # Feature('medIncome', float, name_extended='median household income',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('FemalePctDiv', float,\n    #         name_extended='percentage of females who are divorced',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasSwFTPerPop', float,\n    #         name_extended='sworn full time police officers per 100K population',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctIlleg', float,\n    #         name_extended='percentage of kids born to never married',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctRecImmig10', float,\n    #         name_extended='percent of population who have immigrated within the last 10 years',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctImmigRecent', float,\n    #         name_extended='percentage of immigrants who immigated within last 3 years',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctEmplManu', float,\n    #         name_extended='percentage of people 16 and over who are employed in manufacturing',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('perCapInc', float, name_extended='per capita income',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('community', float, name_extended='numeric code for community',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('NumStreet', float,\n    #         name_extended='number of homeless people counted in the street',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MedOwnCostPctInc', float,\n    #         name_extended='median owners cost as a percentage of household income',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctLess9thGrade', float,\n    #         name_extended='percentage of people 25 and over with less than a 9th grade education',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('pctWPubAsst', float,\n    #         name_extended='percentage of households with public assistance income in 1989',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('NumKindsDrugsSeiz', float,\n    #         name_extended='number of different kinds of drugs seized',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PersPerRentOccHous', float,\n    #         name_extended='mean persons per rental household',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctEmplProfServ', float,\n    #         name_extended='percentage of people 16 and over who are employed in professional services',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctFam2Par', float,\n    #         name_extended='percentage of families (with kids) that are headed by two parents',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MalePctDivorce', float,\n    #         name_extended='percentage of males who are divorced',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MedYrHousBuilt', float,\n    #         name_extended='median year housing units built', na_values=NA_VALUES),\n    # # importance: 0.0\n    # Feature('RentLowQ', float,\n    #         name_extended='rental housing - lower quartile rent',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('MedRent', float,\n    #         name_extended='median gross rent (including utilities)',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctHousLess3BR', float,\n    #         name_extended='percent of housing units with less than 3 bedrooms',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctPersOwnOccup', float,\n    #         name_extended='percent of people in owner occupied households',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctOccupManu', float,\n    #         name_extended='percentage of people 16 and over who are employed in manufacturing',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PolicOperBudg', float, name_extended='police operating budget',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctWOFullPlumb', float,\n    #         name_extended='percent of housing without complete plumbing facilities',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('LemasPctPolicOnPatr', float,\n    #         name_extended='percent of sworn full time police officers on patrol',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('NumInShelters', float,\n    #         name_extended='number of people in homeless shelters',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctRecImmig5', float,\n    #         name_extended='percent of population who have immigrated within the last 5 years',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctHousOccup', float, name_extended='percent of housing occupied',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctUsePubTrans', float,\n    #         name_extended='percent of people using public transit for commuting',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('householdsize', float, name_extended='mean people per household',\n    #         na_values=NA_VALUES),  # importance: 0.0\n    # Feature('PctBSorMore', float,\n    #         name_extended='percentage of people 25 and over with a bachelors degree or higher education',\n    #         na_values=NA_VALUES),  # importance: 0.0\n\n\n],\n    documentation='https://archive.ics.uci.edu/ml/datasets/communities+and+crime')\n\n\ndef preprocess_candc(df: pd.DataFrame,\n                     target_threshold: float = 0.08) -> pd.DataFrame:\n    df = df.rename(columns={'ViolentCrimesPerPop': \"Target\"})\n\n    # The label of a community is 1 if that community is among the\n    # 70% of communities with the highest crime rate and 0 otherwise,\n    # following Khani et al. and Kearns et al. 2018.\n    df[\"Target\"] = (df[\"Target\"] >= target_threshold).astype(int)\n\n    return df\n"}
{"type": "source_file", "path": "tableshift/core/__init__.py", "content": "from .grouper import Grouper\nfrom .splitter import Splitter, FixedSplitter, RandomSplitter, DomainSplitter\nfrom .tabular_dataset import TabularDataset, DatasetConfig, CachedDataset\nfrom .features import PreprocessorConfig\nfrom .getters import get_dataset, get_iid_dataset"}
{"type": "source_file", "path": "tableshift/datasets/__init__.py", "content": "from .acs import *\nfrom .adult import *\nfrom .anes import *\nfrom .assistments import *\nfrom .automl_multimodal_benchmark import *\nfrom .brfss import *\nfrom .catboost_benchmarks import *\nfrom .college_scorecard import *\nfrom .communities_and_crime import *\nfrom .compas import *\nfrom .diabetes_readmission import *\nfrom .german import *\nfrom .grinsztajn import *\nfrom .heloc import *\nfrom .kaggle import *\nfrom .metamimic import *\nfrom .mimic_extract import *\nfrom .mooc import *\nfrom .nhanes import *\nfrom .physionet import *\nfrom .uci import *\n"}
{"type": "source_file", "path": "tableshift/core/discretization.py", "content": "# Author: Henry Lin <hlin117@gmail.com>\n#         Tom Dupré la Tour\n\n# License: BSD\n\n\nimport numbers\nimport numpy as np\nimport warnings\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils.validation import check_random_state\nfrom sklearn.utils.validation import _check_feature_names_in\nfrom sklearn.utils.validation import check_scalar\nfrom sklearn.utils import _safe_indexing\n\n\nclass KBinsDiscretizer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Bin continuous data into intervals.\n\n    Read more in the :ref:`User Guide <preprocessing_discretization>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    n_bins : int or array-like of shape (n_features,), default=5\n        The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n\n    encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'\n        Method used to encode the transformed result.\n\n        - 'onehot': Encode the transformed result with one-hot encoding\n          and return a sparse matrix. Ignored features are always\n          stacked to the right.\n        - 'onehot-dense': Encode the transformed result with one-hot encoding\n          and return a dense array. Ignored features are always\n          stacked to the right.\n        - 'ordinal': Return the bin identifier encoded as an integer value.\n\n    strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'\n        Strategy used to define the widths of the bins.\n\n        - 'uniform': All bins in each feature have identical widths.\n        - 'quantile': All bins in each feature have the same number of points.\n        - 'kmeans': Values in each bin have the same nearest center of a 1D\n          k-means cluster.\n\n    dtype : {np.float32, np.float64}, default=None\n        The desired data-type for the output. If None, output dtype is\n        consistent with input dtype. Only np.float32 and np.float64 are\n        supported.\n\n        .. versionadded:: 0.24\n\n    subsample : int or None (default='warn')\n        Maximum number of samples, used to fit the model, for computational\n        efficiency. Used when `strategy=\"quantile\"`.\n        `subsample=None` means that all the training samples are used when\n        computing the quantiles that determine the binning thresholds.\n        Since quantile computation relies on sorting each column of `X` and\n        that sorting has an `n log(n)` time complexity,\n        it is recommended to use subsampling on datasets with a\n        very large number of samples.\n\n        .. deprecated:: 1.1\n           In version 1.3 and onwards, `subsample=2e5` will be the default.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for subsampling.\n        Pass an int for reproducible results across multiple function calls.\n        See the `subsample` parameter for more details.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    bin_edges_ : ndarray of ndarray of shape (n_features,)\n        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n        Ignored features will have empty arrays.\n\n    n_bins_ : ndarray of shape (n_features,), dtype=np.int_\n        Number of bins per feature. Bins whose width are too small\n        (i.e., <= 1e-8) are removed with a warning.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Binarizer : Class used to bin values as ``0`` or\n        ``1`` based on a parameter ``threshold``.\n\n    Notes\n    -----\n    In bin edges for feature ``i``, the first and last values are used only for\n    ``inverse_transform``. During transform, bin edges are extended to::\n\n      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n\n    You can combine ``KBinsDiscretizer`` with\n    :class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess\n    part of the features.\n\n    ``KBinsDiscretizer`` might produce constant features (e.g., when\n    ``encode = 'onehot'`` and certain bins do not contain any data).\n    These features can be removed with feature selection algorithms\n    (e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import KBinsDiscretizer\n    >>> X = [[-2, 1, -4,   -1],\n    ...      [-1, 2, -3, -0.5],\n    ...      [ 0, 3, -2,  0.5],\n    ...      [ 1, 4, -1,    2]]\n    >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    >>> est.fit(X)\n    KBinsDiscretizer(...)\n    >>> Xt = est.transform(X)\n    >>> Xt  # doctest: +SKIP\n    array([[ 0., 0., 0., 0.],\n           [ 1., 1., 1., 0.],\n           [ 2., 2., 2., 1.],\n           [ 2., 2., 2., 2.]])\n\n    Sometimes it may be useful to convert the data back into the original\n    feature space. The ``inverse_transform`` function converts the binned\n    data into the original feature space. Each value will be equal to the mean\n    of the two bin edges.\n\n    >>> est.bin_edges_[0]\n    array([-2., -1.,  0.,  1.])\n    >>> est.inverse_transform(Xt)\n    array([[-1.5,  1.5, -3.5, -0.5],\n           [-0.5,  2.5, -2.5, -0.5],\n           [ 0.5,  3.5, -1.5,  0.5],\n           [ 0.5,  3.5, -1.5,  1.5]])\n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins=5,\n        *,\n        encode=\"onehot\",\n        strategy=\"quantile\",\n        dtype=None,\n        subsample=\"warn\",\n        random_state=None,\n    ):\n        self.n_bins = n_bins\n        self.encode = encode\n        self.strategy = strategy\n        self.dtype = dtype\n        self.subsample = subsample\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the estimator.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to be discretized.\n\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X, dtype=\"numeric\", force_all_finite=False)\n\n        supported_dtype = (np.float64, np.float32)\n        if self.dtype in supported_dtype:\n            output_dtype = self.dtype\n        elif self.dtype is None:\n            output_dtype = X.dtype\n        else:\n            raise ValueError(\n                \"Valid options for 'dtype' are \"\n                f\"{supported_dtype + (None,)}. Got dtype={self.dtype} \"\n                \" instead.\"\n            )\n\n        n_samples, n_features = X.shape\n\n        if self.strategy == \"quantile\" and self.subsample is not None:\n            if self.subsample == \"warn\":\n                if n_samples > 2e5:\n                    warnings.warn(\n                        \"In version 1.3 onwards, subsample=2e5 \"\n                        \"will be used by default. Set subsample explicitly to \"\n                        \"silence this warning in the mean time. Set \"\n                        \"subsample=None to disable subsampling explicitly.\",\n                        FutureWarning,\n                    )\n            else:\n                self.subsample = check_scalar(\n                    self.subsample, \"subsample\", numbers.Integral, min_val=1\n                )\n                rng = check_random_state(self.random_state)\n                if n_samples > self.subsample:\n                    subsample_idx = rng.choice(\n                        n_samples, size=self.subsample, replace=False\n                    )\n                    X = _safe_indexing(X, subsample_idx)\n        elif self.strategy != \"quantile\" and isinstance(\n            self.subsample, numbers.Integral\n        ):\n            raise ValueError(\n                f\"Invalid parameter for `strategy`: {self.strategy}. \"\n                '`subsample` must be used with `strategy=\"quantile\"`.'\n            )\n\n        valid_encode = (\"onehot\", \"onehot-dense\", \"ordinal\")\n        if self.encode not in valid_encode:\n            raise ValueError(\n                \"Valid options for 'encode' are {}. Got encode={!r} instead.\".format(\n                    valid_encode, self.encode\n                )\n            )\n        valid_strategy = (\"uniform\", \"quantile\", \"kmeans\")\n        if self.strategy not in valid_strategy:\n            raise ValueError(\n                \"Valid options for 'strategy' are {}. \"\n                \"Got strategy={!r} instead.\".format(valid_strategy, self.strategy)\n            )\n\n        n_features = X.shape[1]\n        n_bins = self._validate_n_bins(n_features)\n\n        bin_edges = np.zeros(n_features, dtype=object)\n        contains_nan = []\n        for jj in range(n_features):\n            column = X[:, jj]\n            contains_nan.append(\n                np.isnan(column).any())  # check whether there are NaN\n\n            if np.all(np.isnan(column)):\n                warnings.warn(\n                    \"Feature %d is entirely missing and will be replaced with -1.\" % jj\n                )\n                n_bins[jj] = 1\n                bin_edges[jj] = np.array([-np.inf, np.inf])\n                continue\n\n            column = column[~np.isnan(column)]  # remove NaNs for the fit\n            col_min, col_max = column.min(), column.max()\n\n            if col_min == col_max:\n                warnings.warn(\n                    \"Feature %d is constant and will be replaced with 0.\" % jj\n                )\n                n_bins[jj] = 1\n                bin_edges[jj] = np.array([-np.inf, np.inf])\n                continue\n\n            if self.strategy == \"uniform\":\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\n\n            elif self.strategy == \"quantile\":\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\n                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\n\n            elif self.strategy == \"kmeans\":\n                from ..cluster import KMeans  # fixes import loops\n\n                # Deterministic initialization with uniform spacing\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\n\n                # 1D k-means procedure\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n                # Must sort, centers may be unsorted even with sorted init\n                centers.sort()\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n\n            # Remove bins whose width are too small (i.e., <= 1e-8)\n            if self.strategy in (\"quantile\", \"kmeans\"):\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\n                bin_edges[jj] = bin_edges[jj][mask]\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\n                    warnings.warn(\n                        \"Bins whose width are too small (i.e., <= \"\n                        \"1e-8) in feature %d are removed. Consider \"\n                        \"decreasing the number of bins.\" % jj\n                    )\n                    n_bins[jj] = len(bin_edges[jj]) - 1\n\n        self.bin_edges_ = bin_edges\n        self.n_bins_ = n_bins\n\n        if \"onehot\" in self.encode:\n            self._encoder = OneHotEncoder(\n                categories=[np.arange(-1, self.n_bins_)\n                            if contains_nan[jj] else\n                            np.arange(0, self.n_bins_)\n                            for jj in range(n_features)],\n                sparse=self.encode == \"onehot\",\n                dtype=output_dtype,\n            )\n            # Fit the OneHotEncoder with toy datasets\n            # so that it's ready for use after the KBinsDiscretizer is fitted\n            self._encoder.fit(np.zeros((1, len(self.n_bins_))))\n\n        return self\n\n    def _validate_n_bins(self, n_features):\n        \"\"\"Returns n_bins_, the number of bins per feature.\"\"\"\n        orig_bins = self.n_bins\n        if isinstance(orig_bins, numbers.Number):\n            if not isinstance(orig_bins, numbers.Integral):\n                raise ValueError(\n                    \"{} received an invalid n_bins type. \"\n                    \"Received {}, expected int.\".format(\n                        KBinsDiscretizer.__name__, type(orig_bins).__name__\n                    )\n                )\n            if orig_bins < 2:\n                raise ValueError(\n                    \"{} received an invalid number \"\n                    \"of bins. Received {}, expected at least 2.\".format(\n                        KBinsDiscretizer.__name__, orig_bins\n                    )\n                )\n            return np.full(n_features, orig_bins, dtype=int)\n\n        n_bins = check_array(orig_bins, dtype=int, copy=True, ensure_2d=False)\n\n        if n_bins.ndim > 1 or n_bins.shape[0] != n_features:\n            raise ValueError(\"n_bins must be a scalar or array of shape (n_features,).\")\n\n        bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)\n\n        violating_indices = np.where(bad_nbins_value)[0]\n        if violating_indices.shape[0] > 0:\n            indices = \", \".join(str(i) for i in violating_indices)\n            raise ValueError(\n                \"{} received an invalid number \"\n                \"of bins at indices {}. Number of bins \"\n                \"must be at least 2, and must be an int.\".format(\n                    KBinsDiscretizer.__name__, indices\n                )\n            )\n        return n_bins\n\n    def transform(self, X):\n        \"\"\"\n        Discretize the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to be discretized.\n\n        Returns\n        -------\n        Xt : {ndarray, sparse matrix}, dtype={np.float32, np.float64}\n            Data in the binned space. Will be a sparse matrix if\n            `self.encode='onehot'` and ndarray otherwise.\n        \"\"\"\n        check_is_fitted(self)\n\n        # check input and attribute dtypes\n        dtype = (np.float64, np.float32) if self.dtype is None else self.dtype\n        Xt = self._validate_data(X, copy=True, dtype=dtype, reset=False,\n                                 force_all_finite=False)\n\n        bin_edges = self.bin_edges_\n        for jj in range(Xt.shape[1]):\n            column = Xt[:, jj]\n            Xt[:, jj] = np.where(np.isnan(column), -1,\n                                 np.searchsorted(bin_edges[jj][1:-1],\n                                                 column, side=\"right\"))\n\n        if self.encode == \"ordinal\":\n            return Xt\n\n        dtype_init = None\n        if \"onehot\" in self.encode:\n            dtype_init = self._encoder.dtype\n            self._encoder.dtype = Xt.dtype\n        try:\n            Xt_enc = self._encoder.transform(Xt)\n        finally:\n            # revert the initial dtype to avoid modifying self.\n            self._encoder.dtype = dtype_init\n        return Xt_enc\n\n    def inverse_transform(self, Xt):\n        \"\"\"\n        Transform discretized data back to original feature space.\n\n        Note that this function does not regenerate the original data\n        due to discretization rounding.\n\n        Parameters\n        ----------\n        Xt : array-like of shape (n_samples, n_features)\n            Transformed data in the binned space.\n\n        Returns\n        -------\n        Xinv : ndarray, dtype={np.float32, np.float64}\n            Data in the original feature space.\n        \"\"\"\n        check_is_fitted(self)\n\n        if \"onehot\" in self.encode:\n            Xt = self._encoder.inverse_transform(Xt)\n\n        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))\n        n_features = self.n_bins_.shape[0]\n        if Xinv.shape[1] != n_features:\n            raise ValueError(\n                \"Incorrect number of features. Expecting {}, received {}.\".format(\n                    n_features, Xinv.shape[1]\n                )\n            )\n\n        for jj in range(n_features):\n            bin_edges = self.bin_edges_[jj]\n            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n            column = Xinv[:, jj]\n            column = bin_centers[np.int_(column)]\n            column[Xinv[:, jj] == -1] = np.NaN\n            Xinv[:, jj] = bin_centers[np.int_(column)]\n\n        return Xinv\n\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Input features.\n\n            - If `input_features` is `None`, then `feature_names_in_` is\n              used as feature names in. If `feature_names_in_` is not defined,\n              then the following input feature names are generated:\n              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n            - If `input_features` is an array-like, then `input_features` must\n              match `feature_names_in_` if `feature_names_in_` is defined.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        input_features = _check_feature_names_in(self, input_features)\n        if hasattr(self, \"_encoder\"):\n            return self._encoder.get_feature_names_out(input_features)\n\n        # ordinal encoding\n        return input_features\n\n    def _more_tags(self):\n        return {'allow_nan': True}\n"}
{"type": "source_file", "path": "tableshift/core/tabular_dataset.py", "content": "import glob\nimport json\nimport logging\nimport math\nimport os\nimport pickle\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union, List, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nimport ray.data\nimport torch\nfrom pandas import DataFrame, Series\nfrom tableshift.third_party.domainbed import InfiniteDataLoader\nfrom torch.utils.data import DataLoader\n\nfrom .features import Preprocessor, PreprocessorConfig, is_categorical\nfrom .grouper import Grouper\nfrom .metrics import metrics_by_group\nfrom .splitter import Splitter, DomainSplitter\nfrom .tasks import get_task_config, TaskConfig\nfrom .utils import make_uid, convert_64bit_numeric_cols\n\n\ndef _make_dataloader_from_dataframes(\n        data, batch_size: int, shuffle: bool,\n        infinite=False) -> DataLoader:\n    \"\"\"Construct a (shuffled) DataLoader from a DataFrame.\"\"\"\n    data = tuple(map(lambda x: torch.tensor(x.values).float(), data))\n    tds = torch.utils.data.TensorDataset(*data)\n    if infinite:\n        loader = InfiniteDataLoader(dataset=tds, batch_size=batch_size)\n    else:\n        loader = DataLoader(\n            dataset=tds, batch_size=batch_size,\n            shuffle=shuffle)\n    return loader\n\n\n@dataclass\nclass DatasetConfig:\n    cache_dir: str = \"tableshift_cache\"\n    download: bool = True\n    random_seed: int = 948324\n\n\n@dataclass\nclass Dataset(ABC):\n    \"\"\"Absract class to represent a dataset.\"\"\"\n    name: str\n    preprocessor_config: PreprocessorConfig\n    config: DatasetConfig\n    initialize_data: bool\n    splitter: Splitter = None\n    splits = None  # dict mapping {split_name: list of idxs in split}\n    grouper: Optional[Grouper] = None\n\n    # List of the names of the predictors only.\n    feature_names: Union[List[str], None] = None\n\n    group_feature_names: Union[List[str], None] = None\n    target: str = None\n\n    domain_label_colname: Union[str, None] = None\n\n    # If true, do not do per-domain evals\n    skip_per_domain_eval: bool = False\n\n    @property\n    def cache_dir(self):\n        return self.config.cache_dir\n\n    @property\n    def uid(self) -> str:\n        return make_uid(self.name, self.splitter)\n\n    @property\n    def is_domain_split(self) -> bool:\n        \"\"\"Return True if this dataset uses a DomainSplitter, else False.\"\"\"\n        return isinstance(self.splitter, DomainSplitter)\n\n    @property\n    def eval_split_names(self) -> Tuple:\n        \"\"\"Fetch the names of the eval splits.\"\"\"\n        if self.skip_per_domain_eval:\n            return tuple([x for x in self.splits if\n                          x in (\"test\", \"id_test\", \"ood_test\")])\n\n        else:\n            return tuple([x for x in self.splits if \"train\" not in x])\n\n    @property\n    def domain_split_varname(self):\n        if isinstance(self.splitter, DomainSplitter):\n            return self.splitter.domain_split_varname\n        else:\n            return self.domain_label_colname\n\n    @property\n    @abstractmethod\n    def n_domains(self) -> int:\n        raise\n\n    @property\n    @abstractmethod\n    def base_dir(self) -> str:\n        \"Return the location of the directory {cache_dir}/{uid}.\"\n        raise\n\n    @abstractmethod\n    def _is_valid_split(self, split) -> bool:\n        raise\n\n    @abstractmethod\n    def _initialize_data(self):\n        \"\"\"Load the data/labels/groups from a data source.\"\"\"\n        raise\n\n    @property\n    @abstractmethod\n    def cat_idxs(self) -> List[int]:\n        \"\"\"Return a list of the indices of categorical columns.\"\"\"\n        raise\n\n    @property\n    @abstractmethod\n    def features(self) -> List[str]:\n        \"\"\"Fetch a list of the feature names.\"\"\"\n        raise\n\n    def _get_info(self) -> Dict[str, Any]:\n        raise\n\n    def _check_split(self, split):\n        \"\"\"Check that a split name is valid.\"\"\"\n        assert self._is_valid_split(split), \\\n            f\"split {split} not in {list(self.splits.keys())}\"\n\n    @abstractmethod\n    def _get_split_df(self, split: str, domain=None) -> pd.DataFrame:\n        raise\n\n    def _get_split_xygd(self, split, domain=None) -> Tuple[\n        DataFrame, Series, DataFrame, Optional[Series]]:\n        if domain is not None:\n            raise NotImplementedError(\n                \"support for domain is not implemented in this class.\")\n        for name in (\"feature_names\", \"target\", \"group_feature_names\"):\n            assert getattr(self, name) is not None, f\"{name} is None.\"\n        df = self._get_split_df(split, domain=domain)\n\n        # preserve ordering of columns in df\n        assert all(fn in df.columns for fn in self.feature_names)\n        X = df[[c for c in df.columns if c in self.feature_names]]\n        y = df[self.target]\n        G = df[self.group_feature_names]\n        d = df[self.domain_label_colname] \\\n            if self.domain_label_colname is not None else None\n        return X, y, G, d\n\n    def get_pandas(self, split, domain=None) -> Tuple[\n        DataFrame, Series, DataFrame, Optional[Series]]:\n        \"\"\"Fetch the (data, labels, groups, domains) for this TabularDataset.\"\"\"\n        return self._get_split_xygd(split, domain)\n\n    def get_dataloader(self, split, batch_size=2048,\n                       shuffle=True, infinite=False) -> DataLoader:\n        \"\"\"Fetch a dataloader yielding (X, y, G, d) tuples.\"\"\"\n        data = self._get_split_xygd(split)\n\n        if not self.domain_label_colname:\n            # Drop the empty domain labels.\n            data = data[:-1]\n        return _make_dataloader_from_dataframes(data, batch_size, shuffle,\n                                                infinite=infinite)\n\n    def get_cache_dir(self, split: str, domain: Optional[Any] = None):\n        if domain is None:\n            return os.path.join(self.base_dir, split)\n        else:\n            return os.path.join(self.base_dir, split, str(domain))\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, name: str, config: DatasetConfig,\n                 splitter: Splitter,\n                 preprocessor_config: PreprocessorConfig,\n                 grouper: Optional[Grouper],\n                 initialize_data: bool = True,\n                 task_config: Optional[TaskConfig] = None,\n                 **kwargs):\n        super().__init__(name=name,\n                         preprocessor_config=preprocessor_config,\n                         config=config,\n                         grouper=grouper,\n                         initialize_data=initialize_data,\n                         splitter=splitter)\n\n        # Dataset-specific info: features, data source, preprocessing.\n\n        self.task_config = get_task_config(self.name) if task_config is None else task_config\n        try:\n            self.data_source = self.task_config.data_source_cls(\n                cache_dir=self.config.cache_dir,\n                download=self.config.download,\n                **kwargs)\n        except TypeError:\n            kwargs.update({\"dataset_name\": self.name})\n            self.data_source = self.task_config.data_source_cls(\n                cache_dir=self.config.cache_dir,\n                download=self.config.download,\n                **kwargs)\n\n        self.preprocessor = Preprocessor(\n            config=self.preprocessor_config,\n            feature_list=self.task_config.feature_list)\n\n        # Placeholders for data/labels/groups and split indices.\n        self._df: Union[pd.DataFrame, None] = None  # holds all the data\n\n        if initialize_data:\n            self._initialize_data()\n\n    @property\n    def features(self) -> List[str]:\n        return self.task_config.feature_list.names\n\n    @property\n    def predictors(self) -> List[str]:\n        \"\"\"The list of feature names in the FeatureList.\n\n        Note that these do *not* necessarily correspond to the names in X,\n        the data provided after preprocessing.\"\"\"\n        return self.task_config.feature_list.predictors\n\n    @property\n    def X_shape(self):\n        \"\"\"Shape of the data matrix for training.\"\"\"\n        return [None, len(self.feature_names)]\n\n    @property\n    def grouper_features(self):\n        if self.grouper is not None:\n            return self.grouper.features\n        else:\n            return []\n\n    @property\n    def n_train(self) -> int:\n        \"\"\"Fetch the number of training observations.\"\"\"\n        return len(self.splits[\"train\"])\n\n    @property\n    def n_domains(self) -> int:\n        \"\"\"Number of domains, across all sensitive attributes and splits.\"\"\"\n        if self.domain_label_colname is None:\n            return 0\n        else:\n            return self._df[self.domain_label_colname].nunique()\n\n    @property\n    def cat_idxs(self) -> List[int]:\n        return [i for i, col in enumerate(self._df.columns) if is_categorical(self._df[col])]\n\n    def get_domains(self, split) -> Union[List[str], None]:\n        \"\"\"Fetch a list of the domains.\"\"\"\n        if self.is_domain_split and self._is_valid_split(split):\n            split_df = self._get_split_df(split)\n            return split_df[self.domain_label_colname].unique()\n        else:\n            return None\n\n    def _check_data(self):\n        \"\"\"Helper function to check data after all preprocessing/splitting.\"\"\"\n        target = self._post_transform_target_name()\n        if not pd.api.types.is_numeric_dtype(self._df[target]):\n            logging.warning(\n                f\"y is of type {self._df[target].dtype}; \"\n                f\"non-numeric types are not accepted by all estimators (\"\n                f\"e.g. xgb.XGBClassifier\")\n        if self.domain_label_colname:\n            assert self.domain_label_colname not in self._df[\n                self.feature_names].columns\n\n        if self.grouper and self.grouper.drop:\n            for c in self.grouper_features: assert c not in self._df[\n                self.feature_names].columns\n        return\n\n    @staticmethod\n    def _check_data_source(df: pd.DataFrame):\n        \"\"\"Check the data returned by DataSource.get_data().\"\"\"\n        df = convert_64bit_numeric_cols(df)\n        df.reset_index(drop=True, inplace=True)\n        return df\n\n    def _initialize_data(self):\n        \"\"\"Load the data/labels/groups from a data source.\"\"\"\n        data = self.data_source.get_data()\n        data = self._check_data_source(data)\n        data = self.task_config.feature_list.apply_schema(\n            data, passthrough_columns=[\"Split\"])\n        data = self.preprocessor._dropna(data)\n        data = self._apply_grouper(data)\n        data = self._generate_splits(data)\n        data = self._process_post_split(data)\n        self._df = data\n\n        self._init_feature_names(data)\n        self._check_data()\n\n        return\n\n    def _apply_grouper(self, data: pd.DataFrame):\n        \"\"\"Apply the grouper, if one is used.\"\"\"\n        if self.grouper is not None:\n            return self.grouper.transform(data)\n        else:\n            return data\n\n    def _post_transform_target_name(self) -> str:\n        \"\"\"Return the 'true', possibly mapped, name of the target feature.\n\n        This is the name that should be used once the features have been\n        transformed.\"\"\"\n        target = self.task_config.feature_list.target\n        if self.preprocessor_config.map_targets and (\n                self.task_config.feature_list[\n                    target].name_extended is not None):\n            target = self.task_config.feature_list[target].name_extended\n        return target\n\n    def _pre_transform_target_name(self) -> str:\n        return self.task_config.feature_list.target\n\n    def _init_feature_names(self, data):\n        \"\"\"Set the (data, labels, groups, domain_labels) feature names.\"\"\"\n        target = self._post_transform_target_name()\n\n        data_features = set([x for x in data.columns\n                             if x not in self.grouper_features\n                             and x != target])\n        if self.grouper and (not self.grouper.drop):\n            # Retain the group variables as features.\n            for x in self.grouper_features: data_features.add(x)\n\n        if isinstance(self.splitter, DomainSplitter):\n            domain_split_varname = self.splitter.domain_split_varname\n\n            if self.splitter.drop_domain_split_col and \\\n                    (domain_split_varname in data_features):\n                # Retain the domain split variable as feature in X.\n                data_features.remove(domain_split_varname)\n        else:\n            # Case: domain split is not used; no domain labels exist.\n            domain_split_varname = None\n\n        self.feature_names = list(data_features)\n        self.target = target\n        self.group_feature_names = self.grouper_features\n        self.domain_label_colname = domain_split_varname\n\n        return\n\n    def _generate_splits(self, data):\n        \"\"\"Call the splitter to generate splits for the dataset.\"\"\"\n        assert self.splits is None, \"attempted to overwrite existing splits.\"\n        self._init_feature_names(data)\n        self.splits = self.splitter(\n            data=data[self.feature_names],\n            labels=data[self._pre_transform_target_name()],\n            groups=data[self.group_feature_names],\n            domain_labels=data[self.domain_label_colname] \\\n                if self.domain_label_colname else None)\n        if \"Split\" in data.columns:\n            data.drop(columns=[\"Split\"], inplace=True)\n        return data\n\n    def _process_post_split(self, data) -> pd.DataFrame:\n        \"\"\"Dataset-specific postprocessing function.\n\n        Conducts any processing required **after** splitting (e.g.\n        normalization, drop features needed only for splitting).\"\"\"\n        passthrough_columns = self.grouper_features\n\n        data = self.preprocessor.fit_transform(\n            data,\n            self.splits[\"train\"],\n            domain_label_colname=self.domain_label_colname,\n            target_colname=self.target,\n            passthrough_columns=passthrough_columns)\n\n        # If necessary, cast targets to the default type.\n        target_name = self._post_transform_target_name()\n        if self.preprocessor_config.cast_targets_to_default_type:\n            data[target_name] = data[target_name].astype(\n                self.preprocessor_config.default_targets_dtype)\n        return data\n\n    def _is_valid_split(self, split) -> bool:\n        return split in self.splits.keys()\n\n    def _get_split_idxs(self, split):\n        self._check_split(split)\n        idxs = self.splits[split]\n        return idxs\n\n    def _get_split_df(self, split, domain=None) -> pd.DataFrame:\n\n        split_idxs = self._get_split_idxs(split)\n        split_df = self._df.iloc[split_idxs]\n\n        if domain is None:\n            return split_df\n\n        else:\n            assert self.domain_label_colname, 'domain name is required.'\n            domain_split_idxs = split_df[self.domain_label_colname] == domain\n            return split_df[domain_split_idxs]\n\n    def get_domain_dataloaders(\n            self, split, batch_size=2048,\n            shuffle=True, infinite=True) -> Dict[Any, DataLoader]:\n        \"\"\"Fetch a dict of {domain_id:DataLoader}.\"\"\"\n        loaders = {}\n        split_data = self._get_split_xygd(split)\n        assert self.n_domains, \"sanity check for a domain-split dataset\"\n\n        logging.debug(\"Domain value counts:\\n{}\".format(\n            split_data[3].value_counts()))\n\n        for domain in sorted(split_data[3].unique()):\n            # Boolean vector where True indicates observations in the domain.\n            idxs = split_data[3] == domain\n            assert idxs.sum() >= batch_size, \\\n                \"sanity check at least one full batch per domain.\"\n\n            split_domain_data = [df[idxs] for df in split_data]\n            split_loader = _make_dataloader_from_dataframes(\n                split_domain_data, batch_size, shuffle, infinite=infinite)\n            loaders[domain] = split_loader\n        return loaders\n\n    def get_dataset_baseline_metrics(self, split):\n\n        X_tr, y_tr, g, _ = self.get_pandas(split)\n        n_by_y = pd.value_counts(y_tr).to_dict()\n        y_maj = pd.value_counts(y_tr).idxmax()\n        # maps {class_label: p_class_label}\n        p_y = pd.value_counts(y_tr, normalize=True).to_dict()\n\n        p_y_by_sens = pd.crosstab(y_tr, [X_tr[c] for c in g],\n                                  normalize='columns').to_dict()\n        n_y_by_sens = pd.crosstab(y_tr, [X_tr[c] for c in g]).to_dict()\n        n_by_sens = pd.crosstab(g.iloc[:, 0], g.iloc[:, 1]).unstack().to_dict()\n        return {\"y_maj\": y_maj,\n                \"n_by_y\": n_by_y,\n                \"p_y\": p_y,\n                \"p_y_by_sens\": p_y_by_sens,\n                \"n_y_by_sens\": n_y_by_sens,\n                \"n_by_sens\": n_by_sens}\n\n    def subgroup_majority_classifier_performance(self, split):\n        \"\"\"Compute overall and worst-group acc of a subgroup-conditional\n        majority-class classifier.\"\"\"\n        baseline_metrics = self.get_dataset_baseline_metrics(split)\n        sensitive_subgroup_accuracies = []\n        sensitive_subgroup_n_correct = []\n        for sens, n_y_by_sens in baseline_metrics[\"n_y_by_sens\"].items():\n            p_y_by_sens = baseline_metrics[\"p_y_by_sens\"][sens]\n            y_max = 1 if n_y_by_sens[1] > n_y_by_sens[0] else 0\n            p_y_max = p_y_by_sens[y_max]\n            n_y_max = n_y_by_sens[y_max]\n            sensitive_subgroup_n_correct.append(n_y_max)\n            sensitive_subgroup_accuracies.append(p_y_max)\n        n = sum(baseline_metrics[\"n_by_y\"].values())\n        overall_acc = np.sum(sensitive_subgroup_n_correct) / n\n        return overall_acc, min(sensitive_subgroup_accuracies)\n\n    def evaluate_predictions(self, preds, split):\n        _, labels, groups, _ = self.get_pandas(split)\n        metrics = metrics_by_group(labels, preds, groups, suffix=split)\n        # Add baseline metrics.\n        metrics[\"majority_baseline_\" + split] = max(labels.mean(),\n                                                    1 - labels.mean())\n        sm_overall_acc, sm_wg_acc = self.subgroup_majority_classifier_performance(\n            split)\n        metrics[\"subgroup_majority_overall_acc_\" + split] = sm_overall_acc\n        metrics[\"subgroup_majority_worstgroup_acc_\" + split] = sm_wg_acc\n        return metrics\n\n    def is_cached(self) -> bool:\n        base_dir = os.path.join(self.config.cache_dir, self.uid)\n        if os.path.exists(os.path.join(base_dir, \"info.json\")):\n            return True\n        else:\n            return False\n\n    @property\n    def base_dir(self) -> str:\n        return os.path.join(self.config.cache_dir, self.uid)\n\n    def _get_info(self) -> Dict[str, Any]:\n        return {\n            'target': self.target,\n            'domain_label_colname': self.domain_label_colname,\n            'domain_label_values': self._df[\n                self.domain_label_colname].unique().tolist() \\\n                if self.domain_label_colname else None,\n            'group_feature_names': self.group_feature_names,\n            'feature_names': self.feature_names,\n            'X_shape': self.X_shape,\n            'splits': list(self.splits.keys()),\n            **{f'n_{s}': len(self.splits[s]) for s in self.splits},\n        }\n\n    def _get_schema(self):\n        return self._df.dtypes.to_dict()\n\n    def to_sharded(self, rows_per_shard=4096,\n                   domains_to_subdirectories: bool = True,\n                   file_type=\"csv\"):\n\n        assert file_type in (\n            \"csv\", \"arrow\", \"parquet\"), f\"file type {file_type} not supported.\"\n\n        base_dir = self.base_dir\n\n        def initialize_dir(dirname):\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n\n        for split in self.splits:\n\n            logging.info(f\"caching task split {split} to {self.base_dir}\")\n\n            df = self._get_split_df(split)\n\n            def write_shards(to_shard: DataFrame, dirname: str):\n                num_shards = math.ceil(len(to_shard) / rows_per_shard)\n                for i in range(num_shards):\n                    fp = os.path.join(dirname, f\"{split}_{i:05d}.{file_type}\")\n                    logging.debug('writing file to %s' % fp)\n                    start, end = i * rows_per_shard, (i + 1) * rows_per_shard\n                    shard_df = to_shard.iloc[start:end]\n                    if file_type == \"csv\":\n                        shard_df.to_csv(fp, index=False)\n                    elif file_type == \"arrow\":\n                        shard_df.reset_index(drop=True).to_feather(fp)\n                    elif file_type == \"parquet\":\n                        shard_df.to_parquet(fp, index=False)\n\n            if self.domain_label_colname and domains_to_subdirectories:\n                # Write to {split}/{domain_value}/{shard_filename.csv}\n                for domain in sorted(df[self.domain_label_colname].unique()):\n                    df_ = df[df[self.domain_label_colname] == domain]\n                    domain_dir = self.get_cache_dir(split, domain)\n                    initialize_dir(domain_dir)\n                    write_shards(df_, domain_dir)\n            elif self.domain_label_colname:\n                # Write to {split}/all_{split}_subdomains/{shard_filename.csv}\n                shared_domain_dir = self.get_cache_dir(\n                    split, f\"all_{split}_subdomains\")\n                initialize_dir(shared_domain_dir)\n                write_shards(df, shared_domain_dir)\n            else:\n                # Write to {split}/{shard_filename.csv}\n                outdir = self.get_cache_dir(split)\n                initialize_dir(outdir)\n                write_shards(df, outdir)\n\n        # write metadata\n        schema = self._get_schema()\n        with open(os.path.join(base_dir, \"schema.pickle\"), \"wb\") as f:\n            pickle.dump(schema, f)\n\n        ds_info = self._get_info()\n        with open(os.path.join(base_dir, \"info.json\"), \"w\") as f:\n            f.write(json.dumps(ds_info))\n\n\nclass CachedDataset(Dataset):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.domain_label_values = None\n        self.group_feature_names = None\n\n        # Shape of the full data matrix, including domain label and target\n        self.X_shape = None\n\n        self.schema = None\n\n        self._load_info_from_cache()\n\n    @property\n    def base_dir(self):\n        return os.path.join(self.cache_dir, self.uid)\n\n    @property\n    def cat_idxs(self) -> List[int]:\n        \"\"\"Fetch indices of categorical features in the data.\n\n        These are indices into the `X` array, with columns ordered according to\n        self.feature_names (which is the default ordering provided by all\n        TableShift functions).\n        \"\"\"\n        features_and_dtypes = [(x, self.schema[x]) for x in self.feature_names]\n        idxs = np.nonzero([x[1] == np.int8 for x in features_and_dtypes])[0]\n        return idxs.tolist()\n\n    @property\n    def features(self) -> List[str]:\n        return list(self.schema.keys())\n\n    def _initialize_data(self):\n        raise NotImplementedError\n\n    def is_cached(self) -> bool:\n        base_dir = os.path.join(self.cache_dir, self.uid)\n        if os.path.exists(os.path.join(base_dir, \"info.json\")):\n            return True\n        else:\n            return False\n\n    def _init_feature_names(self):\n        # Hack: since feature names don't match the ordering in the file on\n        # disk, we peek at the file on disk; it is critical that the ordering\n        # of the feature names matches the ordering in the data.\n        f = self._get_split_files(\"train\")[0]\n        df = pd.read_csv(f, nrows=1)\n        assert set(df.columns) == set(self.schema.keys())\n        feature_names = df.columns.tolist()\n        feature_names.remove(self.target)\n        if isinstance(self.splitter, DomainSplitter) \\\n                and self.splitter.drop_domain_split_col \\\n                and self.domain_label_colname in feature_names:\n            feature_names.remove(self.domain_label_colname)\n        self.feature_names = feature_names\n\n    def _load_info_from_cache(self):\n        \"\"\"Load the dataset metadata from cache (data is lazily loaded).\"\"\"\n        logging.info(f\"loading from {self.base_dir}\")\n        with open(os.path.join(self.base_dir, \"info.json\"), \"r\") as f:\n            ds_info = json.loads(f.read())\n\n        for k, v in ds_info.items():\n            if k != \"feature_names\":\n                setattr(self, k, v)\n\n        with open(os.path.join(self.base_dir, \"schema.pickle\"), \"rb\") as f:\n            self.schema = pickle.load(f)\n\n        self._init_feature_names()\n\n    def get_domains(self, split) -> Union[List[str], None]:\n        \"\"\"Fetch a list of the cached domains.\"\"\"\n        dir = os.path.join(self.base_dir, split)\n        if not os.path.exists(dir):\n            return None\n        else:\n            domains = os.listdir(dir)\n            return sorted(domains)\n\n    @property\n    def n_domains(self) -> int:\n        \"\"\"Number of domains, across all sensitive attributes and splits.\"\"\"\n        if self.domain_label_colname is None:\n            return 0\n        else:\n            domains_per_split = [self.get_domains(s) for s in self.splits]\n            domains = list(set(d for ds in domains_per_split for d in ds))\n            return len(domains)\n\n    def _get_split_files(self, split: str, domain: Optional[str] = None):\n        split_cache_dir = self.get_cache_dir(split, domain)\n        if not domain:  # case: match files in any domain subdirectory\n            fileglob = os.path.join(split_cache_dir, \"**\", \"*.csv\")\n            files = glob.glob(fileglob, recursive=True)\n        else:\n            # case: split_cache_dir contains a domain subdirectory, so this\n            # will only match the desired domain.\n            fileglob = os.path.join(split_cache_dir, \"*.csv\")\n            files = glob.glob(fileglob)\n\n        assert len(files), f\"no files detected for split {split} \" \\\n                           f\"matching {fileglob}\"\n        return files\n\n    def _is_valid_split(self, split) -> bool:\n        return split in os.listdir(self.base_dir)\n\n    def _get_split_df(self, split, domain=None) -> pd.DataFrame:\n        self._check_split(split)\n        files = self._get_split_files(split, domain=domain)\n        dfs = []\n        for f in files:\n            dfs.append(pd.read_csv(f))\n        df = pd.concat(dfs)\n\n        return df\n\n    def get_ray(self, split, domain=None, num_partitions_per_file=16):\n        files = self._get_split_files(split, domain)\n        num_partitions = len(files) * num_partitions_per_file\n        return ray.data \\\n            .read_csv(\n            files,\n            meta_provider=ray.data.datasource.FastFileMetadataProvider()) \\\n            .repartition(num_partitions)\n"}
{"type": "source_file", "path": "tableshift/datasets/catboost_benchmarks.py", "content": "\"\"\"\nCatBoost quality benchmarks; adapted from\nhttps://github.com/catboost/benchmarks/tree/master/quality_benchmarks\n\nFor more information on datasets and access TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nimport re\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nAMAZON_FEATURES = FeatureList(features=[\n    Feature('ACTION', float,\n            \"ACTION is 1 if the resource was approved, 0 if the resource was not\",\n            name_extended=\"access to resource was approved\",\n            is_target=True),\n    Feature('RESOURCE', int, \"An ID for each resource\",\n            name_extended=\"resource ID\"),\n    Feature('MGR_ID', int,\n            \"The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time\",\n            name_extended=\"manager ID\"),\n    Feature('ROLE_ROLLUP_1', int,\n            \"Company role grouping category id 1 (e.g. US Engineering)\",\n            name_extended=\"company role grouping category 1\"),\n    Feature('ROLE_ROLLUP_2', int,\n            \"Company role grouping category id 2 (e.g. US Retail)\",\n            name_extended=\"company role grouping category 2\"),\n    Feature('ROLE_DEPTNAME', int,\n            'Company role department description (e.g. Retail)',\n            name_extended='company role department description'),\n    Feature('ROLE_TITLE', int,\n            'Company role business title description (e.g. Senior Engineering Retail Manager)',\n            name_extended='company role business title description'),\n    Feature('ROLE_FAMILY_DESC', int,\n            'Company role family extended description (e.g. Retail Manager, Software Engineering)',\n            name_extended='company role family extended description'),\n    Feature('ROLE_FAMILY', int,\n            'Company role family description (e.g. Retail Manager)',\n            name_extended='company role family description'),\n    Feature('ROLE_CODE', int,\n            'Company role code; this code is unique to each role (e.g. Manager)',\n            name_extended='company role code'),\n], documentation=\"https://www.kaggle.com/c/amazon-employee-access-challenge/overview , \"\n                 \"https://www.kaggle.com/c/amazon-employee-access-challenge/data\")\n\nAPPETENCY_FEATURES = FeatureList(features=[\n    Feature('label', float, name_extended='user will buy new products or services (appetency)', is_target=True),\n    Feature('Var202', cat_dtype),  # importance: 0.0881\n    Feature('Var220', cat_dtype),  # importance: 0.0622\n    Feature('Var218', cat_dtype),  # importance: 0.0532\n    Feature('Var198', cat_dtype),  # importance: 0.0449\n    Feature('Var214', cat_dtype),  # importance: 0.04\n    Feature('Var192', cat_dtype),  # importance: 0.0365\n    Feature('Var199', cat_dtype),  # importance: 0.0359\n    Feature('Var217', cat_dtype),  # importance: 0.0319\n    Feature('Var126', float),  # importance: 0.027\n    Feature('Var222', cat_dtype),  # importance: 0.0234\n    Feature('Var216', cat_dtype),  # importance: 0.0174\n    Feature('Var78', float),  # importance: 0.0168\n    Feature('Var126_imputed', float),  # importance: 0.014\n    Feature('Var212', cat_dtype),  # importance: 0.0138\n    Feature('Var197', cat_dtype),  # importance: 0.0137\n    Feature('Var204', cat_dtype),  # importance: 0.0128\n    Feature('Var7', float),  # importance: 0.0121\n    Feature('Var191', cat_dtype),  # importance: 0.0119\n    Feature('Var211', cat_dtype),  # importance: 0.0118\n    Feature('Var144_imputed', float),  # importance: 0.0111\n    Feature('Var189', float),  # importance: 0.0109\n    Feature('Var194', cat_dtype),  # importance: 0.0108\n    Feature('Var83', float),  # importance: 0.0105\n    Feature('Var228', cat_dtype),  # importance: 0.01\n    Feature('Var229', cat_dtype),  # importance: 0.0098\n    Feature('Var205', cat_dtype),  # importance: 0.0097\n    Feature('Var206', cat_dtype),  # importance: 0.0097\n    Feature('Var38', float),  # importance: 0.0096\n    Feature('Var207', cat_dtype),  # importance: 0.0092\n    Feature('Var223', cat_dtype),  # importance: 0.009\n    Feature('Var24', float),  # importance: 0.009\n    Feature('Var225', cat_dtype),  # importance: 0.0089\n    Feature('Var125', float),  # importance: 0.0088\n    ##################################################\n    ##################################################\n    # Feature('Var173', float),  # importance: 0.0088\n    # Feature('Var132', float),  # importance: 0.0087\n    # Feature('Var144', float),  # importance: 0.0085\n    # Feature('Var72', float),  # importance: 0.0084\n    # Feature('Var65', float),  # importance: 0.0084\n    # Feature('Var109', float),  # importance: 0.0083\n    # Feature('Var149', float),  # importance: 0.0082\n    # Feature('Var134', float),  # importance: 0.0081\n    # Feature('Var81', float),  # importance: 0.0081\n    # Feature('Var133', float),  # importance: 0.0078\n    # Feature('Var227', cat_dtype),  # importance: 0.0077\n    # Feature('Var123', float),  # importance: 0.0076\n    # Feature('Var153', float),  # importance: 0.0076\n    # Feature('Var73', float),  # importance: 0.0075\n    # Feature('Var112', float),  # importance: 0.0074\n    # Feature('Var160', float),  # importance: 0.0074\n    # Feature('Var21', float),  # importance: 0.0073\n    # Feature('Var57', float),  # importance: 0.0072\n    # Feature('Var6', float),  # importance: 0.0072\n    # Feature('Var35', float),  # importance: 0.0071\n    # Feature('Var94', float),  # importance: 0.0071\n    # Feature('Var226', cat_dtype),  # importance: 0.007\n    # Feature('Var113', float),  # importance: 0.0068\n    # Feature('Var140', float),  # importance: 0.0068\n    # Feature('Var28', float),  # importance: 0.0066\n    # Feature('Var13', float),  # importance: 0.0066\n    # Feature('Var76', float),  # importance: 0.0066\n    # Feature('Var219', cat_dtype),  # importance: 0.0063\n    # Feature('Var200', cat_dtype),  # importance: 0.0061\n    # Feature('Var85', float),  # importance: 0.006\n    # Feature('Var196', cat_dtype),  # importance: 0.006\n    # Feature('Var74', float),  # importance: 0.0057\n    # Feature('Var25', float),  # importance: 0.0056\n    # Feature('Var163', float),  # importance: 0.0055\n    # Feature('Var221', cat_dtype),  # importance: 0.0054\n    # Feature('Var193', cat_dtype),  # importance: 0.0052\n    # Feature('Var119', float),  # importance: 0.0051\n    # Feature('Var203', cat_dtype),  # importance: 0.0039\n    # Feature('Var22', float),  # importance: 0.0039\n    # Feature('Var51', float),  # importance: 0.0037\n    # Feature('Var163_imputed', float),  # importance: 0.0036\n    # Feature('Var224', cat_dtype),  # importance: 0.0036\n    # Feature('Var213', cat_dtype),  # importance: 0.0036\n    # Feature('Var189_imputed', float),  # importance: 0.0035\n    # Feature('Var208', cat_dtype),  # importance: 0.0034\n    # Feature('Var143', float),  # importance: 0.0019\n    # Feature('Var210', cat_dtype),  # importance: 0.0017\n    # Feature('Var201', cat_dtype),  # importance: 0.0009\n    # Feature('Var181', float),  # importance: 0.0\n    # Feature('Var143_imputed', float),  # importance: 0.0\n    # Feature('Var119_imputed', float),  # importance: 0.0\n    # Feature('Var76_imputed', float),  # importance: 0.0\n    # Feature('Var149_imputed', float),  # importance: 0.0\n    # Feature('Var85_imputed', float),  # importance: 0.0\n    # Feature('Var7_imputed', float),  # importance: 0.0\n    # Feature('Var160_imputed', float),  # importance: 0.0\n    # Feature('Var72_imputed', float),  # importance: 0.0\n    # Feature('Var78_imputed', float),  # importance: 0.0\n    # Feature('Var153_imputed', float),  # importance: 0.0\n    # Feature('Var173_imputed', float),  # importance: 0.0\n    # Feature('Var112_imputed', float),  # importance: 0.0\n    # Feature('Var44', float),  # importance: 0.0\n    # Feature('Var25_imputed', float),  # importance: 0.0\n    # Feature('Var133_imputed', float),  # importance: 0.0\n    # Feature('Var132_imputed', float),  # importance: 0.0\n    # Feature('Var215', cat_dtype),  # importance: 0.0\n    # Feature('Var35_imputed', float),  # importance: 0.0\n    # Feature('Var6_imputed', float),  # importance: 0.0\n    # Feature('Var140_imputed', float),  # importance: 0.0\n    # Feature('Var51_imputed', float),  # importance: 0.0\n    # Feature('Var13_imputed', float),  # importance: 0.0\n    # Feature('Var94_imputed', float),  # importance: 0.0\n    # Feature('Var125_imputed', float),  # importance: 0.0\n    # Feature('Var181_imputed', float),  # importance: 0.0\n    # Feature('Var28_imputed', float),  # importance: 0.0\n    # Feature('Var22_imputed', float),  # importance: 0.0\n    # Feature('Var83_imputed', float),  # importance: 0.0\n    # Feature('Var195', cat_dtype),  # importance: 0.0\n    # Feature('Var134_imputed', float),  # importance: 0.0\n    # Feature('Var65_imputed', float),  # importance: 0.0\n    # Feature('Var24_imputed', float),  # importance: 0.0\n    # Feature('Var21_imputed', float),  # importance: 0.0\n    # Feature('Var38_imputed', float),  # importance: 0.0\n    # Feature('Var74_imputed', float),  # importance: 0.0\n    # Feature('Var44_imputed', float),  # importance: 0.0\n    # Feature('Var109_imputed', float),  # importance: 0.0\n    # Feature('Var81_imputed', float),  # importance: 0.0\n    # Feature('Var123_imputed', float),  # importance: 0.0\n], documentation='https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data ,'\n                 'https://medium.com/@kushaldps1996/customer-relationship-prediction-kdd-cup-2009-6b57d08ffb0')\n\nCHURN_FEATURES = FeatureList(features=[\n    Feature('label', float, name_extended='customer will switch provider (churn)', is_target=True),\n    Feature('Var202', cat_dtype),  # importance: 0.1061\n    Feature('Var222', cat_dtype),  # importance: 0.0707\n    Feature('Var220', cat_dtype),  # importance: 0.0699\n    Feature('Var217', cat_dtype),  # importance: 0.0606\n    Feature('Var199', cat_dtype),  # importance: 0.0393\n    Feature('Var126', float),  # importance: 0.0371\n    Feature('Var192', cat_dtype),  # importance: 0.0319\n    Feature('Var51_imputed', float),  # importance: 0.0208\n    Feature('Var216', cat_dtype),  # importance: 0.0207\n    Feature('Var126_imputed', float),  # importance: 0.0194\n    Feature('Var198', cat_dtype),  # importance: 0.0181\n    Feature('Var197', cat_dtype),  # importance: 0.0166\n    Feature('Var204', cat_dtype),  # importance: 0.0163\n    Feature('Var74', float),  # importance: 0.0132\n    Feature('Var78', float),  # importance: 0.012\n    Feature('Var7_imputed', float),  # importance: 0.0115\n    Feature('Var207', cat_dtype),  # importance: 0.0107\n    Feature('Var206', cat_dtype),  # importance: 0.0107\n    Feature('Var210', cat_dtype),  # importance: 0.0107\n    Feature('Var194', cat_dtype),  # importance: 0.0105\n    Feature('Var228', cat_dtype),  # importance: 0.0096\n    Feature('Var226', cat_dtype),  # importance: 0.0095\n    Feature('Var218', cat_dtype),  # importance: 0.0093\n    Feature('Var181', float),  # importance: 0.009\n    Feature('Var212', cat_dtype),  # importance: 0.0089\n    Feature('Var73', float),  # importance: 0.0089\n    Feature('Var44', float),  # importance: 0.0088\n    Feature('Var109', float),  # importance: 0.0084\n    Feature('Var13', float),  # importance: 0.0083\n    Feature('Var113', float),  # importance: 0.008\n    Feature('Var229', cat_dtype),  # importance: 0.0079\n    Feature('Var227', cat_dtype),  # importance: 0.0079\n    Feature('Var140', float),  # importance: 0.0078\n    ##################################################\n    ##################################################\n    # Feature('Var225', cat_dtype),  # importance: 0.0078\n    # Feature('Var28', float),  # importance: 0.0076\n    # Feature('Var189', float),  # importance: 0.0076\n    # Feature('Var195', cat_dtype),  # importance: 0.0076\n    # Feature('Var72', float),  # importance: 0.0075\n    # Feature('Var38', float),  # importance: 0.0073\n    # Feature('Var65', float),  # importance: 0.0073\n    # Feature('Var81', float),  # importance: 0.0073\n    # Feature('Var25', float),  # importance: 0.0072\n    # Feature('Var153', float),  # importance: 0.0072\n    # Feature('Var125', float),  # importance: 0.0071\n    # Feature('Var221', cat_dtype),  # importance: 0.0071\n    # Feature('Var21', float),  # importance: 0.007\n    # Feature('Var219', cat_dtype),  # importance: 0.007\n    # Feature('Var134', float),  # importance: 0.0069\n    # Feature('Var6', float),  # importance: 0.0069\n    # Feature('Var205', cat_dtype),  # importance: 0.0068\n    # Feature('Var57', float),  # importance: 0.0068\n    # Feature('Var160', float),  # importance: 0.0067\n    # Feature('Var123', float),  # importance: 0.0066\n    # Feature('Var83', float),  # importance: 0.0066\n    # Feature('Var76', float),  # importance: 0.0065\n    # Feature('Var149', float),  # importance: 0.0065\n    # Feature('Var133', float),  # importance: 0.0064\n    # Feature('Var144', float),  # importance: 0.0064\n    # Feature('Var112', float),  # importance: 0.0061\n    # Feature('Var94', float),  # importance: 0.006\n    # Feature('Var208', cat_dtype),  # importance: 0.006\n    # Feature('Var132', float),  # importance: 0.0058\n    # Feature('Var193', cat_dtype),  # importance: 0.0058\n    # Feature('Var163', float),  # importance: 0.0058\n    # Feature('Var223', cat_dtype),  # importance: 0.0057\n    # Feature('Var24', float),  # importance: 0.0057\n    # Feature('Var203', cat_dtype),  # importance: 0.0055\n    # Feature('Var119', float),  # importance: 0.0055\n    # Feature('Var7', float),  # importance: 0.0053\n    # Feature('Var215', cat_dtype),  # importance: 0.0052\n    # Feature('Var189_imputed', float),  # importance: 0.0049\n    # Feature('Var85', float),  # importance: 0.0049\n    # Feature('Var22', float),  # importance: 0.0039\n    # Feature('Var224', cat_dtype),  # importance: 0.0036\n    # Feature('Var211', cat_dtype),  # importance: 0.0032\n    # Feature('Var51', float),  # importance: 0.0031\n    # Feature('Var35', float),  # importance: 0.0028\n    # Feature('Var213', cat_dtype),  # importance: 0.0027\n    # Feature('Var200', cat_dtype),  # importance: 0.0024\n    # Feature('Var163_imputed', float),  # importance: 0.0023\n    # Feature('Var196', cat_dtype),  # importance: 0.002\n    # Feature('Var191', cat_dtype),  # importance: 0.0009\n    # Feature('Var78_imputed', float),  # importance: 0.0\n    # Feature('Var35_imputed', float),  # importance: 0.0\n    # Feature('Var149_imputed', float),  # importance: 0.0\n    # Feature('Var173', float),  # importance: 0.0\n    # Feature('Var153_imputed', float),  # importance: 0.0\n    # Feature('Var160_imputed', float),  # importance: 0.0\n    # Feature('Var201', cat_dtype),  # importance: 0.0\n    # Feature('Var173_imputed', float),  # importance: 0.0\n    # Feature('Var25_imputed', float),  # importance: 0.0\n    # Feature('Var112_imputed', float),  # importance: 0.0\n    # Feature('Var214', cat_dtype),  # importance: 0.0\n    # Feature('Var38_imputed', float),  # importance: 0.0\n    # Feature('Var83_imputed', float),  # importance: 0.0\n    # Feature('Var144_imputed', float),  # importance: 0.0\n    # Feature('Var143_imputed', float),  # importance: 0.0\n    # Feature('Var72_imputed', float),  # importance: 0.0\n    # Feature('Var119_imputed', float),  # importance: 0.0\n    # Feature('Var125_imputed', float),  # importance: 0.0\n    # Feature('Var133_imputed', float),  # importance: 0.0\n    # Feature('Var22_imputed', float),  # importance: 0.0\n    # Feature('Var123_imputed', float),  # importance: 0.0\n    # Feature('Var109_imputed', float),  # importance: 0.0\n    # Feature('Var140_imputed', float),  # importance: 0.0\n    # Feature('Var13_imputed', float),  # importance: 0.0\n    # Feature('Var94_imputed', float),  # importance: 0.0\n    # Feature('Var21_imputed', float),  # importance: 0.0\n    # Feature('Var181_imputed', float),  # importance: 0.0\n    # Feature('Var28_imputed', float),  # importance: 0.0\n    # Feature('Var81_imputed', float),  # importance: 0.0\n    # Feature('Var132_imputed', float),  # importance: 0.0\n    # Feature('Var6_imputed', float),  # importance: 0.0\n    # Feature('Var65_imputed', float),  # importance: 0.0\n    # Feature('Var24_imputed', float),  # importance: 0.0\n    # Feature('Var143', float),  # importance: 0.0\n    # Feature('Var74_imputed', float),  # importance: 0.0\n    # Feature('Var44_imputed', float),  # importance: 0.0\n    # Feature('Var85_imputed', float),  # importance: 0.0\n    # Feature('Var134_imputed', float),  # importance: 0.0\n    # Feature('Var76_imputed', float),  # importance: 0.0\n], documentation='https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data ,'\n                 'https://medium.com/@kushaldps1996/customer-relationship-prediction-kdd-cup-2009-6b57d08ffb0')\n\nUPSELLING_FEATURES = FeatureList(features=[\n    Feature('label', float, name_extended='customer will buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling)', is_target=True),\n    Feature('Var126', float),  # importance: 0.1205\n    Feature('Var202', cat_dtype),  # importance: 0.0812\n    Feature('Var198', cat_dtype),  # importance: 0.0687\n    Feature('Var211', cat_dtype),  # importance: 0.0635\n    Feature('Var222', cat_dtype),  # importance: 0.0623\n    Feature('Var28', float),  # importance: 0.0424\n    Feature('Var217', cat_dtype),  # importance: 0.0402\n    Feature('Var192', cat_dtype),  # importance: 0.0293\n    Feature('Var199', cat_dtype),  # importance: 0.0273\n    Feature('Var216', cat_dtype),  # importance: 0.0176\n    Feature('Var204', cat_dtype),  # importance: 0.0169\n    Feature('Var197', cat_dtype),  # importance: 0.0157\n    Feature('Var126_imputed', float),  # importance: 0.0141\n    Feature('Var220', cat_dtype),  # importance: 0.0119\n    Feature('Var22', float),  # importance: 0.0116\n    Feature('Var227', cat_dtype),  # importance: 0.0102\n    Feature('Var225', cat_dtype),  # importance: 0.0102\n    Feature('Var65', float),  # importance: 0.0102\n    Feature('Var206', cat_dtype),  # importance: 0.0095\n    Feature('Var153', float),  # importance: 0.009\n    Feature('Var212', cat_dtype),  # importance: 0.0084\n    Feature('Var228', cat_dtype),  # importance: 0.0082\n    Feature('Var207', cat_dtype),  # importance: 0.0079\n    Feature('Var81', float),  # importance: 0.0078\n    Feature('Var223', cat_dtype),  # importance: 0.0076\n    Feature('Var160', float),  # importance: 0.0075\n    Feature('Var218', cat_dtype),  # importance: 0.0074\n    Feature('Var213', cat_dtype),  # importance: 0.0073\n    Feature('Var226', cat_dtype),  # importance: 0.0071\n    Feature('Var140', float),  # importance: 0.0071\n    Feature('Var193', cat_dtype),  # importance: 0.0071\n    Feature('Var78', float),  # importance: 0.007\n    Feature('Var133', float),  # importance: 0.0068\n    ##################################################\n    ##################################################\n    # Feature('Var85', float),  # importance: 0.0066\n    # Feature('Var194', cat_dtype),  # importance: 0.0066\n    # Feature('Var221', cat_dtype),  # importance: 0.0065\n    # Feature('Var113', float),  # importance: 0.0065\n    # Feature('Var119', float),  # importance: 0.0065\n    # Feature('Var219', cat_dtype),  # importance: 0.0062\n    # Feature('Var200', cat_dtype),  # importance: 0.0062\n    # Feature('Var13', float),  # importance: 0.0061\n    # Feature('Var6', float),  # importance: 0.0061\n    # Feature('Var21', float),  # importance: 0.0061\n    # Feature('Var163', float),  # importance: 0.0061\n    # Feature('Var57', float),  # importance: 0.006\n    # Feature('Var125', float),  # importance: 0.0059\n    # Feature('Var205', cat_dtype),  # importance: 0.0058\n    # Feature('Var38', float),  # importance: 0.0058\n    # Feature('Var94', float),  # importance: 0.0058\n    # Feature('Var24', float),  # importance: 0.0057\n    # Feature('Var210', cat_dtype),  # importance: 0.0056\n    # Feature('Var144', float),  # importance: 0.0056\n    # Feature('Var134', float),  # importance: 0.0055\n    # Feature('Var149', float),  # importance: 0.0054\n    # Feature('Var83', float),  # importance: 0.0054\n    # Feature('Var229', cat_dtype),  # importance: 0.0054\n    # Feature('Var25', float),  # importance: 0.0053\n    # Feature('Var123', float),  # importance: 0.0053\n    # Feature('Var76', float),  # importance: 0.005\n    # Feature('Var74', float),  # importance: 0.0049\n    # Feature('Var109', float),  # importance: 0.0048\n    # Feature('Var112', float),  # importance: 0.0048\n    # Feature('Var189', float),  # importance: 0.0047\n    # Feature('Var203', cat_dtype),  # importance: 0.0047\n    # Feature('Var7', float),  # importance: 0.0046\n    # Feature('Var208', cat_dtype),  # importance: 0.0045\n    # Feature('Var191', cat_dtype),  # importance: 0.0045\n    # Feature('Var73', float),  # importance: 0.0045\n    # Feature('Var72', float),  # importance: 0.0045\n    # Feature('Var132', float),  # importance: 0.0043\n    # Feature('Var7_imputed', float),  # importance: 0.004\n    # Feature('Var196', cat_dtype),  # importance: 0.0037\n    # Feature('Var35', float),  # importance: 0.0034\n    # Feature('Var195', cat_dtype),  # importance: 0.0027\n    # Feature('Var173', float),  # importance: 0.0026\n    # Feature('Var51', float),  # importance: 0.0026\n    # Feature('Var189_imputed', float),  # importance: 0.0025\n    # Feature('Var224', cat_dtype),  # importance: 0.002\n    # Feature('Var163_imputed', float),  # importance: 0.0018\n    # Feature('Var181', float),  # importance: 0.0014\n    # Feature('Var83_imputed', float),  # importance: 0.0\n    # Feature('Var119_imputed', float),  # importance: 0.0\n    # Feature('Var215', cat_dtype),  # importance: 0.0\n    # Feature('Var76_imputed', float),  # importance: 0.0\n    # Feature('Var85_imputed', float),  # importance: 0.0\n    # Feature('Var173_imputed', float),  # importance: 0.0\n    # Feature('Var160_imputed', float),  # importance: 0.0\n    # Feature('Var78_imputed', float),  # importance: 0.0\n    # Feature('Var134_imputed', float),  # importance: 0.0\n    # Feature('Var25_imputed', float),  # importance: 0.0\n    # Feature('Var201', cat_dtype),  # importance: 0.0\n    # Feature('Var214', cat_dtype),  # importance: 0.0\n    # Feature('Var144_imputed', float),  # importance: 0.0\n    # Feature('Var143_imputed', float),  # importance: 0.0\n    # Feature('Var44', float),  # importance: 0.0\n    # Feature('Var72_imputed', float),  # importance: 0.0\n    # Feature('Var6_imputed', float),  # importance: 0.0\n    # Feature('Var74_imputed', float),  # importance: 0.0\n    # Feature('Var81_imputed', float),  # importance: 0.0\n    # Feature('Var28_imputed', float),  # importance: 0.0\n    # Feature('Var125_imputed', float),  # importance: 0.0\n    # Feature('Var123_imputed', float),  # importance: 0.0\n    # Feature('Var109_imputed', float),  # importance: 0.0\n    # Feature('Var13_imputed', float),  # importance: 0.0\n    # Feature('Var94_imputed', float),  # importance: 0.0\n    # Feature('Var140_imputed', float),  # importance: 0.0\n    # Feature('Var133_imputed', float),  # importance: 0.0\n    # Feature('Var149_imputed', float),  # importance: 0.0\n    # Feature('Var181_imputed', float),  # importance: 0.0\n    # Feature('Var22_imputed', float),  # importance: 0.0\n    # Feature('Var44_imputed', float),  # importance: 0.0\n    # Feature('Var35_imputed', float),  # importance: 0.0\n    # Feature('Var153_imputed', float),  # importance: 0.0\n    # Feature('Var65_imputed', float),  # importance: 0.0\n    # Feature('Var38_imputed', float),  # importance: 0.0\n    # Feature('Var24_imputed', float),  # importance: 0.0\n    # Feature('Var143', float),  # importance: 0.0\n    # Feature('Var21_imputed', float),  # importance: 0.0\n    # Feature('Var112_imputed', float),  # importance: 0.0\n    # Feature('Var51_imputed', float),  # importance: 0.0\n    # Feature('Var132_imputed', float),  # importance: 0.0\n], documentation='https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data ,'\n                 'https://medium.com/@kushaldps1996/customer-relationship-prediction-kdd-cup-2009-6b57d08ffb0')\n\nCLICK_FEATURES = FeatureList(features=[\n    Feature('click', float, is_target=True,\n            name_extended='user clicked ad at least once'),\n    Feature('impression', cat_dtype),\n    Feature('url_hash', cat_dtype, name_extended='URL hash'),\n    Feature('ad_id', cat_dtype, name_extended='ad ID'),\n    Feature('advertiser_id', float, name_extended='advertiser ID'),\n    Feature('depth', float,\n            name_extended='number of ads impressed in this session'),\n    Feature('position', cat_dtype,\n            name_extended='order of this ad in the impression list'),\n    Feature('query_id', cat_dtype, name_extended='query ID'),\n    Feature('keyword_id', cat_dtype, name_extended='keyword ID'),\n    Feature('title_id', cat_dtype, name_extended='title ID'),\n    Feature('description_id', cat_dtype, name_extended='description ID'),\n    Feature('user_id', float, name_extended='user ID'),\n], documentation='https://www.kaggle.com/competitions/kddcup2012-track2/data ,'\n                 'http://www.kdd.org/kdd-cup/view/kdd-cup-2012-track-2')\n\nKICK_FEATURES = FeatureList(features=[\n    Feature('RefId', cat_dtype,\n            name_extended='Unique (sequential) number assigned to vehicles'),\n    Feature('IsBadBuy', int, is_target=True,\n            name_extended='indicator for whether the kicked vehicle was an avoidable purchase'),\n    Feature('PurchDate', cat_dtype,\n            name_extended='date the vehicle was purchased at auction'),\n    Feature('Auction', cat_dtype,\n            name_extended=\"Auction provider at which the  vehicle was purchased\"),\n    Feature('VehYear', int, name_extended=\"manufacture year of the vehicle\"),\n    Feature('VehicleAge', int,\n            name_extended=\"Years elapsed since the manufacture year\"),\n    Feature('Make', cat_dtype),\n    Feature('Model', cat_dtype),\n    Feature('Trim', cat_dtype, name_extended='Trim Level'),\n    Feature('SubModel', cat_dtype),\n    Feature('Color', cat_dtype),\n    Feature('Transmission', cat_dtype,\n            name_extended='Vehicle transmission type'),\n    Feature('WheelTypeID', cat_dtype,\n            name_extended='type id of the vehicle wheel'),\n    Feature('WheelType', cat_dtype, name_extended='wheel type description'),\n    Feature('VehOdo', int, name_extended='odometer reading'),\n    Feature('Nationality', cat_dtype, name_extended=\"manufacturer's country\"),\n    Feature('Size', cat_dtype, name_extended='size category of the vehicle'),\n    Feature('TopThreeAmericanName', cat_dtype,\n            name_extended='manufacturer is one of the top three American manufacturers'),\n    Feature('MMRAcquisitionAuctionAveragePrice', float,\n            name_extended='average acquisition price at auction for this vehicle in average condition at time of purchase'),\n    Feature('MMRAcquisitionAuctionCleanPrice', float,\n            name_extended='average acquisition price at auction for this vehicle in the above average condition at time of purchase'),\n    Feature('MMRAcquisitionRetailAveragePrice', float,\n            name_extended='average retail price for this vehicle in average condition at time of purchase'),\n    Feature('MMRAcquisitonRetailCleanPrice', float,\n            name_extended='average retail price for this vehicle in above average condition at time of purchase'),\n    Feature('MMRCurrentAuctionAveragePrice', float,\n            name_extended='average acquisition price at auction for this vehicle in average condition as of current day'),\n    Feature('MMRCurrentAuctionCleanPrice', float,\n            name_extended='average acquisition price at auction for this vehicle in above average condition as of current day'),\n    Feature('MMRCurrentRetailAveragePrice', float,\n            name_extended='average retail price for this vehicle in average condition as of current day'),\n    Feature('MMRCurrentRetailCleanPrice', float,\n            name_extended='average retail price for this vehicle in above average condition as of current day'),\n    Feature('PRIMEUNIT', cat_dtype,\n            name_extended='vehicle would have a higher demand than a standard purchase'),\n    Feature('AUCGUART', cat_dtype,\n            name_extended='acquisition method of vehicle'),\n    Feature('BYRNO', int,\n            name_extended='level guarantee provided by auction for the vehicle'),\n    Feature('VNZIP1', cat_dtype, 'ZIP code where the car was purchased'),\n    Feature('VNST', cat_dtype,\n            name_extended='State where the the car was purchased'),\n    Feature('VehBCost', float,\n            name_extended='acquisition cost paid for the vehicle at time of purchase'),\n    Feature('IsOnlineSale', int,\n            name_extended='vehicle was originally purchased online'),\n    Feature('WarrantyCost', int,\n            name_extended='Warranty price (with term=36 month and mileage=36K)'),\n], documentation=\"https://www.kaggle.com/competitions/DontGetKicked/ , \"\n                 \"https://www.kaggle.com/competitions/DontGetKicked/data\")\n\n\ndef preprocess_kick(df: DataFrame) -> DataFrame:\n    return df\n\n\ndef preprocess_click(data: DataFrame) -> DataFrame:\n    categorical_features = {1, 2, 3, 6, 7, 8, 9, 10}\n\n    def clean_string(s):\n        return \"v_\" + re.sub('[^A-Za-z0-9]+', \"_\", str(s))\n\n    for i in categorical_features:\n        data[data.columns[i]] = data[data.columns[i]].apply(clean_string)\n\n    data[\"click\"] = data[\"click\"].apply(lambda x: 1 if x != 0 else -1)\n\n    return data\n\n\ndef preprocess_appetency(data: DataFrame) -> DataFrame:\n    \"\"\"Adapted from https://github.com/catboost/benchmarks/blob/master\n    /quality_benchmarks/prepare_appetency_churn_upselling\n    /prepare_appetency_churn_upselling.ipynb \"\"\"\n\n    # preparing categorical features\n\n    categorical_features = {190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n                            200, 201, 202, 203, 204, 205, 206, 207, 209, 210,\n                            211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n                            221, 222, 223, 224, 225, 226, 227, 228}\n\n    numeric_colnames = {column for i, column in enumerate(data.columns) if\n                        i not in categorical_features}\n\n    # Note: we do not need to explicitly cast categorical features to string;\n    # these are already of dtype object.\n\n    for i in categorical_features:\n        data[data.columns[i]] = data[data.columns[i]].fillna(\"MISSING\").apply(\n            str).astype(\"category\")\n\n    # prepare numerical features\n\n    # drop any numeric column that is >= 95% missing\n    all_missing = data.columns[(pd.isnull(data).sum() >= 0.95 * len(data))]\n    data.drop(columns=all_missing, inplace=True)\n    numeric_colnames -= set(all_missing)\n\n    columns_to_impute = []\n    for column in numeric_colnames:\n        if pd.isnull(data[column]).any():\n            columns_to_impute.append(column)\n\n    for column_name in columns_to_impute:\n        data[column_name + \"_imputed\"] = pd.isnull(data[column_name]).astype(\n            float)\n        data[column_name].fillna(0, inplace=True)\n\n    for column in numeric_colnames:\n        data[column] = data[column].astype(float)\n\n    return data\n"}
{"type": "source_file", "path": "tableshift/core/getters.py", "content": "import copy\nimport logging\nfrom typing import Optional, Dict, Any, Union\n\nfrom tableshift.core.tasks import _TASK_REGISTRY\nfrom tableshift.core.data_source import DataSource\nfrom tableshift import exceptions\nfrom tableshift.configs.experiment_defaults import DEFAULT_RANDOM_STATE\nfrom tableshift.configs.benchmark_configs import BENCHMARK_CONFIGS\nfrom tableshift.configs.non_benchmark_configs import NON_BENCHMARK_CONFIGS\nfrom .tabular_dataset import TabularDataset, DatasetConfig, CachedDataset\nfrom .features import PreprocessorConfig\nfrom .splitter import RandomSplitter\n\nEXPERIMENT_CONFIGS = {\n    **BENCHMARK_CONFIGS,\n    **NON_BENCHMARK_CONFIGS\n}\n\ndef get_data_source(name:str, cache_dir:str, download=True, **kwargs) -> DataSource:\n    \"\"\"Get the data source for a dataset, if it exists in the task registry.\"\"\"\n    if name not in _TASK_REGISTRY:\n        raise ValueError(f\"Dataset '{name}' not in available task registry: {sorted(_TASK_REGISTRY.keys())}\")\n    task_config = _TASK_REGISTRY[name]\n    return task_config.data_source_cls(cache_dir=cache_dir, download=download, **kwargs)\n\ndef get_dataset(name: str, cache_dir: str = \"tmp\",\n                preprocessor_config: Optional[\n                    PreprocessorConfig] = None,\n                initialize_data: bool = True,\n                use_cached: bool = False,\n                **kwargs) -> Union[TabularDataset, CachedDataset]:\n    \"\"\"Helper function to fetch a dataset.\n\n    Args:\n        name: the dataset name.\n        cache_dir: the cache directory to use. TableShift will check for cached\n            data files here before downloading.\n        preprocessor_config: optional Preprocessor to override the default\n            preprocessor config. If using the TableShift benchmark, it is\n            recommended to leave this as None to use the default preprocessor.\n        initialize_data: passed to TabularDataset constructor.\n        use_cached: whether to used cached dataset.\n        kwargs: optional kwargs to be passed to TabularDataset; these will\n            override their respective kwargs in the experiment config.\n        \"\"\"\n    if name not in EXPERIMENT_CONFIGS.keys():\n        raise exceptions.ConfigNotFoundException(\n            f\"Dataset name {name} is not available; choices are: \" \\\n        f\"{sorted(EXPERIMENT_CONFIGS.keys())}\")\n\n    expt_config = EXPERIMENT_CONFIGS[name]\n    dataset_config = DatasetConfig(cache_dir=cache_dir)\n    tabular_dataset_kwargs = copy.copy(expt_config.tabular_dataset_kwargs)\n    if \"name\" not in tabular_dataset_kwargs:\n        tabular_dataset_kwargs[\"name\"] = name\n\n    if preprocessor_config is None:\n        preprocessor_config = expt_config.preprocessor_config\n\n    if not use_cached:\n        dset = TabularDataset(\n            config=dataset_config,\n            splitter=expt_config.splitter,\n            grouper=kwargs.get(\"grouper\", expt_config.grouper),\n            preprocessor_config=preprocessor_config,\n            initialize_data=initialize_data,\n            **tabular_dataset_kwargs)\n    else:\n        dset = CachedDataset(config=dataset_config,\n                             splitter=expt_config.splitter,\n                             grouper=kwargs.get(\"grouper\", expt_config.grouper),\n                             preprocessor_config=preprocessor_config,\n                             initialize_data=initialize_data,\n                             name=name)\n    return dset\n\n\ndef get_iid_dataset(name: str, cache_dir: str = \"tmp\",\n                    val_size: float = 0.1,\n                    test_size: float = 0.25,\n                    random_state: int = DEFAULT_RANDOM_STATE,\n                    preprocessor_config: Optional[\n                        PreprocessorConfig] = None,\n                    initialize_data: bool = True,\n                    use_cached: bool = False,\n                    tabular_dataset_kwargs: Optional[Dict[str, Any]] = None,\n                    **kwargs,\n\n                    ) -> Union[TabularDataset, CachedDataset]:\n    \"\"\"Helper function to fetch an IID dataset.\n\n    This fetches a version of the TableShift benchmark dataset but *witihout*\n    a domain split. This is mostly for testing or exploring non-domain-robust\n    learning methods.\n\n    Args:\n        name: the dataset name.\n        cache_dir: the cache directory to use. TableShift will check for cached\n            data files here before downloading.\n        val_size: fraction of dataset to use for validation split.\n        test_size: fraction of dataset to use for test split.\n        random_state: integer random state to use for splitting,\n            for reproducibility.\n        preprocessor_config: optional Preprocessor to override the default\n            preprocessor config. If using the TableShift benchmark, it is\n            recommended to leave this as None to use the default preprocessor.\n        initialize_data: passed to TabularDataset constructor.\n        use_cached: if True, load a cached dataset from cache_dir\n            with specified uid.\n        uid: uid to use for the cached dataset. Not used when use_cached=False.\n        tabular_dataset_kwargs: optional overrides for tabular dataset kwargs.\n        kwargs: optional kwargs to be passed to TabularDataset; these will\n            override their respective kwargs in the experiment config.\n        \"\"\"\n    if name not in EXPERIMENT_CONFIGS.keys():\n        raise exceptions.ConfigNotFoundException(\n            f\"Dataset name {name} is not available; choices are: \"\n            f\"{sorted(EXPERIMENT_CONFIGS.keys())}\")\n\n    expt_config = EXPERIMENT_CONFIGS[name]\n    dataset_config = DatasetConfig(cache_dir=cache_dir)\n\n    _tabular_dataset_kwargs = copy.copy(expt_config.tabular_dataset_kwargs)\n    if tabular_dataset_kwargs:\n        _tabular_dataset_kwargs.update(tabular_dataset_kwargs)\n    if \"name\" not in _tabular_dataset_kwargs:\n        _tabular_dataset_kwargs[\"name\"] = name\n\n    if preprocessor_config is None:\n        preprocessor_config = expt_config.preprocessor_config\n\n    if not use_cached:\n\n        dset = TabularDataset(\n            config=dataset_config,\n            splitter=RandomSplitter(val_size=val_size,\n                                    random_state=random_state,\n                                    test_size=test_size),\n            grouper=kwargs.get(\"grouper\", expt_config.grouper),\n            preprocessor_config=preprocessor_config,\n            initialize_data=initialize_data,\n            **_tabular_dataset_kwargs)\n    else:\n\n        logging.info(f\"loading cached data from {cache_dir}\")\n        dset = CachedDataset(config=dataset_config,\n                             name=name,\n                             initialize_data=initialize_data,\n                             preprocessor_config=preprocessor_config)\n\n    return dset\n"}
{"type": "source_file", "path": "tableshift/datasets/diabetes_readmission.py", "content": "\"\"\"\nUtilities for the Diabetes readmission dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nimport json\nimport os\nimport re\nfrom typing import Dict\n\nimport pandas as pd\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nDIABETES_READMISSION_RESOURCES = [\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00296\"\n    \"/dataset_diabetes.zip\"\n]\n\n# Common feature description for the 24 features for medications\nDIABETES_MEDICATION_FEAT_DESCRIPTION = \"\"\"Indicates if there was any diabetic \n    medication prescribed. Values: 'yes' and 'no' For the generic names: \n    metformin, repaglinide, nateglinide, chlorpropamide, glimepiride, \n    acetohexamide, glipizide, glyburide, tolbutamide, pioglitazone, \n    rosiglitazone, acarbose, miglitol, troglitazone, tolazamide, examide, \n    sitagliptin, insulin, glyburide-metformin, glipizide-metformin, \n    glimepiride-pioglitazone, metformin-rosiglitazone, \n    and metformin-pioglitazone, the feature indicates whether the drug was \n    prescribed or there was a change in the dosage. Values: 'up' if the \n    dosage was increased during the encounter, 'down' if the dosage was \n    decreased, 'steady' if the dosage did not change, and 'no' if the drug \n    was not prescribed.\"\"\"\n\n# These codes are missing from the list we used.\nSUPERFICIAL_INJURY_ICD9_CODES = {\n    \"910\": \"Superficial injury of face, neck, and scalp except eye\",\n    \"911\": \"Superficial injury of trunk\",\n    \"912\": \"Superficial injury of shoulder and upper arm\",\n    \"913\": \"Superficial injury of elbow, forearm, and wrist\",\n    \"914\": \"Superficial injury of hand(s) except finger(s) alone\",\n    \"915\": \" Superficial injury of finger(s)\",\n    \"916\": \"Superficial injury of hip, thigh, leg, and ankle\",\n    \"917\": \"Superficial injury of foot and toe(s)\",\n    \"918\": \"Superficial injury of eye and adnexa\",\n    \"919\": \"Superficial injury of other, multiple, and unspecified sites\",\n    \"919.0\": \"Abrasion or friction burn of other multiple and unspecified sites without infection\",\n    \"919.1\": \"Abrasion or friction burn of other multiple and unspecified sites infected\",\n    \"919.2\": \"Blister of other multiple and unspecified sites without infection\",\n    \"919.3\": \"Blister of other multiple and unspecified sites infected\",\n    \"919.4\": \"Insect bite nonvenomous of other multiple and unspecified sites without infection\",\n    \"919.5\": \"Insect bite nonvenomous of other multiple and unspecified sites infected\",\n    \"919.6\": \"Superficial foreign body (splinter) of other multiple and unspecified sites without major open wound and without infection\",\n    \"919.7\": \"Superficial foreign body (splinter) of other multiple and unspecified sites without major open wound infected\",\n    \"919.8\": \"Other and unspecified superficial injury of other multiple and unspecified sites without infection\",\n    \"919.9\": \"Other and unspecified superficial injury of other multiple and unspecified sites infected\",\n}\n\n\ndef _diabetes_icd9_codes() -> Dict[str, str]:\n    # See https://en.wikipedia.org/wiki/List_of_ICD-9_codes_240–279:_endocrine,_nutritional_and_metabolic_diseases,_and_immunity_disorders\n    # First digit of decimal\n    first_decimal_codes = {0: 'without mention of complication',\n                           1: 'with ketoacidosis',\n                           2: 'with hyperosmolarity',\n                           3: 'with other coma',\n                           4: 'with renal manifestations',\n                           5: 'with ophthalmic manifestations',\n                           6: 'with neurological manifestations',\n                           7: 'with peripheral circulatory disorders',\n                           8: 'with other specified manifestations',\n                           9: 'with unspecified complication',\n                           }\n\n    # Second digit of decimal\n    second_decimal_codes = {\n        # (250.x0) Diabetes mellitus type 2\n        0: 'Diabetes mellitus type 2',\n        # (250.x1) Diabetes mellitus type 1\n        1: 'Diabetes mellitus type 1',\n        # (250.x2) Diabetes mellitus type 2, uncontrolled\n        2: 'Diabetes mellitus type 2, uncontrolled',\n        # (250.x3) Diabetes mellitus type 1, uncontrolled\n        3: 'Diabetes mellitus type 1, uncontrolled',\n    }\n    diabetes_codes_mapping = {}\n    for first_decimal, desc in first_decimal_codes.items():\n        for second_decimal, diabetes_type in second_decimal_codes.items():\n            code = f'250.{first_decimal}{second_decimal}'\n            desc = ' '.join((diabetes_type, desc))\n            diabetes_codes_mapping[code] = desc\n    return diabetes_codes_mapping\n\n\ndef get_icd9(depths=(3, 4)) -> dict:\n    \"\"\"Fetch a dictionary mapping ICD9 codes to string descriptors.\n\n    Also applies some preprocessing (dropping leading zeros) to match the format used in diabetes dataset.\n    Additionally, since the dataset uses depth-3 codes for non-diabetes diagnoses, but depth-4\n    codes for diabetes diagnosis (i.e. 250.x), we take the union of both depths such that\n    every code present in the dataset is mapped.\n    \"\"\"\n    # via https://raw.githubusercontent.com/sirrice/icd9/master/codes.json\n    fp = os.path.join(os.path.dirname(__file__), \"./icd9-codes.json\")\n    with open(fp, \"r\") as f:\n        raw = f.read()\n    icd9_codes = json.loads(raw)\n\n    def _preprocess_code_text(x: str) -> str:\n        # Drop any leading zeros in codes\n        try:\n            return re.sub(\"^0+\", \"\", x)\n        except:\n            import ipdb;\n            ipdb.set_trace()\n\n    mapping = {}\n    for depth in depths:\n        depth_mapping = {_preprocess_code_text(c[\"code\"]): c[\"descr\"]\n                         for group in icd9_codes for c in group\n                         if c['depth'] == depth}\n        mapping.update(depth_mapping)\n    # Add the detailed diabetes codes; these are also used in this dataset for diabetes only.\n    diabetes_icd9_codes = _diabetes_icd9_codes()\n    mapping.update(diabetes_icd9_codes)\n    mapping.update(SUPERFICIAL_INJURY_ICD9_CODES)\n    mapping.update({\"235\": \"Neoplasm of uncertain behavior of digestive and respiratory systems\",\n                    \"365.44\": \"Glaucoma associated with congenital anomalies, with dystrophies and with systemic syndromes\",\n                    \"752\": \"Congenital anomalies of genital organs\",\n                    \"E849\": \"Place of occurrence at Home\",\n                    \"nan\": \"Not entered, unknown or missing\"})\n    return mapping\n\n\n# Note: the UCI version of this dataset does *not* exactly correspond to the\n# version documented in the linked paper. For example, in the paper, 'weight'\n# is described as a numeric feature, but is discretized into bins in UCI;\n# similarly, many fields are marked as having much higher missing value counts\n# in the paper than are present in the UCI data. The cause of this discrepancy\n# is not clear.\nDIABETES_READMISSION_FEATURES = FeatureList(features=[\n    Feature('race', cat_dtype, \"\"\"Nominal. Values: Caucasian, Asian, African \n    American, Hispanic, and other\"\"\"),\n    Feature('gender', cat_dtype, \"\"\"Nominal. Values: male, female, and \n    unknown/invalid.\"\"\"),\n    Feature('age', cat_dtype, \"\"\"Nominal. Grouped in 10-year intervals: [0, \n    10), [10, 20), . . ., [90, 100)\"\"\"),\n    Feature('weight', cat_dtype, \"Weight in pounds. Grouped in 25-pound \"\n                                 \"intervals.\"),\n    Feature('admission_type_id', int, \"\"\"Integer identifier corresponding \n    to 9 distinct values.\"\"\",\n            value_mapping={\n                1: 'Emergency', 2: 'Urgent', 3: 'Elective', 4: 'Newborn',\n                5: 'Not Available', 6: 'NULL', 7: 'Trauma Center',\n                8: 'Not Mapped',\n            },\n            name_extended=\"Admission type\"),\n    Feature(\n        'discharge_disposition_id', int,\n        \"Integer identifier corresponding to 29 distinct values.\",\n        name_extended=\"Discharge type\",\n        value_mapping={\n            1: 'Discharged to home',\n            2: 'Discharged/transferred to another short term hospital',\n            3: 'Discharged/transferred to SNF',\n            4: 'Discharged/transferred to ICF',\n            5: 'Discharged/transferred to another type of inpatient care institution',\n            6: 'Discharged/transferred to home with home health service',\n            7: 'Left AMA',\n            8: 'Discharged/transferred to home under care of Home IV provider',\n            9: 'Admitted as an inpatient to this hospital',\n            10: 'Neonate discharged to  another hospital for neonatal aftercare',\n            11: 'Expired',\n            12: 'Still patient or expected to return for outpatient services',\n            13: 'Hospice / home',\n            14: 'Hospice / medical facility',\n            15: 'Discharged/transferred within this institution to Medicare approved swing bed',\n            16: 'Discharged/transferred/referred another institution for outpatient services',\n            17: 'Discharged/transferred/referred to this institution for outpatient services',\n            18: 'NULL',\n            19: 'Expired at home. Medicaid only, hospice.',\n            20: 'Expired in a medical facility. Medicaid only, hospice.',\n            21: 'Expired: place unknown. Medicaid only, hospice.',\n            22: 'Discharged/transferred to another rehab fac including rehab units of a hospital.',\n            23: 'Discharged/transferred to a long term care hospital.',\n            24: 'Discharged/transferred to a nursing facility certified under Medicaid  but not certified under Medicare.',\n            25: 'Not Mapped',\n            26: 'Unknown/Invalid',\n            27: 'Discharged/transferred to a federal health care facility.',\n            28: 'Discharged/transferred/referred to a psychiatric hospital of psychiatric distinct part unit of a hospital',\n            29: 'Discharged/transferred to a Critical Access Hospital (CAH).',\n            30: 'Discharged/transferred to another Type of Health Care Institution not Defined Elsewhere',\n        }),\n    Feature('admission_source_id', int, \"\"\"Integer identifier corresponding \n    to 21 distinct values.\"\"\",\n            name_extended=\"Admission source\",\n            value_mapping={\n                1: 'Physician Referral', 2: 'Clinic Referral ',\n                3: 'HMO Referral', 4: 'Transfer from a hospital',\n                5: 'Transfer from a Skilled Nursing Facility (SNF)',\n                6: 'Transfer from another health care facility',\n                7: 'Emergency Room',\n                8: 'Court/Law Enforcement',\n                9: 'Not Available',\n                10: 'Transfer from critial access hospital',\n                11: 'Normal Delivery',\n                12: 'Premature Delivery',\n                13: 'Sick Baby', 14: 'Extramural Birth', 15: 'Not Available',\n                17: 'NULL',\n                18: 'Transfer From Another Home Health Agency',\n                19: 'Readmission to Same Home Health Agency',\n                20: 'Not Mapped', 21: 'Unknown/Invalid',\n                22: 'Transfer from hospital inpt/same fac reslt in a sep claim',\n                23: 'Born inside this hospital',\n                24: 'Born outside this hospital',\n                25: 'Transfer from Ambulatory Surgery Center',\n                26: 'Transfer from Hospice',\n            }),\n    Feature('time_in_hospital', float,\n            \"Integer number of days between admission and discharge\",\n            name_extended=\"Count of days beween admission and discharge\"),\n    Feature('payer_code', cat_dtype, \"Integer identifier corresponding to 23 \"\n                                     \"distinct values, for example, \"\n                                     \"Blue Cross\\Blue Shield, Medicare, \"\n                                     \"and self-pay\",\n            name_extended=\"Payer code\"),\n    Feature('medical_specialty', cat_dtype, \"Specialty of the admitting \"\n                                            \"physician, corresponding to 84 \"\n                                            \"distinct values, for example, \"\n                                            \"cardiology, internal medicine, \"\n                                            \"family/general practice, \"\n                                            \"and surgeon.\",\n            name_extended=\"Medical specialty of the admitting physician\"),\n    Feature('num_lab_procedures', float, \"Number of lab tests performed \"\n                                         \"during the encounter\",\n            name_extended=\"Number of lab tests performed during the encounter\"),\n    Feature('num_procedures', float, \"Number of procedures (other than lab \"\n                                     \"tests) performed during the encounter\",\n            name_extended=\"Number of procedures (other than lab tests) \"\n                          \"performed during the encounter\"),\n    Feature('num_medications', float, \"Number of distinct generic names \"\n                                      \"administered during the encounter\",\n            name_extended=\"Number of distinct generic drugs \"\n                          \"administered during the encounter\"),\n    Feature('number_outpatient', float, \"Number of outpatient visits of the \"\n                                        \"patient in the year preceding the \"\n                                        \"encounter\",\n            name_extended=\"Number of outpatient visits of the \"\n                          \"patient in the year preceding the \"\n                          \"encounter\"),\n    Feature('number_emergency', float,\n            \"Number of emergency visits of the \"\n            \"patient in the year preceding the \"\n            \"encounter\",\n            name_extended=\"Number of emergency visits of the \"\n                          \"patient in the year preceding the \"\n                          \"encounter\"),\n    Feature('number_inpatient', float,\n            \"Number of inpatient visits of the \"\n            \"patient in the year preceding the \"\n            \"encounter\",\n            name_extended=\"Number of inpatient visits of the \"\n                          \"patient in the year preceding the \"\n                          \"encounter\"),\n    Feature('diag_1', cat_dtype, \"The primary diagnosis (coded as first three \"\n                                 \"digits of ICD9); 848 distinct values\",\n            value_mapping=get_icd9(),\n            name_extended=\"Primary diagnosis\",\n            ),\n    Feature('diag_2', cat_dtype, \"Secondary diagnosis (coded as first three \"\n                                 \"digits of ICD9); 923 distinct values\",\n            name_extended=\"Secondary diagnosis\",\n            value_mapping=get_icd9()),\n    Feature('diag_3', cat_dtype, \"Additional secondary diagnosis (coded as \"\n                                 \"first three digits of ICD9); 954 distinct \"\n                                 \"values\",\n            name_extended=\"Additional secondary diagnosis\",\n            value_mapping=get_icd9()),\n    Feature('number_diagnoses', float, \"Number of diagnoses entered to the \"\n                                       \"system\",\n            name_extended=\"Total number of diagnoses\"),\n    Feature('max_glu_serum', cat_dtype, \"Indicates the range of the result or \"\n                                        \"if the test was not taken. Values: \"\n                                        \"'>200,' '>300,' 'normal,' and 'none' \"\n                                        \"if not measured\",\n            name_extended=\"Max glucose serum\"),\n    Feature('A1Cresult', cat_dtype, \"Indicates the range of the result or if \"\n                                    \"the test was not taken. Values: '>8' if \"\n                                    \"the result was greater than 8%, '>7' if \"\n                                    \"the result was greater than 7% but less \"\n                                    \"than 8%, 'normal' if the result was less \"\n                                    \"than 7%, and 'none' if not measured.\",\n            name_extended='Hemoglobin A1c test result',\n            note=\"\"\"From original citation: Hemoglobin A1c (HbA1c) is an \n            important measure of glucose control, which is widely applied to \n            measure performance of diabetes care. (See documentation link.)\"\"\"\n            ),\n    Feature('metformin', cat_dtype, \"Indicates if there was a change in \"\n                                    \"diabetic medications (either dosage or \"\n                                    \"generic name). Values: 'change' and 'no \"\n                                    \"change'\",\n            name_extended='Change in metformin medication'),\n    Feature('repaglinide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in repaglinide medication'),\n    Feature('nateglinide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in nateglinide medication'),\n    Feature('chlorpropamide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in chlorpropamide medication'),\n    Feature('glimepiride', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glimepiride medication'),\n    Feature('acetohexamide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in acetohexamide medication'),\n    Feature('glipizide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glipizide medication'),\n    Feature('glyburide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glyburide medication'),\n    Feature('tolbutamide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in tolbutamide medication'),\n    Feature('pioglitazone', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in pioglitazone medication'),\n    Feature('rosiglitazone', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in rosiglitazone medication'),\n    Feature('acarbose', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in acarbose medication'),\n    Feature('miglitol', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in miglitol medication'),\n    Feature('troglitazone', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in troglitazone medication'),\n    Feature('tolazamide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in tolazamide medication'),\n    Feature('examide', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in examide medication'),\n    Feature('citoglipton', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in citoglipton medication'),\n    Feature('insulin', cat_dtype, DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in insulin medication'),\n    Feature('glyburide-metformin', cat_dtype,\n            DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glyburide-metformin medication'),\n    Feature('glipizide-metformin', cat_dtype,\n            DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glipizide-metformin medication'),\n    Feature('glimepiride-pioglitazone', cat_dtype,\n            DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in glimepiride-pioglitazone medication'),\n    Feature('metformin-rosiglitazone', cat_dtype,\n            DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in metformin-rosiglitazone medication'),\n    Feature('metformin-pioglitazone', cat_dtype,\n            DIABETES_MEDICATION_FEAT_DESCRIPTION,\n            name_extended='Change in metformin-pioglitazone medication'),\n    Feature('change', cat_dtype, \"Indicates if there was a change in diabetic \"\n                                 \"medications (either dosage or generic \"\n                                 \"name). Values: 'change' and 'no change'\",\n            name_extended=\"Change in any medication\"),\n    Feature('diabetesMed', cat_dtype, \"Indicates if there was any diabetic \"\n                                      \"medication prescribed. Values: 'yes' \"\n                                      \"and 'no'\",\n            name_extended=\"Diabetes medication prescribed\"),\n    # Converted to binary (readmit vs. no readmit).\n    Feature('readmitted', float, \"30 days, '>30' if the patient was \"\n                                 \"readmitted in more than 30 days, and 'No' \"\n                                 \"for no record of readmission.\",\n            is_target=True),\n], documentation=\"http://www.hindawi.com/journals/bmri/2014/781670/\")\n\n\ndef preprocess_diabetes_readmission(df: pd.DataFrame):\n    # Drop 2273 obs with missing race (2.2336% of total data)\n    df.dropna(subset=[\"race\"], inplace=True)\n\n    tgt_col = DIABETES_READMISSION_FEATURES.target\n    df[tgt_col] = (df[tgt_col] != \"NO\").astype(float)\n\n    # Some columns contain a small fraction of missing values (~1%); fill them.\n    df.fillna(\"MISSING\")\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/heloc.py", "content": "\"\"\"\nUtilities for the HELOC dataset.\n\nThis is a public credentialized access data source and\nrequires obtaining access through a data use agreement.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\n\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nEXTERNAL_RISK_THRESHOLD = 63\n\nHELOC_FEATURES = FeatureList(features=[\n    Feature('RiskPerformance', int, \"Paid as negotiated flag (12-36 Months). \"\n                                    \"String of Good and Bad\", is_target=True),\n    Feature('ExternalRiskEstimateLow', int,\n            \"Consolidated version of risk markers\",\n            name_extended=f\"Indicator for whether external risk <= {EXTERNAL_RISK_THRESHOLD}\"),\n    Feature('MSinceOldestTradeOpen', int,\n            name_extended='Months Since Oldest Trade Open'),\n    Feature('MSinceMostRecentTradeOpen', int,\n            name_extended='Months Since Most Recent Trade Open'),\n    Feature('AverageMInFile', int,\n            name_extended='Average Months in File'),\n    Feature('NumSatisfactoryTrades', int,\n            name_extended='Number of Satisfactory Trades'),\n    Feature('NumTrades60Ever2DerogPubRec', int,\n            name_extended='Number of Trades 60+ Ever'),\n    Feature('NumTrades90Ever2DerogPubRec', int,\n            name_extended='Number of Trades 90+ Ever'),\n    Feature('PercentTradesNeverDelq', int,\n            name_extended='Percent of Trades Never Delinquent'),\n    Feature('MSinceMostRecentDelq', int,\n            name_extended='Months Since Most Recent Delinquency'),\n    Feature('MaxDelq2PublicRecLast12M', cat_dtype,\n            name_extended=\"Max Delinquent/Public Records Last 12 Months\",\n            value_mapping={\n                -9: \"unknown\",\n                0: \"derogatory comment\",\n                1: \"120+ days delinquent\",\n                2: \"90 days delinquent\",\n                3: \"60 days delinquent\",\n                4: \"30 days delinquent\",\n                5: \"unknown delinquency\",\n                6: \"unknown delinquency\",\n                7: \"current and never delinquent\",\n                9: \"all other\",\n            }),\n    Feature('MaxDelqEver', cat_dtype,\n            \"Max delinquency ever\",\n            value_mapping={\n                -9: \"all other\",\n                9: \"all other\",\n                1: \"No such value\",\n                2: \"derogatory comment\",\n                3: \"120+ days delinquent\",\n                4: \"90 days delinquent\",\n                5: \"60 days delinquent\",\n                6: \"30 days delinquent\",\n                7: \"unknown delinquency\",\n                8: \"current and never delinquent\"}),\n    Feature('NumTotalTrades', int,\n            name_extended='Number of Total Trades (total number of credit accounts)'),\n    Feature('NumTradesOpeninLast12M', int,\n            name_extended='Number of Trades Open in Last 12 Months'),\n    Feature('PercentInstallTrades', int,\n            name_extended='Percent Installment Trades'),\n    Feature('MSinceMostRecentInqexcl7days', int,\n            name_extended='Months Since Most Recent Inq excl 7 days'),\n    Feature('NumInqLast6M', int,\n            name_extended='Number of inquiries last 6 Months'),\n    Feature('NumInqLast6Mexcl7days', int, \"\"\"Number of Inq Last 6 Months excl \n    7days. Excluding the last 7 days removes inquiries that are likely due to \n    price comparision shopping.\"\"\",\n            name_extended=\"Number of inquiries in last 6 months excluding last 7 days\"),\n    Feature('NetFractionRevolvingBurden', int,\n            name_extended=\"Net Fraction Revolving Burden \"\n                          \"(revolving balance divided by credit limit)\"),\n    Feature('NetFractionInstallBurden', int,\n            name_extended=\"Net Fraction Installment Burden \"\n                          \"(installment balance divided by original loan amount)\"),\n    Feature('NumRevolvingTradesWBalance', int,\n            name_extended='Number of revolving trades with balance'),\n    Feature('NumInstallTradesWBalance', int,\n           name_extended= 'Number of installment trades with balance'),\n    Feature('NumBank2NatlTradesWHighUtilization', int,\n            name_extended='Number of bank/national trades with high utilization ratio'),\n    Feature('PercentTradesWBalance', int,\n            name_extended='Percent of trades with balance'),\n], documentation=\"\"\"Data dictionary .xslx file can be accessed after filling \nout the data agreement at https://community.fico.com/s/explainable-machine\n-learning-challenge \"\"\")\n\n\ndef preprocess_heloc(df: pd.DataFrame) -> pd.DataFrame:\n    # Transform target to integer\n    target = HELOC_FEATURES.target\n    df[target] = (df[target] == \"Good\").astype(int)\n\n    df['ExternalRiskEstimateLow'] = (\n            df['ExternalRiskEstimate'] <= EXTERNAL_RISK_THRESHOLD)\n    df.drop(columns=['ExternalRiskEstimate'], inplace=True)\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/metamimic.py", "content": "\"\"\"\nUtilities for MetaMIMIC datasets.\n\nMetaMIMIC requires access to the MIMIC dataset, which is a public\ncredentialized dataset. Obtain access from the Physionet website.\nSee the instructions at the links below.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nfrom tableshift.core.features import Feature, FeatureList\n\nMETAMIMIC_FEATURES = FeatureList(features=[\n    Feature('age', float),\n    # Feature('subject_id', int),\n    Feature('max_220045', float,\n            description='Average Heart Rate (ICU itemid 220045)',\n            name_extended='Average ICU Heart Rate'),\n    Feature('max_220179', float,\n            description='Average Non Invasive Blood Pressure systolic (ICU itemid 220179)',\n            name_extended='Average ICU Non Invasive Blood Pressure systolic'),\n    Feature('max_220180', float,\n            description='Average Non Invasive Blood Pressure diastolic (ICU itemid 220180)',\n            name_extended='Average ICU Non Invasive Blood Pressure diastolic'),\n    Feature('max_220210', float,\n            description='Average Respiratory Rate (ICU itemid 220210)',\n            name_extended='Average ICU Respiratory Rate'),\n    Feature('max_220228', float,\n            description='Average Hemoglobin (ICU itemid 220228)',\n            name_extended='Average ICU Hemoglobin'),\n    Feature('max_220277', float,\n            description='Average O2 saturation pulseoxymetry (ICU itemid 220277)',\n            name_extended='Average ICU O2 saturation pulseoxymetry'),\n    Feature('max_220546', float,\n            description='Average WBC (ICU itemid 220546)',\n            name_extended='Average ICU WBC'),\n    Feature('max_220739', float,\n            description='Average GCS - Eye Opening (ICU itemid 220739)',\n            name_extended='Average ICU GCS - Eye Opening'),\n    Feature('max_223761', float,\n            description='Average Temperature Fahrenheit (ICU itemid 223761)',\n            name_extended='Average ICU Temperature Fahrenheit'),\n    Feature('max_223791', float,\n            description='Average Pain Level (ICU itemid 223791)',\n            name_extended='Average ICU Pain Level'),\n    Feature('max_223834', float,\n            description='Average O2 Flow (ICU itemid 223834)',\n            name_extended='Average ICU O2 Flow'),\n    Feature('max_223900', float,\n            description='Average GCS - Verbal Response (ICU itemid 223900)',\n            name_extended='Average ICU GCS - Verbal Response'),\n    Feature('max_223901', float,\n            description='Average GCS - Motor Response (ICU itemid 223901)',\n            name_extended='Average ICU GCS - Motor Response'),\n    Feature('max_224054', float,\n            description='Average Braden Sensory Perception (ICU itemid 224054)',\n            name_extended='Average ICU Braden Sensory Perception'),\n    Feature('max_224055', float,\n            description='Average Braden Moisture (ICU itemid 224055)',\n            name_extended='Average ICU Braden Moisture'),\n    Feature('max_224056', float,\n            description='Average Braden Activity (ICU itemid 224056)',\n            name_extended='Average ICU Braden Activity'),\n    Feature('max_224057', float,\n            description='Average Braden Mobility (ICU itemid 224057)',\n            name_extended='Average ICU Braden Mobility'),\n    Feature('max_224058', float,\n            description='Average Braden Nutrition (ICU itemid 224058)',\n            name_extended='Average ICU Braden Nutrition'),\n    Feature('max_224059', float,\n            description='Average Braden Friction/Shear (ICU itemid 224059)',\n            name_extended='Average ICU Braden Friction/Shear'),\n    Feature('max_225624', float,\n            description='Average BUN (ICU itemid 225624)',\n            name_extended='Average ICU BUN'),\n    Feature('max_226253', float,\n            description='Average SpO2 Desat Limit (ICU itemid 226253)',\n            name_extended='Average ICU SpO2 Desat Limit'),\n    Feature('max_227073', float,\n            description='Average Anion gap (ICU itemid 227073)',\n            name_extended='Average ICU Anion gap'),\n    Feature('max_227457', float,\n            description='Average Platelet Count (ICU itemid 227457)',\n            name_extended='Average ICU Platelet Count'),\n    Feature('max_227465', float,\n            description='Average Prothrombin time (ICU itemid 227465)',\n            name_extended='Average ICU Prothrombin time'),\n    Feature('max_227466', float,\n            description='Average PTT (ICU itemid 227466)',\n            name_extended='Average ICU PTT'),\n    Feature('max_50802', float,\n            description='Average Base Excess (hospital itemid 50802)',\n            name_extended='Average hospital Base Excess'),\n    Feature('max_50804', float,\n            description='Average Calculated Total CO2 (hospital itemid 50804)',\n            name_extended='Average hospital Calculated Total CO2'),\n    Feature('max_50813', float,\n            description='Average Lactate (hospital itemid 50813)',\n            name_extended='Average hospital Lactate'),\n    Feature('max_50818', float,\n            description='Average pCO2 (hospital itemid 50818)',\n            name_extended='Average hospital pCO2'),\n    Feature('max_50820', float,\n            description='Average pH (hospital itemid 50820)',\n            name_extended='Average hospital pH'),\n    Feature('max_50821', float,\n            description='Average pO2 (hospital itemid 50821)',\n            name_extended='Average hospital pO2'),\n    Feature('max_50861', float,\n            description='Average Alanine Aminotransferase (ALT) (hospital itemid 50861)',\n            name_extended='Average hospital Alanine Aminotransferase (ALT)'),\n    Feature('max_50863', float,\n            description='Average Alkaline Phosphatase (hospital itemid 50863)',\n            name_extended='Average hospital Alkaline Phosphatase'),\n    Feature('max_50868', float,\n            description='Average Anion Gap (hospital itemid 50868)',\n            name_extended='Average hospital Anion Gap'),\n    Feature('max_50878', float,\n            description='Average Asparate Aminotransferase (AST) (hospital itemid 50878)',\n            name_extended='Average hospital Asparate Aminotransferase (AST)'),\n    Feature('max_50882', float,\n            description='Average Bicarbonate (hospital itemid 50882)',\n            name_extended='Average hospital Bicarbonate'),\n    Feature('max_50885', float,\n            description='Average Bilirubin, Total (hospital itemid 50885)',\n            name_extended='Average hospital Bilirubin, Total'),\n    Feature('max_50893', float,\n            description='Average Calcium, Total (hospital itemid 50893)',\n            name_extended='Average hospital Calcium, Total'),\n    Feature('max_50902', float,\n            description='Average Chloride (hospital itemid 50902)',\n            name_extended='Average hospital Chloride'),\n    Feature('max_50912', float,\n            description='Average Creatinine (hospital itemid 50912)',\n            name_extended='Average hospital Creatinine'),\n    Feature('max_50931', float,\n            description='Average Glucose (hospital itemid 50931)',\n            name_extended='Average hospital Glucose'),\n    Feature('max_50960', float,\n            description='Average Magnesium (hospital itemid 50960)',\n            name_extended='Average hospital Magnesium'),\n    Feature('max_50970', float,\n            description='Average Phosphate (hospital itemid 50970)',\n            name_extended='Average hospital Phosphate'),\n    Feature('max_50971', float,\n            description='Average Potassium (hospital itemid 50971)',\n            name_extended='Average hospital Potassium'),\n    Feature('max_50983', float,\n            description='Average Sodium (hospital itemid 50983)',\n            name_extended='Average hospital Sodium'),\n    Feature('max_51006', float,\n            description='Average Urea Nitrogen (hospital itemid 51006)',\n            name_extended='Average hospital Urea Nitrogen'),\n    Feature('max_51221', float,\n            description='Average Hematocrit (hospital itemid 51221)',\n            name_extended='Average hospital Hematocrit'),\n    Feature('max_51222', float,\n            description='Average Hemoglobin (hospital itemid 51222)',\n            name_extended='Average hospital Hemoglobin'),\n    Feature('max_51248', float,\n            description='Average MCH (hospital itemid 51248)',\n            name_extended='Average hospital MCH'),\n    Feature('max_51249', float,\n            description='Average MCHC (hospital itemid 51249)',\n            name_extended='Average hospital MCHC'),\n    Feature('max_51250', float,\n            description='Average MCV (hospital itemid 51250)',\n            name_extended='Average hospital MCV'),\n    Feature('max_51265', float,\n            description='Average Platelet Count (hospital itemid 51265)',\n            name_extended='Average hospital Platelet Count'),\n    Feature('max_51277', float,\n            description='Average RDW (hospital itemid 51277)',\n            name_extended='Average hospital RDW'),\n    Feature('max_51279', float,\n            description='Average Red Blood Cells (hospital itemid 51279)',\n            name_extended='Average hospital Red Blood Cells'),\n    Feature('max_51301', float,\n            description='Average White Blood Cells (hospital itemid 51301)',\n            name_extended='Average hospital White Blood Cells'),\n    Feature('max_51491', float,\n            description='Average pH (hospital itemid 51491)',\n            name_extended='Average hospital pH'),\n    Feature('first_226512', float,\n            description='First ICU admission weight (in Kg)',\n            name_extended='First ICU admission weight (in Kg)'),\n    Feature('first_226730', float,\n            description='First ICU admission height (in cm)',\n            name_extended='First ICU admission height (in cm)'),\n    Feature('gender', float),\n    Feature('max_220045', float,\n            description='Max Heart Rate (ICU itemid 220045)',\n            name_extended='Max ICU Heart Rate'),\n    Feature('max_220179', float,\n            description='Max Non Invasive Blood Pressure systolic (ICU itemid 220179)',\n            name_extended='Max ICU Non Invasive Blood Pressure systolic'),\n    Feature('max_220180', float,\n            description='Max Non Invasive Blood Pressure diastolic (ICU itemid 220180)',\n            name_extended='Max ICU Non Invasive Blood Pressure diastolic'),\n    Feature('max_220210', float,\n            description='Max Respiratory Rate (ICU itemid 220210)',\n            name_extended='Max ICU Respiratory Rate'),\n    Feature('max_220228', float,\n            description='Max Hemoglobin (ICU itemid 220228)',\n            name_extended='Max ICU Hemoglobin'),\n    Feature('max_220277', float,\n            description='Max O2 saturation pulseoxymetry (ICU itemid 220277)',\n            name_extended='Max ICU O2 saturation pulseoxymetry'),\n    Feature('max_220546', float,\n            description='Max WBC (ICU itemid 220546)',\n            name_extended='Max ICU WBC'),\n    Feature('max_220739', float,\n            description='Max GCS - Eye Opening (ICU itemid 220739)',\n            name_extended='Max ICU GCS - Eye Opening'),\n    Feature('max_223761', float,\n            description='Max Temperature Fahrenheit (ICU itemid 223761)',\n            name_extended='Max ICU Temperature Fahrenheit'),\n    Feature('max_223791', float,\n            description='Max Pain Level (ICU itemid 223791)',\n            name_extended='Max ICU Pain Level'),\n    Feature('max_223834', float,\n            description='Max O2 Flow (ICU itemid 223834)',\n            name_extended='Max ICU O2 Flow'),\n    Feature('max_223900', float,\n            description='Max GCS - Verbal Response (ICU itemid 223900)',\n            name_extended='Max ICU GCS - Verbal Response'),\n    Feature('max_223901', float,\n            description='Max GCS - Motor Response (ICU itemid 223901)',\n            name_extended='Max ICU GCS - Motor Response'),\n    Feature('max_224054', float,\n            description='Max Braden Sensory Perception (ICU itemid 224054)',\n            name_extended='Max ICU Braden Sensory Perception'),\n    Feature('max_224055', float,\n            description='Max Braden Moisture (ICU itemid 224055)',\n            name_extended='Max ICU Braden Moisture'),\n    Feature('max_224056', float,\n            description='Max Braden Activity (ICU itemid 224056)',\n            name_extended='Max ICU Braden Activity'),\n    Feature('max_224057', float,\n            description='Max Braden Mobility (ICU itemid 224057)',\n            name_extended='Max ICU Braden Mobility'),\n    Feature('max_224058', float,\n            description='Max Braden Nutrition (ICU itemid 224058)',\n            name_extended='Max ICU Braden Nutrition'),\n    Feature('max_224059', float,\n            description='Max Braden Friction/Shear (ICU itemid 224059)',\n            name_extended='Max ICU Braden Friction/Shear'),\n    Feature('max_225624', float,\n            description='Max BUN (ICU itemid 225624)',\n            name_extended='Max ICU BUN'),\n    Feature('max_226253', float,\n            description='Max SpO2 Desat Limit (ICU itemid 226253)',\n            name_extended='Max ICU SpO2 Desat Limit'),\n    Feature('max_227073', float,\n            description='Max Anion gap (ICU itemid 227073)',\n            name_extended='Max ICU Anion gap'),\n    Feature('max_227457', float,\n            description='Max Platelet Count (ICU itemid 227457)',\n            name_extended='Max ICU Platelet Count'),\n    Feature('max_227465', float,\n            description='Max Prothrombin time (ICU itemid 227465)',\n            name_extended='Max ICU Prothrombin time'),\n    Feature('max_227466', float,\n            description='Max PTT (ICU itemid 227466)',\n            name_extended='Max ICU PTT'),\n    Feature('max_50802', float,\n            description='Max Base Excess (hospital itemid 50802)',\n            name_extended='Max hospital Base Excess'),\n    Feature('max_50804', float,\n            description='Max Calculated Total CO2 (hospital itemid 50804)',\n            name_extended='Max hospital Calculated Total CO2'),\n    Feature('max_50813', float,\n            description='Max Lactate (hospital itemid 50813)',\n            name_extended='Max hospital Lactate'),\n    Feature('max_50818', float,\n            description='Max pCO2 (hospital itemid 50818)',\n            name_extended='Max hospital pCO2'),\n    Feature('max_50820', float,\n            description='Max pH (hospital itemid 50820)',\n            name_extended='Max hospital pH'),\n    Feature('max_50821', float,\n            description='Max pO2 (hospital itemid 50821)',\n            name_extended='Max hospital pO2'),\n    Feature('max_50861', float,\n            description='Max Alanine Aminotransferase (ALT) (hospital itemid 50861)',\n            name_extended='Max hospital Alanine Aminotransferase (ALT)'),\n    Feature('max_50863', float,\n            description='Max Alkaline Phosphatase (hospital itemid 50863)',\n            name_extended='Max hospital Alkaline Phosphatase'),\n    Feature('max_50868', float,\n            description='Max Anion Gap (hospital itemid 50868)',\n            name_extended='Max hospital Anion Gap'),\n    Feature('max_50878', float,\n            description='Max Asparate Aminotransferase (AST) (hospital itemid 50878)',\n            name_extended='Max hospital Asparate Aminotransferase (AST)'),\n    Feature('max_50882', float,\n            description='Max Bicarbonate (hospital itemid 50882)',\n            name_extended='Max hospital Bicarbonate'),\n    Feature('max_50885', float,\n            description='Max Bilirubin, Total (hospital itemid 50885)',\n            name_extended='Max hospital Bilirubin, Total'),\n    Feature('max_50893', float,\n            description='Max Calcium, Total (hospital itemid 50893)',\n            name_extended='Max hospital Calcium, Total'),\n    Feature('max_50902', float,\n            description='Max Chloride (hospital itemid 50902)',\n            name_extended='Max hospital Chloride'),\n    Feature('max_50912', float,\n            description='Max Creatinine (hospital itemid 50912)',\n            name_extended='Max hospital Creatinine'),\n    Feature('max_50931', float,\n            description='Max Glucose (hospital itemid 50931)',\n            name_extended='Max hospital Glucose'),\n    Feature('max_50960', float,\n            description='Max Magnesium (hospital itemid 50960)',\n            name_extended='Max hospital Magnesium'),\n    Feature('max_50970', float,\n            description='Max Phosphate (hospital itemid 50970)',\n            name_extended='Max hospital Phosphate'),\n    Feature('max_50971', float,\n            description='Max Potassium (hospital itemid 50971)',\n            name_extended='Max hospital Potassium'),\n    Feature('max_50983', float,\n            description='Max Sodium (hospital itemid 50983)',\n            name_extended='Max hospital Sodium'),\n    Feature('max_51006', float,\n            description='Max Urea Nitrogen (hospital itemid 51006)',\n            name_extended='Max hospital Urea Nitrogen'),\n    Feature('max_51221', float,\n            description='Max Hematocrit (hospital itemid 51221)',\n            name_extended='Max hospital Hematocrit'),\n    Feature('max_51222', float,\n            description='Max Hemoglobin (hospital itemid 51222)',\n            name_extended='Max hospital Hemoglobin'),\n    Feature('max_51248', float,\n            description='Max MCH (hospital itemid 51248)',\n            name_extended='Max hospital MCH'),\n    Feature('max_51249', float,\n            description='Max MCHC (hospital itemid 51249)',\n            name_extended='Max hospital MCHC'),\n    Feature('max_51250', float,\n            description='Max MCV (hospital itemid 51250)',\n            name_extended='Max hospital MCV'),\n    Feature('max_51265', float,\n            description='Max Platelet Count (hospital itemid 51265)',\n            name_extended='Max hospital Platelet Count'),\n    Feature('max_51277', float,\n            description='Max RDW (hospital itemid 51277)',\n            name_extended='Max hospital RDW'),\n    Feature('max_51279', float,\n            description='Max Red Blood Cells (hospital itemid 51279)',\n            name_extended='Max hospital Red Blood Cells'),\n    Feature('max_51301', float,\n            description='Max White Blood Cells (hospital itemid 51301)',\n            name_extended='Max hospital White Blood Cells'),\n    Feature('max_51491', float,\n            description='Max pH (hospital itemid 51491)',\n            name_extended='Max hospital pH'),\n    Feature('max_220045', float,\n            description='Minimum Heart Rate (ICU itemid 220045)',\n            name_extended='Minimum ICU Heart Rate'),\n    Feature('max_220179', float,\n            description='Minimum Non Invasive Blood Pressure systolic (ICU itemid 220179)',\n            name_extended='Minimum ICU Non Invasive Blood Pressure systolic'),\n    Feature('max_220180', float,\n            description='Minimum Non Invasive Blood Pressure diastolic (ICU itemid 220180)',\n            name_extended='Minimum ICU Non Invasive Blood Pressure diastolic'),\n    Feature('max_220210', float,\n            description='Minimum Respiratory Rate (ICU itemid 220210)',\n            name_extended='Minimum ICU Respiratory Rate'),\n    Feature('max_220228', float,\n            description='Minimum Hemoglobin (ICU itemid 220228)',\n            name_extended='Minimum ICU Hemoglobin'),\n    Feature('max_220277', float,\n            description='Minimum O2 saturation pulseoxymetry (ICU itemid 220277)',\n            name_extended='Minimum ICU O2 saturation pulseoxymetry'),\n    Feature('max_220546', float,\n            description='Minimum WBC (ICU itemid 220546)',\n            name_extended='Minimum ICU WBC'),\n    Feature('max_220739', float,\n            description='Minimum GCS - Eye Opening (ICU itemid 220739)',\n            name_extended='Minimum ICU GCS - Eye Opening'),\n    Feature('max_223761', float,\n            description='Minimum Temperature Fahrenheit (ICU itemid 223761)',\n            name_extended='Minimum ICU Temperature Fahrenheit'),\n    Feature('max_223791', float,\n            description='Minimum Pain Level (ICU itemid 223791)',\n            name_extended='Minimum ICU Pain Level'),\n    Feature('max_223834', float,\n            description='Minimum O2 Flow (ICU itemid 223834)',\n            name_extended='Minimum ICU O2 Flow'),\n    Feature('max_223900', float,\n            description='Minimum GCS - Verbal Response (ICU itemid 223900)',\n            name_extended='Minimum ICU GCS - Verbal Response'),\n    Feature('max_223901', float,\n            description='Minimum GCS - Motor Response (ICU itemid 223901)',\n            name_extended='Minimum ICU GCS - Motor Response'),\n    Feature('max_224054', float,\n            description='Minimum Braden Sensory Perception (ICU itemid 224054)',\n            name_extended='Minimum ICU Braden Sensory Perception'),\n    Feature('max_224055', float,\n            description='Minimum Braden Moisture (ICU itemid 224055)',\n            name_extended='Minimum ICU Braden Moisture'),\n    Feature('max_224056', float,\n            description='Minimum Braden Activity (ICU itemid 224056)',\n            name_extended='Minimum ICU Braden Activity'),\n    Feature('max_224057', float,\n            description='Minimum Braden Mobility (ICU itemid 224057)',\n            name_extended='Minimum ICU Braden Mobility'),\n    Feature('max_224058', float,\n            description='Minimum Braden Nutrition (ICU itemid 224058)',\n            name_extended='Minimum ICU Braden Nutrition'),\n    Feature('max_224059', float,\n            description='Minimum Braden Friction/Shear (ICU itemid 224059)',\n            name_extended='Minimum ICU Braden Friction/Shear'),\n    Feature('max_225624', float,\n            description='Minimum BUN (ICU itemid 225624)',\n            name_extended='Minimum ICU BUN'),\n    Feature('max_226253', float,\n            description='Minimum SpO2 Desat Limit (ICU itemid 226253)',\n            name_extended='Minimum ICU SpO2 Desat Limit'),\n    Feature('max_227073', float,\n            description='Minimum Anion gap (ICU itemid 227073)',\n            name_extended='Minimum ICU Anion gap'),\n    Feature('max_227457', float,\n            description='Minimum Platelet Count (ICU itemid 227457)',\n            name_extended='Minimum ICU Platelet Count'),\n    Feature('max_227465', float,\n            description='Minimum Prothrombin time (ICU itemid 227465)',\n            name_extended='Minimum ICU Prothrombin time'),\n    Feature('max_227466', float,\n            description='Minimum PTT (ICU itemid 227466)',\n            name_extended='Minimum ICU PTT'),\n    Feature('max_50802', float,\n            description='Minimum Base Excess (hospital itemid 50802)',\n            name_extended='Minimum hospital Base Excess'),\n    Feature('max_50804', float,\n            description='Minimum Calculated Total CO2 (hospital itemid 50804)',\n            name_extended='Minimum hospital Calculated Total CO2'),\n    Feature('max_50813', float,\n            description='Minimum Lactate (hospital itemid 50813)',\n            name_extended='Minimum hospital Lactate'),\n    Feature('max_50818', float,\n            description='Minimum pCO2 (hospital itemid 50818)',\n            name_extended='Minimum hospital pCO2'),\n    Feature('max_50820', float,\n            description='Minimum pH (hospital itemid 50820)',\n            name_extended='Minimum hospital pH'),\n    Feature('max_50821', float,\n            description='Minimum pO2 (hospital itemid 50821)',\n            name_extended='Minimum hospital pO2'),\n    Feature('max_50861', float,\n            description='Minimum Alanine Aminotransferase (ALT) (hospital itemid 50861)',\n            name_extended='Minimum hospital Alanine Aminotransferase (ALT)'),\n    Feature('max_50863', float,\n            description='Minimum Alkaline Phosphatase (hospital itemid 50863)',\n            name_extended='Minimum hospital Alkaline Phosphatase'),\n    Feature('max_50868', float,\n            description='Minimum Anion Gap (hospital itemid 50868)',\n            name_extended='Minimum hospital Anion Gap'),\n    Feature('max_50878', float,\n            description='Minimum Asparate Aminotransferase (AST) (hospital itemid 50878)',\n            name_extended='Minimum hospital Asparate Aminotransferase (AST)'),\n    Feature('max_50882', float,\n            description='Minimum Bicarbonate (hospital itemid 50882)',\n            name_extended='Minimum hospital Bicarbonate'),\n    Feature('max_50885', float,\n            description='Minimum Bilirubin, Total (hospital itemid 50885)',\n            name_extended='Minimum hospital Bilirubin, Total'),\n    Feature('max_50893', float,\n            description='Minimum Calcium, Total (hospital itemid 50893)',\n            name_extended='Minimum hospital Calcium, Total'),\n    Feature('max_50902', float,\n            description='Minimum Chloride (hospital itemid 50902)',\n            name_extended='Minimum hospital Chloride'),\n    Feature('max_50912', float,\n            description='Minimum Creatinine (hospital itemid 50912)',\n            name_extended='Minimum hospital Creatinine'),\n    Feature('max_50931', float,\n            description='Minimum Glucose (hospital itemid 50931)',\n            name_extended='Minimum hospital Glucose'),\n    Feature('max_50960', float,\n            description='Minimum Magnesium (hospital itemid 50960)',\n            name_extended='Minimum hospital Magnesium'),\n    Feature('max_50970', float,\n            description='Minimum Phosphate (hospital itemid 50970)',\n            name_extended='Minimum hospital Phosphate'),\n    Feature('max_50971', float,\n            description='Minimum Potassium (hospital itemid 50971)',\n            name_extended='Minimum hospital Potassium'),\n    Feature('max_50983', float,\n            description='Minimum Sodium (hospital itemid 50983)',\n            name_extended='Minimum hospital Sodium'),\n    Feature('max_51006', float,\n            description='Minimum Urea Nitrogen (hospital itemid 51006)',\n            name_extended='Minimum hospital Urea Nitrogen'),\n    Feature('max_51221', float,\n            description='Minimum Hematocrit (hospital itemid 51221)',\n            name_extended='Minimum hospital Hematocrit'),\n    Feature('max_51222', float,\n            description='Minimum Hemoglobin (hospital itemid 51222)',\n            name_extended='Minimum hospital Hemoglobin'),\n    Feature('max_51248', float,\n            description='Minimum MCH (hospital itemid 51248)',\n            name_extended='Minimum hospital MCH'),\n    Feature('max_51249', float,\n            description='Minimum MCHC (hospital itemid 51249)',\n            name_extended='Minimum hospital MCHC'),\n    Feature('max_51250', float,\n            description='Minimum MCV (hospital itemid 51250)',\n            name_extended='Minimum hospital MCV'),\n    Feature('max_51265', float,\n            description='Minimum Platelet Count (hospital itemid 51265)',\n            name_extended='Minimum hospital Platelet Count'),\n    Feature('max_51277', float,\n            description='Minimum RDW (hospital itemid 51277)',\n            name_extended='Minimum hospital RDW'),\n    Feature('max_51279', float,\n            description='Minimum Red Blood Cells (hospital itemid 51279)',\n            name_extended='Minimum hospital Red Blood Cells'),\n    Feature('max_51301', float,\n            description='Minimum White Blood Cells (hospital itemid 51301)',\n            name_extended='Minimum hospital White Blood Cells'),\n    Feature('max_51491', float,\n            description='Minimum pH (hospital itemid 51491)',\n            name_extended='Minimum hospital pH'),\n\n])\n\nMETAMIMIC_ALCOHOL_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('alcohol_diagnosed', float, is_target=True,\n            name_extended=\"diagnosis of alcohol dependence\")])\n\nMETAMIMIC_ANEMIA_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('anemia_diagnosed', float, is_target=True,\n            name_extended='diagnosis of anemia')])\n\nMETAMIMIC_ATRIAL_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('atrial_diagnosed', float, is_target=True,\n            name_extended='diagnosis of atrial fibrillation and flutter')])\n\nMETAMIMIC_DIABETES_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('diabetes_diagnosed', float, is_target=True,\n            name_extended='diagnosis of diabetes')])\n\nMETAMIMIC_HEART_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('heart_diagnosed', float, is_target=True,\n            name_extended='diagnosis of heart failure')])\n\nMETAMIMIC_HYPERTENSIVE_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('hypertensive_diagnosed', float, is_target=True,\n            name_extended='diagnosis of hypertensive diseases')])\n\nMETAMIMIC_HYPOTENSION_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('hypotension_diagnosed', float, is_target=True,\n            name_extended='diagnosis of hypotension')])\n\nMETAMIMIC_ISCHEMATIC_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('ischematic_diagnosed', float, is_target=True,\n            name_extended='diagnosis of ischematic heart disease')])\n\nMETAMIMIC_LIPOID_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('lipoid_diagnosed', float, is_target=True,\n            name_extended='diagnosis of disorders of lipoid metabolism')])\n\nMETAMIMIC_OVERWEIGHT_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('overweight_diagnosed', float, is_target=True,\n            name_extended='diagnosis of overweight, obesity and other '\n                          'hyperalimentation')])\n\nMETAMIMIC_PURPURA_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('purpura_diagnosed', float, is_target=True,\n            name_extended='diagnosis of purpura and other hemorrhagic '\n                          'conditions')])\n\nMETAMIMIC_RESPIRATORY_FEATURES = METAMIMIC_FEATURES + FeatureList([\n    Feature('respiratory_diagnosed', float, is_target=True,\n            name_extended='diagnosis of chronic lower respiratory diseases')])\n"}
{"type": "source_file", "path": "tableshift/datasets/grinsztajn.py", "content": "\"\"\"\nThe benchmark of Grinsztajn at al. 22.\n\nSee https://arxiv.org/abs/2207.08815\n\"\"\"\nfrom typing import Dict, Callable\n\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList\n\nELECTRICITY_FEATURES = FeatureList(features=[\n    Feature('price_increase', int, is_target=True,\n            description='Preprocessed version of original class label ('\n                        'UP/DOWN to integer values). The '\n                        'class label identifies the change of the price ( '\n                        'UP or DOWN) in New South Wales relative to a moving '\n                        'average of the last 24 hours (and removes the impact '\n                        'of longer term price trends).',\n            name_extended='Increase in the price of electricity relative to a moving '\n                          'average of the last 24 hours'),\n    Feature('date', float,\n            'date between 7 May 1996 to 5 December 1998. Here normalized between 0 and 1',\n            name_extended='Relative date between 7 May 1996 and 5 December 1998'),\n    Feature('day', int, name_extended='day of the week',\n            value_mapping={\n                0: 'Monday',\n                1: 'Tuesday',\n                2: 'Wednesday',\n                3: 'Thursday',\n                4: 'Friday',\n                5: 'Saturday',\n                6: 'Sunday'}),\n    Feature('period', float,\n            'Time of the measurement (1-48) in half hour intervals over 24 hours. Here normalized between 0 and 1',\n            name_extended='Time of the measurement relative to 24 hours (normalized between 0 and 1)'),\n    Feature('nswprice', float,\n            'New South Wales electricity price, normalized between 0 and 1',\n            name_extended='New South Wales electricity demand relative to min and max price'),\n    Feature('nswdemand', float,\n            'New South Wales electricity demand, normalized between 0 and 1',\n            name_extended='New South Wales electricity demand relative to min and max demand'),\n    Feature('vicprice', float,\n            'Victoria electricity price, normalized between 0 and 1',\n            name_extended='Victoria electricity demand relative to min and max price'),\n    Feature('vicdemand', float,\n            'Victoria electricity demand, normalized between 0 and 1',\n            name_extended='Victoria electricity demand relative to min and max demand'),\n    Feature('transfer', float,\n            name_extended='scheduled electricity transfer between both states, normalized between 0 and 1')\n\n],\n    documentation='https://www.openml.org/d/44156')\n\nBANK_MARKETING_FEATURES = FeatureList(features=[\n    Feature('default', int, \"default: has credit in default? (binary: yes,no)\",\n            is_target=True,\n            name_extended=\"Had credit in default\"),\n    Feature('V1', int, \"age\", name_extended=\"age of client in years\"),\n    Feature('V6', float, \"balance: average yearly balance, in euros (numeric)\",\n            name_extended=\"Average yearly balance in Euros\"),\n    Feature('V10', int, 'day: last contact day of the month (numeric)',\n            name_extended='number of last contact day of the month'),\n    Feature('V12', float,\n            'duration: last contact duration, in seconds (numeric)',\n            name_extended='duration of last contact in seconds'),\n    Feature('V13', int,\n            'campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)',\n            name_extended='number of days during campaign for which this client was contacted'),\n    Feature('V14', float,\n            'pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)',\n            name_extended='number of days since client was last contacted'),\n    Feature('V15', int,\n            'previous: number of contacts performed before this campaign and for this client (numeric)',\n            name_extended='number of contacts prior to this campaign for this client'),\n],\n    documentation=\"https://www.openml.org/d/44126\")\n\nCALIFORNIA_FEATURES = FeatureList(features=[\n    Feature('price_above_median', int, 'price above median',\n            is_target=True,\n            name_extended='home price in this census block group is above median'),\n    Feature('Longitude', float),\n    Feature('Latitude', float),\n    Feature('AveOccup', float,\n            'see https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html',\n            name_extended='average occupancy of houses in census block group'),\n    Feature('Population', float,\n            'see https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html',\n            name_extended='population of census block group'),\n    Feature('AveBedrms', float,\n            'see https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html',\n            name_extended='average number of bedrooms of homes in census block group'\n            ),\n    Feature('HouseAge', int,\n            'see https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html',\n            name_extended='age of house'),\n    Feature('MedInc', float,\n            'see https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html',\n            name_extended='median income in census block group'),\n], documentation=\"https://www.openml.org/d/44090\")\n\nCOVERTYPE_FEATURES = FeatureList(features=[\n    Feature('class', int, is_target=True,\n            name_extended=\"forest cover type designation is Lodgepole Pine\"),\n    Feature('Elevation', float, description='Elevation in meters',\n            name_extended='normalized elevation in meters'),\n    Feature('Aspect', float, 'Aspect in degrees azimuth',\n            name_extended='normalized aspect in degrees azimuth'),\n    Feature('Slope', int, 'Slope in degrees',\n            name_extended='normalized slope in degrees'),\n    Feature('Horizontal_Distance_To_Hydrology', float,\n            'Horz Dist to nearest surface water features',\n            name_extended='normalized horizontal distance to nearest surface water features'),\n    Feature('Vertical_Distance_To_Hydrology', float,\n            'Vert Dist to nearest surface water features',\n            name_extended='normalized vertical distance to nearest surface water features'),\n    Feature('Horizontal_Distance_To_Roadways', float,\n            'Horz Dist to nearest roadway',\n            name_extended='normalized distance to nearest roadway'),\n    Feature('Hillshade_9am', int, 'Hillshade index at 9am on summer solstice',\n            name_extended='normalized Hillshade index at 9am, summer solstice'),\n    Feature('Hillshade_Noon', int, 'Hillshade index at noon, summer solstice',\n            name_extended='normalized Hillshade index at noon on summer solstice'),\n    Feature('Hillshade_3pm', int, 'Hillshade index at 3pm, summer solstice',\n            name_extended='normalized Hillshade index at 3pm on summer solstice'),\n    Feature('Horizontal_Distance_To_Fire_Points', float,\n            'Horz Dist to nearest wildfire ignition points',\n            name_extended='normalized horizontal distance to nearest wildfire ignition points'),\n    Feature('Wilderness_Area1', int,\n            name_extended='indicator for wilderness area type 1 designation'),\n    Feature('Wilderness_Area2', int,\n            name_extended='indicator for wilderness area type 2 designation'),\n    Feature('Wilderness_Area3', int,\n            name_extended='indicator for wilderness area type 3 designation'),\n    Feature('Wilderness_Area4', int,\n            name_extended='indicator for wilderness area type 4 designation'),\n    Feature('Soil_Type1', int,\n            name_extended='indicator for soil type 1 designation'),\n    Feature('Soil_Type2', int,\n            name_extended='indicator for soil type 2 designation'),\n    Feature('Soil_Type3', int,\n            name_extended='indicator for soil type 3 designation'),\n    Feature('Soil_Type4', int,\n            name_extended='indicator for soil type 4 designation'),\n    Feature('Soil_Type5', int,\n            name_extended='indicator for soil type 5 designation'),\n    Feature('Soil_Type6', int,\n            name_extended='indicator for soil type 6 designation'),\n    Feature('Soil_Type7', int,\n            name_extended='indicator for soil type 7 designation'),\n    Feature('Soil_Type8', int,\n            name_extended='indicator for soil type 8 designation'),\n    Feature('Soil_Type9', int,\n            name_extended='indicator for soil type 9 designation'),\n    Feature('Soil_Type10', int,\n            name_extended='indicator for soil type 10 designation'),\n    Feature('Soil_Type11', int,\n            name_extended='indicator for soil type 11 designation'),\n    Feature('Soil_Type12', int,\n            name_extended='indicator for soil type 12 designation'),\n    Feature('Soil_Type13', int,\n            name_extended='indicator for soil type 13 designation'),\n    Feature('Soil_Type14', int,\n            name_extended='indicator for soil type 14 designation'),\n    Feature('Soil_Type15', int,\n            name_extended='indicator for soil type 15 designation'),\n    Feature('Soil_Type16', int,\n            name_extended='indicator for soil type 16 designation'),\n    Feature('Soil_Type17', int,\n            name_extended='indicator for soil type 17 designation'),\n    Feature('Soil_Type18', int,\n            name_extended='indicator for soil type 18 designation'),\n    Feature('Soil_Type19', int,\n            name_extended='indicator for soil type 19 designation'),\n    Feature('Soil_Type20', int,\n            name_extended='indicator for soil type 20 designation'),\n    Feature('Soil_Type21', int,\n            name_extended='indicator for soil type 21 designation'),\n    Feature('Soil_Type22', int,\n            name_extended='indicator for soil type 22 designation'),\n    Feature('Soil_Type23', int,\n            name_extended='indicator for soil type 23 designation'),\n    Feature('Soil_Type24', int,\n            name_extended='indicator for soil type 24 designation'),\n    Feature('Soil_Type25', int,\n            name_extended='indicator for soil type 25 designation'),\n    Feature('Soil_Type26', int,\n            name_extended='indicator for soil type 26 designation'),\n    Feature('Soil_Type27', int,\n            name_extended='indicator for soil type 27 designation'),\n    Feature('Soil_Type28', int,\n            name_extended='indicator for soil type 28 designation'),\n    Feature('Soil_Type29', int,\n            name_extended='indicator for soil type 29 designation'),\n    Feature('Soil_Type30', int,\n            name_extended='indicator for soil type 30 designation'),\n    Feature('Soil_Type31', int,\n            name_extended='indicator for soil type 31 designation'),\n    Feature('Soil_Type32', int,\n            name_extended='indicator for soil type 32 designation'),\n    Feature('Soil_Type33', int,\n            name_extended='indicator for soil type 33 designation'),\n    Feature('Soil_Type34', int,\n            name_extended='indicator for soil type 34 designation'),\n    Feature('Soil_Type35', int,\n            name_extended='indicator for soil type 35 designation'),\n    Feature('Soil_Type36', int,\n            name_extended='indicator for soil type 36 designation'),\n    Feature('Soil_Type37', int,\n            name_extended='indicator for soil type 37 designation'),\n    Feature('Soil_Type38', int,\n            name_extended='indicator for soil type 38 designation'),\n    Feature('Soil_Type39', int,\n            name_extended='indicator for soil type 39 designation'),\n    Feature('Soil_Type40', int,\n            name_extended='indicator for soil type 40 designation'),\n], documentation=\"\"\"https://www.openml.org/d/44159, https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#covtype.binary.\n                The original binary split is due to Collobert, Bengio and Bengio,\n                'Parallel Mixture of SVMs for Very Large Scale Problems' and\n                the class IDs are defined in the UCI dataset file covtype.info\n                at https://archive.ics.uci.edu/ml/datasets/covertype .\"\"\")\n\nCREDIT_FEATURES = FeatureList(features=[\n    Feature('SeriousDlqin2yrs', int, is_target=True,\n            name_extended='person will experience serious deliquency within the next 2 years'),\n    Feature('RevolvingUtilizationOfUnsecuredLines', float,\n            name_extended='revolving utilization of unsecured credit lines'),\n    Feature('age', int),\n    Feature('NumberOfTime30-59DaysPastDueNotWorse', int,\n            name_extended='number of times between 30 and 59 days past due'),\n    Feature('NumberOfTime60-89DaysPastDueNotWorse', int,\n            name_extended='number of times between 60 and 89 days past due'),\n    Feature('NumberOfTimes90DaysLate', int,\n            name_extended='number of times 90 days late'),\n    Feature('DebtRatio', float, name_extended='debt ratio'),\n    Feature('MonthlyIncome', float, name_extended='monthly income'),\n    Feature('NumberOfOpenCreditLinesAndLoans', int,\n            name_extended='number of open credit lines and loans'),\n    Feature('NumberRealEstateLoansOrLines', int,\n            name_extended='number of real estate loans or lines'),\n    Feature('NumberOfDependents', int, name_extended='number of dependents'),\n])\n\nPAYMENT_STATUS_MAPPING = {\n    -2.0: 'unknown or paid early',\n    -1.0: 'paid duly',\n    0.0: 'payment delay for less than 1 month',\n    1.0: 'payment delay for 1 months',\n    2.0: 'payment delay for 2 months',\n    3.0: 'payment delay for 3 months',\n    4.0: 'payment delay for 4 months',\n    5.0: 'payment delay for 5 months',\n    6.0: 'payment delay for 6 months',\n    7.0: 'payment delay for 7 months',\n    8.0: 'payment delay for 8 months',\n    9.0: 'payment delay for 9 months and above',\n}\n\nDEFAULT_OF_CREDIT_CLIENTS_FEATURES = FeatureList(features=[\n    Feature('x1', float,\n            'Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.',\n            name_extended='total credit amount in NT dollars'),\n    Feature('x2', int, 'gender', name_extended='gender',\n            value_mapping={0: 'male', 1: 'female'}),\n    Feature('x5', int, 'age', name_extended='age in years'),\n    Feature('x6', float, name_extended='repayment status in September 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x7', float, name_extended='repayment status in August 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x8', float, name_extended='repayment status in July 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x9', float, name_extended='repayment status in June 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x10', float, name_extended='repayment status in May 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x11', float, name_extended='repayment status in April 2005',\n            value_mapping=PAYMENT_STATUS_MAPPING),\n    Feature('x12', float,\n            name_extended='amount of bill statement in September 2005 in NT dollars'),\n    Feature('x13', float,\n            name_extended='amount of bill statement in August 2005 in NT dollars'),\n    Feature('x14', float,\n            name_extended='amount of bill statement in July 2005 in NT dollars'),\n    Feature('x15', float,\n            name_extended='amount of bill statement in June 2005 in NT dollars'),\n    Feature('x16', float,\n            name_extended='amount of bill statement in May 2005 in NT dollars'),\n    Feature('x17', float,\n            name_extended='amount of bill statement in April 2005 in NT dollars'),\n    Feature('x18', float,\n            name_extended='amount paid in September 2005 in NT dollars'),\n    Feature('x19', float,\n            name_extended='amount paid in August 2005 in NT dollars'),\n    Feature('x20', float,\n            name_extended='amount paid in July 2005 in NT dollars'),\n    Feature('x21', float,\n            name_extended='amount paid in June 2005 in NT dollars'),\n    Feature('x22', float,\n            name_extended='amount paid in May 2005 in NT dollars'),\n    Feature('x23', float,\n            name_extended='amount paid in April 2005 in NT dollars'),\n    Feature('y', int, is_target=True,\n            name_extended='client defaults on payment'),\n],\n    documentation=\"https://www.openml.org/search?type=data&status=active&id=45016\")\n\nEYE_MOVEMENTS_FEATURES = FeatureList(features=[\n    Feature('lineNo', float, name_extended='line number'),\n    Feature('assgNo', float, name_extended='assignment number'),\n    Feature('P1stFixation', int,\n            \" '1' if fixation occured when the sentence the word was in was encountered the first time\",\n            name_extended=\"indicator for whether fixation occurred when the sentence containing the word was encountered the first time\"),\n    Feature('P2stFixation', int,\n            \" '1' if fixation occured when the sentence the word was in was encountered the second time\",\n            name_extended=\"indicator for whether fixation occurred when the sentence containing the word was encountered the second time\"),\n    Feature('prevFixDur', float,\n            name_extended='duration of previous fixation in milliseconds'),\n    Feature('firstfixDur', float,\n            name_extended='duration (in milliseconds) of the first fixation when the word is first encountered'),\n    Feature('firstPassFixDur', float,\n            name_extended='Sum of durations (in milliseconds) of fixations when the word is first encountered'),\n    Feature('nextFixDur', float,\n            name_extended='Duration (in milliseconds) of the next fixation when gaze initially moves from the word'),\n    Feature('firstSaccLen', float, name_extended='Length of the first saccade'),\n    Feature('lastSaccLen', float,\n            name_extended='Distance (in pixels) between fixation on the word and the next fixation'),\n    Feature('prevFixPos', float,\n            name_extended='Distance (in pixels) between the first fixation preceding the word and the beginning of the word'),\n    Feature('landingPos', float,\n            name_extended='Distance (in pixels) between the first fixation on the word and the beginning of the word'),\n    Feature('leavingPos', float,\n            name_extended='Distance (in pixels) between the last fixation on the word and the beginning of the word'),\n    Feature('totalFixDur', float,\n            name_extended='Sum of all durations (in milliseconds) of fixations to the word'),\n    Feature('meanFixDur', float,\n            name_extended='Mean duration (in milliseconds) of the fixations to the word'),\n    Feature('regressLen', float,\n            name_extended='Sum of durations (in milliseconds) of regressions initiating from this word'),\n    Feature('nextWordRegress', int,\n            \"'1' if a regression initiated from the following word\",\n            name_extended=\"indicator for whether a regression was initiated from the following word\"),\n    Feature('regressDur', float,\n            name_extended='Sum of durations (in milliseconds) of the fixations on the word during regression'),\n    Feature('pupilDiamMax', float, name_extended='Maximum pupil diameter'),\n    Feature('pupilDiamLag', float,\n            name_extended='Maximum pupil diameter 0.5 - 1.5 seconds after the beginning of fixation'),\n    Feature('timePrtctg', float,\n            'First fixation duration divided by the total number of fixations'),\n    Feature('titleNo', int, name_extended='Title number'),\n    Feature('wordNo', int, name_extended='Word number (ordinal) in this title'),\n    Feature('label', int, is_target=True,\n            name_extended=\"sentence is irrelevant to the question\",\n            value_mapping={1: 'irrelevant', 0: 'relevant'},\n            ),\n], documentation=\"https://www.openml.org/d/44157, \"\n                 \"http://research.ics.aalto.fi/events/eyechallenge2005/irem-2005-03-03.pdf\")\n\nHIGGS_FEATURES = FeatureList(features=[\n    Feature('target', int, name_extended='is signal', is_target=True),\n    Feature('lepton_pT', float, name_extended='kinematic property lepton pT'),\n    Feature('lepton_eta', float, name_extended='kinematic property lepton eta'),\n    Feature('lepton_phi', float, name_extended='kinematic property lepton phi'),\n    Feature('missing_energy_magnitude', float,\n            name_extended='kinematic property missing energy magnitude'),\n    Feature('missing_energy_phi', float,\n            name_extended='kinematic property missing energy phi'),\n    Feature('jet_1_pt', float, name_extended='kinematic property jet 1 pT'),\n    Feature('jet_1_eta', float, name_extended='kinematic property jet 1 eta'),\n    Feature('jet_1_phi', float, name_extended='kinematic property jet 1 phi'),\n    Feature('jet_2_pt', float, name_extended='kinematic property jet 2 pT'),\n    Feature('jet_2_eta', float, name_extended='kinematic property jet 2 eta'),\n    Feature('jet_2_phi', float, name_extended='kinematic property jet 2 phi'),\n    Feature('jet_3_pt', float, name_extended='kinematic property jet 3 pT'),\n    Feature('jet_3_eta', float, name_extended='kinematic property jet 3 eta'),\n    Feature('jet_3_phi', float, name_extended='kinematic property jet 3 phi'),\n    Feature('jet_4_pt', float, name_extended='kinematic property jet 4 pT'),\n    Feature('jet_4_eta', float, name_extended='kinematic property jet 4 eta'),\n    Feature('jet_4_phi', float, name_extended='kinematic property jet 4 phi'),\n    Feature('m_jj', float, name_extended='derived feature m_jj'),\n    Feature('m_jjj', float, name_extended='derived feature m_jjj'),\n    Feature('m_lv', float, name_extended='derived feature m_lv'),\n    Feature('m_jlv', float, name_extended='derived feature m_jlv'),\n    Feature('m_bb', float, name_extended='derived feature m_bb'),\n    Feature('m_wbb', float, name_extended='derived feature m_wbb'),\n    Feature('m_wwbb', float, name_extended='derived feature m_wwbb'),\n], documentation='https://www.openml.org/d/44129')\n\nMAGIC_TELESCOPE_FEATURES = FeatureList(features=[\n    Feature('fLength:', float,\n            name_extended='length of major axis of ellipse in mm'),\n    Feature('fWidth:', float,\n            name_extended='length of minor axis of ellipse in mm'),\n    Feature('fSize:', float,\n            name_extended='log base 10 of sum of content of all pixel values in image (fSize)'),\n    Feature('fConc:', float,\n            name_extended='ratio of sum of two highest pixel values to fSize'),\n    Feature('fConc1:', float,\n            name_extended='ratio of largest pixel value to fSize'),\n    Feature('fAsym:', float,\n            name_extended='distance from highest pixel to center, projected onto major axis in mm'),\n    Feature('fM3Long:', float,\n            name_extended='3rd root of third moment along major axis in mm'),\n    Feature('fM3Trans:', float,\n            name_extended='3rd root of third moment along minor axis in mm'),\n    Feature('fAlpha:', float,\n            name_extended='angle of major axis with vector to origin in degrees'),\n    Feature('fDist:', float,\n            name_extended='distance from origin to center of ellipse in mm'),\n    Feature('is_signal', int, is_target=True,\n            name_extended='input is gamma (signal) and not hadron (background)')\n], documentation='https://www.openml.org/d/44125')\n\nMINI_BOONE_FEATURES = FeatureList(features=[\n    Feature('ParticleID_0', float, name_extended='particle ID 0 value'),\n    Feature('ParticleID_1', float, name_extended='particle ID 1 value'),\n    Feature('ParticleID_2', float, name_extended='particle ID 2 value'),\n    Feature('ParticleID_3', float, name_extended='particle ID 3 value'),\n    Feature('ParticleID_4', float, name_extended='particle ID 4 value'),\n    Feature('ParticleID_5', float, name_extended='particle ID 5 value'),\n    Feature('ParticleID_6', float, name_extended='particle ID 6 value'),\n    Feature('ParticleID_7', float, name_extended='particle ID 7 value'),\n    Feature('ParticleID_8', float, name_extended='particle ID 8 value'),\n    Feature('ParticleID_9', float, name_extended='particle ID 9 value'),\n    Feature('ParticleID_10', float, name_extended='particle ID 10 value'),\n    Feature('ParticleID_11', float, name_extended='particle ID 11 value'),\n    Feature('ParticleID_12', float, name_extended='particle ID 12 value'),\n    Feature('ParticleID_13', float, name_extended='particle ID 13 value'),\n    Feature('ParticleID_14', float, name_extended='particle ID 14 value'),\n    Feature('ParticleID_15', float, name_extended='particle ID 15 value'),\n    Feature('ParticleID_16', float, name_extended='particle ID 16 value'),\n    Feature('ParticleID_17', float, name_extended='particle ID 17 value'),\n    Feature('ParticleID_18', float, name_extended='particle ID 18 value'),\n    Feature('ParticleID_19', float, name_extended='particle ID 19 value'),\n    Feature('ParticleID_20', float, name_extended='particle ID 20 value'),\n    Feature('ParticleID_21', float, name_extended='particle ID 21 value'),\n    Feature('ParticleID_22', float, name_extended='particle ID 22 value'),\n    Feature('ParticleID_23', float, name_extended='particle ID 23 value'),\n    Feature('ParticleID_24', float, name_extended='particle ID 24 value'),\n    Feature('ParticleID_25', float, name_extended='particle ID 25 value'),\n    Feature('ParticleID_26', float, name_extended='particle ID 26 value'),\n    Feature('ParticleID_27', float, name_extended='particle ID 27 value'),\n    Feature('ParticleID_28', float, name_extended='particle ID 28 value'),\n    Feature('ParticleID_29', float, name_extended='particle ID 29 value'),\n    Feature('ParticleID_30', float, name_extended='particle ID 30 value'),\n    Feature('ParticleID_31', float, name_extended='particle ID 31 value'),\n    Feature('ParticleID_32', float, name_extended='particle ID 32 value'),\n    Feature('ParticleID_33', float, name_extended='particle ID 33 value'),\n    Feature('ParticleID_34', float, name_extended='particle ID 34 value'),\n    Feature('ParticleID_35', float, name_extended='particle ID 35 value'),\n    Feature('ParticleID_36', float, name_extended='particle ID 36 value'),\n    Feature('ParticleID_37', float, name_extended='particle ID 37 value'),\n    Feature('ParticleID_38', float, name_extended='particle ID 38 value'),\n    Feature('ParticleID_39', float, name_extended='particle ID 39 value'),\n    Feature('ParticleID_40', float, name_extended='particle ID 40 value'),\n    Feature('ParticleID_41', float, name_extended='particle ID 41 value'),\n    Feature('ParticleID_42', float, name_extended='particle ID 42 value'),\n    Feature('ParticleID_43', float, name_extended='particle ID 43 value'),\n    Feature('ParticleID_44', float, name_extended='particle ID 44 value'),\n    Feature('ParticleID_45', float, name_extended='particle ID 45 value'),\n    Feature('ParticleID_46', float, name_extended='particle ID 46 value'),\n    Feature('ParticleID_47', float, name_extended='particle ID 47 value'),\n    Feature('ParticleID_48', float, name_extended='particle ID 48 value'),\n    Feature('ParticleID_49', float, name_extended='particle ID 49 value'),\n    Feature('signal', int, is_target=True,\n            name_extended='is electron neutrino (signal) vs. muon neutrino (background)'),\n], documentation='https://www.openml.org/d/44128')\n\n# Note: the descriptions for these features are not good, and no value\n# mapping is available.\nROAD_SAFETY_FEATURES = FeatureList(features=[\n    Feature('Vehicle_Reference_df_res', int),\n    Feature('Vehicle_Type', int, name_extended='vehicle type',\n            value_mapping={\n                1: 'Pedal cycle', 2: 'Motorcycle 50cc and under',\n                3: 'Motorcycle 125cc and under',\n                4: 'Motorcycle over 125cc and up to 500cc',\n                5: 'Motorcycle over 500cc', 8: 'Taxi/Private hire car',\n                9: 'Car', 10: 'Minibus (8 - 16 passenger seats)',\n                11: 'Bus or coach (17 or more pass seats)', 16: 'Ridden horse',\n                17: 'Agricultural vehicle', 18: 'Tram',\n                19: 'Van / Goods 3.5 tonnes mgw or under',\n                20: 'Goods over 3.5t. and under 7.5t',\n                21: 'Goods 7.5 tonnes mgw and over', 22: 'Mobility scooter',\n                23: 'Electric motorcycle', 90: 'Other vehicle',\n                97: 'Motorcycle - unknown cc',\n                98: 'Goods vehicle - unknown weight',\n                99: 'Unknown vehicle type (self rep only)',\n                103: 'Motorcycle - Scooter (1979-1998)',\n                104: 'Motorcycle (1979-1998)',\n                105: 'Motorcycle - Combination (1979-1998)',\n                106: 'Motorcycle over 125cc (1999-2004)',\n                108: 'Taxi (excluding private hire cars) (1979-2004)',\n                109: 'Car (including private hire cars) (1979-2004)',\n                110: 'Minibus/Motor caravan (1979-1998)',\n                113: 'Goods over 3.5 tonnes (1979-1998)',\n                -1: 'Data missing or out of range'}),\n    Feature('Vehicle_Manoeuvre', int,\n            name_extended='vehicle maneuver type',\n            value_mapping={\n                -1: 'Data missing or out of range', 1: 'Reversing', 2: 'Parked',\n                3: 'Waiting to go - held up', 4: 'Slowing or stopping',\n                5: 'Moving off', 6: 'U-turn', 7: 'Turning left',\n                8: 'Waiting to turn left', 9: 'Turning right',\n                10: 'Waiting to turn right', 11: 'Changing lane to left',\n                12: 'Changing lane to right',\n                13: 'Overtaking moving vehicle - offside',\n                14: 'Overtaking static vehicle - offside',\n                15: 'Overtaking - nearside', 16: 'Going ahead left-hand bend',\n                17: 'Going ahead right-hand bend', 18: 'Going ahead other',\n                99: 'unknown (self reported)'}),\n    Feature('Vehicle_Location-Restricted_Lane', int,\n            name_extended='vehicle location lane restriction type',\n            value_mapping={\n                0: \"On main c'way - not in restricted lane\",\n                1: \"Tram/Light rail track\", 2: \"Bus lane\",\n                3: \"Busway (including guided busway)\",\n                4: \"Cycle lane (on main carriageway)\",\n                5: \"Cycleway or shared use footway (not part of  main carriageway)\",\n                6: \"On lay-by or hard shoulder\",\n                7: \"Entering lay-by or hard shoulder\",\n                8: \"Leaving lay-by or hard shoulder\", 9: \"Footway (pavement)\",\n                10: \"Not on carriageway\", 99: \"unknown (self reported)\",\n                -1: \"Data missing or out of range\",\n            }),\n    Feature('Hit_Object_in_Carriageway', int,\n            name_extended='hit object in carriageway',\n            value_mapping={\n                0: \"None\", 1: \"Previous accident\", 2: \"Road works\",\n                4: \"Parked vehicle\", 5: \"Bridge (roof)\", 6: \"Bridge (side)\",\n                7: \"Bollard or refuge\", 8: \"Open door of vehicle\",\n                9: \"Central island of roundabout\", 10: \"Kerb\",\n                11: \"Other object\", 12: \"Any animal (except ridden horse)\",\n                99: \"unknown (self reported)\",\n                -1: \"Data missing or out of range\"\n            }),\n    Feature('Hit_Object_off_Carriageway', int,\n            name_extended='hit object off carriageway'),\n    Feature('Was_Vehicle_Left_Hand_Drive?', int,\n            name_extended='indicator for whether vehicle was left-hand drive',\n            value_mapping={\n                0: 'Yes',\n                1: 'No',\n                9: 'Unknown',\n                -1: 'Data missing or out of range'}),\n    Feature('Age_of_Driver', int,\n            name_extended='age of driver (in years)'),\n    Feature('Age_Band_of_Driver', int,\n            name_extended='age band of driver',\n            value_mapping={\n                1: '0 - 5', 2: '6 - 10', 3: '11 - 15', 4: '16 - 20',\n                5: '21 - 25', 6: '26 - 35', 7: '36 - 45', 8: '46 - 55',\n                9: '56 - 65', 10: '66 - 75', 11: 'Over 75'\n            }),\n    Feature('Engine_Capacity_(CC)', float,\n            name_extended='engine capacity in cc'),\n    Feature('Propulsion_Code', int, name_extended='propulsion code',\n            value_mapping={\n                1: \"Petrol\", 2: \"Heavy oil\", 3: \"Electric\", 4: \"Steam\",\n                5: \"Gas\", 6: \"Petrol/Gas (LPG)\", 7: \"Gas/Bi-fuel\",\n                8: \"Hybrid electric\", 9: \"Gas Diesel\",\n                10: \"New fuel technology\", 11: \"Fuel cells\",\n                12: \"Electric diesel\", -1: \"Undefined\"\n            }),\n    Feature('Age_of_Vehicle', int, name_extended='age of vehicle in years'),\n    Feature('Location_Easting_OSGR', float,\n            name_extended='Ordnance Survey Grid References (OSGR) Easting coordinate'),\n    Feature('Location_Northing_OSGR', float,\n            name_extended='Ordnance Survey Grid References (OSGR) Northing coordinate'),\n    Feature('Longitude', float),\n    Feature('Latitude', float),\n    Feature('Police_Force', int, name_extended='police force code',\n            value_mapping={\n                1: 'Metropolitan Police', 3: 'Cumbria', 4: 'Lancashire',\n                5: 'Merseyside', 6: 'Greater Manchester', 7: 'Cheshire',\n                10: 'Northumbria', 11: 'Durham', 12: 'North Yorkshire',\n                13: 'West Yorkshire', 14: 'South Yorkshire', 16: 'Humberside',\n                17: 'Cleveland', 20: 'West Midlands', 21: 'Staffordshire',\n                22: 'West Mercia', 23: 'Warwickshire', 30: 'Derbyshire',\n                31: 'Nottinghamshire', 32: 'Lincolnshire', 33: 'Leicestershire',\n                34: 'Northamptonshire', 35: 'Cambridgeshire', 36: 'Norfolk',\n                37: 'Suffolk', 40: 'Bedfordshire', 41: 'Hertfordshire',\n                42: 'Essex', 43: 'Thames Valley', 44: 'Hampshire', 45: 'Surrey',\n                46: 'Kent', 47: 'Sussex', 48: 'City of London',\n                50: 'Devon and Cornwall', 52: 'Avon and Somerset',\n                53: 'Gloucestershire', 54: 'Wiltshire', 55: 'Dorset',\n                60: 'North Wales', 61: 'Gwent', 62: 'South Wales',\n                63: 'Dyfed-Powys', 91: 'Northern', 92: 'Grampian',\n                93: 'Tayside', 94: 'Fife', 95: 'Lothian and Borders',\n                96: 'Central', 97: 'Strathclyde', 98: 'Dumfries and Galloway',\n                99: 'Police Scotland',\n            }),\n    Feature('Number_of_Vehicles', int,\n            name_extended='number of vehicles involved'),\n    Feature('Number_of_Casualties', int, name_extended='number of casualties'),\n    Feature('Local_Authority_(District)', float,\n            name_extended='local authority (district)',\n            value_mapping={\n                1: \"Westminster\", 2: \"Camden\", 3: \"Islington\", 4: \"Hackney\",\n                5: \"Tower Hamlets\", 6: \"Greenwich\", 7: \"Lewisham\",\n                8: \"Southwark\", 9: \"Lambeth\", 10: \"Wandsworth\",\n                11: \"Hammersmith and Fulham\", 12: \"Kensington and Chelsea\",\n                13: \"Waltham Forest\", 14: \"Redbridge\", 15: \"Havering\",\n                16: \"Barking and Dagenham\", 17: \"Newham\", 18: \"Bexley\",\n                19: \"Bromley\", 20: \"Croydon\", 21: \"Sutton\", 22: \"Merton\",\n                23: \"Kingston upon Thames\", 24: \"Richmond upon Thames\",\n                25: \"Hounslow\", 26: \"Hillingdon\", 27: \"Ealing\", 28: \"Brent\",\n                29: \"Harrow\", 30: \"Barnet\", 31: \"Haringey\", 32: \"Enfield\",\n                33: \"Hertsmere\", 38: \"Epsom and Ewell\", 40: \"Spelthorne\",\n                57: \"London Airport (Heathrow)\", 60: \"Allerdale\",\n                61: \"Barrow-in-Furness\", 62: \"Carlisle\", 63: \"Copeland\",\n                64: \"Eden\", 65: \"South Lakeland\", 70: \"Blackburn with Darwen\",\n                71: \"Blackpool\", 72: \"Burnley\", 73: \"Chorley\", 74: \"Fylde\",\n                75: \"Hyndburn\", 76: \"Lancaster\", 77: \"Pendle\", 79: \"Preston\",\n                80: \"Ribble Valley\", 82: \"Rossendale\", 83: \"South Ribble\",\n                84: \"West Lancashire\", 85: \"Wyre\", 90: \"Knowsley\",\n                91: \"Liverpool\", 92: \"St. Helens\", 93: \"Sefton\", 95: \"Wirral\",\n                100: \"Bolton\", 101: \"Bury\", 102: \"Manchester\", 104: \"Oldham\",\n                106: \"Rochdale\", 107: \"Salford\", 109: \"Stockport\",\n                110: \"Tameside\", 112: \"Trafford\", 114: \"Wigan\", 120: \"Chester\",\n                121: \"Congleton\", 122: \"Crewe and Nantwich\",\n                123: \"Ellesmere Port and Neston\", 124: \"Halton\",\n                126: \"Macclesfield\", 127: \"Vale Royal\", 128: \"Warrington\",\n                129: \"Cheshire East\", 130: \"Cheshire West and Chester\",\n                139: \"Northumberland\", 140: \"Alnwick\",\n                141: \"Berwick-upon-Tweed\", 142: \"Blyth Valley\",\n                143: \"Castle Morpeth\", 144: \"Tynedale\", 145: \"Wansbeck\",\n                146: \"Gateshead\", 147: \"Newcastle upon Tyne\",\n                148: \"North Tyneside\", 149: \"South Tyneside\", 150: \"Sunderland\",\n                160: \"Chester-le-Street\", 161: \"Darlington\", 162: \"Derwentside\",\n                163: \"Durham\", 164: \"Easington\", 165: \"Sedgefield\",\n                166: \"Teesdale\", 168: \"Wear Valley\", 169: \"County Durham\",\n                180: \"Craven\", 181: \"Hambleton\", 182: \"Harrogate\",\n                184: \"Richmondshire\", 185: \"Ryedale\", 186: \"Scarborough\",\n                187: \"Selby\", 189: \"York\", 200: \"Bradford\", 202: \"Calderdale\",\n                203: \"Kirklees\", 204: \"Leeds\", 206: \"Wakefield\",\n                210: \"Barnsley\", 211: \"Doncaster\", 213: \"Rotherham\",\n                215: \"Sheffield\", 228: \"Kingston upon Hull, City of\",\n                231: \"East Riding of Yorkshire\", 232: \"North Lincolnshire\",\n                233: \"North East Lincolnshire\", 240: \"Hartlepool\",\n                241: \"Redcar and Cleveland\", 243: \"Middlesbrough\",\n                245: \"Stockton-on-Tees\", 250: \"Cannock Chase\",\n                251: \"East Staffordshire\", 252: \"Lichfield\",\n                253: \"Newcastle-under-Lyme\", 254: \"South Staffordshire\",\n                255: \"Stafford\", 256: \"Staffordshire Moorlands\",\n                257: \"Stoke-on-Trent\", 258: \"Tamworth\", 270: \"Bromsgrove\",\n                273: \"Malvern Hills\", 274: \"Redditch\", 276: \"Worcester\",\n                277: \"Wychavon\", 278: \"Wyre Forest\", 279: \"Bridgnorth\",\n                280: \"North Shropshire\", 281: \"Oswestry\",\n                282: \"Shrewsbury and Atcham\", 283: \"South Shropshire\",\n                284: \"Telford and Wrekin\", 285: \"Herefordshire, County of\",\n                286: \"Shropshire\", 290: \"North Warwickshire\",\n                291: \"Nuneaton and Bedworth\", 292: \"Rugby\",\n                293: \"Stratford-upon-Avon\", 294: \"Warwick\", 300: \"Birmingham\",\n                302: \"Coventry\", 303: \"Dudley\", 305: \"Sandwell\",\n                306: \"Solihull\", 307: \"Walsall\", 309: \"Wolverhampton\",\n                320: \"Amber Valley\", 321: \"Bolsover\", 322: \"Chesterfield\",\n                323: \"Derby\", 324: \"Erewash\", 325: \"High Peak\",\n                327: \"North East Derbyshire\", 328: \"South Derbyshire\",\n                329: \"Derbyshire Dales\", 340: \"Ashfield\", 341: \"Bassetlaw\",\n                342: \"Broxtowe\", 343: \"Gedling\", 344: \"Mansfield\",\n                345: \"Newark and Sherwood\", 346: \"Nottingham\",\n                347: \"Rushcliffe\", 350: \"Boston\", 351: \"East Lindsey\",\n                352: \"Lincoln\", 353: \"North Kesteven\", 354: \"South Holland\",\n                355: \"South Kesteven\", 356: \"West Lindsey\", 360: \"Blaby\",\n                361: \"Hinckley and Bosworth\", 362: \"Charnwood\",\n                363: \"Harborough\", 364: \"Leicester\", 365: \"Melton\",\n                366: \"North West Leicestershire\", 367: \"Oadby and Wigston\",\n                368: \"Rutland\", 380: \"Corby\", 381: \"Daventry\",\n                382: \"East Northamptonshire\", 383: \"Kettering\",\n                384: \"Northampton\", 385: \"South Northamptonshire\",\n                386: \"Wellingborough\", 390: \"Cambridge\",\n                391: \"East Cambridgeshire\", 392: \"Fenland\",\n                393: \"Huntingdonshire\", 394: \"Peterborough\",\n                395: \"South Cambridgeshire\", 400: \"Breckland\", 401: \"Broadland\",\n                402: \"Great Yarmouth\", 404: \"Norwich\", 405: \"North Norfolk\",\n                406: \"South Norfolk\", 407: \"King's Lynn and West Norfolk\",\n                410: \"Babergh\", 411: \"Forest Heath\", 412: \"Ipswich\",\n                413: \"Mid Suffolk\", 414: \"St. Edmundsbury\",\n                415: \"Suffolk Coastal\", 416: \"Waveney\", 420: \"Bedford\",\n                421: \"Luton\", 422: \"Mid Bedfordshire\",\n                423: \"South Bedfordshire\", 424: \"Central Bedfordshire\",\n                430: \"Broxbourne\", 431: \"Dacorum\", 432: \"East Hertfordshire\",\n                433: \"North Hertfordshire\", 434: \"St. Albans\", 435: \"Stevenage\",\n                436: \"Three Rivers\", 437: \"Watford\", 438: \"Welwyn Hatfield\",\n                450: \"Basildon\", 451: \"Braintree\", 452: \"Brentwood\",\n                453: \"Castle Point\", 454: \"Chelmsford\", 455: \"Colchester\",\n                456: \"Epping Forest\", 457: \"Harlow\", 458: \"Maldon\",\n                459: \"Rochford\", 460: \"Southend-on-Sea\", 461: \"Tendring\",\n                462: \"Thurrock\", 463: \"Uttlesford\", 470: \"Bracknell Forest\",\n                471: \"West Berkshire\", 472: \"Reading\", 473: \"Slough\",\n                474: \"Windsor and Maidenhead\", 475: \"Wokingham\",\n                476: \"Aylesbury Vale\", 477: \"South Bucks\", 478: \"Chiltern\",\n                479: \"Milton Keynes\", 480: \"Wycombe\", 481: \"Cherwell\",\n                482: \"Oxford\", 483: \"Vale of White Horse\",\n                484: \"South Oxfordshire\", 485: \"West Oxfordshire\",\n                490: \"Basingstoke and Deane\", 491: \"Eastleigh\", 492: \"Fareham\",\n                493: \"Gosport\", 494: \"Hart\", 495: \"Havant\", 496: \"New Forest\",\n                497: \"East Hampshire\", 498: \"Portsmouth\", 499: \"Rushmoor\",\n                500: \"Southampton\", 501: \"Test Valley\", 502: \"Winchester\",\n                505: \"Isle of Wight\", 510: \"Elmbridge\", 511: \"Guildford\",\n                512: \"Mole Valley\", 513: \"Reigate and Banstead\",\n                514: \"Runnymede\", 515: \"Surrey Heath\", 516: \"Tandridge\",\n                517: \"Waverley\", 518: \"Woking\", 530: \"Ashford\",\n                531: \"Canterbury\", 532: \"Dartford\", 533: \"Dover\",\n                535: \"Gravesham\", 536: \"Maidstone\", 538: \"Sevenoaks\",\n                539: \"Shepway\", 540: \"Swale\", 541: \"Thanet\",\n                542: \"Tonbridge and Malling\", 543: \"Tunbridge Wells\",\n                544: \"Medway\", 551: \"Eastbourne\", 552: \"Hastings\", 554: \"Lewes\",\n                555: \"Rother\", 556: \"Wealden\", 557: \"Adur\", 558: \"Arun\",\n                559: \"Chichester\", 560: \"Crawley\", 562: \"Horsham\",\n                563: \"Mid Sussex\", 564: \"Worthing\", 565: \"Brighton and Hove\",\n                570: \"City of London\", 580: \"East Devon\", 581: \"Exeter\",\n                582: \"North Devon\", 583: \"Plymouth\", 584: \"South Hams\",\n                585: \"Teignbridge\", 586: \"Mid Devon\", 587: \"Torbay\",\n                588: \"Torridge\", 589: \"West Devon\", 590: \"Caradon\",\n                591: \"Carrick\", 592: \"Kerrier\", 593: \"North Cornwall\",\n                594: \"Penwith\", 595: \"Restormel\", 596: \"Cornwall\",\n                601: \"Bristol, City of\", 605: \"North Somerset\", 606: \"Mendip\",\n                607: \"Sedgemoor\", 608: \"Taunton Deane\", 609: \"West Somerset\",\n                610: \"South Somerset\", 611: \"Bath and North East Somerset\",\n                612: \"South Gloucestershire\", 620: \"Cheltenham\",\n                621: \"Cotswold\", 622: \"Forest of Dean\", 623: \"Gloucester\",\n                624: \"Stroud\", 625: \"Tewkesbury\", 630: \"Kennet\",\n                631: \"North Wiltshire\", 632: \"Salisbury\", 633: \"Swindon\",\n                634: \"West Wiltshire\", 635: \"Wiltshire\", 640: \"Bournemouth\",\n                641: \"Christchurch\", 642: \"North Dorset\", 643: \"Poole\",\n                644: \"Purbeck\", 645: \"West Dorset\",\n                646: \"Weymouth and Portland\", 647: \"East Dorset\",\n                720: \"Isle of Anglesey\", 721: \"Conwy\", 722: \"Gwynedd\",\n                723: \"Denbighshire\", 724: \"Flintshire\", 725: \"Wrexham\",\n                730: \"Blaenau Gwent\", 731: \"Caerphilly\", 732: \"Monmouthshire\",\n                733: \"Newport\", 734: \"Torfaen\", 740: \"Bridgend\", 741: \"Cardiff\",\n                742: \"Merthyr Tydfil\", 743: \"Neath Port Talbot\",\n                744: \"Rhondda, Cynon, Taff\", 745: \"Swansea\",\n                746: \"The Vale of Glamorgan\", 750: \"Ceredigion\",\n                751: \"Carmarthenshire\", 752: \"Pembrokeshire\", 753: \"Powys\",\n                910: \"Aberdeen City\", 911: \"Aberdeenshire\", 912: \"Angus\",\n                913: \"Argyll and Bute\", 914: \"Scottish Borders\",\n                915: \"Clackmannanshire\", 916: \"West Dunbartonshire\",\n                917: \"Dumfries and Galloway\", 918: \"Dundee City\",\n                919: \"East Ayrshire\", 920: \"East Dunbartonshire\",\n                921: \"East Lothian\", 922: \"East Renfrewshire\",\n                923: \"Edinburgh, City of\", 924: \"Falkirk\", 925: \"Fife\",\n                926: \"Glasgow City\", 927: \"Highland\", 928: \"Inverclyde\",\n                929: \"Midlothian\", 930: \"Moray\", 931: \"North Ayrshire\",\n                932: \"North Lanarkshire\", 933: \"Orkney Islands\",\n                934: \"Perth and Kinross\", 935: \"Renfrewshire\",\n                936: \"Shetland Islands\", 937: \"South Ayrshire\",\n                938: \"South Lanarkshire\", 939: \"Stirling\", 940: \"West Lothian\",\n                941: \"Western Isles\"}),\n    Feature('1st_Road_Number', float, name_extended='first road number',\n            value_mapping={**{x: str(x) for x in range(10000)},\n                           0: 'no official number', -1: 'unknown'}),\n    Feature('2nd_Road_Number', float, name_extended='second roat number',\n            value_mapping={**{x: str(x) for x in range(10000)},\n                           0: 'no official number', -1: 'unknown'}),\n    Feature('Urban_or_Rural_Area', int,\n            name_extended='urban or rural area indicator',\n            value_mapping={\n                1: 'Urban',\n                2: 'Rural',\n                3: 'Unallocated',\n                -1: 'Data missing or out of range',\n            }),\n    Feature('Vehicle_Reference_df', int, name_extended='vehicle reference DF'),\n    Feature('Casualty_Reference', int, name_extended='casualty reference'),\n    Feature('Sex_of_Casualty', int, name_extended='sex of casualty',\n            value_mapping={\n                1: 'Male',\n                2: 'Female',\n                9: 'unknown (self reported)',\n                -1: 'Data missing or out of range'}),\n    Feature('Age_of_Casualty', int, name_extended='age of casualty in years'),\n    Feature('Age_Band_of_Casualty', int, name_extended='age band of casualty',\n            value_mapping={\n                1: '0 - 5', 2: '6 - 10', 3: '11 - 15', 4: '16 - 20',\n                5: '21 - 25', 6: '26 - 35', 7: '36 - 45', 8: '46 - 55',\n                9: '56 - 65', 10: '66 - 75', 11: 'Over 75'}),\n    Feature('Pedestrian_Location', int, name_extended='pedestrian location',\n            value_mapping={\n                0: \"Not a Pedestrian\",\n                1: \"Crossing on pedestrian crossing facility\",\n                2: \"Crossing in zig-zag approach lines\",\n                3: \"Crossing in zig-zag exit lines\",\n                4: \"Crossing elsewhere within 50m. of pedestrian crossing\",\n                5: \"In carriageway, crossing elsewhere\",\n                6: \"On footway or verge\",\n                7: \"On refuge, central island or central reservation\",\n                8: \"In centre of carriageway - not on refuge, island or central reservation\",\n                9: \"In carriageway, not crossing\", 10: \"Unknown or other\",\n                -1: \"Data missing or out of range\"\n            }),\n    Feature('Pedestrian_Movement', int, name_extended='pedestrian movement',\n            value_mapping={\n                0: \"Not a Pedestrian\", 1: \"Crossing from driver's nearside\",\n                2: \"Crossing from nearside - masked by parked or stationary vehicle\",\n                3: \"Crossing from driver's offside\",\n                4: \"Crossing from offside - masked by  parked or stationary vehicle\",\n                5: \"In carriageway, stationary - not crossing  (standing or playing)\",\n                6: \"In carriageway, stationary - not crossing  (standing or playing) - masked by parked or stationary vehicle\",\n                7: \"Walking along in carriageway, facing traffic\",\n                8: \"Walking along in carriageway, back to traffic\",\n                9: \"Unknown or other\", -1: \"Data missing or out of range\"\n            }),\n    Feature('Casualty_Type', int, name_extended='casualty type',\n            value_mapping={\n                0: \"Pedestrian\", 1: \"Cyclist\",\n                2: \"Motorcycle 50cc and under rider or passenger\",\n                3: \"Motorcycle 125cc and under rider or passenger\",\n                4: \"Motorcycle over 125cc and up to 500cc rider or  passenger\",\n                5: \"Motorcycle over 500cc rider or passenger\",\n                8: \"Taxi/Private hire car occupant\", 9: \"Car occupant\",\n                10: \"Minibus (8 - 16 passenger seats) occupant\",\n                11: \"Bus or coach occupant (17 or more pass seats)\",\n                16: \"Horse rider\", 17: \"Agricultural vehicle occupant\",\n                18: \"Tram occupant\",\n                19: \"Van / Goods vehicle (3.5 tonnes mgw or under) occupant\",\n                20: \"Goods vehicle (over 3.5t. and under 7.5t.) occupant\",\n                21: \"Goods vehicle (7.5 tonnes mgw and over) occupant\",\n                22: \"Mobility scooter rider\",\n                23: \"Electric motorcycle rider or passenger\",\n                90: \"Other vehicle occupant\",\n                97: \"Motorcycle - unknown cc rider or passenger\",\n                98: \"Goods vehicle (unknown weight) occupant\",\n                99: \"Unknown vehicle type (self rep only)\",\n                103: \"Motorcycle - Scooter (1979-1998)\",\n                104: \"Motorcycle (1979-1998)\",\n                105: \"Motorcycle - Combination (1979-1998)\",\n                106: \"Motorcycle over 125cc (1999-2004)\",\n                108: \"Taxi (excluding private hire cars) (1979-2004)\",\n                109: \"Car (including private hire cars) (1979-2004)\",\n                110: \"Minibus/Motor caravan (1979-1998)\",\n                113: \"Goods over 3.5 tonnes (1979-1998)\", }),\n    Feature('Casualty_IMD_Decile', int, name_extended='casualty IMD decile',\n            value_mapping={\n                1: \"Most deprived 10%\", 2: \"More deprived 10-20%\",\n                3: \"More deprived 20-30%\", 4: \"More deprived 30-40%\",\n                5: \"More deprived 40-50%\", 6: \"Less deprived 40-50%\",\n                7: \"Less deprived 30-40%\", 8: \"Less deprived 20-30%\",\n                9: \"Less deprived 10-20%\", 10: \"Least deprived 10%\",\n                -1: \"Data missing or out of range\", }),\n    Feature('SexofDriver', int, is_target=True,\n            name_extended='driver is male'),\n], documentation='https://www.openml.org/d/45038 ,'\n                 'https://www.data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data')\n\nPOL_FEATURES = FeatureList(features=[\n    Feature('binaryClass', int, is_target=True,\n            name_extended=\"target value is below mean\"),\n    Feature('f5', int),\n    Feature('f6', int),\n    Feature('f7', int),\n    Feature('f8', int),\n    Feature('f9', int),\n    Feature('f13', int),\n    Feature('f14', int),\n    Feature('f15', int),\n    Feature('f16', int),\n    Feature('f17', int),\n    Feature('f18', int),\n    Feature('f19', int),\n    Feature('f20', int),\n    Feature('f21', int),\n    Feature('f22', int),\n    Feature('f23', int),\n    Feature('f24', int),\n    Feature('f25', int),\n    Feature('f26', int),\n    Feature('f27', int),\n    Feature('f28', int),\n    Feature('f29', int),\n    Feature('f30', int),\n    Feature('f31', int),\n    Feature('f32', int),\n    Feature('f33', int),\n], documentation='https://www.openml.org/d/44122')\n\nJANNIS_FEATURES = FeatureList(features=[\n    Feature('V1', float),\n    Feature('V2', float),\n    Feature('V3', float),\n    Feature('V4', float),\n    Feature('V5', float),\n    Feature('V6', float),\n    Feature('V7', float),\n    Feature('V8', float),\n    Feature('V9', float),\n    Feature('V10', float),\n    Feature('V11', float),\n    Feature('V12', float),\n    Feature('V13', float),\n    Feature('V14', float),\n    Feature('V15', float),\n    Feature('V16', float),\n    Feature('V17', float),\n    Feature('V18', float),\n    Feature('V19', float),\n    Feature('V20', float),\n    Feature('V21', float),\n    Feature('V22', float),\n    Feature('V23', float),\n    Feature('V24', float),\n    Feature('V25', float),\n    Feature('V26', float),\n    Feature('V27', float),\n    Feature('V28', float),\n    Feature('V29', float),\n    Feature('V30', float),\n    Feature('V31', float),\n    Feature('V32', float),\n    Feature('V33', float),\n    Feature('V34', float),\n    Feature('V35', float),\n    Feature('V36', float),\n    Feature('V37', float),\n    Feature('V38', float),\n    Feature('V39', float),\n    Feature('V40', float),\n    Feature('V41', float),\n    Feature('V42', float),\n    Feature('V43', float),\n    Feature('V44', float),\n    Feature('V45', float),\n    Feature('V46', float),\n    Feature('V47', float),\n    Feature('V48', float),\n    Feature('V49', float),\n    Feature('V50', float),\n    Feature('V51', float),\n    Feature('V52', float),\n    Feature('V53', float),\n    Feature('V54', float),\n    Feature('class', int, name_extended=\"class label\", is_target=True),\n], documentation=\"https://www.openml.org/d/45021\"\n                 \"https://link.springer.com/chapter/10.1007/978-3-030-05318-5_10\")\n\nHOUSE_16H_FEATURES = FeatureList([\n    Feature('P1', float, name_extended=\"total persons count in the region\"),\n    Feature('P5p1', float, name_extended=\"percentage of males\"),\n    Feature('P6p2', float, name_extended=\"percentage of black people\"),\n    Feature('P11p4', float, name_extended=\"percentage over 64 years old\"),\n    Feature('P14p9', float, name_extended=\"percentage widowed females\"),\n    Feature('P15p1', float, name_extended=\"percentage of people in family households\"),\n    Feature('P15p3', float, name_extended=\"percentage of people in group quarters (including jails)\"),\n    Feature('P16p2', float, name_extended=\"percentage of households with 2 or more persons which are family households\"),\n    Feature('P18p2', float, name_extended=\"percentage of households with 1+ persons under 18 which are non-family households\"),\n    Feature('P27p4', float, name_extended=\"percentage of households which are non-family with 2+ persons\"),\n    Feature('H2p2', float, name_extended=\"percentage of housing units vacant\"),\n    Feature('H8p2', float, name_extended=\"percentage of occupied housing units with black householder\"),\n    Feature('H10p1', float, name_extended=\"percentage of occupied housing units with householder not of Hispanic origin\"),\n    Feature('H13p1', float, name_extended=\"percentage of housing units with 1-4 rooms\"),\n    Feature('H18pA', float, name_extended=\"average number of persons per owner-occupied housing units\"),\n    Feature('H40p4', float, name_extended=\"percentage of vacant-for-sale housing units vacant more then 6 months\"),\n    Feature('binaryClass', int, is_target=True, name_extended='class label'),\n], documentation=\"https://www.openml.org/d/44123 , \"\n                 \"https://www.openml.org/search?type=data&status=active&id=574&sort=runs ,\"\n                 \"http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html\")\n\n\ndef preprocess_binaryclass_np_label(df: pd.DataFrame) -> pd.DataFrame:\n    df['binaryClass'] = (df['binaryClass'] == 'P').astype(int)\n    return df\n\n\ndef preprocess_electricity(df: pd.DataFrame) -> pd.DataFrame:\n    df['price_increase'] = (df['class'] == 'UP').astype(int)\n    df.drop(columns=['class'], inplace=True)\n    return df\n\n\ndef preprocess_bank(df: pd.DataFrame) -> pd.DataFrame:\n    # Original target is coded as (1,2), but it is really a binary variable for\n    # default, see\n    # https://www.openml.org/search?type=data&sort=runs&id=44126&status=active\n    df['default'] = df['Class'] - 1\n    df.drop(columns=['Class'], inplace=True)\n    return df\n\n\ndef preprocess_magic_telescope(df: pd.DataFrame) -> pd.DataFrame:\n    df['is_signal'] = (df['class'] == 'g').astype(int)\n    df.drop(columns=['class'], inplace=True)\n    return df\n\n\ndef preprocess_miniboone(df: pd.DataFrame) -> pd.DataFrame:\n    df['signal'] = df['signal'].astype(int)\n    return df\n\n\ndef preprocess_covertype(df: pd.DataFrame) -> pd.DataFrame:\n    # convert class values from 1/2 to 0/1\n    df['class'] = df['class'] - 1\n    return df\n\n\n_PREPROCESS_FNS: Dict[str, Callable[[pd.DataFrame], pd.DataFrame]] = {\n    \"electricity\": preprocess_electricity,\n    \"bank-marketing\": preprocess_bank,\n    \"california\": lambda x: x,\n    'covertype': preprocess_covertype,\n    'credit': lambda x: x,\n    'default-of-credit-card-clients': lambda x: x,\n    'eye_movements': lambda x: x,\n    'Higgs': lambda x: x,\n    'house_16H': preprocess_binaryclass_np_label,\n    'jannis': lambda x: x,\n    'MagicTelescope': preprocess_magic_telescope,\n    'MiniBooNE': preprocess_miniboone,\n    'pol': preprocess_binaryclass_np_label,\n    'road-safety': lambda x: x,\n}\n\n\ndef preprocess_grinsztain_datataset(df: pd.DataFrame, name) -> pd.DataFrame:\n    \"\"\"Helper function to return the correct preprocessor for a dataset.\"\"\"\n    preprocess_fn = _PREPROCESS_FNS[name]\n    return preprocess_fn(df)\n"}
{"type": "source_file", "path": "tableshift/datasets/german.py", "content": "\"\"\"\nUtilities for the German Credit dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\n\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nGERMAN_RESOURCES = [\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"\n    \"statlog/german/german.data\"\n]\n\nGERMAN_FEATURES = FeatureList(features=[\n    Feature(\"status\", cat_dtype,\n            description=\"Status of existing checking account\",\n            name_extended=\"Status of existing checking account\",\n            value_mapping={\n                \"A11\": \"< 0 DM \",\n                \"A12\": \"0 <= ... < 200 DM \",\n                \"A13\": \"... >= 200 DM / salary assignments for at least 1 year \",\n                \"A14\": \"no checking account \",\n            }),\n    Feature(\"duration\", float,\n            description=\"Duration in month\",\n            name_extended=\"Duration in month\"),\n    Feature(\"credit_history\", cat_dtype,\n            description=\"Credit history \",\n            name_extended=\"Applicant's credit history \",\n            value_mapping={\n                \"A30\": \"no credits taken/ all credits paid back duly \",\n                \"A31\": \"all credits at this bank paid back duly \",\n                \"A32\": \"existing credits paid back duly till now\",\n                \"A33\": \"delay in paying off in the past\",\n                \"A34\": \"critical account/ other credits existing (not at this bank)\",\n            }),\n    Feature(\"purpose\", cat_dtype, description=\"Purpose\",\n            name_extended=\"Applicant's stated purpose of requested loan\",\n            value_mapping={\n                \"A40\": \"car (new)\",\n                \"A41\": \"car (used)\",\n                \"A42\": \"furniture/equipment\",\n                \"A43\": \"radio/television\",\n                \"A44\": \"domestic appliances\",\n                \"A45\": \"repairs\",\n                \"A46\": \"education\",\n                \"A47\": \"vacation\",\n                \"A48\": \"retraining\",\n                \"A49\": \"business\",\n                \"A410\": \"others\"}),\n    Feature(\"credit_amt\", float, description=\"Credit amount\",\n            name_extended=\"Credit amount\"),\n    Feature(\"savings_acct_bonds\", cat_dtype,\n            description=\"Savings account/bonds\",\n            name_extended=\"Applicant's current total savings account/bonds\",\n            value_mapping={\n                \"A61\": \"... < 100 DM\",\n                \"A62\": \"100 <= ... < 500 DM\",\n                \"A63\": \"500 <= ... < 1000 DM\",\n                \"A64\": \".. >= 1000 DM \",\n                \"A65\": \"unknown/ no savings account\"}),\n    Feature(\"present_unemployed_since\", cat_dtype,\n            description=\"Present employment since \",\n            name_extended=\"Duration applicant has held present employment\",\n            value_mapping={\n                \"A71\": \"unemployed\",\n                \"A72\": \"less than 1 year\",\n                \"A73\": \"at least 1 but less than 4 years\",\n                \"A74\": \"at least 4 but less than 7 years\",\n                \"A75\": \"at least 7 years \"}),\n    Feature(\"installment_rate\", float,\n            description=\"Installment rate in percentage of disposable income\",\n            name_extended=\"Installment rate in percentage of disposable income\"),\n    Feature(\"other_debtors\", cat_dtype,\n            description=\"Other debtors / guarantors\",\n            name_extended=\"Other debtors / guarantors\"),\n    Feature(\"pres_res_since\", float,\n            description=\"Present residence since\",\n            name_extended=\"Time applicant has resided in present residence\"\n            ),\n    Feature(\"property\", cat_dtype, description=\"Property\",\n            name_extended=\"Applicant's highest level of property owned\",\n            value_mapping={\n                \"A121\": \"real estate\",\n                \"A122\": \"building society savings agreement/ life insurance\",\n                \"A123\": \"car or other\",\n                \"A124\": \"unknown / no property\"}),\n    Feature(\"age_geq_median\", cat_dtype,\n            description=\"Binary indicator for whether applicant's age is \"\n                        \"greater than or equal to the median age of all \"\n                        \"applicants\",\n            name_extended=\"Binary indicator for whether applicant's age is \"\n                          \"greater than or equal to the median age of all \"\n                          \"applicants\",\n            value_mapping={\n                1: \"greater than or equal to median age\",\n                '1.0': \"greater than or equal to median age\",\n                0: \"not greater than or equal to median age\",\n                '0.0': \"not greater than or equal to median age\"\n            }),\n    Feature(\"sex\", cat_dtype, value_mapping={\n        1: \"male\",\n        '1.0': \"male\",\n        0: \"female\",\n        '0.0': \"female\",\n    }),\n    Feature(\"other_installment\", cat_dtype,\n            description=\"Other installment plans\",\n            name_extended=\"Other installment plans\",\n            value_mapping={\n                'A141': 'bank',\n                'A142': 'stores',\n                'A143': 'none'}),\n    Feature(\"housing\", cat_dtype, description=\"Housing\",\n            name_extended=\"Current type of housing occupied\",\n            value_mapping={'A151': 'rent', 'A152': 'own', 'A153': 'for free'}),\n    Feature(\"num_exist_credits\", float,\n            description=\"Number of existing credits at this bank \",\n            name_extended=\"Number of existing credits at this bank\"),\n    Feature(\"job\", cat_dtype,\n            name_extended=\"Type of job\",\n            value_mapping={\n                'A171': 'unemployed/ unskilled - non-resident',\n                'A172': 'unskilled - resident',\n                'A173': 'skilled employee / official',\n                'A174': 'management/ self-employed/highly qualified employee/ '\n                        'officer ',\n            }),\n    Feature(\"num_ppl\", float,\n            description=\"Number of people being liable to provide maintenance \"\n                        \"for\",\n            name_extended=\"Number of people being liable to provide \"\n                          \"maintenance for\"),\n    Feature(\"has_phone\", cat_dtype,\n            name_extended=\"Applicant has phone registered under their name\",\n            value_mapping={'A191': 'no', 'A192': 'yes'}),\n    Feature(\"foreign_worker\", cat_dtype,\n            name_extended=\"Applicant is a foreign worker\",\n            value_mapping={'A201': 'yes', 'A202': 'no'}),\n    Feature(\"Target\", int, is_target=True)],\n    documentation=\"https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\")\n\n\ndef preprocess_german(df: pd.DataFrame):\n    df.columns = [\"status\", \"duration\", \"credit_history\",\n                  \"purpose\", \"credit_amt\", \"savings_acct_bonds\",\n                  \"present_unemployed_since\", \"installment_rate\",\n                  \"per_status_sex\", \"other_debtors\", \"pres_res_since\",\n                  \"property\", \"age\", \"other_installment\", \"housing\",\n                  \"num_exist_credits\", \"job\", \"num_ppl\", \"has_phone\",\n                  \"foreign_worker\", \"Target\"]\n    # Code labels as in tfds; see\n    # https://github.com/tensorflow/datasets/blob/master/\"\\\n    # \"tensorflow_datasets/structured/german_credit_numeric.py\n    df[\"Target\"] = 2 - df[\"Target\"]\n    # convert per_status_sex into separate columns.\n    # Sex is 1 if male; else 0.\n    df[\"sex\"] = df[\"per_status_sex\"].apply(\n        lambda x: 1 if x not in [\"A92\", \"A95\"] else 0)\n    # Age is 1 if above median age, else 0.\n    median_age = df[\"age\"].median()\n    df[\"age_geq_median\"] = df[\"age\"].apply(lambda x: 1 if x > median_age else 0)\n\n    df[\"single\"] = df[\"per_status_sex\"].apply(\n        lambda x: 1 if x in [\"A93\", \"A95\"] else 0)\n\n    df.drop(columns=[\"per_status_sex\", \"age\"], inplace=True)\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/kaggle.py", "content": "\"\"\"\nUtilities for Kaggle competition data sources.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nOTTO_FEATURES = FeatureList(features=[\n    Feature('id', int),\n    Feature('feat_1', int),\n    Feature('feat_2', int),\n    Feature('feat_3', int),\n    Feature('feat_4', int),\n    Feature('feat_5', int),\n    Feature('feat_6', int),\n    Feature('feat_7', int),\n    Feature('feat_8', int),\n    Feature('feat_9', int),\n    Feature('feat_10', int),\n    Feature('feat_11', int),\n    Feature('feat_12', int),\n    Feature('feat_13', int),\n    Feature('feat_14', int),\n    Feature('feat_15', int),\n    Feature('feat_16', int),\n    Feature('feat_17', int),\n    Feature('feat_18', int),\n    Feature('feat_19', int),\n    Feature('feat_20', int),\n    Feature('feat_21', int),\n    Feature('feat_22', int),\n    Feature('feat_23', int),\n    Feature('feat_24', int),\n    Feature('feat_25', int),\n    Feature('feat_26', int),\n    Feature('feat_27', int),\n    Feature('feat_28', int),\n    Feature('feat_29', int),\n    Feature('feat_30', int),\n    Feature('feat_31', int),\n    Feature('feat_32', int),\n    Feature('feat_33', int),\n    Feature('feat_34', int),\n    Feature('feat_35', int),\n    Feature('feat_36', int),\n    Feature('feat_37', int),\n    Feature('feat_38', int),\n    Feature('feat_39', int),\n    Feature('feat_40', int),\n    Feature('feat_41', int),\n    Feature('feat_42', int),\n    Feature('feat_43', int),\n    Feature('feat_44', int),\n    Feature('feat_45', int),\n    Feature('feat_46', int),\n    Feature('feat_47', int),\n    Feature('feat_48', int),\n    Feature('feat_49', int),\n    Feature('feat_50', int),\n    Feature('feat_51', int),\n    Feature('feat_52', int),\n    Feature('feat_53', int),\n    Feature('feat_54', int),\n    Feature('feat_55', int),\n    Feature('feat_56', int),\n    Feature('feat_57', int),\n    Feature('feat_58', int),\n    Feature('feat_59', int),\n    Feature('feat_60', int),\n    Feature('feat_61', int),\n    Feature('feat_62', int),\n    Feature('feat_63', int),\n    Feature('feat_64', int),\n    Feature('feat_65', int),\n    Feature('feat_66', int),\n    Feature('feat_67', int),\n    Feature('feat_68', int),\n    Feature('feat_69', int),\n    Feature('feat_70', int),\n    Feature('feat_71', int),\n    Feature('feat_72', int),\n    Feature('feat_73', int),\n    Feature('feat_74', int),\n    Feature('feat_75', int),\n    Feature('feat_76', int),\n    Feature('feat_77', int),\n    Feature('feat_78', int),\n    Feature('feat_79', int),\n    Feature('feat_80', int),\n    Feature('feat_81', int),\n    Feature('feat_82', int),\n    Feature('feat_83', int),\n    Feature('feat_84', int),\n    Feature('feat_85', int),\n    Feature('feat_86', int),\n    Feature('feat_87', int),\n    Feature('feat_88', int),\n    Feature('feat_89', int),\n    Feature('feat_90', int),\n    Feature('feat_91', int),\n    Feature('feat_92', int),\n    Feature('feat_93', int),\n    Feature('target', cat_dtype, is_target=True,\n            value_mapping={\n                'Class_1': '1',\n                'Class_2': '2',\n                'Class_3': '3',\n                'Class_4': '4',\n                'Class_5': '5',\n                'Class_6': '6',\n                'Class_7': '7',\n                'Class_8': '8',\n                'Class_9': '9',\n            }),\n],\n    documentation=\"https://www.kaggle.com/competitions/otto-group-product-classification-challenge/data\")\n\nSF_CRIME_FEATURES = FeatureList(features=[\n    Feature('Dates', cat_dtype,\n            name_extended=\"date and time of crime incident\"),\n    Feature('Category', cat_dtype, is_target=True),\n    Feature('Descript', cat_dtype,\n            name_extended=\"description of the crime incident\"),\n    Feature('DayOfWeek', cat_dtype, name_extended=\"day of week\"),\n    Feature('PdDistrict', cat_dtype,\n            name_extended=\"Police Department District\"),\n    Feature('Resolution', cat_dtype),\n    Feature('Address', cat_dtype,\n            name_extended=\"approximate street address of the crime incident\"),\n    Feature('X', float, name_extended=\"longitude\"),\n    Feature('Y', float, name_extended=\"latitude\"),\n], documentation='https://www.kaggle.com/competitions/sf-crime/data')\n\nPLASTICC_FEATURES = FeatureList(features=[\n    Feature('object_id', int, name_extended=\"unique object identifier\"),\n    Feature('mjd', float,\n            \"the time in Modified Julian Date (MJD) of the observation. Can \"\n            \"be read as days since November 17, 1858. Can be converted to \"\n            \"Unix epoch time with the formula unix_time = (MJD−40587)×86400\",\n            name_extended=\"observation time in Modified Julian Date (MJD)\"),\n    Feature('passband', int,\n            \"The specific LSST passband integer, such that u, g, r, i, z, \"\n            \"Y = 0, 1, 2, 3, 4, 5 in which it was viewed\",\n            name_extended=\"observation LSST passband\"),\n    Feature('flux', float,\n            \"the measured flux (brightness) in the passband of observation as \"\n            \"listed in the passband column. These values have already been \"\n            \"corrected for dust extinction (mwebv), though heavily extincted \"\n            \"objects will have larger uncertainties (flux_err) in spite of \"\n            \"the correction.\",\n            name_extended=\"flux (brightness)\"),\n    Feature('flux_err', float,\n            \"the uncertainty on the measurement of the flux listed above.\",\n            name_extended=\"uncertainty of flux measurement\"),\n    Feature('detected', int,\n            \"If 1, the object's brightness is significantly different at the \"\n            \"3-sigma level relative to the reference template. Only objects \"\n            \"with at least 2 detections are included in the dataset.\"),\n    Feature('ra', float,\n            \"right ascension, sky coordinate: co-longitude in degrees\",\n            name_extended=\"right ascension, sky coordinate: co-longitude in degrees\"),\n    Feature('decl', float,\n            \"declination, sky coordinate: co-latitude in degrees\",\n            name_extended=\"declination, sky coordinate: co-latitude in degrees\"),\n    Feature('gal_l', float,\n            name_extended=\"galactic longitude in degrees\"),\n    Feature('gal_b', float,\n            name_extended=\"galactic latitude in degrees\"),\n    Feature('ddf', int,\n            \"A flag to identify the object as coming from the DDF survey area \"\n            \"(with value DDF = 1 for the DDF, DDF = 0 for the WFD survey). \"\n            \"Note that while the DDF fields are contained within the full WFD \"\n            \"survey area, the DDF fluxes have significantly smaller \"\n            \"uncertainties\",\n            name_extended=\"indocator for object is from DDF survey area\"),\n    Feature('hostgal_specz', float,\n            \"the spectroscopic redshift of the source. This is an extremely \"\n            \"accurate measure of redshift, available for the training set and \"\n            \"a small fraction of the test set.\",\n            name_extended=\"spectroscopic redshift\"),\n    Feature('hostgal_photoz', float,\n            \"The photometric redshift of the host galaxy of the astronomical \"\n            \"source. While this is meant to be a proxy for hostgal_specz, \"\n            \"there can be large differences between the two and should be \"\n            \"regarded as a far less accurate version of hostgal_specz.\",\n            name_extended=\"photometric redshift of the host galaxy of the astronomical source\"),\n    Feature('hostgal_photoz_err', float,\n            \"The uncertainty on the hostgal_photoz based on LSST survey projections\",\n            name_extended=\"uncertainty of the photometric redshift\"),\n    Feature('distmod', float,\n            \"The distance to the source calculated from hostgal_photoz and using general relativity\",\n            name_extended=\"distance to the source calculated from photometric \"\n                          \"redshift and using general relativity\"),\n    Feature('mwebv', float,\n            \"this ‘extinction’ of light is a property of the Milky Way (MW) \"\n            \"dust along the line of sight to the astronomical source, and is \"\n            \"thus a function of the sky coordinates of the source ra, \"\n            \"decl. This is used to determine a passband dependent dimming and \"\n            \"redenning of light from astronomical sources as described in \"\n            \"subsection 2.1, and based on the Schlafly et al. (2011) and \"\n            \"Schlegel et al. (1998) dust models.\"),\n    Feature('target', int, is_target=True,\n            name_extended=\"class of the astronomical source\"),\n], documentation=\"https://www.kaggle.com/competitions/PLAsTiCC-2018/data\")\n\nWALMART_FEATURES = FeatureList(features=[\n    Feature('TripType', int, is_target=True,\n            name_extended='id representing the type of shopping trip the customer made'),\n    Feature('VisitNumber', int,\n            'an id corresponding to a single trip by a single customer',\n            name_extended='unique visit ID'),\n    Feature('Weekday', cat_dtype, name_extended=\"weekday of the trip\"),\n    Feature('Upc', float, name_extended=\"UPC number of the product purchased\"),\n    Feature('ScanCount', int,\n            \"the number of the given item that was purchased. A negative value indicates a product return.\",\n            name_extended=\"number of the given item purchased\"),\n    Feature('DepartmentDescription', object,\n            name_extended=\"product department description\"),\n    Feature('FinelineNumber', float, name_extended=\"product fine line number\"),\n],\n    documentation=\"https://www.kaggle.com/competitions/walmart-recruiting-trip-type-classification/data\")\n\nTRADESHIFT_FEATURES = FeatureList(features=[\n    Feature('y33', int, is_target=True),\n    Feature('x140', cat_dtype),  # importance: 0.2476\n    Feature('x131', float),  # importance: 0.1324\n    Feature('x24', cat_dtype),  # importance: 0.1102\n    Feature('x126', cat_dtype),  # importance: 0.091\n    Feature('x115', cat_dtype),  # importance: 0.0714\n    Feature('x9', float),  # importance: 0.042\n    Feature('x54', float),  # importance: 0.0418\n    Feature('x114', float),  # importance: 0.0321\n    Feature('x61', cat_dtype),  # importance: 0.0242\n    Feature('x31', cat_dtype),  # importance: 0.022\n    Feature('x91', cat_dtype),  # importance: 0.0159\n    Feature('x109', float),  # importance: 0.0114\n    Feature('x4', cat_dtype),  # importance: 0.0101\n    Feature('x141', cat_dtype),  # importance: 0.0101\n    Feature('x136', float),  # importance: 0.01\n    Feature('x132', float),  # importance: 0.0083\n    Feature('x130', cat_dtype),  # importance: 0.0082\n    Feature('x95', cat_dtype),  # importance: 0.0081\n    Feature('x94', cat_dtype),  # importance: 0.0078\n    Feature('x3', cat_dtype),  # importance: 0.0077\n    Feature('x118', float),  # importance: 0.0072\n    Feature('x143', float),  # importance: 0.0062\n    Feature('x27', float),  # importance: 0.0059\n    Feature('x120', float),  # importance: 0.0053\n    Feature('x18', float),  # importance: 0.005\n    Feature('x133', float),  # importance: 0.005\n    Feature('x23', float),  # importance: 0.0047\n    Feature('x34', cat_dtype),  # importance: 0.0046\n    Feature('x84', float),  # importance: 0.0046\n    Feature('x64', cat_dtype),  # importance: 0.0037\n    Feature('x100', float),  # importance: 0.003\n    Feature('x135', float),  # importance: 0.0029\n    Feature('x106', float),  # importance: 0.0022\n    ##################################################\n    ##################################################\n    # Feature('x145', float),  # importance: 0.0022\n    # Feature('x134', float),  # importance: 0.0017\n    # Feature('x65', cat_dtype),  # importance: 0.0016\n    # Feature('x89', float),  # importance: 0.0015\n    # Feature('x50', float),  # importance: 0.0015\n    # Feature('x56', cat_dtype),  # importance: 0.0013\n    # Feature('x123', float),  # importance: 0.0012\n    # Feature('x40', float),  # importance: 0.0011\n    # Feature('x30', cat_dtype),  # importance: 0.0011\n    # Feature('x17', float),  # importance: 0.001\n    # Feature('x35', cat_dtype),  # importance: 0.0009\n    # Feature('x112', float),  # importance: 0.0008\n    # Feature('x60', float),  # importance: 0.0005\n    # Feature('x116', cat_dtype),  # importance: 0.0004\n    # Feature('x21', float),  # importance: 0.0004\n    # Feature('x83', float),  # importance: 0.0004\n    # Feature('x137', float),  # importance: 0.0004\n    # Feature('x28', float),  # importance: 0.0004\n    # Feature('x5', float),  # importance: 0.0004\n    # Feature('x90', float),  # importance: 0.0004\n    # Feature('x38', float),  # importance: 0.0003\n    # Feature('x22', float),  # importance: 0.0003\n    # Feature('x113', float),  # importance: 0.0002\n    # Feature('x6', float),  # importance: 0.0002\n    # Feature('x16', float),  # importance: 0.0002\n    # Feature('x104', cat_dtype),  # importance: 0.0002\n    # Feature('x79', float),  # importance: 0.0002\n    # Feature('x117', cat_dtype),  # importance: 0.0002\n    # Feature('x70', float),  # importance: 0.0002\n    # Feature('x82', float),  # importance: 0.0002\n    # Feature('x144', float),  # importance: 0.0002\n    # Feature('x2', cat_dtype),  # importance: 0.0001\n    # Feature('x47', float),  # importance: 0.0001\n    # Feature('x53', float),  # importance: 0.0001\n    # Feature('x129', cat_dtype),  # importance: 0.0001\n    # Feature('x62', cat_dtype),  # importance: 0.0001\n    # Feature('x119', float),  # importance: 0.0001\n    # Feature('x101', cat_dtype),  # importance: 0.0001\n    # Feature('x49', float),  # importance: 0.0001\n    # Feature('x110', float),  # importance: 0.0001\n    # Feature('x105', cat_dtype),  # importance: 0.0001\n    # Feature('x68', float),  # importance: 0.0001\n    # Feature('x46', float),  # importance: 0.0001\n    # Feature('x14', cat_dtype),  # importance: 0.0001\n    # Feature('x15', float),  # importance: 0.0001\n    # Feature('x37', float),  # importance: 0.0001\n    # Feature('x121', float),  # importance: 0.0001\n    # Feature('x29', float),  # importance: 0.0001\n    # Feature('x125', float),  # importance: 0.0001\n    # Feature('x66', float),  # importance: 0.0001\n    # Feature('x92', cat_dtype),  # importance: 0.0001\n    # Feature('x128', cat_dtype),  # importance: 0.0001\n    # Feature('x20', float),  # importance: 0.0001\n    # Feature('x76', float),  # importance: 0.0001\n    # Feature('x48', float),  # importance: 0.0001\n    # Feature('x63', cat_dtype),  # importance: 0.0001\n    # Feature('x77', float),  # importance: 0.0001\n    # Feature('x1', cat_dtype),  # importance: 0.0001\n    # Feature('x98', float),  # importance: 0.0001\n    # Feature('x8', float),  # importance: 0.0001\n    # Feature('x43', cat_dtype),  # importance: 0.0001\n    # Feature('x139', float),  # importance: 0.0001\n    # Feature('x96', float),  # importance: 0.0001\n    # Feature('x51', float),  # importance: 0.0001\n    # Feature('x86', cat_dtype),  # importance: 0.0001\n    # Feature('x107', float),  # importance: 0.0001\n    # Feature('x59', float),  # importance: 0.0001\n    # Feature('x80', float),  # importance: 0.0001\n    # Feature('x36', float),  # importance: 0.0001\n    # Feature('x13', cat_dtype),  # importance: 0.0001\n    # Feature('x32', cat_dtype),  # importance: 0.0001\n    # Feature('x138', float),  # importance: 0.0001\n    # Feature('x67', float),  # importance: 0.0001\n    # Feature('x19', float),  # importance: 0.0001\n    # Feature('x69', float),  # importance: 0.0001\n    # Feature('x7', float),  # importance: 0.0001\n    # Feature('x10', cat_dtype),  # importance: 0.0001\n    # Feature('x39', float),  # importance: 0.0001\n    # Feature('x78', float),  # importance: 0.0001\n    # Feature('x88', float),  # importance: 0.0001\n    # Feature('x81', float),  # importance: 0.0001\n    # Feature('x55', cat_dtype),  # importance: 0.0001\n    # Feature('x103', cat_dtype),  # importance: 0.0001\n    # Feature('x97', float),  # importance: 0.0001\n    # Feature('x11', cat_dtype),  # importance: 0.0\n    # Feature('x58', float),  # importance: 0.0\n    # Feature('x142', cat_dtype),  # importance: 0.0\n    # Feature('x108', float),  # importance: 0.0\n    # Feature('x93', cat_dtype),  # importance: 0.0\n    # Feature('x99', float),  # importance: 0.0\n    # Feature('x52', float),  # importance: 0.0\n    # Feature('id', float),  # importance: 0.0\n    # Feature('x124', float),  # importance: 0.0\n    # Feature('x41', cat_dtype),  # importance: 0.0\n    # Feature('x111', float),  # importance: 0.0\n    # Feature('x75', cat_dtype),  # importance: 0.0\n    # Feature('x87', cat_dtype),  # importance: 0.0\n    # Feature('x73', cat_dtype),  # importance: 0.0\n    # Feature('x45', cat_dtype),  # importance: 0.0\n    # Feature('x25', cat_dtype),  # importance: 0.0\n    # Feature('x72', cat_dtype),  # importance: 0.0\n    # Feature('x71', cat_dtype),  # importance: 0.0\n    # Feature('x26', cat_dtype),  # importance: 0.0\n    # Feature('x57', cat_dtype),  # importance: 0.0\n    # Feature('x127', cat_dtype),  # importance: 0.0\n    # Feature('x122', float),  # importance: 0.0\n    # Feature('x12', cat_dtype),  # importance: 0.0\n    # Feature('x85', cat_dtype),  # importance: 0.0\n    # Feature('x102', cat_dtype),  # importance: 0.0\n    # Feature('x33', cat_dtype),  # importance: 0.0\n    # Feature('x74', cat_dtype),  # importance: 0.0\n    # Feature('x44', cat_dtype),  # importance: 0.0\n    # Feature('x42', cat_dtype),  # importance: 0.0\n],\n    documentation=\"https://www.kaggle.com/competitions/tradeshift-text-classification/data\")\n\nSCHIZOPHRENIA_FEATURES = FeatureList(features=[\n    Feature('Class', int, is_target=True,\n                value_mapping={0: 'Healthy Control',\n                               1: 'Schizophrenic Patient'}\n            ),\n    Feature('FNC233', float, 'functional MRI feature 233'),  # importance: 0.1325\n    Feature('FNC237', float, 'functional MRI feature 237'),  # importance: 0.0963\n    Feature('FNC226', float, 'functional MRI feature 226'),  # importance: 0.0812\n    Feature('FNC194', float, 'functional MRI feature 194'),  # importance: 0.0578\n    Feature('SBM_map7', float, 'structural ICA map feature 7'),  # importance: 0.0549\n    Feature('FNC33', float, 'functional MRI feature 33'),  # importance: 0.0538\n    Feature('SBM_map36', float, 'structural ICA map feature 36'),  # importance: 0.0468\n    Feature('FNC105', float, 'functional MRI feature 105'),  # importance: 0.0465\n    Feature('FNC161', float, 'functional MRI feature 161'),  # importance: 0.038\n    Feature('FNC353', float, 'functional MRI feature 353'),  # importance: 0.0371\n    Feature('FNC290', float, 'functional MRI feature 290'),  # importance: 0.0353\n    Feature('FNC80', float, 'functional MRI feature 80'),  # importance: 0.0299\n    Feature('FNC243', float, 'functional MRI feature 243'),  # importance: 0.0277\n    Feature('FNC345', float, 'functional MRI feature 345'),  # importance: 0.025\n    Feature('FNC293', float, 'functional MRI feature 293'),  # importance: 0.022\n    Feature('FNC158', float, 'functional MRI feature 158'),  # importance: 0.0201\n    Feature('FNC48', float, 'functional MRI feature 48'),  # importance: 0.0196\n    Feature('FNC110', float, 'functional MRI feature 110'),  # importance: 0.0175\n    Feature('FNC270', float, 'functional MRI feature 270'),  # importance: 0.017\n    Feature('FNC295', float, 'functional MRI feature 295'),  # importance: 0.0158\n    Feature('FNC301', float, 'functional MRI feature 301'),  # importance: 0.0136\n    Feature('SBM_map67', float, 'structural ICA map feature 67'),  # importance: 0.0134\n    Feature('FNC75', float, 'functional MRI feature 75'),  # importance: 0.0113\n    Feature('FNC337', float, 'functional MRI feature 337'),  # importance: 0.0079\n    Feature('FNC67', float, 'functional MRI feature 67'),  # importance: 0.0076\n    Feature('SBM_map17', float, 'structural ICA map feature 17'),  # importance: 0.0074\n    Feature('FNC165', float, 'functional MRI feature 165'),  # importance: 0.0068\n    Feature('FNC61', float, 'functional MRI feature 61'),  # importance: 0.0068\n    Feature('FNC278', float, 'functional MRI feature 278'),  # importance: 0.0067\n    Feature('FNC208', float, 'functional MRI feature 208'),  # importance: 0.0066\n    Feature('FNC136', float, 'functional MRI feature 136'),  # importance: 0.0065\n    Feature('FNC5', float, 'functional MRI feature 5'),  # importance: 0.0049\n    Feature('FNC189', float, 'functional MRI feature 189'),  # importance: 0.0047\n    ##################################################\n    ##################################################\n    # Feature('FNC232', float, 'functional MRI feature 232'),  # importance: 0.0043\n    # Feature('FNC376', float, 'functional MRI feature 376'),  # importance: 0.0041\n    # Feature('SBM_map13', float, 'structural ICA map feature 13'),  # importance: 0.0035\n    # Feature('FNC167', float, 'functional MRI feature 167'),  # importance: 0.0032\n    # Feature('FNC263', float, 'functional MRI feature 263'),  # importance: 0.0025\n    # Feature('FNC20', float, 'functional MRI feature 20'),  # importance: 0.0025\n    # Feature('FNC357', float, 'functional MRI feature 357'),  # importance: 0.0007\n    # Feature('FNC352', float, 'functional MRI feature 352'),  # importance: 0.0\n    # Feature('FNC39', float, 'functional MRI feature 39'),  # importance: 0.0\n    # Feature('FNC229', float, 'functional MRI feature 229'),  # importance: 0.0\n    # Feature('FNC129', float, 'functional MRI feature 129'),  # importance: 0.0\n    # Feature('SBM_map71', float, 'structural ICA map feature 71'),  # importance: 0.0\n    # Feature('FNC296', float, 'functional MRI feature 296'),  # importance: 0.0\n    # Feature('FNC209', float, 'functional MRI feature 209'),  # importance: 0.0\n    # Feature('FNC305', float, 'functional MRI feature 305'),  # importance: 0.0\n    # Feature('SBM_map26', float, 'structural ICA map feature 26'),  # importance: 0.0\n    # Feature('FNC211', float, 'functional MRI feature 211'),  # importance: 0.0\n    # Feature('FNC144', float, 'functional MRI feature 144'),  # importance: 0.0\n    # Feature('FNC362', float, 'functional MRI feature 362'),  # importance: 0.0\n    # Feature('FNC22', float, 'functional MRI feature 22'),  # importance: 0.0\n    # Feature('FNC182', float, 'functional MRI feature 182'),  # importance: 0.0\n    # Feature('FNC2', float, 'functional MRI feature 2'),  # importance: 0.0\n    # Feature('FNC107', float, 'functional MRI feature 107'),  # importance: 0.0\n    # Feature('FNC98', float, 'functional MRI feature 98'),  # importance: 0.0\n    # Feature('FNC145', float, 'functional MRI feature 145'),  # importance: 0.0\n    # Feature('FNC43', float, 'functional MRI feature 43'),  # importance: 0.0\n    # Feature('FNC162', float, 'functional MRI feature 162'),  # importance: 0.0\n    # Feature('FNC84', float, 'functional MRI feature 84'),  # importance: 0.0\n    # Feature('FNC9', float, 'functional MRI feature 9'),  # importance: 0.0\n    # Feature('FNC283', float, 'functional MRI feature 283'),  # importance: 0.0\n    # Feature('FNC146', float, 'functional MRI feature 146'),  # importance: 0.0\n    # Feature('FNC60', float, 'functional MRI feature 60'),  # importance: 0.0\n    # Feature('FNC1', float, 'functional MRI feature 1'),  # importance: 0.0\n    # Feature('FNC184', float, 'functional MRI feature 184'),  # importance: 0.0\n    # Feature('FNC176', float, 'functional MRI feature 176'),  # importance: 0.0\n    # Feature('SBM_map51', float, 'structural ICA map feature 51'),  # importance: 0.0\n    # Feature('FNC222', float, 'functional MRI feature 222'),  # importance: 0.0\n    # Feature('FNC56', float, 'functional MRI feature 56'),  # importance: 0.0\n    # Feature('FNC254', float, 'functional MRI feature 254'),  # importance: 0.0\n    # Feature('FNC282', float, 'functional MRI feature 282'),  # importance: 0.0\n    # Feature('FNC78', float, 'functional MRI feature 78'),  # importance: 0.0\n    # Feature('FNC50', float, 'functional MRI feature 50'),  # importance: 0.0\n    # Feature('FNC6', float, 'functional MRI feature 6'),  # importance: 0.0\n    # Feature('FNC186', float, 'functional MRI feature 186'),  # importance: 0.0\n    # Feature('FNC331', float, 'functional MRI feature 331'),  # importance: 0.0\n    # Feature('FNC168', float, 'functional MRI feature 168'),  # importance: 0.0\n    # Feature('FNC45', float, 'functional MRI feature 45'),  # importance: 0.0\n    # Feature('FNC69', float, 'functional MRI feature 69'),  # importance: 0.0\n    # Feature('FNC73', float, 'functional MRI feature 73'),  # importance: 0.0\n    # Feature('FNC117', float, 'functional MRI feature 117'),  # importance: 0.0\n    # Feature('FNC173', float, 'functional MRI feature 173'),  # importance: 0.0\n    # Feature('FNC109', float, 'functional MRI feature 109'),  # importance: 0.0\n    # Feature('FNC319', float, 'functional MRI feature 319'),  # importance: 0.0\n    # Feature('FNC7', float, 'functional MRI feature 7'),  # importance: 0.0\n    # Feature('FNC215', float, 'functional MRI feature 215'),  # importance: 0.0\n    # Feature('FNC302', float, 'functional MRI feature 302'),  # importance: 0.0\n    # Feature('FNC36', float, 'functional MRI feature 36'),  # importance: 0.0\n    # Feature('FNC174', float, 'functional MRI feature 174'),  # importance: 0.0\n    # Feature('FNC90', float, 'functional MRI feature 90'),  # importance: 0.0\n    # Feature('FNC225', float, 'functional MRI feature 225'),  # importance: 0.0\n    # Feature('FNC212', float, 'functional MRI feature 212'),  # importance: 0.0\n    # Feature('FNC77', float, 'functional MRI feature 77'),  # importance: 0.0\n    # Feature('SBM_map45', float, 'structural ICA map feature 45'),  # importance: 0.0\n    # Feature('FNC371', float, 'functional MRI feature 371'),  # importance: 0.0\n    # Feature('FNC370', float, 'functional MRI feature 370'),  # importance: 0.0\n    # Feature('FNC157', float, 'functional MRI feature 157'),  # importance: 0.0\n    # Feature('SBM_map1', float, 'structural ICA map feature 1'),  # importance: 0.0\n    # Feature('FNC3', float, 'functional MRI feature 3'),  # importance: 0.0\n    # Feature('FNC164', float, 'functional MRI feature 164'),  # importance: 0.0\n    # Feature('FNC128', float, 'functional MRI feature 128'),  # importance: 0.0\n    # Feature('FNC231', float, 'functional MRI feature 231'),  # importance: 0.0\n    # Feature('FNC286', float, 'functional MRI feature 286'),  # importance: 0.0\n    # Feature('FNC138', float, 'functional MRI feature 138'),  # importance: 0.0\n    # Feature('FNC193', float, 'functional MRI feature 193'),  # importance: 0.0\n    # Feature('SBM_map8', float, 'structural ICA map feature 8'),  # importance: 0.0\n    # Feature('FNC369', float, 'functional MRI feature 369'),  # importance: 0.0\n    # Feature('FNC299', float, 'functional MRI feature 299'),  # importance: 0.0\n    # Feature('SBM_map2', float, 'structural ICA map feature 2'),  # importance: 0.0\n    # Feature('FNC130', float, 'functional MRI feature 130'),  # importance: 0.0\n    # Feature('FNC126', float, 'functional MRI feature 126'),  # importance: 0.0\n    # Feature('FNC197', float, 'functional MRI feature 197'),  # importance: 0.0\n    # Feature('FNC111', float, 'functional MRI feature 111'),  # importance: 0.0\n    # Feature('FNC312', float, 'functional MRI feature 312'),  # importance: 0.0\n    # Feature('FNC142', float, 'functional MRI feature 142'),  # importance: 0.0\n    # Feature('FNC300', float, 'functional MRI feature 300'),  # importance: 0.0\n    # Feature('FNC85', float, 'functional MRI feature 85'),  # importance: 0.0\n    # Feature('FNC104', float, 'functional MRI feature 104'),  # importance: 0.0\n    # Feature('FNC242', float, 'functional MRI feature 242'),  # importance: 0.0\n    # Feature('FNC114', float, 'functional MRI feature 114'),  # importance: 0.0\n    # Feature('FNC342', float, 'functional MRI feature 342'),  # importance: 0.0\n    # Feature('SBM_map55', float, 'structural ICA map feature 55'),  # importance: 0.0\n    # Feature('FNC21', float, 'functional MRI feature 21'),  # importance: 0.0\n    # Feature('FNC15', float, 'functional MRI feature 15'),  # importance: 0.0\n    # Feature('FNC235', float, 'functional MRI feature 235'),  # importance: 0.0\n    # Feature('FNC99', float, 'functional MRI feature 99'),  # importance: 0.0\n    # Feature('FNC47', float, 'functional MRI feature 47'),  # importance: 0.0\n    # Feature('FNC274', float, 'functional MRI feature 274'),  # importance: 0.0\n    # Feature('FNC304', float, 'functional MRI feature 304'),  # importance: 0.0\n    # Feature('FNC100', float, 'functional MRI feature 100'),  # importance: 0.0\n    # Feature('FNC344', float, 'functional MRI feature 344'),  # importance: 0.0\n    # Feature('FNC310', float, 'functional MRI feature 310'),  # importance: 0.0\n    # Feature('FNC377', float, 'functional MRI feature 377'),  # importance: 0.0\n    # Feature('FNC248', float, 'functional MRI feature 248'),  # importance: 0.0\n    # Feature('FNC292', float, 'functional MRI feature 292'),  # importance: 0.0\n    # Feature('FNC323', float, 'functional MRI feature 323'),  # importance: 0.0\n    # Feature('FNC249', float, 'functional MRI feature 249'),  # importance: 0.0\n    # Feature('FNC124', float, 'functional MRI feature 124'),  # importance: 0.0\n    # Feature('FNC332', float, 'functional MRI feature 332'),  # importance: 0.0\n    # Feature('FNC350', float, 'functional MRI feature 350'),  # importance: 0.0\n    # Feature('FNC64', float, 'functional MRI feature 64'),  # importance: 0.0\n    # Feature('FNC375', float, 'functional MRI feature 375'),  # importance: 0.0\n    # Feature('Id', float),  # importance: 0.0\n    # Feature('FNC76', float, 'functional MRI feature 76'),  # importance: 0.0\n    # Feature('FNC70', float, 'functional MRI feature 70'),  # importance: 0.0\n    # Feature('FNC351', float, 'functional MRI feature 351'),  # importance: 0.0\n    # Feature('FNC218', float, 'functional MRI feature 218'),  # importance: 0.0\n    # Feature('FNC199', float, 'functional MRI feature 199'),  # importance: 0.0\n    # Feature('FNC55', float, 'functional MRI feature 55'),  # importance: 0.0\n    # Feature('FNC59', float, 'functional MRI feature 59'),  # importance: 0.0\n    # Feature('FNC267', float, 'functional MRI feature 267'),  # importance: 0.0\n    # Feature('FNC178', float, 'functional MRI feature 178'),  # importance: 0.0\n    # Feature('FNC143', float, 'functional MRI feature 143'),  # importance: 0.0\n    # Feature('FNC269', float, 'functional MRI feature 269'),  # importance: 0.0\n    # Feature('FNC32', float, 'functional MRI feature 32'),  # importance: 0.0\n    # Feature('FNC281', float, 'functional MRI feature 281'),  # importance: 0.0\n    # Feature('FNC373', float, 'functional MRI feature 373'),  # importance: 0.0\n    # Feature('FNC303', float, 'functional MRI feature 303'),  # importance: 0.0\n    # Feature('FNC236', float, 'functional MRI feature 236'),  # importance: 0.0\n    # Feature('FNC185', float, 'functional MRI feature 185'),  # importance: 0.0\n    # Feature('SBM_map32', float, 'structural ICA map feature 32'),  # importance: 0.0\n    # Feature('FNC285', float, 'functional MRI feature 285'),  # importance: 0.0\n    # Feature('FNC10', float, 'functional MRI feature 10'),  # importance: 0.0\n    # Feature('SBM_map52', float, 'structural ICA map feature 52'),  # importance: 0.0\n    # Feature('FNC359', float, 'functional MRI feature 359'),  # importance: 0.0\n    # Feature('FNC53', float, 'functional MRI feature 53'),  # importance: 0.0\n    # Feature('FNC150', float, 'functional MRI feature 150'),  # importance: 0.0\n    # Feature('FNC205', float, 'functional MRI feature 205'),  # importance: 0.0\n    # Feature('FNC82', float, 'functional MRI feature 82'),  # importance: 0.0\n    # Feature('FNC72', float, 'functional MRI feature 72'),  # importance: 0.0\n    # Feature('FNC37', float, 'functional MRI feature 37'),  # importance: 0.0\n    # Feature('FNC35', float, 'functional MRI feature 35'),  # importance: 0.0\n    # Feature('FNC246', float, 'functional MRI feature 246'),  # importance: 0.0\n    # Feature('FNC361', float, 'functional MRI feature 361'),  # importance: 0.0\n    # Feature('FNC333', float, 'functional MRI feature 333'),  # importance: 0.0\n    # Feature('FNC374', float, 'functional MRI feature 374'),  # importance: 0.0\n    # Feature('FNC207', float, 'functional MRI feature 207'),  # importance: 0.0\n    # Feature('FNC135', float, 'functional MRI feature 135'),  # importance: 0.0\n    # Feature('FNC95', float, 'functional MRI feature 95'),  # importance: 0.0\n    # Feature('FNC160', float, 'functional MRI feature 160'),  # importance: 0.0\n    # Feature('FNC42', float, 'functional MRI feature 42'),  # importance: 0.0\n    # Feature('FNC31', float, 'functional MRI feature 31'),  # importance: 0.0\n    # Feature('FNC201', float, 'functional MRI feature 201'),  # importance: 0.0\n    # Feature('FNC358', float, 'functional MRI feature 358'),  # importance: 0.0\n    # Feature('FNC206', float, 'functional MRI feature 206'),  # importance: 0.0\n    # Feature('FNC24', float, 'functional MRI feature 24'),  # importance: 0.0\n    # Feature('FNC87', float, 'functional MRI feature 87'),  # importance: 0.0\n    # Feature('FNC336', float, 'functional MRI feature 336'),  # importance: 0.0\n    # Feature('FNC13', float, 'functional MRI feature 13'),  # importance: 0.0\n    # Feature('FNC297', float, 'functional MRI feature 297'),  # importance: 0.0\n    # Feature('FNC288', float, 'functional MRI feature 288'),  # importance: 0.0\n    # Feature('FNC147', float, 'functional MRI feature 147'),  # importance: 0.0\n    # Feature('FNC329', float, 'functional MRI feature 329'),  # importance: 0.0\n    # Feature('FNC202', float, 'functional MRI feature 202'),  # importance: 0.0\n    # Feature('FNC325', float, 'functional MRI feature 325'),  # importance: 0.0\n    # Feature('FNC343', float, 'functional MRI feature 343'),  # importance: 0.0\n    # Feature('FNC321', float, 'functional MRI feature 321'),  # importance: 0.0\n    # Feature('FNC378', float, 'functional MRI feature 378'),  # importance: 0.0\n    # Feature('FNC28', float, 'functional MRI feature 28'),  # importance: 0.0\n    # Feature('FNC311', float, 'functional MRI feature 311'),  # importance: 0.0\n    # Feature('FNC308', float, 'functional MRI feature 308'),  # importance: 0.0\n    # Feature('FNC102', float, 'functional MRI feature 102'),  # importance: 0.0\n    # Feature('FNC307', float, 'functional MRI feature 307'),  # importance: 0.0\n    # Feature('FNC250', float, 'functional MRI feature 250'),  # importance: 0.0\n    # Feature('FNC188', float, 'functional MRI feature 188'),  # importance: 0.0\n    # Feature('SBM_map6', float, 'structural ICA map feature 6'),  # importance: 0.0\n    # Feature('SBM_map61', float, 'structural ICA map feature 61'),  # importance: 0.0\n    # Feature('FNC91', float, 'functional MRI feature 91'),  # importance: 0.0\n    # Feature('FNC347', float, 'functional MRI feature 347'),  # importance: 0.0\n    # Feature('FNC262', float, 'functional MRI feature 262'),  # importance: 0.0\n    # Feature('FNC169', float, 'functional MRI feature 169'),  # importance: 0.0\n    # Feature('FNC118', float, 'functional MRI feature 118'),  # importance: 0.0\n    # Feature('FNC190', float, 'functional MRI feature 190'),  # importance: 0.0\n    # Feature('FNC68', float, 'functional MRI feature 68'),  # importance: 0.0\n    # Feature('FNC314', float, 'functional MRI feature 314'),  # importance: 0.0\n    # Feature('FNC348', float, 'functional MRI feature 348'),  # importance: 0.0\n    # Feature('FNC210', float, 'functional MRI feature 210'),  # importance: 0.0\n    # Feature('FNC217', float, 'functional MRI feature 217'),  # importance: 0.0\n    # Feature('FNC4', float, 'functional MRI feature 4'),  # importance: 0.0\n    # Feature('FNC94', float, 'functional MRI feature 94'),  # importance: 0.0\n    # Feature('FNC132', float, 'functional MRI feature 132'),  # importance: 0.0\n    # Feature('FNC172', float, 'functional MRI feature 172'),  # importance: 0.0\n    # Feature('FNC83', float, 'functional MRI feature 83'),  # importance: 0.0\n    # Feature('FNC356', float, 'functional MRI feature 356'),  # importance: 0.0\n    # Feature('FNC317', float, 'functional MRI feature 317'),  # importance: 0.0\n    # Feature('FNC239', float, 'functional MRI feature 239'),  # importance: 0.0\n    # Feature('FNC272', float, 'functional MRI feature 272'),  # importance: 0.0\n    # Feature('SBM_map43', float, 'structural ICA map feature 43'),  # importance: 0.0\n    # Feature('FNC341', float, 'functional MRI feature 341'),  # importance: 0.0\n    # Feature('FNC192', float, 'functional MRI feature 192'),  # importance: 0.0\n    # Feature('FNC113', float, 'functional MRI feature 113'),  # importance: 0.0\n    # Feature('FNC58', float, 'functional MRI feature 58'),  # importance: 0.0\n    # Feature('FNC221', float, 'functional MRI feature 221'),  # importance: 0.0\n    # Feature('FNC71', float, 'functional MRI feature 71'),  # importance: 0.0\n    # Feature('FNC334', float, 'functional MRI feature 334'),  # importance: 0.0\n    # Feature('FNC79', float, 'functional MRI feature 79'),  # importance: 0.0\n    # Feature('FNC251', float, 'functional MRI feature 251'),  # importance: 0.0\n    # Feature('FNC141', float, 'functional MRI feature 141'),  # importance: 0.0\n    # Feature('FNC200', float, 'functional MRI feature 200'),  # importance: 0.0\n    # Feature('FNC177', float, 'functional MRI feature 177'),  # importance: 0.0\n    # Feature('FNC322', float, 'functional MRI feature 322'),  # importance: 0.0\n    # Feature('FNC220', float, 'functional MRI feature 220'),  # importance: 0.0\n    # Feature('FNC93', float, 'functional MRI feature 93'),  # importance: 0.0\n    # Feature('FNC8', float, 'functional MRI feature 8'),  # importance: 0.0\n    # Feature('FNC170', float, 'functional MRI feature 170'),  # importance: 0.0\n    # Feature('FNC134', float, 'functional MRI feature 134'),  # importance: 0.0\n    # Feature('FNC44', float, 'functional MRI feature 44'),  # importance: 0.0\n    # Feature('FNC234', float, 'functional MRI feature 234'),  # importance: 0.0\n    # Feature('FNC277', float, 'functional MRI feature 277'),  # importance: 0.0\n    # Feature('FNC223', float, 'functional MRI feature 223'),  # importance: 0.0\n    # Feature('SBM_map28', float, 'structural ICA map feature 28'),  # importance: 0.0\n    # Feature('FNC25', float, 'functional MRI feature 25'),  # importance: 0.0\n    # Feature('FNC152', float, 'functional MRI feature 152'),  # importance: 0.0\n    # Feature('FNC196', float, 'functional MRI feature 196'),  # importance: 0.0\n    # Feature('FNC115', float, 'functional MRI feature 115'),  # importance: 0.0\n    # Feature('FNC155', float, 'functional MRI feature 155'),  # importance: 0.0\n    # Feature('FNC309', float, 'functional MRI feature 309'),  # importance: 0.0\n    # Feature('FNC338', float, 'functional MRI feature 338'),  # importance: 0.0\n    # Feature('FNC180', float, 'functional MRI feature 180'),  # importance: 0.0\n    # Feature('FNC367', float, 'functional MRI feature 367'),  # importance: 0.0\n    # Feature('FNC255', float, 'functional MRI feature 255'),  # importance: 0.0\n    # Feature('FNC137', float, 'functional MRI feature 137'),  # importance: 0.0\n    # Feature('FNC62', float, 'functional MRI feature 62'),  # importance: 0.0\n    # Feature('FNC51', float, 'functional MRI feature 51'),  # importance: 0.0\n    # Feature('FNC14', float, 'functional MRI feature 14'),  # importance: 0.0\n    # Feature('FNC54', float, 'functional MRI feature 54'),  # importance: 0.0\n    # Feature('FNC224', float, 'functional MRI feature 224'),  # importance: 0.0\n    # Feature('FNC198', float, 'functional MRI feature 198'),  # importance: 0.0\n    # Feature('FNC363', float, 'functional MRI feature 363'),  # importance: 0.0\n    # Feature('FNC203', float, 'functional MRI feature 203'),  # importance: 0.0\n    # Feature('FNC227', float, 'functional MRI feature 227'),  # importance: 0.0\n    # Feature('SBM_map64', float, 'structural ICA map feature 64'),  # importance: 0.0\n    # Feature('FNC291', float, 'functional MRI feature 291'),  # importance: 0.0\n    # Feature('FNC258', float, 'functional MRI feature 258'),  # importance: 0.0\n    # Feature('FNC256', float, 'functional MRI feature 256'),  # importance: 0.0\n    # Feature('FNC213', float, 'functional MRI feature 213'),  # importance: 0.0\n    # Feature('FNC204', float, 'functional MRI feature 204'),  # importance: 0.0\n    # Feature('FNC57', float, 'functional MRI feature 57'),  # importance: 0.0\n    # Feature('FNC16', float, 'functional MRI feature 16'),  # importance: 0.0\n    # Feature('FNC238', float, 'functional MRI feature 238'),  # importance: 0.0\n    # Feature('FNC74', float, 'functional MRI feature 74'),  # importance: 0.0\n    # Feature('FNC191', float, 'functional MRI feature 191'),  # importance: 0.0\n    # Feature('FNC276', float, 'functional MRI feature 276'),  # importance: 0.0\n    # Feature('FNC346', float, 'functional MRI feature 346'),  # importance: 0.0\n    # Feature('FNC52', float, 'functional MRI feature 52'),  # importance: 0.0\n    # Feature('FNC97', float, 'functional MRI feature 97'),  # importance: 0.0\n    # Feature('FNC372', float, 'functional MRI feature 372'),  # importance: 0.0\n    # Feature('FNC316', float, 'functional MRI feature 316'),  # importance: 0.0\n    # Feature('FNC253', float, 'functional MRI feature 253'),  # importance: 0.0\n    # Feature('FNC26', float, 'functional MRI feature 26'),  # importance: 0.0\n    # Feature('FNC101', float, 'functional MRI feature 101'),  # importance: 0.0\n    # Feature('FNC89', float, 'functional MRI feature 89'),  # importance: 0.0\n    # Feature('SBM_map74', float, 'structural ICA map feature 74'),  # importance: 0.0\n    # Feature('FNC368', float, 'functional MRI feature 368'),  # importance: 0.0\n    # Feature('FNC179', float, 'functional MRI feature 179'),  # importance: 0.0\n    # Feature('FNC121', float, 'functional MRI feature 121'),  # importance: 0.0\n    # Feature('FNC49', float, 'functional MRI feature 49'),  # importance: 0.0\n    # Feature('FNC355', float, 'functional MRI feature 355'),  # importance: 0.0\n    # Feature('FNC12', float, 'functional MRI feature 12'),  # importance: 0.0\n    # Feature('FNC139', float, 'functional MRI feature 139'),  # importance: 0.0\n    # Feature('FNC116', float, 'functional MRI feature 116'),  # importance: 0.0\n    # Feature('FNC275', float, 'functional MRI feature 275'),  # importance: 0.0\n    # Feature('FNC112', float, 'functional MRI feature 112'),  # importance: 0.0\n    # Feature('FNC81', float, 'functional MRI feature 81'),  # importance: 0.0\n    # Feature('FNC264', float, 'functional MRI feature 264'),  # importance: 0.0\n    # Feature('FNC364', float, 'functional MRI feature 364'),  # importance: 0.0\n    # Feature('FNC271', float, 'functional MRI feature 271'),  # importance: 0.0\n    # Feature('SBM_map40', float, 'structural ICA map feature 40'),  # importance: 0.0\n    # Feature('FNC38', float, 'functional MRI feature 38'),  # importance: 0.0\n    # Feature('FNC153', float, 'functional MRI feature 153'),  # importance: 0.0\n    # Feature('SBM_map73', float, 'structural ICA map feature 73'),  # importance: 0.0\n    # Feature('FNC294', float, 'functional MRI feature 294'),  # importance: 0.0\n    # Feature('FNC328', float, 'functional MRI feature 328'),  # importance: 0.0\n    # Feature('FNC46', float, 'functional MRI feature 46'),  # importance: 0.0\n    # Feature('SBM_map22', float, 'structural ICA map feature 22'),  # importance: 0.0\n    # Feature('FNC339', float, 'functional MRI feature 339'),  # importance: 0.0\n    # Feature('FNC103', float, 'functional MRI feature 103'),  # importance: 0.0\n    # Feature('SBM_map4', float, 'structural ICA map feature 4'),  # importance: 0.0\n    # Feature('FNC287', float, 'functional MRI feature 287'),  # importance: 0.0\n    # Feature('FNC127', float, 'functional MRI feature 127'),  # importance: 0.0\n    # Feature('FNC241', float, 'functional MRI feature 241'),  # importance: 0.0\n    # Feature('FNC140', float, 'functional MRI feature 140'),  # importance: 0.0\n    # Feature('FNC280', float, 'functional MRI feature 280'),  # importance: 0.0\n    # Feature('FNC260', float, 'functional MRI feature 260'),  # importance: 0.0\n    # Feature('FNC88', float, 'functional MRI feature 88'),  # importance: 0.0\n    # Feature('FNC214', float, 'functional MRI feature 214'),  # importance: 0.0\n    # Feature('FNC30', float, 'functional MRI feature 30'),  # importance: 0.0\n    # Feature('FNC86', float, 'functional MRI feature 86'),  # importance: 0.0\n    # Feature('FNC63', float, 'functional MRI feature 63'),  # importance: 0.0\n    # Feature('FNC175', float, 'functional MRI feature 175'),  # importance: 0.0\n    # Feature('FNC340', float, 'functional MRI feature 340'),  # importance: 0.0\n    # Feature('FNC268', float, 'functional MRI feature 268'),  # importance: 0.0\n    # Feature('FNC151', float, 'functional MRI feature 151'),  # importance: 0.0\n    # Feature('SBM_map48', float, 'structural ICA map feature 48'),  # importance: 0.0\n    # Feature('SBM_map3', float, 'structural ICA map feature 3'),  # importance: 0.0\n    # Feature('FNC360', float, 'functional MRI feature 360'),  # importance: 0.0\n    # Feature('FNC259', float, 'functional MRI feature 259'),  # importance: 0.0\n    # Feature('FNC34', float, 'functional MRI feature 34'),  # importance: 0.0\n    # Feature('FNC289', float, 'functional MRI feature 289'),  # importance: 0.0\n    # Feature('FNC326', float, 'functional MRI feature 326'),  # importance: 0.0\n    # Feature('FNC273', float, 'functional MRI feature 273'),  # importance: 0.0\n    # Feature('FNC41', float, 'functional MRI feature 41'),  # importance: 0.0\n    # Feature('FNC266', float, 'functional MRI feature 266'),  # importance: 0.0\n    # Feature('FNC298', float, 'functional MRI feature 298'),  # importance: 0.0\n    # Feature('FNC257', float, 'functional MRI feature 257'),  # importance: 0.0\n    # Feature('FNC240', float, 'functional MRI feature 240'),  # importance: 0.0\n    # Feature('FNC131', float, 'functional MRI feature 131'),  # importance: 0.0\n    # Feature('FNC245', float, 'functional MRI feature 245'),  # importance: 0.0\n    # Feature('FNC366', float, 'functional MRI feature 366'),  # importance: 0.0\n    # Feature('FNC284', float, 'functional MRI feature 284'),  # importance: 0.0\n    # Feature('FNC159', float, 'functional MRI feature 159'),  # importance: 0.0\n    # Feature('FNC120', float, 'functional MRI feature 120'),  # importance: 0.0\n    # Feature('FNC11', float, 'functional MRI feature 11'),  # importance: 0.0\n    # Feature('FNC216', float, 'functional MRI feature 216'),  # importance: 0.0\n    # Feature('FNC156', float, 'functional MRI feature 156'),  # importance: 0.0\n    # Feature('FNC320', float, 'functional MRI feature 320'),  # importance: 0.0\n    # Feature('FNC96', float, 'functional MRI feature 96'),  # importance: 0.0\n    # Feature('FNC119', float, 'functional MRI feature 119'),  # importance: 0.0\n    # Feature('FNC40', float, 'functional MRI feature 40'),  # importance: 0.0\n    # Feature('FNC324', float, 'functional MRI feature 324'),  # importance: 0.0\n    # Feature('FNC187', float, 'functional MRI feature 187'),  # importance: 0.0\n    # Feature('FNC163', float, 'functional MRI feature 163'),  # importance: 0.0\n    # Feature('FNC265', float, 'functional MRI feature 265'),  # importance: 0.0\n    # Feature('FNC349', float, 'functional MRI feature 349'),  # importance: 0.0\n    # Feature('FNC219', float, 'functional MRI feature 219'),  # importance: 0.0\n    # Feature('SBM_map72', float, 'structural ICA map feature 72'),  # importance: 0.0\n    # Feature('FNC195', float, 'functional MRI feature 195'),  # importance: 0.0\n    # Feature('FNC18', float, 'functional MRI feature 18'),  # importance: 0.0\n    # Feature('FNC313', float, 'functional MRI feature 313'),  # importance: 0.0\n    # Feature('FNC354', float, 'functional MRI feature 354'),  # importance: 0.0\n    # Feature('FNC125', float, 'functional MRI feature 125'),  # importance: 0.0\n    # Feature('FNC19', float, 'functional MRI feature 19'),  # importance: 0.0\n    # Feature('FNC122', float, 'functional MRI feature 122'),  # importance: 0.0\n    # Feature('FNC261', float, 'functional MRI feature 261'),  # importance: 0.0\n    # Feature('FNC247', float, 'functional MRI feature 247'),  # importance: 0.0\n    # Feature('FNC17', float, 'functional MRI feature 17'),  # importance: 0.0\n    # Feature('FNC228', float, 'functional MRI feature 228'),  # importance: 0.0\n    # Feature('FNC27', float, 'functional MRI feature 27'),  # importance: 0.0\n    # Feature('FNC92', float, 'functional MRI feature 92'),  # importance: 0.0\n    # Feature('SBM_map75', float, 'structural ICA map feature 75'),  # importance: 0.0\n    # Feature('FNC108', float, 'functional MRI feature 108'),  # importance: 0.0\n    # Feature('FNC29', float, 'functional MRI feature 29'),  # importance: 0.0\n    # Feature('FNC154', float, 'functional MRI feature 154'),  # importance: 0.0\n    # Feature('FNC335', float, 'functional MRI feature 335'),  # importance: 0.0\n    # Feature('FNC65', float, 'functional MRI feature 65'),  # importance: 0.0\n    # Feature('FNC106', float, 'functional MRI feature 106'),  # importance: 0.0\n    # Feature('FNC66', float, 'functional MRI feature 66'),  # importance: 0.0\n    # Feature('FNC23', float, 'functional MRI feature 23'),  # importance: 0.0\n    # Feature('FNC123', float, 'functional MRI feature 123'),  # importance: 0.0\n    # Feature('FNC183', float, 'functional MRI feature 183'),  # importance: 0.0\n    # Feature('FNC365', float, 'functional MRI feature 365'),  # importance: 0.0\n    # Feature('FNC318', float, 'functional MRI feature 318'),  # importance: 0.0\n    # Feature('FNC244', float, 'functional MRI feature 244'),  # importance: 0.0\n    # Feature('FNC252', float, 'functional MRI feature 252'),  # importance: 0.0\n    # Feature('FNC148', float, 'functional MRI feature 148'),  # importance: 0.0\n    # Feature('FNC330', float, 'functional MRI feature 330'),  # importance: 0.0\n    # Feature('FNC171', float, 'functional MRI feature 171'),  # importance: 0.0\n    # Feature('SBM_map10', float, 'structural ICA map feature 10'),  # importance: 0.0\n    # Feature('FNC327', float, 'functional MRI feature 327'),  # importance: 0.0\n    # Feature('SBM_map5', float, 'structural ICA map feature 5'),  # importance: 0.0\n    # Feature('SBM_map69', float, 'structural ICA map feature 69'),  # importance: 0.0\n    # Feature('FNC279', float, 'functional MRI feature 279'),  # importance: 0.0\n    # Feature('FNC181', float, 'functional MRI feature 181'),  # importance: 0.0\n    # Feature('FNC166', float, 'functional MRI feature 166'),  # importance: 0.0\n    # Feature('FNC306', float, 'functional MRI feature 306'),  # importance: 0.0\n    # Feature('FNC149', float, 'functional MRI feature 149'),  # importance: 0.0\n    # Feature('FNC133', float, 'functional MRI feature 133'),  # importance: 0.0\n    # Feature('FNC230', float, 'functional MRI feature 230'),  # importance: 0.0\n    # Feature('FNC315', float, 'functional MRI feature 315'),  # importance: 0.0\n], documentation=\"https://www.kaggle.com/competitions/mlsp-2014-mri/data\")\n\nTITANIC_FEATURES = FeatureList(features=[\n    Feature('PassengerId', int, name_extended=\"passenger ID\"),\n    Feature('Survived', int, is_target=True),\n    Feature('Pclass', int, name_extended=\"passenger class\"),\n    Feature('Name', cat_dtype),\n    Feature('Sex', cat_dtype),\n    Feature('Age', float, name_extended='age in years'),\n    Feature('SibSp', int,\n            name_extended='number of siblings / spouses aboard the Titanic'),\n    Feature('Parch', int,\n            name_extended='number of parents / children aboard the Titanic'),\n    Feature('Ticket', cat_dtype, name_extended='ticket number'),\n    Feature('Fare', float, name_extended='passenger fare'),\n    Feature('Cabin', cat_dtype, name_extended='cabin number'),\n    Feature('Embarked', cat_dtype, name_extended='Port of Embarkation',\n            value_mapping={\"C\": \"Cherbourg\", \"Q\": \"Queenstown\",\n                           \"S\": \"Southampton\"}),\n], documentation=\"https://www.kaggle.com/competitions/titanic/data\")\n\nSANTANDER_TRANSACTION_FEATURES = FeatureList(features=[\n    Feature('target', int, is_target=True),\n    Feature('var_81', float),  # importance: 0.0156\n    Feature('var_139', float),  # importance: 0.0127\n    Feature('var_110', float),  # importance: 0.0119\n    Feature('var_109', float),  # importance: 0.011\n    Feature('var_80', float),  # importance: 0.0107\n    Feature('var_12', float),  # importance: 0.0101\n    Feature('var_26', float),  # importance: 0.0101\n    Feature('var_53', float),  # importance: 0.01\n    Feature('var_0', float),  # importance: 0.0098\n    Feature('var_99', float),  # importance: 0.0097\n    Feature('var_198', float),  # importance: 0.0095\n    Feature('var_191', float),  # importance: 0.0091\n    Feature('var_76', float),  # importance: 0.0091\n    Feature('var_78', float),  # importance: 0.009\n    Feature('var_174', float),  # importance: 0.009\n    Feature('var_166', float),  # importance: 0.0089\n    Feature('var_6', float),  # importance: 0.0088\n    Feature('var_179', float),  # importance: 0.0088\n    Feature('var_22', float),  # importance: 0.0087\n    Feature('var_2', float),  # importance: 0.0087\n    Feature('var_1', float),  # importance: 0.0085\n    Feature('var_33', float),  # importance: 0.0084\n    Feature('var_40', float),  # importance: 0.0081\n    Feature('var_44', float),  # importance: 0.0081\n    Feature('var_13', float),  # importance: 0.008\n    Feature('var_148', float),  # importance: 0.008\n    Feature('var_133', float),  # importance: 0.008\n    Feature('var_146', float),  # importance: 0.0079\n    Feature('var_94', float),  # importance: 0.0079\n    Feature('var_169', float),  # importance: 0.0079\n    Feature('var_190', float),  # importance: 0.0079\n    Feature('var_21', float),  # importance: 0.0079\n    Feature('var_164', float),  # importance: 0.0079\n    ##################################################\n    ##################################################\n    # Feature('var_108', float),  # importance: 0.0078\n    # Feature('var_170', float),  # importance: 0.0078\n    # Feature('var_92', float),  # importance: 0.0076\n    # Feature('var_34', float),  # importance: 0.0074\n    # Feature('var_123', float),  # importance: 0.0074\n    # Feature('var_154', float),  # importance: 0.0072\n    # Feature('var_9', float),  # importance: 0.0069\n    # Feature('var_192', float),  # importance: 0.0069\n    # Feature('var_165', float),  # importance: 0.0069\n    # Feature('var_184', float),  # importance: 0.0068\n    # Feature('var_75', float),  # importance: 0.0067\n    # Feature('var_172', float),  # importance: 0.0067\n    # Feature('var_180', float),  # importance: 0.0067\n    # Feature('var_93', float),  # importance: 0.0067\n    # Feature('var_155', float),  # importance: 0.0066\n    # Feature('var_149', float),  # importance: 0.0066\n    # Feature('var_121', float),  # importance: 0.0065\n    # Feature('var_177', float),  # importance: 0.0065\n    # Feature('var_188', float),  # importance: 0.0063\n    # Feature('var_67', float),  # importance: 0.0061\n    # Feature('var_157', float),  # importance: 0.006\n    # Feature('var_5', float),  # importance: 0.006\n    # Feature('var_173', float),  # importance: 0.006\n    # Feature('var_119', float),  # importance: 0.006\n    # Feature('var_91', float),  # importance: 0.006\n    # Feature('var_127', float),  # importance: 0.0058\n    # Feature('var_18', float),  # importance: 0.0058\n    # Feature('var_107', float),  # importance: 0.0058\n    # Feature('var_115', float),  # importance: 0.0057\n    # Feature('var_35', float),  # importance: 0.0057\n    # Feature('var_122', float),  # importance: 0.0057\n    # Feature('var_95', float),  # importance: 0.0056\n    # Feature('var_118', float),  # importance: 0.0056\n    # Feature('var_86', float),  # importance: 0.0056\n    # Feature('var_147', float),  # importance: 0.0055\n    # Feature('var_32', float),  # importance: 0.0054\n    # Feature('var_89', float),  # importance: 0.0054\n    # Feature('var_141', float),  # importance: 0.0054\n    # Feature('var_162', float),  # importance: 0.0052\n    # Feature('var_130', float),  # importance: 0.0052\n    # Feature('var_106', float),  # importance: 0.0052\n    # Feature('var_150', float),  # importance: 0.0052\n    # Feature('var_56', float),  # importance: 0.0051\n    # Feature('var_111', float),  # importance: 0.0051\n    # Feature('var_197', float),  # importance: 0.0051\n    # Feature('var_87', float),  # importance: 0.0051\n    # Feature('var_125', float),  # importance: 0.005\n    # Feature('var_163', float),  # importance: 0.005\n    # Feature('var_167', float),  # importance: 0.005\n    # Feature('var_48', float),  # importance: 0.0049\n    # Feature('var_186', float),  # importance: 0.0049\n    # Feature('var_43', float),  # importance: 0.0048\n    # Feature('var_70', float),  # importance: 0.0048\n    # Feature('var_36', float),  # importance: 0.0048\n    # Feature('var_51', float),  # importance: 0.0046\n    # Feature('var_135', float),  # importance: 0.0046\n    # Feature('var_145', float),  # importance: 0.0046\n    # Feature('var_71', float),  # importance: 0.0045\n    # Feature('var_85', float),  # importance: 0.0045\n    # Feature('var_131', float),  # importance: 0.0045\n    # Feature('var_49', float),  # importance: 0.0044\n    # Feature('var_175', float),  # importance: 0.0044\n    # Feature('var_52', float),  # importance: 0.0043\n    # Feature('var_116', float),  # importance: 0.0043\n    # Feature('var_128', float),  # importance: 0.0043\n    # Feature('var_132', float),  # importance: 0.0042\n    # Feature('var_105', float),  # importance: 0.0042\n    # Feature('var_24', float),  # importance: 0.0042\n    # Feature('var_195', float),  # importance: 0.0041\n    # Feature('var_199', float),  # importance: 0.0041\n    # Feature('var_151', float),  # importance: 0.0041\n    # Feature('var_112', float),  # importance: 0.0041\n    # Feature('var_144', float),  # importance: 0.004\n    # Feature('var_114', float),  # importance: 0.004\n    # Feature('var_82', float),  # importance: 0.004\n    # Feature('var_88', float),  # importance: 0.0039\n    # Feature('var_23', float),  # importance: 0.0039\n    # Feature('var_194', float),  # importance: 0.0039\n    # Feature('var_90', float),  # importance: 0.0039\n    # Feature('var_102', float),  # importance: 0.0039\n    # Feature('var_31', float),  # importance: 0.0039\n    # Feature('var_28', float),  # importance: 0.0039\n    # Feature('var_11', float),  # importance: 0.0039\n    # Feature('var_74', float),  # importance: 0.0037\n    # Feature('var_58', float),  # importance: 0.0037\n    # Feature('var_183', float),  # importance: 0.0037\n    # Feature('var_45', float),  # importance: 0.0037\n    # Feature('var_143', float),  # importance: 0.0037\n    # Feature('var_50', float),  # importance: 0.0036\n    # Feature('var_196', float),  # importance: 0.0036\n    # Feature('var_156', float),  # importance: 0.0036\n    # Feature('var_77', float),  # importance: 0.0035\n    # Feature('var_97', float),  # importance: 0.0035\n    # Feature('var_3', float),  # importance: 0.0035\n    # Feature('var_54', float),  # importance: 0.0035\n    # Feature('var_27', float),  # importance: 0.0035\n    # Feature('var_138', float),  # importance: 0.0035\n    # Feature('var_168', float),  # importance: 0.0034\n    # Feature('var_134', float),  # importance: 0.0034\n    # Feature('var_187', float),  # importance: 0.0034\n    # Feature('var_37', float),  # importance: 0.0033\n    # Feature('var_83', float),  # importance: 0.0033\n    # Feature('var_38', float),  # importance: 0.0033\n    # Feature('var_64', float),  # importance: 0.0033\n    # Feature('var_176', float),  # importance: 0.0033\n    # Feature('var_137', float),  # importance: 0.0033\n    # Feature('var_66', float),  # importance: 0.0032\n    # Feature('var_10', float),  # importance: 0.0032\n    # Feature('var_55', float),  # importance: 0.0032\n    # Feature('var_113', float),  # importance: 0.0032\n    # Feature('var_152', float),  # importance: 0.0032\n    # Feature('var_68', float),  # importance: 0.0032\n    # Feature('var_63', float),  # importance: 0.0032\n    # Feature('var_104', float),  # importance: 0.0032\n    # Feature('var_96', float),  # importance: 0.0032\n    # Feature('var_14', float),  # importance: 0.0031\n    # Feature('var_4', float),  # importance: 0.0031\n    # Feature('var_193', float),  # importance: 0.0031\n    # Feature('var_20', float),  # importance: 0.0031\n    # Feature('var_62', float),  # importance: 0.0031\n    # Feature('var_142', float),  # importance: 0.0031\n    # Feature('var_182', float),  # importance: 0.0031\n    # Feature('var_178', float),  # importance: 0.0031\n    # Feature('var_181', float),  # importance: 0.0031\n    # Feature('var_171', float),  # importance: 0.003\n    # Feature('var_8', float),  # importance: 0.003\n    # Feature('var_84', float),  # importance: 0.003\n    # Feature('var_19', float),  # importance: 0.003\n    # Feature('var_159', float),  # importance: 0.003\n    # Feature('var_59', float),  # importance: 0.003\n    # Feature('var_101', float),  # importance: 0.003\n    # Feature('var_72', float),  # importance: 0.003\n    # Feature('var_103', float),  # importance: 0.0029\n    # Feature('var_60', float),  # importance: 0.0029\n    # Feature('var_29', float),  # importance: 0.0029\n    # Feature('var_57', float),  # importance: 0.0029\n    # Feature('var_120', float),  # importance: 0.0029\n    # Feature('var_158', float),  # importance: 0.0029\n    # Feature('var_153', float),  # importance: 0.0028\n    # Feature('var_47', float),  # importance: 0.0028\n    # Feature('var_65', float),  # importance: 0.0028\n    # Feature('var_189', float),  # importance: 0.0028\n    # Feature('var_126', float),  # importance: 0.0027\n    # Feature('var_73', float),  # importance: 0.0027\n    # Feature('var_136', float),  # importance: 0.0027\n    # Feature('var_117', float),  # importance: 0.0027\n    # Feature('var_46', float),  # importance: 0.0027\n    # Feature('var_129', float),  # importance: 0.0026\n    # Feature('var_69', float),  # importance: 0.0026\n    # Feature('var_42', float),  # importance: 0.0026\n    # Feature('var_160', float),  # importance: 0.0026\n    # Feature('var_30', float),  # importance: 0.0025\n    # Feature('var_16', float),  # importance: 0.0025\n    # Feature('var_15', float),  # importance: 0.0025\n    # Feature('var_7', float),  # importance: 0.0025\n    # Feature('var_161', float),  # importance: 0.0025\n    # Feature('var_61', float),  # importance: 0.0025\n    # Feature('var_39', float),  # importance: 0.0025\n    # Feature('var_100', float),  # importance: 0.0025\n    # Feature('var_140', float),  # importance: 0.0024\n    # Feature('var_25', float),  # importance: 0.0024\n    # Feature('var_79', float),  # importance: 0.0024\n    # Feature('var_185', float),  # importance: 0.0023\n    # Feature('var_41', float),  # importance: 0.0022\n    # Feature('var_98', float),  # importance: 0.0022\n    # Feature('var_124', float),  # importance: 0.0021\n    # Feature('var_17', float),  # importance: 0.0021\n    # Feature('ID_code', cat_dtype),  # importance: 0.0\n],\n    documentation=\"https://www.kaggle.com/competitions/santander-customer-transaction-prediction/data\")\n\nHOME_CREDIT_DEFAULT_FEATURES = FeatureList(features=[\n    Feature('TARGET', int, is_target=True),\n    Feature('EXT_SOURCE_3', float, name_extended=\"Normalized score from external data source 3\"),  # importance: 0.0379\n    Feature('EXT_SOURCE_2', float, name_extended=\"Normalized score from external data source 2\"),  # importance: 0.0358\n    Feature('FLAG_DOCUMENT_3', float, name_extended=\"Did client provide document 3\",\n            value_mapping={1.: \"Yes\", 0.: \"No\"}),  # importance: 0.0266\n    Feature('CODE_GENDER', cat_dtype, name_extended=\"Gender of the client\",\n            value_mapping={'F': 'female', 'M': 'male'}),  # importance: 0.0221\n    Feature('NAME_EDUCATION_TYPE', cat_dtype, name_extended=\"Level of highest education the client achieved\"),  # importance: 0.0204\n    Feature('NAME_CONTRACT_TYPE', cat_dtype, name_extended=\"Contract status during the month\"),  # importance: 0.0195\n    Feature('REGION_RATING_CLIENT_W_CITY', float, name_extended=\"Our rating of the region where client lives with taking city into account\"),  # importance: 0.016\n    Feature('EXT_SOURCE_1', float, name_extended=\"Normalized score from external data source 1\"),  # importance: 0.0157\n    Feature('AMT_CREDIT', float, name_extended=\"Maximal amount overdue on the Credit Bureau credit at application date\"),  # importance: 0.0146\n    Feature('AMT_GOODS_PRICE', float, name_extended=\"Price of good that client asked for (if applicable) on the previous application\"),  # importance: 0.0142\n    Feature('ORGANIZATION_TYPE', cat_dtype, name_extended=\"Type of organization where client works\"),  # importance: 0.0133\n    Feature('NAME_INCOME_TYPE', cat_dtype, name_extended=\"Client's income type\"),  # importance: 0.0131\n    Feature('FLOORSMAX_AVG', float, name_extended=\"Average number of floors in building where client lives\"),  # importance: 0.0121\n    Feature('FLAG_WORK_PHONE', float, name_extended=\"Did client provide work phone\",\n            value_mapping={1.: \"Yes\", 0.: \"No\"}),  # importance: 0.012\n    Feature('OWN_CAR_AGE', float, name_extended=\"Age of client's car\"),  # importance: 0.012\n    Feature('REG_CITY_NOT_LIVE_CITY', float, name_extended=\"Flag if client's permanent address does not match contact address\",\n            value_mapping={1.:\"different\", 0.: \"same\"}),  # importance: 0.0119\n    Feature('AMT_ANNUITY', float, name_extended=\"Annuity of the Credit Bureau credit\"),  # importance: 0.0116\n    Feature('FLAG_DOCUMENT_18', float, name_extended=\"Did client provide document 3\",\n            value_mapping={1.: \"Yes\", 0.: \"No\"}),  # importance: 0.0115\n    Feature('LIVINGAREA_AVG', float),  # importance: 0.0112\n    Feature('DAYS_BIRTH', float, name_extended=\"Client's age in days at the time of application\"),  # importance: 0.0108\n    Feature('FLAG_DOCUMENT_16', float, name_extended=\"Did client provide document 16\",\n            value_mapping={1.: \"Yes\", 0.: \"No\"}),  # importance: 0.0108\n    Feature('OCCUPATION_TYPE', cat_dtype, name_extended=\"Occupation type of client\"),  # importance: 0.0107\n    Feature('APARTMENTS_MODE', float, name_extended=\"Mode of number of apartments in building where client lives\"),  # importance: 0.0107\n    Feature('DEF_30_CNT_SOCIAL_CIRCLE', float, name_extended=\"Number of client's social surroundings defaulted on 30 days past due\"),  # importance: 0.0104\n    Feature('DEF_60_CNT_SOCIAL_CIRCLE', float, name_extended=\"Number of client's social surroundings defaulted on 60 days past due\"),  # importance: 0.0103\n    Feature('AMT_REQ_CREDIT_BUREAU_QRT', float, name_extended=\"Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application)\"),  # importance: 0.0103\n    Feature('NONLIVINGAPARTMENTS_MEDI', float, name_extended=\"Mode of number of nonliving apartments in building where client lives\"),  # importance: 0.0097\n    Feature('DAYS_EMPLOYED', float, name_extended=\"How many days before the application the person started current employment\"),  # importance: 0.0097\n    Feature('FLAG_DOCUMENT_5', float, name_extended=\"Did client provide document 5\",\n            value_mapping={1.: \"Yes\", 0.: \"No\"}),  # importance: 0.0096\n    Feature('FLOORSMIN_MEDI', float, name_extended=\"Median of min number of floors in building where client lives\"),  # importance: 0.0095\n    Feature('OBS_60_CNT_SOCIAL_CIRCLE', float, name_extended=\"Number of client's social surroundings with observable 60 DPD days past due default\"),  # importance: 0.0095\n    Feature('AMT_REQ_CREDIT_BUREAU_DAY', float, name_extended=\"Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application)\"),  # importance: 0.0092\n    Feature('EMERGENCYSTATE_MODE', cat_dtype, name_extended=\"Mode of number of emergency state apartments in building where client lives\"),  # importance: 0.0091\n    ##################################################\n    ##################################################\n    # Feature('FLAG_DOCUMENT_9', float),  # importance: 0.0089\n    # Feature('ELEVATORS_AVG', float),  # importance: 0.0088\n    # Feature('APARTMENTS_MEDI', float),  # importance: 0.0088\n    # Feature('FLOORSMAX_MEDI', float),  # importance: 0.0087\n    # Feature('NAME_TYPE_SUITE', cat_dtype),  # importance: 0.0086\n    # Feature('FLOORSMIN_AVG', float),  # importance: 0.0086\n    # Feature('LIVINGAPARTMENTS_MEDI', float),  # importance: 0.0086\n    # Feature('DAYS_ID_PUBLISH', float),  # importance: 0.0086\n    # Feature('FLAG_OWN_CAR', cat_dtype),  # importance: 0.0085\n    # Feature('NAME_FAMILY_STATUS', cat_dtype),  # importance: 0.0085\n    # Feature('TOTALAREA_MODE', float),  # importance: 0.0085\n    # Feature('LIVINGAREA_MEDI', float),  # importance: 0.0084\n    # Feature('FLOORSMAX_MODE', float),  # importance: 0.0083\n    # Feature('HOUSETYPE_MODE', cat_dtype),  # importance: 0.0082\n    # Feature('DAYS_LAST_PHONE_CHANGE', float),  # importance: 0.0081\n    # Feature('FLAG_DOCUMENT_14', float),  # importance: 0.0079\n    # Feature('WALLSMATERIAL_MODE', cat_dtype),  # importance: 0.0079\n    # Feature('DAYS_REGISTRATION', float),  # importance: 0.0079\n    # Feature('AMT_REQ_CREDIT_BUREAU_WEEK', float),  # importance: 0.0079\n    # Feature('YEARS_BUILD_AVG', float),  # importance: 0.0079\n    # Feature('WEEKDAY_APPR_PROCESS_START', cat_dtype),  # importance: 0.0077\n    # Feature('LANDAREA_MODE', float),  # importance: 0.0077\n    # Feature('FLAG_PHONE', float),  # importance: 0.0077\n    # Feature('YEARS_BEGINEXPLUATATION_MODE', float),  # importance: 0.0076\n    # Feature('REGION_POPULATION_RELATIVE', float),  # importance: 0.0075\n    # Feature('BASEMENTAREA_AVG', float),  # importance: 0.0075\n    # Feature('NONLIVINGAPARTMENTS_MODE', float),  # importance: 0.0075\n    # Feature('LIVINGAPARTMENTS_AVG', float),  # importance: 0.0074\n    # Feature('NAME_HOUSING_TYPE', cat_dtype),  # importance: 0.0074\n    # Feature('COMMONAREA_MODE', float),  # importance: 0.0073\n    # Feature('AMT_REQ_CREDIT_BUREAU_YEAR', float),  # importance: 0.0073\n    # Feature('SK_ID_CURR', float),  # importance: 0.0072\n    # Feature('ENTRANCES_MODE', float),  # importance: 0.0072\n    # Feature('HOUR_APPR_PROCESS_START', float),  # importance: 0.0071\n    # Feature('OBS_30_CNT_SOCIAL_CIRCLE', float),  # importance: 0.0071\n    # Feature('AMT_INCOME_TOTAL', float),  # importance: 0.0071\n    # Feature('BASEMENTAREA_MODE', float),  # importance: 0.0071\n    # Feature('REG_CITY_NOT_WORK_CITY', float),  # importance: 0.007\n    # Feature('ENTRANCES_MEDI', float),  # importance: 0.007\n    # Feature('APARTMENTS_AVG', float),  # importance: 0.007\n    # Feature('NONLIVINGAREA_MEDI', float),  # importance: 0.0069\n    # Feature('LIVINGAPARTMENTS_MODE', float),  # importance: 0.0069\n    # Feature('ELEVATORS_MODE', float),  # importance: 0.0069\n    # Feature('COMMONAREA_AVG', float),  # importance: 0.0069\n    # Feature('COMMONAREA_MEDI', float),  # importance: 0.0069\n    # Feature('LIVINGAREA_MODE', float),  # importance: 0.0069\n    # Feature('FLOORSMIN_MODE', float),  # importance: 0.0069\n    # Feature('LANDAREA_AVG', float),  # importance: 0.0068\n    # Feature('FONDKAPREMONT_MODE', cat_dtype),  # importance: 0.0067\n    # Feature('LANDAREA_MEDI', float),  # importance: 0.0066\n    # Feature('YEARS_BEGINEXPLUATATION_AVG', float),  # importance: 0.0065\n    # Feature('YEARS_BEGINEXPLUATATION_MEDI', float),  # importance: 0.0065\n    # Feature('REGION_RATING_CLIENT', float),  # importance: 0.0065\n    # Feature('BASEMENTAREA_MEDI', float),  # importance: 0.0064\n    # Feature('FLAG_OWN_REALTY', cat_dtype),  # importance: 0.0064\n    # Feature('CNT_CHILDREN', float),  # importance: 0.0064\n    # Feature('LIVE_REGION_NOT_WORK_REGION', float),  # importance: 0.0064\n    # Feature('NONLIVINGAREA_MODE', float),  # importance: 0.0064\n    # Feature('AMT_REQ_CREDIT_BUREAU_MON', float),  # importance: 0.0064\n    # Feature('AMT_REQ_CREDIT_BUREAU_HOUR', float),  # importance: 0.0063\n    # Feature('ENTRANCES_AVG', float),  # importance: 0.0062\n    # Feature('FLAG_EMAIL', float),  # importance: 0.0062\n    # Feature('YEARS_BUILD_MEDI', float),  # importance: 0.0061\n    # Feature('NONLIVINGAPARTMENTS_AVG', float),  # importance: 0.006\n    # Feature('NONLIVINGAREA_AVG', float),  # importance: 0.006\n    # Feature('YEARS_BUILD_MODE', float),  # importance: 0.0059\n    # Feature('LIVE_CITY_NOT_WORK_CITY', float),  # importance: 0.0058\n    # Feature('CNT_FAM_MEMBERS', float),  # importance: 0.0056\n    # Feature('FLAG_DOCUMENT_13', float),  # importance: 0.0049\n    # Feature('REG_REGION_NOT_LIVE_REGION', float),  # importance: 0.0048\n    # Feature('FLAG_DOCUMENT_8', float),  # importance: 0.0045\n    # Feature('FLAG_DOCUMENT_6', float),  # importance: 0.0039\n    # Feature('ELEVATORS_MEDI', float),  # importance: 0.0035\n    # Feature('FLAG_DOCUMENT_11', float),  # importance: 0.0035\n    # Feature('FLAG_DOCUMENT_2', float),  # importance: 0.0031\n    # Feature('FLAG_DOCUMENT_15', float),  # importance: 0.0025\n    # Feature('REG_REGION_NOT_WORK_REGION', float),  # importance: 0.0024\n    # Feature('FLAG_EMP_PHONE', float),  # importance: 0.0008\n    # Feature('FLAG_DOCUMENT_10', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_21', float),  # importance: 0.0\n    # Feature('FLAG_MOBIL', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_20', float),  # importance: 0.0\n    # Feature('FLAG_CONT_MOBILE', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_4', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_17', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_7', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_12', float),  # importance: 0.0\n    # Feature('FLAG_DOCUMENT_19', float),  # importance: 0.0\n],\n    documentation=\"https://www.kaggle.com/competitions/home-credit-default-risk/data\")\n\nIEEE_FRAUD_DETECTION_FEATURES = FeatureList(features=[\n    Feature('isFraud', int, is_target=True),\n    Feature('V258', float),  # importance: 0.1467\n    Feature('V201', float),  # importance: 0.1091\n    Feature('V244', float),  # importance: 0.0439\n    Feature('V295', float),  # importance: 0.0378\n    Feature('V70', float),  # importance: 0.0366\n    Feature('V225', float),  # importance: 0.0308\n    Feature('V189', float),  # importance: 0.0306\n    Feature('V91', float),  # importance: 0.0251\n    Feature('V294', float),  # importance: 0.0186\n    Feature('V209', float),  # importance: 0.0142\n    Feature('C14', float),  # importance: 0.0113\n    Feature('V274', float),  # importance: 0.0112\n    Feature('id_12', cat_dtype),  # importance: 0.0109\n    Feature('C1', float),  # importance: 0.0077\n    Feature('V172', float),  # importance: 0.0071\n    Feature('V210', float),  # importance: 0.0068\n    Feature('C8', float),  # importance: 0.0067\n    Feature('V48', float),  # importance: 0.0066\n    Feature('V133', float),  # importance: 0.0065\n    Feature('V308', float),  # importance: 0.0063\n    Feature('V45', float),  # importance: 0.0056\n    Feature('V62', float),  # importance: 0.0054\n    Feature('DeviceInfo', cat_dtype, name_extended=\"Device info\"),  # importance: 0.0054\n    Feature('C11', float),  # importance: 0.0053\n    Feature('R_emaildomain', cat_dtype, name_extended='recipient email domain'),  # importance: 0.005\n    Feature('V82', float),  # importance: 0.0049\n    Feature('V315', float),  # importance: 0.0049\n    Feature('C13', float),  # importance: 0.0049\n    Feature('C12', float),  # importance: 0.0048\n    Feature('V253', float),  # importance: 0.0047\n    Feature('V219', float),  # importance: 0.0046\n    Feature('V175', float),  # importance: 0.0045\n    Feature('V296', float),  # importance: 0.0045\n    ##################################################\n    ##################################################\n    # Feature('V53', float),  # importance: 0.0044\n    # Feature('card6', cat_dtype),  # importance: 0.0043\n    # Feature('V12', float),  # importance: 0.0043\n    # Feature('D2', float),  # importance: 0.0042\n    # Feature('V74', float),  # importance: 0.0042\n    # Feature('id_17', float),  # importance: 0.0041\n    # Feature('V46', float),  # importance: 0.004\n    # Feature('V217', float),  # importance: 0.0039\n    # Feature('V192', float),  # importance: 0.0038\n    # Feature('V312', float),  # importance: 0.0038\n    # Feature('V279', float),  # importance: 0.0036\n    # Feature('D3', float),  # importance: 0.0035\n    # Feature('C5', float),  # importance: 0.0034\n    # Feature('ProductCD', cat_dtype),  # importance: 0.0033\n    # Feature('card3', float),  # importance: 0.0033\n    # Feature('V67', float),  # importance: 0.0033\n    # Feature('V198', float),  # importance: 0.0032\n    # Feature('V60', float),  # importance: 0.0032\n    # Feature('V283', float),  # importance: 0.0031\n    # Feature('M5', cat_dtype),  # importance: 0.003\n    # Feature('V191', float),  # importance: 0.003\n    # Feature('V317', float),  # importance: 0.003\n    # Feature('V246', float),  # importance: 0.003\n    # Feature('V262', float),  # importance: 0.0029\n    # Feature('V177', float),  # importance: 0.0029\n    # Feature('V254', float),  # importance: 0.0029\n    # Feature('V245', float),  # importance: 0.0028\n    # Feature('V79', float),  # importance: 0.0028\n    # Feature('V281', float),  # importance: 0.0028\n    # Feature('C2', float),  # importance: 0.0028\n    # Feature('V51', float),  # importance: 0.0027\n    # Feature('V29', float),  # importance: 0.0027\n    # Feature('C9', float),  # importance: 0.0026\n    # Feature('V83', float),  # importance: 0.0025\n    # Feature('V126', float),  # importance: 0.0025\n    # Feature('V55', float),  # importance: 0.0025\n    # Feature('M4', cat_dtype),  # importance: 0.0025\n    # Feature('V194', float),  # importance: 0.0025\n    # Feature('M6', cat_dtype),  # importance: 0.0024\n    # Feature('C7', float),  # importance: 0.0024\n    # Feature('C6', float),  # importance: 0.0024\n    # Feature('P_emaildomain', cat_dtype),  # importance: 0.0023\n    # Feature('V94', float),  # importance: 0.0023\n    # Feature('card4', cat_dtype),  # importance: 0.0022\n    # Feature('V49', float),  # importance: 0.0022\n    # Feature('V320', float),  # importance: 0.0021\n    # Feature('D15', float),  # importance: 0.0021\n    # Feature('V33', float),  # importance: 0.0021\n    # Feature('V76', float),  # importance: 0.0021\n    # Feature('id_31', cat_dtype),  # importance: 0.0021\n    # Feature('V131', float),  # importance: 0.002\n    # Feature('V280', float),  # importance: 0.002\n    # Feature('TransactionAmt', float),  # importance: 0.002\n    # Feature('D4', float),  # importance: 0.002\n    # Feature('V105', float),  # importance: 0.002\n    # Feature('V19', float),  # importance: 0.0019\n    # Feature('V125', float),  # importance: 0.0018\n    # Feature('V124', float),  # importance: 0.0018\n    # Feature('V96', float),  # importance: 0.0018\n    # Feature('V318', float),  # importance: 0.0018\n    # Feature('M3', cat_dtype),  # importance: 0.0018\n    # Feature('V293', float),  # importance: 0.0018\n    # Feature('dist1', float),  # importance: 0.0018\n    # Feature('V130', float),  # importance: 0.0018\n    # Feature('V298', float),  # importance: 0.0018\n    # Feature('V87', float),  # importance: 0.0017\n    # Feature('V277', float),  # importance: 0.0017\n    # Feature('C4', float),  # importance: 0.0017\n    # Feature('card2', float),  # importance: 0.0017\n    # Feature('V313', float),  # importance: 0.0017\n    # Feature('V25', float),  # importance: 0.0017\n    # Feature('V66', float),  # importance: 0.0017\n    # Feature('V271', float),  # importance: 0.0017\n    # Feature('V75', float),  # importance: 0.0016\n    # Feature('V102', float),  # importance: 0.0016\n    # Feature('M2', cat_dtype),  # importance: 0.0016\n    # Feature('V314', float),  # importance: 0.0016\n    # Feature('V310', float),  # importance: 0.0016\n    # Feature('C10', float),  # importance: 0.0016\n    # Feature('D1', float),  # importance: 0.0016\n    # Feature('V57', float),  # importance: 0.0016\n    # Feature('V207', float),  # importance: 0.0015\n    # Feature('D10', float),  # importance: 0.0015\n    # Feature('V54', float),  # importance: 0.0015\n    # Feature('V186', float),  # importance: 0.0015\n    # Feature('card1', float),  # importance: 0.0015\n    # Feature('V10', float),  # importance: 0.0015\n    # Feature('TransactionDT', float),  # importance: 0.0015\n    # Feature('V179', float),  # importance: 0.0015\n    # Feature('V256', float),  # importance: 0.0015\n    # Feature('V178', float),  # importance: 0.0015\n    # Feature('V289', float),  # importance: 0.0015\n    # Feature('V250', float),  # importance: 0.0014\n    # Feature('V78', float),  # importance: 0.0014\n    # Feature('V285', float),  # importance: 0.0014\n    # Feature('addr1', float),  # importance: 0.0014\n    # Feature('V20', float),  # importance: 0.0014\n    # Feature('V169', float),  # importance: 0.0014\n    # Feature('V257', float),  # importance: 0.0014\n    # Feature('V35', float),  # importance: 0.0014\n    # Feature('V80', float),  # importance: 0.0014\n    # Feature('V282', float),  # importance: 0.0014\n    # Feature('V61', float),  # importance: 0.0014\n    # Feature('card5', float),  # importance: 0.0014\n    # Feature('V136', float),  # importance: 0.0013\n    # Feature('V56', float),  # importance: 0.0013\n    # Feature('V86', float),  # importance: 0.0013\n    # Feature('V307', float),  # importance: 0.0013\n    # Feature('M7', cat_dtype),  # importance: 0.0013\n    # Feature('V85', float),  # importance: 0.0013\n    # Feature('V73', float),  # importance: 0.0013\n    # Feature('V248', float),  # importance: 0.0012\n    # Feature('V223', float),  # importance: 0.0012\n    # Feature('V129', float),  # importance: 0.0012\n    # Feature('V227', float),  # importance: 0.0012\n    # Feature('V6', float),  # importance: 0.0012\n    # Feature('C3', float),  # importance: 0.0012\n    # Feature('V311', float),  # importance: 0.0012\n    # Feature('V301', float),  # importance: 0.0012\n    # Feature('V215', float),  # importance: 0.0012\n    # Feature('V5', float),  # importance: 0.0012\n    # Feature('V15', float),  # importance: 0.0012\n    # Feature('V243', float),  # importance: 0.0012\n    # Feature('V224', float),  # importance: 0.0012\n    # Feature('D11', float),  # importance: 0.0011\n    # Feature('id_15', cat_dtype),  # importance: 0.0011\n    # Feature('V98', float),  # importance: 0.0011\n    # Feature('V251', float),  # importance: 0.0011\n    # Feature('V92', float),  # importance: 0.0011\n    # Feature('V300', float),  # importance: 0.0011\n    # Feature('V115', float),  # importance: 0.0011\n    # Feature('id_06', float),  # importance: 0.0011\n    # Feature('V38', float),  # importance: 0.0011\n    # Feature('V97', float),  # importance: 0.0011\n    # Feature('V266', float),  # importance: 0.0011\n    # Feature('V288', float),  # importance: 0.001\n    # Feature('V185', float),  # importance: 0.001\n    # Feature('V99', float),  # importance: 0.001\n    # Feature('V134', float),  # importance: 0.001\n    # Feature('V13', float),  # importance: 0.001\n    # Feature('V44', float),  # importance: 0.001\n    # Feature('V26', float),  # importance: 0.001\n    # Feature('V259', float),  # importance: 0.001\n    # Feature('V202', float),  # importance: 0.001\n    # Feature('M9', cat_dtype),  # importance: 0.001\n    # Feature('V63', float),  # importance: 0.001\n    # Feature('id_01', float),  # importance: 0.001\n    # Feature('V233', float),  # importance: 0.001\n    # Feature('V108', float),  # importance: 0.001\n    # Feature('V261', float),  # importance: 0.001\n    # Feature('id_20', float),  # importance: 0.001\n    # Feature('V268', float),  # importance: 0.001\n    # Feature('V208', float),  # importance: 0.001\n    # Feature('V11', float),  # importance: 0.001\n    # Feature('V32', float),  # importance: 0.0009\n    # Feature('id_11', float),  # importance: 0.0009\n    # Feature('V127', float),  # importance: 0.0009\n    # Feature('V206', float),  # importance: 0.0009\n    # Feature('V24', float),  # importance: 0.0009\n    # Feature('V9', float),  # importance: 0.0009\n    # Feature('V128', float),  # importance: 0.0009\n    # Feature('id_05', float),  # importance: 0.0009\n    # Feature('V263', float),  # importance: 0.0009\n    # Feature('V264', float),  # importance: 0.0009\n    # Feature('V59', float),  # importance: 0.0009\n    # Feature('V47', float),  # importance: 0.0009\n    # Feature('V216', float),  # importance: 0.0009\n    # Feature('V284', float),  # importance: 0.0009\n    # Feature('V234', float),  # importance: 0.0009\n    # Feature('V170', float),  # importance: 0.0009\n    # Feature('id_13', float),  # importance: 0.0009\n    # Feature('id_02', float),  # importance: 0.0009\n    # Feature('V137', float),  # importance: 0.0009\n    # Feature('V58', float),  # importance: 0.0008\n    # Feature('V2', float),  # importance: 0.0008\n    # Feature('V36', float),  # importance: 0.0008\n    # Feature('V214', float),  # importance: 0.0008\n    # Feature('V72', float),  # importance: 0.0008\n    # Feature('V187', float),  # importance: 0.0008\n    # Feature('V309', float),  # importance: 0.0008\n    # Feature('V242', float),  # importance: 0.0008\n    # Feature('V64', float),  # importance: 0.0008\n    # Feature('V291', float),  # importance: 0.0008\n    # Feature('V182', float),  # importance: 0.0008\n    # Feature('V316', float),  # importance: 0.0008\n    # Feature('V265', float),  # importance: 0.0008\n    # Feature('id_19', float),  # importance: 0.0008\n    # Feature('V303', float),  # importance: 0.0007\n    # Feature('V42', float),  # importance: 0.0007\n    # Feature('V199', float),  # importance: 0.0007\n    # Feature('V287', float),  # importance: 0.0007\n    # Feature('V270', float),  # importance: 0.0007\n    # Feature('DeviceType', cat_dtype),  # importance: 0.0007\n    # Feature('V204', float),  # importance: 0.0007\n    # Feature('M8', cat_dtype),  # importance: 0.0007\n    # Feature('V290', float),  # importance: 0.0007\n    # Feature('V37', float),  # importance: 0.0007\n    # Feature('D5', float),  # importance: 0.0007\n    # Feature('V77', float),  # importance: 0.0007\n    # Feature('V306', float),  # importance: 0.0007\n    # Feature('V211', float),  # importance: 0.0007\n    # Feature('V69', float),  # importance: 0.0007\n    # Feature('V267', float),  # importance: 0.0007\n    # Feature('V81', float),  # importance: 0.0007\n    # Feature('V112', float),  # importance: 0.0006\n    # Feature('V321', float),  # importance: 0.0006\n    # Feature('V8', float),  # importance: 0.0006\n    # Feature('id_38', cat_dtype),  # importance: 0.0006\n    # Feature('V276', float),  # importance: 0.0006\n    # Feature('V23', float),  # importance: 0.0006\n    # Feature('V205', float),  # importance: 0.0006\n    # Feature('V135', float),  # importance: 0.0006\n    # Feature('V93', float),  # importance: 0.0006\n    # Feature('id_28', cat_dtype),  # importance: 0.0006\n    # Feature('V220', float),  # importance: 0.0006\n    # Feature('V273', float),  # importance: 0.0006\n    # Feature('V90', float),  # importance: 0.0006\n    # Feature('V43', float),  # importance: 0.0006\n    # Feature('V260', float),  # importance: 0.0006\n    # Feature('V229', float),  # importance: 0.0006\n    # Feature('V109', float),  # importance: 0.0006\n    # Feature('V84', float),  # importance: 0.0006\n    # Feature('V40', float),  # importance: 0.0006\n    # Feature('V319', float),  # importance: 0.0005\n    # Feature('V95', float),  # importance: 0.0005\n    # Feature('V212', float),  # importance: 0.0005\n    # Feature('V255', float),  # importance: 0.0005\n    # Feature('V247', float),  # importance: 0.0005\n    # Feature('V3', float),  # importance: 0.0005\n    # Feature('V16', float),  # importance: 0.0005\n    # Feature('V221', float),  # importance: 0.0005\n    # Feature('V238', float),  # importance: 0.0005\n    # Feature('V34', float),  # importance: 0.0005\n    # Feature('V39', float),  # importance: 0.0005\n    # Feature('V269', float),  # importance: 0.0004\n    # Feature('id_16', cat_dtype),  # importance: 0.0004\n    # Feature('V183', float),  # importance: 0.0004\n    # Feature('V232', float),  # importance: 0.0004\n    # Feature('V292', float),  # importance: 0.0004\n    # Feature('V30', float),  # importance: 0.0004\n    # Feature('id_37', cat_dtype),  # importance: 0.0004\n    # Feature('V275', float),  # importance: 0.0004\n    # Feature('V278', float),  # importance: 0.0004\n    # Feature('V106', float),  # importance: 0.0004\n    # Feature('V171', float),  # importance: 0.0004\n    # Feature('V237', float),  # importance: 0.0004\n    # Feature('V203', float),  # importance: 0.0004\n    # Feature('V286', float),  # importance: 0.0004\n    # Feature('V188', float),  # importance: 0.0004\n    # Feature('V230', float),  # importance: 0.0004\n    # Feature('V222', float),  # importance: 0.0004\n    # Feature('V272', float),  # importance: 0.0003\n    # Feature('V7', float),  # importance: 0.0003\n    # Feature('V123', float),  # importance: 0.0003\n    # Feature('V213', float),  # importance: 0.0003\n    # Feature('V228', float),  # importance: 0.0003\n    # Feature('V297', float),  # importance: 0.0003\n    # Feature('addr2', float),  # importance: 0.0003\n    # Feature('V132', float),  # importance: 0.0003\n    # Feature('V302', float),  # importance: 0.0003\n    # Feature('V121', float),  # importance: 0.0002\n    # Feature('V231', float),  # importance: 0.0002\n    # Feature('V168', float),  # importance: 0.0002\n    # Feature('V200', float),  # importance: 0.0002\n    # Feature('V4', float),  # importance: 0.0002\n    # Feature('V190', float),  # importance: 0.0002\n    # Feature('V22', float),  # importance: 0.0001\n    # Feature('V116', float),  # importance: 0.0\n    # Feature('V113', float),  # importance: 0.0\n    # Feature('V195', float),  # importance: 0.0\n    # Feature('V181', float),  # importance: 0.0\n    # Feature('V27', float),  # importance: 0.0\n    # Feature('V14', float),  # importance: 0.0\n    # Feature('V120', float),  # importance: 0.0\n    # Feature('V305', float),  # importance: 0.0\n    # Feature('V101', float),  # importance: 0.0\n    # Feature('V88', float),  # importance: 0.0\n    # Feature('V18', float),  # importance: 0.0\n    # Feature('TransactionID', float),  # importance: 0.0\n    # Feature('V50', float),  # importance: 0.0\n    # Feature('V218', float),  # importance: 0.0\n    # Feature('V226', float),  # importance: 0.0\n    # Feature('id_36', cat_dtype),  # importance: 0.0\n    # Feature('V249', float),  # importance: 0.0\n    # Feature('V180', float),  # importance: 0.0\n    # Feature('V28', float),  # importance: 0.0\n    # Feature('V21', float),  # importance: 0.0\n    # Feature('id_29', cat_dtype),  # importance: 0.0\n    # Feature('V71', float),  # importance: 0.0\n    # Feature('V299', float),  # importance: 0.0\n    # Feature('V31', float),  # importance: 0.0\n    # Feature('V52', float),  # importance: 0.0\n    # Feature('V304', float),  # importance: 0.0\n    # Feature('V167', float),  # importance: 0.0\n    # Feature('V114', float),  # importance: 0.0\n    # Feature('V110', float),  # importance: 0.0\n    # Feature('V118', float),  # importance: 0.0\n    # Feature('V89', float),  # importance: 0.0\n    # Feature('V240', float),  # importance: 0.0\n    # Feature('V122', float),  # importance: 0.0\n    # Feature('V197', float),  # importance: 0.0\n    # Feature('V241', float),  # importance: 0.0\n    # Feature('V235', float),  # importance: 0.0\n    # Feature('V107', float),  # importance: 0.0\n    # Feature('V184', float),  # importance: 0.0\n    # Feature('V173', float),  # importance: 0.0\n    # Feature('V111', float),  # importance: 0.0\n    # Feature('V41', float),  # importance: 0.0\n    # Feature('V65', float),  # importance: 0.0\n    # Feature('id_35', cat_dtype),  # importance: 0.0\n    # Feature('V104', float),  # importance: 0.0\n    # Feature('V196', float),  # importance: 0.0\n    # Feature('V236', float),  # importance: 0.0\n    # Feature('V1', float),  # importance: 0.0\n    # Feature('V252', float),  # importance: 0.0\n    # Feature('V176', float),  # importance: 0.0\n    # Feature('V103', float),  # importance: 0.0\n    # Feature('V100', float),  # importance: 0.0\n    # Feature('V174', float),  # importance: 0.0\n    # Feature('V193', float),  # importance: 0.0\n    # Feature('V117', float),  # importance: 0.0\n    # Feature('V68', float),  # importance: 0.0\n    # Feature('V119', float),  # importance: 0.0\n    # Feature('M1', cat_dtype),  # importance: 0.0\n    # Feature('V239', float),  # importance: 0.0\n    # Feature('V17', float),  # importance: 0.0\n],\n    documentation='https://www.kaggle.com/competitions/ieee-fraud-detection/data')\n\nSAFE_DRIVER_PREDICTION_FEATURES = FeatureList(features=[\n    Feature('id', int),\n    Feature('target', int, is_target=True),\n    Feature('ps_ind_01', int, name_extended='individual feature 1'),\n    Feature('ps_ind_02_cat', int, name_extended='individual categorical feature 2'),\n    Feature('ps_ind_03', int, name_extended='individual feature 3'),\n    Feature('ps_ind_04_cat', int, name_extended='individual categorical feature 4'),\n    Feature('ps_ind_05_cat', int, name_extended='individual categorical feature 5'),\n    Feature('ps_ind_06_bin', int, name_extended='individual binary feature 6'),\n    Feature('ps_ind_07_bin', int, name_extended='individual binary feature 7'),\n    Feature('ps_ind_08_bin', int, name_extended='individual binary feature 8'),\n    Feature('ps_ind_09_bin', int, name_extended='individual binary feature 9'),\n    Feature('ps_ind_10_bin', int, name_extended='individual binary feature 0'),\n    Feature('ps_ind_11_bin', int, name_extended='individual binary feature 1'),\n    Feature('ps_ind_12_bin', int, name_extended='individual binary feature 2'),\n    Feature('ps_ind_13_bin', int, name_extended='individual binary feature 3'),\n    Feature('ps_ind_14', int, name_extended='individual feature 4'),\n    Feature('ps_ind_15', int, name_extended='individual feature 5'),\n    Feature('ps_ind_16_bin', int, name_extended='individual binary feature 6'),\n    Feature('ps_ind_17_bin', int, name_extended='individual binary feature 7'),\n    Feature('ps_ind_18_bin', int, name_extended='individual binary feature 8'),\n    Feature('ps_reg_01', float, name_extended='registration feature 1'),\n    Feature('ps_reg_02', float, name_extended='registration feature 2'),\n    Feature('ps_reg_03', float, name_extended='registration feature 3'),\n    Feature('ps_car_01_cat', int, name_extended='car categorical feature 1'),\n    Feature('ps_car_02_cat', int, name_extended='car categorical feature 2'),\n    Feature('ps_car_03_cat', int, name_extended='car categorical feature 3'),\n    Feature('ps_car_04_cat', int, name_extended='car categorical feature 4'),\n    Feature('ps_car_05_cat', int, name_extended='car categorical feature 5'),\n    Feature('ps_car_06_cat', int, name_extended='car categorical feature 6'),\n    Feature('ps_car_07_cat', int, name_extended='car categorical feature 7'),\n    Feature('ps_car_08_cat', int, name_extended='car categorical feature 8'),\n    Feature('ps_car_09_cat', int, name_extended='car categorical feature 9'),\n    Feature('ps_car_10_cat', int, name_extended='car categorical feature 0'),\n    Feature('ps_car_11_cat', int, name_extended='car categorical feature 1'),\n    Feature('ps_car_11', int, name_extended='car feature 1'),\n    Feature('ps_car_12', float, name_extended='car feature 2'),\n    Feature('ps_car_13', float, name_extended='car feature 3'),\n    Feature('ps_car_14', float, name_extended='car feature 4'),\n    Feature('ps_car_15', float, name_extended='car feature 5'),\n    Feature('ps_calc_01', float, name_extended='calculated feature 01'),\n    Feature('ps_calc_02', float, name_extended='calculated feature 02'),\n    Feature('ps_calc_03', float, name_extended='calculated feature 03'),\n    Feature('ps_calc_04', int, name_extended='calculated feature 04'),\n    Feature('ps_calc_05', int, name_extended='calculated feature 05'),\n    Feature('ps_calc_06', int, name_extended='calculated feature 06'),\n    Feature('ps_calc_07', int, name_extended='calculated feature 07'),\n    Feature('ps_calc_08', int, name_extended='calculated feature 08'),\n    Feature('ps_calc_09', int, name_extended='calculated feature 09'),\n    Feature('ps_calc_10', int, name_extended='calculated feature 10'),\n    Feature('ps_calc_11', int, name_extended='calculated feature 11'),\n    Feature('ps_calc_12', int, name_extended='calculated feature 12'),\n    Feature('ps_calc_13', int, name_extended='calculated feature 13'),\n    Feature('ps_calc_14', int, name_extended='calculated feature 14'),\n    Feature('ps_calc_15_bin', int, name_extended='calculated binary feature 15'),\n    Feature('ps_calc_16_bin', int, name_extended='calculated binary feature 16'),\n    Feature('ps_calc_17_bin', int, name_extended='calculated binary feature 17'),\n    Feature('ps_calc_18_bin', int, name_extended='calculated binary feature 18'),\n    Feature('ps_calc_19_bin', int, name_extended='calculated binary feature 19'),\n    Feature('ps_calc_20_bin', int, name_extended='calculated binary feature 20'),\n\n],\n    documentation='https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data')\n\nSANTANDER_CUSTOMER_SATISFACTION_FEATURES = FeatureList(features=[\n    Feature('TARGET', int, is_target=True),\n    Feature('saldo_var30', float),  # importance: 0.0634\n    Feature('var15', float),  # importance: 0.0323\n    Feature('num_var30_0', float),  # importance: 0.0238\n    Feature('ind_var25_cte', float),  # importance: 0.0213\n    Feature('saldo_medio_var8_ult1', float),  # importance: 0.0191\n    Feature('var21', float),  # importance: 0.0182\n    Feature('imp_op_var41_efect_ult1', float),  # importance: 0.0162\n    Feature('num_var41_0', float),  # importance: 0.0162\n    Feature('ind_var26_cte', float),  # importance: 0.0151\n    Feature('ind_var39_0', float),  # importance: 0.0144\n    Feature('imp_var7_recib_ult1', float),  # importance: 0.0136\n    Feature('num_var26_0', float),  # importance: 0.0132\n    Feature('ind_var10cte_ult1', float),  # importance: 0.0131\n    Feature('var3', float),  # importance: 0.0128\n    Feature('imp_op_var41_efect_ult3', float),  # importance: 0.0126\n    Feature('num_meses_var12_ult3', float),  # importance: 0.0124\n    Feature('ind_var9_ult1', float),  # importance: 0.012\n    Feature('saldo_medio_var5_ult1', float),  # importance: 0.0117\n    Feature('saldo_medio_var5_hace2', float),  # importance: 0.0117\n    Feature('ind_var12_0', float),  # importance: 0.0115\n    Feature('ind_var8_0', float),  # importance: 0.0113\n    Feature('num_aport_var13_hace3', float),  # importance: 0.0109\n    Feature('saldo_medio_var5_ult3', float),  # importance: 0.0106\n    Feature('num_meses_var5_ult3', float),  # importance: 0.0106\n    Feature('num_var37_0', float),  # importance: 0.0105\n    Feature('ind_var41_0', float),  # importance: 0.0103\n    Feature('ind_var5', float),  # importance: 0.0102\n    Feature('var38', float),  # importance: 0.0101\n    Feature('num_var4', float),  # importance: 0.01\n    Feature('num_meses_var39_vig_ult3', float),  # importance: 0.0098\n    Feature('num_var22_ult1', float),  # importance: 0.0098\n    Feature('saldo_medio_var8_ult3', float),  # importance: 0.0096\n    Feature('var36', float),  # importance: 0.0094\n    ##################################################\n    ##################################################\n    # Feature('imp_op_var39_efect_ult1', float),  # importance: 0.0094\n    # Feature('imp_op_var39_ult1', float),  # importance: 0.0093\n    # Feature('saldo_medio_var5_hace3', float),  # importance: 0.0092\n    # Feature('num_op_var39_comer_ult1', float),  # importance: 0.0092\n    # Feature('num_var22_hace2', float),  # importance: 0.0091\n    # Feature('imp_ent_var16_ult1', float),  # importance: 0.0091\n    # Feature('num_var45_hace2', float),  # importance: 0.009\n    # Feature('num_op_var41_ult1', float),  # importance: 0.0089\n    # Feature('imp_op_var41_ult1', float),  # importance: 0.0089\n    # Feature('imp_var43_emit_ult1', float),  # importance: 0.0089\n    # Feature('saldo_var5', float),  # importance: 0.0085\n    # Feature('imp_op_var41_comer_ult1', float),  # importance: 0.0085\n    # Feature('num_op_var41_ult3', float),  # importance: 0.0083\n    # Feature('num_var45_hace3', float),  # importance: 0.0082\n    # Feature('saldo_var42', float),  # importance: 0.0082\n    # Feature('saldo_medio_var13_corto_hace2', float),  # importance: 0.008\n    # Feature('saldo_var26', float),  # importance: 0.0079\n    # Feature('num_op_var41_comer_ult3', float),  # importance: 0.0078\n    # Feature('ID', float),  # importance: 0.0078\n    # Feature('num_op_var39_efect_ult1', float),  # importance: 0.0078\n    # Feature('ind_var14_0', float),  # importance: 0.0077\n    # Feature('ind_var43_emit_ult1', float),  # importance: 0.0077\n    # Feature('num_var22_ult3', float),  # importance: 0.0077\n    # Feature('imp_op_var41_comer_ult3', float),  # importance: 0.0076\n    # Feature('num_op_var39_hace2', float),  # importance: 0.0076\n    # Feature('num_var45_ult1', float),  # importance: 0.0075\n    # Feature('num_var22_hace3', float),  # importance: 0.0073\n    # Feature('num_op_var39_efect_ult3', float),  # importance: 0.0071\n    # Feature('imp_op_var39_comer_ult1', float),  # importance: 0.0071\n    # Feature('num_var42_0', float),  # importance: 0.0071\n    # Feature('ind_var30', float),  # importance: 0.0071\n    # Feature('num_var35', float),  # importance: 0.007\n    # Feature('num_var45_ult3', float),  # importance: 0.007\n    # Feature('num_var43_recib_ult1', float),  # importance: 0.0069\n    # Feature('num_op_var39_ult1', float),  # importance: 0.0067\n    # Feature('saldo_var37', float),  # importance: 0.0067\n    # Feature('ind_var10_ult1', float),  # importance: 0.0066\n    # Feature('imp_trans_var37_ult1', float),  # importance: 0.0066\n    # Feature('imp_op_var39_comer_ult3', float),  # importance: 0.0065\n    # Feature('ind_var9_cte_ult1', float),  # importance: 0.0065\n    # Feature('saldo_medio_var12_hace2', float),  # importance: 0.0064\n    # Feature('num_var43_emit_ult1', float),  # importance: 0.0062\n    # Feature('saldo_medio_var12_ult3', float),  # importance: 0.006\n    # Feature('saldo_var13_corto', float),  # importance: 0.0059\n    # Feature('saldo_var13_largo', float),  # importance: 0.0057\n    # Feature('imp_sal_var16_ult1', float),  # importance: 0.0057\n    # Feature('num_op_var39_ult3', float),  # importance: 0.0056\n    # Feature('saldo_medio_var8_hace2', float),  # importance: 0.0056\n    # Feature('num_var37_med_ult2', float),  # importance: 0.0056\n    # Feature('num_op_var41_hace2', float),  # importance: 0.0056\n    # Feature('saldo_medio_var8_hace3', float),  # importance: 0.0055\n    # Feature('num_op_var39_comer_ult3', float),  # importance: 0.0055\n    # Feature('num_op_var39_hace3', float),  # importance: 0.0054\n    # Feature('saldo_medio_var13_corto_ult3', float),  # importance: 0.0053\n    # Feature('saldo_var13', float),  # importance: 0.0052\n    # Feature('saldo_medio_var12_ult1', float),  # importance: 0.0047\n    # Feature('imp_aport_var13_ult1', float),  # importance: 0.0047\n    # Feature('num_op_var41_comer_ult1', float),  # importance: 0.0047\n    # Feature('saldo_var24', float),  # importance: 0.0046\n    # Feature('ind_var37_cte', float),  # importance: 0.0045\n    # Feature('num_var5_0', float),  # importance: 0.0045\n    # Feature('ind_var24_0', float),  # importance: 0.0044\n    # Feature('imp_compra_var44_ult1', float),  # importance: 0.0044\n    # Feature('num_op_var41_efect_ult3', float),  # importance: 0.0043\n    # Feature('num_ent_var16_ult1', float),  # importance: 0.0042\n    # Feature('num_op_var41_efect_ult1', float),  # importance: 0.0041\n    # Feature('num_var39_0', float),  # importance: 0.004\n    # Feature('num_var13_0', float),  # importance: 0.004\n    # Feature('num_meses_var8_ult3', float),  # importance: 0.0039\n    # Feature('saldo_var25', float),  # importance: 0.0039\n    # Feature('num_var31_0', float),  # importance: 0.0038\n    # Feature('num_meses_var13_corto_ult3', float),  # importance: 0.0038\n    # Feature('num_var5', float),  # importance: 0.0036\n    # Feature('num_var25', float),  # importance: 0.0035\n    # Feature('num_var32_0', float),  # importance: 0.0033\n    # Feature('saldo_var40', float),  # importance: 0.0032\n    # Feature('num_var30', float),  # importance: 0.0032\n    # Feature('saldo_var12', float),  # importance: 0.003\n    # Feature('num_sal_var16_ult1', float),  # importance: 0.0027\n    # Feature('saldo_medio_var12_hace3', float),  # importance: 0.0024\n    # Feature('imp_op_var39_efect_ult3', float),  # importance: 0.0022\n    # Feature('imp_aport_var13_hace3', float),  # importance: 0.0016\n    # Feature('saldo_var8', float),  # importance: 0.0013\n    # Feature('imp_op_var40_efect_ult3', float),  # importance: 0.0011\n    # Feature('saldo_var2_ult1', float),  # importance: 0.0\n    # Feature('num_aport_var33_hace3', float),  # importance: 0.0\n    # Feature('ind_var34_0', float),  # importance: 0.0\n    # Feature('ind_var13_corto_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var33_hace2', float),  # importance: 0.0\n    # Feature('num_var46_0', float),  # importance: 0.0\n    # Feature('ind_var44_0', float),  # importance: 0.0\n    # Feature('num_var33_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var29_hace3', float),  # importance: 0.0\n    # Feature('ind_var5_0', float),  # importance: 0.0\n    # Feature('ind_var34', float),  # importance: 0.0\n    # Feature('saldo_var13_medio', float),  # importance: 0.0\n    # Feature('ind_var40', float),  # importance: 0.0\n    # Feature('num_trasp_var33_in_ult1', float),  # importance: 0.0\n    # Feature('num_var24_0', float),  # importance: 0.0\n    # Feature('num_var7_recib_ult1', float),  # importance: 0.0\n    # Feature('num_var7_emit_ult1', float),  # importance: 0.0\n    # Feature('ind_var31', float),  # importance: 0.0\n    # Feature('num_reemb_var13_ult1', float),  # importance: 0.0\n    # Feature('num_var46', float),  # importance: 0.0\n    # Feature('num_var39', float),  # importance: 0.0\n    # Feature('imp_venta_var44_ult1', float),  # importance: 0.0\n    # Feature('ind_var32_cte', float),  # importance: 0.0\n    # Feature('imp_aport_var33_hace3', float),  # importance: 0.0\n    # Feature('num_var13_medio', float),  # importance: 0.0\n    # Feature('imp_reemb_var17_hace3', float),  # importance: 0.0\n    # Feature('num_var13_medio_0', float),  # importance: 0.0\n    # Feature('num_var8_0', float),  # importance: 0.0\n    # Feature('saldo_var46', float),  # importance: 0.0\n    # Feature('saldo_var32', float),  # importance: 0.0\n    # Feature('ind_var32_0', float),  # importance: 0.0\n    # Feature('num_op_var40_efect_ult1', float),  # importance: 0.0\n    # Feature('ind_var26', float),  # importance: 0.0\n    # Feature('ind_var13_medio_0', float),  # importance: 0.0\n    # Feature('ind_var39', float),  # importance: 0.0\n    # Feature('num_op_var40_comer_ult1', float),  # importance: 0.0\n    # Feature('ind_var28', float),  # importance: 0.0\n    # Feature('delta_num_venta_var44_1y3', float),  # importance: 0.0\n    # Feature('num_var29', float),  # importance: 0.0\n    # Feature('num_var37', float),  # importance: 0.0\n    # Feature('num_venta_var44_ult1', float),  # importance: 0.0\n    # Feature('ind_var33', float),  # importance: 0.0\n    # Feature('ind_var43_recib_ult1', float),  # importance: 0.0\n    # Feature('saldo_var18', float),  # importance: 0.0\n    # Feature('imp_amort_var18_hace3', float),  # importance: 0.0\n    # Feature('saldo_var27', float),  # importance: 0.0\n    # Feature('saldo_medio_var17_ult1', float),  # importance: 0.0\n    # Feature('num_med_var22_ult3', float),  # importance: 0.0\n    # Feature('ind_var12', float),  # importance: 0.0\n    # Feature('ind_var25', float),  # importance: 0.0\n    # Feature('imp_trasp_var17_in_ult1', float),  # importance: 0.0\n    # Feature('ind_var27_0', float),  # importance: 0.0\n    # Feature('ind_var19', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_medio_ult1', float),  # importance: 0.0\n    # Feature('delta_imp_trasp_var33_out_1y3', float),  # importance: 0.0\n    # Feature('num_var12', float),  # importance: 0.0\n    # Feature('num_var6_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var44_hace2', float),  # importance: 0.0\n    # Feature('saldo_var44', float),  # importance: 0.0\n    # Feature('num_var34', float),  # importance: 0.0\n    # Feature('delta_num_aport_var33_1y3', float),  # importance: 0.0\n    # Feature('ind_var6', float),  # importance: 0.0\n    # Feature('ind_var6_0', float),  # importance: 0.0\n    # Feature('num_var13_largo_0', float),  # importance: 0.0\n    # Feature('num_var41', float),  # importance: 0.0\n    # Feature('ind_var37_0', float),  # importance: 0.0\n    # Feature('num_var25_0', float),  # importance: 0.0\n    # Feature('imp_trasp_var33_in_ult1', float),  # importance: 0.0\n    # Feature('ind_var13_corto', float),  # importance: 0.0\n    # Feature('ind_var30_0', float),  # importance: 0.0\n    # Feature('delta_num_trasp_var33_in_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var33_hace3', float),  # importance: 0.0\n    # Feature('num_var33', float),  # importance: 0.0\n    # Feature('ind_var26_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_largo_hace3', float),  # importance: 0.0\n    # Feature('ind_var13_0', float),  # importance: 0.0\n    # Feature('delta_imp_trasp_var33_in_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_medio_ult3', float),  # importance: 0.0\n    # Feature('delta_imp_amort_var34_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var17_hace3', float),  # importance: 0.0\n    # Feature('num_meses_var33_ult3', float),  # importance: 0.0\n    # Feature('saldo_medio_var17_ult3', float),  # importance: 0.0\n    # Feature('imp_var7_emit_ult1', float),  # importance: 0.0\n    # Feature('num_var12_0', float),  # importance: 0.0\n    # Feature('num_reemb_var33_ult1', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_largo_ult1', float),  # importance: 0.0\n    # Feature('imp_reemb_var13_ult1', float),  # importance: 0.0\n    # Feature('num_trasp_var17_out_hace3', float),  # importance: 0.0\n    # Feature('imp_op_var40_comer_ult1', float),  # importance: 0.0\n    # Feature('num_compra_var44_ult1', float),  # importance: 0.0\n    # Feature('num_var31', float),  # importance: 0.0\n    # Feature('imp_aport_var17_hace3', float),  # importance: 0.0\n    # Feature('num_aport_var17_ult1', float),  # importance: 0.0\n    # Feature('num_var26', float),  # importance: 0.0\n    # Feature('num_var44_0', float),  # importance: 0.0\n    # Feature('ind_var17', float),  # importance: 0.0\n    # Feature('num_op_var40_hace2', float),  # importance: 0.0\n    # Feature('imp_trasp_var33_out_ult1', float),  # importance: 0.0\n    # Feature('num_med_var45_ult3', float),  # importance: 0.0\n    # Feature('ind_var28_0', float),  # importance: 0.0\n    # Feature('num_var13', float),  # importance: 0.0\n    # Feature('imp_reemb_var33_hace3', float),  # importance: 0.0\n    # Feature('delta_num_trasp_var33_out_1y3', float),  # importance: 0.0\n    # Feature('imp_trasp_var17_in_hace3', float),  # importance: 0.0\n    # Feature('delta_imp_amort_var18_1y3', float),  # importance: 0.0\n    # Feature('num_var20_0', float),  # importance: 0.0\n    # Feature('num_var18', float),  # importance: 0.0\n    # Feature('delta_imp_aport_var13_1y3', float),  # importance: 0.0\n    # Feature('delta_imp_venta_var44_1y3', float),  # importance: 0.0\n    # Feature('num_venta_var44_hace3', float),  # importance: 0.0\n    # Feature('num_var20', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_medio_hace2', float),  # importance: 0.0\n    # Feature('num_var40_0', float),  # importance: 0.0\n    # Feature('saldo_var41', float),  # importance: 0.0\n    # Feature('num_var2_0_ult1', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_corto_hace3', float),  # importance: 0.0\n    # Feature('imp_op_var40_efect_ult1', float),  # importance: 0.0\n    # Feature('num_var28_0', float),  # importance: 0.0\n    # Feature('ind_var14', float),  # importance: 0.0\n    # Feature('ind_var37', float),  # importance: 0.0\n    # Feature('delta_imp_reemb_var13_1y3', float),  # importance: 0.0\n    # Feature('imp_reemb_var17_ult1', float),  # importance: 0.0\n    # Feature('delta_imp_trasp_var17_out_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_largo_ult3', float),  # importance: 0.0\n    # Feature('imp_trasp_var33_in_hace3', float),  # importance: 0.0\n    # Feature('num_trasp_var33_out_hace3', float),  # importance: 0.0\n    # Feature('num_var44', float),  # importance: 0.0\n    # Feature('num_var28', float),  # importance: 0.0\n    # Feature('num_var34_0', float),  # importance: 0.0\n    # Feature('ind_var46_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_corto_ult1', float),  # importance: 0.0\n    # Feature('saldo_var33', float),  # importance: 0.0\n    # Feature('ind_var8', float),  # importance: 0.0\n    # Feature('num_trasp_var33_out_ult1', float),  # importance: 0.0\n    # Feature('num_trasp_var33_in_hace3', float),  # importance: 0.0\n    # Feature('imp_op_var40_ult1', float),  # importance: 0.0\n    # Feature('saldo_var14', float),  # importance: 0.0\n    # Feature('num_reemb_var17_ult1', float),  # importance: 0.0\n    # Feature('num_aport_var17_hace3', float),  # importance: 0.0\n    # Feature('ind_var13', float),  # importance: 0.0\n    # Feature('delta_imp_compra_var44_1y3', float),  # importance: 0.0\n    # Feature('ind_var7_emit_ult1', float),  # importance: 0.0\n    # Feature('num_reemb_var13_hace3', float),  # importance: 0.0\n    # Feature('ind_var29_0', float),  # importance: 0.0\n    # Feature('delta_imp_aport_var33_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var44_ult1', float),  # importance: 0.0\n    # Feature('num_var40', float),  # importance: 0.0\n    # Feature('num_var17_0', float),  # importance: 0.0\n    # Feature('num_op_var41_hace3', float),  # importance: 0.0\n    # Feature('ind_var13_medio', float),  # importance: 0.0\n    # Feature('num_aport_var33_ult1', float),  # importance: 0.0\n    # Feature('num_trasp_var11_ult1', float),  # importance: 0.0\n    # Feature('num_op_var40_ult3', float),  # importance: 0.0\n    # Feature('num_aport_var13_ult1', float),  # importance: 0.0\n    # Feature('num_reemb_var33_hace3', float),  # importance: 0.0\n    # Feature('num_var29_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var44_ult3', float),  # importance: 0.0\n    # Feature('delta_num_reemb_var33_1y3', float),  # importance: 0.0\n    # Feature('saldo_medio_var33_ult1', float),  # importance: 0.0\n    # Feature('ind_var41', float),  # importance: 0.0\n    # Feature('ind_var33_0', float),  # importance: 0.0\n    # Feature('num_op_var40_hace3', float),  # importance: 0.0\n    # Feature('ind_var17_0', float),  # importance: 0.0\n    # Feature('ind_var20', float),  # importance: 0.0\n    # Feature('ind_var24', float),  # importance: 0.0\n    # Feature('imp_aport_var33_ult1', float),  # importance: 0.0\n    # Feature('ind_var18', float),  # importance: 0.0\n    # Feature('saldo_var20', float),  # importance: 0.0\n    # Feature('num_var27_0', float),  # importance: 0.0\n    # Feature('delta_num_compra_var44_1y3', float),  # importance: 0.0\n    # Feature('delta_num_trasp_var17_in_1y3', float),  # importance: 0.0\n    # Feature('imp_amort_var34_ult1', float),  # importance: 0.0\n    # Feature('ind_var46', float),  # importance: 0.0\n    # Feature('num_var18_0', float),  # importance: 0.0\n    # Feature('ind_var27', float),  # importance: 0.0\n    # Feature('num_var13_largo', float),  # importance: 0.0\n    # Feature('num_op_var40_comer_ult3', float),  # importance: 0.0\n    # Feature('num_var1', float),  # importance: 0.0\n    # Feature('imp_venta_var44_hace3', float),  # importance: 0.0\n    # Feature('saldo_var28', float),  # importance: 0.0\n    # Feature('num_var2_ult1', float),  # importance: 0.0\n    # Feature('num_var24', float),  # importance: 0.0\n    # Feature('num_compra_var44_hace3', float),  # importance: 0.0\n    # Feature('num_var17', float),  # importance: 0.0\n    # Feature('delta_num_reemb_var13_1y3', float),  # importance: 0.0\n    # Feature('imp_trasp_var33_out_hace3', float),  # importance: 0.0\n    # Feature('saldo_medio_var29_hace2', float),  # importance: 0.0\n    # Feature('ind_var31_0', float),  # importance: 0.0\n    # Feature('ind_var44', float),  # importance: 0.0\n    # Feature('num_meses_var13_largo_ult3', float),  # importance: 0.0\n    # Feature('delta_imp_trasp_var17_in_1y3', float),  # importance: 0.0\n    # Feature('num_trasp_var17_in_ult1', float),  # importance: 0.0\n    # Feature('num_var14_0', float),  # importance: 0.0\n    # Feature('saldo_var31', float),  # importance: 0.0\n    # Feature('saldo_medio_var17_hace2', float),  # importance: 0.0\n    # Feature('num_trasp_var17_out_ult1', float),  # importance: 0.0\n    # Feature('ind_var13_largo_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_medio_hace3', float),  # importance: 0.0\n    # Feature('num_meses_var44_ult3', float),  # importance: 0.0\n    # Feature('ind_var20_0', float),  # importance: 0.0\n    # Feature('ind_var1_0', float),  # importance: 0.0\n    # Feature('saldo_var34', float),  # importance: 0.0\n    # Feature('ind_var13_largo', float),  # importance: 0.0\n    # Feature('ind_var29', float),  # importance: 0.0\n    # Feature('saldo_medio_var29_ult1', float),  # importance: 0.0\n    # Feature('delta_num_trasp_var17_out_1y3', float),  # importance: 0.0\n    # Feature('num_meses_var17_ult3', float),  # importance: 0.0\n    # Feature('num_var13_corto_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var29_ult3', float),  # importance: 0.0\n    # Feature('num_var32', float),  # importance: 0.0\n    # Feature('imp_amort_var18_ult1', float),  # importance: 0.0\n    # Feature('delta_imp_aport_var17_1y3', float),  # importance: 0.0\n    # Feature('num_var6', float),  # importance: 0.0\n    # Feature('ind_var1', float),  # importance: 0.0\n    # Feature('num_var42', float),  # importance: 0.0\n    # Feature('imp_amort_var34_hace3', float),  # importance: 0.0\n    # Feature('num_var27', float),  # importance: 0.0\n    # Feature('num_trasp_var17_in_hace3', float),  # importance: 0.0\n    # Feature('num_var14', float),  # importance: 0.0\n    # Feature('delta_num_reemb_var17_1y3', float),  # importance: 0.0\n    # Feature('num_meses_var29_ult3', float),  # importance: 0.0\n    # Feature('num_var8', float),  # importance: 0.0\n    # Feature('num_reemb_var17_hace3', float),  # importance: 0.0\n    # Feature('delta_num_aport_var13_1y3', float),  # importance: 0.0\n    # Feature('imp_trasp_var17_out_ult1', float),  # importance: 0.0\n    # Feature('imp_reemb_var33_ult1', float),  # importance: 0.0\n    # Feature('saldo_var17', float),  # importance: 0.0\n    # Feature('delta_imp_reemb_var17_1y3', float),  # importance: 0.0\n    # Feature('imp_op_var40_comer_ult3', float),  # importance: 0.0\n    # Feature('ind_var7_recib_ult1', float),  # importance: 0.0\n    # Feature('imp_trasp_var17_out_hace3', float),  # importance: 0.0\n    # Feature('num_var1_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var33_ult3', float),  # importance: 0.0\n    # Feature('ind_var40_0', float),  # importance: 0.0\n    # Feature('ind_var2', float),  # importance: 0.0\n    # Feature('ind_var2_0', float),  # importance: 0.0\n    # Feature('saldo_medio_var13_largo_hace2', float),  # importance: 0.0\n    # Feature('ind_var32', float),  # importance: 0.0\n    # Feature('saldo_var29', float),  # importance: 0.0\n    # Feature('delta_imp_reemb_var33_1y3', float),  # importance: 0.0\n    # Feature('imp_compra_var44_hace3', float),  # importance: 0.0\n    # Feature('saldo_var1', float),  # importance: 0.0\n    # Feature('saldo_var6', float),  # importance: 0.0\n    # Feature('num_op_var40_efect_ult3', float),  # importance: 0.0\n    # Feature('ind_var25_0', float),  # importance: 0.0\n    # Feature('ind_var18_0', float),  # importance: 0.0\n    # Feature('num_meses_var13_medio_ult3', float),  # importance: 0.0\n    # Feature('num_op_var40_ult1', float),  # importance: 0.0\n    # Feature('imp_reemb_var13_hace3', float),  # importance: 0.0\n    # Feature('imp_aport_var17_ult1', float),  # importance: 0.0\n    # Feature('saldo_medio_var44_hace3', float),  # importance: 0.0\n    # Feature('num_var13_corto', float),  # importance: 0.0\n    # Feature('delta_num_aport_var17_1y3', float),  # importance: 0.0\n],\n    documentation='https://www.kaggle.com/competitions/santander-customer-satisfaction/data'\n)\n\nAMEX_DEFAULT_FEATURES = FeatureList(features=[\n\n    Feature('target', int, is_target=True, name_extended='default event within 120 days'),\n    Feature('P_2', float, name_extended='Payment 2'),  # importance: 0.0782\n    Feature('B_9', float, name_extended='Balance 9'),  # importance: 0.0652\n    Feature('D_51', float, name_extended='Delinquency 51'),  # importance: 0.0564\n    Feature('B_1', float, name_extended='Balance 1'),  # importance: 0.0448\n    Feature('D_45', float, name_extended='Delinquency 45'),  # importance: 0.0355\n    Feature('D_61', float, name_extended='Delinquency 61'),  # importance: 0.0335\n    Feature('D_75', float, name_extended='Delinquency 75'),  # importance: 0.0321\n    Feature('D_62', float, name_extended='Delinquency 62'),  # importance: 0.0307\n    Feature('R_27', float, name_extended='Risk 27'),  # importance: 0.0303\n    Feature('D_50', float, name_extended='Delinquency 50'),  # importance: 0.0255\n    Feature('S_3', float, name_extended='Spend 3'),  # importance: 0.0241\n    Feature('D_44', float, name_extended='Delinquency 44'),  # importance: 0.0236\n    Feature('B_3', float, name_extended='Balance 3'),  # importance: 0.0221\n    Feature('B_22', float, name_extended='Balance 22'),  # importance: 0.0209\n    Feature('D_56', float, name_extended='Delinquency 56'),  # importance: 0.02\n    Feature('D_79', float, name_extended='Delinquency 79'),  # importance: 0.0198\n    Feature('D_48', float, name_extended='Delinquency 48'),  # importance: 0.0188\n    Feature('B_38', float, name_extended='Balance 38'),  # importance: 0.0185\n    Feature('D_64', cat_dtype, name_extended='Delinquency 64'),  # importance: 0.0178\n    Feature('D_133', float, name_extended='Delinquency 133'),  # importance: 0.0158\n    Feature('D_43', float, name_extended='Delinquency 43'),  # importance: 0.0158\n    Feature('B_8', float, name_extended='Balance 8'),  # importance: 0.0157\n    Feature('B_10', float, name_extended='Balance 10'),  # importance: 0.0151\n    Feature('S_5', float, name_extended='Spend 5'),  # importance: 0.0148\n    Feature('B_7', float, name_extended='Balance 7'),  # importance: 0.0147\n    Feature('B_2', float, name_extended='Balance 2'),  # importance: 0.0138\n    Feature('S_15', float, name_extended='Spend 15'),  # importance: 0.0126\n    Feature('D_112', float, name_extended='Delinquency 112'),  # importance: 0.0123\n    Feature('D_52', float, name_extended='Delinquency 52'),  # importance: 0.0122\n    Feature('D_117', float, name_extended='Delinquency 117'),  # importance: 0.0121\n    Feature('R_3', float, name_extended='Risk 3'),  # importance: 0.0118\n    Feature('D_77', float, name_extended='Delinquency 77'),  # importance: 0.0111\n    Feature('D_63', cat_dtype, name_extended='Delinquency 63'),  # importance: 0.0111\n    ##################################################\n    ##################################################\n    # Feature('B_4', float, name_extended='Balance 4'),  # importance: 0.0109\n    # Feature('customer_ID', cat_dtype),  # importance: 0.0101\n    # Feature('D_121', float, name_extended='Delinquency 121'),  # importance: 0.0098\n    # Feature('D_47', float, name_extended='Delinquency 47'),  # importance: 0.0097\n    # Feature('B_5', float, name_extended='Balance 5'),  # importance: 0.009\n    # Feature('S_7', float, name_extended='Spend 7'),  # importance: 0.0085\n    # Feature('B_6', float, name_extended='Balance 6'),  # importance: 0.0083\n    # Feature('D_41', float, name_extended='Delinquency 41'),  # importance: 0.0082\n    # Feature('S_11', float, name_extended='Spend 11'),  # importance: 0.0078\n    # Feature('D_131', float, name_extended='Delinquency 131'),  # importance: 0.0073\n    # Feature('S_26', float, name_extended='Spend 26'),  # importance: 0.007\n    # Feature('D_128', float, name_extended='Delinquency 128'),  # importance: 0.0069\n    # Feature('D_46', float, name_extended='Delinquency 46'),  # importance: 0.0065\n    # Feature('S_9', float, name_extended='Spend 9'),  # importance: 0.0064\n    # Feature('S_2', cat_dtype, name_extended='Spend 2'),  # importance: 0.0063\n    # Feature('D_122', float, name_extended='Delinquency 122'),  # importance: 0.0061\n    # Feature('S_23', float, name_extended='Spend 23'),  # importance: 0.0059\n    # Feature('S_24', float, name_extended='Spend 24'),  # importance: 0.0058\n    # Feature('D_39', float, name_extended='Delinquency 39'),  # importance: 0.0055\n    # Feature('D_119', float, name_extended='Delinquency 119'),  # importance: 0.0054\n    # Feature('D_54', float, name_extended='Delinquency 54'),  # importance: 0.0047\n    # Feature('B_24', float, name_extended='Balance 24'),  # importance: 0.0046\n    # Feature('D_70', float, name_extended='Delinquency 70'),  # importance: 0.0043\n    # Feature('B_36', float, name_extended='Balance 36'),  # importance: 0.004\n    # Feature('R_1', float, name_extended='Risk 1'),  # importance: 0.0036\n    # Feature('B_19', float, name_extended='Balance 19'),  # importance: 0.0036\n    # Feature('D_144', float, name_extended='Delinquency 144'),  # importance: 0.0031\n    # Feature('D_53', float, name_extended='Delinquency 53'),  # importance: 0.0029\n    # Feature('B_21', float, name_extended='Balance 21'),  # importance: 0.0027\n    # Feature('S_22', float, name_extended='Spend 22'),  # importance: 0.0023\n    # Feature('B_18', float, name_extended='Balance 18'),  # importance: 0.0016\n    # Feature('B_11', float, name_extended='Balance 11'),  # importance: 0.0016\n    # Feature('P_3', float, name_extended='Payment 3'),  # importance: 0.0009\n    # Feature('D_59', float, name_extended='Delinquency 59'),  # importance: 0.0003\n    # Feature('R_5', float, name_extended='Risk 5'),  # importance: 0.0003\n    # Feature('B_37', float, name_extended='Balance 37'),  # importance: 0.0003\n    # Feature('B_13', float, name_extended='Balance 13'),  # importance: 0.0002\n    # Feature('B_25', float, name_extended='Balance 25'),  # importance: 0.0001\n    # Feature('B_20', float, name_extended='Balance 20'),  # importance: 0.0001\n    # Feature('B_15', float, name_extended='Balance 15'),  # importance: 0.0001\n    # Feature('D_124', float, name_extended='Delinquency 124'),  # importance: 0.0001\n    # Feature('B_17', float, name_extended='Balance 17'),  # importance: 0.0001\n    # Feature('B_23', float, name_extended='Balance 23'),  # importance: 0.0001\n    # Feature('S_19', float, name_extended='Spend 19'),  # importance: 0.0\n    # Feature('D_58', float, name_extended='Delinquency 58'),  # importance: 0.0\n    # Feature('S_17', float, name_extended='Spend 17'),  # importance: 0.0\n    # Feature('D_105', float, name_extended='Delinquency 105'),  # importance: 0.0\n    # Feature('B_33', float, name_extended='Balance 33'),  # importance: 0.0\n    # Feature('D_107', float, name_extended='Delinquency 107'),  # importance: 0.0\n    # Feature('D_82', float, name_extended='Delinquency 82'),  # importance: 0.0\n    # Feature('D_89', float, name_extended='Delinquency 89'),  # importance: 0.0\n    # Feature('D_123', float, name_extended='Delinquency 123'),  # importance: 0.0\n    # Feature('D_103', float, name_extended='Delinquency 103'),  # importance: 0.0\n    # Feature('R_16', float, name_extended='Risk 16'),  # importance: 0.0\n    # Feature('S_16', float, name_extended='Spend 16'),  # importance: 0.0\n    # Feature('D_55', float, name_extended='Delinquency 55'),  # importance: 0.0\n    # Feature('D_84', float, name_extended='Delinquency 84'),  # importance: 0.0\n    # Feature('D_113', float, name_extended='Delinquency 113'),  # importance: 0.0\n    # Feature('D_78', float, name_extended='Delinquency 78'),  # importance: 0.0\n    # Feature('B_26', float, name_extended='Balance 26'),  # importance: 0.0\n    # Feature('R_28', float, name_extended='Risk 28'),  # importance: 0.0\n    # Feature('D_127', float, name_extended='Delinquency 127'),  # importance: 0.0\n    # Feature('D_102', float, name_extended='Delinquency 102'),  # importance: 0.0\n    # Feature('D_104', float, name_extended='Delinquency 104'),  # importance: 0.0\n    # Feature('B_40', float, name_extended='Balance 40'),  # importance: 0.0\n    # Feature('R_15', float, name_extended='Risk 15'),  # importance: 0.0\n    # Feature('R_10', float, name_extended='Risk 10'),  # importance: 0.0\n    # Feature('D_143', float, name_extended='Delinquency 143'),  # importance: 0.0\n    # Feature('R_7', float, name_extended='Risk 7'),  # importance: 0.0\n    # Feature('R_4', float, name_extended='Risk 4'),  # importance: 0.0\n    # Feature('D_92', float, name_extended='Delinquency 92'),  # importance: 0.0\n    # Feature('B_28', float, name_extended='Balance 28'),  # importance: 0.0\n    # Feature('D_65', float, name_extended='Delinquency 65'),  # importance: 0.0\n    # Feature('R_2', float, name_extended='Risk 2'),  # importance: 0.0\n    # Feature('D_140', float, name_extended='Delinquency 140'),  # importance: 0.0\n    # Feature('S_25', float, name_extended='Spend 25'),  # importance: 0.0\n    # Feature('R_6', float, name_extended='Risk 6'),  # importance: 0.0\n    # Feature('P_4', float, name_extended='Payment 4'),  # importance: 0.0\n    # Feature('B_14', float, name_extended='Balance 14'),  # importance: 0.0\n    # Feature('D_60', float, name_extended='Delinquency 60'),  # importance: 0.0\n    # Feature('R_20', float, name_extended='Risk 20'),  # importance: 0.0\n    # Feature('S_6', float, name_extended='Spend 6'),  # importance: 0.0\n    # Feature('S_27', float, name_extended='Spend 27'),  # importance: 0.0\n    # Feature('S_8', float, name_extended='Spend 8'),  # importance: 0.0\n    # Feature('D_74', float, name_extended='Delinquency 74'),  # importance: 0.0\n    # Feature('D_83', float, name_extended='Delinquency 83'),  # importance: 0.0\n    # Feature('S_12', float, name_extended='Spend 12'),  # importance: 0.0\n    # Feature('R_11', float, name_extended='Risk 11'),  # importance: 0.0\n    # Feature('R_17', float, name_extended='Risk 17'),  # importance: 0.0\n    # Feature('D_81', float, name_extended='Delinquency 81'),  # importance: 0.0\n    # Feature('S_13', float, name_extended='Spend 13'),  # importance: 0.0\n    # Feature('B_41', float, name_extended='Balance 41'),  # importance: 0.0\n    # Feature('D_72', float, name_extended='Delinquency 72'),  # importance: 0.0\n    # Feature('B_16', float, name_extended='Balance 16'),  # importance: 0.0\n    # Feature('B_32', float, name_extended='Balance 32'),  # importance: 0.0\n    # Feature('D_141', float, name_extended='Delinquency 141'),  # importance: 0.0\n    # Feature('B_12', float, name_extended='Balance 12'),  # importance: 0.0\n    # Feature('D_80', float, name_extended='Delinquency 80'),  # importance: 0.0\n    # Feature('R_19', float, name_extended='Risk 19'),  # importance: 0.0\n    # Feature('D_68', float, name_extended='Delinquency 68'),  # importance: 0.0\n    # Feature('B_30', float, name_extended='Balance 30'),  # importance: 0.0\n    # Feature('D_69', float, name_extended='Delinquency 69'),  # importance: 0.0\n    # Feature('D_94', float, name_extended='Delinquency 94'),  # importance: 0.0\n    # Feature('D_130', float, name_extended='Delinquency 130'),  # importance: 0.0\n    # Feature('B_27', float, name_extended='Balance 27'),  # importance: 0.0\n    # Feature('D_96', float, name_extended='Delinquency 96'),  # importance: 0.0\n    # Feature('D_115', float, name_extended='Delinquency 115'),  # importance: 0.0\n    # Feature('B_31', float, name_extended='Balance 31'),  # importance: 0.0\n    # Feature('D_125', float, name_extended='Delinquency 125'),  # importance: 0.0\n    # Feature('D_139', float, name_extended='Delinquency 139'),  # importance: 0.0\n    # Feature('D_145', float, name_extended='Delinquency 145'),  # importance: 0.0\n    # Feature('D_116', float, name_extended='Delinquency 116'),  # importance: 0.0\n    # Feature('D_120', float, name_extended='Delinquency 120'),  # importance: 0.0\n    # Feature('S_20', float, name_extended='Spend 20'),  # importance: 0.0\n    # Feature('D_126', float, name_extended='Delinquency 126'),  # importance: 0.0\n    # Feature('R_18', float, name_extended='Risk 18'),  # importance: 0.0\n    # Feature('D_109', float, name_extended='Delinquency 109'),  # importance: 0.0\n    # Feature('D_129', float, name_extended='Delinquency 129'),  # importance: 0.0\n    # Feature('R_13', float, name_extended='Risk 13'),  # importance: 0.0\n    # Feature('D_86', float, name_extended='Delinquency 86'),  # importance: 0.0\n    # Feature('R_22', float, name_extended='Risk 22'),  # importance: 0.0\n    # Feature('R_21', float, name_extended='Risk 21'),  # importance: 0.0\n    # Feature('R_24', float, name_extended='Risk 24'),  # importance: 0.0\n    # Feature('R_8', float, name_extended='Risk 8'),  # importance: 0.0\n    # Feature('R_14', float, name_extended='Risk 14'),  # importance: 0.0\n    # Feature('D_91', float, name_extended='Delinquency 91'),  # importance: 0.0\n    # Feature('R_25', float, name_extended='Risk 25'),  # importance: 0.0\n    # Feature('D_114', float, name_extended='Delinquency 114'),  # importance: 0.0\n    # Feature('D_118', float, name_extended='Delinquency 118'),  # importance: 0.0\n    # Feature('D_93', float, name_extended='Delinquency 93'),  # importance: 0.0\n    # Feature('R_12', float, name_extended='Risk 12'),  # importance: 0.0\n    # Feature('D_71', float, name_extended='Delinquency 71'),  # importance: 0.0\n    # Feature('R_23', float, name_extended='Risk 23'),  # importance: 0.0\n    # Feature('S_18', float),  # importance: 0.0\n],\n    documentation='https://www.kaggle.com/competitions/amex-default-prediction/data')\n\nAD_FRAUD_FEATURES = FeatureList(features=[\n    Feature('ip', int, name_extended='ip address'),\n    Feature('app', int, name_extended='app id'),\n    Feature('device', int, name_extended='device type id of user mobile phone'),\n    Feature('os', int),\n    Feature('channel', int, name_extended='channel id of mobile ad publisher'),\n    Feature('click_time', cat_dtype, name_extended='UTC timestamp of click'),\n    # Feature('attributed_time', cat_dtype),\n    Feature('is_attributed', int, is_target=True,\n            name_extended='indicator for whether app was downloaded'),\n],\n    documentation='https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection/data')\n\n\ndef preprocess_otto(df: pd.DataFrame) -> pd.DataFrame:\n    df['target'] = df['target'].apply(lambda x: x.replace('Class_', '')).astype(\n        int)\n    return df\n\n\ndef preprocess_walmart(df: pd.DataFrame) -> pd.DataFrame:\n    df[WALMART_FEATURES.target] = df[WALMART_FEATURES.target].astype(int)\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/mooc.py", "content": "\"\"\"\nUtilities for the Harvardx-MITx MOOC datasets.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nMOOC_FEATURES = FeatureList(features=[\n    Feature('course_id', cat_dtype, \"\"\"Administrative, string, identifies \n    institution (HarvardX or MITx), course name, and semester, \n    e.g. 'HarvardX/CB22x/2013_Spring'.\"\"\"),\n    Feature('certified', int, \"\"\"Administrative, 0/1; anyone who earned a \n    certificate. Certificates are based on course grades, and depending on \n    the course, the cutoff for a certificate varies from 50% - 80%.\"\"\",\n            is_target=True),\n    Feature('viewed', int, \"\"\"Administrative, 0/1; anyone who accessed the \n    ‘Courseware’ tab (the home of the videos, problem sets, and exams) within \n    the edX platform for the course. Note that there exist course materials \n    outside of the 'Courseware' tab, such as the Syllabus or the Discussion \n    forums.\"\"\"),\n    Feature('explored', int, \"\"\"administrative, 0/1; anyone who accessed at \n    least half of the chapters in the courseware (chapters are the highest \n    level on the 'courseware' menu housing course content).\"\"\"),\n    Feature('final_cc_cname_DI', cat_dtype, \"\"\"mix of administrative (\n    computed from IP address) and user- provided (filled in from student \n    address if available when IP was indeterminate); during \n    de-identification, some country names were replaced with the \n    corresponding continent/region name. Examples: 'Other South Asia' or \n    '“'Russian Federation'”'.\"\"\"),\n    Feature('LoE_DI', cat_dtype, \"\"\"user-provided, highest level of education \n    completed. Possible values: 'Less than Secondary,' 'Secondary,\n    ' 'Bachelor’s,' 'Master’s,' and 'Doctorate.'\"\"\"),\n    Feature('YoB', int, \"\"\"user-provided, year of birth. Example: '1980'.\"\"\"),\n    Feature('gender', cat_dtype, \"\"\"user-provided. Possible values: m (male), \n    f (female) and o (other). Note that 'o' is dropped for this dataset,\n    as it only contained 8 observations.\"\"\"),\n    Feature('nevents', int, \"\"\"administrative, number of interactions with \n    the course, recorded in the tracking logs; blank if no interactions \n    beyond registration. Example: '502'.\"\"\"),\n    Feature('ndays_act', int, \"\"\"administrative, number of unique days \n    student interacted with course. Example: '16'.\"\"\"),\n    Feature('nplay_video', int, \"\"\"administrative, number of play video \n    events within the course. Example: '52'.\"\"\"),\n    Feature('nchapters', int, \"\"\"administrative, number of chapters (within \n    the Courseware) with which the student interacted. Example: '12'.\"\"\"),\n    Feature('nforum_posts', int, \"\"\"administrative, number of posts to the \n    Discussion Forum. Example: '8'.\"\"\"),\n    Feature('days_from_start_to_last_event', int, \"\"\"Derived feature. \n    Computes the number of days from a students' first recorded interction \n    with the course platform to their last interaction.\"\"\"),\n],\n    documentation=\"https://dataverse.harvard.edu/file.xhtml?persistentId=doi\"\n                  \":10.7910/DVN/26147/FD5IES&version=11.2\")\n\n_usecols = ['course_id', 'certified', 'viewed',\n            'explored',\n            'final_cc_cname_DI', 'LoE_DI', 'YoB',\n            'gender',\n            'start_time_DI', 'last_event_DI', 'nevents',\n            'ndays_act',\n            'nplay_video',\n            'nchapters', 'nforum_posts',\n            'incomplete_flag']\n\n\ndef preprocess_mooc(df: pd.DataFrame) -> pd.DataFrame:\n    df = df[_usecols]\n\n    # Drop incomplete records due to data processing issue\n    # (see Person-Course Documentation PDF at link above)\n    df = df[df.incomplete_flag != 1.0]\n\n    for col in ('start_time_DI', 'last_event_DI'):\n        df[col] = df[col].fillna('').apply(\n            lambda x: datetime.strptime(x, '%m/%d/%y') if x else np.nan)\n\n    df['days_from_start_to_last_event'] = (\n            df['last_event_DI'] - df['start_time_DI']) \\\n        .apply(lambda timedelta: timedelta.days).fillna(0)\n\n    df.drop(columns=['start_time_DI', 'last_event_DI', 'incomplete_flag'],\n            inplace=True)\n\n    # Drop students that did not declare gender; note 8 obs with gender\n    # \"other\" are also dropped.\n    df = df[df.gender.isin(['m', 'f'])]\n\n    # Fill in zeros for numeric columns; they are coded as empty values\n    for colname in (\n            'nevents', 'ndays_act', 'nplay_video', 'nchapters',\n            'nforum_posts'):\n        df[colname] = df[colname].fillna(0)\n\n    df['LoE_DI'] = df['LoE_DI'].fillna('NotProvided')\n\n    df.dropna(inplace=True)  # drops users without YoB; appx 4392 obs.\n\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/mimic_extract.py", "content": "\"\"\"\nUtilities for the MIMIC-Extract dataset.\n\nMIMIC-Extract requires access to the MIMIC dataset, which is a public\ncredentialized dataset. Obtain access from the Physionet website.\nSee the instructions at the links below.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\"\"\"\n\nimport logging\nfrom typing import List, Union\n\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\nfrom tableshift.core.utils import sub_illegal_chars\nfrom tableshift.datasets.mimic_extract_feature_lists import \\\n    MIMIC_EXTRACT_SHARED_FEATURES, _MIMIC_EXTRACT_LOS_3_SELECTED_FEATURES, \\\n    _MIMIC_EXTRACT_MORT_HOSP_SELECTED_FEATURES\nfrom tableshift.datasets.utils import convert_numeric_dtypes, complete_cases\n\nID_COLS = ['subject_id', 'hadm_id', 'icustay_id']\n\nMIMIC_EXTRACT_STATIC_FEATURES = FeatureList(features=[\n    Feature(\"gender\", cat_dtype, \"Gender (M/F).\"),\n    Feature(\"age\", float, \"Age.\"),\n    Feature(\"ethnicity\", cat_dtype, \"Ethnicity (41 unique values).\"),\n    Feature(\"insurance\", cat_dtype,\n            \"Medicare, Private, Medicaid, Government, Self Pay.\")\n])\n\nMIMIC_EXTRACT_LOS_3_FEATURES = FeatureList(features=[\n    *MIMIC_EXTRACT_STATIC_FEATURES,\n    *MIMIC_EXTRACT_SHARED_FEATURES,\n    Feature('los_3', int, is_target=True)\n])\n\nMIMIC_EXTRACT_MORT_HOSP_FEATURES = FeatureList(features=[\n    *MIMIC_EXTRACT_STATIC_FEATURES,\n    *MIMIC_EXTRACT_SHARED_FEATURES,\n    Feature('mort_hosp', int, is_target=True)\n])\n\nMIMIC_EXTRACT_LOS_3_SELECTED_FEATURES = FeatureList(features=[\n    *_MIMIC_EXTRACT_LOS_3_SELECTED_FEATURES,\n    Feature('los_3', int, is_target=True)\n])\nMIMIC_EXTRACT_MORT_HOSP_SELECTED_FEATURES = FeatureList(features=[\n    *_MIMIC_EXTRACT_MORT_HOSP_SELECTED_FEATURES,\n    Feature('mort_hosp', int, is_target=True)\n])\n\n\ndef simple_imputer(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Via https://github.com/MLforHealth/MIMIC_Extract/blob/\n    master/notebooks/Baselines%20for%20Mortality%20and%20LOS\n    %20prediction%20-%20Sklearn.ipynb \"\"\"\n    idx = pd.IndexSlice\n    df = df.copy()\n    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(\n        ('label', 'LEVEL1', 'LEVEL2'))\n\n    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n\n    df_out.loc[:, idx[:, 'mean']] = df_out.loc[:, idx[:, 'mean']].groupby(\n        ID_COLS).fillna(\n        method='ffill'\n    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n\n    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(\n        float)\n    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function',\n                  inplace=True)\n\n    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n    hours_of_absence = is_absent.cumsum()\n    time_since_measured = hours_of_absence - hours_of_absence[\n        is_absent == 0].fillna(method='ffill')\n    time_since_measured.rename(columns={'mask': 'time_since_measured'},\n                               level='Aggregation Function', inplace=True)\n\n    df_out = pd.concat((df_out, time_since_measured), axis=1)\n    df_out.loc[:, idx[:, 'time_since_measured']] = \\\n        df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n\n    df_out.sort_index(axis=1, inplace=True)\n    return df_out\n\n\ndef preprocess_mimic_extract(df: pd.DataFrame, task: str,\n                             static_features: List[str]) -> pd.DataFrame:\n    \"\"\"Apply the default MIMIC-extract preprocessing.\n\n    Specifically, this includes flattening the data and imputing missing\n    values. \"\"\"\n    # Remove Ys and static features; this allows us to use the same\n    # preprocessing code to flatten the time-varying features as in the MIMIC\n    # provided notebooks.\n\n    assert task in df.columns, f\"task {task} not in dataframe columns.\"\n    Ys = df.pop(task)\n    # Extract statics via multicolumn 'pop'\n    statics = df[static_features]\n    df.drop(columns=static_features, inplace=True)\n\n    # Merging with \"flat\" (non-hierarchical) static features and labels\n    # flattens the MultiIndex of the data. Here we restore it, so that we can\n    # use the MIMIC-extract imputer.\n    df.columns = pd.MultiIndex.from_tuples(\n        df.columns, names=['LEVEL2', 'Aggregation Function'])\n\n    logging.debug(f\"performing imputation on dataframe of shape {df.shape}\")\n    df = simple_imputer(df)\n    logging.debug(f\"pivoting dataframe of shape {df.shape}\")\n    flattened_df = df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'],\n                                  columns=['hours_in'])\n\n    # Flatten the columns prior to joining and clean up column names\n    flattened_df.columns = flattened_df.columns.to_flat_index()\n    flattened_df.columns = [\"_\".join(str(x).replace(' ', '_') for x in c) for c\n                            in flattened_df.columns]\n\n    # All of these targets do not vary based on time, so we can drop the\n    # 'hours_in' level (which is not present in the flattened DataFrame index\n    # either after flattening) and merge on the remaining 3-level index with\n    # levels (subject_id, hadm_id, icustay_id).\n    NON_TIME_VARYING_TARGETS = ('los_3', 'los_7', 'mort_hosp', 'mort_icu')\n    assert Ys.name in NON_TIME_VARYING_TARGETS, \\\n        f\"sanity check that label {Ys.name} is not time-varying.\"\n\n    def _drop_hours_in_index(df_in: Union[pd.Series, pd.DataFrame]) -> Union[\n        pd.Series, pd.DataFrame]:\n        \"\"\"Helper function to remove the hours_in index level.\"\"\"\n        assert df_in.index.names == ['subject_id', 'hadm_id', 'icustay_id',\n                                     'hours_in']\n        df_in = df_in.droplevel('hours_in')\n        return df_in[~df_in.index.duplicated(keep='first')]\n\n    # Drop the hours_in from index and then drop duplicate indices (the\n    # values are the same at every value of hours_in for each unique (\n    # subject_id, hadm_id, icustay_id) index, since the label and static\n    # values are all not time-varying.\n    Ys = _drop_hours_in_index(Ys)\n    statics = _drop_hours_in_index(statics)\n\n    flattened_df_rows_pre_joins = len(flattened_df)\n    flattened_df = flattened_df.join(Ys, how=\"inner\")\n    flattened_df = flattened_df.join(statics, how=\"inner\")\n    assert len(flattened_df) == flattened_df_rows_pre_joins, \\\n        \"sanity check no data loss when joining data to labels/statics.\"\n\n    # Remove the index and verify that all of the index levels are unique.\n    # This should be the case because MIMIC-extract only keeps the first ICU\n    # visit for each patient. This ensures that any downstream splitting does\n    # not leak data across subject_id/hadm_id/icustay_id.\n    idxnames = flattened_df.index.names\n    flattened_df.reset_index(inplace=True)\n    for idxname in idxnames:\n        assert flattened_df[idxname].nunique() == len(flattened_df), \\\n            f\"values for index level {idxname} are not unique.\"\n    flattened_df.drop(columns=idxnames, inplace=True)\n\n    len_pre_drop = len(flattened_df)\n    flattened_df = complete_cases(flattened_df)\n    logging.info(\n        f\"dropped {len_pre_drop - len(flattened_df)} rows \"\n        f\"({100 * (len_pre_drop - len(flattened_df)) / len_pre_drop:.2f})% \"\n        f\"of data) containing missing values \"\n        f\"after imputation (could be due to missing static features).\")\n\n    flattened_df = convert_numeric_dtypes(flattened_df)\n\n    # Clean up variable names, since most columns are passed-through by the\n    # TableShift preprocessor.\n    flattened_df.columns = [sub_illegal_chars(c) for c in flattened_df.columns]\n\n    return flattened_df\n"}
{"type": "source_file", "path": "tableshift/datasets/physionet.py", "content": "\"\"\"\nUtilities for the Physionet (sepsis prediction) dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nPHYSIONET_FEATURES = FeatureList(features=[\n    Feature(\"HR\", float, name_extended=\"Heart rate (in beats per minute)\"),\n    Feature(\"O2Sat\", float, name_extended=\"Pulse oximetry (%)\"),\n    Feature(\"Temp\", float, name_extended=\"Temperature (deg C)\"),\n    Feature(\"SBP\", float, name_extended=\"Systolic BP (mm Hg)\"),\n    Feature(\"MAP\", float, name_extended=\"Mean arterial pressure (mm Hg)\"),\n    Feature(\"DBP\", float, name_extended=\"Diastolic BP (mm Hg)\"),\n    Feature(\"Resp\", float,\n            name_extended=\"Respiration rate (breaths per minute)\"),\n    Feature(\"EtCO2\", float, name_extended=\"End tidal carbon dioxide (mm Hg)\"),\n    Feature(\"BaseExcess\", float, name_extended=\"Excess bicarbonate (mmol/L)\"),\n    Feature(\"HCO3\", float, name_extended=\"Bicarbonate (mmol/L)\"),\n    Feature(\"FiO2\", float, name_extended=\"Fraction of inspired oxygen (%)\"),\n    Feature(\"pH\", float, name_extended=\"pH\"),\n    Feature(\"PaCO2\", float,\n            name_extended=\"Partial pressure of carbon dioxide from arterial \"\n                          \"blood (mm Hg)\"),\n    Feature(\"SaO2\", float,\n            name_extended=\"Oxygen saturation from arterial blood (%)\"),\n    Feature(\"AST\", float, name_extended=\"Aspartate transaminase (IU/L)\"),\n    Feature(\"BUN\", float, name_extended=\"Blood urea nitrogen (mg/dL)\"),\n    Feature(\"Alkalinephos\", float, name_extended=\"Alkaline phosphatase (IU/L)\"),\n    Feature(\"Calcium\", float, name_extended=\"Calcium (mg/dL)\"),\n    Feature(\"Chloride\", float, name_extended=\"Chloride (mmol/L)\"),\n    Feature(\"Creatinine\", float, name_extended=\"Creatinine (mg/dL)\"),\n    Feature(\"Bilirubin_direct\", float,\n            name_extended=\"Direct bilirubin (mg/dL)\"),\n    Feature(\"Glucose\", float, name_extended=\"Serum glucose (mg/dL)\"),\n    Feature(\"Lactate\", float, name_extended=\"Lactic acid (mg/dL)\"),\n    Feature(\"Magnesium\", float, name_extended=\"Magnesium (mmol/dL)\"),\n    Feature(\"Phosphate\", float, name_extended=\"Phosphate (mg/dL)\"),\n    Feature(\"Potassium\", float, name_extended=\"Potassium (mmol/L)\"),\n    Feature(\"Bilirubin_total\", float, name_extended=\"Total bilirubin (mg/dL)\"),\n    Feature(\"TroponinI\", float, name_extended=\"Troponin I (ng/mL)\"),\n    Feature(\"Hct\", float, name_extended=\"Hematocrit (%)\"),\n    Feature(\"Hgb\", float, name_extended=\"Hemoglobin (g/dL)\"),\n    Feature(\"PTT\", float,\n            name_extended=\"Partial thromboplastin time (seconds)\"),\n    Feature(\"WBC\", float, name_extended=\"Leukocyte count (count/L)\"),\n    Feature(\"Fibrinogen\", float,\n            name_extended=\"Fibrinogen concentration (mg/dL)\"),\n    Feature(\"Platelets\", float, name_extended=\"Platelet count (count/mL)\"),\n    Feature(\"Age\", int, name_extended=\"Age (years)\"),\n    Feature(\"Gender\", int, name_extended=\"Female (0) or male (1)\"),\n    Feature(\"Unit1\", int,\n            name_extended=\"Administrative identifier for ICU unit (MICU); \"\n                          \"false (0) or true (1)\"),\n    Feature(\"Unit2\", int,\n            name_extended=\"Administrative identifier for ICU unit (SICU); \"\n                          \"false (0) or true (1)\"),\n    Feature(\"HospAdmTime\", float,\n            name_extended=\"Time between hospital and ICU admission (\"\n                          \"hours since ICU admission)\"),\n    Feature(\"ICULOS\", float,\n            name_extended=\"ICU length of stay (hours since ICU admission)\"),\n    Feature(\"SepsisLabel\", int,\n            name_extended=\"For septic patients, SepsisLabel is 1 if t ≥ \"\n                          \"t_sepsis − 6 and 0 if t < t_sepsis − 6. For \"\n                          \"non-septic patients, SepsisLabel is 0.\",\n            is_target=True),\n    Feature(\"set\", cat_dtype,\n            \"The training set (i..e hospital) from which an example is drawn \"\n            \"unique (values: 'a', 'b').\")\n], documentation=\"https://physionet.org/content/challenge-2019/1.0.0\"\n                 \"/physionet_challenge_2019_ccm_manuscript.pdf\")\n\n\ndef preprocess_physionet(df: pd.DataFrame) -> pd.DataFrame:\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/nhanes.py", "content": "\"\"\"\nNHANES-related tools. See also the documentation at the link below:\nhttps://www.cdc.gov/Nchs/Nhanes/about_nhanes.htm\n\nNHANES is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nimport json\nimport logging\nimport os\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nDEFAULT_NHANES_CODING = {\n    \"1.0\": \"Yes\",\n    \"2.0\": \"No\",\n    \"7.0\": \"Refused\",\n    \"9.0\": \"Don't know\",\n}\n\nNHANES_YEARS = [1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, 2015, 2017]\n\n# Dictionary mapping years to data sources. Because NHANES uses the same\n# name for each file, we need to manually track the year associated with\n# each dataset.\nNHANES_DATA_SOURCES = os.path.join(os.path.dirname(__file__),\n                                   \"nhanes_data_sources.json\")\n\n# Mapping of NHANES component types to names of data sources to use. See\n# nhanes_data_sources.json. This ensures that only needed files are\n# downloaded/read from disk, because NHANES contains a huge number of sources\n# per year.\n\nNHANES_CHOLESTEROL_DATA_SOURCES_TO_USE = {\n    \"Demographics\": [\n        \"Demographic Variables & Sample Weights\",  # 1999 - 2003\n        \"Demographic Variables and Sample Weights\"],  # 2005- 2017\n    \"Questionnaire\": [\"Blood Pressure & Cholesterol\",  # All years\n                      \"Cardiovascular Health\",  # All years\n                      \"Diabetes\",  # All years\n                      \"Kidney Conditions\",  # 1999\n                      \"Kidney Conditions - Urology\",  # 2001 - 2017\n                      \"Medical Conditions\",  # All years\n                      \"Osteoporosis\",  # Not preset in 2011, 2015\n                      ],\n    \"Laboratory\": [\n        \"Cholesterol - LDL & Triglycerides\",\n        # 1999 - 2003, 2007 - 2013\n        \"Cholesterol - LDL, Triglyceride & Apoliprotein (ApoB)\",\n        # 2005\n        \"Cholesterol - Low - Density Lipoprotein (LDL) & Triglycerides\",\n        # 2015\n        \"Cholesterol - Low-Density Lipoproteins (LDL) & Triglycerides\"\n        # 2017\n    ],\n}\n\nNHANES_LEAD_DATA_SOURCES_TO_USE = {\n    \"Demographics\": [\n        \"Demographic Variables & Sample Weights\",  # 1999 - 2003\n        \"Demographic Variables and Sample Weights\"],  # 2005- 2017\n    \"Questionnaire\": [\"Diet Behavior & Nutrition\",\n                      # Note: prior to 2017 income questions are in\n                      # Demographics file.\n                      \"Income\"\n                      ],\n    \"Laboratory\": [\n        \"Cadmium, Lead, Mercury, Cotinine & Nutritional Biochemistries\",  # 1999\n        \"Cadmium, Lead, Total Mercury, Ferritin, Serum Folate, RBC Folate, \"\n        \"Vitamin B12, Homocysteine, Methylmalonic \"\n        \"acid, Cotinine - Blood, Second Exam\",  # 2001\n        \"Cadmium, Lead, & Total Mercury - Blood\",  # 2003\n        \"Cadmium, Lead, & Total Mercury - Blood\",  # 2005\n        \"Cadmium, Lead, & Total Mercury - Blood\",  # 2007\n        \"Cadmium, Lead, & Total Mercury - Blood\",  # 2009\n        \"Cadmium, Lead, Total Mercury, Selenium, & Manganese - Blood\",  # 2011\n        \"Lead, Cadmium, Total Mercury, Selenium, and Manganese - Blood\",  # 2013\n        \"Lead, Cadmium, Total Mercury, Selenium & Manganese - Blood\",  # 2015\n        \"Lead, Cadmium, Total Mercury, Selenium, & Manganese - Blood\"]  # 2017\n}\n\n\ndef get_nhanes_data_sources(task: str, years=None):\n    \"\"\"Fetch a mapping of {year: list of urls} for NHANES.\"\"\"\n    years = [int(x) for x in years]\n    if task == \"cholesterol\":\n        data_sources_to_use = NHANES_CHOLESTEROL_DATA_SOURCES_TO_USE\n    elif task == \"lead\":\n        data_sources_to_use = NHANES_LEAD_DATA_SOURCES_TO_USE\n    else:\n        raise ValueError\n\n    output = defaultdict(list)\n    with open(NHANES_DATA_SOURCES, \"r\") as f:\n        data_sources = json.load(f)\n    for year, components in data_sources.items():\n        if (years is not None) and (int(year) in years):\n            for component, sources in components.items():\n                for source_name, source_url in sources.items():\n                    if source_name in data_sources_to_use[component]:\n                        output[year].append(source_url)\n    return output\n\n\nNHANES_CHOLESTEROL_FEATURES = FeatureList(features=[\n\n    Feature('LBDLDL', float, is_target=True,\n            description='Direct LDL-Cholesterol (mg/dL)'),\n\n    # Below we use the additional set of risk factors listed in the above report\n    # (Table 6) **which can be asked in a questionnaire** (i.e. those which\n    # do not require laboratory testing).\n\n    ####### Risk Factor: Family history of ASCVD\n\n    # No questions on this topic.\n\n    ####### Risk Factor: Metabolic syndrome (increased waist circumference,\n    # elevated triglycerides [>175 mg/dL], elevated blood pressure,\n    # elevated glucose, and low HDL-C [<40 mg/dL in men; <50 in women\n    # mg/dL] are factors; tally of 3 makes the diagnosis)\n\n    Feature('BPQ020', cat_dtype, \"\"\"{Have you/Has SP} ever been told by a \n    doctor or other health professional # that {you/s/he} had hypertension, \n    also called high blood pressure?\"\"\",\n            name_extended='ever been told by a doctor or other health professional that you had hypertension',\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    Feature('DIQ160', cat_dtype, \"\"\"{Have you/Has SP} ever been told by a \n    doctor or other health professional that {you have/SP has} any of the \n    following: prediabetes, impaired fasting glucose, impaired glucose \n    tolerance, borderline diabetes or that {your/her/his} blood sugar is \n    higher than normal but not high enough to be called diabetes or sugar \n    diabetes?\"\"\",\n            name_extended=\"ever been told by a \"\n                          \"doctor or other health professional that {you have/SP has} any of the \"\n                          \"following: prediabetes, impaired fasting glucose, impaired glucose \"\n                          \"tolerance, borderline diabetes or that {your/her/his} blood sugar is \"\n                          \"higher than normal but not high enough to be called diabetes or sugar \"\n                          \"diabetes?,\",\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    Feature('DIQ010', cat_dtype, \"\"\"{Other than during pregnancy, {have \n    you/has SP}/{Have you/Has SP}} ever been told by a doctor or health \n    professional that {you have/{he/she/SP} has} diabetes or sugar \n    diabetes?\"\"\",\n            name_extended=\"{Other than during pregnancy, have you\"\n                          \" ever been told by a doctor or health \"\n                          \"professional that you have diabetes or sugar diabetes?\",\n            value_mapping={**DEFAULT_NHANES_CODING,\n                           \"3.0\": \"Borderline\"}),\n\n    ####### Risk Factor: Chronic kidney disease\n\n    Feature('KIQ025', cat_dtype, \"\"\"In the past 12 months, {have you/has SP} \n    received dialysis (either hemodialysis or peritoneal dialysis)?\"\"\",\n            name_extended=\"received dialysis (either hemodialysis or \"\n                          \"peritoneal dialysis) in the past 12 months\",\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    Feature('KIQ022', cat_dtype, \"\"\"{Have you/Has SP} ever been told by a \n    doctor or other health professional that {you/s/he} had weak or failing \n    kidneys? Do not include kidney stones, bladder infections, \n    or incontinence.\"\"\",\n            name_extended=\"ever been told by a doctor or other health \"\n                          \"professional that you had weak or failing kidneys\",\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    ####### Risk Factor: Chronic inflammatory conditions such as\n    # psoriasis, RA, or HIV/AIDS\n\n    Feature('MCQ070', cat_dtype,\n            description=\"{Have you/Has SP} ever been told by a doctor or \"\n                        \"other health care professional that {you/s/he} had \"\n                        \"psoriasis ( sore-eye-asis)? (note: not present after \"\n                        \"2013)\",\n            name_extended=\"ever been told by a doctor or other health care \"\n                          \"professional that you had psoriasis\",\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    Feature('MCQ160A', cat_dtype, \"\"\"Has a doctor or other health \n    professional ever told {you/SP} that {you/s/he} . . .had arthritis (\n    ar-thry-tis)?\"\"\",\n            name_extended=\"ever been told by a doctor or other health care \"\n                          \"professional that you had arthritis\",\n            value_mapping=DEFAULT_NHANES_CODING),\n\n    # Note: no questions about HIV/AIDS.\n\n    #######  Risk Factor: History of premature menopause (before age 40 y)\n    # and history of pregnancy-associated conditions that increase later\n    #  ASCVD risk such as preeclampsia\n\n    # Note: no questions on these.\n\n    # #######  Risk Factor: High-risk race/ethnicities (eg South Asian ancestry)\n    # Covered in shared 'RIDRETH' feature\n], documentation=\"https://wwwn.cdc.gov/Nchs/Nhanes/\")\n\nNHANES_LEAD_FEATURES = FeatureList(features=[\n\n    # A ratio of family income to poverty guidelines.\n    Feature('INDFMPIRBelowCutoff', float,\n            'Binary indicator for whether family PIR (poverty-income ratio)'\n            'is <= 1.3. The threshold of 1.3 is selected based on the '\n            'categorization in NHANES, where PIR <= 1.3 is the lowest level ('\n            'see INDFMMPC feature).',\n            name_extended='Binary indicator for whether family PIR ('\n                          'poverty-income ratio) is <= 1.3.',\n            value_mapping={1.: 'yes', 0.: 'no'}),\n\n    Feature(\"LBXBPB\", float, \"Blood lead (ug/dL)\", is_target=True,\n            na_values=(\".\",)),\n], documentation=\"https://wwwn.cdc.gov/Nchs/Nhanes/\")\n\nNHANES_SHARED_FEATURES = FeatureList(features=[\n    # Derived feature for survey year\n    Feature(\"nhanes_year\", int, \"Derived feature for year.\",\n            name_extended='year'),\n\n    Feature('DMDBORN4', cat_dtype, \"\"\"In what country {were you/was SP} born? \n    1\tBorn in 50 US states or Washington, DC 2 Others\"\"\",\n            na_values=(77, 99, \".\"),\n            name_extended=\"country of birth\",\n            value_mapping={\"1.0\": \"born in 50 US states or Washington, DC\",\n                           \"2.0\": \"not born in 50 US states or Washington, DC\"}),\n\n    Feature('DMDEDUC2', cat_dtype, \"\"\"What is the highest grade or level of \n    school {you have/SP has} completed or the highest degree {you have/s/he \n    has} received?\"\"\",\n            name_extended=\"highest grade or level of school completed or \"\n                          \"highest degree received\"),\n\n    Feature('RIDAGEYR', float, \"\"\"Age in years of the participant at the time \n    of screening. Individuals 80 and over are topcoded at 80 years of age.\"\"\",\n            name_extended=\"age in years\"),\n\n    Feature('RIAGENDR', cat_dtype, \"Gender of the participant.\",\n            name_extended=\"gender\"),\n\n    Feature('DMDMARTL', cat_dtype, \"Marital status\",\n            name_extended=\"marital status\"),\n\n    Feature('RIDRETH_merged', int, \"\"\"Derived feature. This feature uses \n    'RIDRETH3' (Recode of reported race and Hispanic origin information, \n    with Non-Hispanic Asian Category) from years where it is available, \n    and otherwise 'RIDRETH1' (Recode of reported race and Hispanic origin \n    information). 'RIDRETH3' contains a superset of the values in 'RIDRETH1' \n    but the shared values are coded identically; 'RIDRETH3' was only added to \n    NHANES in 2011-2012 data year. See _merge_ridreth_features( ) below, \n    and the NHANES documentation, e.g. \n    https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.htm#RIDRETH1 .\"\"\",\n            name_extended=\"race and hispanic origin\",\n            value_mapping={\n                1.: \"Mexican American\",\n                2.: \"Other Hispanic\",\n                3.: \"Non-Hispanic White\",\n                4.: \"Non-Hispanic Black\",\n                5.: \"Other Race - Including Multi-Racial\",\n                6.: \"Non-Hispanic Asian\",\n                7.: \"Other Race - Including Multi-Racial\"\n            }),\n\n], documentation=\"https://wwwn.cdc.gov/Nchs/Nhanes/\")\n\n\ndef _postprocess_nhanes(df: pd.DataFrame,\n                        feature_list: FeatureList) -> pd.DataFrame:\n    # Fill categorical missing values with \"missing\".\n    for feature in feature_list:\n        name = feature.name\n        if name not in df.columns:\n            logging.warning(\n                f\"feature {feature.name} missing; filling with \"\n                f\"indicator; this can happen when data is subset by years since\"\n                f\" some questions are not asked in all survey years.\")\n            df[name] = pd.Series([\"MISSING\"] * len(df))\n\n        elif name != feature_list.target and feature.kind == cat_dtype:\n            logging.debug(f\"filling and casting categorical feature {name}\")\n            df[name] = df[name].fillna(\"MISSING\").apply(str).astype(\"category\")\n\n    df.reset_index(drop=True, inplace=True)\n    return df\n\n\ndef _merge_ridreth_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create a single race/ethnicity feature by using 'RIDRETH3'\n    where available, else 'RIDRETH1'. \"\"\"\n    if ('RIDRETH3' in df.columns) and ('RIDRETH1' in df.columns):\n        race_col = np.where(~np.isnan(df['RIDRETH3']), df['RIDRETH3'],\n                            df['RIDRETH1'])\n        df.drop(columns=['RIDRETH3', 'RIDRETH1'], inplace=True)\n    elif 'RIDRETH3' in df.columns:\n        race_col = df['RIDRETH3']\n        df.drop(columns=['RIDRETH3'], inplace=True)\n    else:\n        race_col = df['RIDRETH1']\n        df.drop(columns=['RIDRETH1'], inplace=True)\n\n    df['RIDRETH_merged'] = race_col\n    return df\n\n\ndef preprocess_nhanes_cholesterol(df: pd.DataFrame, threshold=160.):\n    feature_list = NHANES_CHOLESTEROL_FEATURES + NHANES_SHARED_FEATURES\n    target = feature_list.target\n\n    df = _merge_ridreth_features(df)\n\n    # Drop observations with missing target or missing domain split variable\n    df.dropna(subset=[target, 'RIDRETH_merged'], inplace=True)\n\n    # Binarize the target\n    df[target] = (df[target] >= threshold).astype(float)\n\n    df = _postprocess_nhanes(df, feature_list=feature_list)\n    return df\n\n\ndef preprocess_nhanes_lead(df: pd.DataFrame, threshold: float = 3.5):\n    \"\"\"Preprocess the NHANES lead prediction dataset.\n\n    The value of 3.5 µg/dl is based on the CDC Blood Lead Reference Value\n    (BLRF) https://www.cdc.gov/nceh/lead/prevention/blood-lead-levels.htm\n    \"\"\"\n    feature_list = NHANES_LEAD_FEATURES + NHANES_SHARED_FEATURES\n    target = NHANES_LEAD_FEATURES.target\n    df = _merge_ridreth_features(df)\n\n    # Drop observations with missing target and missing domain split\n    df = df.dropna(subset=[target, 'INDFMPIR', 'RIDAGEYR'])\n\n    # Keep only children\n    df = df[df['RIDAGEYR'] <= 18.]\n\n    # Create the domain split variable for poverty-income ratio\n    df['INDFMPIRBelowCutoff'] = (df['INDFMPIR'] <= 1.3).astype(int)\n    df.drop(columns=['INDFMPIR'])\n\n    # Binarize the target\n    df[target] = (df[target] >= threshold).astype(float)\n\n    df = _postprocess_nhanes(df, feature_list=feature_list)\n    return df\n"}
{"type": "source_file", "path": "tableshift/datasets/mimic_extract_feature_lists.py", "content": ""}
{"type": "source_file", "path": "tableshift/datasets/compas.py", "content": "\"\"\"\nUtilities for the COMPAS dataset.\n\nThis is a public data source and no special action is required\nto access it.\n\nFor more information on datasets and access in TableShift, see:\n* https://tableshift.org/datasets.html\n* https://github.com/mlfoundations/tableshift\n\n\"\"\"\nimport pandas as pd\n\nfrom tableshift.core.features import Feature, FeatureList, cat_dtype\n\nCOMPAS_RESOURCES = [\n    \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"]\n\nCOMPAS_FEATURES = FeatureList(features=[\n    Feature('juv_fel_count', int,\n            name_extended='Count of juvenile felonies'),\n    Feature('juv_misd_count', int,\n            name_extended='Count of juvenile misemeanors'),\n    Feature('juv_other_count', int,\n            name_extended='Count of other juvenile records'),\n    Feature('priors_count', int,\n            name_extended='Count of prior convictions'),\n    Feature('age', int),\n    Feature('c_charge_degree', cat_dtype, name_extended='Charge degree',\n            value_mapping={'F': 'Felony', 'M': 'Misdemeanor'}),\n    Feature('sex', cat_dtype),\n    Feature('race', cat_dtype),\n    Feature('Target', float, is_target=True),\n])\n\n\ndef preprocess_compas(df: pd.DataFrame):\n    \"\"\"Preprocess COMPAS dataset.\n\n    See https://github.com/RuntianZ/doro/blob/master/compas.py .\n    \"\"\"\n\n    columns = ['juv_fel_count', 'juv_misd_count', 'juv_other_count',\n               'priors_count',\n               'age',\n               'c_charge_degree',\n               'sex', 'race', 'is_recid', 'compas_screening_date']\n\n    df = df[['id'] + columns].drop_duplicates()\n    df = df[columns]\n\n    df.rename(columns={\"is_recid\": \"Target\"}, inplace=True)\n    return df\n"}
