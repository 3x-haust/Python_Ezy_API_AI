{"repo_info": {"repo_name": "LARS", "repo_owner": "abgulati", "repo_url": "https://github.com/abgulati/LARS"}}
{"type": "source_file", "path": "dockerized/web_app/unoconv.py", "content": "#!/usr/bin/env python\n\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation: version 2 only.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY, without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n# Copyright 2007-2010 Dag Wieers <dag@wieers.com>\n\nfrom __future__ import print_function\n\nfrom distutils.version import LooseVersion\nimport getopt\nimport glob\nimport os\nimport signal\nimport subprocess\nimport sys\nimport time\n\n__version__ = '0.8.2'\n\ndoctypes = ('document', 'graphics', 'presentation', 'spreadsheet')\n\nglobal convertor, office, ooproc, product\nooproc = None\nuno = unohelper = None\nexitcode = 0\n\n\nclass Office:\n    def __init__(self, basepath, urepath, unopath, pyuno, binary, python, pythonhome):\n        self.basepath = basepath\n        self.urepath = urepath\n        self.unopath = unopath\n        self.pyuno = pyuno\n        self.binary = binary\n        self.python = python\n        self.pythonhome = pythonhome\n\n    def __str__(self):\n        return self.basepath\n\n    def __repr__(self):\n        return self.basepath\n\n\n# Implement a path normalizer to make unoconv work on MacOS X, on\n# which 'program' is a symlink to 'MacOSX,' which seems to break unoconv.\ndef realpath(*args):\n    \"\"\"Implement a combination of os.path.join(), os.path.abspath() and\n        os.path.realpath() in order to normalize path constructions.\"\"\"\n    ret = ''\n    for arg in args:\n        ret = os.path.join(ret, arg)\n    return os.path.realpath(os.path.abspath(ret))\n\n\n# The first thing we should do is find a suitable Office installation\n# with a compatible pyuno library that we can import.\n#\n# See: http://user.services.openoffice.org/en/forum/viewtopic.php?f=45&t=36370&p=166783\ndef find_offices():\n    ret = []\n    extrapaths = []\n\n    # Try using UNO_PATH first (in many incarnations, we'll see what sticks).\n    if 'UNO_PATH' in os.environ:\n        extrapaths += [os.environ['UNO_PATH'],\n                       os.path.dirname(os.environ['UNO_PATH']),\n                       os.path.dirname(os.path.dirname(os.environ['UNO_PATH']))]\n    else:\n        if os.name in ('nt', 'os2'):\n            if 'PROGRAMFILES' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMFILES(X86)' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMW6432' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMW6432']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMW6432']+'\\\\OpenOffice.org*')\n\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            extrapaths += ['/Applications/LibreOffice.app/Contents',\n                           '/Applications/NeoOffice.app/Contents',\n                           '/Applications/OpenOffice.app/Contents',\n                           '/Applications/OpenOffice.org.app/Contents']\n\n        else:\n            extrapaths += glob.glob('/usr/lib*/libreoffice*') + \\\n                          glob.glob('/usr/lib*/openoffice*') + \\\n                          glob.glob('/usr/lib*/ooo*') + \\\n                          glob.glob('/opt/libreoffice*') + \\\n                          glob.glob('/opt/openoffice*') + \\\n                          glob.glob('/opt/ooo*') + \\\n                          glob.glob('/usr/local/libreoffice*') + \\\n                          glob.glob('/usr/local/openoffice*') + \\\n                          glob.glob('/usr/local/ooo*') + \\\n                          glob.glob('/usr/local/lib/libreoffice*')\n\n    # Find a working set for python UNO bindings.\n    for basepath in extrapaths:\n        if os.name in ('nt', 'os2'):\n            officelibraries = ('pyuno.pyd',)\n            officebinaries = ('soffice.exe',)\n            pythonbinaries = ('python.exe',)\n            pythonhomes = ()\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            officelibraries = ('pyuno.so', 'libpyuno.dylib')\n            officebinaries = ('soffice.bin', 'soffice')\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('OOoPython.framework/Versions/*/lib/python*')\n        else:\n            officelibraries = ('pyuno.so',)\n            officebinaries = ('soffice.bin',)\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('python-core-*',)\n\n        # Older LibreOffice/OpenOffice and Windows use basis-link/ or basis/\n        libpath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for lib in officelibraries:\n                for libdir in ('program', 'Frameworks'):\n                    if os.path.isfile(realpath(basepath, basis, libdir, lib)):\n                        libpath = realpath(basepath, basis, libdir)\n                        officelibrary = realpath(libpath, lib)\n                        info(3, \"Found %s in %s\" % (lib, libpath))\n                        # Break the inner loop...\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken, break the outer.\n            break\n        else:\n            continue\n\n        # MacOSX has soffice binaries installed in MacOS subdirectory, not program.\n        unopath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for bin in officebinaries:\n                for bindir in ('program', '', 'MacOS'):\n                    if os.path.isfile(realpath(basepath, basis, bindir, bin)):\n                        unopath = realpath(basepath, basis, bindir)\n                        officebinary = realpath(unopath, bin)\n                        info(3, \"Found %s in %s\" % (bin, unopath))\n                        # Break the inner loop.\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n        else:\n            continue\n\n        # Windows does not provide or need a URE/lib directory?\n        urepath = ''\n        for basis in ('basis-link', 'basis', ''):\n            for ure in ('ure-link', 'ure', 'URE', ''):\n                if os.path.isfile(realpath(basepath, basis, ure, 'lib', 'unorc')):\n                    urepath = realpath(basepath, basis, ure)\n                    info(3, \"Found %s in %s\" % ('unorc', realpath(urepath, 'lib')))\n                    # Break the inner loop.\n                    break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n\n        pythonhome = None\n        for home in pythonhomes:\n            if glob.glob(realpath(libpath, home)):\n                pythonhome = glob.glob(realpath(libpath, home))[0]\n                info(3, \"Found %s in %s\" % (home, pythonhome))\n                break\n\n        # if not os.path.isfile(realpath(basepath, program, officebinary)):\n            # continue\n        # info(3, \"Found %s in %s\" % (officebinary, realpath(basepath, program)))\n\n        # if not glob.glob(realpath(basepath, basis, program, 'python-core-*')):\n            # continue\n\n        # Find suitable Python executable: regular unopath or MacOS version.\n        # LibreOffice 5.4.6.2 on MacOS X 10.13.3 ships the Python executable\n        # in the \"Resources\" folder.\n        pythonpath_candidates = [unopath, realpath(basepath, 'Resources')]\n\n        python_effective = find_executable(pythonpath_candidates, pythonbinaries)\n\n        if python_effective:\n            info(3, \"Found Python at %s\" % python_effective)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            python_effective, pythonhome)\n        else:\n            info(3, \"Considering %s\" % basepath)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            sys.executable, None)\n\n        ret.append(office)\n\n    return ret\n\n\ndef find_executable(folders, filenames):\n    for folder in folders:\n        for filename in filenames:\n            candidate = realpath(folder, filename)\n            if os.path.isfile(candidate):\n                return candidate\n\n\ndef office_environ(office):\n    # Set PATH so crash_report is found.\n    path_prefix = realpath(office.basepath, 'program') + os.pathsep + realpath(office.basepath, 'Resources')\n    if 'PATH' in os.environ:\n        os.environ['PATH'] = path_prefix + os.pathsep + os.environ['PATH']\n    else:\n        os.environ['PATH'] = path_prefix\n\n    # Set UNO_PATH so \"officehelper.bootstrap()\" can find soffice executable:\n    os.environ['UNO_PATH'] = office.unopath\n\n    # Set URE_BOOTSTRAP so \"uno.getComponentContext()\" bootstraps a complete UNO environment.\n    if os.name in ('nt', 'os2'):\n        os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamental.ini')\n    else:\n        if os.path.isfile(realpath(office.basepath, 'program', 'fundamentalrc')):\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamentalrc')\n        else:\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'Resources', 'fundamentalrc')\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n    if office.pythonhome:\n        for libpath in (realpath(office.pythonhome, 'lib'),\n                        realpath(office.pythonhome, 'lib', 'lib-dynload'),\n                        realpath(office.pythonhome, 'lib', 'lib-tk'),\n                        realpath(office.pythonhome, 'lib', 'site-packages'),\n                        office.unopath):\n            sys.path.insert(0, libpath)\n    else:\n        # Still needed for system python using LibreOffice UNO bindings\n        # Although we prefer to use a system UNO binding in this case\n        sys.path.append(office.unopath)\n\n\ndef debug_office():\n    if 'URE_BOOTSTRAP' in os.environ:\n        print('URE_BOOTSTRAP=%s' % os.environ['URE_BOOTSTRAP'], file=sys.stderr)\n    if 'UNO_PATH' in os.environ:\n        print('UNO_PATH=%s' % os.environ['UNO_PATH'], file=sys.stderr)\n    if 'UNO_TYPES' in os.environ:\n        print('UNO_TYPES=%s' % os.environ['UNO_TYPES'], file=sys.stderr)\n    print('PATH=%s' % os.environ['PATH'])\n    if 'PYTHONHOME' in os.environ:\n        print('PYTHONHOME=%s' % os.environ['PYTHONHOME'], file=sys.stderr)\n    if 'PYTHONPATH' in os.environ:\n        print('PYTHONPATH=%s' % os.environ['PYTHONPATH'], file=sys.stderr)\n    if 'LD_LIBRARY_PATH' in os.environ:\n        print('LD_LIBRARY_PATH=%s' % os.environ['LD_LIBRARY_PATH'], file=sys.stderr)\n\n\ndef python_switch(office):\n    if office.pythonhome:\n        os.environ['PYTHONHOME'] = office.pythonhome\n        os.environ['PYTHONPATH'] = realpath(office.pythonhome, 'lib') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-dynload') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-tk') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'site-packages') + os.pathsep + \\\n                                   office.unopath\n\n    os.environ['UNO_PATH'] = office.unopath\n\n    info(3, \"-> Switching from %s to %s\" % (sys.executable, office.python))\n    if os.name in ('nt', 'os2'):\n        # os.execv is broken on Windows and can't properly parse command line\n        # arguments and executable name if they contain whitespaces. subprocess\n        # fixes that behavior.\n        ret = subprocess.call([office.python, ] + sys.argv[0:])\n        sys.exit(ret)\n    else:\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n        try:\n            os.execvpe(office.python, [office.python, ] + sys.argv[0:], os.environ)\n        except OSError:\n            # Mac OS X versions prior to 10.6 do not support execv in\n            # a process that contains multiple threads.  Instead of\n            # re-executing in the current process, start a new one\n            # and cause the current process to exit.  This isn't\n            # ideal since the new process is detached from the parent\n            # terminal and thus cannot easily be killed with ctrl-C,\n            # but it's better than not being able to autoreload at\n            # all.\n            # Unfortunately the errno returned in this case does not\n            # appear to be consistent, so we can't easily check for\n            # this error specifically.\n            ret = os.spawnvpe(os.P_WAIT, office.python, [office.python, ] + sys.argv[0:], os.environ)\n            if ret != 0:\n                error(\"Switching Python to %s failed.\" % (office.python))\n            sys.exit(ret)\n\n\nclass Fmt:\n    def __init__(self, doctype, name, extension, summary, filter):\n        self.doctype = doctype\n        self.name = name\n        self.extension = extension\n        self.summary = summary\n        self.filter = filter\n\n    def __str__(self):\n        return \"%s [.%s]\" % (self.summary, self.extension)\n\n    def __repr__(self):\n        return \"%s/%s\" % (self.name, self.doctype)\n\n\nclass FmtList:\n    def __init__(self):\n        self.list = []\n\n    def add(self, doctype, name, extension, summary, filter):\n        self.list.append(Fmt(doctype, name, extension, summary, filter))\n\n    def byname(self, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name:\n                ret.append(fmt)\n        return ret\n\n    def byextension(self, extension):\n        ret = []\n        for fmt in self.list:\n            if os.extsep + fmt.extension == extension:\n                ret.append(fmt)\n        return ret\n\n    def bydoctype(self, doctype, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name and fmt.doctype == doctype:\n                ret.append(fmt)\n        return ret\n\n    def display(self, doctype):\n        print(\"The following list of %s formats are currently available:\\n\" % doctype, file=sys.stderr)\n        for fmt in self.list:\n            if fmt.doctype == doctype:\n                print(\"  %-8s - %s\" % (fmt.name, fmt), file=sys.stderr)\n        print(file=sys.stderr)\n\n\nfmts = FmtList()\n\n# TextDocument\nfmts.add('document', 'bib', 'bib', 'BibTeX', 'BibTeX_Writer')  # 22\nfmts.add('document', 'doc', 'doc', 'Microsoft Word 97/2000/XP', 'MS Word 97')  # 29\nfmts.add('document', 'doc6', 'doc', 'Microsoft Word 6.0', 'MS WinWord 6.0')  # 24\nfmts.add('document', 'doc95', 'doc', 'Microsoft Word 95', 'MS Word 95')  # 28\nfmts.add('document', 'docbook', 'xml', 'DocBook', 'DocBook File')  # 39\nfmts.add('document', 'docx', 'docx', 'Microsoft Office Open XML', 'Office Open XML Text')\nfmts.add('document', 'docx7', 'docx', 'Microsoft Office Open XML', 'MS Word 2007 XML')\nfmts.add('document', 'fodt', 'fodt', 'OpenDocument Text (Flat XML)', 'OpenDocument Text Flat XML')\nfmts.add('document', 'html', 'html', 'HTML Document (OpenOffice.org Writer)', 'HTML (StarWriter)')  # 3\nfmts.add('document', 'latex', 'ltx', 'LaTeX 2e', 'LaTeX_Writer')  # 31\nfmts.add('document', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki')\nfmts.add('document', 'odt', 'odt', 'ODF Text Document', 'writer8')  # 10\nfmts.add('document', 'ooxml', 'xml', 'Microsoft Office Open XML', 'MS Word 2003 XML')  # 11\nfmts.add('document', 'ott', 'ott', 'Open Document Text', 'writer8_template')  # 21\nfmts.add('document', 'pdb', 'pdb', 'AportisDoc (Palm)', 'AportisDoc Palm DB')\nfmts.add('document', 'pdf', 'pdf', 'Portable Document Format', 'writer_pdf_Export')  # 18\nfmts.add('document', 'psw', 'psw', 'Pocket Word', 'PocketWord File')\nfmts.add('document', 'rtf', 'rtf', 'Rich Text Format', 'Rich Text Format')  # 16\nfmts.add('document', 'sdw', 'sdw', 'StarWriter 5.0', 'StarWriter 5.0')  # 23\nfmts.add('document', 'sdw4', 'sdw', 'StarWriter 4.0', 'StarWriter 4.0')  # 2\nfmts.add('document', 'sdw3', 'sdw', 'StarWriter 3.0', 'StarWriter 3.0')  # 20\nfmts.add('document', 'stw', 'stw', 'Open Office.org 1.0 Text Document Template', 'writer_StarOffice_XML_Writer_Template')  # 9\nfmts.add('document', 'sxw', 'sxw', 'Open Office.org 1.0 Text Document', 'StarOffice XML (Writer)')  # 1\nfmts.add('document', 'text', 'txt', 'Text Encoded', 'Text (encoded)')  # 26\nfmts.add('document', 'txt', 'txt', 'Text', 'Text')  # 34\nfmts.add('document', 'uot', 'uot', 'Unified Office Format text', 'UOF text')  # 27\nfmts.add('document', 'vor', 'vor', 'StarWriter 5.0 Template', 'StarWriter 5.0 Vorlage/Template')  # 6\nfmts.add('document', 'vor4', 'vor', 'StarWriter 4.0 Template', 'StarWriter 4.0 Vorlage/Template')  # 5\nfmts.add('document', 'vor3', 'vor', 'StarWriter 3.0 Template', 'StarWriter 3.0 Vorlage/Template')  # 4\nfmts.add('document', 'wps', 'wps', 'Microsoft Works', 'MS_Works')\nfmts.add('document', 'xhtml', 'html', 'XHTML Document', 'XHTML Writer File')  # 33\nfmts.add('document', 'epub', 'epub', 'Electronic Publication', 'EPUB')\nfmts.add('document', 'png', 'png', 'Portable Network Graphic', 'writer_png_Export') ### 2\n\n# WebDocument\nfmts.add('web', 'etext', 'txt', 'Text Encoded (OpenOffice.org Writer/Web)', 'Text (encoded) (StarWriter/Web)')  # 14\nfmts.add('web', 'html10', 'html', 'OpenOffice.org 1.0 HTML Template', 'writer_web_StarOffice_XML_Writer_Web_Template')  # 11\nfmts.add('web', 'html', 'html', 'HTML Document', 'HTML')  # 2\nfmts.add('web', 'html', 'html', 'HTML Document Template', 'writerweb8_writer_template')  # 13\nfmts.add('web', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki_Web')  # 9\nfmts.add('web', 'pdf', 'pdf', 'PDF - Portable Document Format', 'writer_web_pdf_Export')  # 10\nfmts.add('web', 'sdw3', 'sdw', 'StarWriter 3.0 (OpenOffice.org Writer/Web)', 'StarWriter 3.0 (StarWriter/Web)')  # 3\nfmts.add('web', 'sdw4', 'sdw', 'StarWriter 4.0 (OpenOffice.org Writer/Web)', 'StarWriter 4.0 (StarWriter/Web)')  # 4\nfmts.add('web', 'sdw', 'sdw', 'StarWriter 5.0 (OpenOffice.org Writer/Web)', 'StarWriter 5.0 (StarWriter/Web)')  # 5\nfmts.add('web', 'txt', 'txt', 'OpenOffice.org Text (OpenOffice.org Writer/Web)', 'writerweb8_writer')  # 12\nfmts.add('web', 'text10', 'txt', 'OpenOffice.org 1.0 Text Document (OpenOffice.org Writer/Web)', 'writer_web_StarOffice_XML_Writer')  # 15\nfmts.add('web', 'text', 'txt', 'Text (OpenOffice.org Writer/Web)', 'Text (StarWriter/Web)')  # 8\nfmts.add('web', 'vor4', 'vor', 'StarWriter/Web 4.0 Template', 'StarWriter/Web 4.0 Vorlage/Template')  # 6\nfmts.add('web', 'vor', 'vor', 'StarWriter/Web 5.0 Template', 'StarWriter/Web 5.0 Vorlage/Template')  # 7\n\n# Spreadsheet\nfmts.add('spreadsheet', 'csv', 'csv', 'Text CSV', 'Text - txt - csv (StarCalc)')  # 16\nfmts.add('spreadsheet', 'dbf', 'dbf', 'dBASE', 'dBase')  # 22\nfmts.add('spreadsheet', 'dif', 'dif', 'Data Interchange Format', 'DIF')  # 5\nfmts.add('spreadsheet', 'fods', 'fods', 'OpenDocument Spreadsheet (Flat XML)', 'OpenDocument Spreadsheet Flat XML')\nfmts.add('spreadsheet', 'html', 'html', 'HTML Document (OpenOffice.org Calc)', 'HTML (StarCalc)')  # 7\nfmts.add('spreadsheet', 'ods', 'ods', 'ODF Spreadsheet', 'calc8')  # 15\nfmts.add('spreadsheet', 'ooxml', 'xml', 'Microsoft Excel 2003 XML', 'MS Excel 2003 XML')  # 23\nfmts.add('spreadsheet', 'ots', 'ots', 'ODF Spreadsheet Template', 'calc8_template')  # 14\nfmts.add('spreadsheet', 'pdf', 'pdf', 'Portable Document Format', 'calc_pdf_Export')  # 34\nfmts.add('spreadsheet', 'pxl', 'pxl', 'Pocket Excel', 'Pocket Excel')\nfmts.add('spreadsheet', 'sdc', 'sdc', 'StarCalc 5.0', 'StarCalc 5.0')  # 31\nfmts.add('spreadsheet', 'sdc4', 'sdc', 'StarCalc 4.0', 'StarCalc 4.0')  # 11\nfmts.add('spreadsheet', 'sdc3', 'sdc', 'StarCalc 3.0', 'StarCalc 3.0')  # 29\nfmts.add('spreadsheet', 'slk', 'slk', 'SYLK', 'SYLK')  # 35\nfmts.add('spreadsheet', 'stc', 'stc', 'OpenOffice.org 1.0 Spreadsheet Template', 'calc_StarOffice_XML_Calc_Template')  # 2\nfmts.add('spreadsheet', 'sxc', 'sxc', 'OpenOffice.org 1.0 Spreadsheet', 'StarOffice XML (Calc)')  # 3\nfmts.add('spreadsheet', 'uos', 'uos', 'Unified Office Format spreadsheet', 'UOF spreadsheet')  # 9\nfmts.add('spreadsheet', 'vor3', 'vor', 'StarCalc 3.0 Template', 'StarCalc 3.0 Vorlage/Template')  # 18\nfmts.add('spreadsheet', 'vor4', 'vor', 'StarCalc 4.0 Template', 'StarCalc 4.0 Vorlage/Template')  # 19\nfmts.add('spreadsheet', 'vor', 'vor', 'StarCalc 5.0 Template', 'StarCalc 5.0 Vorlage/Template')  # 20\nfmts.add('spreadsheet', 'xhtml', 'xhtml', 'XHTML', 'XHTML Calc File')  # 26\nfmts.add('spreadsheet', 'xls', 'xls', 'Microsoft Excel 97/2000/XP', 'MS Excel 97')  # 12\nfmts.add('spreadsheet', 'xls5', 'xls', 'Microsoft Excel 5.0', 'MS Excel 5.0/95')  # 8\nfmts.add('spreadsheet', 'xls95', 'xls', 'Microsoft Excel 95', 'MS Excel 95')  # 10\nfmts.add('spreadsheet', 'xlt', 'xlt', 'Microsoft Excel 97/2000/XP Template', 'MS Excel 97 Vorlage/Template')  # 6\nfmts.add('spreadsheet', 'xlt5', 'xlt', 'Microsoft Excel 5.0 Template', 'MS Excel 5.0/95 Vorlage/Template')  # 28\nfmts.add('spreadsheet', 'xlt95', 'xlt', 'Microsoft Excel 95 Template', 'MS Excel 95 Vorlage/Template')  # 21\nfmts.add('spreadsheet', 'xlsx', 'xlsx', 'Microsoft Excel 2007/2010 XML', 'Calc MS Excel 2007 XML')\n\n# Graphics\nfmts.add('graphics', 'bmp', 'bmp', 'Windows Bitmap', 'draw_bmp_Export')  # 21\nfmts.add('graphics', 'emf', 'emf', 'Enhanced Metafile', 'draw_emf_Export')  # 15\nfmts.add('graphics', 'eps', 'eps', 'Encapsulated PostScript', 'draw_eps_Export')  # 48\nfmts.add('graphics', 'fodg', 'fodg', 'OpenDocument Drawing (Flat XML)', 'OpenDocument Drawing Flat XML')\nfmts.add('graphics', 'gif', 'gif', 'Graphics Interchange Format', 'draw_gif_Export')  # 30\nfmts.add('graphics', 'html', 'html', 'HTML Document (OpenOffice.org Draw)', 'draw_html_Export')  # 37\nfmts.add('graphics', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'jpeg', 'jpeg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'met', 'met', 'OS/2 Metafile', 'draw_met_Export')  # 43\nfmts.add('graphics', 'odd', 'odd', 'OpenDocument Drawing', 'draw8')  # 6\nfmts.add('graphics', 'otg', 'otg', 'OpenDocument Drawing Template', 'draw8_template')  # 20\nfmts.add('graphics', 'pbm', 'pbm', 'Portable Bitmap', 'draw_pbm_Export')  # 14\nfmts.add('graphics', 'pct', 'pct', 'Mac Pict', 'draw_pct_Export')  # 41\nfmts.add('graphics', 'pdf', 'pdf', 'Portable Document Format', 'draw_pdf_Export')  # 28\nfmts.add('graphics', 'pgm', 'pgm', 'Portable Graymap', 'draw_pgm_Export')  # 11\nfmts.add('graphics', 'png', 'png', 'Portable Network Graphic', 'draw_png_Export')  # 2\nfmts.add('graphics', 'ppm', 'ppm', 'Portable Pixelmap', 'draw_ppm_Export')  # 5\nfmts.add('graphics', 'ras', 'ras', 'Sun Raster Image', 'draw_ras_Export')  # 31\nfmts.add('graphics', 'std', 'std', 'OpenOffice.org 1.0 Drawing Template', 'draw_StarOffice_XML_Draw_Template')  # 53\nfmts.add('graphics', 'svg', 'svg', 'Scalable Vector Graphics', 'draw_svg_Export')  # 50\nfmts.add('graphics', 'svm', 'svm', 'StarView Metafile', 'draw_svm_Export')  # 55\nfmts.add('graphics', 'swf', 'swf', 'Macromedia Flash (SWF)', 'draw_flash_Export')  # 23\nfmts.add('graphics', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing', 'StarOffice XML (Draw)')  # 26\nfmts.add('graphics', 'sxd3', 'sxd', 'StarDraw 3.0', 'StarDraw 3.0')  # 40\nfmts.add('graphics', 'sxd5', 'sxd', 'StarDraw 5.0', 'StarDraw 5.0')  # 44\nfmts.add('graphics', 'sxw', 'sxw', 'StarOffice XML (Draw)', 'StarOffice XML (Draw)')\nfmts.add('graphics', 'tiff', 'tiff', 'Tagged Image File Format', 'draw_tif_Export')  # 13\nfmts.add('graphics', 'vor', 'vor', 'StarDraw 5.0 Template', 'StarDraw 5.0 Vorlage')  # 36\nfmts.add('graphics', 'vor3', 'vor', 'StarDraw 3.0 Template', 'StarDraw 3.0 Vorlage')  # 35\nfmts.add('graphics', 'wmf', 'wmf', 'Windows Metafile', 'draw_wmf_Export')  # 8\nfmts.add('graphics', 'xhtml', 'xhtml', 'XHTML', 'XHTML Draw File')  # 45\nfmts.add('graphics', 'xpm', 'xpm', 'X PixMap', 'draw_xpm_Export')  # 19\n\n# Presentation\nfmts.add('presentation', 'bmp', 'bmp', 'Windows Bitmap', 'impress_bmp_Export')  # 15\nfmts.add('presentation', 'emf', 'emf', 'Enhanced Metafile', 'impress_emf_Export')  # 16\nfmts.add('presentation', 'eps', 'eps', 'Encapsulated PostScript', 'impress_eps_Export')  # 17\nfmts.add('presentation', 'fodp', 'fodp', 'OpenDocument Presentation (Flat XML)', 'OpenDocument Presentation Flat XML')\nfmts.add('presentation', 'gif', 'gif', 'Graphics Interchange Format', 'impress_gif_Export')  # 18\nfmts.add('presentation', 'html', 'html', 'HTML Document (OpenOffice.org Impress)', 'impress_html_Export')  # 43\nfmts.add('presentation', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'impress_jpg_Export')  # 19\nfmts.add('presentation', 'met', 'met', 'OS/2 Metafile', 'impress_met_Export')  # 20\nfmts.add('presentation', 'odg', 'odg', 'ODF Drawing (Impress)', 'impress8_draw')  # 29\nfmts.add('presentation', 'odp', 'odp', 'ODF Presentation', 'impress8')  # 9\nfmts.add('presentation', 'otp', 'otp', 'ODF Presentation Template', 'impress8_template')  # 38\nfmts.add('presentation', 'pbm', 'pbm', 'Portable Bitmap', 'impress_pbm_Export')  # 21\nfmts.add('presentation', 'pct', 'pct', 'Mac Pict', 'impress_pct_Export')  # 22\nfmts.add('presentation', 'pdf', 'pdf', 'Portable Document Format', 'impress_pdf_Export')  # 23\nfmts.add('presentation', 'pgm', 'pgm', 'Portable Graymap', 'impress_pgm_Export')  # 24\nfmts.add('presentation', 'png', 'png', 'Portable Network Graphic', 'impress_png_Export')  # 25\nfmts.add('presentation', 'potm', 'potm', 'Microsoft PowerPoint 2007/2010 XML Template', 'Impress MS PowerPoint 2007 XML Template')\nfmts.add('presentation', 'pot', 'pot', 'Microsoft PowerPoint 97/2000/XP Template', 'MS PowerPoint 97 Vorlage')  # 3\nfmts.add('presentation', 'ppm', 'ppm', 'Portable Pixelmap', 'impress_ppm_Export')  # 26\nfmts.add('presentation', 'pptx', 'pptx', 'Microsoft PowerPoint 2007/2010 XML', 'Impress MS PowerPoint 2007 XML')  # 36\nfmts.add('presentation', 'pps', 'pps', 'Microsoft PowerPoint 97/2000/XP (Autoplay)', 'MS PowerPoint 97 Autoplay')  # 36\nfmts.add('presentation', 'ppt', 'ppt', 'Microsoft PowerPoint 97/2000/XP', 'MS PowerPoint 97')  # 36\nfmts.add('presentation', 'pwp', 'pwp', 'PlaceWare', 'placeware_Export')  # 30\nfmts.add('presentation', 'ras', 'ras', 'Sun Raster Image', 'impress_ras_Export')  # 27\nfmts.add('presentation', 'sda', 'sda', 'StarDraw 5.0 (OpenOffice.org Impress)', 'StarDraw 5.0 (StarImpress)')  # 8\nfmts.add('presentation', 'sdd', 'sdd', 'StarImpress 5.0', 'StarImpress 5.0')  # 6\nfmts.add('presentation', 'sdd3', 'sdd', 'StarDraw 3.0 (OpenOffice.org Impress)', 'StarDraw 3.0 (StarImpress)')  # 42\nfmts.add('presentation', 'sdd4', 'sdd', 'StarImpress 4.0', 'StarImpress 4.0')  # 37\nfmts.add('presentation', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing (OpenOffice.org Impress)', 'impress_StarOffice_XML_Draw')  # 31\nfmts.add('presentation', 'sti', 'sti', 'OpenOffice.org 1.0 Presentation Template', 'impress_StarOffice_XML_Impress_Template')  # 5\nfmts.add('presentation', 'svg', 'svg', 'Scalable Vector Graphics', 'impress_svg_Export')  # 14\nfmts.add('presentation', 'svm', 'svm', 'StarView Metafile', 'impress_svm_Export')  # 13\nfmts.add('presentation', 'swf', 'swf', 'Macromedia Flash (SWF)', 'impress_flash_Export')  # 34\nfmts.add('presentation', 'sxi', 'sxi', 'OpenOffice.org 1.0 Presentation', 'StarOffice XML (Impress)')  # 41\nfmts.add('presentation', 'tiff', 'tiff', 'Tagged Image File Format', 'impress_tif_Export')  # 12\nfmts.add('presentation', 'uop', 'uop', 'Unified Office Format presentation', 'UOF presentation')  # 4\nfmts.add('presentation', 'vor', 'vor', 'StarImpress 5.0 Template', 'StarImpress 5.0 Vorlage')  # 40\nfmts.add('presentation', 'vor3', 'vor', 'StarDraw 3.0 Template (OpenOffice.org Impress)', 'StarDraw 3.0 Vorlage (StarImpress)')  # 1\nfmts.add('presentation', 'vor4', 'vor', 'StarImpress 4.0 Template', 'StarImpress 4.0 Vorlage')  # 39\nfmts.add('presentation', 'vor5', 'vor', 'StarDraw 5.0 Template (OpenOffice.org Impress)', 'StarDraw 5.0 Vorlage (StarImpress)')  # 2\nfmts.add('presentation', 'wmf', 'wmf', 'Windows Metafile', 'impress_wmf_Export')  # 11\nfmts.add('presentation', 'xhtml', 'xml', 'XHTML', 'XHTML Impress File')  # 33\nfmts.add('presentation', 'xpm', 'xpm', 'X PixMap', 'impress_xpm_Export')  # 10\n\n\nclass Options:\n    def __init__(self, args):\n        self.connection = None\n        self.debug = False\n        self.doctype = None\n        self.exportfilter = []\n        self.exportfilteroptions = \"\"\n        self.fields = {}\n        self.filenames = []\n        self.format = None\n        self.importfilter = []\n        self.importfiltername = None\n        self.importfilteroptions = \"\"\n        self.listener = False\n        self.metadata = {}\n        self.nolaunch = False\n        self.output = None\n        self.paperformat = None\n        self.paperorientation = None\n        self.papersize = None\n        self.password = None\n        self.pipe = None\n        self.port = '2002'\n        self.preserve = False\n        self.server = '127.0.0.1'\n        self.setprinter = False\n        self.showlist = False\n        self.stdin = False\n        self.stdout = False\n        self.template = None\n        self.timeout = 60\n        self.verbose = 0\n        self.userProfile = None\n        self.updateDocMode = NO_UPDATE\n        self.updatehtmllinks = True\n\n        # Get options from the commandline\n        try:\n            opts, args = getopt.getopt(args, 'c:Dd:e:F:f:hi:I:LlM:no:p:s:T:t:P:vV',\n                ['disable-html-update-links', 'connection=', 'debug', 'doctype=', 'export=', 'field=', 'format=',\n                 'help', 'import=', 'import-filter-name=', 'listener', 'meta=', 'no-launch',\n                 'output=', 'outputpath', 'password=', 'pipe=', 'port=', 'preserve',\n                 'server=', 'timeout=', 'user-profile=', 'show', 'stdin',\n                 'stdout', 'template', 'printer=', 'unsafe-quiet-update', 'verbose', 'version'])\n        except getopt.error as exc:\n            print('unoconv: %s, try unoconv -h for a list of all the options' % str(exc))\n            sys.exit(255)\n\n        for opt, arg in opts:\n            if opt in ['-h', '--help']:\n                self.usage()\n                print()\n                self.help()\n                sys.exit(0)\n            elif opt in ['-c', '--connection']:\n                self.connection = arg\n            elif opt in ['--debug']:\n                self.debug = True\n            elif opt in ['-d', '--doctype']:\n                self.doctype = arg\n            elif opt in ['-e', '--export']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.exportfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.exportfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.exportfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.exportfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.exportfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-F', '--field']:\n                l = arg.split('=')\n                self.fields[l[0]] = '='.join(l[1:])\n            elif opt in ['-f', '--format']:\n                self.format = arg\n            elif opt in ['-i', '--import']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.importfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.importfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.importfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.importfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.importfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-I', '--import-filter-name']:\n                self.importfiltername = arg\n            elif opt in ['-l', '--listener']:\n                self.listener = True\n            elif opt in ['-M', '--meta']:\n                l = arg.split('=')\n                self.metadata[l[0]] = '='.join(l[1:])\n            elif opt in ['-n', '--no-launch']:\n                self.nolaunch = True\n            elif opt in ['-o', '--output']:\n                self.output = arg\n            elif opt in ['--outputpath']:\n                print('Warning: This option is deprecated by --output.', file=sys.stderr)\n                self.output = arg\n            elif opt in ['--password']:\n                self.password = arg\n            elif opt in ['--pipe']:\n                self.pipe = arg\n            elif opt in ['-p', '--port']:\n                self.port = arg\n            elif opt in ['--preserve']:\n                self.preserve = True\n            elif opt in ['-s', '--server']:\n                self.server = arg\n            elif opt in ['--show']:\n                self.showlist = True\n            elif opt in ['--stdin']:\n                self.stdin = True\n            elif opt in ['--stdout']:\n                self.stdout = True\n            elif opt in ['-t', '--template']:\n                self.template = arg\n            elif opt in ['--disable-html-update-links']:\n                self.updatehtmllinks = False\n            elif opt in ['-T', '--timeout']:\n                self.timeout = int(arg)\n            elif opt in ['--unsafe-quiet-update']:\n                # ref https://www.openoffice.org/api/docs/common/ref/com/sun/star/document/UpdateDocMode.html\n                print('Warning: Do not use the option --unsafe-quiet-update with untrusted input.')\n                self.updateDocMode = QUIET_UPDATE\n            elif opt in ['-v', '--verbose']:\n                self.verbose = self.verbose + 1\n            elif opt in ['-V', '--version']:\n                self.version()\n                sys.exit(0)\n            elif opt in ['-P', '--printer']:\n                optKey, optValue = arg.split('=')\n                if optKey in ['PaperFormat']:\n                    self.paperformat = optValue\n                    self.setprinter = True\n                elif optKey in ['PaperOrientation']:\n                    self.paperorientation = optValue.upper()\n                    self.setprinter = True\n                elif optKey in ['PaperSize']:\n                    intFunc = int if sys.version_info.major > 2 else long\n                    size = list(map(lambda s: intFunc(s), optValue.split('x')))\n                    if (2 == len(size)):\n                        self.papersize = size\n                        self.setprinter = True\n            elif opt in ['--user-profile']:\n                self.userProfile = arg\n\n        # Enable verbosity\n        if self.verbose >= 2:\n            print('Verbosity set to level %d' % self.verbose, file=sys.stderr)\n\n        self.filenames = args\n\n        if not self.listener and not self.showlist and not self.stdin and self.doctype != 'list' and not self.filenames:\n            print('unoconv: you have to provide a filename or url as argument', file=sys.stderr)\n            print('Try `unoconv -h\\' for more information.', file=sys.stderr)\n            sys.exit(255)\n\n        # Set connection string\n        if not self.connection:\n            if not self.pipe:\n                self.connection = \"socket,host=%s,port=%s,tcpNoDelay=1;urp;StarOffice.ComponentContext\" % (self.server, self.port)\n            else:\n                self.connection = \"pipe,name=%s;urp;StarOffice.ComponentContext\" % (self.pipe)\n\n        # Make it easier for people to use a doctype (first letter is enough)\n        if self.doctype:\n            for doctype in doctypes:\n                if doctype.startswith(self.doctype):\n                    self.doctype = doctype\n\n        # Check if the user request to see the list of formats\n        if self.showlist or self.format == 'list':\n            if self.doctype:\n                fmts.display(self.doctype)\n            else:\n                for t in doctypes:\n                    fmts.display(t)\n            sys.exit(0)\n\n        # If no format was specified, probe it or provide it.\n        if not self.format:\n            # Check if the command is in the form odt2pdf\n            l = sys.argv[0].split('2')\n            if len(l) == 2:\n                self.format = l[1]\n            # Use the extension of the output file\n            elif self.output and os.path.basename(self.output).find('.') >= 0:\n                self.format = os.path.splitext(self.output)[1].lstrip('.')\n\n        # Default to PDF.\n        if not self.format:\n            self.format = 'pdf'\n\n    def version(self):\n        print('unoconv %s' % __version__)\n        print('Written by Dag Wieers <dag@wieers.com>')\n        print('Homepage at http://dag.wieers.com/home-made/unoconv/')\n        print()\n        print('platform %s/%s' % (os.name, sys.platform))\n        print('python %s' % sys.version)\n\n        if uno:\n            # Get office product information.\n            product = uno.getComponentContext().ServiceManager.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            print(product.ooName, product.ooSetupVersionAboutBox)\n\n    def usage(self):\n        print('usage: unoconv [options] file [file2 ..]', file=sys.stderr)\n\n    def help(self):\n        print('''Convert from and to any format supported by LibreOffice\n\nunoconv options:\n  -c, --connection=string             use a custom connection string\n  -d, --doctype=type                  specify document type\n                                        (document, graphics, presentation, spreadsheet)\n  -e, --export=name=value             set export filter options\n                                        eg. -e PageRange=1-2\n  -f, --format=format                 specify the output format\n  -F, --field=name=value              replace user-defined text field with value\n                                        eg. -F Client_Name=\"Oracle\"\n  -i, --import=string                 set import filter option string\n                                        eg. -i utf8\n  -I, --import-filter-name=string     set import filter name, useful when converting stdin\n                                      or files without an extension)\n                                        eg. -I ooxml\n  -l, --listener                      start a permanent listener to use by unoconv clients\n  -n, --no-launch                     fail if no listener is found (default: launch one)\n  -o, --output=name                   output basename, filename or directory\n      --pipe=name                     alternative method of connection using a pipe\n  -p, --port=port                     specify the port (default: 2002)\n                                        to be used by client or listener\n      --password=string               provide a password to decrypt the document\n      --preserve                      keep timestamp and permissions of the original document\n  -s, --server=server                 specify the server address (default: 127.0.0.1)\n                                        to be used by client or listener\n      --show                          list the available output formats\n      --stdin                         read from stdin (filenames are ignored if provided)\n      --stdout                        write output to stdout\n  -t, --template=file                 import the styles from template (.ott)\n  -T, --timeout=secs                  timeout after secs if connection to listener fails\n      --unsafe-quiet-update           allow rendered document to fetch external resources (Warning: this is unsafe with untrusted input)\n  -v, --verbose                       be more and more verbose (-vvv for debugging)\n      --version                       display version number of unoconv, OOo/LO and platform details\n  -P, --printer=name=value            printer options\n                                        PaperFormat: specify printer paper format\n                                          eg. -P PaperFormat=A3\n                                        PaperOrientation: specify printer paper orientation\n                                          eg. -P PaperOrientation=landscape\n                                        PaperSize: specify printer paper size, paper format should set to USER, size=widthxheight\n                                          eg. -P PaperSize=130x200 means width=130, height=200\n  --disable-html-update-links   disables the recheck for updating links missed by libreoffice\n  --user-profile=path                 use a custom user profile path\n''', file=sys.stderr)\n\n\nclass Convertor:\n    def __init__(self):\n        global exitcode, ooproc, office, product\n        unocontext = None\n\n        # Do the LibreOffice component dance\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n\n        # Test for an existing connection\n        info(3, 'Connection type: %s' % op.connection)\n        unocontext = self.connect(resolver)\n\n        if not unocontext:\n            die(251, \"Unable to connect or start own listener. Aborting.\")\n\n        # And some more LibreOffice magic\n        unosvcmgr = unocontext.ServiceManager\n        self.desktop = unosvcmgr.createInstanceWithContext(\"com.sun.star.frame.Desktop\", unocontext)\n        self.cwd = unohelper.systemPathToFileUrl(os.getcwd())\n\n        # List all filters\n        # self.filters = unosvcmgr.createInstanceWithContext(\"com.sun.star.document.FilterFactory\", unocontext)\n        # for filter in self.filters.getElementNames():\n            # print filter\n            # print dir(filter), dir(filter.format)\n\n    def connect(self, resolver):\n        global ooproc, product, office\n        unocontext = None\n\n        try:\n            unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n        except NoConnectException as e:\n            # info(3, \"Existing listener not found.\\n%s\" % e)\n            info(3, \"Existing listener not found.\")\n\n            if op.nolaunch:\n                die(113, \"Existing listener not found. Unable start listener by parameters. Aborting.\")\n\n            # Start our own OpenOffice instance\n            info(3, \"Launching our own listener using %s.\" % office.binary)\n            try:\n                product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n                if product.ooName not in ('LibreOffice', 'LOdev') or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                    args = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-accept=%s\" % op.connection]\n                else:\n                    args = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--accept=%s\" % op.connection]\n                if op.userProfile:\n                    args.append(\"-env:UserInstallation=file://\" + realpath(op.userProfile))\n                info(2, '%s listener arguments are %s.' % (product.ooName, args))\n                ooproc = subprocess.Popen(args, env=os.environ)\n                info(2, '%s listener successfully started. (pid=%s)' % (product.ooName, ooproc.pid))\n\n                # Try connection to it for op.timeout seconds (flakky OpenOffice)\n                timeout = 0\n                while timeout <= op.timeout:\n                    # Is it already/still running?\n                    retcode = ooproc.poll()\n                    if retcode == 81:\n                        info(3, \"Caught exit code 81 (new installation). Restarting listener.\")\n                        return self.connect(resolver)\n                        break\n\n                    elif retcode is not None:\n                        info(3, \"Process %s (pid=%s) exited with %s.\" % (office.binary, ooproc.pid, retcode))\n                        break\n\n                    try:\n                        unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n                        break\n                    except NoConnectException:\n                        time.sleep(0.5)\n                        timeout += 0.5\n                    except:\n                        raise\n                else:\n                    error(\"Failed to connect to %s (pid=%s) in %d seconds.\\n%s\" % (office.binary, ooproc.pid, op.timeout, e))\n            except Exception as e:\n                raise\n                error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n        return unocontext\n\n    def getimportformat(self):\n        if op.doctype:\n            importformat = fmts.bydoctype(op.doctype, op.importfiltername)\n        else:\n            importformat = fmts.byname(op.importfiltername)\n\n        if not importformat:\n            error('Import format [%s] is not known to unoconv.' % importformat)\n\n        return importformat[0]\n\n    def getformat(self, inputfn):\n        doctype = None\n\n        # Get the output format from mapping\n        if op.doctype:\n            outputfmt = fmts.bydoctype(op.doctype, op.format)\n        else:\n            outputfmt = fmts.byname(op.format)\n\n            if not outputfmt:\n                outputfmt = fmts.byextension(os.extsep + op.format)\n\n        # If no doctype given, check list of acceptable formats for input file ext doctype.\n        # FIXME: This should go into the for-loop to match each individual input filename.\n        if outputfmt:\n            inputext = os.path.splitext(inputfn)[1]\n            inputfmt = fmts.byextension(inputext)\n            if inputfmt:\n                for fmt in outputfmt:\n                    if inputfmt[0].doctype == fmt.doctype:\n                        doctype = inputfmt[0].doctype\n                        outputfmt = fmt\n                        break\n                else:\n                    outputfmt = outputfmt[0]\n    #       print >>sys.stderr, 'Format `%s\\' is part of multiple doctypes %s, selecting `%s\\'.' % (format, [fmt.doctype for fmt in outputfmt], outputfmt[0].doctype)\n            else:\n                outputfmt = outputfmt[0]\n\n        # No format found, throw error\n        if not outputfmt:\n            if doctype:\n                error('Format [%s/%s] is not known to unoconv.' % (op.doctype, op.format))\n            else:\n                error('Format [%s] is not known to unoconv.' % op.format)\n            die(1)\n\n        return outputfmt\n\n    def preserve(self, inputfn, outputfn):\n        # Get timestamp of input file.\n        s = os.stat(inputfn)\n        times = (s.st_atime, s.st_mtime)\n        mode = s.st_mode\n        # Set it to output file.\n        with open(outputfn, \"a\") as f:\n            os.utime(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.utime in os.supports_fd else inputfn,\n                     times=times)\n            os.chmod(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.chmod in os.supports_fd else inputfn,\n                     mode)\n\n    def convert(self, inputfn):\n        global exitcode\n\n        document = None\n        outputfmt = self.getformat(inputfn)\n\n        if op.verbose > 0:\n            print('Input file:', inputfn, file=sys.stderr)\n\n        try:\n            # Import phase.\n            phase = \"import\"\n\n            # Load inputfile.\n            inputprops = UnoProps(Hidden=True, ReadOnly=True, UpdateDocMode=op.updateDocMode)\n\n            if op.password:\n                inputprops += UnoProps(Password=op.password)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilteroptions:\n                # print \"Import filter options: %s\" % op.importfilteroptions\n                inputprops += UnoProps(FilterOptions=op.importfilteroptions)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilter:\n                inputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.importfilter), ), 0), )\n\n            if op.importfiltername:\n                importformat = self.getimportformat()\n                inputprops += UnoProps(FilterName=importformat.filter)\n\n            if op.stdin:\n                inputStream = self.svcmgr.createInstanceWithContext(\"com.sun.star.io.SequenceInputStream\", self.context)\n                inputStream.initialize((uno.ByteSequence(inputfn),))\n                inputprops += UnoProps(InputStream=inputStream)\n                inputurl = 'private:stream'\n            elif os.path.exists(inputfn):\n                inputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(inputfn))\n            else:\n                inputurl = inputfn\n            document = self.desktop.loadComponentFromURL(inputurl, \"_blank\", 0, inputprops)\n\n            if not document:\n                raise UnoException(\"The document '%s' could not be opened.\" % inputurl, None)\n\n            # Import style template.\n            phase = \"import-style\"\n            if op.template:\n                if os.path.exists(op.template):\n                    info(1, \"Template file: %s\" % op.template)\n                    templateprops = UnoProps(OverwriteStyles=True)\n                    templateurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(op.template))\n                    document.StyleFamilies.loadStylesFromURL(templateurl, templateprops)\n                else:\n                    print('unoconv: template file `%s\\' does not exist.' % op.template, file=sys.stderr)\n                    exitcode = 1\n\n            # Force all cells to recalculate if we are able to. This will get rid of errors in cells.\n            # FIXME: We cannot recalculate the cells because it breaks issue #97 (cells get #VALUE)\n            # phase = \"recalculate\"\n            # try:\n                # document.calculateAll()\n            # except AttributeError:\n                # pass\n\n            # Update document links if appropriate\n            if op.updateDocMode != NO_UPDATE:\n                phase = \"update-links\"\n                try:\n                    document.updateLinks()\n                    # Found that when converting HTML files with external images, OO would only load five or six of\n                    # the images in the file. In the resulting document, the rest of the images did not appear. Cycling\n                    # through all the image references in the document seems to force OO to actually load them. Found\n                    # some helpful guidance in this thread:\n                    # https://forum.openoffice.org/en/forum/viewtopic.php?f=30&t=23909\n                    # Ideally we would like to have the option to embed the images into the document, but I have not been\n                    # able to figure out how to do this yet.\n                    if op.updatehtmllinks:\n                        graphObjs = document.GraphicObjects\n                        for i in range(0, graphObjs.getCount()):\n                            graphObj = graphObjs.getByIndex(i)\n                except AttributeError:\n                    # the document doesn't implement the XLinkUpdate interface\n                    pass\n\n            # Add/Replace variables\n            phase = \"replace-fields\"\n            for f in op.fields:\n                try:\n                    field = document.TextFieldMasters.getByName(\"com.sun.star.text.fieldmaster.User.%s\" % f)\n                    field.setPropertyValue('Content', op.fields[f])\n                except UnoException:\n                    error(\"unoconv: failed to replace variable '%s' with value '%s' in the document.\" % (f, op.fields[f]))\n                    pass\n\n            # Add/Replace metadata\n            phase = \"replace-metadata\"\n            props = document.getDocumentProperties()\n            user_props = props.getUserDefinedProperties()\n            for prop, value in op.metadata.items():\n                for container in (props, user_props):\n                    curr = getattr(container, prop, None)\n                    if curr is not None:\n                        setattr(container, prop, value)\n                        break\n                else:\n                    user_props.addProperty(prop, 0, '')\n                    user_props.setPropertyValue(prop, value)\n\n            # Update document indexes\n            phase = \"update-indexes\"\n            for ii in range(2):\n                # At first, update Table-of-Contents.\n                # ToC grows, so page numbers grow too.\n                # On second turn, update page numbers in ToC.\n                try:\n                    document.refresh()\n                    indexes = document.getDocumentIndexes()\n                except AttributeError:\n                    # The document doesn't implement the XRefreshable and/or\n                    # XDocumentIndexesSupplier interfaces\n                    break\n                else:\n                    for i in range(0, indexes.getCount()):\n                        indexes.getByIndex(i).update()\n\n            info(1, \"Selected output format: %s\" % outputfmt)\n            info(2, \"Selected office filter: %s\" % outputfmt.filter)\n            info(2, \"Used doctype: %s\" % outputfmt.doctype)\n\n            # Document properties phase\n            phase = \"disable-showchanges\"\n            try:\n                document.ShowChanges = False\n            except AttributeError:\n                pass\n\n            # Export phase\n            phase = \"export\"\n\n            outputprops = UnoProps(FilterName=outputfmt.filter, OutputStream=OutputStream(), Overwrite=True)\n\n            # Set default filter options\n            if op.exportfilteroptions:\n                # print \"Export filter options: %s\" % op.exportfilteroptions\n                outputprops += UnoProps(FilterOptions=op.exportfilteroptions)\n            elif outputfmt.filter == 'Text (encoded)':\n                outputprops += UnoProps(FilterOptions=\"UTF8,LF\")\n            elif outputfmt.filter == 'Text':\n                outputprops += UnoProps(FilterOptions=\"UTF8\")\n            elif outputfmt.filter == 'Text - txt - csv (StarCalc)':\n                outputprops += UnoProps(FilterOptions=\"44,34,UTF8\")\n\n            # Set printer options\n            if op.setprinter:\n                printer = document.getPrinter()\n                for i in range(len(printer)):\n                    if printer[i].Name == 'PaperOrientation' and op.paperorientation is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperOrientation', op.paperorientation)\n                    elif printer[i].Name == 'PaperFormat' and op.paperformat is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperFormat', op.paperformat)\n                    elif (printer[i].Name == 'PaperSize' and op.papersize is not None and len(op.papersize) == 2):\n                        printer[i].Value.Width = op.papersize[0]\n                        printer[i].Value.Height = op.papersize[1]\n                document.setPrinter(printer)\n\n            # Cannot use UnoProps for FilterData property\n            if op.exportfilter:\n                outputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.exportfilter), ), 0), )\n\n            if op.stdout:\n                # Ensure binary data to stdout works\n                # http://stackoverflow.com/questions/2374427/python-2-x-write-binary-output-to-stdout\n                if sys.platform == \"win32\":\n                    import msvcrt\n                    msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n                outputurl = \"private:stream\"\n            else:\n                if os.path.exists(inputfn):\n                    (inbase, ext) = os.path.splitext(inputfn)\n                else:\n                    (inbase, ext) = os.path.splitext(os.path.basename(inputfn))\n                if op.output:\n                    (outbase, ext) = os.path.splitext(op.output)\n                    if len(op.filenames) > 1:\n                        outputfn = realpath(outbase, os.path.basename(inbase) + os.extsep + outputfmt.extension)\n                    else:\n                        outputfn = realpath(outbase + os.extsep + outputfmt.extension)\n                else:\n                    outputfn = realpath(inbase + os.extsep + outputfmt.extension)\n\n                outputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(outputfn))\n\n            info(1, \"Output file: %s\" % outputurl)\n\n            try:\n                document.storeToURL(outputurl, tuple(outputprops))\n            except IOException as e:\n                raise UnoException(\"Unable to store document to %s (Error %s)\\n\\nProperties: %s\" % (outputurl, e.value, outputprops), None)\n\n            phase = \"dispose\"\n            document.dispose()\n            document.close(True)\n            if not op.stdout and op.preserve:\n                self.preserve(inputfn, outputfn)\n\n        except SystemError as e:\n            error(\"unoconv: SystemError during %s phase:\\n%s\" % (phase, e))\n            exitcode = 1\n\n        except RuntimeException as e:\n            error(\"unoconv: RuntimeException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 6\n\n        except DisposedException as e:\n            error(\"unoconv: DisposedException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 7\n\n        except IllegalArgumentException as e:\n            error(\"UNO IllegalArgument during %s phase:\\nSource file cannot be read. %s\" % (phase, e))\n            exitcode = 8\n\n        except IOException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: IOException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 3\n\n        except CannotConvertException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: CannotConvertException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 4\n\n        except UnoException as e:\n            if hasattr(e, 'ErrCode'):\n                error(\"unoconv: UnoException during %s phase in %s (ErrCode %d)\" % (phase, repr(e.__class__), e.ErrCode))\n                exitcode = e.ErrCode\n                pass\n            if hasattr(e, 'Message'):\n                error(\"unoconv: UnoException during %s phase:\\n%s\" % (phase, e.Message))\n                exitcode = 5\n            else:\n                error(\"unoconv: UnoException during %s phase in %s\" % (phase, repr(e.__class__)))\n                exitcode = 2\n                pass\n\n\nclass Listener:\n    def __init__(self):\n        global product\n\n        info(1, \"Start listener on %s:%s\" % (op.server, op.port))\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        try:\n            resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n            product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            try:\n                unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n            except NoConnectException:\n                pass\n            else:\n                info(1, \"Existing %s listener found, nothing to do.\" % product.ooName)\n                return\n            if product.ooName != \"LibreOffice\" or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                cmd = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nologo\", \"-nofirststartwizard\", \"-norestore\", \"-accept=%s\" % op.connection]\n            else:\n                cmd = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nologo\", \"--nofirststartwizard\", \"--norestore\", \"--accept=%s\" % op.connection]\n\n            # The rationale for using subprocess.Popen is to be able to handle\n            # a SIGTERM signal below and properly terminate the started office\n            # process then. This makes it possible to put the command unoconv -l\n            # under control of supervisor to deamonize it. Supervisor terminates\n            # via sending SIGTERM and sending SIGTERM to a running unoconv -l\n            # without the handler below will not terminate the office process\n            # together with it leaving the office process running.\n            office_process = subprocess.Popen(cmd, env=os.environ)\n\n            def sigterm_handler(signum, frame):\n                office_process.terminate()\n                die(6, 'Exiting on SIGTERM')\n\n            signal.signal(signal.SIGTERM, sigterm_handler)\n\n            ret = office_process.wait()\n            if ret == 81:\n                info(1, \"Restarting %s (first start - 81 exit code)\" % product.ooName)\n                office_process = subprocess.Popen(cmd, env=os.environ)\n                office_process.wait()\n            else:\n                raise Exception(\"%s crashed - exit code: %s\" % (product.ooName, ret))\n        except Exception as e:\n            error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n\ndef error(msg, file=sys.stderr):\n    \"\"\"Output error message.\"\"\"\n    print(msg, file=file)\n\n\ndef info(level, msg):\n    \"\"\"Output info message.\"\"\"\n    if 'op' not in globals():\n        pass\n    elif op.verbose >= 3 and level >= 3:\n        print(\"DEBUG:\", msg, file=sys.stderr)\n    elif not op.stdout and level <= op.verbose:\n        print(msg, file=sys.stdout)\n    elif level <= op.verbose:\n        print(msg, file=sys.stderr)\n\n\ndef die(ret, msg=None):\n    \"\"\"Print optional error and exit with errorcode.\"\"\"\n    global convertor, ooproc, office\n\n    if msg:\n        error('Error: %s' % msg)\n\n    # Did we start our own listener instance?\n    if not op.listener and ooproc and convertor:\n\n        # If there is a GUI now attached to the instance, disable listener.\n        if convertor.desktop.getCurrentFrame():\n            info(2, 'Trying to stop %s GUI listener.' % product.ooName)\n            try:\n                if product.ooName != \"LibreOffice\" or product.ooSetupVersion <= 3.3:\n                    subprocess.Popen([office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-unaccept=%s\" % op.connection], env=os.environ)\n                else:\n                    subprocess.Popen([office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--unaccept=%s\" % op.connection], env=os.environ)\n                ooproc.wait()\n                info(2, '%s listener successfully disabled.' % product.ooName)\n            except Exception as e:\n                error(\"Terminate using %s failed.\\n%s\" % (office.binary, e))\n\n        # If there is no GUI attached to the instance, terminate instance.\n        else:\n            info(3, 'Terminating %s instance.' % product.ooName)\n            try:\n                convertor.desktop.terminate()\n            except DisposedException:\n                info(2, '%s instance unsuccessfully closed, sending TERM signal.' % product.ooName)\n                try:\n                    ooproc.terminate()\n                except AttributeError:\n                    os.kill(ooproc.pid, 15)\n            info(3, 'Waiting for %s instance to exit.' % product.ooName)\n            ooproc.wait()\n\n        # LibreOffice processes may get stuck and we have to kill them.\n        # Is it still running?\n        if ooproc.poll() is None:\n            info(1, '%s instance still running, please investigate...' % product.ooName)\n            ooproc.wait()\n            info(2, '%s instance unsuccessfully terminated, sending KILL signal.' % product.ooName)\n            try:\n                ooproc.kill()\n            except AttributeError:\n                os.kill(ooproc.pid, 9)\n            info(3, 'Waiting for %s with pid %s to disappear.' % (ooproc.pid, product.ooName))\n            ooproc.wait()\n\n    # Allow Python GC to garbage collect pyuno object *before* exit call\n    # which avoids random segmentation faults --vpa\n    convertor = None\n\n    sys.exit(ret)\n\n\ndef main():\n    global convertor, exitcode\n    convertor = None\n\n    try:\n        if op.listener:\n            listener = Listener()\n\n        if op.stdin:\n            # Read stdin buffer in Python 3 in order to correctly handle binary streams.\n            # ref: https://docs.python.org/3.1/library/sys.html#sys.stdin\n            if sys.version_info.major > 2:\n                inputfn = sys.stdin.buffer.read()\n            else:\n                inputfn = sys.stdin.read()\n            convertor = Convertor()\n            convertor.convert(inputfn)\n        elif op.filenames:\n            convertor = Convertor()\n            for inputfn in op.filenames:\n                convertor.convert(inputfn)\n\n    except NoConnectException:\n        error(\"unoconv: could not find an existing connection to LibreOffice at %s:%s.\" % (op.server, op.port))\n        if op.connection:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"%s\\\"\" % (op.server, op.server, op.port, op.connection))\n        else:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=%s,port=%s;urp;\\\"\" % (op.server, op.server, op.port, op.server, op.port))\n            info(0, \"Please start an soffice instance on server '%s' by doing:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=127.0.0.1,port=%s;urp;\\\"\" % (op.server, op.port))\n        exitcode = 1\n    # except UnboundLocalError:\n        # die(252, \"Failed to connect to remote listener.\")\n    except OSError:\n        error(\"Warning: failed to launch Office suite. Aborting.\")\n\n\n# Main entrance\nif __name__ == '__main__':\n    exitcode = 0\n\n    info(3, 'sysname=%s, platform=%s, python=%s, python-version=%s' % (os.name, sys.platform, sys.executable, sys.version))\n\n    for of in find_offices():\n        if of.python != sys.executable and not sys.executable.startswith(of.basepath):\n            python_switch(of)\n        office_environ(of)\n        # debug_office()\n        try:\n            import uno\n            import unohelper\n            office = of\n            break\n        except:\n            # debug_office()\n            print(\"unoconv: Cannot find a suitable pyuno library and python binary combination in %s\" % of, file=sys.stderr)\n            print(\"ERROR:\", sys.exc_info()[1], file=sys.stderr)\n            print(file=sys.stderr)\n    else:\n        # debug_office()\n        print(\"unoconv: Cannot find a suitable office installation on your system.\", file=sys.stderr)\n        print(\"ERROR: Please locate your office installation and send your feedback to:\", file=sys.stderr)\n        print(\"       http://github.com/dagwieers/unoconv/issues\", file=sys.stderr)\n        sys.exit(1)\n\n    # Working pyuno library found. Import classes.\n    from com.sun.star.beans import PropertyValue\n    from com.sun.star.connection import NoConnectException\n    from com.sun.star.document.UpdateDocMode import NO_UPDATE, QUIET_UPDATE\n    from com.sun.star.io import IOException, XOutputStream\n    from com.sun.star.lang import DisposedException, IllegalArgumentException\n    from com.sun.star.script import CannotConvertException\n    from com.sun.star.uno import Exception as UnoException\n    from com.sun.star.uno import RuntimeException\n\n    # Build on imported classes.\n    class OutputStream(unohelper.Base, XOutputStream):\n        def __init__(self):\n            self.closed = 0\n\n        def closeOutput(self):\n            self.closed = 1\n\n        def writeBytes(self, seq):\n            try:\n                sys.stdout.buffer.write(seq.value)\n            except AttributeError:\n                sys.stdout.write(seq.value)\n\n        def flush(self):\n            pass\n\n    def UnoProps(**args):\n        props = []\n        for key in args:\n            prop = PropertyValue()\n            prop.Name = key\n            prop.Value = args[key]\n            props.append(prop)\n        return tuple(props)\n\n    op = Options(sys.argv[1:])\n\n    info(2, \"Using office base path: %s\" % office.basepath)\n    info(2, \"Using office binary path: %s\" % office.unopath)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        die(6, 'Exiting on user request')\n    die(exitcode)\n"}
{"type": "source_file", "path": "documents/unused_but_maybe_useful_code.py", "content": "# JS: // let streamed_content = dataObj.replace(/(?<![A-Z]:|\\/|[0-9]|[ivxlcdm])([.?!])(?=\\s|$|[0-9])(?!\\s*\\/)/g, '$1<br><br>');\r\n\r\ndef preprocess_string(s):\r\n    \"\"\"\r\n    This function removes all non-alphanumeric characters from the string, \r\n    converts it to lowercase, and trims whitespace.\r\n    It's not used in the current implementation of LARS.\r\n    \"\"\"\r\n    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\r\n\r\n\r\n\r\n\r\ndef PDFtoMSTrOCR(input_filepath):\r\n    \r\n    print(\"\\n\\nProcessing Document - PDF to MS TrOCR TXT\\n\\n\")\r\n\r\n    try:\r\n        read_return = read_config(['model_dir'])\r\n        model_directory = read_return['model_dir']\r\n    except Exception as e:\r\n        handle_local_error(\"Missing model_dir in config.json for PDFtoMSTrOCR. Error: \", e)\r\n\r\n    try:\r\n        source_filename = os.path.basename(input_filepath)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\r\n\r\n    # Convert PDF to  a list of images\r\n    try:\r\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\r\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\r\n    except Exception as e:\r\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\r\n    \r\n    # Set output path\r\n    output_text_file_path = input_filepath.replace(\".pdf\",\"_ms_tr_ocr_cleaned.txt\") \r\n    raw_output_text_file_path = input_filepath.replace(\".pdf\",\"_ms_tr_ocr_raw.txt\") \r\n\r\n    # Init list for Whoosh indexing\r\n    pdf_data = []\r\n\r\n    # Initialize text output\r\n    try:\r\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\r\n        raw_output_text_file = open(raw_output_text_file_path, 'w', encoding='utf-8')\r\n    except Exception as e:\r\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\r\n    \r\n    # Setting up Cleaner LLM:\r\n    llm_name = 'openhermes-2.5-mistral-7b.Q8_0.gguf'\r\n    llm_dir = model_directory + '/' + llm_name\r\n    config = {'context_length': 8192, 'max_new_tokens': 8192, 'gpu_layers':50}\r\n    cleaner_llm = CTransformers(model=llm_dir, model_type=\"llama\", config=config)\r\n    # cleanup_template = PromptTemplate(template=\"Correct the following text for any gramatical and formatting errors, otherwise leaving it unchanged: {input}\", input_variables=[\"input\"])\r\n    conv_chain = ConversationChain(llm = cleaner_llm)\r\n\r\n    #Load OCR TrOCR model:\r\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\r\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\r\n    \r\n    # Iterate over each page and apply OCR:\r\n    print(\"\\n\\nBeginning image to MS TrOCR\\n\\n\")\r\n    for page_number, page_image in enumerate(pages, start = 1):\r\n\r\n        rgb_image = page_image.convert(\"RGB\")\r\n        width, height = rgb_image.size\r\n\r\n        page_text = \"\"\r\n\r\n        block_no = 0\r\n\r\n        # original_stdout = sys.stdout\r\n\r\n        # Process the page in 240x71 blocks:\r\n        # for y in range(0, height, 71):\r\n        #     for x in range(0, width, 240):\r\n                \r\n        #         print(f\"Processing block {block_no}\")\r\n\r\n        #         # long-term: discard output\r\n        #         # f = open(os.devnull, 'w')\r\n        #         # sys.stdout = f\r\n\r\n        #         # Crop block:\r\n        #         block = rgb_image.crop((x, y, x + 240, y + 71))\r\n\r\n        #         # Process block with TrOCR:\r\n        #         pixel_values = processor(images=block, return_tensors=\"pt\").pixel_values\r\n\r\n        #         generated_ids = model.generate(pixel_values)\r\n        #         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n\r\n        #         print(f\"generated block text: {generated_text}\")\r\n\r\n        #         if str(generated_text) != \"0.00\":\r\n        #             page_text += generated_text\r\n\r\n        #         # Reset stdout to its original value\r\n        #         # sys.stdout = original_stdout\r\n\r\n        #         block_no += 1\r\n\r\n        for y in range(0, height, 50):\r\n                \r\n            print(f\"Processing block {block_no}\")\r\n\r\n            # long-term: discard output\r\n            # f = open(os.devnull, 'w')\r\n            # sys.stdout = f\r\n\r\n            # Crop block:\r\n            block = rgb_image.crop((0, y, width, y + 50))\r\n\r\n            # Process block with TrOCR:\r\n            pixel_values = processor(images=block, return_tensors=\"pt\").pixel_values\r\n\r\n            generated_ids = model.generate(pixel_values)\r\n            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n\r\n            print(f\"generated block text: {generated_text}\")\r\n\r\n            if str(generated_text) != \"0.00\":\r\n                page_text += generated_text\r\n\r\n            # Reset stdout to its original value\r\n            # sys.stdout = original_stdout\r\n\r\n            block_no += 1\r\n\r\n        # Save raw output of the above process for analysis:\r\n        try:\r\n            raw_output_text_file.write(page_text + '\\n')\r\n        except Exception as e:\r\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\r\n        \r\n        # Clean Text:\r\n        print(f\"\\n\\nCleaning Page Text with {llm_name}\\n\\n\")\r\n        llm_input = f\"Correct the following text for any gramatical and formatting errors, otherwise leaving it unchanged: {page_text}\"\r\n        try:\r\n            clean_text = conv_chain.predict(input=llm_input)\r\n        except Exception as e:\r\n            handle_local_error(\"Could not clean text with LLM, encountered error: \", e)\r\n\r\n        # Write the cleaned up text to the file\r\n        try:\r\n            output_text_file.write(clean_text + '\\n')\r\n        except Exception as e:\r\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\r\n\r\n        # Whoosh prep\r\n        #whoosh_clean_text = preprocess_string(clean_text)\r\n        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_number+1}\r\n        pdf_data.append(whoosh_page_dict_entry)\r\n\r\n    # Close all files\r\n    raw_output_text_file.close()\r\n    output_text_file.close()\r\n\r\n     # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\r\n    try:\r\n        whoosh_indexer(pdf_data)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\r\n\r\n    return output_text_file_path\r\n\r\n\r\n\r\n#Local OCR using PyTesseract - Not used in LARS\r\ndef PDFtoOCRTXT(input_filepath):\r\n    \r\n    print(\"\\n\\nProcessing Document - PDF to OCR TXT\\n\\n\")\r\n\r\n    try:\r\n        read_return = read_config(['base_directory'])\r\n        app_base_directory = read_return['base_directory']\r\n    except Exception as e:\r\n        handle_local_error(\"Missing base_directory in config.json for PDFtoOCRTXT. Error: \", e)\r\n\r\n    try:\r\n        source_filename = os.path.basename(input_filepath)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\r\n\r\n    # Convert PDF to  a list of images\r\n    try:\r\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\r\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\r\n    except Exception as e:\r\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\r\n    \r\n    # Set output path\r\n    output_text_file_path = input_filepath.replace(\".pdf\",\"_ocr_300.txt\") \r\n\r\n    # Init list for Whoosh indexing\r\n    pdf_data = []\r\n\r\n    # Initialize text output\r\n    try:\r\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\r\n    except Exception as e:\r\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\r\n    \r\n    # Iterate over each page and apply OCR:\r\n    print(\"\\n\\nBeginning image to Text OCR\\n\\n\")\r\n    for page_number, page_image in enumerate(pages, start=1):\r\n\r\n        try:\r\n            custom_config = r'--oem 3 --psm 3 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\r\n            text = pytesseract.image_to_string(page_image, config='--psm 3')    # Page Segmentation Mode (PSM) 3 - Default; Fully Automatic Page Segmentation & OCR, but no Orientation and Script Detection (OSD). PSM 3,4 & 6 are common for docs. For a full list of PSMs, ask ChatGPT \"Can you give me a walkthrough of all the different Page Segmentation Modes in Python's PyTesseract?\" \r\n            #text = pytesseract.image_to_string(page_image, config=custom_config)\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not OCR text from page, encountered error: \", e)\r\n            \r\n        # Optionally save image for review\r\n        try:\r\n            ocr_img_directory = app_base_directory + '/OCR_IMAGES'\r\n            if not os.path.exists(ocr_img_directory):\r\n                os.makedirs(ocr_img_directory)\r\n            ocr_img_filename = f'{ocr_img_directory}/{source_filename}_page_{page_number}.jpg'\r\n            page_image.save(ocr_img_filename)\r\n        except Exception as e:\r\n            error_message = f\"Could not save OCR image for {source_filename}_page_{page_number}, encountered error: \"\r\n            handle_error_no_return(error_message, e)\r\n\r\n        # clean_text = text\r\n        # Clean text\r\n        clean_text = clean_text_string(text)\r\n        \r\n        # Optionally, you can include page numbers in the text file\r\n        # output_text_file.write(f'\\n\\n--- Page {page_num + 1} ---\\n\\n')\r\n        \r\n        # Write the extracted text to the file\r\n        try:\r\n            output_text_file.write(clean_text + '\\n')\r\n        except Exception as e:\r\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\r\n\r\n        # Whoosh prep\r\n        #whoosh_clean_text = preprocess_string(clean_text)\r\n        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_number+1}\r\n        pdf_data.append(whoosh_page_dict_entry)\r\n\r\n    # Close all files\r\n    output_text_file.close()\r\n\r\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\r\n    try:\r\n        whoosh_indexer(pdf_data)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\r\n\r\n    return output_text_file_path\r\n\r\n\r\n\r\n\r\ndef PDFtoAzureOCRTXT_url(input_filepath):\r\n    \r\n    print(\"\\n\\nProcessing Document - PDF to Azure OCR TXT\\n\\n\")\r\n\r\n    try:\r\n        read_return = read_config(['azure_ocr_endpoint', 'azure_ocr_subscription_key'])\r\n        azure_ocr_endpoint = read_return['azure_ocr_endpoint']\r\n        azure_ocr_subscription_key = read_return['azure_ocr_subscription_key']\r\n    except Exception as e:\r\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureOCRTXT_url, please provide required API config. Error: \", e)\r\n    \r\n    try:\r\n        os.environ[\"azure_ocr_endpoint\"] = azure_ocr_endpoint\r\n        os.environ[\"azure_ocr_subscription_key\"] = azure_ocr_subscription_key\r\n    except Exception as e:\r\n        handle_local_error(\"Could not set OS environment variables for Azure OCR, encountered error: \", e)\r\n\r\n    try:\r\n        source_filename = os.path.basename(input_filepath)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\r\n\r\n    # Convert PDF to  a list of images\r\n    try:\r\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\r\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\r\n    except Exception as e:\r\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\r\n    \r\n    # Set output path\r\n    output_text_file_path = input_filepath.replace(\".pdf\",\"_azure_ocr_300.txt\") \r\n\r\n    # Init list for Whoosh indexing\r\n    pdf_data = []\r\n\r\n    # Initialize text output\r\n    try:\r\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\r\n    except Exception as e:\r\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\r\n    \r\n    # Init Azure VisionServiceOptions\r\n    service_options = sdk.VisionServiceOptions(os.environ[\"azure_ocr_endpoint\"], os.environ[\"azure_ocr_subscription_key\"])\r\n    \r\n    # Iterate over each page and apply OCR:\r\n    print(\"\\n\\nBeginning image to Text OCR\\n\\n\")\r\n    for page_number, image in enumerate(pages, start = 1):\r\n    #for image in pages:\r\n        # Convert to bytes\r\n        img_byte_arr = io.BytesIO()\r\n        image.save(img_byte_arr, format='PNG')\r\n        img_byte_arr = img_byte_arr.getvalue()\r\n\r\n        # Save the image temporarily\r\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as temp_image_file:\r\n            image.save(temp_image_file, format='PNG')\r\n            temp_image_path = temp_image_file.name\r\n\r\n            # Setup cision source with byte array\r\n            vision_source = sdk.VisionSource(url=temp_image_path)\r\n\r\n            # Set analysis options:\r\n            analysis_options = sdk.ImageAnalysisOptions()\r\n            analysis_options.features = sdk.ImageAnalysisFeature.TEXT\r\n\r\n\r\n            # Send to Azure OCR & analyze the image\r\n            image_analyzer = sdk.ImageAnalyzer(service_options, vision_source, analysis_options)\r\n            result = image_analyzer.analyze()\r\n\r\n            if result.reason == sdk.ImageAnalysisResultReason.ANALYZED:\r\n                if result.text is not None:\r\n                    # print(\"Text:\")\r\n                    for line in result.text.lines:\r\n                        # print(f\"Line: {line.content}\")\r\n                        clean_text = line.content\r\n\r\n                        # Write the extracted text to the file:\r\n                        try:\r\n                            output_text_file.write(clean_text + '\\n')\r\n                        except Exception as e:\r\n                            handle_local_error(\"Could not write to output text file, encountered error: \", e)\r\n\r\n                        # Whoosh prep\r\n                        #whoosh_clean_text = preprocess_string(clean_text)\r\n                        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_number+1}\r\n                        pdf_data.append(whoosh_page_dict_entry)\r\n\r\n            else:\r\n                # Handle errors:\r\n                error_details = sdk.ImageAnalysisErrorDetails.from_result(result)\r\n                print(\" Analysis failed.\")\r\n                print(\"   Error reason: {}\".format(error_details.reason))\r\n                print(\"   Error code: {}\".format(error_details.error_code))\r\n                print(\"   Error message: {}\".format(error_details.message))\r\n\r\n    # Close all files\r\n    output_text_file.close()\r\n\r\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\r\n    try:\r\n        whoosh_indexer(pdf_data)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\r\n\r\n    return output_text_file_path\r\n\r\n\r\n\r\ndef TxtCleaner(input_file):\r\n    \"\"\"\r\n    Processes a text file by cleaning and subsequent indexing.\r\n\r\n    :param input_file: Path to the input text file within the app's folder.\r\n    :return: path to the cleaned output text file.\r\n    \"\"\"\r\n\r\n    print(\"\\nProcessing Text File\")\r\n\r\n    # Ensure the file is .txt:\r\n    if not input_file.lower().endswith('.txt'):\r\n        raise ValueError(\"File must be a .txt file\")\r\n    \r\n    # Get filename\r\n    try:\r\n        source_filename = os.path.basename(input_file)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\r\n\r\n    # Set output path:\r\n    output_text_file_path = input_file.replace(\".txt\",\"_cleaned.txt\")\r\n\r\n    # Init list for Whoosh indexing\r\n    text_data = []\r\n\r\n    try:\r\n        # Read and process the file, \\ is a continuation char in Python used to split long lines of code for readibility!\r\n        with open(input_file, 'r', encoding='utf-8') as input_file ,\\\r\n                open(output_text_file_path, 'w', encoding='utf-8') as output_text_file:\r\n            \r\n            # enumerate returns a tuple containing the count and value of an iterable such as a file or list. It starts at 0 but here we specify 1 as the start index:\r\n            for line_num, line in enumerate(input_file, 1):\r\n                \r\n                # Clean text:\r\n                clean_line = line.replace(\"\", \"\").replace(\"\", \"\").replace(\"\", \"\")\r\n                clean_line = clean_line.replace(\"Confidential Copy \\n            for \\n         DKPPU\", \"\")\r\n                clean_line = re.sub(r'\\n(?=[a-z.])', '', clean_line)\r\n                clean_line = re.sub(r'\\n+', '\\n', clean_line)\r\n                clean_line = re.sub(r'[^\\w\\s]', '', clean_line)     # This regex substitutes anything that is not a word character or whitespace with an empty string.\r\n                clean_line = re.sub(r'\\s+', ' ', clean_line).strip()    # This regex substitutes any sequence of whitespace characters with a single space.\r\n\r\n                # Write the cleaned text to the output file \r\n                output_text_file.write(clean_line + '\\n')\r\n\r\n                # Whoosh prep\r\n                whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_line, \"pagenumber\":line_num}\r\n                text_data.append(whoosh_page_dict_entry)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not create text file, encountered error: \", e)\r\n\r\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\r\n    try:\r\n        whoosh_indexer(text_data)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\r\n\r\n    return output_text_file_path\r\n\r\n\r\n\r\ndef find_text_in_pdf_dpr(pdf_path, target_text):\r\n    print(\"pdf_path, target_text: \", pdf_path, \", \", target_text)\r\n\r\n    page_numbers = []\r\n\r\n    try:\r\n        with open(pdf_path, 'rb') as file:\r\n\r\n            reader = PyPDF2.PdfReader(file)\r\n\r\n            for page_num in range(len(reader.pages)):\r\n\r\n                page = reader.pages[page_num]\r\n\r\n                content = page.extract_text()\r\n\r\n                if target_text in content:\r\n                    print(\"found match!\")\r\n                    page_numbers.append(page_num + 1)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not find page numbers from PDF, encountered error: \", e)\r\n\r\n    print(\"page numbers before returning: \")\r\n    print(page_numbers)\r\n    return page_numbers\r\n\r\n\r\n\r\ndef find_text_in_pdf(reference_pages):\r\n\r\n    user_should_refer_pages_in_doc = {}\r\n    docs_have_relevant_info = False\r\n\r\n    for doc_path in reference_pages:\r\n\r\n        source_filename = os.path.basename(doc_path)\r\n\r\n        try:\r\n            text = extract_text(doc_path)\r\n            pages = text.split(\"\\f\")\r\n            page_numbers = []\r\n\r\n            for page_num, content in enumerate(pages):\r\n                for target_text in reference_pages[doc_path]:\r\n                    target_text = preprocess_string(target_text)\r\n                    content = preprocess_string(content)\r\n                    if target_text in content:\r\n                        page_numbers.append(page_num + 1)\r\n                        docs_have_relevant_info = True\r\n            \r\n            page_numbers = set(page_numbers)\r\n\r\n            user_should_refer_pages_in_doc[source_filename] = page_numbers\r\n        except Exception as e:\r\n            handle_local_error(\"Could not find page numbers from PDF, encountered error: \", e)\r\n\r\n    return docs_have_relevant_info, user_should_refer_pages_in_doc\r\n\r\n\r\n\r\n\r\ndef whoosh_text_in_pdf(reference_pages):\r\n\r\n    print(\"Searching Index\")\r\n\r\n    try:\r\n        read_return = read_config(['index_dir'])\r\n        index_dir = read_return['index_dir']\r\n    except Exception as e:\r\n        handle_local_error(\"Missing index_dir in config.json for method whoosh_text_in_pdf. Error: \", e)\r\n\r\n    user_should_refer_pages_in_doc = {}\r\n    docs_have_relevant_info = False\r\n\r\n    try:\r\n        # Open the index\r\n        ix = open_dir(index_dir)\r\n\r\n        # Create a 'searcher' object\r\n        with ix.searcher() as searcher:\r\n            query_parser = QueryParser(\"content\", ix.schema)\r\n\r\n            for doc in reference_pages:\r\n                \r\n                source_filename = os.path.basename(doc)\r\n                page_numbers = []\r\n                \r\n                for search_string in reference_pages[doc]:\r\n\r\n                    # Only search for non-empty search strings\r\n                    if search_string:\r\n\r\n                        query = query_parser.parse(search_string)\r\n\r\n                        results = searcher.search(query)\r\n\r\n                        for hit in results:\r\n                            print(f\"Found in {hit['title']} on page {hit['pagenumber']}\")\r\n                            page_numbers.append(int(hit['pagenumber']))\r\n                            docs_have_relevant_info = True\r\n\r\n                page_numbers = set(page_numbers)\r\n                user_should_refer_pages_in_doc[source_filename] = page_numbers\r\n\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\r\n\r\n    return docs_have_relevant_info, user_should_refer_pages_in_doc\r\n\r\n\r\n\r\n# Route to handle the submission of the second form (file loading)\r\n@app.route('/process_file', methods=['POST'])\r\ndef process_file():\r\n\r\n    use_ocr = False\r\n    try:\r\n        read_return = read_config(['use_ocr', 'ocr_service_choice'])\r\n        use_ocr = read_return['use_ocr']\r\n        ocr_service_choice = read_return['ocr_service_choice']\r\n    except Exception as e:\r\n        handle_api_error(\"Could not determine use_ocr in config.json for process_new_file. Disabling OCR and proceeding. Error: \", e)\r\n\r\n    try:\r\n        load_new_file = request.form.get('load_new_file', 'n').lower()\r\n    except Exception as e:\r\n        handle_api_error(\"Server-side error - could not interpret user selection. Encountered error: \", e)\r\n\r\n    if load_new_file ==  'y':\r\n\r\n        try:\r\n            input_file = request.files['input_file']\r\n        except Exception as e:\r\n            handle_api_error(\"Server-side error recieving file: \", e)\r\n\r\n        # Ensure the filename is secure\r\n        filename = secure_filename(input_file.filename)\r\n        if \"PDF\" in filename:\r\n            filename = filename.replace(\"PDF\", \"pdf\")\r\n\r\n        pdf_file = False\r\n        txt_file = False\r\n\r\n        if filename.endswith('.pdf'):\r\n            pdf_file = True\r\n        elif filename.endswith('.txt'):\r\n            txt_file = True\r\n        else:\r\n            return jsonify(success=False, error=\"Invalid file format, expected a PDF or TXT file\"), 400 #HTTP Bad Request\r\n\r\n\r\n        try:\r\n            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\r\n            \r\n            print(\"Loading new file - filename: \", filename)\r\n            print(\"Loading new file - filepath: \", filepath)\r\n\r\n            # Save the uploaded file to the specified path\r\n            input_file.save(filepath)\r\n        except Exception as e:\r\n            handle_api_error(\"Failed to save document to app folder, encountered error: \", e)\r\n        \r\n        # print(\"input_file: \", input_file)\r\n        \r\n        if pdf_file:\r\n            print(\"Processing PDF file\")\r\n            \r\n            if use_ocr:\r\n                try:\r\n                    if ocr_service_choice == 'AzureVision':\r\n                        input_file = PDFtoAzureOCRTXT(filepath)\r\n                    elif ocr_service_choice == 'AzureDocAi':\r\n                        input_file = PDFtoAzureDocAiTXT(filepath)\r\n                except Exception as e:\r\n                    handle_error_no_return(\"Failed to OCR text from PDF. Will now attempt to extract text via PyPDF2. Encountered error: \", e)\r\n                    try:\r\n                        input_file = PDFtoTXT(filepath)\r\n                    except Exception as e:\r\n                        handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\r\n            else:\r\n                try:\r\n                    input_file = PDFtoTXT(filepath)\r\n                except Exception as e:\r\n                    handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\r\n\r\n            try:\r\n                images = extract_images_from_pdf(filepath)\r\n            except Exception as e:\r\n                handle_error_no_return(\"Failed to extract images from the PDF document, encountered error: \", e)\r\n\r\n            try:\r\n                store_images_to_db(images)\r\n            except Exception as e:\r\n                handle_error_no_return(\"Failed to save images to database, encountered error: \", e)\r\n\r\n        if txt_file:\r\n            print(\"Processing Text file\")\r\n\r\n            try:\r\n                # Need to set to filepath as input_file just contains the file itself from the POST!\r\n                input_file = TxtCleaner(filepath)\r\n            except Exception as e:\r\n                handle_api_error(\"Failed to extract text from PDF: \", e)\r\n        \r\n        try:\r\n            LoadNewDocument(input_file)         \r\n        except Exception as e:\r\n            handle_api_error(\"Failed to extract text from PDF: \", e)\r\n\r\n\r\n    # Don't get confused about not loading the VectorDB here! You'll notice that we're doing an additional VectorDB load step in the route method below, 'process_new_file()', but not here!\r\n    # This is because this current route is triggered BEFORE the initital model and VectorDB loading occurs, so after this '/load_model_and_vectordb' triggers and loads the DB anyway!\r\n    # However, when '/process_new_file' is invoked mid-chat, the VectorDB must be RE-LOADED! Hence the extra step in the route below. \r\n    \r\n    #return \"File processed (or not) and ready for chat!\"\r\n    #return redirect(url_for('load_model_and_vectordb'))\r\n    return jsonify(success=True)\r\n\r\n\r\n@app.route('/load_model_and_vectordb')\r\ndef load_model_and_vectordb():\r\n    \r\n    global LLM\r\n    global VECTOR_STORE\r\n    global LOADED_UP\r\n    global LLM_CHANGE_RELOAD_TRIGGER_SET\r\n    global HISTORY_SUMMARY\r\n    global HISTORY_MEMORY_WITH_BUFFER\r\n    global HF_BGE_EMBEDDINGS\r\n    global AZURE_OPENAI_EMBEDDINGS\r\n\r\n    try:\r\n        read_return = read_config(['model_choice', 'use_gpu_for_embeddings', 'use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder', 'use_azure_open_ai'])\r\n        model_choice = read_return['model_choice']\r\n        use_gpu_for_embeddings = read_return['use_gpu_for_embeddings']\r\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\r\n        use_openai_embeddings = read_return['use_openai_embeddings']\r\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\r\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\r\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\r\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\r\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\r\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\r\n        use_azure_open_ai = read_return['use_azure_open_ai']\r\n    except Exception as e:\r\n        handle_api_error(\"Missing values in config.json when attempting to load_model_and_vectordb. Error: \", e)\r\n\r\n\r\n    # global CONVERSATION_RAG_CHAIN_WITH_SUMMARY_BUFFER\r\n\r\n    if LOADED_UP and not LLM_CHANGE_RELOAD_TRIGGER_SET:\r\n        print(f'\\n\\nAlready loaded! Clearing chat history and returning model choice: {model_choice}\\n\\n')\r\n        HISTORY_MEMORY_WITH_BUFFER.chat_memory.clear()\r\n        HISTORY_MEMORY_WITH_BUFFER = ConversationSummaryBufferMemory(llm=LLM, max_token_limit=300, return_messages=False)\r\n        HISTORY_SUMMARY = {}\r\n        return jsonify({'success': True, 'llm_model': model_choice})\r\n    elif LLM_CHANGE_RELOAD_TRIGGER_SET:\r\n        print('\\n\\nForce restarting app! Preserving chat history and proceeding to reload the VectorDB & LLM. Resetting reset flag too.\\n\\n')\r\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False\r\n        \r\n\r\n    ### 1 - Load VectorDB from disk\r\n    print(\"\\n\\nLoading VectorDB: ChromaDB\\n\\n\")\r\n    try:\r\n        if use_sbert_embeddings:\r\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\r\n            # try:\r\n            #     # chroma_client = VECTOR_STORE.PersistentClient\r\n            #     # max_batch_size = chroma_client._producer.max_batch_size\r\n            #     max_batch_size = VECTOR_STORE.max_batch_size\r\n            #     print(f\"max_batch_size: {max_batch_size}\")\r\n            # except Exception as e:\r\n            #     print(f\"Could not get max_batch_size. Error: {e}\")\r\n        \r\n        elif use_openai_embeddings:\r\n\r\n            try:\r\n                read_return = read_config(['azure_openai_base_url', 'azure_openai_api_key', 'azure_openai_api_type', 'c'])\r\n                azure_openai_base_url = read_return['azure_openai_base_url']\r\n                azure_openai_api_key = read_return['azure_openai_api_key']\r\n                azure_openai_api_type = read_return['azure_openai_api_type']\r\n                azure_openai_api_version = read_return['azure_openai_api_version']\r\n            except Exception as e:\r\n                handle_error_no_return(\"Missing values for Azure OpenAI Embeddings in method load_model_and_vectordb in config.json. Error: \", e)\r\n            \r\n            try:\r\n                os.environ[\"OPENAI_API_BASE\"] = azure_openai_base_url\r\n                os.environ[\"OPENAI_API_KEY\"] = azure_openai_api_key\r\n                os.environ[\"OPENAI_API_TYPE\"] = azure_openai_api_type\r\n                os.environ[\"OPENAI_API_VERSION\"] = azure_openai_api_version\r\n            except Exception as e:\r\n                handle_error_no_return(\"Could not set OS environment variables for Azure OpenAI Embeddings in load_model_and_vectordb, encountered error: \", e)\r\n            \r\n            AZURE_OPENAI_EMBEDDINGS = OpenAIEmbeddings(deployment=\"openai-ada-embedding\")\r\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\r\n        \r\n        elif use_bge_base_embeddings:\r\n            if HF_BGE_EMBEDDINGS is not None:\r\n                VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\r\n            else:\r\n                model_name = \"BAAI/bge-base-en\"\r\n                model_kwargs = {}\r\n                if use_gpu_for_embeddings:\r\n                    model_kwargs.update({\"device\": \"cuda\"})\r\n                else:\r\n                    model_kwargs.update({\"device\": \"cpu\"})\r\n                encode_kwargs = {\"normalize_embeddings\": True}\r\n                HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\r\n                    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\r\n                )\r\n                VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\r\n        \r\n        elif use_bge_large_embeddings:\r\n            if HF_BGE_EMBEDDINGS is not None:\r\n                VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\r\n            else:\r\n                model_name = \"BAAI/bge-large-en\"\r\n                model_kwargs = {}\r\n                if use_gpu_for_embeddings:\r\n                    model_kwargs.update({\"device\": \"cuda\"})\r\n                else:\r\n                    model_kwargs.update({\"device\": \"cpu\"})\r\n                encode_kwargs = {\"normalize_embeddings\": True}\r\n                HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\r\n                    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\r\n                )\r\n                VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\r\n        \r\n        #VECTOR_STORE = Chroma(persist_directory=VECTORDB_SBERT_FOLDER, embedding_function=HuggingFaceEmbeddings())\r\n    except Exception as e:\r\n        handle_api_error(\"Could not load VectorDB, encountered error: \", e)\r\n\r\n\r\n    ### 2 - Load LLM Model from config.json ###\r\n    print(\"\\n\\nLoading LLM from config.json\\n\\n\")\r\n    try:\r\n\r\n        if not use_azure_open_ai:\r\n\r\n            try:\r\n                read_return = read_config(['use_gpu', 'model_dir', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'local_llm_model_type', 'local_llm_temperature'])\r\n                use_gpu = read_return['use_gpu']\r\n                local_llm_context_length = read_return['local_llm_context_length']\r\n                local_llm_max_new_tokens = read_return['local_llm_max_new_tokens']\r\n                local_llm_gpu_layers = read_return['local_llm_gpu_layers']\r\n                local_llm_model_type = read_return['local_llm_model_type']\r\n                local_llm_temperature = read_return['local_llm_temperature']\r\n                model_dir = read_return['model_dir']\r\n            except Exception as e:\r\n                handle_api_error(\"Missing values in config.json for setting-up local-LLM in method load_model_and_vectordb. Error: \", e)\r\n\r\n            llm_model = model_dir + '/' + model_choice\r\n\r\n            config = {'context_length': local_llm_context_length, 'max_new_tokens': local_llm_max_new_tokens, 'temperature': local_llm_temperature}\r\n            \r\n            if use_gpu:\r\n                config.update({'gpu_layers':local_llm_gpu_layers})\r\n            \r\n            LLM = CTransformers(model=llm_model, model_type=local_llm_model_type, config=config, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\r\n\r\n        else:\r\n\r\n            try:\r\n                read_return = read_config(['azure_openai_base_url', 'azure_openai_api_key', 'azure_openai_api_type', 'azure_openai_deployment_name', 'azure_openai_api_version', 'azure_openai_max_tokens', 'azure_openai_temperature'])\r\n                azure_openai_base_url = read_return['azure_openai_base_url']\r\n                azure_openai_api_key = read_return['azure_openai_api_key']\r\n                azure_openai_api_type = read_return['azure_openai_api_type']\r\n                azure_openai_deployment_name = read_return['azure_openai_deployment_name']\r\n                azure_openai_api_version = read_return['azure_openai_api_version']\r\n                azure_openai_max_tokens = read_return['azure_openai_max_tokens']\r\n                azure_openai_temperature = read_return['azure_openai_temperature']\r\n            except Exception as e:\r\n                handle_api_error(\"Missing values in config.json for setting-up Azure-OpenAI-LLM in method load_model_and_vectordb. Error: \", e)\r\n            \r\n            LLM = AzureChatOpenAI(\r\n                openai_api_base=azure_openai_base_url,\r\n                openai_api_version=azure_openai_api_version,\r\n                deployment_name=azure_openai_deployment_name,\r\n                openai_api_key=azure_openai_api_key,\r\n                openai_api_type=azure_openai_api_type,\r\n                max_tokens=azure_openai_max_tokens, \r\n                temperature=azure_openai_temperature,\r\n                streaming=True,\r\n                callbacks=[StreamingStdOutCallbackHandler()]\r\n            )\r\n\r\n    except Exception as e:\r\n        handle_api_error(\"Could not load LLM, encountered error: \", e)\r\n\r\n    print(\"\\n\\n\")\r\n\r\n\r\n    ### 3 - Define History memory w/ buffer:\r\n    try:\r\n        HISTORY_MEMORY_WITH_BUFFER = ConversationSummaryBufferMemory(llm=LLM, max_token_limit=300, return_messages=False)\r\n    except Exception as e:\r\n        handle_api_error(\"Could not setup memory buffer for LLM, encountered error: \", e)\r\n    \r\n    LOADED_UP = True\r\n    print(f'\\n\\nDone loading! Returning model choice: {model_choice}\\n\\n')\r\n    return jsonify({'success': True, 'llm_model': model_choice})\r\n\r\n\r\n # Do not delete as vectorDB folder remains on disk\r\n    # Once new VectorDB is created, proceed to update records DB:\r\n    # try:\r\n    #     read_return = read_config(['sqlite_docs_loaded_db'])\r\n    #     sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\r\n    # except Exception as e:\r\n    #     handle_api_error(\"Missing sqlite_docs_loaded_db in config.json in method reset_vector_db_on_disk. Error: \", e)\r\n    \r\n    # try:\r\n    #     conn = sqlite3.connect(sqlite_docs_loaded_db)\r\n    #     c = conn.cursor()\r\n    # except Exception as e:\r\n    #     handle_api_error(\"Could not connect to sqlite_docs_loaded_db database to delete file list, encountered error: \", e)\r\n\r\n    # try:\r\n    #     c.execute(\"DELETE FROM document_records where embedding_model = ?\", (selected_embedding_model_choice,))\r\n    #     conn.commit()\r\n    #     print(f\"Deleted all records where embedding_model = {selected_embedding_model_choice}\")\r\n    # except Exception as e:\r\n    #     handle_api_error(\"Could not delete document list from document_records db, encountered error: \", e)\r\n\r\n\r\n    \r\n\r\n@app.route('/setup_for_streaming_response', methods=['POST'])\r\ndef setup_for_streaming_response():\r\n\r\n    print(\"\\n\\nSetting up to stream response\\n\\n\")\r\n\r\n    global QUERIES\r\n    do_rag = True   # We will only return an internal server error in the events that do_rag cannot be written, the user_query cannot be read or if a unique stream_session_id cannot be established\r\n\r\n    stream_session_id = \"\"\r\n    # Generate a unique session ID using universally Unique Identifier via the uuid4() method, wherein the randomness of the result is dependent on the randomness of the underlying operating system's random number generator\r\n    # UUI is a standard used for creating unique strings that have a very high likelihood of being unique across all time and space, for ex: f47ac10b-58cc-4372-a567-0e02b2c3d479\r\n    try:\r\n        stream_session_id = str(uuid.uuid4())\r\n    except Exception as e:\r\n        handle_api_error(\"Error creating unique stream_session_id when attempting to setup_for_streaming_response. Error: \", e)\r\n\r\n\r\n    try:\r\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'force_enable_rag', 'force_disable_rag'])\r\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\r\n        use_openai_embeddings = read_return['use_openai_embeddings']\r\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\r\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\r\n        force_enable_rag = read_return['force_enable_rag']\r\n        force_disable_rag = read_return['force_disable_rag']\r\n    except Exception as e:\r\n        handle_api_error(\"Missing values in config.json when attempting to setup_for_streaming_response. Error: \", e)\r\n\r\n\r\n    # We do not modify the force_enable_rag or force_disable_rag flags in this method, we simply respond to them here. UI updates should handle those flags.\r\n    if force_enable_rag:\r\n        \r\n        print(\"\\n\\nFORCE_ENABLE_RAG True, force enabling RAG and returning\\n\\n\")\r\n        \r\n        do_rag = True\r\n        \r\n        try:\r\n            write_config({'do_rag':do_rag})\r\n        except Exception as e:\r\n            handle_api_error(\"Could not force_enable_rag when attempting to setup_for_streaming_response, encountered error: \", e)\r\n        \r\n        return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag})\r\n    \r\n    if force_disable_rag:\r\n\r\n        print(\"\\n\\nFORCE_DISABLE_RAG True, force disabling RAG and returning\\n\\n\")\r\n\r\n        do_rag = False\r\n\r\n        try:\r\n            write_config({'do_rag':do_rag})\r\n        except Exception as e:\r\n            handle_api_error(\"Could not force_disable_rag when attempting to setup_for_streaming_response, encountered error: \", e)\r\n\r\n        return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag})\r\n\r\n    try:\r\n        # Attempt to get the user's query\r\n        user_query = request.json['message']\r\n        # Store the query associated with the ID\r\n        QUERIES[stream_session_id] = user_query\r\n    except KeyError:\r\n        handle_api_error(\"Could not obtain and/or store user_query in setup_for_streaming_response, encountered error: \", e)\r\n\r\n\r\n    # Perform similarity search on the vector DB\r\n    print(\"\\n\\nPerforming similarity search to determine if RAG necessary\\n\\n\")\r\n    embedding_function = None\r\n    try:\r\n        if use_sbert_embeddings:\r\n            embedding_function=HuggingFaceEmbeddings()\r\n        elif use_openai_embeddings:\r\n            embedding_function=AZURE_OPENAI_EMBEDDINGS\r\n        elif use_bge_base_embeddings:\r\n            embedding_function=HF_BGE_EMBEDDINGS\r\n        elif use_bge_large_embeddings:\r\n            embedding_function=HF_BGE_EMBEDDINGS\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not set embedding_function for similarity_search when attempting to setup_for_streaming_response, encountered error: \", e)\r\n    \r\n    try:\r\n        docs = VECTOR_STORE.similarity_search(user_query, embedding_fn=embedding_function)\r\n        # docs_with_relevance_score = VECTOR_STORE.similarity_search_with_relevance_scores(user_query, 10, embedding_fn=embedding_function)\r\n        # docs_list_with_cosine_distance = VECTOR_STORE.similarity_search_with_score(user_query, 10, embedding_fn=embedding_function)\r\n        # print(f'\\n\\nsimple similarity search results: \\n {docs}\\n\\n')\r\n        # print(f'\\n\\nRelevance Score similarity search results (range 0 to 1): \\n {docs_with_relevance_score}\\n\\n')\r\n        # print(f'\\n\\nDocs list most similar to query based on cosine distance: \\n {docs_list_with_cosine_distance}\\n\\n')\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not perform similarity_search to determin do_rag when attempting to setup_for_streaming_response, encountered error: \", e)\r\n\r\n\r\n    print(\"\\n\\nDetermining do_rag \\n\\n\")\r\n    try:\r\n        page_contents, do_rag = filter_relevant_documents(user_query, docs)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Force enabling RAG and returning: could not determine do_rag during setup_for_streaming_response, encountered error: \", e)\r\n    \r\n    print(f'Do RAG? {do_rag}')\r\n\r\n    try:\r\n        write_config({'do_rag':do_rag})\r\n    except Exception as e:\r\n        handle_api_error(\"Could not write do_rag during setup_for_streaming_response, encountered error: \", e)\r\n\r\n\r\n    # Return the stream_session_id\r\n    return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag})\r\n    \r\n    \r\n    # if matched_images_found:\r\n    #     images_iframe_html = \"<br><h6>Refer to the images below:</h6>\"\r\n    #     for image_id, image_bytes_data in matched_images_in_bytes:\r\n    #         #print(f\"\\n\\nmatched image id: {image_id}\")\r\n    #         try:\r\n    #             image_link_url = url_for('image_display', image_id=image_id)\r\n    #             images_iframe_html += f'<br><iframe width=\"750\" height=\"400\" src=\"{image_link_url}\" frameborder=\"0\"></iframe><br>'\r\n    #         except Exception as e:\r\n    #             handle_error_no_return(\"Could not construct images_iframe_html, encountered error: \", e)\r\n\r\n\r\n#############################################################################\r\n##############---NOTES ON THE BELOW CUSTOM CLASS APPROACH---#################\r\n#############################################################################\r\n\r\n# class CustomStream(io.StringIO)\r\n#   defines a new class 'CustomStream' that inherits the StringIO class from the io module.\r\n#   'StringIO' is an in-memory, file-like object that can be used as a string buffer, essentially a file in-memory rather than on disk\r\n\r\n# def __init__(self, callback=None)\r\n#   initialization method for instances of 'CustomStream' accepting one optional argument that defaults to None if not provided\r\n\r\n# super().__init__()\r\n#   calls the 'init' method of the parent class 'StringIO', which here is also the super & base class!\r\n#   necessary to ensure that parent/base/super class 'StringIO' is properly initialized for instances of 'CustomStream'\r\n# \r\n# self.callback = callback\r\n#   The passed 'callback' attribute is stored as an instance attrib, meaning each instance of 'CustomStream' will have its own 'callback' attrib\r\n \r\n# def write(self, data)\r\n#   Overwrites the 'write' method of parent class 'StringIO'; this method is called whenever data is written to our 'CustomStream'\r\n \r\n# PRIMARY MOTIVATION FOR THIS CUSTOM CLASS!! If we have a callback, call it:\r\n# if self.callback:\r\n#   self.callback(data)\r\n#\r\n#   The method checks if a 'callback' function has been set for the instance, i.e. 'self.callback' is not 'None'\r\n#   If there is a 'callback', it calls that function with the provided data, \r\n#   which allows us to \"hook\" into the write process & execute additional logic whenever data is written to the 'CustomStream'   \r\n\r\n# return super().write(data)\r\n#   Finally, this calls the 'write' method of the parent class 'StringIO' using the 'super()' function\r\n#   This ensures that the actual writing of the data to the in-memory buffer, which is the primary function of 'StringIO' still happens!\r\n#   The provided data is passed for this to the base method\r\n \r\n# In summary, this 'CustomStream' class provides a custom implementation of 'StringIO' that supports a callback mechanism:\r\n# Everytime data is written to this custom stream, the 'callback', if provided, is executed thus allowing for additional functionality during the write\r\n\r\n# In this application, this mechanism is used to queue data for the streaming response! \r\n# It extends the 'StringIO' class by adding a new feature: the ability to trigger a 'callback' function whenever a 'write()' occurs!\r\n\r\n# So to use this:\r\n\r\n# 1. We define a queue: \r\n#       data_queue = queue.Queue()\r\n\r\n# 2. We define a callback function that puts data in this queue:\r\n#        def callback(data):\r\n#           data_queue.put(data)\r\n\r\n# 3. We create an instance of our custom stream passing this callback function:\r\n#        custom_stream = CustomStream(callback=callback)\r\n\r\n# 4. We redirect stdout to our custom stream temporarily\r\n#       original_stdout = sys.stdout\r\n#        sys.stdout = custom_stream\r\n\r\n# 5. We start the llm_task() thread & the LLM() function now outputs here, finally resetting stdout and putting None into the queue: data_queue.put(None)\r\n\r\n# 6. While the thread runs, we start a while loop that keeps yielding from the queue and stopping when None is read: yield f\"data: {line}\\n\\n\"\r\n\r\n# 7. A final yield signals the end of the stream, to be handled at the client-side:  yield \"event: END\\ndata: null\\n\\n\"\r\n\r\n#############################################################################\r\n#################---XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX---####################\r\n#############################################################################\r\n\r\n\r\nclass CustomStream(io.StringIO):\r\n    def __init__(self, callback=None):\r\n        super().__init__()\r\n        self.callback = callback\r\n\r\n    def write(self, data):\r\n        # If we have a callback, call it\r\n        if self.callback:\r\n            self.callback(data)\r\n\r\n        return super().write(data)\r\n\r\n\r\n@app.route('/stream/<stream_session_id>')\r\ndef stream(stream_session_id):\r\n\r\n    print(\"stream route triggered\")\r\n\r\n    global QUERIES\r\n    global HISTORY_MEMORY_WITH_BUFFER\r\n\r\n    try:\r\n        read_return = read_config(['do_rag', 'base_template'])\r\n        do_rag = read_return['do_rag']\r\n        base_template = read_return['base_template']\r\n    except Exception as e:\r\n        error_message = f\"\\n\\Missing values in config.json in main method stream!. Error: {e}\\n\\n\"\r\n        if logger:\r\n            logger.error(error_message)\r\n            print(error_message)\r\n        else:\r\n            print(error_message)\r\n        return jsonify(success=False, error=error_message), 500 # internal server error\r\n\r\n    key_for_llm_result = \"LlmResponseforQueryID_\" + stream_session_id\r\n    key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\r\n\r\n    user_query = request.args.get('input')\r\n    #print(f\"do_rag: {do_rag}\")\r\n\r\n    print(f'\\n\\nuser query passed to the LLM: {user_query}\\n\\n')\r\n\r\n    if do_rag:\r\n        ### 0 - If memory has been reset due to an old chat loading up, delete the additional key that added for a non-RAG resuming scenario:\r\n        if 'has_been_reset' in HISTORY_SUMMARY:\r\n            del HISTORY_SUMMARY['has_been_reset']\r\n\r\n        ### 1 - Define Template:\r\n        rag_prompt_template_variables = \"\"\"\r\n\r\n        Use the following context to answer the user's question:\r\n        \r\n        Context:{context}\r\n        Question:{question}\r\n        \"\"\"\r\n        history_summary_for_rag = re.sub(r\"\\{|\\}\", \"\", str(HISTORY_SUMMARY))    #search through the string str(HISTORY_SUMMARY) for all instances of { and } and replace them with an empty string \"\", effectively removing these characters from the string\r\n\r\n        rag_history_prompt = \"The conversation so far: \" + history_summary_for_rag\r\n\r\n        rag_prompt_template = rag_history_prompt + \"\\n\" + base_template + \"\\n\" + rag_prompt_template_variables\r\n\r\n        print(f\"\\n\\nrag_prompt_template: {rag_prompt_template}\\n\\n\")\r\n\r\n        rag_qa_chain_prompt = PromptTemplate.from_template(rag_prompt_template)\r\n\r\n        ### 2 - Setup Chain\r\n        qa_chain = RetrievalQA.from_chain_type(LLM, retriever=VECTOR_STORE.as_retriever(), return_source_documents=True, chain_type_kwargs={\"prompt\":rag_qa_chain_prompt})\r\n\r\n    else:\r\n        ### 0 - If memory has been reset due to an old chat loading up, delete the additional key that added for a non-RAG resuming scenario:\r\n        if 'has_been_reset' in HISTORY_SUMMARY:\r\n            del HISTORY_SUMMARY['has_been_reset']\r\n\r\n        ### 1 - Define Template keping in mind if memory has been reset due to an an old chat loading up:\r\n        non_rag_prompt_template_variables = \"\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI Assistant:\"\r\n\r\n        history_summary_for_non_rag = re.sub(r\"\\{|\\}\", \"\", str(HISTORY_SUMMARY))    #search through the string str(HISTORY_SUMMARY) for all instances of { and } and replace them with an empty string \"\", effectively removing these characters from the string\r\n        \r\n        non_rag_history_prompt = \"The conversation so far: \" + history_summary_for_non_rag\r\n        \r\n        non_rag_prompt_template = non_rag_history_prompt + \"\\n\" + base_template + \"\\n\" + non_rag_prompt_template_variables\r\n\r\n        #print(f\"\\n\\nnon_rag_prompt_template: {non_rag_prompt_template}\\n\\n\")\r\n        non_rag_qa_chain_prompt = PromptTemplate(template=non_rag_prompt_template, input_variables=[\"history\",\"input\"])\r\n        print(f\"\\n\\non_rag_qa_chain_prompt: {non_rag_qa_chain_prompt}\\n\\n\")\r\n\r\n        ### 2 - Setup Chain\r\n        conversation_chain_with_summary_buffer = ConversationChain(\r\n            llm = LLM,\r\n            prompt=non_rag_qa_chain_prompt\r\n        )\r\n\r\n    if not user_query:\r\n        return \"Session not found\", 404\r\n    \r\n    stop_thread = threading.Event()\r\n    # Will be set() in llm_task() methods 'finally' block to stop the thread: Once the inferencing is complete in the 'try' block, the 'finally' block adds a 'None' object to the queue \r\n    # and sets the threading event below. The 'None' causes the yielding while loop to invoke join() on the llm_task() thread for synchronization, as it causes the invoking thread, \r\n    # in this case main thread, to wait until the ll_task thread object completes execution before resuming. Meanwhile setting the stop_thread causes the llm_task threads while() to complete!\r\n\r\n    def generate():\r\n\r\n        data_queue = queue.Queue()\r\n\r\n        def callback(data):\r\n            data_queue.put(data)\r\n\r\n        custom_stream = CustomStream(callback=callback)\r\n\r\n        # Redirect stdout to our custom stream temporarily\r\n        original_stdout = sys.stdout\r\n        sys.stdout = custom_stream\r\n\r\n        def llm_task():\r\n            global QUERIES\r\n            global HISTORY_SUMMARY\r\n            result = \"\"\r\n            while not stop_thread.is_set():\r\n                # Call LLM\r\n                try:\r\n                    \r\n                    if do_rag:\r\n                        result = qa_chain({\"query\": user_query})\r\n\r\n                        # Experimental RAG chains here:\r\n\r\n                    else:\r\n                        result = conversation_chain_with_summary_buffer.predict(input=user_query)\r\n                        \r\n                        # Experimental chains here:\r\n                finally:\r\n                    # Reset stdout to its original value\r\n                    sys.stdout = original_stdout\r\n\r\n                    # experimental outputs here:\r\n                    #print(f\"history_buffer_result: {history_buffer_result}\")\r\n\r\n                    # Save the LLM's formatted response for reference searching \r\n                    formatted_llm_output = \"\"\r\n                    \r\n                    if do_rag:\r\n                        print(\"\\n\\nStoring RAG-Context history:\\n\")\r\n\r\n                        formatted_llm_output = str(result['result'])\r\n                        QUERIES[key_for_llm_result] = formatted_llm_output\r\n                        QUERIES[key_for_vector_results] = result\r\n\r\n                        HISTORY_MEMORY_WITH_BUFFER.save_context({\"input\":user_query}, {\"output\":formatted_llm_output})\r\n                    else:\r\n                        HISTORY_MEMORY_WITH_BUFFER.save_context({\"input\":user_query}, {\"output\":result})\r\n\r\n                    \r\n                    HISTORY_SUMMARY = HISTORY_MEMORY_WITH_BUFFER.load_memory_variables({})\r\n                    print(f\"\\n\\nHISTORY_SUMMARY:{HISTORY_SUMMARY}\\n\\n\")\r\n                    print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.chat_memory.messages: {HISTORY_MEMORY_WITH_BUFFER.chat_memory.messages}\\n\\n\")\r\n\r\n\r\n                    if not do_rag:\r\n                        formatted_user_query = str(user_query).strip('\\n')\r\n                        formatted_llm_output = str(result)\r\n                        formatted_llm_output = formatted_llm_output.strip('\\n')\r\n                        \r\n                        # If RAG is done, get_references() method stores history. If not, we store history right here\r\n                        print(f\"\\n\\nStoring chat history with non-RAG LLM output: {formatted_llm_output}\\n\\n\")\r\n                        \r\n                        # Storing to history DB as get_references() will not be invoked in non-RAG chains!\r\n                        store_chat_history_to_db(formatted_user_query, formatted_llm_output, HISTORY_SUMMARY)\r\n\r\n                    # Stop thread\r\n                    data_queue.put(None)\r\n                    stop_thread.set()\r\n\r\n        # Start the LLM task in a separate thread\r\n        thread = threading.Thread(target=llm_task)\r\n        thread.start()\r\n\r\n        i = 0\r\n        # Continuously yield data as it becomes available\r\n        while True:\r\n            line = data_queue.get()\r\n            if line is None:\r\n                print(\"None read, breaking & stopping thread\")\r\n                thread.join()\r\n                break\r\n            if i == 0:\r\n                line = line.strip('\\n')\r\n                i += 1\r\n            line = line.replace('\\n\\n', '</br></br>')\r\n            line = line.replace('\\n', '</br>')\r\n            #line = re.sub(r'\\s{2,}', lambda match: '&nbsp;' * len(match.group()), line)\r\n            yield f\"data: {line}\\n\\n\"\r\n\r\n        # This part ensures that after LLM finishes, the stream is closed\r\n        yield \"event: END\\ndata: null\\n\\n\"\r\n\r\n        print(\"LLM stream done\")\r\n\r\n    print(\"\\n\\nStarting inferencing!\\n\\n\")\r\n    return Response(generate(), content_type='text/event-stream')\r\n\r\n\r\n@app.route('/lc_get_references', methods=['POST'])\r\ndef lc_get_references():\r\n\r\n    print(\"\\n\\nGetting References\\n\\n\")\r\n\r\n    try:\r\n        read_return = read_config(['do_rag', 'upload_folder'])\r\n        do_rag = read_return['do_rag']\r\n        upload_folder = read_return['upload_folder']\r\n    except Exception as e:\r\n        handle_api_error(\"Missing values in config.json when attempting to get_references. Error: \", e)\r\n\r\n    if not do_rag:\r\n        print(\"\\n\\nSkipping RAG and returning\\n\\n\")\r\n        return jsonify({'success': True, 'chat_id': CHAT_ID, 'sequence_id': SEQUENCE_ID})\r\n\r\n    try:\r\n        stream_session_id = request.json['stream_session_id']\r\n        user_query = request.json['message']\r\n    except Exception as e:\r\n        handle_api_error(\"Could not read request content in method get_references, encountered error: \", e)\r\n        \r\n    try:\r\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\r\n        key_for_llm_result = \"LlmResponseforQueryID_\" + stream_session_id\r\n\r\n        docs = QUERIES[key_for_vector_results]\r\n        llm_response = QUERIES[key_for_llm_result]\r\n    except Exception as e:\r\n        handle_api_error(\"Could not obtain relevant data from QUERIES dict, encountered error: \", e)\r\n\r\n    # Having obtained the relevant info, clear the QUERIES{} dict so as to not bloat it!\r\n    try:\r\n        del QUERIES[key_for_vector_results]\r\n        del QUERIES[key_for_llm_result]\r\n    except Exception as e:\r\n        handle_error_no_return(\"Error clearing queries dict in method get_references: \", e)\r\n\r\n    reference_response = \"\"\r\n\r\n    all_sources = {}\r\n    reference_pages = {}\r\n\r\n    try:\r\n        print(f\"\\n\\ndocs['source_documents']: {docs['source_documents']}\\n\\n\")\r\n        print(f\"\\n\\ndocs['result']: {docs['result']}\\n\\n\")\r\n    except Exception as e:\r\n        handle_api_error(\"Could not parse vector DB search results during get_references() ops, encountered error: \", e)\r\n    \r\n\r\n    relevant_pages = \"<br><br>Relevant Pages & Topics:<br><br>\"\r\n\r\n    for doc in docs['source_documents']:\r\n        try:\r\n            relevant_pages += str(doc.page_content)\r\n            relevant_pages += \"<br>In Source Document:<br>\"\r\n            relevant_pages += str(doc.metadata)\r\n            relevant_pages += \"<br><br>\"\r\n\r\n            relevant_page_text = str(doc.page_content)\r\n\r\n            source_filepath = str(doc.metadata[\"source\"])\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not access doc.page_content and/or doc.metadata, encountered error: \", e)\r\n            continue\r\n    \r\n        relevant_page_text = relevant_page_text.split('\\n', 1)[0]\r\n        relevant_page_text = relevant_page_text.strip()\r\n        relevant_page_text = re.sub(r'[\\W_]+Page \\d+[\\W_]+', '', relevant_page_text)\r\n\r\n        source_filepath = source_filepath.replace('\\\\', '/')\r\n        \r\n        try:\r\n            source_filename = os.path.basename(source_filepath)\r\n            _, file_extension = os.path.splitext(source_filepath)\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not parse path with OS lib, encountered error: \", e)\r\n            continue\r\n\r\n        # The source_filepath will likely always reference a TXT file because of how we're loading the VectorDB!\r\n        # Check if the PDF version of the source doc exists\r\n        if file_extension == '.txt':\r\n\r\n            #print(\"\\n\\ntxt file\\n\\n\")\r\n\r\n            # Construct the path to the potential PDF version\r\n            pdf_version_path = os.path.join(upload_folder, os.path.basename(source_filepath).replace('.txt', '.pdf'))   # not catching an error here as os.path.basename(source_filepath) has already been caught just above!\r\n\r\n            # Check if PDF version of the source TXT exists!\r\n            if os.path.exists(pdf_version_path):\r\n\r\n                source_filename = source_filename.replace('.txt', '.pdf')\r\n                \r\n                if pdf_version_path in reference_pages:\r\n                    reference_pages[pdf_version_path].extend([relevant_page_text])\r\n                else:\r\n                    reference_pages[pdf_version_path] = [relevant_page_text]\r\n\r\n                # Add this file to our sources dictionary if it's not already present\r\n                if source_filename not in all_sources:\r\n                    source_filepath = pdf_version_path\r\n                    all_sources.update({source_filename: source_filepath})\r\n\r\n            # Else PDF does not exist, TXT is the source\r\n            else:\r\n                # Check if the TXT is already in the sources dict\r\n                if source_filename not in all_sources:\r\n                    try:\r\n                        source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\r\n                        all_sources.update({source_filename: source_filepath})\r\n                    except Exception as e:\r\n                        handle_error_no_return(\"Could not construct filepath for TXT file, encountered error: \", e)\r\n\r\n\r\n        # If file is not a TXT file\r\n        else:\r\n            # Check if the TXT is already in the sources dict\r\n            if source_filename not in all_sources:\r\n                try:\r\n                    source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\r\n                    all_sources.update({source_filename: source_filepath})\r\n                except Exception as e:\r\n                    handle_error_no_return(\"Could not construct filepath for non-TXT file, encountered error: \", e)\r\n\r\n    # print(f\"\\n\\nreference_pages: {reference_pages}\\n\\n\")\r\n\r\n    try:\r\n        docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf_and_highlight(reference_pages, stream_session_id)\r\n        # docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf(reference_pages)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\r\n\r\n    try:\r\n        matched_images_found, matched_images_in_bytes = find_images_in_db(reference_pages)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not search for images, encountered error: \", e)\r\n\r\n    refer_pages_string = \"\"\r\n    download_link_html = \"\"\r\n    images_iframe_html = \"\"\r\n\r\n    if docs_have_relevant_info:\r\n        \r\n        # refer_pages_string = \"<br><br>Refer to the following pages in the mentioned docs:<br>\"\r\n        # for doc in user_should_refer_pages_in_doc:\r\n        #     try:\r\n        #         # Remove duplicates from reference_pages dict\r\n        #         refer_pages_string += \"<br>\" + str(doc) + \": \" + str(user_should_refer_pages_in_doc[doc]).replace(\"{\", \"\").replace(\"}\", \"\") + \"<br>\"\r\n        #     except Exception as e:\r\n        #         error_message = f\"\\n\\nCould not construct refer_pages_string, encountered error: {e}\\n\\n\"\r\n        #         if logger:\r\n        #             logger.error(error_message)\r\n        #             print(error_message)\r\n        #         else:\r\n        #             print(error_message)\r\n\r\n\r\n        refer_pages_string = \"<br><br><h6>Refer to the following pages in the mentioned docs:</h6><br>\"\r\n        \r\n        # for doc in user_should_refer_pages_in_doc:\r\n        for index, doc in enumerate(user_should_refer_pages_in_doc, start=1):\r\n            # pdf_iframe_id = str(doc) + \"PdfViewer\"\r\n            pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\r\n            frame_doc_path = f\"/pdf/{doc}\"\r\n            # frame_doc_path = upload_folder + f\"/{doc}\" \r\n            try:\r\n                refer_pages_string += f\"<br><h6>{doc}: \"\r\n                for page in user_should_refer_pages_in_doc[doc]:\r\n                    frame_doc_path += \"#page=\" + str(page) \r\n                    refer_pages_string += f'<a href=\"javascript:void(0)\" onclick=\"goToPage(\\'{pdf_iframe_id}\\', \\'{frame_doc_path}\\')\">Page {page}</a>, '\r\n                    frame_doc_path = f\"/pdf/{doc}\"\r\n                refer_pages_string = refer_pages_string.strip(', ') + \"</h6><br>\"\r\n            except Exception as e:\r\n                handle_error_no_return(\"Could not construct refer_pages_string, encountered error: \", e)\r\n\r\n        # download_link_html = \"<br><h6>Refer to the source documents below:</h6>\"\r\n        pdf_right_pane_id = \"stream\" + stream_session_id + \"PdfPane\"\r\n        download_link_html = f'<div class=\"pdf-viewer\" id={pdf_right_pane_id}>'\r\n\r\n        for index, source in enumerate(user_should_refer_pages_in_doc, start=1):\r\n            try:\r\n                # print(\"\\n\\nlooping sources\\n\\n\")\r\n                download_link_url = url_for('download_file', filename=source)\r\n                pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\r\n                download_link_html += f'<br><h6><a href=\"{download_link_url}\" target=\"_blank\"><iframe id=\"{pdf_iframe_id}\" src=\"{download_link_url}\" width=\"100%\" height=\"600\"></iframe></a></h6><br>'\r\n            except Exception as e:\r\n                handle_error_no_return(\"Could not construct download_link_html, encountered error: \", e)\r\n\r\n        download_link_html += \"</div>\"\r\n        \r\n        # print(f\"\\n\\nall_sources: {all_sources}\\n\\n\")\r\n        # for source in all_sources:\r\n        #     try:\r\n        #         # print(\"\\n\\nlooping sources\\n\\n\")\r\n        #         download_link_url = url_for('download_file', filename=source)\r\n        #         pdf_iframe_id = str(source) + \"PdfViewer\"\r\n        #         download_link_html += f'<br><a href=\"{download_link_url}\" target=\"_blank\"><iframe id=\"{pdf_iframe_id}\" src=\"{download_link_url}\" width=\"600\" height=\"400\"></iframe></a><br>'\r\n        #     except Exception as e:\r\n        #         error_message = f\"\\n\\nCould not construct download_link_html, encountered error: {e}\\n\\n\"\r\n        #         if logger:\r\n        #             logger.error(error_message)\r\n        #             print(error_message)\r\n        #         else:\r\n        #             print(error_message)\r\n    \r\n    if matched_images_found:\r\n        images_iframe_html = \"<br><h6>Refer to the images below:</h6>\"\r\n        for image_id, image_bytes_data in matched_images_in_bytes:\r\n            #print(f\"\\n\\nmatched image id: {image_id}\")\r\n            try:\r\n                image_link_url = url_for('image_display', image_id=image_id)\r\n                images_iframe_html += f'<br><iframe width=\"750\" height=\"400\" src=\"{image_link_url}\" frameborder=\"0\"></iframe><br>'\r\n            except Exception as e:\r\n                handle_error_no_return(\"Could not construct images_iframe_html, encountered error: \", e)\r\n\r\n    \r\n    # reference_response = refer_pages_string + download_link_html + images_iframe_html\r\n    reference_response = refer_pages_string + images_iframe_html\r\n\r\n    try:\r\n        # model_response_for_history_db = str(llm_response) + refer_pages_string\r\n        model_response_for_history_db = str(llm_response)\r\n        model_response_for_history_db += f\"\\n\\n{reference_response}\"\r\n        model_response_for_history_db += f\"\\n\\npdf_pane_data={download_link_html}\"\r\n        model_response_for_history_db = model_response_for_history_db.strip('\\n')\r\n\r\n        formatted_user_query = str(user_query).strip('\\n')\r\n\r\n        user_query_for_history_db = formatted_user_query\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not prep data to store_chat_history_to_db in get_references(), encountered error: \", e)\r\n\r\n    try:\r\n        store_chat_history_to_db(user_query_for_history_db, model_response_for_history_db, HISTORY_SUMMARY)\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not store_chat_history_to_db in get_references(), encountered error: \", e)\r\n\r\n    return jsonify({'success': True, 'response': reference_response, 'pdf_frame':download_link_html, 'chat_id': CHAT_ID, 'sequence_id': SEQUENCE_ID})\r\n\r\n\r\n\r\n//Make a GET request to the server to load the LLM & vectorDB\r\n                // fetch('/load_model_and_vectordb')\r\n                // .then(response => {\r\n                //     if (!response.ok) {\r\n                //         return response.json().then(err => { throw new Error(err.error)});\r\n                //     }\r\n                //     return response\r\n                // })\r\n                // .then(response => response.json())\r\n                // .then(data => {\r\n                //     if (data.success) {\r\n\r\n                //         llm_model = data.llm_model\r\n                //         LLM_MODEL = String(data.llm_model)\r\n                        \r\n                //         // If LLM & VectorDB loaded successfully, init the chat history DB \r\n                //         fetch('/init_chat_history_db')\r\n                //             .then(response => {\r\n                //                 if (!response.ok) {\r\n                //                     return response.json().then(err => { throw new Error(err.error)});\r\n                //                 }\r\n                //                 return response\r\n                //             })\r\n                //             .then(response => response.json())\r\n                //             .then(data => {\r\n                //                 if (data.success) {\r\n                //                     // If LLM, VectorDB and chat history DB initialized successfully, continue\r\n\r\n                //                     curr_chat_id = data.chat_id\r\n\r\n                //                     curr_chat_id = \" Chat \".concat(String(curr_chat_id))\r\n\r\n                //                     display_chatid_and_model = String(curr_chat_id).concat(\": \", String(llm_model))\r\n\r\n                //                     document.getElementById('model_header').innerHTML = display_chatid_and_model;\r\n                //                     document.getElementById('model_header').style.display = 'block';\r\n\r\n                //                     // Load menu items for the chat history menu\r\n                //                     loadChatHistoryMenu();\r\n                //                     document.getElementById('ModelAndDBLoading').style.display = 'none';\r\n                //                     document.getElementById('ReadyToChat').style.display = 'block';\r\n                                    \r\n                //                     var timeoutDelayInMilliseconds = 1500; //1.5 seconds\r\n                //                     setTimeout(function() {\r\n                //                         document.getElementById('ReadyToChat').style.display = 'none';\r\n                //                     }, timeoutDelayInMilliseconds);\r\n                                    \r\n                //                 } else {\r\n                //                     throw new Error('Error when initializing the chat history DB');\r\n                //                 }\r\n                //             })\r\n                //             .catch(error => {\r\n                //                 let full_error_message = \"There was an error in initializing the chat history DB: \" + String(error.message);\r\n                //                 console.error(full_error_message);\r\n                //                 alert(full_error_message);\r\n                //             });\r\n                //     } else {\r\n                //         throw new Error('Data error when loading the model or vectorDB');\r\n                //     }\r\n                // })\r\n                // .catch(error => {\r\n                //     let full_error_message = \"There was an error in loading the model or vectorDB: \" + String(error.message);\r\n                //     console.error(full_error_message);\r\n                //     alert(full_error_message);\r\n                // });\r\n                \r\n\r\n                // TEMPLATE: Make a GET request to the server\r\n            //    fetch('/init_chat_history_db')\r\n            //         .then(response => {\r\n            //             if (!response.ok) {\r\n            //                 throw new Error(`Server-side HTTP error! Status: ${response.status}`);\r\n            //             }\r\n            //             return response\r\n            //         })\r\n            //         .then(response => response.json())\r\n            //         .then(data => {\r\n            //             if (data.success) {\r\n\r\n                            \r\n            //             } else {\r\n            //                 throw new Error('Data error when fetching history-menu list');\r\n            //             }\r\n            //         })\r\n            //         .catch(error => {\r\n            //             console.error(\"There was an error in fetching the history-menu list: \", error.message);\r\n            //         });\r\n\r\n\r\n            function sendMessage() {\r\n    \r\n                document.getElementById('processingQ').style.display = 'block';\r\n    \r\n                let userInput = document.getElementById('user-input').value;\r\n    \r\n                // Append user input to the chat area\r\n                document.getElementById('chat-area').innerHTML += '<div class=\"user-message\">' + userInput + '</div>';\r\n    \r\n                // Make AJAX call to the app.py server to get the models response\r\n                fetch('/get_response', {\r\n                    method: 'POST',\r\n                    headers: {\r\n                        'Content-Type': 'application/json'\r\n                    },\r\n                    body: JSON.stringify({'message': userInput})\r\n                })\r\n                .then(response => {\r\n                    if (!response.ok) {\r\n                        throw new Error(`Server-side error! Status: ${response.status}`);\r\n                    }\r\n                    return response\r\n                })\r\n                .then(response => response.json())\r\n                .then(data => {\r\n                    if (data.success) {\r\n                        CHAT_ID = data.chat_id;\r\n                        SEQUENCE_ID = data.sequence_id;\r\n\r\n                        document.getElementById('processingQ').style.display = 'none';\r\n                        const responseAndRating = `\r\n                        <div class=\"llm-wrapper\">\r\n                            <div class=\"llm-response\">\r\n                                ${data.response}\r\n                            </div>\r\n                            <div class=\"star-rating\" data-rated=\"False\" rating-chat-id=${data.chat_id} rating-sequence-id=${data.sequence_id}>\r\n                                <i class=\"far fa-star\" data-rate=\"1\"></i>\r\n                                <i class=\"far fa-star\" data-rate=\"2\"></i>\r\n                                <i class=\"far fa-star\" data-rate=\"3\"></i>\r\n                                <i class=\"far fa-star\" data-rate=\"4\"></i>\r\n                                <i class=\"far fa-star\" data-rate=\"5\"></i>\r\n                            </div>\r\n                        </div>\r\n                        `;\r\n                        document.getElementById('chat-area').innerHTML += responseAndRating;\r\n                        //document.getElementById('chat-area').innerHTML += '<div class=\"llm-response\">' + data.response + '</div>';\r\n                    } else {\r\n                        throw new Error('Internal Server Error: Check server-log and server command-line for more details.');\r\n                    }\r\n                })\r\n                .catch(error => {\r\n                    errorHandler(\"fetching response\", \"/get_response\", String(error.message))\r\n                });\r\n    \r\n                // Clear the input field\r\n                document.getElementById('user-input').value = '';\r\n            }"}
{"type": "source_file", "path": "dockerized_nvidia_cuda_gpu/web_app/unoconv.py", "content": "#!/usr/bin/env python\n\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation: version 2 only.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY, without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n# Copyright 2007-2010 Dag Wieers <dag@wieers.com>\n\nfrom __future__ import print_function\n\nfrom distutils.version import LooseVersion\nimport getopt\nimport glob\nimport os\nimport signal\nimport subprocess\nimport sys\nimport time\n\n__version__ = '0.8.2'\n\ndoctypes = ('document', 'graphics', 'presentation', 'spreadsheet')\n\nglobal convertor, office, ooproc, product\nooproc = None\nuno = unohelper = None\nexitcode = 0\n\n\nclass Office:\n    def __init__(self, basepath, urepath, unopath, pyuno, binary, python, pythonhome):\n        self.basepath = basepath\n        self.urepath = urepath\n        self.unopath = unopath\n        self.pyuno = pyuno\n        self.binary = binary\n        self.python = python\n        self.pythonhome = pythonhome\n\n    def __str__(self):\n        return self.basepath\n\n    def __repr__(self):\n        return self.basepath\n\n\n# Implement a path normalizer to make unoconv work on MacOS X, on\n# which 'program' is a symlink to 'MacOSX,' which seems to break unoconv.\ndef realpath(*args):\n    \"\"\"Implement a combination of os.path.join(), os.path.abspath() and\n        os.path.realpath() in order to normalize path constructions.\"\"\"\n    ret = ''\n    for arg in args:\n        ret = os.path.join(ret, arg)\n    return os.path.realpath(os.path.abspath(ret))\n\n\n# The first thing we should do is find a suitable Office installation\n# with a compatible pyuno library that we can import.\n#\n# See: http://user.services.openoffice.org/en/forum/viewtopic.php?f=45&t=36370&p=166783\ndef find_offices():\n    ret = []\n    extrapaths = []\n\n    # Try using UNO_PATH first (in many incarnations, we'll see what sticks).\n    if 'UNO_PATH' in os.environ:\n        extrapaths += [os.environ['UNO_PATH'],\n                       os.path.dirname(os.environ['UNO_PATH']),\n                       os.path.dirname(os.path.dirname(os.environ['UNO_PATH']))]\n    else:\n        if os.name in ('nt', 'os2'):\n            if 'PROGRAMFILES' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMFILES(X86)' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMW6432' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMW6432']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMW6432']+'\\\\OpenOffice.org*')\n\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            extrapaths += ['/Applications/LibreOffice.app/Contents',\n                           '/Applications/NeoOffice.app/Contents',\n                           '/Applications/OpenOffice.app/Contents',\n                           '/Applications/OpenOffice.org.app/Contents']\n\n        else:\n            extrapaths += glob.glob('/usr/lib*/libreoffice*') + \\\n                          glob.glob('/usr/lib*/openoffice*') + \\\n                          glob.glob('/usr/lib*/ooo*') + \\\n                          glob.glob('/opt/libreoffice*') + \\\n                          glob.glob('/opt/openoffice*') + \\\n                          glob.glob('/opt/ooo*') + \\\n                          glob.glob('/usr/local/libreoffice*') + \\\n                          glob.glob('/usr/local/openoffice*') + \\\n                          glob.glob('/usr/local/ooo*') + \\\n                          glob.glob('/usr/local/lib/libreoffice*')\n\n    # Find a working set for python UNO bindings.\n    for basepath in extrapaths:\n        if os.name in ('nt', 'os2'):\n            officelibraries = ('pyuno.pyd',)\n            officebinaries = ('soffice.exe',)\n            pythonbinaries = ('python.exe',)\n            pythonhomes = ()\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            officelibraries = ('pyuno.so', 'libpyuno.dylib')\n            officebinaries = ('soffice.bin', 'soffice')\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('OOoPython.framework/Versions/*/lib/python*')\n        else:\n            officelibraries = ('pyuno.so',)\n            officebinaries = ('soffice.bin',)\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('python-core-*',)\n\n        # Older LibreOffice/OpenOffice and Windows use basis-link/ or basis/\n        libpath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for lib in officelibraries:\n                for libdir in ('program', 'Frameworks'):\n                    if os.path.isfile(realpath(basepath, basis, libdir, lib)):\n                        libpath = realpath(basepath, basis, libdir)\n                        officelibrary = realpath(libpath, lib)\n                        info(3, \"Found %s in %s\" % (lib, libpath))\n                        # Break the inner loop...\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken, break the outer.\n            break\n        else:\n            continue\n\n        # MacOSX has soffice binaries installed in MacOS subdirectory, not program.\n        unopath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for bin in officebinaries:\n                for bindir in ('program', '', 'MacOS'):\n                    if os.path.isfile(realpath(basepath, basis, bindir, bin)):\n                        unopath = realpath(basepath, basis, bindir)\n                        officebinary = realpath(unopath, bin)\n                        info(3, \"Found %s in %s\" % (bin, unopath))\n                        # Break the inner loop.\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n        else:\n            continue\n\n        # Windows does not provide or need a URE/lib directory?\n        urepath = ''\n        for basis in ('basis-link', 'basis', ''):\n            for ure in ('ure-link', 'ure', 'URE', ''):\n                if os.path.isfile(realpath(basepath, basis, ure, 'lib', 'unorc')):\n                    urepath = realpath(basepath, basis, ure)\n                    info(3, \"Found %s in %s\" % ('unorc', realpath(urepath, 'lib')))\n                    # Break the inner loop.\n                    break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n\n        pythonhome = None\n        for home in pythonhomes:\n            if glob.glob(realpath(libpath, home)):\n                pythonhome = glob.glob(realpath(libpath, home))[0]\n                info(3, \"Found %s in %s\" % (home, pythonhome))\n                break\n\n        # if not os.path.isfile(realpath(basepath, program, officebinary)):\n            # continue\n        # info(3, \"Found %s in %s\" % (officebinary, realpath(basepath, program)))\n\n        # if not glob.glob(realpath(basepath, basis, program, 'python-core-*')):\n            # continue\n\n        # Find suitable Python executable: regular unopath or MacOS version.\n        # LibreOffice 5.4.6.2 on MacOS X 10.13.3 ships the Python executable\n        # in the \"Resources\" folder.\n        pythonpath_candidates = [unopath, realpath(basepath, 'Resources')]\n\n        python_effective = find_executable(pythonpath_candidates, pythonbinaries)\n\n        if python_effective:\n            info(3, \"Found Python at %s\" % python_effective)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            python_effective, pythonhome)\n        else:\n            info(3, \"Considering %s\" % basepath)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            sys.executable, None)\n\n        ret.append(office)\n\n    return ret\n\n\ndef find_executable(folders, filenames):\n    for folder in folders:\n        for filename in filenames:\n            candidate = realpath(folder, filename)\n            if os.path.isfile(candidate):\n                return candidate\n\n\ndef office_environ(office):\n    # Set PATH so crash_report is found.\n    path_prefix = realpath(office.basepath, 'program') + os.pathsep + realpath(office.basepath, 'Resources')\n    if 'PATH' in os.environ:\n        os.environ['PATH'] = path_prefix + os.pathsep + os.environ['PATH']\n    else:\n        os.environ['PATH'] = path_prefix\n\n    # Set UNO_PATH so \"officehelper.bootstrap()\" can find soffice executable:\n    os.environ['UNO_PATH'] = office.unopath\n\n    # Set URE_BOOTSTRAP so \"uno.getComponentContext()\" bootstraps a complete UNO environment.\n    if os.name in ('nt', 'os2'):\n        os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamental.ini')\n    else:\n        if os.path.isfile(realpath(office.basepath, 'program', 'fundamentalrc')):\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamentalrc')\n        else:\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'Resources', 'fundamentalrc')\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n    if office.pythonhome:\n        for libpath in (realpath(office.pythonhome, 'lib'),\n                        realpath(office.pythonhome, 'lib', 'lib-dynload'),\n                        realpath(office.pythonhome, 'lib', 'lib-tk'),\n                        realpath(office.pythonhome, 'lib', 'site-packages'),\n                        office.unopath):\n            sys.path.insert(0, libpath)\n    else:\n        # Still needed for system python using LibreOffice UNO bindings\n        # Although we prefer to use a system UNO binding in this case\n        sys.path.append(office.unopath)\n\n\ndef debug_office():\n    if 'URE_BOOTSTRAP' in os.environ:\n        print('URE_BOOTSTRAP=%s' % os.environ['URE_BOOTSTRAP'], file=sys.stderr)\n    if 'UNO_PATH' in os.environ:\n        print('UNO_PATH=%s' % os.environ['UNO_PATH'], file=sys.stderr)\n    if 'UNO_TYPES' in os.environ:\n        print('UNO_TYPES=%s' % os.environ['UNO_TYPES'], file=sys.stderr)\n    print('PATH=%s' % os.environ['PATH'])\n    if 'PYTHONHOME' in os.environ:\n        print('PYTHONHOME=%s' % os.environ['PYTHONHOME'], file=sys.stderr)\n    if 'PYTHONPATH' in os.environ:\n        print('PYTHONPATH=%s' % os.environ['PYTHONPATH'], file=sys.stderr)\n    if 'LD_LIBRARY_PATH' in os.environ:\n        print('LD_LIBRARY_PATH=%s' % os.environ['LD_LIBRARY_PATH'], file=sys.stderr)\n\n\ndef python_switch(office):\n    if office.pythonhome:\n        os.environ['PYTHONHOME'] = office.pythonhome\n        os.environ['PYTHONPATH'] = realpath(office.pythonhome, 'lib') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-dynload') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-tk') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'site-packages') + os.pathsep + \\\n                                   office.unopath\n\n    os.environ['UNO_PATH'] = office.unopath\n\n    info(3, \"-> Switching from %s to %s\" % (sys.executable, office.python))\n    if os.name in ('nt', 'os2'):\n        # os.execv is broken on Windows and can't properly parse command line\n        # arguments and executable name if they contain whitespaces. subprocess\n        # fixes that behavior.\n        ret = subprocess.call([office.python, ] + sys.argv[0:])\n        sys.exit(ret)\n    else:\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n        try:\n            os.execvpe(office.python, [office.python, ] + sys.argv[0:], os.environ)\n        except OSError:\n            # Mac OS X versions prior to 10.6 do not support execv in\n            # a process that contains multiple threads.  Instead of\n            # re-executing in the current process, start a new one\n            # and cause the current process to exit.  This isn't\n            # ideal since the new process is detached from the parent\n            # terminal and thus cannot easily be killed with ctrl-C,\n            # but it's better than not being able to autoreload at\n            # all.\n            # Unfortunately the errno returned in this case does not\n            # appear to be consistent, so we can't easily check for\n            # this error specifically.\n            ret = os.spawnvpe(os.P_WAIT, office.python, [office.python, ] + sys.argv[0:], os.environ)\n            if ret != 0:\n                error(\"Switching Python to %s failed.\" % (office.python))\n            sys.exit(ret)\n\n\nclass Fmt:\n    def __init__(self, doctype, name, extension, summary, filter):\n        self.doctype = doctype\n        self.name = name\n        self.extension = extension\n        self.summary = summary\n        self.filter = filter\n\n    def __str__(self):\n        return \"%s [.%s]\" % (self.summary, self.extension)\n\n    def __repr__(self):\n        return \"%s/%s\" % (self.name, self.doctype)\n\n\nclass FmtList:\n    def __init__(self):\n        self.list = []\n\n    def add(self, doctype, name, extension, summary, filter):\n        self.list.append(Fmt(doctype, name, extension, summary, filter))\n\n    def byname(self, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name:\n                ret.append(fmt)\n        return ret\n\n    def byextension(self, extension):\n        ret = []\n        for fmt in self.list:\n            if os.extsep + fmt.extension == extension:\n                ret.append(fmt)\n        return ret\n\n    def bydoctype(self, doctype, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name and fmt.doctype == doctype:\n                ret.append(fmt)\n        return ret\n\n    def display(self, doctype):\n        print(\"The following list of %s formats are currently available:\\n\" % doctype, file=sys.stderr)\n        for fmt in self.list:\n            if fmt.doctype == doctype:\n                print(\"  %-8s - %s\" % (fmt.name, fmt), file=sys.stderr)\n        print(file=sys.stderr)\n\n\nfmts = FmtList()\n\n# TextDocument\nfmts.add('document', 'bib', 'bib', 'BibTeX', 'BibTeX_Writer')  # 22\nfmts.add('document', 'doc', 'doc', 'Microsoft Word 97/2000/XP', 'MS Word 97')  # 29\nfmts.add('document', 'doc6', 'doc', 'Microsoft Word 6.0', 'MS WinWord 6.0')  # 24\nfmts.add('document', 'doc95', 'doc', 'Microsoft Word 95', 'MS Word 95')  # 28\nfmts.add('document', 'docbook', 'xml', 'DocBook', 'DocBook File')  # 39\nfmts.add('document', 'docx', 'docx', 'Microsoft Office Open XML', 'Office Open XML Text')\nfmts.add('document', 'docx7', 'docx', 'Microsoft Office Open XML', 'MS Word 2007 XML')\nfmts.add('document', 'fodt', 'fodt', 'OpenDocument Text (Flat XML)', 'OpenDocument Text Flat XML')\nfmts.add('document', 'html', 'html', 'HTML Document (OpenOffice.org Writer)', 'HTML (StarWriter)')  # 3\nfmts.add('document', 'latex', 'ltx', 'LaTeX 2e', 'LaTeX_Writer')  # 31\nfmts.add('document', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki')\nfmts.add('document', 'odt', 'odt', 'ODF Text Document', 'writer8')  # 10\nfmts.add('document', 'ooxml', 'xml', 'Microsoft Office Open XML', 'MS Word 2003 XML')  # 11\nfmts.add('document', 'ott', 'ott', 'Open Document Text', 'writer8_template')  # 21\nfmts.add('document', 'pdb', 'pdb', 'AportisDoc (Palm)', 'AportisDoc Palm DB')\nfmts.add('document', 'pdf', 'pdf', 'Portable Document Format', 'writer_pdf_Export')  # 18\nfmts.add('document', 'psw', 'psw', 'Pocket Word', 'PocketWord File')\nfmts.add('document', 'rtf', 'rtf', 'Rich Text Format', 'Rich Text Format')  # 16\nfmts.add('document', 'sdw', 'sdw', 'StarWriter 5.0', 'StarWriter 5.0')  # 23\nfmts.add('document', 'sdw4', 'sdw', 'StarWriter 4.0', 'StarWriter 4.0')  # 2\nfmts.add('document', 'sdw3', 'sdw', 'StarWriter 3.0', 'StarWriter 3.0')  # 20\nfmts.add('document', 'stw', 'stw', 'Open Office.org 1.0 Text Document Template', 'writer_StarOffice_XML_Writer_Template')  # 9\nfmts.add('document', 'sxw', 'sxw', 'Open Office.org 1.0 Text Document', 'StarOffice XML (Writer)')  # 1\nfmts.add('document', 'text', 'txt', 'Text Encoded', 'Text (encoded)')  # 26\nfmts.add('document', 'txt', 'txt', 'Text', 'Text')  # 34\nfmts.add('document', 'uot', 'uot', 'Unified Office Format text', 'UOF text')  # 27\nfmts.add('document', 'vor', 'vor', 'StarWriter 5.0 Template', 'StarWriter 5.0 Vorlage/Template')  # 6\nfmts.add('document', 'vor4', 'vor', 'StarWriter 4.0 Template', 'StarWriter 4.0 Vorlage/Template')  # 5\nfmts.add('document', 'vor3', 'vor', 'StarWriter 3.0 Template', 'StarWriter 3.0 Vorlage/Template')  # 4\nfmts.add('document', 'wps', 'wps', 'Microsoft Works', 'MS_Works')\nfmts.add('document', 'xhtml', 'html', 'XHTML Document', 'XHTML Writer File')  # 33\nfmts.add('document', 'epub', 'epub', 'Electronic Publication', 'EPUB')\nfmts.add('document', 'png', 'png', 'Portable Network Graphic', 'writer_png_Export') ### 2\n\n# WebDocument\nfmts.add('web', 'etext', 'txt', 'Text Encoded (OpenOffice.org Writer/Web)', 'Text (encoded) (StarWriter/Web)')  # 14\nfmts.add('web', 'html10', 'html', 'OpenOffice.org 1.0 HTML Template', 'writer_web_StarOffice_XML_Writer_Web_Template')  # 11\nfmts.add('web', 'html', 'html', 'HTML Document', 'HTML')  # 2\nfmts.add('web', 'html', 'html', 'HTML Document Template', 'writerweb8_writer_template')  # 13\nfmts.add('web', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki_Web')  # 9\nfmts.add('web', 'pdf', 'pdf', 'PDF - Portable Document Format', 'writer_web_pdf_Export')  # 10\nfmts.add('web', 'sdw3', 'sdw', 'StarWriter 3.0 (OpenOffice.org Writer/Web)', 'StarWriter 3.0 (StarWriter/Web)')  # 3\nfmts.add('web', 'sdw4', 'sdw', 'StarWriter 4.0 (OpenOffice.org Writer/Web)', 'StarWriter 4.0 (StarWriter/Web)')  # 4\nfmts.add('web', 'sdw', 'sdw', 'StarWriter 5.0 (OpenOffice.org Writer/Web)', 'StarWriter 5.0 (StarWriter/Web)')  # 5\nfmts.add('web', 'txt', 'txt', 'OpenOffice.org Text (OpenOffice.org Writer/Web)', 'writerweb8_writer')  # 12\nfmts.add('web', 'text10', 'txt', 'OpenOffice.org 1.0 Text Document (OpenOffice.org Writer/Web)', 'writer_web_StarOffice_XML_Writer')  # 15\nfmts.add('web', 'text', 'txt', 'Text (OpenOffice.org Writer/Web)', 'Text (StarWriter/Web)')  # 8\nfmts.add('web', 'vor4', 'vor', 'StarWriter/Web 4.0 Template', 'StarWriter/Web 4.0 Vorlage/Template')  # 6\nfmts.add('web', 'vor', 'vor', 'StarWriter/Web 5.0 Template', 'StarWriter/Web 5.0 Vorlage/Template')  # 7\n\n# Spreadsheet\nfmts.add('spreadsheet', 'csv', 'csv', 'Text CSV', 'Text - txt - csv (StarCalc)')  # 16\nfmts.add('spreadsheet', 'dbf', 'dbf', 'dBASE', 'dBase')  # 22\nfmts.add('spreadsheet', 'dif', 'dif', 'Data Interchange Format', 'DIF')  # 5\nfmts.add('spreadsheet', 'fods', 'fods', 'OpenDocument Spreadsheet (Flat XML)', 'OpenDocument Spreadsheet Flat XML')\nfmts.add('spreadsheet', 'html', 'html', 'HTML Document (OpenOffice.org Calc)', 'HTML (StarCalc)')  # 7\nfmts.add('spreadsheet', 'ods', 'ods', 'ODF Spreadsheet', 'calc8')  # 15\nfmts.add('spreadsheet', 'ooxml', 'xml', 'Microsoft Excel 2003 XML', 'MS Excel 2003 XML')  # 23\nfmts.add('spreadsheet', 'ots', 'ots', 'ODF Spreadsheet Template', 'calc8_template')  # 14\nfmts.add('spreadsheet', 'pdf', 'pdf', 'Portable Document Format', 'calc_pdf_Export')  # 34\nfmts.add('spreadsheet', 'pxl', 'pxl', 'Pocket Excel', 'Pocket Excel')\nfmts.add('spreadsheet', 'sdc', 'sdc', 'StarCalc 5.0', 'StarCalc 5.0')  # 31\nfmts.add('spreadsheet', 'sdc4', 'sdc', 'StarCalc 4.0', 'StarCalc 4.0')  # 11\nfmts.add('spreadsheet', 'sdc3', 'sdc', 'StarCalc 3.0', 'StarCalc 3.0')  # 29\nfmts.add('spreadsheet', 'slk', 'slk', 'SYLK', 'SYLK')  # 35\nfmts.add('spreadsheet', 'stc', 'stc', 'OpenOffice.org 1.0 Spreadsheet Template', 'calc_StarOffice_XML_Calc_Template')  # 2\nfmts.add('spreadsheet', 'sxc', 'sxc', 'OpenOffice.org 1.0 Spreadsheet', 'StarOffice XML (Calc)')  # 3\nfmts.add('spreadsheet', 'uos', 'uos', 'Unified Office Format spreadsheet', 'UOF spreadsheet')  # 9\nfmts.add('spreadsheet', 'vor3', 'vor', 'StarCalc 3.0 Template', 'StarCalc 3.0 Vorlage/Template')  # 18\nfmts.add('spreadsheet', 'vor4', 'vor', 'StarCalc 4.0 Template', 'StarCalc 4.0 Vorlage/Template')  # 19\nfmts.add('spreadsheet', 'vor', 'vor', 'StarCalc 5.0 Template', 'StarCalc 5.0 Vorlage/Template')  # 20\nfmts.add('spreadsheet', 'xhtml', 'xhtml', 'XHTML', 'XHTML Calc File')  # 26\nfmts.add('spreadsheet', 'xls', 'xls', 'Microsoft Excel 97/2000/XP', 'MS Excel 97')  # 12\nfmts.add('spreadsheet', 'xls5', 'xls', 'Microsoft Excel 5.0', 'MS Excel 5.0/95')  # 8\nfmts.add('spreadsheet', 'xls95', 'xls', 'Microsoft Excel 95', 'MS Excel 95')  # 10\nfmts.add('spreadsheet', 'xlt', 'xlt', 'Microsoft Excel 97/2000/XP Template', 'MS Excel 97 Vorlage/Template')  # 6\nfmts.add('spreadsheet', 'xlt5', 'xlt', 'Microsoft Excel 5.0 Template', 'MS Excel 5.0/95 Vorlage/Template')  # 28\nfmts.add('spreadsheet', 'xlt95', 'xlt', 'Microsoft Excel 95 Template', 'MS Excel 95 Vorlage/Template')  # 21\nfmts.add('spreadsheet', 'xlsx', 'xlsx', 'Microsoft Excel 2007/2010 XML', 'Calc MS Excel 2007 XML')\n\n# Graphics\nfmts.add('graphics', 'bmp', 'bmp', 'Windows Bitmap', 'draw_bmp_Export')  # 21\nfmts.add('graphics', 'emf', 'emf', 'Enhanced Metafile', 'draw_emf_Export')  # 15\nfmts.add('graphics', 'eps', 'eps', 'Encapsulated PostScript', 'draw_eps_Export')  # 48\nfmts.add('graphics', 'fodg', 'fodg', 'OpenDocument Drawing (Flat XML)', 'OpenDocument Drawing Flat XML')\nfmts.add('graphics', 'gif', 'gif', 'Graphics Interchange Format', 'draw_gif_Export')  # 30\nfmts.add('graphics', 'html', 'html', 'HTML Document (OpenOffice.org Draw)', 'draw_html_Export')  # 37\nfmts.add('graphics', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'jpeg', 'jpeg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'met', 'met', 'OS/2 Metafile', 'draw_met_Export')  # 43\nfmts.add('graphics', 'odd', 'odd', 'OpenDocument Drawing', 'draw8')  # 6\nfmts.add('graphics', 'otg', 'otg', 'OpenDocument Drawing Template', 'draw8_template')  # 20\nfmts.add('graphics', 'pbm', 'pbm', 'Portable Bitmap', 'draw_pbm_Export')  # 14\nfmts.add('graphics', 'pct', 'pct', 'Mac Pict', 'draw_pct_Export')  # 41\nfmts.add('graphics', 'pdf', 'pdf', 'Portable Document Format', 'draw_pdf_Export')  # 28\nfmts.add('graphics', 'pgm', 'pgm', 'Portable Graymap', 'draw_pgm_Export')  # 11\nfmts.add('graphics', 'png', 'png', 'Portable Network Graphic', 'draw_png_Export')  # 2\nfmts.add('graphics', 'ppm', 'ppm', 'Portable Pixelmap', 'draw_ppm_Export')  # 5\nfmts.add('graphics', 'ras', 'ras', 'Sun Raster Image', 'draw_ras_Export')  # 31\nfmts.add('graphics', 'std', 'std', 'OpenOffice.org 1.0 Drawing Template', 'draw_StarOffice_XML_Draw_Template')  # 53\nfmts.add('graphics', 'svg', 'svg', 'Scalable Vector Graphics', 'draw_svg_Export')  # 50\nfmts.add('graphics', 'svm', 'svm', 'StarView Metafile', 'draw_svm_Export')  # 55\nfmts.add('graphics', 'swf', 'swf', 'Macromedia Flash (SWF)', 'draw_flash_Export')  # 23\nfmts.add('graphics', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing', 'StarOffice XML (Draw)')  # 26\nfmts.add('graphics', 'sxd3', 'sxd', 'StarDraw 3.0', 'StarDraw 3.0')  # 40\nfmts.add('graphics', 'sxd5', 'sxd', 'StarDraw 5.0', 'StarDraw 5.0')  # 44\nfmts.add('graphics', 'sxw', 'sxw', 'StarOffice XML (Draw)', 'StarOffice XML (Draw)')\nfmts.add('graphics', 'tiff', 'tiff', 'Tagged Image File Format', 'draw_tif_Export')  # 13\nfmts.add('graphics', 'vor', 'vor', 'StarDraw 5.0 Template', 'StarDraw 5.0 Vorlage')  # 36\nfmts.add('graphics', 'vor3', 'vor', 'StarDraw 3.0 Template', 'StarDraw 3.0 Vorlage')  # 35\nfmts.add('graphics', 'wmf', 'wmf', 'Windows Metafile', 'draw_wmf_Export')  # 8\nfmts.add('graphics', 'xhtml', 'xhtml', 'XHTML', 'XHTML Draw File')  # 45\nfmts.add('graphics', 'xpm', 'xpm', 'X PixMap', 'draw_xpm_Export')  # 19\n\n# Presentation\nfmts.add('presentation', 'bmp', 'bmp', 'Windows Bitmap', 'impress_bmp_Export')  # 15\nfmts.add('presentation', 'emf', 'emf', 'Enhanced Metafile', 'impress_emf_Export')  # 16\nfmts.add('presentation', 'eps', 'eps', 'Encapsulated PostScript', 'impress_eps_Export')  # 17\nfmts.add('presentation', 'fodp', 'fodp', 'OpenDocument Presentation (Flat XML)', 'OpenDocument Presentation Flat XML')\nfmts.add('presentation', 'gif', 'gif', 'Graphics Interchange Format', 'impress_gif_Export')  # 18\nfmts.add('presentation', 'html', 'html', 'HTML Document (OpenOffice.org Impress)', 'impress_html_Export')  # 43\nfmts.add('presentation', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'impress_jpg_Export')  # 19\nfmts.add('presentation', 'met', 'met', 'OS/2 Metafile', 'impress_met_Export')  # 20\nfmts.add('presentation', 'odg', 'odg', 'ODF Drawing (Impress)', 'impress8_draw')  # 29\nfmts.add('presentation', 'odp', 'odp', 'ODF Presentation', 'impress8')  # 9\nfmts.add('presentation', 'otp', 'otp', 'ODF Presentation Template', 'impress8_template')  # 38\nfmts.add('presentation', 'pbm', 'pbm', 'Portable Bitmap', 'impress_pbm_Export')  # 21\nfmts.add('presentation', 'pct', 'pct', 'Mac Pict', 'impress_pct_Export')  # 22\nfmts.add('presentation', 'pdf', 'pdf', 'Portable Document Format', 'impress_pdf_Export')  # 23\nfmts.add('presentation', 'pgm', 'pgm', 'Portable Graymap', 'impress_pgm_Export')  # 24\nfmts.add('presentation', 'png', 'png', 'Portable Network Graphic', 'impress_png_Export')  # 25\nfmts.add('presentation', 'potm', 'potm', 'Microsoft PowerPoint 2007/2010 XML Template', 'Impress MS PowerPoint 2007 XML Template')\nfmts.add('presentation', 'pot', 'pot', 'Microsoft PowerPoint 97/2000/XP Template', 'MS PowerPoint 97 Vorlage')  # 3\nfmts.add('presentation', 'ppm', 'ppm', 'Portable Pixelmap', 'impress_ppm_Export')  # 26\nfmts.add('presentation', 'pptx', 'pptx', 'Microsoft PowerPoint 2007/2010 XML', 'Impress MS PowerPoint 2007 XML')  # 36\nfmts.add('presentation', 'pps', 'pps', 'Microsoft PowerPoint 97/2000/XP (Autoplay)', 'MS PowerPoint 97 Autoplay')  # 36\nfmts.add('presentation', 'ppt', 'ppt', 'Microsoft PowerPoint 97/2000/XP', 'MS PowerPoint 97')  # 36\nfmts.add('presentation', 'pwp', 'pwp', 'PlaceWare', 'placeware_Export')  # 30\nfmts.add('presentation', 'ras', 'ras', 'Sun Raster Image', 'impress_ras_Export')  # 27\nfmts.add('presentation', 'sda', 'sda', 'StarDraw 5.0 (OpenOffice.org Impress)', 'StarDraw 5.0 (StarImpress)')  # 8\nfmts.add('presentation', 'sdd', 'sdd', 'StarImpress 5.0', 'StarImpress 5.0')  # 6\nfmts.add('presentation', 'sdd3', 'sdd', 'StarDraw 3.0 (OpenOffice.org Impress)', 'StarDraw 3.0 (StarImpress)')  # 42\nfmts.add('presentation', 'sdd4', 'sdd', 'StarImpress 4.0', 'StarImpress 4.0')  # 37\nfmts.add('presentation', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing (OpenOffice.org Impress)', 'impress_StarOffice_XML_Draw')  # 31\nfmts.add('presentation', 'sti', 'sti', 'OpenOffice.org 1.0 Presentation Template', 'impress_StarOffice_XML_Impress_Template')  # 5\nfmts.add('presentation', 'svg', 'svg', 'Scalable Vector Graphics', 'impress_svg_Export')  # 14\nfmts.add('presentation', 'svm', 'svm', 'StarView Metafile', 'impress_svm_Export')  # 13\nfmts.add('presentation', 'swf', 'swf', 'Macromedia Flash (SWF)', 'impress_flash_Export')  # 34\nfmts.add('presentation', 'sxi', 'sxi', 'OpenOffice.org 1.0 Presentation', 'StarOffice XML (Impress)')  # 41\nfmts.add('presentation', 'tiff', 'tiff', 'Tagged Image File Format', 'impress_tif_Export')  # 12\nfmts.add('presentation', 'uop', 'uop', 'Unified Office Format presentation', 'UOF presentation')  # 4\nfmts.add('presentation', 'vor', 'vor', 'StarImpress 5.0 Template', 'StarImpress 5.0 Vorlage')  # 40\nfmts.add('presentation', 'vor3', 'vor', 'StarDraw 3.0 Template (OpenOffice.org Impress)', 'StarDraw 3.0 Vorlage (StarImpress)')  # 1\nfmts.add('presentation', 'vor4', 'vor', 'StarImpress 4.0 Template', 'StarImpress 4.0 Vorlage')  # 39\nfmts.add('presentation', 'vor5', 'vor', 'StarDraw 5.0 Template (OpenOffice.org Impress)', 'StarDraw 5.0 Vorlage (StarImpress)')  # 2\nfmts.add('presentation', 'wmf', 'wmf', 'Windows Metafile', 'impress_wmf_Export')  # 11\nfmts.add('presentation', 'xhtml', 'xml', 'XHTML', 'XHTML Impress File')  # 33\nfmts.add('presentation', 'xpm', 'xpm', 'X PixMap', 'impress_xpm_Export')  # 10\n\n\nclass Options:\n    def __init__(self, args):\n        self.connection = None\n        self.debug = False\n        self.doctype = None\n        self.exportfilter = []\n        self.exportfilteroptions = \"\"\n        self.fields = {}\n        self.filenames = []\n        self.format = None\n        self.importfilter = []\n        self.importfiltername = None\n        self.importfilteroptions = \"\"\n        self.listener = False\n        self.metadata = {}\n        self.nolaunch = False\n        self.output = None\n        self.paperformat = None\n        self.paperorientation = None\n        self.papersize = None\n        self.password = None\n        self.pipe = None\n        self.port = '2002'\n        self.preserve = False\n        self.server = '127.0.0.1'\n        self.setprinter = False\n        self.showlist = False\n        self.stdin = False\n        self.stdout = False\n        self.template = None\n        self.timeout = 60\n        self.verbose = 0\n        self.userProfile = None\n        self.updateDocMode = NO_UPDATE\n        self.updatehtmllinks = True\n\n        # Get options from the commandline\n        try:\n            opts, args = getopt.getopt(args, 'c:Dd:e:F:f:hi:I:LlM:no:p:s:T:t:P:vV',\n                ['disable-html-update-links', 'connection=', 'debug', 'doctype=', 'export=', 'field=', 'format=',\n                 'help', 'import=', 'import-filter-name=', 'listener', 'meta=', 'no-launch',\n                 'output=', 'outputpath', 'password=', 'pipe=', 'port=', 'preserve',\n                 'server=', 'timeout=', 'user-profile=', 'show', 'stdin',\n                 'stdout', 'template', 'printer=', 'unsafe-quiet-update', 'verbose', 'version'])\n        except getopt.error as exc:\n            print('unoconv: %s, try unoconv -h for a list of all the options' % str(exc))\n            sys.exit(255)\n\n        for opt, arg in opts:\n            if opt in ['-h', '--help']:\n                self.usage()\n                print()\n                self.help()\n                sys.exit(0)\n            elif opt in ['-c', '--connection']:\n                self.connection = arg\n            elif opt in ['--debug']:\n                self.debug = True\n            elif opt in ['-d', '--doctype']:\n                self.doctype = arg\n            elif opt in ['-e', '--export']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.exportfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.exportfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.exportfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.exportfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.exportfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-F', '--field']:\n                l = arg.split('=')\n                self.fields[l[0]] = '='.join(l[1:])\n            elif opt in ['-f', '--format']:\n                self.format = arg\n            elif opt in ['-i', '--import']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.importfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.importfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.importfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.importfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.importfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-I', '--import-filter-name']:\n                self.importfiltername = arg\n            elif opt in ['-l', '--listener']:\n                self.listener = True\n            elif opt in ['-M', '--meta']:\n                l = arg.split('=')\n                self.metadata[l[0]] = '='.join(l[1:])\n            elif opt in ['-n', '--no-launch']:\n                self.nolaunch = True\n            elif opt in ['-o', '--output']:\n                self.output = arg\n            elif opt in ['--outputpath']:\n                print('Warning: This option is deprecated by --output.', file=sys.stderr)\n                self.output = arg\n            elif opt in ['--password']:\n                self.password = arg\n            elif opt in ['--pipe']:\n                self.pipe = arg\n            elif opt in ['-p', '--port']:\n                self.port = arg\n            elif opt in ['--preserve']:\n                self.preserve = True\n            elif opt in ['-s', '--server']:\n                self.server = arg\n            elif opt in ['--show']:\n                self.showlist = True\n            elif opt in ['--stdin']:\n                self.stdin = True\n            elif opt in ['--stdout']:\n                self.stdout = True\n            elif opt in ['-t', '--template']:\n                self.template = arg\n            elif opt in ['--disable-html-update-links']:\n                self.updatehtmllinks = False\n            elif opt in ['-T', '--timeout']:\n                self.timeout = int(arg)\n            elif opt in ['--unsafe-quiet-update']:\n                # ref https://www.openoffice.org/api/docs/common/ref/com/sun/star/document/UpdateDocMode.html\n                print('Warning: Do not use the option --unsafe-quiet-update with untrusted input.')\n                self.updateDocMode = QUIET_UPDATE\n            elif opt in ['-v', '--verbose']:\n                self.verbose = self.verbose + 1\n            elif opt in ['-V', '--version']:\n                self.version()\n                sys.exit(0)\n            elif opt in ['-P', '--printer']:\n                optKey, optValue = arg.split('=')\n                if optKey in ['PaperFormat']:\n                    self.paperformat = optValue\n                    self.setprinter = True\n                elif optKey in ['PaperOrientation']:\n                    self.paperorientation = optValue.upper()\n                    self.setprinter = True\n                elif optKey in ['PaperSize']:\n                    intFunc = int if sys.version_info.major > 2 else long\n                    size = list(map(lambda s: intFunc(s), optValue.split('x')))\n                    if (2 == len(size)):\n                        self.papersize = size\n                        self.setprinter = True\n            elif opt in ['--user-profile']:\n                self.userProfile = arg\n\n        # Enable verbosity\n        if self.verbose >= 2:\n            print('Verbosity set to level %d' % self.verbose, file=sys.stderr)\n\n        self.filenames = args\n\n        if not self.listener and not self.showlist and not self.stdin and self.doctype != 'list' and not self.filenames:\n            print('unoconv: you have to provide a filename or url as argument', file=sys.stderr)\n            print('Try `unoconv -h\\' for more information.', file=sys.stderr)\n            sys.exit(255)\n\n        # Set connection string\n        if not self.connection:\n            if not self.pipe:\n                self.connection = \"socket,host=%s,port=%s,tcpNoDelay=1;urp;StarOffice.ComponentContext\" % (self.server, self.port)\n            else:\n                self.connection = \"pipe,name=%s;urp;StarOffice.ComponentContext\" % (self.pipe)\n\n        # Make it easier for people to use a doctype (first letter is enough)\n        if self.doctype:\n            for doctype in doctypes:\n                if doctype.startswith(self.doctype):\n                    self.doctype = doctype\n\n        # Check if the user request to see the list of formats\n        if self.showlist or self.format == 'list':\n            if self.doctype:\n                fmts.display(self.doctype)\n            else:\n                for t in doctypes:\n                    fmts.display(t)\n            sys.exit(0)\n\n        # If no format was specified, probe it or provide it.\n        if not self.format:\n            # Check if the command is in the form odt2pdf\n            l = sys.argv[0].split('2')\n            if len(l) == 2:\n                self.format = l[1]\n            # Use the extension of the output file\n            elif self.output and os.path.basename(self.output).find('.') >= 0:\n                self.format = os.path.splitext(self.output)[1].lstrip('.')\n\n        # Default to PDF.\n        if not self.format:\n            self.format = 'pdf'\n\n    def version(self):\n        print('unoconv %s' % __version__)\n        print('Written by Dag Wieers <dag@wieers.com>')\n        print('Homepage at http://dag.wieers.com/home-made/unoconv/')\n        print()\n        print('platform %s/%s' % (os.name, sys.platform))\n        print('python %s' % sys.version)\n\n        if uno:\n            # Get office product information.\n            product = uno.getComponentContext().ServiceManager.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            print(product.ooName, product.ooSetupVersionAboutBox)\n\n    def usage(self):\n        print('usage: unoconv [options] file [file2 ..]', file=sys.stderr)\n\n    def help(self):\n        print('''Convert from and to any format supported by LibreOffice\n\nunoconv options:\n  -c, --connection=string             use a custom connection string\n  -d, --doctype=type                  specify document type\n                                        (document, graphics, presentation, spreadsheet)\n  -e, --export=name=value             set export filter options\n                                        eg. -e PageRange=1-2\n  -f, --format=format                 specify the output format\n  -F, --field=name=value              replace user-defined text field with value\n                                        eg. -F Client_Name=\"Oracle\"\n  -i, --import=string                 set import filter option string\n                                        eg. -i utf8\n  -I, --import-filter-name=string     set import filter name, useful when converting stdin\n                                      or files without an extension)\n                                        eg. -I ooxml\n  -l, --listener                      start a permanent listener to use by unoconv clients\n  -n, --no-launch                     fail if no listener is found (default: launch one)\n  -o, --output=name                   output basename, filename or directory\n      --pipe=name                     alternative method of connection using a pipe\n  -p, --port=port                     specify the port (default: 2002)\n                                        to be used by client or listener\n      --password=string               provide a password to decrypt the document\n      --preserve                      keep timestamp and permissions of the original document\n  -s, --server=server                 specify the server address (default: 127.0.0.1)\n                                        to be used by client or listener\n      --show                          list the available output formats\n      --stdin                         read from stdin (filenames are ignored if provided)\n      --stdout                        write output to stdout\n  -t, --template=file                 import the styles from template (.ott)\n  -T, --timeout=secs                  timeout after secs if connection to listener fails\n      --unsafe-quiet-update           allow rendered document to fetch external resources (Warning: this is unsafe with untrusted input)\n  -v, --verbose                       be more and more verbose (-vvv for debugging)\n      --version                       display version number of unoconv, OOo/LO and platform details\n  -P, --printer=name=value            printer options\n                                        PaperFormat: specify printer paper format\n                                          eg. -P PaperFormat=A3\n                                        PaperOrientation: specify printer paper orientation\n                                          eg. -P PaperOrientation=landscape\n                                        PaperSize: specify printer paper size, paper format should set to USER, size=widthxheight\n                                          eg. -P PaperSize=130x200 means width=130, height=200\n  --disable-html-update-links   disables the recheck for updating links missed by libreoffice\n  --user-profile=path                 use a custom user profile path\n''', file=sys.stderr)\n\n\nclass Convertor:\n    def __init__(self):\n        global exitcode, ooproc, office, product\n        unocontext = None\n\n        # Do the LibreOffice component dance\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n\n        # Test for an existing connection\n        info(3, 'Connection type: %s' % op.connection)\n        unocontext = self.connect(resolver)\n\n        if not unocontext:\n            die(251, \"Unable to connect or start own listener. Aborting.\")\n\n        # And some more LibreOffice magic\n        unosvcmgr = unocontext.ServiceManager\n        self.desktop = unosvcmgr.createInstanceWithContext(\"com.sun.star.frame.Desktop\", unocontext)\n        self.cwd = unohelper.systemPathToFileUrl(os.getcwd())\n\n        # List all filters\n        # self.filters = unosvcmgr.createInstanceWithContext(\"com.sun.star.document.FilterFactory\", unocontext)\n        # for filter in self.filters.getElementNames():\n            # print filter\n            # print dir(filter), dir(filter.format)\n\n    def connect(self, resolver):\n        global ooproc, product, office\n        unocontext = None\n\n        try:\n            unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n        except NoConnectException as e:\n            # info(3, \"Existing listener not found.\\n%s\" % e)\n            info(3, \"Existing listener not found.\")\n\n            if op.nolaunch:\n                die(113, \"Existing listener not found. Unable start listener by parameters. Aborting.\")\n\n            # Start our own OpenOffice instance\n            info(3, \"Launching our own listener using %s.\" % office.binary)\n            try:\n                product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n                if product.ooName not in ('LibreOffice', 'LOdev') or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                    args = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-accept=%s\" % op.connection]\n                else:\n                    args = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--accept=%s\" % op.connection]\n                if op.userProfile:\n                    args.append(\"-env:UserInstallation=file://\" + realpath(op.userProfile))\n                info(2, '%s listener arguments are %s.' % (product.ooName, args))\n                ooproc = subprocess.Popen(args, env=os.environ)\n                info(2, '%s listener successfully started. (pid=%s)' % (product.ooName, ooproc.pid))\n\n                # Try connection to it for op.timeout seconds (flakky OpenOffice)\n                timeout = 0\n                while timeout <= op.timeout:\n                    # Is it already/still running?\n                    retcode = ooproc.poll()\n                    if retcode == 81:\n                        info(3, \"Caught exit code 81 (new installation). Restarting listener.\")\n                        return self.connect(resolver)\n                        break\n\n                    elif retcode is not None:\n                        info(3, \"Process %s (pid=%s) exited with %s.\" % (office.binary, ooproc.pid, retcode))\n                        break\n\n                    try:\n                        unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n                        break\n                    except NoConnectException:\n                        time.sleep(0.5)\n                        timeout += 0.5\n                    except:\n                        raise\n                else:\n                    error(\"Failed to connect to %s (pid=%s) in %d seconds.\\n%s\" % (office.binary, ooproc.pid, op.timeout, e))\n            except Exception as e:\n                raise\n                error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n        return unocontext\n\n    def getimportformat(self):\n        if op.doctype:\n            importformat = fmts.bydoctype(op.doctype, op.importfiltername)\n        else:\n            importformat = fmts.byname(op.importfiltername)\n\n        if not importformat:\n            error('Import format [%s] is not known to unoconv.' % importformat)\n\n        return importformat[0]\n\n    def getformat(self, inputfn):\n        doctype = None\n\n        # Get the output format from mapping\n        if op.doctype:\n            outputfmt = fmts.bydoctype(op.doctype, op.format)\n        else:\n            outputfmt = fmts.byname(op.format)\n\n            if not outputfmt:\n                outputfmt = fmts.byextension(os.extsep + op.format)\n\n        # If no doctype given, check list of acceptable formats for input file ext doctype.\n        # FIXME: This should go into the for-loop to match each individual input filename.\n        if outputfmt:\n            inputext = os.path.splitext(inputfn)[1]\n            inputfmt = fmts.byextension(inputext)\n            if inputfmt:\n                for fmt in outputfmt:\n                    if inputfmt[0].doctype == fmt.doctype:\n                        doctype = inputfmt[0].doctype\n                        outputfmt = fmt\n                        break\n                else:\n                    outputfmt = outputfmt[0]\n    #       print >>sys.stderr, 'Format `%s\\' is part of multiple doctypes %s, selecting `%s\\'.' % (format, [fmt.doctype for fmt in outputfmt], outputfmt[0].doctype)\n            else:\n                outputfmt = outputfmt[0]\n\n        # No format found, throw error\n        if not outputfmt:\n            if doctype:\n                error('Format [%s/%s] is not known to unoconv.' % (op.doctype, op.format))\n            else:\n                error('Format [%s] is not known to unoconv.' % op.format)\n            die(1)\n\n        return outputfmt\n\n    def preserve(self, inputfn, outputfn):\n        # Get timestamp of input file.\n        s = os.stat(inputfn)\n        times = (s.st_atime, s.st_mtime)\n        mode = s.st_mode\n        # Set it to output file.\n        with open(outputfn, \"a\") as f:\n            os.utime(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.utime in os.supports_fd else inputfn,\n                     times=times)\n            os.chmod(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.chmod in os.supports_fd else inputfn,\n                     mode)\n\n    def convert(self, inputfn):\n        global exitcode\n\n        document = None\n        outputfmt = self.getformat(inputfn)\n\n        if op.verbose > 0:\n            print('Input file:', inputfn, file=sys.stderr)\n\n        try:\n            # Import phase.\n            phase = \"import\"\n\n            # Load inputfile.\n            inputprops = UnoProps(Hidden=True, ReadOnly=True, UpdateDocMode=op.updateDocMode)\n\n            if op.password:\n                inputprops += UnoProps(Password=op.password)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilteroptions:\n                # print \"Import filter options: %s\" % op.importfilteroptions\n                inputprops += UnoProps(FilterOptions=op.importfilteroptions)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilter:\n                inputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.importfilter), ), 0), )\n\n            if op.importfiltername:\n                importformat = self.getimportformat()\n                inputprops += UnoProps(FilterName=importformat.filter)\n\n            if op.stdin:\n                inputStream = self.svcmgr.createInstanceWithContext(\"com.sun.star.io.SequenceInputStream\", self.context)\n                inputStream.initialize((uno.ByteSequence(inputfn),))\n                inputprops += UnoProps(InputStream=inputStream)\n                inputurl = 'private:stream'\n            elif os.path.exists(inputfn):\n                inputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(inputfn))\n            else:\n                inputurl = inputfn\n            document = self.desktop.loadComponentFromURL(inputurl, \"_blank\", 0, inputprops)\n\n            if not document:\n                raise UnoException(\"The document '%s' could not be opened.\" % inputurl, None)\n\n            # Import style template.\n            phase = \"import-style\"\n            if op.template:\n                if os.path.exists(op.template):\n                    info(1, \"Template file: %s\" % op.template)\n                    templateprops = UnoProps(OverwriteStyles=True)\n                    templateurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(op.template))\n                    document.StyleFamilies.loadStylesFromURL(templateurl, templateprops)\n                else:\n                    print('unoconv: template file `%s\\' does not exist.' % op.template, file=sys.stderr)\n                    exitcode = 1\n\n            # Force all cells to recalculate if we are able to. This will get rid of errors in cells.\n            # FIXME: We cannot recalculate the cells because it breaks issue #97 (cells get #VALUE)\n            # phase = \"recalculate\"\n            # try:\n                # document.calculateAll()\n            # except AttributeError:\n                # pass\n\n            # Update document links if appropriate\n            if op.updateDocMode != NO_UPDATE:\n                phase = \"update-links\"\n                try:\n                    document.updateLinks()\n                    # Found that when converting HTML files with external images, OO would only load five or six of\n                    # the images in the file. In the resulting document, the rest of the images did not appear. Cycling\n                    # through all the image references in the document seems to force OO to actually load them. Found\n                    # some helpful guidance in this thread:\n                    # https://forum.openoffice.org/en/forum/viewtopic.php?f=30&t=23909\n                    # Ideally we would like to have the option to embed the images into the document, but I have not been\n                    # able to figure out how to do this yet.\n                    if op.updatehtmllinks:\n                        graphObjs = document.GraphicObjects\n                        for i in range(0, graphObjs.getCount()):\n                            graphObj = graphObjs.getByIndex(i)\n                except AttributeError:\n                    # the document doesn't implement the XLinkUpdate interface\n                    pass\n\n            # Add/Replace variables\n            phase = \"replace-fields\"\n            for f in op.fields:\n                try:\n                    field = document.TextFieldMasters.getByName(\"com.sun.star.text.fieldmaster.User.%s\" % f)\n                    field.setPropertyValue('Content', op.fields[f])\n                except UnoException:\n                    error(\"unoconv: failed to replace variable '%s' with value '%s' in the document.\" % (f, op.fields[f]))\n                    pass\n\n            # Add/Replace metadata\n            phase = \"replace-metadata\"\n            props = document.getDocumentProperties()\n            user_props = props.getUserDefinedProperties()\n            for prop, value in op.metadata.items():\n                for container in (props, user_props):\n                    curr = getattr(container, prop, None)\n                    if curr is not None:\n                        setattr(container, prop, value)\n                        break\n                else:\n                    user_props.addProperty(prop, 0, '')\n                    user_props.setPropertyValue(prop, value)\n\n            # Update document indexes\n            phase = \"update-indexes\"\n            for ii in range(2):\n                # At first, update Table-of-Contents.\n                # ToC grows, so page numbers grow too.\n                # On second turn, update page numbers in ToC.\n                try:\n                    document.refresh()\n                    indexes = document.getDocumentIndexes()\n                except AttributeError:\n                    # The document doesn't implement the XRefreshable and/or\n                    # XDocumentIndexesSupplier interfaces\n                    break\n                else:\n                    for i in range(0, indexes.getCount()):\n                        indexes.getByIndex(i).update()\n\n            info(1, \"Selected output format: %s\" % outputfmt)\n            info(2, \"Selected office filter: %s\" % outputfmt.filter)\n            info(2, \"Used doctype: %s\" % outputfmt.doctype)\n\n            # Document properties phase\n            phase = \"disable-showchanges\"\n            try:\n                document.ShowChanges = False\n            except AttributeError:\n                pass\n\n            # Export phase\n            phase = \"export\"\n\n            outputprops = UnoProps(FilterName=outputfmt.filter, OutputStream=OutputStream(), Overwrite=True)\n\n            # Set default filter options\n            if op.exportfilteroptions:\n                # print \"Export filter options: %s\" % op.exportfilteroptions\n                outputprops += UnoProps(FilterOptions=op.exportfilteroptions)\n            elif outputfmt.filter == 'Text (encoded)':\n                outputprops += UnoProps(FilterOptions=\"UTF8,LF\")\n            elif outputfmt.filter == 'Text':\n                outputprops += UnoProps(FilterOptions=\"UTF8\")\n            elif outputfmt.filter == 'Text - txt - csv (StarCalc)':\n                outputprops += UnoProps(FilterOptions=\"44,34,UTF8\")\n\n            # Set printer options\n            if op.setprinter:\n                printer = document.getPrinter()\n                for i in range(len(printer)):\n                    if printer[i].Name == 'PaperOrientation' and op.paperorientation is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperOrientation', op.paperorientation)\n                    elif printer[i].Name == 'PaperFormat' and op.paperformat is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperFormat', op.paperformat)\n                    elif (printer[i].Name == 'PaperSize' and op.papersize is not None and len(op.papersize) == 2):\n                        printer[i].Value.Width = op.papersize[0]\n                        printer[i].Value.Height = op.papersize[1]\n                document.setPrinter(printer)\n\n            # Cannot use UnoProps for FilterData property\n            if op.exportfilter:\n                outputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.exportfilter), ), 0), )\n\n            if op.stdout:\n                # Ensure binary data to stdout works\n                # http://stackoverflow.com/questions/2374427/python-2-x-write-binary-output-to-stdout\n                if sys.platform == \"win32\":\n                    import msvcrt\n                    msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n                outputurl = \"private:stream\"\n            else:\n                if os.path.exists(inputfn):\n                    (inbase, ext) = os.path.splitext(inputfn)\n                else:\n                    (inbase, ext) = os.path.splitext(os.path.basename(inputfn))\n                if op.output:\n                    (outbase, ext) = os.path.splitext(op.output)\n                    if len(op.filenames) > 1:\n                        outputfn = realpath(outbase, os.path.basename(inbase) + os.extsep + outputfmt.extension)\n                    else:\n                        outputfn = realpath(outbase + os.extsep + outputfmt.extension)\n                else:\n                    outputfn = realpath(inbase + os.extsep + outputfmt.extension)\n\n                outputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(outputfn))\n\n            info(1, \"Output file: %s\" % outputurl)\n\n            try:\n                document.storeToURL(outputurl, tuple(outputprops))\n            except IOException as e:\n                raise UnoException(\"Unable to store document to %s (Error %s)\\n\\nProperties: %s\" % (outputurl, e.value, outputprops), None)\n\n            phase = \"dispose\"\n            document.dispose()\n            document.close(True)\n            if not op.stdout and op.preserve:\n                self.preserve(inputfn, outputfn)\n\n        except SystemError as e:\n            error(\"unoconv: SystemError during %s phase:\\n%s\" % (phase, e))\n            exitcode = 1\n\n        except RuntimeException as e:\n            error(\"unoconv: RuntimeException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 6\n\n        except DisposedException as e:\n            error(\"unoconv: DisposedException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 7\n\n        except IllegalArgumentException as e:\n            error(\"UNO IllegalArgument during %s phase:\\nSource file cannot be read. %s\" % (phase, e))\n            exitcode = 8\n\n        except IOException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: IOException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 3\n\n        except CannotConvertException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: CannotConvertException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 4\n\n        except UnoException as e:\n            if hasattr(e, 'ErrCode'):\n                error(\"unoconv: UnoException during %s phase in %s (ErrCode %d)\" % (phase, repr(e.__class__), e.ErrCode))\n                exitcode = e.ErrCode\n                pass\n            if hasattr(e, 'Message'):\n                error(\"unoconv: UnoException during %s phase:\\n%s\" % (phase, e.Message))\n                exitcode = 5\n            else:\n                error(\"unoconv: UnoException during %s phase in %s\" % (phase, repr(e.__class__)))\n                exitcode = 2\n                pass\n\n\nclass Listener:\n    def __init__(self):\n        global product\n\n        info(1, \"Start listener on %s:%s\" % (op.server, op.port))\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        try:\n            resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n            product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            try:\n                unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n            except NoConnectException:\n                pass\n            else:\n                info(1, \"Existing %s listener found, nothing to do.\" % product.ooName)\n                return\n            if product.ooName != \"LibreOffice\" or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                cmd = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nologo\", \"-nofirststartwizard\", \"-norestore\", \"-accept=%s\" % op.connection]\n            else:\n                cmd = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nologo\", \"--nofirststartwizard\", \"--norestore\", \"--accept=%s\" % op.connection]\n\n            # The rationale for using subprocess.Popen is to be able to handle\n            # a SIGTERM signal below and properly terminate the started office\n            # process then. This makes it possible to put the command unoconv -l\n            # under control of supervisor to deamonize it. Supervisor terminates\n            # via sending SIGTERM and sending SIGTERM to a running unoconv -l\n            # without the handler below will not terminate the office process\n            # together with it leaving the office process running.\n            office_process = subprocess.Popen(cmd, env=os.environ)\n\n            def sigterm_handler(signum, frame):\n                office_process.terminate()\n                die(6, 'Exiting on SIGTERM')\n\n            signal.signal(signal.SIGTERM, sigterm_handler)\n\n            ret = office_process.wait()\n            if ret == 81:\n                info(1, \"Restarting %s (first start - 81 exit code)\" % product.ooName)\n                office_process = subprocess.Popen(cmd, env=os.environ)\n                office_process.wait()\n            else:\n                raise Exception(\"%s crashed - exit code: %s\" % (product.ooName, ret))\n        except Exception as e:\n            error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n\ndef error(msg, file=sys.stderr):\n    \"\"\"Output error message.\"\"\"\n    print(msg, file=file)\n\n\ndef info(level, msg):\n    \"\"\"Output info message.\"\"\"\n    if 'op' not in globals():\n        pass\n    elif op.verbose >= 3 and level >= 3:\n        print(\"DEBUG:\", msg, file=sys.stderr)\n    elif not op.stdout and level <= op.verbose:\n        print(msg, file=sys.stdout)\n    elif level <= op.verbose:\n        print(msg, file=sys.stderr)\n\n\ndef die(ret, msg=None):\n    \"\"\"Print optional error and exit with errorcode.\"\"\"\n    global convertor, ooproc, office\n\n    if msg:\n        error('Error: %s' % msg)\n\n    # Did we start our own listener instance?\n    if not op.listener and ooproc and convertor:\n\n        # If there is a GUI now attached to the instance, disable listener.\n        if convertor.desktop.getCurrentFrame():\n            info(2, 'Trying to stop %s GUI listener.' % product.ooName)\n            try:\n                if product.ooName != \"LibreOffice\" or product.ooSetupVersion <= 3.3:\n                    subprocess.Popen([office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-unaccept=%s\" % op.connection], env=os.environ)\n                else:\n                    subprocess.Popen([office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--unaccept=%s\" % op.connection], env=os.environ)\n                ooproc.wait()\n                info(2, '%s listener successfully disabled.' % product.ooName)\n            except Exception as e:\n                error(\"Terminate using %s failed.\\n%s\" % (office.binary, e))\n\n        # If there is no GUI attached to the instance, terminate instance.\n        else:\n            info(3, 'Terminating %s instance.' % product.ooName)\n            try:\n                convertor.desktop.terminate()\n            except DisposedException:\n                info(2, '%s instance unsuccessfully closed, sending TERM signal.' % product.ooName)\n                try:\n                    ooproc.terminate()\n                except AttributeError:\n                    os.kill(ooproc.pid, 15)\n            info(3, 'Waiting for %s instance to exit.' % product.ooName)\n            ooproc.wait()\n\n        # LibreOffice processes may get stuck and we have to kill them.\n        # Is it still running?\n        if ooproc.poll() is None:\n            info(1, '%s instance still running, please investigate...' % product.ooName)\n            ooproc.wait()\n            info(2, '%s instance unsuccessfully terminated, sending KILL signal.' % product.ooName)\n            try:\n                ooproc.kill()\n            except AttributeError:\n                os.kill(ooproc.pid, 9)\n            info(3, 'Waiting for %s with pid %s to disappear.' % (ooproc.pid, product.ooName))\n            ooproc.wait()\n\n    # Allow Python GC to garbage collect pyuno object *before* exit call\n    # which avoids random segmentation faults --vpa\n    convertor = None\n\n    sys.exit(ret)\n\n\ndef main():\n    global convertor, exitcode\n    convertor = None\n\n    try:\n        if op.listener:\n            listener = Listener()\n\n        if op.stdin:\n            # Read stdin buffer in Python 3 in order to correctly handle binary streams.\n            # ref: https://docs.python.org/3.1/library/sys.html#sys.stdin\n            if sys.version_info.major > 2:\n                inputfn = sys.stdin.buffer.read()\n            else:\n                inputfn = sys.stdin.read()\n            convertor = Convertor()\n            convertor.convert(inputfn)\n        elif op.filenames:\n            convertor = Convertor()\n            for inputfn in op.filenames:\n                convertor.convert(inputfn)\n\n    except NoConnectException:\n        error(\"unoconv: could not find an existing connection to LibreOffice at %s:%s.\" % (op.server, op.port))\n        if op.connection:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"%s\\\"\" % (op.server, op.server, op.port, op.connection))\n        else:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=%s,port=%s;urp;\\\"\" % (op.server, op.server, op.port, op.server, op.port))\n            info(0, \"Please start an soffice instance on server '%s' by doing:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=127.0.0.1,port=%s;urp;\\\"\" % (op.server, op.port))\n        exitcode = 1\n    # except UnboundLocalError:\n        # die(252, \"Failed to connect to remote listener.\")\n    except OSError:\n        error(\"Warning: failed to launch Office suite. Aborting.\")\n\n\n# Main entrance\nif __name__ == '__main__':\n    exitcode = 0\n\n    info(3, 'sysname=%s, platform=%s, python=%s, python-version=%s' % (os.name, sys.platform, sys.executable, sys.version))\n\n    for of in find_offices():\n        if of.python != sys.executable and not sys.executable.startswith(of.basepath):\n            python_switch(of)\n        office_environ(of)\n        # debug_office()\n        try:\n            import uno\n            import unohelper\n            office = of\n            break\n        except:\n            # debug_office()\n            print(\"unoconv: Cannot find a suitable pyuno library and python binary combination in %s\" % of, file=sys.stderr)\n            print(\"ERROR:\", sys.exc_info()[1], file=sys.stderr)\n            print(file=sys.stderr)\n    else:\n        # debug_office()\n        print(\"unoconv: Cannot find a suitable office installation on your system.\", file=sys.stderr)\n        print(\"ERROR: Please locate your office installation and send your feedback to:\", file=sys.stderr)\n        print(\"       http://github.com/dagwieers/unoconv/issues\", file=sys.stderr)\n        sys.exit(1)\n\n    # Working pyuno library found. Import classes.\n    from com.sun.star.beans import PropertyValue\n    from com.sun.star.connection import NoConnectException\n    from com.sun.star.document.UpdateDocMode import NO_UPDATE, QUIET_UPDATE\n    from com.sun.star.io import IOException, XOutputStream\n    from com.sun.star.lang import DisposedException, IllegalArgumentException\n    from com.sun.star.script import CannotConvertException\n    from com.sun.star.uno import Exception as UnoException\n    from com.sun.star.uno import RuntimeException\n\n    # Build on imported classes.\n    class OutputStream(unohelper.Base, XOutputStream):\n        def __init__(self):\n            self.closed = 0\n\n        def closeOutput(self):\n            self.closed = 1\n\n        def writeBytes(self, seq):\n            try:\n                sys.stdout.buffer.write(seq.value)\n            except AttributeError:\n                sys.stdout.write(seq.value)\n\n        def flush(self):\n            pass\n\n    def UnoProps(**args):\n        props = []\n        for key in args:\n            prop = PropertyValue()\n            prop.Name = key\n            prop.Value = args[key]\n            props.append(prop)\n        return tuple(props)\n\n    op = Options(sys.argv[1:])\n\n    info(2, \"Using office base path: %s\" % office.basepath)\n    info(2, \"Using office binary path: %s\" % office.unopath)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        die(6, 'Exiting on user request')\n    die(exitcode)\n"}
{"type": "source_file", "path": "web_app/hf_waitress.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer, BitsAndBytesConfig, QuantoConfig, HqqConfig\r\nfrom huggingface_hub import login\r\nimport torch\r\n\r\nimport subprocess\r\nimport threading\r\nimport traceback\r\nimport argparse\r\nimport logging\r\nimport queue\r\nimport time\r\nimport json\r\nimport uuid\r\nimport sys\r\nimport os\r\nimport io\r\n\r\nfrom functools import wraps\r\nfrom logging.handlers import RotatingFileHandler\r\nfrom flask import Flask, request, jsonify, Response\r\nfrom flask_cors import CORS\r\n\r\nfrom waitress import serve\r\n\r\napp = Flask(__name__)\r\nCORS(app)\r\n\r\nPIPE = None\r\n\r\nllm_semaphore = threading.Semaphore(1)\r\nconfig_writer_semaphore = threading.Semaphore(1)\r\nerror_logging_semaphore = threading.Semaphore(1)\r\nreader_semaphore = threading.Semaphore(3)\r\n\r\n\r\n#########################------------Setup & Handle Logging-------------###############################\r\ntry:\r\n    # 1 - Create a logger\r\n    logger = logging.getLogger('my_logger')\r\n    logger.setLevel(logging.ERROR)\r\n\r\n    # 2 - Create a RotatingFileHandler\r\n    # maxBytes: max file size of log file after which a new file is created; set to 1024 * 1024 * 5 for 5MB: 1024x1024 is 1MB, then a multiplyer for the number of MB\r\n    # backupCount: number of backup files to keep specifying how many old log files to keep\r\n    handler = RotatingFileHandler('hf_server_log.log', maxBytes=1024*1024*5, backupCount=2)\r\n    handler.setLevel(logging.ERROR)\r\n\r\n    # 3 - Create a formatter and set it for the handler\r\n    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\r\n    handler.setFormatter(formatter)\r\n\r\n    # 4 - Add the handler to the logger\r\n    logger.addHandler(handler)\r\n    # Logger ready! Usage: logger.error(f\"This is an error message with error {e}\")\r\nexcept Exception as e:\r\n    print(f\"\\n\\nCould not establish logger, encountered error: {e}\")\r\n\r\n\r\ndef handle_api_error(message, exception=None):\r\n    with error_logging_semaphore:\r\n        error_message = f\"\\n\\n{message} {str(exception) if exception else '; No exception info.'}\\n\\n\"\r\n        traceback_details = traceback.format_exc()\r\n        full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\r\n\r\n        if logger:\r\n            logger.error(full_message)\r\n            print(error_message)\r\n        else:\r\n            print(error_message)\r\n        \r\n        return jsonify(success=False, error=error_message), 500 #internal server error\r\n\r\n\r\ndef handle_local_error(message, exception=None):\r\n    with error_logging_semaphore:\r\n        error_message = f\"\\n\\n{message} {str(exception) if exception else '; No exception info.'}\\n\\n\"\r\n        traceback_details = traceback.format_exc()\r\n        full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\r\n\r\n        if logger:\r\n            logger.error(full_message)\r\n            print(error_message)\r\n        else:\r\n            print(error_message)\r\n        \r\n        raise Exception(exception)\r\n\r\n\r\ndef handle_error_no_return(message, exception=None):\r\n    with error_logging_semaphore:\r\n        error_message = f\"\\n\\n{message} {str(exception) if exception else '; No exception info.'}\\n\\n\"\r\n        traceback_details = traceback.format_exc()\r\n        full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\r\n\r\n        if logger:\r\n            logger.error(full_message)\r\n            print(error_message)\r\n        else:\r\n            print(error_message)\r\n\r\n############################----------------------------------------------###############################\r\n\r\n\r\n\r\n############################------------configuration manager-------------###############################\r\n\r\nif not os.path.exists('hf_config.json'):\r\n    with config_writer_semaphore:\r\n        try:\r\n            with open('hf_config.json', 'w') as file:\r\n                json.dump({}, file)\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not init config.json. Multiple app restarts may be required to get the app to init correctly. Printing error and proceeding: \", e)\r\n\r\n\r\n# Method to write to hf_config.json | input- dict of key:values to be written to hf_config.json\r\ndef write_config(config_updates, filename='hf_config.json'):\r\n\r\n    with config_writer_semaphore:\r\n\r\n        # Open hf_config file to read-in all current params:\r\n        try:\r\n            with open(filename, 'r') as file:\r\n                hf_config = json.load(file)\r\n        except Exception as e:\r\n            hf_config = {}     #init emply hf_config dict\r\n            handle_error_no_return(\"Could not read hf_config.json when attempting to write, encountered error: \", e)\r\n\r\n        #restart logic in write_config() might be unnecessary, circle back later\r\n        restart_required = False\r\n        triggers_for_hf_restart = ['torch_device_map', 'torch_dtype', 'model_id', 'awq', 'attn_implementation', 'pipeline_task', 'quantize', 'quant_level', 'port', 'use_flash_attention_2', 'hqq_group_size']\r\n        for key in config_updates:\r\n            if key in triggers_for_hf_restart and config_updates[key] != hf_config.get(key):\r\n                restart_required = True\r\n\r\n        hf_config.update(config_updates)\r\n\r\n        # Write updated hf_config.json:\r\n        try:\r\n            with open(filename, 'w') as file:\r\n                json.dump(hf_config, file, indent=4)\r\n        except Exception as e:\r\n            handle_local_error(\"Could not update hf_config.json, encountered error: \", e)\r\n        \r\n        return {'success': True, 'restart_required':restart_required}\r\n            \r\n\r\n# Method to read from hf_config.json | input- list of keys to be read from hf_config.json; output- dict of key:value pairs; MANAGE DEFAULTS HERE!\r\ndef read_config(keys, default_value=None, filename='hf_config.json'):\r\n\r\n    with reader_semaphore:\r\n    \r\n        # Open hf_config file to read-in all current params:\r\n        try:\r\n            with open(filename, 'r') as file:\r\n                hf_config = json.load(file)\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not read hf_config.json, encountered error: \", e)\r\n            return {key: default_value for key in keys}     #because a read scenario wherein hf_config.json does not exist shouldn't occur!\r\n        \r\n        return_dict = {}\r\n        update_config_dict = {}\r\n\r\n        for key in keys:\r\n            if key in hf_config:\r\n                return_dict[key] = hf_config[key]\r\n            else:\r\n                default_value = {\r\n                    'access_gated':False,\r\n                    'access_token':\"\",\r\n                    'model_id':\"microsoft/Phi-3-mini-4k-instruct\",\r\n                    'gguf':False,\r\n                    'awq':False,\r\n                    'gguf_model_id':None,\r\n                    'gguf_filename':None,\r\n                    'quantize':\"quanto\",\r\n                    'quant_level':\"int4\",\r\n                    'hqq_group_size':64,\r\n                    'push_to_hub':False,\r\n                    'torch_device_map':\"auto\", \r\n                    'torch_dtype':\"auto\", \r\n                    'trust_remote_code':True, \r\n                    'use_flash_attention_2':False, \r\n                    'pipeline_task':\"text-generation\", \r\n                    'max_new_tokens':500, \r\n                    'return_full_text':False, \r\n                    'temperature':0.0,\r\n                    'do_sample':False, \r\n                    'top_k':40, \r\n                    'top_p':0.95, \r\n                    'min_p':0.05, \r\n                    'n_keep':0,\r\n                    'port':9069,\r\n                    'model_list': ['mistralai/Mistral-Nemo-Instruct-2407', \r\n                                   'meta-llama/Meta-Llama-3.1-8B-Instruct', \r\n                                   'meta-llama/Meta-Llama-3.1-70B-Instruct', \r\n                                   'meta-llama/Meta-Llama-3.1-405B-Instruct-FP8',\r\n                                   'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\r\n                                   'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4',\r\n                                   'microsoft/Phi-3.5-mini-instruct',\r\n                                   'microsoft/Phi-3.5-MoE-instruct',\r\n                                   'microsoft/Phi-3-mini-4k-instruct',\r\n                                   'microsoft/Phi-3-mini-128k-instruct',\r\n                                   'microsoft/Phi-3-small-8k-instruct',\r\n                                   'microsoft/Phi-3-small-128k-instruct',\r\n                                   'microsoft/Phi-3-medium-4k-instruct',\r\n                                   'microsoft/Phi-3-medium-128k-instruct',\r\n                                   'CohereForAI/c4ai-command-r-plus',\r\n                                   'CohereForAI/c4ai-command-r-v01',\r\n                                   'google/gemma-2-2b-it',\r\n                                   'google/gemma-2-9b-it',\r\n                                   'google/gemma-2-27b-it',\r\n                                   'Qwen/Qwen2-7B-Instruct',\r\n                                   'Qwen/Qwen2-72B-Instruct',\r\n                                   'alpindale/goliath-120b',\r\n                                   'TheBloke/goliath-120b-AWQ'\r\n                                   ]\r\n                }.get(key, 'undefined')\r\n\r\n                if default_value == 'undefined':\r\n                    raise KeyError(f\"Key \\'{key}\\' not found in hf_config.json and no default value has been defined either.\\n\")\r\n                \r\n                return_dict[key] = default_value\r\n                update_config_dict[key] = default_value\r\n        \r\n        if update_config_dict:\r\n            # Write defaults\r\n            try:\r\n                write_config(update_config_dict)\r\n            except Exception as e:\r\n                handle_error_no_return(\"Could not write defaults to hf_config.json. Encountered error: \", e)\r\n\r\n        return return_dict\r\n\r\n\r\n# Method for API route to read from hf_config.json\r\n# Deviates from typical RESTful principals to use a POST call to fetch values but practical & justifyable because we:\r\n# 1. Do not want to make the URL huge with a ever-growing list of query-params 2. Do not wish to expose values via query-params\r\n@app.route('/hf_config_reader_api', methods=['POST'])\r\ndef hf_config_reader_api():\r\n    # keys = request.args.getlist('keys') # Assuming keys are passed as query parameters\r\n    \r\n    try:\r\n        keys = request.json.get('keys', []) # Could also do keys = request.json['keys'] but this way we can provide a default list should 'keys' be missing!\r\n    except Exception as e:\r\n        handle_api_error(\"Server-side error - could not read keys for hf_config_reader_api request. Encountered error:\", e)\r\n\r\n    try:\r\n        values = read_config(keys)  # send list of keys, get dict of key:values\r\n    except Exception as e:\r\n        handle_api_error(\"Server-side error - could not read keys from hf_config.json. Encountered error: \", e)\r\n    \r\n    return jsonify(success=True, values=values)\r\n\r\n\r\n# Method for API route to write to hf_config.json\r\n@app.route('/hf_config_writer_api', methods=['POST'])\r\ndef hf_config_writer_api():\r\n\r\n    try:\r\n        config_updates = request.json['config_updates']\r\n        print(f\"\\n\\nconfig_updates for hf_config_writer_api:\\n{config_updates}\\n\\n\")\r\n    except Exception as e:\r\n        handle_api_error(\"Server-side error - could not read values for hf_config_writer_api request. Encountered error: \", e)\r\n    \r\n    try:\r\n        write_return = write_config(config_updates)\r\n    except Exception as e:\r\n        handle_api_error(\"Server-side error - could not write keys to hf_config.json. Encountered error: \", e)\r\n    \r\n    return jsonify({\"success\": write_return['success'], \"restart_required\": write_return['restart_required']})\r\n\r\n\r\n############################----------------------------------------------###############################\r\n\r\n\r\n\r\ndef safe_int(value, default):\r\n    if value is None:\r\n        handle_error_no_return(\"Null value, cannot convert to integer type. Proceeding with default value.\")\r\n        return default\r\n    try:\r\n        return int(value)\r\n    except(ValueError, TypeError) as e:\r\n        handle_error_no_return(f\"Could not convert {value} to an integer, proceeding with default value {default}. Encountered error: \", e)\r\n        return default\r\n\r\n\r\ndef safe_float(value, default):\r\n    if value is None:\r\n        handle_error_no_return(\"Null value, cannot convert to float type. Proceeding with default value.\")\r\n        return default\r\n    try:\r\n        return float(value)\r\n    except(ValueError, TypeError) as e:\r\n        handle_error_no_return(f\"Could not convert {value} to a float, proceeding with default value {default}. Encountered error: \", e)\r\n        return default\r\n\r\n\r\ndef hf_login_for_gated_models():\r\n    access_token = \"\"\r\n    try:\r\n        read_return = read_config(['access_token'])\r\n        access_token = str(read_return['access_token'])\r\n    except Exception as e:\r\n        handle_api_error(\"403 - No access token found, please submit an access token via the /hf_login endpoint\")\r\n\r\n    try:\r\n        login(token=access_token)\r\n    except Exception as e:\r\n        handle_api_error(\"Unable to login to the HuggingFace-Hub, please ensure the correct access token has been provided. Encountered error: \", e)\r\n\r\n\r\ndef parse_arguments():\r\n\r\n    try:\r\n        parser = argparse.ArgumentParser(description=\"Server for HuggingFace Transformers models\")\r\n    except Exception as e:\r\n        handle_local_error(\"Could not create parser to parse_arguments(), proceeding with defaults. Encountered error: \", e)\r\n\r\n    # Even if a parser object could not be created, a read_request will write & return defaults \r\n    try:\r\n        read_return = read_config(['access_gated', 'access_token', 'model_id',  'gguf', 'awq', 'gguf_model_id', 'gguf_filename', 'quantize', 'quant_level', 'hqq_group_size', 'push_to_hub', 'torch_device_map', 'torch_dtype', 'trust_remote_code', 'use_flash_attention_2', 'pipeline_task', 'max_new_tokens', 'return_full_text', 'temperature', 'do_sample', 'top_k', 'top_p', 'min_p', 'n_keep', 'port'])\r\n        access_gated = str(read_return['access_gated']).lower() == 'true'\r\n        access_token = str(read_return['access_token'])\r\n        model_id = str(read_return['model_id'])\r\n        quantize = str(read_return['quantize'])\r\n        quant_level = str(read_return['quant_level'])\r\n        hqq_group_size = int(read_return['hqq_group_size'])\r\n        push_to_hub = str(read_return['push_to_hub']).lower() == 'true'\r\n        torch_device_map = str(read_return['torch_device_map'])\r\n        torch_dtype = str(read_return['torch_dtype'])\r\n        trust_remote_code = str(read_return['trust_remote_code']).lower() == 'true'\r\n        pipeline_task = str(read_return['pipeline_task'])\r\n        max_new_tokens = int(read_return['max_new_tokens'])\r\n        return_full_text = str(read_return['return_full_text']).lower() == 'true'\r\n        temperature = float(read_return['temperature'])\r\n        do_sample = str(read_return['do_sample']).lower() == 'true'\r\n        top_k = int(read_return['top_k'])\r\n        top_p = float(read_return['top_p'])\r\n        min_p = float(read_return['min_p'])\r\n        n_keep = int(read_return['n_keep'])\r\n        port = int(read_return['port'])\r\n    except Exception as e:\r\n        handle_local_error(\"Could not read values from hf_config.json when trying to parse_arguments(), encountered error: \", e)\r\n\r\n    if parser:\r\n\r\n        parser.add_argument(\"--reset_to_defaults\", action=\"store_true\", default=False, help=\"Use default settings\")\r\n        parser.add_argument(\"--access_gated\", action=\"store_true\", default=access_gated, help=\"Specify True if you will be accessing gated models you've been approved to access\")\r\n        parser.add_argument(\"--access_token\", type=str, default=access_token, help=\"Access Token obtained from HF-Settings -> Access Tokens\")\r\n        parser.add_argument(\"--model_id\", type=str, default=model_id, help=\"model_id for for LLM in HF-Transformers format obtained from the model card. Remembers previously set value and falls-back to Phi3-mini-4k-instruct as the default.\")\r\n        parser.add_argument(\"--gguf\", action=\"store_true\", default=False, help=\"Add this flag if you'll be loading a GGUF LLM. Defaults to False.\")\r\n        parser.add_argument(\"--awq\", action=\"store_true\", default=False, help=\"Add this flag when loading AWQ-quantized models directly off the HF-Hub.\")\r\n        parser.add_argument(\"--gguf_model_id\", type=str, default=None, help=\"GGUF model_id of the target repo. Defaults to None\")\r\n        parser.add_argument(\"--gguf_filename\", type=str, default=None, help=\"GGUF filename from the target repo. Defaults to None\")\r\n        parser.add_argument(\"--quantize\", type=str, default=quantize, help=\"Quantization method to be utilized. Simply type 'n' to not use quantization. Remembers previously set value and falls-back to bitsandbytes as the default.\")\r\n        parser.add_argument(\"--quant_level\", type=str, default=quant_level, help=\"Specify quantization level. Valid values -  BitsAndBytes: int8 & int4; Quanto: int8, int4 and int2; HQQ: int8, int4, int3, int2, int1. Remembers previously set value and falls-back to int8 as the default.\")\r\n        parser.add_argument(\"--hqq_group_size\", type=int, default=hqq_group_size, help=\"Specify group_size for HQQ quantization. No restrictions as long as weight.numel() is divisible by the group_size. Remembers previously set value and falls-back to 64 as a default.\")\r\n        parser.add_argument(\"--push_to_hub\", action=\"store_true\", default=push_to_hub, help=\"Push quantized LLM to your HF-hub. Remembers previously set value and falls-back to False as the default.\")\r\n        parser.add_argument(\"--torch_device_map\", type=str, default=torch_device_map, help=\"Specify inference device, example: cuda. Remembers previously set value and falls-back to auto as the default.\")\r\n        parser.add_argument(\"--torch_dtype\", type=str, default=torch_dtype, help=\"Specify model tensor type, example: bfloat16. Remembers previously set value and falls-back to auto as the default.\")\r\n        parser.add_argument(\"--trust_remote_code\", action=\"store_true\", default=trust_remote_code, help=\"Allows the model to execute custom code that's part of the model's HF-repository. Remembers previously set value and falls-back to False by default as a security measure to prevent potentially malicious code from running automatically.\")\r\n        parser.add_argument(\"--use_flash_attention_2\", action=\"store_true\", default=False, help=\"Set to True to attempt using Flash Attention 2. Defaults to False. Failed attempt to use FA2 will proceed to load the model without FA2.\")\r\n        parser.add_argument(\"--pipeline_task\", type=str, default=pipeline_task, help=\"Defaults to text-generation. For more details, open a Python shell, `import transformers`, and Run `help(transfomers.pipeline)`.\")\r\n        parser.add_argument(\"--max_new_tokens\", type=int, default=max_new_tokens, help=\"Set a hard limit on the maximum number of tokens an LLM can generate when responding. Remembers previously set value and falls-back to 500 as a default.\")\r\n        parser.add_argument(\"--return_full_text\", action=\"store_true\", default=return_full_text, help=\"When set to True, the LLM response contains the entire messages list with the latest response appended at the end.\")\r\n        parser.add_argument(\"--temperature\", type=float, default=temperature, help=\"Set LLM temperature on a scale of 0.0 to 2.0. Remembers previously set value and falls-back to 0.0 as a default.\")\r\n        parser.add_argument(\"--do_sample\", action=\"store_true\", default=do_sample, help=\"Perform sampling when selecting response tokens. Remembers previously set value and falls-back to Flase as a default. Must be set to True when temperature is above 0.0. For greedy decoding, leave this as False and set temp to 0.0\")\r\n        parser.add_argument(\"--top_k\", type=int, default=top_k, help=\"Limit the next token selection to the K most probable tokens. Remembers previously set value and falls-back to 40 as a default.\")\r\n        parser.add_argument(\"--top_p\", type=float, default=top_p, help=\"Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P. Remembers previously set value and falls-back to 0.95 as a default.\")\r\n        parser.add_argument(\"--min_p\", type=float, default=min_p, help=\"The minimum probability for a token to be considered, relative to the probability of the most likely token. Remembers previously set value and falls-back to 0.05 as a default.\")\r\n        parser.add_argument(\"--n_keep\", type=int, default=n_keep, help=\"Specify the number of tokens from the prompt to retain when the context size is exceeded and tokens need to be discarded. Remembers previously set value and falls-back to 0 as a default, meaning no tokens are kept. Use -1 to retain all tokens from the prompt.\")\r\n        parser.add_argument(\"--port\", type=int, default=port, help=\"Specify the port to be used by the server. Remembers previously set value and falls-back to 9069 as a default.\")\r\n\r\n        args = parser.parse_args()\r\n        print(f\"\\n\\nparser.parse_args():\\n\\n{args}\\n\\n\")\r\n\r\n        if args.reset_to_defaults:\r\n\r\n            try:\r\n                # Empty hf_config.json\r\n                config_writer_semaphore.acquire()\r\n                with open('hf_config.json', 'w') as file:\r\n                    json.dump({}, file, indent=4)\r\n                config_writer_semaphore.release()\r\n                \r\n                # Set defaults\r\n                read_config(['access_gated', 'access_token', 'model_id',  'gguf', 'awq', 'gguf_model_id', 'gguf_filename', 'quantize', 'quant_level', 'hqq_group_size', 'push_to_hub', 'torch_device_map', 'torch_dtype', 'trust_remote_code', 'use_flash_attention_2', 'pipeline_task', 'max_new_tokens', 'return_full_text', 'temperature', 'do_sample', 'top_k', 'top_p', 'min_p', 'n_keep', 'port'])\r\n\r\n            except Exception as e:\r\n                handle_local_error(\"Could not reset hf_config.json, encountered error: \", e)\r\n        else:\r\n            try:\r\n                write_config({\r\n                    'access_gated':args.access_gated,\r\n                    'access_token':args.access_token,\r\n                    'model_id':args.model_id,\r\n                    'gguf':args.gguf,\r\n                    'awq':args.awq,\r\n                    'gguf_model_id':args.gguf_model_id,\r\n                    'gguf_filename':args.gguf_filename,\r\n                    'quantize':args.quantize,\r\n                    'quant_level':args.quant_level,\r\n                    'hqq_group_size':args.hqq_group_size,\r\n                    'push_to_hub':args.push_to_hub, \r\n                    'torch_device_map':args.torch_device_map, \r\n                    'torch_dtype':args.torch_dtype, \r\n                    'trust_remote_code':args.trust_remote_code, \r\n                    'use_flash_attention_2':args.use_flash_attention_2, \r\n                    'pipeline_task':args.pipeline_task, \r\n                    'max_new_tokens':args.max_new_tokens, \r\n                    'return_full_text':args.return_full_text, \r\n                    'temperature':args.temperature,\r\n                    'do_sample':args.do_sample, \r\n                    'top_k':args.top_k, \r\n                    'top_p':args.top_p, \r\n                    'min_p':args.min_p, \r\n                    'n_keep':args.n_keep,\r\n                    'port':args.port\r\n                })\r\n            except Exception as e:\r\n                handle_local_error(\"Could not write launch arguments to hf_config.json, encountered error: \", e)\r\n\r\n            if args.access_gated:\r\n                try:\r\n                    hf_login_for_gated_models()\r\n                except Exception as e:\r\n                    handle_local_error(\"Login to HF-Hub unsuccessful, encountered error: \", e)\r\n\r\n        return args\r\n\r\n    # Return None if parser was not created\r\n    return None\r\n\r\n\r\ndef str_to_torch_dtype(dtype_str):\r\n\r\n    print(f\"\\n\\nstr_to_torch_dtype({dtype_str})\\n\\n\")\r\n\r\n    dtype_map = {\r\n        \"torch.float16\": torch.float16,\r\n        \"torch.float32\": torch.float32,\r\n        \"torch.float64\": torch.float64,\r\n        \"torch.int8\": torch.int8,\r\n        \"torch.int16\": torch.int16,\r\n        \"torch.int32\": torch.int32,\r\n        \"torch.int64\": torch.int64,\r\n        \"torch.uint8\": torch.uint8,\r\n        \"torch.bool\": torch.bool,\r\n        \"torch.bfloat16\": torch.bfloat16,\r\n        \"auto\":\"auto\"\r\n    }\r\n    return dtype_map.get(dtype_str, None)\r\n\r\n\r\ndef initialize_model():\r\n\r\n    print(\"\\n\\ninitializing model\\n\\n\")\r\n\r\n    global PIPE\r\n\r\n    try:\r\n        read_return = read_config(['model_id', 'gguf', 'awq', 'gguf_model_id', 'gguf_filename', 'quantize', 'quant_level', 'hqq_group_size', 'push_to_hub', 'torch_device_map', 'torch_dtype', 'trust_remote_code', 'use_flash_attention_2', 'pipeline_task'])\r\n        model_id = str(read_return['model_id'])\r\n        gguf = str(read_return['gguf']).lower() == 'true'\r\n        awq = str(read_return['awq']).lower() == 'true'\r\n        gguf_model_id = str(read_return['gguf_model_id'])\r\n        gguf_filename = str(read_return['gguf_filename'])\r\n        quantize = str(read_return['quantize'])\r\n        quant_level = str(read_return['quant_level'])\r\n        hqq_group_size = int(read_return['hqq_group_size'])\r\n        push_to_hub = str(read_return['push_to_hub']).lower() == 'true'\r\n        torch_device_map = str(read_return['torch_device_map'])\r\n        torch_dtype = str(read_return['torch_dtype'])\r\n        trust_remote_code = str(read_return['trust_remote_code']).lower() == 'true'\r\n        use_flash_attention_2 = str(read_return['use_flash_attention_2']).lower() == 'true'\r\n        pipeline_task = str(read_return['pipeline_task'])\r\n    except Exception as e:\r\n        handle_local_error(\"Could not read values from hf_config.json when trying to parse_arguments(), encountered error: \", e)\r\n\r\n    if gguf:\r\n        print(gguf)\r\n        print(\"\\n\\nLoading GGUF\\n\\n\")\r\n        try:\r\n            model = AutoModelForCausalLM.from_pretrained(gguf_model_id, gguf_file=gguf_filename)\r\n        except Exception as e:\r\n            handle_local_error(\"Could not create AutoModelForCausalLM, encountered error: \", e)\r\n        try:\r\n            tokenizer = AutoTokenizer.from_pretrained(gguf_model_id, gguf_file=gguf_filename)\r\n        except Exception as e:\r\n            handle_local_error(\"Could not set AutoTokenizer, encountered error: \", e)\r\n        try:\r\n            PIPE = pipeline(\r\n                pipeline_task,\r\n                model=model,\r\n                tokenizer=tokenizer,\r\n            )\r\n        except Exception as e:\r\n            handle_local_error(\"Could not create model PIPELINE, encountered error: \", e)\r\n\r\n        return True\r\n\r\n    if awq:\r\n        print(\"Proceed to load AWQ-quantized model from the HF-Hub, setting torch_dtype=torch.float16 and quantize=n and proceeding.\")\r\n        torch_dtype_obj = torch.float16\r\n        quantize = \"n\"\r\n    else:\r\n        try:\r\n            torch_dtype_obj = str_to_torch_dtype(torch_dtype)\r\n        except Exception as e:\r\n            handle_error_no_return(\"Error determining torch data-type, setting to auto and proceeding: \", e)\r\n            torch_dtype_obj = \"auto\"\r\n        if torch_dtype_obj is None:\r\n            handle_error_no_return(\"Could not obtain torch dtype object, check if the value passed is correct. Setting to auto and proceeding.\")\r\n            torch_dtype_obj = \"auto\"\r\n\r\n    model_params = {\r\n        \"device_map\": torch_device_map,\r\n        \"torch_dtype\": torch_dtype_obj,\r\n        \"trust_remote_code\": trust_remote_code,\r\n    }\r\n\r\n    if use_flash_attention_2:\r\n        model_params[\"attn_implementation\"] = \"flash_attention_2\"\r\n\r\n    quantize = quantize.lower().strip()\r\n    if quantize != \"n\":\r\n\r\n        if quantize == \"bitsandbytes\":\r\n            print(\"Quantizing with BitsAndBytes\")\r\n            quant_level = quant_level.lower().strip()\r\n\r\n            try:\r\n                if quant_level == \"int8\":\r\n                    print(\"Proceeding with BitsAndBytes-Int8 Quant\")\r\n                    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\r\n                    model_params[\"quantization_config\"] = quantization_config\r\n                elif quant_level == \"int4\":\r\n                    print(\"Proceeding with BitsAndBytes-Int4 Quant\")\r\n                    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\r\n                    model_params[\"quantization_config\"] = quantization_config\r\n                else:\r\n                    print(f\"Invalid quant_level setting, BitsAndBytes supports only int8 and int4 quants but you set {quant_level}; proceeding with BitsAndBytes-Int4 Quant\")\r\n                    print(\"Proceeding with BitsAndBytes-Int4 Quant\")\r\n                    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\r\n                    model_params[\"quantization_config\"] = quantization_config\r\n            except Exception as e:\r\n                handle_local_error(\"Could not set BitsAndBytes config to initialize_model(), encountered error: \", e)\r\n        elif quantize == \"quanto\":\r\n            print(\"Quanto-Quantizing\")\r\n            quant_level = quant_level.lower().strip()\r\n\r\n            if quant_level == \"int8\":\r\n                print(\"Proceeding with Quanto-Int8 Weights\")\r\n                quantization_config  = QuantoConfig(weights=\"int8\")\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int4\":\r\n                print(\"Proceeding with Quanto-Int4 Weights\")\r\n                quantization_config  = QuantoConfig(weights=\"int4\")\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int2\":\r\n                print(\"Proceeding with Quanto-Int2 Weights\")\r\n                quantization_config  = QuantoConfig(weights=\"int2\")\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            else:\r\n                print(f\"Invalid quant_level setting, Quanto supports only int8, int4 and int2 quants but you set {quant_level}; proceeding with Quanto-Int4 Quant\")\r\n                quantization_config  = QuantoConfig(weights=\"int4\")\r\n                model_params[\"quantization_config\"] = quantization_config\r\n        elif quantize == \"hqq\":\r\n            print(\"HQQ-Quantizing - Force-setting torch_dtype to torch.bfloat16\")\r\n            model_params[\"torch_dtype\"] = torch.bfloat16\r\n            quant_level = quant_level.lower().strip()\r\n\r\n            if quant_level == \"int8\":\r\n                print(\"Proceeding with HQQ-Int8 Weights\")\r\n                quantization_config  = HqqConfig(nbits=8, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int4\":\r\n                print(\"Proceeding with HQQ-Int4 Weights\")\r\n                quantization_config  = HqqConfig(nbits=4, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int3\":\r\n                print(\"Proceeding with HQQ-Int3 Weights\")\r\n                quantization_config  = HqqConfig(nbits=3, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int2\":\r\n                print(\"Proceeding with HQQ-Int2 Weights\")\r\n                quantization_config  = HqqConfig(nbits=2, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            elif quant_level == \"int1\":\r\n                print(\"Proceeding with HQQ-Int1 Weights\")\r\n                quantization_config  = HqqConfig(nbits=1, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n            else:\r\n                print(f\"Invalid quant_level setting, HQQ supports int8, int4, int3, int2 & int1 quants but you set {quant_level}; proceeding with HQQ-Int4 Quant\")\r\n                quantization_config  = HqqConfig(nbits=4, group_size=hqq_group_size, quant_zero=False, quant_scale=False, axis=0)\r\n                model_params[\"quantization_config\"] = quantization_config\r\n\r\n    print(f\"model_params: {model_params}\")\r\n\r\n    try:\r\n        model = AutoModelForCausalLM.from_pretrained(model_id, **model_params)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not create AutoModelForCausalLM, encountered error: \", e)\r\n\r\n    try:\r\n        print(f\"Your model's memory footprint is: {model.get_memory_footprint()}\")\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not determine the model's memory footprint, encountered error: \", e)\r\n\r\n    try:\r\n        if push_to_hub:\r\n            if quant_level == \"int8\":\r\n                model.push_to_hub(model_id + \"-Int8\")\r\n            elif quant_level == \"int4\":\r\n                model.push_to_hub(model_id + \"-Int4\")\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not push the model to your hub, encountered error: \", e)\r\n\r\n    try:\r\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\r\n    except Exception as e:\r\n        handle_local_error(\"Could not set AutoTokenizer, encountered error: \", e)\r\n\r\n    try:\r\n        PIPE = pipeline(\r\n            pipeline_task,\r\n            model=model,\r\n            tokenizer=tokenizer,\r\n        )\r\n    except Exception as e:\r\n        handle_local_error(\"Could not create model PIPELINE, encountered error: \", e)\r\n\r\n    return True\r\n\r\n\r\ndef empty_cuda_cache():\r\n    print(\"\\n\\nEmptying CUDA cache\\n\\n\")\r\n    # check if torch.cuda is available\r\n    if torch.cuda.is_available():\r\n        try:\r\n            torch.cuda.empty_cache()\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not empty cuda cache, encountered error: \", e)\r\n    else:\r\n        print(\"\\n\\nCUDA is not available, skipping cache-emptying\\n\\n\")\r\n        return\r\n\r\n\r\n@app.route('/completions', methods=['POST'])\r\ndef completions():\r\n\r\n    print(\"\\n\\ncompletions route triggered - attempting to acquire LLM semaphore\\n\\n\")\r\n\r\n    with llm_semaphore:\r\n\r\n        print(\"\\n\\nLLM semaphore acquired by /completions\\n\\n\")\r\n\r\n        try:\r\n            data = request.json\r\n            messages = data.get('messages', [])\r\n        except Exception as e:\r\n            handle_api_error(\"Could not read POST-request messages for /completions, encountered error: \", e)\r\n\r\n        try:\r\n            read_return = read_config(['max_new_tokens', 'return_full_text', 'temperature', 'do_sample', 'top_k', 'top_p', 'min_p', 'n_keep'])\r\n            max_new_tokens = int(read_return['max_new_tokens'])\r\n            return_full_text = str(read_return['return_full_text']).lower() == 'true'\r\n            temperature = float(read_return['temperature'])\r\n            do_sample = str(read_return['do_sample']).lower() == 'true'\r\n            top_k = int(read_return['top_k'])\r\n            top_p = float(read_return['top_p'])\r\n            min_p = float(read_return['min_p'])\r\n            n_keep = int(read_return['n_keep'])\r\n        except Exception as e:\r\n            handle_local_error(\"Could not read values from hf_config.json when trying to parse_arguments(), encountered error: \", e)\r\n\r\n        try:\r\n            generation_args = {\r\n                \"max_new_tokens\": int(request.headers.get('X-Max-New-Tokens', str(max_new_tokens))),\r\n                \"return_full_text\": request.headers.get('X-Return-Full-Text', str(return_full_text)).lower() == 'true',\r\n                \"temperature\": float(request.headers.get('X-Temperature', str(temperature))),\r\n                \"do_sample\": request.headers.get('X-Do-Sample', str(do_sample)).lower() == 'true',\r\n                \"top_k\": int(request.headers.get('X-Top-K', str(top_k))),\r\n                \"top_p\": float(request.headers.get('X-Top-P', str(top_p))),\r\n                \"min_p\": float(request.headers.get('X-Min-P', str(min_p)))\r\n            }\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not set generation-arguments for /completions, proceeding without them. Encountered error: \", e)\r\n\r\n        try:\r\n            if generation_args:\r\n                output = PIPE(messages, **generation_args)\r\n            else:\r\n                output = PIPE(messages)\r\n        except Exception as e:\r\n            handle_api_error(\"Could not generate output, encountered error: \", e)\r\n\r\n        print(\"\\n\\nCompletions done - releasing LLM semaphore\\n\\n\")\r\n\r\n        try:\r\n            empty_cuda_cache()\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not empty cuda cache, encountered error: \", e)\r\n\r\n        return jsonify({\"success\": True, \"response\": output})\r\n\r\n\r\n\r\nclass CustomTextStreamer(TextStreamer):\r\n    def __init__(self, tokenizer, skip_special_tokens=True, skip_prompt=True, **kwargs):\r\n        super().__init__(tokenizer, skip_special_tokens=skip_special_tokens, skip_prompt=skip_prompt, **kwargs)\r\n        self.callback = None\r\n        self.buffer = io.StringIO()\r\n\r\n    def on_finalized_text(self, text: str, stream_end: bool = False):\r\n        if self.callback:\r\n            self.callback(text)\r\n        return self.buffer.write(text)\r\n    \r\n    def flush(self):\r\n        self.buffer.flush()\r\n\r\n\r\n@app.route('/completions_stream', methods=['POST'])\r\ndef completions_stream():\r\n\r\n    print(\"\\n\\ncompletions_stream route triggered - attempting to acquire LLM semaphore\\n\\n\")\r\n\r\n    llm_semaphore.acquire()\r\n\r\n    print(\"\\n\\nLLM semaphore acquired by /completions_stream\\n\\n\")\r\n\r\n    try:\r\n        data = request.json\r\n        messages = data.get('messages', [])\r\n    except Exception as e:\r\n        handle_api_error(\"Could not read POST-request messages for /completions_stream, encountered error: \", e)\r\n\r\n    try:\r\n        read_return = read_config(['max_new_tokens', 'return_full_text', 'temperature', 'do_sample', 'top_k', 'top_p', 'min_p', 'n_keep'])\r\n        max_new_tokens = int(read_return['max_new_tokens'])\r\n        return_full_text = str(read_return['return_full_text']).lower() == 'true'\r\n        temperature = float(read_return['temperature'])\r\n        do_sample = str(read_return['do_sample']).lower() == 'true'\r\n        top_k = int(read_return['top_k'])\r\n        top_p = float(read_return['top_p'])\r\n        min_p = float(read_return['min_p'])\r\n        n_keep = int(read_return['n_keep'])\r\n    except Exception as e:\r\n        handle_local_error(\"Could not read values from hf_config.json when trying to parse_arguments(), encountered error: \", e)\r\n\r\n    try:\r\n        generation_args = {\r\n            \"max_new_tokens\": int(request.headers.get('X-Max-New-Tokens', str(max_new_tokens))),\r\n            \"return_full_text\": request.headers.get('X-Return-Full-Text', str(return_full_text)).lower() == 'true',\r\n            \"temperature\": float(request.headers.get('X-Temperature', str(temperature))),\r\n            \"do_sample\": request.headers.get('X-Do-Sample', str(do_sample)).lower() == 'true',\r\n            \"top_k\": int(request.headers.get('X-Top-K', str(top_k))),\r\n            \"top_p\": float(request.headers.get('X-Top-P', str(top_p))),\r\n            \"min_p\": float(request.headers.get('X-Min-P', str(min_p)))\r\n        }\r\n    except Exception as e:\r\n        handle_error_no_return(\"Could not set generation-arguments for /completions_stream, proceeding without them. Encountered error: \", e)\r\n\r\n\r\n    stop_thread = threading.Event()\r\n\r\n    data_queue = queue.Queue()\r\n\r\n    def callback(data):\r\n        data_queue.put(data)\r\n\r\n    custom_streamer = CustomTextStreamer(PIPE.tokenizer, skip_special_tokens=True, skip_prompt=True)\r\n    custom_streamer.callback = callback\r\n\r\n    def llm_task():\r\n\r\n        global PIPE\r\n\r\n        try:                \r\n            if generation_args:\r\n                generation_args[\"streamer\"] = custom_streamer\r\n                output = PIPE(messages, **generation_args)\r\n            else:\r\n                output = PIPE(messages, streamer=custom_streamer)\r\n        finally:\r\n            data_queue.put(None)\r\n            print(\"\\n\\nLLM stream done, releasing semaphore\\n\\n\")\r\n            llm_semaphore.release()\r\n            stop_thread.set()\r\n\r\n    def generate():        \r\n        \r\n        thread = threading.Thread(target=llm_task)\r\n        thread.start()\r\n\r\n        while True:\r\n            line = data_queue.get()\r\n            if line is None:\r\n                print(\"\\n\\nNone read, breaking and stopping thread\\n\\n\")\r\n                thread.join()\r\n                break\r\n            yield f\"data: {json.dumps(line)}\\n\\n\"\r\n        \r\n        yield f\"event: END\\ndata: \\\"null\\\"\\n\\n\"\r\n\r\n        try:\r\n            empty_cuda_cache()\r\n        except Exception as e:\r\n            handle_error_no_return(\"Could not empty cuda cache, encountered error: \", e)\r\n            \r\n    print(\"\\n\\nInferencing Begins!\\n\\n\")\r\n    return Response(generate(), content_type='text/event-stream')\r\n\r\n\r\n@app.route('/health')\r\ndef health():\r\n\r\n    def throw_health_check_error(attrib, e):\r\n        handle_error_no_return(f\"Could not determine {attrib} while checking HF-Waitress server health. This is not a critical error and the HF-Waitress LLM server is online! More details of the issue encountered follows: \", e)\r\n        return True\r\n\r\n    print(\"\\n\\nHF-Waitress LLM health-check in-progress...\\n\\n\")\r\n    \r\n    with reader_semaphore:\r\n    \r\n        try:\r\n            if PIPE is None:\r\n                return jsonify(status=\"error\", message=\"Model not loaded\"), 503 # Service Unavailable\r\n            \r\n            model_info = {}\r\n\r\n            # print(f\"\\n\\nmodel details: {PIPE.model}\\n\\n\")\r\n            # print(f\"\\n\\nmodel.config details: {PIPE.model.config}\\n\\n\")\r\n            # print(f\"\\n\\ntokenizer details: {PIPE.tokenizer}\\n\\n\")\r\n            \r\n            try:\r\n                model_info[\"model_id\"] = str(PIPE.model.config._name_or_path)\r\n            except Exception as e:\r\n                throw_health_check_error(\"model_id\", e)\r\n\r\n            try:\r\n                model_info[\"transformers_version\"] = str(PIPE.model.config.transformers_version)\r\n            except Exception as e:\r\n                throw_health_check_error(\"transformers_version\", e)\r\n\r\n            try:\r\n                model_info[\"architecture\"] = str(PIPE.model.config.architectures)\r\n            except Exception as e:\r\n                throw_health_check_error(\"model architecture\", e)\r\n\r\n            try:\r\n                model_info[\"model_type\"] = str(PIPE.model.config.model_type)\r\n            except Exception as e:\r\n                throw_health_check_error(\"model_type\", e)\r\n\r\n            try:\r\n                model_info[\"torch_dtype\"] = str(PIPE.model.config.torch_dtype)\r\n            except Exception as e:\r\n                throw_health_check_error(\"torch_dtype\", e)\r\n\r\n            try:\r\n                model_info[\"device\"] = str(PIPE.device)\r\n            except Exception as e:\r\n                throw_health_check_error(\"inference device\", e)\r\n\r\n            try:\r\n                if hasattr(PIPE.model.config, \"quantization_config\"):\r\n                    model_info[\"is_quantized\"] = True\r\n                    model_info[\"quant_method\"] = str(PIPE.model.config.quantization_config.quant_method)\r\n                    model_info[\"quantization_config\"] = str(PIPE.model.config.quantization_config)\r\n                else:\r\n                    model_info[\"is_quantized\"] = False\r\n            except Exception as e:\r\n                throw_health_check_error(\"quantization status\", e)\r\n\r\n            try:\r\n                model_info[\"memory_footprint\"] = str(PIPE.model.get_memory_footprint())\r\n            except Exception as e:\r\n                throw_health_check_error(\"memory_footprint\", e)\r\n\r\n            try:\r\n                model_info[\"model_vocab_size\"] = str(PIPE.model.config.vocab_size)\r\n            except Exception as e:\r\n                throw_health_check_error(\"model_vocab_size\", e)\r\n                try:\r\n                    model_info[\"tokenizer_vocab_length\"] = len(PIPE.tokenizer)\r\n                except Exception as e:\r\n                    throw_health_check_error(\"tokenizer_vocab_length\", e)\r\n            \r\n            try:\r\n                model_info[\"tokenizer_vocab_size\"] = str(PIPE.tokenizer.vocab_size)\r\n            except Exception as e:\r\n                throw_health_check_error(\"tokenizer_vocab_size\", e)\r\n            \r\n            try:\r\n                model_info[\"number_of_hidden_layers\"] = str(PIPE.model.config.num_hidden_layers)\r\n            except Exception as e:\r\n                throw_health_check_error(\"number_of_hidden_layers\", e)\r\n            \r\n            try:\r\n                model_info[\"number_of_attention_heads\"] = str(PIPE.model.config.num_attention_heads)\r\n            except Exception as e:\r\n                throw_health_check_error(\"number_of_attention_heads\", e)\r\n\r\n            try:\r\n                model_info[\"hidden_dimensions\"] = str(PIPE.model.config.head_dim)\r\n            except Exception as e:\r\n                throw_health_check_error(\"hidden_dimensions\", e)\r\n\r\n            try:\r\n                model_info[\"number_of_key_value_heads\"] = str(PIPE.model.config.num_key_value_heads)\r\n            except Exception as e:\r\n                throw_health_check_error(\"number_of_key_value_heads\", e)\r\n            \r\n            try:\r\n                model_info[\"hidden_activation\"] = str(PIPE.model.config.hidden_act)\r\n            except Exception as e:\r\n                throw_health_check_error(\"hidden_activation\", e)\r\n            \r\n            try:\r\n                model_info[\"hidden_size\"] = str(PIPE.model.config.hidden_size)\r\n            except Exception as e:\r\n                throw_health_check_error(\"hidden_size\", e)\r\n\r\n            try:\r\n                model_info[\"intermediate_size\"] = str(PIPE.model.config.intermediate_size)\r\n            except Exception as e:\r\n                throw_health_check_error(\"intermediate_size\", e)\r\n\r\n            try:\r\n                model_info[\"max_position_embeddings\"] = str(PIPE.model.config.max_position_embeddings)\r\n            except Exception as e:\r\n                throw_health_check_error(\"max_position_embeddings\", e)\r\n\r\n            try:\r\n                model_info[\"tokenizer\"] = str(PIPE.tokenizer.name_or_path)\r\n            except Exception as e:\r\n                throw_health_check_error(\"tokenizer\", e)\r\n\r\n            try:\r\n                model_info[\"max_seq_length\"] = str(PIPE.tokenizer.model_max_length)\r\n            except Exception as e:\r\n                throw_health_check_error(\"max_seq_length\", e)\r\n\r\n            print(f\"HF-Waitress LLM-server health-check completed successfully, returning.\\n\")\r\n            return jsonify(status=\"ok\", model_info=model_info), 200\r\n\r\n        except Exception as e:\r\n            handle_api_error(\"Error checking hf-server health, encountered error: \", e)\r\n\r\n\r\n@app.route('/restart_server')\r\ndef restart_server():\r\n    \r\n    with llm_semaphore:\r\n        print(\"\\n\\n/restart_server acquired llm_semaphore, proceeding...\\n\\n\")\r\n        with config_writer_semaphore:\r\n            print(\"\\n\\n/restart_server acquired config_writer_semaphore, proceeding...\\n\\n\")\r\n            with error_logging_semaphore:\r\n                print(\"\\n\\n/restart_server acquired error_logging_semaphore, proceeding...\\n\\n\")\r\n    \r\n                global PIPE\r\n\r\n                try:\r\n                    PIPE = None\r\n                    initialize_model()\r\n                except Exception as e:\r\n                    handle_api_error(\"Could not restart server, encountered error: \", e)\r\n                \r\n                return jsonify(success=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = parse_arguments()\r\n    initialize_model()\r\n    port = getattr(args, 'port', 9069)\r\n    serve(app, host='0.0.0.0', port=port)"}
{"type": "source_file", "path": "web_app/unoconv.py", "content": "#!/usr/bin/env python\n\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation: version 2 only.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY, without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n# Copyright 2007-2010 Dag Wieers <dag@wieers.com>\n\nfrom __future__ import print_function\n\nfrom distutils.version import LooseVersion\nimport getopt\nimport glob\nimport os\nimport signal\nimport subprocess\nimport sys\nimport time\n\n__version__ = '0.8.2'\n\ndoctypes = ('document', 'graphics', 'presentation', 'spreadsheet')\n\nglobal convertor, office, ooproc, product\nooproc = None\nuno = unohelper = None\nexitcode = 0\n\n\nclass Office:\n    def __init__(self, basepath, urepath, unopath, pyuno, binary, python, pythonhome):\n        self.basepath = basepath\n        self.urepath = urepath\n        self.unopath = unopath\n        self.pyuno = pyuno\n        self.binary = binary\n        self.python = python\n        self.pythonhome = pythonhome\n\n    def __str__(self):\n        return self.basepath\n\n    def __repr__(self):\n        return self.basepath\n\n\n# Implement a path normalizer to make unoconv work on MacOS X, on\n# which 'program' is a symlink to 'MacOSX,' which seems to break unoconv.\ndef realpath(*args):\n    \"\"\"Implement a combination of os.path.join(), os.path.abspath() and\n        os.path.realpath() in order to normalize path constructions.\"\"\"\n    ret = ''\n    for arg in args:\n        ret = os.path.join(ret, arg)\n    return os.path.realpath(os.path.abspath(ret))\n\n\n# The first thing we should do is find a suitable Office installation\n# with a compatible pyuno library that we can import.\n#\n# See: http://user.services.openoffice.org/en/forum/viewtopic.php?f=45&t=36370&p=166783\ndef find_offices():\n    ret = []\n    extrapaths = []\n\n    # Try using UNO_PATH first (in many incarnations, we'll see what sticks).\n    if 'UNO_PATH' in os.environ:\n        extrapaths += [os.environ['UNO_PATH'],\n                       os.path.dirname(os.environ['UNO_PATH']),\n                       os.path.dirname(os.path.dirname(os.environ['UNO_PATH']))]\n    else:\n        if os.name in ('nt', 'os2'):\n            if 'PROGRAMFILES' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMFILES(X86)' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMFILES(X86)']+'\\\\OpenOffice.org*')\n\n            if 'PROGRAMW6432' in list(os.environ.keys()):\n                extrapaths += glob.glob(os.environ['PROGRAMW6432']+'\\\\LibreOffice*') + \\\n                              glob.glob(os.environ['PROGRAMW6432']+'\\\\OpenOffice.org*')\n\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            extrapaths += ['/Applications/LibreOffice.app/Contents',\n                           '/Applications/NeoOffice.app/Contents',\n                           '/Applications/OpenOffice.app/Contents',\n                           '/Applications/OpenOffice.org.app/Contents']\n\n        else:\n            extrapaths += glob.glob('/usr/lib*/libreoffice*') + \\\n                          glob.glob('/usr/lib*/openoffice*') + \\\n                          glob.glob('/usr/lib*/ooo*') + \\\n                          glob.glob('/opt/libreoffice*') + \\\n                          glob.glob('/opt/openoffice*') + \\\n                          glob.glob('/opt/ooo*') + \\\n                          glob.glob('/usr/local/libreoffice*') + \\\n                          glob.glob('/usr/local/openoffice*') + \\\n                          glob.glob('/usr/local/ooo*') + \\\n                          glob.glob('/usr/local/lib/libreoffice*')\n\n    # Find a working set for python UNO bindings.\n    for basepath in extrapaths:\n        if os.name in ('nt', 'os2'):\n            officelibraries = ('pyuno.pyd',)\n            officebinaries = ('soffice.exe',)\n            pythonbinaries = ('python.exe',)\n            pythonhomes = ()\n        elif os.name == 'mac' or sys.platform == 'darwin':\n            officelibraries = ('pyuno.so', 'libpyuno.dylib')\n            officebinaries = ('soffice.bin', 'soffice')\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('OOoPython.framework/Versions/*/lib/python*')\n        else:\n            officelibraries = ('pyuno.so',)\n            officebinaries = ('soffice.bin',)\n            pythonbinaries = ('python.bin', 'python')\n            pythonhomes = ('python-core-*',)\n\n        # Older LibreOffice/OpenOffice and Windows use basis-link/ or basis/\n        libpath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for lib in officelibraries:\n                for libdir in ('program', 'Frameworks'):\n                    if os.path.isfile(realpath(basepath, basis, libdir, lib)):\n                        libpath = realpath(basepath, basis, libdir)\n                        officelibrary = realpath(libpath, lib)\n                        info(3, \"Found %s in %s\" % (lib, libpath))\n                        # Break the inner loop...\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken, break the outer.\n            break\n        else:\n            continue\n\n        # MacOSX has soffice binaries installed in MacOS subdirectory, not program.\n        unopath = 'error'\n        for basis in ('basis-link', 'basis', ''):\n            for bin in officebinaries:\n                for bindir in ('program', '', 'MacOS'):\n                    if os.path.isfile(realpath(basepath, basis, bindir, bin)):\n                        unopath = realpath(basepath, basis, bindir)\n                        officebinary = realpath(unopath, bin)\n                        info(3, \"Found %s in %s\" % (bin, unopath))\n                        # Break the inner loop.\n                        break\n                # Continue if the inner loop wasn't broken.\n                else:\n                    continue\n                break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n        else:\n            continue\n\n        # Windows does not provide or need a URE/lib directory?\n        urepath = ''\n        for basis in ('basis-link', 'basis', ''):\n            for ure in ('ure-link', 'ure', 'URE', ''):\n                if os.path.isfile(realpath(basepath, basis, ure, 'lib', 'unorc')):\n                    urepath = realpath(basepath, basis, ure)\n                    info(3, \"Found %s in %s\" % ('unorc', realpath(urepath, 'lib')))\n                    # Break the inner loop.\n                    break\n            # Continue if the inner loop wasn't broken.\n            else:\n                continue\n            # Inner loop was broken; break the outer.\n            break\n\n        pythonhome = None\n        for home in pythonhomes:\n            if glob.glob(realpath(libpath, home)):\n                pythonhome = glob.glob(realpath(libpath, home))[0]\n                info(3, \"Found %s in %s\" % (home, pythonhome))\n                break\n\n        # if not os.path.isfile(realpath(basepath, program, officebinary)):\n            # continue\n        # info(3, \"Found %s in %s\" % (officebinary, realpath(basepath, program)))\n\n        # if not glob.glob(realpath(basepath, basis, program, 'python-core-*')):\n            # continue\n\n        # Find suitable Python executable: regular unopath or MacOS version.\n        # LibreOffice 5.4.6.2 on MacOS X 10.13.3 ships the Python executable\n        # in the \"Resources\" folder.\n        pythonpath_candidates = [unopath, realpath(basepath, 'Resources')]\n\n        python_effective = find_executable(pythonpath_candidates, pythonbinaries)\n\n        if python_effective:\n            info(3, \"Found Python at %s\" % python_effective)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            python_effective, pythonhome)\n        else:\n            info(3, \"Considering %s\" % basepath)\n            office = Office(basepath, urepath, unopath, officelibrary, officebinary,\n                            sys.executable, None)\n\n        ret.append(office)\n\n    return ret\n\n\ndef find_executable(folders, filenames):\n    for folder in folders:\n        for filename in filenames:\n            candidate = realpath(folder, filename)\n            if os.path.isfile(candidate):\n                return candidate\n\n\ndef office_environ(office):\n    # Set PATH so crash_report is found.\n    path_prefix = realpath(office.basepath, 'program') + os.pathsep + realpath(office.basepath, 'Resources')\n    if 'PATH' in os.environ:\n        os.environ['PATH'] = path_prefix + os.pathsep + os.environ['PATH']\n    else:\n        os.environ['PATH'] = path_prefix\n\n    # Set UNO_PATH so \"officehelper.bootstrap()\" can find soffice executable:\n    os.environ['UNO_PATH'] = office.unopath\n\n    # Set URE_BOOTSTRAP so \"uno.getComponentContext()\" bootstraps a complete UNO environment.\n    if os.name in ('nt', 'os2'):\n        os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamental.ini')\n    else:\n        if os.path.isfile(realpath(office.basepath, 'program', 'fundamentalrc')):\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'program', 'fundamentalrc')\n        else:\n            os.environ['URE_BOOTSTRAP'] = 'vnd.sun.star.pathname:' + realpath(office.basepath, 'Resources', 'fundamentalrc')\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n    if office.pythonhome:\n        for libpath in (realpath(office.pythonhome, 'lib'),\n                        realpath(office.pythonhome, 'lib', 'lib-dynload'),\n                        realpath(office.pythonhome, 'lib', 'lib-tk'),\n                        realpath(office.pythonhome, 'lib', 'site-packages'),\n                        office.unopath):\n            sys.path.insert(0, libpath)\n    else:\n        # Still needed for system python using LibreOffice UNO bindings\n        # Although we prefer to use a system UNO binding in this case\n        sys.path.append(office.unopath)\n\n\ndef debug_office():\n    if 'URE_BOOTSTRAP' in os.environ:\n        print('URE_BOOTSTRAP=%s' % os.environ['URE_BOOTSTRAP'], file=sys.stderr)\n    if 'UNO_PATH' in os.environ:\n        print('UNO_PATH=%s' % os.environ['UNO_PATH'], file=sys.stderr)\n    if 'UNO_TYPES' in os.environ:\n        print('UNO_TYPES=%s' % os.environ['UNO_TYPES'], file=sys.stderr)\n    print('PATH=%s' % os.environ['PATH'])\n    if 'PYTHONHOME' in os.environ:\n        print('PYTHONHOME=%s' % os.environ['PYTHONHOME'], file=sys.stderr)\n    if 'PYTHONPATH' in os.environ:\n        print('PYTHONPATH=%s' % os.environ['PYTHONPATH'], file=sys.stderr)\n    if 'LD_LIBRARY_PATH' in os.environ:\n        print('LD_LIBRARY_PATH=%s' % os.environ['LD_LIBRARY_PATH'], file=sys.stderr)\n\n\ndef python_switch(office):\n    if office.pythonhome:\n        os.environ['PYTHONHOME'] = office.pythonhome\n        os.environ['PYTHONPATH'] = realpath(office.pythonhome, 'lib') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-dynload') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'lib-tk') + os.pathsep + \\\n                                   realpath(office.pythonhome, 'lib', 'site-packages') + os.pathsep + \\\n                                   office.unopath\n\n    os.environ['UNO_PATH'] = office.unopath\n\n    info(3, \"-> Switching from %s to %s\" % (sys.executable, office.python))\n    if os.name in ('nt', 'os2'):\n        # os.execv is broken on Windows and can't properly parse command line\n        # arguments and executable name if they contain whitespaces. subprocess\n        # fixes that behavior.\n        ret = subprocess.call([office.python, ] + sys.argv[0:])\n        sys.exit(ret)\n    else:\n\n        # Set LD_LIBRARY_PATH so that \"import pyuno\" finds libpyuno.so:\n        if 'LD_LIBRARY_PATH' in os.environ:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib') + os.pathsep + \\\n                                            os.environ['LD_LIBRARY_PATH']\n        else:\n            os.environ['LD_LIBRARY_PATH'] = office.unopath + os.pathsep + \\\n                                            realpath(office.urepath, 'lib')\n\n        try:\n            os.execvpe(office.python, [office.python, ] + sys.argv[0:], os.environ)\n        except OSError:\n            # Mac OS X versions prior to 10.6 do not support execv in\n            # a process that contains multiple threads.  Instead of\n            # re-executing in the current process, start a new one\n            # and cause the current process to exit.  This isn't\n            # ideal since the new process is detached from the parent\n            # terminal and thus cannot easily be killed with ctrl-C,\n            # but it's better than not being able to autoreload at\n            # all.\n            # Unfortunately the errno returned in this case does not\n            # appear to be consistent, so we can't easily check for\n            # this error specifically.\n            ret = os.spawnvpe(os.P_WAIT, office.python, [office.python, ] + sys.argv[0:], os.environ)\n            if ret != 0:\n                error(\"Switching Python to %s failed.\" % (office.python))\n            sys.exit(ret)\n\n\nclass Fmt:\n    def __init__(self, doctype, name, extension, summary, filter):\n        self.doctype = doctype\n        self.name = name\n        self.extension = extension\n        self.summary = summary\n        self.filter = filter\n\n    def __str__(self):\n        return \"%s [.%s]\" % (self.summary, self.extension)\n\n    def __repr__(self):\n        return \"%s/%s\" % (self.name, self.doctype)\n\n\nclass FmtList:\n    def __init__(self):\n        self.list = []\n\n    def add(self, doctype, name, extension, summary, filter):\n        self.list.append(Fmt(doctype, name, extension, summary, filter))\n\n    def byname(self, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name:\n                ret.append(fmt)\n        return ret\n\n    def byextension(self, extension):\n        ret = []\n        for fmt in self.list:\n            if os.extsep + fmt.extension == extension:\n                ret.append(fmt)\n        return ret\n\n    def bydoctype(self, doctype, name):\n        ret = []\n        for fmt in self.list:\n            if fmt.name == name and fmt.doctype == doctype:\n                ret.append(fmt)\n        return ret\n\n    def display(self, doctype):\n        print(\"The following list of %s formats are currently available:\\n\" % doctype, file=sys.stderr)\n        for fmt in self.list:\n            if fmt.doctype == doctype:\n                print(\"  %-8s - %s\" % (fmt.name, fmt), file=sys.stderr)\n        print(file=sys.stderr)\n\n\nfmts = FmtList()\n\n# TextDocument\nfmts.add('document', 'bib', 'bib', 'BibTeX', 'BibTeX_Writer')  # 22\nfmts.add('document', 'doc', 'doc', 'Microsoft Word 97/2000/XP', 'MS Word 97')  # 29\nfmts.add('document', 'doc6', 'doc', 'Microsoft Word 6.0', 'MS WinWord 6.0')  # 24\nfmts.add('document', 'doc95', 'doc', 'Microsoft Word 95', 'MS Word 95')  # 28\nfmts.add('document', 'docbook', 'xml', 'DocBook', 'DocBook File')  # 39\nfmts.add('document', 'docx', 'docx', 'Microsoft Office Open XML', 'Office Open XML Text')\nfmts.add('document', 'docx7', 'docx', 'Microsoft Office Open XML', 'MS Word 2007 XML')\nfmts.add('document', 'fodt', 'fodt', 'OpenDocument Text (Flat XML)', 'OpenDocument Text Flat XML')\nfmts.add('document', 'html', 'html', 'HTML Document (OpenOffice.org Writer)', 'HTML (StarWriter)')  # 3\nfmts.add('document', 'latex', 'ltx', 'LaTeX 2e', 'LaTeX_Writer')  # 31\nfmts.add('document', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki')\nfmts.add('document', 'odt', 'odt', 'ODF Text Document', 'writer8')  # 10\nfmts.add('document', 'ooxml', 'xml', 'Microsoft Office Open XML', 'MS Word 2003 XML')  # 11\nfmts.add('document', 'ott', 'ott', 'Open Document Text', 'writer8_template')  # 21\nfmts.add('document', 'pdb', 'pdb', 'AportisDoc (Palm)', 'AportisDoc Palm DB')\nfmts.add('document', 'pdf', 'pdf', 'Portable Document Format', 'writer_pdf_Export')  # 18\nfmts.add('document', 'psw', 'psw', 'Pocket Word', 'PocketWord File')\nfmts.add('document', 'rtf', 'rtf', 'Rich Text Format', 'Rich Text Format')  # 16\nfmts.add('document', 'sdw', 'sdw', 'StarWriter 5.0', 'StarWriter 5.0')  # 23\nfmts.add('document', 'sdw4', 'sdw', 'StarWriter 4.0', 'StarWriter 4.0')  # 2\nfmts.add('document', 'sdw3', 'sdw', 'StarWriter 3.0', 'StarWriter 3.0')  # 20\nfmts.add('document', 'stw', 'stw', 'Open Office.org 1.0 Text Document Template', 'writer_StarOffice_XML_Writer_Template')  # 9\nfmts.add('document', 'sxw', 'sxw', 'Open Office.org 1.0 Text Document', 'StarOffice XML (Writer)')  # 1\nfmts.add('document', 'text', 'txt', 'Text Encoded', 'Text (encoded)')  # 26\nfmts.add('document', 'txt', 'txt', 'Text', 'Text')  # 34\nfmts.add('document', 'uot', 'uot', 'Unified Office Format text', 'UOF text')  # 27\nfmts.add('document', 'vor', 'vor', 'StarWriter 5.0 Template', 'StarWriter 5.0 Vorlage/Template')  # 6\nfmts.add('document', 'vor4', 'vor', 'StarWriter 4.0 Template', 'StarWriter 4.0 Vorlage/Template')  # 5\nfmts.add('document', 'vor3', 'vor', 'StarWriter 3.0 Template', 'StarWriter 3.0 Vorlage/Template')  # 4\nfmts.add('document', 'wps', 'wps', 'Microsoft Works', 'MS_Works')\nfmts.add('document', 'xhtml', 'html', 'XHTML Document', 'XHTML Writer File')  # 33\nfmts.add('document', 'epub', 'epub', 'Electronic Publication', 'EPUB')\nfmts.add('document', 'png', 'png', 'Portable Network Graphic', 'writer_png_Export') ### 2\n\n# WebDocument\nfmts.add('web', 'etext', 'txt', 'Text Encoded (OpenOffice.org Writer/Web)', 'Text (encoded) (StarWriter/Web)')  # 14\nfmts.add('web', 'html10', 'html', 'OpenOffice.org 1.0 HTML Template', 'writer_web_StarOffice_XML_Writer_Web_Template')  # 11\nfmts.add('web', 'html', 'html', 'HTML Document', 'HTML')  # 2\nfmts.add('web', 'html', 'html', 'HTML Document Template', 'writerweb8_writer_template')  # 13\nfmts.add('web', 'mediawiki', 'txt', 'MediaWiki', 'MediaWiki_Web')  # 9\nfmts.add('web', 'pdf', 'pdf', 'PDF - Portable Document Format', 'writer_web_pdf_Export')  # 10\nfmts.add('web', 'sdw3', 'sdw', 'StarWriter 3.0 (OpenOffice.org Writer/Web)', 'StarWriter 3.0 (StarWriter/Web)')  # 3\nfmts.add('web', 'sdw4', 'sdw', 'StarWriter 4.0 (OpenOffice.org Writer/Web)', 'StarWriter 4.0 (StarWriter/Web)')  # 4\nfmts.add('web', 'sdw', 'sdw', 'StarWriter 5.0 (OpenOffice.org Writer/Web)', 'StarWriter 5.0 (StarWriter/Web)')  # 5\nfmts.add('web', 'txt', 'txt', 'OpenOffice.org Text (OpenOffice.org Writer/Web)', 'writerweb8_writer')  # 12\nfmts.add('web', 'text10', 'txt', 'OpenOffice.org 1.0 Text Document (OpenOffice.org Writer/Web)', 'writer_web_StarOffice_XML_Writer')  # 15\nfmts.add('web', 'text', 'txt', 'Text (OpenOffice.org Writer/Web)', 'Text (StarWriter/Web)')  # 8\nfmts.add('web', 'vor4', 'vor', 'StarWriter/Web 4.0 Template', 'StarWriter/Web 4.0 Vorlage/Template')  # 6\nfmts.add('web', 'vor', 'vor', 'StarWriter/Web 5.0 Template', 'StarWriter/Web 5.0 Vorlage/Template')  # 7\n\n# Spreadsheet\nfmts.add('spreadsheet', 'csv', 'csv', 'Text CSV', 'Text - txt - csv (StarCalc)')  # 16\nfmts.add('spreadsheet', 'dbf', 'dbf', 'dBASE', 'dBase')  # 22\nfmts.add('spreadsheet', 'dif', 'dif', 'Data Interchange Format', 'DIF')  # 5\nfmts.add('spreadsheet', 'fods', 'fods', 'OpenDocument Spreadsheet (Flat XML)', 'OpenDocument Spreadsheet Flat XML')\nfmts.add('spreadsheet', 'html', 'html', 'HTML Document (OpenOffice.org Calc)', 'HTML (StarCalc)')  # 7\nfmts.add('spreadsheet', 'ods', 'ods', 'ODF Spreadsheet', 'calc8')  # 15\nfmts.add('spreadsheet', 'ooxml', 'xml', 'Microsoft Excel 2003 XML', 'MS Excel 2003 XML')  # 23\nfmts.add('spreadsheet', 'ots', 'ots', 'ODF Spreadsheet Template', 'calc8_template')  # 14\nfmts.add('spreadsheet', 'pdf', 'pdf', 'Portable Document Format', 'calc_pdf_Export')  # 34\nfmts.add('spreadsheet', 'pxl', 'pxl', 'Pocket Excel', 'Pocket Excel')\nfmts.add('spreadsheet', 'sdc', 'sdc', 'StarCalc 5.0', 'StarCalc 5.0')  # 31\nfmts.add('spreadsheet', 'sdc4', 'sdc', 'StarCalc 4.0', 'StarCalc 4.0')  # 11\nfmts.add('spreadsheet', 'sdc3', 'sdc', 'StarCalc 3.0', 'StarCalc 3.0')  # 29\nfmts.add('spreadsheet', 'slk', 'slk', 'SYLK', 'SYLK')  # 35\nfmts.add('spreadsheet', 'stc', 'stc', 'OpenOffice.org 1.0 Spreadsheet Template', 'calc_StarOffice_XML_Calc_Template')  # 2\nfmts.add('spreadsheet', 'sxc', 'sxc', 'OpenOffice.org 1.0 Spreadsheet', 'StarOffice XML (Calc)')  # 3\nfmts.add('spreadsheet', 'uos', 'uos', 'Unified Office Format spreadsheet', 'UOF spreadsheet')  # 9\nfmts.add('spreadsheet', 'vor3', 'vor', 'StarCalc 3.0 Template', 'StarCalc 3.0 Vorlage/Template')  # 18\nfmts.add('spreadsheet', 'vor4', 'vor', 'StarCalc 4.0 Template', 'StarCalc 4.0 Vorlage/Template')  # 19\nfmts.add('spreadsheet', 'vor', 'vor', 'StarCalc 5.0 Template', 'StarCalc 5.0 Vorlage/Template')  # 20\nfmts.add('spreadsheet', 'xhtml', 'xhtml', 'XHTML', 'XHTML Calc File')  # 26\nfmts.add('spreadsheet', 'xls', 'xls', 'Microsoft Excel 97/2000/XP', 'MS Excel 97')  # 12\nfmts.add('spreadsheet', 'xls5', 'xls', 'Microsoft Excel 5.0', 'MS Excel 5.0/95')  # 8\nfmts.add('spreadsheet', 'xls95', 'xls', 'Microsoft Excel 95', 'MS Excel 95')  # 10\nfmts.add('spreadsheet', 'xlt', 'xlt', 'Microsoft Excel 97/2000/XP Template', 'MS Excel 97 Vorlage/Template')  # 6\nfmts.add('spreadsheet', 'xlt5', 'xlt', 'Microsoft Excel 5.0 Template', 'MS Excel 5.0/95 Vorlage/Template')  # 28\nfmts.add('spreadsheet', 'xlt95', 'xlt', 'Microsoft Excel 95 Template', 'MS Excel 95 Vorlage/Template')  # 21\nfmts.add('spreadsheet', 'xlsx', 'xlsx', 'Microsoft Excel 2007/2010 XML', 'Calc MS Excel 2007 XML')\n\n# Graphics\nfmts.add('graphics', 'bmp', 'bmp', 'Windows Bitmap', 'draw_bmp_Export')  # 21\nfmts.add('graphics', 'emf', 'emf', 'Enhanced Metafile', 'draw_emf_Export')  # 15\nfmts.add('graphics', 'eps', 'eps', 'Encapsulated PostScript', 'draw_eps_Export')  # 48\nfmts.add('graphics', 'fodg', 'fodg', 'OpenDocument Drawing (Flat XML)', 'OpenDocument Drawing Flat XML')\nfmts.add('graphics', 'gif', 'gif', 'Graphics Interchange Format', 'draw_gif_Export')  # 30\nfmts.add('graphics', 'html', 'html', 'HTML Document (OpenOffice.org Draw)', 'draw_html_Export')  # 37\nfmts.add('graphics', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'jpeg', 'jpeg', 'Joint Photographic Experts Group', 'draw_jpg_Export')  # 3\nfmts.add('graphics', 'met', 'met', 'OS/2 Metafile', 'draw_met_Export')  # 43\nfmts.add('graphics', 'odd', 'odd', 'OpenDocument Drawing', 'draw8')  # 6\nfmts.add('graphics', 'otg', 'otg', 'OpenDocument Drawing Template', 'draw8_template')  # 20\nfmts.add('graphics', 'pbm', 'pbm', 'Portable Bitmap', 'draw_pbm_Export')  # 14\nfmts.add('graphics', 'pct', 'pct', 'Mac Pict', 'draw_pct_Export')  # 41\nfmts.add('graphics', 'pdf', 'pdf', 'Portable Document Format', 'draw_pdf_Export')  # 28\nfmts.add('graphics', 'pgm', 'pgm', 'Portable Graymap', 'draw_pgm_Export')  # 11\nfmts.add('graphics', 'png', 'png', 'Portable Network Graphic', 'draw_png_Export')  # 2\nfmts.add('graphics', 'ppm', 'ppm', 'Portable Pixelmap', 'draw_ppm_Export')  # 5\nfmts.add('graphics', 'ras', 'ras', 'Sun Raster Image', 'draw_ras_Export')  # 31\nfmts.add('graphics', 'std', 'std', 'OpenOffice.org 1.0 Drawing Template', 'draw_StarOffice_XML_Draw_Template')  # 53\nfmts.add('graphics', 'svg', 'svg', 'Scalable Vector Graphics', 'draw_svg_Export')  # 50\nfmts.add('graphics', 'svm', 'svm', 'StarView Metafile', 'draw_svm_Export')  # 55\nfmts.add('graphics', 'swf', 'swf', 'Macromedia Flash (SWF)', 'draw_flash_Export')  # 23\nfmts.add('graphics', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing', 'StarOffice XML (Draw)')  # 26\nfmts.add('graphics', 'sxd3', 'sxd', 'StarDraw 3.0', 'StarDraw 3.0')  # 40\nfmts.add('graphics', 'sxd5', 'sxd', 'StarDraw 5.0', 'StarDraw 5.0')  # 44\nfmts.add('graphics', 'sxw', 'sxw', 'StarOffice XML (Draw)', 'StarOffice XML (Draw)')\nfmts.add('graphics', 'tiff', 'tiff', 'Tagged Image File Format', 'draw_tif_Export')  # 13\nfmts.add('graphics', 'vor', 'vor', 'StarDraw 5.0 Template', 'StarDraw 5.0 Vorlage')  # 36\nfmts.add('graphics', 'vor3', 'vor', 'StarDraw 3.0 Template', 'StarDraw 3.0 Vorlage')  # 35\nfmts.add('graphics', 'wmf', 'wmf', 'Windows Metafile', 'draw_wmf_Export')  # 8\nfmts.add('graphics', 'xhtml', 'xhtml', 'XHTML', 'XHTML Draw File')  # 45\nfmts.add('graphics', 'xpm', 'xpm', 'X PixMap', 'draw_xpm_Export')  # 19\n\n# Presentation\nfmts.add('presentation', 'bmp', 'bmp', 'Windows Bitmap', 'impress_bmp_Export')  # 15\nfmts.add('presentation', 'emf', 'emf', 'Enhanced Metafile', 'impress_emf_Export')  # 16\nfmts.add('presentation', 'eps', 'eps', 'Encapsulated PostScript', 'impress_eps_Export')  # 17\nfmts.add('presentation', 'fodp', 'fodp', 'OpenDocument Presentation (Flat XML)', 'OpenDocument Presentation Flat XML')\nfmts.add('presentation', 'gif', 'gif', 'Graphics Interchange Format', 'impress_gif_Export')  # 18\nfmts.add('presentation', 'html', 'html', 'HTML Document (OpenOffice.org Impress)', 'impress_html_Export')  # 43\nfmts.add('presentation', 'jpg', 'jpg', 'Joint Photographic Experts Group', 'impress_jpg_Export')  # 19\nfmts.add('presentation', 'met', 'met', 'OS/2 Metafile', 'impress_met_Export')  # 20\nfmts.add('presentation', 'odg', 'odg', 'ODF Drawing (Impress)', 'impress8_draw')  # 29\nfmts.add('presentation', 'odp', 'odp', 'ODF Presentation', 'impress8')  # 9\nfmts.add('presentation', 'otp', 'otp', 'ODF Presentation Template', 'impress8_template')  # 38\nfmts.add('presentation', 'pbm', 'pbm', 'Portable Bitmap', 'impress_pbm_Export')  # 21\nfmts.add('presentation', 'pct', 'pct', 'Mac Pict', 'impress_pct_Export')  # 22\nfmts.add('presentation', 'pdf', 'pdf', 'Portable Document Format', 'impress_pdf_Export')  # 23\nfmts.add('presentation', 'pgm', 'pgm', 'Portable Graymap', 'impress_pgm_Export')  # 24\nfmts.add('presentation', 'png', 'png', 'Portable Network Graphic', 'impress_png_Export')  # 25\nfmts.add('presentation', 'potm', 'potm', 'Microsoft PowerPoint 2007/2010 XML Template', 'Impress MS PowerPoint 2007 XML Template')\nfmts.add('presentation', 'pot', 'pot', 'Microsoft PowerPoint 97/2000/XP Template', 'MS PowerPoint 97 Vorlage')  # 3\nfmts.add('presentation', 'ppm', 'ppm', 'Portable Pixelmap', 'impress_ppm_Export')  # 26\nfmts.add('presentation', 'pptx', 'pptx', 'Microsoft PowerPoint 2007/2010 XML', 'Impress MS PowerPoint 2007 XML')  # 36\nfmts.add('presentation', 'pps', 'pps', 'Microsoft PowerPoint 97/2000/XP (Autoplay)', 'MS PowerPoint 97 Autoplay')  # 36\nfmts.add('presentation', 'ppt', 'ppt', 'Microsoft PowerPoint 97/2000/XP', 'MS PowerPoint 97')  # 36\nfmts.add('presentation', 'pwp', 'pwp', 'PlaceWare', 'placeware_Export')  # 30\nfmts.add('presentation', 'ras', 'ras', 'Sun Raster Image', 'impress_ras_Export')  # 27\nfmts.add('presentation', 'sda', 'sda', 'StarDraw 5.0 (OpenOffice.org Impress)', 'StarDraw 5.0 (StarImpress)')  # 8\nfmts.add('presentation', 'sdd', 'sdd', 'StarImpress 5.0', 'StarImpress 5.0')  # 6\nfmts.add('presentation', 'sdd3', 'sdd', 'StarDraw 3.0 (OpenOffice.org Impress)', 'StarDraw 3.0 (StarImpress)')  # 42\nfmts.add('presentation', 'sdd4', 'sdd', 'StarImpress 4.0', 'StarImpress 4.0')  # 37\nfmts.add('presentation', 'sxd', 'sxd', 'OpenOffice.org 1.0 Drawing (OpenOffice.org Impress)', 'impress_StarOffice_XML_Draw')  # 31\nfmts.add('presentation', 'sti', 'sti', 'OpenOffice.org 1.0 Presentation Template', 'impress_StarOffice_XML_Impress_Template')  # 5\nfmts.add('presentation', 'svg', 'svg', 'Scalable Vector Graphics', 'impress_svg_Export')  # 14\nfmts.add('presentation', 'svm', 'svm', 'StarView Metafile', 'impress_svm_Export')  # 13\nfmts.add('presentation', 'swf', 'swf', 'Macromedia Flash (SWF)', 'impress_flash_Export')  # 34\nfmts.add('presentation', 'sxi', 'sxi', 'OpenOffice.org 1.0 Presentation', 'StarOffice XML (Impress)')  # 41\nfmts.add('presentation', 'tiff', 'tiff', 'Tagged Image File Format', 'impress_tif_Export')  # 12\nfmts.add('presentation', 'uop', 'uop', 'Unified Office Format presentation', 'UOF presentation')  # 4\nfmts.add('presentation', 'vor', 'vor', 'StarImpress 5.0 Template', 'StarImpress 5.0 Vorlage')  # 40\nfmts.add('presentation', 'vor3', 'vor', 'StarDraw 3.0 Template (OpenOffice.org Impress)', 'StarDraw 3.0 Vorlage (StarImpress)')  # 1\nfmts.add('presentation', 'vor4', 'vor', 'StarImpress 4.0 Template', 'StarImpress 4.0 Vorlage')  # 39\nfmts.add('presentation', 'vor5', 'vor', 'StarDraw 5.0 Template (OpenOffice.org Impress)', 'StarDraw 5.0 Vorlage (StarImpress)')  # 2\nfmts.add('presentation', 'wmf', 'wmf', 'Windows Metafile', 'impress_wmf_Export')  # 11\nfmts.add('presentation', 'xhtml', 'xml', 'XHTML', 'XHTML Impress File')  # 33\nfmts.add('presentation', 'xpm', 'xpm', 'X PixMap', 'impress_xpm_Export')  # 10\n\n\nclass Options:\n    def __init__(self, args):\n        self.connection = None\n        self.debug = False\n        self.doctype = None\n        self.exportfilter = []\n        self.exportfilteroptions = \"\"\n        self.fields = {}\n        self.filenames = []\n        self.format = None\n        self.importfilter = []\n        self.importfiltername = None\n        self.importfilteroptions = \"\"\n        self.listener = False\n        self.metadata = {}\n        self.nolaunch = False\n        self.output = None\n        self.paperformat = None\n        self.paperorientation = None\n        self.papersize = None\n        self.password = None\n        self.pipe = None\n        self.port = '2002'\n        self.preserve = False\n        self.server = '127.0.0.1'\n        self.setprinter = False\n        self.showlist = False\n        self.stdin = False\n        self.stdout = False\n        self.template = None\n        self.timeout = 60\n        self.verbose = 0\n        self.userProfile = None\n        self.updateDocMode = NO_UPDATE\n        self.updatehtmllinks = True\n\n        # Get options from the commandline\n        try:\n            opts, args = getopt.getopt(args, 'c:Dd:e:F:f:hi:I:LlM:no:p:s:T:t:P:vV',\n                ['disable-html-update-links', 'connection=', 'debug', 'doctype=', 'export=', 'field=', 'format=',\n                 'help', 'import=', 'import-filter-name=', 'listener', 'meta=', 'no-launch',\n                 'output=', 'outputpath', 'password=', 'pipe=', 'port=', 'preserve',\n                 'server=', 'timeout=', 'user-profile=', 'show', 'stdin',\n                 'stdout', 'template', 'printer=', 'unsafe-quiet-update', 'verbose', 'version'])\n        except getopt.error as exc:\n            print('unoconv: %s, try unoconv -h for a list of all the options' % str(exc))\n            sys.exit(255)\n\n        for opt, arg in opts:\n            if opt in ['-h', '--help']:\n                self.usage()\n                print()\n                self.help()\n                sys.exit(0)\n            elif opt in ['-c', '--connection']:\n                self.connection = arg\n            elif opt in ['--debug']:\n                self.debug = True\n            elif opt in ['-d', '--doctype']:\n                self.doctype = arg\n            elif opt in ['-e', '--export']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.exportfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.exportfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.exportfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.exportfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.exportfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-F', '--field']:\n                l = arg.split('=')\n                self.fields[l[0]] = '='.join(l[1:])\n            elif opt in ['-f', '--format']:\n                self.format = arg\n            elif opt in ['-i', '--import']:\n                l = arg.split('=')\n                if len(l) == 2:\n                    (name, value) = l\n                    if name in ('FilterOptions'):\n                        self.importfilteroptions = value\n                    elif value in ('True', 'true'):\n                        self.importfilter.append(PropertyValue(name, 0, True, 0))\n                    elif value in ('False', 'false'):\n                        self.importfilter.append(PropertyValue(name, 0, False, 0))\n                    else:\n                        try:\n                            self.importfilter.append(PropertyValue(name, 0, int(value), 0))\n                        except ValueError:\n                            self.importfilter.append(PropertyValue(name, 0, value, 0))\n                else:\n                    print('Warning: Option %s cannot be parsed, ignoring.' % arg, file=sys.stderr)\n            elif opt in ['-I', '--import-filter-name']:\n                self.importfiltername = arg\n            elif opt in ['-l', '--listener']:\n                self.listener = True\n            elif opt in ['-M', '--meta']:\n                l = arg.split('=')\n                self.metadata[l[0]] = '='.join(l[1:])\n            elif opt in ['-n', '--no-launch']:\n                self.nolaunch = True\n            elif opt in ['-o', '--output']:\n                self.output = arg\n            elif opt in ['--outputpath']:\n                print('Warning: This option is deprecated by --output.', file=sys.stderr)\n                self.output = arg\n            elif opt in ['--password']:\n                self.password = arg\n            elif opt in ['--pipe']:\n                self.pipe = arg\n            elif opt in ['-p', '--port']:\n                self.port = arg\n            elif opt in ['--preserve']:\n                self.preserve = True\n            elif opt in ['-s', '--server']:\n                self.server = arg\n            elif opt in ['--show']:\n                self.showlist = True\n            elif opt in ['--stdin']:\n                self.stdin = True\n            elif opt in ['--stdout']:\n                self.stdout = True\n            elif opt in ['-t', '--template']:\n                self.template = arg\n            elif opt in ['--disable-html-update-links']:\n                self.updatehtmllinks = False\n            elif opt in ['-T', '--timeout']:\n                self.timeout = int(arg)\n            elif opt in ['--unsafe-quiet-update']:\n                # ref https://www.openoffice.org/api/docs/common/ref/com/sun/star/document/UpdateDocMode.html\n                print('Warning: Do not use the option --unsafe-quiet-update with untrusted input.')\n                self.updateDocMode = QUIET_UPDATE\n            elif opt in ['-v', '--verbose']:\n                self.verbose = self.verbose + 1\n            elif opt in ['-V', '--version']:\n                self.version()\n                sys.exit(0)\n            elif opt in ['-P', '--printer']:\n                optKey, optValue = arg.split('=')\n                if optKey in ['PaperFormat']:\n                    self.paperformat = optValue\n                    self.setprinter = True\n                elif optKey in ['PaperOrientation']:\n                    self.paperorientation = optValue.upper()\n                    self.setprinter = True\n                elif optKey in ['PaperSize']:\n                    intFunc = int if sys.version_info.major > 2 else long\n                    size = list(map(lambda s: intFunc(s), optValue.split('x')))\n                    if (2 == len(size)):\n                        self.papersize = size\n                        self.setprinter = True\n            elif opt in ['--user-profile']:\n                self.userProfile = arg\n\n        # Enable verbosity\n        if self.verbose >= 2:\n            print('Verbosity set to level %d' % self.verbose, file=sys.stderr)\n\n        self.filenames = args\n\n        if not self.listener and not self.showlist and not self.stdin and self.doctype != 'list' and not self.filenames:\n            print('unoconv: you have to provide a filename or url as argument', file=sys.stderr)\n            print('Try `unoconv -h\\' for more information.', file=sys.stderr)\n            sys.exit(255)\n\n        # Set connection string\n        if not self.connection:\n            if not self.pipe:\n                self.connection = \"socket,host=%s,port=%s,tcpNoDelay=1;urp;StarOffice.ComponentContext\" % (self.server, self.port)\n            else:\n                self.connection = \"pipe,name=%s;urp;StarOffice.ComponentContext\" % (self.pipe)\n\n        # Make it easier for people to use a doctype (first letter is enough)\n        if self.doctype:\n            for doctype in doctypes:\n                if doctype.startswith(self.doctype):\n                    self.doctype = doctype\n\n        # Check if the user request to see the list of formats\n        if self.showlist or self.format == 'list':\n            if self.doctype:\n                fmts.display(self.doctype)\n            else:\n                for t in doctypes:\n                    fmts.display(t)\n            sys.exit(0)\n\n        # If no format was specified, probe it or provide it.\n        if not self.format:\n            # Check if the command is in the form odt2pdf\n            l = sys.argv[0].split('2')\n            if len(l) == 2:\n                self.format = l[1]\n            # Use the extension of the output file\n            elif self.output and os.path.basename(self.output).find('.') >= 0:\n                self.format = os.path.splitext(self.output)[1].lstrip('.')\n\n        # Default to PDF.\n        if not self.format:\n            self.format = 'pdf'\n\n    def version(self):\n        print('unoconv %s' % __version__)\n        print('Written by Dag Wieers <dag@wieers.com>')\n        print('Homepage at http://dag.wieers.com/home-made/unoconv/')\n        print()\n        print('platform %s/%s' % (os.name, sys.platform))\n        print('python %s' % sys.version)\n\n        if uno:\n            # Get office product information.\n            product = uno.getComponentContext().ServiceManager.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            print(product.ooName, product.ooSetupVersionAboutBox)\n\n    def usage(self):\n        print('usage: unoconv [options] file [file2 ..]', file=sys.stderr)\n\n    def help(self):\n        print('''Convert from and to any format supported by LibreOffice\n\nunoconv options:\n  -c, --connection=string             use a custom connection string\n  -d, --doctype=type                  specify document type\n                                        (document, graphics, presentation, spreadsheet)\n  -e, --export=name=value             set export filter options\n                                        eg. -e PageRange=1-2\n  -f, --format=format                 specify the output format\n  -F, --field=name=value              replace user-defined text field with value\n                                        eg. -F Client_Name=\"Oracle\"\n  -i, --import=string                 set import filter option string\n                                        eg. -i utf8\n  -I, --import-filter-name=string     set import filter name, useful when converting stdin\n                                      or files without an extension)\n                                        eg. -I ooxml\n  -l, --listener                      start a permanent listener to use by unoconv clients\n  -n, --no-launch                     fail if no listener is found (default: launch one)\n  -o, --output=name                   output basename, filename or directory\n      --pipe=name                     alternative method of connection using a pipe\n  -p, --port=port                     specify the port (default: 2002)\n                                        to be used by client or listener\n      --password=string               provide a password to decrypt the document\n      --preserve                      keep timestamp and permissions of the original document\n  -s, --server=server                 specify the server address (default: 127.0.0.1)\n                                        to be used by client or listener\n      --show                          list the available output formats\n      --stdin                         read from stdin (filenames are ignored if provided)\n      --stdout                        write output to stdout\n  -t, --template=file                 import the styles from template (.ott)\n  -T, --timeout=secs                  timeout after secs if connection to listener fails\n      --unsafe-quiet-update           allow rendered document to fetch external resources (Warning: this is unsafe with untrusted input)\n  -v, --verbose                       be more and more verbose (-vvv for debugging)\n      --version                       display version number of unoconv, OOo/LO and platform details\n  -P, --printer=name=value            printer options\n                                        PaperFormat: specify printer paper format\n                                          eg. -P PaperFormat=A3\n                                        PaperOrientation: specify printer paper orientation\n                                          eg. -P PaperOrientation=landscape\n                                        PaperSize: specify printer paper size, paper format should set to USER, size=widthxheight\n                                          eg. -P PaperSize=130x200 means width=130, height=200\n  --disable-html-update-links   disables the recheck for updating links missed by libreoffice\n  --user-profile=path                 use a custom user profile path\n''', file=sys.stderr)\n\n\nclass Convertor:\n    def __init__(self):\n        global exitcode, ooproc, office, product\n        unocontext = None\n\n        # Do the LibreOffice component dance\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n\n        # Test for an existing connection\n        info(3, 'Connection type: %s' % op.connection)\n        unocontext = self.connect(resolver)\n\n        if not unocontext:\n            die(251, \"Unable to connect or start own listener. Aborting.\")\n\n        # And some more LibreOffice magic\n        unosvcmgr = unocontext.ServiceManager\n        self.desktop = unosvcmgr.createInstanceWithContext(\"com.sun.star.frame.Desktop\", unocontext)\n        self.cwd = unohelper.systemPathToFileUrl(os.getcwd())\n\n        # List all filters\n        # self.filters = unosvcmgr.createInstanceWithContext(\"com.sun.star.document.FilterFactory\", unocontext)\n        # for filter in self.filters.getElementNames():\n            # print filter\n            # print dir(filter), dir(filter.format)\n\n    def connect(self, resolver):\n        global ooproc, product, office\n        unocontext = None\n\n        try:\n            unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n        except NoConnectException as e:\n            # info(3, \"Existing listener not found.\\n%s\" % e)\n            info(3, \"Existing listener not found.\")\n\n            if op.nolaunch:\n                die(113, \"Existing listener not found. Unable start listener by parameters. Aborting.\")\n\n            # Start our own OpenOffice instance\n            info(3, \"Launching our own listener using %s.\" % office.binary)\n            try:\n                product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n                if product.ooName not in ('LibreOffice', 'LOdev') or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                    args = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-accept=%s\" % op.connection]\n                else:\n                    args = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--accept=%s\" % op.connection]\n                if op.userProfile:\n                    args.append(\"-env:UserInstallation=file://\" + realpath(op.userProfile))\n                info(2, '%s listener arguments are %s.' % (product.ooName, args))\n                ooproc = subprocess.Popen(args, env=os.environ)\n                info(2, '%s listener successfully started. (pid=%s)' % (product.ooName, ooproc.pid))\n\n                # Try connection to it for op.timeout seconds (flakky OpenOffice)\n                timeout = 0\n                while timeout <= op.timeout:\n                    # Is it already/still running?\n                    retcode = ooproc.poll()\n                    if retcode == 81:\n                        info(3, \"Caught exit code 81 (new installation). Restarting listener.\")\n                        return self.connect(resolver)\n                        break\n\n                    elif retcode is not None:\n                        info(3, \"Process %s (pid=%s) exited with %s.\" % (office.binary, ooproc.pid, retcode))\n                        break\n\n                    try:\n                        unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n                        break\n                    except NoConnectException:\n                        time.sleep(0.5)\n                        timeout += 0.5\n                    except:\n                        raise\n                else:\n                    error(\"Failed to connect to %s (pid=%s) in %d seconds.\\n%s\" % (office.binary, ooproc.pid, op.timeout, e))\n            except Exception as e:\n                raise\n                error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n        return unocontext\n\n    def getimportformat(self):\n        if op.doctype:\n            importformat = fmts.bydoctype(op.doctype, op.importfiltername)\n        else:\n            importformat = fmts.byname(op.importfiltername)\n\n        if not importformat:\n            error('Import format [%s] is not known to unoconv.' % importformat)\n\n        return importformat[0]\n\n    def getformat(self, inputfn):\n        doctype = None\n\n        # Get the output format from mapping\n        if op.doctype:\n            outputfmt = fmts.bydoctype(op.doctype, op.format)\n        else:\n            outputfmt = fmts.byname(op.format)\n\n            if not outputfmt:\n                outputfmt = fmts.byextension(os.extsep + op.format)\n\n        # If no doctype given, check list of acceptable formats for input file ext doctype.\n        # FIXME: This should go into the for-loop to match each individual input filename.\n        if outputfmt:\n            inputext = os.path.splitext(inputfn)[1]\n            inputfmt = fmts.byextension(inputext)\n            if inputfmt:\n                for fmt in outputfmt:\n                    if inputfmt[0].doctype == fmt.doctype:\n                        doctype = inputfmt[0].doctype\n                        outputfmt = fmt\n                        break\n                else:\n                    outputfmt = outputfmt[0]\n    #       print >>sys.stderr, 'Format `%s\\' is part of multiple doctypes %s, selecting `%s\\'.' % (format, [fmt.doctype for fmt in outputfmt], outputfmt[0].doctype)\n            else:\n                outputfmt = outputfmt[0]\n\n        # No format found, throw error\n        if not outputfmt:\n            if doctype:\n                error('Format [%s/%s] is not known to unoconv.' % (op.doctype, op.format))\n            else:\n                error('Format [%s] is not known to unoconv.' % op.format)\n            die(1)\n\n        return outputfmt\n\n    def preserve(self, inputfn, outputfn):\n        # Get timestamp of input file.\n        s = os.stat(inputfn)\n        times = (s.st_atime, s.st_mtime)\n        mode = s.st_mode\n        # Set it to output file.\n        with open(outputfn, \"a\") as f:\n            os.utime(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.utime in os.supports_fd else inputfn,\n                     times=times)\n            os.chmod(f.fileno()\n                     if hasattr(os, \"supports_fd\") and os.chmod in os.supports_fd else inputfn,\n                     mode)\n\n    def convert(self, inputfn):\n        global exitcode\n\n        document = None\n        outputfmt = self.getformat(inputfn)\n\n        if op.verbose > 0:\n            print('Input file:', inputfn, file=sys.stderr)\n\n        try:\n            # Import phase.\n            phase = \"import\"\n\n            # Load inputfile.\n            inputprops = UnoProps(Hidden=True, ReadOnly=True, UpdateDocMode=op.updateDocMode)\n\n            if op.password:\n                inputprops += UnoProps(Password=op.password)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilteroptions:\n                # print \"Import filter options: %s\" % op.importfilteroptions\n                inputprops += UnoProps(FilterOptions=op.importfilteroptions)\n\n            # Cannot use UnoProps for FilterData property.\n            if op.importfilter:\n                inputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.importfilter), ), 0), )\n\n            if op.importfiltername:\n                importformat = self.getimportformat()\n                inputprops += UnoProps(FilterName=importformat.filter)\n\n            if op.stdin:\n                inputStream = self.svcmgr.createInstanceWithContext(\"com.sun.star.io.SequenceInputStream\", self.context)\n                inputStream.initialize((uno.ByteSequence(inputfn),))\n                inputprops += UnoProps(InputStream=inputStream)\n                inputurl = 'private:stream'\n            elif os.path.exists(inputfn):\n                inputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(inputfn))\n            else:\n                inputurl = inputfn\n            document = self.desktop.loadComponentFromURL(inputurl, \"_blank\", 0, inputprops)\n\n            if not document:\n                raise UnoException(\"The document '%s' could not be opened.\" % inputurl, None)\n\n            # Import style template.\n            phase = \"import-style\"\n            if op.template:\n                if os.path.exists(op.template):\n                    info(1, \"Template file: %s\" % op.template)\n                    templateprops = UnoProps(OverwriteStyles=True)\n                    templateurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(op.template))\n                    document.StyleFamilies.loadStylesFromURL(templateurl, templateprops)\n                else:\n                    print('unoconv: template file `%s\\' does not exist.' % op.template, file=sys.stderr)\n                    exitcode = 1\n\n            # Force all cells to recalculate if we are able to. This will get rid of errors in cells.\n            # FIXME: We cannot recalculate the cells because it breaks issue #97 (cells get #VALUE)\n            # phase = \"recalculate\"\n            # try:\n                # document.calculateAll()\n            # except AttributeError:\n                # pass\n\n            # Update document links if appropriate\n            if op.updateDocMode != NO_UPDATE:\n                phase = \"update-links\"\n                try:\n                    document.updateLinks()\n                    # Found that when converting HTML files with external images, OO would only load five or six of\n                    # the images in the file. In the resulting document, the rest of the images did not appear. Cycling\n                    # through all the image references in the document seems to force OO to actually load them. Found\n                    # some helpful guidance in this thread:\n                    # https://forum.openoffice.org/en/forum/viewtopic.php?f=30&t=23909\n                    # Ideally we would like to have the option to embed the images into the document, but I have not been\n                    # able to figure out how to do this yet.\n                    if op.updatehtmllinks:\n                        graphObjs = document.GraphicObjects\n                        for i in range(0, graphObjs.getCount()):\n                            graphObj = graphObjs.getByIndex(i)\n                except AttributeError:\n                    # the document doesn't implement the XLinkUpdate interface\n                    pass\n\n            # Add/Replace variables\n            phase = \"replace-fields\"\n            for f in op.fields:\n                try:\n                    field = document.TextFieldMasters.getByName(\"com.sun.star.text.fieldmaster.User.%s\" % f)\n                    field.setPropertyValue('Content', op.fields[f])\n                except UnoException:\n                    error(\"unoconv: failed to replace variable '%s' with value '%s' in the document.\" % (f, op.fields[f]))\n                    pass\n\n            # Add/Replace metadata\n            phase = \"replace-metadata\"\n            props = document.getDocumentProperties()\n            user_props = props.getUserDefinedProperties()\n            for prop, value in op.metadata.items():\n                for container in (props, user_props):\n                    curr = getattr(container, prop, None)\n                    if curr is not None:\n                        setattr(container, prop, value)\n                        break\n                else:\n                    user_props.addProperty(prop, 0, '')\n                    user_props.setPropertyValue(prop, value)\n\n            # Update document indexes\n            phase = \"update-indexes\"\n            for ii in range(2):\n                # At first, update Table-of-Contents.\n                # ToC grows, so page numbers grow too.\n                # On second turn, update page numbers in ToC.\n                try:\n                    document.refresh()\n                    indexes = document.getDocumentIndexes()\n                except AttributeError:\n                    # The document doesn't implement the XRefreshable and/or\n                    # XDocumentIndexesSupplier interfaces\n                    break\n                else:\n                    for i in range(0, indexes.getCount()):\n                        indexes.getByIndex(i).update()\n\n            info(1, \"Selected output format: %s\" % outputfmt)\n            info(2, \"Selected office filter: %s\" % outputfmt.filter)\n            info(2, \"Used doctype: %s\" % outputfmt.doctype)\n\n            # Document properties phase\n            phase = \"disable-showchanges\"\n            try:\n                document.ShowChanges = False\n            except AttributeError:\n                pass\n\n            # Export phase\n            phase = \"export\"\n\n            outputprops = UnoProps(FilterName=outputfmt.filter, OutputStream=OutputStream(), Overwrite=True)\n\n            # Set default filter options\n            if op.exportfilteroptions:\n                # print \"Export filter options: %s\" % op.exportfilteroptions\n                outputprops += UnoProps(FilterOptions=op.exportfilteroptions)\n            elif outputfmt.filter == 'Text (encoded)':\n                outputprops += UnoProps(FilterOptions=\"UTF8,LF\")\n            elif outputfmt.filter == 'Text':\n                outputprops += UnoProps(FilterOptions=\"UTF8\")\n            elif outputfmt.filter == 'Text - txt - csv (StarCalc)':\n                outputprops += UnoProps(FilterOptions=\"44,34,UTF8\")\n\n            # Set printer options\n            if op.setprinter:\n                printer = document.getPrinter()\n                for i in range(len(printer)):\n                    if printer[i].Name == 'PaperOrientation' and op.paperorientation is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperOrientation', op.paperorientation)\n                    elif printer[i].Name == 'PaperFormat' and op.paperformat is not None:\n                        printer[i].Value = uno.Enum('com.sun.star.view.PaperFormat', op.paperformat)\n                    elif (printer[i].Name == 'PaperSize' and op.papersize is not None and len(op.papersize) == 2):\n                        printer[i].Value.Width = op.papersize[0]\n                        printer[i].Value.Height = op.papersize[1]\n                document.setPrinter(printer)\n\n            # Cannot use UnoProps for FilterData property\n            if op.exportfilter:\n                outputprops += (PropertyValue(\"FilterData\", 0, uno.Any(\"[]com.sun.star.beans.PropertyValue\", tuple(op.exportfilter), ), 0), )\n\n            if op.stdout:\n                # Ensure binary data to stdout works\n                # http://stackoverflow.com/questions/2374427/python-2-x-write-binary-output-to-stdout\n                if sys.platform == \"win32\":\n                    import msvcrt\n                    msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n                outputurl = \"private:stream\"\n            else:\n                if os.path.exists(inputfn):\n                    (inbase, ext) = os.path.splitext(inputfn)\n                else:\n                    (inbase, ext) = os.path.splitext(os.path.basename(inputfn))\n                if op.output:\n                    (outbase, ext) = os.path.splitext(op.output)\n                    if len(op.filenames) > 1:\n                        outputfn = realpath(outbase, os.path.basename(inbase) + os.extsep + outputfmt.extension)\n                    else:\n                        outputfn = realpath(outbase + os.extsep + outputfmt.extension)\n                else:\n                    outputfn = realpath(inbase + os.extsep + outputfmt.extension)\n\n                outputurl = unohelper.absolutize(self.cwd, unohelper.systemPathToFileUrl(outputfn))\n\n            info(1, \"Output file: %s\" % outputurl)\n\n            try:\n                document.storeToURL(outputurl, tuple(outputprops))\n            except IOException as e:\n                raise UnoException(\"Unable to store document to %s (Error %s)\\n\\nProperties: %s\" % (outputurl, e.value, outputprops), None)\n\n            phase = \"dispose\"\n            document.dispose()\n            document.close(True)\n            if not op.stdout and op.preserve:\n                self.preserve(inputfn, outputfn)\n\n        except SystemError as e:\n            error(\"unoconv: SystemError during %s phase:\\n%s\" % (phase, e))\n            exitcode = 1\n\n        except RuntimeException as e:\n            error(\"unoconv: RuntimeException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 6\n\n        except DisposedException as e:\n            error(\"unoconv: DisposedException during %s phase:\\nOffice probably died. %s\" % (phase, e))\n            exitcode = 7\n\n        except IllegalArgumentException as e:\n            error(\"UNO IllegalArgument during %s phase:\\nSource file cannot be read. %s\" % (phase, e))\n            exitcode = 8\n\n        except IOException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: IOException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 3\n\n        except CannotConvertException as e:\n            # for attr in dir(e): print '%s: %s', (attr, getattr(e, attr))\n            error(\"unoconv: CannotConvertException during %s phase:\\n%s\" % (phase, e.Message))\n            exitcode = 4\n\n        except UnoException as e:\n            if hasattr(e, 'ErrCode'):\n                error(\"unoconv: UnoException during %s phase in %s (ErrCode %d)\" % (phase, repr(e.__class__), e.ErrCode))\n                exitcode = e.ErrCode\n                pass\n            if hasattr(e, 'Message'):\n                error(\"unoconv: UnoException during %s phase:\\n%s\" % (phase, e.Message))\n                exitcode = 5\n            else:\n                error(\"unoconv: UnoException during %s phase in %s\" % (phase, repr(e.__class__)))\n                exitcode = 2\n                pass\n\n\nclass Listener:\n    def __init__(self):\n        global product\n\n        info(1, \"Start listener on %s:%s\" % (op.server, op.port))\n        self.context = uno.getComponentContext()\n        self.svcmgr = self.context.ServiceManager\n        try:\n            resolver = self.svcmgr.createInstanceWithContext(\"com.sun.star.bridge.UnoUrlResolver\", self.context)\n            product = self.svcmgr.createInstance(\"com.sun.star.configuration.ConfigurationProvider\").createInstanceWithArguments(\"com.sun.star.configuration.ConfigurationAccess\", UnoProps(nodepath=\"/org.openoffice.Setup/Product\"))\n            try:\n                unocontext = resolver.resolve(\"uno:%s\" % op.connection)\n            except NoConnectException:\n                pass\n            else:\n                info(1, \"Existing %s listener found, nothing to do.\" % product.ooName)\n                return\n            if product.ooName != \"LibreOffice\" or LooseVersion(product.ooSetupVersion) <= LooseVersion('3.3'):\n                cmd = [office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nologo\", \"-nofirststartwizard\", \"-norestore\", \"-accept=%s\" % op.connection]\n            else:\n                cmd = [office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nologo\", \"--nofirststartwizard\", \"--norestore\", \"--accept=%s\" % op.connection]\n\n            # The rationale for using subprocess.Popen is to be able to handle\n            # a SIGTERM signal below and properly terminate the started office\n            # process then. This makes it possible to put the command unoconv -l\n            # under control of supervisor to deamonize it. Supervisor terminates\n            # via sending SIGTERM and sending SIGTERM to a running unoconv -l\n            # without the handler below will not terminate the office process\n            # together with it leaving the office process running.\n            office_process = subprocess.Popen(cmd, env=os.environ)\n\n            def sigterm_handler(signum, frame):\n                office_process.terminate()\n                die(6, 'Exiting on SIGTERM')\n\n            signal.signal(signal.SIGTERM, sigterm_handler)\n\n            ret = office_process.wait()\n            if ret == 81:\n                info(1, \"Restarting %s (first start - 81 exit code)\" % product.ooName)\n                office_process = subprocess.Popen(cmd, env=os.environ)\n                office_process.wait()\n            else:\n                raise Exception(\"%s crashed - exit code: %s\" % (product.ooName, ret))\n        except Exception as e:\n            error(\"Launch of %s failed.\\n%s\" % (office.binary, e))\n\n\ndef error(msg, file=sys.stderr):\n    \"\"\"Output error message.\"\"\"\n    print(msg, file=file)\n\n\ndef info(level, msg):\n    \"\"\"Output info message.\"\"\"\n    if 'op' not in globals():\n        pass\n    elif op.verbose >= 3 and level >= 3:\n        print(\"DEBUG:\", msg, file=sys.stderr)\n    elif not op.stdout and level <= op.verbose:\n        print(msg, file=sys.stdout)\n    elif level <= op.verbose:\n        print(msg, file=sys.stderr)\n\n\ndef die(ret, msg=None):\n    \"\"\"Print optional error and exit with errorcode.\"\"\"\n    global convertor, ooproc, office\n\n    if msg:\n        error('Error: %s' % msg)\n\n    # Did we start our own listener instance?\n    if not op.listener and ooproc and convertor:\n\n        # If there is a GUI now attached to the instance, disable listener.\n        if convertor.desktop.getCurrentFrame():\n            info(2, 'Trying to stop %s GUI listener.' % product.ooName)\n            try:\n                if product.ooName != \"LibreOffice\" or product.ooSetupVersion <= 3.3:\n                    subprocess.Popen([office.binary, \"-headless\", \"-invisible\", \"-nocrashreport\", \"-nodefault\", \"-nofirststartwizard\", \"-nologo\", \"-norestore\", \"-unaccept=%s\" % op.connection], env=os.environ)\n                else:\n                    subprocess.Popen([office.binary, \"--headless\", \"--invisible\", \"--nocrashreport\", \"--nodefault\", \"--nofirststartwizard\", \"--nologo\", \"--norestore\", \"--unaccept=%s\" % op.connection], env=os.environ)\n                ooproc.wait()\n                info(2, '%s listener successfully disabled.' % product.ooName)\n            except Exception as e:\n                error(\"Terminate using %s failed.\\n%s\" % (office.binary, e))\n\n        # If there is no GUI attached to the instance, terminate instance.\n        else:\n            info(3, 'Terminating %s instance.' % product.ooName)\n            try:\n                convertor.desktop.terminate()\n            except DisposedException:\n                info(2, '%s instance unsuccessfully closed, sending TERM signal.' % product.ooName)\n                try:\n                    ooproc.terminate()\n                except AttributeError:\n                    os.kill(ooproc.pid, 15)\n            info(3, 'Waiting for %s instance to exit.' % product.ooName)\n            ooproc.wait()\n\n        # LibreOffice processes may get stuck and we have to kill them.\n        # Is it still running?\n        if ooproc.poll() is None:\n            info(1, '%s instance still running, please investigate...' % product.ooName)\n            ooproc.wait()\n            info(2, '%s instance unsuccessfully terminated, sending KILL signal.' % product.ooName)\n            try:\n                ooproc.kill()\n            except AttributeError:\n                os.kill(ooproc.pid, 9)\n            info(3, 'Waiting for %s with pid %s to disappear.' % (ooproc.pid, product.ooName))\n            ooproc.wait()\n\n    # Allow Python GC to garbage collect pyuno object *before* exit call\n    # which avoids random segmentation faults --vpa\n    convertor = None\n\n    sys.exit(ret)\n\n\ndef main():\n    global convertor, exitcode\n    convertor = None\n\n    try:\n        if op.listener:\n            listener = Listener()\n\n        if op.stdin:\n            # Read stdin buffer in Python 3 in order to correctly handle binary streams.\n            # ref: https://docs.python.org/3.1/library/sys.html#sys.stdin\n            if sys.version_info.major > 2:\n                inputfn = sys.stdin.buffer.read()\n            else:\n                inputfn = sys.stdin.read()\n            convertor = Convertor()\n            convertor.convert(inputfn)\n        elif op.filenames:\n            convertor = Convertor()\n            for inputfn in op.filenames:\n                convertor.convert(inputfn)\n\n    except NoConnectException:\n        error(\"unoconv: could not find an existing connection to LibreOffice at %s:%s.\" % (op.server, op.port))\n        if op.connection:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"%s\\\"\" % (op.server, op.server, op.port, op.connection))\n        else:\n            info(0, \"Please start an LibreOffice instance on server '%s' by doing:\\n\\n    unoconv --listener --server %s --port %s\\n\\nor alternatively:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=%s,port=%s;urp;\\\"\" % (op.server, op.server, op.port, op.server, op.port))\n            info(0, \"Please start an soffice instance on server '%s' by doing:\\n\\n    soffice -nologo -nodefault -accept=\\\"socket,host=127.0.0.1,port=%s;urp;\\\"\" % (op.server, op.port))\n        exitcode = 1\n    # except UnboundLocalError:\n        # die(252, \"Failed to connect to remote listener.\")\n    except OSError:\n        error(\"Warning: failed to launch Office suite. Aborting.\")\n\n\n# Main entrance\nif __name__ == '__main__':\n    exitcode = 0\n\n    info(3, 'sysname=%s, platform=%s, python=%s, python-version=%s' % (os.name, sys.platform, sys.executable, sys.version))\n\n    for of in find_offices():\n        if of.python != sys.executable and not sys.executable.startswith(of.basepath):\n            python_switch(of)\n        office_environ(of)\n        # debug_office()\n        try:\n            import uno\n            import unohelper\n            office = of\n            break\n        except:\n            # debug_office()\n            print(\"unoconv: Cannot find a suitable pyuno library and python binary combination in %s\" % of, file=sys.stderr)\n            print(\"ERROR:\", sys.exc_info()[1], file=sys.stderr)\n            print(file=sys.stderr)\n    else:\n        # debug_office()\n        print(\"unoconv: Cannot find a suitable office installation on your system.\", file=sys.stderr)\n        print(\"ERROR: Please locate your office installation and send your feedback to:\", file=sys.stderr)\n        print(\"       http://github.com/dagwieers/unoconv/issues\", file=sys.stderr)\n        sys.exit(1)\n\n    # Working pyuno library found. Import classes.\n    from com.sun.star.beans import PropertyValue\n    from com.sun.star.connection import NoConnectException\n    from com.sun.star.document.UpdateDocMode import NO_UPDATE, QUIET_UPDATE\n    from com.sun.star.io import IOException, XOutputStream\n    from com.sun.star.lang import DisposedException, IllegalArgumentException\n    from com.sun.star.script import CannotConvertException\n    from com.sun.star.uno import Exception as UnoException\n    from com.sun.star.uno import RuntimeException\n\n    # Build on imported classes.\n    class OutputStream(unohelper.Base, XOutputStream):\n        def __init__(self):\n            self.closed = 0\n\n        def closeOutput(self):\n            self.closed = 1\n\n        def writeBytes(self, seq):\n            try:\n                sys.stdout.buffer.write(seq.value)\n            except AttributeError:\n                sys.stdout.write(seq.value)\n\n        def flush(self):\n            pass\n\n    def UnoProps(**args):\n        props = []\n        for key in args:\n            prop = PropertyValue()\n            prop.Name = key\n            prop.Value = args[key]\n            props.append(prop)\n        return tuple(props)\n\n    op = Options(sys.argv[1:])\n\n    info(2, \"Using office base path: %s\" % office.basepath)\n    info(2, \"Using office binary path: %s\" % office.unopath)\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        die(6, 'Exiting on user request')\n    die(exitcode)\n"}
{"type": "source_file", "path": "dockerized/web_app/app.py", "content": "from flask import Flask, render_template, request, redirect, url_for, Response, stream_with_context\nfrom flask import send_from_directory\nfrom flask import jsonify\n\nfrom pdfminer.high_level import extract_text\nfrom werkzeug.utils import secure_filename\n\nfrom whoosh.fields import Schema, TEXT, ID, NUMERIC\nfrom whoosh.qparser import MultifieldParser\nfrom whoosh.qparser import QueryParser\nfrom whoosh.index import create_in\nfrom whoosh.index import open_dir\n\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.document_loaders import TextLoader\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import ConversationChain\nfrom langchain.llms import CTransformers\n\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport pytesseract\n\nimport fitz # PyMuPDF\n\nfrom urllib.parse import unquote\nfrom threading import Thread\nimport subprocess\nimport threading\nimport traceback\nimport platform\nimport tempfile\nimport datetime\nimport requests\nimport logging\nimport sqlite3\nimport signal\nimport PyPDF2\nimport base64\nimport queue\nimport uuid\nimport json\nimport time\nimport nltk\nimport zlib\nimport ast\nimport sys\nimport os\nimport io\nimport re\n\nfrom logging.handlers import RotatingFileHandler\nfrom nltk.corpus import stopwords\n\n\n\napp = Flask(__name__)\n\n# Route for the home page, rendering the initial model selection form (legacy)\n@app.route('/')\ndef index():\n    return render_template('chat.html')\n\n# model_selection.html triggers window.location.href to '/chat', which triggers this route, which loads the chat.html template at the end!\n@app.route('/chat')\ndef chat():\n    return render_template('chat.html')\n\n# Route to display the file loading form\n@app.route('/load_file')\ndef load_file():\n    return render_template('model_selection.html', show_file_form=True)\n\n@app.route('/download/<filename>')\ndef download_file(filename):\n    # return send_from_directory(app.config['UPLOAD_FOLDER'], filename, as_attachment=True)\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename, as_attachment=False, mimetype='application/pdf')\n\n@app.route('/pdf/<filename>')\ndef pdf_viewer(filename):\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename)\n\n\n\n#########################------------------GLOBALS!----------------------###############################\nLLAMA_CPP_PROCESS = None\nLLM = None\nCHAT_ID = None\nSEQUENCE_ID = None\nLOADED_UP = False\nLLM_LOADED_UP = False\nVECTORDB_LOADED_UP = False\nLLM_CHANGE_RELOAD_TRIGGER_SET = False\nVECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\nVECTOR_STORE = None\nHF_BGE_EMBEDDINGS = None\nAZURE_OPENAI_EMBEDDINGS = None\nHISTORY_MEMORY_WITH_BUFFER = None   #Init in load_model_and_vectordb(); reset in load_chat_history() when old chats loaded, and in load_model_and_vectordb() when 'New Chat' selected; used for non-RAG convChain init in stream, and for saving context in stream for RAG chains and lastly, for setting HISTORY_SUMMARY in stream() via load_memory_variables({})\nHISTORY_SUMMARY = {}    #Set in stream() via HISTORY_MEMORY_WITH_BUFFER.load_memory_variables({}), and in load_chat_history() from chat_history DB; cleared in load_model_and_vectordb() when 'New Chat' selected; used to init prompt templates in stream() and lastly, for storage to chat_history DB in stream() and get_references()\n\n# Dict for user queries:  queries[session_id] = user_input\nQUERIES = {}\n#########################------------------------------------------------###############################\n\n\n\n#########################------------Setup & Handle Logging-------------###############################\ntry:\n    # 1 - Create a logger\n    logger = logging.getLogger('my_logger')\n    logger.setLevel(logging.ERROR)\n\n    # 2 - Create a RotatingFileHandler\n    # maxBytes: max file size of log file after which a new file is created; set to 1024 * 1024 * 5 for 5MB: 1024x1024 is 1MB, then a multiplyer for the number of MB\n    # backupCount: number of backup files to keep specifying how many old log files to keep\n    handler = RotatingFileHandler('server_log.log', maxBytes=1024*1024*5, backupCount=2)\n    handler.setLevel(logging.ERROR)\n\n    # 3 - Create a formatter and set it for the handler\n    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n    handler.setFormatter(formatter)\n\n    # 4 - Add the handler to the logger\n    logger.addHandler(handler)\n    # Logger ready! Usage: logger.error(f\"This is an error message with error {e}\")\nexcept Exception as e:\n    print(f\"\\n\\nCould not establish logger, encountered error: {e}\")\n\n\ndef handle_api_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    #full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    return jsonify(success=False, error=error_message), 500 #internal server error\n\n\ndef handle_local_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    raise Exception(exception)\n\n\ndef handle_error_no_return(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n\n#########################-------------------------------------###############################\n\n\ndocker_only_config_path = '/app/storage/config.json'    # we already know that a docker env is linux-debian, no need to wait for platform detection logic further below!\ndocker_directory = os.path.dirname(docker_only_config_path)\n\ntry:\n    os.makedirs(docker_directory, exist_ok=True)    # Creates the directory, and all directories in the specified path if they don't exist, no errors otherwise\nexcept Exception as e:\n    handle_local_error(\"Failed to create base app dir in the docker storage volume, encountered error: \", e)\n\nif not os.path.exists(docker_only_config_path):\n    try:\n        with open(docker_only_config_path, 'w') as file:\n            json.dump({}, file)\n    except Exception as e:\n        handle_error_no_return(\"Could not init config.json. Multiple app restarts may be required to get the app to init correctly. Printing error and proceeding: \", e)\n\n\n\n# Method to write to config.json | input- dict of key:values to be written to config.json\ndef write_config(config_updates, filename=docker_only_config_path):\n\n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        config = {}     #init emply config dict\n        handle_error_no_return(\"Could not read config.json when attempting to write, encountered error: \", e)\n        \n    restart_required = False\n    if LLM_LOADED_UP:\n        llm_trigger_keys_for_app_restart = ['use_local_llm', 'use_azure_open_ai', 'use_gpu', 'model_choice', 'local_llm_chat_template_format', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'base_template']\n                \n        for key in llm_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global LLM_CHANGE_RELOAD_TRIGGER_SET\n                LLM_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n    \n    if VECTORDB_LOADED_UP:\n        vectordb_trigger_keys_for_app_restart = ['embedding_model_choice']\n\n        for key in vectordb_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n                VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n\n    config.update(config_updates)\n\n    # Write updated config.json:\n    try:\n        with open(filename, 'w') as file:\n            json.dump(config, file, indent=4)\n    except Exception as e:\n        handle_local_error(\"Could not update config.json, encountered error: \", e)\n     \n    return {'success': True, 'restart_required':restart_required}\n            \n\n# Method to read from config.json | input- list of keys to be read from config.json; output- dict of key:value pairs; MANAGE DEFAULTS HERE!\ndef read_config(keys, default_value=None, filename=docker_only_config_path):\n    \n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        handle_error_no_return(\"Could not read config.json, encountered error: \", e)\n        return {key: default_value for key in keys}     #because a read scenario wherein config.json does not exist shouldn't occur!\n    \n    return_dict = {}\n    update_config_dict = {}\n    base_directory = config.get('base_directory', '/app/storage')   # specifying default if not found\n\n    for key in keys:\n        if key in config:\n            return_dict[key] = config[key]\n        else:\n            default_value = {\n                'windows_base_directory':'C:/web_app_storage',\n                'unix_and_docker_base_directory':'/app/storage',\n                'mac_base_directory':'app',\n                'upload_folder':base_directory + '/uploaded_pdfs',\n                'vectordb_sbert_folder':base_directory + '/chroma_db_250_sbert_embeddings',\n                'vectordb_openai_folder':base_directory + '/chroma_db_openai_embeddings',\n                'vectordb_bge_large_folder':base_directory + '/chroma_db_bge_large_embeddings',\n                'vectordb_bge_base_folder':base_directory + '/chroma_db_bge_base_embeddings',\n                'index_dir':base_directory + '/indexdir_main',\n                'sqlite_images_db':base_directory + '/images_database_main.db',\n                'sqlite_history_db':base_directory + '/chat_history.db',\n                'sqlite_docs_loaded_db':base_directory + '/docs_loaded.db',\n                'model_dir':base_directory + '/models',\n                'highlighted_docs':base_directory + '/highlighted_pdfs',\n                'ocr_pdfs':base_directory + '/ocr_pdfs',\n                'pdfs_to_txts':base_directory + '/pdfs_to_txts',\n                'model_choice':'Meta-Llama-3-8B-Instruct.f16.gguf',\n                'do_rag':True,\n                'force_enable_rag':False,\n                'force_disable_rag':False,\n                'use_local_llm':True,\n                'use_gpu':True,\n                'use_gpu_for_embeddings':False,\n                'azure_cv_free_tier':True,\n                'use_azure_open_ai':False,\n                'use_openai_embeddings':False,\n                'azure_openai_api_type':'azure',\n                'azure_openai_api_version':'2023-05-15',\n                'azure_openai_max_tokens':4096,\n                'azure_openai_temperature':0.7,\n                'use_bge_large_embeddings':False,\n                'use_bge_base_embeddings':False,\n                'use_sbert_embeddings':True,\n                'embedding_model_choice':'sbert_mpnet_base_v2',\n                'use_ocr':False,\n                'ocr_service_choice':'None',\n                'local_llm_model_type':'llama',\n                'local_llm_chat_template_format':'llama3',\n                'local_llm_context_length':8192,\n                'local_llm_max_new_tokens':2048,\n                'local_llm_gpu_layers':47,\n                'local_llm_temperature':0.8,\n                'local_llm_top_k':40,\n                'local_llm_top_p':0.95,\n                'local_llm_min_p':0.05,\n                'local_llm_n_keep':0,\n                'server_timeout_seconds':10,\n                'server_retry_attempts':3,\n                'base_template':\"Answer the user's question in as much detail as possible.\",\n            }.get(key, 'undefined')\n\n            if default_value == 'undefined':\n                raise KeyError(f\"Key \\'{key}\\' not found in config.json and no default value has been defined either.\\n\")\n            \n            return_dict[key] = default_value\n            update_config_dict[key] = default_value\n\n    if update_config_dict:\n        # Write Defaults\n        try:\n            write_config(update_config_dict)\n        except Exception as e:\n            handle_error_no_return(\"Could not write defaults to config.json. Encountered error: \", e)\n    \n    ##print(f\"return_dict: {return_dict}\")\n\n    return return_dict\n\n\n# Method for API route to read from config.json\n# Deviates from typical RESTful principals to use a POST call to fetch values but practical & justifyable because we:\n# 1. Do not want to make the URL huge with a ever-growing list of query-params 2. Do not wish to expose values via query-params\n@app.route('/config_reader_api', methods=['POST'])\ndef config_reader_api():\n    # keys = request.args.getlist('keys') # Assuming keys are passed as query parameters\n    \n    try:\n        keys = request.json.get('keys', []) # Could also do keys = request.json['keys'] but this way we can provide a default list should 'keys' be missing!\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read keys for config_reader_api request. Encountered error:\", e)\n\n    try:\n        values = read_config(keys)  # send list of keys, get dict of key:values\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read keys from config.json. Encountered error: \", e)\n    \n    return jsonify(success=True, values=values)\n\n\n# Method for API route to write to config.json\n@app.route('/config_writer_api', methods=['POST'])\ndef config_writer_api():\n\n    try:\n        config_updates = request.json['config_updates']\n        print(f\"config_updates for config_writer_api: {config_updates}\")\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read values for config_writer_api request. Encountered error: \", e)\n    \n    try:\n        write_return = write_config(config_updates)\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not write keys to config.json. Encountered error: \", e)\n    \n    return jsonify({\"success\": write_return['success'], \"restart_required\": write_return['restart_required']})\n\n\n\n#########################------------Setup Directories-------------###############################\nBASE_DIRECTORY = \"\"\n\nif platform.system() == 'Windows':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = 'C:/temp_web_app_storage'\n    try:\n        read_return = read_config(['windows_base_directory'])   #passing list of values to read\n        BASE_DIRECTORY = str(read_return['windows_base_directory']) #received dict of key:values\n    except Exception as e:\n        handle_local_error(\"Could not read windows_base_directory on boot, encountered error: \", e)\n\nelif platform.system() == 'Linux':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = '/app/storage'\n    try:\n        read_return = read_config(['unix_and_docker_base_directory'])\n        BASE_DIRECTORY = str(read_return['unix_and_docker_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read unix_and_docker_base_directory on boot, encountered error: \", e)\n\nelse:   #Likely 'Darwin' and hence MacOS\n    #BASE_DIRECTORY = 'app'\n    try:\n        read_return = read_config(['mac_base_directory'])\n        BASE_DIRECTORY = str(read_return['mac_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read mac_base_directory on boot, encountered error: \", e)\n\ntry:\n    write_config({'base_directory':BASE_DIRECTORY})\nexcept Exception as e:\n    handle_local_error(\"Could not write OS BASE_DIRECTORY on boot, encountered error: \", e)\n\n\n###---Notes on the above workflow:---###\n# 1. Everytime the app runs, the OS platform is detected\n# 2. Following which the apporpriate base directory is requested as above\n# 3. If this is the very first run:\n#   a. read_config does not find the directory data in config.json\n#   b. the else clause is triggered and defaults set for both, write_config and return\n# 4. If this isn't the very first run:\n#   a. read_config simply returns the OS specific directory - this allows the user to update the directory via config.json!\n# 4. On return, BASE_DIRECTORY is set and write_config has os specific directories set (windows_base_directory, unix_and_docker_base_directory, and mac_base_directory)\n# 5. write_config is invoked for BASE_DIRECTORY\n# 6. write_config detects a write-attempt for BASE_DIRECTORY and updates all related app directories too, which can be subsequently read as required\n# 7. This ensures that directories are set correctly at each run while also allowing the user to set their preferred directory via config.json\n\n\n# Having set the values for the directories above, proceed to actually create them on disk IF they don't alread exist!\nif not os.path.exists(BASE_DIRECTORY):\n\n    # Create a directory for app storage \n    try:\n        os.mkdir(BASE_DIRECTORY)\n    except Exception as e:\n        handle_local_error(\"Failed to create Base App Directory, encountered error: \", e)\n        \ntry:\n    read_return = read_config(['model_dir', 'highlighted_docs', 'upload_folder', 'ocr_pdfs', 'pdfs_to_txts', 'index_dir'])\n    model_dir = read_return['model_dir']\n    highlighted_docs = read_return['highlighted_docs']\n    upload_folder = read_return['upload_folder']\n    ocr_pdfs = read_return['ocr_pdfs']\n    pdfs_to_txts = read_return['pdfs_to_txts']\n    index_dir = read_return['index_dir']\nexcept Exception as e:\n    handle_local_error(\"Could not read paths for app directories (model_dir, highlighted_docs, upload_folder) from config.json on boot, encountered error: \", e)\n\n\n# If the base directory does not currently exist...\nif not os.path.exists(model_dir):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(model_dir)\n    except Exception as e:\n        handle_local_error(\"Failed to create Model Directory (model_dir), encountered error: \", e)\n\n\n# If the highlighted_docs directory does not currently exist...\nif not os.path.exists(highlighted_docs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(highlighted_docs)\n    except Exception as e:\n        handle_local_error(\"Failed to create Highlighted Docs Directory (highlighted_docs), encountered error: \", e)\n\n\n# If the upload_folder directory does not currently exist...\nif not os.path.exists(upload_folder):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(upload_folder)\n    except Exception as e:\n        handle_local_error(\"Failed to create Uploaded Docs Directory (upload_folder), encountered error: \", e)\n        \n\n# If the ocr_pdfs directory does not currently exist...\nif not os.path.exists(ocr_pdfs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(ocr_pdfs)\n    except Exception as e:\n        handle_local_error(\"Failed to create OCR'ed Docs Directory (ocr_pdfs), encountered error: \", e)\n\n\n# If the pdfs_to_txts directory does not currently exist...\nif not os.path.exists(pdfs_to_txts):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(pdfs_to_txts)\n    except Exception as e:\n        handle_local_error(\"Failed to create txt-docs Directory (pdfs_to_txts), encountered error: \", e)\n\n\n# If the index does not currently exist...\nif not os.path.exists(index_dir):\n\n    # Define the Index schema: what fields it contains\n    schema = Schema(title=ID(unique=True, stored=True), content=TEXT(stored=True), pagenumber=NUMERIC(stored=True))\n    \n    # Create a directory for persistent storage of the index to disk\n    try:\n        os.mkdir(index_dir)\n    except Exception as e:\n        handle_local_error(\"Failed to create directory for the Whoosh Index, encountered error: \", e)\n\n    # Create the index based on the schema definted above\n    try:\n        create_in(index_dir, schema)\n    except Exception as e:\n        handle_local_error(\"Failed to create Whoosh Index, encountered error: \", e)\n\n\napp.config['UPLOAD_FOLDER'] = upload_folder\napp.config['DOWNLOAD_FOLDER'] = highlighted_docs\n\n\ndef clean_text_string(text_to_be_cleaned):\n    \n    # Clean text\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"\", \"\").replace(\"\", \"\").replace(\"\", \"\")\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"Confidential Copy \\n            for \\n         DKPPU\", \"\")\n    #clean_text = re.sub(r'\\n(?=[a-z.])', ' ', text)     # replaces newline chars immediately followed by a small-letter or dot with a space as they're likely to be the same sentence split-up across lines.\n    clean_text = re.sub(r'\\n+', '\\n', text_to_be_cleaned)\n\n    # This regex substitutes anything that is not a word character or whitespace with an empty string.\n    clean_text = re.sub(r'[^\\w\\s]', ' ', clean_text)\n\n    # This regex substitutes any sequence of whitespace characters with a single space.\n    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n\n    return clean_text\n\ndef whoosh_indexer(pdf_data):\n\n    print(\"\\n\\nIndexing File\\n\\n\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for whoosh_indexer. Error: \", e)\n\n    # Define the Index schema: what fields it contains\n    schema = Schema(title=ID(unique=True, stored=True), content=TEXT(stored=True), pagenumber=NUMERIC(stored=True))\n\n    # If the index does not currently exist...\n    if not os.path.exists(index_dir):\n        \n        # Create a directory for persistent storage of the index to disk\n        try:\n            os.mkdir(index_dir)\n        except Exception as e:\n            handle_local_error(\"Failed to create directory for the Whoosh Index, encountered error: \", e)\n\n        # Create the index based on the schema definted above\n        try:\n            ix = create_in(index_dir, schema)\n        except Exception as e:\n            handle_local_error(\"Failed to create Whoosh Index, encountered error: \", e)\n            \n    else:\n        try:\n            ix = open_dir(index_dir)\n        except Exception as e:\n            handle_local_error(\"Failed to open Whoosh Index, encountered error: \", e)\n        \n    # init writer and write to the index:\n    try:\n        writer = ix.writer()\n        #searcher = ix.searcher()\n\n        for doc in pdf_data:\n            # query = QueryParser(\"title\", ix.schema).parse(doc[\"title\"])\n            # results = searcher.search(query)\n            # print(\"\\nAlready indexed page content, skipping\\n\")\n\n            #if not results:\n            writer.add_document(title=doc[\"title\"], content=doc[\"content\"], pagenumber=doc[\"pagenumber\"])\n\n        writer.commit()\n        #searcher.close()\n        \n    except Exception as e:\n        handle_local_error(\"Failed to write to Whoosh Index, encountered error: \", e)\n\n\ndef PDFtoAzureDocAiTXT(input_filepath):\n\n    print(\"\\n\\nProcessing Document - PDF to Azure DocAI TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_doc_ai_endpoint', 'azure_doc_ai_subscription_key', 'ocr_pdfs'])\n        azure_doc_ai_endpoint = read_return['azure_doc_ai_endpoint']\n        azure_doc_ai_subscription_key = read_return['azure_doc_ai_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureDocAiTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"Azure-OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        docai_client = DocumentAnalysisClient(azure_doc_ai_endpoint, AzureKeyCredential(azure_doc_ai_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure DocAI, encountered error: \", e)\n\n    try:\n        with open(input_filepath, \"rb\") as pdf_file:\n            # 1 - Get page count:\n            try:\n                pypdf_reader = PyPDF2.PdfReader(pdf_file)\n                page_count = len(pypdf_reader.pages)\n                page_range = f\"1-{page_count}\" if page_count > 1 else \"1\"\n                print(f\"page_range: {page_range}\")\n            except Exception as e:\n                handle_local_error(\"Could not get page count for call to Azure DocAI, encountered error: \", e)\n\n            # 2 - Reset file-read stream's internal pointer, which has now been set to the end of the file due to the above read operation!\n            pdf_file.seek(0)\n\n            # 3 - Call Azure DocAI:\n            try:\n                poller = docai_client.begin_analyze_document(\"prebuilt-layout\", pdf_file, pages=page_range)\n                result = poller.result()\n            except Exception as e:\n                handle_local_error(\"Could not get results for begin_analyze_document for Azure DocAI, encountered error: \", e)\n\n        # print(f\"result: \\n{result}\")\n\n        used_regions = set()   # set will avoid duplicates\n\n        if hasattr(result, 'tables'):\n            for table in result.tables:\n                #print(\"Found table\")\n                if table.cells:     # Check if there are cells in the table \n                    for cell in table.cells:\n                        #print(f\"Row {cell.row_index}, Column {cell.column_index}, Text: {cell.content}\")\n                        cell_text = f'Row {cell.row_index}, Column {cell.column_index}: {cell.content}'\n                        \n                        try:\n                            output_text_file.write(cell_text + '\\n')\n                        except Exception as e:\n                            handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n                        # Get page number\n                        page_number = \"\"\n                        if cell.bounding_regions:   # Check if there are bounding regions\n                            for region in cell.bounding_regions:\n                                page_number = region.page_number\n                                cell_polygon = region.polygon\n                                cell_polygon_tuple = tuple((point.x, point.y) for point in cell_polygon)    # lists aren't hashable to cast to a tuple\n                                used_regions.add(cell_polygon_tuple)\n\n                        # Whoosh prep\n                        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": cell_text, \"pagenumber\":page_number}\n                        pdf_data.append(whoosh_page_dict_entry)\n\n        # Get paragraphs\n        if hasattr(result, 'paragraphs'):\n            for paragraph in result.paragraphs:\n                para_page_number = paragraph.bounding_regions[0].page_number\n                para_polygon = paragraph.bounding_regions[0].polygon\n                para_polygon_tuple = tuple((point.x, point.y) for point in para_polygon)\n                \n                if para_polygon_tuple in used_regions:\n                    continue\n                \n                para_content = paragraph.content\n                #print(f\"\\n---Processing Page: {para_page_number}---\\n\")\n                #print(f\"paragraph: {para_content}\")\n\n                # write the extracted text to the file:\n                try:\n                    output_text_file.write(para_content + '\\n')\n                    used_regions.add(para_polygon_tuple)\n                except Exception as e:\n                    handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n                # whoosh prep\n                whoosh_page_dict_entry = {\"title\": source_filename, \"content\": para_content, \"pagenumber\":para_page_number}\n                pdf_data.append(whoosh_page_dict_entry)\n\n    except Exception as e:\n        handle_local_error(\"Error processing document with azure DocAI: \", e)\n\n    # Close all files\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef PDFtoAzureOCRTXT(input_filepath):\n    \n    print(\"\\n\\nProcessing Document - PDF to Azure OCR TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_ocr_endpoint', 'azure_ocr_subscription_key', 'ocr_pdfs', 'azure_cv_free_tier'])\n        azure_ocr_endpoint = read_return['azure_ocr_endpoint']\n        azure_ocr_subscription_key = read_return['azure_ocr_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n        azure_cv_free_tier = read_return['azure_cv_free_tier']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureOCRTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Convert PDF to  a list of images\n    try:\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\n    except Exception as e:\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        computervision_client = ComputerVisionClient(azure_ocr_endpoint, CognitiveServicesCredentials(azure_ocr_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure OCR, encountered error: \", e)\n    \n    calls_made = 0\n\n    # Iterate over each page and apply OCR:\n    print(\"\\n\\nBeginning image to Text OCR\\n\\n\")\n    for page_number, image in enumerate(pages, start = 1):\n        \n        # Convert to bytes and create a stream\n        try:\n            img_stream = io.BytesIO()\n            image.save(img_stream, format='PNG')\n            img_stream.seek(0)  # Reset the stream position to the beginning\n        except Exception as e:\n            handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n            continue\n\n        # Send to Azure OCR\n        try:\n            if azure_cv_free_tier:\n                if calls_made < 20:\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made += 1\n                else:\n                    print(\"Sleeping for 60secs due to AzureOCR free-tier restrictions!\")\n                    time.sleep(63)  #free tier restrictions!\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made = 1  #reset counter\n            else:\n                print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n        except Exception as e:\n            handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n\n        for region in result.regions:\n            for line in region.lines:\n                #print(\" \".join([word.text for word in line.words]))\n\n                try:\n                    clean_text = str(\" \".join([word.text for word in line.words]))\n                except Exception as e:\n                    handle_error_no_return(\"Could not obtain line from Azure OCR result, encountered error: \", e)\n                    continue\n\n                # Write the extracted text to the file:\n                try:\n                    output_text_file.write(clean_text + '\\n')\n                except Exception as e:\n                    handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n                # Whoosh prep\n                #whoosh_clean_text = preprocess_string(clean_text)\n                whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_number}\n                pdf_data.append(whoosh_page_dict_entry)\n\n    # Close all files\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef PDFtoTXT(input_file):\n\n    print(\"\\n\\nProcessing Document - PDF to TXT\\n\\n\")\n\n    try:\n        read_return = read_config(['pdfs_to_txts'])\n        pdfs_to_txts = read_return['pdfs_to_txts']\n    except Exception as e:\n        handle_local_error(\"Missing pdfs_to_txts directory for PDFtoTXT in config.json, encountered error: \", e)\n    \n    # Initialize PDF file reader\n    try:\n        pdf_file = open(input_file, 'rb')\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_file)\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    # Initialize PDF reader\n    try:\n        pdf_reader = PyPDF2.PdfReader(pdf_file)\n    except Exception as e:\n        handle_local_error(\"Could not initialize PDF reader, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(pdfs_to_txts, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"PyPDF2-extracted .txt already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    # Loop through all the pages and extract text\n    for page_num in range(len(pdf_reader.pages)):\n        \n        try:\n            page = pdf_reader.pages[page_num]\n            text = page.extract_text()\n        except Exception as e:\n            handle_error_no_return(\"Could not extract text from page, encountered error: \", e)\n\n        #clean_text = text\n        # Clean text\n        clean_text = clean_text_string(text)\n        \n        # Optionally, you can include page numbers in the text file\n        # output_text_file.write(f'\\n\\n--- Page {page_num + 1} ---\\n\\n')\n        \n        # Write the extracted text to the file\n        try:\n            output_text_file.write(clean_text + '\\n')\n        except Exception as e:\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n        # Whoosh prep\n        #whoosh_clean_text = preprocess_string(clean_text)\n        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_num+1}\n        pdf_data.append(whoosh_page_dict_entry)\n\n    # Close all files\n    pdf_file.close()\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef get_page_content_from_whoosh_index(title, pagenumber):\n\n    print(\"\\n\\nSearching Index for Page Content\\n\\n\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for get_page_content_from_whoosh_index. Error: \", e)\n\n    try:\n        ix = open_dir(index_dir)\n        searcher = ix.searcher()\n\n        parser = MultifieldParser([\"title\", \"pagenumber\"], schema=ix.schema)\n        query = parser.parse(f'title:\"{title}\" AND pagenumber:{pagenumber}')\n\n        results = searcher.search(query)\n\n        if results:\n            return results[0][\"content\"]\n        else:\n            return None\n    except Exception as e:\n        handle_local_error(\"Failed to open & search Whoosh Index for page content, encountered error: \", e)\n    finally:\n        searcher.close()\n\n\ndef extract_images_from_pdf(pdf_path):\n    \n    print(\"Extracting Images from PDF\")\n\n    try:\n        source_filename = os.path.basename(pdf_path)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n    \n    with open(pdf_path, 'rb') as file:\n        \n        try:\n            pdf_reader = PyPDF2.PdfReader(file)\n        except Exception as e:\n            handle_local_error(\"Could not read PDF, encountered error: \", e)\n\n        images = []\n\n        for page_num in range(len(pdf_reader.pages)):\n            page = pdf_reader.pages[page_num]\n            if '/XObject' in page['/Resources']:\n                xObject = page['/Resources']['/XObject'].get_object()\n                for obj in xObject:\n                    if xObject[obj]['/Subtype'] == '/Image':\n\n                        # Log details about the image object:\n                        try:\n                            image_obj = xObject[obj]\n                            obj_details = {\n                                'Object Reference': obj,\n                                'Width': image_obj.get('/Width', 'Unknown'),\n                                'Height': image_obj.get('/Height', 'Unknown'),\n                                'Color Space': image_obj.get('/ColorSpace', 'Unkown'),\n                                'Filter': image_obj.get('/Filter', 'Unknown'),\n                                'Bits Per Component': image_obj.get('/BitsPerComponent', 'Unknown')\n                            }\n                            #print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                            # data  = image_obj._data\n\n                            if obj_details['Filter'] == '/FlateDecode':\n                                #print(\"\\n\\nDecoding PNG!\\n\\n\")\n                                try:\n                                    data  = image_obj._data\n                                    decompressed_data = zlib.decompress(data)\n                                except Exception as e:\n                                    error_message = f\"\\n\\nPNG decompression exception: {e}\\n\\n\"\n                                    if logger:\n                                        logger.error(error_message)\n                                        print(error_message)\n                                    else:\n                                        print(error_message)\n                            else:\n                                decompressed_data  = image_obj._data\n\n                            text = page.extract_text()  \n                            # clean_text = text\n\n                            # Clean text\n                            clean_text = clean_text_string(text)\n\n                            # clean_text = get_page_content_from_whoosh_index(source_filename, page_num)\n\n                            try:\n                                if obj_details['Filter'] == '/FlateDecode':\n                                    # Determine Color Space:\n                                    color_space = image_obj.get('/ColorSpace')\n\n                                    if color_space == '/DeviceRGB':\n                                        mode = 'RGB'\n                                    elif color_space == '/DeviceCMYK':\n                                        mode = 'CMYK'\n                                    elif color_space == '/DeviceGray':\n                                        mode = 'L'\n                                    else:\n                                        mode = 'L'  # Default to grayscale if unsure\n\n                                    # Create image from bytes\n                                    image = Image.frombytes(mode, ((obj_details['Width']), (obj_details['Height'])), decompressed_data) # 'L' for 8-bit pixels, black and white\n                                    with io.BytesIO() as output:\n                                        image.save(output, format='JPEG')\n                                        binary_data = output.getvalue()\n                                        format = \"JPEG\"\n                                        images.append((binary_data,clean_text,format))\n\n                                else:\n                                    # Load image from bytes\n                                    image = Image.open(io.BytesIO(decompressed_data))\n\n                                    # Determine format (JPEG)\n                                    format = image.format\n                                    \n                                    #print(f\"\\n\\nImage format: {format}\\n\\n\")  # This will print the format\n\n                                    # If image loads, append image to images DB\n                                    images.append((decompressed_data,clean_text,format))\n\n                            except Exception as e:\n                                error_message = f\"\\n\\nEncountered unrecognized or invalid image data for object detailed below. Exception: {e}\\n\\n\"\n                                if logger:\n                                    logger.error(error_message)\n                                    logger.error(obj_details)\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                                else:\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n\n                        except Exception as e:\n                            handle_error_no_return(\"Could not process image object. Exception: \", e)\n\n        # print(\"Images array:\")\n        # print(images)\n        return images\n\n\ndef store_images_to_db(images):\n\n    print(\"\\n\\nStoring Images to Database\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to Images DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS images (\n                    id INTEGER PRIMARY KEY,\n                    image_data BLOB NOT NULL,\n                    surrounding_text TEXT,\n                    metadata TEXT,\n                    format TEXT\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create Images DB, encountered error: \", e)\n    \n    try:\n        for image_data, surrounding_text, format in images:\n            #print(\"surrounding_text: \", surrounding_text)\n            # Check if the image_data already exists in the database:\n            cursor.execute(\"SELECT COUNT(*) FROM images WHERE image_data = ?\", (image_data,))\n            if cursor.fetchone()[0] == 0:\n                print(\"\\nInserting new image into images DB\\n\")\n                cursor.execute(\"INSERT INTO images (image_data, surrounding_text, format) VALUES (?, ?, ?)\", (image_data, surrounding_text, format))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not store images to Images DB, encountered error: \", e)\n    finally:\n        conn.close()\n\n\ndef record_doc_loaded_to_db(document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap):\n\n    print(\"\\n\\nRecording document loading to records DB\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_docs_loaded_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to document_records DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS document_records (\n                    id INTEGER PRIMARY KEY,\n                    document_name TEXT NOT NULL,\n                    embedding_model TEXT NOT NULL,\n                    vectordb_used TEXT,\n                    chunk_size INTEGER,\n                    chunk_overlap INTEGER\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create document_records DB, encountered error: \", e)\n    \n    try:\n        cursor.execute(\"INSERT INTO document_records (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap) VALUES (?, ?, ?, ?, ?)\", (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap))\n        conn.commit()\n        conn.close()\n    except Exception as e:\n        handle_local_error(\"Could not update document_records DB, encountered error: \", e)\n\n\n\n# List-splitter function for a large number of embeddings!\ndef split_embeddings_list(all_splits, max_emmbeddings_list_size):\n    for i in range(0, len(all_splits), max_emmbeddings_list_size):  # Step through the large list in steps of max size\n        yield all_splits[i:i + max_emmbeddings_list_size]   # Yield a slice of all_splits from index i upto but NOT including i+max_size \n\n\n# Document vectorization and chunking\ndef LoadNewDocument(input_file):\n\n    global VECTOR_STORE\n    \n    ### L1 - Load Data from Source ###\n    #loader = UnstructuredPDFLoader(\"737.pdf\", mode=\"elements\", strategy=\"fast\")\n    print(\"\\nLoading Document\")\n    #loader.start()\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        handle_local_error(\"Missing values in config.json, could not LoadNewDocument. Error: \", e)\n\n    try:\n        txt_loader = TextLoader(input_file, encoding=\"UTF-8\", autodetect_encoding=\"true\")\n        docs = txt_loader.load()\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Failed to load document for storage to VectorDB, encountered error: \", e)\n    #finally:\n        #loader.stop()\n\n    chunk_sz = 250\n    chunk_olp = 0\n\n    ### L2 - Chunk Source Data ###\n    print(\"Chunking Doc\")\n    #loader.start()\n    try:\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_sz, chunk_overlap = chunk_olp) # chunk_size refers to max size; nice to have some small sliding-window overlap between chunks such as 20-50 chars\n        all_splits = text_splitter.split_documents(docs)\n        \n        #For JINA2 Embeddings:\n        # Initialize an empty list to hold the page_content values\n        page_contents = []\n        # Iterate through each Document in the list\n        for doc in all_splits:\n            # Access the 'page_content' attribute and append it to the 'page_contents' list\n            page_contents.append(doc.page_content)\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Failed to chunk document for storage to VectorDB, encountered error: \", e)\n    #finally:\n        #oader.stop()\n\n\n    ### L3 - Store Chunks in VectorDB ###\n    print(\"Storing to VectorDB: ChromaDB\")\n    #chroma_persist_directory=upload_folder + '/chroma_db_250'\n    #loader.start()\n    try:\n        # Return VectorStore initialized from documents and embeddings.\n        if use_sbert_embeddings:\n            # Ideally should use MAX_BATCH_SIZE obtained elsewhere \n            if len(all_splits) > 5000:\n                split_docs = split_embeddings_list(all_splits, 5000)\n                for split_docs_list in split_docs:\n                    VECTOR_STORE = Chroma.from_documents(documents=split_docs_list, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n            else:\n                VECTOR_STORE = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n        \n        elif use_openai_embeddings:\n            print(\"Using OpenAI Text Ada Model via Azure OpenAI\")\n\n            list_position = 0\n            token_count = 0\n\n            for i in range(list_position, len(all_splits)):\n\n                token_count += len(str(all_splits[i]))\n                if token_count >= 108000:\n                    VECTOR_STORE = Chroma.from_documents(documents=all_splits[list_position:i+1], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder)  #AZURE_OPENAI_EMBEDDINGS defined on line 407\n                    list_position = i+1\n                    token_count = 0\n                    print(\"Loaded batch, sleeping for one minute to stay within rate-limit\")\n                    time.sleep(63)\n                    continue\n\n            # post-loop, if any splits are left to be processed but were missed due to token_count not reaching the limit:\n            if list_position < len(all_splits):\n                VECTOR_STORE = Chroma.from_documents(documents=all_splits[list_position:], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder) #AZURE_OPENAI_EMBEDDINGS defined on line 407\n\n        elif use_bge_base_embeddings or use_bge_large_embeddings:\n            persist_directory = \"\"\n            if use_bge_base_embeddings:\n                persist_directory = vectordb_bge_base_folder\n            elif use_bge_large_embeddings:\n                persist_directory = vectordb_bge_large_folder\n            VECTOR_STORE = Chroma.from_documents(documents=all_splits, embedding=HF_BGE_EMBEDDINGS, persist_directory=persist_directory)    #HF_BGE_EMBEDDINGS defined in process_model() line 2133\n\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Could not store to VectorDB, encountered error: \", e)\n    #finally:\n        #loader.stop()\n\n    return chunk_sz, chunk_olp\n\n\ndef find_images_in_db(reference_pages):\n\n    print(\"Searching for relevant Images\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method find_images_in_db. Error: \", e)\n\n    matched_images = []\n    matched_images_found = False\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        conn.row_factory = sqlite3.Row\n        print(\"Database connected for image search\")\n    except Exception as e:\n        handle_local_error(\"Could not connect to images DB for image search, encountered error: \", e)\n\n    for doc in reference_pages:\n                \n        #source_filename = os.path.basename(doc)\n\n        for search_string in reference_pages[doc]:\n\n            # Only search for non-empty search strings\n            if search_string:\n                print(f\"String found for image search: {search_string}\")\n                try:\n                    images = conn.execute('SELECT DISTINCT id, image_data FROM images WHERE surrounding_text LIKE ?', ('%' + search_string + '%',)).fetchall()\n                except Exception as e:\n                    handle_error_no_return(\"Could not select images from Images DB, encountered error: \", e)\n                for row in images:\n                    print(\"Matching image found!\")\n                    matched_images_found = True\n                    image_id = row['id']\n                    image_data = row['image_data']\n                    matched_images.append((image_id, image_data))\n\n    conn.close()\n    matched_images = set(matched_images)\n    return matched_images_found, matched_images\n\n\ndef highlight_text_on_page(highlight_list, stream_session_id):\n\n    print(f\"highlight_list: {highlight_list}\")\n\n    try:\n        read_return = read_config(['upload_folder', 'highlighted_docs'])\n        upload_folder = read_return['upload_folder']\n        highlighted_pdfs = read_return['highlighted_docs']\n    except Exception as e:\n        handle_local_error(\"Missing upload_folder in config.json for method highlight_text_on_page. Error: \", e)\n    \n    for index, doc in enumerate(highlight_list, start=1):\n\n        try:\n            pdf_path = os.path.join(upload_folder, doc).replace(\"\\\\\",\"/\")\n            output_file_extension = \"_\" + stream_session_id + '.pdf'\n            output_file_name = doc.replace(\".pdf\",output_file_extension) \n            output_pdf_path = os.path.join(highlighted_pdfs, output_file_name).replace(\"\\\\\",\"/\")\n\n            print(f\"stream_session_id:{stream_session_id}\")\n            print(f\"\\npdf_path:{pdf_path}\")\n\n            highlight_doc = fitz.open(pdf_path)\n        except Exception as e:\n            handle_error_no_return(\"Could not open doc for highlighting, encountered error: \", e)\n            continue\n        \n        for target in highlight_list[doc]:\n            try:\n                text_to_highlight = str(target[1])\n                text_to_highlight = re.sub(r'Row \\d+, Column \\d+: ', '', text_to_highlight)\n                page_number = int(target[0])\n                \n                print(f\"text_to_highlight: {text_to_highlight}\")\n                print(f\"page_number: {page_number}\")\n\n                page = highlight_doc.load_page(page_number-1)\n                text_instances = page.search_for(text_to_highlight)\n            except Exception as e:\n                handle_error_no_return(\"Error loading page or searching for text to highlight, encountered error: \", e)\n                continue\n            \n            for inst in text_instances:\n                try:\n                    print(\"HIGHLIGHTING\", inst)\n                    page.add_highlight_annot(inst)\n                except Exception as e:\n                    handle_error_no_return(\"Could not highlight text instance, encountered error: \", e)\n                    continue\n        \n        try:\n            highlight_doc.save(output_pdf_path, garbage=0, deflate=False, clean=False)\n        except Exception as e:\n            handle_error_no_return(\"Could not save highlighted doc, encountered error: \", e)\n            continue\n\n    return True\n\n\ndef whoosh_text_in_pdf_and_highlight(reference_pages, stream_session_id):\n\n    print(\"Searching Index\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for method whoosh_text_in_pdf_and_highlight. Error: \", e)\n\n    user_should_refer_pages_in_doc = {}\n    docs_have_relevant_info = False\n\n    highlight_list = {}\n\n    try:\n        # Open the index\n        ix = open_dir(index_dir)\n\n        # Create a 'searcher' object\n        with ix.searcher() as searcher:\n            query_parser = QueryParser(\"content\", ix.schema)\n\n            for doc in reference_pages:\n                \n                source_filename = os.path.basename(doc)\n                output_file_extension = \"_\" + stream_session_id + '.pdf'\n                output_file_name = source_filename.replace(\".pdf\",output_file_extension) \n                page_numbers = []\n                highlight_strings = []\n                \n                for search_string in reference_pages[doc]:\n\n                    # Only search for non-empty search strings\n                    if search_string:\n\n                        query = query_parser.parse(search_string)\n\n                        results = searcher.search(query)\n\n                        for hit in results:\n                            print(f\"\\n\\nFound in {hit['title']} on page {hit['pagenumber']}\")\n                            page_numbers.append(int(hit['pagenumber']))\n                            docs_have_relevant_info = True\n                            \n                            highlight_target = [hit['pagenumber'], search_string]\n                            highlight_strings.append(highlight_target)\n\n                page_numbers = set(page_numbers)\n                user_should_refer_pages_in_doc[output_file_name] = page_numbers\n\n                highlight_strings_set = set(tuple(inner_list) for inner_list in highlight_strings)  # Because using a set directly on a list of lists won't work because lists are mutable and cannot be hashed, which is a requirement for the elements of a set. \n                highlight_strings = [list(inner_tuple) for inner_tuple in highlight_strings_set]\n                highlight_list[source_filename] = highlight_strings\n\n    except Exception as e:\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\n    \n    # Highlight line in PDF\n    if docs_have_relevant_info:\n        try:\n            highlight_text_on_page(highlight_list, str(stream_session_id))\n        except Exception as e:\n            handle_error_no_return(\"Could not highlight text, encountered error: \", e)\n\n    return docs_have_relevant_info, user_should_refer_pages_in_doc\n\n\ndef determine_sequence_id_for_chat(chat_id):\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        # Determine sequence_id\n        cursor.execute(\"SELECT COALESCE(MAX(sequence_id), 0) FROM chat_history WHERE chat_id = ?\", (int(chat_id),))\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for a new chat!\n        # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n        # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n        result = cursor.fetchone()\n        current_sequence_id = result[0]     # 'result' will be a list, so extract the first value\n        \n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n\n    return int(current_sequence_id)\n\n\ndef store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, current_prompt_template):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {chat_id}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(chat_id)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template) VALUES (?, ?, ?, ?, ?, ?)\", (int(chat_id), int(sequence_id), user_query_for_history_db, model_response_for_history_db, model_choice, str(current_prompt_template)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n\n\ndef store_chat_history_to_db(user_query_for_history_db, model_response_for_history_db, current_historical_summary):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {CHAT_ID}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n        base_template = read_return['base_template']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(CHAT_ID)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template, history_summary) VALUES (?, ?, ?, ?, ?, ?, ?)\", (int(CHAT_ID), int(SEQUENCE_ID), user_query_for_history_db, model_response_for_history_db, model_choice, str(base_template), str(current_historical_summary)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n    conn.close()\n\n\n# Route for loading all models from model dir\n@app.route('/load_local_models')\ndef load_local_models():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        handle_api_error(\"Missing model_dir in config.json for method load_local_models. Error: \", e)\n    \n    try:\n        models = [f for f in os.listdir(model_dir) if os.path.isfile(os.path.join(model_dir, f))]\n    except Exception as e:\n        handle_api_error(\"Could not load list of local models, encountered error: \", e)\n        \n    #print(f\"locally available models: {models}\")\n    return jsonify({'success': True, 'models': models})\n\n\n@app.route('/upload_new_llm', methods=['POST'])\ndef upload_new_llm():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        handle_api_error(\"Could not determine model_dir in upload_new_llm. Error: \", e)\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        handle_api_error(\"Server-side error recieving LLM file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n\n    try:\n        filepath = os.path.join(model_dir, filename)\n\n        print(\"Loading new LLM - filename: \", filename)\n        print(\"Loading new LLM - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        handle_api_error(\"Failed to save LLM to model_dir, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to handle the submission of the first form (LLM & embeddings model and GPU selection)\n@app.route('/process_model', methods=['POST'])\ndef process_model():\n    \n    global HF_BGE_EMBEDDINGS\n\n    ###---New config.json---###\n\n    config_update_dict = {}\n\n    use_azure_open_ai = 'use_azure' in request.form\n    use_openai_embeddings = 'use_openai_embeddings' in request.form\n    use_sbert_embeddings = 'use_sbert_embeddings' in request.form\n    use_bge_large_embeddings = 'use_bge_large_embeddings' in request.form\n    use_bge_base_embeddings = 'use_bge_base_embeddings' in request.form\n    use_gpu_for_embeddings = request.form.get('use_gpu_for_embeds', False)    # default no\n    model_choice = str(request.form['model_choice'])\n    use_gpu = request.form.get('use_gpu', False)\n\n    config_update_dict.update({'use_azure_open_ai':use_azure_open_ai, 'use_openai_embeddings':use_openai_embeddings, 'use_sbert_embeddings':use_sbert_embeddings, 'use_bge_large_embeddings':use_bge_large_embeddings, 'use_bge_base_embeddings':use_bge_base_embeddings, 'use_gpu_for_embeddings':use_gpu_for_embeddings, 'model_choice':model_choice, 'use_gpu':use_gpu})\n\n    try:\n        if use_bge_base_embeddings or use_bge_large_embeddings:\n            model_name = \"\"\n            if use_bge_base_embeddings:\n                model_name = \"BAAI/bge-base-en\"\n            elif use_bge_large_embeddings:\n                model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n    except Exception as e:\n        handle_api_error(\"Could not load BGE embeddings in process_model, encountered error: \", e)\n    \n    try:\n        write_config(config_update_dict)\n    except Exception as e:\n        handle_local_error(\"Could not write updates to config.json, encountered error: \", e)\n\n    # Redirect to the next step\n    return redirect(url_for('load_file'))\n\n\ndef convert_to_pdf_with_unoconv(input_file_path, output_file_path):\n    print(\"\\n\\nConverting non-PDF document to PDF format\\n\\n\")\n    if platform.system() == 'Windows':\n        subprocess.run(['python', 'unoconv.py', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n    else:\n        subprocess.run(['unoconv', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n\n\n# Route to handle the submission of the second form (file loading)\n@app.route('/process_new_file', methods=['POST'])\ndef process_new_file():\n\n    use_ocr = False\n    try:\n        read_return = read_config(['use_ocr', 'ocr_service_choice'])\n        use_ocr = read_return['use_ocr']\n        ocr_service_choice = read_return['ocr_service_choice']\n    except Exception as e:\n        handle_api_error(\"Could not determine use_ocr in config.json for process_new_file. Disabling OCR and proceeding. Error: \", e)\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        handle_api_error(\"Server-side error recieving file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n    if \"PDF\" in filename:\n        filename = filename.replace(\"PDF\", \"pdf\")\n\n    try:\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n        print(\"Loading new file - filename: \", filename)\n        print(\"Loading new file - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        handle_api_error(\"Failed to save document to app folder, encountered error: \", e)\n\n    if not filename.lower().endswith('.pdf'):\n        try:\n            conv_filename = os.path.splitext(filename)[0] + '.pdf'\n            conv_filepath = os.path.join(app.config['UPLOAD_FOLDER'], conv_filename)\n\n            convert_to_pdf_with_unoconv(filepath, conv_filepath)\n\n            filepath = conv_filepath\n        except subprocess.CalledProcessError as e:\n            handle_api_error(\"Could not convert file to PDF, encountered error: \", e)\n        except Exception as e:\n            handle_api_error(\"Unexpected error when converting file to PDF, encountered error: \", e)\n\n    print(\"Processing PDF file\")\n    \n    if use_ocr:\n        try:\n            if ocr_service_choice == 'AzureVision':\n                input_file = PDFtoAzureOCRTXT(filepath)\n            elif ocr_service_choice == 'AzureDocAi':\n                input_file = PDFtoAzureDocAiTXT(filepath)\n        except Exception as e:\n            handle_error_no_return(\"Failed to OCR text from PDF. Will now attempt to extract text via PyPDF2. Encountered error: \", e)\n            try:\n                input_file = PDFtoTXT(filepath)\n            except Exception as e:\n                handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    else:\n        try:\n            input_file = PDFtoTXT(filepath)\n        except Exception as e:\n            handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    \n    try:\n        images = extract_images_from_pdf(filepath)\n    except Exception as e:\n        handle_error_no_return(\"Failed to extract images from the PDF document, encountered error: \", e)\n\n    try:\n        store_images_to_db(images)\n    except Exception as e:\n        handle_error_no_return(\"Failed to save images to database, encountered error: \", e)\n    \n    try:\n        chunk_size, chunk_overlap = LoadNewDocument(input_file)\n    except Exception as e:\n        handle_api_error(\"Failed to extract text from PDF: \", e)\n    \n\n    global VECTOR_STORE\n    print(\"\\nRe-Loading VectorDB: ChromaDB\")\n\n    vectordb_used = \"\"\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder', 'embedding_model_choice'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n        embedding_model_choice = read_return['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when reloading VectorDB, could not fully complete process_new_file. Please try restarting the application. Error: \", e)\n\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            vectordb_used = vectordb_sbert_folder\n        elif use_openai_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n            vectordb_used = vectordb_openai_folder\n        elif use_bge_base_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_base_folder\n        elif use_bge_large_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_large_folder\n    except Exception as e:\n        handle_api_error(\"Could not reload VectorDB when trying to process_new_file. Please try restarting the application. Error: \", e)\n\n    try:\n        record_doc_loaded_to_db(filename, embedding_model_choice, vectordb_used, chunk_size, chunk_overlap)\n    except Exception as e:\n        handle_error_no_return(\"Unable to record document loading to records DB, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to store user rating: \n# ATTN: comment out print() statements, as users may elect to leave a rating as a response is being generated, which is when the stdout is redirected to the event stream! \n@app.route('/store_user_rating', methods=['POST'])\ndef store_user_rating():\n    \n    # print(\"Stroing user rating\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json for method store_user_rating. Error: \", e)\n    \n    try:\n        user_rating = request.form['rating']\n        chat_id_for_rating = request.form['chat_id']\n        sequence_id_for_rating = request.form['sequence_id']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read user rating or failed to obtain chat/sequence ID, encountered error: \", e)\n\n    # print(\"user_rating: \", user_rating)\n    # print(\"chat_id_for_rating: \", chat_id_for_rating)\n    # print(\"sequence_id_for_rating: \", sequence_id_for_rating)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history DB for storage of user-rating, encountered error: \", e)\n\n    try:\n        cursor.execute(\n            '''\n            UPDATE chat_history\n            SET user_rating = ?\n            WHERE chat_id = ? AND sequence_id = ?\n            ''',\n            (user_rating, chat_id_for_rating, sequence_id_for_rating)\n        )\n        conn.commit()\n    except Exception as e:\n        handle_api_error(\"Could not store user-rating to chat history db, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify(success=True)\n\n\n\ndef is_llama_cpp_local_server_online():\n    try:\n        response = requests.get('http://localhost:8080/health')\n        \n        if response.status_code == 200:\n            data = response.json()  # parse the JSON response to determine the server status\n            if data['status'] == 'ok':\n                print(f\"Server ready: {data['slots_idle']} idle slots, {data['slots_processing']} processing slots.\")\n                return {\"server_available\":True, \"loading_model\":False, \"status_code\":200}\n            elif data['status'] == 'no slot available':\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":200}\n            \n        elif response.status_code == 503:   # model still loading or no slots\n            data = response.json()\n            if data['status'] == 'loading model':\n                print(\"Server is loading the selected LLM, please wait\")\n                return {\"server_available\":False, \"loading_model\":True, \"status_code\":503}\n            else:\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":503}\n        \n        elif response.status_code == 500:\n            print(\"Server error: Failed to load LLM.\")\n            logger.error(\"llama.cpp - 500 event\")\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n        \n        else:\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n    except requests.exceptions.ConnectionError as e:\n        error_message = \"\\n\\nECONNREFUSED event\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":True, \"status_code\":500}\n    except Exception as e:\n        error_message = f\"\\n\\nCould not check llama.cpp local-server health, encountered error: {e}\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n\ndef send_ctrl_c_to_process(process):\n    if process.poll() is None:  # check if process is still running via poll(), which returns None if a process is still running \n        if platform.system() == 'Windows':\n            process.send_signal(signal.CTRL_BREAK_EVENT)\n        else:\n            process.send_signal(signal.SIGINT)\n        try:\n            # Wait a bit for the process to terminate gracefully:\n            process.wait(timeout=3)\n        except subprocess.TimeoutExpired:\n            print(\"Process did not terminate within timeout, will be force-killed.\")\n            process.kill()  # Sends 'SIGKILL' on Unix-like to force-kill immediately / 'TerminateProcess' on Windows which still allows for graceful termination\n            process.wait()\n            if process.poll() is not None:\n                print(\"Process has been killed successfully.\")\n            else:\n                print(\"Process still running after force kill attempt.\")\n\n\ndef terminate_llama_cpp_process(process):\n    try:\n        # process.terminate() sends 'SIGTERM' on Unix-like systems / 'TerminateProcess' on Windows, allows for graceful termination\n        # process.wait()\n        send_ctrl_c_to_process(process)\n        if process.poll() is not None:  # process has indeed terminated\n            print(\"Process terminated gracefully.\")\n    except Exception as e:\n        handle_local_error(\"Failed to terminate llama.cpp process, encountered error: \", e)\n\n\n@app.route('/llama_cpp_server_starter')\ndef llama_cpp_server_starter():\n\n    global LLM_CHANGE_RELOAD_TRIGGER_SET\n    global LLAMA_CPP_PROCESS\n    global LLM_LOADED_UP\n\n    if LLM_LOADED_UP and not LLM_CHANGE_RELOAD_TRIGGER_SET:\n        model_choice = 'undefined'\n        try:\n            read_return = read_config(['model_choice'])\n            model_choice = read_return['model_choice']\n        except Exception as e:\n            handle_error_no_return(\"Missing model_choice in config.json when attempting to return without re-loading. Printing error and proceeding: \", e)\n        print(f'\\n\\nAlready loaded! Simply returning model choice: {model_choice}\\n\\n')\n        return jsonify({'success': True, 'llm_model': model_choice})\n    elif LLM_CHANGE_RELOAD_TRIGGER_SET:\n        print('\\n\\nProceeding to reload the LLM & resetting the LLM_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False\n\n    try:\n        if is_llama_cpp_local_server_online()['server_available']:\n            print(\"Server online. Terminating and reloading from config.json\")\n            try:\n                terminate_llama_cpp_process(LLAMA_CPP_PROCESS)\n                LLAMA_CPP_PROCESS = None\n            except Exception as e:\n                LLM_LOADED_UP = True\n                handle_api_error(\"Failed to terminate running llama.cpp process, server was likely launched by a previous session. Retruning with the currently loaded LLM. To change, shutdown the previously launched server manually and reload this page. Technical error-details follow: \", e)\n                try:\n                    read_return = read_config(['model_choice'])\n                    model_choice = read_return['model_choice']\n                    return jsonify({'success': True, 'llm_model': model_choice})\n                except Exception as e:\n                    handle_api_error(\"Missing values in config.json when preparing to launch llama.cpp server, encountered error: \", e)\n                return jsonify({'success': True, 'llm_model': 'undefined'})\n                \n                \n    except Exception as e:\n        handle_error_no_return(\"Could not pre-check if llama.cpp server is running, it may be offline. Printing error and proceeding: \", e)\n\n\n    try:\n        read_return = read_config(['model_dir', 'model_choice', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'server_timeout_seconds', 'server_retry_attempts', 'use_gpu'])\n        model_dir = read_return['model_dir']\n        model_choice = read_return['model_choice']\n        local_llm_context_length = read_return['local_llm_context_length']\n        local_llm_max_new_tokens = read_return['local_llm_max_new_tokens']\n        local_llm_gpu_layers = read_return['local_llm_gpu_layers']\n        server_timeout_seconds = read_return['server_timeout_seconds']\n        server_retry_attempts = read_return['server_retry_attempts']\n        use_gpu = read_return['use_gpu']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when preparing to launch llama.cpp server, encountered error: \", e)\n\n\n    try:\n        cpp_model = os.path.join(model_dir, model_choice)\n    except Exception as e:\n        handle_api_error(\"Could not os.join path to model file to launch llama.cpp server, encountered error: \", e)\n\n    if not use_gpu:\n        local_llm_gpu_layers = 0\n\n    try:\n        cpp_app = ['llama-server', '-m', cpp_model, '-ngl', str(local_llm_gpu_layers), '-c', str(local_llm_context_length), '-n', str(local_llm_max_new_tokens), '--host', '0.0.0.0']\n\n        if platform.system() == 'Windows':\n            LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, creationflags=subprocess.CREATE_NEW_CONSOLE)  # Windows only! Comment when containerizing or deploying to Linux/MacOS!\n        else:           \n            # Platform & container agnostic:\n            with open('llama_cpp_server_output_log.txt', 'w') as f:\n                LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, stdout=f, stderr=subprocess.STDOUT, text=True)    #stdout has already been redirected to the file, so simply direct stderr to stdout!\n\n    except Exception as e:\n        handle_api_error(\"Could not launch llama.cpp process, encountered error: \", e)\n\n\n    timeout = server_timeout_seconds   \n    attempts = server_retry_attempts\n\n    try:\n        for _ in range(attempts):\n            if is_llama_cpp_local_server_online()['server_available']:\n                print(\"llama.cpp server launched succesfully! Returning.\")\n                LLM_LOADED_UP = True\n                return jsonify({'success': True, 'llm_model': model_choice})\n            time.sleep(timeout)\n    except Exception as e:\n        handle_error_no_return(\"Could not check server status after launch attempt, printing error and retrying: \", e)\n\n    return handle_api_error(\"Failed to start llama.cpp local-server\")\n\n\n\n@app.route('/load_vectordb')\ndef load_vectordb():\n\n    global VECTOR_STORE\n    global HF_BGE_EMBEDDINGS\n    global AZURE_OPENAI_EMBEDDINGS\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    global VECTORDB_LOADED_UP\n\n    if VECTORDB_LOADED_UP and not VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print(f'\\n\\nVectorDB already loaded! Simply returning.\\n\\n')\n        return jsonify({'success': True})\n    elif VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print('\\n\\nProceeding to reload VectorDB & resetting the VECTORDB_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n\n    try:\n        read_return = read_config(['use_gpu_for_embeddings', 'use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_gpu_for_embeddings = read_return['use_gpu_for_embeddings']\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to load_vectordb. Error: \", e)\n    \n    \n    ### 1 - Load VectorDB from disk\n    print(\"\\n\\nLoading VectorDB: ChromaDB\\n\\n\")\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            # try:\n            #     # chroma_client = VECTOR_STORE.PersistentClient\n            #     # max_batch_size = chroma_client._producer.max_batch_size\n            #     max_batch_size = VECTOR_STORE.max_batch_size\n            #     print(f\"max_batch_size: {max_batch_size}\")\n            # except Exception as e:\n            #     print(f\"Could not get max_batch_size. Error: {e}\")\n        \n        elif use_openai_embeddings:\n\n            try:\n                read_return = read_config(['azure_openai_text_ada_api_url', 'azure_openai_text_ada_api_key', 'azure_openai_api_type', 'azure_openai_api_version', 'azure_openai_text_ada_deployment_name'])\n                azure_openai_text_ada_api_url = read_return['azure_openai_text_ada_api_url']\n                azure_openai_text_ada_api_key = read_return['azure_openai_text_ada_api_key']\n                azure_openai_api_type = read_return['azure_openai_api_type']\n                azure_openai_api_version = read_return['azure_openai_api_version']\n                azure_openai_text_ada_deployment_name = read_return['azure_openai_text_ada_deployment_name']\n            except Exception as e:\n                handle_api_error(\"Missing values for Azure OpenAI Embeddings in method load_model_and_vectordb in config.json. Error: \", e)\n            \n            try:\n                os.environ[\"OPENAI_API_BASE\"] = azure_openai_text_ada_api_url\n                os.environ[\"OPENAI_API_KEY\"] = azure_openai_text_ada_api_key\n                os.environ[\"OPENAI_API_TYPE\"] = azure_openai_api_type\n                os.environ[\"OPENAI_API_VERSION\"] = azure_openai_api_version\n            except Exception as e:\n                handle_api_error(\"Could not set OS environment variables for Azure OpenAI Embeddings in load_model_and_vectordb, encountered error: \", e)\n\n            \n            AZURE_OPENAI_EMBEDDINGS = OpenAIEmbeddings(deployment=azure_openai_text_ada_deployment_name)\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n        \n        elif use_bge_base_embeddings:\n            model_name = \"BAAI/bge-base-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n                \n        \n        elif use_bge_large_embeddings:\n            model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n        \n        #VECTOR_STORE = Chroma(persist_directory=VECTORDB_SBERT_FOLDER, embedding_function=HuggingFaceEmbeddings())\n    except Exception as e:\n        handle_api_error(\"Could not load VectorDB, encountered error: \", e)\n    \n    VECTORDB_LOADED_UP = True\n    return jsonify(success=True)\n\n\n@app.route('/set_prompt_template', methods=['POST'])\ndef set_prompt_template():\n\n    base_template = \"\"\n\n    try:\n        base_template = request.form['prompt_template']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read prompt_template from the POST request in method set_prompt_template, encountered error: \", e)\n    \n    try:\n        write_config({'base_template':base_template})\n    except Exception as e:\n        handle_api_error(\"Could not update base_template in method set_prompt_template, encountered error: \", e)\n\n    return jsonify({'success':True})\n\n\n@app.route('/fetch_file_list_for_vector_db', methods=['POST'])\ndef fetch_file_list_for_vector_db():\n\n    print(\"Loading file list for selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method fetch_file_list_for_vector_db, encountered error: \", e)\n\n    # For the VectorDB presently picked by the user in the dropdown, obtain the associated VectorDB folder for the select query:\n    vdb_for_select = \"\"\n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            read_return = read_config(['vectordb_bge_large_folder'])\n            vdb_for_select = read_return['vectordb_bge_large_folder']\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            read_return = read_config(['vectordb_bge_base_folder'])\n            vdb_for_select = read_return['vectordb_bge_base_folder']\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            read_return = read_config(['vectordb_sbert_folder'])\n            vdb_for_select = read_return['vectordb_sbert_folder']\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            read_return = read_config(['vectordb_openai_folder'])\n            vdb_for_select = read_return['vectordb_openai_folder']\n\n    except Exception as e:\n        handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_docs_loaded_db in config.json in method fetch_file_list_for_vector_db. Error: \", e)\n\n    file_row_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to sqlite_docs_loaded_db database to load file list, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT document_name, vectordb_used, chunk_size, chunk_overlap FROM document_records where vectordb_used = ?\", (vdb_for_select,))\n    except Exception as e:\n        handle_api_error(\"Could not get document list from document_records db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            file_row_list.append(list(list_item))\n    except Exception as e:\n        handle_api_error(\"Could not parse document list from document_records db, encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, 'file_row_list': file_row_list})\n\n\n@app.route('/reset_vector_db_on_disk', methods=['POST'])\ndef reset_vector_db_on_disk():\n\n    print(\"Resetting selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['base_directory'])\n        base_directory = read_return['base_directory']\n    except Exception as e:\n        handle_local_error(\"Could not read base_directory from config.json for reset_vector_db_on_disk. Error: \", e)\n\n    try:\n        current_datetime = datetime.datetime.now()\n        formatted_datetime = current_datetime.strftime('%Y-%m-%d-%Hhr-%Mmin-%Ssec')\n    except Exception as e:\n        handle_api_error(\"Could not obtain timestamp in reset_vector_db_on_disk, encountered error: \", e)\n\n    # Now that we have all pre-requisite data to create a new VectorDB, proceed to do so by checking the model the user had currently picked from the dropdown: \n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            vectordb_bge_large_folder = base_directory + '/chroma_db_bge_large_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_large_folder':vectordb_bge_large_folder})\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            vectordb_bge_base_folder = base_directory + '/chroma_db_bge_base_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_base_folder':vectordb_bge_base_folder})\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            vectordb_sbert_folder = base_directory + '/chroma_db_250_sbert_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_sbert_folder':vectordb_sbert_folder})\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            vectordb_openai_folder = base_directory + '/chroma_db_openai_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_openai_folder':vectordb_openai_folder})\n\n    except Exception as e:\n        handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    restart_required = True\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n    try:\n        read_return = read_config(['embedding_model_choice'])\n        set_embedding_model_choice = read_return['embedding_model_choice']\n        if set_embedding_model_choice != selected_embedding_model_choice:\n            restart_required = False\n            VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n    except Exception as e:\n        handle_error_no_return(\"Could not compare selected and set embedding models when determining if restart_required in reset_vector_db_on_disk(), encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, \"restart_required\": restart_required})\n\n\n@app.route('/load_chat_history_list')\ndef load_chat_history_list():\n\n    print(\"loading chat history list for sidebar\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_history_db in config.json in method load_chat_history_list. Error: \", e)\n\n    history_id_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to sqlite_history_db database to load chat history list, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT DISTINCT chat_id FROM chat_history\")\n    except Exception as e:\n        handle_api_error(\"Could not get list from chat history db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            history_id_list.append(list_item)\n    except Exception as e:\n        handle_api_error(\"Could not parse chat history list from db, encountered error: \", e)\n\n    #print(f'returning chat hsitory list: {history_id_list}')\n\n    return jsonify({'success': True, 'history_list': history_id_list})\n\n\n@app.route('/load_chat_history', methods=['POST'])\ndef load_chat_history():\n\n    global CHAT_ID\n    global SEQUENCE_ID\n    global HISTORY_SUMMARY\n    global HISTORY_MEMORY_WITH_BUFFER\n\n    print(\"loading chat history\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method load_chat_history. Error: \", e)\n\n    # Clear chat history of current chat, prep for loading historical chat summary:\n    # try:\n    #     HISTORY_MEMORY_WITH_BUFFER.chat_memory.clear()\n    #     HISTORY_MEMORY_WITH_BUFFER = ConversationSummaryBufferMemory(llm=LLM, max_token_limit=300, return_messages=False)\n    #     HISTORY_SUMMARY = {}\n    # except Exception as e:\n    #     handle_error_no_return(\"Could not clear memory when loading chat history, encountered error: \", e)\n\n    try:\n        chat_id_for_history_search = request.form['chat_id']\n        CHAT_ID = request.form['chat_id']\n    except Exception as e:\n        handle_api_error(\"Could not retrieve Chat ID from request form, encountered error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    sequence_id_for_history_search = 1\n    retrieve_history = True\n    chat_history = []\n    old_chat_model = \"\"\n\n    while(retrieve_history):\n\n        try:\n            c.execute(\"SELECT user_query FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n            \n            user_message = str(result[0])\n\n            user_message = user_message.strip('\\n')\n            regex_to_swap_multiple_spaces_with_newline = r' {2,}'\n            user_message = re.sub(regex_to_swap_multiple_spaces_with_newline, '<br>', user_message)\n\n            user_message = '<div class=\"user-message\">' + user_message + '</div>'\n\n            chat_history.append(user_message)\n\n            c.execute(\"SELECT llm_response FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n\n            result = str(result[0])\n            result_parts = result.split(\"pdf_pane_data=\",1)\n            # llm_response = '<div class=\"llm-wrapper\"> <div class=\"llm-response\">' + str(result[0]) + '</div>'\n            llm_response = '<div class=\"response-and-viewer-container\"><div class=\"llm-wrapper\"> <div class=\"llm-response\">' + result_parts[0]\n\n        except Exception as e:\n            handle_api_error(\"Could not retrieve chat history, encountered error: \", e)\n        \n        llm_response = llm_response.strip('\\n')\n        llm_response = llm_response.replace('\\n\\n', '<br><br>')\n        llm_response = llm_response.replace('\\n', '<br>')\n        \n        try:\n            c.execute(\"SELECT user_rating FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n        except Exception as e:\n            handle_error_no_return(\"Could not fetch user rating, encountered error: \", e)\n\n        response_rated = False\n        user_rating_for_history_chat = None\n\n        if result[0]:\n            response_rated = True\n            try:\n                user_rating_for_history_chat = int(result[0])\n                #print(f'rating exists: {user_rating_for_history_chat}')\n            except Exception as e:\n                handle_error_no_return(\"Could not retrieve integer value of user rating, encountered error: \", e)\n\n\n        llm_rating = f'''<div class=\"star-rating\" data-rated={response_rated} rating-chat-id={chat_id_for_history_search} rating-sequence-id={sequence_id_for_history_search}>\n        <i class=\"far fa-star\" data-rate=\"1\"></i>\n        <i class=\"far fa-star\" data-rate=\"2\"></i>\n        <i class=\"far fa-star\" data-rate=\"3\"></i>\n        <i class=\"far fa-star\" data-rate=\"4\"></i>\n        <i class=\"far fa-star\" data-rate=\"5\"></i>\n        </div>\n        </div>\n        </div>'''\n\n\n        if user_rating_for_history_chat:\n            rating_parts = llm_rating.split(\"far\", user_rating_for_history_chat)\n            if len(rating_parts) <= user_rating_for_history_chat:\n                llm_rating = \"fas\".join(rating_parts)\n            else:\n                llm_rating = \"fas\".join(rating_parts[:-1]) + \"fas\" + \"far\".join(rating_parts[-1:])\n\n        llm_response += llm_rating\n\n        if len(result_parts) > 1:\n            llm_response += result_parts[1]\n            llm_response += \"</div>\"\n            llm_response = llm_response.strip('\\n')\n            llm_response = llm_response.replace('\\n\\n', '<br><br>')\n            llm_response = llm_response.replace('\\n', '<br>')\n\n        chat_history.append(llm_response)\n\n        # Increment sequence ID for next iteration:\n        sequence_id_for_history_search += 1\n\n        # But first, check to see if next sequence exists!\n        try:\n            c.execute(\"SELECT EXISTS(SELECT 1 FROM chat_history WHERE chat_id = ? AND sequence_id = ?)\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            exists = c.fetchone()[0]\n        except Exception as e:\n            handle_api_error(\"Could not determine if next sequence exists in chat history DB, encountered error: \", e)\n            \n        if not exists:\n            SEQUENCE_ID = sequence_id_for_history_search - 1\n            retrieve_history = False\n            try:\n                c.execute(\"SELECT llm_model FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                old_chat_model = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not determine previously used LLM in chat, encountered error: \", e)\n            try:\n                c.execute(\"SELECT history_summary FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                history_summary_dict = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not fetch history summary of last chat, encountered error: \", e)\n            c.close()\n\n    # Convert History Summary and add a new key indicating it was recently cleared!\n    if history_summary_dict is not None and history_summary_dict != \"\" and history_summary_dict != 'None':\n        print(f\"\\n\\history_summary_dict string from old chat: {history_summary_dict}\\n\\n\")\n        try:\n            HISTORY_SUMMARY = ast.literal_eval(history_summary_dict)    #cast as dictionary\n            HISTORY_SUMMARY[\"has_been_reset\"] = True\n        except Exception as e:\n            handle_error_no_return(\"Could not cast history summary string from DB to dict and/or set has_been_reset boolean, encountered error: \", e)\n\n    # Temp prints:\n    print(f\"\\n\\nHISTORY_SUMMARY: {HISTORY_SUMMARY}\\n\\n\")\n    # print(f\"\\n\\history_summary_dict: {history_summary_dict}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.summary: {HISTORY_MEMORY_WITH_BUFFER.summary}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.chat_memory.messages: {HISTORY_MEMORY_WITH_BUFFER.chat_memory.messages}\\n\\n\")\n    print(f'\\n\\nChat history loaded for chat with model: {old_chat_model}\\n\\n')\n\n    return jsonify({'success': True, 'chat_history': chat_history, 'old_chat_model': old_chat_model})\n\n\n@app.route('/init_chat_history_db')\ndef init_chat_history_db():\n\n    global CHAT_ID\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_history_db in config.json in method init_chat_history_db. Error: \", e)\n\n    # Connect to chat_history.db to determine appropriate chat_id\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    # If the database does not currently exist...\n    try:\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS chat_history (\n                        id INTEGER PRIMARY KEY,\n                        chat_id INTEGER,\n                        sequence_id INTEGER,\n                        user_query TEXT,\n                        llm_response TEXT,\n                        user_rating INTEGER,\n                        llm_model TEXT, \n                        prompt_template TEXT,\n                        history_summary TEXT\n            )\n        ''')\n        conn.commit()\n    except Exception as e:\n        handle_api_error(\"Could not create new chat history db, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT COALESCE(MAX(chat_id), 0) FROM chat_history\")\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for an empty DB!\n\n        result = c.fetchone()\n\n        # 'result' will be a tuple, so extract the first element\n        max_chat_id = result[0]\n\n        new_chat_id = max_chat_id + 1\n        CHAT_ID = new_chat_id\n\n        print(f\"Chat history DB initialised with CHAT_ID: {CHAT_ID}\")\n    except Exception as e:\n        handle_api_error(\"Could not set CHAT_ID, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify({'success': True, 'chat_id': CHAT_ID})\n\n\ndef fetch_image_from_db(image_id):\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method fetch_image_from_db. Error: \", e)\n    \n    # 1 - Connect to DB\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n    except Exception as e:\n        handle_local_error(\"Could not connect to images database, encountered error: \", e)\n    \n    # 2 - Get Images\n    try:\n        conn.row_factory = sqlite3.Row\n        row = conn.execute('SELECT image_data FROM images WHERE id = ?', (image_id,)).fetchone()\n        images_bytes = row['image_data'] if row else None\n    except Exception as e:\n        handle_local_error(\"Could not fetch image from DB, encountered error: \", e)\n    \n    conn.close()\n    \n    return images_bytes\n\n\n@app.route('/image_display/<int:image_id>')\ndef image_display(image_id):\n    print(f\"\\n\\nprepping image for display: {image_id}\\n\\n\")\n\n    try: \n        image_bytes = fetch_image_from_db(image_id)\n    except Exception as e:\n        handle_local_error(\"Could not fetch image for display, encountered error: \", e)\n    \n    try:\n        encoded = base64.b64encode(image_bytes).decode('utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not encode image for display URI, encountered error: \", e)\n\n    # Return an HTML response with the embedded image:\n    data_uri = f\"data:image/jpeg;base64,{encoded}\"\n    html_content = f'<img src=\"{data_uri}\" alt=\"Image\">'\n\n    return html_content\n\n\ndef extract_significant_phrases(query):\n    print(\"Extracting significant phrases\")\n\n    try:\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n    except Exception as e:\n        handle_error_no_return(\"Failed to download & set stopwords, encountered error: \", e)\n    \n    try:\n        tokens = [token for token in query.lower().split() if token not in stop_words]\n    except Exception as e:\n        handle_local_error(\"Could not extract significant tokens, encountered error: \", e)\n\n    return tokens\n\n\ndef calculate_relevance_score(phrases, document_content):\n    #print(\"calculating relevance score\")\n    \n    try:\n        content_lower = document_content.lower()\n    except Exception as e:\n        handle_local_error(\"Could not read document_content in calculate_relevance_score(), encountered error: \", e)\n    \n    #print(f\"document content: {content_lower}\")\n    \n    #score = sum(1 for phrase in phrases if phrase in content_lower)\n    \n    score = 0\n    try:\n        for phrase in phrases:\n            if phrase in content_lower:\n                print(f\"Match found to enable RAG: {phrase}\")\n                score += 1\n    except Exception as e:\n        handle_local_error(\"Could not compare phrases in calculate_relevance_score(), encountered error: \", e)\n    \n    return score\n\n\ndef filter_relevant_documents(query, search_results, threshold=1):\n\n    print(\"Checking relevant docs to determin if RAG is required\")\n\n    do_rag = False\n    page_contents = []\n\n    try:\n        significant_phrases = extract_significant_phrases(query)\n    except Exception as e:\n        handle_local_error(\"Could not extract significant phrases, encountered error: \", e)\n    \n    print(f\"significant tokens: {significant_phrases}\")\n    #relevant_documents = []\n\n    try:\n        for document in search_results:\n            # check for non-empty source field\n            if document.page_content:\n                page_contents.append(document.page_content)\n\n            if not do_rag:  # if do_rag has already been set to true, why look?\n                if document.metadata.get('source'):\n                    score = calculate_relevance_score(significant_phrases, document.page_content)\n                    if score >= threshold:\n                        #relevant_documents.append(document)\n                        print(\"Must do RAG!\")\n                        do_rag = True\n    except Exception as e:\n        handle_local_error(\"Could not read calculate relevance score, encountered error: \", e)\n\n    #return relevant_documents\n    return page_contents, do_rag\n\n\n\n@app.route('/setup_for_llama_cpp_response', methods=['POST'])\ndef setup_for_llama_cpp_response():\n\n    global QUERIES\n\n    do_rag = True\n\n    stream_session_id = \"\"\n    key_for_vector_results = \"\"\n    # Generate a unique session ID using universally Unique Identifier via the uuid4() method, wherein the randomness of the result is dependent on the randomness of the underlying operating system's random number generator\n    # UUI is a standard used for creating unique strings that have a very high likelihood of being unique across all time and space, for ex: f47ac10b-58cc-4372-a567-0e02b2c3d479\n    try:\n        stream_session_id = str(uuid.uuid4())\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n    except Exception as e:\n        handle_api_error(\"Error creating unique stream_session_id when attempting to setup_for_streaming_response. Error: \", e)\n    \n\n    # Determine do_rag\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'force_enable_rag', 'force_disable_rag', 'local_llm_chat_template_format', 'base_template'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        force_enable_rag = read_return['force_enable_rag']\n        force_disable_rag = read_return['force_disable_rag']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n        base_template = read_return['base_template']\n\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to setup_for_streaming_response. Error: \", e)\n\n    try:\n        # Attempt to get query data\n        user_query = request.json['user_query']\n        chat_id = request.json['chat_id']\n\n        # Store the query associated with the ID\n        QUERIES[stream_session_id] = user_query\n    except KeyError:\n        handle_api_error(\"Could not obtain and/or store user_query in setup_for_streaming_response, encountered error: \", e)\n\n    print(\"chat_id: \", chat_id)\n\n    # Perform similarity search on the vector DB\n    print(\"\\n\\nPerforming similarity search to determine if RAG necessary\\n\\n\")\n    embedding_function = None\n    try:\n        if use_sbert_embeddings:\n            embedding_function=HuggingFaceEmbeddings()\n        elif use_openai_embeddings:\n            embedding_function=AZURE_OPENAI_EMBEDDINGS\n        elif use_bge_base_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n        elif use_bge_large_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n    except Exception as e:\n        handle_error_no_return(\"Could not set embedding_function for similarity_search when attempting to setup_for_streaming_response, encountered error: \", e)\n    \n    try:\n        docs = VECTOR_STORE.similarity_search(user_query, embedding_fn=embedding_function)\n        # docs_with_relevance_score = VECTOR_STORE.similarity_search_with_relevance_scores(user_query, 10, embedding_fn=embedding_function)\n        # docs_list_with_cosine_distance = VECTOR_STORE.similarity_search_with_score(user_query, 10, embedding_fn=embedding_function)\n        # print(f'\\n\\nsimple similarity search results: \\n {docs}\\n\\n')\n        # print(f'\\n\\nRelevance Score similarity search results (range 0 to 1): \\n {docs_with_relevance_score}\\n\\n')\n        # print(f'\\n\\nDocs list most similar to query based on cosine distance: \\n {docs_list_with_cosine_distance}\\n\\n')\n    except Exception as e:\n        handle_error_no_return(\"Could not perform similarity_search to determine do_rag when attempting to setup_for_streaming_response, encountered error: \", e)\n\n    print(\"\\n\\nDetermining do_rag \\n\\n\")\n    # We do not modify the force_enable_rag or force_disable_rag flags in this method, we simply respond to them here. UI updates should handle those flags.\n    if force_enable_rag:\n        print(\"\\n\\nFORCE_ENABLE_RAG True, force enabling RAG and returning\\n\\n\")\n        try:\n            page_contents, do_rag = filter_relevant_documents(user_query, docs)\n            do_rag = True\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"Error force-enabling RAG, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n    elif force_disable_rag:\n        print(\"\\n\\nFORCE_DISABLE_RAG True, force disabling RAG and returning\\n\\n\")\n        do_rag = False\n    else:\n        try:\n            page_contents, do_rag = filter_relevant_documents(user_query, docs)\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"RAG Error, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n    \n    print(f'Do RAG? {do_rag}')\n\n    try:\n        write_config({'do_rag':do_rag})\n    except Exception as e:\n        handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n\n    \n    # Having determined do_rag, time to build the prompt template!\n    \n    if do_rag:  # add similarity search results for RAG!\n        try:\n            QUERIES[key_for_vector_results] = docs\n            user_query += f\"\\n\\nThe following context might be helpful in answering the user query above:\\n{page_contents}\"\n            print(f\"RAG formatted user_query: {user_query}\")\n        except Exception as e:\n            try:\n                write_config({'do_rag':False})\n            except Exception as e:\n                handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n            handle_error_no_return(\"RAG Error: Could not update QUERIES dict and user_query during setup_for_streaming_response, proceeding without RAG. Encountered error: \", e)\n\n    current_sequence_id = determine_sequence_id_for_chat(chat_id)\n    formatted_prompt = \"\"\n    print(\"current_sequence_id: \", current_sequence_id)\n    if current_sequence_id > 0:    # get the last prompt so we can continue the completions\n\n        try:\n            read_return = read_config(['sqlite_history_db'])\n            sqlite_history_db = read_return['sqlite_history_db']\n        except Exception as e:\n            handle_error_no_return(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n        # Connect to or create the DB\n        try:\n            conn = sqlite3.connect(sqlite_history_db)\n            cursor = conn.cursor()\n        except Exception as e:\n            handle_error_no_return(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n        try:\n            # Determine sequence_id\n            cursor.execute(\"SELECT prompt_template FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id), int(current_sequence_id)))\n            # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n            # This accounts for a new chat!\n            # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n            # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n            result = cursor.fetchone()\n            formatted_prompt = str(result[0])\n            \n        except Exception as e:\n            handle_error_no_return(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n    \n    if formatted_prompt == \"\":  # could not be updated above\n        current_sequence_id = 0 # reset chat sequence id\n\n    if local_llm_chat_template_format == 'llama3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        else:\n            formatted_prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{base_template}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    elif local_llm_chat_template_format == 'llama2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<s>[INST] {user_query} [/INST]\"\n        else:\n            formatted_prompt += f\"<s>[INST] <<SYS>>\\n {base_template} \\n<</SYS>>\\n\\n {user_query}  [/INST]\"\n\n    elif local_llm_chat_template_format == 'chatml':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"\\n<|im_start|>user\\n{user_query}<|im_end|>\\n\"\n        else:\n            formatted_prompt += f\"<|im_start|>system\\n{base_template}<|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\"\n\n    elif local_llm_chat_template_format == 'phi3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n        else:\n            formatted_prompt += f\"<|system|>\\n{base_template}<|end|>\\n<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n\n    elif local_llm_chat_template_format == 'command-r':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        else:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{base_template}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n\n    elif local_llm_chat_template_format == 'deepseek':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"### Instruction:\\n{user_query}\\n### Response:\\n\"\n        else:\n            formatted_prompt += f\"{base_template}### Instruction:\\n{user_query}\\n### Response:\\n\"\n\n    elif local_llm_chat_template_format == 'deepseek-coder-v2':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"User: {user_query}\\nAssistant: \"\n        else:\n            formatted_prompt += f\"<|begin_of_sentence|>{base_template}\\nUser: {user_query}\\nAssistant: \"\n\n    elif local_llm_chat_template_format == 'vicuna':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"USER: {user_query}\\nASSISTANT: \"\n        else:\n            formatted_prompt += f\"{base_template}\\n\\nUSER: {user_query}\\nASSISTANT: \"\n\n    elif local_llm_chat_template_format == 'openchat':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n        else:\n            formatted_prompt += f\"<s>GPT4 Correct System: {base_template}<|end_of_turn|>GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n\n    elif local_llm_chat_template_format == 'gemma2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<start_of_turn>user\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n        else:\n            formatted_prompt += f\"<start_of_turn>user\\n{base_template}\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n\n    # Return a bunch of stuff\n    new_sequence_id = int(current_sequence_id) + 1\n    return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag, \"formatted_user_prompt\": formatted_prompt, \"sequence_id\":new_sequence_id})\n\n\n\n@app.route('/get_references', methods=['POST'])\ndef get_references():\n\n    print(\"\\n\\nGetting References\\n\\n\")\n\n    try:\n        read_return = read_config(['do_rag', 'upload_folder', 'local_llm_chat_template_format'])\n        do_rag = read_return['do_rag']\n        upload_folder = read_return['upload_folder']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to get_references. Error: \", e)\n\n    try:\n        stream_session_id = request.json['stream_session_id']\n        user_query = request.json['user_query']\n        llm_response = request.json['llm_response']\n        formatted_user_prompt = request.json['formatted_user_prompt']\n        chat_id = request.json['chat_id']\n        sequence_id = request.json['sequence_id']\n    except Exception as e:\n        handle_api_error(\"Could not read request content in method get_references, encountered error: \", e)\n\n    if local_llm_chat_template_format == 'llama3':\n        formatted_user_prompt += f\"{llm_response}<|eot_id|>\"\n    elif local_llm_chat_template_format == 'llama2':\n        formatted_user_prompt += f\"{llm_response}</s>\"\n    elif local_llm_chat_template_format == 'chatml':\n        formatted_user_prompt += f\"{llm_response}<|im_end|>\\n\"\n    elif local_llm_chat_template_format == 'phi3':\n        formatted_user_prompt += f\"{llm_response}<|end|>\\n\"\n    elif local_llm_chat_template_format == 'command-r':\n        formatted_user_prompt += f\"{llm_response}<|END_OF_TURN_TOKEN|>\"\n    elif local_llm_chat_template_format == 'deepseek':\n        formatted_user_prompt += f\"{llm_response}\\n<|EOT|>\\n\"\n    elif local_llm_chat_template_format == 'deepseek-coder-v2':\n        formatted_user_prompt += f\"{llm_response}<|end_of_sentence|>\"\n    elif local_llm_chat_template_format == 'vicuna':\n        formatted_user_prompt += f\"{llm_response} </s>\\n\"\n    elif local_llm_chat_template_format == 'openchat':\n        formatted_user_prompt += f\"{llm_response}<|end_of_turn|>\"\n    elif local_llm_chat_template_format == 'gemma2':\n        formatted_user_prompt += f\"{llm_response}<end_of_turn>\\n\"\n\n    if not do_rag:\n        print(\"\\n\\nSkipping RAG, storing chat history and returning\\n\\n\")\n        try:\n            store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query, llm_response, formatted_user_prompt)\n        except Exception as e:\n            handle_error_no_return(\"Could not store_llama_cpp_chat_history_to_db in get_references(), encountered error: \", e)\n        return jsonify({'success': True})\n        \n    try:\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n        docs = QUERIES[key_for_vector_results]\n        print(f\"\\n\\ntype(docs): {type(docs)}\\n\\n\")\n    except Exception as e:\n        handle_api_error(\"Could not obtain relevant data from QUERIES dict, encountered error: \", e)\n\n    # Having obtained the relevant info, clear the QUERIES{} dict so as to not bloat it!\n    try:\n        del QUERIES[key_for_vector_results]\n    except Exception as e:\n        handle_error_no_return(\"Error clearing queries dict in method get_references: \", e)\n\n    reference_response = \"\"\n\n    all_sources = {}\n    reference_pages = {}\n\n    for doc in docs:\n        \n        try:\n            relevant_page_text = str(doc.page_content)\n            # relevant_page_text = relevant_page_text.replace('\\n', ' ')\n            source_filepath = str(doc.metadata.get('source'))\n            #print(relevant_page_text)\n            #print(source_filepath)\n        except Exception as e:\n            handle_error_no_return(\"Could not access doc.page_content and/or doc.metadata, encountered error: \", e)\n            continue\n        \n        relevant_page_text = relevant_page_text.split('\\n', 1)[0]\n        relevant_page_text = relevant_page_text.strip()\n        relevant_page_text = re.sub(r'[\\W_]+Page \\d+[\\W_]+', '', relevant_page_text)\n        source_filepath = source_filepath.replace('\\\\', '/')\n        \n        try:\n            source_filename = os.path.basename(source_filepath)\n            _, file_extension = os.path.splitext(source_filepath)\n            #print(source_filename)\n            #print(file_extension)\n        except Exception as e:\n            handle_error_no_return(\"Could not parse path with OS lib, encountered error: \", e)\n            continue\n\n        # The source_filepath will likely always reference a TXT file because of how we're loading the VectorDB!\n        # Check if the PDF version of the source doc exists\n        if file_extension == '.txt':\n\n            #print(\"\\n\\ntxt file\\n\\n\")\n\n            # Construct the path to the potential PDF version\n            pdf_version_path = os.path.join(upload_folder, os.path.basename(source_filepath).replace('.txt', '.pdf'))   # not catching an error here as os.path.basename(source_filepath) has already been caught just above!\n\n            # Check if PDF version of the source TXT exists!\n            if os.path.exists(pdf_version_path):\n\n                #print(\"\\n\\pdf exists\\n\\n\")\n\n                source_filename = source_filename.replace('.txt', '.pdf')\n                \n                if pdf_version_path in reference_pages:\n                    reference_pages[pdf_version_path].extend([relevant_page_text])\n                else:\n                    reference_pages[pdf_version_path] = [relevant_page_text]\n\n                # Add this file to our sources dictionary if it's not already present\n                if source_filename not in all_sources:\n                    source_filepath = pdf_version_path\n                    all_sources.update({source_filename: source_filepath})\n\n            # Else PDF does not exist, TXT is the source\n            else:\n                # Check if the TXT is already in the sources dict\n                if source_filename not in all_sources:\n                    try:\n                        source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                        all_sources.update({source_filename: source_filepath})\n                    except Exception as e:\n                        handle_error_no_return(\"Could not construct filepath for TXT file, encountered error: \", e)\n\n\n        # If file is not a TXT file\n        else:\n            # Check if the TXT is already in the sources dict\n            if source_filename not in all_sources:\n                try:\n                    source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                    all_sources.update({source_filename: source_filepath})\n                except Exception as e:\n                    handle_error_no_return(\"Could not construct filepath for non-TXT file, encountered error: \", e)\n\n    # print(f\"\\n\\nreference_pages: {reference_pages}\\n\\n\")\n\n    try:\n        docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf_and_highlight(reference_pages, stream_session_id)\n        # docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf(reference_pages)\n    except Exception as e:\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\n\n    try:\n        matched_images_found, matched_images_in_bytes = find_images_in_db(reference_pages)\n    except Exception as e:\n        handle_error_no_return(\"Could not search for images, encountered error: \", e)\n\n    refer_pages_string = \"\"\n    download_link_html = \"\"\n    images_iframe_html = \"\"\n\n    if docs_have_relevant_info:\n\n        refer_pages_string = \"<br><br><h6>Refer to the following pages in the mentioned docs:</h6><br>\"\n        \n        # for doc in user_should_refer_pages_in_doc:\n        for index, doc in enumerate(user_should_refer_pages_in_doc, start=1):\n            # pdf_iframe_id = str(doc) + \"PdfViewer\"\n            pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\n            frame_doc_path = f\"/pdf/{doc}\"\n            # frame_doc_path = upload_folder + f\"/{doc}\" \n            try:\n                refer_pages_string += f\"<br><h6>{doc}: \"\n                for page in user_should_refer_pages_in_doc[doc]:\n                    frame_doc_path += \"#page=\" + str(page) \n                    refer_pages_string += f'<a href=\"javascript:void(0)\" onclick=\"goToPage(\\'{pdf_iframe_id}\\', \\'{frame_doc_path}\\')\">Page {page}</a>, '\n                    frame_doc_path = f\"/pdf/{doc}\"\n                refer_pages_string = refer_pages_string.strip(', ') + \"</h6><br>\"\n            except Exception as e:\n                handle_error_no_return(\"Could not construct refer_pages_string, encountered error: \", e)\n\n        # download_link_html = \"<br><h6>Refer to the source documents below:</h6>\"\n        pdf_right_pane_id = \"stream\" + stream_session_id + \"PdfPane\"\n        download_link_html = f'<div class=\"pdf-viewer\" id={pdf_right_pane_id}>'\n\n        for index, source in enumerate(user_should_refer_pages_in_doc, start=1):\n            try:\n                # print(\"\\n\\nlooping sources\\n\\n\")\n                download_link_url = url_for('download_file', filename=source)\n                pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\n                download_link_html += f'<br><h6><a href=\"{download_link_url}\" target=\"_blank\"><iframe id=\"{pdf_iframe_id}\" src=\"{download_link_url}\" width=\"100%\" height=\"600\"></iframe></a></h6><br>'\n            except Exception as e:\n                handle_error_no_return(\"Could not construct download_link_html, encountered error: \", e)\n\n        download_link_html += \"</div>\"\n\n    if matched_images_found:\n        image_gallery_id = f\"image_gallery_for_stream_{stream_session_id}\"\n        images_iframe_html = f'''\n        <h6>Browse a gallery of relevant images by clicking on the thumbnail below:</h6>\n        <i class=\"fas fa-images thumbnail-icon\" onclick=\"openImageGalleryModal('{image_gallery_id}')\"></i>\n        <div id=\"{image_gallery_id}\" class=\"image-gallery-modal\">\n        <span class=\"image-gallery-close\" onclick=\"closeImageGalleryModal('{image_gallery_id}')\">&times;</span>\n        <div class=\"image-gallery-content\">\n        '''\n        for image_id, image_bytes_data in matched_images_in_bytes:\n            #print(f\"\\n\\nmatched image id: {image_id}\")\n            try:\n                image_link_url = url_for('image_display', image_id=image_id)\n                images_iframe_html += f'<iframe src=\"{image_link_url}\" frameborder=\"0\" class=\"gallery-thumbnail\"></iframe>'\n            except Exception as e:\n                handle_error_no_return(\"Could not construct images_iframe_html, encountered error: \", e)\n        \n        images_iframe_html += f'</div></div>'\n\n    \n    # reference_response = refer_pages_string + download_link_html + images_iframe_html\n    reference_response = refer_pages_string + images_iframe_html\n\n    try:\n        # model_response_for_history_db = str(llm_response) + refer_pages_string\n        model_response_for_history_db = str(llm_response)\n        model_response_for_history_db += f\"\\n\\n{reference_response}\"\n        model_response_for_history_db += f\"\\n\\npdf_pane_data={download_link_html}\"\n        model_response_for_history_db = model_response_for_history_db.strip('\\n')\n\n        formatted_user_query = str(user_query).strip('\\n')\n\n        user_query_for_history_db = formatted_user_query\n    except Exception as e:\n        handle_error_no_return(\"Could not prep data to store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    try:\n        store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, formatted_user_prompt)\n    except Exception as e:\n        handle_error_no_return(\"Could not store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    return jsonify({'success': True, 'response': reference_response, 'pdf_frame':download_link_html})\n\n\nif __name__ == '__main__':\n    # app.run(debug=True)\n    app.run(host='0.0.0.0', port=5000)"}
{"type": "source_file", "path": "dockerized_nvidia_cuda_gpu/web_app/app.py", "content": "from flask import Flask, render_template, request, redirect, url_for, Response, stream_with_context\nfrom flask import send_from_directory\nfrom flask import jsonify\n\nfrom pdfminer.high_level import extract_text\nfrom werkzeug.utils import secure_filename\n\nfrom whoosh.fields import Schema, TEXT, ID, NUMERIC\nfrom whoosh.qparser import MultifieldParser\nfrom whoosh.qparser import QueryParser\nfrom whoosh.index import create_in\nfrom whoosh.index import open_dir\n\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.document_loaders import TextLoader\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import ConversationChain\nfrom langchain.llms import CTransformers\n\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport pytesseract\n\nimport fitz # PyMuPDF\n\nfrom urllib.parse import unquote\nfrom threading import Thread\nimport subprocess\nimport threading\nimport traceback\nimport platform\nimport tempfile\nimport datetime\nimport requests\nimport logging\nimport sqlite3\nimport signal\nimport PyPDF2\nimport base64\nimport queue\nimport uuid\nimport json\nimport time\nimport nltk\nimport zlib\nimport ast\nimport sys\nimport os\nimport io\nimport re\n\nfrom logging.handlers import RotatingFileHandler\nfrom nltk.corpus import stopwords\n\n\n\napp = Flask(__name__)\n\n# Route for the home page, rendering the initial model selection form (legacy)\n@app.route('/')\ndef index():\n    return render_template('chat.html')\n\n# model_selection.html triggers window.location.href to '/chat', which triggers this route, which loads the chat.html template at the end!\n@app.route('/chat')\ndef chat():\n    return render_template('chat.html')\n\n# Route to display the file loading form\n@app.route('/load_file')\ndef load_file():\n    return render_template('model_selection.html', show_file_form=True)\n\n@app.route('/download/<filename>')\ndef download_file(filename):\n    # return send_from_directory(app.config['UPLOAD_FOLDER'], filename, as_attachment=True)\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename, as_attachment=False, mimetype='application/pdf')\n\n@app.route('/pdf/<filename>')\ndef pdf_viewer(filename):\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename)\n\n\n\n#########################------------------GLOBALS!----------------------###############################\nLLAMA_CPP_PROCESS = None\nLLM = None\nCHAT_ID = None\nSEQUENCE_ID = None\nLOADED_UP = False\nLLM_LOADED_UP = False\nVECTORDB_LOADED_UP = False\nLLM_CHANGE_RELOAD_TRIGGER_SET = False\nVECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\nVECTOR_STORE = None\nHF_BGE_EMBEDDINGS = None\nAZURE_OPENAI_EMBEDDINGS = None\nHISTORY_MEMORY_WITH_BUFFER = None   #Init in load_model_and_vectordb(); reset in load_chat_history() when old chats loaded, and in load_model_and_vectordb() when 'New Chat' selected; used for non-RAG convChain init in stream, and for saving context in stream for RAG chains and lastly, for setting HISTORY_SUMMARY in stream() via load_memory_variables({})\nHISTORY_SUMMARY = {}    #Set in stream() via HISTORY_MEMORY_WITH_BUFFER.load_memory_variables({}), and in load_chat_history() from chat_history DB; cleared in load_model_and_vectordb() when 'New Chat' selected; used to init prompt templates in stream() and lastly, for storage to chat_history DB in stream() and get_references()\n\n# Dict for user queries:  queries[session_id] = user_input\nQUERIES = {}\n#########################------------------------------------------------###############################\n\n\n\n#########################------------Setup & Handle Logging-------------###############################\ntry:\n    # 1 - Create a logger\n    logger = logging.getLogger('my_logger')\n    logger.setLevel(logging.ERROR)\n\n    # 2 - Create a RotatingFileHandler\n    # maxBytes: max file size of log file after which a new file is created; set to 1024 * 1024 * 5 for 5MB: 1024x1024 is 1MB, then a multiplyer for the number of MB\n    # backupCount: number of backup files to keep specifying how many old log files to keep\n    handler = RotatingFileHandler('server_log.log', maxBytes=1024*1024*5, backupCount=2)\n    handler.setLevel(logging.ERROR)\n\n    # 3 - Create a formatter and set it for the handler\n    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n    handler.setFormatter(formatter)\n\n    # 4 - Add the handler to the logger\n    logger.addHandler(handler)\n    # Logger ready! Usage: logger.error(f\"This is an error message with error {e}\")\nexcept Exception as e:\n    print(f\"\\n\\nCould not establish logger, encountered error: {e}\")\n\n\ndef handle_api_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    #full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    return jsonify(success=False, error=error_message), 500 #internal server error\n\n\ndef handle_local_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    raise Exception(exception)\n\n\ndef handle_error_no_return(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n\n#########################-------------------------------------###############################\n\n\ndocker_only_config_path = '/app/storage/config.json'    # we already know that a docker env is linux-debian, no need to wait for platform detection logic further below!\ndocker_directory = os.path.dirname(docker_only_config_path)\n\ntry:\n    os.makedirs(docker_directory, exist_ok=True)    # Creates the directory, and all directories in the specified path if they don't exist, no errors otherwise\nexcept Exception as e:\n    handle_local_error(\"Failed to create base app dir in the docker storage volume, encountered error: \", e)\n\nif not os.path.exists(docker_only_config_path):\n    try:\n        with open(docker_only_config_path, 'w') as file:\n            json.dump({}, file)\n    except Exception as e:\n        handle_error_no_return(\"Could not init config.json. Multiple app restarts may be required to get the app to init correctly. Printing error and proceeding: \", e)\n\n\n\n# Method to write to config.json | input- dict of key:values to be written to config.json\ndef write_config(config_updates, filename=docker_only_config_path):\n\n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        config = {}     #init emply config dict\n        handle_error_no_return(\"Could not read config.json when attempting to write, encountered error: \", e)\n        \n    restart_required = False\n    if LLM_LOADED_UP:\n        llm_trigger_keys_for_app_restart = ['use_local_llm', 'use_azure_open_ai', 'use_gpu', 'model_choice', 'local_llm_chat_template_format', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'base_template']\n                \n        for key in llm_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global LLM_CHANGE_RELOAD_TRIGGER_SET\n                LLM_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n    \n    if VECTORDB_LOADED_UP:\n        vectordb_trigger_keys_for_app_restart = ['embedding_model_choice']\n\n        for key in vectordb_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n                VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n\n    config.update(config_updates)\n\n    # Write updated config.json:\n    try:\n        with open(filename, 'w') as file:\n            json.dump(config, file, indent=4)\n    except Exception as e:\n        handle_local_error(\"Could not update config.json, encountered error: \", e)\n     \n    return {'success': True, 'restart_required':restart_required}\n            \n\n# Method to read from config.json | input- list of keys to be read from config.json; output- dict of key:value pairs; MANAGE DEFAULTS HERE!\ndef read_config(keys, default_value=None, filename=docker_only_config_path):\n    \n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        handle_error_no_return(\"Could not read config.json, encountered error: \", e)\n        return {key: default_value for key in keys}     #because a read scenario wherein config.json does not exist shouldn't occur!\n    \n    return_dict = {}\n    update_config_dict = {}\n    base_directory = config.get('base_directory', '/app/storage')   # specifying default if not found\n\n    for key in keys:\n        if key in config:\n            return_dict[key] = config[key]\n        else:\n            default_value = {\n                'windows_base_directory':'C:/web_app_storage',\n                'unix_and_docker_base_directory':'/app/storage',\n                'mac_base_directory':'app',\n                'upload_folder':base_directory + '/uploaded_pdfs',\n                'vectordb_sbert_folder':base_directory + '/chroma_db_250_sbert_embeddings',\n                'vectordb_openai_folder':base_directory + '/chroma_db_openai_embeddings',\n                'vectordb_bge_large_folder':base_directory + '/chroma_db_bge_large_embeddings',\n                'vectordb_bge_base_folder':base_directory + '/chroma_db_bge_base_embeddings',\n                'index_dir':base_directory + '/indexdir_main',\n                'sqlite_images_db':base_directory + '/images_database_main.db',\n                'sqlite_history_db':base_directory + '/chat_history.db',\n                'sqlite_docs_loaded_db':base_directory + '/docs_loaded.db',\n                'model_dir':base_directory + '/models',\n                'highlighted_docs':base_directory + '/highlighted_pdfs',\n                'ocr_pdfs':base_directory + '/ocr_pdfs',\n                'pdfs_to_txts':base_directory + '/pdfs_to_txts',\n                'model_choice':'Meta-Llama-3-8B-Instruct.f16.gguf',\n                'do_rag':True,\n                'force_enable_rag':False,\n                'force_disable_rag':False,\n                'use_local_llm':True,\n                'use_gpu':True,\n                'use_gpu_for_embeddings':False,\n                'azure_cv_free_tier':True,\n                'use_azure_open_ai':False,\n                'use_openai_embeddings':False,\n                'azure_openai_api_type':'azure',\n                'azure_openai_api_version':'2023-05-15',\n                'azure_openai_max_tokens':4096,\n                'azure_openai_temperature':0.7,\n                'use_bge_large_embeddings':False,\n                'use_bge_base_embeddings':False,\n                'use_sbert_embeddings':True,\n                'embedding_model_choice':'sbert_mpnet_base_v2',\n                'use_ocr':False,\n                'ocr_service_choice':'None',\n                'local_llm_model_type':'llama',\n                'local_llm_chat_template_format':'llama3',\n                'local_llm_context_length':8192,\n                'local_llm_max_new_tokens':2048,\n                'local_llm_gpu_layers':47,\n                'local_llm_temperature':0.8,\n                'local_llm_top_k':40,\n                'local_llm_top_p':0.95,\n                'local_llm_min_p':0.05,\n                'local_llm_n_keep':0,\n                'server_timeout_seconds':10,\n                'server_retry_attempts':3,\n                'base_template':\"Answer the user's question in as much detail as possible.\",\n            }.get(key, 'undefined')\n\n            if default_value == 'undefined':\n                raise KeyError(f\"Key \\'{key}\\' not found in config.json and no default value has been defined either.\\n\")\n            \n            return_dict[key] = default_value\n            update_config_dict[key] = default_value\n\n    if update_config_dict:\n        # Write Defaults\n        try:\n            write_config(update_config_dict)\n        except Exception as e:\n            handle_error_no_return(\"Could not write defaults to config.json. Encountered error: \", e)\n    \n    ##print(f\"return_dict: {return_dict}\")\n\n    return return_dict\n\n\n# Method for API route to read from config.json\n# Deviates from typical RESTful principals to use a POST call to fetch values but practical & justifyable because we:\n# 1. Do not want to make the URL huge with a ever-growing list of query-params 2. Do not wish to expose values via query-params\n@app.route('/config_reader_api', methods=['POST'])\ndef config_reader_api():\n    # keys = request.args.getlist('keys') # Assuming keys are passed as query parameters\n    \n    try:\n        keys = request.json.get('keys', []) # Could also do keys = request.json['keys'] but this way we can provide a default list should 'keys' be missing!\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read keys for config_reader_api request. Encountered error:\", e)\n\n    try:\n        values = read_config(keys)  # send list of keys, get dict of key:values\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read keys from config.json. Encountered error: \", e)\n    \n    return jsonify(success=True, values=values)\n\n\n# Method for API route to write to config.json\n@app.route('/config_writer_api', methods=['POST'])\ndef config_writer_api():\n\n    try:\n        config_updates = request.json['config_updates']\n        print(f\"config_updates for config_writer_api: {config_updates}\")\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not read values for config_writer_api request. Encountered error: \", e)\n    \n    try:\n        write_return = write_config(config_updates)\n    except Exception as e:\n        handle_api_error(\"Server-side error - could not write keys to config.json. Encountered error: \", e)\n    \n    return jsonify({\"success\": write_return['success'], \"restart_required\": write_return['restart_required']})\n\n\n\n#########################------------Setup Directories-------------###############################\nBASE_DIRECTORY = \"\"\n\nif platform.system() == 'Windows':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = 'C:/temp_web_app_storage'\n    try:\n        read_return = read_config(['windows_base_directory'])   #passing list of values to read\n        BASE_DIRECTORY = str(read_return['windows_base_directory']) #received dict of key:values\n    except Exception as e:\n        handle_local_error(\"Could not read windows_base_directory on boot, encountered error: \", e)\n\nelif platform.system() == 'Linux':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = '/app/storage'\n    try:\n        read_return = read_config(['unix_and_docker_base_directory'])\n        BASE_DIRECTORY = str(read_return['unix_and_docker_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read unix_and_docker_base_directory on boot, encountered error: \", e)\n\nelse:   #Likely 'Darwin' and hence MacOS\n    #BASE_DIRECTORY = 'app'\n    try:\n        read_return = read_config(['mac_base_directory'])\n        BASE_DIRECTORY = str(read_return['mac_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read mac_base_directory on boot, encountered error: \", e)\n\ntry:\n    write_config({'base_directory':BASE_DIRECTORY})\nexcept Exception as e:\n    handle_local_error(\"Could not write OS BASE_DIRECTORY on boot, encountered error: \", e)\n\n\n###---Notes on the above workflow:---###\n# 1. Everytime the app runs, the OS platform is detected\n# 2. Following which the apporpriate base directory is requested as above\n# 3. If this is the very first run:\n#   a. read_config does not find the directory data in config.json\n#   b. the else clause is triggered and defaults set for both, write_config and return\n# 4. If this isn't the very first run:\n#   a. read_config simply returns the OS specific directory - this allows the user to update the directory via config.json!\n# 4. On return, BASE_DIRECTORY is set and write_config has os specific directories set (windows_base_directory, unix_and_docker_base_directory, and mac_base_directory)\n# 5. write_config is invoked for BASE_DIRECTORY\n# 6. write_config detects a write-attempt for BASE_DIRECTORY and updates all related app directories too, which can be subsequently read as required\n# 7. This ensures that directories are set correctly at each run while also allowing the user to set their preferred directory via config.json\n\n\n# Having set the values for the directories above, proceed to actually create them on disk IF they don't alread exist!\nif not os.path.exists(BASE_DIRECTORY):\n\n    # Create a directory for app storage \n    try:\n        os.mkdir(BASE_DIRECTORY)\n    except Exception as e:\n        handle_local_error(\"Failed to create Base App Directory, encountered error: \", e)\n        \ntry:\n    read_return = read_config(['model_dir', 'highlighted_docs', 'upload_folder', 'ocr_pdfs', 'pdfs_to_txts', 'index_dir'])\n    model_dir = read_return['model_dir']\n    highlighted_docs = read_return['highlighted_docs']\n    upload_folder = read_return['upload_folder']\n    ocr_pdfs = read_return['ocr_pdfs']\n    pdfs_to_txts = read_return['pdfs_to_txts']\n    index_dir = read_return['index_dir']\nexcept Exception as e:\n    handle_local_error(\"Could not read paths for app directories (model_dir, highlighted_docs, upload_folder) from config.json on boot, encountered error: \", e)\n\n\n# If the base directory does not currently exist...\nif not os.path.exists(model_dir):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(model_dir)\n    except Exception as e:\n        handle_local_error(\"Failed to create Model Directory (model_dir), encountered error: \", e)\n\n\n# If the highlighted_docs directory does not currently exist...\nif not os.path.exists(highlighted_docs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(highlighted_docs)\n    except Exception as e:\n        handle_local_error(\"Failed to create Highlighted Docs Directory (highlighted_docs), encountered error: \", e)\n\n\n# If the upload_folder directory does not currently exist...\nif not os.path.exists(upload_folder):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(upload_folder)\n    except Exception as e:\n        handle_local_error(\"Failed to create Uploaded Docs Directory (upload_folder), encountered error: \", e)\n        \n\n# If the ocr_pdfs directory does not currently exist...\nif not os.path.exists(ocr_pdfs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(ocr_pdfs)\n    except Exception as e:\n        handle_local_error(\"Failed to create OCR'ed Docs Directory (ocr_pdfs), encountered error: \", e)\n\n\n# If the pdfs_to_txts directory does not currently exist...\nif not os.path.exists(pdfs_to_txts):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(pdfs_to_txts)\n    except Exception as e:\n        handle_local_error(\"Failed to create txt-docs Directory (pdfs_to_txts), encountered error: \", e)\n\n\n# If the index does not currently exist...\nif not os.path.exists(index_dir):\n\n    # Define the Index schema: what fields it contains\n    schema = Schema(title=ID(unique=True, stored=True), content=TEXT(stored=True), pagenumber=NUMERIC(stored=True))\n    \n    # Create a directory for persistent storage of the index to disk\n    try:\n        os.mkdir(index_dir)\n    except Exception as e:\n        handle_local_error(\"Failed to create directory for the Whoosh Index, encountered error: \", e)\n\n    # Create the index based on the schema definted above\n    try:\n        create_in(index_dir, schema)\n    except Exception as e:\n        handle_local_error(\"Failed to create Whoosh Index, encountered error: \", e)\n\n\napp.config['UPLOAD_FOLDER'] = upload_folder\napp.config['DOWNLOAD_FOLDER'] = highlighted_docs\n\n\ndef clean_text_string(text_to_be_cleaned):\n    \n    # Clean text\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"\", \"\").replace(\"\", \"\").replace(\"\", \"\")\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"Confidential Copy \\n            for \\n         DKPPU\", \"\")\n    #clean_text = re.sub(r'\\n(?=[a-z.])', ' ', text)     # replaces newline chars immediately followed by a small-letter or dot with a space as they're likely to be the same sentence split-up across lines.\n    clean_text = re.sub(r'\\n+', '\\n', text_to_be_cleaned)\n\n    # This regex substitutes anything that is not a word character or whitespace with an empty string.\n    clean_text = re.sub(r'[^\\w\\s]', ' ', clean_text)\n\n    # This regex substitutes any sequence of whitespace characters with a single space.\n    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n\n    return clean_text\n\ndef whoosh_indexer(pdf_data):\n\n    print(\"\\n\\nIndexing File\\n\\n\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for whoosh_indexer. Error: \", e)\n\n    # Define the Index schema: what fields it contains\n    schema = Schema(title=ID(unique=True, stored=True), content=TEXT(stored=True), pagenumber=NUMERIC(stored=True))\n\n    # If the index does not currently exist...\n    if not os.path.exists(index_dir):\n        \n        # Create a directory for persistent storage of the index to disk\n        try:\n            os.mkdir(index_dir)\n        except Exception as e:\n            handle_local_error(\"Failed to create directory for the Whoosh Index, encountered error: \", e)\n\n        # Create the index based on the schema definted above\n        try:\n            ix = create_in(index_dir, schema)\n        except Exception as e:\n            handle_local_error(\"Failed to create Whoosh Index, encountered error: \", e)\n            \n    else:\n        try:\n            ix = open_dir(index_dir)\n        except Exception as e:\n            handle_local_error(\"Failed to open Whoosh Index, encountered error: \", e)\n        \n    # init writer and write to the index:\n    try:\n        writer = ix.writer()\n        #searcher = ix.searcher()\n\n        for doc in pdf_data:\n            # query = QueryParser(\"title\", ix.schema).parse(doc[\"title\"])\n            # results = searcher.search(query)\n            # print(\"\\nAlready indexed page content, skipping\\n\")\n\n            #if not results:\n            writer.add_document(title=doc[\"title\"], content=doc[\"content\"], pagenumber=doc[\"pagenumber\"])\n\n        writer.commit()\n        #searcher.close()\n        \n    except Exception as e:\n        handle_local_error(\"Failed to write to Whoosh Index, encountered error: \", e)\n\n\ndef PDFtoAzureDocAiTXT(input_filepath):\n\n    print(\"\\n\\nProcessing Document - PDF to Azure DocAI TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_doc_ai_endpoint', 'azure_doc_ai_subscription_key', 'ocr_pdfs'])\n        azure_doc_ai_endpoint = read_return['azure_doc_ai_endpoint']\n        azure_doc_ai_subscription_key = read_return['azure_doc_ai_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureDocAiTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"Azure-OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        docai_client = DocumentAnalysisClient(azure_doc_ai_endpoint, AzureKeyCredential(azure_doc_ai_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure DocAI, encountered error: \", e)\n\n    try:\n        with open(input_filepath, \"rb\") as pdf_file:\n            # 1 - Get page count:\n            try:\n                pypdf_reader = PyPDF2.PdfReader(pdf_file)\n                page_count = len(pypdf_reader.pages)\n                page_range = f\"1-{page_count}\" if page_count > 1 else \"1\"\n                print(f\"page_range: {page_range}\")\n            except Exception as e:\n                handle_local_error(\"Could not get page count for call to Azure DocAI, encountered error: \", e)\n\n            # 2 - Reset file-read stream's internal pointer, which has now been set to the end of the file due to the above read operation!\n            pdf_file.seek(0)\n\n            # 3 - Call Azure DocAI:\n            try:\n                poller = docai_client.begin_analyze_document(\"prebuilt-layout\", pdf_file, pages=page_range)\n                result = poller.result()\n            except Exception as e:\n                handle_local_error(\"Could not get results for begin_analyze_document for Azure DocAI, encountered error: \", e)\n\n        # print(f\"result: \\n{result}\")\n\n        used_regions = set()   # set will avoid duplicates\n\n        if hasattr(result, 'tables'):\n            for table in result.tables:\n                #print(\"Found table\")\n                if table.cells:     # Check if there are cells in the table \n                    for cell in table.cells:\n                        #print(f\"Row {cell.row_index}, Column {cell.column_index}, Text: {cell.content}\")\n                        cell_text = f'Row {cell.row_index}, Column {cell.column_index}: {cell.content}'\n                        \n                        try:\n                            output_text_file.write(cell_text + '\\n')\n                        except Exception as e:\n                            handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n                        # Get page number\n                        page_number = \"\"\n                        if cell.bounding_regions:   # Check if there are bounding regions\n                            for region in cell.bounding_regions:\n                                page_number = region.page_number\n                                cell_polygon = region.polygon\n                                cell_polygon_tuple = tuple((point.x, point.y) for point in cell_polygon)    # lists aren't hashable to cast to a tuple\n                                used_regions.add(cell_polygon_tuple)\n\n                        # Whoosh prep\n                        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": cell_text, \"pagenumber\":page_number}\n                        pdf_data.append(whoosh_page_dict_entry)\n\n        # Get paragraphs\n        if hasattr(result, 'paragraphs'):\n            for paragraph in result.paragraphs:\n                para_page_number = paragraph.bounding_regions[0].page_number\n                para_polygon = paragraph.bounding_regions[0].polygon\n                para_polygon_tuple = tuple((point.x, point.y) for point in para_polygon)\n                \n                if para_polygon_tuple in used_regions:\n                    continue\n                \n                para_content = paragraph.content\n                #print(f\"\\n---Processing Page: {para_page_number}---\\n\")\n                #print(f\"paragraph: {para_content}\")\n\n                # write the extracted text to the file:\n                try:\n                    output_text_file.write(para_content + '\\n')\n                    used_regions.add(para_polygon_tuple)\n                except Exception as e:\n                    handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n                # whoosh prep\n                whoosh_page_dict_entry = {\"title\": source_filename, \"content\": para_content, \"pagenumber\":para_page_number}\n                pdf_data.append(whoosh_page_dict_entry)\n\n    except Exception as e:\n        handle_local_error(\"Error processing document with azure DocAI: \", e)\n\n    # Close all files\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef PDFtoAzureOCRTXT(input_filepath):\n    \n    print(\"\\n\\nProcessing Document - PDF to Azure OCR TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_ocr_endpoint', 'azure_ocr_subscription_key', 'ocr_pdfs', 'azure_cv_free_tier'])\n        azure_ocr_endpoint = read_return['azure_ocr_endpoint']\n        azure_ocr_subscription_key = read_return['azure_ocr_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n        azure_cv_free_tier = read_return['azure_cv_free_tier']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureOCRTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Convert PDF to  a list of images\n    try:\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\n    except Exception as e:\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        computervision_client = ComputerVisionClient(azure_ocr_endpoint, CognitiveServicesCredentials(azure_ocr_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure OCR, encountered error: \", e)\n    \n    calls_made = 0\n\n    # Iterate over each page and apply OCR:\n    print(\"\\n\\nBeginning image to Text OCR\\n\\n\")\n    for page_number, image in enumerate(pages, start = 1):\n        \n        # Convert to bytes and create a stream\n        try:\n            img_stream = io.BytesIO()\n            image.save(img_stream, format='PNG')\n            img_stream.seek(0)  # Reset the stream position to the beginning\n        except Exception as e:\n            handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n            continue\n\n        # Send to Azure OCR\n        try:\n            if azure_cv_free_tier:\n                if calls_made < 20:\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made += 1\n                else:\n                    print(\"Sleeping for 60secs due to AzureOCR free-tier restrictions!\")\n                    time.sleep(63)  #free tier restrictions!\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made = 1  #reset counter\n            else:\n                print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n        except Exception as e:\n            handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n\n        for region in result.regions:\n            for line in region.lines:\n                #print(\" \".join([word.text for word in line.words]))\n\n                try:\n                    clean_text = str(\" \".join([word.text for word in line.words]))\n                except Exception as e:\n                    handle_error_no_return(\"Could not obtain line from Azure OCR result, encountered error: \", e)\n                    continue\n\n                # Write the extracted text to the file:\n                try:\n                    output_text_file.write(clean_text + '\\n')\n                except Exception as e:\n                    handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n                # Whoosh prep\n                #whoosh_clean_text = preprocess_string(clean_text)\n                whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_number}\n                pdf_data.append(whoosh_page_dict_entry)\n\n    # Close all files\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef PDFtoTXT(input_file):\n\n    print(\"\\n\\nProcessing Document - PDF to TXT\\n\\n\")\n\n    try:\n        read_return = read_config(['pdfs_to_txts'])\n        pdfs_to_txts = read_return['pdfs_to_txts']\n    except Exception as e:\n        handle_local_error(\"Missing pdfs_to_txts directory for PDFtoTXT in config.json, encountered error: \", e)\n    \n    # Initialize PDF file reader\n    try:\n        pdf_file = open(input_file, 'rb')\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_file)\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    # Initialize PDF reader\n    try:\n        pdf_reader = PyPDF2.PdfReader(pdf_file)\n    except Exception as e:\n        handle_local_error(\"Could not initialize PDF reader, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(pdfs_to_txts, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"PyPDF2-extracted .txt already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Init list for Whoosh indexing\n    pdf_data = []\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    # Loop through all the pages and extract text\n    for page_num in range(len(pdf_reader.pages)):\n        \n        try:\n            page = pdf_reader.pages[page_num]\n            text = page.extract_text()\n        except Exception as e:\n            handle_error_no_return(\"Could not extract text from page, encountered error: \", e)\n\n        #clean_text = text\n        # Clean text\n        clean_text = clean_text_string(text)\n        \n        # Optionally, you can include page numbers in the text file\n        # output_text_file.write(f'\\n\\n--- Page {page_num + 1} ---\\n\\n')\n        \n        # Write the extracted text to the file\n        try:\n            output_text_file.write(clean_text + '\\n')\n        except Exception as e:\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n        # Whoosh prep\n        #whoosh_clean_text = preprocess_string(clean_text)\n        whoosh_page_dict_entry = {\"title\": source_filename, \"content\": clean_text, \"pagenumber\":page_num+1}\n        pdf_data.append(whoosh_page_dict_entry)\n\n    # Close all files\n    pdf_file.close()\n    output_text_file.close()\n\n    # Create Whoosh Index; if error, log exception and proceed to returning output_text_file_path\n    try:\n        whoosh_indexer(pdf_data)\n    except Exception as e:\n        handle_error_no_return(\"Could not index file, encountered error: \", e)\n\n    return output_text_file_path\n\n\ndef get_page_content_from_whoosh_index(title, pagenumber):\n\n    print(\"\\n\\nSearching Index for Page Content\\n\\n\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for get_page_content_from_whoosh_index. Error: \", e)\n\n    try:\n        ix = open_dir(index_dir)\n        searcher = ix.searcher()\n\n        parser = MultifieldParser([\"title\", \"pagenumber\"], schema=ix.schema)\n        query = parser.parse(f'title:\"{title}\" AND pagenumber:{pagenumber}')\n\n        results = searcher.search(query)\n\n        if results:\n            return results[0][\"content\"]\n        else:\n            return None\n    except Exception as e:\n        handle_local_error(\"Failed to open & search Whoosh Index for page content, encountered error: \", e)\n    finally:\n        searcher.close()\n\n\ndef extract_images_from_pdf(pdf_path):\n    \n    print(\"Extracting Images from PDF\")\n\n    try:\n        source_filename = os.path.basename(pdf_path)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n    \n    with open(pdf_path, 'rb') as file:\n        \n        try:\n            pdf_reader = PyPDF2.PdfReader(file)\n        except Exception as e:\n            handle_local_error(\"Could not read PDF, encountered error: \", e)\n\n        images = []\n\n        for page_num in range(len(pdf_reader.pages)):\n            page = pdf_reader.pages[page_num]\n            if '/XObject' in page['/Resources']:\n                xObject = page['/Resources']['/XObject'].get_object()\n                for obj in xObject:\n                    if xObject[obj]['/Subtype'] == '/Image':\n\n                        # Log details about the image object:\n                        try:\n                            image_obj = xObject[obj]\n                            obj_details = {\n                                'Object Reference': obj,\n                                'Width': image_obj.get('/Width', 'Unknown'),\n                                'Height': image_obj.get('/Height', 'Unknown'),\n                                'Color Space': image_obj.get('/ColorSpace', 'Unkown'),\n                                'Filter': image_obj.get('/Filter', 'Unknown'),\n                                'Bits Per Component': image_obj.get('/BitsPerComponent', 'Unknown')\n                            }\n                            #print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                            # data  = image_obj._data\n\n                            if obj_details['Filter'] == '/FlateDecode':\n                                #print(\"\\n\\nDecoding PNG!\\n\\n\")\n                                try:\n                                    data  = image_obj._data\n                                    decompressed_data = zlib.decompress(data)\n                                except Exception as e:\n                                    error_message = f\"\\n\\nPNG decompression exception: {e}\\n\\n\"\n                                    if logger:\n                                        logger.error(error_message)\n                                        print(error_message)\n                                    else:\n                                        print(error_message)\n                            else:\n                                decompressed_data  = image_obj._data\n\n                            text = page.extract_text()  \n                            # clean_text = text\n\n                            # Clean text\n                            clean_text = clean_text_string(text)\n\n                            # clean_text = get_page_content_from_whoosh_index(source_filename, page_num)\n\n                            try:\n                                if obj_details['Filter'] == '/FlateDecode':\n                                    # Determine Color Space:\n                                    color_space = image_obj.get('/ColorSpace')\n\n                                    if color_space == '/DeviceRGB':\n                                        mode = 'RGB'\n                                    elif color_space == '/DeviceCMYK':\n                                        mode = 'CMYK'\n                                    elif color_space == '/DeviceGray':\n                                        mode = 'L'\n                                    else:\n                                        mode = 'L'  # Default to grayscale if unsure\n\n                                    # Create image from bytes\n                                    image = Image.frombytes(mode, ((obj_details['Width']), (obj_details['Height'])), decompressed_data) # 'L' for 8-bit pixels, black and white\n                                    with io.BytesIO() as output:\n                                        image.save(output, format='JPEG')\n                                        binary_data = output.getvalue()\n                                        format = \"JPEG\"\n                                        images.append((binary_data,clean_text,format))\n\n                                else:\n                                    # Load image from bytes\n                                    image = Image.open(io.BytesIO(decompressed_data))\n\n                                    # Determine format (JPEG)\n                                    format = image.format\n                                    \n                                    #print(f\"\\n\\nImage format: {format}\\n\\n\")  # This will print the format\n\n                                    # If image loads, append image to images DB\n                                    images.append((decompressed_data,clean_text,format))\n\n                            except Exception as e:\n                                error_message = f\"\\n\\nEncountered unrecognized or invalid image data for object detailed below. Exception: {e}\\n\\n\"\n                                if logger:\n                                    logger.error(error_message)\n                                    logger.error(obj_details)\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                                else:\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n\n                        except Exception as e:\n                            handle_error_no_return(\"Could not process image object. Exception: \", e)\n\n        # print(\"Images array:\")\n        # print(images)\n        return images\n\n\ndef store_images_to_db(images):\n\n    print(\"\\n\\nStoring Images to Database\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to Images DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS images (\n                    id INTEGER PRIMARY KEY,\n                    image_data BLOB NOT NULL,\n                    surrounding_text TEXT,\n                    metadata TEXT,\n                    format TEXT\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create Images DB, encountered error: \", e)\n    \n    try:\n        for image_data, surrounding_text, format in images:\n            #print(\"surrounding_text: \", surrounding_text)\n            # Check if the image_data already exists in the database:\n            cursor.execute(\"SELECT COUNT(*) FROM images WHERE image_data = ?\", (image_data,))\n            if cursor.fetchone()[0] == 0:\n                print(\"\\nInserting new image into images DB\\n\")\n                cursor.execute(\"INSERT INTO images (image_data, surrounding_text, format) VALUES (?, ?, ?)\", (image_data, surrounding_text, format))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not store images to Images DB, encountered error: \", e)\n    finally:\n        conn.close()\n\n\ndef record_doc_loaded_to_db(document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap):\n\n    print(\"\\n\\nRecording document loading to records DB\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_docs_loaded_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to document_records DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS document_records (\n                    id INTEGER PRIMARY KEY,\n                    document_name TEXT NOT NULL,\n                    embedding_model TEXT NOT NULL,\n                    vectordb_used TEXT,\n                    chunk_size INTEGER,\n                    chunk_overlap INTEGER\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create document_records DB, encountered error: \", e)\n    \n    try:\n        cursor.execute(\"INSERT INTO document_records (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap) VALUES (?, ?, ?, ?, ?)\", (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap))\n        conn.commit()\n        conn.close()\n    except Exception as e:\n        handle_local_error(\"Could not update document_records DB, encountered error: \", e)\n\n\n\n# List-splitter function for a large number of embeddings!\ndef split_embeddings_list(all_splits, max_emmbeddings_list_size):\n    for i in range(0, len(all_splits), max_emmbeddings_list_size):  # Step through the large list in steps of max size\n        yield all_splits[i:i + max_emmbeddings_list_size]   # Yield a slice of all_splits from index i upto but NOT including i+max_size \n\n\n# Document vectorization and chunking\ndef LoadNewDocument(input_file):\n\n    global VECTOR_STORE\n    \n    ### L1 - Load Data from Source ###\n    #loader = UnstructuredPDFLoader(\"737.pdf\", mode=\"elements\", strategy=\"fast\")\n    print(\"\\nLoading Document\")\n    #loader.start()\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        handle_local_error(\"Missing values in config.json, could not LoadNewDocument. Error: \", e)\n\n    try:\n        txt_loader = TextLoader(input_file, encoding=\"UTF-8\", autodetect_encoding=\"true\")\n        docs = txt_loader.load()\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Failed to load document for storage to VectorDB, encountered error: \", e)\n    #finally:\n        #loader.stop()\n\n    chunk_sz = 250\n    chunk_olp = 0\n\n    ### L2 - Chunk Source Data ###\n    print(\"Chunking Doc\")\n    #loader.start()\n    try:\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_sz, chunk_overlap = chunk_olp) # chunk_size refers to max size; nice to have some small sliding-window overlap between chunks such as 20-50 chars\n        all_splits = text_splitter.split_documents(docs)\n        \n        #For JINA2 Embeddings:\n        # Initialize an empty list to hold the page_content values\n        page_contents = []\n        # Iterate through each Document in the list\n        for doc in all_splits:\n            # Access the 'page_content' attribute and append it to the 'page_contents' list\n            page_contents.append(doc.page_content)\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Failed to chunk document for storage to VectorDB, encountered error: \", e)\n    #finally:\n        #oader.stop()\n\n\n    ### L3 - Store Chunks in VectorDB ###\n    print(\"Storing to VectorDB: ChromaDB\")\n    #chroma_persist_directory=upload_folder + '/chroma_db_250'\n    #loader.start()\n    try:\n        # Return VectorStore initialized from documents and embeddings.\n        if use_sbert_embeddings:\n            # Ideally should use MAX_BATCH_SIZE obtained elsewhere \n            if len(all_splits) > 5000:\n                split_docs = split_embeddings_list(all_splits, 5000)\n                for split_docs_list in split_docs:\n                    VECTOR_STORE = Chroma.from_documents(documents=split_docs_list, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n            else:\n                VECTOR_STORE = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n        \n        elif use_openai_embeddings:\n            print(\"Using OpenAI Text Ada Model via Azure OpenAI\")\n\n            list_position = 0\n            token_count = 0\n\n            for i in range(list_position, len(all_splits)):\n\n                token_count += len(str(all_splits[i]))\n                if token_count >= 108000:\n                    VECTOR_STORE = Chroma.from_documents(documents=all_splits[list_position:i+1], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder)  #AZURE_OPENAI_EMBEDDINGS defined on line 407\n                    list_position = i+1\n                    token_count = 0\n                    print(\"Loaded batch, sleeping for one minute to stay within rate-limit\")\n                    time.sleep(63)\n                    continue\n\n            # post-loop, if any splits are left to be processed but were missed due to token_count not reaching the limit:\n            if list_position < len(all_splits):\n                VECTOR_STORE = Chroma.from_documents(documents=all_splits[list_position:], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder) #AZURE_OPENAI_EMBEDDINGS defined on line 407\n\n        elif use_bge_base_embeddings or use_bge_large_embeddings:\n            persist_directory = \"\"\n            if use_bge_base_embeddings:\n                persist_directory = vectordb_bge_base_folder\n            elif use_bge_large_embeddings:\n                persist_directory = vectordb_bge_large_folder\n            VECTOR_STORE = Chroma.from_documents(documents=all_splits, embedding=HF_BGE_EMBEDDINGS, persist_directory=persist_directory)    #HF_BGE_EMBEDDINGS defined in process_model() line 2133\n\n        #loader.stop()\n    except Exception as e:\n        #loader.stop()\n        handle_local_error(\"Could not store to VectorDB, encountered error: \", e)\n    #finally:\n        #loader.stop()\n\n    return chunk_sz, chunk_olp\n\n\ndef find_images_in_db(reference_pages):\n\n    print(\"Searching for relevant Images\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method find_images_in_db. Error: \", e)\n\n    matched_images = []\n    matched_images_found = False\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        conn.row_factory = sqlite3.Row\n        print(\"Database connected for image search\")\n    except Exception as e:\n        handle_local_error(\"Could not connect to images DB for image search, encountered error: \", e)\n\n    for doc in reference_pages:\n                \n        #source_filename = os.path.basename(doc)\n\n        for search_string in reference_pages[doc]:\n\n            # Only search for non-empty search strings\n            if search_string:\n                print(f\"String found for image search: {search_string}\")\n                try:\n                    images = conn.execute('SELECT DISTINCT id, image_data FROM images WHERE surrounding_text LIKE ?', ('%' + search_string + '%',)).fetchall()\n                except Exception as e:\n                    handle_error_no_return(\"Could not select images from Images DB, encountered error: \", e)\n                for row in images:\n                    print(\"Matching image found!\")\n                    matched_images_found = True\n                    image_id = row['id']\n                    image_data = row['image_data']\n                    matched_images.append((image_id, image_data))\n\n    conn.close()\n    matched_images = set(matched_images)\n    return matched_images_found, matched_images\n\n\ndef highlight_text_on_page(highlight_list, stream_session_id):\n\n    print(f\"highlight_list: {highlight_list}\")\n\n    try:\n        read_return = read_config(['upload_folder', 'highlighted_docs'])\n        upload_folder = read_return['upload_folder']\n        highlighted_pdfs = read_return['highlighted_docs']\n    except Exception as e:\n        handle_local_error(\"Missing upload_folder in config.json for method highlight_text_on_page. Error: \", e)\n    \n    for index, doc in enumerate(highlight_list, start=1):\n\n        try:\n            pdf_path = os.path.join(upload_folder, doc).replace(\"\\\\\",\"/\")\n            output_file_extension = \"_\" + stream_session_id + '.pdf'\n            output_file_name = doc.replace(\".pdf\",output_file_extension) \n            output_pdf_path = os.path.join(highlighted_pdfs, output_file_name).replace(\"\\\\\",\"/\")\n\n            print(f\"stream_session_id:{stream_session_id}\")\n            print(f\"\\npdf_path:{pdf_path}\")\n\n            highlight_doc = fitz.open(pdf_path)\n        except Exception as e:\n            handle_error_no_return(\"Could not open doc for highlighting, encountered error: \", e)\n            continue\n        \n        for target in highlight_list[doc]:\n            try:\n                text_to_highlight = str(target[1])\n                text_to_highlight = re.sub(r'Row \\d+, Column \\d+: ', '', text_to_highlight)\n                page_number = int(target[0])\n                \n                print(f\"text_to_highlight: {text_to_highlight}\")\n                print(f\"page_number: {page_number}\")\n\n                page = highlight_doc.load_page(page_number-1)\n                text_instances = page.search_for(text_to_highlight)\n            except Exception as e:\n                handle_error_no_return(\"Error loading page or searching for text to highlight, encountered error: \", e)\n                continue\n            \n            for inst in text_instances:\n                try:\n                    print(\"HIGHLIGHTING\", inst)\n                    page.add_highlight_annot(inst)\n                except Exception as e:\n                    handle_error_no_return(\"Could not highlight text instance, encountered error: \", e)\n                    continue\n        \n        try:\n            highlight_doc.save(output_pdf_path, garbage=0, deflate=False, clean=False)\n        except Exception as e:\n            handle_error_no_return(\"Could not save highlighted doc, encountered error: \", e)\n            continue\n\n    return True\n\n\ndef whoosh_text_in_pdf_and_highlight(reference_pages, stream_session_id):\n\n    print(\"Searching Index\")\n\n    try:\n        read_return = read_config(['index_dir'])\n        index_dir = read_return['index_dir']\n    except Exception as e:\n        handle_local_error(\"Missing index_dir in config.json for method whoosh_text_in_pdf_and_highlight. Error: \", e)\n\n    user_should_refer_pages_in_doc = {}\n    docs_have_relevant_info = False\n\n    highlight_list = {}\n\n    try:\n        # Open the index\n        ix = open_dir(index_dir)\n\n        # Create a 'searcher' object\n        with ix.searcher() as searcher:\n            query_parser = QueryParser(\"content\", ix.schema)\n\n            for doc in reference_pages:\n                \n                source_filename = os.path.basename(doc)\n                output_file_extension = \"_\" + stream_session_id + '.pdf'\n                output_file_name = source_filename.replace(\".pdf\",output_file_extension) \n                page_numbers = []\n                highlight_strings = []\n                \n                for search_string in reference_pages[doc]:\n\n                    # Only search for non-empty search strings\n                    if search_string:\n\n                        query = query_parser.parse(search_string)\n\n                        results = searcher.search(query)\n\n                        for hit in results:\n                            print(f\"\\n\\nFound in {hit['title']} on page {hit['pagenumber']}\")\n                            page_numbers.append(int(hit['pagenumber']))\n                            docs_have_relevant_info = True\n                            \n                            highlight_target = [hit['pagenumber'], search_string]\n                            highlight_strings.append(highlight_target)\n\n                page_numbers = set(page_numbers)\n                user_should_refer_pages_in_doc[output_file_name] = page_numbers\n\n                highlight_strings_set = set(tuple(inner_list) for inner_list in highlight_strings)  # Because using a set directly on a list of lists won't work because lists are mutable and cannot be hashed, which is a requirement for the elements of a set. \n                highlight_strings = [list(inner_tuple) for inner_tuple in highlight_strings_set]\n                highlight_list[source_filename] = highlight_strings\n\n    except Exception as e:\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\n    \n    # Highlight line in PDF\n    if docs_have_relevant_info:\n        try:\n            highlight_text_on_page(highlight_list, str(stream_session_id))\n        except Exception as e:\n            handle_error_no_return(\"Could not highlight text, encountered error: \", e)\n\n    return docs_have_relevant_info, user_should_refer_pages_in_doc\n\n\ndef determine_sequence_id_for_chat(chat_id):\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        # Determine sequence_id\n        cursor.execute(\"SELECT COALESCE(MAX(sequence_id), 0) FROM chat_history WHERE chat_id = ?\", (int(chat_id),))\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for a new chat!\n        # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n        # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n        result = cursor.fetchone()\n        current_sequence_id = result[0]     # 'result' will be a list, so extract the first value\n        \n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n\n    return int(current_sequence_id)\n\n\ndef store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, current_prompt_template):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {chat_id}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(chat_id)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template) VALUES (?, ?, ?, ?, ?, ?)\", (int(chat_id), int(sequence_id), user_query_for_history_db, model_response_for_history_db, model_choice, str(current_prompt_template)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n\n\ndef store_chat_history_to_db(user_query_for_history_db, model_response_for_history_db, current_historical_summary):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {CHAT_ID}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n        base_template = read_return['base_template']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(CHAT_ID)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template, history_summary) VALUES (?, ?, ?, ?, ?, ?, ?)\", (int(CHAT_ID), int(SEQUENCE_ID), user_query_for_history_db, model_response_for_history_db, model_choice, str(base_template), str(current_historical_summary)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n    conn.close()\n\n\n# Route for loading all models from model dir\n@app.route('/load_local_models')\ndef load_local_models():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        handle_api_error(\"Missing model_dir in config.json for method load_local_models. Error: \", e)\n    \n    try:\n        models = [f for f in os.listdir(model_dir) if os.path.isfile(os.path.join(model_dir, f))]\n    except Exception as e:\n        handle_api_error(\"Could not load list of local models, encountered error: \", e)\n        \n    #print(f\"locally available models: {models}\")\n    return jsonify({'success': True, 'models': models})\n\n\n@app.route('/upload_new_llm', methods=['POST'])\ndef upload_new_llm():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        handle_api_error(\"Could not determine model_dir in upload_new_llm. Error: \", e)\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        handle_api_error(\"Server-side error recieving LLM file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n\n    try:\n        filepath = os.path.join(model_dir, filename)\n\n        print(\"Loading new LLM - filename: \", filename)\n        print(\"Loading new LLM - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        handle_api_error(\"Failed to save LLM to model_dir, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to handle the submission of the first form (LLM & embeddings model and GPU selection)\n@app.route('/process_model', methods=['POST'])\ndef process_model():\n    \n    global HF_BGE_EMBEDDINGS\n\n    ###---New config.json---###\n\n    config_update_dict = {}\n\n    use_azure_open_ai = 'use_azure' in request.form\n    use_openai_embeddings = 'use_openai_embeddings' in request.form\n    use_sbert_embeddings = 'use_sbert_embeddings' in request.form\n    use_bge_large_embeddings = 'use_bge_large_embeddings' in request.form\n    use_bge_base_embeddings = 'use_bge_base_embeddings' in request.form\n    use_gpu_for_embeddings = request.form.get('use_gpu_for_embeds', False)    # default no\n    model_choice = str(request.form['model_choice'])\n    use_gpu = request.form.get('use_gpu', False)\n\n    config_update_dict.update({'use_azure_open_ai':use_azure_open_ai, 'use_openai_embeddings':use_openai_embeddings, 'use_sbert_embeddings':use_sbert_embeddings, 'use_bge_large_embeddings':use_bge_large_embeddings, 'use_bge_base_embeddings':use_bge_base_embeddings, 'use_gpu_for_embeddings':use_gpu_for_embeddings, 'model_choice':model_choice, 'use_gpu':use_gpu})\n\n    try:\n        if use_bge_base_embeddings or use_bge_large_embeddings:\n            model_name = \"\"\n            if use_bge_base_embeddings:\n                model_name = \"BAAI/bge-base-en\"\n            elif use_bge_large_embeddings:\n                model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n    except Exception as e:\n        handle_api_error(\"Could not load BGE embeddings in process_model, encountered error: \", e)\n    \n    try:\n        write_config(config_update_dict)\n    except Exception as e:\n        handle_local_error(\"Could not write updates to config.json, encountered error: \", e)\n\n    # Redirect to the next step\n    return redirect(url_for('load_file'))\n\n\ndef convert_to_pdf_with_unoconv(input_file_path, output_file_path):\n    print(\"\\n\\nConverting non-PDF document to PDF format\\n\\n\")\n    if platform.system() == 'Windows':\n        subprocess.run(['python', 'unoconv.py', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n    else:\n        subprocess.run(['unoconv', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n\n\n# Route to handle the submission of the second form (file loading)\n@app.route('/process_new_file', methods=['POST'])\ndef process_new_file():\n\n    use_ocr = False\n    try:\n        read_return = read_config(['use_ocr', 'ocr_service_choice'])\n        use_ocr = read_return['use_ocr']\n        ocr_service_choice = read_return['ocr_service_choice']\n    except Exception as e:\n        handle_api_error(\"Could not determine use_ocr in config.json for process_new_file. Disabling OCR and proceeding. Error: \", e)\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        handle_api_error(\"Server-side error recieving file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n    if \"PDF\" in filename:\n        filename = filename.replace(\"PDF\", \"pdf\")\n\n    try:\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n        print(\"Loading new file - filename: \", filename)\n        print(\"Loading new file - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        handle_api_error(\"Failed to save document to app folder, encountered error: \", e)\n\n    if not filename.lower().endswith('.pdf'):\n        try:\n            conv_filename = os.path.splitext(filename)[0] + '.pdf'\n            conv_filepath = os.path.join(app.config['UPLOAD_FOLDER'], conv_filename)\n\n            convert_to_pdf_with_unoconv(filepath, conv_filepath)\n\n            filepath = conv_filepath\n        except subprocess.CalledProcessError as e:\n            handle_api_error(\"Could not convert file to PDF, encountered error: \", e)\n        except Exception as e:\n            handle_api_error(\"Unexpected error when converting file to PDF, encountered error: \", e)\n\n    print(\"Processing PDF file\")\n    \n    if use_ocr:\n        try:\n            if ocr_service_choice == 'AzureVision':\n                input_file = PDFtoAzureOCRTXT(filepath)\n            elif ocr_service_choice == 'AzureDocAi':\n                input_file = PDFtoAzureDocAiTXT(filepath)\n        except Exception as e:\n            handle_error_no_return(\"Failed to OCR text from PDF. Will now attempt to extract text via PyPDF2. Encountered error: \", e)\n            try:\n                input_file = PDFtoTXT(filepath)\n            except Exception as e:\n                handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    else:\n        try:\n            input_file = PDFtoTXT(filepath)\n        except Exception as e:\n            handle_api_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    \n    try:\n        images = extract_images_from_pdf(filepath)\n    except Exception as e:\n        handle_error_no_return(\"Failed to extract images from the PDF document, encountered error: \", e)\n\n    try:\n        store_images_to_db(images)\n    except Exception as e:\n        handle_error_no_return(\"Failed to save images to database, encountered error: \", e)\n    \n    try:\n        chunk_size, chunk_overlap = LoadNewDocument(input_file)\n    except Exception as e:\n        handle_api_error(\"Failed to extract text from PDF: \", e)\n    \n\n    global VECTOR_STORE\n    print(\"\\nRe-Loading VectorDB: ChromaDB\")\n\n    vectordb_used = \"\"\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder', 'embedding_model_choice'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n        embedding_model_choice = read_return['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when reloading VectorDB, could not fully complete process_new_file. Please try restarting the application. Error: \", e)\n\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            vectordb_used = vectordb_sbert_folder\n        elif use_openai_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n            vectordb_used = vectordb_openai_folder\n        elif use_bge_base_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_base_folder\n        elif use_bge_large_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_large_folder\n    except Exception as e:\n        handle_api_error(\"Could not reload VectorDB when trying to process_new_file. Please try restarting the application. Error: \", e)\n\n    try:\n        record_doc_loaded_to_db(filename, embedding_model_choice, vectordb_used, chunk_size, chunk_overlap)\n    except Exception as e:\n        handle_error_no_return(\"Unable to record document loading to records DB, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to store user rating: \n# ATTN: comment out print() statements, as users may elect to leave a rating as a response is being generated, which is when the stdout is redirected to the event stream! \n@app.route('/store_user_rating', methods=['POST'])\ndef store_user_rating():\n    \n    # print(\"Stroing user rating\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json for method store_user_rating. Error: \", e)\n    \n    try:\n        user_rating = request.form['rating']\n        chat_id_for_rating = request.form['chat_id']\n        sequence_id_for_rating = request.form['sequence_id']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read user rating or failed to obtain chat/sequence ID, encountered error: \", e)\n\n    # print(\"user_rating: \", user_rating)\n    # print(\"chat_id_for_rating: \", chat_id_for_rating)\n    # print(\"sequence_id_for_rating: \", sequence_id_for_rating)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history DB for storage of user-rating, encountered error: \", e)\n\n    try:\n        cursor.execute(\n            '''\n            UPDATE chat_history\n            SET user_rating = ?\n            WHERE chat_id = ? AND sequence_id = ?\n            ''',\n            (user_rating, chat_id_for_rating, sequence_id_for_rating)\n        )\n        conn.commit()\n    except Exception as e:\n        handle_api_error(\"Could not store user-rating to chat history db, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify(success=True)\n\n\n\ndef is_llama_cpp_local_server_online():\n    try:\n        response = requests.get('http://localhost:8080/health')\n        \n        if response.status_code == 200:\n            data = response.json()  # parse the JSON response to determine the server status\n            if data['status'] == 'ok':\n                print(f\"Server ready: {data['slots_idle']} idle slots, {data['slots_processing']} processing slots.\")\n                return {\"server_available\":True, \"loading_model\":False, \"status_code\":200}\n            elif data['status'] == 'no slot available':\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":200}\n            \n        elif response.status_code == 503:   # model still loading or no slots\n            data = response.json()\n            if data['status'] == 'loading model':\n                print(\"Server is loading the selected LLM, please wait\")\n                return {\"server_available\":False, \"loading_model\":True, \"status_code\":503}\n            else:\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":503}\n        \n        elif response.status_code == 500:\n            print(\"Server error: Failed to load LLM.\")\n            logger.error(\"llama.cpp - 500 event\")\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n        \n        else:\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n    except requests.exceptions.ConnectionError as e:\n        error_message = \"\\n\\nECONNREFUSED event\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":True, \"status_code\":500}\n    except Exception as e:\n        error_message = f\"\\n\\nCould not check llama.cpp local-server health, encountered error: {e}\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n\ndef send_ctrl_c_to_process(process):\n    if process.poll() is None:  # check if process is still running via poll(), which returns None if a process is still running \n        if platform.system() == 'Windows':\n            process.send_signal(signal.CTRL_BREAK_EVENT)\n        else:\n            process.send_signal(signal.SIGINT)\n        try:\n            # Wait a bit for the process to terminate gracefully:\n            process.wait(timeout=3)\n        except subprocess.TimeoutExpired:\n            print(\"Process did not terminate within timeout, will be force-killed.\")\n            process.kill()  # Sends 'SIGKILL' on Unix-like to force-kill immediately / 'TerminateProcess' on Windows which still allows for graceful termination\n            process.wait()\n            if process.poll() is not None:\n                print(\"Process has been killed successfully.\")\n            else:\n                print(\"Process still running after force kill attempt.\")\n\n\ndef terminate_llama_cpp_process(process):\n    try:\n        # process.terminate() sends 'SIGTERM' on Unix-like systems / 'TerminateProcess' on Windows, allows for graceful termination\n        # process.wait()\n        send_ctrl_c_to_process(process)\n        if process.poll() is not None:  # process has indeed terminated\n            print(\"Process terminated gracefully.\")\n    except Exception as e:\n        handle_local_error(\"Failed to terminate llama.cpp process, encountered error: \", e)\n\n\n@app.route('/llama_cpp_server_starter')\ndef llama_cpp_server_starter():\n\n    global LLM_CHANGE_RELOAD_TRIGGER_SET\n    global LLAMA_CPP_PROCESS\n    global LLM_LOADED_UP\n\n    if LLM_LOADED_UP and not LLM_CHANGE_RELOAD_TRIGGER_SET:\n        model_choice = 'undefined'\n        try:\n            read_return = read_config(['model_choice'])\n            model_choice = read_return['model_choice']\n        except Exception as e:\n            handle_error_no_return(\"Missing model_choice in config.json when attempting to return without re-loading. Printing error and proceeding: \", e)\n        print(f'\\n\\nAlready loaded! Simply returning model choice: {model_choice}\\n\\n')\n        return jsonify({'success': True, 'llm_model': model_choice})\n    elif LLM_CHANGE_RELOAD_TRIGGER_SET:\n        print('\\n\\nProceeding to reload the LLM & resetting the LLM_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False\n\n    try:\n        if is_llama_cpp_local_server_online()['server_available']:\n            print(\"Server online. Terminating and reloading from config.json\")\n            try:\n                terminate_llama_cpp_process(LLAMA_CPP_PROCESS)\n                LLAMA_CPP_PROCESS = None\n            except Exception as e:\n                LLM_LOADED_UP = True\n                handle_api_error(\"Failed to terminate running llama.cpp process, server was likely launched by a previous session. Retruning with the currently loaded LLM. To change, shutdown the previously launched server manually and reload this page. Technical error-details follow: \", e)\n                try:\n                    read_return = read_config(['model_choice'])\n                    model_choice = read_return['model_choice']\n                    return jsonify({'success': True, 'llm_model': model_choice})\n                except Exception as e:\n                    handle_api_error(\"Missing values in config.json when preparing to launch llama.cpp server, encountered error: \", e)\n                return jsonify({'success': True, 'llm_model': 'undefined'})\n                \n                \n    except Exception as e:\n        handle_error_no_return(\"Could not pre-check if llama.cpp server is running, it may be offline. Printing error and proceeding: \", e)\n\n\n    try:\n        read_return = read_config(['model_dir', 'model_choice', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'server_timeout_seconds', 'server_retry_attempts', 'use_gpu'])\n        model_dir = read_return['model_dir']\n        model_choice = read_return['model_choice']\n        local_llm_context_length = read_return['local_llm_context_length']\n        local_llm_max_new_tokens = read_return['local_llm_max_new_tokens']\n        local_llm_gpu_layers = read_return['local_llm_gpu_layers']\n        server_timeout_seconds = read_return['server_timeout_seconds']\n        server_retry_attempts = read_return['server_retry_attempts']\n        use_gpu = read_return['use_gpu']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when preparing to launch llama.cpp server, encountered error: \", e)\n\n\n    try:\n        cpp_model = os.path.join(model_dir, model_choice)\n    except Exception as e:\n        handle_api_error(\"Could not os.join path to model file to launch llama.cpp server, encountered error: \", e)\n\n    if not use_gpu:\n        local_llm_gpu_layers = 0\n\n    try:\n        cpp_app = ['llama-server', '-m', cpp_model, '-ngl', str(local_llm_gpu_layers), '-c', str(local_llm_context_length), '-n', str(local_llm_max_new_tokens), '--host', '0.0.0.0']\n\n        if platform.system() == 'Windows':\n            LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, creationflags=subprocess.CREATE_NEW_CONSOLE)  # Windows only! Comment when containerizing or deploying to Linux/MacOS!\n        else:           \n            # Platform & container agnostic:\n            with open('llama_cpp_server_output_log.txt', 'w') as f:\n                LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, stdout=f, stderr=subprocess.STDOUT, text=True)    #stdout has already been redirected to the file, so simply direct stderr to stdout!\n\n    except Exception as e:\n        handle_api_error(\"Could not launch llama.cpp process, encountered error: \", e)\n\n\n    timeout = server_timeout_seconds   \n    attempts = server_retry_attempts\n\n    try:\n        for _ in range(attempts):\n            if is_llama_cpp_local_server_online()['server_available']:\n                print(\"llama.cpp server launched succesfully! Returning.\")\n                LLM_LOADED_UP = True\n                return jsonify({'success': True, 'llm_model': model_choice})\n            time.sleep(timeout)\n    except Exception as e:\n        handle_error_no_return(\"Could not check server status after launch attempt, printing error and retrying: \", e)\n\n    return handle_api_error(\"Failed to start llama.cpp local-server\")\n\n\n\n@app.route('/load_vectordb')\ndef load_vectordb():\n\n    global VECTOR_STORE\n    global HF_BGE_EMBEDDINGS\n    global AZURE_OPENAI_EMBEDDINGS\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    global VECTORDB_LOADED_UP\n\n    if VECTORDB_LOADED_UP and not VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print(f'\\n\\nVectorDB already loaded! Simply returning.\\n\\n')\n        return jsonify({'success': True})\n    elif VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print('\\n\\nProceeding to reload VectorDB & resetting the VECTORDB_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n\n    try:\n        read_return = read_config(['use_gpu_for_embeddings', 'use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_gpu_for_embeddings = read_return['use_gpu_for_embeddings']\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to load_vectordb. Error: \", e)\n    \n    \n    ### 1 - Load VectorDB from disk\n    print(\"\\n\\nLoading VectorDB: ChromaDB\\n\\n\")\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            # try:\n            #     # chroma_client = VECTOR_STORE.PersistentClient\n            #     # max_batch_size = chroma_client._producer.max_batch_size\n            #     max_batch_size = VECTOR_STORE.max_batch_size\n            #     print(f\"max_batch_size: {max_batch_size}\")\n            # except Exception as e:\n            #     print(f\"Could not get max_batch_size. Error: {e}\")\n        \n        elif use_openai_embeddings:\n\n            try:\n                read_return = read_config(['azure_openai_text_ada_api_url', 'azure_openai_text_ada_api_key', 'azure_openai_api_type', 'azure_openai_api_version', 'azure_openai_text_ada_deployment_name'])\n                azure_openai_text_ada_api_url = read_return['azure_openai_text_ada_api_url']\n                azure_openai_text_ada_api_key = read_return['azure_openai_text_ada_api_key']\n                azure_openai_api_type = read_return['azure_openai_api_type']\n                azure_openai_api_version = read_return['azure_openai_api_version']\n                azure_openai_text_ada_deployment_name = read_return['azure_openai_text_ada_deployment_name']\n            except Exception as e:\n                handle_api_error(\"Missing values for Azure OpenAI Embeddings in method load_model_and_vectordb in config.json. Error: \", e)\n            \n            try:\n                os.environ[\"OPENAI_API_BASE\"] = azure_openai_text_ada_api_url\n                os.environ[\"OPENAI_API_KEY\"] = azure_openai_text_ada_api_key\n                os.environ[\"OPENAI_API_TYPE\"] = azure_openai_api_type\n                os.environ[\"OPENAI_API_VERSION\"] = azure_openai_api_version\n            except Exception as e:\n                handle_api_error(\"Could not set OS environment variables for Azure OpenAI Embeddings in load_model_and_vectordb, encountered error: \", e)\n\n            \n            AZURE_OPENAI_EMBEDDINGS = OpenAIEmbeddings(deployment=azure_openai_text_ada_deployment_name)\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n        \n        elif use_bge_base_embeddings:\n            model_name = \"BAAI/bge-base-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n                \n        \n        elif use_bge_large_embeddings:\n            model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n        \n        #VECTOR_STORE = Chroma(persist_directory=VECTORDB_SBERT_FOLDER, embedding_function=HuggingFaceEmbeddings())\n    except Exception as e:\n        handle_api_error(\"Could not load VectorDB, encountered error: \", e)\n    \n    VECTORDB_LOADED_UP = True\n    return jsonify(success=True)\n\n\n@app.route('/set_prompt_template', methods=['POST'])\ndef set_prompt_template():\n\n    base_template = \"\"\n\n    try:\n        base_template = request.form['prompt_template']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read prompt_template from the POST request in method set_prompt_template, encountered error: \", e)\n    \n    try:\n        write_config({'base_template':base_template})\n    except Exception as e:\n        handle_api_error(\"Could not update base_template in method set_prompt_template, encountered error: \", e)\n\n    return jsonify({'success':True})\n\n\n@app.route('/fetch_file_list_for_vector_db', methods=['POST'])\ndef fetch_file_list_for_vector_db():\n\n    print(\"Loading file list for selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method fetch_file_list_for_vector_db, encountered error: \", e)\n\n    # For the VectorDB presently picked by the user in the dropdown, obtain the associated VectorDB folder for the select query:\n    vdb_for_select = \"\"\n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            read_return = read_config(['vectordb_bge_large_folder'])\n            vdb_for_select = read_return['vectordb_bge_large_folder']\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            read_return = read_config(['vectordb_bge_base_folder'])\n            vdb_for_select = read_return['vectordb_bge_base_folder']\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            read_return = read_config(['vectordb_sbert_folder'])\n            vdb_for_select = read_return['vectordb_sbert_folder']\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            read_return = read_config(['vectordb_openai_folder'])\n            vdb_for_select = read_return['vectordb_openai_folder']\n\n    except Exception as e:\n        handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_docs_loaded_db in config.json in method fetch_file_list_for_vector_db. Error: \", e)\n\n    file_row_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to sqlite_docs_loaded_db database to load file list, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT document_name, vectordb_used, chunk_size, chunk_overlap FROM document_records where vectordb_used = ?\", (vdb_for_select,))\n    except Exception as e:\n        handle_api_error(\"Could not get document list from document_records db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            file_row_list.append(list(list_item))\n    except Exception as e:\n        handle_api_error(\"Could not parse document list from document_records db, encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, 'file_row_list': file_row_list})\n\n\n@app.route('/reset_vector_db_on_disk', methods=['POST'])\ndef reset_vector_db_on_disk():\n\n    print(\"Resetting selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['base_directory'])\n        base_directory = read_return['base_directory']\n    except Exception as e:\n        handle_local_error(\"Could not read base_directory from config.json for reset_vector_db_on_disk. Error: \", e)\n\n    try:\n        current_datetime = datetime.datetime.now()\n        formatted_datetime = current_datetime.strftime('%Y-%m-%d-%Hhr-%Mmin-%Ssec')\n    except Exception as e:\n        handle_api_error(\"Could not obtain timestamp in reset_vector_db_on_disk, encountered error: \", e)\n\n    # Now that we have all pre-requisite data to create a new VectorDB, proceed to do so by checking the model the user had currently picked from the dropdown: \n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            vectordb_bge_large_folder = base_directory + '/chroma_db_bge_large_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_large_folder':vectordb_bge_large_folder})\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            vectordb_bge_base_folder = base_directory + '/chroma_db_bge_base_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_base_folder':vectordb_bge_base_folder})\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            vectordb_sbert_folder = base_directory + '/chroma_db_250_sbert_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_sbert_folder':vectordb_sbert_folder})\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            vectordb_openai_folder = base_directory + '/chroma_db_openai_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_openai_folder':vectordb_openai_folder})\n\n    except Exception as e:\n        handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    restart_required = True\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n    try:\n        read_return = read_config(['embedding_model_choice'])\n        set_embedding_model_choice = read_return['embedding_model_choice']\n        if set_embedding_model_choice != selected_embedding_model_choice:\n            restart_required = False\n            VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n    except Exception as e:\n        handle_error_no_return(\"Could not compare selected and set embedding models when determining if restart_required in reset_vector_db_on_disk(), encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, \"restart_required\": restart_required})\n\n\n@app.route('/load_chat_history_list')\ndef load_chat_history_list():\n\n    print(\"loading chat history list for sidebar\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_history_db in config.json in method load_chat_history_list. Error: \", e)\n\n    history_id_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to sqlite_history_db database to load chat history list, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT DISTINCT chat_id FROM chat_history\")\n    except Exception as e:\n        handle_api_error(\"Could not get list from chat history db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            history_id_list.append(list_item)\n    except Exception as e:\n        handle_api_error(\"Could not parse chat history list from db, encountered error: \", e)\n\n    #print(f'returning chat hsitory list: {history_id_list}')\n\n    return jsonify({'success': True, 'history_list': history_id_list})\n\n\n@app.route('/load_chat_history', methods=['POST'])\ndef load_chat_history():\n\n    global CHAT_ID\n    global SEQUENCE_ID\n    global HISTORY_SUMMARY\n    global HISTORY_MEMORY_WITH_BUFFER\n\n    print(\"loading chat history\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method load_chat_history. Error: \", e)\n\n    # Clear chat history of current chat, prep for loading historical chat summary:\n    # try:\n    #     HISTORY_MEMORY_WITH_BUFFER.chat_memory.clear()\n    #     HISTORY_MEMORY_WITH_BUFFER = ConversationSummaryBufferMemory(llm=LLM, max_token_limit=300, return_messages=False)\n    #     HISTORY_SUMMARY = {}\n    # except Exception as e:\n    #     handle_error_no_return(\"Could not clear memory when loading chat history, encountered error: \", e)\n\n    try:\n        chat_id_for_history_search = request.form['chat_id']\n        CHAT_ID = request.form['chat_id']\n    except Exception as e:\n        handle_api_error(\"Could not retrieve Chat ID from request form, encountered error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    sequence_id_for_history_search = 1\n    retrieve_history = True\n    chat_history = []\n    old_chat_model = \"\"\n\n    while(retrieve_history):\n\n        try:\n            c.execute(\"SELECT user_query FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n            \n            user_message = str(result[0])\n\n            user_message = user_message.strip('\\n')\n            regex_to_swap_multiple_spaces_with_newline = r' {2,}'\n            user_message = re.sub(regex_to_swap_multiple_spaces_with_newline, '<br>', user_message)\n\n            user_message = '<div class=\"user-message\">' + user_message + '</div>'\n\n            chat_history.append(user_message)\n\n            c.execute(\"SELECT llm_response FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n\n            result = str(result[0])\n            result_parts = result.split(\"pdf_pane_data=\",1)\n            # llm_response = '<div class=\"llm-wrapper\"> <div class=\"llm-response\">' + str(result[0]) + '</div>'\n            llm_response = '<div class=\"response-and-viewer-container\"><div class=\"llm-wrapper\"> <div class=\"llm-response\">' + result_parts[0]\n\n        except Exception as e:\n            handle_api_error(\"Could not retrieve chat history, encountered error: \", e)\n        \n        llm_response = llm_response.strip('\\n')\n        llm_response = llm_response.replace('\\n\\n', '<br><br>')\n        llm_response = llm_response.replace('\\n', '<br>')\n        \n        try:\n            c.execute(\"SELECT user_rating FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n        except Exception as e:\n            handle_error_no_return(\"Could not fetch user rating, encountered error: \", e)\n\n        response_rated = False\n        user_rating_for_history_chat = None\n\n        if result[0]:\n            response_rated = True\n            try:\n                user_rating_for_history_chat = int(result[0])\n                #print(f'rating exists: {user_rating_for_history_chat}')\n            except Exception as e:\n                handle_error_no_return(\"Could not retrieve integer value of user rating, encountered error: \", e)\n\n\n        llm_rating = f'''<div class=\"star-rating\" data-rated={response_rated} rating-chat-id={chat_id_for_history_search} rating-sequence-id={sequence_id_for_history_search}>\n        <i class=\"far fa-star\" data-rate=\"1\"></i>\n        <i class=\"far fa-star\" data-rate=\"2\"></i>\n        <i class=\"far fa-star\" data-rate=\"3\"></i>\n        <i class=\"far fa-star\" data-rate=\"4\"></i>\n        <i class=\"far fa-star\" data-rate=\"5\"></i>\n        </div>\n        </div>\n        </div>'''\n\n\n        if user_rating_for_history_chat:\n            rating_parts = llm_rating.split(\"far\", user_rating_for_history_chat)\n            if len(rating_parts) <= user_rating_for_history_chat:\n                llm_rating = \"fas\".join(rating_parts)\n            else:\n                llm_rating = \"fas\".join(rating_parts[:-1]) + \"fas\" + \"far\".join(rating_parts[-1:])\n\n        llm_response += llm_rating\n\n        if len(result_parts) > 1:\n            llm_response += result_parts[1]\n            llm_response += \"</div>\"\n            llm_response = llm_response.strip('\\n')\n            llm_response = llm_response.replace('\\n\\n', '<br><br>')\n            llm_response = llm_response.replace('\\n', '<br>')\n\n        chat_history.append(llm_response)\n\n        # Increment sequence ID for next iteration:\n        sequence_id_for_history_search += 1\n\n        # But first, check to see if next sequence exists!\n        try:\n            c.execute(\"SELECT EXISTS(SELECT 1 FROM chat_history WHERE chat_id = ? AND sequence_id = ?)\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            exists = c.fetchone()[0]\n        except Exception as e:\n            handle_api_error(\"Could not determine if next sequence exists in chat history DB, encountered error: \", e)\n            \n        if not exists:\n            SEQUENCE_ID = sequence_id_for_history_search - 1\n            retrieve_history = False\n            try:\n                c.execute(\"SELECT llm_model FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                old_chat_model = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not determine previously used LLM in chat, encountered error: \", e)\n            try:\n                c.execute(\"SELECT history_summary FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                history_summary_dict = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not fetch history summary of last chat, encountered error: \", e)\n            c.close()\n\n    # Convert History Summary and add a new key indicating it was recently cleared!\n    if history_summary_dict is not None and history_summary_dict != \"\" and history_summary_dict != 'None':\n        print(f\"\\n\\history_summary_dict string from old chat: {history_summary_dict}\\n\\n\")\n        try:\n            HISTORY_SUMMARY = ast.literal_eval(history_summary_dict)    #cast as dictionary\n            HISTORY_SUMMARY[\"has_been_reset\"] = True\n        except Exception as e:\n            handle_error_no_return(\"Could not cast history summary string from DB to dict and/or set has_been_reset boolean, encountered error: \", e)\n\n    # Temp prints:\n    print(f\"\\n\\nHISTORY_SUMMARY: {HISTORY_SUMMARY}\\n\\n\")\n    # print(f\"\\n\\history_summary_dict: {history_summary_dict}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.summary: {HISTORY_MEMORY_WITH_BUFFER.summary}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.chat_memory.messages: {HISTORY_MEMORY_WITH_BUFFER.chat_memory.messages}\\n\\n\")\n    print(f'\\n\\nChat history loaded for chat with model: {old_chat_model}\\n\\n')\n\n    return jsonify({'success': True, 'chat_history': chat_history, 'old_chat_model': old_chat_model})\n\n\n@app.route('/init_chat_history_db')\ndef init_chat_history_db():\n\n    global CHAT_ID\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_api_error(\"Missing sqlite_history_db in config.json in method init_chat_history_db. Error: \", e)\n\n    # Connect to chat_history.db to determine appropriate chat_id\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    # If the database does not currently exist...\n    try:\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS chat_history (\n                        id INTEGER PRIMARY KEY,\n                        chat_id INTEGER,\n                        sequence_id INTEGER,\n                        user_query TEXT,\n                        llm_response TEXT,\n                        user_rating INTEGER,\n                        llm_model TEXT, \n                        prompt_template TEXT,\n                        history_summary TEXT\n            )\n        ''')\n        conn.commit()\n    except Exception as e:\n        handle_api_error(\"Could not create new chat history db, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT COALESCE(MAX(chat_id), 0) FROM chat_history\")\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for an empty DB!\n\n        result = c.fetchone()\n\n        # 'result' will be a tuple, so extract the first element\n        max_chat_id = result[0]\n\n        new_chat_id = max_chat_id + 1\n        CHAT_ID = new_chat_id\n\n        print(f\"Chat history DB initialised with CHAT_ID: {CHAT_ID}\")\n    except Exception as e:\n        handle_api_error(\"Could not set CHAT_ID, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify({'success': True, 'chat_id': CHAT_ID})\n\n\ndef fetch_image_from_db(image_id):\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method fetch_image_from_db. Error: \", e)\n    \n    # 1 - Connect to DB\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n    except Exception as e:\n        handle_local_error(\"Could not connect to images database, encountered error: \", e)\n    \n    # 2 - Get Images\n    try:\n        conn.row_factory = sqlite3.Row\n        row = conn.execute('SELECT image_data FROM images WHERE id = ?', (image_id,)).fetchone()\n        images_bytes = row['image_data'] if row else None\n    except Exception as e:\n        handle_local_error(\"Could not fetch image from DB, encountered error: \", e)\n    \n    conn.close()\n    \n    return images_bytes\n\n\n@app.route('/image_display/<int:image_id>')\ndef image_display(image_id):\n    print(f\"\\n\\nprepping image for display: {image_id}\\n\\n\")\n\n    try: \n        image_bytes = fetch_image_from_db(image_id)\n    except Exception as e:\n        handle_local_error(\"Could not fetch image for display, encountered error: \", e)\n    \n    try:\n        encoded = base64.b64encode(image_bytes).decode('utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not encode image for display URI, encountered error: \", e)\n\n    # Return an HTML response with the embedded image:\n    data_uri = f\"data:image/jpeg;base64,{encoded}\"\n    html_content = f'<img src=\"{data_uri}\" alt=\"Image\">'\n\n    return html_content\n\n\ndef extract_significant_phrases(query):\n    print(\"Extracting significant phrases\")\n\n    try:\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n    except Exception as e:\n        handle_error_no_return(\"Failed to download & set stopwords, encountered error: \", e)\n    \n    try:\n        tokens = [token for token in query.lower().split() if token not in stop_words]\n    except Exception as e:\n        handle_local_error(\"Could not extract significant tokens, encountered error: \", e)\n\n    return tokens\n\n\ndef calculate_relevance_score(phrases, document_content):\n    #print(\"calculating relevance score\")\n    \n    try:\n        content_lower = document_content.lower()\n    except Exception as e:\n        handle_local_error(\"Could not read document_content in calculate_relevance_score(), encountered error: \", e)\n    \n    #print(f\"document content: {content_lower}\")\n    \n    #score = sum(1 for phrase in phrases if phrase in content_lower)\n    \n    score = 0\n    try:\n        for phrase in phrases:\n            if phrase in content_lower:\n                print(f\"Match found to enable RAG: {phrase}\")\n                score += 1\n    except Exception as e:\n        handle_local_error(\"Could not compare phrases in calculate_relevance_score(), encountered error: \", e)\n    \n    return score\n\n\ndef filter_relevant_documents(query, search_results, threshold=1):\n\n    print(\"Checking relevant docs to determin if RAG is required\")\n\n    do_rag = False\n    page_contents = []\n\n    try:\n        significant_phrases = extract_significant_phrases(query)\n    except Exception as e:\n        handle_local_error(\"Could not extract significant phrases, encountered error: \", e)\n    \n    print(f\"significant tokens: {significant_phrases}\")\n    #relevant_documents = []\n\n    try:\n        for document in search_results:\n            # check for non-empty source field\n            if document.page_content:\n                page_contents.append(document.page_content)\n\n            if not do_rag:  # if do_rag has already been set to true, why look?\n                if document.metadata.get('source'):\n                    score = calculate_relevance_score(significant_phrases, document.page_content)\n                    if score >= threshold:\n                        #relevant_documents.append(document)\n                        print(\"Must do RAG!\")\n                        do_rag = True\n    except Exception as e:\n        handle_local_error(\"Could not read calculate relevance score, encountered error: \", e)\n\n    #return relevant_documents\n    return page_contents, do_rag\n\n\n\n@app.route('/setup_for_llama_cpp_response', methods=['POST'])\ndef setup_for_llama_cpp_response():\n\n    global QUERIES\n\n    do_rag = True\n\n    stream_session_id = \"\"\n    key_for_vector_results = \"\"\n    # Generate a unique session ID using universally Unique Identifier via the uuid4() method, wherein the randomness of the result is dependent on the randomness of the underlying operating system's random number generator\n    # UUI is a standard used for creating unique strings that have a very high likelihood of being unique across all time and space, for ex: f47ac10b-58cc-4372-a567-0e02b2c3d479\n    try:\n        stream_session_id = str(uuid.uuid4())\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n    except Exception as e:\n        handle_api_error(\"Error creating unique stream_session_id when attempting to setup_for_streaming_response. Error: \", e)\n    \n\n    # Determine do_rag\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'force_enable_rag', 'force_disable_rag', 'local_llm_chat_template_format', 'base_template'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        force_enable_rag = read_return['force_enable_rag']\n        force_disable_rag = read_return['force_disable_rag']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n        base_template = read_return['base_template']\n\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to setup_for_streaming_response. Error: \", e)\n\n    try:\n        # Attempt to get query data\n        user_query = request.json['user_query']\n        chat_id = request.json['chat_id']\n\n        # Store the query associated with the ID\n        QUERIES[stream_session_id] = user_query\n    except KeyError:\n        handle_api_error(\"Could not obtain and/or store user_query in setup_for_streaming_response, encountered error: \", e)\n\n    print(\"chat_id: \", chat_id)\n\n    # Perform similarity search on the vector DB\n    print(\"\\n\\nPerforming similarity search to determine if RAG necessary\\n\\n\")\n    embedding_function = None\n    try:\n        if use_sbert_embeddings:\n            embedding_function=HuggingFaceEmbeddings()\n        elif use_openai_embeddings:\n            embedding_function=AZURE_OPENAI_EMBEDDINGS\n        elif use_bge_base_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n        elif use_bge_large_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n    except Exception as e:\n        handle_error_no_return(\"Could not set embedding_function for similarity_search when attempting to setup_for_streaming_response, encountered error: \", e)\n    \n    try:\n        docs = VECTOR_STORE.similarity_search(user_query, embedding_fn=embedding_function)\n        # docs_with_relevance_score = VECTOR_STORE.similarity_search_with_relevance_scores(user_query, 10, embedding_fn=embedding_function)\n        # docs_list_with_cosine_distance = VECTOR_STORE.similarity_search_with_score(user_query, 10, embedding_fn=embedding_function)\n        # print(f'\\n\\nsimple similarity search results: \\n {docs}\\n\\n')\n        # print(f'\\n\\nRelevance Score similarity search results (range 0 to 1): \\n {docs_with_relevance_score}\\n\\n')\n        # print(f'\\n\\nDocs list most similar to query based on cosine distance: \\n {docs_list_with_cosine_distance}\\n\\n')\n    except Exception as e:\n        handle_error_no_return(\"Could not perform similarity_search to determine do_rag when attempting to setup_for_streaming_response, encountered error: \", e)\n\n    print(\"\\n\\nDetermining do_rag \\n\\n\")\n    # We do not modify the force_enable_rag or force_disable_rag flags in this method, we simply respond to them here. UI updates should handle those flags.\n    if force_enable_rag:\n        print(\"\\n\\nFORCE_ENABLE_RAG True, force enabling RAG and returning\\n\\n\")\n        try:\n            page_contents, do_rag = filter_relevant_documents(user_query, docs)\n            do_rag = True\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"Error force-enabling RAG, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n    elif force_disable_rag:\n        print(\"\\n\\nFORCE_DISABLE_RAG True, force disabling RAG and returning\\n\\n\")\n        do_rag = False\n    else:\n        try:\n            page_contents, do_rag = filter_relevant_documents(user_query, docs)\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"RAG Error, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n    \n    print(f'Do RAG? {do_rag}')\n\n    try:\n        write_config({'do_rag':do_rag})\n    except Exception as e:\n        handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n\n    \n    # Having determined do_rag, time to build the prompt template!\n    \n    if do_rag:  # add similarity search results for RAG!\n        try:\n            QUERIES[key_for_vector_results] = docs\n            user_query += f\"\\n\\nThe following context might be helpful in answering the user query above:\\n{page_contents}\"\n            print(f\"RAG formatted user_query: {user_query}\")\n        except Exception as e:\n            try:\n                write_config({'do_rag':False})\n            except Exception as e:\n                handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n            handle_error_no_return(\"RAG Error: Could not update QUERIES dict and user_query during setup_for_streaming_response, proceeding without RAG. Encountered error: \", e)\n\n    current_sequence_id = determine_sequence_id_for_chat(chat_id)\n    formatted_prompt = \"\"\n    print(\"current_sequence_id: \", current_sequence_id)\n    if current_sequence_id > 0:    # get the last prompt so we can continue the completions\n\n        try:\n            read_return = read_config(['sqlite_history_db'])\n            sqlite_history_db = read_return['sqlite_history_db']\n        except Exception as e:\n            handle_error_no_return(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n        # Connect to or create the DB\n        try:\n            conn = sqlite3.connect(sqlite_history_db)\n            cursor = conn.cursor()\n        except Exception as e:\n            handle_error_no_return(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n        try:\n            # Determine sequence_id\n            cursor.execute(\"SELECT prompt_template FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id), int(current_sequence_id)))\n            # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n            # This accounts for a new chat!\n            # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n            # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n            result = cursor.fetchone()\n            formatted_prompt = str(result[0])\n            \n        except Exception as e:\n            handle_error_no_return(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n    \n    if formatted_prompt == \"\":  # could not be updated above\n        current_sequence_id = 0 # reset chat sequence id\n\n    if local_llm_chat_template_format == 'llama3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        else:\n            formatted_prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{base_template}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    elif local_llm_chat_template_format == 'llama2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<s>[INST] {user_query} [/INST]\"\n        else:\n            formatted_prompt += f\"<s>[INST] <<SYS>>\\n {base_template} \\n<</SYS>>\\n\\n {user_query}  [/INST]\"\n\n    elif local_llm_chat_template_format == 'chatml':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"\\n<|im_start|>user\\n{user_query}<|im_end|>\\n\"\n        else:\n            formatted_prompt += f\"<|im_start|>system\\n{base_template}<|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\"\n\n    elif local_llm_chat_template_format == 'phi3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n        else:\n            formatted_prompt += f\"<|system|>\\n{base_template}<|end|>\\n<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n\n    elif local_llm_chat_template_format == 'command-r':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        else:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{base_template}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n\n    elif local_llm_chat_template_format == 'deepseek':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"### Instruction:\\n{user_query}\\n### Response:\\n\"\n        else:\n            formatted_prompt += f\"{base_template}### Instruction:\\n{user_query}\\n### Response:\\n\"\n\n    elif local_llm_chat_template_format == 'deepseek-coder-v2':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"User: {user_query}\\nAssistant: \"\n        else:\n            formatted_prompt += f\"<|begin_of_sentence|>{base_template}\\nUser: {user_query}\\nAssistant: \"\n\n    elif local_llm_chat_template_format == 'vicuna':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"USER: {user_query}\\nASSISTANT: \"\n        else:\n            formatted_prompt += f\"{base_template}\\n\\nUSER: {user_query}\\nASSISTANT: \"\n\n    elif local_llm_chat_template_format == 'openchat':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n        else:\n            formatted_prompt += f\"<s>GPT4 Correct System: {base_template}<|end_of_turn|>GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n\n    elif local_llm_chat_template_format == 'gemma2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<start_of_turn>user\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n        else:\n            formatted_prompt += f\"<start_of_turn>user\\n{base_template}\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n\n    # Return a bunch of stuff\n    new_sequence_id = int(current_sequence_id) + 1\n    return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag, \"formatted_user_prompt\": formatted_prompt, \"sequence_id\":new_sequence_id})\n\n\n\n@app.route('/get_references', methods=['POST'])\ndef get_references():\n\n    print(\"\\n\\nGetting References\\n\\n\")\n\n    try:\n        read_return = read_config(['do_rag', 'upload_folder', 'local_llm_chat_template_format'])\n        do_rag = read_return['do_rag']\n        upload_folder = read_return['upload_folder']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n    except Exception as e:\n        handle_api_error(\"Missing values in config.json when attempting to get_references. Error: \", e)\n\n    try:\n        stream_session_id = request.json['stream_session_id']\n        user_query = request.json['user_query']\n        llm_response = request.json['llm_response']\n        formatted_user_prompt = request.json['formatted_user_prompt']\n        chat_id = request.json['chat_id']\n        sequence_id = request.json['sequence_id']\n    except Exception as e:\n        handle_api_error(\"Could not read request content in method get_references, encountered error: \", e)\n\n    if local_llm_chat_template_format == 'llama3':\n        formatted_user_prompt += f\"{llm_response}<|eot_id|>\"\n    elif local_llm_chat_template_format == 'llama2':\n        formatted_user_prompt += f\"{llm_response}</s>\"\n    elif local_llm_chat_template_format == 'chatml':\n        formatted_user_prompt += f\"{llm_response}<|im_end|>\\n\"\n    elif local_llm_chat_template_format == 'phi3':\n        formatted_user_prompt += f\"{llm_response}<|end|>\\n\"\n    elif local_llm_chat_template_format == 'command-r':\n        formatted_user_prompt += f\"{llm_response}<|END_OF_TURN_TOKEN|>\"\n    elif local_llm_chat_template_format == 'deepseek':\n        formatted_user_prompt += f\"{llm_response}\\n<|EOT|>\\n\"\n    elif local_llm_chat_template_format == 'deepseek-coder-v2':\n        formatted_user_prompt += f\"{llm_response}<|end_of_sentence|>\"\n    elif local_llm_chat_template_format == 'vicuna':\n        formatted_user_prompt += f\"{llm_response} </s>\\n\"\n    elif local_llm_chat_template_format == 'openchat':\n        formatted_user_prompt += f\"{llm_response}<|end_of_turn|>\"\n    elif local_llm_chat_template_format == 'gemma2':\n        formatted_user_prompt += f\"{llm_response}<end_of_turn>\\n\"\n\n    if not do_rag:\n        print(\"\\n\\nSkipping RAG, storing chat history and returning\\n\\n\")\n        try:\n            store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query, llm_response, formatted_user_prompt)\n        except Exception as e:\n            handle_error_no_return(\"Could not store_llama_cpp_chat_history_to_db in get_references(), encountered error: \", e)\n        return jsonify({'success': True})\n        \n    try:\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n        docs = QUERIES[key_for_vector_results]\n        print(f\"\\n\\ntype(docs): {type(docs)}\\n\\n\")\n    except Exception as e:\n        handle_api_error(\"Could not obtain relevant data from QUERIES dict, encountered error: \", e)\n\n    # Having obtained the relevant info, clear the QUERIES{} dict so as to not bloat it!\n    try:\n        del QUERIES[key_for_vector_results]\n    except Exception as e:\n        handle_error_no_return(\"Error clearing queries dict in method get_references: \", e)\n\n    reference_response = \"\"\n\n    all_sources = {}\n    reference_pages = {}\n\n    for doc in docs:\n        \n        try:\n            relevant_page_text = str(doc.page_content)\n            # relevant_page_text = relevant_page_text.replace('\\n', ' ')\n            source_filepath = str(doc.metadata.get('source'))\n            #print(relevant_page_text)\n            #print(source_filepath)\n        except Exception as e:\n            handle_error_no_return(\"Could not access doc.page_content and/or doc.metadata, encountered error: \", e)\n            continue\n        \n        relevant_page_text = relevant_page_text.split('\\n', 1)[0]\n        relevant_page_text = relevant_page_text.strip()\n        relevant_page_text = re.sub(r'[\\W_]+Page \\d+[\\W_]+', '', relevant_page_text)\n        source_filepath = source_filepath.replace('\\\\', '/')\n        \n        try:\n            source_filename = os.path.basename(source_filepath)\n            _, file_extension = os.path.splitext(source_filepath)\n            #print(source_filename)\n            #print(file_extension)\n        except Exception as e:\n            handle_error_no_return(\"Could not parse path with OS lib, encountered error: \", e)\n            continue\n\n        # The source_filepath will likely always reference a TXT file because of how we're loading the VectorDB!\n        # Check if the PDF version of the source doc exists\n        if file_extension == '.txt':\n\n            #print(\"\\n\\ntxt file\\n\\n\")\n\n            # Construct the path to the potential PDF version\n            pdf_version_path = os.path.join(upload_folder, os.path.basename(source_filepath).replace('.txt', '.pdf'))   # not catching an error here as os.path.basename(source_filepath) has already been caught just above!\n\n            # Check if PDF version of the source TXT exists!\n            if os.path.exists(pdf_version_path):\n\n                #print(\"\\n\\pdf exists\\n\\n\")\n\n                source_filename = source_filename.replace('.txt', '.pdf')\n                \n                if pdf_version_path in reference_pages:\n                    reference_pages[pdf_version_path].extend([relevant_page_text])\n                else:\n                    reference_pages[pdf_version_path] = [relevant_page_text]\n\n                # Add this file to our sources dictionary if it's not already present\n                if source_filename not in all_sources:\n                    source_filepath = pdf_version_path\n                    all_sources.update({source_filename: source_filepath})\n\n            # Else PDF does not exist, TXT is the source\n            else:\n                # Check if the TXT is already in the sources dict\n                if source_filename not in all_sources:\n                    try:\n                        source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                        all_sources.update({source_filename: source_filepath})\n                    except Exception as e:\n                        handle_error_no_return(\"Could not construct filepath for TXT file, encountered error: \", e)\n\n\n        # If file is not a TXT file\n        else:\n            # Check if the TXT is already in the sources dict\n            if source_filename not in all_sources:\n                try:\n                    source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                    all_sources.update({source_filename: source_filepath})\n                except Exception as e:\n                    handle_error_no_return(\"Could not construct filepath for non-TXT file, encountered error: \", e)\n\n    # print(f\"\\n\\nreference_pages: {reference_pages}\\n\\n\")\n\n    try:\n        docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf_and_highlight(reference_pages, stream_session_id)\n        # docs_have_relevant_info, user_should_refer_pages_in_doc = whoosh_text_in_pdf(reference_pages)\n    except Exception as e:\n        handle_error_no_return(\"Could not search Whoosh Index, encountered error: \", e)\n\n    try:\n        matched_images_found, matched_images_in_bytes = find_images_in_db(reference_pages)\n    except Exception as e:\n        handle_error_no_return(\"Could not search for images, encountered error: \", e)\n\n    refer_pages_string = \"\"\n    download_link_html = \"\"\n    images_iframe_html = \"\"\n\n    if docs_have_relevant_info:\n\n        refer_pages_string = \"<br><br><h6>Refer to the following pages in the mentioned docs:</h6><br>\"\n        \n        # for doc in user_should_refer_pages_in_doc:\n        for index, doc in enumerate(user_should_refer_pages_in_doc, start=1):\n            # pdf_iframe_id = str(doc) + \"PdfViewer\"\n            pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\n            frame_doc_path = f\"/pdf/{doc}\"\n            # frame_doc_path = upload_folder + f\"/{doc}\" \n            try:\n                refer_pages_string += f\"<br><h6>{doc}: \"\n                for page in user_should_refer_pages_in_doc[doc]:\n                    frame_doc_path += \"#page=\" + str(page) \n                    refer_pages_string += f'<a href=\"javascript:void(0)\" onclick=\"goToPage(\\'{pdf_iframe_id}\\', \\'{frame_doc_path}\\')\">Page {page}</a>, '\n                    frame_doc_path = f\"/pdf/{doc}\"\n                refer_pages_string = refer_pages_string.strip(', ') + \"</h6><br>\"\n            except Exception as e:\n                handle_error_no_return(\"Could not construct refer_pages_string, encountered error: \", e)\n\n        # download_link_html = \"<br><h6>Refer to the source documents below:</h6>\"\n        pdf_right_pane_id = \"stream\" + stream_session_id + \"PdfPane\"\n        download_link_html = f'<div class=\"pdf-viewer\" id={pdf_right_pane_id}>'\n\n        for index, source in enumerate(user_should_refer_pages_in_doc, start=1):\n            try:\n                # print(\"\\n\\nlooping sources\\n\\n\")\n                download_link_url = url_for('download_file', filename=source)\n                pdf_iframe_id = \"stream\" + stream_session_id + \"PdfViewer\" + str(index)\n                download_link_html += f'<br><h6><a href=\"{download_link_url}\" target=\"_blank\"><iframe id=\"{pdf_iframe_id}\" src=\"{download_link_url}\" width=\"100%\" height=\"600\"></iframe></a></h6><br>'\n            except Exception as e:\n                handle_error_no_return(\"Could not construct download_link_html, encountered error: \", e)\n\n        download_link_html += \"</div>\"\n\n    if matched_images_found:\n        image_gallery_id = f\"image_gallery_for_stream_{stream_session_id}\"\n        images_iframe_html = f'''\n        <h6>Browse a gallery of relevant images by clicking on the thumbnail below:</h6>\n        <i class=\"fas fa-images thumbnail-icon\" onclick=\"openImageGalleryModal('{image_gallery_id}')\"></i>\n        <div id=\"{image_gallery_id}\" class=\"image-gallery-modal\">\n        <span class=\"image-gallery-close\" onclick=\"closeImageGalleryModal('{image_gallery_id}')\">&times;</span>\n        <div class=\"image-gallery-content\">\n        '''\n        for image_id, image_bytes_data in matched_images_in_bytes:\n            #print(f\"\\n\\nmatched image id: {image_id}\")\n            try:\n                image_link_url = url_for('image_display', image_id=image_id)\n                images_iframe_html += f'<iframe src=\"{image_link_url}\" frameborder=\"0\" class=\"gallery-thumbnail\"></iframe>'\n            except Exception as e:\n                handle_error_no_return(\"Could not construct images_iframe_html, encountered error: \", e)\n        \n        images_iframe_html += f'</div></div>'\n\n    \n    # reference_response = refer_pages_string + download_link_html + images_iframe_html\n    reference_response = refer_pages_string + images_iframe_html\n\n    try:\n        # model_response_for_history_db = str(llm_response) + refer_pages_string\n        model_response_for_history_db = str(llm_response)\n        model_response_for_history_db += f\"\\n\\n{reference_response}\"\n        model_response_for_history_db += f\"\\n\\npdf_pane_data={download_link_html}\"\n        model_response_for_history_db = model_response_for_history_db.strip('\\n')\n\n        formatted_user_query = str(user_query).strip('\\n')\n\n        user_query_for_history_db = formatted_user_query\n    except Exception as e:\n        handle_error_no_return(\"Could not prep data to store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    try:\n        store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, formatted_user_prompt)\n    except Exception as e:\n        handle_error_no_return(\"Could not store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    return jsonify({'success': True, 'response': reference_response, 'pdf_frame':download_link_html})\n\n\nif __name__ == '__main__':\n    # app.run(debug=True)\n    app.run(host='0.0.0.0', port=5000)"}
{"type": "source_file", "path": "web_app/app.py", "content": "from flask import Flask, render_template, request, redirect, url_for, Response, stream_with_context\nfrom flask import send_from_directory\nfrom flask import jsonify\n\nfrom sentence_transformers import SentenceTransformer, util\n\nfrom pdfminer.high_level import extract_text\nfrom werkzeug.utils import secure_filename\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom googleapiclient.http import MediaIoBaseDownload, MediaDownloadProgress\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.document_loaders import TextLoader\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import ConversationChain\n\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom pdf2image import convert_from_path\nfrom PIL import Image\n\nimport fitz # PyMuPDF\nfrom rapidfuzz import process, fuzz\n\nfrom urllib.parse import unquote\nfrom threading import Thread\nimport subprocess\nimport threading\nimport traceback\nimport platform\nimport tempfile\nimport datetime\nimport requests\nimport logging\nimport sqlite3\nimport signal\nimport PyPDF2\nimport base64\nimport queue\nimport uuid\nimport json\nimport time\nimport nltk\nimport zlib\nimport ast\nimport sys\nimport os\nimport io\nimport re\n\nfrom logging.handlers import RotatingFileHandler\nfrom nltk.corpus import stopwords\n\n\n\napp = Flask(__name__)\n\n# Route for the home page, rendering the initial model selection form (legacy)\n@app.route('/')\ndef index():\n    return render_template('chat.html')\n\n# model_selection.html triggers window.location.href to '/chat', which triggers this route, which loads the chat.html template at the end!\n@app.route('/chat')\ndef chat():\n    return render_template('chat.html')\n\n# Route to display the file loading form\n@app.route('/load_file')\ndef load_file():\n    return render_template('model_selection.html', show_file_form=True)\n\n@app.route('/download/<filename>')\ndef download_file(filename):\n    # return send_from_directory(app.config['UPLOAD_FOLDER'], filename, as_attachment=True)\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename, as_attachment=False, mimetype='application/pdf')\n\n@app.route('/pdf/<filename>')\ndef pdf_viewer(filename):\n    return send_from_directory(app.config['DOWNLOAD_FOLDER'], filename)\n\n\n\n#########################------------------GLOBALS!----------------------###############################\nLLAMA_CPP_PROCESS = None\nHF_WAITRESS_PROCESS = None\nLLM = None\nCHAT_ID = None\nSEQUENCE_ID = None\nLOADED_UP = False\nLLM_LOADED_UP = False\nVECTORDB_LOADED_UP = False\nLLM_CHANGE_RELOAD_TRIGGER_SET = False\nVECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\nVECTOR_STORE = None\nHF_BGE_EMBEDDINGS = None\nAZURE_OPENAI_EMBEDDINGS = None\nHISTORY_MEMORY_WITH_BUFFER = None   #Init in load_model_and_vectordb(); reset in load_chat_history() when old chats loaded, and in load_model_and_vectordb() when 'New Chat' selected; used for non-RAG convChain init in stream, and for saving context in stream for RAG chains and lastly, for setting HISTORY_SUMMARY in stream() via load_memory_variables({})\nHISTORY_SUMMARY = {}    #Set in stream() via HISTORY_MEMORY_WITH_BUFFER.load_memory_variables({}), and in load_chat_history() from chat_history DB; cleared in load_model_and_vectordb() when 'New Chat' selected; used to init prompt templates in stream() and lastly, for storage to chat_history DB in stream() and get_references()\n\n# Dict for user queries:  queries[session_id] = user_input\nQUERIES = {}\n\n# If modifying these scopes, delete the file token.json.\nGDRIVE_SCOPES = [\"https://www.googleapis.com/auth/drive.metadata.readonly\", \"https://www.googleapis.com/auth/drive.readonly\"]\nGDRIVE_CREDS = None\n#########################------------------------------------------------###############################\n\n\n\n#########################------------Setup & Handle Logging-------------###############################\ntry:\n    # 1 - Create a logger\n    logger = logging.getLogger('my_logger')\n    logger.setLevel(logging.ERROR)\n\n    # 2 - Create a RotatingFileHandler\n    # maxBytes: max file size of log file after which a new file is created; set to 1024 * 1024 * 5 for 5MB: 1024x1024 is 1MB, then a multiplyer for the number of MB\n    # backupCount: number of backup files to keep specifying how many old log files to keep\n    handler = RotatingFileHandler('lars_server_log.log', maxBytes=1024*1024*5, backupCount=2)\n    handler.setLevel(logging.ERROR)\n\n    # 3 - Create a formatter and set it for the handler\n    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n    handler.setFormatter(formatter)\n\n    # 4 - Add the handler to the logger\n    logger.addHandler(handler)\n    # Logger ready! Usage: logger.error(f\"This is an error message with error {e}\")\nexcept Exception as e:\n    print(f\"\\n\\nCould not establish logger, encountered error: {e}\")\n\n\ndef handle_api_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    #full_message = f\"\\n\\n{error_message}\\n\\nTraceback: {traceback_details}\\n\\n\"\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    return jsonify(success=False, error=error_message), 500 #internal server error\n\n\ndef handle_local_error(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n    raise Exception(exception)\n\n\ndef handle_error_no_return(message, exception=None):\n    error_message = f\"{message} {str(exception) if exception else '; No exception info.'}\".strip()\n    #traceback_details = traceback.format_exc()\n    full_message = f\"\\n\\n{error_message}\\n\\n\"\n    if logger:\n        logger.error(full_message)\n        print(full_message)\n    else:\n        print(full_message)\n\n#########################-------------------------------------###############################\n\n\n\nif not os.path.exists('config.json'):\n    try:\n        with open('config.json', 'w') as file:\n            json.dump({}, file)\n    except Exception as e:\n        handle_error_no_return(\"Could not init config.json. Multiple app restarts may be required to get the app to init correctly. Printing error and proceeding: \", e)\n\n\n\n# Method to write to config.json | input- dict of key:values to be written to config.json\ndef write_config(config_updates, filename='config.json'):\n\n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        config = {}     #init emply config dict\n        handle_error_no_return(\"Could not read config.json when attempting to write, encountered error: \", e)\n        \n    restart_required = False\n    if LLM_LOADED_UP:\n        llm_trigger_keys_for_app_restart = ['use_local_llm', 'local_llm_server', 'use_azure_open_ai', 'use_gpu', 'model_choice', 'local_llm_chat_template_format', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'base_template']\n                \n        for key in llm_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global LLM_CHANGE_RELOAD_TRIGGER_SET\n                LLM_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n    \n    if VECTORDB_LOADED_UP:\n        vectordb_trigger_keys_for_app_restart = ['embedding_model_choice']\n\n        for key in vectordb_trigger_keys_for_app_restart:\n            if key in config_updates and config_updates[key] != config.get(key):\n                global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n                VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n                restart_required = True\n                break\n\n    config.update(config_updates)\n\n    # Write updated config.json:\n    try:\n        with open(filename, 'w') as file:\n            json.dump(config, file, indent=4)\n    except Exception as e:\n        handle_local_error(\"Could not update config.json, encountered error: \", e)\n     \n    return {'success': True, 'restart_required':restart_required}\n            \n\n# Method to read from config.json | input- list of keys to be read from config.json; output- dict of key:value pairs; MANAGE DEFAULTS HERE!\ndef read_config(keys, default_value=None, filename='config.json'):\n    \n    # Open config file to read-in all current params:\n    try:\n        with open(filename, 'r') as file:\n            config = json.load(file)\n    except Exception as e:\n        handle_error_no_return(\"Could not read config.json, encountered error: \", e)\n        return {key: default_value for key in keys}     #because a read scenario wherein config.json does not exist shouldn't occur!\n    \n    return_dict = {}\n    update_config_dict = {}\n    base_directory = config.get('base_directory', '/app/lars_storage')   # specifying default if not found\n\n    for key in keys:\n        if key in config:\n            return_dict[key] = config[key]\n        else:\n            default_value = {\n                'windows_base_directory':'C:/lars_storage',\n                'unix_and_docker_base_directory':'/app/lars_storage',\n                'mac_base_directory':'app',\n                'upload_folder':base_directory + '/uploaded_pdfs',\n                'vectordb_sbert_folder':base_directory + '/chroma_db_sbert_embeddings',\n                'vectordb_openai_folder':base_directory + '/chroma_db_openai_embeddings',\n                'vectordb_bge_large_folder':base_directory + '/chroma_db_bge_large_embeddings',\n                'vectordb_bge_base_folder':base_directory + '/chroma_db_bge_base_embeddings',\n                'sqlite_images_db':base_directory + '/images_database_main.db',\n                'sqlite_history_db':base_directory + '/chat_history.db',\n                'sqlite_docs_loaded_db':base_directory + '/docs_loaded.db',\n                'model_dir':base_directory + '/models',\n                'highlighted_docs':base_directory + '/highlighted_pdfs',\n                'ocr_pdfs':base_directory + '/ocr_pdfs',\n                'pdfs_to_txts':base_directory + '/pdfs_to_txts',\n                'local_llm_server':'hf-waitress',\n                'model_choice':'Meta-Llama-3-8B-Instruct.f16.gguf',\n                'do_rag':True,\n                'force_enable_rag':False,\n                'force_disable_rag':False,\n                'use_local_llm':True,\n                'use_gpu':True,\n                'use_gpu_for_embeddings':False,\n                'azure_cv_free_tier':True,\n                'use_azure_open_ai':False,\n                'use_openai_embeddings':False,\n                'azure_openai_api_type':'azure',\n                'azure_openai_api_version':'2023-05-15',\n                'azure_openai_max_tokens':4096,\n                'azure_openai_temperature':0.7,\n                'use_bge_large_embeddings':False,\n                'use_bge_base_embeddings':False,\n                'use_sbert_embeddings':True,\n                'embedding_model_choice':'sbert_mpnet_base_v2',\n                'use_ocr':False,\n                'ocr_service_choice':'None',\n                'local_llm_model_type':'llama',\n                'local_llm_chat_template_format':'llama3',\n                'local_llm_context_length':8192,\n                'local_llm_max_new_tokens':2048,\n                'local_llm_gpu_layers':47,\n                'local_llm_temperature':0.8,\n                'local_llm_top_k':40,\n                'local_llm_top_p':0.95,\n                'local_llm_min_p':0.05,\n                'local_llm_n_keep':0,\n                'server_timeout_seconds':10,\n                'server_retry_attempts':3,\n                'base_template':\"Answer the user's question in as much detail as possible. Be as accurate as possible. Do not make up answers or fabricate false information! Whenever additional context is provided, mention the document names and page numbers the user should reference as per your best judgement.\",\n            }.get(key, 'undefined')\n\n            if default_value == 'undefined':\n                raise KeyError(f\"Key \\'{key}\\' not found in config.json and no default value has been defined either.\\n\")\n            \n            return_dict[key] = default_value\n            update_config_dict[key] = default_value\n\n    if update_config_dict:\n        # Write Defaults\n        try:\n            write_config(update_config_dict)\n        except Exception as e:\n            handle_error_no_return(\"Could not write defaults to config.json. Encountered error: \", e)\n    \n    ##print(f\"return_dict: {return_dict}\")\n\n    return return_dict\n\n\n# Method for API route to read from config.json\n# Deviates from typical RESTful principals to use a POST call to fetch values but practical & justifyable because we:\n# 1. Do not want to make the URL huge with a ever-growing list of query-params 2. Do not wish to expose values via query-params\n@app.route('/config_reader_api', methods=['POST'])\ndef config_reader_api():\n    # keys = request.args.getlist('keys') # Assuming keys are passed as query parameters\n    \n    try:\n        keys = request.json.get('keys', []) # Could also do keys = request.json['keys'] but this way we can provide a default list should 'keys' be missing!\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not read keys for config_reader_api request. Encountered error:\", e)\n\n    try:\n        values = read_config(keys)  # send list of keys, get dict of key:values\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not read keys from config.json. Encountered error: \", e)\n    \n    return jsonify(success=True, values=values)\n\n\n# Method for API route to write to config.json\n@app.route('/config_writer_api', methods=['POST'])\ndef config_writer_api():\n\n    try:\n        config_updates = request.json['config_updates']\n        print(f\"config_updates for config_writer_api: {config_updates}\")\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not read values for config_writer_api request. Encountered error: \", e)\n    \n    try:\n        write_return = write_config(config_updates)\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not write keys to config.json. Encountered error: \", e)\n    \n    return jsonify({\"success\": write_return['success'], \"restart_required\": write_return['restart_required']})\n\n\n\n#########################------------Setup Directories-------------###############################\nBASE_DIRECTORY = \"\"\n\nif platform.system() == 'Windows':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    from azure.core.exceptions import HttpResponseError\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = 'C:/lars_storage'\n    try:\n        read_return = read_config(['windows_base_directory'])   #passing list of values to read\n        BASE_DIRECTORY = str(read_return['windows_base_directory']) #received dict of key:values\n    except Exception as e:\n        handle_local_error(\"Could not read windows_base_directory on boot, encountered error: \", e)\n\nelif platform.system() == 'Linux':\n    from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n    from msrest.authentication import CognitiveServicesCredentials\n    from azure.ai.formrecognizer import DocumentAnalysisClient\n    from azure.core.credentials import AzureKeyCredential\n    from azure.core.exceptions import HttpResponseError\n    import azure.ai.vision as sdk\n    \n    #BASE_DIRECTORY = '/app/lars_storage'\n    try:\n        read_return = read_config(['unix_and_docker_base_directory'])\n        BASE_DIRECTORY = str(read_return['unix_and_docker_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read unix_and_docker_base_directory on boot, encountered error: \", e)\n\nelse:   #Likely 'Darwin' and hence MacOS\n    #BASE_DIRECTORY = 'app'\n    try:\n        read_return = read_config(['mac_base_directory'])\n        BASE_DIRECTORY = str(read_return['mac_base_directory'])\n    except Exception as e:\n        handle_local_error(\"Could not read mac_base_directory on boot, encountered error: \", e)\n\ntry:\n    write_config({'base_directory':BASE_DIRECTORY})\nexcept Exception as e:\n    handle_local_error(\"Could not write OS BASE_DIRECTORY on boot, encountered error: \", e)\n\n\n###---Notes on the above workflow:---###\n# 1. Everytime the app runs, the OS platform is detected\n# 2. Following which the apporpriate base directory is requested as above\n# 3. If this is the very first run:\n#   a. read_config does not find the directory data in config.json\n#   b. the else clause is triggered and defaults set for both, write_config and return\n# 4. If this isn't the very first run:\n#   a. read_config simply returns the OS specific directory - this allows the user to update the directory via config.json!\n# 4. On return, BASE_DIRECTORY is set and write_config has os specific directories set (windows_base_directory, unix_and_docker_base_directory, and mac_base_directory)\n# 5. write_config is invoked for BASE_DIRECTORY\n# 6. write_config detects a write-attempt for BASE_DIRECTORY and updates all related app directories too, which can be subsequently read as required\n# 7. This ensures that directories are set correctly at each run while also allowing the user to set their preferred directory via config.json\n\n\n# Having set the values for the directories above, proceed to actually create them on disk IF they don't alread exist!\nif not os.path.exists(BASE_DIRECTORY):\n\n    # Create a directory for app storage \n    try:\n        os.mkdir(BASE_DIRECTORY)\n    except Exception as e:\n        handle_local_error(\"Failed to create Base App Directory, encountered error: \", e)\n        \ntry:\n    read_return = read_config(['model_dir', 'highlighted_docs', 'upload_folder', 'ocr_pdfs', 'pdfs_to_txts'])\n    model_dir = read_return['model_dir']\n    highlighted_docs = read_return['highlighted_docs']\n    upload_folder = read_return['upload_folder']\n    ocr_pdfs = read_return['ocr_pdfs']\n    pdfs_to_txts = read_return['pdfs_to_txts']\nexcept Exception as e:\n    handle_local_error(\"Could not read paths for app directories (model_dir, highlighted_docs, upload_folder) from config.json on boot, encountered error: \", e)\n\n\n# If the base directory does not currently exist...\nif not os.path.exists(model_dir):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(model_dir)\n    except Exception as e:\n        handle_local_error(\"Failed to create Model Directory (model_dir), encountered error: \", e)\n\n\n# If the highlighted_docs directory does not currently exist...\nif not os.path.exists(highlighted_docs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(highlighted_docs)\n    except Exception as e:\n        handle_local_error(\"Failed to create Highlighted Docs Directory (highlighted_docs), encountered error: \", e)\n\n\n# If the upload_folder directory does not currently exist...\nif not os.path.exists(upload_folder):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(upload_folder)\n    except Exception as e:\n        handle_local_error(\"Failed to create Uploaded Docs Directory (upload_folder), encountered error: \", e)\n        \n\n# If the ocr_pdfs directory does not currently exist...\nif not os.path.exists(ocr_pdfs):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(ocr_pdfs)\n    except Exception as e:\n        handle_local_error(\"Failed to create OCR'ed Docs Directory (ocr_pdfs), encountered error: \", e)\n\n\n# If the pdfs_to_txts directory does not currently exist...\nif not os.path.exists(pdfs_to_txts):\n\n    # Create a directory for app storage\n    try:\n        os.mkdir(pdfs_to_txts)\n    except Exception as e:\n        handle_local_error(\"Failed to create txt-docs Directory (pdfs_to_txts), encountered error: \", e)\n\n\napp.config['UPLOAD_FOLDER'] = upload_folder\napp.config['DOWNLOAD_FOLDER'] = highlighted_docs\n\n\ndef clean_text_string(text_to_be_cleaned):\n    \n    # Clean text\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"\", \"\").replace(\"\", \"\").replace(\"\", \"\")\n    # text_to_be_cleaned = text_to_be_cleaned.replace(\"Confidential Copy \\n            for \\n         DKPPU\", \"\")\n    #clean_text = re.sub(r'\\n(?=[a-z.])', ' ', text)     # replaces newline chars immediately followed by a small-letter or dot with a space as they're likely to be the same sentence split-up across lines.\n    clean_text = re.sub(r'\\n+', '\\n', text_to_be_cleaned)\n\n    # This regex substitutes anything that is not a word character or whitespace with an empty string.\n    clean_text = re.sub(r'[^\\w\\s]', ' ', clean_text)\n\n    # This regex substitutes any sequence of whitespace characters with a single space.\n    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n\n    return clean_text\n\n\ndef PDFtoAzureDocAiTXT(input_filepath):\n\n    print(\"\\n\\nProcessing Document - PDF to Azure DocAI TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_doc_ai_endpoint', 'azure_doc_ai_subscription_key', 'ocr_pdfs'])\n        azure_doc_ai_endpoint = read_return['azure_doc_ai_endpoint']\n        azure_doc_ai_subscription_key = read_return['azure_doc_ai_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureDocAiTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"Azure-OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        docai_client = DocumentAnalysisClient(azure_doc_ai_endpoint, AzureKeyCredential(azure_doc_ai_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure DocAI, encountered error: \", e)\n\n    try:\n        with open(input_filepath, \"rb\") as pdf_file:\n            # 1 - Get page count:\n            try:\n                pypdf_reader = PyPDF2.PdfReader(pdf_file)\n                page_count = len(pypdf_reader.pages)\n                page_range = f\"1-{page_count}\" if page_count > 1 else \"1\"\n                print(f\"page_range: {page_range}\")\n            except Exception as e:\n                handle_local_error(\"Could not get page count for call to Azure DocAI, encountered error: \", e)\n\n            # 2 - Reset file-read stream's internal pointer, which has now been set to the end of the file due to the above read operation!\n            pdf_file.seek(0)\n\n            # 3 - Call Azure DocAI:\n            try:\n                poller = docai_client.begin_analyze_document(\"prebuilt-layout\", pdf_file, pages=page_range)\n                result = poller.result()\n            except Exception as e:\n                handle_local_error(\"Could not get results for begin_analyze_document for Azure DocAI, encountered error: \", e)\n\n        # print(f\"result: \\n{result}\")\n\n        used_regions = set()   # set will avoid duplicates\n\n        if hasattr(result, 'tables'):\n            for table in result.tables:\n                #print(\"Found table\")\n                if table.cells:     # Check if there are cells in the table \n                    for cell in table.cells:\n                        #print(f\"Row {cell.row_index}, Column {cell.column_index}, Text: {cell.content}\")\n                        cell_text = f'Row {cell.row_index}, Column {cell.column_index}: {cell.content}'\n\n                        # Get page number\n                        page_number = \"\"\n                        if cell.bounding_regions:   # Check if there are bounding regions\n                            for region in cell.bounding_regions:\n                                page_number = region.page_number\n                                cell_polygon = region.polygon\n                                cell_polygon_tuple = tuple((point.x, point.y) for point in cell_polygon)    # lists aren't hashable to cast to a tuple\n                                used_regions.add(cell_polygon_tuple)\n\n                        try:\n                            output_text_file.write(f\"[PAGE:{page_number}]\\n{cell_text}\\n\")\n                        except Exception as e:\n                            handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n        # Get paragraphs\n        if hasattr(result, 'paragraphs'):\n            for paragraph in result.paragraphs:\n                para_page_number = paragraph.bounding_regions[0].page_number\n                para_polygon = paragraph.bounding_regions[0].polygon\n                para_polygon_tuple = tuple((point.x, point.y) for point in para_polygon)\n                \n                if para_polygon_tuple in used_regions:\n                    continue\n                \n                para_content = paragraph.content\n                #print(f\"\\n---Processing Page: {para_page_number}---\\n\")\n                #print(f\"paragraph: {para_content}\")\n\n                # write the extracted text to the file:\n                try:\n                    output_text_file.write(f\"[PAGE:{para_page_number}]\\n{para_content}\\n\")\n                    used_regions.add(para_polygon_tuple)\n                except Exception as e:\n                    handle_local_error(\"could not write to output text file, encountered error: \", e)\n\n    except Exception as e:\n        handle_local_error(\"Error processing document with azure DocAI: \", e)\n\n    # Close all files\n    output_text_file.close()\n\n    return output_text_file_path\n\n\ndef PDFtoAzureOCRTXT(input_filepath):\n    \n    print(\"\\n\\nProcessing Document - PDF to Azure OCR TXT\\n\\n\")\n    \n    try:\n        read_return = read_config(['azure_ocr_endpoint', 'azure_ocr_subscription_key', 'ocr_pdfs', 'azure_cv_free_tier'])\n        azure_ocr_endpoint = read_return['azure_ocr_endpoint']\n        azure_ocr_subscription_key = read_return['azure_ocr_subscription_key']\n        ocr_pdfs = read_return['ocr_pdfs']\n        azure_cv_free_tier = read_return['azure_cv_free_tier']\n    except Exception as e:\n        handle_local_error(\"Missing Azure OCR Endpoint URL & Subscription Key for PDFtoAzureOCRTXT, please provide required API config. Error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_filepath)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(ocr_pdfs, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"OCR'ed doc already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Convert PDF to  a list of images\n    try:\n        print(\"\\n\\nConverting PDF to a list of Images\\n\\n\")\n        pages = convert_from_path(input_filepath, 300) # 300dpi - good balance between quality and performance\n    except Exception as e:\n        handle_local_error(\"Could not image PDF file, encountered error: \", e)\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    try:\n        computervision_client = ComputerVisionClient(azure_ocr_endpoint, CognitiveServicesCredentials(azure_ocr_subscription_key))\n    except Exception as e:\n        handle_local_error(\"Could not create ComputerVisionClient for Azure OCR, encountered error: \", e)\n    \n    calls_made = 0\n\n    # Iterate over each page and apply OCR:\n    print(\"\\n\\nBeginning image to Text OCR\\n\\n\")\n    for page_number, image in enumerate(pages, start = 1):\n        \n        # Convert to bytes and create a stream\n        try:\n            img_stream = io.BytesIO()\n            image.save(img_stream, format='PNG')\n            img_stream.seek(0)  # Reset the stream position to the beginning\n        except Exception as e:\n            handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n            continue\n\n        # Send to Azure OCR\n        try:\n            if azure_cv_free_tier:\n                if calls_made < 20:\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made += 1\n                else:\n                    print(\"Sleeping for 60secs due to AzureOCR free-tier restrictions!\")\n                    time.sleep(63)  #free tier restrictions!\n                    print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                    result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                    #analyze_result = computervision_client.begin_analyze_document(\"prebuilt-layout\", img_stream).result()\n                    calls_made = 1  #reset counter\n            else:\n                print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n        except HttpResponseError as e:\n            print(f\"\\n\\nHttpResponseError e: {e}\\n\\n\")\n            if e.status_code == 429:\n                print(\"Exceeded free-tier usage limits, waiting for one-minute and retrying\")\n                time.sleep(63)  #free tier restrictions!\n                img_stream.seek(0)\n                print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                calls_made = 1  #reset counter\n        except Exception as e:\n            if \"(429)\" in str(e):\n                print(\"Exceeded free-tier usage limits, waiting for one-minute and retrying\")\n                time.sleep(63)  #free tier restrictions!\n                img_stream.seek(0)\n                print(f\"Submitting page {page_number} to AzureComputerVision for OCR\")\n                result = computervision_client.recognize_printed_text_in_stream(image=img_stream)\n                calls_made = 1  #reset counter\\\n            else:\n                handle_local_error(\"Could not convert image to Byte Stream for Azure OCR, encountered error: \", e)\n\n        for region in result.regions:\n            for line in region.lines:\n                #print(\" \".join([word.text for word in line.words]))\n\n                try:\n                    clean_text = str(\" \".join([word.text for word in line.words]))\n                except Exception as e:\n                    handle_error_no_return(\"Could not obtain line from Azure OCR result, encountered error: \", e)\n                    continue\n\n                # Write the extracted text to the file:\n                try:\n                    output_text_file.write(f\"[PAGE:{page_number}]\\n{clean_text}\\n\")\n                except Exception as e:\n                    handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n    # Close all files\n    output_text_file.close()\n\n    return output_text_file_path\n\n\ndef PDFtoTXT(input_file):\n\n    print(\"\\n\\nProcessing Document - PDF to TXT\\n\\n\")\n\n    try:\n        read_return = read_config(['pdfs_to_txts'])\n        pdfs_to_txts = read_return['pdfs_to_txts']\n    except Exception as e:\n        handle_local_error(\"Missing pdfs_to_txts directory for PDFtoTXT in config.json, encountered error: \", e)\n    \n    # Initialize PDF file reader\n    try:\n        pdf_file = open(input_file, 'rb')\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    try:\n        source_filename = os.path.basename(input_file)\n    except Exception as e:\n        handle_local_error(\"Could not open PDF file, encountered error: \", e)\n\n    # Initialize PDF reader\n    try:\n        pdf_reader = PyPDF2.PdfReader(pdf_file)\n    except Exception as e:\n        handle_local_error(\"Could not initialize PDF reader, encountered error: \", e)\n\n    # Set output path\n    output_text_file_name = source_filename.replace(\".pdf\",\".txt\")\n    output_text_file_path = os.path.join(pdfs_to_txts, output_text_file_name).replace(\"\\\\\",\"/\")\n\n    if os.path.exists(output_text_file_path):\n        print(\"PyPDF2-extracted .txt already exists! Returning existing file.\")\n        return output_text_file_path\n\n    # Initialize text output\n    try:\n        output_text_file = open(output_text_file_path, 'w', encoding='utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not initialize/access output text file, encountered error: \", e)\n\n    # Loop through all the pages and extract text\n    for page_num in range(len(pdf_reader.pages)):\n        \n        try:\n            page = pdf_reader.pages[page_num]\n            text = page.extract_text()\n        except Exception as e:\n            handle_error_no_return(\"Could not extract text from page, encountered error: \", e)\n\n        #clean_text = text\n        # Clean text\n        clean_text = clean_text_string(text)\n        page_number = int(page_num) + 1\n        \n        # Optionally, you can include page numbers in the text file\n        # output_text_file.write(f'\\n\\n--- Page {page_num + 1} ---\\n\\n')\n        \n        # Write the extracted text to the file\n        try:\n            output_text_file.write(f\"[PAGE:{page_number}]\\n{clean_text}\\n\")\n        except Exception as e:\n            handle_local_error(\"Could not write to output text file, encountered error: \", e)\n\n    # Close all files\n    pdf_file.close()\n    output_text_file.close()\n\n    return output_text_file_path\n\n\ndef extract_images_from_pdf(pdf_path):\n    \n    print(\"Extracting Images from PDF\")\n\n    try:\n        source_filename = os.path.basename(pdf_path)\n    except Exception as e:\n        handle_local_error(\"Could not extract filename, encountered error: \", e)\n    \n    with open(pdf_path, 'rb') as file:\n        \n        try:\n            pdf_reader = PyPDF2.PdfReader(file)\n        except Exception as e:\n            handle_local_error(\"Could not read PDF, encountered error: \", e)\n\n        images = []\n\n        for page_num in range(len(pdf_reader.pages)):\n            page = pdf_reader.pages[page_num]\n\n            text = page.extract_text().strip()\n            if not text:\n                print(\"Scanned page, skipping\")\n                continue\n\n            if '/XObject' in page['/Resources']:\n                xObject = page['/Resources']['/XObject'].get_object()\n                for obj in xObject:\n                    if xObject[obj]['/Subtype'] == '/Image':\n\n                        # Log details about the image object:\n                        try:\n                            image_obj = xObject[obj]\n                            obj_details = {\n                                'Object Reference': obj,\n                                'Width': image_obj.get('/Width', 'Unknown'),\n                                'Height': image_obj.get('/Height', 'Unknown'),\n                                'Color Space': image_obj.get('/ColorSpace', 'Unkown'),\n                                'Filter': image_obj.get('/Filter', 'Unknown'),\n                                'Bits Per Component': image_obj.get('/BitsPerComponent', 'Unknown')\n                            }\n                            #print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                            # data  = image_obj._data\n\n                            if obj_details['Filter'] == '/FlateDecode':\n                                #print(\"\\n\\nDecoding PNG!\\n\\n\")\n                                try:\n                                    data  = image_obj._data\n                                    decompressed_data = zlib.decompress(data)\n                                except Exception as e:\n                                    error_message = f\"\\n\\nPNG decompression exception: {e}\\n\\n\"\n                                    if logger:\n                                        logger.error(error_message)\n                                        print(error_message)\n                                    else:\n                                        print(error_message)\n                            else:\n                                decompressed_data  = image_obj._data\n\n                            text = page.extract_text()  \n                            # clean_text = text\n\n                            # Clean text\n                            clean_text = clean_text_string(text)\n\n                            try:\n                                if obj_details['Filter'] == '/FlateDecode':\n                                    # Determine Color Space:\n                                    color_space = image_obj.get('/ColorSpace')\n\n                                    if color_space == '/DeviceRGB':\n                                        mode = 'RGB'\n                                    elif color_space == '/DeviceCMYK':\n                                        mode = 'CMYK'\n                                    elif color_space == '/DeviceGray':\n                                        mode = 'L'\n                                    else:\n                                        mode = 'L'  # Default to grayscale if unsure\n\n                                    # Create image from bytes\n                                    image = Image.frombytes(mode, ((obj_details['Width']), (obj_details['Height'])), decompressed_data) # 'L' for 8-bit pixels, black and white\n                                    with io.BytesIO() as output:\n                                        image.save(output, format='JPEG')\n                                        binary_data = output.getvalue()\n                                        format = \"JPEG\"\n                                        images.append((binary_data,clean_text,format))\n\n                                else:\n                                    # Load image from bytes\n                                    image = Image.open(io.BytesIO(decompressed_data))\n\n                                    # Determine format (JPEG)\n                                    format = image.format\n                                    \n                                    #print(f\"\\n\\nImage format: {format}\\n\\n\")  # This will print the format\n\n                                    # If image loads, append image to images DB\n                                    images.append((decompressed_data,clean_text,format))\n\n                            except Exception as e:\n                                error_message = f\"\\n\\nEncountered unrecognized or invalid image data for object detailed below. Exception: {e}\\n\\n\"\n                                if logger:\n                                    logger.error(error_message)\n                                    logger.error(obj_details)\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n                                else:\n                                    print(error_message)\n                                    print(f\"\\n\\nImage Object Details: {obj_details}\\n\\n\")   # Filter is indicative of format: '/DCTDecode': 'JPEG', '/FlateDecode': 'PNG or others','/JPXDecode': 'JPEG 2000', etc.\n\n\n                        except Exception as e:\n                            handle_error_no_return(\"Could not process image object. Exception: \", e)\n\n        # print(\"Images array:\")\n        # print(images)\n        return images\n\n\ndef store_images_to_db(images):\n\n    print(\"\\n\\nStoring Images to Database\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to Images DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS images (\n                    id INTEGER PRIMARY KEY,\n                    image_data BLOB NOT NULL,\n                    surrounding_text TEXT,\n                    metadata TEXT,\n                    format TEXT\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create Images DB, encountered error: \", e)\n    \n    try:\n        for image_data, surrounding_text, format in images:\n            #print(\"surrounding_text: \", surrounding_text)\n            # Check if the image_data already exists in the database:\n            cursor.execute(\"SELECT COUNT(*) FROM images WHERE image_data = ?\", (image_data,))\n            if cursor.fetchone()[0] == 0:\n                print(\"\\nInserting new image into images DB\\n\")\n                cursor.execute(\"INSERT INTO images (image_data, surrounding_text, format) VALUES (?, ?, ?)\", (image_data, surrounding_text, format))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not store images to Images DB, encountered error: \", e)\n    finally:\n        conn.close()\n\n\ndef record_doc_loaded_to_db(document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap):\n\n    print(\"\\n\\nRecording document loading to records DB\\n\\n\")\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_docs_loaded_db in config.json for method store_images_to_db. Error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to document_records DB, encountered error: \", e)\n    \n    # If the database does not currently exist...\n    try:\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS document_records (\n                    id INTEGER PRIMARY KEY,\n                    document_name TEXT NOT NULL,\n                    embedding_model TEXT NOT NULL,\n                    vectordb_used TEXT,\n                    chunk_size INTEGER,\n                    chunk_overlap INTEGER\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create document_records DB, encountered error: \", e)\n    \n    try:\n        cursor.execute(\"INSERT INTO document_records (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap) VALUES (?, ?, ?, ?, ?)\", (document_name, embedding_model, vectordb_used, chunk_size, chunk_overlap))\n        conn.commit()\n        conn.close()\n    except Exception as e:\n        handle_local_error(\"Could not update document_records DB, encountered error: \", e)\n\n\n\n# List-splitter function for a large number of embeddings!\ndef split_embeddings_list(all_splits, max_emmbeddings_list_size):\n    for i in range(0, len(all_splits), max_emmbeddings_list_size):  # Step through the large list in steps of max size\n        yield all_splits[i:i + max_emmbeddings_list_size]   # Yield a slice of all_splits from index i upto but NOT including i+max_size \n\n\nclass Document:\n    def __init__(self, page_content, metadata):\n        self.page_content = page_content\n        self.metadata = metadata\n\n    def __repr__(self): #to provide string-representation of an object\n        return f\"Document(page_content='{self.page_content[:50]}...', metadata={self.metadata})\"\n\n\n\ndef chunk_docs_with_page_numbers(input_file, chunk_size=250):\n    documents = []\n    current_chunk = \"\"\n    current_page = 1\n\n    def add_chunk(chunk, page):\n        if chunk:\n            documents.append(Document(\n                page_content=chunk.strip(),\n                metadata={'source': input_file, 'page_number': page}\n            ))\n\n    try:\n        with open(input_file, 'r', encoding='utf-8') as file:\n            for line in file:\n                if line.startswith('[PAGE:'):\n                    new_page = int(line.strip()[6:-1])\n                    if new_page != current_page:\n                        add_chunk(current_chunk, current_page)\n                        current_chunk = \"\"\n                        current_page = new_page\n                    continue\n\n                if len(current_chunk) + len(line) > chunk_size:\n                    add_chunk(current_chunk, current_page)\n                    current_chunk = line\n                else:\n                    current_chunk += line\n\n                if len(current_chunk) >= chunk_size:\n                    add_chunk(current_chunk, current_page)\n                    current_chunk = \"\"\n        \n        # Add any remaining content\n        add_chunk(current_chunk, current_page)\n\n    except Exception as e:\n        handle_local_error(\"Could not chunk document, encountered error: \", e)\n\n    return documents\n\n\n# Document vectorization and chunking\ndef LoadNewDocument(input_file):\n\n    global VECTOR_STORE\n    \n    ### L1 - Load Data from Source ###\n    print(\"\\nLoading Document\")\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        handle_local_error(\"Missing values in config.json, could not LoadNewDocument. Error: \", e)\n\n    chunk_sz = 250\n    chunk_olp = 0\n\n    ### L2 - Chunk Source Data ###\n    print(\"Chunking Doc\")\n    try:\n        numbered_splits = chunk_docs_with_page_numbers(input_file, chunk_sz)\n\n        # print(f\"\\n\\nnumbered_splits sample: {numbered_splits[:3]}\\n\\n\")\n        # print(f\"\\n\\nnumbered_splits type: {type(numbered_splits[3])}\\n\\n\")\n        \n    except Exception as e:\n        handle_local_error(\"Failed to chunk document for storage to VectorDB, encountered error: \", e)\n\n    ### L3 - Store Chunks in VectorDB ###\n    print(\"Storing to VectorDB: ChromaDB\")\n    try:\n        # Return VectorStore initialized from documents and embeddings.\n        if use_sbert_embeddings:\n            # Ideally should use MAX_BATCH_SIZE obtained elsewhere \n            if len(numbered_splits) > 5000:\n                split_docs = split_embeddings_list(numbered_splits, 5000)\n                for split_docs_list in split_docs:\n                    VECTOR_STORE = Chroma.from_documents(documents=split_docs_list, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n            else:\n                VECTOR_STORE = Chroma.from_documents(documents=numbered_splits, embedding=HuggingFaceEmbeddings(), persist_directory=vectordb_sbert_folder)\n        \n        elif use_openai_embeddings:\n            print(\"Using OpenAI Text Ada Model via Azure OpenAI\")\n\n            list_position = 0\n            token_count = 0\n\n            for i in range(list_position, len(numbered_splits)):\n\n                token_count += len(str(numbered_splits[i]))\n                if token_count >= 108000:\n                    VECTOR_STORE = Chroma.from_documents(documents=numbered_splits[list_position:i+1], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder)  #AZURE_OPENAI_EMBEDDINGS defined on line 407\n                    list_position = i+1\n                    token_count = 0\n                    print(\"Loaded batch, sleeping for one minute to stay within rate-limit\")\n                    time.sleep(63)\n                    continue\n\n            # post-loop, if any splits are left to be processed but were missed due to token_count not reaching the limit:\n            if list_position < len(numbered_splits):\n                VECTOR_STORE = Chroma.from_documents(documents=numbered_splits[list_position:], embedding=AZURE_OPENAI_EMBEDDINGS, persist_directory=vectordb_openai_folder) #AZURE_OPENAI_EMBEDDINGS defined on line 407\n\n        elif use_bge_base_embeddings or use_bge_large_embeddings:\n            persist_directory = \"\"\n            if use_bge_base_embeddings:\n                persist_directory = vectordb_bge_base_folder\n            elif use_bge_large_embeddings:\n                persist_directory = vectordb_bge_large_folder\n            VECTOR_STORE = Chroma.from_documents(documents=numbered_splits, embedding=HF_BGE_EMBEDDINGS, persist_directory=persist_directory)    #HF_BGE_EMBEDDINGS defined in process_model() line 2133\n\n    except Exception as e:\n        handle_local_error(\"Could not store to VectorDB, encountered error: \", e)\n\n    return chunk_sz, chunk_olp\n\n\ndef find_images_in_db(reference_pages):\n\n    print(\"\\nSearching for relevant Images\\n\")\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_images_db in config.json for method find_images_in_db. Error: \", e)\n\n    matched_images = []\n    matched_images_found = False\n\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n        conn.row_factory = sqlite3.Row\n        print(\"Database connected for image search\")\n    except Exception as e:\n        handle_local_error(\"Could not connect to images DB for image search, encountered error: \", e)\n\n    for file_path, content in reference_pages.items():\n\n        for item in content:\n            # Each item in the list has two elements\n            page_text, page_number = item\n\n            # Only search for non-empty search strings\n            if page_text:\n                try:\n                    images = conn.execute('SELECT DISTINCT id, image_data FROM images WHERE surrounding_text LIKE ?', ('%' + page_text[:50] + '%',)).fetchall()\n                except Exception as e:\n                    handle_error_no_return(\"Could not select images from Images DB, encountered error: \", e)\n                for row in images:\n                    print(\"Matching image found!\")\n                    matched_images_found = True\n                    image_id = row['id']\n                    image_data = row['image_data']\n                    matched_images.append((image_id, image_data))\n        \n    conn.close()\n    matched_images = set(matched_images)\n    return matched_images_found, matched_images\n\n\ndef highlight_text_on_page(highlight_list, stream_session_id):\n\n    print(\"\\nHighlighting Document\\n\")\n    threshold = 80\n\n    try:\n        read_return = read_config(['upload_folder', 'highlighted_docs'])\n        upload_folder = read_return['upload_folder']\n        highlighted_pdfs_path = read_return['highlighted_docs']\n    except Exception as e:\n        handle_local_error(\"Missing upload_folder in config.json for method highlight_text_on_page. Error: \", e)\n    \n    for index, doc in enumerate(highlight_list, start=1):\n\n        try:\n            pdf_path = os.path.join(upload_folder, doc).replace(\"\\\\\",\"/\")\n            output_file_extension = \"_\" + stream_session_id + '.pdf'\n            output_file_name = doc.replace(\".pdf\",output_file_extension) \n            output_pdf_path = os.path.join(highlighted_pdfs_path, output_file_name).replace(\"\\\\\",\"/\")\n            highlight_doc = fitz.open(pdf_path)\n        except Exception as e:\n            handle_error_no_return(\"Could not open doc for highlighting, encountered error: \", e)\n            continue\n        \n        for target in highlight_list[doc]:\n            try:\n                text_to_highlight = str(target[1])\n                text_to_highlight = re.sub(r'Row \\d+, Column \\d+: ', '', text_to_highlight)\n                page_number = int(target[0])\n                page = highlight_doc.load_page(page_number-1)\n                page_text = page.get_text(\"text\")\n\n                # Split the page text into overlapping phrases\n                words = page_text.split()\n                phrases = [' '.join(words[i:i+len(text_to_highlight.split())]) for i in range(len(words))]\n\n                # Find fuzzy matches\n                good_matches = []\n                for phrase in phrases:\n                    score = fuzz.partial_ratio(text_to_highlight.lower(), phrase.lower())\n                    if score >= threshold:\n                        good_matches.append(phrase)\n\n                for match in good_matches:\n                    if len(str(match)) > 3:\n                        text_instances = page.search_for(match)\n                        for inst in text_instances:\n                            try:\n                                #print(f\"HIGHLIGHTING inst {inst} in document {doc}\")\n                                page.add_highlight_annot(inst)\n                            except Exception as e:\n                                handle_error_no_return(\"Could not highlight text instance, encountered error: \", e)\n                                continue\n\n            except Exception as e:\n                handle_error_no_return(\"Error loading page or searching for text to highlight, encountered error: \", e)\n                continue\n            \n        try:\n            highlight_doc.save(output_pdf_path, garbage=0, deflate=False, clean=False)\n        except Exception as e:\n            handle_error_no_return(\"Could not save highlighted doc, encountered error: \", e)\n            continue\n\n    return True\n\n\ndef highlighter_interface(reference_pages, stream_session_id):\n\n    user_should_refer_pages_in_doc = {}\n    highlight_list = {}\n    docs_have_relevant_info = False\n\n    print(f\"\\n\\nreference_pages: {reference_pages}\\n\\n\")\n\n    for file_path, content in reference_pages.items():\n        source_filename = os.path.basename(file_path)\n        print(f\"\\nsource_filename basename: {source_filename}\\n\")\n        output_file_extension = \"_\" + stream_session_id + '.pdf'\n        output_file_name = source_filename.replace(\".pdf\",output_file_extension) \n        page_numbers = set()\n        highlight_strings = set()\n\n        for item in content:\n            # Each item in the list has two elements\n            page_text, page_number = item\n            page_numbers.add(int(page_number))\n            highlight_strings.add((int(page_number), str(page_text[:50])))\n\n        if page_numbers:\n            user_should_refer_pages_in_doc[output_file_name] = page_numbers\n            docs_have_relevant_info = True\n\n        if highlight_strings:\n            highlight_list[source_filename] = list(highlight_strings)\n\n    if docs_have_relevant_info:\n        try:\n            highlight_text_on_page(highlight_list, str(stream_session_id))\n        except Exception as e:\n            handle_error_no_return(\"Could not highlight text, encountered error: \", e)\n\n    return docs_have_relevant_info, user_should_refer_pages_in_doc\n\n\ndef determine_sequence_id_for_chat(chat_id):\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        # Determine sequence_id\n        cursor.execute(\"SELECT COALESCE(MAX(sequence_id), 0) FROM chat_history WHERE chat_id = ?\", (int(chat_id),))\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for a new chat!\n        # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n        # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n        result = cursor.fetchone()\n        current_sequence_id = result[0]     # 'result' will be a list, so extract the first value\n        \n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n\n    return int(current_sequence_id)\n\n\ndef store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, current_prompt_template):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {chat_id}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(chat_id)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template) VALUES (?, ?, ?, ?, ?, ?)\", (int(chat_id), int(sequence_id), user_query_for_history_db, model_response_for_history_db, model_choice, str(current_prompt_template)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n\n\ndef store_chat_history_to_db(user_query_for_history_db, model_response_for_history_db, current_historical_summary):\n\n    global SEQUENCE_ID\n\n    print(f\"\\n\\nStoring chat history for chat with CHAT_ID: {CHAT_ID}\")\n\n    try:\n        read_return = read_config(['sqlite_history_db', 'model_choice', 'base_template'])\n        sqlite_history_db = read_return['sqlite_history_db']\n        model_choice = read_return['model_choice']\n        base_template = read_return['base_template']\n    except Exception as e:\n        handle_local_error(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_local_error(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        prev_sequence_id = determine_sequence_id_for_chat(CHAT_ID)\n        #print(\"prev_sequence_id: \", prev_sequence_id)\n        SEQUENCE_ID = prev_sequence_id + 1\n        #print(\"current_sequence_id: \", SEQUENCE_ID)\n    except Exception as e:\n        handle_local_error(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n       \n    # print(type(CHAT_ID))\n    # print(type(current_sequence_id))\n    # print(type(user_query_for_history_db))\n    # print(type(model_response_for_history_db))\n\n    try:\n        # Store conversation history into DB\n        cursor.execute(\"INSERT INTO chat_history (chat_id, sequence_id, user_query, llm_response, llm_model, prompt_template, history_summary) VALUES (?, ?, ?, ?, ?, ?, ?)\", (int(CHAT_ID), int(SEQUENCE_ID), user_query_for_history_db, model_response_for_history_db, model_choice, str(base_template), str(current_historical_summary)))\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not insert chat history into DB, encountered error: \", e)\n\n    conn.close()\n\n\n@app.route('/login_to_google_drive')\ndef login_to_google_drive():\n    global GDRIVE_CREDS\n    if os.path.exists(\"gdrive_token.json\"):\n        GDRIVE_CREDS = Credentials.from_authorized_user_file(\"gdrive_token.json\", GDRIVE_SCOPES)\n    if not GDRIVE_CREDS or not GDRIVE_CREDS.valid:\n        if GDRIVE_CREDS and GDRIVE_CREDS.expired and GDRIVE_CREDS.refresh_token:\n            GDRIVE_CREDS.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                \"gdrive_credentials.json\", GDRIVE_SCOPES\n            )\n            GDRIVE_CREDS = flow.run_local_server(port=0)\n            with open(\"gdrive_token.json\", \"w\") as token:\n                token.write(GDRIVE_CREDS.to_json())\n    return jsonify(success=True)\n\n\ndef categorize_mimetype(mimetype):\n    mimetype = mimetype.lower()\n\n    word_mimetypes = {\n        # Microsoft Word (modern formats)\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n        \"application/vnd.ms-word.document.macroEnabled.12\",\n        \"application/vnd.ms-word.template.macroEnabled.12\",\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.template\",\n\n        # Microsoft Word (legacy format)\n        \"application/msword\",\n\n        # Google Docs\n        \"application/vnd.google-apps.document\",\n\n        # OpenDocument Presentation\n        \"application/vnd.oasis.opendocument.text\",\n        \"application/vnd.oasis.opendocument.text-template\",\n        \"application/vnd.oasis.opendocument.text-web\"\n    }\n\n    excel_mimetypes = {\n        # Microsoft Excel (modern formats)\n        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n        \"application/vnd.ms-excel.sheet.macroEnabled.12\",\n        \"application/vnd.ms-excel.template.macroEnabled.12\",\n        \"application/vnd.openxmlformats-officedocument.spreadsheetml.template\",\n\n        # Microsoft Excel (legacy format)\n        \"application/vnd.ms-excel\",\n\n        # Google Sheets\n        \"application/vnd.google-apps.spreadsheet\",\n\n        # OpenDocument Presentation\n        \"application/vnd.oasis.opendocument.spreadsheet\",\n        \"application/vnd.oasis.opendocument.spreadsheet-template\",\n        \"text/csv\",\n        \"text/tab-separated-values\"\n    }\n\n    presentation_mimetypes = {\n        # Microsoft PowerPoint (modern formats)\n        \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",  # .pptx\n        \"application/vnd.ms-powerpoint.presentation.macroEnabled.12\",  # .pptm\n        \"application/vnd.openxmlformats-officedocument.presentationml.template\",  # .potx\n        \"application/vnd.ms-powerpoint.template.macroEnabled.12\",  # .potm\n        \"application/vnd.openxmlformats-officedocument.presentationml.slideshow\",  # .ppsx\n        \"application/vnd.ms-powerpoint.slideshow.macroEnabled.12\",  # .ppsm\n\n        # Microsoft PowerPoint (legacy format)\n        \"application/vnd.ms-powerpoint\",  # .ppt, .pot, .pps\n\n        # Google Slides\n        \"application/vnd.google-apps.presentation\",\n\n        # OpenDocument Presentation\n        \"application/vnd.oasis.opendocument.presentation\",  # .odp\n        \"application/vnd.oasis.opendocument.presentation-template\"  # .otp\n    }\n\n    pdf_mimetypes = {\n        \"application/pdf\",\n        \"application/x-pdf\",\n        \"application/acrobat\",\n        \"application/vnd.pdf\",\n        \"text/pdf\",\n        \"text/x-pdf\"\n    }\n\n    text_mimetypes = {\n        \"application/rtf\",\n        \"text/rtf\",\n        \"text/plain\"\n    }\n\n    image_mimetypes = {\n        \"image/jpeg\",\n        \"image/png\",\n        \"image/gif\",\n        \"image/bmp\",\n        \"image/webp\",\n        \"image/svg+xml\",\n        \"image/tiff\",\n        \"image/x-icon\",\n        \"image/vnd.microsoft.icon\",\n        \"image/heic\",\n        \"image/heif\"\n    }\n\n    video_mimetypes = {\n        \"video/mp4\",\n        \"video/mpeg\",\n        \"video/x-msvideo\",\n        \"video/quicktime\",\n        \"video/x-ms-wmv\",\n        \"video/x-flv\",\n        \"video/webm\",\n        \"video/3gpp\",\n        \"video/3gpp2\",\n        \"video/x-matroska\"\n    }\n\n    audio_mimetypes = {\n        \"audio/mpeg\",\n        \"audio/x-wav\",\n        \"audio/wav\",\n        \"audio/x-m4a\",\n        \"audio/aac\",\n        \"audio/ogg\",\n        \"audio/webm\",\n        \"audio/flac\",\n        \"audio/x-ms-wma\",\n        \"audio/x-aiff\"\n    }\n\n    folder_mimetypes = {\n        \"application/vnd.google-apps.folder\",  # Google Drive folder\n        \"application/x-directory\",             # Generic directory mime type\n        \"inode/directory\",                     # Often used in Unix-like systems\n        \"folder\",                              # Some systems might use this\n    }\n\n    # Word file\n    if mimetype in word_mimetypes:\n        return \"word\"\n    # Excel files\n    elif mimetype in excel_mimetypes:\n        return \"excel\"\n    # Presentation files\n    elif mimetype in presentation_mimetypes:\n        return \"presentation\"\n    # Text files\n    elif mimetype in text_mimetypes:\n        return \"text\"\n    # PDFs\n    elif mimetype in pdf_mimetypes:\n        return \"pdf\"\n    # Images\n    elif mimetype in image_mimetypes:\n        return \"image\"\n    # Videos\n    elif mimetype in video_mimetypes:\n        return \"video\"\n    # Audio files\n    elif mimetype in audio_mimetypes:\n        return \"audio\"\n    # Folders\n    elif mimetype in folder_mimetypes:\n        return \"folder\"\n    # Other\n    else:\n        return \"other\"\n\n@app.route('/fetch_file_list_from_google_drive')\ndef fetch_file_list_from_google_drive():\n    gdrive_files = []\n    try:\n        service = build(\"drive\", \"v3\", credentials=GDRIVE_CREDS)\n\n        about_result = service.about().get(fields=\"storageQuota,user\").execute()\n        print(f\"about_result: {about_result}\")\n\n        results = (\n            service.files().list(\n                q=\"trashed=false\",\n                pageSize=1000,\n                fields=\"nextPageToken, files(id, name, mimeType, version)\"\n            ).execute()\n        )\n        \n        items = results.get(\"files\", [])\n        print(f\"len(items): {len(items)}\")\n\n        if not items:\n            print(\"No files found.\")\n            return jsonify(success=True, gdrive_files=gdrive_files)\n        else:\n            # print(\"\\n\\nFiles:\\n\\n\")\n            # print(\"Name       ID      mimeType        fileExtension       Category      version\")\n            for item in items:\n                category = categorize_mimetype(item['mimeType'])\n                #print(f\"{item['name']}      ({item['id']})      {item['mimeType']}      {category}        {item['version']}\")\n                gdrive_files.append({\n                    'name': item['name'],\n                    'id': item['id'],\n                    'mimeType': item['mimeType'],\n                    'version': item['version'],\n                    'type': category\n                })\n\n    except Exception as e:\n        return handle_api_error(\"Could not fetch GDrive files, encountered error: \", e)\n    \n    return jsonify({'success': True, 'gdrive_files': gdrive_files})\n\n\ndef download_folder(service, folder_id, path, indent=''):\n\n    print(f\"\\n\\nDownloading GoogleDrive Folder with id: {folder_id}\\n\\n\")\n\n    try:\n        if not os.path.exists(path):\n            os.makedirs(path)\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not create nested directory in the download_folder() method: \", e)\n\n    query = f\"'{folder_id}' in parents\"\n    fields = \"files(id, name, mimeType)\"\n    \n    gdrive_folder_contents = service.files().list(q=query, fields=fields).execute()\n    items = gdrive_folder_contents.get('files', [])\n\n    for item in items:\n        file_id = item['id']\n        filename = item['name']\n        mime_type = item['mimeType']\n        print(f\"folder item mime_type f{mime_type}\")\n\n        if \"folder\" in str(mime_type):\n            sub_folder_path = os.path.join(path, filename)    # in this case, filename will be the folder name\n\n            try:\n                download_folder(service, file_id, sub_folder_path)  # in this case, file_or_folder_id will be the folder id\n            except Exception as e:\n                return handle_api_error(\"Could not download_folder in the download_folder() method, encountered error: \", e)\n        else:\n            try:\n                filename_with_extension, file_content = download_gdrive_file(service, file_id, filename, mime_type)\n            except Exception as e:\n                return handle_api_error(\"Server-side error - could not get_file_content in the download_folder() method: \", e)\n\n            try:\n                # filepath = os.path.join(path, secure_filename(filename_with_extension))\n                filepath = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(filename_with_extension))\n                print(f\"Saving {filename_with_extension} to {filepath}\")\n                with open(filepath, 'wb') as f:\n                    f.write(file_content)\n\n                try:\n                    vector_embed_filepath(filename_with_extension, filepath)\n                except Exception as e:\n                    return handle_api_error(\"Could not vector_embed_filepath() in the download_folder() method, encountered error: \", e)\n\n            except Exception as e:\n                return handle_api_error(\"Server-side error - could not save Google Drive file in the download_folder() method: \", e)\n\n    return True\n\n\ndef download_gdrive_file(service, file_id, filename, mime_type):\n\n    print(f\"\\n\\nDownloading GoogleDrive File with mime_type: {mime_type}\\n\\n\")\n\n    file_mime_category = categorize_mimetype(mime_type)\n\n    filename_with_extension = filename\n\n    try:\n        if file_mime_category in [\"word\", \"excel\", \"presentation\"]:\n            # Handle Google Docs Editors files\n            if 'google-apps' in mime_type:\n                if file_mime_category == \"word\":\n                    export_mime_type = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n                    file_extension = '.docx'\n                elif file_mime_category == \"excel\":\n                    export_mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n                    file_extension = '.xlsx'\n                elif file_mime_category == \"presentation\":\n                    export_mime_type = 'application/vnd.openxmlformats-officedocument.presentationml.presentation'\n                    file_extension = '.pptx'\n                \n                print(f\"Downloading google-apps file with mimeType {file_mime_category}\")\n                gdrive_request = service.files().export_media(fileId=file_id, mimeType=export_mime_type)\n                if not filename.endswith(file_extension):\n                    filename_with_extension += file_extension\n            \n            else:\n                # For non-Google formats, use get_media\n                print(f\"Downloading office file with non google-apps mimeType {file_mime_category}\")\n                gdrive_request = service.files().get_media(fileId=file_id)\n        else:\n            print(f\"Downloading non-office file with mimeType {file_mime_category}\")\n            gdrive_request = service.files().get_media(fileId=file_id)\n        \n        file = io.BytesIO()\n        downloader = MediaIoBaseDownload(file, gdrive_request)\n        \n        done = False\n        while done is False:\n            status, done = downloader.next_chunk()\n            print(f\"Download {int(status.progress() * 100)}%.\")\n    \n    except Exception as e:\n        return handle_api_error(\"Error downloading file from Google Drive in the download_gdrive_file() method: \", e)\n    \n    try:\n        file_content = file.getvalue()\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not getValue() for downloaded Google Drive file  in the download_gdrive_file() method: \", e)\n\n    return filename_with_extension, file_content\n\n\ndef gdrive_downloader(service, file_or_folder_id, filename, mime_type, path=app.config['UPLOAD_FOLDER']):\n    file_mime_category = categorize_mimetype(mime_type)\n\n    if file_mime_category == \"folder\":\n        download_path = os.path.join(path, secure_filename(filename))    # in this case, filename will be the folder name\n        download_folder(service, file_or_folder_id, download_path)\n        return filename, None    # Return None for file_content as it's a folder\n    else:\n        filename_with_extension, file_content = download_gdrive_file(service, file_or_folder_id, filename, mime_type)\n        return filename_with_extension, file_content\n\n\n@app.route('/google_drive_loader', methods=['POST'])\ndef google_drive_loader():\n\n    try:\n        gdrive_file_id = str(request.form['file_id'])\n        gdrive_file_mimeType = str(request.form['file_mimeType'])\n    except Exception as e:\n        return handle_api_error(\"Server-side error reading Google Drive file details for download in the google_drive_loader() method: \", e)\n    \n    try:\n        service = build(\"drive\", \"v3\", credentials=GDRIVE_CREDS)\n    except Exception as e:\n        return handle_api_error(\"Could not create Google service handler in the google_drive_loader() method, check credentials and re-try: \", e)\n\n    try:\n        file_metadata = service.files().get(fileId=gdrive_file_id, fields='name, mimeType').execute()\n        original_filename = file_metadata.get('name', 'untitled')\n        mime_type = file_metadata.get('mimeType', gdrive_file_mimeType)\n    except Exception as e:\n        return handle_api_error(\"Could not read GoogleDrive file metadata in the google_drive_loader() method, encountered error: \", e)\n    \n    try:\n        filename_with_extension, file_content = gdrive_downloader(service, gdrive_file_id, original_filename, mime_type)\n    except Exception as e:\n        return handle_api_error(\"Server-side error - could not getValue() for downloaded Google Drive file in the google_drive_loader() method: \", e)\n\n    if file_content is not None:\n        try:\n            filepath = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(filename_with_extension))\n\n            print(f\"Saving {filename_with_extension} to {filepath}\")\n            with open(filepath, 'wb') as f:\n                f.write(file_content)\n\n            try:\n                vector_embed_filepath(filename_with_extension, filepath)\n            except Exception as e:\n                return handle_api_error(\"Could not vector_embed_filepath() in the google_drive_loader() method, encountered error: \", e)\n\n        except Exception as e:\n            return handle_api_error(\"Server-side error - could not save file downloaded from Google Drive in the google_drive_loader() method: \", e)\n    \n    return jsonify({'success': True})\n\n\n# Route for loading all models from model dir\n@app.route('/load_local_models')\ndef load_local_models():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        return handle_api_error(\"Missing model_dir in config.json for method load_local_models. Error: \", e)\n    \n    try:\n        models = [f for f in os.listdir(model_dir) if os.path.isfile(os.path.join(model_dir, f))]\n    except Exception as e:\n        return handle_api_error(\"Could not load list of local models, encountered error: \", e)\n        \n    #print(f\"locally available models: {models}\")\n    return jsonify({'success': True, 'models': models})\n\n\n@app.route('/upload_new_llm', methods=['POST'])\ndef upload_new_llm():\n\n    try:\n        read_return = read_config(['model_dir'])\n        model_dir = read_return['model_dir']\n    except Exception as e:\n        return handle_api_error(\"Could not determine model_dir in upload_new_llm. Error: \", e)\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        return handle_api_error(\"Server-side error recieving LLM file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n\n    try:\n        filepath = os.path.join(model_dir, filename)\n\n        print(\"Loading new LLM - filename: \", filename)\n        print(\"Loading new LLM - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        return handle_api_error(\"Failed to save LLM to model_dir, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to handle the submission of the first form (LLM & embeddings model and GPU selection)\n@app.route('/process_model', methods=['POST'])\ndef process_model():\n    \n    global HF_BGE_EMBEDDINGS\n\n    ###---New config.json---###\n\n    config_update_dict = {}\n\n    use_azure_open_ai = 'use_azure' in request.form\n    use_openai_embeddings = 'use_openai_embeddings' in request.form\n    use_sbert_embeddings = 'use_sbert_embeddings' in request.form\n    use_bge_large_embeddings = 'use_bge_large_embeddings' in request.form\n    use_bge_base_embeddings = 'use_bge_base_embeddings' in request.form\n    use_gpu_for_embeddings = request.form.get('use_gpu_for_embeds', False)    # default no\n    model_choice = str(request.form['model_choice'])\n    use_gpu = request.form.get('use_gpu', False)\n\n    config_update_dict.update({'use_azure_open_ai':use_azure_open_ai, 'use_openai_embeddings':use_openai_embeddings, 'use_sbert_embeddings':use_sbert_embeddings, 'use_bge_large_embeddings':use_bge_large_embeddings, 'use_bge_base_embeddings':use_bge_base_embeddings, 'use_gpu_for_embeddings':use_gpu_for_embeddings, 'model_choice':model_choice, 'use_gpu':use_gpu})\n\n    try:\n        if use_bge_base_embeddings or use_bge_large_embeddings:\n            model_name = \"\"\n            if use_bge_base_embeddings:\n                model_name = \"BAAI/bge-base-en\"\n            elif use_bge_large_embeddings:\n                model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n    except Exception as e:\n        return handle_api_error(\"Could not load BGE embeddings in process_model, encountered error: \", e)\n    \n    try:\n        write_config(config_update_dict)\n    except Exception as e:\n        handle_local_error(\"Could not write updates to config.json, encountered error: \", e)\n\n    # Redirect to the next step\n    return redirect(url_for('load_file'))\n\n\ndef convert_to_pdf_with_unoconv(input_file_path, output_file_path):\n    print(\"\\n\\nConverting non-PDF document to PDF format\\n\\n\")\n    if platform.system() == 'Windows':\n        subprocess.run(['python', 'unoconv.py', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n    else:\n        subprocess.run(['unoconv', '-f', 'pdf', '-o', output_file_path, input_file_path], check=True)\n\n\ndef reload_vector_store():\n    global VECTOR_STORE\n    print(\"\\nRe-Loading VectorDB: ChromaDB\")\n\n    vectordb_used = \"\"\n\n    try:\n        read_return = read_config(['use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder', 'embedding_model_choice'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n        embedding_model_choice = read_return['embedding_model_choice']\n    except Exception as e:\n        handle_local_error(\"Missing values in config.json when reloading VectorDB, could not fully complete process_new_file. Please try restarting the application. Error: \", e)\n\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            vectordb_used = vectordb_sbert_folder\n        elif use_openai_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n            vectordb_used = vectordb_openai_folder\n        elif use_bge_base_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_base_folder\n        elif use_bge_large_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n            vectordb_used = vectordb_bge_large_folder\n    except Exception as e:\n        handle_local_error(\"Could not reload VectorDB when trying to process_new_file. Please try restarting the application. Error: \", e)\n    \n    return embedding_model_choice, vectordb_used\n\n\ndef convert_non_pdf_to_pdf_with_unoconv(filename, filepath):\n    print(\"Converting to PDF file\")\n\n    try:\n        conv_filename = os.path.splitext(filename)[0] + '.pdf'\n        conv_filepath = os.path.join(app.config['UPLOAD_FOLDER'], conv_filename)\n\n        convert_to_pdf_with_unoconv(filepath, conv_filepath)\n\n        return conv_filename, conv_filepath\n    except subprocess.CalledProcessError as e:\n        return handle_api_error(\"Could not convert file to PDF, encountered error: \", e)\n    except Exception as e:\n        return handle_api_error(\"Unexpected error when converting file to PDF, encountered error: \", e)\n\n\ndef vector_embed_filepath(filename, filepath):\n    print(\"Vector Embedding Document\")\n\n    if not filename.lower().endswith('.pdf'):\n        _, filepath = convert_non_pdf_to_pdf_with_unoconv(filename, filepath)\n\n    use_ocr = False\n    try:\n        read_return = read_config(['use_ocr', 'ocr_service_choice'])\n        use_ocr = read_return['use_ocr']\n        ocr_service_choice = read_return['ocr_service_choice']\n    except Exception as e:\n        handle_local_error(\"Could not determine use_ocr in config.json for process_new_file. Disabling OCR and proceeding. Error: \", e)\n    \n    print(\"Processing PDF file\")\n    \n    if use_ocr:\n        try:\n            if ocr_service_choice == 'AzureVision':\n                input_file = PDFtoAzureOCRTXT(filepath)\n            elif ocr_service_choice == 'AzureDocAi':\n                input_file = PDFtoAzureDocAiTXT(filepath)\n        except Exception as e:\n            handle_error_no_return(\"Failed to OCR text from PDF. Will now attempt to extract text via PyPDF2. Encountered error: \", e)\n            try:\n                input_file = PDFtoTXT(filepath)\n            except Exception as e:\n                handle_local_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    else:\n        try:\n            input_file = PDFtoTXT(filepath)\n        except Exception as e:\n            handle_local_error(\"Failed to extract text from the PDF document, even via fallback PyPDF2, encountered error: \", e)\n    \n    # try:\n    #     images = extract_images_from_pdf(filepath)\n    # except Exception as e:\n    #     handle_error_no_return(\"Failed to extract images from the PDF document, encountered error: \", e)\n\n    # try:\n    #     store_images_to_db(images)\n    # except Exception as e:\n    #     handle_error_no_return(\"Failed to save images to database, encountered error: \", e)\n    \n    try:\n        chunk_size, chunk_overlap = LoadNewDocument(input_file)\n    except Exception as e:\n        handle_local_error(\"Failed to extract text from PDF: \", e)\n    \n    try:\n        embedding_model_choice, vectordb_used = reload_vector_store()\n    except Exception as e:\n        handle_local_error(\"Could not reload vector store when attempting to vector_embed_filepath(), encountered error: \", e)\n\n    try:\n        record_doc_loaded_to_db(filename, embedding_model_choice, vectordb_used, chunk_size, chunk_overlap)\n    except Exception as e:\n        handle_error_no_return(\"Unable to record document loading to records DB, encountered error: \", e)\n\n    return True\n\n\n# Route to handle the submission of the second form (file loading)\n@app.route('/process_new_file', methods=['POST'])\ndef process_new_file():\n\n    try:\n        input_file = request.files['file']\n    except Exception as e:\n        return handle_api_error(\"Server-side error recieving file: \", e)\n\n    # Ensure the filename is secure\n    filename = secure_filename(input_file.filename)\n    if \"PDF\" in filename:\n        filename = filename.replace(\"PDF\", \"pdf\")\n\n    try:\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n        print(\"Loading new file - filename: \", filename)\n        print(\"Loading new file - filepath: \", filepath)\n\n        # Save the uploaded file to the specified path\n        input_file.save(filepath)\n    except Exception as e:\n        return handle_api_error(\"Failed to save document to app folder, encountered error: \", e)\n\n    try:\n        vector_embed_filepath(filename, filepath)\n    except Exception as e:\n        return handle_api_error(\"Could not vector_embed_filepath() in the process_new_file() method, encountered error: \", e)\n\n    return jsonify(success=True)\n\n\n# Route to store user rating: \n# ATTN: comment out print() statements, as users may elect to leave a rating as a response is being generated, which is when the stdout is redirected to the event stream! \n@app.route('/store_user_rating', methods=['POST'])\ndef store_user_rating():\n    \n    # print(\"Stroing user rating\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json for method store_user_rating. Error: \", e)\n    \n    try:\n        user_rating = request.form['rating']\n        chat_id_for_rating = request.form['chat_id']\n        sequence_id_for_rating = request.form['sequence_id']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read user rating or failed to obtain chat/sequence ID, encountered error: \", e)\n\n    # print(\"user_rating: \", user_rating)\n    # print(\"chat_id_for_rating: \", chat_id_for_rating)\n    # print(\"sequence_id_for_rating: \", sequence_id_for_rating)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        return handle_api_error(\"Could not connect to chat history DB for storage of user-rating, encountered error: \", e)\n\n    try:\n        cursor.execute(\n            '''\n            UPDATE chat_history\n            SET user_rating = ?\n            WHERE chat_id = ? AND sequence_id = ?\n            ''',\n            (user_rating, chat_id_for_rating, sequence_id_for_rating)\n        )\n        conn.commit()\n    except Exception as e:\n        return handle_api_error(\"Could not store user-rating to chat history db, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify(success=True)\n\n\n\ndef is_local_server_online(server_to_check):\n    server_health_url = ''\n    if server_to_check == 'llama-cpp':\n        server_health_url = 'http://localhost:8080/health'\n    elif server_to_check == 'hf-waitress':\n        server_health_url = 'http://localhost:9069/health'\n    else:\n        return handle_api_error(\"Invalid server_to_check in method is_local_server_online, encountered error: \", e)\n\n    print(f\"\\n\\nChecking {server_to_check} server status\\n\\n\")\n\n    try:\n        response = requests.get(server_health_url)\n        \n        if response.status_code == 200:\n            data = response.json()  # parse the JSON response to determine the server status\n            if data['status'] == 'ok' and server_to_check == 'llama-cpp':\n                print(f\"llama.cpp Server ready: {data['slots_idle']} idle slots, {data['slots_processing']} processing slots.\")\n                return {\"server_available\":True, \"loading_model\":False, \"status_code\":200}\n            elif data['status'] == 'ok' and server_to_check == 'hf-waitress':\n                print(f\"hf-waitress Server ready and online\")\n                return {\"server_available\":True, \"loading_model\":False, \"status_code\":200}\n            elif data['status'] == 'no slot available':\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":200}\n            \n        elif response.status_code == 503:   # model still loading or no slots\n            data = response.json()\n            if data['status'] == 'loading model':\n                print(\"Server is loading the selected LLM, please wait\")\n                return {\"server_available\":False, \"loading_model\":True, \"status_code\":503}\n            else:\n                print(\"No slots available. Server is running but cannot handle more requests.\")\n                return {\"server_available\":False, \"loading_model\":False, \"status_code\":503}\n        \n        elif response.status_code == 500:\n            print(\"Server error: Failed to load LLM.\")\n            logger.error(\"Local LLM Server - 500 event\")\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n        \n        else:\n            return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n    except requests.exceptions.ConnectionError as e:\n        error_message = \"\\n\\nECONNREFUSED event\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":True, \"status_code\":500}\n    except Exception as e:\n        error_message = f\"\\n\\nCould not check local LLM Server health, encountered error: {e}\\n\\n\"\n        if logger:\n            logger.error(error_message)\n            print(error_message)\n        else:\n            print(error_message)\n        return {\"server_available\":False, \"loading_model\":False, \"status_code\":500}\n    \n\ndef send_ctrl_c_to_process(process):\n    if process.poll() is None:  # check if process is still running via poll(), which returns None if a process is still running \n        if platform.system() == 'Windows':\n            process.send_signal(signal.CTRL_BREAK_EVENT)\n        else:\n            process.send_signal(signal.SIGINT)\n        try:\n            # Wait a bit for the process to terminate gracefully:\n            process.wait(timeout=3)\n        except subprocess.TimeoutExpired:\n            print(\"\\n\\nProcess did not terminate within timeout, will be force-killed.\\n\\n\")\n            process.kill()  # Sends 'SIGKILL' on Unix-like to force-kill immediately / 'TerminateProcess' on Windows which still allows for graceful termination\n            process.wait()\n            if process.poll() is not None:\n                print(\"\\n\\nProcess has been killed successfully.\\n\\n\")\n            else:\n                print(\"\\n\\nProcess still running after force kill attempt.\\n\\n\")\n\n\ndef terminate_local_llm_server_process(process):\n    try:\n        # process.terminate() sends 'SIGTERM' on Unix-like systems / 'TerminateProcess' on Windows, allows for graceful termination\n        # process.wait()\n        send_ctrl_c_to_process(process)\n        if process.poll() is not None:  # process has indeed terminated\n            print(\"\\n\\nProcess terminated gracefully.\\n\\n\")\n    except Exception as e:\n        handle_local_error(\"Failed to terminate local LLM server process, encountered error: \", e)\n\n\n@app.route('/llama_cpp_server_starter')\ndef llama_cpp_server_starter():\n    print(\"\\n\\nStarting llama.cpp Server\\n\\n\")\n\n    global LLM_CHANGE_RELOAD_TRIGGER_SET\n    global LLAMA_CPP_PROCESS\n    global HF_WAITRESS_PROCESS\n    global LLM_LOADED_UP\n\n    other_server_running = False\n\n    # Before attempting to start the llama.cpp server, check if HF-Waitress is running and if so, shut it down:\n    try:\n        if is_local_server_online('hf-waitress')['server_available']:\n            print(\"\\n\\nThe HF-Waitress server is running. Attempting to shut it down before starting the llama.cpp server.\\n\\n\")\n            try:\n                if HF_WAITRESS_PROCESS is not None:\n                    terminate_local_llm_server_process(HF_WAITRESS_PROCESS)\n                    HF_WAITRESS_PROCESS = None\n                    LLM_LOADED_UP = False\n                else:\n                    raise Exception(\"HF_WAITRESS_PROCESS is None but server_available\")\n            except Exception as e:\n                LLM_LOADED_UP = True    # We know the HF-Waitress server is running, which means `hf_waitress.py` is available, so we set LLM_LOADED_UP to True\n                other_server_running = True # Set to True as we've determined the other server is running and we failed to terminate it\n                handle_error_no_return(\"Warning: Failed to terminate running HF-Waitress process before launching llama.cpp. It was likely launched by a previous session or external process. Consider manually shutting down this server to conserve memory. Technical error-details follow: \", e)\n    except Exception as e:\n        handle_error_no_return(\"Warning: Could not check if HF-Waitress server is running. Proceeding to launch llama.cpp server. Encountered error: \", e)   \n\n    is_llama_cpp_running = False\n    try:\n        is_llama_cpp_running = is_local_server_online('llama-cpp')['server_available']\n    except Exception as e:\n        handle_error_no_return(\"Warning: Could not check if llama.cpp server is running. Proceeding to launch llama.cpp server. Encountered error: \", e)\n\n    model_choice = 'undefined'\n    try:\n        read_return = read_config(['model_choice'])\n        model_choice = read_return['model_choice']\n    except Exception as e:\n        handle_error_no_return(\"Missing model_choice in config.json in method llama_cpp_server_starter. Printing error and proceeding with model_choice: 'undefined' \", e)\n\n    if is_llama_cpp_running and not LLM_CHANGE_RELOAD_TRIGGER_SET:\n        LLM_LOADED_UP = True\n        print(f'\\n\\nThe llama.cpp server is already loaded and the reload trigger is not set. Simply returning with model choice: {model_choice}\\n\\n')\n        return jsonify({'success': True, 'llm_model': model_choice, 'other_server_running': other_server_running})\n    \n    elif is_llama_cpp_running and LLM_CHANGE_RELOAD_TRIGGER_SET:\n        print(\"\\n\\nllama.cpp server online and LLM_CHANGE_RELOAD_TRIGGER_SET is set. Attempting to terminate and reload from config.json\\n\\n\")\n        try:\n            terminate_local_llm_server_process(LLAMA_CPP_PROCESS)\n            LLAMA_CPP_PROCESS = None\n            LLM_CHANGE_RELOAD_TRIGGER_SET = False\n        except Exception as e:\n            LLM_LOADED_UP = True  # We know the llama.cpp server is running but there was an error terminating it, so we set LLM_LOADED_UP to True while leaving LLM_CHANGE_RELOAD_TRIGGER_SET to True as we know the server needs to be re-loaded.\n            handle_error_no_return(\"Failed to terminate running llama.cpp process, server was likely launched by a previous session. Returning with the currently loaded LLM. To change, shutdown the previously launched server manually and reload this page. Technical error-details follow: \", e)\n            return jsonify({'success': True, 'llm_model': 'undefined', 'other_server_running': other_server_running})   # We still return success:True as we've at least determined llama.cpp is running and loaded with a model, even if we cannot reload it.\n                 \n    elif LLM_CHANGE_RELOAD_TRIGGER_SET: # llama.cpp is not running, set flags to false and new settings will be loaded on next llama.cpp launch\n        print(\"\\n\\nResetting the LLM_CHANGE_RELOAD_TRIGGER_SET flag and attemping to launch the server with the currently selected LLM.\\n\\n\")\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False\n        LLM_LOADED_UP = False\n\n\n    try:\n        read_return = read_config(['model_dir', 'model_choice', 'local_llm_context_length', 'local_llm_max_new_tokens', 'local_llm_gpu_layers', 'server_timeout_seconds', 'server_retry_attempts', 'use_gpu'])\n        model_dir = read_return['model_dir']\n        model_choice = read_return['model_choice']\n        local_llm_context_length = read_return['local_llm_context_length']\n        local_llm_max_new_tokens = read_return['local_llm_max_new_tokens']\n        local_llm_gpu_layers = read_return['local_llm_gpu_layers']\n        server_timeout_seconds = read_return['server_timeout_seconds']\n        server_retry_attempts = read_return['server_retry_attempts']\n        use_gpu = read_return['use_gpu']\n    except Exception as e:\n        return handle_api_error(\"Missing values in config.json when preparing to launch llama.cpp server, encountered error: \", e)\n\n\n    try:\n        cpp_model = os.path.join(model_dir, model_choice)\n    except Exception as e:\n        return handle_api_error(\"Could not os.join path to model file to launch llama.cpp server, encountered error: \", e)\n\n    if not use_gpu:\n        local_llm_gpu_layers = 0\n\n    try:\n        cpp_app = ['llama-server', '-m', cpp_model, '-ngl', str(local_llm_gpu_layers), '-c', str(local_llm_context_length), '-n', str(local_llm_max_new_tokens), '--host', '0.0.0.0']\n\n        if platform.system() == 'Windows':\n            LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, creationflags=subprocess.CREATE_NEW_CONSOLE)  # Windows only! Comment when containerizing or deploying to Linux/MacOS!\n        else:           \n            # Platform & container agnostic:\n            with open('llama_cpp_server_output_log.txt', 'w') as f:\n                LLAMA_CPP_PROCESS = subprocess.Popen(cpp_app, stdout=f, stderr=subprocess.STDOUT, text=True)    #stdout has already been redirected to the file, so simply direct stderr to stdout!\n\n    except Exception as e:\n        return handle_api_error(\"Could not launch llama.cpp process, encountered error: \", e)\n\n\n    timeout = server_timeout_seconds   \n    attempts = server_retry_attempts\n\n    try:\n        for _ in range(attempts):\n            if is_local_server_online('llama-cpp')['server_available']:\n                print(\"\\n\\nllama.cpp server launched succesfully! Returning.\\n\\n\")\n                LLM_LOADED_UP = True\n                return jsonify({'success': True, 'llm_model': model_choice, 'other_server_running': other_server_running})\n            time.sleep(timeout)\n    except Exception as e:\n        handle_error_no_return(\"Could not check server status after launch attempt, printing error and retrying: \", e)\n\n    return handle_api_error(\"Failed to start llama.cpp local-server\")\n\n\n@app.route('/hf_waitress_server_starter')\ndef hf_waitress_server_starter():\n    print(\"\\n\\nStarting HF-Waitress Server\\n\\n\")\n\n    global LLM_CHANGE_RELOAD_TRIGGER_SET\n    global LLM_LOADED_UP\n    global HF_WAITRESS_PROCESS\n    global LLAMA_CPP_PROCESS\n\n    other_server_running = False\n\n    # Before attempting to start the HF-Waitress server, check if llama.cpp is running and if so, shut it down:\n    try:\n        if is_local_server_online('llama-cpp')['server_available']:\n            print(\"\\n\\nThe llama.cpp server is running. Attempting to shut it down before starting the HF-Waitress server.\\n\\n\")\n            try:\n                if LLAMA_CPP_PROCESS is not None:\n                    terminate_local_llm_server_process(LLAMA_CPP_PROCESS)\n                    LLAMA_CPP_PROCESS = None\n                    LLM_LOADED_UP = False\n                else:\n                    raise Exception(\"LLAMA_CPP_PROCESS is None but server_available\")\n            except Exception as e:\n                LLM_LOADED_UP = True    # We know the llama.cpp server is running, which means `llama-server` is available, so we set LLM_LOADED_UP to True\n                other_server_running = True # Set to True as we've determined the other server is running and we failed to terminate it\n                handle_error_no_return(\"Warning: Failed to terminate running llama.cpp process before launching HF-Waitress. It was likely launched by a previous session or external process. Consider manually shutting down this server to conserve memory. Technical error-details follow: \", e)\n    except Exception as e:\n        handle_error_no_return(\"Could not check if llama.cpp server is running. Proceeding to launch HF-Waitress server. Encountered error: \", e)\n\n    is_awq = False\n    model_choice = 'microsoft/Phi-3-mini-4k-instruct'   # match default in hf_waitress.py as this will only be used in the very first run, as the hf_config.json file is created in the first run!\n    use_flash_attention_2 = False\n    try:\n        with open('hf_config.json', 'r') as file:\n            hf_config = json.load(file)\n            is_awq = hf_config['awq']\n            use_flash_attention_2 = hf_config['use_flash_attention_2']\n            model_choice = hf_config['model_id']\n    except Exception as e:\n        handle_error_no_return(\"Could not read hf_config.json in method hf_waitress_server_starter. Continuing with defaults. encountered error: \", e)\n\n    if is_local_server_online('hf-waitress')['server_available']:\n        print(\"\\n\\nHF-Waitress server already running. Resetting LLM_CHANGE_RELOAD_TRIGGER_SET and simply returning!\\n\\n\")\n        LLM_LOADED_UP = True\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False   # The only instance where we're in this method and LLM_CHANGE_RELOAD_TRIGGER_SET is set while the HF-Waitress server is running is when we're trying to switch back to it after running llama.cpp. So we simply reset the flag and return.\n        return jsonify({'success': True, 'llm_model': model_choice, 'other_server_running': other_server_running})\n    elif LLM_CHANGE_RELOAD_TRIGGER_SET:  # Switching to HF-Waitress and it's offline, so we set LLM_CHANGE_RELOAD_TRIGGER_SET to False and proceed to launch the server.\n        print('\\n\\nProceeding to reload the LLM & resetting the LLM_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        LLM_CHANGE_RELOAD_TRIGGER_SET = False\n        LLM_LOADED_UP = False\n    \n    print(\"\\n\\nProceeding to launch HF-Waitress server\\n\\n\")\n\n    launch_args = ' '\n    if is_awq:\n        launch_args += '--awq '\n    if use_flash_attention_2:\n        launch_args += '--use_flash_attention_2 '\n    \n    launch_args = launch_args.strip()\n    base_command = 'python' if platform.system() == 'Windows' else 'python3'\n    full_command = f\"{base_command} hf_waitress.py {launch_args}\"\n\n    try:\n        if platform.system() == 'Windows':\n            HF_WAITRESS_PROCESS = subprocess.Popen(full_command, creationflags=subprocess.CREATE_NEW_CONSOLE)   #Popen is non-blocking, so the server will keep running in the background\n        else:\n            # Platform & container agnostic:\n            with open('hf_waitress_output_log.txt', 'w') as f:\n                HF_WAITRESS_PROCESS = subprocess.Popen(full_command, stdout=f, stderr=subprocess.STDOUT, text=True)\n\n    except Exception as e:\n        return handle_api_error(\"Could not launch HF-Waitress process, encountered error: \", e)\n\n    timeout = 5   # seconds\n    attempts = 25\n\n    try:\n        for _ in range(attempts):\n            if is_local_server_online('hf-waitress')['server_available']:\n                print(\"\\n\\nHF-Waitress server launched succesfully! Returning.\\n\\n\")\n                LLM_LOADED_UP = True\n                return jsonify({'success': True, 'llm_model': model_choice, 'other_server_running': other_server_running})\n            time.sleep(timeout)\n    except Exception as e:\n        handle_error_no_return(\"Could not check server status after launch attempt, printing error and retrying: \", e)\n\n    return handle_api_error(\"Failed to start HF-Waitress Server. It may be taking a while to download the model, try refreshing LARS in a few minutes.\")\n\n\n@app.route('/check_local_llm_server_status', methods=['POST'])\ndef check_local_llm_server_status():\n    \n    server_online = False\n    \n    try:\n        server_to_check = request.form['server_to_check']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read server_to_check from the POST request in method check_local_llm_server_status, encountered error: \", e)\n    \n    try:\n        server_online = is_local_server_online(server_to_check)['server_available']\n    except Exception as e:\n        return handle_api_error(f\"Error checking {server_to_check} server status in method check_local_llm_server_status, encountered error: \", e)\n\n    return jsonify({'success': True, 'server_online': server_online})\n\n\n@app.route('/local_llm_server_starter')\ndef local_llm_server_starter():\n    print(\"Starting Local LLM Server\")\n\n    try:\n        read_return = read_config(['local_llm_server'])\n        server_to_start = read_return['local_llm_server']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read local_llm_server from config.json in method local_llm_server_starter, encountered error: \", e)\n\n    try:\n        if server_to_start == 'hf-waitress':\n            return hf_waitress_server_starter()\n        elif server_to_start == 'llama-cpp':\n            return llama_cpp_server_starter()\n        else:\n            return handle_api_error(f\"Invalid local LLM server choice in method local_llm_server_starter: {server_to_start}\")\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not start local LLM server in method local_llm_server_starter, encountered error: \", e)\n\n    return jsonify({'success': True})\n\n\n@app.route('/load_vectordb')\ndef load_vectordb():\n\n    global VECTOR_STORE\n    global HF_BGE_EMBEDDINGS\n    global AZURE_OPENAI_EMBEDDINGS\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    global VECTORDB_LOADED_UP\n\n    if VECTORDB_LOADED_UP and not VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print(f'\\n\\nVectorDB already loaded! Simply returning.\\n\\n')\n        return jsonify({'success': True})\n    elif VECTORDB_CHANGE_RELOAD_TRIGGER_SET:\n        print('\\n\\nProceeding to reload VectorDB & resetting the VECTORDB_CHANGE_RELOAD_TRIGGER_SET flag.\\n\\n')\n        VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n\n    try:\n        read_return = read_config(['use_gpu_for_embeddings', 'use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'vectordb_sbert_folder', 'vectordb_openai_folder', 'vectordb_bge_base_folder', 'vectordb_bge_large_folder'])\n        use_gpu_for_embeddings = read_return['use_gpu_for_embeddings']\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        vectordb_sbert_folder = read_return['vectordb_sbert_folder']\n        vectordb_openai_folder = read_return['vectordb_openai_folder']\n        vectordb_bge_base_folder = read_return['vectordb_bge_base_folder']\n        vectordb_bge_large_folder = read_return['vectordb_bge_large_folder']\n    except Exception as e:\n        return handle_api_error(\"Missing values in config.json when attempting to load_vectordb. Error: \", e)\n    \n    \n    ### 1 - Load VectorDB from disk\n    print(\"\\n\\nLoading VectorDB: ChromaDB\\n\\n\")\n    try:\n        if use_sbert_embeddings:\n            VECTOR_STORE = Chroma(persist_directory=vectordb_sbert_folder, embedding_function=HuggingFaceEmbeddings())\n            # try:\n            #     # chroma_client = VECTOR_STORE.PersistentClient\n            #     # max_batch_size = chroma_client._producer.max_batch_size\n            #     max_batch_size = VECTOR_STORE.max_batch_size\n            #     print(f\"max_batch_size: {max_batch_size}\")\n            # except Exception as e:\n            #     print(f\"Could not get max_batch_size. Error: {e}\")\n        \n        elif use_openai_embeddings:\n\n            try:\n                read_return = read_config(['azure_openai_text_ada_api_url', 'azure_openai_text_ada_api_key', 'azure_openai_api_type', 'azure_openai_api_version', 'azure_openai_text_ada_deployment_name'])\n                azure_openai_text_ada_api_url = read_return['azure_openai_text_ada_api_url']\n                azure_openai_text_ada_api_key = read_return['azure_openai_text_ada_api_key']\n                azure_openai_api_type = read_return['azure_openai_api_type']\n                azure_openai_api_version = read_return['azure_openai_api_version']\n                azure_openai_text_ada_deployment_name = read_return['azure_openai_text_ada_deployment_name']\n            except Exception as e:\n                return handle_api_error(\"Missing values for Azure OpenAI Embeddings in method load_model_and_vectordb in config.json. Error: \", e)\n            \n            try:\n                os.environ[\"OPENAI_API_BASE\"] = azure_openai_text_ada_api_url\n                os.environ[\"OPENAI_API_KEY\"] = azure_openai_text_ada_api_key\n                os.environ[\"OPENAI_API_TYPE\"] = azure_openai_api_type\n                os.environ[\"OPENAI_API_VERSION\"] = azure_openai_api_version\n            except Exception as e:\n                return handle_api_error(\"Could not set OS environment variables for Azure OpenAI Embeddings in load_model_and_vectordb, encountered error: \", e)\n\n            \n            AZURE_OPENAI_EMBEDDINGS = OpenAIEmbeddings(deployment=azure_openai_text_ada_deployment_name)\n            VECTOR_STORE = Chroma(persist_directory=vectordb_openai_folder, embedding_function=AZURE_OPENAI_EMBEDDINGS)\n        \n        elif use_bge_base_embeddings:\n            model_name = \"BAAI/bge-base-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_base_folder, embedding_function=HF_BGE_EMBEDDINGS)\n                \n        \n        elif use_bge_large_embeddings:\n            model_name = \"BAAI/bge-large-en\"\n            model_kwargs = {}\n            if use_gpu_for_embeddings:\n                model_kwargs.update({\"device\": \"cuda\"})\n            else:\n                model_kwargs.update({\"device\": \"cpu\"})\n            encode_kwargs = {\"normalize_embeddings\": True}\n            HF_BGE_EMBEDDINGS = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n            )\n            VECTOR_STORE = Chroma(persist_directory=vectordb_bge_large_folder, embedding_function=HF_BGE_EMBEDDINGS)\n        \n        #VECTOR_STORE = Chroma(persist_directory=VECTORDB_SBERT_FOLDER, embedding_function=HuggingFaceEmbeddings())\n    except Exception as e:\n        return handle_api_error(\"Could not load VectorDB, encountered error: \", e)\n    \n    VECTORDB_LOADED_UP = True\n    return jsonify(success=True)\n\n\n@app.route('/set_prompt_template', methods=['POST'])\ndef set_prompt_template():\n\n    base_template = \"\"\n\n    try:\n        base_template = request.form['prompt_template']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read prompt_template from the POST request in method set_prompt_template, encountered error: \", e)\n    \n    try:\n        write_config({'base_template':base_template})\n    except Exception as e:\n        return handle_api_error(\"Could not update base_template in method set_prompt_template, encountered error: \", e)\n\n    return jsonify({'success':True})\n\n\n@app.route('/fetch_file_list_for_vector_db', methods=['POST'])\ndef fetch_file_list_for_vector_db():\n\n    print(\"Loading file list for selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method fetch_file_list_for_vector_db, encountered error: \", e)\n\n    # For the VectorDB presently picked by the user in the dropdown, obtain the associated VectorDB folder for the select query:\n    vdb_for_select = \"\"\n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            read_return = read_config(['vectordb_bge_large_folder'])\n            vdb_for_select = read_return['vectordb_bge_large_folder']\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            read_return = read_config(['vectordb_bge_base_folder'])\n            vdb_for_select = read_return['vectordb_bge_base_folder']\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            read_return = read_config(['vectordb_sbert_folder'])\n            vdb_for_select = read_return['vectordb_sbert_folder']\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            read_return = read_config(['vectordb_openai_folder'])\n            vdb_for_select = read_return['vectordb_openai_folder']\n\n        vdb_for_select = '%' + os.path.basename(vdb_for_select)\n        print(f'vdb_for_select: {vdb_for_select}')\n\n    except Exception as e:\n        return handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['sqlite_docs_loaded_db'])\n        sqlite_docs_loaded_db = read_return['sqlite_docs_loaded_db']\n    except Exception as e:\n        return handle_api_error(\"Missing sqlite_docs_loaded_db in config.json in method fetch_file_list_for_vector_db. Error: \", e)\n\n    file_row_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_docs_loaded_db)\n        c = conn.cursor()\n    except Exception as e:\n        return handle_api_error(\"Could not connect to sqlite_docs_loaded_db database to load file list, encountered error: \", e)\n\n    # If the database does not currently exist...\n    try:\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS document_records (\n                    id INTEGER PRIMARY KEY,\n                    document_name TEXT NOT NULL,\n                    embedding_model TEXT NOT NULL,\n                    vectordb_used TEXT,\n                    chunk_size INTEGER,\n                    chunk_overlap INTEGER\n            )\n        ''')\n\n        conn.commit()\n    except Exception as e:\n        handle_local_error(\"Could not create document_records DB, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT document_name, vectordb_used, chunk_size, chunk_overlap FROM document_records where vectordb_used LIKE ?\", (vdb_for_select,))\n    except Exception as e:\n        return handle_api_error(\"Could not get document list from document_records db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            file_row_list.append(list(list_item))\n    except Exception as e:\n        return handle_api_error(\"Could not parse document list from document_records db, encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, 'file_row_list': file_row_list})\n\n\n@app.route('/reset_vector_db_on_disk', methods=['POST'])\ndef reset_vector_db_on_disk():\n\n    print(\"Resetting selected VectorDB\")\n\n    try:\n        selected_embedding_model_choice = request.form['embedding_model_choice']\n    except Exception as e:\n        return handle_api_error(\"Server-side error, could not read embedding_model_choice from the POST request in method reset_vector_db_on_disk, encountered error: \", e)\n\n    try:\n        read_return = read_config(['base_directory'])\n        base_directory = read_return['base_directory']\n    except Exception as e:\n        handle_local_error(\"Could not read base_directory from config.json for reset_vector_db_on_disk. Error: \", e)\n\n    try:\n        current_datetime = datetime.datetime.now()\n        formatted_datetime = current_datetime.strftime('%Y-%m-%d-%Hhr-%Mmin-%Ssec')\n    except Exception as e:\n        return handle_api_error(\"Could not obtain timestamp in reset_vector_db_on_disk, encountered error: \", e)\n\n    # Now that we have all pre-requisite data to create a new VectorDB, proceed to do so by checking the model the user had currently picked from the dropdown: \n    try:\n        if selected_embedding_model_choice == 'bge_large':\n            vectordb_bge_large_folder = base_directory + '/chroma_db_bge_large_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_large_folder':vectordb_bge_large_folder})\n            \n        elif selected_embedding_model_choice == 'bge_base':\n            vectordb_bge_base_folder = base_directory + '/chroma_db_bge_base_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_bge_base_folder':vectordb_bge_base_folder})\n\n        elif selected_embedding_model_choice == 'sbert_mpnet_base_v2':\n            vectordb_sbert_folder = base_directory + '/chroma_db_sbert_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_sbert_folder':vectordb_sbert_folder})\n\n        elif selected_embedding_model_choice == 'openai_text_ada':\n            vectordb_openai_folder = base_directory + '/chroma_db_openai_embeddings' + '-' + formatted_datetime\n            write_config({'vectordb_openai_folder':vectordb_openai_folder})\n\n    except Exception as e:\n        return handle_api_error(\"Could not create new VectorDB in reset_vector_db_on_disk, encountered error: \", e)\n\n    restart_required = True\n    global VECTORDB_CHANGE_RELOAD_TRIGGER_SET\n    VECTORDB_CHANGE_RELOAD_TRIGGER_SET = True\n    try:\n        read_return = read_config(['embedding_model_choice'])\n        set_embedding_model_choice = read_return['embedding_model_choice']\n        if set_embedding_model_choice != selected_embedding_model_choice:\n            restart_required = False\n            VECTORDB_CHANGE_RELOAD_TRIGGER_SET = False\n    except Exception as e:\n        handle_error_no_return(\"Could not compare selected and set embedding models when determining if restart_required in reset_vector_db_on_disk(), encountered error: \", e)\n\n    #print(f'returning docs loaded list: {file_row_list}')\n\n    return jsonify({'success': True, \"restart_required\": restart_required})\n\n\n@app.route('/load_chat_history_list')\ndef load_chat_history_list():\n\n    print(\"loading chat history list for sidebar\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        return handle_api_error(\"Missing sqlite_history_db in config.json in method load_chat_history_list. Error: \", e)\n\n    history_id_list = []\n    \n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        return handle_api_error(\"Could not connect to sqlite_history_db database to load chat history list, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT DISTINCT chat_id FROM chat_history\")\n    except Exception as e:\n        return handle_api_error(\"Could not get list from chat history db, encountered error: \", e)\n    \n    try:\n        result = c.fetchall()\n\n        for list_item in result:\n            history_id_list.append(list_item)\n    except Exception as e:\n        return handle_api_error(\"Could not parse chat history list from db, encountered error: \", e)\n\n    #print(f'returning chat hsitory list: {history_id_list}')\n\n    return jsonify({'success': True, 'history_list': history_id_list})\n\n\n@app.route('/load_chat_history', methods=['POST'])\ndef load_chat_history():\n\n    global CHAT_ID\n    global SEQUENCE_ID\n    global HISTORY_SUMMARY\n    global HISTORY_MEMORY_WITH_BUFFER\n\n    print(\"loading chat history\")\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method load_chat_history. Error: \", e)\n\n    # Clear chat history of current chat, prep for loading historical chat summary:\n    # try:\n    #     HISTORY_MEMORY_WITH_BUFFER.chat_memory.clear()\n    #     HISTORY_MEMORY_WITH_BUFFER = ConversationSummaryBufferMemory(llm=LLM, max_token_limit=300, return_messages=False)\n    #     HISTORY_SUMMARY = {}\n    # except Exception as e:\n    #     handle_error_no_return(\"Could not clear memory when loading chat history, encountered error: \", e)\n\n    try:\n        chat_id_for_history_search = request.form['chat_id']\n        CHAT_ID = request.form['chat_id']\n    except Exception as e:\n        return handle_api_error(\"Could not retrieve Chat ID from request form, encountered error: \", e)\n\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        return handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    sequence_id_for_history_search = 1\n    retrieve_history = True\n    chat_history = []\n    old_chat_model = \"\"\n\n    while(retrieve_history):\n\n        try:\n            c.execute(\"SELECT user_query FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n            \n            user_message = str(result[0])\n\n            user_message = user_message.strip('\\n')\n            regex_to_swap_multiple_spaces_with_newline = r' {2,}'\n            user_message = re.sub(regex_to_swap_multiple_spaces_with_newline, '<br>', user_message)\n\n            user_message = '<div class=\"user-message\">' + user_message + '</div>'\n\n            chat_history.append(user_message)\n\n            c.execute(\"SELECT llm_response FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n\n            result = str(result[0])\n            result_parts = result.split(\"pdf_pane_data=\",1)\n            # llm_response = '<div class=\"llm-wrapper\"> <div class=\"llm-response\">' + str(result[0]) + '</div>'\n            llm_response = '<div class=\"response-and-viewer-container\"><div class=\"llm-wrapper\"> <div class=\"llm-response\">' + result_parts[0]\n\n        except Exception as e:\n            return handle_api_error(\"Could not retrieve chat history, encountered error: \", e)\n        \n        llm_response = llm_response.strip('\\n')\n        llm_response = llm_response.replace('\\n\\n', '<br><br>')\n        llm_response = llm_response.replace('\\n', '<br>')\n        \n        try:\n            c.execute(\"SELECT user_rating FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            result = c.fetchone()\n        except Exception as e:\n            handle_error_no_return(\"Could not fetch user rating, encountered error: \", e)\n\n        response_rated = False\n        user_rating_for_history_chat = None\n\n        if result[0]:\n            response_rated = True\n            try:\n                user_rating_for_history_chat = int(result[0])\n                #print(f'rating exists: {user_rating_for_history_chat}')\n            except Exception as e:\n                handle_error_no_return(\"Could not retrieve integer value of user rating, encountered error: \", e)\n\n\n        llm_rating = f'''<div class=\"star-rating\" data-rated={response_rated} rating-chat-id={chat_id_for_history_search} rating-sequence-id={sequence_id_for_history_search}>\n        <i class=\"far fa-star\" data-rate=\"1\"></i>\n        <i class=\"far fa-star\" data-rate=\"2\"></i>\n        <i class=\"far fa-star\" data-rate=\"3\"></i>\n        <i class=\"far fa-star\" data-rate=\"4\"></i>\n        <i class=\"far fa-star\" data-rate=\"5\"></i>\n        </div>\n        </div>\n        </div>'''\n\n\n        if user_rating_for_history_chat:\n            rating_parts = llm_rating.split(\"far\", user_rating_for_history_chat)\n            if len(rating_parts) <= user_rating_for_history_chat:\n                llm_rating = \"fas\".join(rating_parts)\n            else:\n                llm_rating = \"fas\".join(rating_parts[:-1]) + \"fas\" + \"far\".join(rating_parts[-1:])\n\n        llm_response += llm_rating\n\n        if len(result_parts) > 1:\n            llm_response += result_parts[1]\n            llm_response += \"</div>\"\n            llm_response = llm_response.strip('\\n')\n            llm_response = llm_response.replace('\\n\\n', '<br><br>')\n            llm_response = llm_response.replace('\\n', '<br>')\n\n        chat_history.append(llm_response)\n\n        # Increment sequence ID for next iteration:\n        sequence_id_for_history_search += 1\n\n        # But first, check to see if next sequence exists!\n        try:\n            c.execute(\"SELECT EXISTS(SELECT 1 FROM chat_history WHERE chat_id = ? AND sequence_id = ?)\", (int(chat_id_for_history_search), int(sequence_id_for_history_search)))\n            exists = c.fetchone()[0]\n        except Exception as e:\n            return handle_api_error(\"Could not determine if next sequence exists in chat history DB, encountered error: \", e)\n            \n        if not exists:\n            SEQUENCE_ID = sequence_id_for_history_search - 1\n            retrieve_history = False\n            try:\n                c.execute(\"SELECT llm_model FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                old_chat_model = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not determine previously used LLM in chat, encountered error: \", e)\n            try:\n                c.execute(\"SELECT history_summary FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (CHAT_ID, SEQUENCE_ID))\n                result = c.fetchone()\n                history_summary_dict = str(result[0])\n            except Exception as e:\n                handle_error_no_return(\"Could not fetch history summary of last chat, encountered error: \", e)\n            c.close()\n\n    # Convert History Summary and add a new key indicating it was recently cleared!\n    if history_summary_dict is not None and history_summary_dict != \"\" and history_summary_dict != 'None':\n        print(f\"\\n\\history_summary_dict string from old chat: {history_summary_dict}\\n\\n\")\n        try:\n            HISTORY_SUMMARY = ast.literal_eval(history_summary_dict)    #cast as dictionary\n            HISTORY_SUMMARY[\"has_been_reset\"] = True\n        except Exception as e:\n            handle_error_no_return(\"Could not cast history summary string from DB to dict and/or set has_been_reset boolean, encountered error: \", e)\n\n    # Temp prints:\n    # print(f\"\\n\\nHISTORY_SUMMARY: {HISTORY_SUMMARY}\\n\\n\")\n    # print(f\"\\n\\history_summary_dict: {history_summary_dict}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.summary: {HISTORY_MEMORY_WITH_BUFFER.summary}\\n\\n\")\n    # print(f\"\\n\\nHISTORY_MEMORY_WITH_BUFFER.chat_memory.messages: {HISTORY_MEMORY_WITH_BUFFER.chat_memory.messages}\\n\\n\")\n    print(f'\\n\\nChat history loaded for chat with model: {old_chat_model}\\n\\n')\n\n    return jsonify({'success': True, 'chat_history': chat_history, 'old_chat_model': old_chat_model})\n\n\n@app.route('/init_chat_history_db')\ndef init_chat_history_db():\n\n    global CHAT_ID\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        return handle_api_error(\"Missing sqlite_history_db in config.json in method init_chat_history_db. Error: \", e)\n\n    # Connect to chat_history.db to determine appropriate chat_id\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        c = conn.cursor()\n    except Exception as e:\n        return handle_api_error(\"Could not connect to chat history database, encountered error: \", e)\n\n    # If the database does not currently exist...\n    try:\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS chat_history (\n                        id INTEGER PRIMARY KEY,\n                        chat_id INTEGER,\n                        sequence_id INTEGER,\n                        user_query TEXT,\n                        llm_response TEXT,\n                        user_rating INTEGER,\n                        llm_model TEXT, \n                        prompt_template TEXT,\n                        history_summary TEXT\n            )\n        ''')\n        conn.commit()\n    except Exception as e:\n        return handle_api_error(\"Could not create new chat history db, encountered error: \", e)\n\n    try:\n        c.execute(\"SELECT COALESCE(MAX(chat_id), 0) FROM chat_history\")\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for an empty DB!\n\n        result = c.fetchone()\n\n        # 'result' will be a tuple, so extract the first element\n        max_chat_id = result[0]\n\n        new_chat_id = max_chat_id + 1\n        CHAT_ID = new_chat_id\n\n        print(f\"Chat history DB initialised with CHAT_ID: {CHAT_ID}\")\n    except Exception as e:\n        return handle_api_error(\"Could not set CHAT_ID, encountered error: \", e)\n\n    conn.close()\n\n    return jsonify({'success': True, 'chat_id': CHAT_ID})\n\n\ndef fetch_image_from_db(image_id):\n\n    try:\n        read_return = read_config(['sqlite_images_db'])\n        sqlite_images_db = read_return['sqlite_images_db']\n    except Exception as e:\n        handle_local_error(\"Missing sqlite_history_db in config.json in method fetch_image_from_db. Error: \", e)\n    \n    # 1 - Connect to DB\n    try:\n        conn = sqlite3.connect(sqlite_images_db)\n    except Exception as e:\n        handle_local_error(\"Could not connect to images database, encountered error: \", e)\n    \n    # 2 - Get Images\n    try:\n        conn.row_factory = sqlite3.Row\n        row = conn.execute('SELECT image_data FROM images WHERE id = ?', (image_id,)).fetchone()\n        images_bytes = row['image_data'] if row else None\n    except Exception as e:\n        handle_local_error(\"Could not fetch image from DB, encountered error: \", e)\n    \n    conn.close()\n    \n    return images_bytes\n\n\n@app.route('/image_display/<int:image_id>')\ndef image_display(image_id):\n    print(f\"\\n\\nprepping image for display: {image_id}\\n\\n\")\n\n    try: \n        image_bytes = fetch_image_from_db(image_id)\n    except Exception as e:\n        handle_local_error(\"Could not fetch image for display, encountered error: \", e)\n    \n    try:\n        encoded = base64.b64encode(image_bytes).decode('utf-8')\n    except Exception as e:\n        handle_local_error(\"Could not encode image for display URI, encountered error: \", e)\n\n    # Return an HTML response with the embedded image:\n    data_uri = f\"data:image/jpeg;base64,{encoded}\"\n    html_content = f'<img src=\"{data_uri}\" alt=\"Image\">'\n\n    return html_content\n\n\ndef extract_significant_phrases(query):\n    print(\"Extracting significant phrases\")\n\n    try:\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n    except Exception as e:\n        handle_error_no_return(\"Failed to download & set stopwords, encountered error: \", e)\n    \n    try:\n        tokens = [token for token in query.lower().split() if token not in stop_words]\n    except Exception as e:\n        handle_local_error(\"Could not extract significant tokens, encountered error: \", e)\n\n    print(f\"\\nReturning tokens: {tokens}\\n\")\n    return tokens\n\n\ndef calculate_relevance_score(phrases, document_content):\n    #print(\"calculating relevance score\")\n    \n    try:\n        content_lower = document_content.lower()\n    except Exception as e:\n        handle_local_error(\"Could not read document_content in calculate_relevance_score(), encountered error: \", e)\n    \n    #print(f\"document content: {content_lower}\")\n    \n    #score = sum(1 for phrase in phrases if phrase in content_lower)\n    \n    score = 0\n    try:\n        for phrase in phrases:\n            if phrase in content_lower:\n                print(f\"Match found to enable RAG: {phrase}\")\n                score += 1\n    except Exception as e:\n        handle_local_error(\"Could not compare phrases in calculate_relevance_score(), encountered error: \", e)\n    \n    return score\n\n\ndef filter_relevant_documents(query, search_results, threshold=1):\n\n    print(\"Checking relevant docs to determin if RAG is required\")\n\n    do_rag = False\n    page_contents = []\n\n    try:\n        significant_phrases = extract_significant_phrases(query)\n    except Exception as e:\n        handle_local_error(\"Could not extract significant phrases, encountered error: \", e)\n    \n    print(f\"significant tokens: {significant_phrases}\")\n    #relevant_documents = []\n\n    try:\n        for document in search_results:\n            # check for non-empty source field\n            if document.page_content:\n                page_contents.append(document.page_content)\n\n            if not do_rag:  # if do_rag has already been set to true, why look?\n                if document.metadata.get('source'):\n                    score = calculate_relevance_score(significant_phrases, document.page_content)\n                    if score >= threshold:\n                        #relevant_documents.append(document)\n                        print(\"Must do RAG!\")\n                        do_rag = True\n    except Exception as e:\n        handle_local_error(\"Could not read calculate relevance score, encountered error: \", e)\n\n    #return relevant_documents\n    return page_contents, do_rag\n\n\ndef rerank_results_ml(query, documents, top_n=5):\n    # Load pre-trained SBERT model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    # Encode the query\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    \n    # Encode the documents\n    doc_embeddings = model.encode([doc.page_content for doc in documents], convert_to_tensor=True)\n    \n    # Compute cosine similarities\n    cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]\n    \n    # Create a list of (index, score) tuples\n    indexed_scores = list(enumerate(cosine_scores))\n    \n    # Sort by score in descending order\n    sorted_indexes = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n    \n    # Reorder the original documents based on the sorted indexes\n    ranked_documents = [documents[idx] for idx, _ in sorted_indexes[:top_n]]\n    \n    return ranked_documents\n\n\ndef determine_do_rag(query, docs, force_enable_rag, force_disable_rag):\n\n    print(\"\\n\\nDetermining do_rag \\n\\n\")\n\n    do_rag = False\n    \n    # We do not modify the force_enable_rag or force_disable_rag flags in this method, we simply respond to them here. UI updates should handle those flags.\n    if force_enable_rag:\n        print(\"\\n\\nFORCE_ENABLE_RAG True, force enabling RAG and returning\\n\\n\")\n        try:\n            do_rag = True\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"Error force-enabling RAG, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n    elif force_disable_rag:\n        print(\"\\n\\nFORCE_DISABLE_RAG True, force disabling RAG and returning\\n\\n\")\n        do_rag = False\n    else:\n        try:\n            _, do_rag = filter_relevant_documents(query, docs)\n        except Exception as e:\n            do_rag = False\n            handle_error_no_return(\"RAG Error, disabling RAG and continuing: could not filter_relevant_documents during setup_for_streaming_response, encountered error: \", e)\n\n    return do_rag\n\n\ndef format_prompt_from_history(chat_id, sequence_id):\n\n    formatted_prompt = \"\"\n\n    try:\n        read_return = read_config(['sqlite_history_db'])\n        sqlite_history_db = read_return['sqlite_history_db']\n    except Exception as e:\n        handle_error_no_return(\"Missing keys in config.json for method store_chat_history_to_db. Error: \", e)\n\n    # Connect to or create the DB\n    try:\n        conn = sqlite3.connect(sqlite_history_db)\n        cursor = conn.cursor()\n    except Exception as e:\n        handle_error_no_return(\"Could not establish connection to DB for chat history storage, encountered error: \", e)\n\n    try:\n        # Determine sequence_id\n        cursor.execute(\"SELECT prompt_template FROM chat_history WHERE chat_id = ? AND sequence_id = ?\", (int(chat_id), int(sequence_id)))\n        # \"The COALESCE function accepts two or more arguments and returns the first non-null argument.\"\n        # This accounts for a new chat!\n        # Note that trailing comma! Without it, the simple select query will produce an error: \"parameters are of unsupported type\" !!\n        # This is because the SQLite3 module can have trouble recognizing single-item tuples as tuples, so a trailing comma helps alleviate this! \n\n        result = cursor.fetchone()\n        formatted_prompt = str(result[0])\n        \n    except Exception as e:\n        handle_error_no_return(\"Could not determine sequence ID for storage to chat history DB, encountered error: \", e)\n\n    return formatted_prompt\n\n\ndef format_prompt_for_llama_cpp(formatted_prompt, user_query, current_sequence_id, base_template, local_llm_chat_template_format):\n\n    print(\"\\n\\nFormatting prompt for llama-cpp\\n\\n\")\n\n    if local_llm_chat_template_format == 'llama3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        else:\n            formatted_prompt += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{base_template}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    elif local_llm_chat_template_format == 'llama2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<s>[INST] {user_query} [/INST]\"\n        else:\n            formatted_prompt += f\"<s>[INST] <<SYS>>\\n {base_template} \\n<</SYS>>\\n\\n {user_query}  [/INST]\"\n\n    elif local_llm_chat_template_format == 'chatml':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"\\n<|im_start|>user\\n{user_query}<|im_end|>\\n\"\n        else:\n            formatted_prompt += f\"<|im_start|>system\\n{base_template}<|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\"\n\n    elif local_llm_chat_template_format == 'phi3':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n        else:\n            formatted_prompt += f\"<|system|>\\n{base_template}<|end|>\\n<|user|>\\n{user_query}<|end|>\\n<|assistant|>\\n\"\n\n    elif local_llm_chat_template_format == 'command-r':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        else:\n            formatted_prompt += f\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{base_template}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_query}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n\n    elif local_llm_chat_template_format == 'deepseek':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"### Instruction:\\n{user_query}\\n### Response:\\n\"\n        else:\n            formatted_prompt += f\"{base_template}### Instruction:\\n{user_query}\\n### Response:\\n\"\n\n    elif local_llm_chat_template_format == 'deepseek-coder-v2':\n        \n        if current_sequence_id > 0:\n            formatted_prompt += f\"User: {user_query}\\nAssistant: \"\n        else:\n            formatted_prompt += f\"<|begin_of_sentence|>{base_template}\\nUser: {user_query}\\nAssistant: \"\n\n    elif local_llm_chat_template_format == 'vicuna':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"USER: {user_query}\\nASSISTANT: \"\n        else:\n            formatted_prompt += f\"{base_template}\\n\\nUSER: {user_query}\\nASSISTANT: \"\n\n    elif local_llm_chat_template_format == 'openchat':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n        else:\n            formatted_prompt += f\"<s>GPT4 Correct System: {base_template}<|end_of_turn|>GPT4 Correct User: {user_query}<|end_of_turn|>GPT4 Correct Assistant: \"\n\n    elif local_llm_chat_template_format == 'gemma2':\n\n        if current_sequence_id > 0:\n            formatted_prompt += f\"<start_of_turn>user\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n        else:\n            formatted_prompt += f\"<start_of_turn>user\\n{base_template}\\n{user_query}<end_of_turn>\\n<start_of_turn>model\\n\"\n\n    return formatted_prompt\n\n\ndef format_prompt_for_hf_waitress(formatted_prompt, user_query, current_sequence_id, base_template):\n\n    print(\"\\n\\nFormatting prompt for hf-waitress\\n\\n\")\n\n    # base_template += \" Important: The user will be reading your responses in a web browser, so make sure to use <br> liberally to ensure the user can read your responses easily. When in doubt, add a <br> tag!\"\n\n    if current_sequence_id > 0:\n        history_prompt_json = json.loads(formatted_prompt)\n        new_message = {\"role\":\"user\", \"content\":user_query}\n        history_prompt_json['messages'].append(new_message)\n        updated_history_prompt_json = json.dumps(history_prompt_json, indent=4)\n        formatted_prompt = str(updated_history_prompt_json)\n    else:\n        first_prompt_json = f'''\n        {{\n                \"messages\": [\n                    {{\"role\": \"system\", \"content\": {json.dumps(base_template)}}},\n                    {{\"role\": \"user\", \"content\": {json.dumps(user_query)}}}\n                ]\n            }}\n        '''\n\n        formatted_prompt = str(first_prompt_json)\n\n    return formatted_prompt\n\n\n@app.route('/setup_for_llama_cpp_response', methods=['POST'])\ndef setup_for_llama_cpp_response():\n\n    global QUERIES\n\n    do_rag = True\n    similarity_threshold = 0.8\n\n    stream_session_id = \"\"\n    key_for_vector_results = \"\"\n    # Generate a unique session ID using universally Unique Identifier via the uuid4() method, wherein the randomness of the result is dependent on the randomness of the underlying operating system's random number generator\n    # UUI is a standard used for creating unique strings that have a very high likelihood of being unique across all time and space, for ex: f47ac10b-58cc-4372-a567-0e02b2c3d479\n    try:\n        stream_session_id = str(uuid.uuid4())\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n    except Exception as e:\n        return handle_api_error(\"Error creating unique stream_session_id when attempting to setup_for_streaming_response. Error: \", e)\n    \n\n    # Determine do_rag\n    try:\n        read_return = read_config(['local_llm_server', 'use_sbert_embeddings', 'use_openai_embeddings', 'use_bge_base_embeddings', 'use_bge_large_embeddings', 'force_enable_rag', 'force_disable_rag', 'local_llm_chat_template_format', 'base_template'])\n        use_sbert_embeddings = read_return['use_sbert_embeddings']\n        use_openai_embeddings = read_return['use_openai_embeddings']\n        use_bge_base_embeddings = read_return['use_bge_base_embeddings']\n        use_bge_large_embeddings = read_return['use_bge_large_embeddings']\n        force_enable_rag = read_return['force_enable_rag']\n        force_disable_rag = read_return['force_disable_rag']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n        base_template = read_return['base_template']\n        local_llm_server = read_return['local_llm_server']\n\n    except Exception as e:\n        return handle_api_error(\"Missing values in config.json when attempting to setup_for_streaming_response. Error: \", e)\n\n    try:\n        # Attempt to get query data\n        user_query = request.json['user_query']\n        chat_id = request.json['chat_id']\n\n        # Store the query associated with the ID\n        QUERIES[stream_session_id] = user_query\n    except KeyError:\n        return handle_api_error(\"Could not obtain and/or store user_query in setup_for_streaming_response, encountered error: \", e)\n\n    print(\"chat_id: \", chat_id)\n\n    # Perform similarity search on the vector DB\n    print(\"\\n\\nPerforming similarity search to determine if RAG necessary\\n\\n\")\n    embedding_function = None\n    try:\n        if use_sbert_embeddings:\n            embedding_function=HuggingFaceEmbeddings()\n        elif use_openai_embeddings:\n            embedding_function=AZURE_OPENAI_EMBEDDINGS\n        elif use_bge_base_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n        elif use_bge_large_embeddings:\n            embedding_function=HF_BGE_EMBEDDINGS\n    except Exception as e:\n        handle_error_no_return(\"Could not set embedding_function for similarity_search when attempting to setup_for_streaming_response, encountered error: \", e)\n    \n    try:\n        # docs = VECTOR_STORE.similarity_search(user_query, embedding_fn=embedding_function)\n        # docs_with_relevance_score = VECTOR_STORE.similarity_search_with_relevance_scores(user_query, 10, embedding_fn=embedding_function)\n        docs_list_with_cosine_distance = VECTOR_STORE.similarity_search_with_score(user_query, 11, embedding_fn=embedding_function)\n    except Exception as e:\n        handle_error_no_return(\"Could not perform similarity_search to determine do_rag when attempting to setup_for_streaming_response, encountered error: \", e)\n\n    filtered_docs = [doc for doc, score in docs_list_with_cosine_distance]\n\n    docs = []\n    if filtered_docs:\n        docs = rerank_results_ml(user_query, filtered_docs, top_n=5)\n        do_rag = determine_do_rag(user_query, docs, force_enable_rag, force_disable_rag)\n    \n    print(f'Do RAG? {do_rag}')\n\n    try:\n        write_config({'do_rag':do_rag})\n    except Exception as e:\n        handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n\n    \n    # Having determined do_rag, time to build the prompt template!\n    \n    if do_rag:  # add similarity search results for RAG!\n        try:\n            QUERIES[key_for_vector_results] = docs\n            user_query += f\"\\n\\nThe following context might be helpful in answering the user query above:\\n{docs}\"\n            print(f\"RAG formatted user_query: \\n{user_query}\\n\")\n        except Exception as e:\n            try:\n                write_config({'do_rag':False})\n            except Exception as e:\n                handle_error_no_return(\"Could not write do_rag to config during setup_for_streaming_response, encountered error: \", e)\n            handle_error_no_return(\"RAG Error: Could not update QUERIES dict and user_query during setup_for_streaming_response, proceeding without RAG. Encountered error: \", e)\n\n    current_sequence_id = determine_sequence_id_for_chat(chat_id)\n    formatted_prompt = \"\"\n    print(\"current_sequence_id: \", current_sequence_id)\n    if current_sequence_id > 0:    # get the last prompt so we can continue the completions\n        formatted_prompt = format_prompt_from_history(chat_id, current_sequence_id)\n\n    \n    if formatted_prompt == \"\":  # could not be updated above\n        current_sequence_id = 0 # so reset chat sequence id\n\n    \n    if local_llm_server == 'llama-cpp':\n        formatted_prompt = format_prompt_for_llama_cpp(formatted_prompt, user_query, current_sequence_id, base_template, local_llm_chat_template_format)\n    elif local_llm_server == 'hf-waitress':\n        formatted_prompt = format_prompt_for_hf_waitress(formatted_prompt, user_query, current_sequence_id, base_template)\n\n    print(\"Returning formatted_prompt: \", formatted_prompt)\n\n    # Return a bunch of stuff\n    new_sequence_id = int(current_sequence_id) + 1\n    return jsonify({\"success\": True, \"stream_session_id\": stream_session_id, \"do_rag\": do_rag, \"formatted_user_prompt\": formatted_prompt, \"sequence_id\":new_sequence_id})\n\n\n\n@app.route('/get_references', methods=['POST'])\ndef get_references():\n\n    print(\"\\n\\nGetting References\\n\\n\")\n\n    try:\n        read_return = read_config(['local_llm_server', 'do_rag', 'upload_folder', 'local_llm_chat_template_format'])\n        local_llm_server = read_return['local_llm_server']\n        do_rag = read_return['do_rag']\n        upload_folder = read_return['upload_folder']\n        local_llm_chat_template_format = read_return['local_llm_chat_template_format']\n    except Exception as e:\n        return handle_api_error(\"Missing values in config.json when attempting to get_references. Error: \", e)\n\n    try:\n        stream_session_id = request.json['stream_session_id']\n        user_query = request.json['user_query']\n        llm_response = request.json['llm_response']\n        formatted_user_prompt = request.json['formatted_user_prompt']\n        chat_id = request.json['chat_id']\n        sequence_id = request.json['sequence_id']\n    except Exception as e:\n        return handle_api_error(\"Could not read request content in method get_references, encountered error: \", e)\n\n    if local_llm_server == 'llama-cpp':\n        if local_llm_chat_template_format == 'llama3':\n            formatted_user_prompt += f\"{llm_response}<|eot_id|>\"\n        elif local_llm_chat_template_format == 'llama2':\n            formatted_user_prompt += f\"{llm_response}</s>\"\n        elif local_llm_chat_template_format == 'chatml':\n            formatted_user_prompt += f\"{llm_response}<|im_end|>\\n\"\n        elif local_llm_chat_template_format == 'phi3':\n            formatted_user_prompt += f\"{llm_response}<|end|>\\n\"\n        elif local_llm_chat_template_format == 'command-r':\n            formatted_user_prompt += f\"{llm_response}<|END_OF_TURN_TOKEN|>\"\n        elif local_llm_chat_template_format == 'deepseek':\n            formatted_user_prompt += f\"{llm_response}\\n<|EOT|>\\n\"\n        elif local_llm_chat_template_format == 'deepseek-coder-v2':\n            formatted_user_prompt += f\"{llm_response}<|end_of_sentence|>\"\n        elif local_llm_chat_template_format == 'vicuna':\n            formatted_user_prompt += f\"{llm_response} </s>\\n\"\n        elif local_llm_chat_template_format == 'openchat':\n            formatted_user_prompt += f\"{llm_response}<|end_of_turn|>\"\n        elif local_llm_chat_template_format == 'gemma2':\n            formatted_user_prompt += f\"{llm_response}<end_of_turn>\\n\"\n    elif local_llm_server == 'hf-waitress':\n        history_prompt_json = json.loads(formatted_user_prompt)\n        new_response = {\"role\":\"assistant\", \"content\":llm_response}\n        history_prompt_json['messages'].append(new_response)\n        updated_history_prompt_json = json.dumps(history_prompt_json, indent=4)\n        formatted_user_prompt = str(updated_history_prompt_json)\n\n    if not do_rag:\n        print(\"\\n\\nSkipping RAG, storing chat history and returning\\n\\n\")\n        try:\n            store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query, llm_response, formatted_user_prompt)\n        except Exception as e:\n            handle_error_no_return(\"Could not store_llama_cpp_chat_history_to_db in get_references(), encountered error: \", e)\n        return jsonify({'success': True})\n        \n    try:\n        key_for_vector_results = \"VectorDocsforQueryID_\" + stream_session_id\n        docs = QUERIES[key_for_vector_results]\n    except Exception as e:\n        return handle_api_error(\"Could not obtain relevant data from QUERIES dict, encountered error: \", e)\n\n    # Having obtained the relevant info, clear the QUERIES{} dict so as to not bloat it!\n    try:\n        del QUERIES[key_for_vector_results]\n    except Exception as e:\n        handle_error_no_return(\"Error clearing queries dict in method get_references: \", e)\n\n    reference_response = \"\"\n\n    all_sources = {}\n    reference_pages = {}\n\n    for doc in docs:\n        \n        try:\n            relevant_page_text = str(doc.page_content)\n            relevant_page_number = str(doc.metadata.get('page_number'))\n            source_filepath = str(doc.metadata.get('source'))\n        except Exception as e:\n            handle_error_no_return(\"Could not access doc.page_content and/or doc.metadata, encountered error: \", e)\n            continue\n        \n        relevant_page_text = relevant_page_text.replace('\\n', ' ')\n\n        # relevant_page_text = relevant_page_text.split('\\n', 1)[0]\n        # relevant_page_text = relevant_page_text.strip()\n        # relevant_page_text = re.sub(r'[\\W_]+Page \\d+[\\W_]+', '', relevant_page_text)\n        \n        try:\n            source_filename = os.path.basename(source_filepath)\n            _, file_extension = os.path.splitext(source_filepath)\n        except Exception as e:\n            handle_error_no_return(\"Could not parse path with OS lib, encountered error: \", e)\n            continue\n\n        # The source_filepath will likely always reference a TXT file because of how we're loading the VectorDB!\n        # Check if the PDF version of the source doc exists\n        if file_extension == '.txt':\n\n            #print(\"\\n\\ntxt file\\n\\n\")\n\n            # Construct the path to the potential PDF version\n            pdf_version_path = os.path.join(upload_folder, os.path.basename(source_filepath).replace('.txt', '.pdf'))   # not catching an error here as os.path.basename(source_filepath) has already been caught just above!\n\n            # Check if PDF version of the source TXT exists!\n            if os.path.exists(pdf_version_path):\n\n                #print(\"\\n\\pdf exists\\n\\n\")\n\n                source_filename = source_filename.replace('.txt', '.pdf')\n                \n                if pdf_version_path in reference_pages:\n                    reference_pages[pdf_version_path].extend([[relevant_page_text,relevant_page_number]])\n                    #reference_pages[pdf_version_path].extend({'page_content': relevant_page_text, 'page_number': relevant_page_number})\n                else:\n                    reference_pages[pdf_version_path] = [[relevant_page_text,relevant_page_number]]\n                    #reference_pages[pdf_version_path] = {'page_content': relevant_page_text, 'page_number': relevant_page_number}\n\n                # Add this file to our sources dictionary if it's not already present\n                if source_filename not in all_sources:\n                    source_filepath = pdf_version_path\n                    all_sources.update({source_filename: source_filepath})\n\n            # Else PDF does not exist, TXT is the source\n            else:\n                print(\"\\n\\nNo PDF source doc found in the 'uploaded_pdfs' dir, RAG ACTIVE BUT REFERENCING WILL NOT DISPLAY!\\n\\n\")\n                # Check if the TXT is already in the sources dict\n                if source_filename not in all_sources:\n                    try:\n                        source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                        all_sources.update({source_filename: source_filepath})\n                    except Exception as e:\n                        handle_error_no_return(\"Could not construct filepath for TXT file, encountered error: \", e)\n\n\n        # If file is not a TXT file\n        else:\n            # Check if the TXT is already in the sources dict\n            if source_filename not in all_sources:\n                try:\n                    source_filepath = os.path.join(upload_folder, source_filename) # reconstructed path using the OS module just to be safe\n                    all_sources.update({source_filename: source_filepath})\n                except Exception as e:\n                    handle_error_no_return(\"Could not construct filepath for non-TXT file, encountered error: \", e)\n\n\n    try:\n        docs_have_relevant_info, user_should_refer_pages_in_doc = highlighter_interface(reference_pages, stream_session_id)\n    except Exception as e:\n        handle_error_no_return(\"Could not complete highlighter_interface, encountered error: \", e)\n\n    matched_images_found = False\n    # try:\n    #     matched_images_found, matched_images_in_bytes = find_images_in_db(reference_pages)\n    # except Exception as e:\n    #     handle_error_no_return(\"Could not search for images, encountered error: \", e)\n\n    refer_pages_string = \"\"\n    download_link_html = \"\"\n    images_iframe_html = \"\"\n    matched_images_found = False    #temp over-ride\n\n    if docs_have_relevant_info:\n\n        refer_pages_string = \"<br><h6>Additional data may be found in the following documents & pages:</h6>\"\n        \n        for index, doc in enumerate(user_should_refer_pages_in_doc, start=1):\n            pdf_iframe_id = f\"stream{stream_session_id}PdfViewer{str(index)}\"\n            tab_name_string = f\"stream{stream_session_id}tabName{str(index)}\"\n            frame_doc_path = f\"/pdf/{doc}\"\n            try:\n                stream_id_string_to_remove = f\"_{stream_session_id}\"\n                doc_name_without_stream_id = str(doc).replace(stream_id_string_to_remove, \"\")\n                refer_pages_string += f\"<br><h6>{doc_name_without_stream_id}: \"\n                for page in user_should_refer_pages_in_doc[doc]:\n                    frame_doc_path += f\"#page={str(page)}\" \n                    refer_pages_string += f'<a href=\"javascript:void(0)\" onclick=\"goToPageAndSwitchTab(\\'{pdf_iframe_id}\\', \\'{frame_doc_path}\\', \\'tab{tab_name_string}\\', \\'{stream_session_id}\\')\">Page {page}</a>, '\n                    frame_doc_path = f\"/pdf/{doc}\"\n                refer_pages_string = refer_pages_string.strip(', ') + \"</h6>\"\n            except Exception as e:\n                handle_error_no_return(\"Could not construct refer_pages_string, encountered error: \", e)\n\n        pdf_right_pane_id = f\"stream{stream_session_id}PdfPane\"\n        download_link_html = f'<div class=\"pdf-viewer-container\" id=\"{pdf_right_pane_id}\">'\n\n        # Add tab buttons\n        download_link_html += '<div class=\"tab-buttons\">'\n        for index, source in enumerate(user_should_refer_pages_in_doc, start=1):\n            tab_name_string = f\"stream{stream_session_id}tabName{str(index)}\"\n            stream_id_string_to_remove = f\"_{stream_session_id}\"\n            doc_name_without_stream_id = str(source).replace(stream_id_string_to_remove, \"\")\n            default_open = ' defaultTabs' if index == 1 else ''\n            download_link_html += f'<button class=\"tab-button{default_open}\" stream-session-id=\"{stream_session_id}\" onclick=\"openTab(event, \\'tab{tab_name_string}\\', \\'{stream_session_id}\\')\">{doc_name_without_stream_id}</button>'\n        download_link_html += '</div>'\n\n        # Add tab content\n        for index, source in enumerate(user_should_refer_pages_in_doc, start=1):\n            try:\n                download_link_url = url_for('download_file', filename=source)\n                pdf_iframe_id = f\"stream{stream_session_id}PdfViewer{str(index)}\"\n                tab_name_string = f\"stream{stream_session_id}tabName{str(index)}\"\n                download_link_html += f'<div id=\"tab{tab_name_string}\" class=\"tab-content\" stream-session-id=\"{stream_session_id}\">'\n                download_link_html += f'<iframe id=\"{pdf_iframe_id}\" src=\"{download_link_url}\" width=\"100%\" height=\"600\"></iframe>'\n                download_link_html += \"</div>\"\n            except Exception as e:\n                handle_error_no_return(\"Could not construct download_link_html, encountered error: \", e)\n\n        download_link_html += \"</div>\"\n\n    if matched_images_found:\n        image_gallery_id = f\"image_gallery_for_stream_{stream_session_id}\"\n        images_iframe_html = f'''\n        <h6>Browse a gallery of relevant images by clicking on the thumbnail below:</h6>\n        <i class=\"fas fa-images thumbnail-icon\" onclick=\"openImageGalleryModal('{image_gallery_id}')\"></i>\n        <div id=\"{image_gallery_id}\" class=\"image-gallery-modal\">\n        <span class=\"image-gallery-close\" onclick=\"closeImageGalleryModal('{image_gallery_id}')\">&times;</span>\n        <div class=\"image-gallery-content\">\n        '''\n        for image_id, image_bytes_data in matched_images_in_bytes:\n            #print(f\"\\n\\nmatched image id: {image_id}\")\n            try:\n                image_link_url = url_for('image_display', image_id=image_id)\n                images_iframe_html += f'<iframe src=\"{image_link_url}\" frameborder=\"0\" class=\"gallery-thumbnail\"></iframe>'\n            except Exception as e:\n                handle_error_no_return(\"Could not construct images_iframe_html, encountered error: \", e)\n        \n        images_iframe_html += f'</div></div>'\n\n    \n    # reference_response = refer_pages_string + download_link_html + images_iframe_html\n    reference_response = refer_pages_string + images_iframe_html\n\n    try:\n        # model_response_for_history_db = str(llm_response) + refer_pages_string\n        model_response_for_history_db = str(llm_response)\n        model_response_for_history_db += f\"\\n\\n{reference_response}\"\n        model_response_for_history_db += f\"\\n\\npdf_pane_data={download_link_html}\"\n        model_response_for_history_db = model_response_for_history_db.strip('\\n')\n\n        formatted_user_query = str(user_query).strip('\\n')\n\n        user_query_for_history_db = formatted_user_query\n    except Exception as e:\n        handle_error_no_return(\"Could not prep data to store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    try:\n        store_llama_cpp_chat_history_to_db(chat_id, sequence_id, user_query_for_history_db, model_response_for_history_db, formatted_user_prompt)\n    except Exception as e:\n        handle_error_no_return(\"Could not store_chat_history_to_db in get_references(), encountered error: \", e)\n\n    return jsonify({'success': True, 'response': reference_response, 'pdf_frame':download_link_html})\n\n\nif __name__ == '__main__':\n    # app.run(debug=True)\n    app.run(host='0.0.0.0', port=5000)"}
