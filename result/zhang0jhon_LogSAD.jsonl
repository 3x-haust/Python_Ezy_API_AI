{"repo_info": {"repo_name": "LogSAD", "repo_owner": "zhang0jhon", "repo_url": "https://github.com/zhang0jhon/LogSAD"}}
{"type": "source_file", "path": "compute_coreset.py", "content": "\"\"\"Sample evaluation script for track 2.\"\"\"\n\nimport argparse\nimport importlib\nimport importlib.util\n\nimport torch\nimport logging\nfrom torch import nn\n\n# NOTE: The following MVTecLoco import is not available in anomalib v1.0.1.\n# It will be available in v1.1.0 which will be released on April 29th, 2024.\n# If you are using an earlier version of anomalib, you could install anomalib\n# from the anomalib source code from the following branch:\n# https://github.com/openvinotoolkit/anomalib/tree/feature/mvtec-loco\nfrom anomalib.data import MVTecLoco\nfrom anomalib.metrics.f1_max import F1Max\nfrom anomalib.metrics.auroc import AUROC\nfrom tabulate import tabulate\nimport numpy as np\n\n# FEW_SHOT_SAMPLES = [0, 1, 2, 3]\n\ndef parse_args() -> argparse.Namespace:\n    \"\"\"Parse command line arguments.\n\n    Returns:\n        argparse.Namespace: Parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--module_path\", type=str, required=True)\n    parser.add_argument(\"--class_name\", default='MyModel', type=str, required=False)\n    parser.add_argument(\"--weights_path\", type=str, required=False)\n    parser.add_argument(\"--dataset_path\", default='/home/bhu/Project/datasets/mvtec_loco_anomaly_detection/', type=str, required=False)\n    parser.add_argument(\"--category\", type=str, required=True)\n    parser.add_argument(\"--viz\", action='store_true', default=False)\n    return parser.parse_args()\n\n\ndef load_model(module_path: str, class_name: str, weights_path: str) -> nn.Module:\n    \"\"\"Load model.\n\n    Args:\n        module_path (str): Path to the module containing the model class.\n        class_name (str): Name of the model class.\n        weights_path (str): Path to the model weights.\n\n    Returns:\n        nn.Module: Loaded model.\n    \"\"\"\n    # get model class\n    model_class = getattr(importlib.import_module(module_path), class_name)\n    # instantiate model\n    model = model_class()\n    # load weights\n    if weights_path:\n        model.load_state_dict(torch.load(weights_path))\n    return model\n\n\ndef run(module_path: str, class_name: str, weights_path: str, dataset_path: str, category: str, viz: bool) -> None:\n    \"\"\"Run the evaluation script.\n\n    Args:\n        module_path (str): Path to the module containing the model class.\n        class_name (str): Name of the model class.\n        weights_path (str): Path to the model weights.\n        dataset_path (str): Path to the dataset.\n        category (str): Category of the dataset.\n    \"\"\"\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    # Instantiate model class here\n    # Load the model here from checkpoint.\n    model = load_model(module_path, class_name, weights_path)\n    model.to(device)\n\n    # Create the dataset\n    datamodule = MVTecLoco(root=dataset_path, eval_batch_size=1, image_size=(448, 448), category=category)\n    datamodule.setup()\n\n    model.set_viz(viz)\n    model.set_save_coreset_features(True)\n    \n\n    FEW_SHOT_SAMPLES = range(len(datamodule.train_data)) # traverse all dataset to build coreset\n    \n    # pass few-shot images and dataset category to model\n    setup_data = {\n        \"few_shot_samples\": torch.stack([datamodule.train_data[idx][\"image\"] for idx in FEW_SHOT_SAMPLES]).to(device),\n        \"few_shot_samples_path\": [datamodule.train_data[idx][\"image_path\"] for idx in FEW_SHOT_SAMPLES],\n        \"dataset_category\": category,\n    }\n    model.setup(setup_data)\n    \n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run(args.module_path, args.class_name, args.weights_path, args.dataset_path, args.category, args.viz)\n"}
{"type": "source_file", "path": "dinov2/dinov2/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\n__version__ = \"0.0.1\"\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/augmentations.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport logging\n\nfrom torchvision import transforms\n\nfrom .transforms import (\n    GaussianBlur,\n    make_normalize_transform,\n)\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_crops_size = global_crops_size\n        self.local_crops_size = local_crops_size\n\n        logger.info(\"###################################\")\n        logger.info(\"Using data augmentation parameters:\")\n        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n        logger.info(f\"local_crops_number: {local_crops_number}\")\n        logger.info(f\"global_crops_size: {global_crops_size}\")\n        logger.info(f\"local_crops_size: {local_crops_size}\")\n        logger.info(\"###################################\")\n\n        # random resized crop and flip\n        self.geometric_augmentation_global = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        self.geometric_augmentation_local = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n                ),\n                transforms.RandomHorizontalFlip(p=0.5),\n            ]\n        )\n\n        # color distorsions / blurring\n        color_jittering = transforms.Compose(\n            [\n                transforms.RandomApply(\n                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                    p=0.8,\n                ),\n                transforms.RandomGrayscale(p=0.2),\n            ]\n        )\n\n        global_transfo1_extra = GaussianBlur(p=1.0)\n\n        global_transfo2_extra = transforms.Compose(\n            [\n                GaussianBlur(p=0.1),\n                transforms.RandomSolarize(threshold=128, p=0.2),\n            ]\n        )\n\n        local_transfo_extra = GaussianBlur(p=0.5)\n\n        # normalization\n        self.normalize = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                make_normalize_transform(),\n            ]\n        )\n\n        self.global_transfo1 = transforms.Compose([color_jittering, global_transfo1_extra, self.normalize])\n        self.global_transfo2 = transforms.Compose([color_jittering, global_transfo2_extra, self.normalize])\n        self.local_transfo = transforms.Compose([color_jittering, local_transfo_extra, self.normalize])\n\n    def __call__(self, image):\n        output = {}\n\n        # global crops:\n        im1_base = self.geometric_augmentation_global(image)\n        global_crop_1 = self.global_transfo1(im1_base)\n\n        im2_base = self.geometric_augmentation_global(image)\n        global_crop_2 = self.global_transfo2(im2_base)\n\n        output[\"global_crops\"] = [global_crop_1, global_crop_2]\n\n        # global crops for teacher:\n        output[\"global_crops_teacher\"] = [global_crop_1, global_crop_2]\n\n        # local crops:\n        local_crops = [\n            self.local_transfo(self.geometric_augmentation_local(image)) for _ in range(self.local_crops_number)\n        ]\n        output[\"local_crops\"] = local_crops\n        output[\"offsets\"] = ()\n\n        return output\n"}
{"type": "source_file", "path": "dinov2/dinov2/configs/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport pathlib\n\nfrom omegaconf import OmegaConf\n\n\ndef load_config(config_name: str):\n    config_filename = config_name + \".yaml\"\n    return OmegaConf.load(pathlib.Path(__file__).parent.resolve() / config_filename)\n\n\ndinov2_default_config = load_config(\"ssl_default_config\")\n\n\ndef load_and_merge_config(config_name: str):\n    default_config = OmegaConf.create(dinov2_default_config)\n    loaded_config = load_config(config_name)\n    return OmegaConf.merge(default_config, loaded_config)\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .adapters import DatasetWithEnumeratedTargets\nfrom .loaders import make_data_loader, make_dataset, SamplerType\nfrom .collate import collate_data_and_cast\nfrom .masking import MaskingGenerator\nfrom .augmentations import DataAugmentationDINO\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/masking.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport random\nimport math\nimport numpy as np\n\n\nclass MaskingGenerator:\n    def __init__(\n        self,\n        input_size,\n        num_masking_patches=None,\n        min_num_patches=4,\n        max_num_patches=None,\n        min_aspect=0.3,\n        max_aspect=None,\n    ):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size,) * 2\n        self.height, self.width = input_size\n\n        self.num_patches = self.height * self.width\n        self.num_masking_patches = num_masking_patches\n\n        self.min_num_patches = min_num_patches\n        self.max_num_patches = num_masking_patches if max_num_patches is None else max_num_patches\n\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n\n    def __repr__(self):\n        repr_str = \"Generator(%d, %d -> [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n            self.height,\n            self.width,\n            self.min_num_patches,\n            self.max_num_patches,\n            self.num_masking_patches,\n            self.log_aspect_ratio[0],\n            self.log_aspect_ratio[1],\n        )\n        return repr_str\n\n    def get_shape(self):\n        return self.height, self.width\n\n    def _mask(self, mask, max_mask_patches):\n        delta = 0\n        for _ in range(10):\n            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w < self.width and h < self.height:\n                top = random.randint(0, self.height - h)\n                left = random.randint(0, self.width - w)\n\n                num_masked = mask[top : top + h, left : left + w].sum()\n                # Overlap\n                if 0 < h * w - num_masked <= max_mask_patches:\n                    for i in range(top, top + h):\n                        for j in range(left, left + w):\n                            if mask[i, j] == 0:\n                                mask[i, j] = 1\n                                delta += 1\n\n                if delta > 0:\n                    break\n        return delta\n\n    def __call__(self, num_masking_patches=0):\n        mask = np.zeros(shape=self.get_shape(), dtype=bool)\n        mask_count = 0\n        while mask_count < num_masking_patches:\n            max_mask_patches = num_masking_patches - mask_count\n            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n\n            delta = self._mask(mask, max_mask_patches)\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n\n        return mask\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/datasets/image_net.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport csv\nfrom enum import Enum\nimport logging\nimport os\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport numpy as np\n\nfrom .extended import ExtendedVisionDataset\n\n\nlogger = logging.getLogger(\"dinov2\")\n_Target = int\n\n\nclass _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"  # NOTE: torchvision does not support the test split\n\n    @property\n    def length(self) -> int:\n        split_lengths = {\n            _Split.TRAIN: 1_281_167,\n            _Split.VAL: 50_000,\n            _Split.TEST: 100_000,\n        }\n        return split_lengths[self]\n\n    def get_dirname(self, class_id: Optional[str] = None) -> str:\n        return self.value if class_id is None else os.path.join(self.value, class_id)\n\n    def get_image_relpath(self, actual_index: int, class_id: Optional[str] = None) -> str:\n        dirname = self.get_dirname(class_id)\n        if self == _Split.TRAIN:\n            basename = f\"{class_id}_{actual_index}\"\n        else:  # self in (_Split.VAL, _Split.TEST):\n            basename = f\"ILSVRC2012_{self.value}_{actual_index:08d}\"\n        return os.path.join(dirname, basename + \".JPEG\")\n\n    def parse_image_relpath(self, image_relpath: str) -> Tuple[str, int]:\n        assert self != _Split.TEST\n        dirname, filename = os.path.split(image_relpath)\n        class_id = os.path.split(dirname)[-1]\n        basename, _ = os.path.splitext(filename)\n        actual_index = int(basename.split(\"_\")[-1])\n        return class_id, actual_index\n\n\nclass ImageNet(ExtendedVisionDataset):\n    Target = Union[_Target]\n    Split = Union[_Split]\n\n    def __init__(\n        self,\n        *,\n        split: \"ImageNet.Split\",\n        root: str,\n        extra: str,\n        transforms: Optional[Callable] = None,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n    ) -> None:\n        super().__init__(root, transforms, transform, target_transform)\n        self._extra_root = extra\n        self._split = split\n\n        self._entries = None\n        self._class_ids = None\n        self._class_names = None\n\n    @property\n    def split(self) -> \"ImageNet.Split\":\n        return self._split\n\n    def _get_extra_full_path(self, extra_path: str) -> str:\n        return os.path.join(self._extra_root, extra_path)\n\n    def _load_extra(self, extra_path: str) -> np.ndarray:\n        extra_full_path = self._get_extra_full_path(extra_path)\n        return np.load(extra_full_path, mmap_mode=\"r\")\n\n    def _save_extra(self, extra_array: np.ndarray, extra_path: str) -> None:\n        extra_full_path = self._get_extra_full_path(extra_path)\n        os.makedirs(self._extra_root, exist_ok=True)\n        np.save(extra_full_path, extra_array)\n\n    @property\n    def _entries_path(self) -> str:\n        return f\"entries-{self._split.value.upper()}.npy\"\n\n    @property\n    def _class_ids_path(self) -> str:\n        return f\"class-ids-{self._split.value.upper()}.npy\"\n\n    @property\n    def _class_names_path(self) -> str:\n        return f\"class-names-{self._split.value.upper()}.npy\"\n\n    def _get_entries(self) -> np.ndarray:\n        if self._entries is None:\n            self._entries = self._load_extra(self._entries_path)\n        assert self._entries is not None\n        return self._entries\n\n    def _get_class_ids(self) -> np.ndarray:\n        if self._split == _Split.TEST:\n            assert False, \"Class IDs are not available in TEST split\"\n        if self._class_ids is None:\n            self._class_ids = self._load_extra(self._class_ids_path)\n        assert self._class_ids is not None\n        return self._class_ids\n\n    def _get_class_names(self) -> np.ndarray:\n        if self._split == _Split.TEST:\n            assert False, \"Class names are not available in TEST split\"\n        if self._class_names is None:\n            self._class_names = self._load_extra(self._class_names_path)\n        assert self._class_names is not None\n        return self._class_names\n\n    def find_class_id(self, class_index: int) -> str:\n        class_ids = self._get_class_ids()\n        return str(class_ids[class_index])\n\n    def find_class_name(self, class_index: int) -> str:\n        class_names = self._get_class_names()\n        return str(class_names[class_index])\n\n    def get_image_data(self, index: int) -> bytes:\n        entries = self._get_entries()\n        actual_index = entries[index][\"actual_index\"]\n\n        class_id = self.get_class_id(index)\n\n        image_relpath = self.split.get_image_relpath(actual_index, class_id)\n        image_full_path = os.path.join(self.root, image_relpath)\n        with open(image_full_path, mode=\"rb\") as f:\n            image_data = f.read()\n        return image_data\n\n    def get_target(self, index: int) -> Optional[Target]:\n        entries = self._get_entries()\n        class_index = entries[index][\"class_index\"]\n        return None if self.split == _Split.TEST else int(class_index)\n\n    def get_targets(self) -> Optional[np.ndarray]:\n        entries = self._get_entries()\n        return None if self.split == _Split.TEST else entries[\"class_index\"]\n\n    def get_class_id(self, index: int) -> Optional[str]:\n        entries = self._get_entries()\n        class_id = entries[index][\"class_id\"]\n        return None if self.split == _Split.TEST else str(class_id)\n\n    def get_class_name(self, index: int) -> Optional[str]:\n        entries = self._get_entries()\n        class_name = entries[index][\"class_name\"]\n        return None if self.split == _Split.TEST else str(class_name)\n\n    def __len__(self) -> int:\n        entries = self._get_entries()\n        assert len(entries) == self.split.length\n        return len(entries)\n\n    def _load_labels(self, labels_path: str) -> List[Tuple[str, str]]:\n        labels_full_path = os.path.join(self.root, labels_path)\n        labels = []\n\n        try:\n            with open(labels_full_path, \"r\") as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    class_id, class_name = row\n                    labels.append((class_id, class_name))\n        except OSError as e:\n            raise RuntimeError(f'can not read labels file \"{labels_full_path}\"') from e\n\n        return labels\n\n    def _dump_entries(self) -> None:\n        split = self.split\n        if split == ImageNet.Split.TEST:\n            dataset = None\n            sample_count = split.length\n            max_class_id_length, max_class_name_length = 0, 0\n        else:\n            labels_path = \"labels.txt\"\n            logger.info(f'loading labels from \"{labels_path}\"')\n            labels = self._load_labels(labels_path)\n\n            # NOTE: Using torchvision ImageFolder for consistency\n            from torchvision.datasets import ImageFolder\n\n            dataset_root = os.path.join(self.root, split.get_dirname())\n            dataset = ImageFolder(dataset_root)\n            sample_count = len(dataset)\n            max_class_id_length, max_class_name_length = -1, -1\n            for sample in dataset.samples:\n                _, class_index = sample\n                class_id, class_name = labels[class_index]\n                max_class_id_length = max(len(class_id), max_class_id_length)\n                max_class_name_length = max(len(class_name), max_class_name_length)\n\n        dtype = np.dtype(\n            [\n                (\"actual_index\", \"<u4\"),\n                (\"class_index\", \"<u4\"),\n                (\"class_id\", f\"U{max_class_id_length}\"),\n                (\"class_name\", f\"U{max_class_name_length}\"),\n            ]\n        )\n        entries_array = np.empty(sample_count, dtype=dtype)\n\n        if split == ImageNet.Split.TEST:\n            old_percent = -1\n            for index in range(sample_count):\n                percent = 100 * (index + 1) // sample_count\n                if percent > old_percent:\n                    logger.info(f\"creating entries: {percent}%\")\n                    old_percent = percent\n\n                actual_index = index + 1\n                class_index = np.uint32(-1)\n                class_id, class_name = \"\", \"\"\n                entries_array[index] = (actual_index, class_index, class_id, class_name)\n        else:\n            class_names = {class_id: class_name for class_id, class_name in labels}\n\n            assert dataset\n            old_percent = -1\n            for index in range(sample_count):\n                percent = 100 * (index + 1) // sample_count\n                if percent > old_percent:\n                    logger.info(f\"creating entries: {percent}%\")\n                    old_percent = percent\n\n                image_full_path, class_index = dataset.samples[index]\n                image_relpath = os.path.relpath(image_full_path, self.root)\n                class_id, actual_index = split.parse_image_relpath(image_relpath)\n                class_name = class_names[class_id]\n                entries_array[index] = (actual_index, class_index, class_id, class_name)\n\n        logger.info(f'saving entries to \"{self._entries_path}\"')\n        self._save_extra(entries_array, self._entries_path)\n\n    def _dump_class_ids_and_names(self) -> None:\n        split = self.split\n        if split == ImageNet.Split.TEST:\n            return\n\n        entries_array = self._load_extra(self._entries_path)\n\n        max_class_id_length, max_class_name_length, max_class_index = -1, -1, -1\n        for entry in entries_array:\n            class_index, class_id, class_name = (\n                entry[\"class_index\"],\n                entry[\"class_id\"],\n                entry[\"class_name\"],\n            )\n            max_class_index = max(int(class_index), max_class_index)\n            max_class_id_length = max(len(str(class_id)), max_class_id_length)\n            max_class_name_length = max(len(str(class_name)), max_class_name_length)\n\n        class_count = max_class_index + 1\n        class_ids_array = np.empty(class_count, dtype=f\"U{max_class_id_length}\")\n        class_names_array = np.empty(class_count, dtype=f\"U{max_class_name_length}\")\n        for entry in entries_array:\n            class_index, class_id, class_name = (\n                entry[\"class_index\"],\n                entry[\"class_id\"],\n                entry[\"class_name\"],\n            )\n            class_ids_array[class_index] = class_id\n            class_names_array[class_index] = class_name\n\n        logger.info(f'saving class IDs to \"{self._class_ids_path}\"')\n        self._save_extra(class_ids_array, self._class_ids_path)\n\n        logger.info(f'saving class names to \"{self._class_names_path}\"')\n        self._save_extra(class_names_array, self._class_names_path)\n\n    def dump_extra(self) -> None:\n        self._dump_entries()\n        self._dump_class_ids_and_names()\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/datasets/decoders.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom io import BytesIO\nfrom typing import Any\n\nfrom PIL import Image\n\n\nclass Decoder:\n    def decode(self) -> Any:\n        raise NotImplementedError\n\n\nclass ImageDataDecoder(Decoder):\n    def __init__(self, image_data: bytes) -> None:\n        self._image_data = image_data\n\n    def decode(self) -> Image:\n        f = BytesIO(self._image_data)\n        return Image.open(f).convert(mode=\"RGB\")\n\n\nclass TargetDecoder(Decoder):\n    def __init__(self, target: Any):\n        self._target = target\n\n    def decode(self) -> Any:\n        return self._target\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/adapters.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Tuple\n\nfrom torch.utils.data import Dataset\n\n\nclass DatasetWithEnumeratedTargets(Dataset):\n    def __init__(self, dataset):\n        self._dataset = dataset\n\n    def get_image_data(self, index: int) -> bytes:\n        return self._dataset.get_image_data(index)\n\n    def get_target(self, index: int) -> Tuple[Any, int]:\n        target = self._dataset.get_target(index)\n        return (index, target)\n\n    def __getitem__(self, index: int) -> Tuple[Any, Tuple[Any, int]]:\n        image, target = self._dataset[index]\n        target = index if target is None else target\n        return image, (index, target)\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/samplers.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport itertools\nfrom typing import Any, Optional\nimport warnings\n\nimport numpy as np\nimport torch\nfrom torch.utils.data.sampler import Sampler\n\nimport dinov2.distributed as distributed\n\n\nclass EpochSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        size: int,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,\n    ):\n        self._size = size\n        self._sample_count = sample_count\n        self._shuffle = shuffle\n        self._seed = seed\n        self._start = distributed.get_global_rank() if start is None else start\n        self._step = distributed.get_global_size() if step is None else step\n        self._epoch = 0\n\n    def __iter__(self):\n        count = (self._size + self._sample_count - 1) // self._sample_count\n        tiled_indices = np.tile(np.arange(self._sample_count), count)\n        if self._shuffle:\n            seed = self._seed * self._epoch if self._seed != 0 else self._epoch\n            rng = np.random.default_rng(seed)\n            iterable = rng.choice(tiled_indices, self._size, replace=False)\n        else:\n            iterable = tiled_indices[: self._size]\n\n        yield from itertools.islice(iterable, self._start, None, self._step)\n\n    def __len__(self):\n        return (self._size - self._start + self._step - 1) // self._step\n\n    def set_epoch(self, epoch):\n        self._epoch = epoch\n\n\ndef _get_numpy_dtype(size: int) -> Any:\n    return np.int32 if size <= 2**31 else np.int64\n\n\ndef _get_torch_dtype(size: int) -> Any:\n    return torch.int32 if size <= 2**31 else torch.int64\n\n\ndef _generate_randperm_indices(*, size: int, generator: torch.Generator):\n    \"\"\"Generate the indices of a random permutation.\"\"\"\n    dtype = _get_torch_dtype(size)\n    # This is actually matching PyTorch's CPU implementation, see: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorFactories.cpp#L900-L921\n    perm = torch.arange(size, dtype=dtype)\n    for i in range(size):\n        j = torch.randint(i, size, size=(1,), generator=generator).item()\n\n        # Always swap even if no-op\n        value = perm[j].item()\n        perm[j] = perm[i].item()\n        perm[i] = value\n        yield value\n\n\nclass InfiniteSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,\n        advance: int = 0,\n    ):\n        self._sample_count = sample_count\n        self._seed = seed\n        self._shuffle = shuffle\n        self._start = distributed.get_global_rank() if start is None else start\n        self._step = distributed.get_global_size() if step is None else step\n        self._advance = advance\n\n    def __iter__(self):\n        if self._shuffle:\n            iterator = self._shuffled_iterator()\n        else:\n            iterator = self._iterator()\n\n        yield from itertools.islice(iterator, self._advance, None)\n\n    def _iterator(self):\n        assert not self._shuffle\n\n        while True:\n            iterable = range(self._sample_count)\n            yield from itertools.islice(iterable, self._start, None, self._step)\n\n    def _shuffled_iterator(self):\n        assert self._shuffle\n\n        # Instantiate a generator here (rather than in the ctor) to keep the class\n        # picklable (requirement of mp.spawn)\n        generator = torch.Generator().manual_seed(self._seed)\n\n        while True:\n            iterable = _generate_randperm_indices(size=self._sample_count, generator=generator)\n            yield from itertools.islice(iterable, self._start, None, self._step)\n\n\n# The following function is somewhat equivalent to _new_shuffle_tensor_slice below,\n# but avoids a full in-place random permutation generation.\ndef _shuffle_tensor_slice(\n    *, tensor: torch.Tensor, start: int = 0, step: int = 1, generator: torch.Generator\n) -> np.ndarray:\n    stop = len(tensor)\n    count = stop // step\n    drop_count = stop - step * count\n    if drop_count:\n        warnings.warn(f\"# of dropped samples: {drop_count}\")\n\n    dtype = _get_numpy_dtype(stop)\n    result = np.empty(count, dtype=dtype)\n\n    for i in range(count):\n        j = torch.randint(0, i + 1, size=(1,), generator=generator).item() if i > 0 else 0\n\n        result[i] = result[j]\n        result[j] = tensor[start + i * step].item()\n\n    return result\n\n\ndef _new_shuffle_tensor_slice(\n    *, tensor: torch.Tensor, start: int = 0, step: int = 1, generator: torch.Generator\n) -> np.ndarray:\n    stop = len(tensor)\n    count = stop // step\n    dtype = torch.int64  # Needed for using randperm result as indices\n    count = stop // step\n    drop_count = stop - step * count\n    if drop_count:\n        warnings.warn(f\"# of dropped samples: {drop_count}\")\n    indices = torch.randperm(count, dtype=dtype, generator=generator)\n    return tensor[start::step][indices].numpy()\n\n\ndef _make_seed(seed: int, start: int, iter_count: int) -> int:\n    # NOTE: Tried a few variants (including iter_count << 32), this one worked best.\n    return seed + start + (iter_count << 24)\n\n\nclass ShardedInfiniteSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,\n        advance: int = 0,\n        use_new_shuffle_tensor_slice: bool = False,\n    ):\n        self._sample_count = sample_count\n        self._seed = seed\n        self._shuffle = shuffle\n        self._start = distributed.get_global_rank() if start is None else start\n        self._step = distributed.get_global_size() if step is None else step\n        self._advance = advance\n        self._iter_count = 0\n        self._shuffle_tensor_slice_fn = (\n            _new_shuffle_tensor_slice if use_new_shuffle_tensor_slice else _shuffle_tensor_slice\n        )\n\n    def __iter__(self):\n        iter_count = self._advance // self._sample_count\n        if iter_count > 0:\n            self._advance -= iter_count * self._sample_count\n            self._iter_count += iter_count\n\n        if self._shuffle:\n            iterator = self._shuffled_iterator()\n        else:\n            iterator = self._iterator()\n\n        yield from itertools.islice(iterator, self._advance, None)\n\n    def _iterator(self):\n        assert not self._shuffle\n\n        while True:\n            iterable = range(self._sample_count)\n            yield from itertools.islice(iterable, self._start, None, self._step)\n\n    def _shuffled_iterator(self):\n        assert self._shuffle\n\n        # Instantiate a generator here (rather than in the ctor) to be keep the class\n        # picklable (requirement of mp.spawn)\n        generator = torch.Generator()\n\n        # Always shuffle everything first\n        generator.manual_seed(self._seed)\n        dtype = _get_torch_dtype(self._sample_count)\n        perm = torch.randperm(self._sample_count, dtype=dtype, generator=generator)\n\n        while True:\n            # Re-seed on each iteration to allow skipping whole permutations\n            seed = _make_seed(self._seed, self._start, self._iter_count)\n            generator.manual_seed(seed)\n\n            iterable = self._shuffle_tensor_slice_fn(\n                tensor=perm, start=self._start, step=self._step, generator=generator\n            )\n            yield from iterable\n            self._iter_count += 1\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/datasets/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .image_net import ImageNet\nfrom .image_net_22k import ImageNet22k\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/datasets/image_net_22k.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom functools import lru_cache\nfrom gzip import GzipFile\nfrom io import BytesIO\nfrom mmap import ACCESS_READ, mmap\nimport os\nfrom typing import Any, Callable, List, Optional, Set, Tuple\nimport warnings\n\nimport numpy as np\n\nfrom .extended import ExtendedVisionDataset\n\n\n_Labels = int\n\n_DEFAULT_MMAP_CACHE_SIZE = 16  # Warning: This can exhaust file descriptors\n\n\n@dataclass\nclass _ClassEntry:\n    block_offset: int\n    maybe_filename: Optional[str] = None\n\n\n@dataclass\nclass _Entry:\n    class_index: int  # noqa: E701\n    start_offset: int\n    end_offset: int\n    filename: str\n\n\nclass _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n\n    @property\n    def length(self) -> int:\n        return {\n            _Split.TRAIN: 11_797_647,\n            _Split.VAL: 561_050,\n        }[self]\n\n    def entries_path(self):\n        return f\"imagenet21kp_{self.value}.txt\"\n\n\ndef _get_tarball_path(class_id: str) -> str:\n    return f\"{class_id}.tar\"\n\n\ndef _make_mmap_tarball(tarballs_root: str, mmap_cache_size: int):\n    @lru_cache(maxsize=mmap_cache_size)\n    def _mmap_tarball(class_id: str) -> mmap:\n        tarball_path = _get_tarball_path(class_id)\n        tarball_full_path = os.path.join(tarballs_root, tarball_path)\n        with open(tarball_full_path) as f:\n            return mmap(fileno=f.fileno(), length=0, access=ACCESS_READ)\n\n    return _mmap_tarball\n\n\nclass ImageNet22k(ExtendedVisionDataset):\n    _GZIPPED_INDICES: Set[int] = {\n        841_545,\n        1_304_131,\n        2_437_921,\n        2_672_079,\n        2_795_676,\n        2_969_786,\n        6_902_965,\n        6_903_550,\n        6_903_628,\n        7_432_557,\n        7_432_589,\n        7_813_809,\n        8_329_633,\n        10_296_990,\n        10_417_652,\n        10_492_265,\n        10_598_078,\n        10_782_398,\n        10_902_612,\n        11_203_736,\n        11_342_890,\n        11_397_596,\n        11_589_762,\n        11_705_103,\n        12_936_875,\n        13_289_782,\n    }\n    Labels = _Labels\n\n    def __init__(\n        self,\n        *,\n        root: str,\n        extra: str,\n        transforms: Optional[Callable] = None,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n        mmap_cache_size: int = _DEFAULT_MMAP_CACHE_SIZE,\n    ) -> None:\n        super().__init__(root, transforms, transform, target_transform)\n        self._extra_root = extra\n\n        entries_path = self._get_entries_path(root)\n        self._entries = self._load_extra(entries_path)\n\n        class_ids_path = self._get_class_ids_path(root)\n        self._class_ids = self._load_extra(class_ids_path)\n\n        self._gzipped_indices = ImageNet22k._GZIPPED_INDICES\n        self._mmap_tarball = _make_mmap_tarball(self._tarballs_root, mmap_cache_size)\n\n    def _get_entries_path(self, root: Optional[str] = None) -> str:\n        return \"entries.npy\"\n\n    def _get_class_ids_path(self, root: Optional[str] = None) -> str:\n        return \"class-ids.npy\"\n\n    def _find_class_ids(self, path: str) -> List[str]:\n        class_ids = []\n\n        with os.scandir(path) as entries:\n            for entry in entries:\n                root, ext = os.path.splitext(entry.name)\n                if ext != \".tar\":\n                    continue\n                class_ids.append(root)\n\n        return sorted(class_ids)\n\n    def _load_entries_class_ids(self, root: Optional[str] = None) -> Tuple[List[_Entry], List[str]]:\n        root = self.get_root(root)\n        entries: List[_Entry] = []\n        class_ids = self._find_class_ids(root)\n\n        for class_index, class_id in enumerate(class_ids):\n            path = os.path.join(root, \"blocks\", f\"{class_id}.log\")\n            class_entries = []\n\n            try:\n                with open(path) as f:\n                    for line in f:\n                        line = line.rstrip()\n                        block, filename = line.split(\":\")\n                        block_offset = int(block[6:])\n                        filename = filename[1:]\n\n                        maybe_filename = None\n                        if filename != \"** Block of NULs **\":\n                            maybe_filename = filename\n                            _, ext = os.path.splitext(filename)\n                            # assert ext == \".JPEG\"\n\n                        class_entry = _ClassEntry(block_offset, maybe_filename)\n                        class_entries.append(class_entry)\n            except OSError as e:\n                raise RuntimeError(f'can not read blocks file \"{path}\"') from e\n\n            assert class_entries[-1].maybe_filename is None\n\n            for class_entry1, class_entry2 in zip(class_entries, class_entries[1:]):\n                assert class_entry1.block_offset <= class_entry2.block_offset\n                start_offset = 512 * class_entry1.block_offset\n                end_offset = 512 * class_entry2.block_offset\n                assert class_entry1.maybe_filename is not None\n                filename = class_entry1.maybe_filename\n                entry = _Entry(class_index, start_offset, end_offset, filename)\n                # Skip invalid image files (PIL throws UnidentifiedImageError)\n                if filename == \"n06470073_47249.JPEG\":\n                    continue\n                entries.append(entry)\n\n        return entries, class_ids\n\n    def _load_extra(self, extra_path: str) -> np.ndarray:\n        extra_root = self._extra_root\n        extra_full_path = os.path.join(extra_root, extra_path)\n        return np.load(extra_full_path, mmap_mode=\"r\")\n\n    def _save_extra(self, extra_array: np.ndarray, extra_path: str) -> None:\n        extra_root = self._extra_root\n        extra_full_path = os.path.join(extra_root, extra_path)\n        os.makedirs(extra_root, exist_ok=True)\n        np.save(extra_full_path, extra_array)\n\n    @property\n    def _tarballs_root(self) -> str:\n        return self.root\n\n    def find_class_id(self, class_index: int) -> str:\n        return str(self._class_ids[class_index])\n\n    def get_image_data(self, index: int) -> bytes:\n        entry = self._entries[index]\n        class_id = entry[\"class_id\"]\n        class_mmap = self._mmap_tarball(class_id)\n\n        start_offset, end_offset = entry[\"start_offset\"], entry[\"end_offset\"]\n        try:\n            mapped_data = class_mmap[start_offset:end_offset]\n            data = mapped_data[512:]  # Skip entry header block\n\n            if len(data) >= 2 and tuple(data[:2]) == (0x1F, 0x8B):\n                assert index in self._gzipped_indices, f\"unexpected gzip header for sample {index}\"\n                with GzipFile(fileobj=BytesIO(data)) as g:\n                    data = g.read()\n        except Exception as e:\n            raise RuntimeError(f\"can not retrieve image data for sample {index} \" f'from \"{class_id}\" tarball') from e\n\n        return data\n\n    def get_target(self, index: int) -> Any:\n        return int(self._entries[index][\"class_index\"])\n\n    def get_targets(self) -> np.ndarray:\n        return self._entries[\"class_index\"]\n\n    def get_class_id(self, index: int) -> str:\n        return str(self._entries[index][\"class_id\"])\n\n    def get_class_ids(self) -> np.ndarray:\n        return self._entries[\"class_id\"]\n\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            return super().__getitem__(index)\n\n    def __len__(self) -> int:\n        return len(self._entries)\n\n    def _dump_entries(self, *args, **kwargs) -> None:\n        entries, class_ids = self._load_entries_class_ids(*args, **kwargs)\n\n        max_class_id_length, max_filename_length, max_class_index = -1, -1, -1\n        for entry in entries:\n            class_id = class_ids[entry.class_index]\n            max_class_index = max(entry.class_index, max_class_index)\n            max_class_id_length = max(len(class_id), max_class_id_length)\n            max_filename_length = max(len(entry.filename), max_filename_length)\n\n        dtype = np.dtype(\n            [\n                (\"class_index\", \"<u4\"),\n                (\"class_id\", f\"U{max_class_id_length}\"),\n                (\"start_offset\", \"<u4\"),\n                (\"end_offset\", \"<u4\"),\n                (\"filename\", f\"U{max_filename_length}\"),\n            ]\n        )\n        sample_count = len(entries)\n        entries_array = np.empty(sample_count, dtype=dtype)\n        for i, entry in enumerate(entries):\n            class_index = entry.class_index\n            class_id = class_ids[class_index]\n            start_offset = entry.start_offset\n            end_offset = entry.end_offset\n            filename = entry.filename\n            entries_array[i] = (\n                class_index,\n                class_id,\n                start_offset,\n                end_offset,\n                filename,\n            )\n\n        entries_path = self._get_entries_path(*args, **kwargs)\n        self._save_extra(entries_array, entries_path)\n\n    def _dump_class_ids(self, *args, **kwargs) -> None:\n        entries_path = self._get_entries_path(*args, **kwargs)\n        entries_array = self._load_extra(entries_path)\n\n        max_class_id_length, max_class_index = -1, -1\n        for entry in entries_array:\n            class_index, class_id = entry[\"class_index\"], entry[\"class_id\"]\n            max_class_index = max(int(class_index), max_class_index)\n            max_class_id_length = max(len(str(class_id)), max_class_id_length)\n\n        class_ids_array = np.empty(max_class_index + 1, dtype=f\"U{max_class_id_length}\")\n        for entry in entries_array:\n            class_index, class_id = entry[\"class_index\"], entry[\"class_id\"]\n            class_ids_array[class_index] = class_id\n        class_ids_path = self._get_class_ids_path(*args, **kwargs)\n        self._save_extra(class_ids_array, class_ids_path)\n\n    def _dump_extra(self, *args, **kwargs) -> None:\n        self._dump_entries(*args, *kwargs)\n        self._dump_class_ids(*args, *kwargs)\n\n    def dump_extra(self, root: Optional[str] = None) -> None:\n        return self._dump_extra(root)\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/datasets/extended.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Tuple\n\nfrom torchvision.datasets import VisionDataset\n\nfrom .decoders import TargetDecoder, ImageDataDecoder\n\n\nclass ExtendedVisionDataset(VisionDataset):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)  # type: ignore\n\n    def get_image_data(self, index: int) -> bytes:\n        raise NotImplementedError\n\n    def get_target(self, index: int) -> Any:\n        raise NotImplementedError\n\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        try:\n            image_data = self.get_image_data(index)\n            image = ImageDataDecoder(image_data).decode()\n        except Exception as e:\n            raise RuntimeError(f\"can not read image for sample {index}\") from e\n        target = self.get_target(index)\n        target = TargetDecoder(target).decode()\n\n        if self.transforms is not None:\n            image, target = self.transforms(image, target)\n\n        return image, target\n\n    def __len__(self) -> int:\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/loaders.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom enum import Enum\nfrom typing import Any, Callable, List, Optional, TypeVar\n\nimport torch\nfrom torch.utils.data import Sampler\n\nfrom .datasets import ImageNet, ImageNet22k\nfrom .samplers import EpochSampler, InfiniteSampler, ShardedInfiniteSampler\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass SamplerType(Enum):\n    DISTRIBUTED = 0\n    EPOCH = 1\n    INFINITE = 2\n    SHARDED_INFINITE = 3\n    SHARDED_INFINITE_NEW = 4\n\n\ndef _make_bool_str(b: bool) -> str:\n    return \"yes\" if b else \"no\"\n\n\ndef _make_sample_transform(image_transform: Optional[Callable] = None, target_transform: Optional[Callable] = None):\n    def transform(sample):\n        image, target = sample\n        if image_transform is not None:\n            image = image_transform(image)\n        if target_transform is not None:\n            target = target_transform(target)\n        return image, target\n\n    return transform\n\n\ndef _parse_dataset_str(dataset_str: str):\n    tokens = dataset_str.split(\":\")\n\n    name = tokens[0]\n    kwargs = {}\n\n    for token in tokens[1:]:\n        key, value = token.split(\"=\")\n        assert key in (\"root\", \"extra\", \"split\")\n        kwargs[key] = value\n\n    if name == \"ImageNet\":\n        class_ = ImageNet\n        if \"split\" in kwargs:\n            kwargs[\"split\"] = ImageNet.Split[kwargs[\"split\"]]\n    elif name == \"ImageNet22k\":\n        class_ = ImageNet22k\n    else:\n        raise ValueError(f'Unsupported dataset \"{name}\"')\n\n    return class_, kwargs\n\n\ndef make_dataset(\n    *,\n    dataset_str: str,\n    transform: Optional[Callable] = None,\n    target_transform: Optional[Callable] = None,\n):\n    \"\"\"\n    Creates a dataset with the specified parameters.\n\n    Args:\n        dataset_str: A dataset string description (e.g. ImageNet:split=TRAIN).\n        transform: A transform to apply to images.\n        target_transform: A transform to apply to targets.\n\n    Returns:\n        The created dataset.\n    \"\"\"\n    logger.info(f'using dataset: \"{dataset_str}\"')\n\n    class_, kwargs = _parse_dataset_str(dataset_str)\n    dataset = class_(transform=transform, target_transform=target_transform, **kwargs)\n\n    logger.info(f\"# of dataset samples: {len(dataset):,d}\")\n\n    # Aggregated datasets do not expose (yet) these attributes, so add them.\n    if not hasattr(dataset, \"transform\"):\n        setattr(dataset, \"transform\", transform)\n    if not hasattr(dataset, \"target_transform\"):\n        setattr(dataset, \"target_transform\", target_transform)\n\n    return dataset\n\n\ndef _make_sampler(\n    *,\n    dataset,\n    type: Optional[SamplerType] = None,\n    shuffle: bool = False,\n    seed: int = 0,\n    size: int = -1,\n    advance: int = 0,\n) -> Optional[Sampler]:\n    sample_count = len(dataset)\n\n    if type == SamplerType.INFINITE:\n        logger.info(\"sampler: infinite\")\n        if size > 0:\n            raise ValueError(\"sampler size > 0 is invalid\")\n        return InfiniteSampler(\n            sample_count=sample_count,\n            shuffle=shuffle,\n            seed=seed,\n            advance=advance,\n        )\n    elif type in (SamplerType.SHARDED_INFINITE, SamplerType.SHARDED_INFINITE_NEW):\n        logger.info(\"sampler: sharded infinite\")\n        if size > 0:\n            raise ValueError(\"sampler size > 0 is invalid\")\n        # TODO: Remove support for old shuffling\n        use_new_shuffle_tensor_slice = type == SamplerType.SHARDED_INFINITE_NEW\n        return ShardedInfiniteSampler(\n            sample_count=sample_count,\n            shuffle=shuffle,\n            seed=seed,\n            advance=advance,\n            use_new_shuffle_tensor_slice=use_new_shuffle_tensor_slice,\n        )\n    elif type == SamplerType.EPOCH:\n        logger.info(\"sampler: epoch\")\n        if advance > 0:\n            raise NotImplementedError(\"sampler advance > 0 is not supported\")\n        size = size if size > 0 else sample_count\n        logger.info(f\"# of samples / epoch: {size:,d}\")\n        return EpochSampler(\n            size=size,\n            sample_count=sample_count,\n            shuffle=shuffle,\n            seed=seed,\n        )\n    elif type == SamplerType.DISTRIBUTED:\n        logger.info(\"sampler: distributed\")\n        if size > 0:\n            raise ValueError(\"sampler size > 0 is invalid\")\n        if advance > 0:\n            raise ValueError(\"sampler advance > 0 is invalid\")\n        return torch.utils.data.DistributedSampler(\n            dataset=dataset,\n            shuffle=shuffle,\n            seed=seed,\n            drop_last=False,\n        )\n\n    logger.info(\"sampler: none\")\n    return None\n\n\nT = TypeVar(\"T\")\n\n\ndef make_data_loader(\n    *,\n    dataset,\n    batch_size: int,\n    num_workers: int,\n    shuffle: bool = True,\n    seed: int = 0,\n    sampler_type: Optional[SamplerType] = SamplerType.INFINITE,\n    sampler_size: int = -1,\n    sampler_advance: int = 0,\n    drop_last: bool = True,\n    persistent_workers: bool = False,\n    collate_fn: Optional[Callable[[List[T]], Any]] = None,\n):\n    \"\"\"\n    Creates a data loader with the specified parameters.\n\n    Args:\n        dataset: A dataset (third party, LaViDa or WebDataset).\n        batch_size: The size of batches to generate.\n        num_workers: The number of workers to use.\n        shuffle: Whether to shuffle samples.\n        seed: The random seed to use.\n        sampler_type: Which sampler to use: EPOCH, INFINITE, SHARDED_INFINITE, SHARDED_INFINITE_NEW, DISTRIBUTED or None.\n        sampler_size: The number of images per epoch (when applicable) or -1 for the entire dataset.\n        sampler_advance: How many samples to skip (when applicable).\n        drop_last: Whether the last non-full batch of data should be dropped.\n        persistent_workers: maintain the workers Dataset instances alive after a dataset has been consumed once.\n        collate_fn: Function that performs batch collation\n    \"\"\"\n\n    sampler = _make_sampler(\n        dataset=dataset,\n        type=sampler_type,\n        shuffle=shuffle,\n        seed=seed,\n        size=sampler_size,\n        advance=sampler_advance,\n    )\n\n    logger.info(\"using PyTorch data loader\")\n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        sampler=sampler,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=drop_last,\n        persistent_workers=persistent_workers,\n        collate_fn=collate_fn,\n    )\n\n    try:\n        logger.info(f\"# of batches: {len(data_loader):,d}\")\n    except TypeError:  # data loader has no length\n        logger.info(\"infinite data loader\")\n    return data_loader\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/collate.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport random\n\n\ndef collate_data_and_cast(samples_list, mask_ratio_tuple, mask_probability, dtype, n_tokens=None, mask_generator=None):\n    # dtype = torch.half  # TODO: Remove\n\n    n_global_crops = len(samples_list[0][0][\"global_crops\"])\n    n_local_crops = len(samples_list[0][0][\"local_crops\"])\n\n    collated_global_crops = torch.stack([s[0][\"global_crops\"][i] for i in range(n_global_crops) for s in samples_list])\n\n    collated_local_crops = torch.stack([s[0][\"local_crops\"][i] for i in range(n_local_crops) for s in samples_list])\n\n    B = len(collated_global_crops)\n    N = n_tokens\n    n_samples_masked = int(B * mask_probability)\n    probs = torch.linspace(*mask_ratio_tuple, n_samples_masked + 1)\n    upperbound = 0\n    masks_list = []\n    for i in range(0, n_samples_masked):\n        prob_min = probs[i]\n        prob_max = probs[i + 1]\n        masks_list.append(torch.BoolTensor(mask_generator(int(N * random.uniform(prob_min, prob_max)))))\n        upperbound += int(N * prob_max)\n    for i in range(n_samples_masked, B):\n        masks_list.append(torch.BoolTensor(mask_generator(0)))\n\n    random.shuffle(masks_list)\n\n    collated_masks = torch.stack(masks_list).flatten(1)\n    mask_indices_list = collated_masks.flatten().nonzero().flatten()\n\n    masks_weight = (1 / collated_masks.sum(-1).clamp(min=1.0)).unsqueeze(-1).expand_as(collated_masks)[collated_masks]\n\n    return {\n        \"collated_global_crops\": collated_global_crops.to(dtype),\n        \"collated_local_crops\": collated_local_crops.to(dtype),\n        \"collated_masks\": collated_masks,\n        \"mask_indices_list\": mask_indices_list,\n        \"masks_weight\": masks_weight,\n        \"upperbound\": upperbound,\n        \"n_masked_patches\": torch.full((1,), fill_value=mask_indices_list.shape[0], dtype=torch.long),\n    }\n"}
{"type": "source_file", "path": "dinov2/dinov2/data/transforms.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom typing import Sequence\n\nimport torch\nfrom torchvision import transforms\n\n\nclass GaussianBlur(transforms.RandomApply):\n    \"\"\"\n    Apply Gaussian Blur to the PIL image.\n    \"\"\"\n\n    def __init__(self, *, p: float = 0.5, radius_min: float = 0.1, radius_max: float = 2.0):\n        # NOTE: torchvision is applying 1 - probability to return the original image\n        keep_p = 1 - p\n        transform = transforms.GaussianBlur(kernel_size=9, sigma=(radius_min, radius_max))\n        super().__init__(transforms=[transform], p=keep_p)\n\n\nclass MaybeToTensor(transforms.ToTensor):\n    \"\"\"\n    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.\n    \"\"\"\n\n    def __call__(self, pic):\n        \"\"\"\n        Args:\n            pic (PIL Image, numpy.ndarray or torch.tensor): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        if isinstance(pic, torch.Tensor):\n            return pic\n        return super().__call__(pic)\n\n\n# Use timm's names\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n\n\ndef make_normalize_transform(\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Normalize:\n    return transforms.Normalize(mean=mean, std=std)\n\n\n# This roughly matches torchvision's preset for classification training:\n#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\ndef make_classification_train_transform(\n    *,\n    crop_size: int = 224,\n    interpolation=transforms.InterpolationMode.BICUBIC,\n    hflip_prob: float = 0.5,\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n):\n    transforms_list = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]\n    if hflip_prob > 0.0:\n        transforms_list.append(transforms.RandomHorizontalFlip(hflip_prob))\n    transforms_list.extend(\n        [\n            MaybeToTensor(),\n            make_normalize_transform(mean=mean, std=std),\n        ]\n    )\n    return transforms.Compose(transforms_list)\n\n\n# This matches (roughly) torchvision's preset for classification evaluation:\n#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L47-L69\ndef make_classification_eval_transform(\n    *,\n    resize_size: int = 256,\n    interpolation=transforms.InterpolationMode.BICUBIC,\n    crop_size: int = 224,\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Compose:\n    transforms_list = [\n        transforms.Resize(resize_size, interpolation=interpolation),\n        transforms.CenterCrop(crop_size),\n        MaybeToTensor(),\n        make_normalize_transform(mean=mean, std=std),\n    ]\n    return transforms.Compose(transforms_list)\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/builder.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport warnings\n\nfrom mmcv.cnn import MODELS as MMCV_MODELS\nfrom mmcv.cnn.bricks.registry import ATTENTION as MMCV_ATTENTION\nfrom mmcv.utils import Registry\n\nMODELS = Registry(\"models\", parent=MMCV_MODELS)\nATTENTION = Registry(\"attention\", parent=MMCV_ATTENTION)\n\n\nBACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\n\n\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\n\n\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\n\n\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\n\n\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\n\n\ndef build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \"train_cfg specified in both outer field and model field \"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \"test_cfg specified in both outer field and model field \"\n    return DEPTHER.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/depther/encoder_decoder.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom ...models import builder\nfrom ...models.builder import DEPTHER\nfrom ...ops import resize\nfrom .base import BaseDepther\n\n\ndef add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n\n    Returns:\n\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n\n    outputs = dict()\n    for name, value in inputs.items():\n        outputs[f\"{prefix}.{name}\"] = value\n\n    return outputs\n\n\n@DEPTHER.register_module()\nclass DepthEncoderDecoder(BaseDepther):\n    \"\"\"Encoder Decoder depther.\n\n    EncoderDecoder typically consists of backbone, (neck) and decode_head.\n    \"\"\"\n\n    def __init__(self, backbone, decode_head, neck=None, train_cfg=None, test_cfg=None, pretrained=None, init_cfg=None):\n        super(DepthEncoderDecoder, self).__init__(init_cfg)\n        if pretrained is not None:\n            assert backbone.get(\"pretrained\") is None, \"both backbone and depther set pretrained weight\"\n            backbone.pretrained = pretrained\n        self.backbone = builder.build_backbone(backbone)\n        self._init_decode_head(decode_head)\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        assert self.with_decode_head\n\n    def _init_decode_head(self, decode_head):\n        \"\"\"Initialize ``decode_head``\"\"\"\n        self.decode_head = builder.build_head(decode_head)\n        self.align_corners = self.decode_head.align_corners\n\n    def extract_feat(self, img):\n        \"\"\"Extract features from images.\"\"\"\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def encode_decode(self, img, img_metas, rescale=True, size=None):\n        \"\"\"Encode images with backbone and decode into a depth estimation\n        map of the same size as input.\"\"\"\n        x = self.extract_feat(img)\n        out = self._decode_head_forward_test(x, img_metas)\n        # crop the pred depth to the certain range.\n        out = torch.clamp(out, min=self.decode_head.min_depth, max=self.decode_head.max_depth)\n        if rescale:\n            if size is None:\n                if img_metas is not None:\n                    size = img_metas[0][\"ori_shape\"][:2]\n                else:\n                    size = img.shape[2:]\n            out = resize(input=out, size=size, mode=\"bilinear\", align_corners=self.align_corners)\n        return out\n\n    def _decode_head_forward_train(self, img, x, img_metas, depth_gt, **kwargs):\n        \"\"\"Run forward function and calculate loss for decode head in\n        training.\"\"\"\n        losses = dict()\n        loss_decode = self.decode_head.forward_train(img, x, img_metas, depth_gt, self.train_cfg, **kwargs)\n        losses.update(add_prefix(loss_decode, \"decode\"))\n        return losses\n\n    def _decode_head_forward_test(self, x, img_metas):\n        \"\"\"Run forward function and calculate loss for decode head in\n        inference.\"\"\"\n        depth_pred = self.decode_head.forward_test(x, img_metas, self.test_cfg)\n        return depth_pred\n\n    def forward_dummy(self, img):\n        \"\"\"Dummy forward function.\"\"\"\n        depth = self.encode_decode(img, None)\n\n        return depth\n\n    def forward_train(self, img, img_metas, depth_gt, **kwargs):\n        \"\"\"Forward function for training.\n\n        Args:\n            img (Tensor): Input images.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `depth/datasets/pipelines/formatting.py:Collect`.\n            depth_gt (Tensor): Depth gt\n                used if the architecture supports depth estimation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # the last of x saves the info from neck\n        loss_decode = self._decode_head_forward_train(img, x, img_metas, depth_gt, **kwargs)\n\n        losses.update(loss_decode)\n\n        return losses\n\n    def whole_inference(self, img, img_meta, rescale, size=None):\n        \"\"\"Inference with full image.\"\"\"\n        depth_pred = self.encode_decode(img, img_meta, rescale, size=size)\n\n        return depth_pred\n\n    def slide_inference(self, img, img_meta, rescale):\n        \"\"\"Inference by sliding-window with overlap.\n\n        If h_crop > h_img or w_crop > w_img, the small patch will be used to\n        decode without padding.\n        \"\"\"\n\n        h_stride, w_stride = self.test_cfg.stride\n        h_crop, w_crop = self.test_cfg.crop_size\n        batch_size, _, h_img, w_img = img.size()\n        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n        preds = img.new_zeros((batch_size, 1, h_img, w_img))\n        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))\n        for h_idx in range(h_grids):\n            for w_idx in range(w_grids):\n                y1 = h_idx * h_stride\n                x1 = w_idx * w_stride\n                y2 = min(y1 + h_crop, h_img)\n                x2 = min(x1 + w_crop, w_img)\n                y1 = max(y2 - h_crop, 0)\n                x1 = max(x2 - w_crop, 0)\n                crop_img = img[:, :, y1:y2, x1:x2]\n                depth_pred = self.encode_decode(crop_img, img_meta, rescale)\n                preds += F.pad(depth_pred, (int(x1), int(preds.shape[3] - x2), int(y1), int(preds.shape[2] - y2)))\n\n                count_mat[:, :, y1:y2, x1:x2] += 1\n        assert (count_mat == 0).sum() == 0\n        if torch.onnx.is_in_onnx_export():\n            # cast count_mat to constant while exporting to ONNX\n            count_mat = torch.from_numpy(count_mat.cpu().detach().numpy()).to(device=img.device)\n        preds = preds / count_mat\n        return preds\n\n    def inference(self, img, img_meta, rescale, size=None):\n        \"\"\"Inference with slide/whole style.\n\n        Args:\n            img (Tensor): The input image of shape (N, 3, H, W).\n            img_meta (dict): Image info dict where each dict has: 'img_shape',\n                'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `depth/datasets/pipelines/formatting.py:Collect`.\n            rescale (bool): Whether rescale back to original shape.\n\n        Returns:\n            Tensor: The output depth map.\n        \"\"\"\n\n        assert self.test_cfg.mode in [\"slide\", \"whole\"]\n        ori_shape = img_meta[0][\"ori_shape\"]\n        assert all(_[\"ori_shape\"] == ori_shape for _ in img_meta)\n        if self.test_cfg.mode == \"slide\":\n            depth_pred = self.slide_inference(img, img_meta, rescale)\n        else:\n            depth_pred = self.whole_inference(img, img_meta, rescale, size=size)\n        output = depth_pred\n        flip = img_meta[0][\"flip\"]\n        if flip:\n            flip_direction = img_meta[0][\"flip_direction\"]\n            assert flip_direction in [\"horizontal\", \"vertical\"]\n            if flip_direction == \"horizontal\":\n                output = output.flip(dims=(3,))\n            elif flip_direction == \"vertical\":\n                output = output.flip(dims=(2,))\n\n        return output\n\n    def simple_test(self, img, img_meta, rescale=True):\n        \"\"\"Simple test with single image.\"\"\"\n        depth_pred = self.inference(img, img_meta, rescale)\n        if torch.onnx.is_in_onnx_export():\n            # our inference backend only support 4D output\n            depth_pred = depth_pred.unsqueeze(0)\n            return depth_pred\n        depth_pred = depth_pred.cpu().numpy()\n        # unravel batch dim\n        depth_pred = list(depth_pred)\n        return depth_pred\n\n    def aug_test(self, imgs, img_metas, rescale=True):\n        \"\"\"Test with augmentations.\n\n        Only rescale=True is supported.\n        \"\"\"\n        # aug_test rescale all imgs back to ori_shape for now\n        assert rescale\n        # to save memory, we get augmented depth logit inplace\n        depth_pred = self.inference(imgs[0], img_metas[0], rescale)\n        for i in range(1, len(imgs)):\n            cur_depth_pred = self.inference(imgs[i], img_metas[i], rescale, size=depth_pred.shape[-2:])\n            depth_pred += cur_depth_pred\n        depth_pred /= len(imgs)\n        depth_pred = depth_pred.cpu().numpy()\n        # unravel batch dim\n        depth_pred = list(depth_pred)\n        return depth_pred\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/linear.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport argparse\nfrom functools import partial\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import List, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\nfrom fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n\nfrom dinov2.data import SamplerType, make_data_loader, make_dataset\nfrom dinov2.data.transforms import make_classification_eval_transform, make_classification_train_transform\nimport dinov2.distributed as distributed\nfrom dinov2.eval.metrics import MetricType, build_metric\nfrom dinov2.eval.setup import get_args_parser as get_setup_args_parser\nfrom dinov2.eval.setup import setup_and_build_model\nfrom dinov2.eval.utils import ModelWithIntermediateLayers, evaluate\nfrom dinov2.logging import MetricLogger\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents,\n        add_help=add_help,\n    )\n    parser.add_argument(\n        \"--train-dataset\",\n        dest=\"train_dataset_str\",\n        type=str,\n        help=\"Training dataset\",\n    )\n    parser.add_argument(\n        \"--val-dataset\",\n        dest=\"val_dataset_str\",\n        type=str,\n        help=\"Validation dataset\",\n    )\n    parser.add_argument(\n        \"--test-datasets\",\n        dest=\"test_dataset_strs\",\n        type=str,\n        nargs=\"+\",\n        help=\"Test datasets, none to reuse the validation dataset\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        help=\"Number of training epochs\",\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        help=\"Batch Size (per GPU)\",\n    )\n    parser.add_argument(\n        \"--num-workers\",\n        type=int,\n        help=\"Number de Workers\",\n    )\n    parser.add_argument(\n        \"--epoch-length\",\n        type=int,\n        help=\"Length of an epoch in number of iterations\",\n    )\n    parser.add_argument(\n        \"--save-checkpoint-frequency\",\n        type=int,\n        help=\"Number of epochs between two named checkpoint saves.\",\n    )\n    parser.add_argument(\n        \"--eval-period-iterations\",\n        type=int,\n        help=\"Number of iterations between two evaluations.\",\n    )\n    parser.add_argument(\n        \"--learning-rates\",\n        nargs=\"+\",\n        type=float,\n        help=\"Learning rates to grid search.\",\n    )\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not resume from existing checkpoints\",\n    )\n    parser.add_argument(\n        \"--val-metric-type\",\n        type=MetricType,\n        choices=list(MetricType),\n        help=\"Validation metric\",\n    )\n    parser.add_argument(\n        \"--test-metric-types\",\n        type=MetricType,\n        choices=list(MetricType),\n        nargs=\"+\",\n        help=\"Evaluation metric\",\n    )\n    parser.add_argument(\n        \"--classifier-fpath\",\n        type=str,\n        help=\"Path to a file containing pretrained linear classifiers\",\n    )\n    parser.add_argument(\n        \"--val-class-mapping-fpath\",\n        type=str,\n        help=\"Path to a file containing a mapping to adjust classifier outputs\",\n    )\n    parser.add_argument(\n        \"--test-class-mapping-fpaths\",\n        nargs=\"+\",\n        type=str,\n        help=\"Path to a file containing a mapping to adjust classifier outputs\",\n    )\n    parser.set_defaults(\n        train_dataset_str=\"ImageNet:split=TRAIN\",\n        val_dataset_str=\"ImageNet:split=VAL\",\n        test_dataset_strs=None,\n        epochs=10,\n        batch_size=128,\n        num_workers=8,\n        epoch_length=1250,\n        save_checkpoint_frequency=20,\n        eval_period_iterations=1250,\n        learning_rates=[1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 0.1],\n        val_metric_type=MetricType.MEAN_ACCURACY,\n        test_metric_types=None,\n        classifier_fpath=None,\n        val_class_mapping_fpath=None,\n        test_class_mapping_fpaths=[None],\n    )\n    return parser\n\n\ndef has_ddp_wrapper(m: nn.Module) -> bool:\n    return isinstance(m, DistributedDataParallel)\n\n\ndef remove_ddp_wrapper(m: nn.Module) -> nn.Module:\n    return m.module if has_ddp_wrapper(m) else m\n\n\ndef _pad_and_collate(batch):\n    maxlen = max(len(targets) for image, targets in batch)\n    padded_batch = [\n        (image, np.pad(targets, (0, maxlen - len(targets)), constant_values=-1)) for image, targets in batch\n    ]\n    return torch.utils.data.default_collate(padded_batch)\n\n\ndef create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n    intermediate_output = x_tokens_list[-use_n_blocks:]\n    output = torch.cat([class_token for _, class_token in intermediate_output], dim=-1)\n    if use_avgpool:\n        output = torch.cat(\n            (\n                output,\n                torch.mean(intermediate_output[-1][0], dim=1),  # patch tokens\n            ),\n            dim=-1,\n        )\n        output = output.reshape(output.shape[0], -1)\n    return output.float()\n\n\nclass LinearClassifier(nn.Module):\n    \"\"\"Linear layer to train on top of frozen features\"\"\"\n\n    def __init__(self, out_dim, use_n_blocks, use_avgpool, num_classes=1000):\n        super().__init__()\n        self.out_dim = out_dim\n        self.use_n_blocks = use_n_blocks\n        self.use_avgpool = use_avgpool\n        self.num_classes = num_classes\n        self.linear = nn.Linear(out_dim, num_classes)\n        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n        self.linear.bias.data.zero_()\n\n    def forward(self, x_tokens_list):\n        output = create_linear_input(x_tokens_list, self.use_n_blocks, self.use_avgpool)\n        return self.linear(output)\n\n\nclass AllClassifiers(nn.Module):\n    def __init__(self, classifiers_dict):\n        super().__init__()\n        self.classifiers_dict = nn.ModuleDict()\n        self.classifiers_dict.update(classifiers_dict)\n\n    def forward(self, inputs):\n        return {k: v.forward(inputs) for k, v in self.classifiers_dict.items()}\n\n    def __len__(self):\n        return len(self.classifiers_dict)\n\n\nclass LinearPostprocessor(nn.Module):\n    def __init__(self, linear_classifier, class_mapping=None):\n        super().__init__()\n        self.linear_classifier = linear_classifier\n        self.register_buffer(\"class_mapping\", None if class_mapping is None else torch.LongTensor(class_mapping))\n\n    def forward(self, samples, targets):\n        preds = self.linear_classifier(samples)\n        return {\n            \"preds\": preds[:, self.class_mapping] if self.class_mapping is not None else preds,\n            \"target\": targets,\n        }\n\n\ndef scale_lr(learning_rates, batch_size):\n    return learning_rates * (batch_size * distributed.get_global_size()) / 256.0\n\n\ndef setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, batch_size, num_classes=1000):\n    linear_classifiers_dict = nn.ModuleDict()\n    optim_param_groups = []\n    for n in n_last_blocks_list:\n        for avgpool in [False, True]:\n            for _lr in learning_rates:\n                lr = scale_lr(_lr, batch_size)\n                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]\n                linear_classifier = LinearClassifier(\n                    out_dim, use_n_blocks=n, use_avgpool=avgpool, num_classes=num_classes\n                )\n                linear_classifier = linear_classifier.cuda()\n                linear_classifiers_dict[\n                    f\"classifier_{n}_blocks_avgpool_{avgpool}_lr_{lr:.5f}\".replace(\".\", \"_\")\n                ] = linear_classifier\n                optim_param_groups.append({\"params\": linear_classifier.parameters(), \"lr\": lr})\n\n    linear_classifiers = AllClassifiers(linear_classifiers_dict)\n    if distributed.is_enabled():\n        linear_classifiers = nn.parallel.DistributedDataParallel(linear_classifiers)\n\n    return linear_classifiers, optim_param_groups\n\n\n@torch.no_grad()\ndef evaluate_linear_classifiers(\n    feature_model,\n    linear_classifiers,\n    data_loader,\n    metric_type,\n    metrics_file_path,\n    training_num_classes,\n    iteration,\n    prefixstring=\"\",\n    class_mapping=None,\n    best_classifier_on_val=None,\n):\n    logger.info(\"running validation !\")\n\n    num_classes = len(class_mapping) if class_mapping is not None else training_num_classes\n    metric = build_metric(metric_type, num_classes=num_classes)\n    postprocessors = {k: LinearPostprocessor(v, class_mapping) for k, v in linear_classifiers.classifiers_dict.items()}\n    metrics = {k: metric.clone() for k in linear_classifiers.classifiers_dict}\n\n    _, results_dict_temp = evaluate(\n        feature_model,\n        data_loader,\n        postprocessors,\n        metrics,\n        torch.cuda.current_device(),\n    )\n\n    logger.info(\"\")\n    results_dict = {}\n    max_accuracy = 0\n    best_classifier = \"\"\n    for i, (classifier_string, metric) in enumerate(results_dict_temp.items()):\n        logger.info(f\"{prefixstring} -- Classifier: {classifier_string} * {metric}\")\n        if (\n            best_classifier_on_val is None and metric[\"top-1\"].item() > max_accuracy\n        ) or classifier_string == best_classifier_on_val:\n            max_accuracy = metric[\"top-1\"].item()\n            best_classifier = classifier_string\n\n    results_dict[\"best_classifier\"] = {\"name\": best_classifier, \"accuracy\": max_accuracy}\n\n    logger.info(f\"best classifier: {results_dict['best_classifier']}\")\n\n    if distributed.is_main_process():\n        with open(metrics_file_path, \"a\") as f:\n            f.write(f\"iter: {iteration}\\n\")\n            for k, v in results_dict.items():\n                f.write(json.dumps({k: v}) + \"\\n\")\n            f.write(\"\\n\")\n\n    return results_dict\n\n\ndef eval_linear(\n    *,\n    feature_model,\n    linear_classifiers,\n    train_data_loader,\n    val_data_loader,\n    metrics_file_path,\n    optimizer,\n    scheduler,\n    output_dir,\n    max_iter,\n    checkpoint_period,  # In number of iter, creates a new file every period\n    running_checkpoint_period,  # Period to update main checkpoint file\n    eval_period,\n    metric_type,\n    training_num_classes,\n    resume=True,\n    classifier_fpath=None,\n    val_class_mapping=None,\n):\n    checkpointer = Checkpointer(linear_classifiers, output_dir, optimizer=optimizer, scheduler=scheduler)\n    start_iter = checkpointer.resume_or_load(classifier_fpath or \"\", resume=resume).get(\"iteration\", -1) + 1\n\n    periodic_checkpointer = PeriodicCheckpointer(checkpointer, checkpoint_period, max_iter=max_iter)\n    iteration = start_iter\n    logger.info(\"Starting training from iteration {}\".format(start_iter))\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"Training\"\n\n    for data, labels in metric_logger.log_every(\n        train_data_loader,\n        10,\n        header,\n        max_iter,\n        start_iter,\n    ):\n        data = data.cuda(non_blocking=True)\n        labels = labels.cuda(non_blocking=True)\n\n        features = feature_model(data)\n        outputs = linear_classifiers(features)\n\n        losses = {f\"loss_{k}\": nn.CrossEntropyLoss()(v, labels) for k, v in outputs.items()}\n        loss = sum(losses.values())\n\n        # compute the gradients\n        optimizer.zero_grad()\n        loss.backward()\n\n        # step\n        optimizer.step()\n        scheduler.step()\n\n        # log\n        if iteration % 10 == 0:\n            torch.cuda.synchronize()\n            metric_logger.update(loss=loss.item())\n            metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n            print(\"lr\", optimizer.param_groups[0][\"lr\"])\n\n        if iteration - start_iter > 5:\n            if iteration % running_checkpoint_period == 0:\n                torch.cuda.synchronize()\n                if distributed.is_main_process():\n                    logger.info(\"Checkpointing running_checkpoint\")\n                    periodic_checkpointer.save(\"running_checkpoint_linear_eval\", iteration=iteration)\n                torch.cuda.synchronize()\n        periodic_checkpointer.step(iteration)\n\n        if eval_period > 0 and (iteration + 1) % eval_period == 0 and iteration != max_iter - 1:\n            _ = evaluate_linear_classifiers(\n                feature_model=feature_model,\n                linear_classifiers=remove_ddp_wrapper(linear_classifiers),\n                data_loader=val_data_loader,\n                metrics_file_path=metrics_file_path,\n                prefixstring=f\"ITER: {iteration}\",\n                metric_type=metric_type,\n                training_num_classes=training_num_classes,\n                iteration=iteration,\n                class_mapping=val_class_mapping,\n            )\n            torch.cuda.synchronize()\n\n        iteration = iteration + 1\n\n    val_results_dict = evaluate_linear_classifiers(\n        feature_model=feature_model,\n        linear_classifiers=remove_ddp_wrapper(linear_classifiers),\n        data_loader=val_data_loader,\n        metrics_file_path=metrics_file_path,\n        metric_type=metric_type,\n        training_num_classes=training_num_classes,\n        iteration=iteration,\n        class_mapping=val_class_mapping,\n    )\n    return val_results_dict, feature_model, linear_classifiers, iteration\n\n\ndef make_eval_data_loader(test_dataset_str, batch_size, num_workers, metric_type):\n    test_dataset = make_dataset(\n        dataset_str=test_dataset_str,\n        transform=make_classification_eval_transform(),\n    )\n    test_data_loader = make_data_loader(\n        dataset=test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,\n        drop_last=False,\n        shuffle=False,\n        persistent_workers=False,\n        collate_fn=_pad_and_collate if metric_type == MetricType.IMAGENET_REAL_ACCURACY else None,\n    )\n    return test_data_loader\n\n\ndef test_on_datasets(\n    feature_model,\n    linear_classifiers,\n    test_dataset_strs,\n    batch_size,\n    num_workers,\n    test_metric_types,\n    metrics_file_path,\n    training_num_classes,\n    iteration,\n    best_classifier_on_val,\n    prefixstring=\"\",\n    test_class_mappings=[None],\n):\n    results_dict = {}\n    for test_dataset_str, class_mapping, metric_type in zip(test_dataset_strs, test_class_mappings, test_metric_types):\n        logger.info(f\"Testing on {test_dataset_str}\")\n        test_data_loader = make_eval_data_loader(test_dataset_str, batch_size, num_workers, metric_type)\n        dataset_results_dict = evaluate_linear_classifiers(\n            feature_model,\n            remove_ddp_wrapper(linear_classifiers),\n            test_data_loader,\n            metric_type,\n            metrics_file_path,\n            training_num_classes,\n            iteration,\n            prefixstring=\"\",\n            class_mapping=class_mapping,\n            best_classifier_on_val=best_classifier_on_val,\n        )\n        results_dict[f\"{test_dataset_str}_accuracy\"] = 100.0 * dataset_results_dict[\"best_classifier\"][\"accuracy\"]\n    return results_dict\n\n\ndef run_eval_linear(\n    model,\n    output_dir,\n    train_dataset_str,\n    val_dataset_str,\n    batch_size,\n    epochs,\n    epoch_length,\n    num_workers,\n    save_checkpoint_frequency,\n    eval_period_iterations,\n    learning_rates,\n    autocast_dtype,\n    test_dataset_strs=None,\n    resume=True,\n    classifier_fpath=None,\n    val_class_mapping_fpath=None,\n    test_class_mapping_fpaths=[None],\n    val_metric_type=MetricType.MEAN_ACCURACY,\n    test_metric_types=None,\n):\n    seed = 0\n\n    if test_dataset_strs is None:\n        test_dataset_strs = [val_dataset_str]\n    if test_metric_types is None:\n        test_metric_types = [val_metric_type] * len(test_dataset_strs)\n    else:\n        assert len(test_metric_types) == len(test_dataset_strs)\n    assert len(test_dataset_strs) == len(test_class_mapping_fpaths)\n\n    train_transform = make_classification_train_transform()\n    train_dataset = make_dataset(\n        dataset_str=train_dataset_str,\n        transform=train_transform,\n    )\n    training_num_classes = len(torch.unique(torch.Tensor(train_dataset.get_targets().astype(int))))\n    sampler_type = SamplerType.SHARDED_INFINITE\n    # sampler_type = SamplerType.INFINITE\n\n    n_last_blocks_list = [1, 4]\n    n_last_blocks = max(n_last_blocks_list)\n    autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n    feature_model = ModelWithIntermediateLayers(model, n_last_blocks, autocast_ctx)\n    sample_output = feature_model(train_dataset[0][0].unsqueeze(0).cuda())\n\n    linear_classifiers, optim_param_groups = setup_linear_classifiers(\n        sample_output,\n        n_last_blocks_list,\n        learning_rates,\n        batch_size,\n        training_num_classes,\n    )\n\n    optimizer = torch.optim.SGD(optim_param_groups, momentum=0.9, weight_decay=0)\n    max_iter = epochs * epoch_length\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter, eta_min=0)\n    checkpointer = Checkpointer(linear_classifiers, output_dir, optimizer=optimizer, scheduler=scheduler)\n    start_iter = checkpointer.resume_or_load(classifier_fpath or \"\", resume=resume).get(\"iteration\", -1) + 1\n    train_data_loader = make_data_loader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=True,\n        seed=seed,\n        sampler_type=sampler_type,\n        sampler_advance=start_iter,\n        drop_last=True,\n        persistent_workers=True,\n    )\n    val_data_loader = make_eval_data_loader(val_dataset_str, batch_size, num_workers, val_metric_type)\n\n    checkpoint_period = save_checkpoint_frequency * epoch_length\n\n    if val_class_mapping_fpath is not None:\n        logger.info(f\"Using class mapping from {val_class_mapping_fpath}\")\n        val_class_mapping = np.load(val_class_mapping_fpath)\n    else:\n        val_class_mapping = None\n\n    test_class_mappings = []\n    for class_mapping_fpath in test_class_mapping_fpaths:\n        if class_mapping_fpath is not None and class_mapping_fpath != \"None\":\n            logger.info(f\"Using class mapping from {class_mapping_fpath}\")\n            class_mapping = np.load(class_mapping_fpath)\n        else:\n            class_mapping = None\n        test_class_mappings.append(class_mapping)\n\n    metrics_file_path = os.path.join(output_dir, \"results_eval_linear.json\")\n    val_results_dict, feature_model, linear_classifiers, iteration = eval_linear(\n        feature_model=feature_model,\n        linear_classifiers=linear_classifiers,\n        train_data_loader=train_data_loader,\n        val_data_loader=val_data_loader,\n        metrics_file_path=metrics_file_path,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        output_dir=output_dir,\n        max_iter=max_iter,\n        checkpoint_period=checkpoint_period,\n        running_checkpoint_period=epoch_length,\n        eval_period=eval_period_iterations,\n        metric_type=val_metric_type,\n        training_num_classes=training_num_classes,\n        resume=resume,\n        val_class_mapping=val_class_mapping,\n        classifier_fpath=classifier_fpath,\n    )\n    results_dict = {}\n    if len(test_dataset_strs) > 1 or test_dataset_strs[0] != val_dataset_str:\n        results_dict = test_on_datasets(\n            feature_model,\n            linear_classifiers,\n            test_dataset_strs,\n            batch_size,\n            0,  # num_workers,\n            test_metric_types,\n            metrics_file_path,\n            training_num_classes,\n            iteration,\n            val_results_dict[\"best_classifier\"][\"name\"],\n            prefixstring=\"\",\n            test_class_mappings=test_class_mappings,\n        )\n    results_dict[\"best_classifier\"] = val_results_dict[\"best_classifier\"][\"name\"]\n    results_dict[f\"{val_dataset_str}_accuracy\"] = 100.0 * val_results_dict[\"best_classifier\"][\"accuracy\"]\n    logger.info(\"Test Results Dict \" + str(results_dict))\n\n    return results_dict\n\n\ndef main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    run_eval_linear(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        test_dataset_strs=args.test_dataset_strs,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        epoch_length=args.epoch_length,\n        num_workers=args.num_workers,\n        save_checkpoint_frequency=args.save_checkpoint_frequency,\n        eval_period_iterations=args.eval_period_iterations,\n        learning_rates=args.learning_rates,\n        autocast_dtype=autocast_dtype,\n        resume=not args.no_resume,\n        classifier_fpath=args.classifier_fpath,\n        val_metric_type=args.val_metric_type,\n        test_metric_types=args.test_metric_types,\n        val_class_mapping_fpath=args.val_class_mapping_fpath,\n        test_class_mapping_fpaths=args.test_class_mapping_fpaths,\n    )\n    return 0\n\n\nif __name__ == \"__main__\":\n    description = \"DINOv2 linear evaluation\"\n    args_parser = get_args_parser(description=description)\n    args = args_parser.parse_args()\n    sys.exit(main(args))\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/decode_heads/dpt_head.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, Linear, build_activation_layer\nfrom mmcv.runner import BaseModule\n\nfrom ...ops import resize\nfrom ..builder import HEADS\nfrom .decode_head import DepthBaseDecodeHead\n\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n        return x\n\n\nclass HeadDepth(nn.Module):\n    def __init__(self, features):\n        super(HeadDepth, self).__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n        )\n\n    def forward(self, x):\n        x = self.head(x)\n        return x\n\n\nclass ReassembleBlocks(BaseModule):\n    \"\"\"ViTPostProcessBlock, process cls_token in ViT backbone output and\n    rearrange the feature vector to feature map.\n    Args:\n        in_channels (int): ViT feature channels. Default: 768.\n        out_channels (List): output channels of each stage.\n            Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n        init_cfg (dict, optional): Initialization config dict. Default: None.\n    \"\"\"\n\n    def __init__(\n        self, in_channels=768, out_channels=[96, 192, 384, 768], readout_type=\"ignore\", patch_size=16, init_cfg=None\n    ):\n        super(ReassembleBlocks, self).__init__(init_cfg)\n\n        assert readout_type in [\"ignore\", \"add\", \"project\"]\n        self.readout_type = readout_type\n        self.patch_size = patch_size\n\n        self.projects = nn.ModuleList(\n            [\n                ConvModule(\n                    in_channels=in_channels,\n                    out_channels=out_channel,\n                    kernel_size=1,\n                    act_cfg=None,\n                )\n                for out_channel in out_channels\n            ]\n        )\n\n        self.resize_layers = nn.ModuleList(\n            [\n                nn.ConvTranspose2d(\n                    in_channels=out_channels[0], out_channels=out_channels[0], kernel_size=4, stride=4, padding=0\n                ),\n                nn.ConvTranspose2d(\n                    in_channels=out_channels[1], out_channels=out_channels[1], kernel_size=2, stride=2, padding=0\n                ),\n                nn.Identity(),\n                nn.Conv2d(\n                    in_channels=out_channels[3], out_channels=out_channels[3], kernel_size=3, stride=2, padding=1\n                ),\n            ]\n        )\n        if self.readout_type == \"project\":\n            self.readout_projects = nn.ModuleList()\n            for _ in range(len(self.projects)):\n                self.readout_projects.append(\n                    nn.Sequential(Linear(2 * in_channels, in_channels), build_activation_layer(dict(type=\"GELU\")))\n                )\n\n    def forward(self, inputs):\n        assert isinstance(inputs, list)\n        out = []\n        for i, x in enumerate(inputs):\n            assert len(x) == 2\n            x, cls_token = x[0], x[1]\n            feature_shape = x.shape\n            if self.readout_type == \"project\":\n                x = x.flatten(2).permute((0, 2, 1))\n                readout = cls_token.unsqueeze(1).expand_as(x)\n                x = self.readout_projects[i](torch.cat((x, readout), -1))\n                x = x.permute(0, 2, 1).reshape(feature_shape)\n            elif self.readout_type == \"add\":\n                x = x.flatten(2) + cls_token.unsqueeze(-1)\n                x = x.reshape(feature_shape)\n            else:\n                pass\n            x = self.projects[i](x)\n            x = self.resize_layers[i](x)\n            out.append(x)\n        return out\n\n\nclass PreActResidualConvUnit(BaseModule):\n    \"\"\"ResidualConvUnit, pre-activate residual unit.\n    Args:\n        in_channels (int): number of channels in the input feature map.\n        act_cfg (dict): dictionary to construct and config activation layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        stride (int): stride of the first block. Default: 1\n        dilation (int): dilation rate for convs layers. Default: 1.\n        init_cfg (dict, optional): Initialization config dict. Default: None.\n    \"\"\"\n\n    def __init__(self, in_channels, act_cfg, norm_cfg, stride=1, dilation=1, init_cfg=None):\n        super(PreActResidualConvUnit, self).__init__(init_cfg)\n\n        self.conv1 = ConvModule(\n            in_channels,\n            in_channels,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            norm_cfg=norm_cfg,\n            act_cfg=act_cfg,\n            bias=False,\n            order=(\"act\", \"conv\", \"norm\"),\n        )\n\n        self.conv2 = ConvModule(\n            in_channels,\n            in_channels,\n            3,\n            padding=1,\n            norm_cfg=norm_cfg,\n            act_cfg=act_cfg,\n            bias=False,\n            order=(\"act\", \"conv\", \"norm\"),\n        )\n\n    def forward(self, inputs):\n        inputs_ = inputs.clone()\n        x = self.conv1(inputs)\n        x = self.conv2(x)\n        return x + inputs_\n\n\nclass FeatureFusionBlock(BaseModule):\n    \"\"\"FeatureFusionBlock, merge feature map from different stages.\n    Args:\n        in_channels (int): Input channels.\n        act_cfg (dict): The activation config for ResidualConvUnit.\n        norm_cfg (dict): Config dict for normalization layer.\n        expand (bool): Whether expand the channels in post process block.\n            Default: False.\n        align_corners (bool): align_corner setting for bilinear upsample.\n            Default: True.\n        init_cfg (dict, optional): Initialization config dict. Default: None.\n    \"\"\"\n\n    def __init__(self, in_channels, act_cfg, norm_cfg, expand=False, align_corners=True, init_cfg=None):\n        super(FeatureFusionBlock, self).__init__(init_cfg)\n\n        self.in_channels = in_channels\n        self.expand = expand\n        self.align_corners = align_corners\n\n        self.out_channels = in_channels\n        if self.expand:\n            self.out_channels = in_channels // 2\n\n        self.project = ConvModule(self.in_channels, self.out_channels, kernel_size=1, act_cfg=None, bias=True)\n\n        self.res_conv_unit1 = PreActResidualConvUnit(in_channels=self.in_channels, act_cfg=act_cfg, norm_cfg=norm_cfg)\n        self.res_conv_unit2 = PreActResidualConvUnit(in_channels=self.in_channels, act_cfg=act_cfg, norm_cfg=norm_cfg)\n\n    def forward(self, *inputs):\n        x = inputs[0]\n        if len(inputs) == 2:\n            if x.shape != inputs[1].shape:\n                res = resize(inputs[1], size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n            else:\n                res = inputs[1]\n            x = x + self.res_conv_unit1(res)\n        x = self.res_conv_unit2(x)\n        x = resize(x, scale_factor=2, mode=\"bilinear\", align_corners=self.align_corners)\n        x = self.project(x)\n        return x\n\n\n@HEADS.register_module()\nclass DPTHead(DepthBaseDecodeHead):\n    \"\"\"Vision Transformers for Dense Prediction.\n    This head is implemented of `DPT <https://arxiv.org/abs/2103.13413>`_.\n    Args:\n        embed_dims (int): The embed dimension of the ViT backbone.\n            Default: 768.\n        post_process_channels (List): Out channels of post process conv\n            layers. Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n        expand_channels (bool): Whether expand the channels in post process\n            block. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims=768,\n        post_process_channels=[96, 192, 384, 768],\n        readout_type=\"ignore\",\n        patch_size=16,\n        expand_channels=False,\n        **kwargs\n    ):\n        super(DPTHead, self).__init__(**kwargs)\n\n        self.in_channels = self.in_channels\n        self.expand_channels = expand_channels\n        self.reassemble_blocks = ReassembleBlocks(embed_dims, post_process_channels, readout_type, patch_size)\n\n        self.post_process_channels = [\n            channel * math.pow(2, i) if expand_channels else channel for i, channel in enumerate(post_process_channels)\n        ]\n        self.convs = nn.ModuleList()\n        for channel in self.post_process_channels:\n            self.convs.append(ConvModule(channel, self.channels, kernel_size=3, padding=1, act_cfg=None, bias=False))\n        self.fusion_blocks = nn.ModuleList()\n        for _ in range(len(self.convs)):\n            self.fusion_blocks.append(FeatureFusionBlock(self.channels, self.act_cfg, self.norm_cfg))\n        self.fusion_blocks[0].res_conv_unit1 = None\n        self.project = ConvModule(self.channels, self.channels, kernel_size=3, padding=1, norm_cfg=self.norm_cfg)\n        self.num_fusion_blocks = len(self.fusion_blocks)\n        self.num_reassemble_blocks = len(self.reassemble_blocks.resize_layers)\n        self.num_post_process_channels = len(self.post_process_channels)\n        assert self.num_fusion_blocks == self.num_reassemble_blocks\n        assert self.num_reassemble_blocks == self.num_post_process_channels\n        self.conv_depth = HeadDepth(self.channels)\n\n    def forward(self, inputs, img_metas):\n        assert len(inputs) == self.num_reassemble_blocks\n        x = [inp for inp in inputs]\n        x = self.reassemble_blocks(x)\n        x = [self.convs[i](feature) for i, feature in enumerate(x)]\n        out = self.fusion_blocks[0](x[-1])\n        for i in range(1, len(self.fusion_blocks)):\n            out = self.fusion_blocks[i](out, x[-(i + 1)])\n        out = self.project(out)\n        out = self.depth_pred(out)\n        return out\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .backbones import *  # noqa: F403\nfrom .builder import BACKBONES, DEPTHER, HEADS, LOSSES, build_backbone, build_depther, build_head, build_loss\nfrom .decode_heads import *  # noqa: F403\nfrom .depther import *  # noqa: F403\nfrom .losses import *  # noqa: F403\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/ops/wrappers.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport warnings\n\nimport torch.nn.functional as F\n\n\ndef resize(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None, warning=False):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if (\n                    (output_h > 1 and output_w > 1 and input_h > 1 and input_w > 1)\n                    and (output_h - 1) % (input_h - 1)\n                    and (output_w - 1) % (input_w - 1)\n                ):\n                    warnings.warn(\n                        f\"When align_corners={align_corners}, \"\n                        \"the output would more aligned if \"\n                        f\"input size {(input_h, input_w)} is `x+1` and \"\n                        f\"out size {(output_h, output_w)} is `nx+1`\"\n                    )\n    return F.interpolate(input, size, scale_factor, mode, align_corners)\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/backbones/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .vision_transformer import DinoVisionTransformer\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/ops/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .wrappers import resize\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/decode_heads/decode_head.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule, auto_fp16, force_fp32\n\nfrom ...ops import resize\nfrom ..builder import build_loss\n\n\nclass DepthBaseDecodeHead(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for BaseDecodeHead.\n\n    Args:\n        in_channels (List): Input channels.\n        channels (int): Channels after modules, before conv_depth.\n        conv_cfg (dict|None): Config of conv layers. Default: None.\n        act_cfg (dict): Config of activation layers.\n            Default: dict(type='ReLU')\n        loss_decode (dict): Config of decode loss.\n            Default: dict(type='SigLoss').\n        sampler (dict|None): The config of depth map sampler.\n            Default: None.\n        align_corners (bool): align_corners argument of F.interpolate.\n            Default: False.\n        min_depth (int): Min depth in dataset setting.\n            Default: 1e-3.\n        max_depth (int): Max depth in dataset setting.\n            Default: None.\n        norm_cfg (dict|None): Config of norm layers.\n            Default: None.\n        classify (bool): Whether predict depth in a cls.-reg. manner.\n            Default: False.\n        n_bins (int): The number of bins used in cls. step.\n            Default: 256.\n        bins_strategy (str): The discrete strategy used in cls. step.\n            Default: 'UD'.\n        norm_strategy (str): The norm strategy on cls. probability\n            distribution. Default: 'linear'\n        scale_up (str): Whether predict depth in a scale-up manner.\n            Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        channels=96,\n        conv_cfg=None,\n        act_cfg=dict(type=\"ReLU\"),\n        loss_decode=dict(type=\"SigLoss\", valid_mask=True, loss_weight=10),\n        sampler=None,\n        align_corners=False,\n        min_depth=1e-3,\n        max_depth=None,\n        norm_cfg=None,\n        classify=False,\n        n_bins=256,\n        bins_strategy=\"UD\",\n        norm_strategy=\"linear\",\n        scale_up=False,\n    ):\n        super(DepthBaseDecodeHead, self).__init__()\n\n        self.in_channels = in_channels\n        self.channels = channels\n        self.conv_cfg = conv_cfg\n        self.act_cfg = act_cfg\n        if isinstance(loss_decode, dict):\n            self.loss_decode = build_loss(loss_decode)\n        elif isinstance(loss_decode, (list, tuple)):\n            self.loss_decode = nn.ModuleList()\n            for loss in loss_decode:\n                self.loss_decode.append(build_loss(loss))\n        self.align_corners = align_corners\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n        self.norm_cfg = norm_cfg\n        self.classify = classify\n        self.n_bins = n_bins\n        self.scale_up = scale_up\n\n        if self.classify:\n            assert bins_strategy in [\"UD\", \"SID\"], \"Support bins_strategy: UD, SID\"\n            assert norm_strategy in [\"linear\", \"softmax\", \"sigmoid\"], \"Support norm_strategy: linear, softmax, sigmoid\"\n\n            self.bins_strategy = bins_strategy\n            self.norm_strategy = norm_strategy\n            self.softmax = nn.Softmax(dim=1)\n            self.conv_depth = nn.Conv2d(channels, n_bins, kernel_size=3, padding=1, stride=1)\n        else:\n            self.conv_depth = nn.Conv2d(channels, 1, kernel_size=3, padding=1, stride=1)\n\n        self.fp16_enabled = False\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def extra_repr(self):\n        \"\"\"Extra repr.\"\"\"\n        s = f\"align_corners={self.align_corners}\"\n        return s\n\n    @auto_fp16()\n    @abstractmethod\n    def forward(self, inputs, img_metas):\n        \"\"\"Placeholder of forward function.\"\"\"\n        pass\n\n    def forward_train(self, img, inputs, img_metas, depth_gt, train_cfg):\n        \"\"\"Forward function for training.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `depth/datasets/pipelines/formatting.py:Collect`.\n            depth_gt (Tensor): GT depth\n            train_cfg (dict): The training config.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n        depth_pred = self.forward(inputs, img_metas)\n        losses = self.losses(depth_pred, depth_gt)\n\n        log_imgs = self.log_images(img[0], depth_pred[0], depth_gt[0], img_metas[0])\n        losses.update(**log_imgs)\n\n        return losses\n\n    def forward_test(self, inputs, img_metas, test_cfg):\n        \"\"\"Forward function for testing.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `depth/datasets/pipelines/formatting.py:Collect`.\n            test_cfg (dict): The testing config.\n\n        Returns:\n            Tensor: Output depth map.\n        \"\"\"\n        return self.forward(inputs, img_metas)\n\n    def depth_pred(self, feat):\n        \"\"\"Prediction each pixel.\"\"\"\n        if self.classify:\n            logit = self.conv_depth(feat)\n\n            if self.bins_strategy == \"UD\":\n                bins = torch.linspace(self.min_depth, self.max_depth, self.n_bins, device=feat.device)\n            elif self.bins_strategy == \"SID\":\n                bins = torch.logspace(self.min_depth, self.max_depth, self.n_bins, device=feat.device)\n\n            # following Adabins, default linear\n            if self.norm_strategy == \"linear\":\n                logit = torch.relu(logit)\n                eps = 0.1\n                logit = logit + eps\n                logit = logit / logit.sum(dim=1, keepdim=True)\n            elif self.norm_strategy == \"softmax\":\n                logit = torch.softmax(logit, dim=1)\n            elif self.norm_strategy == \"sigmoid\":\n                logit = torch.sigmoid(logit)\n                logit = logit / logit.sum(dim=1, keepdim=True)\n\n            output = torch.einsum(\"ikmn,k->imn\", [logit, bins]).unsqueeze(dim=1)\n\n        else:\n            if self.scale_up:\n                output = self.sigmoid(self.conv_depth(feat)) * self.max_depth\n            else:\n                output = self.relu(self.conv_depth(feat)) + self.min_depth\n        return output\n\n    @force_fp32(apply_to=(\"depth_pred\",))\n    def losses(self, depth_pred, depth_gt):\n        \"\"\"Compute depth loss.\"\"\"\n        loss = dict()\n        depth_pred = resize(\n            input=depth_pred, size=depth_gt.shape[2:], mode=\"bilinear\", align_corners=self.align_corners, warning=False\n        )\n        if not isinstance(self.loss_decode, nn.ModuleList):\n            losses_decode = [self.loss_decode]\n        else:\n            losses_decode = self.loss_decode\n        for loss_decode in losses_decode:\n            if loss_decode.loss_name not in loss:\n                loss[loss_decode.loss_name] = loss_decode(depth_pred, depth_gt)\n            else:\n                loss[loss_decode.loss_name] += loss_decode(depth_pred, depth_gt)\n        return loss\n\n    def log_images(self, img_path, depth_pred, depth_gt, img_meta):\n        show_img = copy.deepcopy(img_path.detach().cpu().permute(1, 2, 0))\n        show_img = show_img.numpy().astype(np.float32)\n        show_img = mmcv.imdenormalize(\n            show_img,\n            img_meta[\"img_norm_cfg\"][\"mean\"],\n            img_meta[\"img_norm_cfg\"][\"std\"],\n            img_meta[\"img_norm_cfg\"][\"to_rgb\"],\n        )\n        show_img = np.clip(show_img, 0, 255)\n        show_img = show_img.astype(np.uint8)\n        show_img = show_img[:, :, ::-1]\n        show_img = show_img.transpose(0, 2, 1)\n        show_img = show_img.transpose(1, 0, 2)\n\n        depth_pred = depth_pred / torch.max(depth_pred)\n        depth_gt = depth_gt / torch.max(depth_gt)\n\n        depth_pred_color = copy.deepcopy(depth_pred.detach().cpu())\n        depth_gt_color = copy.deepcopy(depth_gt.detach().cpu())\n\n        return {\"img_rgb\": show_img, \"img_depth_pred\": depth_pred_color, \"img_depth_gt\": depth_gt_color}\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/decode_heads/linear_head.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom ...ops import resize\nfrom ..builder import HEADS\nfrom .decode_head import DepthBaseDecodeHead\n\n\n@HEADS.register_module()\nclass BNHead(DepthBaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n\n    def __init__(self, input_transform=\"resize_concat\", in_index=(0, 1, 2, 3), upsample=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_transform = input_transform\n        self.in_index = in_index\n        self.upsample = upsample\n        # self.bn = nn.SyncBatchNorm(self.in_channels)\n        if self.classify:\n            self.conv_depth = nn.Conv2d(self.channels, self.n_bins, kernel_size=1, padding=0, stride=1)\n        else:\n            self.conv_depth = nn.Conv2d(self.channels, 1, kernel_size=1, padding=0, stride=1)\n\n    def _transform_inputs(self, inputs):\n        \"\"\"Transform inputs for decoder.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"\n\n        if \"concat\" in self.input_transform:\n            inputs = [inputs[i] for i in self.in_index]\n            if \"resize\" in self.input_transform:\n                inputs = [\n                    resize(\n                        input=x,\n                        size=[s * self.upsample for s in inputs[0].shape[2:]],\n                        mode=\"bilinear\",\n                        align_corners=self.align_corners,\n                    )\n                    for x in inputs\n                ]\n            inputs = torch.cat(inputs, dim=1)\n        elif self.input_transform == \"multiple_select\":\n            inputs = [inputs[i] for i in self.in_index]\n        else:\n            inputs = inputs[self.in_index]\n\n        return inputs\n\n    def _forward_feature(self, inputs, img_metas=None, **kwargs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        ``self.cls_seg`` fc.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n        Returns:\n            feats (Tensor): A tensor of shape (batch_size, self.channels,\n                H, W) which is feature map for last layer of decoder head.\n        \"\"\"\n        # accept lists (for cls token)\n        inputs = list(inputs)\n        for i, x in enumerate(inputs):\n            if len(x) == 2:\n                x, cls_token = x[0], x[1]\n                if len(x.shape) == 2:\n                    x = x[:, :, None, None]\n                cls_token = cls_token[:, :, None, None].expand_as(x)\n                inputs[i] = torch.cat((x, cls_token), 1)\n            else:\n                x = x[0]\n                if len(x.shape) == 2:\n                    x = x[:, :, None, None]\n                inputs[i] = x\n        x = self._transform_inputs(inputs)\n        # feats = self.bn(x)\n        return x\n\n    def forward(self, inputs, img_metas=None, **kwargs):\n        \"\"\"Forward function.\"\"\"\n        output = self._forward_feature(inputs, img_metas=img_metas, **kwargs)\n        output = self.depth_pred(output)\n\n        return output\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/losses/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .gradientloss import GradientLoss\nfrom .sigloss import SigLoss\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/hooks/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .optimizer import DistOptimizerHook\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/losses/sigloss.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom ...models.builder import LOSSES\n\n\n@LOSSES.register_module()\nclass SigLoss(nn.Module):\n    \"\"\"SigLoss.\n\n        This follows `AdaBins <https://arxiv.org/abs/2011.14141>`_.\n\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n        warm_up (bool): A simple warm up stage to help convergence. Default: False.\n        warm_iter (int): The number of warm up stage. Default: 100.\n    \"\"\"\n\n    def __init__(\n        self, valid_mask=True, loss_weight=1.0, max_depth=None, warm_up=False, warm_iter=100, loss_name=\"sigloss\"\n    ):\n        super(SigLoss, self).__init__()\n        self.valid_mask = valid_mask\n        self.loss_weight = loss_weight\n        self.max_depth = max_depth\n        self.loss_name = loss_name\n\n        self.eps = 0.001  # avoid grad explode\n\n        # HACK: a hack implementation for warmup sigloss\n        self.warm_up = warm_up\n        self.warm_iter = warm_iter\n        self.warm_up_counter = 0\n\n    def sigloss(self, input, target):\n        if self.valid_mask:\n            valid_mask = target > 0\n            if self.max_depth is not None:\n                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n            input = input[valid_mask]\n            target = target[valid_mask]\n\n        if self.warm_up:\n            if self.warm_up_counter < self.warm_iter:\n                g = torch.log(input + self.eps) - torch.log(target + self.eps)\n                g = 0.15 * torch.pow(torch.mean(g), 2)\n                self.warm_up_counter += 1\n                return torch.sqrt(g)\n\n        g = torch.log(input + self.eps) - torch.log(target + self.eps)\n        Dg = torch.var(g) + 0.15 * torch.pow(torch.mean(g), 2)\n        return torch.sqrt(Dg)\n\n    def forward(self, depth_pred, depth_gt):\n        \"\"\"Forward function.\"\"\"\n\n        loss_depth = self.loss_weight * self.sigloss(depth_pred, depth_gt)\n        return loss_depth\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/decode_heads/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .dpt_head import DPTHead\nfrom .linear_head import BNHead\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/depther/base.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import OrderedDict\n\nimport torch\nimport torch.distributed as dist\nfrom mmcv.runner import BaseModule, auto_fp16\n\n\nclass BaseDepther(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for depther.\"\"\"\n\n    def __init__(self, init_cfg=None):\n        super(BaseDepther, self).__init__(init_cfg)\n        self.fp16_enabled = False\n\n    @property\n    def with_neck(self):\n        \"\"\"bool: whether the depther has neck\"\"\"\n        return hasattr(self, \"neck\") and self.neck is not None\n\n    @property\n    def with_auxiliary_head(self):\n        \"\"\"bool: whether the depther has auxiliary head\"\"\"\n        return hasattr(self, \"auxiliary_head\") and self.auxiliary_head is not None\n\n    @property\n    def with_decode_head(self):\n        \"\"\"bool: whether the depther has decode head\"\"\"\n        return hasattr(self, \"decode_head\") and self.decode_head is not None\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        \"\"\"Placeholder for extract features from images.\"\"\"\n        pass\n\n    @abstractmethod\n    def encode_decode(self, img, img_metas):\n        \"\"\"Placeholder for encode images with backbone and decode into a\n        semantic depth map of the same size as input.\"\"\"\n        pass\n\n    @abstractmethod\n    def forward_train(self, imgs, img_metas, **kwargs):\n        \"\"\"Placeholder for Forward function for training.\"\"\"\n        pass\n\n    @abstractmethod\n    def simple_test(self, img, img_meta, **kwargs):\n        \"\"\"Placeholder for single image test.\"\"\"\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        \"\"\"Placeholder for augmentation test.\"\"\"\n        pass\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        \"\"\"\n        Args:\n            imgs (List[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (List[List[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch.\n        \"\"\"\n        for var, name in [(imgs, \"imgs\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(f\"{name} must be a list, but got \" f\"{type(var)}\")\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(f\"num of augmentations ({len(imgs)}) != \" f\"num of image meta ({len(img_metas)})\")\n        # all images in the same aug batch all of the same ori_shape and pad\n        # shape\n        for img_meta in img_metas:\n            ori_shapes = [_[\"ori_shape\"] for _ in img_meta]\n            assert all(shape == ori_shapes[0] for shape in ori_shapes)\n            img_shapes = [_[\"img_shape\"] for _ in img_meta]\n            assert all(shape == img_shapes[0] for shape in img_shapes)\n            pad_shapes = [_[\"pad_shape\"] for _ in img_meta]\n            assert all(shape == pad_shapes[0] for shape in pad_shapes)\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    @auto_fp16(apply_to=(\"img\",))\n    def forward(self, img, img_metas, return_loss=True, **kwargs):\n        \"\"\"Calls either :func:`forward_train` or :func:`forward_test` depending\n        on whether ``return_loss`` is ``True``.\n\n        Note this setting will change the expected inputs. When\n        ``return_loss=True``, img and img_meta are single-nested (i.e. Tensor\n        and List[dict]), and when ``resturn_loss=False``, img and img_meta\n        should be double nested (i.e.  List[Tensor], List[List[dict]]), with\n        the outer list indicating test time augmentations.\n        \"\"\"\n        if return_loss:\n            return self.forward_train(img, img_metas, **kwargs)\n        else:\n            return self.forward_test(img, img_metas, **kwargs)\n\n    def train_step(self, data_batch, optimizer, **kwargs):\n        \"\"\"The iteration step during training.\n\n        This method defines an iteration step during training, except for the\n        back propagation and optimizer updating, which are done in an optimizer\n        hook. Note that in some complicated cases or models, the whole process\n        including back propagation and optimizer updating is also defined in\n        this method, such as GAN.\n\n        Args:\n            data (dict): The output of dataloader.\n            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n                runner is passed to ``train_step()``. This argument is unused\n                and reserved.\n\n        Returns:\n            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,\n                ``num_samples``.\n                ``loss`` is a tensor for back propagation, which can be a\n                weighted sum of multiple losses.\n                ``log_vars`` contains all the variables to be sent to the\n                logger.\n                ``num_samples`` indicates the batch size (when the model is\n                DDP, it means the batch size on each GPU), which is used for\n                averaging the logs.\n        \"\"\"\n        losses = self(**data_batch)\n\n        # split losses and images\n        real_losses = {}\n        log_imgs = {}\n        for k, v in losses.items():\n            if \"img\" in k:\n                log_imgs[k] = v\n            else:\n                real_losses[k] = v\n\n        loss, log_vars = self._parse_losses(real_losses)\n\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data_batch[\"img_metas\"]), log_imgs=log_imgs)\n\n        return outputs\n\n    def val_step(self, data_batch, **kwargs):\n        \"\"\"The iteration step during validation.\n\n        This method shares the same signature as :func:`train_step`, but used\n        during val epochs. Note that the evaluation after training epochs is\n        not implemented with this method, but an evaluation hook.\n        \"\"\"\n        output = self(**data_batch, **kwargs)\n        return output\n\n    @staticmethod\n    def _parse_losses(losses):\n        \"\"\"Parse the raw outputs (losses) of the network.\n\n        Args:\n            losses (dict): Raw output of the network, which usually contain\n                losses and other necessary information.\n\n        Returns:\n            tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor\n                which may be a weighted sum of all losses, log_vars contains\n                all the variables to be sent to the logger.\n        \"\"\"\n        log_vars = OrderedDict()\n        for loss_name, loss_value in losses.items():\n            if isinstance(loss_value, torch.Tensor):\n                log_vars[loss_name] = loss_value.mean()\n            elif isinstance(loss_value, list):\n                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n            else:\n                raise TypeError(f\"{loss_name} is not a tensor or list of tensors\")\n\n        loss = sum(_value for _key, _value in log_vars.items() if \"loss\" in _key)\n\n        log_vars[\"loss\"] = loss\n        for loss_name, loss_value in log_vars.items():\n            # reduce loss when distributed training\n            if dist.is_available() and dist.is_initialized():\n                loss_value = loss_value.data.clone()\n                dist.all_reduce(loss_value.div_(dist.get_world_size()))\n            log_vars[loss_name] = loss_value.item()\n\n        return loss, log_vars\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/metrics.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom enum import Enum\nimport logging\nfrom typing import Any, Dict, Optional\n\nimport torch\nfrom torch import Tensor\nfrom torchmetrics import Metric, MetricCollection\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom torchmetrics.utilities.data import dim_zero_cat, select_topk\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\nclass MetricType(Enum):\n    MEAN_ACCURACY = \"mean_accuracy\"\n    MEAN_PER_CLASS_ACCURACY = \"mean_per_class_accuracy\"\n    PER_CLASS_ACCURACY = \"per_class_accuracy\"\n    IMAGENET_REAL_ACCURACY = \"imagenet_real_accuracy\"\n\n    @property\n    def accuracy_averaging(self):\n        return getattr(AccuracyAveraging, self.name, None)\n\n    def __str__(self):\n        return self.value\n\n\nclass AccuracyAveraging(Enum):\n    MEAN_ACCURACY = \"micro\"\n    MEAN_PER_CLASS_ACCURACY = \"macro\"\n    PER_CLASS_ACCURACY = \"none\"\n\n    def __str__(self):\n        return self.value\n\n\ndef build_metric(metric_type: MetricType, *, num_classes: int, ks: Optional[tuple] = None):\n    if metric_type.accuracy_averaging is not None:\n        return build_topk_accuracy_metric(\n            average_type=metric_type.accuracy_averaging,\n            num_classes=num_classes,\n            ks=(1, 5) if ks is None else ks,\n        )\n    elif metric_type == MetricType.IMAGENET_REAL_ACCURACY:\n        return build_topk_imagenet_real_accuracy_metric(\n            num_classes=num_classes,\n            ks=(1, 5) if ks is None else ks,\n        )\n\n    raise ValueError(f\"Unknown metric type {metric_type}\")\n\n\ndef build_topk_accuracy_metric(average_type: AccuracyAveraging, num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {\n        f\"top-{k}\": MulticlassAccuracy(top_k=k, num_classes=int(num_classes), average=average_type.value) for k in ks\n    }\n    return MetricCollection(metrics)\n\n\ndef build_topk_imagenet_real_accuracy_metric(num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {f\"top-{k}\": ImageNetReaLAccuracy(top_k=k, num_classes=int(num_classes)) for k in ks}\n    return MetricCollection(metrics)\n\n\nclass ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n\n    def __init__(\n        self,\n        num_classes: int,\n        top_k: int = 1,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(**kwargs)\n        self.num_classes = num_classes\n        self.top_k = top_k\n        self.add_state(\"tp\", [], dist_reduce_fx=\"cat\")\n\n    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore\n        # preds [B, D]\n        # target [B, A]\n        # preds_oh [B, D] with 0 and 1\n        # select top K highest probabilities, use one hot representation\n        preds_oh = select_topk(preds, self.top_k)\n        # target_oh [B, D + 1] with 0 and 1\n        target_oh = torch.zeros((preds_oh.shape[0], preds_oh.shape[1] + 1), device=target.device, dtype=torch.int32)\n        target = target.long()\n        # for undefined targets (-1) use a fake value `num_classes`\n        target[target == -1] = self.num_classes\n        # fill targets, use one hot representation\n        target_oh.scatter_(1, target, 1)\n        # target_oh [B, D] (remove the fake target at index `num_classes`)\n        target_oh = target_oh[:, :-1]\n        # tp [B] with 0 and 1\n        tp = (preds_oh * target_oh == 1).sum(dim=1)\n        # at least one match between prediction and target\n        tp.clip_(max=1)\n        # ignore instances where no targets are defined\n        mask = target_oh.sum(dim=1) > 0\n        tp = tp[mask]\n        self.tp.append(tp)  # type: ignore\n\n    def compute(self) -> Tensor:\n        tp = dim_zero_cat(self.tp)  # type: ignore\n        return tp.float().mean()\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/backbones/vision_transformer.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom mmcv.runner import BaseModule\n\nfrom ..builder import BACKBONES\n\n\n@BACKBONES.register_module()\nclass DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/depther/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .base import BaseDepther\nfrom .encoder_decoder import DepthEncoderDecoder\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/depth/models/losses/gradientloss.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom ...models.builder import LOSSES\n\n\n@LOSSES.register_module()\nclass GradientLoss(nn.Module):\n    \"\"\"GradientLoss.\n\n    Adapted from https://www.cs.cornell.edu/projects/megadepth/\n\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n    \"\"\"\n\n    def __init__(self, valid_mask=True, loss_weight=1.0, max_depth=None, loss_name=\"loss_grad\"):\n        super(GradientLoss, self).__init__()\n        self.valid_mask = valid_mask\n        self.loss_weight = loss_weight\n        self.max_depth = max_depth\n        self.loss_name = loss_name\n\n        self.eps = 0.001  # avoid grad explode\n\n    def gradientloss(self, input, target):\n        input_downscaled = [input] + [input[:: 2 * i, :: 2 * i] for i in range(1, 4)]\n        target_downscaled = [target] + [target[:: 2 * i, :: 2 * i] for i in range(1, 4)]\n\n        gradient_loss = 0\n        for input, target in zip(input_downscaled, target_downscaled):\n            if self.valid_mask:\n                mask = target > 0\n                if self.max_depth is not None:\n                    mask = torch.logical_and(target > 0, target <= self.max_depth)\n                N = torch.sum(mask)\n            else:\n                mask = torch.ones_like(target)\n                N = input.numel()\n            input_log = torch.log(input + self.eps)\n            target_log = torch.log(target + self.eps)\n            log_d_diff = input_log - target_log\n\n            log_d_diff = torch.mul(log_d_diff, mask)\n\n            v_gradient = torch.abs(log_d_diff[0:-2, :] - log_d_diff[2:, :])\n            v_mask = torch.mul(mask[0:-2, :], mask[2:, :])\n            v_gradient = torch.mul(v_gradient, v_mask)\n\n            h_gradient = torch.abs(log_d_diff[:, 0:-2] - log_d_diff[:, 2:])\n            h_mask = torch.mul(mask[:, 0:-2], mask[:, 2:])\n            h_gradient = torch.mul(h_gradient, h_mask)\n\n            gradient_loss += (torch.sum(h_gradient) + torch.sum(v_gradient)) / N\n\n        return gradient_loss\n\n    def forward(self, depth_pred, depth_gt):\n        \"\"\"Forward function.\"\"\"\n\n        gradient_loss = self.loss_weight * self.gradientloss(depth_pred, depth_gt)\n        return gradient_loss\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/knn.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport argparse\nfrom functools import partial\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import List, Optional\n\nimport torch\nfrom torch.nn.functional import one_hot, softmax\n\nimport dinov2.distributed as distributed\nfrom dinov2.data import SamplerType, make_data_loader, make_dataset\nfrom dinov2.data.transforms import make_classification_eval_transform\nfrom dinov2.eval.metrics import AccuracyAveraging, build_topk_accuracy_metric\nfrom dinov2.eval.setup import get_args_parser as get_setup_args_parser\nfrom dinov2.eval.setup import setup_and_build_model\nfrom dinov2.eval.utils import ModelWithNormalize, evaluate, extract_features\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents,\n        add_help=add_help,\n    )\n    parser.add_argument(\n        \"--train-dataset\",\n        dest=\"train_dataset_str\",\n        type=str,\n        help=\"Training dataset\",\n    )\n    parser.add_argument(\n        \"--val-dataset\",\n        dest=\"val_dataset_str\",\n        type=str,\n        help=\"Validation dataset\",\n    )\n    parser.add_argument(\n        \"--nb_knn\",\n        nargs=\"+\",\n        type=int,\n        help=\"Number of NN to use. 20 is usually working the best.\",\n    )\n    parser.add_argument(\n        \"--temperature\",\n        type=float,\n        help=\"Temperature used in the voting coefficient\",\n    )\n    parser.add_argument(\n        \"--gather-on-cpu\",\n        action=\"store_true\",\n        help=\"Whether to gather the train features on cpu, slower\"\n        \"but useful to avoid OOM for large datasets (e.g. ImageNet22k).\",\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        help=\"Batch size.\",\n    )\n    parser.add_argument(\n        \"--n-per-class-list\",\n        nargs=\"+\",\n        type=int,\n        help=\"Number to take per class\",\n    )\n    parser.add_argument(\n        \"--n-tries\",\n        type=int,\n        help=\"Number of tries\",\n    )\n    parser.set_defaults(\n        train_dataset_str=\"ImageNet:split=TRAIN\",\n        val_dataset_str=\"ImageNet:split=VAL\",\n        nb_knn=[10, 20, 100, 200],\n        temperature=0.07,\n        batch_size=256,\n        n_per_class_list=[-1],\n        n_tries=1,\n    )\n    return parser\n\n\nclass KnnModule(torch.nn.Module):\n    \"\"\"\n    Gets knn of test features from all processes on a chunk of the train features\n\n    Each rank gets a chunk of the train features as well as a chunk of the test features.\n    In `compute_neighbors`, for each rank one after the other, its chunk of test features\n    is sent to all devices, partial knns are computed with each chunk of train features\n    then collated back on the original device.\n    \"\"\"\n\n    def __init__(self, train_features, train_labels, nb_knn, T, device, num_classes=1000):\n        super().__init__()\n\n        self.global_rank = distributed.get_global_rank()\n        self.global_size = distributed.get_global_size()\n\n        self.device = device\n        self.train_features_rank_T = train_features.chunk(self.global_size)[self.global_rank].T.to(self.device)\n        self.candidates = train_labels.chunk(self.global_size)[self.global_rank].view(1, -1).to(self.device)\n\n        self.nb_knn = nb_knn\n        self.max_k = max(self.nb_knn)\n        self.T = T\n        self.num_classes = num_classes\n\n    def _get_knn_sims_and_labels(self, similarity, train_labels):\n        topk_sims, indices = similarity.topk(self.max_k, largest=True, sorted=True)\n        neighbors_labels = torch.gather(train_labels, 1, indices)\n        return topk_sims, neighbors_labels\n\n    def _similarity_for_rank(self, features_rank, source_rank):\n        # Send the features from `source_rank` to all ranks\n        broadcast_shape = torch.tensor(features_rank.shape).to(self.device)\n        torch.distributed.broadcast(broadcast_shape, source_rank)\n\n        broadcasted = features_rank\n        if self.global_rank != source_rank:\n            broadcasted = torch.zeros(*broadcast_shape, dtype=features_rank.dtype, device=self.device)\n        torch.distributed.broadcast(broadcasted, source_rank)\n\n        # Compute the neighbors for `source_rank` among `train_features_rank_T`\n        similarity_rank = torch.mm(broadcasted, self.train_features_rank_T)\n        candidate_labels = self.candidates.expand(len(similarity_rank), -1)\n        return self._get_knn_sims_and_labels(similarity_rank, candidate_labels)\n\n    def _gather_all_knn_for_rank(self, topk_sims, neighbors_labels, target_rank):\n        # Gather all neighbors for `target_rank`\n        topk_sims_rank = retrieved_rank = None\n        if self.global_rank == target_rank:\n            topk_sims_rank = [torch.zeros_like(topk_sims) for _ in range(self.global_size)]\n            retrieved_rank = [torch.zeros_like(neighbors_labels) for _ in range(self.global_size)]\n\n        torch.distributed.gather(topk_sims, topk_sims_rank, dst=target_rank)\n        torch.distributed.gather(neighbors_labels, retrieved_rank, dst=target_rank)\n\n        if self.global_rank == target_rank:\n            # Perform a second top-k on the k * global_size retrieved neighbors\n            topk_sims_rank = torch.cat(topk_sims_rank, dim=1)\n            retrieved_rank = torch.cat(retrieved_rank, dim=1)\n            results = self._get_knn_sims_and_labels(topk_sims_rank, retrieved_rank)\n            return results\n        return None\n\n    def compute_neighbors(self, features_rank):\n        for rank in range(self.global_size):\n            topk_sims, neighbors_labels = self._similarity_for_rank(features_rank, rank)\n            results = self._gather_all_knn_for_rank(topk_sims, neighbors_labels, rank)\n            if results is not None:\n                topk_sims_rank, neighbors_labels_rank = results\n        return topk_sims_rank, neighbors_labels_rank\n\n    def forward(self, features_rank):\n        \"\"\"\n        Compute the results on all values of `self.nb_knn` neighbors from the full `self.max_k`\n        \"\"\"\n        assert all(k <= self.max_k for k in self.nb_knn)\n\n        topk_sims, neighbors_labels = self.compute_neighbors(features_rank)\n        batch_size = neighbors_labels.shape[0]\n        topk_sims_transform = softmax(topk_sims / self.T, 1)\n        matmul = torch.mul(\n            one_hot(neighbors_labels, num_classes=self.num_classes),\n            topk_sims_transform.view(batch_size, -1, 1),\n        )\n        probas_for_k = {k: torch.sum(matmul[:, :k, :], 1) for k in self.nb_knn}\n        return probas_for_k\n\n\nclass DictKeysModule(torch.nn.Module):\n    def __init__(self, keys):\n        super().__init__()\n        self.keys = keys\n\n    def forward(self, features_dict, targets):\n        for k in self.keys:\n            features_dict = features_dict[k]\n        return {\"preds\": features_dict, \"target\": targets}\n\n\ndef create_module_dict(*, module, n_per_class_list, n_tries, nb_knn, train_features, train_labels):\n    modules = {}\n    mapping = create_class_indices_mapping(train_labels)\n    for npc in n_per_class_list:\n        if npc < 0:  # Only one try needed when using the full data\n            full_module = module(\n                train_features=train_features,\n                train_labels=train_labels,\n                nb_knn=nb_knn,\n            )\n            modules[\"full\"] = ModuleDictWithForward({\"1\": full_module})\n            continue\n        all_tries = {}\n        for t in range(n_tries):\n            final_indices = filter_train(mapping, npc, seed=t)\n            k_list = list(set(nb_knn + [npc]))\n            k_list = sorted([el for el in k_list if el <= npc])\n            all_tries[str(t)] = module(\n                train_features=train_features[final_indices],\n                train_labels=train_labels[final_indices],\n                nb_knn=k_list,\n            )\n        modules[f\"{npc} per class\"] = ModuleDictWithForward(all_tries)\n\n    return ModuleDictWithForward(modules)\n\n\ndef filter_train(mapping, n_per_class, seed):\n    torch.manual_seed(seed)\n    final_indices = []\n    for k in mapping.keys():\n        index = torch.randperm(len(mapping[k]))[:n_per_class]\n        final_indices.append(mapping[k][index])\n    return torch.cat(final_indices).squeeze()\n\n\ndef create_class_indices_mapping(labels):\n    unique_labels, inverse = torch.unique(labels, return_inverse=True)\n    mapping = {unique_labels[i]: (inverse == i).nonzero() for i in range(len(unique_labels))}\n    return mapping\n\n\nclass ModuleDictWithForward(torch.nn.ModuleDict):\n    def forward(self, *args, **kwargs):\n        return {k: module(*args, **kwargs) for k, module in self._modules.items()}\n\n\ndef eval_knn(\n    model,\n    train_dataset,\n    val_dataset,\n    accuracy_averaging,\n    nb_knn,\n    temperature,\n    batch_size,\n    num_workers,\n    gather_on_cpu,\n    n_per_class_list=[-1],\n    n_tries=1,\n):\n    model = ModelWithNormalize(model)\n\n    logger.info(\"Extracting features for train set...\")\n    train_features, train_labels = extract_features(\n        model, train_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n    )\n    logger.info(f\"Train features created, shape {train_features.shape}.\")\n\n    val_dataloader = make_data_loader(\n        dataset=val_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,\n        drop_last=False,\n        shuffle=False,\n        persistent_workers=True,\n    )\n    num_classes = train_labels.max() + 1\n    metric_collection = build_topk_accuracy_metric(accuracy_averaging, num_classes=num_classes)\n\n    device = torch.cuda.current_device()\n    partial_module = partial(KnnModule, T=temperature, device=device, num_classes=num_classes)\n    knn_module_dict = create_module_dict(\n        module=partial_module,\n        n_per_class_list=n_per_class_list,\n        n_tries=n_tries,\n        nb_knn=nb_knn,\n        train_features=train_features,\n        train_labels=train_labels,\n    )\n    postprocessors, metrics = {}, {}\n    for n_per_class, knn_module in knn_module_dict.items():\n        for t, knn_try in knn_module.items():\n            postprocessors = {\n                **postprocessors,\n                **{(n_per_class, t, k): DictKeysModule([n_per_class, t, k]) for k in knn_try.nb_knn},\n            }\n            metrics = {**metrics, **{(n_per_class, t, k): metric_collection.clone() for k in knn_try.nb_knn}}\n    model_with_knn = torch.nn.Sequential(model, knn_module_dict)\n\n    # ============ evaluation ... ============\n    logger.info(\"Start the k-NN classification.\")\n    _, results_dict = evaluate(model_with_knn, val_dataloader, postprocessors, metrics, device)\n\n    # Averaging the results over the n tries for each value of n_per_class\n    for n_per_class, knn_module in knn_module_dict.items():\n        first_try = list(knn_module.keys())[0]\n        k_list = knn_module[first_try].nb_knn\n        for k in k_list:\n            keys = results_dict[(n_per_class, first_try, k)].keys()  # keys are e.g. `top-1` and `top-5`\n            results_dict[(n_per_class, k)] = {\n                key: torch.mean(torch.stack([results_dict[(n_per_class, t, k)][key] for t in knn_module.keys()]))\n                for key in keys\n            }\n            for t in knn_module.keys():\n                del results_dict[(n_per_class, t, k)]\n\n    return results_dict\n\n\ndef eval_knn_with_model(\n    model,\n    output_dir,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    nb_knn=(10, 20, 100, 200),\n    temperature=0.07,\n    autocast_dtype=torch.float,\n    accuracy_averaging=AccuracyAveraging.MEAN_ACCURACY,\n    transform=None,\n    gather_on_cpu=False,\n    batch_size=256,\n    num_workers=5,\n    n_per_class_list=[-1],\n    n_tries=1,\n):\n    transform = transform or make_classification_eval_transform()\n\n    train_dataset = make_dataset(\n        dataset_str=train_dataset_str,\n        transform=transform,\n    )\n    val_dataset = make_dataset(\n        dataset_str=val_dataset_str,\n        transform=transform,\n    )\n\n    with torch.cuda.amp.autocast(dtype=autocast_dtype):\n        results_dict_knn = eval_knn(\n            model=model,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            accuracy_averaging=accuracy_averaging,\n            nb_knn=nb_knn,\n            temperature=temperature,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            gather_on_cpu=gather_on_cpu,\n            n_per_class_list=n_per_class_list,\n            n_tries=n_tries,\n        )\n\n    results_dict = {}\n    if distributed.is_main_process():\n        for knn_ in results_dict_knn.keys():\n            top1 = results_dict_knn[knn_][\"top-1\"].item() * 100.0\n            top5 = results_dict_knn[knn_][\"top-5\"].item() * 100.0\n            results_dict[f\"{knn_} Top 1\"] = top1\n            results_dict[f\"{knn_} Top 5\"] = top5\n            logger.info(f\"{knn_} classifier result: Top1: {top1:.2f} Top5: {top5:.2f}\")\n\n    metrics_file_path = os.path.join(output_dir, \"results_eval_knn.json\")\n    with open(metrics_file_path, \"a\") as f:\n        for k, v in results_dict.items():\n            f.write(json.dumps({k: v}) + \"\\n\")\n\n    if distributed.is_enabled():\n        torch.distributed.barrier()\n    return results_dict\n\n\ndef main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_knn_with_model(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        nb_knn=args.nb_knn,\n        temperature=args.temperature,\n        autocast_dtype=autocast_dtype,\n        accuracy_averaging=AccuracyAveraging.MEAN_ACCURACY,\n        transform=None,\n        gather_on_cpu=args.gather_on_cpu,\n        batch_size=args.batch_size,\n        num_workers=5,\n        n_per_class_list=args.n_per_class_list,\n        n_tries=args.n_tries,\n    )\n    return 0\n\n\nif __name__ == \"__main__\":\n    description = \"DINOv2 k-NN evaluation\"\n    args_parser = get_args_parser(description=description)\n    args = args_parser.parse_args()\n    sys.exit(main(args))\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/models/backbones/vision_transformer.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom mmcv.runner import BaseModule\nfrom mmseg.models.builder import BACKBONES\n\n\n@BACKBONES.register_module()\nclass DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/hooks/optimizer.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\ntry:\n    import apex\nexcept ImportError:\n    print(\"apex is not installed\")\n\nfrom mmcv.runner import OptimizerHook, HOOKS\n\n\n@HOOKS.register_module()\nclass DistOptimizerHook(OptimizerHook):\n    \"\"\"Optimizer hook for distributed training.\"\"\"\n\n    def __init__(self, update_interval=1, grad_clip=None, coalesce=True, bucket_size_mb=-1, use_fp16=False):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n        self.update_interval = update_interval\n        self.use_fp16 = use_fp16\n\n    def before_run(self, runner):\n        runner.optimizer.zero_grad()\n\n    def after_train_iter(self, runner):\n        runner.outputs[\"loss\"] /= self.update_interval\n        if self.use_fp16:\n            # runner.outputs['loss'].backward()\n            with apex.amp.scale_loss(runner.outputs[\"loss\"], runner.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            runner.outputs[\"loss\"].backward()\n        if self.every_n_iters(runner, self.update_interval):\n            if self.grad_clip is not None:\n                self.clip_grads(runner.model.parameters())\n            runner.optimizer.step()\n            runner.optimizer.zero_grad()\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/models/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .backbones import *  # noqa: F403\nfrom .decode_heads import *  # noqa: F403\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/log_regression.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport gc\nimport logging\nimport sys\nimport time\nfrom typing import List, Optional\n\nfrom cuml.linear_model import LogisticRegression\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed\nfrom torch import nn\nfrom torch.utils.data import TensorDataset\nfrom torchmetrics import MetricTracker\n\nfrom dinov2.data import make_dataset\nfrom dinov2.data.transforms import make_classification_eval_transform\nfrom dinov2.distributed import get_global_rank, get_global_size\nfrom dinov2.eval.metrics import MetricType, build_metric\nfrom dinov2.eval.setup import get_args_parser as get_setup_args_parser\nfrom dinov2.eval.setup import setup_and_build_model\nfrom dinov2.eval.utils import evaluate, extract_features\nfrom dinov2.utils.dtype import as_torch_dtype\n\n\nlogger = logging.getLogger(\"dinov2\")\n\nDEFAULT_MAX_ITER = 1_000\nC_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\n\n\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents,\n        add_help=add_help,\n    )\n    parser.add_argument(\n        \"--train-dataset\",\n        dest=\"train_dataset_str\",\n        type=str,\n        help=\"Training dataset\",\n    )\n    parser.add_argument(\n        \"--val-dataset\",\n        dest=\"val_dataset_str\",\n        type=str,\n        help=\"Validation dataset\",\n    )\n    parser.add_argument(\n        \"--finetune-dataset-str\",\n        dest=\"finetune_dataset_str\",\n        type=str,\n        help=\"Fine-tuning dataset\",\n    )\n    parser.add_argument(\n        \"--finetune-on-val\",\n        action=\"store_true\",\n        help=\"If there is no finetune dataset, whether to choose the \"\n        \"hyperparameters on the val set instead of 10%% of the train dataset\",\n    )\n    parser.add_argument(\n        \"--metric-type\",\n        type=MetricType,\n        choices=list(MetricType),\n        help=\"Metric type\",\n    )\n    parser.add_argument(\n        \"--train-features-device\",\n        type=str,\n        help=\"Device to gather train features (cpu, cuda, cuda:0, etc.), default: %(default)s\",\n    )\n    parser.add_argument(\n        \"--train-dtype\",\n        type=str,\n        help=\"Data type to convert the train features to (default: %(default)s)\",\n    )\n    parser.add_argument(\n        \"--max-train-iters\",\n        type=int,\n        help=\"Maximum number of train iterations (default: %(default)s)\",\n    )\n    parser.set_defaults(\n        train_dataset_str=\"ImageNet:split=TRAIN\",\n        val_dataset_str=\"ImageNet:split=VAL\",\n        finetune_dataset_str=None,\n        metric_type=MetricType.MEAN_ACCURACY,\n        train_features_device=\"cpu\",\n        train_dtype=\"float64\",\n        max_train_iters=DEFAULT_MAX_ITER,\n        finetune_on_val=False,\n    )\n    return parser\n\n\nclass LogRegModule(nn.Module):\n    def __init__(\n        self,\n        C,\n        max_iter=DEFAULT_MAX_ITER,\n        dtype=torch.float64,\n        device=_CPU_DEVICE,\n    ):\n        super().__init__()\n        self.dtype = dtype\n        self.device = device\n        self.estimator = LogisticRegression(\n            penalty=\"l2\",\n            C=C,\n            max_iter=max_iter,\n            output_type=\"numpy\",\n            tol=1e-12,\n            linesearch_max_iter=50,\n        )\n\n    def forward(self, samples, targets):\n        samples_device = samples.device\n        samples = samples.to(dtype=self.dtype, device=self.device)\n        if self.device == _CPU_DEVICE:\n            samples = samples.numpy()\n        probas = self.estimator.predict_proba(samples)\n        return {\"preds\": torch.from_numpy(probas).to(samples_device), \"target\": targets}\n\n    def fit(self, train_features, train_labels):\n        train_features = train_features.to(dtype=self.dtype, device=self.device)\n        train_labels = train_labels.to(dtype=self.dtype, device=self.device)\n        if self.device == _CPU_DEVICE:\n            # both cuML and sklearn only work with numpy arrays on CPU\n            train_features = train_features.numpy()\n            train_labels = train_labels.numpy()\n        self.estimator.fit(train_features, train_labels)\n\n\ndef evaluate_model(*, logreg_model, logreg_metric, test_data_loader, device):\n    postprocessors = {\"metrics\": logreg_model}\n    metrics = {\"metrics\": logreg_metric}\n    return evaluate(nn.Identity(), test_data_loader, postprocessors, metrics, device)\n\n\ndef train_for_C(*, C, max_iter, train_features, train_labels, dtype=torch.float64, device=_CPU_DEVICE):\n    logreg_model = LogRegModule(C, max_iter=max_iter, dtype=dtype, device=device)\n    logreg_model.fit(train_features, train_labels)\n    return logreg_model\n\n\ndef train_and_evaluate(\n    *,\n    C,\n    max_iter,\n    train_features,\n    train_labels,\n    logreg_metric,\n    test_data_loader,\n    train_dtype=torch.float64,\n    train_features_device,\n    eval_device,\n):\n    logreg_model = train_for_C(\n        C=C,\n        max_iter=max_iter,\n        train_features=train_features,\n        train_labels=train_labels,\n        dtype=train_dtype,\n        device=train_features_device,\n    )\n    return evaluate_model(\n        logreg_model=logreg_model,\n        logreg_metric=logreg_metric,\n        test_data_loader=test_data_loader,\n        device=eval_device,\n    )\n\n\ndef sweep_C_values(\n    *,\n    train_features,\n    train_labels,\n    test_data_loader,\n    metric_type,\n    num_classes,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,\n    max_train_iters=DEFAULT_MAX_ITER,\n):\n    if metric_type == MetricType.PER_CLASS_ACCURACY:\n        # If we want to output per-class accuracy, we select the hyperparameters with mean per class\n        metric_type = MetricType.MEAN_PER_CLASS_ACCURACY\n    logreg_metric = build_metric(metric_type, num_classes=num_classes)\n    metric_tracker = MetricTracker(logreg_metric, maximize=True)\n    ALL_C = 10**C_POWER_RANGE\n    logreg_models = {}\n\n    train_features = train_features.to(dtype=train_dtype, device=train_features_device)\n    train_labels = train_labels.to(device=train_features_device)\n\n    for i in range(get_global_rank(), len(ALL_C), get_global_size()):\n        C = ALL_C[i].item()\n        logger.info(\n            f\"Training for C = {C:.5f}, dtype={train_dtype}, \"\n            f\"features: {train_features.shape}, {train_features.dtype}, \"\n            f\"labels: {train_labels.shape}, {train_labels.dtype}\"\n        )\n        logreg_models[C] = train_for_C(\n            C=C,\n            max_iter=max_train_iters,\n            train_features=train_features,\n            train_labels=train_labels,\n            dtype=train_dtype,\n            device=train_features_device,\n        )\n\n    gather_list = [None for _ in range(get_global_size())]\n    torch.distributed.all_gather_object(gather_list, logreg_models)\n\n    logreg_models_gathered = {}\n    for logreg_dict in gather_list:\n        logreg_models_gathered.update(logreg_dict)\n\n    for i in range(len(ALL_C)):\n        metric_tracker.increment()\n        C = ALL_C[i].item()\n        evals = evaluate_model(\n            logreg_model=logreg_models_gathered[C],\n            logreg_metric=metric_tracker,\n            test_data_loader=test_data_loader,\n            device=torch.cuda.current_device(),\n        )\n        logger.info(f\"Trained for C = {C:.5f}, accuracies = {evals}\")\n\n        best_stats, which_epoch = metric_tracker.best_metric(return_step=True)\n        best_stats_100 = {k: 100.0 * v for k, v in best_stats.items()}\n        if which_epoch[\"top-1\"] == i:\n            best_C = C\n    logger.info(f\"Sweep best {best_stats_100}, best C = {best_C:.6f}\")\n\n    return best_stats, best_C\n\n\ndef eval_log_regression(\n    *,\n    model,\n    train_dataset,\n    val_dataset,\n    finetune_dataset,\n    metric_type,\n    batch_size,\n    num_workers,\n    finetune_on_val=False,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,\n    max_train_iters=DEFAULT_MAX_ITER,\n):\n    \"\"\"\n    Implements the \"standard\" process for log regression evaluation:\n    The value of C is chosen by training on train_dataset and evaluating on\n    finetune_dataset. Then, the final model is trained on a concatenation of\n    train_dataset and finetune_dataset, and is evaluated on val_dataset.\n    If there is no finetune_dataset, the value of C is the one that yields\n    the best results on a random 10% subset of the train dataset\n    \"\"\"\n\n    start = time.time()\n\n    train_features, train_labels = extract_features(\n        model, train_dataset, batch_size, num_workers, gather_on_cpu=(train_features_device == _CPU_DEVICE)\n    )\n    val_features, val_labels = extract_features(\n        model, val_dataset, batch_size, num_workers, gather_on_cpu=(train_features_device == _CPU_DEVICE)\n    )\n    val_data_loader = torch.utils.data.DataLoader(\n        TensorDataset(val_features, val_labels),\n        batch_size=batch_size,\n        drop_last=False,\n        num_workers=0,\n        persistent_workers=False,\n    )\n\n    if finetune_dataset is None and finetune_on_val:\n        logger.info(\"Choosing hyperparameters on the val dataset\")\n        finetune_features, finetune_labels = val_features, val_labels\n    elif finetune_dataset is None and not finetune_on_val:\n        logger.info(\"Choosing hyperparameters on 10% of the train dataset\")\n        torch.manual_seed(0)\n        indices = torch.randperm(len(train_features), device=train_features.device)\n        finetune_index = indices[: len(train_features) // 10]\n        train_index = indices[len(train_features) // 10 :]\n        finetune_features, finetune_labels = train_features[finetune_index], train_labels[finetune_index]\n        train_features, train_labels = train_features[train_index], train_labels[train_index]\n    else:\n        logger.info(\"Choosing hyperparameters on the finetune dataset\")\n        finetune_features, finetune_labels = extract_features(\n            model, finetune_dataset, batch_size, num_workers, gather_on_cpu=(train_features_device == _CPU_DEVICE)\n        )\n    # release the model - free GPU memory\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    finetune_data_loader = torch.utils.data.DataLoader(\n        TensorDataset(finetune_features, finetune_labels),\n        batch_size=batch_size,\n        drop_last=False,\n    )\n\n    if len(train_labels.shape) > 1:\n        num_classes = train_labels.shape[1]\n    else:\n        num_classes = train_labels.max() + 1\n\n    logger.info(\"Using cuML for logistic regression\")\n\n    best_stats, best_C = sweep_C_values(\n        train_features=train_features,\n        train_labels=train_labels,\n        test_data_loader=finetune_data_loader,\n        metric_type=metric_type,\n        num_classes=num_classes,\n        train_dtype=train_dtype,\n        train_features_device=train_features_device,\n        max_train_iters=max_train_iters,\n    )\n\n    if not finetune_on_val:\n        logger.info(\"Best parameter found, concatenating features\")\n        train_features = torch.cat((train_features, finetune_features))\n        train_labels = torch.cat((train_labels, finetune_labels))\n\n    logger.info(\"Training final model\")\n    logreg_metric = build_metric(metric_type, num_classes=num_classes)\n    evals = train_and_evaluate(\n        C=best_C,\n        max_iter=max_train_iters,\n        train_features=train_features,\n        train_labels=train_labels,\n        logreg_metric=logreg_metric.clone(),\n        test_data_loader=val_data_loader,\n        eval_device=torch.cuda.current_device(),\n        train_dtype=train_dtype,\n        train_features_device=train_features_device,\n    )\n\n    best_stats = evals[1][\"metrics\"]\n\n    best_stats[\"best_C\"] = best_C\n\n    logger.info(f\"Log regression evaluation done in {int(time.time() - start)}s\")\n    return best_stats\n\n\ndef eval_log_regression_with_model(\n    model,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    finetune_dataset_str=None,\n    autocast_dtype=torch.float,\n    finetune_on_val=False,\n    metric_type=MetricType.MEAN_ACCURACY,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,\n    max_train_iters=DEFAULT_MAX_ITER,\n):\n    cudnn.benchmark = True\n\n    transform = make_classification_eval_transform(resize_size=224)\n    target_transform = None\n\n    train_dataset = make_dataset(dataset_str=train_dataset_str, transform=transform, target_transform=target_transform)\n    val_dataset = make_dataset(dataset_str=val_dataset_str, transform=transform, target_transform=target_transform)\n    if finetune_dataset_str is not None:\n        finetune_dataset = make_dataset(\n            dataset_str=finetune_dataset_str, transform=transform, target_transform=target_transform\n        )\n    else:\n        finetune_dataset = None\n\n    with torch.cuda.amp.autocast(dtype=autocast_dtype):\n        results_dict_logreg = eval_log_regression(\n            model=model,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            finetune_dataset=finetune_dataset,\n            metric_type=metric_type,\n            batch_size=256,\n            num_workers=0,  # 5,\n            finetune_on_val=finetune_on_val,\n            train_dtype=train_dtype,\n            train_features_device=train_features_device,\n            max_train_iters=max_train_iters,\n        )\n\n    results_dict = {\n        \"top-1\": results_dict_logreg[\"top-1\"].cpu().numpy() * 100.0,\n        \"top-5\": results_dict_logreg.get(\"top-5\", torch.tensor(0.0)).cpu().numpy() * 100.0,\n        \"best_C\": results_dict_logreg[\"best_C\"],\n    }\n    logger.info(\n        \"\\n\".join(\n            [\n                \"Training of the supervised logistic regression on frozen features completed.\\n\"\n                \"Top-1 test accuracy: {acc:.1f}\".format(acc=results_dict[\"top-1\"]),\n                \"Top-5 test accuracy: {acc:.1f}\".format(acc=results_dict[\"top-5\"]),\n                \"obtained for C = {c:.6f}\".format(c=results_dict[\"best_C\"]),\n            ]\n        )\n    )\n\n    torch.distributed.barrier()\n    return results_dict\n\n\ndef main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_log_regression_with_model(\n        model=model,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        finetune_dataset_str=args.finetune_dataset_str,\n        autocast_dtype=autocast_dtype,\n        finetune_on_val=args.finetune_on_val,\n        metric_type=args.metric_type,\n        train_dtype=as_torch_dtype(args.train_dtype),\n        train_features_device=torch.device(args.train_features_device),\n        max_train_iters=args.max_train_iters,\n    )\n    return 0\n\n\nif __name__ == \"__main__\":\n    description = \"DINOv2 logistic regression evaluation\"\n    args_parser = get_args_parser(description=description)\n    args = args_parser.parse_args()\n    sys.exit(main(args))\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/models/decode_heads/linear_head.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom mmseg.models.builder import HEADS\nfrom mmseg.models.decode_heads.decode_head import BaseDecodeHead\nfrom mmseg.ops import resize\n\n\n@HEADS.register_module()\nclass BNHead(BaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n\n    def __init__(self, resize_factors=None, **kwargs):\n        super().__init__(**kwargs)\n        assert self.in_channels == self.channels\n        self.bn = nn.SyncBatchNorm(self.in_channels)\n        self.resize_factors = resize_factors\n\n    def _forward_feature(self, inputs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        ``self.cls_seg`` fc.\n\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            feats (Tensor): A tensor of shape (batch_size, self.channels,\n                H, W) which is feature map for last layer of decoder head.\n        \"\"\"\n        # print(\"inputs\", [i.shape for i in inputs])\n        x = self._transform_inputs(inputs)\n        # print(\"x\", x.shape)\n        feats = self.bn(x)\n        # print(\"feats\", feats.shape)\n        return feats\n\n    def _transform_inputs(self, inputs):\n        \"\"\"Transform inputs for decoder.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"\n\n        if self.input_transform == \"resize_concat\":\n            # accept lists (for cls token)\n            input_list = []\n            for x in inputs:\n                if isinstance(x, list):\n                    input_list.extend(x)\n                else:\n                    input_list.append(x)\n            inputs = input_list\n            # an image descriptor can be a local descriptor with resolution 1x1\n            for i, x in enumerate(inputs):\n                if len(x.shape) == 2:\n                    inputs[i] = x[:, :, None, None]\n            # select indices\n            inputs = [inputs[i] for i in self.in_index]\n            # Resizing shenanigans\n            # print(\"before\", *(x.shape for x in inputs))\n            if self.resize_factors is not None:\n                assert len(self.resize_factors) == len(inputs), (len(self.resize_factors), len(inputs))\n                inputs = [\n                    resize(input=x, scale_factor=f, mode=\"bilinear\" if f >= 1 else \"area\")\n                    for x, f in zip(inputs, self.resize_factors)\n                ]\n                # print(\"after\", *(x.shape for x in inputs))\n            upsampled_inputs = [\n                resize(input=x, size=inputs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners)\n                for x in inputs\n            ]\n            inputs = torch.cat(upsampled_inputs, dim=1)\n        elif self.input_transform == \"multiple_select\":\n            inputs = [inputs[i] for i in self.in_index]\n        else:\n            inputs = inputs[self.in_index]\n\n        return inputs\n\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        output = self._forward_feature(inputs)\n        output = self.cls_seg(output)\n        return output\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/utils/colormaps.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nADE20K_COLORMAP = [\n    (0, 0, 0),\n    (120, 120, 120),\n    (180, 120, 120),\n    (6, 230, 230),\n    (80, 50, 50),\n    (4, 200, 3),\n    (120, 120, 80),\n    (140, 140, 140),\n    (204, 5, 255),\n    (230, 230, 230),\n    (4, 250, 7),\n    (224, 5, 255),\n    (235, 255, 7),\n    (150, 5, 61),\n    (120, 120, 70),\n    (8, 255, 51),\n    (255, 6, 82),\n    (143, 255, 140),\n    (204, 255, 4),\n    (255, 51, 7),\n    (204, 70, 3),\n    (0, 102, 200),\n    (61, 230, 250),\n    (255, 6, 51),\n    (11, 102, 255),\n    (255, 7, 71),\n    (255, 9, 224),\n    (9, 7, 230),\n    (220, 220, 220),\n    (255, 9, 92),\n    (112, 9, 255),\n    (8, 255, 214),\n    (7, 255, 224),\n    (255, 184, 6),\n    (10, 255, 71),\n    (255, 41, 10),\n    (7, 255, 255),\n    (224, 255, 8),\n    (102, 8, 255),\n    (255, 61, 6),\n    (255, 194, 7),\n    (255, 122, 8),\n    (0, 255, 20),\n    (255, 8, 41),\n    (255, 5, 153),\n    (6, 51, 255),\n    (235, 12, 255),\n    (160, 150, 20),\n    (0, 163, 255),\n    (140, 140, 140),\n    (250, 10, 15),\n    (20, 255, 0),\n    (31, 255, 0),\n    (255, 31, 0),\n    (255, 224, 0),\n    (153, 255, 0),\n    (0, 0, 255),\n    (255, 71, 0),\n    (0, 235, 255),\n    (0, 173, 255),\n    (31, 0, 255),\n    (11, 200, 200),\n    (255, 82, 0),\n    (0, 255, 245),\n    (0, 61, 255),\n    (0, 255, 112),\n    (0, 255, 133),\n    (255, 0, 0),\n    (255, 163, 0),\n    (255, 102, 0),\n    (194, 255, 0),\n    (0, 143, 255),\n    (51, 255, 0),\n    (0, 82, 255),\n    (0, 255, 41),\n    (0, 255, 173),\n    (10, 0, 255),\n    (173, 255, 0),\n    (0, 255, 153),\n    (255, 92, 0),\n    (255, 0, 255),\n    (255, 0, 245),\n    (255, 0, 102),\n    (255, 173, 0),\n    (255, 0, 20),\n    (255, 184, 184),\n    (0, 31, 255),\n    (0, 255, 61),\n    (0, 71, 255),\n    (255, 0, 204),\n    (0, 255, 194),\n    (0, 255, 82),\n    (0, 10, 255),\n    (0, 112, 255),\n    (51, 0, 255),\n    (0, 194, 255),\n    (0, 122, 255),\n    (0, 255, 163),\n    (255, 153, 0),\n    (0, 255, 10),\n    (255, 112, 0),\n    (143, 255, 0),\n    (82, 0, 255),\n    (163, 255, 0),\n    (255, 235, 0),\n    (8, 184, 170),\n    (133, 0, 255),\n    (0, 255, 92),\n    (184, 0, 255),\n    (255, 0, 31),\n    (0, 184, 255),\n    (0, 214, 255),\n    (255, 0, 112),\n    (92, 255, 0),\n    (0, 224, 255),\n    (112, 224, 255),\n    (70, 184, 160),\n    (163, 0, 255),\n    (153, 0, 255),\n    (71, 255, 0),\n    (255, 0, 163),\n    (255, 204, 0),\n    (255, 0, 143),\n    (0, 255, 235),\n    (133, 255, 0),\n    (255, 0, 235),\n    (245, 0, 255),\n    (255, 0, 122),\n    (255, 245, 0),\n    (10, 190, 212),\n    (214, 255, 0),\n    (0, 204, 255),\n    (20, 0, 255),\n    (255, 255, 0),\n    (0, 153, 255),\n    (0, 41, 255),\n    (0, 255, 204),\n    (41, 0, 255),\n    (41, 255, 0),\n    (173, 0, 255),\n    (0, 245, 255),\n    (71, 0, 255),\n    (122, 0, 255),\n    (0, 255, 184),\n    (0, 92, 255),\n    (184, 255, 0),\n    (0, 133, 255),\n    (255, 214, 0),\n    (25, 194, 194),\n    (102, 255, 0),\n    (92, 0, 255),\n]\n\nADE20K_CLASS_NAMES = [\n    \"\",\n    \"wall\",\n    \"building;edifice\",\n    \"sky\",\n    \"floor;flooring\",\n    \"tree\",\n    \"ceiling\",\n    \"road;route\",\n    \"bed\",\n    \"windowpane;window\",\n    \"grass\",\n    \"cabinet\",\n    \"sidewalk;pavement\",\n    \"person;individual;someone;somebody;mortal;soul\",\n    \"earth;ground\",\n    \"door;double;door\",\n    \"table\",\n    \"mountain;mount\",\n    \"plant;flora;plant;life\",\n    \"curtain;drape;drapery;mantle;pall\",\n    \"chair\",\n    \"car;auto;automobile;machine;motorcar\",\n    \"water\",\n    \"painting;picture\",\n    \"sofa;couch;lounge\",\n    \"shelf\",\n    \"house\",\n    \"sea\",\n    \"mirror\",\n    \"rug;carpet;carpeting\",\n    \"field\",\n    \"armchair\",\n    \"seat\",\n    \"fence;fencing\",\n    \"desk\",\n    \"rock;stone\",\n    \"wardrobe;closet;press\",\n    \"lamp\",\n    \"bathtub;bathing;tub;bath;tub\",\n    \"railing;rail\",\n    \"cushion\",\n    \"base;pedestal;stand\",\n    \"box\",\n    \"column;pillar\",\n    \"signboard;sign\",\n    \"chest;of;drawers;chest;bureau;dresser\",\n    \"counter\",\n    \"sand\",\n    \"sink\",\n    \"skyscraper\",\n    \"fireplace;hearth;open;fireplace\",\n    \"refrigerator;icebox\",\n    \"grandstand;covered;stand\",\n    \"path\",\n    \"stairs;steps\",\n    \"runway\",\n    \"case;display;case;showcase;vitrine\",\n    \"pool;table;billiard;table;snooker;table\",\n    \"pillow\",\n    \"screen;door;screen\",\n    \"stairway;staircase\",\n    \"river\",\n    \"bridge;span\",\n    \"bookcase\",\n    \"blind;screen\",\n    \"coffee;table;cocktail;table\",\n    \"toilet;can;commode;crapper;pot;potty;stool;throne\",\n    \"flower\",\n    \"book\",\n    \"hill\",\n    \"bench\",\n    \"countertop\",\n    \"stove;kitchen;stove;range;kitchen;range;cooking;stove\",\n    \"palm;palm;tree\",\n    \"kitchen;island\",\n    \"computer;computing;machine;computing;device;data;processor;electronic;computer;information;processing;system\",\n    \"swivel;chair\",\n    \"boat\",\n    \"bar\",\n    \"arcade;machine\",\n    \"hovel;hut;hutch;shack;shanty\",\n    \"bus;autobus;coach;charabanc;double-decker;jitney;motorbus;motorcoach;omnibus;passenger;vehicle\",\n    \"towel\",\n    \"light;light;source\",\n    \"truck;motortruck\",\n    \"tower\",\n    \"chandelier;pendant;pendent\",\n    \"awning;sunshade;sunblind\",\n    \"streetlight;street;lamp\",\n    \"booth;cubicle;stall;kiosk\",\n    \"television;television;receiver;television;set;tv;tv;set;idiot;box;boob;tube;telly;goggle;box\",\n    \"airplane;aeroplane;plane\",\n    \"dirt;track\",\n    \"apparel;wearing;apparel;dress;clothes\",\n    \"pole\",\n    \"land;ground;soil\",\n    \"bannister;banister;balustrade;balusters;handrail\",\n    \"escalator;moving;staircase;moving;stairway\",\n    \"ottoman;pouf;pouffe;puff;hassock\",\n    \"bottle\",\n    \"buffet;counter;sideboard\",\n    \"poster;posting;placard;notice;bill;card\",\n    \"stage\",\n    \"van\",\n    \"ship\",\n    \"fountain\",\n    \"conveyer;belt;conveyor;belt;conveyer;conveyor;transporter\",\n    \"canopy\",\n    \"washer;automatic;washer;washing;machine\",\n    \"plaything;toy\",\n    \"swimming;pool;swimming;bath;natatorium\",\n    \"stool\",\n    \"barrel;cask\",\n    \"basket;handbasket\",\n    \"waterfall;falls\",\n    \"tent;collapsible;shelter\",\n    \"bag\",\n    \"minibike;motorbike\",\n    \"cradle\",\n    \"oven\",\n    \"ball\",\n    \"food;solid;food\",\n    \"step;stair\",\n    \"tank;storage;tank\",\n    \"trade;name;brand;name;brand;marque\",\n    \"microwave;microwave;oven\",\n    \"pot;flowerpot\",\n    \"animal;animate;being;beast;brute;creature;fauna\",\n    \"bicycle;bike;wheel;cycle\",\n    \"lake\",\n    \"dishwasher;dish;washer;dishwashing;machine\",\n    \"screen;silver;screen;projection;screen\",\n    \"blanket;cover\",\n    \"sculpture\",\n    \"hood;exhaust;hood\",\n    \"sconce\",\n    \"vase\",\n    \"traffic;light;traffic;signal;stoplight\",\n    \"tray\",\n    \"ashcan;trash;can;garbage;can;wastebin;ash;bin;ash-bin;ashbin;dustbin;trash;barrel;trash;bin\",\n    \"fan\",\n    \"pier;wharf;wharfage;dock\",\n    \"crt;screen\",\n    \"plate\",\n    \"monitor;monitoring;device\",\n    \"bulletin;board;notice;board\",\n    \"shower\",\n    \"radiator\",\n    \"glass;drinking;glass\",\n    \"clock\",\n    \"flag\",\n]\n\n\nVOC2012_COLORMAP = [\n    (0, 0, 0),\n    (128, 0, 0),\n    (0, 128, 0),\n    (128, 128, 0),\n    (0, 0, 128),\n    (128, 0, 128),\n    (0, 128, 128),\n    (128, 128, 128),\n    (64, 0, 0),\n    (192, 0, 0),\n    (64, 128, 0),\n    (192, 128, 0),\n    (64, 0, 128),\n    (192, 0, 128),\n    (64, 128, 128),\n    (192, 128, 128),\n    (0, 64, 0),\n    (128, 64, 0),\n    (0, 192, 0),\n    (128, 192, 0),\n    (0, 64, 128),\n]\n\n\nVOC2012_CLASS_NAMES = [\n    \"\",\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\",\n]\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/models/backbones/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .vision_transformer import DinoVisionTransformer\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/models/decode_heads/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .linear_head import BNHead\n"}
{"type": "source_file", "path": "dinov2/dinov2/distributed/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nimport os\nimport random\nimport re\nimport socket\nfrom typing import Dict, List\n\nimport torch\nimport torch.distributed as dist\n\n_LOCAL_RANK = -1\n_LOCAL_WORLD_SIZE = -1\n\n\ndef is_enabled() -> bool:\n    \"\"\"\n    Returns:\n        True if distributed training is enabled\n    \"\"\"\n    return dist.is_available() and dist.is_initialized()\n\n\ndef get_global_size() -> int:\n    \"\"\"\n    Returns:\n        The number of processes in the process group\n    \"\"\"\n    return dist.get_world_size() if is_enabled() else 1\n\n\ndef get_global_rank() -> int:\n    \"\"\"\n    Returns:\n        The rank of the current process within the global process group.\n    \"\"\"\n    return dist.get_rank() if is_enabled() else 0\n\n\ndef get_local_rank() -> int:\n    \"\"\"\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    \"\"\"\n    if not is_enabled():\n        return 0\n    assert 0 <= _LOCAL_RANK < _LOCAL_WORLD_SIZE\n    return _LOCAL_RANK\n\n\ndef get_local_size() -> int:\n    \"\"\"\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    \"\"\"\n    if not is_enabled():\n        return 1\n    assert 0 <= _LOCAL_RANK < _LOCAL_WORLD_SIZE\n    return _LOCAL_WORLD_SIZE\n\n\ndef is_main_process() -> bool:\n    \"\"\"\n    Returns:\n        True if the current process is the main one.\n    \"\"\"\n    return get_global_rank() == 0\n\n\ndef _restrict_print_to_main_process() -> None:\n    \"\"\"\n    This function disables printing when not in the main process\n    \"\"\"\n    import builtins as __builtin__\n\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_main_process() or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef _get_master_port(seed: int = 0) -> int:\n    MIN_MASTER_PORT, MAX_MASTER_PORT = (20_000, 60_000)\n\n    master_port_str = os.environ.get(\"MASTER_PORT\")\n    if master_port_str is None:\n        rng = random.Random(seed)\n        return rng.randint(MIN_MASTER_PORT, MAX_MASTER_PORT)\n\n    return int(master_port_str)\n\n\ndef _get_available_port() -> int:\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        # A \"\" host address means INADDR_ANY i.e. binding to all interfaces.\n        # Note this is not compatible with IPv6.\n        s.bind((\"\", 0))\n        port = s.getsockname()[1]\n        return port\n\n\n_TORCH_DISTRIBUTED_ENV_VARS = (\n    \"MASTER_ADDR\",\n    \"MASTER_PORT\",\n    \"RANK\",\n    \"WORLD_SIZE\",\n    \"LOCAL_RANK\",\n    \"LOCAL_WORLD_SIZE\",\n)\n\n\ndef _collect_env_vars() -> Dict[str, str]:\n    return {env_var: os.environ[env_var] for env_var in _TORCH_DISTRIBUTED_ENV_VARS if env_var in os.environ}\n\n\ndef _is_slurm_job_process() -> bool:\n    return \"SLURM_JOB_ID\" in os.environ\n\n\ndef _parse_slurm_node_list(s: str) -> List[str]:\n    nodes = []\n    # Extract \"hostname\", \"hostname[1-2,3,4-5],\" substrings\n    p = re.compile(r\"(([^\\[]+)(?:\\[([^\\]]+)\\])?),?\")\n    for m in p.finditer(s):\n        prefix, suffixes = s[m.start(2) : m.end(2)], s[m.start(3) : m.end(3)]\n        for suffix in suffixes.split(\",\"):\n            span = suffix.split(\"-\")\n            if len(span) == 1:\n                nodes.append(prefix + suffix)\n            else:\n                width = len(span[0])\n                start, end = int(span[0]), int(span[1]) + 1\n                nodes.extend([prefix + f\"{i:0{width}}\" for i in range(start, end)])\n    return nodes\n\n\ndef _check_env_variable(key: str, new_value: str):\n    # Only check for difference with preset environment variables\n    if key in os.environ and os.environ[key] != new_value:\n        raise RuntimeError(f\"Cannot export environment variables as {key} is already set\")\n\n\nclass _TorchDistributedEnvironment:\n    def __init__(self):\n        self.master_addr = \"127.0.0.1\"\n        self.master_port = 0\n        self.rank = -1\n        self.world_size = -1\n        self.local_rank = -1\n        self.local_world_size = -1\n\n        if _is_slurm_job_process():\n            return self._set_from_slurm_env()\n\n        env_vars = _collect_env_vars()\n        if not env_vars:\n            # Environment is not set\n            pass\n        elif len(env_vars) == len(_TORCH_DISTRIBUTED_ENV_VARS):\n            # Environment is fully set\n            return self._set_from_preset_env()\n        else:\n            # Environment is partially set\n            collected_env_vars = \", \".join(env_vars.keys())\n            raise RuntimeError(f\"Partially set environment: {collected_env_vars}\")\n\n        if torch.cuda.device_count() > 0:\n            return self._set_from_local()\n\n        raise RuntimeError(\"Can't initialize PyTorch distributed environment\")\n\n    # Slurm job created with sbatch, submitit, etc...\n    def _set_from_slurm_env(self):\n        # logger.info(\"Initialization from Slurm environment\")\n        job_id = int(os.environ[\"SLURM_JOB_ID\"])\n        node_count = int(os.environ[\"SLURM_JOB_NUM_NODES\"])\n        nodes = _parse_slurm_node_list(os.environ[\"SLURM_JOB_NODELIST\"])\n        assert len(nodes) == node_count\n\n        self.master_addr = nodes[0]\n        self.master_port = _get_master_port(seed=job_id)\n        self.rank = int(os.environ[\"SLURM_PROCID\"])\n        self.world_size = int(os.environ[\"SLURM_NTASKS\"])\n        assert self.rank < self.world_size\n        self.local_rank = int(os.environ[\"SLURM_LOCALID\"])\n        self.local_world_size = self.world_size // node_count\n        assert self.local_rank < self.local_world_size\n\n    # Single node job with preset environment (i.e. torchrun)\n    def _set_from_preset_env(self):\n        # logger.info(\"Initialization from preset environment\")\n        self.master_addr = os.environ[\"MASTER_ADDR\"]\n        self.master_port = os.environ[\"MASTER_PORT\"]\n        self.rank = int(os.environ[\"RANK\"])\n        self.world_size = int(os.environ[\"WORLD_SIZE\"])\n        assert self.rank < self.world_size\n        self.local_rank = int(os.environ[\"LOCAL_RANK\"])\n        self.local_world_size = int(os.environ[\"LOCAL_WORLD_SIZE\"])\n        assert self.local_rank < self.local_world_size\n\n    # Single node and GPU job (i.e. local script run)\n    def _set_from_local(self):\n        # logger.info(\"Initialization from local\")\n        self.master_addr = \"127.0.0.1\"\n        self.master_port = _get_available_port()\n        self.rank = 0\n        self.world_size = 1\n        self.local_rank = 0\n        self.local_world_size = 1\n\n    def export(self, *, overwrite: bool) -> \"_TorchDistributedEnvironment\":\n        # See the \"Environment variable initialization\" section from\n        # https://pytorch.org/docs/stable/distributed.html for the complete list of\n        # environment variables required for the env:// initialization method.\n        env_vars = {\n            \"MASTER_ADDR\": self.master_addr,\n            \"MASTER_PORT\": str(self.master_port),\n            \"RANK\": str(self.rank),\n            \"WORLD_SIZE\": str(self.world_size),\n            \"LOCAL_RANK\": str(self.local_rank),\n            \"LOCAL_WORLD_SIZE\": str(self.local_world_size),\n        }\n        if not overwrite:\n            for k, v in env_vars.items():\n                _check_env_variable(k, v)\n\n        os.environ.update(env_vars)\n        return self\n\n\ndef enable(*, set_cuda_current_device: bool = True, overwrite: bool = False, allow_nccl_timeout: bool = False):\n    \"\"\"Enable distributed mode\n\n    Args:\n        set_cuda_current_device: If True, call torch.cuda.set_device() to set the\n            current PyTorch CUDA device to the one matching the local rank.\n        overwrite: If True, overwrites already set variables. Else fails.\n    \"\"\"\n\n    global _LOCAL_RANK, _LOCAL_WORLD_SIZE\n    if _LOCAL_RANK >= 0 or _LOCAL_WORLD_SIZE >= 0:\n        raise RuntimeError(\"Distributed mode has already been enabled\")\n    torch_env = _TorchDistributedEnvironment()\n    torch_env.export(overwrite=overwrite)\n\n    if set_cuda_current_device:\n        torch.cuda.set_device(torch_env.local_rank)\n\n    if allow_nccl_timeout:\n        # This allows to use torch distributed timeout in a NCCL backend\n        key, value = \"NCCL_ASYNC_ERROR_HANDLING\", \"1\"\n        if not overwrite:\n            _check_env_variable(key, value)\n        os.environ[key] = value\n\n    dist.init_process_group(backend=\"nccl\")\n    dist.barrier()\n\n    # Finalize setup\n    _LOCAL_RANK = torch_env.local_rank\n    _LOCAL_WORLD_SIZE = torch_env.local_world_size\n    _restrict_print_to_main_process()\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation/utils/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n"}
{"type": "source_file", "path": "dinov2/dinov2/eval/segmentation_m2f/__init__.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the Apache License, Version 2.0\n# found in the LICENSE file in the root directory of this source tree.\n\nfrom .core import *  # noqa: F403\nfrom .models import *  # noqa: F403\nfrom .ops import *  # noqa: F403\n"}
