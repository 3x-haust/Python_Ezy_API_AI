{"repo_info": {"repo_name": "E2E-AI-Chatbot", "repo_owner": "vnk8071", "repo_url": "https://github.com/vnk8071/E2E-AI-Chatbot"}}
{"type": "source_file", "path": "__init__.py", "content": "from fastapi import FastAPI, staticfiles\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\n\n\ndef build_app():\n    app = FastAPI(title=\"E2E-AI-CHATBOT\")\n    app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    return app\n\n\napp = build_app()\n"}
{"type": "source_file", "path": "databases/mongoDB.py", "content": "import json\nfrom typing import Any, Iterable, List, Type, Optional\n\nfrom databases import DB, Document, DatabaseBase\nfrom loggers import AppLogger\n\nlogger = AppLogger().get_logger()\n\n\nclass MongoDBClient(DatabaseBase):\n    def __init__(\n        self,\n        mongodb_host: str = \"mongodb://localhost:27017/\",\n        database_name: str = \"document\"\n    ) -> None:\n        \"\"\"Initialize with MongoDB client.\"\"\"\n        try:\n            from pymongo import MongoClient\n        except ImportError:\n            raise ValueError(\n                \"Could not import pymongo python package. \"\n                \"Please install it with `poetry install pymongo`.\"\n            )\n        self.mongodb_host = mongodb_host\n        self.mongo_client = MongoClient(self.mongodb_host)\n        self.mongo_database = self.mongo_client[database_name]\n        self.mongo_connect = self.mongo_database[database_name]\n\n    def check_database(\n        self,\n        database_test: str = \"database_test\"\n    ) -> None:\n        \"\"\"\"\"\"\n        if database_test in self.mongo_client.list_databases():\n            logger.info(\"The database exists.\")\n        else:\n            logger.info(f\"The {database_test} not exists and auto create.\")\n\n    def add_contents(\n            self,\n            contents: Iterable[str],\n            ids: Optional[List[str]] = None,\n            **kargs: Any\n    ) -> List[str]:\n        self.mongo_connect.insert_many(documents=contents)\n        return ids\n\n    def get_contents(self):\n        \"\"\"Return all items in database\"\"\"\n        return [self._doc_to_json(doc) for doc in self.mongo_connect.find()]\n\n    def _doc_to_json(self, doc):\n        doc_str = json.dumps(doc, default=str)\n        doc_json = json.loads(doc_str)\n        return doc_json\n\n    @classmethod\n    def _add_contents(\n        cls: Type[DB],\n        contents: Iterable[str],\n        mongodb_host: str = \"mongodb://localhost:27017/\",\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Create a Mongo database from a list of documents.\n\n        Args:\n            content <Iterable[str]>: List of contents to add to the database.\n            mongodb_host <str>: Mongo host\n        Returns:\n            MongoDBClient: MongoDBClient database.\n        \"\"\"\n        mongo_collection = cls(\n            mongodb_host=mongodb_host\n        )\n        mongo_collection.add_contents(contents=contents)\n        return mongo_collection\n\n    @classmethod\n    def _get_documents(\n        cls: Type[DB],\n        documents: List[Document],\n        mongodb_host: str = \"mongodb://localhost:27017/\",\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Create a Mongo database from a list of documents.\n\n        Args:\n            documents <List[Document]>: List of documents to add to the database.\n            mongodb_host <str>: Mongo host\n        Returns:\n            MongoDBClient: MongoDBClient database.\n        \"\"\"\n        contents = [document.content for document in documents]\n        return cls(\n            contents=contents,\n            mongodb_host=mongodb_host\n        )\n\n\nif __name__ == '__main__':\n    mongo_database = MongoDBClient()\n"}
{"type": "source_file", "path": "app.py", "content": "import argparse\nimport mimetypes\nimport secrets\nimport fastapi\nimport gradio as gr\nimport uvicorn\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import (\n    FileResponse,\n    HTMLResponse,\n    JSONResponse,\n    PlainTextResponse,\n)\nfrom fastapi.security import OAuth2PasswordRequestForm\nfrom fastapi.templating import Jinja2Templates\nfrom importlib_resources import files\nfrom starlette.responses import RedirectResponse, StreamingResponse\n\nfrom __init__ import app\nfrom config import settings\nfrom loggers import AppLogger\nfrom routers import (\n    db_router,\n    ui_router,\n    chatbot_router,\n    ingest_router,\n    safe_join,\n    strip_url,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--host\", default=settings.HOST, type=str)\nparser.add_argument(\"--port\", default=settings.PORT, type=int)\nargs = parser.parse_args()\nlogger = AppLogger().get_logger()\ntemplates = Jinja2Templates(directory=\"templates\")\nTOKEN = secrets.token_urlsafe(16)\nSTATIC_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"static\").as_posix()  # type: ignore\nBUILD_PATH_LIB = files(\"gradio\").joinpath(\"templates\", \"frontend\", \"assets\").as_posix()  # type: ignore\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    app.auth = (settings.USERNAME, settings.PASSWORD)\n    app.tokens = {}\n    app.log_in = False\n    logger.info(\"Starting up...\")\n    logger.info(f\"Host: {args.host}\")\n    logger.info(f\"Port: {args.port}\")\n    if args.host == \"0.0.0.0\":\n        logger.info(f\"Access URL: http://localhost:{args.port}\")\n    else:\n        logger.info(f\"Access URL: http://{args.host}:{args.port}\")\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    logger.info(\"Shutting down...\")\n\n\n@app.get(\"/user\")\ndef get_current_user(request: fastapi.Request):\n    token = request.cookies.get(\"access-token\") or request.cookies.get(\n        \"access-token-unsecure\"\n    )\n    logger.info(f\"token: {token}\")\n    return app.tokens.get(token)\n\n\n@app.get(\"/login_check\")\ndef login_check(user: str = Depends(get_current_user)):\n    if app.auth is None or user is not None:\n        return\n    raise HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\"\n    )\n\n\n@app.head(\"/\", response_class=HTMLResponse)\n@app.get(\"/\", response_class=HTMLResponse)\ndef main(request: fastapi.Request):\n    if app.log_in:\n        return RedirectResponse(\"/show-app\")\n    mimetypes.add_type(\"application/javascript\", \".js\")\n\n    return templates.TemplateResponse(\n        \"frontend/index.html\",\n        {\"request\": request},\n    )\n\n\n@app.get(\"/show-app\", dependencies=[Depends(login_check)])\nasync def show(user: str = Depends(get_current_user)):\n    gr.mount_gradio_app(app, chatbot_router, \"/chat/\")\n    return RedirectResponse(\"/chat/\")\n\n\n@app.get(\"/show-ingest\", dependencies=[Depends(login_check)])\nasync def ingest_page():\n    gr.mount_gradio_app(app, ingest_router, \"/ingest/\")\n    return RedirectResponse(\"/ingest/\")\n\n\n@app.get(\"/token\")\ndef get_token(request: fastapi.Request) -> dict:\n    token = request.cookies.get(\"access-token\")\n    return {\"token\": token, \"user\": app.tokens.get(token)}\n\n\n@app.post(\"/login\")\ndef login(form_data: OAuth2PasswordRequestForm = Depends()):\n    username, password = form_data.username.strip(), form_data.password\n    if username == settings.USERNAME and password == settings.PASSWORD:\n        token = TOKEN\n        app.tokens[token] = username\n        app.log_in = True\n        response = JSONResponse(content={\"success\": True})\n        response.set_cookie(\n            key=\"access-token\",\n            value=token,\n            httponly=True,\n            samesite=\"none\",\n            secure=True,\n        )\n        response.set_cookie(\n            key=\"access-token-unsecure\",\n            value=token,\n            httponly=True,\n        )\n        logger.info(f\"response: {response}\")\n        return response\n    else:\n        raise HTTPException(status_code=400, detail=\"Incorrect credentials.\")\n\n\n@app.get(\"/config\", dependencies=[Depends(login_check)])\ndef get_config(request: fastapi.Request):\n    root_path = (\n        request.scope.get(\"root_path\") or request.headers.get(\"X-Direct-Url\") or \"\"\n    )\n    config = app.get_blocks().config\n    config[\"root\"] = strip_url(root_path)\n    return config\n\n\n@app.get(\"/static/{path:path}\")\ndef static_resource(path: str):\n    static_file = safe_join(STATIC_PATH_LIB, path)\n    return FileResponse(static_file)\n\n\n@app.get(\"/assets/{path:path}\")\ndef build_resource(path: str):\n    build_file = safe_join(BUILD_PATH_LIB, path)\n    return FileResponse(build_file)\n\n\n@app.get(\"/favicon.ico\")\nasync def favicon():\n    return static_resource(\"img/logo.svg\")\n\n\n@app.get(\"/theme.css\", response_class=PlainTextResponse)\ndef theme_css():\n    return PlainTextResponse(\n        gr.themes.Default()._get_theme_css(), media_type=\"text/css\"\n    )\n\n\napp.include_router(router=ui_router, dependencies=[Depends(login_check)])\napp.include_router(router=db_router, dependencies=[Depends(login_check)])\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app=app, host=args.host, port=args.port)\n"}
{"type": "source_file", "path": "config.py", "content": "from pydantic import BaseSettings\n\n\nclass Settings(BaseSettings):\n    PROJECT_NAME: str = \"End-to-End AI Chatbot\"\n    USERNAME = \"admin\"\n    PASSWORD = \"admin\"\n    HOST = \"0.0.0.0\"\n    PORT = 8071\n\n    # Config GPT4ALL model\n    MODEL_PATH = \"models/ggml-gpt4all-j-v1.3-groovy.bin\"\n    MODEL_TYPE = \"GPT4All\"\n    SERVER_HOST = \"http://localhost\"\n    INDEX_NAME = \"document\"\n    SYSTEM_DEFAULT = (\n        \"\"\"You are GPT4All Assistant help to answer questions about private document.\"\"\"\n    )\n    SERVER_ERROR_MSG = \"\"\"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\"\"\n\n    # Config database\n    DATA_PATH = \"static/pdf/\"\n\n\nsettings = Settings()\n"}
{"type": "source_file", "path": "searchers/__init__.py", "content": "from .base import SearchBase\n\nfrom .elastic_search import ElasticSearch\n__all__ = [\n    \"SearchBase\",\n    \"ElasticSearch\"\n]\n"}
{"type": "source_file", "path": "extractors/__init__.py", "content": "from .base import (\n    BaseLoader,\n    Blob,\n    BlobLoader,\n    BaseBlobParser,\n    TextSplitter,\n    RecursiveCharacterTextSplitter\n)\n\nfrom .pdf import PyPDFLoader, PyPDFDirectoryLoader\n\n__all__ = [\n    \"BaseLoader\",\n    \"Blob\",\n    \"BlobLoader\",\n    \"BaseBlobParser\",\n    \"TextSplitter\",\n    \"RecursiveCharacterTextSplitter\",\n    \"PyPDFLoader\"\n]\n"}
{"type": "source_file", "path": "loggers/custom.py", "content": "import logging\nimport logging.config\nfrom datetime import datetime\n\n\ndef logging_custom(__name__):\n    logging.config.fileConfig(\n        'loggers/logging.conf',\n        defaults={'logfilename': 'loggers/{:%Y-%m-%d}.log'.format(datetime.now())},\n    )\n    logger = logging.getLogger(__name__)\n    return logger\n\n\nclass AppLogger:\n    _logger_file = 'loggers/{:%Y-%m-%d}.log'.format(datetime.now())\n    _LOGGER_CONFIG = 'loggers/logging.conf'\n\n    def get_logger(self):\n        self._logger = logging.getLogger(__name__)\n        logging.config.fileConfig(\n            self._LOGGER_CONFIG,\n            defaults={'logfilename': self._logger_file}\n        )\n        return self._logger\n"}
{"type": "source_file", "path": "llms/gpt4all.py", "content": "from gpt4all import GPT4All\n\nfrom llms.base import LLMBase\n\n\nclass GPT4AllModel(LLMBase):\n    \"\"\"Wrapper for the GPT4All model.\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n    ):\n        try:\n            model_path, delimiter, model_name = model_path.rpartition(\"/\")\n            model_path += delimiter\n            self.model = GPT4All(model_name=model_name, model_path=model_path)\n        except ImportError:\n            raise ValueError(\n                \"Could not import gpt4all python package. \"\n                \"Please install it with `poetry add gpt4all`.\"\n            )\n\n    def generate_prompt(self, system_content: str, question: str, context: str) -> str:\n        if context is not None:\n            context_ = \"\\n\".join(doc.content for doc in context)\n            system_prompt = f\"\"\"{system_content}.\n\n            Use below context to answer the question.\n            Context:\n            {context_}\n\n            Question:\n            {question}\n            \"\"\"\n        else:\n            system_prompt = f\"\"\"{system_content}.\n\n            Question:\n            {question}\n            \"\"\"\n        return system_prompt\n\n    def __call__(self, system_content: str, question: str, context: str = \"\") -> str:\n        \"\"\"Call out to GPT4All's generate method.\n\n        Args:\n            question: The question to pass into the model.\n            context: The related context to pass into the model.\n\n        Returns:\n            The string generated by the model.\n        \"\"\"\n        text = \"\"\n        for token in self.model.generate(\n            self.generate_prompt(\n                system_content=system_content, question=question, context=context\n            )\n        ):\n            text += token\n        return text\n\n\nif __name__ == \"__main__\":\n    model = GPT4AllModel(model_path=\"./models/ggml-gpt4all-j-v1.3-groovy.bin\")\n    question = \"Hi, who are you?\"\n    answer = model(question)\n"}
{"type": "source_file", "path": "databases/base.py", "content": "from typing import List, Iterable, Any, Type, TypeVar, Optional, Dict\nfrom abc import ABC, abstractmethod\nfrom pydantic import BaseModel\n\nDB = TypeVar(\"DB\", bound=\"DatabaseBase\")\n\n\nclass Document(BaseModel):\n    \"\"\"Interface for interacting with a document.\"\"\"\n    content: str\n    metadata: Optional[Dict]\n\n\nclass DatabaseBase(ABC):\n    @abstractmethod\n    def add_contents(\n        self,\n        contents: Iterable[str],\n        **kargs: Any\n    ) -> List[str]:\n        \"\"\"Run more contents through the items and add to the database.\n\n        Args:\n            contents: Iterable of strings to add to the database.\n            kwargs: database specific parameters\n\n        Returns:\n            List of ids from adding the texts into the database.\n        \"\"\"\n\n    def add_documents(\n        self,\n        documents: List[Document],\n        **kargs: Any\n    ) -> List[str]:\n        \"\"\"Run more documents through the items and add to the database.\n\n        Args:\n            documents (List[Document]: Documents to add to the database.\n\n        Returns:\n            List[str]: List of IDs of the added contents.\n        \"\"\"\n        contents = [document[\"content\"] for document in documents]\n        return self.add_contents(contents, **kargs)\n\n    @classmethod\n    @abstractmethod\n    def get_contents(\n        cls: Type[DB],\n        contents: Iterable[str],\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Return database initialized from contents.\"\"\"\n\n    @classmethod\n    def get_documents(\n        cls: Type[DB],\n        documents: List[Document],\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Return database initialized from documents.\"\"\"\n        contents = [document.content for document in documents]\n        return cls.get_contents(contents, **kargs)\n"}
{"type": "source_file", "path": "extractors/base.py", "content": "import copy\nimport logging\nimport contextlib\nimport mimetypes\nfrom abc import ABC, abstractmethod\nfrom io import BufferedReader, BytesIO\nfrom pathlib import PurePath\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Iterator,\n    Mapping,\n    Generator,\n    Union\n)\nfrom pydantic import BaseModel, root_validator\n\nfrom databases import Document\n\nPathLike = Union[str, PurePath]\nlogger = logging.getLogger(__name__)\n\n\nclass TextSplitter(ABC):\n    \"\"\"Interface for splitting text into chunks.\"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 100,\n        length_function: Callable[[str], int] = len,\n    ):\n        \"\"\"Create a new TextSplitter.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self._length_function = length_function\n\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into multiple components.\"\"\"\n\n    def create_documents(\n        self, texts: List[str],\n        metadatas: Optional[List[dict]] = None\n    ) -> List[Document]:\n        \"\"\"Create documents from a list of texts.\"\"\"\n        _metadatas = metadatas or [{}] * len(texts)\n        documents = []\n        for i, text in enumerate(texts):\n            for chunk in self.split_text(text):\n                new_doc = Document(\n                    content=chunk,\n                    metadata=copy.deepcopy(_metadatas[i])\n                )\n                documents.append(new_doc)\n        return documents\n\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n        \"\"\"Split documents.\"\"\"\n        texts, metadatas = [], []\n        for doc in documents:\n            texts.append(doc.content)\n            metadatas.append(doc.metadata)\n        return self.create_documents(texts, metadatas=metadatas)\n\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n        text = separator.join(docs)\n        text = text.strip()\n        if text == \"\":\n            return None\n        else:\n            return text\n\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        separator_len = self._length_function(separator)\n\n        docs = []\n        current_doc: List[str] = []\n        total = 0\n        for d in splits:\n            _len = self._length_function(d)\n            if (\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\n                > self._chunk_size\n            ):\n                if total > self._chunk_size:\n                    logger.warning(\n                        f\"Created a chunk of size {total}, \"\n                        f\"which is longer than the specified {self._chunk_size}\"\n                    )\n                if len(current_doc) > 0:\n                    doc = self._join_docs(current_doc, separator)\n                    if doc is not None:\n                        docs.append(doc)\n                    # Keep on popping if:\n                    # - we have a larger chunk than in the chunk overlap\n                    # - or if we still have any chunks and the length is long\n                    while total > self._chunk_overlap or (\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n                        > self._chunk_size\n                        and total > 0\n                    ):\n                        total -= self._length_function(current_doc[0]) + (\n                            separator_len if len(current_doc) > 1 else 0\n                        )\n                        current_doc = current_doc[1:]\n            current_doc.append(d)\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\n        doc = self._join_docs(current_doc, separator)\n        if doc is not None:\n            docs.append(doc)\n        return docs\n\n    def transform_documents(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> Sequence[Document]:\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\n        return self.split_documents(list(documents))\n\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at characters.\n\n    Recursively tries to split by different characters to find one\n    that works.\n    \"\"\"\n\n    def __init__(self, separators: Optional[List[str]] = None, **kwargs: Any):\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(**kwargs)\n        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = self._separators[-1]\n        for _s in self._separators:\n            if _s == \"\":\n                separator = _s\n                break\n            if _s in text:\n                separator = _s\n                break\n        # Now that we have the separator, split the text\n        if separator:\n            splits = text.split(separator)\n        else:\n            splits = list(text)\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                other_info = self.split_text(s)\n                final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, separator)\n            final_chunks.extend(merged_text)\n        return final_chunks\n\n\nclass BaseLoader(ABC):\n    \"\"\"Interface for loading documents.\n\n    Implementations should implement the lazy-loading method using generators\n    to avoid loading all documents into memory at once.\n\n    The `load` method will remain as is for backwards compatibility, but its\n    implementation should be just `list(self.lazy_load())`.\n    \"\"\"\n\n    # Sub-classes should implement this method\n    # as return list(self.lazy_load()).\n    # This method returns a List which is materialized in memory.\n    @abstractmethod\n    def load(self) -> List[Document]:\n        \"\"\"Load data into document objects.\"\"\"\n\n    def load_and_split(\n        self, text_splitter: Optional[TextSplitter] = None\n    ) -> List[Document]:\n        \"\"\"Load documents and split into chunks.\"\"\"\n        if text_splitter is None:\n            _text_splitter: TextSplitter = RecursiveCharacterTextSplitter()\n        else:\n            _text_splitter = text_splitter\n        docs = self.load()\n        return _text_splitter.split_documents(docs)\n\n    # Attention: This method will be upgraded into an abstractmethod once it's\n    #            implemented in all the existing subclasses.\n    def lazy_load(\n        self,\n    ) -> Iterator[Document]:\n        \"\"\"A lazy loader for document content.\"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not implement lazy_load()\"\n        )\n\n\nclass Blob(BaseModel):\n    \"\"\"A blob is used to represent raw data by either reference or value.\n\n    Provides an interface to materialize the blob in different representations, and\n    help to decouple the development of data loaders from the downstream parsing of\n    the raw data.\n\n    Inspired by: https://developer.mozilla.org/en-US/docs/Web/API/Blob\n    \"\"\"\n\n    data: Union[bytes, str, None]  # Raw data\n    mimetype: Optional[str] = None  # Not to be confused with a file extension\n    encoding: str = \"utf-8\"  # Use utf-8 as default encoding, if decoding to string\n    # Location where the original content was found\n    # Represent location on the local file system\n    # Useful for situations where downstream code assumes it must work with file paths\n    # rather than in-memory content.\n    path: Optional[PathLike] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n        frozen = True\n\n    @property\n    def source(self) -> Optional[str]:\n        \"\"\"The source location of the blob as string if known otherwise none.\"\"\"\n        return str(self.path) if self.path else None\n\n    @root_validator(pre=True)\n    def check_blob_is_valid(cls, values: Mapping[str, Any]) -> Mapping[str, Any]:\n        \"\"\"Verify that either data or path is provided.\"\"\"\n        if \"data\" not in values and \"path\" not in values:\n            raise ValueError(\"Either data or path must be provided\")\n        return values\n\n    def as_string(self) -> str:\n        \"\"\"Read data as a string.\"\"\"\n        if self.data is None and self.path:\n            with open(str(self.path), \"r\", encoding=self.encoding) as f:\n                return f.read()\n        elif isinstance(self.data, bytes):\n            return self.data.decode(self.encoding)\n        elif isinstance(self.data, str):\n            return self.data\n        else:\n            raise ValueError(f\"Unable to get string for blob {self}\")\n\n    def as_bytes(self) -> bytes:\n        \"\"\"Read data as bytes.\"\"\"\n        if isinstance(self.data, bytes):\n            return self.data\n        elif isinstance(self.data, str):\n            return self.data.encode(self.encoding)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                return f.read()\n        else:\n            raise ValueError(f\"Unable to get bytes for blob {self}\")\n\n    @contextlib.contextmanager\n    def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:\n        \"\"\"Read data as a byte stream.\"\"\"\n        if isinstance(self.data, bytes):\n            yield BytesIO(self.data)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                yield f\n        else:\n            raise NotImplementedError(f\"Unable to convert blob {self}\")\n\n    @classmethod\n    def from_path(\n        cls,\n        path: PathLike,\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        guess_type: bool = True,\n    ):\n        \"\"\"Load the blob from a path like object.\n\n        Args:\n            path: path like object to file to be read\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            guess_type: If True, the mimetype will be guessed from the file extension,\n                        if a mime-type was not provided\n\n        Returns:\n            Blob instance\n        \"\"\"\n        if mime_type is None and guess_type:\n            _mimetype = mimetypes.guess_type(path)[0] if guess_type else None\n        else:\n            _mimetype = mime_type\n        # We do not load the data immediately, instead we treat the blob as a\n        # reference to the underlying data.\n        return cls(data=None, mimetype=_mimetype, encoding=encoding, path=path)\n\n    @classmethod\n    def from_data(\n        cls,\n        data: Union[str, bytes],\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        path: Optional[str] = None,\n    ):\n        \"\"\"Initialize the blob from in-memory data.\n\n        Args:\n            data: the in-memory data associated with the blob\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            path: if provided, will be set as the source from which the data came\n\n        Returns:\n            Blob instance\n        \"\"\"\n        return cls(data=data, mime_type=mime_type, encoding=encoding, path=path)\n\n    def __repr__(self) -> str:\n        \"\"\"Define the blob representation.\"\"\"\n        str_repr = f\"Blob {id(self)}\"\n        if self.source:\n            str_repr += f\" {self.source}\"\n        return str_repr\n\n\nclass BaseBlobParser(ABC):\n    \"\"\"Abstract interface for blob parsers.\n\n    A blob parser is provides a way to parse raw data stored in a blob into one\n    or more documents.\n\n    The parser can be composed with blob loaders, making it easy to re-use\n    a parser independent of how the blob was originally loaded.\n    \"\"\"\n\n    @abstractmethod\n    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n        \"\"\"Lazy parsing interface.\n\n        Subclasses are required to implement this method.\n\n        Args:\n            blob: Blob instance\n\n        Returns:\n            Generator of documents\n        \"\"\"\n\n    def parse(self, blob: Blob) -> List[Document]:\n        \"\"\"Eagerly parse the blob into a document or documents.\n\n        This is a convenience method for interactive development environment.\n\n        Production applications should favor the lazy_parse method instead.\n\n        Subclasses should generally not over-ride this parse method.\n\n        Args:\n            blob: Blob instance\n\n        Returns:\n            List of documents\n        \"\"\"\n        return list(self.lazy_parse(blob))\n\n\nclass BlobLoader(ABC):\n    \"\"\"Abstract interface for blob loaders implementation.\n\n    Implementer should be able to load raw content from a storage system according\n    to some criteria and return the raw content lazily as a stream of blobs.\n    \"\"\"\n\n    @abstractmethod\n    def yield_blobs(\n        self,\n    ) -> Iterable[Blob]:\n        \"\"\"A lazy loader for raw data represented by LangChain's Blob object.\n\n        Returns:\n            A generator over blobs\n        \"\"\"\n"}
{"type": "source_file", "path": "loggers/__init__.py", "content": "from .custom import logging_custom, AppLogger\n\n__all__ = [\n    \"logging_custom\",\n    \"AppLogger\"\n]\n"}
{"type": "source_file", "path": "databases/__init__.py", "content": "from .base import DB, Document, DatabaseBase\nfrom .mongoDB import MongoDBClient\n\n__all__ = [\n    \"DB\",\n    \"Document\",\n    \"DatabaseBase\",\n    \"MongoDBClient\"\n]\n"}
{"type": "source_file", "path": "llms/base.py", "content": "from abc import ABC, abstractmethod\n\n\nclass LLMBase(ABC):\n    \"\"\"LLM wrapper should take in a prompt and return a string.\"\"\"\n    @abstractmethod\n    def generate_prompt(\n        self,\n        system_content: str,\n        question: str,\n        context: str = \"\"\n    ) -> str:\n        \"\"\"Make prompt template based on context and question\"\"\"\n\n    @abstractmethod\n    def __call__(\n        self,\n        system_content: str,\n        question: str,\n        context: str = \"\"\n    ) -> str:\n        \"\"\"Call out to GPT4All's generate method.\n\n        Args:\n            question: The question to pass into the model.\n            context: The related context to pass into the model.\n\n        Returns:\n            The string generated by the model.\n\n        Example:\n            .. code-block:: python\n\n                context = \"\"\n                question = \"Once upon a time, \"\n                response = model(question)\n        \"\"\"\n"}
{"type": "source_file", "path": "memories/base.py", "content": "from abc import ABC, abstractmethod\n\n\nclass MemoryBase(ABC):\n    \"\"\"Base class for all memories.\"\"\"\n\n    @abstractmethod\n    def key(self):\n        \"\"\"Return all keys in the memory.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_answer(self, question):\n        \"\"\"Return the answer for the question.\"\"\"\n        pass\n\n    @abstractmethod\n    def add_chat_history(self, question, answer):\n        \"\"\"Add a pair of (question, answer) to the memory.\"\"\"\n        pass\n\n    @abstractmethod\n    def check_length(self, key):\n        \"\"\"Check if the memory is full.\"\"\"\n        pass\n"}
{"type": "source_file", "path": "memories/redis.py", "content": "import json\nfrom typing import List, Optional\n\nfrom loggers import AppLogger\nfrom memories.base import MemoryBase\n\nlogger = AppLogger().get_logger()\n\n\nclass RedisMemory(MemoryBase):\n    \"\"\"Redis-based memory.\"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        url: str = \"redis://localhost:6379/0\",\n        key_prefix: str = \"message_store:\",\n    ):\n        try:\n            import redis\n        except ImportError:\n            raise ImportError(\n                \"Could not import redis python package. \"\n                \"Please install it with `pip install redis`.\"\n            )\n\n        try:\n            self.redis_client = redis.from_url(url=url)\n            self.session_id = session_id\n            self.key_prefix = key_prefix\n        except redis.exceptions.ConnectionError as error:\n            logger.error(error)\n\n    @property\n    def key(self) -> str:\n        \"\"\"Construct the record key to use\"\"\"\n        return self.key_prefix + self.session_id\n\n    def get_answer(self, question) -> Optional[str]:\n        \"\"\"Retrieve the messages from Redis\"\"\"\n        _items = self.redis_client.lrange(self.key, 0, -1)\n        items = [json.loads(m.decode(\"utf-8\")) for m in _items[::-1]]\n        for item in items:\n            if item[0].lower() == question.lower():\n                return item[1]\n\n    def add_chat_history(self, question, answer) -> None:\n        \"\"\"Append the pair of (question, answer) to the record in Redis\"\"\"\n        self.redis_client.lpush(self.key, json.dumps((question, answer)))\n\n    def remove_chat_history(self) -> None:\n        \"\"\"Remove the pair of (question, answer) from the record in Redis\"\"\"\n        self.redis_client.rpop(self.key)\n\n    def clear(self) -> None:\n        \"\"\"Clear session memory from Redis\"\"\"\n        self.redis_client.delete(self.key)\n\n    def check_length(self) -> int:\n        \"\"\"Check the length of the session memory\"\"\"\n        return self.redis_client.llen(self.key)\n"}
{"type": "source_file", "path": "routers/__init__.py", "content": "from .service_api import ui_router\nfrom .db_api import db_router, ingest_router\nfrom .chatbot_ui import chatbot_router\nfrom .utils import safe_join, strip_url\n\n__all__ = [\n    \"ui_router\",\n    \"db_router\",\n    \"chatbot_router\",\n    \"ingest_router\",\n    \"safe_join\",\n    \"strip_url\",\n]\n"}
{"type": "source_file", "path": "routers/chatbot_ui.py", "content": "import gradio as gr\n\nfrom config import settings\nfrom llms import GPT4AllModel\nfrom loggers import AppLogger\nfrom memories import RedisMemory\nfrom searchers import ElasticSearch\nfrom src import (\n    clear_history,\n    post_process_answer,\n    post_process_code,\n    reset_textbox,\n)\n\nlogger = AppLogger().get_logger()\nllm = GPT4AllModel(model_path=settings.MODEL_PATH)\n\n\ndef predict(\n    system_content: str,\n    question: str,\n    index_name: str,\n    server_host: str,\n    model_type: str,\n    model_path: str,\n    chatbot: list,\n    history: list,\n):\n    try:\n        # Prepare the LLM and Elasticsearch\n        global llm\n        if model_path != settings.MODEL_PATH:\n            llm = GPT4AllModel(model_path=model_path)\n        elif model_type != \"GPT4All\":\n            logger.info(f\"Model {model_type} not supported!\")\n        es = ElasticSearch(\n            elasticsearch_host=f\"{server_host}:9200\", index_name=index_name\n        )\n        redis_memory = RedisMemory(session_id=\"chatbot\")\n\n        # Get the answer from the chain\n        logger.info(f\"Question: {question}\")\n        if len(question) >= 20:\n            documents = es.simple_search(query=question)\n            logger.info(f\"Document: {documents}\")\n        else:\n            documents = None\n            logger.info(\"Simple question\")\n\n        answer_from_redis = redis_memory.get_answer(question)\n        if answer_from_redis:\n            history.append(question)\n            history.append(answer_from_redis)\n            chatbot = [(history[i], history[i + 1]) for i in range(0, len(history), 2)]\n            return chatbot, history\n\n        answer = llm(\n            system_content=system_content, question=question, context=documents\n        )\n        answer = post_process_code(answer)\n        answer = post_process_answer(answer, documents, server_host)\n        logger.info(f\"Answer: {answer}\")\n        redis_memory.add_chat_history(question, answer)\n        history.append(question)\n        history.append(answer)\n        chatbot = [(history[i], history[i + 1]) for i in range(0, len(history), 2)]\n        return chatbot, history\n\n    except Exception as e:\n        logger.info(f\"Question: {e}\")\n        answer = settings.SERVER_ERROR_MSG + \" (error_code: 503)\"\n        logger.info(f\"Answer: {answer}\")\n        history.append(question)\n        history.append(answer)\n        chatbot = [(history[i], history[i + 1]) for i in range(0, len(history), 2)]\n        return chatbot, history\n\n\ntitle = \"\"\"\n<h1 align=\"center\">Chat with AI Chatbot 🤖</h1>\n\"\"\"\ncurrent_version = \"2.0.0\"\nversion = f\"\"\"\n- Version 1.0.0: Pipeline with GPT4All, Elasticsearch, MongoDB and Gradio\n- Version {current_version}: Add Redis Memory, Version, Login, and change User Interface\n\"\"\"\n\nwith gr.Blocks(\n    css=\"\"\"\n    footer .svelte-1ax1toq {display: none !important;}\n    #col_container {margin-left: auto; margin-right: auto;}\n    #chatbot .block.svelte-90oupt {height:600px;}\n    #chatbot .message.user.svelte-1pjfiar.svelte-1pjfiar \\\n    {width:fit-content; background:orange; border-bottom-right-radius:0}\n    #chatbot .message.bot.svelte-1pjfiar.svelte-1pjfiar \\\n    {width:fit-content; padding-left: 16px; border-bottom-left-radius:0}\n    #chatbot .pre {border:2px solid white;}\n    pre {\n    white-space: pre-wrap;       /* Since CSS 2.1 */\n    white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */\n    white-space: -pre-wrap;      /* Opera 4-6 */\n    white-space: -o-pre-wrap;    /* Opera 7 */\n    word-wrap: break-word;       /* Internet Explorer 5.5+ */\n    }\n    \"\"\"\n) as chatbot_router:\n    gr.HTML(value=title)\n    with gr.Row():\n        with gr.Column(elem_id=\"col_container\", scale=1):\n            with gr.Accordion(label=\"Prompt\", open=True):\n                system_content = gr.Textbox(\n                    value=settings.SYSTEM_DEFAULT, show_label=False\n                )\n            with gr.Accordion(label=\"Config\", open=True):\n                index_name = gr.Textbox(value=settings.INDEX_NAME, label=\"index_name\")\n                server_host = gr.Textbox(\n                    value=settings.SERVER_HOST, label=\"server_host\"\n                )\n                model_type = gr.Textbox(value=settings.MODEL_TYPE, label=\"model_type\")\n                model_path = gr.Textbox(value=settings.MODEL_PATH, label=\"model_path\")\n\n        with gr.Column(elem_id=\"col_container\", scale=3):\n            inital_chat = [\"👋\", \"Hi user, I'm an AI Assistant 🤖 trained from GPT4ALL!\"]\n            chatbot = gr.Chatbot(\n                value=[inital_chat],\n                elem_id=\"chatbot\",\n                label=f\"AI Chatbot 🤖 - version {current_version}\",\n            )\n            question = gr.Textbox(\n                placeholder=\"Ask something\", show_label=False, value=\"\"\n            )\n            state = gr.State(inital_chat)\n            with gr.Row():\n                with gr.Column():\n                    submit_btn = gr.Button(value=\"🚀 Send\")\n                with gr.Column():\n                    clear_btn = gr.Button(value=\"🗑️ Clear history\")\n\n    gr.HTML(value=\"<h3>📋 Services User Interface</h3>\")\n    with gr.Row():\n        with gr.Column():\n            gr.Button(value=\"Ingest Database\", link=\"/show-ingest\")\n        with gr.Column():\n            gr.Button(value=\"Mongo Express\", link=\"/mongoexpress\")\n        with gr.Column():\n            gr.Button(value=\"Elastic Search\", link=\"/elasticsearch\")\n        with gr.Column():\n            gr.Button(value=\"Kibana\", link=\"/kibana\")\n\n    gr.Markdown(version)\n\n    question.submit(\n        predict,\n        [\n            system_content,\n            question,\n            index_name,\n            server_host,\n            model_type,\n            model_path,\n            chatbot,\n            state,\n        ],\n        [chatbot, state],\n    )\n    submit_btn.click(\n        predict,\n        [\n            system_content,\n            question,\n            index_name,\n            server_host,\n            model_type,\n            model_path,\n            chatbot,\n            state,\n        ],\n        [chatbot, state],\n    )\n    submit_btn.click(reset_textbox, [], [question])\n    clear_btn.click(clear_history, None, [chatbot, state, question])\n    question.submit(reset_textbox, [], [question])\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--server-name\", default=\"0.0.0.0\")\n    parser.add_argument(\"--server-port\", default=8071)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--verbose\", action=\"store_true\")\n    args = parser.parse_args()\n    chatbot_router.launch(\n        server_name=args.server_name,\n        server_port=args.server_port,\n        share=args.share,\n        debug=args.debug,\n        ssl_verify=False,\n    )\n"}
{"type": "source_file", "path": "llms/__init__.py", "content": "from .base import LLMBase\nfrom .gpt4all import GPT4AllModel\n\n__all__ = [\"LLMBase\", \"GPT4AllModel\"]\n"}
{"type": "source_file", "path": "routers/utils.py", "content": "import httpx\nimport os\nimport posixpath\nfrom fastapi import HTTPException\n\n\ndef strip_url(orig_url: str) -> str:\n    \"\"\"\n    Strips the query parameters and trailing slash from a URL.\n    \"\"\"\n    parsed_url = httpx.URL(orig_url)\n    stripped_url = parsed_url.copy_with(query=None)\n    stripped_url = str(stripped_url)\n    return stripped_url.rstrip(\"/\")\n\n\ndef safe_join(directory: str, path: str) -> str:\n    \"\"\"Safely path to a base directory to avoid escaping the base directory.\n    Borrowed from: werkzeug.security.safe_join\"\"\"\n    _os_alt_seps = [\n        sep for sep in [os.path.sep, os.path.altsep] if sep is not None and sep != \"/\"\n    ]\n\n    if path == \"\":\n        raise HTTPException(400)\n\n    filename = posixpath.normpath(path)\n    fullpath = os.path.join(directory, filename)\n    if (\n        any(sep in filename for sep in _os_alt_seps)\n        or os.path.isabs(filename)\n        or filename == \"..\"\n        or filename.startswith(\"../\")\n        or os.path.isdir(fullpath)\n    ):\n        raise HTTPException(403)\n\n    if not os.path.exists(fullpath):\n        raise HTTPException(404, \"File not found\")\n\n    return fullpath\n"}
{"type": "source_file", "path": "extractors/pdf.py", "content": "import os\nimport logging\nimport tempfile\nimport requests\nfrom abc import ABC\nfrom typing import Iterator, List\nfrom urllib.parse import urlparse\nfrom pathlib import Path\n\nfrom extractors import BaseLoader, Blob, BaseBlobParser\nfrom databases import Document\n\n\nlogger = logging.getLogger(__file__)\n\n\nclass BasePDFLoader(BaseLoader, ABC):\n    \"\"\"Base loader class for PDF files.\n\n    Defaults to check for local file, but if the file is a web path, it will download it\n    to a temporary file, and use that, then clean up the temporary file after completion\n    \"\"\"\n\n    def __init__(self, file_path: str):\n        \"\"\"Initialize with file path.\"\"\"\n        self.file_path = file_path\n        self.web_path = None\n        if \"~\" in self.file_path:\n            self.file_path = os.path.expanduser(self.file_path)\n\n        # If the file is a web path, download it to a temporary file, and use that\n        if not os.path.isfile(self.file_path) and self._is_valid_url(self.file_path):\n            r = requests.get(self.file_path)\n\n            if r.status_code != 200:\n                raise ValueError(\n                    \"Check the url of your file; returned status code %s\"\n                    % r.status_code\n                )\n\n            self.web_path = self.file_path\n            self.temp_file = tempfile.NamedTemporaryFile()\n            self.temp_file.write(r.content)\n            self.file_path = self.temp_file.name\n        elif not os.path.isfile(self.file_path):\n            raise ValueError(\"File path %s is not a valid file or url\" % self.file_path)\n\n    def __del__(self) -> None:\n        if hasattr(self, \"temp_file\"):\n            self.temp_file.close()\n\n    @staticmethod\n    def _is_valid_url(url: str) -> bool:\n        \"\"\"Check if the url is valid.\"\"\"\n        parsed = urlparse(url)\n        return bool(parsed.netloc) and bool(parsed.scheme)\n\n    @property\n    def source(self) -> str:\n        return self.web_path if self.web_path is not None else self.file_path\n\n\nclass PyPDFParser(BaseBlobParser):\n    \"\"\"Loads a PDF with pypdf and chunks at character level.\"\"\"\n\n    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n        \"\"\"Lazily parse the blob.\"\"\"\n        import pypdf\n\n        with blob.as_bytes_io() as pdf_file_obj:\n            pdf_reader = pypdf.PdfReader(pdf_file_obj)\n            yield from [\n                Document(\n                    content=page.extract_text(),\n                    metadata={\"source\": blob.source, \"page\": page_number + 1},\n                )\n                for page_number, page in enumerate(pdf_reader.pages)\n            ]\n\n\nclass PyPDFLoader(BasePDFLoader):\n    \"\"\"Loads a PDF with pypdf and chunks at character level.\n\n    Loader also stores page numbers in metadatas.\n    \"\"\"\n\n    def __init__(self, file_path: str) -> None:\n        \"\"\"Initialize with file path.\"\"\"\n        try:\n            import pypdf  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"pypdf package not found, please install it with \" \"`poetry add pypdf`\"\n            )\n        self.parser = PyPDFParser()\n        super().__init__(file_path)\n\n    def load(self) -> List[Document]:\n        \"\"\"Load given path as pages.\"\"\"\n        return list(self.lazy_load())\n\n    def lazy_load(\n        self,\n    ) -> Iterator[Document]:\n        \"\"\"Lazy load given path as pages.\"\"\"\n        blob = Blob.from_path(self.file_path)\n        yield from self.parser.parse(blob)\n\n\nclass PyPDFDirectoryLoader(BaseLoader):\n    \"\"\"Loads a directory with PDF files with pypdf and chunks at character level.\n\n    Loader also stores page numbers in metadatas.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        glob: str = \"**/[!.]*.pdf\",\n        silent_errors: bool = False,\n        load_hidden: bool = False,\n        recursive: bool = False,\n    ):\n        self.path = path\n        self.glob = glob\n        self.load_hidden = load_hidden\n        self.recursive = recursive\n        self.silent_errors = silent_errors\n\n    @staticmethod\n    def _is_visible(path: Path) -> bool:\n        return not any(part.startswith(\".\") for part in path.parts)\n\n    def load(self) -> List[Document]:\n        p = Path(self.path)\n        docs = []\n        items = p.rglob(self.glob) if self.recursive else p.glob(self.glob)\n        for i in items:\n            if i.is_file():\n                if self._is_visible(i.relative_to(p)) or self.load_hidden:\n                    try:\n                        loader = PyPDFLoader(str(i))\n                        sub_docs = loader.load()\n                        for doc in sub_docs:\n                            doc.metadata[\"source\"] = str(i)\n                        docs.extend(sub_docs)\n                    except Exception as e:\n                        if self.silent_errors:\n                            logger.warning(e)\n                        else:\n                            raise e\n        return docs\n"}
{"type": "source_file", "path": "searchers/base.py", "content": "from typing import List, Iterable, Any, Tuple, Type, TypeVar\nfrom abc import ABC, abstractmethod\n\nfrom databases import Document\n\nDB = TypeVar(\"DB\", bound=\"SearchBase\")\n\n\nclass SearchBase(ABC):\n    @abstractmethod\n    def add_contents(\n        self,\n        contents: Iterable[str],\n        **kargs: Any\n    ) -> List[str]:\n        \"\"\"Run more contents through the items and add to the database.\n\n        Args:\n            contents: Iterable of strings to add to the database.\n            kwargs: database specific parameters\n\n        Returns:\n            List of ids from adding the texts into the database.\n        \"\"\"\n\n    def add_documents(\n        self,\n        documents: List[Document],\n        **kargs: Any\n    ) -> List[str]:\n        \"\"\"Run more documents through the items and add to the database.\n\n        Args:\n            documents (List[Document]: Documents to add to the database.\n\n        Returns:\n            List[str]: List of IDs of the added contents.\n        \"\"\"\n        contents = [document[\"content\"] for document in documents]\n        return self.add_contents(contents, **kargs)\n\n    def search(\n        self,\n        query: str,\n        search_type: str,\n        **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to query using specified search type.\"\"\"\n        if search_type == \"simple\":\n            return self.simple_search(query, **kwargs)\n        else:\n            raise ValueError(\n                f\"search_type of {search_type} not allowed. Expected \"\n                \"search_type to be 'simple'.\"\n            )\n\n    @abstractmethod\n    def simple_search(\n        self,\n        query: str,\n        k: int = 2,\n        **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to query.\"\"\"\n\n    def simple_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 2,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs and relevance scores.\n\n        Args:\n            query: input text\n            k: Number of Documents to return. Defaults to 2.\n            **kwargs: kwargs to be passed to simple search. Should include:\n                score_threshold: Optional, a floating point value to\n                    filter the resulting set of retrieved docs\n\n        Returns:\n            List of Tuples of (doc, simple_score)\n        \"\"\"\n        docs_and_simple_scores = self._simple_search_with_relevance_scores(\n            query, k=k, **kwargs\n        )\n\n        score_threshold = kwargs.get(\"score_threshold\")\n        if score_threshold is not None:\n            docs_and_simple_scores = [\n                (doc, score)\n                for doc, score in docs_and_simple_scores\n                if score >= score_threshold\n            ]\n        return docs_and_simple_scores\n\n    def _simple_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 2,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs and relevance scores.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def get_contents(\n        cls: Type[DB],\n        contents: Iterable[str],\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Return database initialized from contents.\"\"\"\n\n    @classmethod\n    def get_documents(\n        cls: Type[DB],\n        documents: List[Document],\n        **kargs: Any\n    ) -> DB:\n        \"\"\"Return database initialized from documents.\"\"\"\n        contents = [document.content for document in documents]\n        return cls.get_contents(contents, **kargs)\n"}
{"type": "source_file", "path": "routers/service_api.py", "content": "from fastapi import APIRouter\nfrom starlette.responses import RedirectResponse\n\nfrom config import settings\nfrom loggers import AppLogger\n\nlogger = AppLogger().get_logger()\nui_router = APIRouter()\n\n\n@ui_router.get(\"/elasticsearch\")\nasync def route_elasticsearch():\n    return RedirectResponse(url=settings.SERVER_HOST + \":9200/\")\n\n\n@ui_router.get(\"/kibana\")\nasync def route_kibana():\n    return RedirectResponse(url=settings.SERVER_HOST + \":5601/\")\n\n\n@ui_router.get(\"/mongodb\")\nasync def route_mongodb():\n    return RedirectResponse(url=settings.SERVER_HOST + \":27017/\")\n\n\n@ui_router.get(\"/mongoexpress\")\nasync def route_mongo_express():\n    return RedirectResponse(url=settings.SERVER_HOST + \":8081/\")\n\n\n@ui_router.get(\"/website\")\nasync def route_website():\n    return RedirectResponse(url=\"https://khoivn.space/\")\n"}
{"type": "source_file", "path": "routers/db_api.py", "content": "import gradio as gr\nfrom fastapi import APIRouter\n\nfrom loggers import AppLogger\nfrom src import save_upload_file, ingest_database, ingest_search\n\nlogger = AppLogger().get_logger()\ndb_router = APIRouter()\n\n\ndef upload_file(files):\n    file_paths = [file.name for file in files]\n    return file_paths\n\n\ndef ingest_process(files):\n    try:\n        save_upload_file(uploaded_files=files)\n        ingest_database(database_name=\"document\")\n        ingest_search(index_name=\"document\")\n        return \"Done\"\n    except Exception as e:\n        logger.info(\"Please check server host\")\n        logger.error(f\"Exception: {e}\")\n\n\nwith gr.Blocks() as ingest_router:\n    gr.HTML(value=\"\"\"<h1 align=\"center\">Ingest with AI Chatbot 🤖</h1>\"\"\")\n    uploaded_file = gr.Files(file_types=[\".pdf\"])\n\n    text_output = gr.Textbox(label=\"Ingest Status\", default_text=\"\")\n    text_button = gr.Button(\"🚀 Start Ingest\")\n    text_button.click(ingest_process, uploaded_file, text_output)\n"}
{"type": "source_file", "path": "src/__init__.py", "content": "from .utils import post_process_answer, post_process_code, reset_textbox, clear_history\nfrom .functions import save_upload_file, ingest_search, ingest_database\n\n__all__ = [\n    \"post_process_answer\",\n    \"post_process_code\",\n    \"reset_textbox\",\n    \"clear_history\",\n    \"save_upload_file\",\n    \"ingest_search\",\n    \"ingest_database\",\n]\n"}
{"type": "source_file", "path": "memories/__init__.py", "content": "from .redis import RedisMemory\n\n__all__ = [\"RedisMemory\"]\n"}
{"type": "source_file", "path": "searchers/elastic_search.py", "content": "import uuid\nfrom typing import List, Optional, Dict, Any, Tuple\nfrom abc import ABC\n\nfrom searchers import SearchBase\nfrom databases import Document\n\n\ndef _default_script_query(query: str, filter: Optional[dict]) -> Dict:\n    if filter:\n        ((key, value),) = filter.items()\n        filter = {\"match\": {f\"metadata.{key}.keyword\": f\"{value}\"}}\n    else:\n        filter = {\"match_all\": {}}\n    return {\n        \"query\": {\n            \"bool\": {\n                \"should\": [\n                    {\"multi_match\": {\"query\": f\"{query}\"}},\n                    {\"match_phrase\": {\"content\": {\"query\": f\"{query}\", \"boost\": 2}}},\n                ]\n            }\n        }\n    }\n\n\nclass ElasticSearch(SearchBase, ABC):\n    \"\"\"Wrapper around Elasticsearch as a database.\"\"\"\n\n    def __init__(\n        self,\n        elasticsearch_host: str,\n        index_name: str,\n    ):\n        \"\"\"Initialize with necessary components.\"\"\"\n        try:\n            import elasticsearch\n        except ImportError:\n            raise ImportError(\n                \"Could not import elasticsearch python package. \"\n                \"Please install it with `poetry add elasticsearch`.\"\n            )\n        self.index_name = index_name\n        try:\n            self.client = elasticsearch.Elasticsearch(elasticsearch_host)\n        except ValueError as e:\n            raise ValueError(\n                f\"Your elasticsearch client string is mis-formatted. Got error: {e} \"\n            )\n\n    def rec_to_actions(self, documents):\n        for i, document in enumerate(documents):\n            _id = str(uuid.uuid4())\n            yield {\n                \"_op_type\": \"index\",\n                \"_index\": self.index_name,\n                \"content\": document[\"content\"],\n                \"metadata\": document[\"metadata\"],\n                \"_id\": _id,\n            }\n\n    def add_contents(\n        self,\n        documents: List[Document],\n        refresh_indices: bool = True,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more contents through the embeddings and add to the vectorstore.\n\n        Args:\n            contents: Iterable of strings to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the contents.\n            refresh_indices: bool to refresh ElasticSearch indices\n\n        Returns:\n            List of ids from adding the contents into the Elasticsearch.\n        \"\"\"\n        try:\n            from elasticsearch.helpers import bulk\n        except ImportError:\n            raise ImportError(\n                \"Could not import elasticsearch python package. \"\n                \"Please install it with `poetry add elasticsearch`.\"\n            )\n\n        # check to see if the index already exists\n        if not (self.client.indices.exists(index=self.index_name)):\n            self.client.indices.create(index=self.index_name)\n\n        bulk(self.client, self.rec_to_actions(documents))\n        if refresh_indices:\n            self.client.indices.refresh(index=self.index_name)\n\n    def simple_search(\n        self, query: str, k: int = 2, filter: Optional[dict] = None, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to query.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 2.\n\n        Returns:\n            List of Documents most similar to the query.\n        \"\"\"\n        docs_and_scores = self.simple_search_with_score(query, k, filter=filter)\n        if docs_and_scores is not None:\n            documents = [d[0] for d in docs_and_scores]\n            return documents\n        return None\n\n    def simple_search_with_score(\n        self, query: str, k: int = 2, filter: Optional[dict] = None, **kwargs: Any\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs most similar to query.\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n        Returns:\n            List of Documents most similar to the query.\n        \"\"\"\n        try:\n            script_query = _default_script_query(query, filter)\n            response = self.client.search(\n                index=self.index_name, body=script_query, size=k\n            )\n            hits = [hit for hit in response[\"hits\"][\"hits\"]]\n            docs_and_scores = [\n                (\n                    Document(\n                        content=hit[\"_source\"][\"content\"],\n                        metadata=hit[\"_source\"][\"metadata\"],\n                    ),\n                    hit[\"_score\"],\n                )\n                for hit in hits\n            ]\n            return docs_and_scores\n        except ValueError:\n            return None\n\n    @classmethod\n    def get_contents(\n        cls,\n        contents: List[str],\n        metadatas: Optional[List[dict]] = None,\n        elasticsearch_host: Optional[str] = None,\n        index_name: Optional[str] = None,\n        refresh_indices: bool = True,\n        **kwargs: Any,\n    ):\n        \"\"\"Construct ElasticSearch wrapper from raw documents.\"\"\"\n        elasticsearch_host = elasticsearch_host\n        index_name = index_name or uuid.uuid4().hex\n        scoresearch = cls(elasticsearch_host, index_name, **kwargs)\n        scoresearch.add_contents(\n            contents, metadatas=metadatas, refresh_indices=refresh_indices\n        )\n        return scoresearch\n"}
{"type": "source_file", "path": "src/functions.py", "content": "import os\nimport shutil\n\nfrom databases import MongoDBClient\nfrom extractors import PyPDFDirectoryLoader\nfrom loggers import AppLogger\nfrom searchers import ElasticSearch\nfrom config import settings\n\nlogger = AppLogger().get_logger()\n\n\ndef save_upload_file(uploaded_files):\n    for uploaded_file in uploaded_files:\n        file_name = os.path.basename(uploaded_file.name)\n        logger.info(f\"file_name {file_name}\")\n        shutil.copy(src=uploaded_file.orig_name, dst=f\"{settings.DATA_PATH}{file_name}\")\n\n\ndef ingest_database(database_name: str):\n    try:\n        logger.info(f\"Start ingest fodler {settings.DATA_PATH} to Mongodb\")\n        pdf_loader = PyPDFDirectoryLoader(settings.DATA_PATH)\n        pages = pdf_loader.load_and_split()\n        mongo_database = MongoDBClient(\n            mongodb_host=settings.SERVER_HOST.replace(\"http\", \"mongodb\") + \":27017/\",\n            database_name=database_name,\n        )\n        mongo_database.check_database()\n        for page in pages:\n            mongo_database.add_contents(contents=[page.dict()])\n        logger.info(\n            \"Please check Mongo express \\\n        in database {database_name} with port 8081\"\n        )\n    except Exception as e:\n        logger.info(\"Please check server host\")\n        logger.info(f\"Exception: {e}\")\n\n\ndef ingest_search(index_name: str):\n    try:\n        logger.info(\"Start ingest data to ElasticSearch\")\n        mongo_database = MongoDBClient(\n            mongodb_host=settings.SERVER_HOST.replace(\"http\", \"mongodb\") + \":27017/\"\n        )\n        mongo_database.check_database()\n        documents = mongo_database.get_contents()\n        es = ElasticSearch(\n            elasticsearch_host=settings.SERVER_HOST + \":9200/\", index_name=index_name\n        )\n        es.add_contents(documents=documents)\n        logger.info(\n            \"Please check Kibana \\\n        in index {index_name} port 5601\"\n        )\n    except Exception as e:\n        logger.info(\"Please check server host\")\n        logger.info(f\"Exception: {e}\")\n"}
{"type": "source_file", "path": "src/utils.py", "content": "import gradio as gr\n\nfrom loggers import AppLogger\n\nlogger = AppLogger().get_logger()\n\n\ndef clear_history():\n    state = None\n    return ([], state, \"\")\n\n\ndef post_process_code(code):\n    sep = \"\\n```\"\n    if sep in code:\n        blocks = code.split(sep)\n        if len(blocks) % 2 == 1:\n            for i in range(1, len(blocks), 2):\n                blocks[i] = blocks[i].replace(\"\\\\_\", \"_\")\n        code = sep.join(blocks)\n    return code\n\n\ndef post_process_answer(answer, metadata=None, server_host=None):\n    if metadata is not None:\n        source = metadata[0].metadata[\"source\"]\n        page = metadata[0].metadata[\"page\"]\n        path = f\"{source}#page={page}\"\n        url_pdf = f\"{server_host}/static/pdf/{path}\"\n        answer += f\"<br><br>Source: <a href='{url_pdf}' _target='blank'>{path}</a>\"\n    answer = answer.replace(\"\\n\", \"<br>\")\n    return answer\n\n\ndef reset_textbox():\n    return gr.update(value=\"\")\n"}
{"type": "source_file", "path": "src/ingest_database.py", "content": "import argparse\n\nfrom databases import MongoDBClient\nfrom extractors import PyPDFDirectoryLoader\nfrom loggers import AppLogger\n\nlogger = AppLogger().get_logger()\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--mongodb-host\", default=\"mongodb://localhost:27017/\")\nparser.add_argument(\"--data-path\", default=\"samples/\")\nargs = parser.parse_args()\nlogger.info(f\"MongoDB Host: {args.mongodb_host}\")\nlogger.info(f\"Data path: {args.data_path}\")\n\n\ndef main(args):\n    pdf_loader = PyPDFDirectoryLoader(args.data_path)\n    pages = pdf_loader.load_and_split()\n    mongo_database = MongoDBClient(mongodb_host=args.mongodb_host)\n    mongo_database.check_database()\n    for page in pages:\n        mongo_database.add_contents(contents=[page.dict()])\n\n\nif __name__ == '__main__':\n    main(args)\n"}
{"type": "source_file", "path": "src/ingest_search.py", "content": "import argparse\nfrom databases import MongoDBClient\nfrom searchers import ElasticSearch\nfrom loggers import AppLogger\n\nlogger = AppLogger().get_logger()\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--mongodb-host\", default=\"mongodb://localhost:27017/\")\nparser.add_argument(\"--es-host\", default=\"http://localhost:9200/\")\nparser.add_argument(\"--index-name\", default=\"document\")\nargs = parser.parse_args()\nlogger.info(f\"MongoDB Host: {args.mongodb_host}\")\nlogger.info(f\"ElasticSearch Host: {args.es_host}\")\nlogger.info(f\"Index name: {args.index_name}\")\n\n\ndef main(args):\n    mongo_database = MongoDBClient(mongodb_host=args.mongodb_host)\n    mongo_database.check_database()\n    documents = mongo_database.get_contents()\n    es = ElasticSearch(\n        elasticsearch_host=args.es_host,\n        index_name=\"document\"\n    )\n    es.add_contents(documents=documents)\n\n\nif __name__ == '__main__':\n    main(args)\n"}
