{"repo_info": {"repo_name": "Pano2Room", "repo_owner": "TrickyGo", "repo_url": "https://github.com/TrickyGo/Pano2Room"}}
{"type": "test_file", "path": "modules/inpainters/lama/models/ade20k/segm_lib/nn/modules/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "modules/inpainters/lama/models/ade20k/segm_lib/nn/modules/tests/test_numeric_batchnorm.py", "content": "# -*- coding: utf-8 -*-\n# File   : test_numeric_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\nclass NumericTestCase(TorchTestCase):\n    def testNumericBatchNorm(self):\n        a = torch.rand(16, 10)\n        bn = nn.BatchNorm2d(10, momentum=1, eps=1e-5, affine=False)\n        bn.train()\n\n        a_var1 = Variable(a, requires_grad=True)\n        b_var1 = bn(a_var1)\n        loss1 = b_var1.sum()\n        loss1.backward()\n\n        a_var2 = Variable(a, requires_grad=True)\n        a_mean2 = a_var2.mean(dim=0, keepdim=True)\n        a_std2 = torch.sqrt(handy_var(a_var2, unbias=False).clamp(min=1e-5))\n        # a_std2 = torch.sqrt(a_var2.var(dim=0, keepdim=True, unbiased=False) + 1e-5)\n        b_var2 = (a_var2 - a_mean2) / a_std2\n        loss2 = b_var2.sum()\n        loss2.backward()\n\n        self.assertTensorClose(bn.running_mean, a.mean(dim=0))\n        self.assertTensorClose(bn.running_var, handy_var(a))\n        self.assertTensorClose(a_var1.data, a_var2.data)\n        self.assertTensorClose(b_var1.data, b_var2.data)\n        self.assertTensorClose(a_var1.grad, a_var2.grad)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "modules/inpainters/lama/models/ade20k/segm_lib/nn/modules/tests/test_sync_batchnorm.py", "content": "# -*- coding: utf-8 -*-\n# File   : test_sync_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, DataParallelWithCallback\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\ndef _find_bn(module):\n    for m in module.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, SynchronizedBatchNorm1d, SynchronizedBatchNorm2d)):\n            return m\n\n\nclass SyncTestCase(TorchTestCase):\n    def _syncParameters(self, bn1, bn2):\n        bn1.reset_parameters()\n        bn2.reset_parameters()\n        if bn1.affine and bn2.affine:\n            bn2.weight.data.copy_(bn1.weight.data)\n            bn2.bias.data.copy_(bn1.bias.data)\n\n    def _checkBatchNormResult(self, bn1, bn2, input, is_train, cuda=False):\n        \"\"\"Check the forward and backward for the customized batch normalization.\"\"\"\n        bn1.train(mode=is_train)\n        bn2.train(mode=is_train)\n\n        if cuda:\n            input = input.cuda()\n\n        self._syncParameters(_find_bn(bn1), _find_bn(bn2))\n\n        input1 = Variable(input, requires_grad=True)\n        output1 = bn1(input1)\n        output1.sum().backward()\n        input2 = Variable(input, requires_grad=True)\n        output2 = bn2(input2)\n        output2.sum().backward()\n\n        self.assertTensorClose(input1.data, input2.data)\n        self.assertTensorClose(output1.data, output2.data)\n        self.assertTensorClose(input1.grad, input2.grad)\n        self.assertTensorClose(_find_bn(bn1).running_mean, _find_bn(bn2).running_mean)\n        self.assertTensorClose(_find_bn(bn1).running_var, _find_bn(bn2).running_var)\n\n    def testSyncBatchNormNormalTrain(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True)\n\n    def testSyncBatchNormNormalEval(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False)\n\n    def testSyncBatchNormSyncTrain(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True, cuda=True)\n\n    def testSyncBatchNormSyncEval(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False, cuda=True)\n\n    def testSyncBatchNorm2DSyncTrain(self):\n        bn = nn.BatchNorm2d(10)\n        sync_bn = SynchronizedBatchNorm2d(10)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10, 16, 16), True, cuda=True)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "source_file", "path": "modules/equilib/torch_utils/grid.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Optional\n\nimport torch\n\nfrom modules.equilib.torch_utils.intrinsic import pi\n\n\ndef create_grid(\n    height: int,\n    width: int,\n    batch: Optional[int] = None,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create coordinate grid with height and width\n\n    `z-axis` scale is `1`\n\n    params:\n    - height (int)\n    - width (int)\n    - batch (Optional[int])\n    - dtype (torch.dtype)\n    - device (torch.device)\n\n    return:\n    - grid (torch.Tensor)\n    \"\"\"\n\n    # NOTE: RuntimeError: \"linspace_cpu\" not implemented for Half\n    if device.type == \"cpu\":\n        assert dtype in (torch.float32, torch.float64), (\n            f\"ERR: {dtype} is not supported by {device.type}\\n\"\n            \"If device is `cpu`, use float32 or float64\"\n        )\n\n    _xs = torch.linspace(0, width - 1, width, dtype=dtype, device=device)\n    _ys = torch.linspace(0, height - 1, height, dtype=dtype, device=device)\n    # NOTE: https://github.com/pytorch/pytorch/issues/15301\n    # Torch meshgrid behaves differently than numpy\n    ys, xs = torch.meshgrid([_ys, _xs], indexing=\"ij\")\n    zs = torch.ones_like(xs, dtype=dtype, device=device)\n    grid = torch.stack((xs, ys, zs), dim=2)\n    # grid shape (h, w, 3)\n\n    # batched (stacked copies)\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        grid = torch.cat([grid.unsqueeze(0)] * batch)\n        # grid shape is (b, h, w, 3)\n\n    return grid\n\n\ndef create_normalized_grid(\n    height: int,\n    width: int,\n    batch: Optional[int] = None,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create coordinate grid with height and width\n\n    NOTE: primarly used for equi2equi\n\n    params:\n    - height (int)\n    - width (int)\n    - batch (Optional[int])\n    - dtype (torch.dtype)\n\n    return:\n    - grid (torch.Tensor)\n\n    \"\"\"\n\n    # NOTE: RuntimeError: \"linspace_cpu\" not implemented for Half\n    if device.type == \"cpu\":\n        assert dtype in (torch.float32, torch.float64), (\n            f\"ERR: {dtype} is not supported by {device.type}\\n\"\n            \"If device is `cpu`, use float32 or float64\"\n        )\n\n    xs = torch.linspace(0, width - 1, width, dtype=dtype, device=device)\n    ys = torch.linspace(0, height - 1, height, dtype=dtype, device=device)\n    theta = xs * 2 * pi / width - pi\n    phi = ys * pi / height - pi / 2\n    phi, theta = torch.meshgrid([phi, theta], indexing=\"ij\")\n    a = torch.stack((theta, phi), dim=-1)\n    norm_A = 1\n    x = norm_A * torch.cos(a[..., 1]) * torch.cos(a[..., 0])\n    y = norm_A * torch.cos(a[..., 1]) * torch.sin(a[..., 0])\n    z = norm_A * torch.sin(a[..., 1])\n    grid = torch.stack((x, y, z), dim=-1)\n\n    # batched (stacked copies)\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        grid = torch.cat([grid.unsqueeze(0)] * batch)\n        # grid shape is (b, h, w, 3)\n\n    return grid\n\n\ndef create_xyz_grid(\n    w_face: int,\n    batch: Optional[int] = None,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"xyz coordinates of the faces of the cube\"\"\"\n\n    ratio = (w_face - 1) / w_face\n\n    out = torch.zeros((w_face, w_face * 6, 3), dtype=dtype, device=device)\n    rng = torch.linspace(\n        -0.5 * ratio, 0.5 * ratio, w_face, dtype=dtype, device=device\n    )\n\n    # NOTE: https://github.com/pytorch/pytorch/issues/15301\n    # Torch meshgrid behaves differently than numpy\n\n    # Front face (x = 0.5)\n    out[:, 0 * w_face : 1 * w_face, [2, 1]] = torch.stack(\n        torch.meshgrid([-rng, rng], indexing=\"ij\"), -1\n    )\n    out[:, 0 * w_face : 1 * w_face, 0] = 0.5\n\n    # Right face (y = -0.5)\n    out[:, 1 * w_face : 2 * w_face, [2, 0]] = torch.stack(\n        torch.meshgrid([-rng, -rng], indexing=\"ij\"), -1\n    )\n    out[:, 1 * w_face : 2 * w_face, 1] = 0.5\n\n    # Back face (x = -0.5)\n    out[:, 2 * w_face : 3 * w_face, [2, 1]] = torch.stack(\n        torch.meshgrid([-rng, -rng], indexing=\"ij\"), -1\n    )\n    out[:, 2 * w_face : 3 * w_face, 0] = -0.5\n\n    # Left face (y = 0.5)\n    out[:, 3 * w_face : 4 * w_face, [2, 0]] = torch.stack(\n        torch.meshgrid([-rng, rng], indexing=\"ij\"), -1\n    )\n    out[:, 3 * w_face : 4 * w_face, 1] = -0.5\n\n    # Up face (z = 0.5)\n    out[:, 4 * w_face : 5 * w_face, [0, 1]] = torch.stack(\n        torch.meshgrid([rng, rng], indexing=\"ij\"), -1\n    )\n    out[:, 4 * w_face : 5 * w_face, 2] = 0.5\n\n    # Down face (z = -0.5)\n    out[:, 5 * w_face : 6 * w_face, [0, 1]] = torch.stack(\n        torch.meshgrid([-rng, rng], indexing=\"ij\"), -1\n    )\n    out[:, 5 * w_face : 6 * w_face, 2] = -0.5\n\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        out = torch.cat([out.unsqueeze(0)] * batch)\n        # grid shape is (b, h, w, 3)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/torch_utils/func.py", "content": "#!/usr/bin/env python3\n\nimport torch\n\n\ndef sizeof(tensor: torch.Tensor) -> float:\n    \"\"\"Get the size of a tensor\"\"\"\n    assert torch.is_tensor(tensor), \"ERR: is not tensor\"\n    return tensor.element_size() * tensor.nelement()\n\n\ndef get_device(a: torch.Tensor) -> torch.device:\n    \"\"\"Get device of a Tensor\"\"\"\n    return torch.device(a.get_device() if a.get_device() >= 0 else \"cpu\")\n"}
{"type": "source_file", "path": "modules/equilib/torch_utils/intrinsic.py", "content": "#!/usr/bin/env python3\n\nimport torch\n\npi = torch.Tensor([3.14159265358979323846])\n\n\ndef deg2rad(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"Function that converts angles from degrees to radians\"\"\"\n    return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.0\n\n\ndef create_intrinsic_matrix(\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create intrinsic matrix\n\n    params:\n    - height, width (int)\n    - fov_x (float): make sure it's in degrees\n    - skew (float): 0.0\n    - dtype (torch.dtype): torch.float32\n    - device (torch.device): torch.device(\"cpu\")\n\n    returns:\n    - K (torch.tensor): 3x3 intrinsic matrix\n    \"\"\"\n    f = width / (2 * torch.tan(deg2rad(torch.tensor(fov_x, dtype=dtype)) / 2))\n    K = torch.tensor(\n        [[f, skew, width / 2], [0.0, f, height / 2], [0.0, 0.0, 1.0]],\n        dtype=dtype,\n        device=device,\n    )\n    return K\n"}
{"type": "source_file", "path": "modules/geo_predictors/PanoFusionDistancePredictor.py", "content": "import os.path\nimport numpy as np\nimport torch\nimport trimesh\n\nfrom utils.camera_utils import *\nfrom modules.geo_predictors import PanoFusionInvPredictor, PanoFusionNormalPredictor, PanoGeoRefiner, PanoJointPredictor\n\nimport torchvision\nfrom PIL import Image\n\nclass PanoFusionDistance:\n    def __init__(self):\n        self.image_path = None\n        self.ref_distance_path = None\n        self.ref_normal_path = None\n        self.ref_geometry_path = None\n        self.image = None\n        self.gt_distance = None\n        self.ref_distance = None\n        self.ref_normal = None\n        self.pano_width, self.pano_height = 2048, 1024\n        self.data_dir = None\n        self.case_name = 'wp'\n\n    def get_ref_distance(self):\n        assert self.image is not None\n        assert self.ref_distance_path is not None\n        assert self.height > 0 and self.width > 0\n\n        ref_distance = None\n        if os.path.exists(self.ref_distance_path):\n            ref_distance = np.load(self.ref_distance_path)\n            ref_distance = torch.from_numpy(ref_distance.astype(np.float32)).cuda()\n        else:\n            distance_predictor = PanoFusionInvPredictor()\n            ref_distance, _ = distance_predictor(self.image,\n                                                 torch.zeros([self.height, self.width]),\n                                                 torch.ones([self.height, self.width]))\n        return ref_distance\n\n    def get_ref_normal(self):\n\n        normal_predictor = PanoFusionNormalPredictor()\n        ref_normal = normal_predictor.inpaint_normal(self.image,\n                                                        torch.ones([self.height, self.width, 3]) / np.sqrt(3.),\n                                                        torch.ones([self.height, self.width]))\n\n        return ref_normal\n\n    def refine_geometry(self, distance_map, normal_map):\n        refiner = PanoGeoRefiner()\n        return refiner.refine(distance_map, normal_map)\n\n    def get_joint_distance_normal(self, init_distance=None, init_mask=None):\n\n        joint_predictor = PanoJointPredictor()\n        idx = 0\n        ref_distance, ref_normal = joint_predictor(idx, self.image,\n                                                    torch.ones([self.pano_height, self.pano_width, 1]),\n                                                    torch.ones([self.pano_height, self.pano_width]))\n\n        return ref_distance, ref_normal\n\n    def normalization(self):\n        scale = self.ref_distance.max().item() * 1.05\n        self.ref_distance /= scale\n\n    def save_ref_geometry(self):\n        if self.ref_distance_path is not None:\n            np.save(self.ref_distance_path, self.ref_distance.cpu().numpy())\n        if self.ref_normal_path is not None:\n            np.save(self.ref_normal_path, self.ref_normal.cpu().numpy())\n\n        # Save point cloud\n        pano_dirs = img_coord_to_pano_direction(img_coord_from_hw(self.height, self.width))\n        pts = pano_dirs * self.ref_distance.squeeze()[..., None]\n        pts = pts.cpu().numpy().reshape(-1, 3)\n        if self.image is not None:\n            pcd = trimesh.PointCloud(pts, vertex_colors=self.image.reshape(-1, 3).cpu().numpy())\n        else:\n            pcd = trimesh.PointCloud(pts)\n\n        assert self.ref_geometry_path is not None and self.ref_geometry_path[-4:] == '.ply'\n        pcd.export(self.ref_geometry_path)\n\n    @torch.no_grad()\n    def ref_point_cloud(self):\n        pano_dirs = img_coord_to_pano_direction(img_coord_from_hw(self.height, self.width))\n        pts = pano_dirs * self.ref_distance.squeeze()[..., None]\n        return pts\n\n\nclass PanoFusionDistancePredictor(PanoFusionDistance):\n    def __init__(self):\n        super().__init__()\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\n    def predict(self, pano_tensor, init_distance=None, init_mask=None, pano_width=2048, pano_height=1024):\n        self.pano_width, self.pano_height = pano_width, pano_height\n        self.image = pano_tensor.cuda()\n\n        self.ref_distance, self.ref_normal = self.get_joint_distance_normal(init_distance, init_mask)\n        \n        return self.ref_distance.squeeze(-1)\n"}
{"type": "source_file", "path": "modules/equilib/torch_utils/rotation.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List\n\nimport numpy as np\n\nimport torch\n\n\ndef create_global2camera_rotation_matrix(\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Rotation from global (world) to camera coordinates\"\"\"\n    R_XY = torch.tensor(\n        [[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],  # X <-> Y\n        dtype=dtype,\n    )\n    R_YZ = torch.tensor(\n        [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]],  # Y <-> Z\n        dtype=dtype,\n    )\n    R = R_XY @ R_YZ\n    return R.to(device)\n\n\ndef create_rotation_matrix(\n    roll: float,\n    pitch: float,\n    yaw: float,\n    z_down: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create Rotation Matrix\n\n    params:\n    - roll, pitch, yaw (float): in radians\n    - z_down (bool): flips pitch and yaw directions\n    - dtype (torch.dtype): data types\n\n    returns:\n    - R (torch.Tensor): 3x3 rotation matrix\n    \"\"\"\n\n    # calculate rotation about the x-axis\n    R_x = torch.tensor(\n        [\n            [1.0, 0.0, 0.0],\n            [0.0, np.cos(roll), -np.sin(roll)],\n            [0.0, np.sin(roll), np.cos(roll)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the y-axis\n    if not z_down:\n        pitch = -pitch\n    R_y = torch.tensor(\n        [\n            [np.cos(pitch), 0.0, np.sin(pitch)],\n            [0.0, 1.0, 0.0],\n            [-np.sin(pitch), 0.0, np.cos(pitch)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the z-axis\n    if not z_down:\n        yaw = -yaw\n    R_z = torch.tensor(\n        [\n            [np.cos(yaw), -np.sin(yaw), 0.0],\n            [np.sin(yaw), np.cos(yaw), 0.0],\n            [0.0, 0.0, 1.0],\n        ],\n        dtype=dtype,\n    )\n    R = R_z @ R_y @ R_x\n    return R.to(device)\n\n\ndef create_rotation_matrix_at_once(\n    roll: float,\n    pitch: float,\n    yaw: float,\n    z_down: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create rotation matrix at once\"\n\n    params:\n    - roll, pitch, yaw (float): in radians\n    - z_down (bool): flips pitch and yaw directions\n    - dtype (torch.dtype): data types\n    - device (torch.device): torch.device(\"cpu\")\n\n    returns:\n    - R (torch.Tensor): 3x3 rotation matrix\n\n    NOTE: same results as `create_rotation_matrix` but a little bit faster\n    \"\"\"\n\n    if not z_down:\n        pitch = -pitch\n        yaw = -yaw\n\n    return torch.tensor(\n        [\n            [\n                np.cos(yaw) * np.cos(pitch),\n                np.cos(yaw) * np.sin(pitch) * np.sin(roll)\n                - np.sin(yaw) * np.cos(roll),\n                np.cos(yaw) * np.sin(pitch) * np.cos(roll)\n                + np.sin(yaw) * np.sin(roll),\n            ],\n            [\n                np.sin(yaw) * np.cos(pitch),\n                np.sin(yaw) * np.sin(yaw) * np.sin(pitch) * np.sin(roll)\n                + np.cos(yaw) * np.cos(roll),\n                np.sin(yaw) * np.sin(pitch) * np.cos(roll)\n                - np.cos(yaw) * np.sin(roll),\n            ],\n            [\n                -np.sin(pitch),\n                np.cos(pitch) * np.sin(roll),\n                np.cos(pitch) * np.cos(roll),\n            ],\n        ],\n        dtype=dtype,\n        device=device,\n    )\n\n\ndef create_rotation_matrices(\n    rots: List[Dict[str, float]],\n    z_down: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create rotation matrices from batch of rotations\n\n    This methods creates a bx3x3 np.ndarray where `b` referes to the number\n    of rotations (rots) given in the input\n    \"\"\"\n\n    R = torch.empty((len(rots), 3, 3), dtype=dtype, device=device)\n    for i, rot in enumerate(rots):\n        # FIXME: maybe default to `create_rotation_matrix_at_once`?\n        # NOTE: at_once is faster with cpu, while slower on GPU\n        R[i, ...] = create_rotation_matrix(\n            **rot, z_down=z_down, dtype=dtype, device=device\n        )\n\n    return R\n\n\ndef create_rotation_matrix_dep(\n    x: float,\n    y: float,\n    z: float,\n    z_down: bool = True,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    \"\"\"Create Rotation Matrix\n\n    NOTE: DEPRECATED\n    \"\"\"\n    # calculate rotation about the x-axis\n    R_x = torch.tensor(\n        [\n            [1.0, 0.0, 0.0],\n            [0.0, np.cos(x), -np.sin(x)],\n            [0.0, np.sin(x), np.cos(x)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the y-axis\n    if not z_down:\n        y = -y\n    R_y = torch.tensor(\n        [\n            [np.cos(y), 0.0, -np.sin(y)],\n            [0.0, 1.0, 0.0],\n            [np.sin(y), 0.0, np.cos(y)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the z-axis\n    if not z_down:\n        z = -z\n    R_z = torch.tensor(\n        [\n            [np.cos(z), np.sin(z), 0.0],\n            [-np.sin(z), np.cos(z), 0.0],\n            [0.0, 0.0, 1.0],\n        ],\n        dtype=dtype,\n    )\n    R = R_z @ R_y @ R_x\n    return R.to(device)\n"}
{"type": "source_file", "path": "modules/geo_predictors/networks.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef get_activation(activation):\n    if activation == 'identity':\n        return lambda x: x\n    elif activation == 'relu':\n        return lambda x: F.relu(x)\n    else:\n        raise NotImplementedError\n\n\nclass VanillaMLP(nn.Module):\n    def __init__(self,\n                 dim_in,\n                 dim_out,\n                 n_neurons,\n                 n_hidden_layers,\n                 sphere_init=False,\n                 weight_norm=False,\n                 sphere_init_radius=0.5,\n                 output_activation='identity'):\n        super().__init__()\n        self.n_neurons, self.n_hidden_layers = n_neurons, n_hidden_layers\n        self.sphere_init, self.weight_norm = sphere_init, weight_norm\n        self.sphere_init_radius = sphere_init_radius\n        self.layers = [self.make_linear(dim_in, self.n_neurons, is_first=True, is_last=False), self.make_activation()]\n        for i in range(self.n_hidden_layers - 1):\n            self.layers += [self.make_linear(self.n_neurons, self.n_neurons, is_first=False, is_last=False),\n                            self.make_activation()]\n        self.layers += [self.make_linear(self.n_neurons, dim_out, is_first=False, is_last=True)]\n        self.layers = nn.Sequential(*self.layers)\n\n    def forward(self, x):\n        x = self.layers(x.float())\n        return -x\n\n    def make_linear(self, dim_in, dim_out, is_first, is_last):\n        layer = nn.Linear(dim_in, dim_out, bias=True)  # network without bias will degrade quality\n        if self.sphere_init:\n            if is_last:\n                torch.nn.init.constant_(layer.bias, -self.sphere_init_radius)\n                torch.nn.init.normal_(layer.weight, mean=np.sqrt(np.pi) / np.sqrt(dim_in), std=0.0001)\n            elif is_first:\n                torch.nn.init.constant_(layer.bias, 0.0)\n                torch.nn.init.constant_(layer.weight[:, 3:], 0.0)\n                torch.nn.init.normal_(layer.weight[:, :3], 0.0, np.sqrt(2) / np.sqrt(dim_out))\n            else:\n                torch.nn.init.constant_(layer.bias, 0.0)\n                torch.nn.init.normal_(layer.weight, 0.0, np.sqrt(2) / np.sqrt(dim_out))\n        else:\n            torch.nn.init.constant_(layer.bias, 0.0)\n            torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n\n        if self.weight_norm:\n            layer = nn.utils.weight_norm(layer)\n        return layer\n\n    def make_activation(self):\n        if self.sphere_init:\n            return nn.Softplus(beta=100)\n        else:\n            return nn.ReLU(inplace=True)\n"}
{"type": "source_file", "path": "modules/geo_predictors/geo_predictor.py", "content": "\nclass GeoPredictor:\n    def __init__(self):\n        pass\n\n    def inpaint_distance(self, img, ref_distance, mask):\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "modules/geo_predictors/__init__.py", "content": "from modules.geo_predictors.omnidata.omnidata_predictor import OmnidataPredictor\nfrom .pano_fusion_inv_predictor import PanoFusionInvPredictor\nfrom .pano_fusion_normal_predictor import PanoFusionNormalPredictor\nfrom .pano_geo_refiner import PanoGeoRefiner\nfrom .pano_joint_predictor import PanoJointPredictor\n"}
{"type": "source_file", "path": "modules/geo_predictors/omnidata/modules/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/geo_predictors/omnidata/modules/midas/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/geo_predictors/omnidata/modules/channel_attention.py", "content": "import torch\nfrom torch import nn\n\n\nclass ECALayer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n    \"\"\"\n    def __init__(self, channel, k_size=3):\n        super(ECALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: input features with shape [b, c, h, w]\n        b, c, h, w = x.size()\n\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        y = self.sigmoid(y)\n\n        return x * y.expand_as(x)\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, num_features, reduction):\n        super(ChannelAttention, self).__init__()\n        self.module = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(num_features, num_features // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_features // reduction, num_features, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return x * self.module(x)\n\n\nclass RCAB(nn.Module):\n    def __init__(self, num_features, reduction):\n        super(RCAB, self).__init__()\n        self.module = nn.Sequential(\n            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n            ChannelAttention(num_features, reduction)\n        )\n\n    def forward(self, x):\n        return x + self.module(x)\n\n\nclass RG(nn.Module):\n    def __init__(self, num_features, num_rcab, reduction):\n        super(RG, self).__init__()\n        self.module = [RCAB(num_features, reduction) for _ in range(num_rcab)]\n        self.module.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1))\n        self.module = nn.Sequential(*self.module)\n\n    def forward(self, x):\n        return x + self.module(x)\n\n\nclass RCAN(nn.Module):\n    def __init__(self, scale, num_features, num_rg, num_rcab, reduction):\n        super(RCAN, self).__init__()\n        self.sf = nn.Conv2d(3, num_features, kernel_size=3, padding=1)\n        self.rgs = nn.Sequential(*[RG(num_features, num_rcab, reduction) for _ in range(num_rg)])\n        self.conv1 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n        self.upscale = nn.Sequential(\n            nn.Conv2d(num_features, num_features * (scale ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale)\n        )\n        self.conv2 = nn.Conv2d(num_features, 3, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.sf(x)\n        residual = x\n        x = self.rgs(x)\n        x = self.conv1(x)\n        x += residual\n        x = self.upscale(x)\n        x = self.conv2(x)\n        return x\n\n\nclass CBAMChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(CBAMChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass CBAMSpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(CBAMSpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n"}
{"type": "source_file", "path": "modules/equilib/equi2cube/base.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Union\n\nimport numpy as np\n\nimport torch\n\nfrom .numpy import run as run_numpy\nfrom .torch import run as run_torch\n\n__all__ = [\"Equi2Cube\", \"equi2cube\"]\n\nArrayLike = Union[np.ndarray, torch.Tensor]\nRot = Union[Dict[str, float], List[Dict[str, float]]]\nCubeMaps = Union[\n    # single/batch 'horizon' or 'dice'\n    np.ndarray,\n    torch.Tensor,\n    # single 'list'\n    List[np.ndarray],\n    List[torch.Tensor],\n    # batch 'list'\n    List[List[np.ndarray]],\n    List[List[torch.Tensor]],\n    # single 'dict'\n    Dict[str, np.ndarray],\n    Dict[str, np.ndarray],\n    # batch 'dict'\n    List[Dict[str, np.ndarray]],\n    List[Dict[str, np.ndarray]],\n]\n\n\nclass Equi2Cube(object):\n    \"\"\"\n    params:\n    - w_face (int): cube face width\n    - cube_format (str): (\"dice\", \"horizon\", \"dict\", \"list\")\n    - mode (str)\n    - z_down (bool)\n\n    inputs:\n    - equi (np.ndarray, torch.Tensor)\n    - rots (dict, list[dict]): {\"roll\", \"pitch\", \"yaw\"}\n\n    returns:\n    - cube (np.ndarray, torch.Tensor, list, dict)\n    \"\"\"\n\n    def __init__(\n        self,\n        w_face: int,\n        cube_format: str,\n        z_down: bool = False,\n        mode: str = \"bilinear\",\n    ) -> None:\n        self.w_face = w_face\n        self.cube_format = cube_format\n        self.z_down = z_down\n        self.mode = mode\n\n    def __call__(self, equi: ArrayLike, rots: Rot) -> CubeMaps:\n        return equi2cube(\n            equi=equi,\n            rots=rots,\n            w_face=self.w_face,\n            cube_format=self.cube_format,\n            z_down=self.z_down,\n            mode=self.mode,\n        )\n\n\ndef equi2cube(\n    equi: ArrayLike,\n    rots: Rot,\n    w_face: int,\n    cube_format: str,\n    z_down: bool = False,\n    mode: str = \"bilinear\",\n    **kwargs,\n) -> CubeMaps:\n    \"\"\"\n    params:\n    - equi (np.ndarray, torch.Tensor)\n    - rot (dict, list[dict]): {\"roll\", \"pitch\", \"yaw\"}\n    - w_face (int): cube face width\n    - cube_format (str): (\"dice\", \"horizon\", \"dict\", \"list\")\n    - z_down (bool)\n    - mode (str)\n\n    returns:\n    - cube (np.ndarray, torch.Tensor, dict, list)\n\n    \"\"\"\n\n    _type = None\n    if isinstance(equi, np.ndarray):\n        _type = \"numpy\"\n    elif torch.is_tensor(equi):\n        _type = \"torch\"\n    else:\n        raise ValueError\n\n    is_single = False\n    if len(equi.shape) == 3 and isinstance(rots, dict):\n        # probably the input was a single image\n        equi = equi[None, ...]\n        rots = [rots]\n        is_single = True\n    elif len(equi.shape) == 3:\n        # probably a grayscale image\n        equi = equi[:, None, ...]\n\n    assert isinstance(rots, list), \"ERR: rots is not a list\"\n    if _type == \"numpy\":\n        out = run_numpy(\n            equi=equi,\n            rots=rots,\n            w_face=w_face,\n            cube_format=cube_format,\n            z_down=z_down,\n            mode=mode,\n            **kwargs,\n        )\n    elif _type == \"torch\":\n        out = run_torch(\n            equi=equi,\n            rots=rots,\n            w_face=w_face,\n            cube_format=cube_format,\n            z_down=z_down,\n            mode=mode,\n            **kwargs,\n        )\n    else:\n        raise ValueError\n\n    # make sure that the output batch dim is removed if it's only a single cubemap\n    if is_single:\n        out = out[0]\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/cube2equi/torch.py", "content": "#!/usr/bin/env python3\n\nimport math\n\nfrom typing import Dict, List, Union\n\nimport torch\n\nfrom modules.equilib.grid_sample import torch_grid_sample\nfrom modules.equilib.torch_utils import get_device\n\n__all__ = [\"convert2horizon\", \"run\"]\n\n\ndef single_list2horizon(cube: List[torch.Tensor]) -> torch.Tensor:\n    _, _, w = cube[0].shape\n    assert len(cube) == 6\n    assert sum(face.shape[-1] == w for face in cube) == 6\n    return torch.cat(cube, dim=-1)\n\n\ndef dice2horizon(dices: torch.Tensor) -> torch.Tensor:\n    assert len(dices.shape) == 4\n    w = dices.shape[-2] // 3\n    assert dices.shape[-2] == w * 3 and dices.shape[-1] == w * 4\n\n    # create a (b, c, h, w) horizon array\n    device = get_device(dices)\n    horizons = torch.empty(\n        (*dices.shape[0:2], w, w * 6), dtype=dices.dtype, device=device\n    )\n\n    # Order: F R B L U D\n    sxy = [(1, 1), (2, 1), (3, 1), (0, 1), (1, 0), (1, 2)]\n    for i, (sx, sy) in enumerate(sxy):\n        horizons[..., i * w : (i + 1) * w] = dices[\n            ..., sy * w : (sy + 1) * w, sx * w : (sx + 1) * w\n        ]\n    return horizons\n\n\ndef dict2horizon(dicts: List[Dict[str, torch.Tensor]]) -> torch.Tensor:\n    face_key = (\"F\", \"R\", \"B\", \"L\", \"U\", \"D\")\n    c, _, w = dicts[0][face_key[0]].shape\n    dtype = dicts[0][face_key[0]].dtype\n    device = get_device(dicts[0][face_key[0]])\n    horizons = torch.empty(\n        (len(dicts), c, w, w * 6), dtype=dtype, device=device\n    )\n    for b, cube in enumerate(dicts):\n        horizons[b, ...] = single_list2horizon([cube[k] for k in face_key])\n    return horizons\n\n\ndef list2horizon(lists: List[List[torch.Tensor]]) -> torch.Tensor:\n    assert len(lists[0][0].shape) == 3\n    c, w, _ = lists[0][0].shape\n    dtype = lists[0][0].dtype\n    device = get_device(lists[0][0])\n    horizons = torch.empty(\n        (len(lists), c, w, w * 6), dtype=dtype, device=device\n    )\n    for b, cube in enumerate(lists):\n        horizons[b, ...] = single_list2horizon(cube)\n    return horizons\n\n\ndef convert2horizon(\n    cubemap: Union[\n        torch.Tensor,\n        List[torch.Tensor],\n        List[List[torch.Tensor]],\n        Dict[str, torch.Tensor],\n        List[Dict[str, torch.Tensor]],\n    ],\n    cube_format: str,\n) -> torch.Tensor:\n    \"\"\"Converts supported cubemap formats to horizon\n\n    params:\n    - cubemap\n    - cube_format (str): ('horizon', 'dice', 'dict', 'list')\n\n    return:\n    - horizon (torch.Tensor)\n\n    \"\"\"\n\n    # FIXME: better typing for mypy...\n\n    if cube_format in (\"horizon\", \"dice\"):\n        assert isinstance(\n            cubemap, torch.Tensor\n        ), f\"ERR: cubemap {cube_format} needs to be a torch.Tensor\"\n        if len(cubemap.shape) == 2:\n            # single grayscale\n            # NOTE: this rarely happens since we assume grayscales are (1, h, w)\n            cubemap = cubemap[None, None, ...]\n        elif len(cubemap.shape) == 3:\n            # batched grayscale\n            # single rgb\n            # FIXME: how to tell apart batched grayscale and rgb?\n            # Assume that grayscale images are also 3 dim (from loading images)\n            cubemap = cubemap[None, ...]\n\n        if cube_format == \"dice\":\n            cubemap = dice2horizon(cubemap)\n    elif cube_format == \"list\":\n        assert isinstance(\n            cubemap, list\n        ), f\"ERR: cubemap {cube_format} needs to be a list\"\n        if isinstance(cubemap[0], torch.Tensor):\n            # single\n            cubemap = list2horizon([cubemap])  # type: ignore\n        else:\n            cubemap = list2horizon(cubemap)  # type: ignore\n    elif cube_format == \"dict\":\n        if isinstance(cubemap, dict):\n            cubemap = [cubemap]\n        assert isinstance(cubemap, list)\n        assert isinstance(\n            cubemap[0], dict\n        ), f\"ERR: cubemap {cube_format} needs to have dict inside the list\"\n        cubemap = dict2horizon(cubemap)  # type: ignore\n    else:\n        raise ValueError(f\"ERR: {cube_format} is not supported\")\n\n    assert (\n        len(cubemap.shape) == 4\n    ), f\"ERR: cubemap needs to be 4 dim, but got {cubemap.shape}\"\n\n    return cubemap\n\n\ndef _equirect_facetype(h: int, w: int) -> torch.Tensor:\n    \"\"\"0F 1R 2B 3L 4U 5D\"\"\"\n\n    int_dtype = torch.int64\n\n    w_ratio = (w - 1) / w\n    h_ratio = (h - 1) / h\n\n    tp = torch.roll(\n        torch.arange(4)  # 1\n        .repeat_interleave(w // 4)  # 2 same as np.repeat\n        .unsqueeze(0)\n        .transpose(0, 1)  # 3\n        .repeat(1, h)  # 4\n        .view(-1, h)  # 5\n        .transpose(0, 1),  # 6\n        shifts=3 * w // 8,\n        dims=1,\n    )\n\n    # Prepare ceil mask\n    mask = torch.zeros((h, w // 4), dtype=torch.bool)\n    idx = torch.linspace(-(math.pi * w_ratio), math.pi * w_ratio, w // 4) / 4\n    idx = h // 2 - torch.round(\n        torch.atan(torch.cos(idx)) * h / (math.pi * h_ratio)\n    )\n    idx = idx.type(int_dtype)\n    for i, j in enumerate(idx):\n        mask[:j, i] = 1\n    mask = torch.roll(torch.cat([mask] * 4, 1), 3 * w // 8, 1)\n\n    tp[mask] = 4\n    tp[torch.flip(mask, dims=(0,))] = 5\n\n    return tp.type(int_dtype)\n\n\ndef create_equi_grid(\n    h_out: int,\n    w_out: int,\n    w_face: int,\n    batch: int,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n    w_ratio = (w_out - 1) / w_out\n    h_ratio = (h_out - 1) / h_out\n    theta = torch.linspace(\n        -(math.pi * w_ratio),\n        math.pi * w_ratio,\n        steps=w_out,\n        dtype=dtype,\n        device=device,\n    )\n    phi = torch.linspace(\n        (math.pi * h_ratio) / 2,\n        -(math.pi * h_ratio) / 2,\n        steps=h_out,\n        dtype=dtype,\n        device=device,\n    )\n    phi, theta = torch.meshgrid([phi, theta], indexing=\"ij\")\n\n    # Get face id to each pixel: 0F 1R 2B 3L 4U 5D\n    tp = _equirect_facetype(h_out, w_out)\n\n    # xy coordinate map\n    coor_x = torch.zeros((h_out, w_out), dtype=dtype, device=device)\n    coor_y = torch.zeros((h_out, w_out), dtype=dtype, device=device)\n\n    # FIXME: there's a bug where left section (3L) has artifacts\n    # on top and bottom\n    # It might have to do with 4U or 5D\n    for i in range(6):\n        mask = tp == i\n\n        if i < 4:\n            coor_x[mask] = 0.5 * torch.tan(theta[mask] - math.pi * i / 2)\n            coor_y[mask] = (\n                -0.5\n                * torch.tan(phi[mask])\n                / torch.cos(theta[mask] - math.pi * i / 2)\n            )\n        elif i == 4:\n            c = 0.5 * torch.tan(math.pi / 2 - phi[mask])\n            coor_x[mask] = c * torch.sin(theta[mask])\n            coor_y[mask] = c * torch.cos(theta[mask])\n        elif i == 5:\n            c = 0.5 * torch.tan(math.pi / 2 - torch.abs(phi[mask]))\n            coor_x[mask] = c * torch.sin(theta[mask])\n            coor_y[mask] = -c * torch.cos(theta[mask])\n\n    # Final renormalize\n    coor_x = torch.clamp(\n        torch.clamp(coor_x + 0.5, 0, 1) * w_face, 0, w_face - 1\n    )\n    coor_y = torch.clamp(\n        torch.clamp(coor_y + 0.5, 0, 1) * w_face, 0, w_face - 1\n    )\n\n    # change x axis of the x coordinate map\n    for i in range(6):\n        mask = tp == i\n        coor_x[mask] = coor_x[mask] + w_face * i\n\n    # repeat batch\n    coor_x = coor_x.repeat(batch, 1, 1) - 0.5\n    coor_y = coor_y.repeat(batch, 1, 1) - 0.5\n\n    grid = torch.stack((coor_y, coor_x), dim=-3).to(device)\n    return grid\n\n\ndef run(\n    horizon: torch.Tensor,\n    height: int,\n    width: int,\n    mode: str,\n    backend: str = \"native\",\n) -> torch.Tensor:\n    \"\"\"Run Cube2Equi\n\n    params:\n    - horizon (torch.Tensor)\n    - height, widht (int): output equirectangular image's size\n    - mode (str)\n    - backend (str): backend of torch `grid_sample` (default: `native`)\n\n    return:\n    - equi (torch.Tensor)\n\n    NOTE: we assume that the input `horizon` is a 4 dim array\n\n    \"\"\"\n\n    assert (\n        len(horizon.shape) == 4\n    ), f\"ERR: `horizon` should be 4-dim (b, c, h, w), but got {horizon.shape}\"\n\n    horizon_dtype = horizon.dtype\n    assert horizon_dtype in (\n        torch.uint8,\n        torch.float16,\n        torch.float32,\n        torch.float64,\n    ), (\n        f\"ERR: input horizon has dtype of {horizon_dtype}which is\\n\"\n        f\"incompatible: try {(torch.uint8, torch.float16, torch.float32, torch.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as horizon\n    if horizon.device.type == \"cuda\":\n        dtype = torch.float32 if horizon_dtype == torch.uint8 else horizon_dtype\n        assert dtype in (torch.float16, torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float16, torch.float32, torch.float64)}\"\n        )\n    else:\n        # NOTE: for cpu, it can't use half-precision\n        dtype = torch.float32 if horizon_dtype == torch.uint8 else horizon_dtype\n        assert dtype in (torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float32, torch.float64)}\"\n        )\n    if backend == \"native\" and horizon_dtype == torch.uint8:\n        # FIXME: hacky way of dealing with images that are uint8 when using\n        # torch.grid_sample\n        horizon = horizon.type(torch.float32)\n\n    bs, c, w_face, _ = horizon.shape\n    horizon_device = get_device(horizon)\n\n    # initialize output tensor\n    if backend == \"native\":\n        # NOTE: don't need to initilaize for `native`\n        out = None\n    else:\n        out = torch.empty(\n            (bs, c, height, width), dtype=dtype, device=horizon_device\n        )\n\n    # FIXME: for now, calculate the grid in cpu\n    # I need to benchmark performance of it when grid is created on cuda\n    tmp_device = torch.device(\"cpu\")\n    if horizon.device.type == \"cuda\" and dtype == torch.float16:\n        tmp_dtype = torch.float32\n    else:\n        tmp_dtype = dtype\n\n    # create sampling grid\n    grid = create_equi_grid(\n        h_out=height,\n        w_out=width,\n        w_face=w_face,\n        batch=bs,\n        dtype=tmp_dtype,\n        device=tmp_device,\n    )\n\n    # FIXME: putting `grid` to device since `pure`'s bilinear interpolation requires it\n    # FIXME: better way of forcing `grid` to be the same dtype?\n    if horizon.dtype != grid.dtype:\n        grid = grid.type(horizon.dtype)\n    if horizon.device != grid.device:\n        grid = grid.to(horizon.device)\n\n    # grid sample\n    out = torch_grid_sample(\n        img=horizon, grid=grid, out=out, mode=mode, backend=backend\n    )\n\n    out = (\n        out.type(horizon_dtype)\n        # if horizon_dtype == torch.uint8\n        # else torch.clip(out, 0.0, 1.0)\n    )\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/cube2equi/numpy.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\n\n# from modules.equilib.grid_sample import numpy_grid_sample\nfrom modules.equilib.grid_sample.numpy.bilinear import interp2d\n\n__all__ = [\"convert2horizon\", \"run\"]\n\n\ndef single_list2horizon(cube: List[np.ndarray]) -> np.ndarray:\n    _, _, w = cube[0].shape\n    assert len(cube) == 6\n    assert sum(face.shape[-1] == w for face in cube) == 6\n    return np.concatenate(cube, axis=-1)\n\n\ndef dice2horizon(dices: np.ndarray) -> np.ndarray:\n    assert len(dices.shape) == 4\n    w = dices.shape[-2] // 3\n    assert dices.shape[-2] == w * 3 and dices.shape[-1] == w * 4\n\n    # create a (b, c, h, w) horizon array\n    horizons = np.empty((*dices.shape[0:2], w, w * 6), dtype=dices.dtype)\n\n    # Order: F R B L U D\n    sxy = [(1, 1), (2, 1), (3, 1), (0, 1), (1, 0), (1, 2)]\n    for i, (sx, sy) in enumerate(sxy):\n        horizons[..., i * w : (i + 1) * w] = dices[\n            ..., sy * w : (sy + 1) * w, sx * w : (sx + 1) * w\n        ]\n    return horizons\n\n\ndef dict2horizon(dicts: List[Dict[str, np.ndarray]]) -> np.ndarray:\n    face_key = (\"F\", \"R\", \"B\", \"L\", \"U\", \"D\")\n    c, _, w = dicts[0][face_key[0]].shape\n    dtype = dicts[0][face_key[0]].dtype\n    horizons = np.empty((len(dicts), c, w, w * 6), dtype=dtype)\n    for b, cube in enumerate(dicts):\n        horizons[b, ...] = single_list2horizon([cube[k] for k in face_key])\n    return horizons\n\n\ndef list2horizon(lists: List[List[np.ndarray]]) -> np.ndarray:\n    assert len(lists[0][0].shape) == 3\n    c, w, _ = lists[0][0].shape\n    dtype = lists[0][0].dtype\n    horizons = np.empty((len(lists), c, w, w * 6), dtype=dtype)\n    for b, cube in enumerate(lists):\n        horizons[b, ...] = single_list2horizon(cube)\n    return horizons\n\n\ndef convert2horizon(\n    cubemap: Union[\n        np.ndarray,\n        List[np.ndarray],\n        List[List[np.ndarray]],\n        Dict[str, np.ndarray],\n        List[Dict[str, np.ndarray]],\n    ],\n    cube_format: str,\n) -> np.ndarray:\n    \"\"\"Converts supported cubemap formats to horizon\n\n    params:\n    - cubemap\n    - cube_format (str): ('horizon', 'dice', 'dict', 'list')\n\n    return:\n    - horizon (np.ndarray)\n\n    \"\"\"\n\n    # FIXME: better typing for mypy...\n\n    if cube_format in (\"horizon\", \"dice\"):\n        assert isinstance(\n            cubemap, np.ndarray\n        ), f\"ERR: cubemap {cube_format} needs to be a np.ndarray\"\n        if len(cubemap.shape) == 2:\n            # single grayscale\n            # NOTE: this rarely happens since we assume grayscales are (1, h, w)\n            cubemap = cubemap[None, None, ...]\n        elif len(cubemap.shape) == 3:\n            # batched grayscale\n            # single rgb\n            # FIXME: how to tell apart batched grayscale and rgb?\n            # Assume that grayscale images are also 3 dim (from loading images)\n            cubemap = cubemap[None, ...]\n\n        if cube_format == \"dice\":\n            cubemap = dice2horizon(cubemap)\n    elif cube_format == \"list\":\n        assert isinstance(\n            cubemap, list\n        ), f\"ERR: cubemap {cube_format} needs to be a list\"\n        if isinstance(cubemap[0], np.ndarray):\n            # single\n            cubemap = [cubemap]\n        cubemap = list2horizon(cubemap)  # type: ignore\n    elif cube_format == \"dict\":\n        if isinstance(cubemap, dict):\n            cubemap = [cubemap]\n        assert isinstance(cubemap, list)\n        assert isinstance(\n            cubemap[0], dict\n        ), f\"ERR: cubemap {cube_format} needs to have dict inside the list\"\n        cubemap = dict2horizon(cubemap)  # type: ignore\n    else:\n        raise ValueError(f\"ERR: {cube_format} is not supported\")\n\n    assert (\n        len(cubemap.shape) == 4\n    ), f\"ERR: cubemap needs to be 4 dim, but got {cubemap.shape}\"\n\n    return cubemap\n\n\ndef _equirect_facetype(h: int, w: int) -> np.ndarray:\n    \"\"\"0F 1R 2B 3L 4U 5D\"\"\"\n\n    int_dtype = np.dtype(np.int64)\n\n    w_ratio = (w - 1) / w\n    h_ratio = (h - 1) / h\n\n    tp = np.roll(\n        np.arange(4).repeat(w // 4)[None, :].repeat(h, 0), 3 * w // 8, 1\n    )\n\n    # Prepare ceil mask\n    mask = np.zeros((h, w // 4), bool)\n    idx = np.linspace(-(np.pi * w_ratio), np.pi * w_ratio, w // 4) / 4\n    idx = h // 2 - np.around(np.arctan(np.cos(idx)) * h / (np.pi * h_ratio))\n    idx = idx.astype(int_dtype)\n    for i, j in enumerate(idx):\n        mask[:j, i] = 1\n    mask = np.roll(np.concatenate([mask] * 4, 1), 3 * w // 8, 1)\n\n    tp[mask] = 4\n    tp[np.flip(mask, 0)] = 5\n\n    return tp.astype(int_dtype)\n\n\ndef create_equi_grid(\n    h_out: int,\n    w_out: int,\n    w_face: int,\n    batch: int,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n\n    w_ratio = (w_out - 1) / w_out\n    h_ratio = (h_out - 1) / h_out\n    theta = np.linspace(\n        -(np.pi * w_ratio), np.pi * w_ratio, num=w_out, dtype=dtype\n    )\n    phi = np.linspace(\n        np.pi * h_ratio / 2, -(np.pi * h_ratio) / 2, num=h_out, dtype=dtype\n    )\n    theta, phi = np.meshgrid(theta, phi)\n\n    # Get face id to each pixel: 0F 1R 2B 3L 4U 5D\n    tp = _equirect_facetype(h_out, w_out)\n\n    # xy coordinate map\n    coor_x = np.zeros((h_out, w_out), dtype=dtype)\n    coor_y = np.zeros((h_out, w_out), dtype=dtype)\n\n    for i in range(6):\n        mask = tp == i\n\n        if i < 4:\n            coor_x[mask] = 0.5 * np.tan(theta[mask] - np.pi * i / 2)\n            coor_y[mask] = (\n                -0.5 * np.tan(phi[mask]) / np.cos(theta[mask] - np.pi * i / 2)\n            )\n        elif i == 4:\n            c = 0.5 * np.tan(np.pi / 2 - phi[mask])\n            coor_x[mask] = c * np.sin(theta[mask])\n            coor_y[mask] = c * np.cos(theta[mask])\n        elif i == 5:\n            c = 0.5 * np.tan(np.pi / 2 - np.abs(phi[mask]))\n            coor_x[mask] = c * np.sin(theta[mask])\n            coor_y[mask] = -c * np.cos(theta[mask])\n\n    # Final renormalize\n    # coor_x = np.clip(np.clip(coor_x + 0.5, 0, 1) * w_face, 0, w_face - 1)\n    # coor_y = np.clip(np.clip(coor_y + 0.5, 0, 1) * w_face, 0, w_face - 1)\n\n    coor_x = (np.clip(coor_x, -0.5, 0.5) + 0.5) * w_face\n    coor_y = (np.clip(coor_y, -0.5, 0.5) + 0.5) * w_face\n\n    # change x axis of the x coordinate map\n    for i in range(6):\n        mask = tp == i\n        coor_x[mask] = coor_x[mask] + w_face * i\n\n    grid = np.stack((coor_y, coor_x), axis=0) - 0.5\n    grid = np.concatenate([grid[np.newaxis, ...]] * batch)\n    return grid, tp\n\n\ndef numpy_grid_sample(\n    img: np.ndarray, grid: np.ndarray, out: np.ndarray, cube_face_id: np.ndarray\n):\n    b, _, h, w = img.shape\n\n    min_grid = np.floor(grid).astype(np.int64)\n    max_grid = min_grid + 1\n    d_grid = grid - min_grid\n\n    _, _, grid_h, grid_w = grid.shape\n    cube_face_min_grid = min_grid // h\n    cube_face_max_grid = max_grid // h\n\n    min_grid[:, 0, :, :] = np.clip(min_grid[:, 0, :, :], 0, None)\n    max_grid[:, 0, :, :] = np.clip(max_grid[:, 0, :, :], None, h - 1)\n\n    # FIXME: any way to do efficient batch?\n    for i in range(b):\n\n        for y in range(grid_h):\n            for x in range(grid_w):\n                if (\n                    cube_face_max_grid[i, 1, y, x]\n                    != cube_face_min_grid[i, 1, y, x]\n                ):\n                    if cube_face_max_grid[i, 1, y, x] != cube_face_id[y, x]:\n                        max_grid[i, 1, y, x] -= 1\n                    else:\n                        min_grid[i, 1, y, x] += 1\n\n        dy = d_grid[i, 0, ...]\n        dx = d_grid[i, 1, ...]\n        min_ys = min_grid[i, 0, ...]\n        min_xs = min_grid[i, 1, ...]\n        max_ys = max_grid[i, 0, ...]\n        max_xs = max_grid[i, 1, ...]\n\n        p00 = img[i][:, min_ys, min_xs]\n        p10 = img[i][:, max_ys, min_xs]\n        p01 = img[i][:, min_ys, max_xs]\n        p11 = img[i][:, max_ys, max_xs]\n\n        out[i, ...] = interp2d(p00, p10, p01, p11, dy, dx)\n\n    return out\n\n\ndef run(\n    horizon: np.ndarray,\n    height: int,\n    width: int,\n    mode: str,\n    override_func: Optional[Callable[[], Any]] = None,\n) -> np.ndarray:\n    \"\"\"Run Cube2Equi\n\n    params:\n    - horizon (np.ndarray)\n    - height, widht (int): output equirectangular image's size\n    - mode (str)\n\n    return:\n    - equi (np.ndarray)\n\n    NOTE: we assume that the input `horizon` is a 4 dim array\n\n    \"\"\"\n\n    assert (\n        len(horizon.shape) == 4\n    ), f\"ERR: `horizon` should be 4-dim (b, c, h, w), but got {horizon.shape}\"\n\n    horizon_dtype = horizon.dtype\n    assert horizon_dtype in (np.uint8, np.float32, np.float64), (\n        f\"ERR: input horizon has dtype of {horizon_dtype}\\n\"\n        f\"which is incompatible: try {(np.uint8, np.float32, np.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as horizon\n    dtype = (\n        np.dtype(np.float32)\n        if horizon_dtype == np.dtype(np.uint8)\n        else horizon_dtype\n    )\n    assert dtype in (np.float32, np.float64), (\n        f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n        f\"try {(np.float32, np.float64)}\"\n    )\n\n    bs, c, w_face, _ = horizon.shape\n\n    # initialize output equi\n    out = np.empty((bs, c, height, width), dtype=dtype)\n\n    # create sampling grid\n    grid, tp = create_equi_grid(\n        h_out=height, w_out=width, w_face=w_face, batch=bs, dtype=dtype\n    )\n\n    # grid sample\n    if override_func is not None:\n        out = override_func(  # type: ignore\n            img=horizon, grid=grid, out=out, mode=mode\n        )\n    else:\n        # out = numpy_grid_sample(img=horizon, grid=grid, out=out, mode=mode)\n        out = numpy_grid_sample(\n            img=horizon, grid=grid, out=out, cube_face_id=tp\n        )\n\n    out = (\n        out.astype(horizon_dtype)\n        if horizon_dtype == np.dtype(np.uint8)\n        else np.clip(out, 0.0, 1.0)\n    )\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/cube2equi/__init__.py", "content": "#!/usr/bin/env python3\n"}
{"type": "source_file", "path": "modules/equilib/equi2equi/__init__.py", "content": "#!/usr/bin/env python3\n"}
{"type": "source_file", "path": "modules/equilib/__init__.py", "content": "#!/usr/bin/env python3\n\nfrom modules.equilib.cube2equi.base import Cube2Equi, cube2equi\nfrom modules.equilib.equi2cube.base import Equi2Cube, equi2cube\nfrom modules.equilib.equi2equi.base import Equi2Equi, equi2equi\nfrom modules.equilib.equi2pers.base import Equi2Pers, equi2pers\n\n__all__ = [\n    \"Cube2Equi\",\n    \"Equi2Cube\",\n    \"Equi2Equi\",\n    \"Equi2Pers\",\n    \"cube2equi\",\n    \"equi2cube\",\n    \"equi2equi\",\n    \"equi2pers\",\n]\n"}
{"type": "source_file", "path": "modules/equilib/cube2equi/base.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Union\n\nimport numpy as np\n\nimport torch\n\nfrom .numpy import convert2horizon as convert2horizon_numpy, run as run_numpy\nfrom .torch import convert2horizon as convert2horizon_torch, run as run_torch\n\n__all__ = [\"Cube2Equi\", \"cube2equi\"]\n\nArrayLike = Union[np.ndarray, torch.Tensor]\nCubeMaps = Union[\n    # single/batch 'horizon' or 'dice'\n    np.ndarray,\n    torch.Tensor,\n    # single 'list'\n    List[np.ndarray],\n    List[torch.Tensor],\n    # batch 'list'\n    List[List[np.ndarray]],\n    List[List[torch.Tensor]],\n    # single 'dict'\n    Dict[str, np.ndarray],\n    Dict[str, np.ndarray],\n    # batch 'dict'\n    List[Dict[str, np.ndarray]],\n    List[Dict[str, np.ndarray]],\n]\n\n\nclass Cube2Equi(object):\n    \"\"\"\n    params:\n    - w_out, h_out (int): equirectangular image size\n    - cube_format (str): input cube format(\"dice\", \"horizon\", \"dict\", \"list\")\n    - mode (str): interpolation mode, defaults to \"bilinear\"\n\n    inputs:\n    - cubemap (np.ndarray, torch.Tensor, dict, list)\n\n    returns:\n    - equi (np.ndarray, torch.Tensor)\n    \"\"\"\n\n    def __init__(\n        self, height: int, width: int, cube_format: str, mode: str = \"bilinear\"\n    ) -> None:\n        self.height = height\n        self.width = width\n        self.cube_format = cube_format\n        self.mode = mode\n\n    def __call__(self, cubemap: CubeMaps, **kwargs) -> ArrayLike:\n        return cube2equi(\n            cubemap=cubemap,\n            cube_format=self.cube_format,\n            width=self.width,\n            height=self.height,\n            mode=self.mode,\n            **kwargs,\n        )\n\n\ndef cube2equi(\n    cubemap: CubeMaps,\n    cube_format: str,\n    height: int,\n    width: int,\n    mode: str = \"bilinear\",\n    **kwargs,\n) -> ArrayLike:\n    \"\"\"\n    params:\n    - cubemap\n    - cube_format (str): (\"dice\", \"horizon\", \"dict\", \"list\")\n    - height, width (int): output size\n    - mode (str): \"bilinear\"\n\n    return:\n    - equi (np.ndarray, torch.Tensor)\n    \"\"\"\n\n    assert width % 8 == 0 and height % 8 == 0\n\n    # Try and detect which type it is (\"numpy\" or \"torch\")\n    # FIXME: any cleaner way of detecting?\n    _type = None\n    if cube_format in (\"dice\", \"horizon\"):\n        if isinstance(cubemap, np.ndarray):\n            _type = \"numpy\"\n        elif torch.is_tensor(cubemap):\n            _type = \"torch\"\n    elif cube_format == \"dict\":\n        if isinstance(cubemap, dict):\n            if isinstance(cubemap[\"F\"], np.ndarray):\n                _type = \"numpy\"\n            elif isinstance(cubemap[\"F\"], torch.Tensor):\n                _type = \"torch\"\n        elif isinstance(cubemap, list):\n            assert isinstance(cubemap[0], dict)\n            if isinstance(cubemap[0][\"F\"], np.ndarray):\n                _type = \"numpy\"\n            elif isinstance(cubemap[0][\"F\"], torch.Tensor):\n                _type = \"torch\"\n    elif cube_format == \"list\":\n        assert isinstance(cubemap, list)\n        if isinstance(cubemap[0], list):\n            if isinstance(cubemap[0][0], np.ndarray):\n                _type = \"numpy\"\n            elif isinstance(cubemap[0][0], torch.Tensor):\n                _type = \"torch\"\n        else:\n            if isinstance(cubemap[0], np.ndarray):\n                _type = \"numpy\"\n            elif isinstance(cubemap[0], torch.Tensor):\n                _type = \"torch\"\n    assert _type is not None, \"ERR: input type is not numpy or torch\"\n\n    if _type == \"numpy\":\n        horizon = convert2horizon_numpy(\n            cubemap=cubemap, cube_format=cube_format\n        )\n        out = run_numpy(\n            horizon=horizon, height=height, width=width, mode=mode, **kwargs\n        )\n    elif _type == \"torch\":\n        horizon = convert2horizon_torch(\n            cubemap=cubemap, cube_format=cube_format\n        )\n\n        out = run_torch(\n            horizon=horizon, height=height, width=width, mode=mode, **kwargs\n        )\n    else:\n        raise ValueError(\"Oops something went wrong here\")\n\n    if out.shape[0] == 1:\n        out = out.squeeze(0)\n\n    return out\n"}
{"type": "source_file", "path": "gaussian_renderer/__init__.py", "content": "#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nimport torch\nimport math\nfrom depth_diff_gaussian_rasterization_min import GaussianRasterizationSettings, GaussianRasterizer\nfrom scene.gaussian_model import GaussianModel\nfrom utils.sh import eval_sh\n\ndef render(viewpoint_camera, pc: GaussianModel, opt, bg_color: torch.Tensor, scaling_modifier=1.0, override_color=None, render_only=False):\n    \"\"\"\n    Render the scene. \n    \n    Background tensor (bg_color) must be on GPU!\n    \"\"\"\n \n    # Create zero tensor. We will use it to make pytorch return gradients of the 2D (screen-space) means\n    screenspace_points = torch.zeros_like(pc.get_xyz, dtype=pc.get_xyz.dtype, requires_grad=True, device=\"cuda\") + 0\n    try:\n        screenspace_points.retain_grad()\n    except:\n        pass\n\n    # Set up rasterization configuration\n    tanfovx = math.tan(viewpoint_camera.FoVx * 0.5)\n    tanfovy = math.tan(viewpoint_camera.FoVy * 0.5)\n\n    raster_settings = GaussianRasterizationSettings(\n        image_height=int(viewpoint_camera.image_height),\n        image_width=int(viewpoint_camera.image_width),\n        tanfovx=tanfovx,\n        tanfovy=tanfovy,\n        bg=bg_color,\n        scale_modifier=scaling_modifier,\n        viewmatrix=viewpoint_camera.world_view_transform,\n        projmatrix=viewpoint_camera.full_proj_transform,\n        sh_degree=pc.active_sh_degree,\n        campos=viewpoint_camera.camera_center,\n        prefiltered=False,\n        debug=opt.debug\n    )\n\n    rasterizer = GaussianRasterizer(raster_settings=raster_settings)\n\n    means3D = pc.get_xyz\n    means2D = screenspace_points\n    opacity = pc.get_opacity\n\n    # If precomputed 3d covariance is provided, use it. If not, then it will be computed from\n    # scaling / rotation by the rasterizer.\n    scales = None\n    rotations = None\n    cov3D_precomp = None\n    if opt.compute_cov3D_python:\n        cov3D_precomp = pc.get_covariance(scaling_modifier)\n    else:\n        scales = pc.get_scaling\n        rotations = pc.get_rotation\n\n    # If precomputed colors are provided, use them. Otherwise, if it is desired to precompute colors\n    # from SHs in Python, do it. If not, then SH -> RGB conversion will be done by rasterizer.\n    shs = None\n    colors_precomp = None\n    if override_color is None:\n        if opt.convert_SHs_python:\n            shs_view = pc.get_features.transpose(1, 2).view(-1, 3, (pc.max_sh_degree+1)**2)\n            dir_pp = (pc.get_xyz - viewpoint_camera.camera_center.repeat(pc.get_features.shape[0], 1))\n            dir_pp_normalized = dir_pp/dir_pp.norm(dim=1, keepdim=True)\n            sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)\n            colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)\n        else:\n            shs = pc.get_features\n    else:\n        colors_precomp = override_color\n\n    # Rasterize visible Gaussians to image, obtain their radii (on screen). \n    rendered_image, radii, depth = rasterizer(\n        means3D = means3D,\n        means2D = means2D,\n        shs = shs,\n        colors_precomp = colors_precomp,\n        opacities = opacity,\n        scales = scales,\n        rotations = rotations,\n        cov3D_precomp = cov3D_precomp)\n\n    # Those Gaussians that were frustum culled or had a radius of 0 were not visible.\n    # They will be excluded from value updates used in the splitting criteria.\n    if render_only:\n        return {\"render\": rendered_image, \"depth\": depth}\n    else:\n        return {\"render\": rendered_image,\n                \"viewspace_points\": screenspace_points,\n                \"visibility_filter\" : radii > 0,\n                \"radii\": radii,\n                \"depth\": depth}\n"}
{"type": "source_file", "path": "modules/equilib/equi2pers/base.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Union\n\nimport numpy as np\n\nimport torch\n\nfrom .numpy import get_bounding_fov as get_bfov_numpy, run as run_numpy\nfrom .torch import get_bounding_fov as get_bfov_torch, run as run_torch\n\n__all__ = [\"Equi2Pers\", \"equi2pers\"]\n\nArrayLike = Union[np.ndarray, torch.Tensor]\nRot = Union[Dict[str, float], List[Dict[str, float]]]\n\n\nclass Equi2Pers(object):\n    \"\"\"\n    params:\n    - height, width (int): perspective size\n    - fov_x (float): perspective image fov of x-axis\n    - skew (float): skew intrinsic parameter\n    - sampling_method (str)\n    - z_down (bool)\n    - mode (str)\n\n    inputs:\n    - equi (np.ndarray, torch.Tensor)\n    - rot (dict, list): Dict[str, float]\n\n    returns:\n    - pers (np.ndarray, torch.Tensor)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        height: int,\n        width: int,\n        fov_x: float,\n        skew: float = 0.0,\n        z_down: bool = False,\n        mode: str = \"bilinear\",\n    ) -> None:\n        self.height = height\n        self.width = width\n        self.fov_x = fov_x\n        self.skew = skew\n        self.mode = mode\n        self.z_down = z_down\n        # FIXME: maybe do useful stuff like precalculating the grid or something\n\n    def __call__(self, equi: ArrayLike, rots: Rot, **kwargs) -> ArrayLike:\n        # FIXME: should optimize since some parts of the code can be calculated\n        # before hand.\n        # 1. calculate grid\n        # 2. grid sample\n        return equi2pers(\n            equi=equi,\n            rots=rots,\n            height=self.height,\n            width=self.width,\n            fov_x=self.fov_x,\n            skew=self.skew,\n            z_down=self.z_down,\n            mode=self.mode,\n            **kwargs,\n        )\n\n    def get_bounding_fov(self, equi: ArrayLike, rots: Rot) -> ArrayLike:\n        return get_bounding_fov(\n            equi=equi,\n            rots=rots,\n            height=self.height,\n            width=self.width,\n            fov_x=self.fov_x,\n            skew=self.skew,\n            z_down=self.z_down,\n        )\n\n\ndef equi2pers(\n    equi: ArrayLike,\n    rots: Rot,\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float = 0.0,\n    mode: str = \"bilinear\",\n    z_down: bool = False,\n    **kwargs,\n) -> ArrayLike:\n    \"\"\"\n    params:\n    - equi\n    - rots\n    - height, width (int): perspective size\n    - fov_x (float): perspective image fov of x-axis\n    - z_down (bool)\n    - skew (float): skew intrinsic parameter\n\n    returns:\n    - pers (np.ndarray, torch.Tensor)\n\n    \"\"\"\n\n    _type = None\n    if isinstance(equi, np.ndarray):\n        _type = \"numpy\"\n    elif torch.is_tensor(equi):\n        _type = \"torch\"\n    else:\n        raise ValueError\n\n    is_single = False\n    if len(equi.shape) == 3 and isinstance(rots, dict):\n        # probably the input was a single image\n        equi = equi[None, ...]\n        rots = [rots]\n        is_single = True\n    elif len(equi.shape) == 3:\n        # probably a grayscale image\n        equi = equi[:, None, ...]\n\n    assert isinstance(rots, list), \"ERR: rots is not a list\"\n    if _type == \"numpy\":\n        out = run_numpy(\n            equi=equi,\n            rots=rots,\n            height=height,\n            width=width,\n            fov_x=fov_x,\n            skew=skew,\n            z_down=z_down,\n            mode=mode,\n            **kwargs,\n        )\n    elif _type == \"torch\":\n        out = run_torch(\n            equi=equi,\n            rots=rots,\n            height=height,\n            width=width,\n            fov_x=fov_x,\n            skew=skew,\n            z_down=z_down,\n            mode=mode,\n            **kwargs,\n        )\n    else:\n        raise ValueError\n\n    # make sure that the output batch dim is removed if it's only a single image\n    if is_single:\n        out = out.squeeze(0)\n\n    return out\n\n\ndef get_bounding_fov(\n    equi: ArrayLike,\n    rots: Rot,\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float = 0.0,\n    z_down: bool = False,\n) -> np.ndarray:\n    _type = None\n    if isinstance(equi, np.ndarray):\n        _type = \"numpy\"\n    elif torch.is_tensor(equi):\n        _type = \"torch\"\n    else:\n        raise ValueError\n\n    is_single = False\n    if len(equi.shape) == 3 and isinstance(rots, dict):\n        # probably the input was a single image\n        equi = equi[None, ...]\n        rots = [rots]\n        is_single = True\n    elif len(equi.shape) == 3:\n        # probably a grayscale image\n        equi = equi[:, None, ...]\n\n    assert isinstance(rots, list), \"ERR: rots is not a list\"\n    if _type == \"numpy\":\n        out = get_bfov_numpy(\n            equi=equi,\n            rots=rots,\n            height=height,\n            width=width,\n            fov_x=fov_x,\n            skew=skew,\n            z_down=z_down,\n        )\n    elif _type == \"torch\":\n        out = get_bfov_torch(\n            equi=equi,\n            rots=rots,\n            height=height,\n            width=width,\n            fov_x=fov_x,\n            skew=skew,\n            z_down=z_down,\n        )\n    else:\n        raise ValueError\n\n    # make sure that the output batch dim is removed if it's only a single image\n    if is_single:\n        out = out.squeeze(0)\n\n    return out\n"}
{"type": "source_file", "path": "create_SDFT_pairs.py", "content": "import torch\nimport math\nimport os\nimport cv2\nfrom PIL import Image\nimport numpy as np\n\nfrom modules.mesh_fusion.util import get_pinhole_intrinsics_from_fov\nfrom modules.mesh_fusion.render import (\n    features_to_world_space_mesh,\n    render_mesh,\n)\nfrom utils.common_utils import (\n    visualize_depth_numpy,\n    save_rgbd,\n)\nimport torch.nn.functional as F\n\nimport utils.functions as functions\nimport time\nfrom modules.geo_predictors.PanoFusionDistancePredictor import PanoFusionDistancePredictor\nfrom utils.camera_utils import *\n\nfrom modules.equilib import equi2pers, cube2equi, equi2cube\nfrom utils.warp_utils import transformation_from_parameters\n\nclass PanoWarp(torch.nn.Module):\n    def __init__(self):\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\n        # renderer setting\n        self.blur_radius = 0\n        self.faces_per_pixel = 8\n        self.fov = 90\n        self.R, self.T = torch.Tensor([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]]), torch.Tensor([[0., 0., 0.]])\n        self.pano_width, self.pano_height = 1024 * 2, 512 * 2\n        self.H, self.W = 512, 512\n        self.device = \"cuda:0\"\n\n        self.models_path = 'checkpoints'\n        self.fix_structure = False\n\n        # initialize global point-cloud / mesh structures\n        self.rendered_depth = torch.zeros((self.H, self.W), device=self.device)  # depth rendered from point cloud\n        self.inpaint_mask = torch.ones((self.H, self.W), device=self.device, dtype=torch.bool)  # 1: no projected points (need to be inpainted) | 0: have projected points\n        self.vertices = torch.empty((3, 0), device=self.device, requires_grad=False)\n        self.colors = torch.empty((3, 0), device=self.device, requires_grad=False)\n        self.faces = torch.empty((3, 0), device=self.device, dtype=torch.long, requires_grad=False)\n        self.pix_to_face = None\n\n        # create exp dir\n        timestamp = str(int(time.time()))\n        self.setting = f\"SDFT_pseudo_pairs\"\n        self.save_path = f'output/{self.setting}'\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        self.world_to_cam = torch.eye(4, dtype=torch.float32, device=self.device)\n        self.K_44 = get_pinhole_intrinsics_from_fov(H=self.H, W=self.W, fov_in_degrees=self.fov).to(self.world_to_cam)\n        self.K_b33 = self.K_44[:3,:3].unsqueeze(0)\n\n    def empty_mesh(self):\n        # initialize global point-cloud / mesh structures\n        self.rendered_depth = torch.zeros((self.H, self.W), device=self.device)  # depth rendered from point cloud\n        self.inpaint_mask = torch.ones((self.H, self.W), device=self.device, dtype=torch.bool)  # 1: no projected points (need to be inpainted) | 0: have projected points\n        self.vertices = torch.empty((3, 0), device=self.device, requires_grad=False)\n        self.colors = torch.empty((3, 0), device=self.device, requires_grad=False)\n        self.faces = torch.empty((3, 0), device=self.device, dtype=torch.long, requires_grad=False)\n        self.pix_to_face = None\n\n    def project(self, world_to_cam):\n        # project mesh into pose and render (rgb, depth, mask)\n        rendered_image_tensor, self.rendered_depth, self.inpaint_mask, self.pix_to_face, self.z_buf, self.mesh = render_mesh(\n            vertices=self.vertices,\n            faces=self.faces,\n            vertex_features=self.colors,\n            H=self.H,\n            W=self.W,\n            fov_in_degrees=self.fov,\n            RT=world_to_cam,\n            blur_radius=self.blur_radius,\n            faces_per_pixel=self.faces_per_pixel\n        )\n        # mask rendered_image_tensor\n        rendered_image_tensor = rendered_image_tensor * ~self.inpaint_mask\n\n        # stable diffusion models want the mask and image as PIL images\n        rendered_image_pil = Image.fromarray((rendered_image_tensor.permute(1, 2, 0).detach().cpu().numpy()[..., :3] * 255).astype(np.uint8))\n        self.inpaint_mask_pil = Image.fromarray(self.inpaint_mask.detach().cpu().squeeze().float().numpy() * 255).convert(\"RGB\")\n\n        self.inpaint_mask_restore = self.inpaint_mask\n        self.inpaint_mask_pil_restore = self.inpaint_mask_pil\n\n        return rendered_image_tensor, rendered_image_pil\n\n    def rgbd_to_mesh(self, rgb, depth, world_to_cam=None, mask=None, pix_to_face=None, using_distance_map=False):\n        predicted_depth = depth.cuda()\n        rgb = rgb.squeeze(0).cuda()\n        if world_to_cam is None:\n            world_to_cam = torch.eye(4, dtype=torch.float32)\n        world_to_cam = world_to_cam.cuda()\n        if pix_to_face is not None:\n            self.pix_to_face = pix_to_face\n        if mask is None:\n            self.inpaint_mask = torch.ones_like(predicted_depth).bool()\n        else:\n            self.inpaint_mask = mask #[H,W]\n\n        if self.inpaint_mask.sum() == 0:\n            return\n        \n        vertices, faces, colors = features_to_world_space_mesh(\n            colors=rgb,\n            depth=predicted_depth,\n            fov_in_degrees=self.fov,\n            world_to_cam=world_to_cam,\n            mask=self.inpaint_mask,\n            pix_to_face=self.pix_to_face,\n            faces=self.faces,\n            vertices=self.vertices,\n            using_distance_map=using_distance_map,\n        )\n\n        faces += self.vertices.shape[1] \n\n        self.vertices_restore = self.vertices.clone()\n        self.colors_restore = self.colors.clone()\n        self.faces_restore = self.faces.clone()\n\n        self.vertices = torch.cat([self.vertices, vertices], dim=1)\n        self.colors = torch.cat([self.colors, colors], dim=1)\n        self.faces = torch.cat([self.faces, faces], dim=1)\n\n\n    def load_pano(self):\n        image_path = f\"input/input_panorama.png\"\n        image = Image.open(image_path)\n        if image.size[0] < image.size[1]:\n            image = image.transpose(Image.TRANSPOSE)\n        image = functions.resize_image_with_aspect_ratio(image, new_width=self.pano_width)\n\n        panorama_tensor = torch.tensor(np.array(image))[...,:3].permute(2,0,1).unsqueeze(0).float()/255\n\n        pano_fusion_distance_predictor = PanoFusionDistancePredictor()\n        depth = pano_fusion_distance_predictor.predict(panorama_tensor.squeeze(0).permute(1,2,0)) #input:HW3\n        print(f\"pano_fusion_distance...[{depth.min(), depth.mean(),depth.max()}]\")\n                \n        return panorama_tensor, depth# panorama_tensor:BCHW, depth:HW\n\n    def pano_to_perpective(self, pano_bchw, pitch, yaw):\n        rots = {\n            'roll': 0.,\n            'pitch': pitch,  # rotate vertical\n            'yaw': yaw,  # rotate horizontal\n        }\n\n        perspective = equi2pers(\n            equi=pano_bchw.squeeze(0),\n            rots=rots,\n            height=self.H,\n            width=self.W,\n            fov_x=self.fov,\n            mode=\"bilinear\",\n        ).unsqueeze(0) # BCHW\n\n        return perspective\n\n    def get_rand_ext(self, bs=1, range_scale=2):\n        def rand_tensor(r, l):\n            if r < 0:  \n                return torch.zeros((l, 1, 1))\n            rand = torch.rand((l, 1, 1))        \n            sign = 2 * (torch.randn_like(rand) > 0).float() - 1\n            return sign * (r / 2 + r / 2 * rand)\n\n        trans_range={\"x\":0.4*range_scale, \"y\":-0.4*range_scale, \"z\":-0.4*range_scale, \"a\":-0.4*range_scale, \"b\":-0.4*range_scale, \"c\":-0.4*range_scale}\n        x, y, z = trans_range['x'], trans_range['y'], trans_range['z']\n        a, b, c = trans_range['a'], trans_range['b'], trans_range['c']\n        cix = rand_tensor(x, bs)\n        ciy = rand_tensor(y, bs)\n        ciz = rand_tensor(z, bs)\n        aix = rand_tensor(math.pi / a, bs)\n        aiy = rand_tensor(math.pi / b, bs)\n        aiz = rand_tensor(math.pi / c, bs)\n        \n        axisangle = torch.cat([aix, aiy, aiz], dim=-1)  # [b,1,3]\n        translation = torch.cat([cix, ciy, ciz], dim=-1)\n        \n        cam_ext = transformation_from_parameters(axisangle, translation)  # [b,4,4]\n        cam_ext_inv = torch.inverse(cam_ext)  # [b,4,4]\n        return cam_ext, cam_ext_inv\n\n    def get_pairs(self, view_rgb, view_depth, pose_cnt=2):\n        all_poses = []\n        \n        for i in range(pose_cnt):\n            cam_ext, cam_ext_inv = self.get_rand_ext()  # [b,4,4]\n            cur_pose = cam_ext\n            all_poses += [cur_pose]\n\n        ref_depth = view_depth\n        ref_img = view_rgb\n        W, H = 512, 512\n\n        inpaint_pairs = []  #(warp_back_image, warp_back_disp, warp_back_mask, ref_img, ref_depth)\n        val_pairs = [] #(cam_ext, ref_img, warp_image, warp_disp, warp_mask, gt_img)\n\n        for i, cur_pose in enumerate(all_poses[:]):\n            print(\"-poses_idx:\",i)\n            cur_pose = all_poses[i]\n            c2w = cur_pose\n            cur_pose = torch.tensor(cur_pose.squeeze(0)).cuda()\n\n            cam_int = self.K_b33.repeat(1, 1, 1)  # [b,3,3]\n\n            #load cam_ext\n            cam_ext = c2w\n            cam_ext_inv = torch.inverse(cam_ext)\n            cam_ext = cam_ext.repeat(1, 1, 1)[:,:-1,:]\n            cam_ext_inv = cam_ext_inv.repeat(1, 1, 1)[:,:-1,:]\n\n            rgbd = torch.cat([ref_img, ref_depth], dim=1).cuda()\n            cam_int = cam_int.cuda()\n            cam_ext = cam_ext.cuda()\n            cam_ext_inv = cam_ext_inv.cuda()\n\n            # warp to a random novel view\n            self.rgbd_to_mesh(ref_img, ref_depth.squeeze(0).squeeze(0), self.world_to_cam)\n            warp_image, _ = self.project(cur_pose)\n            warp_disp = self.rendered_depth\n            warp_mask = ~self.inpaint_mask\n            self.empty_mesh()\n\n            # warp back to the original view\n            self.rgbd_to_mesh(warp_image[:3, ...].unsqueeze(0), warp_disp, cur_pose)\n            warp_back_image, _ = self.project(self.world_to_cam)\n            warp_back_disp = self.rendered_depth\n            warp_back_mask = ~self.inpaint_mask            \n            self.empty_mesh()\n\n            # filter occlusion: warp_back_depth should not be smaller that ref_depth\n            margin = 0.1\n            occlusion_mask = ((warp_back_disp * ~self.inpaint_mask + margin) < \n                              (ref_depth.squeeze(0).squeeze(0) * ~self.inpaint_mask))\n            warp_back_image *= ~occlusion_mask\n            warp_back_mask *= ~occlusion_mask\n\n            ref_depth_2 = ref_depth\n            # all depth should be in [0~1]\n            inpaint_pairs.append((ref_img, ref_depth_2, cur_pose,\n                                    warp_image, warp_disp, warp_mask,\n                                    warp_back_image, warp_back_disp, warp_back_mask))\n\n        return inpaint_pairs\n\n    def run(self):\n        # load pano and project to tangent views\n        panorama_tensor, init_depth = self.load_pano()\n        \n        cubemaps_pitch_yaw = [(0, 0), (0, 1/2 * np.pi), (0, 1 * np.pi), (0, 3/2 * np.pi),\\\n                                (1/2 * np.pi, 0), (-1/2 * np.pi, 0)]\n\n        pitch_yaw_list = cubemaps_pitch_yaw\n      \n        view_rgb_depth_pairs = []\n        for view_idx, (pitch, yaw) in enumerate(pitch_yaw_list):\n            view_rgb = self.pano_to_perpective(panorama_tensor, pitch, yaw)\n            view_depth = self.pano_to_perpective(init_depth.unsqueeze(0).unsqueeze(0), pitch, yaw)\n            view_rgb_depth_pairs += [(view_rgb, view_depth)]\n\n            view_rgb_pil = Image.fromarray((view_rgb.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()[..., :3] * 255).astype(np.uint8))\n            view_rgb_pil.save(f\"{self.save_path}/view_rgb_{view_idx}.png\")    \n\n            \n        # create pseudo <masked image, GT image> pairs using warp-back strategy\n        video_frames = []\n        for view_idx, (view_rgb, view_depth) in enumerate(view_rgb_depth_pairs):\n            print(f\"-view_idx:{view_idx}\")\n            pairs_per_view = 5\n            inpaint_pairs = self.get_pairs(view_rgb, view_depth, pairs_per_view)\n            for pair_idx, inpaint_pair in enumerate(inpaint_pairs):\n                (ref_img, ref_depth, cur_pose,\n                warp_rgb, warp_disp, warp_mask, \n                warp_back_image, warp_back_disp, warp_back_mask)  = inpaint_pair\n                warp_back_mask = ~warp_back_mask\n\n                ref_rgb_pil = Image.fromarray((ref_img.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()[..., :3] * 255).astype(np.uint8))\n                ref_depth_pil = Image.fromarray(visualize_depth_numpy(warp_disp.squeeze(0).squeeze(0).cpu().detach().numpy())[0].astype(np.uint8))\n\n                warp_rgb_pil = Image.fromarray((warp_rgb.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()[..., :3] * 255).astype(np.uint8))\n                warp_mask_pil = Image.fromarray(warp_mask.squeeze(0).squeeze(0).detach().cpu().squeeze().float().numpy() * 255).convert(\"RGB\")\n            \n                warp_back_rgb_pil = Image.fromarray((warp_back_image.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()[..., :3] * 255).astype(np.uint8))\n                warp_back_mask_pil = Image.fromarray(warp_back_mask.squeeze(0).squeeze(0).detach().cpu().squeeze().float().numpy() * 255).convert(\"RGB\")\n\n                ref_rgb_pil.save(f\"{self.save_path}/ref_rgb_{view_idx}_{pair_idx}.png\")\n                warp_back_rgb_pil.save(f\"{self.save_path}/warp_back_rgb_{view_idx}_{pair_idx}.png\")\n                warp_back_mask_pil.save(f\"{self.save_path}/warp_back_mask_{view_idx}_{pair_idx}.png\")\n\n\npipeline = PanoWarp()\npipeline.run()"}
{"type": "source_file", "path": "modules/equilib/equi2pers/__init__.py", "content": "#!/usr/bin/env python3\n"}
{"type": "source_file", "path": "modules/equilib/equi2equi/numpy.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport numpy as np\n\nfrom modules.equilib.grid_sample import numpy_grid_sample\nfrom modules.equilib.numpy_utils import create_normalized_grid, create_rotation_matrices\n\n\ndef matmul(m: np.ndarray, R: np.ndarray, method: str = \"faster\") -> np.ndarray:\n\n    if method == \"robust\":\n        # When target image size is smaller, it might be faster with `matmul`\n        # but not by much\n        M = np.matmul(R[:, np.newaxis, np.newaxis, ...], m)\n    elif method == \"faster\":\n        # `einsum` is probably fastest, but it might not be accurate\n        # I've tested it, and it's really close when it is float64,\n        # but loses precision for float32\n        # trade off between precision and speed i guess\n        # around x3 ~ x10 faster (faster when batch size is high)\n        batch_size = m.shape[0]\n        M = np.empty_like(m)\n        for b in range(batch_size):\n            M[b, ...] = np.einsum(\n                \"ik,...kj->...ij\", R[b, ...], m[b, ...], optimize=True\n            )\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    M = M.squeeze(-1)\n    return M\n\n\ndef convert_grid(\n    M: np.ndarray, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> np.ndarray:\n\n    # convert to rotation\n    phi = np.arcsin(M[..., 2] / np.linalg.norm(M, axis=-1))\n    theta = np.arctan2(M[..., 1], M[..., 0])\n\n    if method == \"robust\":\n        # convert to pixel\n        # I thought it would be faster if it was done all at once,\n        # but it was faster separately\n        ui = (theta - np.pi) * w_equi / (2 * np.pi)\n        uj = (phi - np.pi / 2) * h_equi / np.pi\n        ui += 0.5\n        uj += 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        # NOTE: this asserts that theta and phi are in range\n        # the range of theta is -pi ~ pi\n        # the range of phi is -pi/2 ~ pi/2\n        # this means that if the input `rots` have rotations that are\n        # out of range, it will not work with `faster`\n        ui = (theta - np.pi) * w_equi / (2 * np.pi)\n        uj = (phi - np.pi / 2) * h_equi / np.pi\n        ui += 0.5\n        uj += 0.5\n        ui = np.where(ui < 0, ui + w_equi, ui)\n        ui = np.where(ui >= w_equi, ui - w_equi, ui)\n        uj = np.where(uj < 0, uj + h_equi, uj)\n        uj = np.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = np.stack((uj, ui), axis=-3)\n\n    return grid\n\n\ndef run(\n    src: np.ndarray,\n    rots: List[Dict[str, float]],\n    z_down: bool,\n    mode: str,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    override_func: Optional[Callable[[], Any]] = None,\n) -> np.ndarray:\n    \"\"\"Run Equi2Equi\n\n    params:\n    - src (np.ndarray): 4 dims (b, c, h, w)\n    - rot (List[dict]): dict of ('yaw', 'pitch', 'roll')\n    - z_down (bool)\n    - mode (str): sampling mode for grid_sample\n    - height, width (Optional[int]): height and width of the target\n    - override_func (Callable): function for overriding `grid_sample`\n\n    return:\n    - out (np.ndarray)\n\n    NOTE: acceptable dtypes for `src` are currently uint8, float32, and float64.\n    Floats are prefered since numpy calculations are optimized for floats.\n\n    NOTE: output array has the same dtype as `src`\n\n    NOTE: you can override `equilib`'s grid_sample with over grid sampling methods\n    using `override_func`. The input to this function have to match `grid_sample`.\n\n    \"\"\"\n\n    assert (\n        len(src.shape) == 4\n    ), f\"ERR: input `src` should be 4-dim (b, c, h, w), but got {len(src.shape)}\"\n    assert len(src) == len(\n        rots\n    ), f\"ERR: batch size of `src` and `rot` differs: {len(src)} vs {len(rots)}\"\n\n    src_dtype = src.dtype\n    assert src_dtype in (np.uint8, np.float32, np.float64), (\n        f\"ERR: input equirectangular image has dtype of {src_dtype}\\n\"\n        f\"which is incompatible: try {(np.uint8, np.float32, np.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    dtype = (\n        np.dtype(np.float32) if src_dtype == np.dtype(np.uint8) else src_dtype\n    )\n    assert dtype in (np.float32, np.float64), (\n        f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n        f\"try {(np.float32, np.float64)}\"\n    )\n\n    bs, c, h_equi, w_equi = src.shape\n\n    assert type(height) == type(\n        width\n    ), \"ERR: `height` and `width` does not match types (maybe it was set separately?)\"\n    if height is None and width is None:\n        height = h_equi\n        width = w_equi\n    else:\n        assert isinstance(height, int) and isinstance(width, int)\n\n    # initialize output array\n    out = np.empty((bs, c, height, width), dtype=dtype)\n\n    # create grid and transfrom matrix\n    m = create_normalized_grid(\n        height=height, width=width, batch=bs, dtype=dtype\n    )\n    m = m[..., np.newaxis]\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(rots=rots, z_down=z_down, dtype=dtype)\n\n    # rotate the grid\n    M = matmul(m, R, method=\"faster\")\n\n    # create a pixel map grid\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # grid sample\n    if override_func is not None:\n        out = override_func(  # type: ignore\n            img=src, grid=grid, out=out, mode=mode\n        )\n    else:\n        out = numpy_grid_sample(img=src, grid=grid, out=out, mode=mode)\n\n    out = (\n        out.astype(src_dtype)\n        if src_dtype == np.dtype(np.uint8)\n        else np.clip(out, 0.0, 1.0)\n    )\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/equi2equi/torch.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Optional\n\nimport torch\n\nfrom modules.equilib.grid_sample import torch_grid_sample\nfrom modules.equilib.torch_utils import (\n    create_normalized_grid,\n    create_rotation_matrices,\n    get_device,\n    pi,\n)\n\n\ndef matmul(m: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n\n    M = torch.matmul(R[:, None, None, ...], m)\n    M = M.squeeze(-1)\n\n    return M\n\n\ndef convert_grid(\n    M: torch.Tensor, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> torch.Tensor:\n\n    # convert to rotation\n    phi = torch.asin(M[..., 2] / torch.norm(M, dim=-1))\n    theta = torch.atan2(M[..., 1], M[..., 0])\n\n    if method == \"robust\":\n        ui = (theta - pi) * w_equi / (2 * pi)\n        uj = (phi - pi / 2) * h_equi / pi\n        ui += 0.5\n        uj += 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        ui = (theta - pi) * w_equi / (2 * pi)\n        uj = (phi - pi / 2) * h_equi / pi\n        ui += 0.5\n        uj += 0.5\n        ui = torch.where(ui < 0, ui + w_equi, ui)\n        ui = torch.where(ui >= w_equi, ui - w_equi, ui)\n        uj = torch.where(uj < 0, uj + h_equi, uj)\n        uj = torch.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = torch.stack((uj, ui), dim=-3)\n\n    return grid\n\n\ndef run(\n    src: torch.Tensor,\n    rots: List[Dict[str, float]],\n    z_down: bool,\n    mode: str,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    backend: str = \"native\",\n) -> torch.Tensor:\n    \"\"\"Run Equi2Equi\n\n    params:\n    - src (torch.Tensor): 4 dims (b, c, h, w)\n    - rot (List[dict]): dict of ('yaw', 'pitch', 'roll')\n    - z_down (bool)\n    - mode (str): sampling mode for grid_sample\n    - height, width (Optional[int]): height and width of the target\n    - backend (str): backend of torch `grid_sample` (default: `native`)\n\n    return:\n    - out (torch.Tensor)\n\n    NOTE: acceptable dtypes for `src` are currently uint8, float32, and float64.\n    Floats are prefered since numpy calculations are optimized for floats.\n\n    NOTE: output array has the same dtype as `src`\n\n    NOTE: you can override `equilib`'s grid_sample with over grid sampling methods\n    using `override_func`. The input to this function have to match `grid_sample`.\n\n    \"\"\"\n\n    assert (\n        len(src.shape) == 4\n    ), f\"ERR: input `src` should be 4-dim (b, c, h, w), but got {len(src.shape)}\"\n    assert len(src) == len(\n        rots\n    ), f\"ERR: length of `src` and `rot` differs: {len(src)} vs {len(rots)}\"\n\n    src_dtype = src.dtype\n    assert src_dtype in (\n        torch.uint8,\n        torch.float16,\n        torch.float32,\n        torch.float64,\n    ), (\n        f\"ERR: input equirectangular image has dtype of {src_dtype}which is\\n\"\n        f\"incompatible: try {(torch.uint8, torch.float16, torch.float32, torch.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    if src.device.type == \"cuda\":\n        dtype = torch.float32 if src_dtype == torch.uint8 else src_dtype\n        assert dtype in (torch.float16, torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float16, torch.float32, torch.float64)}\"\n        )\n    else:\n        # NOTE: for cpu, it can't use half-precision\n        dtype = torch.float32 if src_dtype == torch.uint8 else src_dtype\n        assert dtype in (torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float32, torch.float64)}\"\n        )\n    if backend == \"native\" and src_dtype == torch.uint8:\n        # FIXME: hacky way of dealing with images that are uint8 when using\n        # torch.grid_sample\n        src = src.type(torch.float32)\n\n    bs, c, h_equi, w_equi = src.shape\n    src_device = get_device(src)\n\n    assert type(height) == type(\n        width\n    ), \"ERR: `height` and `width` does not match types (maybe it was set separately?)\"\n    if height is None and width is None:\n        height = h_equi\n        width = w_equi\n    else:\n        assert isinstance(height, int) and isinstance(width, int)\n\n    # initialize output tensor\n    if backend == \"native\":\n        # NOTE: don't need to initialize for `native`\n        out = None\n    else:\n        out = torch.empty(\n            (bs, c, height, width),  # type: ignore\n            dtype=dtype,\n            device=src_device,\n        )\n\n    # FIXME: for now, calculate the grid in cpu\n    # I need to benchmark performance of it when grid is created on cuda\n    tmp_device = torch.device(\"cpu\")\n    if src.device.type == \"cuda\" and dtype == torch.float16:\n        tmp_dtype = torch.float32\n    else:\n        tmp_dtype = dtype\n\n    m = create_normalized_grid(\n        height=height, width=width, batch=bs, dtype=tmp_dtype, device=tmp_device\n    )\n    m = m.unsqueeze(-1)\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(\n        rots=rots, z_down=z_down, dtype=tmp_dtype, device=tmp_device\n    )\n\n    # rotate the grid\n    M = matmul(m, R)\n\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # FIXME: putting `grid` to device since `pure`'s bilinear interpolation requires it\n    # FIXME: better way of forcing `grid` to be the same dtype?\n    if src.dtype != grid.dtype:\n        grid = grid.type(src.dtype)\n    if src.device != grid.device:\n        grid = grid.to(src.device)\n\n    # grid sample\n    out = torch_grid_sample(\n        img=src,\n        grid=grid,\n        out=out,  # FIXME: is this necessary?\n        mode=mode,\n        backend=backend,\n    )\n\n    # NOTE: we assume that `out` keeps it's dtype\n\n    out = (\n        out.type(src_dtype)\n        if src_dtype == torch.uint8\n        else torch.clip(out, 0.0, 1.0)\n    )\n\n    return out\n"}
{"type": "source_file", "path": "gaussian_renderer/network_gui.py", "content": "#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nimport torch\nimport traceback\nimport socket\nimport json\nfrom scene.cameras import MiniCam\n\nhost = \"127.0.0.1\"\nport = 6009\n\nconn = None\naddr = None\n\nlistener = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ndef init(wish_host, wish_port):\n    global host, port, listener\n    host = wish_host\n    port = wish_port\n    listener.bind((host, port))\n    listener.listen()\n    listener.settimeout(0)\n\ndef try_connect():\n    global conn, addr, listener\n    try:\n        conn, addr = listener.accept()\n        print(f\"\\nConnected by {addr}\")\n        conn.settimeout(None)\n    except Exception as inst:\n        pass\n            \ndef read():\n    global conn\n    messageLength = conn.recv(4)\n    messageLength = int.from_bytes(messageLength, 'little')\n    message = conn.recv(messageLength)\n    return json.loads(message.decode(\"utf-8\"))\n\ndef send(message_bytes, verify):\n    global conn\n    if message_bytes != None:\n        conn.sendall(message_bytes)\n    conn.sendall(len(verify).to_bytes(4, 'little'))\n    conn.sendall(bytes(verify, 'ascii'))\n\ndef receive():\n    message = read()\n\n    width = message[\"resolution_x\"]\n    height = message[\"resolution_y\"]\n\n    if width != 0 and height != 0:\n        try:\n            do_training = bool(message[\"train\"])\n            fovy = message[\"fov_y\"]\n            fovx = message[\"fov_x\"]\n            znear = message[\"z_near\"]\n            zfar = message[\"z_far\"]\n            do_shs_python = bool(message[\"shs_python\"])\n            do_rot_scale_python = bool(message[\"rot_scale_python\"])\n            keep_alive = bool(message[\"keep_alive\"])\n            scaling_modifier = message[\"scaling_modifier\"]\n            world_view_transform = torch.reshape(torch.tensor(message[\"view_matrix\"]), (4, 4)).cuda()\n            world_view_transform[:,1] = -world_view_transform[:,1]\n            world_view_transform[:,2] = -world_view_transform[:,2]\n            full_proj_transform = torch.reshape(torch.tensor(message[\"view_projection_matrix\"]), (4, 4)).cuda()\n            full_proj_transform[:,1] = -full_proj_transform[:,1]\n            custom_cam = MiniCam(width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform)\n        except Exception as e:\n            print(\"\")\n            traceback.print_exc()\n            raise e\n        return custom_cam, do_training, do_shs_python, do_rot_scale_python, keep_alive, scaling_modifier\n    else:\n        return None, None, None, None, None, None"}
{"type": "source_file", "path": "modules/equilib/equi2cube/numpy.py", "content": "#!/usr/bin/env python3\n\nfrom copy import deepcopy\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\n\nfrom modules.equilib.grid_sample import numpy_grid_sample\nfrom modules.equilib.numpy_utils import create_xyz_grid, create_rotation_matrices\n\n\ndef cube_hsplits(cube_h: np.ndarray) -> List[np.ndarray]:\n    \"\"\"Returns list of horizontal splits (doesn't split batch)\"\"\"\n    assert cube_h.shape[-2] * 6 == cube_h.shape[-1]\n    # order [\"F\", \"R\", \"B\", \"L\", \"U\", \"D\"]\n    splits = np.split(cube_h, 6, axis=-1)  # works batched\n    assert len(splits) == 6\n    assert splits[0].shape == (*cube_h.shape[0:3], cube_h.shape[-2])\n    return splits\n\n\ndef cube_h2list(cube_h: np.ndarray) -> List[List[np.ndarray]]:\n    bs = cube_h.shape[0]\n    cube_lists = []\n    for b in range(bs):\n        cube_lists.append(np.split(cube_h[b], 6, axis=-1))\n    return cube_lists\n\n\ndef cube_h2dict(cube_h: np.ndarray) -> List[Dict[str, np.ndarray]]:\n    bs = cube_h.shape[0]\n    cube_list = cube_hsplits(cube_h)\n\n    cube_dicts = []\n    for b in range(bs):\n        cube_dicts.append(\n            {\n                k: deepcopy(cube_list[i][b])\n                for i, k in enumerate([\"F\", \"R\", \"B\", \"L\", \"U\", \"D\"])\n            }\n        )\n    return cube_dicts\n\n\ndef cube_h2dice(cube_h: np.ndarray) -> np.ndarray:\n    bs = cube_h.shape[0]\n    cube_list = cube_hsplits(cube_h)\n\n    w = cube_h.shape[-2]\n    cube_dice = np.zeros(\n        (bs, cube_h.shape[-3], w * 3, w * 4), dtype=cube_h.dtype\n    )\n    # Order: F R B L U D\n    sxy = [(1, 1), (2, 1), (3, 1), (0, 1), (1, 0), (1, 2)]\n    for b in range(bs):\n        for i, (sx, sy) in enumerate(sxy):\n            cube_dice[\n                b, :, sy * w : (sy + 1) * w, sx * w : (sx + 1) * w\n            ] = deepcopy(cube_list[i][b, ...])\n\n    return cube_dice\n\n\ndef matmul(m: np.ndarray, R: np.ndarray, method: str = \"faster\") -> np.ndarray:\n\n    if method == \"robust\":\n        # When target image size is smaller, it might be faster with `matmul`\n        # but not by much\n        M = np.matmul(R[:, np.newaxis, np.newaxis, ...], m)\n    elif method == \"faster\":\n        # `einsum` is probably fastest, but it might not be accurate\n        # I've tested it, and it's really close when it is float64,\n        # but loses precision for float32\n        # trade off between precision and speed i guess\n        # around x3 ~ x10 faster (faster when batch size is high)\n        batch_size = m.shape[0]\n        M = np.empty_like(m)\n        for b in range(batch_size):\n            M[b, ...] = np.einsum(\n                \"ik,...kj->...ij\", R[b, ...], m[b, ...], optimize=True\n            )\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    M = M.squeeze(-1)\n    return M\n\n\ndef convert_grid(\n    xyz: np.ndarray, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> np.ndarray:\n\n    # convert to rotation\n    phi = np.arcsin(xyz[..., 2] / np.linalg.norm(xyz, axis=-1))\n    theta = np.arctan2(xyz[..., 1], xyz[..., 0])\n\n    if method == \"robust\":\n        # convert to pixel\n        # I thought it would be faster if it was done all at once,\n        # but it was faster separately\n        ui = (theta / (2 * np.pi) - 0.5) * w_equi - 0.5\n        uj = (0.5 - phi / np.pi) * h_equi - 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        # NOTE: this asserts that theta and phi are in range\n        # the range of theta is -pi ~ pi\n        # the range of phi is -pi/2 ~ pi/2\n        # this means that if the input `rots` have rotations that are\n        # out of range, it will not work with `faster`\n        ui = (theta / (2 * np.pi) - 0.5) * w_equi - 0.5\n        uj = (0.5 - phi / np.pi) * h_equi - 0.5\n        ui = np.where(ui < 0, ui + w_equi, ui)\n        ui = np.where(ui >= w_equi, ui - w_equi, ui)\n        uj = np.where(uj < 0, uj + h_equi, uj)\n        uj = np.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = np.stack((uj, ui), axis=-3)\n    return grid\n\n\ndef run(\n    equi: np.ndarray,\n    rots: List[Dict[str, float]],\n    w_face: int,\n    cube_format: str,\n    z_down: bool,\n    mode: str,\n    override_func: Optional[Callable[[], Any]] = None,\n) -> Union[np.ndarray, List[List[np.ndarray]], List[Dict[str, np.ndarray]]]:\n    \"\"\"Call Equi2Cube\n\n    params:\n    - equi (np.ndarray)\n    - rots (List[Dict[str, float]])\n    - w_face (int)\n    - cube_format (str): ('horizon', 'list', 'dict', 'dice')\n    - z_down (str)\n    - mode (str)\n    - override_func (Callable): function for overriding `grid_sample`\n\n    returns:\n    - cubemaps\n\n    \"\"\"\n\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: batch size of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (np.uint8, np.float32, np.float64), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}\\n\"\n        f\"which is incompatible: try {(np.uint8, np.float32, np.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    dtype = (\n        np.dtype(np.float32) if equi_dtype == np.dtype(np.uint8) else equi_dtype\n    )\n    assert dtype in (np.float32, np.float64), (\n        f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n        f\"try {(np.float32, np.float64)}\"\n    )\n\n    bs, c, h_equi, w_equi = equi.shape\n\n    # initialize output array (horizon)\n    out = np.empty((bs, c, w_face, w_face * 6), dtype=dtype)\n\n    # create grid\n    xyz = create_xyz_grid(w_face=w_face, batch=bs, dtype=dtype)\n    xyz = xyz[..., np.newaxis]\n\n    # FIXME: not sure why, but z-axis is facing the opposite\n    # probably I need to change the way I choose the xyz coordinates\n    # this is a temporary fix for now\n    z_down = not z_down\n    # create batched rotation matrices\n    R = create_rotation_matrices(rots=rots, z_down=z_down, dtype=dtype)\n\n    # rotate grid\n    xyz = matmul(xyz, R, method=\"faster\")\n\n    # create a pixel map grid\n    grid = convert_grid(xyz=xyz, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # grid sample\n    if override_func is not None:\n        out = override_func(  # type: ignore\n            img=equi, grid=grid, out=out, mode=mode\n        )\n    else:\n        out = numpy_grid_sample(img=equi, grid=grid, out=out, mode=mode)\n\n    out = (\n        out.astype(equi_dtype)\n        if equi_dtype == np.dtype(np.uint8)\n        else np.clip(out, 0.0, 1.0)\n    )\n\n    # reformat the output\n    # FIXME: needs to test this\n    if cube_format == \"horizon\":\n        pass\n    elif cube_format == \"list\":\n        out = cube_h2list(out)\n    elif cube_format == \"dict\":\n        out = cube_h2dict(out)\n    elif cube_format == \"dice\":\n        out = cube_h2dice(out)\n    else:\n        raise NotImplementedError(\"{} is not supported\".format(cube_format))\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/equi2cube/__init__.py", "content": "#!/usr/bin/env python3\n"}
{"type": "source_file", "path": "modules/equilib/equi2cube/torch.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Union\n\nimport torch\n\nfrom modules.equilib.grid_sample import torch_grid_sample\nfrom modules.equilib.torch_utils import (\n    create_rotation_matrices,\n    create_xyz_grid,\n    get_device,\n    pi,\n)\n\n\ndef cube_hsplits(cube_h: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"Returns list of horizontal splits (doesn't split batch)\"\"\"\n    assert cube_h.shape[-2] * 6 == cube_h.shape[-1]\n    # order [\"F\", \"R\", \"B\", \"L\", \"U\", \"D\"]\n    splits = torch.split(\n        cube_h, split_size_or_sections=cube_h.shape[-2], dim=-1\n    )\n    assert len(splits) == 6\n    assert splits[0].shape == (*cube_h.shape[0:3], cube_h.shape[-2])\n    return splits\n\n\ndef cube_h2list(cube_h: torch.Tensor) -> List[List[torch.Tensor]]:\n    bs = cube_h.shape[0]\n    cube_lists = []\n    for b in range(bs):\n        cube_lists.append(\n            list(\n                torch.split(\n                    cube_h[b], split_size_or_sections=cube_h.shape[-2], dim=-1\n                )\n            )\n        )\n    return cube_lists\n\n\ndef cube_h2dict(cube_h: torch.Tensor) -> List[Dict[str, torch.Tensor]]:\n    bs = cube_h.shape[0]\n    cube_list = cube_hsplits(cube_h)\n\n    cube_dicts = []\n    for b in range(bs):\n        cube_dicts.append(\n            {\n                k: cube_list[i][b].clone()\n                for i, k in enumerate([\"F\", \"R\", \"B\", \"L\", \"U\", \"D\"])\n            }\n        )\n    return cube_dicts\n\n\ndef cube_h2dice(cube_h: torch.Tensor) -> torch.Tensor:\n    bs = cube_h.shape[0]\n    cube_list = cube_hsplits(cube_h)\n\n    w = cube_h.shape[-2]\n    cube_dice = torch.zeros(\n        (bs, cube_h.shape[-3], w * 3, w * 4),\n        dtype=cube_h.dtype,\n        device=cube_h.device,\n    )\n    # Order: F R B L U D\n    sxy = [(1, 1), (2, 1), (3, 1), (0, 1), (1, 0), (1, 2)]\n    for b in range(bs):\n        for i, (sx, sy) in enumerate(sxy):\n            cube_dice[\n                b, :, sy * w : (sy + 1) * w, sx * w : (sx + 1) * w\n            ] = cube_list[i][b, ...].clone()\n\n    return cube_dice\n\n\ndef matmul(m: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n\n    M = torch.matmul(R[:, None, None, ...], m)\n    M = M.squeeze(-1)\n\n    return M\n\n\ndef convert_grid(\n    xyz: torch.Tensor, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> torch.Tensor:\n\n    # convert to rotation\n    phi = torch.asin(xyz[..., 2] / torch.norm(xyz, dim=-1))\n    theta = torch.atan2(xyz[..., 1], xyz[..., 0])\n\n    if method == \"robust\":\n        ui = (theta / (2 * pi) - 0.5) * w_equi - 0.5\n        uj = (0.5 - phi / pi) * h_equi - 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        ui = (theta / (2 * pi) - 0.5) * w_equi - 0.5\n        uj = (0.5 - phi / pi) * h_equi - 0.5\n        ui = torch.where(ui < 0, ui + w_equi, ui)\n        ui = torch.where(ui >= w_equi, ui - w_equi, ui)\n        uj = torch.where(uj < 0, uj + h_equi, uj)\n        uj = torch.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = torch.stack((uj, ui), dim=-3)\n    return grid\n\n\ndef run(\n    equi: torch.Tensor,\n    rots: List[Dict[str, float]],\n    w_face: int,\n    cube_format: str,\n    z_down: bool,\n    mode: str,\n    backend: str = \"native\",\n) -> Union[torch.Tensor, List[torch.Tensor], List[Dict[str, torch.Tensor]]]:\n    \"\"\"Run Equi2Cube\n\n    params:\n    - equi (torch.Tensor): 4 dims (b, c, h, w)\n    - rots (List[dict]): dict of ('yaw', 'pitch', 'roll')\n    - w_face (int): width of the cube face\n    - cube_format (str): ('horizon', 'list', 'dict', 'dice')\n    - z_down (bool)\n    - mode (str): sampling mode for grid_sample\n    - backend (str): backend of torch `grid_sample` (default: `native`)\n\n    returns:\n    - cubemaps\n\n    NOTE: `backend` can be either `native` or `pure`\n\n    \"\"\"\n\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: length of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (\n        torch.uint8,\n        torch.float16,\n        torch.float32,\n        torch.float64,\n    ), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}which is\\n\"\n        f\"incompatible: try {(torch.uint8, torch.float16, torch.float32, torch.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    # if equi.device.type == \"cuda\":\n    #     dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n    #     assert dtype in (torch.float16, torch.float32, torch.float64), (\n    #         f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n    #         f\"try {(torch.float16, torch.float32, torch.float64)}\"\n    #     )\n    # else:\n    #     # NOTE: for cpu, it can't use half-precision\n    #     dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n    #     assert dtype in (torch.float32, torch.float64), (\n    #         f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n    #         f\"try {(torch.float32, torch.float64)}\"\n    #     )\n    # if backend == \"native\" and equi_dtype == torch.uint8:\n    #     # FIXME: hacky way of dealing with images that are uint8 when using\n    #     # torch.grid_sample\n    #     equi = equi.type(torch.float32)\n    dtype = equi_dtype\n\n    bs, c, h_equi, w_equi = equi.shape\n    img_device = get_device(equi)\n\n    # initialize output tensor\n    if backend == \"native\":\n        # NOTE: don't need to initialize for `native`\n        out = None\n    else:\n        out = torch.empty(\n            (bs, c, w_face, w_face * 6), dtype=dtype, device=img_device\n        )\n\n    # FIXME: for now, calculate the grid in cpu\n    # I need to benchmark performance of it when grid is created on cuda\n    tmp_device = torch.device(\"cpu\")\n    if equi.device.type == \"cuda\" and dtype == torch.float16:\n        tmp_dtype = torch.float32\n    else:\n        tmp_dtype = dtype\n\n    # create grid\n    xyz = create_xyz_grid(\n        w_face=w_face, batch=bs, dtype=tmp_dtype, device=tmp_device\n    )\n    xyz = xyz.unsqueeze(-1)\n\n    # FIXME: not sure why, but z-axis is facing the opposite\n    # probably I need to change the way I choose the xyz coordinates\n    # this is a temporary fix for now\n    z_down = not z_down\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(\n        rots=rots, z_down=z_down, dtype=tmp_dtype, device=tmp_device\n    )\n\n    # rotate grid\n    xyz = matmul(xyz, R)\n\n    # create a pixel map grid\n    grid = convert_grid(xyz=xyz, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # FIXME: putting `grid` to device since `pure`'s bilinear interpolation requires it\n    # FIXME: better way of forcing `grid` to be the same dtype?\n    if equi.dtype != grid.dtype:\n        grid = grid.type(equi.dtype)\n    if equi.device != grid.device:\n        grid = grid.to(equi.device)\n\n    # grid sample\n    out = torch_grid_sample(\n        img=equi, grid=grid, out=out, mode=mode, backend=backend\n    )\n\n    out = (\n        out.type(equi_dtype)\n        # if equi_dtype == torch.uint8\n        # else torch.clip(out, 0.0, 1.0)\n    )\n\n    # reformat the output\n    # FIXME: needs to test this\n    if cube_format == \"horizon\":\n        pass\n    elif cube_format == \"list\":\n        out = cube_h2list(out)  # type: ignore\n    elif cube_format == \"dict\":\n        out = cube_h2dict(out)  # type: ignore\n    elif cube_format == \"dice\":\n        out = cube_h2dice(out)\n    else:\n        raise NotImplementedError(\"{} is not supported\".format(cube_format))\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/equi2equi/base.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List, Optional, Union\n\nimport numpy as np\n\nimport torch\n\nfrom .numpy import run as run_numpy\nfrom .torch import run as run_torch\n\n__all__ = [\"Equi2Equi\", \"equi2equi\"]\n\nArrayLike = Union[np.ndarray, torch.Tensor]\nRot = Union[Dict[str, float], List[Dict[str, float]]]\n\n\nclass Equi2Equi(object):\n    \"\"\"\n    params:\n    - w_out, h_out (optional int): equi image size\n    - sampling_method (str): defaults to \"default\"\n    - mode (str): interpolation mode, defaults to \"bilinear\"\n    - z_down (bool)\n\n    input params:\n    - src (np.ndarray, torch.Tensor)\n    - rots (dict, list[dict])\n\n    return:\n    - equi (np.ndarray, torch.Tensor)\n    \"\"\"\n\n    def __init__(\n        self,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        mode: str = \"bilinear\",\n        z_down: bool = False,\n    ) -> None:\n        self.height = height\n        self.width = width\n        self.mode = mode\n        self.z_down = z_down\n\n    def __call__(self, src: ArrayLike, rots: Rot, **kwargs) -> ArrayLike:\n        return equi2equi(\n            src=src, rots=rots, mode=self.mode, z_down=self.z_down, **kwargs\n        )\n\n\ndef equi2equi(\n    src: ArrayLike,\n    rots: Rot,\n    mode: str = \"bilinear\",\n    z_down: bool = False,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n    **kwargs,\n) -> ArrayLike:\n    \"\"\"\n    params:\n    - src\n    - rots\n    - mode (str): interpolation mode, defaults to \"bilinear\"\n    - z_down (bool)\n    - height, width (optional int): output image size\n\n    returns:\n    - out\n\n    \"\"\"\n\n    _type = None\n    if isinstance(src, np.ndarray):\n        _type = \"numpy\"\n    elif torch.is_tensor(src):\n        _type = \"torch\"\n    else:\n        raise ValueError\n\n    is_single = False\n    if len(src.shape) == 3 and isinstance(rots, dict):\n        # probably the input was a single image\n        src = src[None, ...]\n        rots = [rots]\n        is_single = True\n    elif len(src.shape) == 3:\n        # probably a grayscale image\n        src = src[:, None, ...]\n\n    assert isinstance(rots, list), \"ERR: rots is not a list\"\n    if _type == \"numpy\":\n        out = run_numpy(\n            src=src,\n            rots=rots,\n            mode=mode,\n            z_down=z_down,\n            height=height,\n            width=width,\n            **kwargs,\n        )\n    elif _type == \"torch\":\n        out = run_torch(\n            src=src,\n            rots=rots,\n            mode=mode,\n            z_down=z_down,\n            height=height,\n            width=width,\n            **kwargs,\n        )\n    else:\n        raise ValueError\n\n    # make sure that the output batch dim is removed if it's only a single image\n    if is_single:\n        out = out.squeeze(0)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/native.py", "content": "#!/usr/bin/env python3\n\nfrom functools import partial\n\nimport torch\nimport torch.nn.functional as F\n\n__all__ = [\"native\", \"native_bicubic\", \"native_bilinear\", \"native_nearest\"]\n\n\ndef native(\n    img: torch.Tensor, grid: torch.Tensor, mode: str = \"bilinear\"\n) -> torch.Tensor:\n    \"\"\"Torch Grid Sample (default)\n\n    - Uses `torch.nn.functional.grid_sample`\n    - By far the best way to sample\n\n    params:\n    - img (torch.Tensor): Tensor[B, C, H, W]  or Tensor[C, H, W]\n    - grid (torch.Tensor): Tensor[B, 2, H, W] or Tensor[2, H, W]\n    - device (int or str): torch.device\n    - mode (str): (`bilinear`, `bicubic`, `nearest`)\n\n    returns:\n    - out (torch.Tensor): Tensor[B, C, H, W] or Tensor[C, H, W]\n        where H, W are grid size\n\n    NOTE: `img` and `grid` needs to be on the same device\n\n    NOTE: `img` and `grid` is somehow mutated (inplace?), so if you need\n    to reuse `img` and `grid` somewhere else, use `.clone()` before\n    passing it to this function\n\n    NOTE: this method is different from other grid sampling that\n    the padding cannot be wrapped. There might be pixel inaccuracies\n    when sampling from the boundaries of the image (the seam).\n\n    I hope later on, we can add wrap padding to this since the function\n    is super fast.\n\n    \"\"\"\n\n    assert (\n        grid.dtype == img.dtype\n    ), \"ERR: img and grid should have the same dtype\"\n\n    _, _, h, w = img.shape\n\n    # grid in shape: (batch, channel, h_out, w_out)\n    # grid out shape: (batch, h_out, w_out, channel)\n    grid = grid.permute(0, 2, 3, 1)\n\n    \"\"\"Preprocess for grid_sample\n    normalize grid -1 ~ 1\n\n    assumptions:\n    - values of `grid` is between `0 ~ (h-1)` and `0 ~ (w-1)`\n    - input of `grid_sample` need to be between `-1 ~ 1`\n    - maybe lose some precision when we map the values (int to float)?\n\n    mapping (e.g. mapping of height):\n    1. 0 <= y <= (h-1)\n    2. -1/2 <= y' <= 1/2  <- y' = y/(h-1) - 1/2\n    3. -1 <= y\" <= 1  <- y\" = 2y'\n    \"\"\"\n\n    # FIXME: this is not necessary when we are already preprocessing grid before\n    # this method is called\n    # grid[..., 0] %= h\n    # grid[..., 1] %= w\n\n    norm_uj = torch.clamp(2 * grid[..., 0] / (h - 1) - 1, -1, 1)\n    norm_ui = torch.clamp(2 * grid[..., 1] / (w - 1) - 1, -1, 1)\n\n    # reverse: grid sample takes xy, not (height, width)\n    grid[..., 0] = norm_ui\n    grid[..., 1] = norm_uj\n\n    # grid.requires_grad = True\n\n    out = F.grid_sample(\n        img,\n        grid,\n        mode=mode,\n        # use center of pixel instead of corner\n        align_corners=True,\n        # padding mode defaults to 'zeros' and there is no 'wrapping' mode\n        padding_mode=\"reflection\",\n    )\n    \n    return out\n\n\n# aliases\nnative_nearest = partial(native, mode=\"nearest\")\nnative_bilinear = partial(native, mode=\"bilinear\")\nnative_bicubic = partial(native, mode=\"bicubic\")\n"}
{"type": "source_file", "path": "modules/equilib/equi2pers/torch.py", "content": "#!/usr/bin/env python3\n\nfrom functools import lru_cache\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\n\nfrom modules.equilib.grid_sample import torch_grid_sample\nfrom modules.equilib.torch_utils import (\n    create_global2camera_rotation_matrix,\n    create_grid,\n    create_intrinsic_matrix,\n    create_rotation_matrices,\n    get_device,\n    pi,\n)\n\n\n@lru_cache(maxsize=128)\ndef create_cam2global_matrix(\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float = 0.0,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> torch.Tensor:\n\n    K = create_intrinsic_matrix(\n        height=height,\n        width=width,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=dtype,\n        device=device,\n    )\n    g2c_rot = create_global2camera_rotation_matrix(dtype=dtype, device=device)\n\n    return g2c_rot @ K.inverse()\n\n\ndef prep_matrices(\n    height: int,\n    width: int,\n    batch: int,\n    fov_x: float,\n    skew: float = 0.0,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device = torch.device(\"cpu\"),\n) -> Tuple[torch.Tensor, torch.Tensor]:\n\n    m = create_grid(\n        height=height, width=width, batch=batch, dtype=dtype, device=device\n    )\n    m = m.unsqueeze(-1)\n    G = create_cam2global_matrix(\n        height=height,\n        width=width,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=dtype,\n        device=device,\n    )\n\n    return m, G\n\n\ndef matmul(m: torch.Tensor, G: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n\n    M = torch.matmul(torch.matmul(R, G)[:, None, None, ...], m)\n    M = M.squeeze(-1)\n\n    return M\n\n\ndef convert_grid(\n    M: torch.Tensor, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> torch.Tensor:\n\n    # convert to rotation\n    phi = torch.asin(M[..., 2] / torch.norm(M, dim=-1))\n    theta = torch.atan2(M[..., 1], M[..., 0])\n\n    if method == \"robust\":\n        ui = (theta - pi) * w_equi / (2 * pi)\n        uj = (phi - pi / 2) * h_equi / pi\n        ui += 0.5\n        uj += 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        ui = (theta - pi) * w_equi / (2 * pi)\n        uj = (phi - pi / 2) * h_equi / pi\n        ui += 0.5\n        uj += 0.5\n        ui = torch.where(ui < 0, ui + w_equi, ui)\n        ui = torch.where(ui >= w_equi, ui - w_equi, ui)\n        uj = torch.where(uj < 0, uj + h_equi, uj)\n        uj = torch.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = torch.stack((uj, ui), dim=-3)\n\n    return grid\n\n\ndef run(\n    equi: torch.Tensor,\n    rots: List[Dict[str, float]],\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    z_down: bool,\n    mode: str,\n    backend: str = \"native\",\n    clip_output: bool = True,\n) -> torch.Tensor:\n    \"\"\"Run Equi2Pers\n\n    params:\n    - equi (torch.Tensor): 4 dims (b, c, h, w)\n    - rots (List[dict]): dict of ('yaw', 'pitch', 'roll')\n    - height, width (int): height and width of perspective view\n    - fov_x (float): fov of horizontal axis in degrees\n    - skew (float): skew of the camera\n    - z_down (bool)\n    - mode (str): sampling mode for grid_sample\n    - backend (str): backend of torch `grid_sample` (default: `native`)\n\n    returns:\n    - out (torch.Tensor)\n\n    NOTE: `backend` can be either `native` or `pure`\n\n    \"\"\"\n\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: length of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (\n        torch.uint8,\n        torch.float16,\n        torch.float32,\n        torch.float64,\n    ), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}which is\\n\"\n        f\"incompatible: try {(torch.uint8, torch.float16, torch.float32, torch.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    # if equi.device.type == \"cuda\":\n    #     dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n    #     assert dtype in (torch.float16, torch.float32, torch.float64), (\n    #         f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n    #         f\"try {(torch.float16, torch.float32, torch.float64)}\"\n    #     )\n    # else:\n    #     # NOTE: for cpu, it can't use half-precision\n    #     dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n    #     assert dtype in (torch.float32, torch.float64), (\n    #         f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n    #         f\"try {(torch.float32, torch.float64)}\"\n    #     )\n    # if backend == \"native\" and equi_dtype == torch.uint8:\n    #     # FIXME: hacky way of dealing with images that are uint8 when using\n    #     # torch.grid_sample\n    #     equi = equi.type(torch.float32)\n    dtype = equi_dtype\n\n    bs, c, h_equi, w_equi = equi.shape\n    img_device = get_device(equi)\n\n    # initialize output tensor\n    if backend == \"native\":\n        # NOTE: don't need to initialize for `native`\n        out = None\n    else:\n        out = torch.empty(\n            (bs, c, height, width), dtype=dtype, device=img_device\n        )\n\n\n    # FIXME: for now, calculate the grid in cpu\n    # I need to benchmark performance of it when grid is created on cuda\n    tmp_device = torch.device(\"cpu\")\n    if equi.device.type == \"cuda\" and dtype == torch.float16:\n        tmp_dtype = torch.float32\n    else:\n        tmp_dtype = dtype\n\n    # create grid and transform matrix\n    m, G = prep_matrices(\n        height=height,\n        width=width,\n        batch=bs,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=tmp_dtype,\n        device=tmp_device,\n    )\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(\n        rots=rots, z_down=z_down, dtype=tmp_dtype, device=tmp_device\n    )\n\n    # rotate and transform the grid\n    M = matmul(m, G, R)\n\n    # create a pixel map grid\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # if backend == \"native\":\n    #     grid = grid.to(img_device)\n    # FIXME: putting `grid` to device since `pure`'s bilinear interpolation requires it\n    # FIXME: better way of forcing `grid` to be the same dtype?\n    if equi.dtype != grid.dtype:\n        grid = grid.type(equi.dtype)\n    if equi.device != grid.device:\n        grid = grid.to(equi.device)\n\n    # grid sample\n    out = torch_grid_sample(\n        img=equi,\n        grid=grid,\n        out=out,  # FIXME: is this necessary?\n        mode=mode,\n        backend=backend,\n    )\n\n    # NOTE: we assume that `out` keeps it's dtype\n\n    out = (\n        out.type(equi_dtype)\n        # if equi_dtype == torch.uint8 or not clip_output\n        # else torch.clip(out, 0.0, 1.0)\n    )\n\n    return out\n\n\ndef get_bounding_fov(\n    equi: torch.Tensor,\n    rots: List[Dict[str, float]],\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    z_down: bool,\n) -> torch.Tensor:\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: length of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (\n        torch.uint8,\n        torch.float16,\n        torch.float32,\n        torch.float64,\n    ), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}which is\\n\"\n        f\"incompatible: try {(torch.uint8, torch.float16, torch.float32, torch.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    if equi.device.type == \"cuda\":\n        dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n        assert dtype in (torch.float16, torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float16, torch.float32, torch.float64)}\"\n        )\n    else:\n        # NOTE: for cpu, it can't use half-precision\n        dtype = torch.float32 if equi_dtype == torch.uint8 else equi_dtype\n        assert dtype in (torch.float32, torch.float64), (\n            f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n            f\"try {(torch.float32, torch.float64)}\"\n        )\n\n    bs, c, h_equi, w_equi = equi.shape\n\n    # FIXME: for now, calculate the grid in cpu\n    # I need to benchmark performance of it when grid is created on cuda\n    tmp_device = torch.device(\"cpu\")\n    if equi.device.type == \"cuda\" and dtype == torch.float16:\n        tmp_dtype = torch.float32\n    else:\n        tmp_dtype = dtype\n\n    # create grid and transform matrix\n    m, G = prep_matrices(\n        height=height,\n        width=width,\n        batch=bs,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=tmp_dtype,\n        device=tmp_device,\n    )\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(\n        rots=rots, z_down=z_down, dtype=tmp_dtype, device=tmp_device\n    )\n\n    # rotate and transform the grid\n    M = matmul(m, G, R)\n\n    # create a pixel map grid\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    bboxs = []\n\n    # top row\n    for out_x in range(width):\n        bboxs.append(grid[:, :, 0, out_x])\n\n    # right column\n    for out_y in range(height):\n        if out_y > 0:  # exclude first\n            bboxs.append(grid[:, :, out_y, width - 1])\n\n    # bottom row\n    for out_x in range(width - 2, 0, -1):\n        bboxs.append(grid[:, :, height - 1, out_x])\n\n    # left column\n    for out_y in range(height - 1, 0, -1):\n        bboxs.append(grid[:, :, out_y, 0])\n\n    assert len(bboxs) == width * 2 + (height - 2) * 2\n\n    bboxs = torch.stack(bboxs, dim=1)\n\n    bboxs = bboxs.numpy()\n    bboxs = np.rint(bboxs).astype(np.int64)\n\n    return bboxs\n"}
{"type": "source_file", "path": "modules/equilib/numpy_utils/rotation.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Dict, List\n\nimport numpy as np\n\n\n\"\"\"Rotations\n\n- http://planning.cs.uiuc.edu/node102.html\n- https://en.wikipedia.org/wiki/Rotation_matrix\n    - Use Rodrigues' rotation formula?\n\"\"\"\n\n\ndef create_global2camera_rotation_matrix(\n    dtype=np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Rotation from global (world) to camera coordinates\n\n    - camera coordinate has the z-axis as forward and x- and y-axis for image.\n    - when we rotate the mapping grid, we want to convert the camera coordinate\n      to the world coordinate where x-axis is forward.\n    \"\"\"\n\n    R_XY = np.array(\n        [[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],  # X <-> Y\n        dtype=dtype,\n    )\n    R_YZ = np.array(\n        [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0]],  # Y <-> Z\n        dtype=dtype,\n    )\n\n    return R_XY @ R_YZ\n\n\ndef create_rotation_matrix(\n    roll: float,\n    pitch: float,\n    yaw: float,\n    z_down: bool = True,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create Rotation Matrix\n\n    params:\n    - roll, pitch, yaw (float): in radians\n    - z_down (bool): flips pitch and yaw directions\n    - dtype (np.dtype): data types\n\n    returns:\n    - R (np.ndarray): 3x3 rotation matrix\n    \"\"\"\n\n    # calculate rotation about the x-axis (roll)\n    R_x = np.array(\n        [\n            [1.0, 0.0, 0.0],\n            [0.0, np.cos(roll), -np.sin(roll)],\n            [0.0, np.sin(roll), np.cos(roll)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the y-axis (pitch)\n    if not z_down:\n        pitch = -pitch\n    R_y = np.array(\n        [\n            [np.cos(pitch), 0.0, np.sin(pitch)],\n            [0.0, 1.0, 0.0],\n            [-np.sin(pitch), 0.0, np.cos(pitch)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the z-axis (yaw)\n    if not z_down:\n        yaw = -yaw\n    R_z = np.array(\n        [\n            [np.cos(yaw), -np.sin(yaw), 0.0],\n            [np.sin(yaw), np.cos(yaw), 0.0],\n            [0.0, 0.0, 1.0],\n        ],\n        dtype=dtype,\n    )\n\n    return R_z @ R_y @ R_x\n\n\ndef create_rotation_matrix_at_once(\n    roll: float,\n    pitch: float,\n    yaw: float,\n    z_down: bool = True,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create rotation matrix at once\"\n\n    params:\n    - roll, pitch, yaw (float): in radians\n    - z_down (bool): flips pitch and yaw directions\n    - dtype (np.dtype): data types\n\n    returns:\n    - R (np.ndarray): 3x3 rotation matrix\n\n    NOTE: same results as `create_rotation_matrix` but a little bit faster\n    \"\"\"\n\n    if not z_down:\n        pitch = -pitch\n        yaw = -yaw\n\n    return np.array(\n        [\n            [\n                np.cos(yaw) * np.cos(pitch),\n                np.cos(yaw) * np.sin(pitch) * np.sin(roll)\n                - np.sin(yaw) * np.cos(roll),\n                np.cos(yaw) * np.sin(pitch) * np.cos(roll)\n                + np.sin(yaw) * np.sin(roll),\n            ],\n            [\n                np.sin(yaw) * np.cos(pitch),\n                np.sin(yaw) * np.sin(yaw) * np.sin(pitch) * np.sin(roll)\n                + np.cos(yaw) * np.cos(roll),\n                np.sin(yaw) * np.sin(pitch) * np.cos(roll)\n                - np.cos(yaw) * np.sin(roll),\n            ],\n            [\n                -np.sin(pitch),\n                np.cos(pitch) * np.sin(roll),\n                np.cos(pitch) * np.cos(roll),\n            ],\n        ],\n        dtype=dtype,\n    )\n\n\ndef create_rotation_matrices(\n    rots: List[Dict[str, float]],\n    z_down: bool = True,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create rotation matrices from batch of rotations\n\n    This methods creates a bx3x3 np.ndarray where `b` referes to the number\n    of rotations (rots) given in the input\n    \"\"\"\n\n    R = np.empty((len(rots), 3, 3), dtype=dtype)\n    for i, rot in enumerate(rots):\n        # FIXME: maybe default to `create_rotation_matrix_at_once`?\n        R[i, ...] = create_rotation_matrix(**rot, z_down=z_down, dtype=dtype)\n\n    return R\n\n\ndef create_rotation_matrix_dep(\n    x: float,\n    y: float,\n    z: float,\n    z_down: bool = True,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create rotation matrix\n\n    NOTE: DEPRECATED\n    \"\"\"\n\n    # calculate rotation about the x-axis\n    R_x = np.array(\n        [\n            [1.0, 0.0, 0.0],\n            [0.0, np.cos(x), -np.sin(x)],\n            [0.0, np.sin(x), np.cos(x)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the y-axis\n    if not z_down:\n        y = -y\n    R_y = np.array(\n        [\n            [np.cos(y), 0.0, -np.sin(y)],\n            [0.0, 1.0, 0.0],\n            [np.sin(y), 0.0, np.cos(y)],\n        ],\n        dtype=dtype,\n    )\n    # calculate rotation about the z-axis\n    if not z_down:\n        z = -z\n    R_z = np.array(\n        [\n            [np.cos(z), np.sin(z), 0.0],\n            [-np.sin(z), np.cos(z), 0.0],\n            [0.0, 0.0, 1.0],\n        ],\n        dtype=dtype,\n    )\n\n    return R_z @ R_y @ R_x\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/cpp/__init__.py", "content": "#!/usr/bin/env python3\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/cpp/setup.py", "content": "#!/usr/bin/env python3\n\nfrom setuptools import Extension, setup\n\nfrom torch.utils import cpp_extension\n\n\nsetup(\n    name=\"grid_sample_cpp\",\n    ext_modules=[\n        cpp_extension.CppExtension(\"grid_sample\", [\"grid_sample.cpp\"])\n    ],\n    cmdclass={\"build_ext\": cpp_extension.BuildExtension},\n)\n\nExtension(\n    name=\"grid_sample_cpp\",\n    sources=[\"grid_sample.cpp\"],\n    include_dirs=cpp_extension.include_paths(),\n    language=\"c++\",\n)\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/__init__.py", "content": "#!/usr/bin/env python3\n\nfrom .grid_sample import grid_sample\n\n__all__ = [\"grid_sample\"]\n"}
{"type": "source_file", "path": "modules/equilib/equi2pers/numpy.py", "content": "#!/usr/bin/env python3\n\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, List, Tuple, Optional\n\nimport numpy as np\n\nfrom modules.equilib.grid_sample import numpy_grid_sample\nfrom modules.equilib.numpy_utils import (\n    create_global2camera_rotation_matrix,\n    create_grid,\n    create_intrinsic_matrix,\n    create_rotation_matrices,\n)\n\n\n@lru_cache(maxsize=128)\ndef create_cam2global_matrix(\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float = 0.0,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n\n    K = create_intrinsic_matrix(\n        height=height, width=width, fov_x=fov_x, skew=skew, dtype=dtype\n    )\n    g2c_rot = create_global2camera_rotation_matrix(dtype=dtype)\n\n    # FIXME: change to faster inverse\n    K_inv = np.linalg.inv(K)\n\n    return g2c_rot @ K_inv\n\n\ndef prep_matrices(\n    height: int,\n    width: int,\n    batch: int,\n    fov_x: float,\n    skew: float = 0.0,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> Tuple[np.ndarray, np.ndarray]:\n\n    m = create_grid(height=height, width=width, batch=batch, dtype=dtype)\n    m = m[..., np.newaxis]\n    G = create_cam2global_matrix(\n        height=height, width=width, fov_x=fov_x, skew=skew, dtype=dtype\n    )\n\n    return m, G\n\n\ndef matmul(\n    m: np.ndarray, G: np.ndarray, R: np.ndarray, method: str = \"faster\"\n) -> np.ndarray:\n\n    if method == \"robust\":\n        # When target image size is smaller, it might be faster with `matmul`\n        # but not by much\n        M = np.matmul(np.matmul(R, G)[:, np.newaxis, np.newaxis, ...], m)\n    elif method == \"faster\":\n        # `einsum` is probably fastest, but it might not be accurate\n        # I've tested it, and it's really close when it is float64,\n        # but loses precision for float32\n        # trade off between precision and speed i guess\n        # around x3 ~ x10 faster (faster when batch size is high)\n        batch_size = m.shape[0]\n        M = np.empty_like(m)\n        C = np.einsum(\"bik,kj->bij\", R, G, optimize=True)\n        for b in range(batch_size):\n            M[b, ...] = np.einsum(\n                \"ik,...kj->...ij\", C[b, ...], m[b, ...], optimize=True\n            )\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    M = M.squeeze(-1)\n    return M\n\n\ndef convert_grid(\n    M: np.ndarray, h_equi: int, w_equi: int, method: str = \"robust\"\n) -> np.ndarray:\n\n    # convert to rotation\n    phi = np.arcsin(M[..., 2] / np.linalg.norm(M, axis=-1))\n    theta = np.arctan2(M[..., 1], M[..., 0])\n\n    if method == \"robust\":\n        # convert to pixel\n        # I thought it would be faster if it was done all at once,\n        # but it was faster separately\n        ui = (theta - np.pi) * w_equi / (2 * np.pi)\n        uj = (phi - np.pi / 2) * h_equi / np.pi\n        ui += 0.5\n        uj += 0.5\n        ui %= w_equi\n        uj %= h_equi\n    elif method == \"faster\":\n        # NOTE: this asserts that theta and phi are in range\n        # the range of theta is -pi ~ pi\n        # the range of phi is -pi/2 ~ pi/2\n        # this means that if the input `rots` have rotations that are\n        # out of range, it will not work with `faster`\n        ui = (theta - np.pi) * w_equi / (2 * np.pi)\n        uj = (phi - np.pi / 2) * h_equi / np.pi\n        ui += 0.5\n        uj += 0.5\n        ui = np.where(ui < 0, ui + w_equi, ui)\n        ui = np.where(ui >= w_equi, ui - w_equi, ui)\n        uj = np.where(uj < 0, uj + h_equi, uj)\n        uj = np.where(uj >= h_equi, uj - h_equi, uj)\n    else:\n        raise ValueError(f\"ERR: {method} is not supported\")\n\n    # stack the pixel maps into a grid\n    grid = np.stack((uj, ui), axis=-3)\n\n    return grid\n\n\ndef run(\n    equi: np.ndarray,\n    rots: List[Dict[str, float]],\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    z_down: bool,\n    mode: str,\n    override_func: Optional[Callable[[], Any]] = None,\n    clip_output: bool = True,\n) -> np.ndarray:\n    \"\"\"Run Equi2Pers\n\n    params:\n    - equi (np.ndarray): 4 dims (b, c, h, w)\n    - rot (List[dict]): dict of ('yaw', 'pitch', 'roll')\n    - height, width (int): height and width of perspective view\n    - fov_x (float): fov of horizontal axis in degrees\n    - skew (float): skew of the camera\n    - z_down (bool)\n    - mode (str): sampling mode for grid_sample\n    - override_func (Callable): function for overriding `grid_sample`\n\n    return:\n    - out (np.ndarray)\n\n    NOTE: acceptable dtypes for `equi` are currently uint8, float32, and float64.\n    Floats are prefered since numpy calculations are optimized for floats.\n\n    NOTE: output array has the same dtype as `equi`\n\n    NOTE: you can override `equilib`'s grid_sample with over grid sampling methods\n    using `override_func`. The input to this function have to match `grid_sample`.\n\n    \"\"\"\n\n    # NOTE: Assume that the inputs `equi` and `rots` are already batched up\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: batch size of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (np.uint8, np.float32, np.float64), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}\\n\"\n        f\"which is incompatible: try {(np.uint8, np.float32, np.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    dtype = (\n        np.dtype(np.float32) if equi_dtype == np.dtype(np.uint8) else equi_dtype\n    )\n    assert dtype in (np.float32, np.float64), (\n        f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n        f\"try {(np.float32, np.float64)}\"\n    )\n\n    bs, c, h_equi, w_equi = equi.shape\n\n    # initialize output array\n    out = np.empty((bs, c, height, width), dtype=dtype)\n\n    # create grid and transfrom matrix\n    m, G = prep_matrices(\n        height=height,\n        width=width,\n        batch=bs,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=dtype,\n    )\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(rots=rots, z_down=z_down, dtype=dtype)\n\n    # rotate and transform the grid\n    M = matmul(m, G, R)\n\n    # create a pixel map grid\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    # grid sample\n    if override_func is not None:\n        # NOTE: override func is used for test purposes\n        out = override_func(  # type: ignore\n            img=equi, grid=grid, out=out, mode=mode\n        )\n    else:\n        out = numpy_grid_sample(\n            img=equi,\n            grid=grid,\n            out=out,  # FIXME: pass-by-reference confusing?\n            mode=mode,\n        )\n\n    out = (\n        out.astype(equi_dtype)\n        if equi_dtype == np.dtype(np.uint8) or not clip_output\n        else np.clip(out, 0.0, 1.0)\n    )\n\n    return out\n\n\ndef get_bounding_fov(\n    equi: np.ndarray,\n    rots: List[Dict[str, float]],\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    z_down: bool,\n) -> np.ndarray:\n    # NOTE: Assume that the inputs `equi` and `rots` are already batched up\n    assert (\n        len(equi.shape) == 4\n    ), f\"ERR: input `equi` should be 4-dim (b, c, h, w), but got {len(equi.shape)}\"\n    assert len(equi) == len(\n        rots\n    ), f\"ERR: batch size of equi and rot differs: {len(equi)} vs {len(rots)}\"\n\n    equi_dtype = equi.dtype\n    assert equi_dtype in (np.uint8, np.float32, np.float64), (\n        f\"ERR: input equirectangular image has dtype of {equi_dtype}\\n\"\n        f\"which is incompatible: try {(np.uint8, np.float32, np.float64)}\"\n    )\n\n    # NOTE: we don't want to use uint8 as output array's dtype yet since\n    # floating point calculations (matmul, etc) are faster\n    # NOTE: we are also assuming that uint8 is in range of 0-255 (obviously)\n    # and float is in range of 0.0-1.0; later we will refine it\n    # NOTE: for the sake of consistency, we will try to use the same dtype as equi\n    dtype = (\n        np.dtype(np.float32) if equi_dtype == np.dtype(np.uint8) else equi_dtype\n    )\n    assert dtype in (np.float32, np.float64), (\n        f\"ERR: argument `dtype` is {dtype} which is incompatible:\\n\"\n        f\"try {(np.float32, np.float64)}\"\n    )\n\n    bs, c, h_equi, w_equi = equi.shape\n\n    # create grid and transfrom matrix\n    m, G = prep_matrices(\n        height=height,\n        width=width,\n        batch=bs,\n        fov_x=fov_x,\n        skew=skew,\n        dtype=dtype,\n    )\n\n    # create batched rotation matrices\n    R = create_rotation_matrices(rots=rots, z_down=z_down, dtype=dtype)\n\n    # rotate and transform the grid\n    M = matmul(m, G, R)\n\n    # create a pixel map grid\n    grid = convert_grid(M=M, h_equi=h_equi, w_equi=w_equi, method=\"robust\")\n\n    bboxs = []\n\n    # top row\n    for out_x in range(width):\n        bboxs.append(grid[:, :, 0, out_x])\n\n    # right column\n    for out_y in range(height):\n        if out_y > 0:  # exclude first\n            bboxs.append(grid[:, :, out_y, width - 1])\n\n    # bottom row\n    for out_x in range(width - 2, 0, -1):\n        bboxs.append(grid[:, :, height - 1, out_x])\n\n    # left column\n    for out_y in range(height - 1, 0, -1):\n        bboxs.append(grid[:, :, out_y, 0])\n\n    assert len(bboxs) == width * 2 + (height - 2) * 2\n\n    bboxs = np.stack(bboxs, axis=1)\n    bboxs = np.rint(bboxs).astype(np.int64)\n\n    return bboxs\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/numpy/grid_sample.py", "content": "#!/usr/bin/env python3\n\nimport warnings\n\nimport numpy as np\n\nfrom .bicubic import bicubic\nfrom .bilinear import bilinear\nfrom .nearest import nearest\n\n\ndef grid_sample(\n    img: np.ndarray, grid: np.ndarray, out: np.ndarray, mode: str = \"bilinear\"\n) -> np.ndarray:\n    \"\"\"Numpy grid sampling algorithm\n\n    params:\n    - img (np.ndarray)\n    - grid (np.ndarray)\n    - out (np.ndarray)\n    - mode (str): ('bilinear', 'bicubic', 'nearest')\n\n    return:\n    - out (np.ndarray)\n\n    NOTE:\n    - assumes that `img`, `grid`, and `out` have the same dimension of\n      (batch, channel, height, width).\n    - channel for `grid` should be 2 (yx)\n\n    \"\"\"\n\n    if mode == \"nearest\":\n        out = nearest(img, grid, out)\n    elif mode == \"bilinear\":\n        out = bilinear(img, grid, out)\n    elif mode == \"bicubic\":\n        # FIXME: bicubic algorithm is not perfect yet\n        warnings.warn(\n            \"Bicubic interpolation is not perfect (especially when upsampling). Use with care!\"\n        )\n        out = bicubic(img, grid, out)\n    else:\n        raise ValueError(f\"ERR: {mode} is not supported\")\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/numpy/__init__.py", "content": "#!/usr/bin/env python\n\nfrom .grid_sample import grid_sample\n\n__all__ = [\"grid_sample\"]\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/__init__.py", "content": "#!/usr/bin/env python3\n\nfrom .numpy import grid_sample as numpy_grid_sample\nfrom .torch import grid_sample as torch_grid_sample\n\n__all__ = [\"numpy_grid_sample\", \"torch_grid_sample\"]\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/nearest.py", "content": "#!/usr/bin/env python3\n\nimport torch\n\n__all__ = [\"nearest\"]\n\n\ndef nearest(\n    img: torch.Tensor, grid: torch.Tensor, out: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"Nearest Neighbor Interpolation\n\n    Merit of using this nearest instead is that the grid doesn't need to be a\n    cuda tensor. Although it is a little bit slow since it is iterating batches\n    \"\"\"\n\n    b, _, h, w = img.shape\n\n    round_grid = torch.round(grid).type(torch.int64)\n    round_grid[:, 0, ...] %= h\n    round_grid[:, 1, ...] %= w\n\n    # FIXME: find a better way of sampling batches\n    for i in range(b):\n        y = round_grid[i, 0, :, :]\n        x = round_grid[i, 1, :, :]\n        out[i, ...] = img[i][:, y, x]\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/torch_utils/__init__.py", "content": "#!/usr/bin/env python3\n\nfrom .grid import create_grid, create_normalized_grid, create_xyz_grid\nfrom .intrinsic import create_intrinsic_matrix, pi\nfrom .rotation import (\n    create_global2camera_rotation_matrix,\n    create_rotation_matrices,\n    create_rotation_matrix,\n    create_rotation_matrix_at_once,\n)\nfrom .func import get_device, sizeof\n\n__all__ = [\n    \"create_global2camera_rotation_matrix\",\n    \"create_grid\",\n    \"create_intrinsic_matrix\",\n    \"create_normalized_grid\",\n    \"create_rotation_matrices\",\n    \"create_rotation_matrix\",\n    \"create_rotation_matrix_at_once\",\n    \"create_xyz_grid\",\n    \"get_device\",\n    \"sizeof\",\n    \"pi\",\n]\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/bicubic.py", "content": "#!/usr/bin/env python3\n\nimport torch\n\nfrom modules.equilib.torch_utils.func import get_device\n\n__all__ = [\"bicubic\"]\n\n\ndef kernel(s, a):\n    out = torch.zeros_like(s)\n    s = torch.abs(s)\n    mask1 = torch.logical_and(0 <= s, s <= 1)\n    mask2 = torch.logical_and(1 < s, s <= 2)\n    out[mask1] = (a + 2) * (s[mask1] ** 3) - (a + 3) * (s[mask1] ** 2) + 1\n    out[mask2] = (\n        a * (s[mask2] ** 3)\n        - (5 * a) * (s[mask2] ** 2)\n        + (8 * a) * s[mask2]\n        - 4 * a\n    )\n    return out\n\n\ndef bicubic(\n    img: torch.Tensor, grid: torch.Tensor, out: torch.Tensor\n) -> torch.Tensor:\n\n    # FIXME: out being initialized doesn't really matter?\n\n    b_in, c_in, h_in, w_in = img.shape\n    b_out, _, h_out, w_out = out.shape\n    dtype = out.dtype\n    device = get_device(out)\n\n    a = -0.75\n\n    int_dtype = torch.int64\n\n    min_grid = torch.floor(grid).type(int_dtype)\n\n    d1 = 1 + (grid - min_grid)\n    d2 = grid - min_grid\n    d3 = min_grid + 1 - grid\n    d4 = min_grid + 2 - grid\n\n    c1 = (grid - d1).type(int_dtype)\n    c2 = (grid - d2).type(int_dtype)\n    c3 = (grid + d3).type(int_dtype)\n    c4 = (grid + d4).type(int_dtype)\n\n    c1[:, 0, ...] %= h_in\n    c1[:, 1, ...] %= w_in\n    c2[:, 0, ...] %= h_in\n    c2[:, 1, ...] %= w_in\n    c3[:, 0, ...] %= h_in\n    c3[:, 1, ...] %= w_in\n    c4[:, 0, ...] %= h_in\n    c4[:, 1, ...] %= w_in\n\n    k1 = kernel(d1, a).type(dtype)\n    k2 = kernel(d2, a).type(dtype)\n    k3 = kernel(d3, a).type(dtype)\n    k4 = kernel(d4, a).type(dtype)\n\n    mat_l = torch.stack(\n        [k1[:, 1, ...], k2[:, 1, ...], k3[:, 1, ...], k4[:, 1, ...]], dim=-1\n    ).to(device)\n    mat_r = torch.stack(\n        [k1[:, 0, ...], k2[:, 0, ...], k3[:, 0, ...], k4[:, 0, ...]], dim=-1\n    ).to(device)\n\n    mat_m = torch.empty(\n        (b_out, c_in, h_out, w_out, 4, 4), dtype=dtype, device=device\n    )\n    for b in range(b_out):\n        y1 = c1[b, 0, ...]  # (h, w)\n        y2 = c2[b, 0, ...]\n        y3 = c3[b, 0, ...]\n        y4 = c4[b, 0, ...]\n\n        x1 = c1[b, 1, ...]\n        x2 = c2[b, 1, ...]\n        x3 = c3[b, 1, ...]\n        x4 = c4[b, 1, ...]\n\n        mat_m_x1 = torch.stack(\n            [\n                img[b][:, y1, x1],  # (c, h, w)\n                img[b][:, y2, x1],\n                img[b][:, y3, x1],\n                img[b][:, y4, x1],\n            ],\n            dim=-1,\n        )\n        mat_m_x2 = torch.stack(\n            [\n                img[b][:, y1, x2],\n                img[b][:, y2, x2],\n                img[b][:, y3, x2],\n                img[b][:, y4, x2],\n            ],\n            dim=-1,\n        )\n        mat_m_x3 = torch.stack(\n            [\n                img[b][:, y1, x3],\n                img[b][:, y2, x3],\n                img[b][:, y3, x3],\n                img[b][:, y4, x3],\n            ],\n            dim=-1,\n        )\n        mat_m_x4 = torch.stack(\n            [\n                img[b][:, y1, x4],\n                img[b][:, y2, x4],\n                img[b][:, y3, x4],\n                img[b][:, y4, x4],\n            ],\n            dim=-1,\n        )\n\n        mat_m[b, ...] = torch.stack(\n            [mat_m_x1, mat_m_x2, mat_m_x3, mat_m_x4], dim=-2\n        )\n\n    mat_l = mat_l.unsqueeze(1).unsqueeze(-2)\n    mat_r = mat_r.unsqueeze(1).unsqueeze(-1)\n    out = (mat_l @ mat_m @ mat_r).squeeze(-1).squeeze(-1)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/numpy/bilinear.py", "content": "#!/usr/bin/env python3\n\nimport numpy as np\n\n__all__ = [\"bilinear\"]\n\n\ndef interp(v0, v1, d, L):\n    return v0 * (1 - d) / L + v1 * d / L\n\n\ndef interp2d(q00, q10, q01, q11, dy, dx):\n    f0 = interp(q00, q01, dx, 1)\n    f1 = interp(q10, q11, dx, 1)\n    return interp(f0, f1, dy, 1)\n\n\ndef bilinear(img: np.ndarray, grid: np.ndarray, out: np.ndarray) -> np.ndarray:\n    \"\"\"Bilinear Interpolation\n\n    NOTE: asserts are removed\n    \"\"\"\n\n    b, _, h, w = img.shape\n\n    min_grid = np.floor(grid).astype(np.int64)\n    max_grid = min_grid + 1\n    d_grid = grid - min_grid\n\n    min_grid[:, 0, :, :] %= h\n    min_grid[:, 1, :, :] %= w\n    max_grid[:, 0, :, :] %= h\n    max_grid[:, 1, :, :] %= w\n\n    # FIXME: any way to do efficient batch?\n    for i in range(b):\n        dy = d_grid[i, 0, ...]\n        dx = d_grid[i, 1, ...]\n        min_ys = min_grid[i, 0, ...]\n        min_xs = min_grid[i, 1, ...]\n        max_ys = max_grid[i, 0, ...]\n        max_xs = max_grid[i, 1, ...]\n\n        p00 = img[i][:, min_ys, min_xs]\n        p10 = img[i][:, max_ys, min_xs]\n        p01 = img[i][:, min_ys, max_xs]\n        p11 = img[i][:, max_ys, max_xs]\n\n        out[i, ...] = interp2d(p00, p10, p01, p11, dy, dx)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/bilinear.py", "content": "#!/usr/bin/env python3\n\nimport torch\n\n__all__ = [\"bilinear\"]\n\n\ndef linear_interp(v0, v1, d, L):\n    return v0 * (1 - d) / L + v1 * d / L\n\n\ndef interp2d(q00, q10, q01, q11, dy, dx):\n    f0 = linear_interp(q00, q01, dx, 1)\n    f1 = linear_interp(q10, q11, dx, 1)\n    return linear_interp(f0, f1, dy, 1)\n\n\ndef bilinear(\n    img: torch.Tensor, grid: torch.Tensor, out: torch.Tensor\n) -> torch.Tensor:\n\n    b, _, h, w = img.shape\n\n    min_grid = torch.floor(grid).type(torch.int64)\n    max_grid = min_grid + 1\n    d_grid = grid - min_grid\n\n    min_grid[:, 0, :, :] %= h\n    min_grid[:, 1, :, :] %= w\n    max_grid[:, 0, :, :] %= h\n    max_grid[:, 1, :, :] %= w\n\n    # FIXME: anyway to do efficient batch?\n    for i in range(b):\n        dy = d_grid[i, 0, ...]\n        dx = d_grid[i, 1, ...]\n        min_ys = min_grid[i, 0, ...]\n        min_xs = min_grid[i, 1, ...]\n        max_ys = max_grid[i, 0, ...]\n        max_xs = max_grid[i, 1, ...]\n\n        min_ys %= h\n        min_xs %= w\n\n        p00 = img[i][:, min_ys, min_xs]\n        p10 = img[i][:, max_ys, min_xs]\n        p01 = img[i][:, min_ys, max_xs]\n        p11 = img[i][:, min_ys, max_xs]\n\n        out[i, ...] = interp2d(p00, p10, p01, p11, dy, dx)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/numpy/bicubic.py", "content": "#!/usr/bin/env python3\n\nimport numpy as np\n\n__all__ = [\"bicubic\"]\n\n\ndef kernel(\n    s: np.ndarray, a: float = -0.75, dtype: np.dtype = np.dtype(np.float32)\n) -> np.ndarray:\n    out = np.zeros_like(s, dtype)\n    s = np.abs(s)\n    mask1 = np.logical_and(0 <= s, s <= 1)\n    mask2 = np.logical_and(1 < s, s <= 2)\n    out[mask1] = (a + 2) * (s[mask1] ** 3) - (a + 3) * (s[mask1] ** 2) + 1\n    out[mask2] = (\n        a * (s[mask2] ** 3)\n        - (5 * a) * (s[mask2] ** 2)\n        + (8 * a) * s[mask2]\n        - 4 * a\n    )\n    return out\n\n\ndef bicubic(img: np.ndarray, grid: np.ndarray, out: np.ndarray) -> np.ndarray:\n    \"\"\"Bicubic Interpolation\"\"\"\n\n    b_in, c_in, h_in, w_in = img.shape\n    b_out, _, h_out, w_out = out.shape\n    dtype = out.dtype\n    # NOTE: this is hardcoded since pytorch is also -0.75\n    a = -0.75\n\n    int_dtype = np.dtype(np.int64)\n    min_grid = np.floor(grid).astype(int_dtype)\n\n    d1 = 1 + (grid - min_grid)  # (b, 2, h, w)\n    d2 = grid - min_grid\n    d3 = min_grid + 1 - grid\n    d4 = min_grid + 2 - grid\n\n    c1 = (grid - d1).astype(int_dtype)  # (b, 2, h, w)\n    c2 = (grid - d2).astype(int_dtype)\n    c3 = (grid + d3).astype(int_dtype)\n    c4 = (grid + d4).astype(int_dtype)\n\n    c1[:, 0, ...] %= h_in\n    c1[:, 1, ...] %= w_in\n    c2[:, 0, ...] %= h_in\n    c2[:, 1, ...] %= w_in\n    c3[:, 0, ...] %= h_in\n    c3[:, 1, ...] %= w_in\n    c4[:, 0, ...] %= h_in\n    c4[:, 1, ...] %= w_in\n\n    # FIXME: this part is slow\n    k1 = kernel(d1, a, dtype)  # (b, 2, h, w)\n    k2 = kernel(d2, a, dtype)\n    k3 = kernel(d3, a, dtype)\n    k4 = kernel(d4, a, dtype)\n\n    mat_l = np.stack(\n        [k1[:, 1, ...], k2[:, 1, ...], k3[:, 1, ...], k4[:, 1, ...]], axis=-1\n    )\n    mat_r = np.stack(\n        [k1[:, 0, ...], k2[:, 0, ...], k3[:, 0, ...], k4[:, 0, ...]], axis=-1\n    )\n\n    # FIXME: this part is slow\n    mat_m = np.empty((b_out, c_in, h_out, w_out, 4, 4), dtype=dtype)\n    for b in range(b_out):\n        y1 = c1[b, 0, ...]  # (h, w)\n        y2 = c2[b, 0, ...]\n        y3 = c3[b, 0, ...]\n        y4 = c4[b, 0, ...]\n\n        x1 = c1[b, 1, ...]\n        x2 = c2[b, 1, ...]\n        x3 = c3[b, 1, ...]\n        x4 = c4[b, 1, ...]\n\n        mat_m_x1 = np.stack(\n            [\n                img[b][:, y1, x1],  # (c, h, w)\n                img[b][:, y2, x1],\n                img[b][:, y3, x1],\n                img[b][:, y4, x1],\n            ],\n            axis=-1,\n        )\n        mat_m_x2 = np.stack(\n            [\n                img[b][:, y1, x2],\n                img[b][:, y2, x2],\n                img[b][:, y3, x2],\n                img[b][:, y4, x2],\n            ],\n            axis=-1,\n        )\n        mat_m_x3 = np.stack(\n            [\n                img[b][:, y1, x3],\n                img[b][:, y2, x3],\n                img[b][:, y3, x3],\n                img[b][:, y4, x3],\n            ],\n            axis=-1,\n        )\n        mat_m_x4 = np.stack(\n            [\n                img[b][:, y1, x4],\n                img[b][:, y2, x4],\n                img[b][:, y3, x4],\n                img[b][:, y4, x4],\n            ],\n            axis=-1,\n        )\n\n        mat_m[b, ...] = np.stack(\n            [mat_m_x1, mat_m_x2, mat_m_x3, mat_m_x4], axis=-2\n        )\n\n    mat_l = mat_l[:, np.newaxis, ..., np.newaxis, :]\n    mat_r = mat_r[:, np.newaxis, ..., np.newaxis]\n    out = (mat_l @ mat_m @ mat_r).squeeze(-1).squeeze(-1)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/numpy_utils/grid.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Optional\n\nimport numpy as np\n\n\ndef create_grid(\n    height: int,\n    width: int,\n    batch: Optional[int] = None,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create coordinate grid with height and width\n\n    NOTE: primarly used for equi2pers\n\n    `z-axis` scale is `1`\n\n    params:\n    - height (int)\n    - width (int)\n    - batch (Optional[int])\n    - dtype (np.dtype)\n\n    return:\n    - grid (np.ndarray)\n\n    \"\"\"\n\n    _xs = np.linspace(0, width - 1, width, dtype=dtype)\n    _ys = np.linspace(0, height - 1, height, dtype=dtype)\n    xs, ys = np.meshgrid(_xs, _ys)\n    zs = np.ones_like(xs, dtype=dtype)\n    grid = np.stack((xs, ys, zs), axis=-1)\n    # grid shape is (h, w, 3)\n\n    # batched (stacked copies)\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        grid = np.concatenate([grid[np.newaxis, ...]] * batch)\n        # grid shape is (b, h, w, 3)\n\n    return grid\n\n\ndef create_normalized_grid(\n    height: int,\n    width: int,\n    batch: Optional[int] = None,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create coordinate grid with height and width\n\n    NOTE: primarly used for equi2equi\n\n    params:\n    - height (int)\n    - width (int)\n    - batch (Optional[int])\n    - dtype (np.dtype)\n\n    return:\n    - grid (np.ndarray)\n\n    \"\"\"\n\n    xs = np.linspace(0, width - 1, width, dtype=dtype)\n    ys = np.linspace(0, height - 1, height, dtype=dtype)\n    theta = xs * 2 * np.pi / width - np.pi\n    phi = ys * np.pi / height - np.pi / 2\n    theta, phi = np.meshgrid(theta, phi)\n    a = np.stack((theta, phi), axis=-1)\n    norm_A = 1\n    x = norm_A * np.cos(a[..., 1]) * np.cos(a[..., 0])\n    y = norm_A * np.cos(a[..., 1]) * np.sin(a[..., 0])\n    z = norm_A * np.sin(a[..., 1])\n    grid = np.stack((x, y, z), axis=-1)\n\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        grid = np.concatenate([grid[np.newaxis, ...]] * batch)\n        # grid shape is (b, h, w, 3)\n\n    return grid\n\n\ndef create_xyz_grid(\n    w_face: int,\n    batch: Optional[int] = None,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"xyz coordinates of the faces of the cube\"\"\"\n\n    ratio = (w_face - 1) / w_face\n\n    out = np.zeros((w_face, w_face * 6, 3), dtype=dtype)\n    rng = np.linspace(-0.5 * ratio, 0.5 * ratio, num=w_face, dtype=dtype)\n\n    # Front face (x = 0.5)\n    out[:, 0 * w_face : 1 * w_face, [1, 2]] = np.stack(\n        np.meshgrid(rng, -rng), -1\n    )\n    out[:, 0 * w_face : 1 * w_face, 0] = 0.5\n\n    # Right face (y = -0.5)\n    out[:, 1 * w_face : 2 * w_face, [0, 2]] = np.stack(\n        np.meshgrid(-rng, -rng), -1\n    )\n    out[:, 1 * w_face : 2 * w_face, 1] = 0.5\n\n    # Back face (x = -0.5)\n    out[:, 2 * w_face : 3 * w_face, [1, 2]] = np.stack(\n        np.meshgrid(-rng, -rng), -1\n    )\n    out[:, 2 * w_face : 3 * w_face, 0] = -0.5\n\n    # Left face (y = 0.5)\n    out[:, 3 * w_face : 4 * w_face, [0, 2]] = np.stack(\n        np.meshgrid(rng, -rng), -1\n    )\n    out[:, 3 * w_face : 4 * w_face, 1] = -0.5\n\n    # Up face (z = 0.5)\n    out[:, 4 * w_face : 5 * w_face, [1, 0]] = np.stack(\n        np.meshgrid(rng, rng), -1\n    )\n    out[:, 4 * w_face : 5 * w_face, 2] = 0.5\n\n    # Down face (z = -0.5)\n    out[:, 5 * w_face : 6 * w_face, [1, 0]] = np.stack(\n        np.meshgrid(rng, -rng), -1\n    )\n    out[:, 5 * w_face : 6 * w_face, 2] = -0.5\n\n    if batch is not None:\n        assert isinstance(\n            batch, int\n        ), f\"ERR: batch needs to be integer: batch={batch}\"\n        assert (\n            batch > 0\n        ), f\"ERR: batch size needs to be larger than 0: batch={batch}\"\n        # FIXME: faster way of copying?\n        out = np.concatenate([out[np.newaxis, ...]] * batch)\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/numpy/nearest.py", "content": "#!/usr/bin/env python3\n\nimport numpy as np\n\n__all__ = [\"nearest\"]\n\n\ndef nearest(img: np.ndarray, grid: np.ndarray, out: np.ndarray) -> np.ndarray:\n    \"\"\"Nearest Neightbor Sampling\"\"\"\n\n    b, _, h, w = img.shape\n\n    round_grid = np.rint(grid).astype(np.int64)\n    round_grid[:, 0, ...] %= h\n    round_grid[:, 1, ...] %= w\n\n    for i in range(b):\n        y = round_grid[i, 0, ...]\n        x = round_grid[i, 1, ...]\n        out[i, ...] = img[i][:, y, x]\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/grid_sample/torch/grid_sample.py", "content": "#!/usr/bin/env python3\n\nfrom typing import Optional\nimport warnings\n\nimport torch\n\nfrom .native import native_bicubic, native_bilinear, native_nearest\nfrom .nearest import nearest\nfrom .bilinear import bilinear\nfrom .bicubic import bicubic\n\nDTYPES = (torch.uint8, torch.float16, torch.float32, torch.float64)\n\n\ndef grid_sample(\n    img: torch.Tensor,\n    grid: torch.Tensor,\n    out: Optional[torch.Tensor] = None,\n    mode: str = \"bilinear\",\n    backend: str = \"native\",\n) -> torch.Tensor:\n    \"\"\"Torch grid sampling algorithm\n\n    params:\n    - img (torch.Tensor)\n    - grid (torch.Tensor)\n    - out (Optional[torch.Tensor]): defaults to None\n    - mode (str): ('bilinear', 'bicubic', 'nearest')\n    - backend (str): ('native', 'pure')\n\n    return:\n    - img (torch.Tensor)\n\n    NOTE: for `backend`, `pure` is relatively efficient since grid doesn't need\n    to be in the same device as the `img`. However, `native` is faster.\n\n    NOTE: for `pure` backends, we need to pass reference to `out`.\n\n    NOTE: for `native` backends, we should pass anything for `out`\n\n    \"\"\"\n\n    if backend == \"native\":\n        if out is not None:\n            # NOTE: out is created\n            warnings.warn(\n                \"don't need to pass preallocated `out` to `grid_sample`\"\n            )\n        assert img.device == grid.device, (\n            f\"ERR: when using {backend}, the devices of `img` and `grid` need\"\n            \"to be on the same device\"\n        )\n        if mode == \"nearest\":\n            out = native_nearest(img, grid)\n        elif mode == \"bilinear\":\n            out = native_bilinear(img, grid)\n        elif mode == \"bicubic\":\n            out = native_bicubic(img, grid)\n        else:\n            raise ValueError(f\"ERR: {mode} is not supported\")\n    elif backend == \"pure\":\n        # NOTE: img and grid can be on different devices, but grid should be on the cpu\n        # FIXME: since bilinear implementation depends on `grid` being on device, I'm removing\n        # this warning and will put `grid` onto the same device until a fix is found\n        # if grid.device.type == \"cuda\":\n        #     warnings.warn(\"input `grid` should be on the cpu, but got a cuda tensor\")\n        assert (\n            out is not None\n        ), \"ERR: need to pass reference to `out`, but got None\"\n        assert img.device == grid.device, (\n            f\"ERR: when using {backend}, the devices of `img` and `grid` need\"\n            \"to be on the same device\"\n        )\n        if mode == \"nearest\":\n            out = nearest(img, grid, out)\n        elif mode == \"bilinear\":\n            out = bilinear(img, grid, out)\n        elif mode == \"bicubic\":\n            out = bicubic(img, grid, out)\n        else:\n            raise ValueError(f\"ERR: {mode} is not supported\")\n    else:\n        raise ValueError(f\"ERR: {backend} is not supported\")\n\n    return out\n"}
{"type": "source_file", "path": "modules/equilib/numpy_utils/intrinsic.py", "content": "#!/usr/bin/env python3\n\nimport numpy as np\n\n\ndef create_intrinsic_matrix(\n    height: int,\n    width: int,\n    fov_x: float,\n    skew: float,\n    dtype: np.dtype = np.dtype(np.float32),\n) -> np.ndarray:\n    \"\"\"Create intrinsic matrix\n\n    params:\n    - height, width (int)\n    - fov_x (float): make sure it's in degrees\n    - skew (float): 0.0\n    - dtype (np.dtype): np.float32\n\n    returns:\n    - K (np.ndarray): 3x3 intrinsic matrix\n    \"\"\"\n\n    # perspective projection (focal length)\n    f = width / (2.0 * np.tan(np.radians(fov_x).astype(dtype) / 2.0))\n    # transform between camera frame and pixel coordinates\n    K = np.array(\n        [[f, skew, width / 2], [0.0, f, height / 2], [0.0, 0.0, 1.0]],\n        dtype=dtype,\n    )\n\n    return K\n"}
{"type": "source_file", "path": "modules/equilib/numpy_utils/__init__.py", "content": "#!/usr/bin/env python3\n\nfrom .grid import create_grid, create_normalized_grid, create_xyz_grid\nfrom .intrinsic import create_intrinsic_matrix\nfrom .rotation import (\n    create_global2camera_rotation_matrix,\n    create_rotation_matrices,\n    create_rotation_matrix,\n    create_rotation_matrix_at_once,\n)\n\n__all__ = [\n    \"create_grid\",\n    \"create_intrinsic_matrix\",\n    \"create_global2camera_rotation_matrix\",\n    \"create_normalized_grid\",\n    \"create_rotation_matrices\",\n    \"create_rotation_matrix\",\n    \"create_rotation_matrix_at_once\",\n    \"create_xyz_grid\",\n]\n"}
