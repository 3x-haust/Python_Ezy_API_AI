{"repo_info": {"repo_name": "OmniMamba", "repo_owner": "hustvl", "repo_url": "https://github.com/hustvl/OmniMamba"}}
{"type": "source_file", "path": "llamagen_tokenizer/consistencydecoder/cd_demo.py", "content": "import argparse\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import ConsistencyDecoderVAE\n\n\ndef main(args):\n    # Setup PyTorch:\n    torch.manual_seed(args.seed)\n    torch.set_grad_enabled(False)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # create and load model\n    vae = ConsistencyDecoderVAE.from_pretrained(\"openai/consistency-decoder\", torch_dtype=torch.float16).to(device)\n\n    # load image\n    img_path = args.image_path\n    out_path = args.image_path.replace('.jpg', '_cd.jpg').replace('.jpeg', '_cd.jpeg').replace('.png', '_cd.png')\n    input_size = args.image_size\n    img = Image.open(img_path).convert(\"RGB\")\n\n    # preprocess\n    size_org = img.size\n    img = img.resize((input_size, input_size))\n    img = np.array(img) / 255.\n    x = 2.0 * img - 1.0 # x value is between [-1, 1]\n    x = torch.tensor(x)\n    x = x.unsqueeze(dim=0)\n    x = torch.einsum('nhwc->nchw', x)\n    x_input = x.half().to(device)\n\n    # inference\n    with torch.no_grad():\n        # Map input images to latent space + normalize latents:\n        latent = vae.encode(x_input).latent_dist.sample().mul_(0.18215)\n        # reconstruct:\n        output = vae.decode(latent / 0.18215).sample # output value is between [-1, 1]\n\n    # postprocess\n    output = F.interpolate(output, size=[size_org[1], size_org[0]], mode='bilinear').permute(0, 2, 3, 1)[0]\n    sample = torch.clamp(127.5 * output + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8).numpy()\n\n    # save        \n    Image.fromarray(sample).save(out_path)\n    print(\"Reconstructed image is saved to {}\".format(out_path))\n\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--image-path\", type=str, default=\"assets/example.jpg\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512, 1024], default=512)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "llamagen_tokenizer/consistencydecoder/reconstruction_cd_ddp.py", "content": "import torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.distributed as dist\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport os\nimport itertools\nfrom PIL import Image\nimport numpy as np\nimport argparse\nimport random\n\nfrom skimage.metrics import peak_signal_noise_ratio as psnr_loss\nfrom skimage.metrics import structural_similarity as ssim_loss\nfrom diffusers.models import ConsistencyDecoderVAE\n\n\nclass SingleFolderDataset(Dataset):\n    def __init__(self, directory, transform=None):\n        super().__init__()\n        self.directory = directory\n        self.transform = transform\n        self.image_paths = [os.path.join(directory, file_name) for file_name in os.listdir(directory)\n                            if os.path.isfile(os.path.join(directory, file_name))]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(0)\n\n\ndef create_npz_from_sample_folder(sample_dir, num=50_000):\n    \"\"\"\n    Builds a single .npz file from a folder of .png samples.\n    \"\"\"\n    samples = []\n    for i in tqdm(range(num), desc=\"Building .npz file from samples\"):\n        sample_pil = Image.open(f\"{sample_dir}/{i:06d}.png\")\n        sample_np = np.asarray(sample_pil).astype(np.uint8)\n        samples.append(sample_np)\n\n    random.shuffle(samples) # This is very important for IS(Inception Score) !!!\n    samples = np.stack(samples)\n    assert samples.shape == (num, samples.shape[1], samples.shape[2], 3)\n    npz_path = f\"{sample_dir}.npz\"\n    np.savez(npz_path, arr_0=samples)\n    print(f\"Saved .npz file to {npz_path} [shape={samples.shape}].\")\n    return npz_path\n\n\ndef center_crop_arr(pil_image, image_size):\n    \"\"\"\n    Center cropping implementation from ADM.\n    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n    \"\"\"\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n\n\ndef main(args):\n    # Setup PyTorch:\n    assert torch.cuda.is_available(), \"Sampling with DDP requires at least one GPU. sample.py supports CPU-only usage\"\n    torch.set_grad_enabled(False)\n\n    # Setup env\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # create and load model\n    vae = ConsistencyDecoderVAE.from_pretrained(\"openai/consistency-decoder\", torch_dtype=torch.float16).to(\"cuda:{}\".format(device))\n\n    # Create folder to save samples:\n    folder_name = f\"openai-consistencydecoder-{args.dataset}-size-{args.image_size}-seed-{args.global_seed}\"\n    sample_folder_dir = f\"{args.sample_dir}/{folder_name}\"\n    if rank == 0:\n        os.makedirs(sample_folder_dir, exist_ok=True)\n        print(f\"Saving .png samples at {sample_folder_dir}\")\n    dist.barrier()\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n    if args.dataset == 'imagenet':\n        dataset = ImageFolder(args.data_path, transform=transform)\n        num_fid_samples = 50000\n    elif args.dataset == 'coco':\n        dataset = SingleFolderDataset(args.data_path, transform=transform)\n        num_fid_samples = 5000\n    else:\n        raise Exception(\"please check dataset\")\n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=False,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=args.per_proc_batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=False\n    )    \n\n    # Figure out how many samples we need to generate on each GPU and how many iterations we need to run:\n    n = args.per_proc_batch_size\n    global_batch_size = n * dist.get_world_size()\n    psnr_val_rgb = []\n    ssim_val_rgb = []\n\n    loader = tqdm(loader) if rank == 0 else loader\n    total = 0\n    for x, _ in loader:\n        rgb_gts = x\n        rgb_gts = (rgb_gts.permute(0, 2, 3, 1).to(\"cpu\").numpy() + 1.0) / 2.0 # rgb_gt value is between [0, 1]\n        x = x.half().to(\"cuda:{}\".format(device))\n        with torch.no_grad():\n            # Map input images to latent space + normalize latents:\n            latent = vae.encode(x).latent_dist.sample().mul_(0.18215)\n            # reconstruct:\n            samples = vae.decode(latent / 0.18215).sample # output value is between [-1, 1]\n        samples = torch.clamp(127.5 * samples + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n        \n        # Save samples to disk as individual .png files\n        for i, (sample, rgb_gt) in enumerate(zip(samples, rgb_gts)):\n            index = i * dist.get_world_size() + rank + total\n            Image.fromarray(sample).save(f\"{sample_folder_dir}/{index:06d}.png\")\n            # metric\n            rgb_restored = sample.astype(np.float32) / 255. # rgb_restored value is between [0, 1]\n            psnr = psnr_loss(rgb_restored, rgb_gt)\n            ssim = ssim_loss(rgb_restored, rgb_gt, multichannel=True, data_range=2.0, channel_axis=-1)\n            psnr_val_rgb.append(psnr)\n            ssim_val_rgb.append(ssim)\n        total += global_batch_size\n\n    # ------------------------------------\n    #       Summary\n    # ------------------------------------\n    # Make sure all processes have finished saving their samples\n    dist.barrier()\n    world_size = dist.get_world_size()\n    gather_psnr_val = [None for _ in range(world_size)]\n    gather_ssim_val = [None for _ in range(world_size)]\n    dist.all_gather_object(gather_psnr_val, psnr_val_rgb)\n    dist.all_gather_object(gather_ssim_val, ssim_val_rgb)\n\n    if rank == 0:\n        gather_psnr_val = list(itertools.chain(*gather_psnr_val))\n        gather_ssim_val = list(itertools.chain(*gather_ssim_val))        \n        psnr_val_rgb = sum(gather_psnr_val) / len(gather_psnr_val)\n        ssim_val_rgb = sum(gather_ssim_val) / len(gather_ssim_val)\n        print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb))\n\n        result_file = f\"{sample_folder_dir}_results.txt\"\n        print(\"writing results to {}\".format(result_file))\n        with open(result_file, 'w') as f:\n            print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb), file=f)\n\n        create_npz_from_sample_folder(sample_folder_dir, num_fid_samples)\n        print(\"Done.\")\n    \n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, choices=['imagenet', 'coco'], default='imagenet')\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n    parser.add_argument(\"--sample-dir\", type=str, default=\"reconstructions\")\n    parser.add_argument(\"--per-proc-batch-size\", type=int, default=32)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=4)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/vq_demo.py", "content": "import torch\nimport torch.nn.functional as F\n\nimport os\nimport argparse\nimport numpy as np\nfrom PIL import Image\n\nfrom tokenizer.tokenizer_image.vq_model import VQ_models\nfrom dataset.augmentation import center_crop_arr\n\n\ndef main(args):\n    # Setup PyTorch:\n    torch.manual_seed(args.seed)\n    torch.set_grad_enabled(False)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # create and load model\n    model = VQ_models[args.vq_model](\n        codebook_size=args.codebook_size,\n        codebook_embed_dim=args.codebook_embed_dim)\n    model.to(device)\n    model.eval()\n    checkpoint = torch.load(args.vq_ckpt, map_location=\"cpu\")\n    if \"ema\" in checkpoint:  # ema\n        model_weight = checkpoint[\"ema\"]\n    elif \"model\" in checkpoint:  # ddp\n        model_weight = checkpoint[\"model\"]\n    elif \"state_dict\" in checkpoint:\n        model_weight = checkpoint[\"state_dict\"]\n    else:\n        raise Exception(\"please check model weight\")\n    model.load_state_dict(model_weight)\n    del checkpoint\n\n    # output dir\n    os.makedirs(args.output_dir, exist_ok=True)\n    out_path = args.image_path.replace('.jpg', '_{}.jpg'.format(args.suffix))\n    out_path = out_path.replace('.jpeg', '_{}.jpeg'.format(args.suffix))\n    out_path = out_path.replace('.png', '_{}.png'.format(args.suffix))\n    out_filename = out_path.split('/')[-1]\n    out_path = os.path.join(args.output_dir, out_filename)\n    \n    # load image\n    pil_image = Image.open(args.image_path).convert(\"RGB\")\n    img = center_crop_arr(pil_image, args.image_size)\n    # # preprocess\n    # size_org = img.size\n    # img = img.resize((input_size, input_size))\n    img = np.array(img) / 255.\n    x = 2.0 * img - 1.0 # x value is between [-1, 1]\n    x = torch.tensor(x)\n    x = x.unsqueeze(dim=0)\n    x = torch.einsum('nhwc->nchw', x)\n    x_input = x.float().to(\"cuda\")\n\n    # inference\n    with torch.no_grad():\n        latent, _, [_, _, indices] = model.encode(x_input)\n        output = model.decode_code(indices, latent.shape) # output value is between [-1, 1]\n\n    # postprocess\n    output = F.interpolate(output, size=[args.image_size, args.image_size], mode='bicubic').permute(0, 2, 3, 1)[0]\n    sample = torch.clamp(127.5 * output + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8).numpy()\n\n    # save        \n    Image.fromarray(sample).save(out_path)\n    print(\"Reconstructed image is saved to {}\".format(out_path))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--image-path\", type=str, default=\"assets/example.jpg\")\n    parser.add_argument(\"--output-dir\", type=str, default=\"output_vq_demo\")\n    parser.add_argument(\"--suffix\", type=str, default=\"tokenizer_image\")\n    parser.add_argument(\"--vq-model\", type=str, choices=list(VQ_models.keys()), default=\"VQ-16\")\n    parser.add_argument(\"--vq-ckpt\", type=str, default=None, help=\"ckpt path for vq model\")\n    parser.add_argument(\"--codebook-size\", type=int, default=16384, help=\"codebook size for vector quantization\")\n    parser.add_argument(\"--codebook-embed-dim\", type=int, default=8, help=\"codebook dimension for vector quantization\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 384, 448, 512, 1024], default=512)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/vq_model.py", "content": "# Modified from:\n#   taming-transformers: https://github.com/CompVis/taming-transformers\n#   maskgit: https://github.com/google-research/maskgit\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n@dataclass\nclass ModelArgs:\n    codebook_size: int = 16384\n    codebook_embed_dim: int = 8\n    codebook_l2_norm: bool = True\n    codebook_show_usage: bool = True\n    commit_loss_beta: float = 0.25\n    entropy_loss_ratio: float = 0.0\n    \n    encoder_ch_mult: List[int] = field(default_factory=lambda: [1, 1, 2, 2, 4])\n    decoder_ch_mult: List[int] = field(default_factory=lambda: [1, 1, 2, 2, 4])\n    z_channels: int = 256\n    dropout_p: float = 0.0\n\n\n\nclass VQModel(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.config = config\n        self.encoder = Encoder(ch_mult=config.encoder_ch_mult, z_channels=config.z_channels, dropout=config.dropout_p)\n        self.decoder = Decoder(ch_mult=config.decoder_ch_mult, z_channels=config.z_channels, dropout=config.dropout_p)\n\n        self.quantize = VectorQuantizer(config.codebook_size, config.codebook_embed_dim, \n                                        config.commit_loss_beta, config.entropy_loss_ratio,\n                                        config.codebook_l2_norm, config.codebook_show_usage)\n        self.quant_conv = nn.Conv2d(config.z_channels, config.codebook_embed_dim, 1)\n        self.post_quant_conv = nn.Conv2d(config.codebook_embed_dim, config.z_channels, 1)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b, shape=None, channel_first=True):\n        quant_b = self.quantize.get_codebook_entry(code_b, shape, channel_first)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        return dec, diff\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels=3, ch=128, ch_mult=(1,1,2,2,4), num_res_blocks=2, \n                 norm_type='group', dropout=0.0, resamp_with_conv=True, z_channels=256):\n        super().__init__()\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n\n        # downsampling\n        in_ch_mult = (1,) + tuple(ch_mult)\n        self.conv_blocks = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            conv_block = nn.Module()\n            # res & attn\n            res_block = nn.ModuleList()\n            attn_block = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for _ in range(self.num_res_blocks):\n                res_block.append(ResnetBlock(block_in, block_out, dropout=dropout, norm_type=norm_type))\n                block_in = block_out\n                if i_level == self.num_resolutions - 1:\n                    attn_block.append(AttnBlock(block_in, norm_type))\n            conv_block.res = res_block\n            conv_block.attn = attn_block\n            # downsample\n            if i_level != self.num_resolutions-1:\n                conv_block.downsample = Downsample(block_in, resamp_with_conv)\n            self.conv_blocks.append(conv_block)\n\n        # middle\n        self.mid = nn.ModuleList()\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n        self.mid.append(AttnBlock(block_in, norm_type=norm_type))\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n\n        # end\n        self.norm_out = Normalize(block_in, norm_type)\n        self.conv_out = nn.Conv2d(block_in, z_channels, kernel_size=3, stride=1, padding=1)\n\n\n    def forward(self, x):\n        h = self.conv_in(x)\n        # downsampling\n        for i_level, block in enumerate(self.conv_blocks):\n            for i_block in range(self.num_res_blocks):\n                h = block.res[i_block](h)\n                if len(block.attn) > 0:\n                    h = block.attn[i_block](h)\n            if i_level != self.num_resolutions - 1:\n                h = block.downsample(h)\n        \n        # middle\n        for mid_block in self.mid:\n            h = mid_block(h)\n        \n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, z_channels=256, ch=128, ch_mult=(1,1,2,2,4), num_res_blocks=2, norm_type=\"group\",\n                 dropout=0.0, resamp_with_conv=True, out_channels=3):\n        super().__init__()\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        # z to block_in\n        self.conv_in = nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n\n       # middle\n        self.mid = nn.ModuleList()\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n        self.mid.append(AttnBlock(block_in, norm_type=norm_type))\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n\n        # upsampling\n        self.conv_blocks = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            conv_block = nn.Module()\n            # res & attn\n            res_block = nn.ModuleList()\n            attn_block = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for _ in range(self.num_res_blocks + 1):\n                res_block.append(ResnetBlock(block_in, block_out, dropout=dropout, norm_type=norm_type))\n                block_in = block_out\n                if i_level == self.num_resolutions - 1:\n                    attn_block.append(AttnBlock(block_in, norm_type))\n            conv_block.res = res_block\n            conv_block.attn = attn_block\n            # downsample\n            if i_level != 0:\n                conv_block.upsample = Upsample(block_in, resamp_with_conv)\n            self.conv_blocks.append(conv_block)\n\n        # end\n        self.norm_out = Normalize(block_in, norm_type)\n        self.conv_out = nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n\n    @property\n    def last_layer(self):\n        return self.conv_out.weight\n    \n    def forward(self, z):\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        for mid_block in self.mid:\n            h = mid_block(h)\n        \n        # upsampling\n        for i_level, block in enumerate(self.conv_blocks):\n            for i_block in range(self.num_res_blocks + 1):\n                h = block.res[i_block](h)\n                if len(block.attn) > 0:\n                    h = block.attn[i_block](h)\n            if i_level != self.num_resolutions - 1:\n                h = block.upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass VectorQuantizer(nn.Module):\n    def __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage):\n        super().__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n        self.entropy_loss_ratio = entropy_loss_ratio\n        self.l2_norm = l2_norm\n        self.show_usage = show_usage\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n        if self.l2_norm:\n            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=-1)\n        if self.show_usage:\n            self.register_buffer(\"codebook_used\", nn.Parameter(torch.zeros(65536)))\n\n    \n    def forward(self, z):\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = torch.einsum('b c h w -> b h w c', z).contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        if self.l2_norm:\n            z = F.normalize(z, p=2, dim=-1)\n            z_flattened = F.normalize(z_flattened, p=2, dim=-1)\n            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n        else:\n            embedding = self.embedding.weight\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(embedding**2, dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, torch.einsum('n d -> d n', embedding))\n\n        min_encoding_indices = torch.argmin(d, dim=1)\n        z_q = embedding[min_encoding_indices].view(z.shape)\n        perplexity = None\n        min_encodings = None\n        vq_loss = None\n        commit_loss = None\n        entropy_loss = None\n        codebook_usage = 0\n\n        if self.show_usage and self.training:\n            cur_len = min_encoding_indices.shape[0]\n            self.codebook_used[:-cur_len] = self.codebook_used[cur_len:].clone()\n            self.codebook_used[-cur_len:] = min_encoding_indices\n            codebook_usage = len(torch.unique(self.codebook_used)) / self.n_e\n\n        # compute loss for embedding\n        if self.training:\n            vq_loss = torch.mean((z_q - z.detach()) ** 2) \n            commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2) \n            entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        z_q = torch.einsum('b h w c -> b c h w', z_q)\n\n        return z_q, (vq_loss, commit_loss, entropy_loss, codebook_usage), (perplexity, min_encodings, min_encoding_indices)\n\n    def get_codebook_entry(self, indices, shape=None, channel_first=True):\n        # shape = (batch, channel, height, width) if channel_first else (batch, height, width, channel)\n        if self.l2_norm:\n            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n        else:\n            embedding = self.embedding.weight\n        z_q = embedding[indices]  # (b*h*w, c)\n\n        if shape is not None:\n            if channel_first:\n                z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[1])\n                # reshape back to match original input shape\n                z_q = z_q.permute(0, 3, 1, 2).contiguous()\n            else:\n                z_q = z_q.view(shape)\n        return z_q\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None, conv_shortcut=False, dropout=0.0, norm_type='group'):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels, norm_type)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.norm2 = Normalize(out_channels, norm_type)\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            else:\n                self.nin_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n        return x+h\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels, norm_type='group'):\n        super().__init__()\n        self.norm = Normalize(in_channels, norm_type)\n        self.q = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.k = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.v = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.proj_out = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w)\n        q = q.permute(0,2,1)   # b,hw,c\n        k = k.reshape(b,c,h*w) # b,c,hw\n        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = F.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, norm_type='group'):\n    assert norm_type in ['group', 'batch']\n    if norm_type == 'group':\n        return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n    elif norm_type == 'batch':\n        return nn.SyncBatchNorm(in_channels)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = F.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = F.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\ndef compute_entropy_loss(affinity, loss_type=\"softmax\", temperature=0.01):\n    flat_affinity = affinity.reshape(-1, affinity.shape[-1])\n    flat_affinity /= temperature\n    probs = F.softmax(flat_affinity, dim=-1)\n    log_probs = F.log_softmax(flat_affinity + 1e-5, dim=-1)\n    if loss_type == \"softmax\":\n        target_probs = probs\n    else:\n        raise ValueError(\"Entropy loss {} not supported\".format(loss_type))\n    avg_probs = torch.mean(target_probs, dim=0)\n    avg_entropy = - torch.sum(avg_probs * torch.log(avg_probs + 1e-5))\n    sample_entropy = - torch.mean(torch.sum(target_probs * log_probs, dim=-1))\n    loss = sample_entropy - avg_entropy\n    return loss\n\n\n#################################################################################\n#                              VQ Model Configs                                 #\n#################################################################################\ndef VQ_8(**kwargs):\n    return VQModel(ModelArgs(encoder_ch_mult=[1, 2, 2, 4], decoder_ch_mult=[1, 2, 2, 4], **kwargs))\n\ndef VQ_16(**kwargs):\n    return VQModel(ModelArgs(encoder_ch_mult=[1, 1, 2, 2, 4], decoder_ch_mult=[1, 1, 2, 2, 4], **kwargs))\n\nVQ_models = {'VQ-16': VQ_16, 'VQ-8': VQ_8}"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/lpips.py", "content": "\"\"\"Stripped version of https://github.com/richzhang/PerceptualSimilarity/tree/master/models\"\"\"\n\nimport os, hashlib\nimport requests\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom collections import namedtuple\n\nURL_MAP = {\n    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n}\n\nCKPT_MAP = {\n    \"vgg_lpips\": \"vgg.pth\"\n}\n\nMD5_MAP = {\n    \"vgg_lpips\": \"d507d7349b931f0638a25a48a722f98a\"\n}\n\ndef download(url, local_path, chunk_size=1024):\n    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n    with requests.get(url, stream=True) as r:\n        total_size = int(r.headers.get(\"content-length\", 0))\n        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n            with open(local_path, \"wb\") as f:\n                for data in r.iter_content(chunk_size=chunk_size):\n                    if data:\n                        f.write(data)\n                        pbar.update(chunk_size)\n\n\ndef md5_hash(path):\n    with open(path, \"rb\") as f:\n        content = f.read()\n    return hashlib.md5(content).hexdigest()\n\n\ndef get_ckpt_path(name, root, check=False):\n    assert name in URL_MAP\n    path = os.path.join(root, CKPT_MAP[name])\n    if not os.path.exists(path) or (check and not md5_hash(path) == MD5_MAP[name]):\n        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n        download(URL_MAP[name], path)\n        md5 = md5_hash(path)\n        assert md5 == MD5_MAP[name], md5\n    return path\n\n\nclass LPIPS(nn.Module):\n    # Learned perceptual metric\n    def __init__(self, use_dropout=True):\n        super().__init__()\n        self.scaling_layer = ScalingLayer()\n        self.chns = [64, 128, 256, 512, 512]  # vg16 features\n        self.net = vgg16(pretrained=True, requires_grad=False)\n        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n        self.load_from_pretrained()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def load_from_pretrained(self, name=\"vgg_lpips\"):\n        ckpt = get_ckpt_path(name, os.path.join(os.path.dirname(os.path.abspath(__file__)), \"cache\"))\n        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n        print(\"loaded pretrained LPIPS loss from {}\".format(ckpt))\n\n    @classmethod\n    def from_pretrained(cls, name=\"vgg_lpips\"):\n        if name != \"vgg_lpips\":\n            raise NotImplementedError\n        model = cls()\n        ckpt = get_ckpt_path(name, os.path.join(os.path.dirname(os.path.abspath(__file__)), \"cache\"))\n        model.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n        return model\n\n    def forward(self, input, target):\n        in0_input, in1_input = (self.scaling_layer(input), self.scaling_layer(target))\n        outs0, outs1 = self.net(in0_input), self.net(in1_input)\n        feats0, feats1, diffs = {}, {}, {}\n        lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n        for kk in range(len(self.chns)):\n            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n\n        res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=True) for kk in range(len(self.chns))]\n        val = res[0]\n        for l in range(1, len(self.chns)):\n            val += res[l]\n        return val\n\n\nclass ScalingLayer(nn.Module):\n    def __init__(self):\n        super(ScalingLayer, self).__init__()\n        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n\n    def forward(self, inp):\n        return (inp - self.shift) / self.scale\n\n\nclass NetLinLayer(nn.Module):\n    \"\"\" A single linear layer which does a 1x1 conv \"\"\"\n    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n        super(NetLinLayer, self).__init__()\n        layers = [nn.Dropout(), ] if (use_dropout) else []\n        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n        self.model = nn.Sequential(*layers)\n\n\nclass vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n        return out\n\n\ndef normalize_tensor(x,eps=1e-10):\n    norm_factor = torch.sqrt(torch.sum(x**2,dim=1,keepdim=True))\n    return x/(norm_factor+eps)\n\n\ndef spatial_average(x, keepdim=True):\n    return x.mean([2,3],keepdim=keepdim)"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/discriminator_patchgan.py", "content": "# Modified from:\n#   taming-transformers:  https://github.com/CompVis/taming-transformers\nimport functools\nimport torch\nimport torch.nn as nn\n\n\nclass NLayerDiscriminator(nn.Module):\n    \"\"\"Defines a PatchGAN discriminator as in Pix2Pix\n        --> see https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n    \"\"\"\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, use_actnorm=False):\n        \"\"\"Construct a PatchGAN discriminator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        \"\"\"\n        super(NLayerDiscriminator, self).__init__()\n        if not use_actnorm:\n            norm_layer = nn.BatchNorm2d\n        else:\n            norm_layer = ActNorm\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [\n            nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.main = nn.Sequential(*sequence)\n\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):    \n        if isinstance(module, nn.Conv2d):\n            nn.init.normal_(module.weight.data, 0.0, 0.02)\n        elif isinstance(module, nn.BatchNorm2d):\n            nn.init.normal_(module.weight.data, 1.0, 0.02)\n            nn.init.constant_(module.bias.data, 0)\n\n    def forward(self, input):\n        \"\"\"Standard forward.\"\"\"\n        return self.main(input)\n\n\nclass ActNorm(nn.Module):\n    def __init__(self, num_features, logdet=False, affine=True,\n                 allow_reverse_init=False):\n        assert affine\n        super().__init__()\n        self.logdet = logdet\n        self.loc = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n        self.scale = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.allow_reverse_init = allow_reverse_init\n\n        self.register_buffer('initialized', torch.tensor(0, dtype=torch.uint8))\n\n    def initialize(self, input):\n        with torch.no_grad():\n            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n            mean = (\n                flatten.mean(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .unsqueeze(3)\n                .permute(1, 0, 2, 3)\n            )\n            std = (\n                flatten.std(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .unsqueeze(3)\n                .permute(1, 0, 2, 3)\n            )\n\n            self.loc.data.copy_(-mean)\n            self.scale.data.copy_(1 / (std + 1e-6))\n\n    def forward(self, input, reverse=False):\n        if reverse:\n            return self.reverse(input)\n        if len(input.shape) == 2:\n            input = input[:,:,None,None]\n            squeeze = True\n        else:\n            squeeze = False\n\n        _, _, height, width = input.shape\n\n        if self.training and self.initialized.item() == 0:\n            self.initialize(input)\n            self.initialized.fill_(1)\n\n        h = self.scale * (input + self.loc)\n\n        if squeeze:\n            h = h.squeeze(-1).squeeze(-1)\n\n        if self.logdet:\n            log_abs = torch.log(torch.abs(self.scale))\n            logdet = height*width*torch.sum(log_abs)\n            logdet = logdet * torch.ones(input.shape[0]).to(input)\n            return h, logdet\n\n        return h\n\n    def reverse(self, output):\n        if self.training and self.initialized.item() == 0:\n            if not self.allow_reverse_init:\n                raise RuntimeError(\n                    \"Initializing ActNorm in reverse direction is \"\n                    \"disabled by default. Use allow_reverse_init=True to enable.\"\n                )\n            else:\n                self.initialize(output)\n                self.initialized.fill_(1)\n\n        if len(output.shape) == 2:\n            output = output[:,:,None,None]\n            squeeze = True\n        else:\n            squeeze = False\n\n        h = output / self.scale - self.loc\n\n        if squeeze:\n            h = h.squeeze(-1).squeeze(-1)\n        return h"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/vq_loss.py", "content": "# Modified from:\n#   taming-transformers:  https://github.com/CompVis/taming-transformers\n#   muse-maskgit-pytorch: https://github.com/lucidrains/muse-maskgit-pytorch/blob/main/muse_maskgit_pytorch/vqgan_vae.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tokenizer.tokenizer_image.lpips import LPIPS\nfrom tokenizer.tokenizer_image.discriminator_patchgan import NLayerDiscriminator as PatchGANDiscriminator\nfrom tokenizer.tokenizer_image.discriminator_stylegan import Discriminator as StyleGANDiscriminator\n\n\n\ndef hinge_d_loss(logits_real, logits_fake):\n    loss_real = torch.mean(F.relu(1. - logits_real))\n    loss_fake = torch.mean(F.relu(1. + logits_fake))\n    d_loss = 0.5 * (loss_real + loss_fake)\n    return d_loss\n\n\ndef vanilla_d_loss(logits_real, logits_fake):\n    loss_real = torch.mean(F.softplus(-logits_real))\n    loss_fake = torch.mean(F.softplus(logits_fake))\n    d_loss = 0.5 * (loss_real + loss_fake)\n    return d_loss\n\n\ndef non_saturating_d_loss(logits_real, logits_fake):\n    loss_real = torch.mean(F.binary_cross_entropy_with_logits(torch.ones_like(logits_real),  logits_real))\n    loss_fake = torch.mean(F.binary_cross_entropy_with_logits(torch.zeros_like(logits_fake), logits_fake))\n    d_loss = 0.5 * (loss_real + loss_fake)\n    return d_loss\n\n\ndef hinge_gen_loss(logit_fake):\n    return -torch.mean(logit_fake)\n\n\ndef non_saturating_gen_loss(logit_fake):\n    return torch.mean(F.binary_cross_entropy_with_logits(torch.ones_like(logit_fake),  logit_fake))\n\n\ndef adopt_weight(weight, global_step, threshold=0, value=0.):\n    if global_step < threshold:\n        weight = value\n    return weight\n\n\nclass VQLoss(nn.Module):\n    def __init__(self, disc_start, disc_loss=\"hinge\", disc_dim=64, disc_type='patchgan', image_size=256,\n                 disc_num_layers=3, disc_in_channels=3, disc_weight=1.0, disc_adaptive_weight = False,\n                 gen_adv_loss='hinge', reconstruction_loss='l2', reconstruction_weight=1.0, \n                 codebook_weight=1.0, perceptual_weight=1.0, \n    ):\n        super().__init__()\n        # discriminator loss\n        assert disc_type in [\"patchgan\", \"stylegan\"]\n        assert disc_loss in [\"hinge\", \"vanilla\", \"non-saturating\"]\n        if disc_type == \"patchgan\":\n            self.discriminator = PatchGANDiscriminator(\n                input_nc=disc_in_channels, \n                n_layers=disc_num_layers,\n                ndf=disc_dim,\n            )\n        elif disc_type == \"stylegan\":\n            self.discriminator = StyleGANDiscriminator(\n                input_nc=disc_in_channels, \n                image_size=image_size,\n            )\n        else:\n            raise ValueError(f\"Unknown GAN discriminator type '{disc_type}'.\")\n        if disc_loss == \"hinge\":\n            self.disc_loss = hinge_d_loss\n        elif disc_loss == \"vanilla\":\n            self.disc_loss = vanilla_d_loss\n        elif disc_loss == \"non-saturating\":\n            self.disc_loss = non_saturating_d_loss\n        else:\n            raise ValueError(f\"Unknown GAN discriminator loss '{disc_loss}'.\")\n        self.discriminator_iter_start = disc_start\n        self.disc_weight = disc_weight\n        self.disc_adaptive_weight = disc_adaptive_weight\n\n        assert gen_adv_loss in [\"hinge\", \"non-saturating\"]\n        # gen_adv_loss\n        if gen_adv_loss == \"hinge\":\n            self.gen_adv_loss = hinge_gen_loss\n        elif gen_adv_loss == \"non-saturating\":\n            self.gen_adv_loss = non_saturating_gen_loss\n        else:\n            raise ValueError(f\"Unknown GAN generator loss '{gen_adv_loss}'.\")\n\n        # perceptual loss\n        self.perceptual_loss = LPIPS().eval()\n        self.perceptual_weight = perceptual_weight\n\n        # reconstruction loss\n        if reconstruction_loss == \"l1\":\n            self.rec_loss = F.l1_loss\n        elif reconstruction_loss == \"l2\":\n            self.rec_loss = F.mse_loss\n        else:\n            raise ValueError(f\"Unknown rec loss '{reconstruction_loss}'.\")\n        self.rec_weight = reconstruction_weight\n\n        # codebook loss\n        self.codebook_weight = codebook_weight\n\n    def calculate_adaptive_weight(self, nll_loss, g_loss, last_layer):\n        nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]\n        g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=True)[0]\n\n        d_weight = torch.norm(nll_grads) / (torch.norm(g_grads) + 1e-4)\n        d_weight = torch.clamp(d_weight, 0.0, 1e4).detach()\n        return d_weight.detach()\n\n    def forward(self, codebook_loss, inputs, reconstructions, optimizer_idx, global_step, last_layer=None, \n                logger=None, log_every=100):\n        # generator update\n        if optimizer_idx == 0:\n            # reconstruction loss\n            rec_loss = self.rec_loss(inputs.contiguous(), reconstructions.contiguous())\n\n            # perceptual loss\n            p_loss = self.perceptual_loss(inputs.contiguous(), reconstructions.contiguous())\n            p_loss = torch.mean(p_loss)\n\n            # discriminator loss\n            logits_fake = self.discriminator(reconstructions.contiguous())\n            generator_adv_loss = self.gen_adv_loss(logits_fake)\n            \n            if self.disc_adaptive_weight:\n                null_loss = self.rec_weight * rec_loss + self.perceptual_weight * p_loss\n                disc_adaptive_weight = self.calculate_adaptive_weight(null_loss, generator_adv_loss, last_layer=last_layer)\n            else:\n                disc_adaptive_weight = 1\n            disc_weight = adopt_weight(self.disc_weight, global_step, threshold=self.discriminator_iter_start)\n            \n            loss = self.rec_weight * rec_loss + \\\n                self.perceptual_weight * p_loss + \\\n                disc_adaptive_weight * disc_weight * generator_adv_loss + \\\n                codebook_loss[0] + codebook_loss[1] + codebook_loss[2]\n            \n            if global_step % log_every == 0:\n                rec_loss = self.rec_weight * rec_loss\n                p_loss = self.perceptual_weight * p_loss\n                generator_adv_loss = disc_adaptive_weight * disc_weight * generator_adv_loss\n                logger.info(f\"(Generator) rec_loss: {rec_loss:.4f}, perceptual_loss: {p_loss:.4f}, \"\n                            f\"vq_loss: {codebook_loss[0]:.4f}, commit_loss: {codebook_loss[1]:.4f}, entropy_loss: {codebook_loss[2]:.4f}, \"\n                            f\"codebook_usage: {codebook_loss[3]:.4f}, generator_adv_loss: {generator_adv_loss:.4f}, \"\n                            f\"disc_adaptive_weight: {disc_adaptive_weight:.4f}, disc_weight: {disc_weight:.4f}\")\n            return loss\n\n        # discriminator update\n        if optimizer_idx == 1:\n            logits_real = self.discriminator(inputs.contiguous().detach())\n            logits_fake = self.discriminator(reconstructions.contiguous().detach())\n\n            disc_weight = adopt_weight(self.disc_weight, global_step, threshold=self.discriminator_iter_start)\n            d_adversarial_loss = disc_weight * self.disc_loss(logits_real, logits_fake)\n            \n            if global_step % log_every == 0:\n                logits_real = logits_real.detach().mean()\n                logits_fake = logits_fake.detach().mean()\n                logger.info(f\"(Discriminator) \" \n                            f\"discriminator_adv_loss: {d_adversarial_loss:.4f}, disc_weight: {disc_weight:.4f}, \"\n                            f\"logits_real: {logits_real:.4f}, logits_fake: {logits_fake:.4f}\")\n            return d_adversarial_loss"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/discriminator.py", "content": "# Modified from:\n#   taming-transformers:  https://github.com/CompVis/taming-transformers\n#   stylegan2-pytorch:    https://github.com/rosinality/stylegan2-pytorch/blob/master/model.py\n#   maskgit: https://github.com/google-research/maskgit/blob/main/maskgit/nets/discriminator.py\nimport functools\nimport math\nimport torch\nimport torch.nn as nn\ntry:\n    from kornia.filters import filter2d\nexcept:\n    pass\n\n#################################################################################\n#                                    PatchGAN                                   #\n#################################################################################\nclass PatchGANDiscriminator(nn.Module):\n    \"\"\"Defines a PatchGAN discriminator as in Pix2Pix\n        --> see https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n    \"\"\"\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, use_actnorm=False):\n        \"\"\"Construct a PatchGAN discriminator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        \"\"\"\n        super(PatchGANDiscriminator, self).__init__()\n        if not use_actnorm:\n            norm_layer = nn.BatchNorm2d\n        else:\n            norm_layer = ActNorm\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [\n            nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.main = nn.Sequential(*sequence)\n\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):    \n        if isinstance(module, nn.Conv2d):\n            nn.init.normal_(module.weight.data, 0.0, 0.02)\n        elif isinstance(module, nn.BatchNorm2d):\n            nn.init.normal_(module.weight.data, 1.0, 0.02)\n            nn.init.constant_(module.bias.data, 0)\n\n    def forward(self, input):\n        \"\"\"Standard forward.\"\"\"\n        return self.main(input)\n\n\nclass ActNorm(nn.Module):\n    def __init__(self, num_features, logdet=False, affine=True,\n                 allow_reverse_init=False):\n        assert affine\n        super().__init__()\n        self.logdet = logdet\n        self.loc = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n        self.scale = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.allow_reverse_init = allow_reverse_init\n\n        self.register_buffer('initialized', torch.tensor(0, dtype=torch.uint8))\n\n    def initialize(self, input):\n        with torch.no_grad():\n            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n            mean = (\n                flatten.mean(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .unsqueeze(3)\n                .permute(1, 0, 2, 3)\n            )\n            std = (\n                flatten.std(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .unsqueeze(3)\n                .permute(1, 0, 2, 3)\n            )\n\n            self.loc.data.copy_(-mean)\n            self.scale.data.copy_(1 / (std + 1e-6))\n\n    def forward(self, input, reverse=False):\n        if reverse:\n            return self.reverse(input)\n        if len(input.shape) == 2:\n            input = input[:,:,None,None]\n            squeeze = True\n        else:\n            squeeze = False\n\n        _, _, height, width = input.shape\n\n        if self.training and self.initialized.item() == 0:\n            self.initialize(input)\n            self.initialized.fill_(1)\n\n        h = self.scale * (input + self.loc)\n\n        if squeeze:\n            h = h.squeeze(-1).squeeze(-1)\n\n        if self.logdet:\n            log_abs = torch.log(torch.abs(self.scale))\n            logdet = height*width*torch.sum(log_abs)\n            logdet = logdet * torch.ones(input.shape[0]).to(input)\n            return h, logdet\n\n        return h\n\n    def reverse(self, output):\n        if self.training and self.initialized.item() == 0:\n            if not self.allow_reverse_init:\n                raise RuntimeError(\n                    \"Initializing ActNorm in reverse direction is \"\n                    \"disabled by default. Use allow_reverse_init=True to enable.\"\n                )\n            else:\n                self.initialize(output)\n                self.initialized.fill_(1)\n\n        if len(output.shape) == 2:\n            output = output[:,:,None,None]\n            squeeze = True\n        else:\n            squeeze = False\n\n        h = output / self.scale - self.loc\n\n        if squeeze:\n            h = h.squeeze(-1).squeeze(-1)\n        return h\n\n\n\n#################################################################################\n#                                    StyleGAN                                   #\n#################################################################################\nclass StyleGANDiscriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, channel_multiplier=1, image_size=256):\n        super().__init__()\n        channels = {\n            4: 512,\n            8: 512,\n            16: 512,\n            32: 512,\n            64: 256 * channel_multiplier,\n            128: 128 * channel_multiplier,\n            256: 64 * channel_multiplier,\n            512: 32 * channel_multiplier,\n            1024: 16 * channel_multiplier,\n        }\n        \n        log_size = int(math.log(image_size, 2))\n        in_channel = channels[image_size]\n\n        blocks = [nn.Conv2d(input_nc, in_channel, 3, padding=1), leaky_relu()]\n        for i in range(log_size, 2, -1):\n            out_channel = channels[2 ** (i - 1)]\n            blocks.append(DiscriminatorBlock(in_channel, out_channel))\n            in_channel = out_channel\n        self.blocks = nn.ModuleList(blocks)\n\n        self.final_conv = nn.Sequential(\n            nn.Conv2d(in_channel, channels[4], 3, padding=1),\n            leaky_relu(),\n        )\n        self.final_linear = nn.Sequential(\n            nn.Linear(channels[4] * 4 * 4, channels[4]),\n            leaky_relu(),\n            nn.Linear(channels[4], 1)\n        )\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        x = self.final_conv(x)\n        x = x.view(x.shape[0], -1)\n        x = self.final_linear(x)\n        return x\n\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, input_channels, filters, downsample=True):\n        super().__init__()\n        self.conv_res = nn.Conv2d(input_channels, filters, 1, stride = (2 if downsample else 1))\n\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, filters, 3, padding=1),\n            leaky_relu(),\n            nn.Conv2d(filters, filters, 3, padding=1),\n            leaky_relu()\n        )\n\n        self.downsample = nn.Sequential(\n            Blur(),\n            nn.Conv2d(filters, filters, 3, padding = 1, stride = 2)\n        ) if downsample else None\n\n    def forward(self, x):\n        res = self.conv_res(x)\n        x = self.net(x)\n        if exists(self.downsample):\n            x = self.downsample(x)\n        x = (x + res) * (1 / math.sqrt(2))\n        return x\n\n\nclass Blur(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = torch.Tensor([1, 2, 1])\n        self.register_buffer('f', f)\n    \n    def forward(self, x):\n        f = self.f\n        f = f[None, None, :] * f [None, :, None]\n        return filter2d(x, f, normalized=True)\n\n\ndef leaky_relu(p=0.2):\n    return nn.LeakyReLU(p, inplace=True)\n\n\ndef exists(val):\n    return val is not None"}
{"type": "source_file", "path": "llamagen_tokenizer/vqgan/layer.py", "content": "# pytorch_diffusion + derived encoder decoder\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w)\n        q = q.permute(0,2,1)   # b,hw,c\n        k = k.reshape(b,c,h*w) # b,c,hw\n        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, **ignore_kwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = AttnBlock(block_in)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n\n    def forward(self, x):\n        #assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)\n\n        # timestep embedding\n        temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = AttnBlock(block_in)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\n"}
{"type": "source_file", "path": "models/cobra/backbones/vision/dinov2_vit.py", "content": "\"\"\"\ndinov2_vit.py\n\"\"\"\nfrom models.cobra.backbones.vision.base_vision import TimmViTBackbone\n\n# Registry =>> Supported DINOv2 Vision Backbones (from TIMM) =>> Note:: Using DINOv2 w/ Registers!\n#   => Reference: https://arxiv.org/abs/2309.16588\nDINOv2_VISION_BACKBONES = {\"dinov2-vit-l\": \"vit_large_patch14_reg4_dinov2.lvd142m\"}\n\n\nclass DinoV2ViTBackbone(TimmViTBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(\n            vision_backbone_id,\n            DINOv2_VISION_BACKBONES[vision_backbone_id],\n            image_resize_strategy,\n            default_image_size=default_image_size,\n        )\n"}
{"type": "source_file", "path": "llamagen_tokenizer/vqgan/reconstruction_vqgan_ddp.py", "content": "import torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.distributed as dist\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\nimport numpy as np\nimport itertools\nimport argparse\nimport random\n\nfrom skimage.metrics import peak_signal_noise_ratio as psnr_loss\nfrom skimage.metrics import structural_similarity as ssim_loss\nfrom omegaconf import OmegaConf\nfrom tokenizer.vqgan.model import VQModel\nfrom tokenizer.vqgan.model import VQGAN_FROM_TAMING\n\n\nclass SingleFolderDataset(Dataset):\n    def __init__(self, directory, transform=None):\n        super().__init__()\n        self.directory = directory\n        self.transform = transform\n        self.image_paths = [os.path.join(directory, file_name) for file_name in os.listdir(directory)\n                            if os.path.isfile(os.path.join(directory, file_name))]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(0)\n\n\ndef create_npz_from_sample_folder(sample_dir, num=50_000):\n    \"\"\"\n    Builds a single .npz file from a folder of .png samples.\n    \"\"\"\n    samples = []\n    for i in tqdm(range(num), desc=\"Building .npz file from samples\"):\n        sample_pil = Image.open(f\"{sample_dir}/{i:06d}.png\")\n        sample_np = np.asarray(sample_pil).astype(np.uint8)\n        samples.append(sample_np)\n\n    random.shuffle(samples) # This is very important for IS(Inception Score) !!!\n    samples = np.stack(samples)\n    assert samples.shape == (num, samples.shape[1], samples.shape[2], 3)\n    npz_path = f\"{sample_dir}.npz\"\n    np.savez(npz_path, arr_0=samples)\n    print(f\"Saved .npz file to {npz_path} [shape={samples.shape}].\")\n    return npz_path\n\n\ndef center_crop_arr(pil_image, image_size):\n    \"\"\"\n    Center cropping implementation from ADM.\n    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n    \"\"\"\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n\n\ndef main(args):\n    # Setup PyTorch:\n    assert torch.cuda.is_available(), \"Sampling with DDP requires at least one GPU. sample.py supports CPU-only usage\"\n    torch.set_grad_enabled(False)\n\n    # Setup DDP:\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # create and load vqgan\n    cfg, ckpt = VQGAN_FROM_TAMING[args.vqgan]\n    config = OmegaConf.load(cfg)\n    vq_model = VQModel(**config.model.get(\"params\", dict())).to(device)\n    vq_model.init_from_ckpt(ckpt, logging=False)\n    vq_model.eval()\n\n    # Create folder to save samples:\n    folder_name = f\"{args.vqgan}-{args.dataset}-size-{args.image_size}-seed-{args.global_seed}\"\n    sample_folder_dir = f\"{args.sample_dir}/{folder_name}\"\n    if rank == 0:\n        os.makedirs(sample_folder_dir, exist_ok=True)\n        print(f\"Saving .png samples at {sample_folder_dir}\")\n    dist.barrier()\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n\n    if args.dataset == 'imagenet':\n        dataset = ImageFolder(args.data_path, transform=transform)\n        num_fid_samples = 50000\n    elif args.dataset == 'coco':\n        dataset = SingleFolderDataset(args.data_path, transform=transform)\n        num_fid_samples = 5000\n    else:\n        raise Exception(\"please check dataset\")\n    \n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=False,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=args.per_proc_batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=False\n    )    \n\n    # Figure out how many samples we need to generate on each GPU and how many iterations we need to run:\n    n = args.per_proc_batch_size\n    global_batch_size = n * dist.get_world_size()\n    \n    psnr_val_rgb = []\n    ssim_val_rgb = []\n    loader = tqdm(loader) if rank == 0 else loader\n    total = 0\n    for x, _ in loader:\n        rgb_gts = x\n        rgb_gts = (rgb_gts.permute(0, 2, 3, 1).to(\"cpu\").numpy() + 1.0) / 2.0 # rgb_gt value is between [0, 1]\n        x = x.to(device)\n        with torch.no_grad():\n            latent, _, [_, _, indices] = vq_model.encode(x)\n            samples = vq_model.decode_code(indices, latent.shape) # output value is between [-1, 1]\n        samples = torch.clamp(127.5 * samples + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n        \n        # Save samples to disk as individual .png files\n        for i, (sample, rgb_gt) in enumerate(zip(samples, rgb_gts)):\n            index = i * dist.get_world_size() + rank + total\n            Image.fromarray(sample).save(f\"{sample_folder_dir}/{index:06d}.png\")\n            # metric\n            rgb_restored = sample.astype(np.float32) / 255. # rgb_restored value is between [0, 1]\n            psnr = psnr_loss(rgb_restored, rgb_gt)\n            ssim = ssim_loss(rgb_restored, rgb_gt, multichannel=True, data_range=2.0, channel_axis=-1)\n            psnr_val_rgb.append(psnr)\n            ssim_val_rgb.append(ssim)\n        total += global_batch_size\n\n    # ------------------------------------\n    #       Summary\n    # ------------------------------------\n    # Make sure all processes have finished saving their samples\n    dist.barrier()\n    world_size = dist.get_world_size()\n    gather_psnr_val = [None for _ in range(world_size)]\n    gather_ssim_val = [None for _ in range(world_size)]\n    dist.all_gather_object(gather_psnr_val, psnr_val_rgb)\n    dist.all_gather_object(gather_ssim_val, ssim_val_rgb)\n\n    if rank == 0:\n        gather_psnr_val = list(itertools.chain(*gather_psnr_val))\n        gather_ssim_val = list(itertools.chain(*gather_ssim_val))        \n        psnr_val_rgb = sum(gather_psnr_val) / len(gather_psnr_val)\n        ssim_val_rgb = sum(gather_ssim_val) / len(gather_ssim_val)\n        print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb))\n\n        result_file = f\"{sample_folder_dir}_results.txt\"\n        print(\"writing results to {}\".format(result_file))\n        with open(result_file, 'w') as f:\n            print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb), file=f)\n\n        create_npz_from_sample_folder(sample_folder_dir, num_fid_samples)\n        print(\"Done.\")\n    \n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, choices=['imagenet', 'coco'], default='imagenet')\n    parser.add_argument(\"--vqgan\", type=str, choices=list(VQGAN_FROM_TAMING.keys()), default=\"vqgan_imagenet_f16_16384\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n    parser.add_argument(\"--sample-dir\", type=str, default=\"reconstructions\")\n    parser.add_argument(\"--per-proc-batch-size\", type=int, default=32)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=4)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "llamagen_tokenizer/vqgan/model.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tokenizer.vqgan.layer import Encoder, Decoder\nfrom tokenizer.vqgan.quantize import VectorQuantizer2 as VectorQuantizer\n\n\nVQGAN_FROM_TAMING = {\n    'vqgan_imagenet_f16_1024': (\n        'tokenizer/vqgan/configs/vqgan_imagenet_f16_1024.yaml',\n        'pretrained_models/vqgan_imagenet_f16_1024/ckpts/last.pth'),\n    'vqgan_imagenet_f16_16384': (\n        'tokenizer/vqgan/configs/vqgan_imagenet_f16_16384.yaml', \n        'pretrained_models/vqgan_imagenet_f16_16384/ckpts/last.pth'),\n    'vqgan_openimage_f8_256': (\n        'tokenizer/vqgan/configs/vqgan_openimage_f8_256.yaml', \n        'pretrained_models/vq-f8-n256/model.pth'),\n    'vqgan_openimage_f8_16384': (\n        'tokenizer/vqgan/configs/vqgan_openimage_f8_16384.yaml',\n        'pretrained_models/vq-f8/model.pth'),\n}\n\nclass VQModel(nn.Module):\n    def __init__(self,\n                 ddconfig,\n                 n_embed,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=\"image\",\n                 colorize_nlabels=None,\n                 monitor=None,\n                 remap=None,\n                 sane_index_shape=False,  # tell vector quantizer to return indices as bhw\n                 **kwargs,\n                 ):\n        super().__init__()\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n                                        remap=remap, sane_index_shape=sane_index_shape)\n        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n        self.image_key = image_key\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n\n    def init_from_ckpt(self, path, ignore_keys=list(), logging=True):\n        model_weight = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n        keys = list(model_weight.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del model_weight[k]\n        missing, unexpected = self.load_state_dict(model_weight, strict=False)\n        if logging:\n            print(f\"Restored from {path}\")\n            print(f\"Missing Keys in State Dict: {missing}\")\n            print(f\"Unexpected Keys in State Dict: {unexpected}\")\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b, shape, channel_first=True):\n        quant_b = self.quantize.get_codebook_entry(code_b, shape, channel_first)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        return dec, diff\n"}
{"type": "source_file", "path": "models/cobra/backbones/__init__.py", "content": ""}
{"type": "source_file", "path": "models/cobra/backbones/vision/in1k_vit.py", "content": "\"\"\"\nin1k_vit.py\n\nVision Transformers trained / finetuned on ImageNet (ImageNet-21K =>> ImageNet-1K)\n\"\"\"\nfrom models.cobra.backbones.vision.base_vision import TimmViTBackbone\n\n# Registry =>> Supported Vision Backbones (from TIMM)\nIN1K_VISION_BACKBONES = {\n    \"in1k-vit-l\": \"vit_large_patch16_224.augreg_in21k_ft_in1k\",\n}\n\n\nclass IN1KViTBackbone(TimmViTBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(\n            vision_backbone_id,\n            IN1K_VISION_BACKBONES[vision_backbone_id],\n            image_resize_strategy,\n            default_image_size=default_image_size,\n        )\n"}
{"type": "source_file", "path": "models/cobra/__init__.py", "content": "\n"}
{"type": "source_file", "path": "models/cobra/backbones/vision/clip_vit.py", "content": "\"\"\"\nclip_vit.py\n\"\"\"\nfrom models.cobra.backbones.vision.base_vision import TimmViTBackbone\n\n# Registry =>> Supported CLIP Vision Backbones (from TIMM)\nCLIP_VISION_BACKBONES = {\n    \"clip-vit-b\": \"vit_base_patch16_clip_224.openai\",\n    \"clip-vit-l\": \"vit_large_patch14_clip_224.openai\",\n    \"clip-vit-l-336px\": \"vit_large_patch14_clip_336.openai\",\n}\n\n\n# [IMPORTANT] By Default, TIMM initialized OpenAI CLIP models with the standard GELU activation from PyTorch.\n#             HOWEVER =>> Original OpenAI models were trained with the quick_gelu *approximation* -- while it's\n#                         a decent approximation, the resulting features are *worse*; this was a super tricky bug\n#                         to identify, but luckily there's an easy fix (`override_act_layer`)\nclass CLIPViTBackbone(TimmViTBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(\n            vision_backbone_id,\n            CLIP_VISION_BACKBONES[vision_backbone_id],\n            image_resize_strategy,\n            default_image_size=default_image_size,\n            pretrained_cfg_overlay=dict(file=\"ckpts/vit_base_patch16_clip_224/pytorch_model.bin\"),\n            override_act_layer=\"quick_gelu\" if CLIP_VISION_BACKBONES[vision_backbone_id].endswith(\".openai\") else None,\n        )\n"}
{"type": "source_file", "path": "models/cobra/backbones/llm/prompting/zephyr_prompter.py", "content": "\"\"\"\nzephyr_prompter.py\n\nDefines a PromptBuilder for building Mamba Chat Prompts.\n\nReference: https://huggingface.co/havenhq/mamba-chat\n\"\"\"\nfrom typing import Optional\n\nfrom models.cobra.backbones.llm.prompting.base_prompter import PromptBuilder\n\n\nclass ZephyrChatPromptBuilder(PromptBuilder):\n    def __init__(self, model_family: str, system_prompt: Optional[str] = None) -> None:\n        super().__init__(model_family, system_prompt)\n        self.system_prompt = \"\"\n        \n        # Zephyr Specific\n        self.bos, self.eos = \"\", \"<|endoftext|>\"\n\n        # Get role-specific \"wrap\" functions\n        self.wrap_human = lambda msg: f\"<|user|>\\n{msg}{self.eos}\\n<|assistant|>\\n\"\n        self.wrap_gpt = lambda msg: f\"{msg if msg != '' else ' '}{self.eos}\"\n\n        # === `self.prompt` gets built up over multiple turns ===\n        self.prompt, self.turn_count = \"\", 0\n\n    def add_turn(self, role: str, message: str) -> str:\n        assert (role == \"human\") if (self.turn_count % 2 == 0) else (role == \"gpt\")\n        message = message.replace(\"<image>\", \"\").strip()\n\n        # Special Handling for \"system\" prompt (turn_count == 0)\n        if self.turn_count == 0:\n            sys_message = self.system_prompt + self.wrap_human(message)\n            wrapped_message = sys_message\n        elif (self.turn_count % 2) == 0:\n            human_message = \"\\n\" + self.wrap_human(message)\n            wrapped_message = human_message\n        else:\n            gpt_message = self.wrap_gpt(message)\n            wrapped_message = gpt_message\n\n        # Update Prompt\n        self.prompt += wrapped_message\n\n        # Bump Turn Counter\n        self.turn_count += 1\n\n        # Return \"wrapped_message\" (effective string added to context)\n        return wrapped_message\n\n    def get_potential_prompt(self, message: str) -> None:\n        # Assumes that it's always the user's (human's) turn!\n        prompt_copy = str(self.prompt)\n\n        # Special Handling for \"system\" prompt (turn_count == 0)\n        if self.turn_count == 0:\n            sys_message = self.system_prompt + self.wrap_human(message)\n            prompt_copy += sys_message\n\n        else:\n            human_message = self.wrap_human(message)\n            prompt_copy += human_message\n\n        return prompt_copy.removeprefix(self.bos)\n\n    def get_prompt(self) -> str:\n        # Remove prefix <bos> (if exists) because it gets auto-inserted by tokenizer!\n        return self.prompt.removeprefix(self.bos)\n"}
{"type": "source_file", "path": "models/cobra/backbones/vision/siglip_vit.py", "content": "\"\"\"\nsiglip_vit.py\n\"\"\"\nfrom models.cobra.backbones.vision.base_vision import TimmViTBackbone\n\n# Registry =>> Supported SigLIP Vision Backbones (from TIMM) =>> Note:: Using SigLIP w/ Patch = 14 (but SO400M Arch)\nSIGLIP_VISION_BACKBONES = {\n    \"siglip-vit-b16-224px\": \"vit_base_patch16_siglip_224\",\n    \"siglip-vit-b16-256px\": \"vit_base_patch16_siglip_256\",\n    \"siglip-vit-b16-384px\": \"vit_base_patch16_siglip_384\",\n    \"siglip-vit-so400m\": \"vit_so400m_patch14_siglip_224\",\n    \"siglip-vit-so400m-384px\": \"vit_so400m_patch14_siglip_384\",\n}\n\n\nclass SigLIPViTBackbone(TimmViTBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(\n            vision_backbone_id,\n            SIGLIP_VISION_BACKBONES[vision_backbone_id],\n            image_resize_strategy,\n            default_image_size=default_image_size,\n        )\n"}
{"type": "source_file", "path": "models/cobra/backbones/llm/prompting/base_prompter.py", "content": "\"\"\"\nbase_prompter.py\n\nAbstract class definition of a multi-turn prompt builder for ensuring consistent formatting for chat-based LLMs.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\n\nclass PromptBuilder(ABC):\n    def __init__(self, model_family: str, system_prompt: Optional[str] = None) -> None:\n        self.model_family = model_family\n\n        # Only some models define a system prompt => let subclasses handle this logic!\n        self.system_prompt = system_prompt\n\n    @abstractmethod\n    def add_turn(self, role: str, message: str) -> str: ...\n\n    @abstractmethod\n    def get_potential_prompt(self, user_msg: str) -> None: ...\n\n    @abstractmethod\n    def get_prompt(self) -> str: ...\n\n\nclass PurePromptBuilder(PromptBuilder):\n    def __init__(self, model_family: str, system_prompt: Optional[str] = None) -> None:\n        super().__init__(model_family, system_prompt)\n\n        # TODO (siddk) =>> Can't always assume LlamaTokenizer --> FIX ME!\n        self.bos, self.eos = \"<s>\", \"</s>\"\n\n        # Get role-specific \"wrap\" functions\n        self.wrap_human = lambda msg: f\"In: {msg}\\nOut: \"\n        self.wrap_gpt = lambda msg: f\"{msg if msg != '' else ' '}{self.eos}\"\n\n        # === `self.prompt` gets built up over multiple turns ===\n        self.prompt, self.turn_count = \"\", 0\n\n    def add_turn(self, role: str, message: str) -> str:\n        assert (role == \"human\") if (self.turn_count % 2 == 0) else (role == \"gpt\")\n        message = message.replace(\"<image>\", \"\").strip()\n\n        if (self.turn_count % 2) == 0:\n            human_message = self.wrap_human(message)\n            wrapped_message = human_message\n        else:\n            gpt_message = self.wrap_gpt(message)\n            wrapped_message = gpt_message\n\n        # Update Prompt\n        self.prompt += wrapped_message\n\n        # Bump Turn Counter\n        self.turn_count += 1\n\n        # Return \"wrapped_message\" (effective string added to context)\n        return wrapped_message\n\n    def get_potential_prompt(self, message: str) -> None:\n        # Assumes that it's always the user's (human's) turn!\n        prompt_copy = str(self.prompt)\n\n        human_message = self.wrap_human(message)\n        prompt_copy += human_message\n\n        return prompt_copy.removeprefix(self.bos).rstrip()\n\n    def get_prompt(self) -> str:\n        # Remove prefix <bos> (if exists) because it gets auto-inserted by tokenizer!\n        return self.prompt.removeprefix(self.bos).rstrip()\n"}
{"type": "source_file", "path": "models/cobra/overwatch.py", "content": "\"\"\"\noverwatch.py\n\nUtility class for creating a centralized/standardized logger (built on Rich) and accelerate handler.\n\"\"\"\nimport logging\nimport logging.config\nimport os\nfrom logging import LoggerAdapter\nfrom typing import Any, Callable, ClassVar, Dict, MutableMapping, Tuple, Union\n\n# Overwatch Default Format String\nRICH_FORMATTER, DATEFMT = \"| >> %(message)s\", \"%m/%d [%H:%M:%S]\"\n\n# Set Logging Configuration\nLOG_CONFIG = {\n    \"version\": 1,\n    \"disable_existing_loggers\": True,\n    \"formatters\": {\"simple-console\": {\"format\": RICH_FORMATTER, \"datefmt\": DATEFMT}},\n    \"handlers\": {\n        \"console\": {\n            \"class\": \"rich.logging.RichHandler\",\n            \"formatter\": \"simple-console\",\n            \"markup\": True,\n            \"rich_tracebacks\": True,\n            \"show_level\": True,\n            \"show_path\": True,\n            \"show_time\": True,\n        }\n    },\n    \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n}\nlogging.config.dictConfig(LOG_CONFIG)\n\n\n# === Custom Contextual Logging Logic ===\nclass ContextAdapter(LoggerAdapter):\n    CTX_PREFIXES: ClassVar[Dict[int, str]] = {**{0: \"[*] \"}, **{idx: \"|=> \".rjust(4 + (idx * 4)) for idx in [1, 2, 3]}}\n\n    def process(self, msg: str, kwargs: MutableMapping[str, Any]) -> Tuple[str, MutableMapping[str, Any]]:\n        ctx_level = kwargs.pop(\"ctx_level\", 0)\n        return f\"{self.CTX_PREFIXES[ctx_level]}{msg}\", kwargs\n\n\nclass DistributedOverwatch:\n    def __init__(self, name: str) -> None:\n        \"\"\"Initializer for an Overwatch object that wraps logging & `accelerate.PartialState`.\"\"\"\n        from accelerate import PartialState\n\n        # Note that PartialState is always safe to initialize regardless of `accelerate launch` or `torchrun`\n        #   =>> However, might be worth actually figuring out if we need the `accelerate` dependency at all!\n        self.logger, self.distributed_state = ContextAdapter(logging.getLogger(name), extra={}), PartialState()\n\n        # Logger Delegation (for convenience; would be nice to just compose & dynamic dispatch eventually)\n        self.debug = self.logger.debug\n        self.info = self.logger.info\n        self.warning = self.logger.warning\n        self.error = self.logger.error\n        self.critical = self.logger.critical\n\n        # Logging Defaults =>> only Log `INFO` on Main Process, `ERROR` on others!\n        self.logger.setLevel(logging.INFO if self.distributed_state.is_main_process else logging.ERROR)\n\n    def rank_zero_only(self) -> Callable[..., Any]:\n        return self.distributed_state.on_main_process\n\n    def is_rank_zero(self) -> bool:\n        return self.distributed_state.is_main_process\n\n    def rank(self) -> int:\n        return self.distributed_state.process_index\n\n    def world_size(self) -> int:\n        return self.distributed_state.num_processes\n\n\nclass PureOverwatch:\n    def __init__(self, name: str) -> None:\n        \"\"\"Initializer for an Overwatch object that just wraps logging.\"\"\"\n        self.logger = ContextAdapter(logging.getLogger(name), extra={})\n\n        # Logger Delegation (for convenience; would be nice to just compose & dynamic dispatch eventually)\n        self.debug = self.logger.debug\n        self.info = self.logger.info\n        self.warning = self.logger.warning\n        self.error = self.logger.error\n        self.critical = self.logger.critical\n\n        # Logging Defaults =>> INFO\n        self.logger.setLevel(logging.INFO)\n\n    @staticmethod\n    def rank_zero_only() -> Callable[[Callable[..., Any]], Callable[..., Any]]:\n        def identity(fn: Callable[..., Any]) -> Callable[..., Any]:\n            return fn\n\n        return identity\n\n    @staticmethod\n    def is_rank_zero() -> bool:\n        return True\n\n    @staticmethod\n    def rank() -> int:\n        return 0\n\n    @staticmethod\n    def world_size() -> int:\n        return 1\n\n\ndef initialize_overwatch(name: str) -> Union[DistributedOverwatch, PureOverwatch]:\n    return DistributedOverwatch(name) if int(os.environ.get(\"WORLD_SIZE\", -1)) != -1 else PureOverwatch(name)\n"}
{"type": "source_file", "path": "models/cobra/vlms/__init__.py", "content": "from .cobra import CobraVLM\n"}
{"type": "source_file", "path": "models/cobra/registry.py", "content": "\"\"\"\nregistry.py\n\nExhaustive list of pretrained VLMs (with full descriptions / links to corresponding names and sections of paper).\n\"\"\"\n\n\n# === Pretrained Model Registry ===\n# fmt: off\nMODEL_REGISTRY = {\n    \"cobra+3b\": {\n        \"model_id\": \"cobra+3b\",\n        \"names\": [\"Cobra-DINOSigLIP 3B\"],\n        \"description\": {\n            \"name\": \"Cobra 3B\",\n            \"optimization_procedure\": \"single-stage\",\n            \"visual_representation\": \"DINOv2 ViT-L/14 + SigLIP ViT-SO/14 @ 384px\",\n            \"image_processing\": \"Naive Resize\",\n            \"language_model\": \"Mamba 2.8B Zephyr\",\n            \"datasets\": [\"LLaVa v1.5 Instruct\", \"LVIS-Instruct-4V\", \"LRV-Instruct\"],\n            \"train_epochs\": 2,\n        },\n    },\n}\n\n# Build Global Registry (Model ID, Name) -> Metadata\nGLOBAL_REGISTRY = {name: v for k, v in MODEL_REGISTRY.items() for name in [k] + v[\"names\"]}\n\n# fmt: on\n"}
{"type": "source_file", "path": "models/cobra/prompting_utils.py", "content": "# coding=utf-8\n# Copyright 2024 NUS Show Lab.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n# TODO - SHOULD BE FURTHER IMPROVED.\nclass UniversalPrompting():\n    def __init__(self, text_tokenizer,\n                 special_tokens=(\"<|soi|>\", \"<|eoi|>\", \"<|sov|>\", \"<|eov|>\", \"<|t2i|>\", \"<|mmu|>\", \"<|t2v|>\", \"<|v2v|>\", \"<|lvg|>\"),\n                 max_text_len=8000, ignore_id=-100, cond_dropout_prob=0.1):\n        \"\"\"\n        :param text_tokenizer: original text tokenizer\n        \"\"\"\n        self.text_tokenizer = text_tokenizer\n        self.text_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        self.text_tokenizer.add_tokens(list(special_tokens))\n        self.sptids_dict = {token: torch.tensor(self.text_tokenizer.convert_tokens_to_ids([token])) for token in\n                            special_tokens}\n        # self.sptids_dict['<|sot|>'] = torch.tensor([self.text_tokenizer.bos_token_id])\n        # self.sptids_dict['<|eot|>'] = torch.tensor([self.text_tokenizer.eos_token_id])\n        self.sptids_dict['<|pad|>'] = torch.tensor([self.text_tokenizer.pad_token_id])\n        # plus 1 because at this time we add a task token before\n        self.max_text_len = max_text_len + 1\n        self.pad_id = self.text_tokenizer.convert_tokens_to_ids('[PAD]')\n        self.ignore_id = ignore_id\n        self.cond_dropout_prob = cond_dropout_prob\n\n    def t2i_prompt(self, text_ids, image_ids, labels):\n\n        device = image_ids.device\n        sequence_ids = []\n        attention_masks = []\n        label_ids = []\n        probs = torch.rand(len(text_ids))\n        for i in range(len(text_ids)):\n\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.bos_token_id] + text_ids[i]\n\n            temp_ids = [int(self.sptids_dict['<|t2i|>'])] + text_ids[i] + [self.text_tokenizer.eos_token_id]\n\n            # randomly dropout text condition\n            if probs[i] < self.cond_dropout_prob:\n                temp_ids = [int(self.sptids_dict['<|t2i|>']), self.text_tokenizer.bos_token_id, self.text_tokenizer.eos_token_id]\n\n            if self.max_text_len >= len(temp_ids):\n                temp_ids = [self.pad_id] * (self.max_text_len - len(temp_ids)) + temp_ids\n                temp_masks = [0] * (self.max_text_len - len(temp_ids)) + [1] * (len(temp_ids) + image_ids.shape[-1] + 3)\n            else:\n                # should add the eos token\n                temp_ids = temp_ids[:self.max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                temp_masks = [1] * (len(temp_ids) + image_ids.shape[-1] + 3)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_label_ids = torch.cat([\n                # should we predict text tokens when doing image reconstruction?\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                labels[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_label_ids = torch.where(temp_label_ids == self.pad_id, self.ignore_id, temp_label_ids)\n\n            temp_ids = torch.cat([\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                image_ids[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n            label_ids.append(temp_label_ids.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0), torch.cat(label_ids, dim=0)\n\n    def t2i_mamba_prompt(self, text_ids, image_ids):\n\n        device = image_ids.device\n        sequence_ids = []\n        probs = torch.rand(len(text_ids))\n        for i in range(len(text_ids)):\n\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.bos_token_id] + text_ids[i]\n\n            temp_ids = [int(self.sptids_dict['<|t2i|>'])] + text_ids[i] + [self.text_tokenizer.eos_token_id]\n\n            # # randomly dropout text condition\n            # if probs[i] < self.cond_dropout_prob:\n            #     temp_ids = [int(self.sptids_dict['<|t2i|>']), self.text_tokenizer.bos_token_id, self.text_tokenizer.eos_token_id]\n\n            temp_max_text_len = 10 # for class-condition\n            if temp_max_text_len >= len(temp_ids):\n                temp_ids = [self.pad_id] * (temp_max_text_len - len(temp_ids)) + temp_ids\n                # temp_masks = [0] * (self.max_text_len - len(temp_ids)) + [1] * (len(temp_ids) + image_ids.shape[-1] + 3)\n            else:\n                # should add the eos token\n                temp_ids = temp_ids[:temp_max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                # temp_masks = [1] * (len(temp_ids) + image_ids.shape[-1] + 3)  # +2 for two special tokens\n\n\n            temp_ids = torch.cat([\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                # image_ids[i], # image_tokens_t2i, place img embedding here\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            # temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            # attention_masks.append(temp_masks.unsqueeze(0))\n            # label_ids.append(temp_label_ids.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0)\n\n    def t2i_gen_prompt(self, text_ids, image_ids):\n\n        device = image_ids.device\n        sequence_ids = []\n        attention_masks = []\n        for i in range(len(text_ids)):\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.bos_token_id] + text_ids[i]\n            # note that, llama3 tokenizer automatically add the bot token at first but without eot\n            temp_ids = [int(self.sptids_dict['<|t2i|>'])] + text_ids[i] + [self.text_tokenizer.eos_token_id]\n            if self.max_text_len >= len(temp_ids):\n                temp_ids = [self.pad_id] * (self.max_text_len - len(temp_ids)) + temp_ids\n                temp_masks = [0] * (self.max_text_len - len(temp_ids)) + [1] * len(temp_ids)\n            else:\n                temp_ids = temp_ids[:self.max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                temp_masks = [1] * len(temp_ids)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_ids = torch.cat([\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                image_ids[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n    # language modeling\n    def lm_prompt(self, text_ids, max_seq_len):\n\n        sequence_ids = []\n        attention_masks = []\n        label_ids = []\n        for i in range(len(text_ids)):\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.eos_token_id] + text_ids[i]\n\n            temp_ids = text_ids[i] + [self.text_tokenizer.eos_token_id]\n\n            if max_seq_len >= len(temp_ids):\n                temp_labels_ids = temp_ids + [self.ignore_id] * (max_seq_len - len(temp_ids))\n                temp_ids = temp_ids + [self.pad_id] * (max_seq_len - len(temp_ids))\n                temp_masks = [1] * len(temp_ids) + [0] * (max_seq_len - len(temp_ids))\n            else:\n                # In language modeling, we only process text tokens. We do not add the eos token if the text length\n                # exceeds the max sequence length\n                temp_labels_ids = temp_ids[:max_seq_len]\n                temp_ids = temp_ids[:max_seq_len]\n                temp_masks = [1] * len(temp_ids)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_ids = torch.tensor(temp_ids)\n            temp_masks = torch.tensor(temp_masks)\n            temp_labels_ids = torch.tensor(temp_labels_ids)\n\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n            label_ids.append(temp_labels_ids.unsqueeze(0))\n\n        # input_ids, masks, labels\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0), torch.cat(label_ids, dim=0)\n\n    def mmu_prompt(self, image_ids, text_ids):\n        device = image_ids.device\n        sequence_ids = []\n        attention_masks = []\n        label_ids = []\n        max_text_len = self.max_text_len - 1\n        for i in range(len(text_ids)):\n            # note that, llama3 tokenizer automatically add the bot token at first but without eot\n            # for empty list []\n\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.eos_token_id] + text_ids[i]\n\n            temp_ids = text_ids[i] + [self.text_tokenizer.eos_token_id]\n\n            if max_text_len >= len(temp_ids):\n                # minus 1 because task token was prepended to the former image tokens\n                temp_ids = temp_ids + [self.pad_id] * (max_text_len - len(temp_ids))\n                temp_masks = [1] * (len(temp_ids) + image_ids.shape[-1] + 3) + [0] * (max_text_len - len(temp_ids))\n            else:\n                # should add the eos token\n                temp_ids = temp_ids[:max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                temp_masks = [1] * (len(temp_ids) + image_ids.shape[-1] + 3)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_label_ids = torch.cat([\n                torch.tensor([self.ignore_id]).to(device),\n                torch.tensor([self.ignore_id]).to(device),\n                torch.ones_like(image_ids[i]) * self.ignore_id,\n                torch.tensor([self.ignore_id]).to(device),\n                torch.tensor(temp_ids).to(device),\n            ], dim=0)\n\n            temp_label_ids = torch.where(temp_label_ids == self.pad_id, self.ignore_id, temp_label_ids)\n\n            temp_ids = torch.cat([\n                self.sptids_dict['<|mmu|>'].to(device),  # task token\n                self.sptids_dict['<|soi|>'].to(device),\n                image_ids[i],\n                self.sptids_dict['<|eoi|>'].to(device),\n                torch.tensor(temp_ids).to(device),\n            ], dim=0)\n\n            temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n            label_ids.append(temp_label_ids.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0), torch.cat(label_ids, dim=0)\n\n    def t2v_prompt(self, text_ids, video_ids):\n        \"\"\"\n        :param text_ids:\n        :param video_ids:\n        :return:\n        \"\"\"\n        pass\n\n    def i2v_prompt(self, image_ids, video_ids):\n        \"\"\"\n        :param image_ids:\n        :param video_ids:\n        :return:\n        \"\"\"\n        pass\n\n    def lvg_prompt(self, text_ids, image_ids, labels):\n\n        device = image_ids.device\n        sequence_ids = []\n        attention_masks = []\n        label_ids = []\n        probs = torch.rand(len(text_ids))\n        probs2 = torch.rand(len(text_ids))\n        for i in range(len(text_ids)):\n\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.bos_token_id] + text_ids[i]\n\n            temp_ids = [int(self.sptids_dict['<|t2i|>'])] + text_ids[i] + [self.text_tokenizer.eos_token_id]\n\n            # randomly dropout text condition\n            if probs[i] < self.cond_dropout_prob:\n                temp_ids = [int(self.sptids_dict['<|t2i|>']), self.text_tokenizer.bos_token_id,\n                            self.text_tokenizer.eos_token_id]\n\n            if self.max_text_len >= len(temp_ids):\n                temp_ids = [self.pad_id] * (self.max_text_len - len(temp_ids)) + temp_ids\n                temp_masks = [0] * (self.max_text_len - len(temp_ids)) + [1] * (len(temp_ids) + image_ids.shape[-1] + 3)\n            else:\n                # should add the eos token\n                temp_ids = temp_ids[:self.max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                temp_masks = [1] * (len(temp_ids) + image_ids.shape[-1] + 3)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_label_ids = torch.cat([\n                # should we predict text tokens when doing image reconstruction?\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                labels[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_label_ids = torch.where(temp_label_ids == self.pad_id, self.ignore_id, temp_label_ids)\n\n            temp_ids = torch.cat([\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                image_ids[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n            label_ids.append(temp_label_ids.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0), torch.cat(label_ids, dim=0)\n\n    def lvg_gen_prompt(self, text_ids, image_ids):\n\n        device = image_ids.device\n        sequence_ids = []\n        attention_masks = []\n        for i in range(len(text_ids)):\n            if len(text_ids[i]) == 0:\n                text_ids[i] = [self.text_tokenizer.bos_token_id]\n            elif text_ids[i][0] != self.text_tokenizer.bos_token_id:\n                text_ids[i] = [self.text_tokenizer.bos_token_id] + text_ids[i]\n            # note that, llama3 tokenizer automatically add the bot token at first but without eot\n            temp_ids = [int(self.sptids_dict['<|t2i|>'])] + text_ids[i] + [self.text_tokenizer.eos_token_id]\n            if self.max_text_len >= len(temp_ids):\n                temp_ids = [self.pad_id] * (self.max_text_len - len(temp_ids)) + temp_ids\n                temp_masks = [0] * (self.max_text_len - len(temp_ids)) + [1] * len(temp_ids)\n            else:\n                temp_ids = temp_ids[:self.max_text_len - 1] + [self.text_tokenizer.eos_token_id]\n                temp_masks = [1] * len(temp_ids)  # +2 for two special tokens\n\n            # prompting -- [task token] [sot] [text tokens] [eot] [soi] [image tokens] [eoi]\n            temp_ids = torch.cat([\n                torch.tensor(temp_ids).to(device),\n                self.sptids_dict['<|soi|>'].to(device),\n                image_ids[i],\n                self.sptids_dict['<|eoi|>'].to(device)\n            ], dim=0)\n\n            temp_masks = torch.tensor(temp_masks).to(device)\n            sequence_ids.append(temp_ids.unsqueeze(0))\n            attention_masks.append(temp_masks.unsqueeze(0))\n\n        return torch.cat(sequence_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n    def mask_prompt(self):\n        pass\n\n    def __call__(self, input, task, padding=True, config=None):\n        \"\"\"\n        input (tuple) : data pairs contain text(str), image(tensor), or videos(tensor).\n        task (str) : a flag indicates the current task.\n        \"\"\"\n        if task == \"t2i\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']  # (B, max_len)\n            image_ids = input[1]  # (B, #tokens)\n            sequence_ids_with_masks = self.t2i_prompt(text_ids, image_ids, input[2])\n        \n        elif task == \"t2i_mamba\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']\n            image_ids = input[1]\n            sequence_ids_with_masks = self.t2i_mamba_prompt(text_ids, image_ids)\n\n        elif task == \"t2i_plus_lm\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']  # (B, max_len)\n            image_ids = input[1]  # (B, #tokens)\n            sequence_ids_with_masks = self.t2i_prompt(text_ids[:config.training.batch_size], image_ids,\n                                                                   input[2])\n            sequence_ids_with_masks_lm = self.lm_prompt(text_ids[config.training.batch_size:], input[3])\n            return sequence_ids_with_masks, sequence_ids_with_masks_lm\n\n        elif task == \"t2i_gen\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']  # (B, max_len)\n            image_ids = input[1]  # (B, #tokens)\n            sequence_ids_with_masks = self.t2i_gen_prompt(text_ids, image_ids)\n\n        elif task == \"lm\":\n            text_ids = self.text_tokenizer(input[0], truncation=True)['input_ids']  # (B, max_len)\n            sequence_ids_with_masks = self.lm_prompt(text_ids, input[1])\n\n        elif task == \"mmu\":\n            image_ids = input[0]\n            text_ids = self.text_tokenizer(input[1])['input_ids']\n            sequence_ids_with_masks = self.mmu_prompt(image_ids, text_ids)\n\n        elif task == \"t2v\":\n            text_ids = self.text_tokenizer(input[0]['input_ids'])\n            video_ids = self.vision_tokenizer(input[1])\n            sequence_ids_with_masks = self.t2v_prompt(text_ids, video_ids)\n\n        elif task == \"i2v\":\n            image_ids = self.text_tokenizer(input[0])\n            video_ids = self.vision_tokenizer(input[1])\n            sequence_ids_with_masks = self.i2v_prompt(image_ids, video_ids)\n\n        elif task == \"lvg\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']  # (B, max_len)\n            image_ids = input[1]  # (B, #tokens)\n            sequence_ids_with_masks = self.lvg_prompt(text_ids, image_ids, input[2])\n\n        elif task == \"lvg_gen\":\n            text_ids = self.text_tokenizer(input[0])['input_ids']  # (B, max_len)\n            image_ids = input[1]  # (B, #tokens)\n            sequence_ids_with_masks = self.lvg_gen_prompt(text_ids, image_ids)\n        else:\n            raise NotImplementedError\n\n        return sequence_ids_with_masks\n\ndef create_attention_mask_predict_next(sequence, pad_id=128256, soi_id=128257, eoi_id=128258, rm_pad_in_image=False,\n                                       return_inverse_mask=True):\n    # sequence is expected to be of shape [N, L]\n    N, L = sequence.shape\n\n    # Masks to identify different types of tokens\n    is_padding = sequence == pad_id\n\n    is_start_image = sequence == soi_id\n\n    is_end_image = sequence == eoi_id\n\n    # Create cumulative sum masks to identify regions of image tokens\n    cumulative_start = torch.cumsum(is_start_image, dim=1)\n    cumulative_end = torch.cumsum(is_end_image, dim=1)\n    in_image_segment = (cumulative_start > cumulative_end) | is_start_image | is_end_image\n\n    is_text = ~(in_image_segment)\n\n    causal_mask = torch.tril(torch.ones((L, L), dtype=torch.bool)).to(sequence.device)\n\n    mask_text = is_text[:, :, None] * causal_mask[None, :, :]\n\n    is_text_image = is_text | in_image_segment\n\n    mask_text_image_bi = is_text_image[:, :, None] * is_text_image[:, None, :]\n    if rm_pad_in_image:\n        sid_img = torch.where(sequence == soi_id)[1]\n        for i in range(mask_text_image_bi.shape[0]):\n            pad_end_idx = torch.where(sequence[i] == pad_id)\n            if len(pad_end_idx[0]) != 0:\n                pad_end_idx = pad_end_idx[0][-1]\n                mask_text[i][pad_end_idx + 1:, :pad_end_idx + 1] = 0\n            id_padding = torch.where(is_padding[i] == True)\n            mask_text_image_bi[i][sid_img[i]:, id_padding[0]] = 0\n\n    mask_text[in_image_segment] = mask_text_image_bi[in_image_segment]\n    # No token attends to padding tokens and padding tokens do not attend to any token\n    if return_inverse_mask:\n        inverted_mask = 1.0 - mask_text.type(sequence.dtype)\n        inverted_mask = inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.iinfo(sequence.dtype).min\n        )\n        return inverted_mask.unsqueeze(1)\n    else:\n        return mask_text.unsqueeze(1)\n\ndef create_attention_mask_lvg(sequence, pad_id=128256, soi_id=128257, eoi_id=128258, return_inverse_mask=True):\n    # sequence is expected to be of shape [N, L]\n    N, L = sequence.shape\n    # Masks to identify different types of tokens\n    is_padding = sequence == pad_id\n    mask_text_image_bi = torch.tril(torch.ones(N, L, L), diagonal=0).to(sequence.device)\n\n    sid_img = torch.where(sequence == soi_id)[1].reshape(mask_text_image_bi.shape[0], -1)[:, 0]\n    sid_img_for_bi = torch.where(sequence == soi_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    eid_img_for_bi = torch.where(sequence == eoi_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    for i in range(N):\n        id_padding = torch.where(is_padding[i] == True)\n        mask_text_image_bi[i][sid_img[i]:, id_padding[0]] = 0\n        for j in range(sid_img_for_bi.shape[-1]):\n            mask_text_image_bi[i][sid_img_for_bi[i, j]:eid_img_for_bi[i, j] + 1,\n            sid_img_for_bi[i, j]:eid_img_for_bi[i, j] + 1] = 1\n\n    # No token attends to padding tokens and padding tokens do not attend to any token\n    if return_inverse_mask:\n        inverted_mask = 1.0 - mask_text_image_bi.type(sequence.dtype)\n        inverted_mask = inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.iinfo(sequence.dtype).min\n        )\n        return inverted_mask.unsqueeze(1)\n    else:\n        return mask_text_image_bi.unsqueeze(1)\n\n# texts without attending image regions\ndef create_attention_mask_lvg_v2(sequence, pad_id=128256, soi_id=128257, eoi_id=128258, sot_id=1000, eot_id=1001, return_inverse_mask=True):\n    # sequence is expected to be of shape [N, L]\n    N, L = sequence.shape\n    # Masks to identify different types of tokens\n    is_padding = sequence == pad_id\n    # is_text = torch.where(sequence < 2000, True, False)\n    is_text = torch.where(sequence < pad_id, True, False)\n    mask_text_image_bi = torch.tril(torch.ones(N, L, L), diagonal=0).to(sequence.device).int()\n    sid_text_for_bi = torch.where(sequence == sot_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    eid_text_for_bi = torch.where(sequence == eot_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    # import ipdb\n    # ipdb.set_trace()\n    if sot_id == eot_id:\n        if sid_text_for_bi.shape[-1] % 2 != 0:\n            sid_text_for_bi = sid_text_for_bi[:, :-1]\n            eid_text_for_bi = eid_text_for_bi[:, :-1]\n        select_idx = [i for i in range(0, sid_text_for_bi.shape[1], 2)]\n        sid_text_for_bi = sid_text_for_bi[:, select_idx]\n        select_idx = [i+1 for i in range(0, eid_text_for_bi.shape[1], 2)]\n        eid_text_for_bi = eid_text_for_bi[:, select_idx]\n    sid_img_for_bi = torch.where(sequence == soi_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    eid_img_for_bi = torch.where(sequence == eoi_id)[1].reshape(mask_text_image_bi.shape[0], -1)\n    all_zeros = torch.zeros_like(mask_text_image_bi).int()\n    for i in range(N):\n        all_zeros[i, :, is_text[i]] = 1\n        for j in range(sid_text_for_bi.shape[-1]):\n            all_zeros[i][is_text[i], sid_text_for_bi[i, j]:eid_text_for_bi[i, j]+1] = 1\n            all_zeros[i][~is_text[i], sid_text_for_bi[i, j]:eid_text_for_bi[i, j]+1] = 1\n        for j in range(sid_img_for_bi.shape[-1]):\n            all_zeros[i][~is_text[i], sid_img_for_bi[i, j]:eid_img_for_bi[i, j]+1] = 1\n    mask_text_image_bi = mask_text_image_bi * all_zeros\n    sid_img = torch.where(sequence == soi_id)[1].reshape(mask_text_image_bi.shape[0], -1)[:, 0]\n\n    for i in range(N):\n        id_padding = torch.where(is_padding[i] == True)\n        mask_text_image_bi[i][sid_img[i]:, id_padding[0]] = 0\n        for j in range(sid_img_for_bi.shape[-1]):\n            mask_text_image_bi[i][sid_img_for_bi[i, j]:eid_img_for_bi[i, j]+1, sid_img_for_bi[i, j]:eid_img_for_bi[i, j]+1] = 1\n\n    mask_text_image_bi[:, :, 0] = 1\n    # No token attends to padding tokens and padding tokens do not attend to any token\n    if return_inverse_mask:\n        inverted_mask = 1.0 - mask_text_image_bi.type(sequence.dtype)\n        inverted_mask = inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.iinfo(sequence.dtype).min\n        )\n        return inverted_mask.unsqueeze(1)\n    else:\n        return mask_text_image_bi.unsqueeze(1)\n\ndef create_attention_mask_for_mmu(sequence, eoi_id=128258, return_inverse_mask=True):\n    N, L = sequence.shape\n    causal_mask = torch.tril(torch.ones((N, 1, L, L), dtype=torch.bool)).to(sequence.device)\n    eoi_image = torch.where(sequence == eoi_id)[1]\n    causal_mask[:, :, :, :eoi_image[0] + 1] = 1\n\n    if return_inverse_mask:\n        inverted_mask = 1.0 - causal_mask.type(sequence.dtype)\n        inverted_mask = inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.iinfo(sequence.dtype).min\n        )\n        return inverted_mask\n    else:\n        return causal_mask\n\ndef create_attention_mask_for_mmu_vit(\n        sequence,\n        return_inverse_mask=True,\n        system_prompt_len=0\n):\n    N, L, H = sequence.shape\n    causal_mask = torch.tril(torch.ones((N, 1, L, L), dtype=torch.bool)).to(sequence.device)\n    index = 1 + system_prompt_len + 1 + 576\n    # TODO: PART OF SYSTEM PROMPT SHOULD BE CAUSAL ALSO\n    causal_mask[:, :, :, :index] = 1\n    if return_inverse_mask:\n        inverted_mask = 1.0 - causal_mask.type(torch.int64)\n        inverted_mask = inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.iinfo(torch.int64).min\n        )\n        return inverted_mask\n    else:\n        return causal_mask\n\n\nif __name__ == '__main__':\n    pass"}
{"type": "source_file", "path": "models/cobra/vlms/base_vlm.py", "content": "\"\"\"\nbase_vlm.py\n\"\"\"\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Callable, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import GenerationMixin, PretrainedConfig\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom models.cobra.backbones.llm import LLMBackbone\nfrom models.cobra.backbones.llm.prompting import PromptBuilder\nfrom models.cobra.backbones.vision import VisionBackbone\n\n\n# === Abstract Base Class for arbitrary Vision-Language Models ===\nclass VLM(nn.Module, GenerationMixin, ABC):\n    def __init__(\n        self,\n        model_family: str,\n        model_id: str,\n        vision_backbone: VisionBackbone,\n        llm_backbone: LLMBackbone,\n        enable_mixed_precision_training: bool = True,\n    ) -> None:\n        super().__init__()\n        self.model_family, self.model_id = model_family, model_id\n        self.vision_backbone, self.llm_backbone = vision_backbone, llm_backbone\n        self.enable_mixed_precision_training = enable_mixed_precision_training\n\n        # Instance Attributes for a generic VLM\n        self.all_module_keys, self.trainable_module_keys = None, None\n\n        # === GenerationMixin Expected Attributes =>> *DO NOT MODIFY* ===\n        # self.generation_config = self.llm_backbone.llm.generation_config\n        self.main_input_name = \"input_ids\"\n\n    @property\n    def device(self) -> torch.device:\n        \"\"\"Borrowed from `transformers.modeling_utils.py` -- checks parameter device; assumes model on *ONE* device!\"\"\"\n        return next(self.parameters()).device\n\n    @classmethod\n    @abstractmethod\n    def from_pretrained(\n        cls,\n        pretrained_checkpoint: Path,\n        model_family: str,\n        model_id: str,\n        vision_backbone: VisionBackbone,\n        llm_backbone: LLMBackbone,\n        **kwargs: str,\n    ) -> VLM: ...\n\n    @abstractmethod\n    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder: ...\n\n    @abstractmethod\n    def freeze_backbones(self, stage: str) -> None: ...\n\n    @abstractmethod\n    def load_from_checkpoint(self, stage: str, run_dir: Path, pretrained_checkpoint: Optional[Path] = None) -> None: ...\n\n    @abstractmethod\n    def get_fsdp_wrapping_policy(self) -> Callable: ...\n\n    @abstractmethod\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        multimodal_indices: Optional[torch.LongTensor] = None,\n    ) -> CausalLMOutputWithPast: ...\n\n    # === GenerationMixin Expected Properties & Methods (DO NOT MODIFY) ===\n    @staticmethod\n    def can_generate() -> bool:\n        return True\n\n    @property\n    def config(self) -> PretrainedConfig:\n        return self.llm_backbone.llm.config\n\n    # => Beam Search Utility\n    def _reorder_cache(self, past_key_values, beam_idx):\n        return self.llm_backbone.llm._reorder_cache(past_key_values, beam_idx)\n"}
{"type": "source_file", "path": "models/mamba_vlm.py", "content": "import torch\nimport torch.nn as nn\nfrom .stage2.config_mamba import MambaConfig\nfrom .stage2.mixer_seq_simple import MambaLMHeadModel\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom transformers import AutoTokenizer\nfrom typing import Type\nfrom models.cobra.backbones.llm.prompting import (\n    PromptBuilder,\n    MambaPromptBuilder\n)\nfrom models.cobra.prompting_utils import UniversalPrompting\nfrom llamagen_tokenizer.tokenizer_image.vq_model import VQ_models as llamagen_VQ_models\n\nclass MambaVLM(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        # init all models\n        self.vqvae = self.init_1st_stage_model()\n        self.mamba = self.init_2nd_stage_model(config)\n        self.identifier = 'MambaVLM'\n        self.d_model = config.d_model\n        self.num_classes = config.num_classes\n        self.num_tokens = config.num_tokens\n        self.pad_vocab_size_multiple = config.pad_vocab_size_multiple\n\n        self.tokenizer = AutoTokenizer.from_pretrained('ckpts/gpt-neox-20b/', model_max_length=2048)\n        self.uni_prompting = UniversalPrompting(self.tokenizer, max_text_len=499,\n                                        special_tokens=(\n                                            \"<|soi|>\", \"<|eoi|>\", \"<|sot|>\", \"<|eot|>\", \"<|t2i|>\",\n                                            \"<|mmu|>\", \"<|soc|>\", \"<|eoc|>\", \"<|lvg|>\"\n                                        ),\n                                        ignore_id=-100, cond_dropout_prob=0.1)\n\n        print('special tokens : \\n', self.uni_prompting.sptids_dict)\n  \n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6))\n\n    def get_tokenizer(self):\n        return self.tokenizer\n\n    def get_uni_prompting(self):\n        return self.uni_prompting\n\n    @property\n    def prompt_builder_fn(self) -> Type[PromptBuilder]:\n        return MambaPromptBuilder\n\n    #llamagen_t2i\n    def init_1st_stage_model(self):\n        vq_model = llamagen_VQ_models['VQ-16']()\n        checkpoint = torch.load('ckpts/llamagen/vq_ds16_t2i.pt', map_location=\"cpu\")\n        vq_model.load_state_dict(checkpoint[\"model\"])\n        vq_model.eval()\n        [p.requires_grad_(False) for p in vq_model.parameters()]\n        return vq_model\n\n\n    def resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of):\n        self.mamba.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n\n    def embed_input_ids(self, input_ids: torch.LongTensor) -> torch.Tensor:\n        return self.mamba.get_input_embeddings()(input_ids)\n\n    def init_2nd_stage_model(self, config):\n        model = MambaLMHeadModel(config)\n        return model\n\n    def get_num_params(self, non_embedding=False):\n        n_params = sum(p.numel() for p in self.mamba.backbone.layers.parameters())\n        if non_embedding:\n            n_params -= self.mamba.backbone.embeddings.word_embeddings.weight.numel()\n        return n_params\n\n    def forward(self, x, c, cond, task='t2i'):\n        logits = self.mamba(input_ids=None, input_embeddings=x, cond=cond, task=task)\n        target = c\n        if task == 't2i':\n            logits = logits.t2i_logits\n        elif task == 'mmu':\n            logits = logits.mmu_logits\n\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = target[..., 1:].contiguous()\n        # Flatten the tokens\n        logits = shift_logits.view(-1, shift_logits.shape[-1])\n        target = shift_labels.view(-1)\n        \n        return logits, target\n    \n    @torch.no_grad()\n    def decode_to_img(self, index):\n        z_shape = [index.shape[0], 8, 16, 16]\n        x = self.vqvae.decode_code(index, shape=z_shape)\n        return x\n\n     \n\ndef OmniMamba_L(t2i_task, mmu_task, **kwargs):\n    return MambaVLM(MambaConfig(d_model=1024, n_layer=48, adaln_group=True, num_groups=4, t2i_task=t2i_task, mmu_task=mmu_task, **kwargs))\n\ndef OmniMamba_1_3B(t2i_task, mmu_task, **kwargs):\n    return MambaVLM(MambaConfig(d_model=2048, n_layer=48, adaln_group=True, num_groups=4, t2i_task=t2i_task, mmu_task=mmu_task, **kwargs))\n\n\n\nMambaVLMs = {'OmniMamba-L': OmniMamba_L, 'OmniMamba-1.3B': OmniMamba_1_3B}"}
{"type": "source_file", "path": "models/stage2/lora.py", "content": "# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport re\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .lora_config import PeftConfig, PeftType\n\n\ndef get_peft_model_state_dict(model, state_dict=None):\n    \"\"\"\n    Get the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\n        the model should be the underlying model/unwrapped model (i.e. model.module).\n        state_dict (`dict`, *optional*, defaults to `None`):\n            The state dict of the model. If not provided, the state dict of the model\n        will be used.\n    \"\"\"\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if model.peft_config.peft_type == PeftType.LORA:\n        # to_return = lora_state_dict(model, bias=model.peft_config.bias)\n        # adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`\n        # to directly with the state dict which is necessary when using DeepSpeed or FSDP\n        bias = model.peft_config.bias\n        if bias == \"none\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k}\n        elif bias == \"all\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"bias\" in k}\n        elif bias == \"lora_only\":\n            to_return = {}\n            for k in state_dict:\n                if \"lora_\" in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split(\"lora_\")[0] + \"bias\"\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n    else:\n        to_return = {}\n        if model.peft_config.inference_mode:\n            prompt_embeddings = model.prompt_encoder.embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save()\n        to_return[\"prompt_embeddings\"] = prompt_embeddings\n    if model.modules_to_save is not None:\n        for key, value in state_dict.items():\n            if any(module_name in key for module_name in model.modules_to_save):\n                to_return[key] = value\n    return to_return\n\n\ndef transpose(weight, fan_in_fan_out):\n    return weight.T if fan_in_fan_out else weight\n\n\n\ndef _find_and_replace(model):\n    is_target_modules_in_base_model = False\n    kwargs = {\n        \"r\": 8,\n        \"lora_alpha\": 32,\n        \"lora_dropout\": 0.05,\n        \"lora_nums\": 1,\n        \"blc_alpha\": 0.0,\n        \"blc_weight\": 0.0,\n        \"fan_in_fan_out\": False,\n        \"merge_weights\": False,\n    }\n    target_modules = ['in_proj']\n    key_list = [key for key, _ in model.named_modules()]\n    for key in key_list:\n        if isinstance(target_modules, str):\n            target_module_found = re.fullmatch(target_modules, key)\n        else:\n            target_module_found = any(key.endswith(target_key) for target_key in target_modules)\n        if target_module_found: # here\n            if not is_target_modules_in_base_model:\n                is_target_modules_in_base_model = True\n            parent, target, target_name = _get_submodules(model, key)\n            bias = target.bias is not None\n\n            if isinstance(target, torch.nn.Linear):\n                new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)\n\n            _replace_module(parent, target_name, new_module, target)\n    if not is_target_modules_in_base_model:\n        raise ValueError(\n            f\"Target modules {target_modules} not found in the base model. \"\n            f\"Please check the target modules and try again.\"\n        )\n    return model\n\ndef _get_submodules(model, key):\n    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n    target_name = key.split(\".\")[-1]\n    target = model.get_submodule(key)\n    return parent, target, target_name\n\ndef _replace_module(parent_module, child_name, new_module, old_module):\n    setattr(parent_module, child_name, new_module)\n    new_module.weight = old_module.weight\n    if old_module.bias is not None:\n        new_module.bias = old_module.bias\n    if getattr(old_module, \"state\", None) is not None:\n        new_module.state = old_module.state\n        new_module.to(old_module.weight.device)\n\n    # dispatch to correct device\n    for name, module in new_module.named_modules():\n        if \"lora_\" in name:\n            module.to(old_module.weight.device)\n\n\n\n# Below code is based on https://github.com/microsoft/LoRA/blob/main/loralib/layers.py\n# and modified to work with PyTorch FSDP\n\n\n#  ------------------------------------------------------------------------------------------\n#  Copyright (c) Microsoft Corporation. All rights reserved.\n#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------\n\n\n# had to adapt it for `lora_only` to work\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n    for n, p in model.named_parameters():\n        if \"lora_\" not in n:\n            p.requires_grad = False\n    if bias == \"none\":\n        return\n    elif bias == \"all\":\n        for n, p in model.named_parameters():\n            if \"bias\" in n:\n                p.requires_grad = True\n    elif bias == \"lora_only\":\n        for m in model.modules():\n            if isinstance(m, LoraLayer) and hasattr(m, \"bias\") and m.bias is not None:\n                m.bias.requires_grad = True\n    else:\n        raise NotImplementedError\n\n\nclass LoraLayer:\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n        # Optional dropout\n        if lora_dropout > 0.0:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights\n        self.disable_adapters = False\n\nclass Linear(nn.Linear, LoraLayer):\n    # Lora implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_nums: int = 2,\n        blc_alpha: float = 0.0,\n        blc_weight: float = 0.0,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        merge_weights: bool = True,\n        **kwargs,\n    ):\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n\n        self.lora_num = lora_nums\n        self.blc_alpha = blc_alpha\n        self.blc_weight = blc_weight\n        self.task_types = 't2i'\n        \n        self.fan_in_fan_out = fan_in_fan_out\n        # self.lora_route = nn.Linear(in_features, self.lora_num, bias=False)\n        for i in range(self.lora_num):\n            setattr(self, f\"mmu_lora_A{i}\", nn.Linear(in_features, r, bias=False))\n            setattr(self, f\"mmu_lora_B{i}\", nn.Linear(r, out_features, bias=False))\n            setattr(self, f\"t2i_lora_A{i}\", nn.Linear(in_features, r, bias=False))\n            setattr(self, f\"t2i_lora_B{i}\", nn.Linear(r, out_features, bias=False))\n        self.scaling = self.lora_alpha / self.r\n        self.weight.requires_grad = False\n        self.reset_parameters()\n\n\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, \"mmu_lora_A0\"):\n            for i in range(self.lora_num):\n                nn.init.kaiming_uniform_(getattr(self, f\"mmu_lora_A{i}\").weight, a=math.sqrt(5))\n                nn.init.zeros_(getattr(self, f\"mmu_lora_B{i}\").weight)\n                nn.init.kaiming_uniform_(getattr(self, f\"t2i_lora_A{i}\").weight, a=math.sqrt(5))\n                nn.init.zeros_(getattr(self, f\"t2i_lora_B{i}\").weight)\n\n\n    def train(self, mode: bool = True):\n        nn.Linear.train(self, mode)\n        for i in range(self.lora_num):\n            getattr(self, f\"mmu_lora_A{i}\").train(mode)\n            getattr(self, f\"mmu_lora_B{i}\").train(mode)\n            getattr(self, f\"t2i_lora_A{i}\").train(mode)\n            getattr(self, f\"t2i_lora_B{i}\").train(mode)\n\n\n    def eval(self):\n        nn.Linear.eval(self)\n        for i in range(self.lora_num):\n            getattr(self, f\"mmu_lora_A{i}\").eval()\n            getattr(self, f\"mmu_lora_B{i}\").eval()\n            getattr(self, f\"t2i_lora_A{i}\").eval()\n            getattr(self, f\"t2i_lora_B{i}\").eval()\n\n    def cv_squared(self, x):\n        \"\"\"The squared coefficient of variation of a sample.\n        Useful as a loss to encourage a positive distribution to be more uniform.\n        Epsilons added for numerical stability.\n        Returns 0 for an empty Tensor.\n        Args:\n        x: a `Tensor`.\n        Returns:\n        a `Scalar`.\n        \"\"\"\n        eps = 1e-10\n        if x.shape[0] == 1:\n            return torch.tensor([0], device=x.device, dtype=x.dtype)[0]\n        return x.float().var() / (x.float().mean()**2 + eps)\n\n    def forward(self, x: torch.Tensor):\n\n        if self.disable_adapters:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.task_types == 't2i':\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            for i in range(self.lora_num):\n                result = result + getattr(self, f\"t2i_lora_B{i}\")(getattr(self, f\"t2i_lora_A{i}\")(self.lora_dropout(x))) * self.scaling\n        elif self.task_types == 'mmu':\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            for i in range(self.lora_num):\n                result = result + getattr(self, f\"mmu_lora_B{i}\")(getattr(self, f\"mmu_lora_A{i}\")(self.lora_dropout(x))) * self.scaling\n            \n        elif self.r > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            \n        return result\n"}
{"type": "source_file", "path": "models/stage2/block.py", "content": "# References:\n#   Mamba:  https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n#   VAR:    https://github.com/FoundationVision/VAR/blob/main/models/var.py\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn, Tensor\n\nfrom mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn\n\n\ndef modulate(x, shift, scale):\n    # return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n    return x * (1 + scale) + shift\n \n\nclass Block(nn.Module):\n    def __init__(\n        self, dim, mixer_cls, mlp_cls, norm_cls=nn.LayerNorm, fused_add_norm=False,\n        residual_in_fp32=False, adaln_group=False, mixer_drop=0.0, mlp_drop=0.0, adaln=False,\n    ):\n        \"\"\"\n        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n\n        This Block has a slightly different structure compared to a regular\n        prenorm Transformer block.\n        The standard block is: LN -> MHA/MLP -> Add.\n        [Ref: https://arxiv.org/abs/2002.04745]\n        Here we have: Add -> LN -> Mixer, returning both\n        the hidden_states (output of the mixer) and the residual.\n        This is purely for performance reasons, as we can fuse add and LayerNorm.\n        The residual needs to be provided (except for the very first block).\n        \"\"\"\n        super().__init__()\n        self.residual_in_fp32 = residual_in_fp32\n        self.fused_add_norm = fused_add_norm\n        self.norm = norm_cls(dim)\n        self.mixer = mixer_cls(dim)\n\n        # modify\n        self.mixer_dropout = nn.Dropout(mixer_drop)\n        self.adaln_group = adaln_group\n        self.adaln_factor = 3   # alpha, beta, gamma\n\n        if mlp_cls is not nn.Identity:\n            self.norm2 = norm_cls(dim)\n            self.mlp = mlp_cls(dim)\n            self.adaln_factor += 3\n            self.mlp_dropout = nn.Dropout(0.0)\n        else:\n            self.mlp = None\n        if self.fused_add_norm:\n            assert RMSNorm is not None, \"RMSNorm import fails\"\n            assert isinstance(\n                self.norm, (nn.LayerNorm, RMSNorm)\n            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n        # adaLN\n        if adaln:\n            if adaln_group:\n                self.scale_shift_table = nn.Parameter(torch.randn(1, 1, self.adaln_factor, dim) / dim**0.5)\n            else:\n                self.adaLN_modulation = nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(dim, self.adaln_factor * dim, bias=True)\n                )\n                # zero-out\n                nn.init.constant_(self.adaLN_modulation[-1].weight, 0)\n                nn.init.constant_(self.adaLN_modulation[-1].bias, 0)\n\n    def forward(\n            self, hidden_states: Tensor, residual: Optional[Tensor] = None, cls_embed=None, task=None, inference_params=None, **mixer_kwargs\n    ):\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            hidden_states: the sequence to the encoder layer (required).\n            residual: hidden_states = Mixer(LN(residual))\n        \"\"\"\n        if not self.fused_add_norm:\n            residual = (hidden_states + residual) if residual is not None else hidden_states\n            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n            if self.residual_in_fp32:\n                residual = residual.to(torch.float32)\n        else:\n            hidden_states, residual = layer_norm_fn(\n                hidden_states,\n                self.norm.weight,\n                self.norm.bias,\n                residual=residual,\n                prenorm=True,\n                residual_in_fp32=self.residual_in_fp32,\n                eps=self.norm.eps,\n                is_rms_norm=isinstance(self.norm, RMSNorm)\n            )\n        # adaLN\n        if cls_embed is not None:\n            if self.adaln_group:\n                scale_shift_params = (self.scale_shift_table + cls_embed).unbind(2)\n            else:\n                scale_shift_params = self.adaLN_modulation(cls_embed).chunk(self.adaln_factor, dim=-1)\n\n            if self.adaln_factor == 3:\n                shift_mixer, scale_mixer, gate_mixer = scale_shift_params\n            elif self.adaln_factor == 6:\n                shift_mixer, shift_mlp, scale_mixer, scale_mlp, gate_mixer, gate_mlp = scale_shift_params\n            else:\n                raise ValueError(\"Unsupported adaln_factor value\")\n            hidden_states = self.mixer_dropout(\n                gate_mixer * self.mixer(\n                    modulate(hidden_states, shift_mixer, scale_mixer),\n                    inference_params=inference_params,\n                    **mixer_kwargs\n                )\n            )\n        else:\n            hidden_states = self.mixer(hidden_states, inference_params=inference_params, **mixer_kwargs)\n\n\n        if self.mlp is not None:\n            if not self.fused_add_norm:\n                residual = hidden_states + residual\n                residual = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n                if self.residual_in_fp32:\n                    residual = residual.to(torch.float32)\n            else:\n                hidden_states, residual = layer_norm_fn(\n                    hidden_states,\n                    self.norm2.weight,\n                    self.norm2.bias,\n                    residual=residual,\n                    prenorm=True,\n                    residual_in_fp32=self.residual_in_fp32,\n                    eps=self.norm2.eps,\n                    is_rms_norm=isinstance(self.norm2, RMSNorm)\n                )\n            if cls_embed is not None:\n                hidden_states = self.mlp_dropout(\n                    gate_mlp * self.mlp(\n                        modulate(hidden_states, shift_mlp, scale_mlp),task_types=task\n                    )\n                )\n            else:\n                hidden_states = self.mlp(hidden_states,task_types=task)\n\n\n        return hidden_states, residual\n\n    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)"}
{"type": "source_file", "path": "models/cobra/vlms/cobra.py", "content": "\"\"\"\ncobra.py\n\nPyTorch Module defining a CobraVLM, our general interface for defining the various different VLMs in our work.\n\nNotes:\n    - For now, we don't subclass `transformers.PretrainedModel` (or CausalLM). Instead, we assume a very limited subset\n      of the {Model}ForCausalLM API that enables dispatch to the underlying LLM's `generate` utilities (feeding inputs\n      through our custom projection shim).\n\"\"\"\nfrom __future__ import annotations\n\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Type, Union\n\nimport torch\nfrom torch import nn\nfrom PIL import Image\nfrom torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\n\nfrom cobra.models.backbones.llm import LLMBackbone, MambaLLMBackbone\nfrom cobra.models.backbones.llm.prompting import PromptBuilder\nfrom cobra.models.backbones.vision import VisionBackbone\nfrom cobra.models.vlms.base_vlm import VLM\nfrom cobra.overwatch import initialize_overwatch\nfrom cobra.util.nn_utils import FusedMLPProjector, LinearProjector, MLPProjector, FusedLDPProjector\nfrom cobra.models.mamba2.modeling_mamba import GenerationMixin as MambaGenerationMixin\nfrom models.modeling_utils import ConfigMixin, ModelMixin\nfrom diffusion.models_dit import TimestepEmbedder\n# Initialize Overwatch =>> Wraps `logging.Logger`\noverwatch = initialize_overwatch(__name__)\n\n\n# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)\nIGNORE_INDEX = -100\n\n\n#################################################################################\n#                      Embedding Layers for Class Labels                        #\n#################################################################################\nclass LabelEmbedder(nn.Module):\n    \"\"\"\n    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n    \"\"\"\n    def __init__(self, num_classes, hidden_size, dropout_prob=0.1):\n        super().__init__()\n        use_cfg_embedding = dropout_prob > 0\n        self.embedding_table = torch.nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n        self.num_classes = num_classes\n        self.dropout_prob = dropout_prob\n\n    def token_drop(self, labels, force_drop_ids=None):\n        \"\"\"\n        Drops labels to enable classifier-free guidance.\n        \"\"\"\n        if force_drop_ids is None:\n            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n        else:\n            drop_ids = force_drop_ids == 1\n        labels = torch.where(drop_ids, self.num_classes, labels)\n        return labels\n\n    def forward(self, labels, train, force_drop_ids=None):\n        use_dropout = self.dropout_prob > 0\n        if (train and use_dropout) or (force_drop_ids is not None):\n            labels = self.token_drop(labels, force_drop_ids)\n        embeddings = self.embedding_table(labels).unsqueeze(1)\n        return embeddings\n\nclass GPT2Embeddings(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        vocab_size,\n        max_position_embeddings,\n        padding_idx=None,\n        word_embed_proj_dim=None,\n        token_drop=0.0,\n        device=None,\n        dtype=None,\n    ):\n        \"\"\"\n        If max_position_embeddings <= 0, there's no position embeddings\n        If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n            the project up to embed_dim\n        \"\"\"\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        if word_embed_proj_dim is None:\n            self.word_embeddings = nn.Embedding(\n                vocab_size, embed_dim, padding_idx=padding_idx, **factory_kwargs\n            )\n            self.project_in = None\n        else:\n            self.word_embeddings = nn.Embedding(\n                vocab_size, word_embed_proj_dim, padding_idx=padding_idx, **factory_kwargs\n            )\n            self.project_in = nn.Linear(\n                word_embed_proj_dim, embed_dim, bias=False, **factory_kwargs\n            )\n        self.max_position_embeddings = max_position_embeddings\n        if self.max_position_embeddings > 0:\n            self.position_embeddings = nn.Embedding(\n                max_position_embeddings, embed_dim, **factory_kwargs\n            )\n        self.token_dropout = nn.Dropout(token_drop)\n        # nn.init.normal_(self.word_embeddings.weight, std=0.02)\n\n    def forward(self, input_ids, position_ids=None):\n        \"\"\"\n        input_ids: (batch, seqlen)\n        position_ids: (batch, seqlen)\n        \"\"\"\n        batch_size, seqlen = input_ids.shape\n        embeddings = self.token_dropout(self.word_embeddings(input_ids))\n        if self.project_in is not None:\n            embeddings = self.project_in(embeddings)\n        if self.max_position_embeddings > 0:\n            if position_ids is None:\n                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n            position_embeddings = self.position_embeddings(position_ids)\n\n            embeddings = embeddings + position_embeddings\n        return embeddings\n\n\n\nclass CobraVLM(VLM, ModelMixin, ConfigMixin):\n    def __init__(\n        self,\n        model_id: str,\n        vision_backbone: VisionBackbone,\n        llm_backbone: MambaLLMBackbone,\n        enable_mixed_precision_training: bool = True,\n        arch_specifier: str = \"gelu-mlp\",\n    ) -> None:\n        super().__init__(\n            \"cobra\",\n            model_id,\n            vision_backbone,\n            llm_backbone,\n            enable_mixed_precision_training=enable_mixed_precision_training,\n        )\n\n        # Set Weight Initialization Seed for Projector Consistency\n        torch.manual_seed(vision_backbone.embed_dim)\n\n        # Initialize Projection (Adapter) based on `arch_specifier`\n        self.arch_specifier = arch_specifier\n        if arch_specifier == \"linear\":\n            self.projector = LinearProjector(vision_backbone.embed_dim, llm_backbone.embed_dim)\n        elif arch_specifier.endswith(\"fused-gelu-mlp\"):\n            self.projector = FusedMLPProjector(vision_backbone.embed_dim, llm_backbone.mamba_cfg.d_model)\n        elif arch_specifier.endswith(\"gelu-mlp\"):\n            self.projector = MLPProjector(vision_backbone.embed_dim, llm_backbone.embed_dim)\n        elif arch_specifier.endswith(\"fused-ldpnet\"):\n            self.projector = FusedLDPProjector(vision_backbone.embed_dim, llm_backbone.embed_dim)\n        else:\n            raise ValueError(f\"CobraVLM with `{arch_specifier = }` is not supported!\")\n\n        # self.t_embedder = TimestepEmbedder(llm_backbone.embed_dim)\n        # self.img_token_embedder = torch.nn.Embedding(4096, llm_backbone.embed_dim)\n        # self.img_token_embedder = GPT2Embeddings(embed_dim=llm_backbone.embed_dim, vocab_size=16384, max_position_embeddings=256+1, token_drop=0.0,)\n        # self.cls_embedding = LabelEmbedder(1000, llm_backbone.embed_dim, 0.1)\n\n        # torch.nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n        # torch.nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n\n        # Trackers\n        self.vision_backbone_requires_grad = False\n\n        # Set Module Keys =>> used in Checkpoint Saving / Model Loading\n        self.all_module_keys = [\"vision_backbone\", \"llm_backbone\", \"projector\"]\n        self.trainable_module_keys = []\n\n        # === Generation Utilities ===\n        #   => For computing likelihoods --> get tokens corresponding to \"True\", \"False\" and \"Yes\", \"No\"\n        self.string2idx = {}\n        for trigger_string in [\"True\", \"False\", \"Yes\", \"No\"] + [chr(ord(\"A\") + i) for i in range(26)]:\n            token_idx_list = self.llm_backbone.tokenizer.encode(trigger_string, add_special_tokens=False)\n            assert len(token_idx_list) == 1, f'String \"{trigger_string}\" is tokenized as more than one token!'\n            self.string2idx[trigger_string] = token_idx_list[0]\n            \n        self.eos_token_id = self.llm_backbone.tokenizer.eos_token_id\n\n    def mamba_generate(self, *args, **kwargs):\n        return MambaGenerationMixin.generate(self, *args, **kwargs)\n\n    def get_projector_weight_dtype(self):\n        #  dtype\n        for layer in self.projector.projector:\n            if isinstance(layer, torch.nn.Linear):\n                return layer.weight.dtype\n\n    # @torch.inference_mode()\n    # def generate(self, image: Image, prompt_text: str, **kwargs: str) -> str:\n    #     # For now, only support generation with a batch size of 1 for simplicity\n    #     image_transform, tokenizer = self.vision_backbone.image_transform, self.llm_backbone.tokenizer\n\n    #     # Prepare Inputs\n    #     input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n    #     pixel_values = image_transform(image)\n    #     if isinstance(pixel_values, torch.Tensor):\n    #         pixel_values = pixel_values[None, ...].to(self.device)\n    #     elif isinstance(pixel_values, dict):\n    #         pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n    #     else:\n    #         raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n    #     # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`\n    #     autocast_dtype = self.llm_backbone.half_precision_dtype\n    #     with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.enable_mixed_precision_training):\n    #         # fmt: off\n    #         generated_ids = self.mamba_generate(\n    #             input_ids=input_ids,            # Shape: [1, seq]\n    #             pixel_values=pixel_values,      # Shape: [1, 3, res, res] or Dict[str, Shape[1, 3, res, res]]\n    #             eos_token_id=self.eos_token_id,\n    #             **kwargs\n    #         )\n    #         # fmt: on\n\n    #     generated_text = tokenizer.decode(generated_ids[0, input_ids.shape[1] :], skip_special_tokens=True).strip()\n\n    #     return generated_text\n\n    def allocate_inference_cache(self, *args, **kwargs):\n        return self.llm_backbone.allocate_inference_cache(*args, **kwargs)\n       \n    @classmethod\n    def from_pretrained(\n        cls,\n        pretrained_checkpoint: Path,\n        model_id: str,\n        vision_backbone: VisionBackbone,\n        llm_backbone: LLMBackbone,\n        enable_mixed_precision_training: bool = True,\n        arch_specifier: str = \"gelu-mlp\",\n    ) -> CobraVLM:\n        \"\"\"Initialize a CobraVLM from a pretrained checkpoint, freezing all weights, tailored for inference.\"\"\"\n        vlm = cls(\n            model_id,\n            vision_backbone,\n            llm_backbone,\n            enable_mixed_precision_training=enable_mixed_precision_training,\n            arch_specifier=arch_specifier,\n        )\n\n        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)\n        # model_state_dict = torch.load(pretrained_checkpoint, map_location=\"cpu\")[\"model\"]\n        # assert (\n        #     \"projector\" in model_state_dict and \"llm_backbone\" in model_state_dict\n        # ), \"CobraVLM `from_pretrained` expects checkpoint with keys for `projector` AND `llm_backbone`!\"\n\n        # vlm.projector.load_state_dict(model_state_dict[\"projector\"])\n        # vlm.llm_backbone.load_state_dict(model_state_dict[\"llm_backbone\"])\n\n        # Freeze Weights\n        vlm.requires_grad_(False)\n        vlm.eval()\n\n        return vlm\n\n    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder:\n        prompt_initializer: Type[PromptBuilder] = self.llm_backbone.prompt_builder_fn\n        return prompt_initializer(self.model_family, system_prompt=system_prompt)\n\n    def freeze_backbones(self, stage: str) -> None:\n        \"\"\"\n        This function sets `requires_grad_` on each of the component modules explicitly, depending on stage.\n\n        We support two separate stages --> \"align\" and \"finetune\".\n            => \"align\" --> vision_backbone*, llm_backbone* are frozen; only the `projector` is trained.\n            => \"finetune\" --> vision_backbone* is frozen; both `projector` and `llm_backbone` are trained.\n\n        :param stage: Pretraining stage in < \"align\" | \"finetune\" | \"full-finetune\" >\n        \"\"\"\n        if stage == \"align\":\n            self.vision_backbone.requires_grad_(False)\n            self.llm_backbone.requires_grad_(False)\n            self.projector.requires_grad_(True)\n\n            # Add to `self.trainable_module_keys`\n            self.trainable_module_keys = [\"projector\"]\n\n            # Update Trackers\n            self.vision_backbone_requires_grad = False\n\n            # Explicitly Log Frozen / Trainable Components\n            overwatch.info(f\"[Frozen]     =>> Vision Backbone `{self.vision_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[Frozen]     =>> LLM Backbone `{self.llm_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> Projector `{self.arch_specifier}`\", ctx_level=1)\n\n        elif stage == \"finetune\":\n            self.vision_backbone.requires_grad_(False)\n            self.llm_backbone.requires_grad_(True)\n            self.projector.requires_grad_(True)\n\n            self.llm_backbone.vqvae.eval()\n            [p.requires_grad_(False) for p in self.llm_backbone.vqvae.parameters()]\n            self.llm_backbone.vqvae.requires_grad_(False)\n\n            # Add to `self.trainable_module_keys`\n            self.trainable_module_keys = [\"projector\", \"llm_backbone\"]\n\n            # Update Trackers\n            self.vision_backbone_requires_grad = False\n\n            # Explicitly Log Frozen / Unfrozen Components\n            overwatch.info(f\"[Frozen]     =>> Vision Backbone `{self.vision_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> LLM Backbone `{self.llm_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> Projector `{self.arch_specifier}`\", ctx_level=1)\n\n        elif stage == \"full-finetune\":\n            self.vision_backbone.dtype = torch.float32\n            self.vision_backbone.requires_grad_(True)\n            self.llm_backbone.requires_grad_(True)\n            self.projector.requires_grad_(True)\n\n            # Add to `self.trainable_module_keys`\n            self.trainable_module_keys = [\"vision_backbone\", \"projector\", \"llm_backbone\"]\n\n            # Update Trackers\n            self.vision_backbone_requires_grad = True\n\n            # Explicitly Log Frozen / Unfrozen Components\n            overwatch.info(f\"[TRAINABLE]  =>> Vision Backbone `{self.vision_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> LLM Backbone `{self.llm_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> Projector `{self.arch_specifier}`\", ctx_level=1)\n\n        else:\n            raise ValueError(f\"Stage `{stage}` is not supported for LLaVa! Try < align | finetune >\")\n\n    def load_from_checkpoint(self, stage: str, run_dir: Path, pretrained_checkpoint: Optional[Path] = None) -> None:\n        \"\"\"Load weights from checkpoint (if required by the given stage).\"\"\"\n        assert stage in {\"align\", \"finetune\", \"full-finetune\"}, f\"Stage {stage} is not supported!\"\n\n        # If we're running a `no-align` architecture, we're good!\n        if self.arch_specifier.startswith(\"no-align\"):\n            overwatch.info(\n                f\"CobraVLM with `{self.arch_specifier = }` does not require pretrained weights!\", ctx_level=1\n            )\n            return\n\n        # Otherwise, handle stage-specific logic!\n        if stage == \"align\":\n            overwatch.info(\"Stage `align` does not require pretrained weights =>> Starting Training\", ctx_level=1)\n            return\n\n        # Otherwise, load from `pretrained_checkpoint` or match on `run_dir` (s/+stage-finetune/+stage-align/g)\n        overwatch.info(\"Stage `finetune` requires `align` pretrained weights\", ctx_level=1)\n\n        # Config specifies path to a checkpoint to load\n        if pretrained_checkpoint is not None:\n            overwatch.info(f\"Loading from Provided Checkpoint `{pretrained_checkpoint}`\", ctx_level=1)\n            model_state_dict = torch.load(pretrained_checkpoint)[\"model\"]\n            self.projector.load_state_dict(model_state_dict[\"projector\"])\n\n            return\n\n        # [Contract] If no `pretrained_checkpoint`, assume `align` lives in the run directory; string substitution!\n        model, scale, _, seed = run_dir.name.split(\"+\")\n        align_dirs = [\n            d\n            for d in run_dir.parent.iterdir()\n            if (d.name.startswith(f\"{model}+{scale}\") and d.name.endswith(f\"+stage-align+{seed}\"))\n        ]\n        assert len(align_dirs) == 1, \"Multiple or No Valid Pretrained Directories Exist -- Double Check `runs`!\"\n        if (pretrained_checkpoint := (align_dirs[0] / \"checkpoints\" / \"latest-checkpoint.pt\")).exists():\n            overwatch.info(f\"Loading from Discovered Checkpoint `{pretrained_checkpoint}`\", ctx_level=1)\n            model_state_dict = torch.load(pretrained_checkpoint)[\"model\"]\n            self.projector.load_state_dict(model_state_dict[\"projector\"])\n        else:\n            raise ValueError(f\"Could not find valid `align` checkpoint at {pretrained_checkpoint}!\")\n\n    def get_fsdp_wrapping_policy(self) -> Callable:\n        \"\"\"Return an FSDP _or_policy over the policies returned by each individual backbone (and our VLM policy).\"\"\"\n        vision_fsdp_wrapping_policy = self.vision_backbone.get_fsdp_wrapping_policy()\n        llm_fsdp_wrapping_policy = self.llm_backbone.get_fsdp_wrapping_policy()\n\n        # Get Cobra Wrapping Policy =>> just a module wrapping policy around `self.projector`\n        cobra_fsdp_wrapping_policy = partial(\n            _module_wrap_policy,\n            module_classes={LinearProjector, MLPProjector, FusedMLPProjector, FusedLDPProjector},\n        )\n\n        # Return union (_or_) over constituent policies\n        #   => Note: there is *not* a fall-through policy; any module that isn't covered by the above constituents will\n        #            automatically be folded into the root VLM FSDP instance.\n        return partial(\n            _or_policy,\n            policies=[\n                vision_fsdp_wrapping_policy,\n                llm_fsdp_wrapping_policy,\n                cobra_fsdp_wrapping_policy,\n            ],\n        )\n\n    # Note =>> We're not explicitly subclassing `PreTrainedModel` because we don't need the bloat; however, `forward()`\n    #          *must* match the signature of a `{Model}ForCausalLM` so that we can inherit from `GenerationMixin`\n\n    # ruff: noqa: C901\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        multimodal_indices: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inference_params = None,\n        num_last_tokens: int = 0,\n    ) -> CausalLMOutputWithPast:\n        \"\"\"Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss).\"\"\"\n\n        if input_ids.shape[1] != 1: \n            # Run Visual Feature Extraction\n            with torch.set_grad_enabled(self.vision_backbone_requires_grad):\n                if isinstance(pixel_values, dict):\n                    patch_features = self.vision_backbone({k: pixel_values[k][multimodal_indices] for k in pixel_values})\n                elif pixel_values is None:  # For cache phase in mamba's generate()\n                    return self.llm_backbone(\n                    input_ids=input_ids,\n                    attention_mask=None,\n                    position_ids=None,\n                    past_key_values=past_key_values,\n                    inputs_embeds=inputs_embeds,\n                    labels=labels,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                    inference_params=inference_params,\n                    num_last_tokens=num_last_tokens,\n                )\n                else:\n                    patch_features = self.vision_backbone(pixel_values[multimodal_indices])\n\n            # Projection Logic :: [bsz, num_patches, llm_embed_dim] =>> num_patches = (2 *) (256 + 1) for ViT-L + CLS\n            projected_patch_embeddings = self.projector(patch_features)\n\n            # Get Input Embeddings from LLM Backbone :: [bsz, input_seq_len, llm_embed_dim]\n            input_embeddings = self.llm_backbone.embed_input_ids(input_ids) # input_ids: [bsz, input_seq_len]\n            SYSTEM_PROMPT_LEN = 28\n            input_embeddings_part1 = input_embeddings[:, :2+SYSTEM_PROMPT_LEN, :]\n            input_embeddings_part2 = input_embeddings[:, 2+SYSTEM_PROMPT_LEN:, :]\n            input_embeddings_partshape = [input_embeddings_part1.shape[1], projected_patch_embeddings.shape[1], input_embeddings_part2.shape[1]]\n            # Build Multimodal Embeddings\n            # multimodal_embeddings = torch.cat(\n            #     [\n            #         projected_patch_embeddings,\n            #         input_embeddings[multimodal_indices, :, :],\n            #     ],\n            #     dim=1,\n            # )\n            multimodal_embeddings = torch.cat((input_embeddings_part1, projected_patch_embeddings, input_embeddings_part2), dim=1)\n\n            #   => We'll ignore the per-token outputs for each of the patch embeddings as well!\n            multimodal_labels = None\n            if labels is not None:\n                projected_patch_labels = torch.full(\n                    (projected_patch_embeddings.shape[0], projected_patch_embeddings.shape[1]),\n                    IGNORE_INDEX,\n                    dtype=labels.dtype,\n                    device=labels.device,\n                )\n                multimodal_labels = torch.cat(\n                    [projected_patch_labels, labels[multimodal_indices, :]], dim=1\n                )\n\n            # === Add Unimodal Handling ===\n\n            # Create Fused Embeddings, Attention Mask, and Labels by Merging with \"unimodal\" Inputs (if applicable)\n            unimodal_indices = torch.tensor(\n                [idx for idx in range(len(input_ids)) if idx not in multimodal_indices],\n                dtype=torch.long,\n                device=multimodal_indices.device,\n            )\n\n            # No \"unimodal\" data --> Fused == Multimodal\n            if len(unimodal_indices) == 0:\n                fused_embeddings = multimodal_embeddings\n                fused_labels = multimodal_labels\n\n            else:\n                # Otherwise --> Merge w/ unimodal data\n\n                # This doesn't matter --> but in the \"normal\" case this is the embedding of the <PAD> token\n                #   => NOTE :: Verified that `zeros/randn/empty/<PAD> embedding` all return the same result!\n                unimodal_embeddings_pad = torch.zeros(\n                    (len(unimodal_indices), projected_patch_embeddings.shape[1], input_embeddings.shape[2]),\n                    dtype=input_embeddings.dtype,\n                    device=input_embeddings.device,\n                )\n                unimodal_labels_pad = torch.full(\n                    (len(unimodal_indices), projected_patch_embeddings.shape[1]),\n                    IGNORE_INDEX,\n                    dtype=labels.dtype,\n                    device=labels.device,\n                )\n\n                unimodal_embeddings = torch.cat([input_embeddings[unimodal_indices], unimodal_embeddings_pad], dim=1)\n                unimodal_labels = torch.cat([labels[unimodal_indices], unimodal_labels_pad], dim=1)\n\n                # Create \"Fused\" Tensors by Stacking Multimodal & Unimodal\n                fused_embeddings = torch.vstack([multimodal_embeddings, unimodal_embeddings])\n                fused_labels = torch.vstack([multimodal_labels, unimodal_labels])\n\n            # Run LLM Forward --> returns CausalLMOutputWithPast!\n            return self.llm_backbone(\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                past_key_values=past_key_values,\n                inputs_embeds=fused_embeddings,\n                labels=fused_labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                inference_params=inference_params,\n                shape=input_embeddings_partshape,\n                num_last_tokens=num_last_tokens,\n            )\n        else:\n            # input_embeddings = self.llm_backbone.embed_input_ids(input_ids)\n            if inference_params.seqlen_offset>0:\n                input_embeddings = self.img_token_embedder(input_ids, position_ids)\n            else:\n                input_embeddings = self.cls_embedding(input_ids, train=False)\n                input_embeddings = input_embeddings.squeeze(1)\n            return self.llm_backbone(\n                input_ids=None,\n                attention_mask=None,\n                position_ids=None,\n                past_key_values=past_key_values,\n                inputs_embeds=input_embeddings,\n                labels=None,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                inference_params=inference_params,\n                num_last_tokens=num_last_tokens,)\n\n\n    # === GenerationMixin Methods ===\n    #   => Note: The following methods override the functionality of `transformers.GenerationMixin`; these expect the\n    #            contract in each of the function signatures, and also expect our `forward` function to roughly take\n    #            the same arguments as the underlying LLM (see `LlamaModelForCausalLM` as an example)\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        **kwargs: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Borrowed from `LlamaForCausalLM` --> in general, just handles caching logic during generation.\"\"\"\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        # Make sure `pixel_values` are preserved in `model_inputs`\n        model_inputs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"pixel_values\": pixel_values,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n            }\n        )\n\n        return model_inputs\n\n    # @torch.inference_mode()\n    def generate_batch(\n        self,\n        pixel_values: Union[torch.Tensor, Dict[str, torch.Tensor]],\n        texts: List[str],\n        return_string_probabilities: Optional[List[str]] = None,\n        **kwargs: str,\n    ) -> Union[List[str], List[List[float]]]:\n        # For now, only support generation with a batch size of 1 for simplicity\n        tokenizer = self.llm_backbone.tokenizer\n\n        # Prepare Inputs\n        batch_input_ids = [\n            tokenizer(text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device) for text in texts\n        ]\n        if isinstance(pixel_values, torch.Tensor):\n            pixel_values = pixel_values[None, ...].to(self.device)\n        elif isinstance(pixel_values, dict):\n            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n        else:\n            raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n        # Create Output Lists\n        gen_texts, gen_probabilities = [], []\n\n        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`\n        autocast_dtype = self.llm_backbone.half_precision_dtype\n        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.enable_mixed_precision_training):\n            for idx, input_ids in enumerate(batch_input_ids):\n                if isinstance(pixel_values, torch.Tensor):\n                    pixel_values = pixel_values[idx]\n                elif isinstance(pixel_values, dict):\n                    pixel_values = {k: pixel_values[k][idx] for k in pixel_values}\n                else:\n                    raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n                # Handle `return_string_probabilities`\n                if return_string_probabilities is None:\n                    full_out_ids = self.mamba_generate(input_ids=input_ids, pixel_values=pixel_values, eos_token_id=self.eos_token_id, **kwargs)                    \n                    gen_ids = full_out_ids[0, input_ids.shape[1] :]\n\n                    # Decode `gen_ids` and strip any <EOS> tokens\n                    gen_texts.append(tokenizer.decode(gen_ids, skip_special_tokens=True).strip())\n\n                else:\n                    full_out_dict = self.mamba_generate(\n                        input_ids=input_ids,\n                        pixel_values=pixel_values,\n                        output_scores=True,\n                        return_dict_in_generate=True,\n                        eos_token_id=self.eos_token_id,\n                        **kwargs,\n                    )\n\n                    # Generation pattern should usually be [TOKEN] <EOS> for True/False and Yes/No Generations\n                    gen_ids = full_out_dict.sequences[0, input_ids.shape[1] :]\n\n                    # [Debug] Verify that the first token generated is in `self.string2idx.values()`\n                    # assert gen_ids[0] in self.string2idx.values(), \"Generated ID not in mapping!\"\n\n                    # Decode `gen_ids` and strip any <EOS> tokens\n                    gen_texts.append(tokenizer.decode(gen_ids, skip_special_tokens=True).strip())\n\n                    # Get all token probabilities --> softmax over logits\n                    token_probs = torch.softmax(full_out_dict.scores[0][0], dim=0)\n\n                    # Get *normalized* probabilities for all values in `return_token_probabilities`\n                    slice_idxs = torch.tensor([self.string2idx[s] for s in return_string_probabilities])\n                    string_probs_unnormalized = token_probs[slice_idxs]\n                    string_probs = string_probs_unnormalized / string_probs_unnormalized.sum()\n                    gen_probabilities.append(string_probs.cpu().numpy().tolist())\n        return gen_texts if return_string_probabilities is None else gen_probabilities\n\n    @torch.inference_mode()\n    def generate(self, image: Image, prompt_text: str, **kwargs: str) -> str:\n        # For now, only support generation with a batch size of 1 for simplicity\n        image_transform, tokenizer = self.vision_backbone.image_transform, self.llm_backbone.tokenizer\n\n        # Prepare Inputs\n        input_ids = tokenizer(prompt_text, truncation=True, return_tensors=\"pt\").input_ids.to(self.device)\n        pixel_values = image_transform(image)\n        if isinstance(pixel_values, torch.Tensor):\n            pixel_values = pixel_values[None, ...].to(self.device)\n        elif isinstance(pixel_values, dict):\n            pixel_values = {k: v[None, ...].to(self.device) for k, v in pixel_values.items()}\n        else:\n            raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`\n        autocast_dtype = self.llm_backbone.half_precision_dtype\n        with torch.autocast(\"cuda\", dtype=autocast_dtype, enabled=self.enable_mixed_precision_training):\n            # fmt: off\n            generated_ids = self.mamba_generate(\n                input_ids=input_ids,            # Shape: [1, seq]\n                pixel_values=pixel_values,      # Shape: [1, 3, res, res] or Dict[str, Shape[1, 3, res, res]]\n                eos_token_id=self.eos_token_id,\n                **kwargs\n            )\n            # fmt: on\n\n        generated_text = tokenizer.decode(generated_ids[0, input_ids.shape[1] :], skip_special_tokens=True).strip()\n\n        return generated_text\n"}
{"type": "source_file", "path": "models/stage2/config_mamba.py", "content": "from dataclasses import dataclass, field\n\n\n@dataclass\nclass MambaConfig:\n    d_model: int = 1024\n    d_intermediate: int = 0\n    n_layer: int = 48\n\n    # llamagen_t2i\n    vqvae_vocab_size: int = 16384\n    num_tokens: int = 256\n\n    vocab_size: int = 50277\n\n    ssm_cfg: dict = field(default_factory=lambda: {'layer': 'Mamba2'})\n    attn_layer_idx: list = field(default_factory=list)\n    attn_cfg: dict = field(default_factory=dict)\n    rms_norm: bool = True\n    residual_in_fp32: bool = True\n    fused_add_norm: bool = True\n\n    # for mamba_130m\n    pad_vocab_size_multiple: int = 16 \n    \n    tie_embeddings: bool = True\n    # update\n    num_classes: int = 1000\n    \n    # adaLN\n    adaln_group: bool = False\n    num_groups: int = 1\n    # dropout\n    token_drop: float = 0.0\n    mixer_drop: float = 0.0\n    mlp_drop: float = 0.0\n    # task\n    t2i_task: bool = False\n    mmu_task: bool = False"}
{"type": "source_file", "path": "models/stage2/generation.py", "content": "# References:\n#   Mamba:  https://github.com/state-spaces/mamba/blob/main/mamba_ssm/utils/generation.py\n\nimport gc\nimport time\nfrom collections import namedtuple\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import Callable, Optional, Sequence, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom torch import Tensor\nfrom torch.profiler import ProfilerActivity, profile, record_function\nfrom transformers.generation import GreedySearchDecoderOnlyOutput, SampleDecoderOnlyOutput, TextStreamer\n# from mamba_ssm.utils.generation import sample\n\n@dataclass\nclass InferenceParams:\n    \"\"\"Inference parameters that are passed to the main model in order\n    to efficienly calculate and store the context during inference.\"\"\"\n\n    max_seqlen: int\n    max_batch_size: int\n    seqlen_offset: int = 0\n    batch_size_offset: int = 0\n    key_value_memory_dict: dict = field(default_factory=dict)\n    lengths_per_sample: Optional[Tensor] = None\n\n    def reset(self, max_seqlen, max_batch_size):\n        self.max_seqlen = max_seqlen\n        self.max_batch_size = max_batch_size\n        self.seqlen_offset = 0\n        if self.lengths_per_sample is not None:\n            self.lengths_per_sample.zero_()\n\n\ndef modify_logits_for_min_p_filtering(logits, min_p):\n    \"\"\"Set the logits for none min_p values to -inf. Done in-place.\"\"\"\n    if (min_p <= 0.0).any() or (min_p >= 1.0).any():\n        return\n    indices_to_remove = logits < min_p\n    logits.masked_fill_(indices_to_remove, float(\"-Inf\"))\n\n\n# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py\n# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L231\ndef modify_logits_for_top_k_filtering(logits, top_k):\n    \"\"\"Set the logits for none top-k values to -inf. Done in-place.\"\"\"\n    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n    logits.masked_fill_(indices_to_remove, float(\"-Inf\"))\n\n\n# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py\n# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L170\ndef modify_logits_for_top_p_filtering(logits, top_p):\n    \"\"\"Set the logits for none top-p values to -inf. Done in-place.\"\"\"\n    if top_p <= 0.0 or top_p >= 1.0:\n        return\n    # First sort and calculate cumulative sum of probabilities.\n    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n    # scatter sorted tensors to original indexing\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        1, sorted_indices, sorted_indices_to_remove\n    )\n    logits.masked_fill_(indices_to_remove, float(\"-inf\"))\n\n\ndef modify_logit_for_repetition_penalty(logits, prev_output_tokens, repetition_penalty=1.0):\n    \"\"\"Apply repetition penalty. See https://arxiv.org/abs/1909.05858\n    logits: (batch_size, vocab_size)\n    prev_output_tokens: (batch_size, seq_len)\n    \"\"\"\n    if repetition_penalty == 1.0:\n        return logits\n    score = torch.gather(logits, 1, prev_output_tokens)\n    # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability\n    score = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)\n    logits.scatter_(1, prev_output_tokens, score)\n    return logits\n\n\ndef sample(logits, top_k=1, top_p=0.0, min_p=0.0, temperature=1.0):\n    \"\"\"Sample from top-k logits.\n    Arguments:\n        logits: Tensor of shape (batch_size, vocab_size)\n    \"\"\"\n    if top_k == 1:  # Short-circuit for greedy decoding\n        return logits.argmax(dim=-1)\n    else:\n        if top_p > 0.0:\n            assert top_p <= 1.0, \"top-p should be in (0, 1].\"\n        if top_k > 0:\n            top_k = min(top_k, logits.size(-1))  # Safety check\n            logits_top, indices = torch.topk(logits, top_k, dim=-1)\n            if temperature != 1.0:\n                logits_top /= temperature\n            modify_logits_for_top_p_filtering(logits_top, top_p)\n            return indices[\n                torch.arange(indices.shape[0], device=indices.device),\n                torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(dim=-1),\n            ]\n        else:\n            if min_p > 0.0:\n                logits_top = logits.clone()\n                max_prob, _ = (torch.softmax(logits_top, dim=-1)).max(dim=-1, keepdim=True)\n                min_prob = max_prob * min_p\n                modify_logits_for_min_p_filtering(logits_top, min_prob)\n                if temperature != 1.0:\n                    logits_top /= temperature\n                return torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(dim=-1)\n            # Clone so that when we modify for top_p we don't change the original logits\n            logits_top = logits / temperature if temperature != 1.0 else logits.clone()\n            modify_logits_for_top_p_filtering(logits_top, top_p)\n            return torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(\n                dim=-1\n            )\n\n\n\n@torch.inference_mode()\ndef decode(\n    input_ids,\n    input_embeddings,\n    model,\n    max_length,\n    top_k=1,\n    top_p=0.0,\n    min_p=0.0,\n    temperature=1.0,\n    repetition_penalty=1.0,\n    eos_token_id=None,\n    teacher_outputs=None,\n    vocab_size=None,\n    cg=False,\n    task='t2i',\n    enable_timing=False,\n    cond=None,  # added 2024-06-11\n    streamer: Optional[TextStreamer] = None\n):\n    \"\"\"Decoding, either greedy or with top-k or top-p sampling.\n    If top-k = 0, don't limit the number of candidates (pure sampling).\n    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,\n    then top-p.\n    We assume that all sequences in the same batch have the same length.\n\n    Arguments:\n        input_ids: (batch, seq_len)\n        max_length: int\n        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the\n            logits, the next token is taken from the teacher_outputs. Useful for testing.\n    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:\n        sequences: (batch, max_length)\n        scores: tuples of (batch, vocab_size)\n    \"\"\"\n    if streamer is not None:\n        streamer.put(input_ids.cpu())\n\n    batch_size, seqlen_og = input_ids.shape\n    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0\n    if cg:\n        if not hasattr(model, \"_decoding_cache\"):\n            model._decoding_cache = None\n        model._decoding_cache = update_graph_cache(\n            model,\n            model._decoding_cache,\n            batch_size,\n            seqlen_og,\n            max_length,\n            cond=cond,\n            task=task,\n        )\n        inference_params = model._decoding_cache.inference_params\n        inference_params.reset(max_length, batch_size)\n    else:\n        inference_params = InferenceParams(max_seqlen=max_length, max_batch_size=batch_size)\n\n    def get_logits(inference_params, input_ids=None, input_embeddings=None,  cond=None, task=None): # added 2024-06-11\n        decoding = inference_params.seqlen_offset > 0\n        device = input_ids.device if input_ids is not None else input_embeddings.device\n        if decoding:\n            position_ids = torch.full(\n                (batch_size, 1),\n                inference_params.seqlen_offset,\n                dtype=torch.long,\n                device=device,\n            )\n        else:\n            position_ids = None\n        if not cg or not decoding:\n            logits = model(\n                input_ids,\n                input_embeddings,\n                position_ids=position_ids,\n                cond=cond,\n                task=task,\n                inference_params=inference_params,\n                num_last_tokens=1,\n            )\n            if task == 'mmu':\n                logits = logits.mmu_logits.squeeze(dim=1)\n            if task == 't2i':\n                logits = logits.t2i_logits.squeeze(dim=1)\n        else:\n            logits = model._decoding_cache.run(\n                input_ids, position_ids, inference_params.seqlen_offset, cond=cond, task=task  # added 2024-06-11\n            ).squeeze(dim=1)\n        return logits[..., :vocab_size] if vocab_size is not None else logits\n\n    def sample_tokens(logits, inference_params):\n        if teacher_outputs is None or teacher_output_len <= inference_params.seqlen_offset:\n            token = sample(logits, top_k=top_k, top_p=top_p, min_p=min_p, temperature=temperature)\n        else:\n            token = teacher_outputs[:, inference_params.seqlen_offset]\n        # return rearrange(token, \"b -> b 1\")\n        return token.unsqueeze(1)\n\n    def should_stop(current_token, inference_params):\n        if inference_params.seqlen_offset == 0:\n            return False\n        if eos_token_id is not None and (current_token == eos_token_id).all():\n            return True\n        if inference_params.seqlen_offset >= max_length - 1:\n            return True\n        return False\n\n    start = torch.cuda.Event(enable_timing=enable_timing)\n    end = torch.cuda.Event(enable_timing=enable_timing)\n\n    if enable_timing:\n        start.record()\n    scores, sequences = [], [input_embeddings]\n    sequences_cat = input_ids\n    first = True\n    while not should_stop(sequences[-1], inference_params):\n        if first:\n            scores.append(get_logits(inference_params, input_embeddings = sequences[-1], cond=cond, task=task))   # added 2024-06-11\n            first = False\n        else:\n            scores.append(get_logits(inference_params, input_ids=sequences[-1], cond=cond, task=task))\n        inference_params.seqlen_offset += sequences[-1].shape[1]\n        if repetition_penalty == 1.0:\n            sampled_tokens = sample_tokens(scores[-1], inference_params)\n        else:\n            logits = modify_logit_for_repetition_penalty(\n                scores[-1].clone(), sequences_cat, repetition_penalty\n            )\n            sampled_tokens = sample_tokens(logits, inference_params)\n            sequences_cat = torch.cat([sequences_cat, sampled_tokens], dim=1)\n        sequences_cat = torch.cat([sequences_cat, sampled_tokens], dim=1)\n        sequences.append(sampled_tokens)\n        if streamer is not None:\n            streamer.put(sampled_tokens.cpu())\n    if streamer is not None:\n        streamer.end()\n    if enable_timing:\n        end.record()\n        torch.cuda.synchronize()\n        print(f\"Prompt processing + decoding time: {(start.elapsed_time(end)):.0f}ms\")\n    output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput\n    # return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))\n    return output_cls(sequences=sequences_cat, scores=tuple(scores))\n\n\nclass GenerationMixin:\n    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n        raise NotImplementedError\n\n    def generate(\n        self,\n        input_ids,\n        input_embeddings,\n        max_length,\n        top_k=1,\n        top_p=0.0,\n        min_p=0.0,\n        temperature=1.0,\n        eos_token_id=None,\n        return_dict_in_generate=False,\n        output_scores=False,\n        cond=None, # added new parameter\n        **kwargs,\n    ):\n        output = decode(\n            input_ids, input_embeddings, self, max_length, top_k=top_k, top_p=top_p, min_p = min_p, temperature=temperature, cond=cond, eos_token_id=eos_token_id, **kwargs\n        )\n        if not output_scores:\n            output.scores = None\n        return output if return_dict_in_generate else output.sequences\n\n\n@dataclass\nclass DecodingCGCache:\n    max_batch_size: int = 0\n    max_seqlen: int = 0\n    device = None\n    dtype = None\n    callables: dict = field(default_factory=dict)\n    mempool = None\n    inference_params: Optional[InferenceParams] = None\n    run: Optional[Callable] = None\n\n\n@torch.inference_mode()\ndef update_graph_cache(\n    model,\n    cache,\n    batch_size,\n    seqlen_og,\n    max_seqlen,\n    decoding_seqlens=(1,),\n    dtype=None,\n    n_warmups=2,\n    cond=None,   \n    task='t2i', \n):\n    if cache is None:\n        cache = DecodingCGCache()\n    param_example = next(iter(model.parameters()))\n    device = param_example.device\n    if dtype is None:\n        dtype = param_example.dtype\n    if (\n        (device, dtype) != (cache.device, cache.dtype)\n        or batch_size > cache.max_batch_size\n        or max_seqlen > cache.max_seqlen\n    ):  # Invalidate the cache\n        cache.callables = {}\n        cache.mempool = None\n        cache.inference_params = None\n        gc.collect()\n        cache.device, cache.dtype = device, dtype\n        cache.max_batch_size, cache.max_seqlen = batch_size, max_seqlen\n        assert hasattr(model, \"allocate_inference_cache\"), \"CUDA graph decoding requires that the model has a method allocate_inference_cache\"\n        inf_cache = model.allocate_inference_cache(batch_size, max_seqlen, dtype)\n        lengths_per_sample = torch.full((batch_size,), seqlen_og, dtype=torch.int32, device=device)\n        cache.inference_params = InferenceParams(\n            max_seqlen=max_seqlen,\n            max_batch_size=batch_size,\n            seqlen_offset=seqlen_og,\n            key_value_memory_dict=inf_cache,\n            lengths_per_sample=lengths_per_sample,\n        )\n        cache.mempool = torch.cuda.graphs.graph_pool_handle()\n    for decoding_seqlen in decoding_seqlens:\n        if (batch_size, decoding_seqlen) not in cache.callables:\n            cache.callables[batch_size, decoding_seqlen] = capture_graph(\n                model,\n                cache.inference_params,\n                batch_size,\n                max_seqlen,\n                decoding_seqlen=decoding_seqlen,\n                mempool=cache.mempool,\n                n_warmups=n_warmups,\n                cond=cond,   # added 2024-06-11\n                task=task,\n            )\n\n    def dispatch(input_ids, position_ids, seqlen, cond=None, task=None):\n        batch_size, decoding_seqlen = input_ids.shape[:2]\n        return cache.callables[batch_size, decoding_seqlen](input_ids, position_ids, seqlen, cond=cond, task=task)\n\n    cache.run = dispatch\n    cache.inference_params.seqlen_offset = 0  # Reset so it's not confusing\n    return cache\n\n\ndef capture_graph(\n    model, inference_params, batch_size, max_seqlen, decoding_seqlen=1, mempool=None, n_warmups=2, cond=None, task='t2i'    # added\n):\n    device = next(iter(model.parameters())).device\n    input_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)\n    input_embeddings = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)\n    position_ids = torch.full((batch_size, decoding_seqlen), 0, dtype=torch.long, device=device)\n    seqlen_offset_og = inference_params.seqlen_offset\n    inference_params.seqlen_offset = max_seqlen - decoding_seqlen\n    inference_params.lengths_per_sample[:] = inference_params.seqlen_offset\n\n    # Warmup before capture\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        for _ in range(n_warmups):\n            logits = model(\n                input_ids,\n                input_embeddings=None,\n                position_ids=position_ids,\n                cond=cond,  # added 2024-06-11\n                task=task,\n                inference_params=inference_params,\n                num_last_tokens=decoding_seqlen,\n            )\n            if task == 'mmu':\n                logits = logits.mmu_logits.squeeze(dim=1)\n            if task == 't2i':\n                logits = logits.t2i_logits.squeeze(dim=1)\n        s.synchronize()\n        # This might be needed for correctness if we run with NCCL_GRAPH_MIXING_SUPPORT=0,\n        # which requires that graph launch and non-captured launch to not overlap (I think,\n        # that's how I interpret the documentation). I'm not sure if this is required.\n        if torch.distributed.is_initialized():\n            torch.distributed.barrier()\n    torch.cuda.current_stream().wait_stream(s)\n    # Captures the graph\n    # To allow capture, automatically sets a side stream as the current stream in the context\n    graph = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(graph, pool=mempool):\n        logits = model(\n            input_ids,\n            input_embeddings=None,\n            position_ids=position_ids,\n            cond=cond,\n            task=task,\n            inference_params=inference_params,\n            num_last_tokens=decoding_seqlen,\n        )\n        if task == 'mmu':\n            logits = logits.mmu_logits.squeeze(dim=1)\n        if task == 't2i':\n            logits = logits.t2i_logits.squeeze(dim=1)\n\n    def run(new_input_ids, new_position_ids, seqlen, cond=None, task=None):\n        inference_params.lengths_per_sample[:] = seqlen\n        input_ids.copy_(new_input_ids)\n        position_ids.copy_(new_position_ids)\n        graph.replay()\n        return logits.clone()\n\n    inference_params.seqlen_offset = seqlen_offset_og\n    return run"}
{"type": "source_file", "path": "llamagen_tokenizer/validation/val_ddp.py", "content": "import torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.distributed as dist\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\nimport numpy as np\nimport argparse\nimport random\n\n\nclass SingleFolderDataset(Dataset):\n    def __init__(self, directory, transform=None):\n        super().__init__()\n        self.directory = directory\n        self.transform = transform\n        self.image_paths = [os.path.join(directory, file_name) for file_name in os.listdir(directory)\n                            if os.path.isfile(os.path.join(directory, file_name))]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(0)\n\n\ndef create_npz_from_sample_folder(sample_dir, num=50_000):\n    \"\"\"\n    Builds a single .npz file from a folder of .png samples.\n    \"\"\"\n    samples = []\n    for i in tqdm(range(num), desc=\"Building .npz file from samples\"):\n        sample_pil = Image.open(f\"{sample_dir}/{i:06d}.png\")\n        sample_np = np.asarray(sample_pil).astype(np.uint8)\n        samples.append(sample_np)\n\n    random.shuffle(samples) # This is very important for IS(Inception Score) !!!\n    samples = np.stack(samples)\n    assert samples.shape == (num, samples.shape[1], samples.shape[2], 3)\n    npz_path = f\"{sample_dir}.npz\"\n    np.savez(npz_path, arr_0=samples)\n    print(f\"Saved .npz file to {npz_path} [shape={samples.shape}].\")\n    return npz_path\n\n\ndef center_crop_arr(pil_image, image_size):\n    \"\"\"\n    Center cropping implementation from ADM.\n    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n    \"\"\"\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n\n\ndef main(args):\n    # Setup PyTorch:\n    assert torch.cuda.is_available(), \"Sampling with DDP requires at least one GPU. sample.py supports CPU-only usage\"\n    torch.set_grad_enabled(False)\n\n    # Setup env\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # Create folder to save samples:\n    folder_name = f\"val_{args.dataset}\"\n    sample_folder_dir = f\"{args.sample_dir}/{folder_name}\"\n    if rank == 0:\n        os.makedirs(sample_folder_dir, exist_ok=True)\n        print(f\"Saving .png samples at {sample_folder_dir}\")\n    dist.barrier()\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n\n    if args.dataset == 'imagenet':\n        dataset = ImageFolder(args.data_path, transform=transform)\n        num_fid_samples = 50000\n    elif args.dataset == 'coco':\n        dataset = SingleFolderDataset(args.data_path, transform=transform)\n        num_fid_samples = 5000\n    else:\n        raise Exception(\"please check dataset\")\n    \n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=False,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=args.per_proc_batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=False\n    )    \n\n    # Figure out how many samples we need to generate on each GPU and how many iterations we need to run:\n    n = args.per_proc_batch_size\n    global_batch_size = n * dist.get_world_size()\n\n    loader = tqdm(loader) if rank == 0 else loader\n    total = 0\n    for x, _ in loader:\n        samples = torch.clamp(127.5 * x + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n        # Save samples to disk as individual .png files\n        for i, sample in enumerate(samples):\n            index = i * dist.get_world_size() + rank + total\n            Image.fromarray(sample).save(f\"{sample_folder_dir}/{index:06d}.png\")\n\n        total += global_batch_size\n\n    # Make sure all processes have finished saving their samples before attempting to convert to .npz\n    dist.barrier()\n    if rank == 0:\n        create_npz_from_sample_folder(sample_folder_dir, num_fid_samples)\n        print(\"Done.\")\n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, choices=['imagenet', 'coco'], default='imagenet')\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n    parser.add_argument(\"--sample-dir\", type=str, default=\"reconstructions\")\n    parser.add_argument(\"--per-proc-batch-size\", type=int, default=32)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=4)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "models/omnimamba.py", "content": "\"\"\"\ncobra.py\n\nPyTorch Module defining a CobraVLM, our general interface for defining the various different VLMs in our work.\n\nNotes:\n    - For now, we don't subclass `transformers.PretrainedModel` (or CausalLM). Instead, we assume a very limited subset\n      of the {Model}ForCausalLM API that enables dispatch to the underlying LLM's `generate` utilities (feeding inputs\n      through our custom projection shim).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Optional, Type\n\nfrom huggingface_hub import PyTorchModelHubMixin\n\nimport torch\nfrom torch import nn\nfrom models.cobra.backbones.llm.prompting import PromptBuilder\nfrom models.cobra.overwatch import initialize_overwatch\nfrom models.cobra.nn_utils import FusedMLPProjector, LinearProjector, MLPProjector, FusedLDPProjector\nfrom models.cobra.materialize import get_vision_backbone_and_transform\nfrom models.mamba_vlm import MambaVLMs\n\n\n# Initialize Overwatch =>> Wraps `logging.Logger`\noverwatch = initialize_overwatch(__name__)\n\n\n# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)\nIGNORE_INDEX = -100\n\n\ndef print_trainable_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total trainable parameters: {total_params}\")\n    print(\"Trainable parameters:\")\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(f\"  {name}: {param.numel()}\")\n\n\nclass OmniMamba(nn.Module,\n                PyTorchModelHubMixin,\n                repo_url=\"https://github.com/hustvl/OmniMamba\",\n                paper_url=\"https://arxiv.org/abs/2407.11015\",\n                pipeline_tag=\"any-to-any\",\n                license=\"mit\"):\n    def __init__(\n        self,\n        args,\n        arch_specifier= \"fused-gelu-mlp\",\n        stage='finetune',\n    ):\n        super().__init__()\n        self.args = args\n        self.vision_backbone, self.image_transform = get_vision_backbone_and_transform(args.image_backbone)\n        self.llm_backbone = MambaVLMs[args.omnimamba_model](args.t2i_task, args.mmu_task)\n        # Set Weight Initialization Seed for Projector Consistency\n        if stage != 'inference':\n            torch.manual_seed(self.vision_backbone.embed_dim)\n        self.llm_backbone.mamba.backbone.stage = stage\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n        # Initialize Projection (Adapter) based on `arch_specifier`\n        if args.mmu_task:\n            self.arch_specifier = arch_specifier\n            if arch_specifier == \"linear\":\n                self.projector = LinearProjector(self.vision_backbone.embed_dim, self.llm_backbone.embed_dim)\n            elif arch_specifier.endswith(\"fused-gelu-mlp\"):\n                self.projector = FusedMLPProjector(self.vision_backbone.embed_dim, self.llm_backbone.d_model)\n            elif arch_specifier.endswith(\"gelu-mlp\"):\n                self.projector = MLPProjector(self.vision_backbone.embed_dim, self.llm_backbone.embed_dim)\n            elif arch_specifier.endswith(\"fused-ldpnet\"):\n                self.projector = FusedLDPProjector(self.vision_backbone.embed_dim, self.llm_backbone.embed_dim)\n            else:\n                raise ValueError(f\"CobraVLM with `{arch_specifier = }` is not supported!\")\n\n            \n        self.eos_token_id = self.llm_backbone.tokenizer.eos_token_id\n        \n        self.load_pretrain_model(args)\n        overwatch.info(\"Load pretrain model done\", ctx_level=1)\n        self.freeze_backbones(stage=stage)\n        overwatch.info(\"Freeze backbones done\", ctx_level=1)\n        \n\n    def load_pretrain_model(self, args):\n        if args.vq_ckpt is not None:\n            vqvae_state_dict = torch.load(args.vq_ckpt, map_location=\"cpu\")\n            if 'quantize.codebook_used' in vqvae_state_dict:\n                vqvae_state_dict.pop('quantize.codebook_used')\n            self.llm_backbone.vqvae.load_state_dict(vqvae_state_dict)\n        if args.omnimamba_ckpt is not None:\n            overwatch.info(f\"Loading omnimamba model from {args.omnimamba_ckpt}\", ctx_level=1)\n            state_dict = torch.load(args.omnimamba_ckpt, map_location=\"cpu\")\n            self.load_state_dict(state_dict)\n        else:\n            if args.mamba_pretrain is not None:\n                mamba_state_dict = torch.load(args.mamba_pretrain, map_location=\"cpu\")\n                self.llm_backbone.mamba.load_state_dict(mamba_state_dict, strict=False) \n        if args.mmu_task:\n            self.llm_backbone.resize_token_embeddings(len(self.llm_backbone.uni_prompting.text_tokenizer), pad_to_multiple_of=self.llm_backbone.pad_vocab_size_multiple)         \n\n    def get_projector_weight_dtype(self):\n        #  dtype\n        for layer in self.projector.projector:\n            if isinstance(layer, torch.nn.Linear):\n                return layer.weight.dtype\n\n    def allocate_inference_cache(self, *args, **kwargs):\n        return self.llm_backbone.allocate_inference_cache(*args, **kwargs)\n       \n\n    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder:\n        prompt_initializer: Type[PromptBuilder] = self.llm_backbone.prompt_builder_fn\n        return prompt_initializer(self.model_family, system_prompt=system_prompt)\n\n    def freeze_backbones(self, stage: str) -> None:\n        \"\"\"\n        This function sets `requires_grad_` on each of the component modules explicitly, depending on stage.\n\n        We support two separate stages --> \"align\" and \"finetune\".\n            => \"align\" --> vision_backbone*, llm_backbone* are frozen; only the `projector` is trained.\n            => \"finetune\" --> vision_backbone* is frozen; both `projector` and `llm_backbone` are trained.\n\n        :param stage: Pretraining stage in < \"align\" | \"finetune\" | \"full-finetune\" >\n        \"\"\"\n        if stage == \"align\":\n            self.train()\n            self.vision_backbone.eval()\n            self.vision_backbone.requires_grad_(False)\n            self.llm_backbone.eval()\n            self.llm_backbone.requires_grad_(False)\n            if self.args.t2i_task:\n                self.llm_backbone.mamba.backbone.img_embeddings.train()\n                self.llm_backbone.mamba.backbone.img_embeddings.requires_grad_(True)\n                self.llm_backbone.mamba.backbone.embedding.requires_grad_(True)\n                self.llm_backbone.mamba.backbone.pos_embed.requires_grad_(True)\n                self.llm_backbone.mamba.backbone.caption_embed.train()\n                self.llm_backbone.mamba.backbone.caption_embed.requires_grad_(True)\n                self.llm_backbone.mamba.img_head.train()\n                self.llm_backbone.mamba.img_head.requires_grad_(True)\n                # lora\n                for name, module in self.llm_backbone.mamba.backbone.named_modules():\n                    if 'lora' in name.lower():\n                        module.train()\n                        for param in module.parameters():\n                            param.requires_grad_(True)\n            if self.args.mmu_task:\n                self.projector.train()\n                self.projector.requires_grad_(True)\n                # lora\n                for name, module in self.llm_backbone.mamba.backbone.named_modules():\n                    if 'lora' in name.lower():\n                        module.train()\n                        for param in module.parameters():\n                            param.requires_grad_(True)\n\n            # Explicitly Log Frozen / Trainable Components\n            overwatch.info(f\"[Frozen]     =>> Vision Backbone `{self.vision_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[Frozen]     =>> LLM Backbone `{self.llm_backbone.identifier}`\", ctx_level=1)\n            if self.args.mmu_task:\n                overwatch.info(f\"[TRAINABLE]  =>> ALL Projector `{self.arch_specifier}`\", ctx_level=1)\n\n        elif stage == \"finetune\":\n            self.train()\n            self.vision_backbone.eval()\n            self.vision_backbone.requires_grad_(False)\n            self.llm_backbone.mamba.train()\n            self.llm_backbone.mamba.requires_grad_(True)\n            self.llm_backbone.vqvae.eval()\n            self.llm_backbone.vqvae.requires_grad_(False)\n            if self.args.mmu_task:\n                self.projector.train()\n                self.projector.requires_grad_(True)\n\n            # Explicitly Log Frozen / Unfrozen Components\n            overwatch.info(f\"[Frozen]     =>> Vision Backbone `{self.vision_backbone.identifier}`\", ctx_level=1)\n            overwatch.info(f\"[Frozen]     =>> VQVAE\", ctx_level=1)\n            overwatch.info(f\"[TRAINABLE]  =>> LLM Backbone `{self.llm_backbone.identifier}`\", ctx_level=1)\n            if self.args.mmu_task:\n                overwatch.info(f\"[TRAINABLE]  =>> Projector `{self.arch_specifier}`\", ctx_level=1)\n        elif stage == 'inference':\n            self.eval()\n            self.requires_grad_(False)\n        else:\n            raise ValueError(f\"Stage `{stage}` is not supported for LLaVa! Try < align | finetune >\")\n\n    def mmu_multi_ids2embed(self, pixel_values_mmu, input_ids_mmu, labels_mmu):\n        input_ids_mmu = torch.cat([\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|mmu|>']).to(input_ids_mmu.device),\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|soi|>']).to(input_ids_mmu.device),\n            # image_tokens_mmu, place img embedding here\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|eoi|>']).to(input_ids_mmu.device),\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|sot|>']).to(input_ids_mmu.device),\n            input_ids_mmu,\n            # <eot> is in dataset\n        ], dim=1).long()\n\n        images_feat = self.vision_backbone(pixel_values_mmu)    \n        images_embeddings = self.projector(images_feat)\n        text_embeddings = self.llm_backbone.embed_input_ids(input_ids_mmu)\n\n        part1 = text_embeddings[:, :2, :]\n        part2 = text_embeddings[:, 2:, :]\n        input_embeddings_mmu = torch.cat((part1, images_embeddings, part2), dim=1)\n\n        labels_mmu = torch.cat([\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # mmu\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # soi\n            torch.ones_like(images_embeddings[:, :, 0]) * self.llm_backbone.uni_prompting.ignore_id,  # ignore image embedding\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # eoi\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # sot\n            labels_mmu.to(input_ids_mmu.device)\n            # <eot> is in dataset\n        ], dim=1).long()\n        return input_embeddings_mmu, labels_mmu\n\n    # for text only\n    def mmu_uni_ids2embed(self, pixel_values_mmu, input_ids_mmu, labels_mmu):\n        input_ids_mmu = torch.cat([\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|mmu|>']).to(input_ids_mmu.device),\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|soi|>']).to(input_ids_mmu.device),\n            # image_tokens_mmu, place img embedding here\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|eoi|>']).to(input_ids_mmu.device),\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.sptids_dict['<|sot|>']).to(input_ids_mmu.device),\n            input_ids_mmu,\n            # <eot> is in dataset\n        ], dim=1).long()\n\n        \n        text_embeddings = self.llm_backbone.embed_input_ids(input_ids_mmu)\n        pad_images_embeddings = torch.zeros(input_ids_mmu.shape[0], self.llm_backbone.mamba.backbone.img_sq_len, text_embeddings.shape[-1], device=text_embeddings.device)        \n        \n        part1 = text_embeddings[:, :2, :]\n        part2 = text_embeddings[:, 2:, :]\n        input_embeddings_mmu = torch.cat((part1, pad_images_embeddings, part2), dim=1)\n        \n        labels_mmu = torch.cat([\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # mmu\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # soi\n            torch.ones_like(pad_images_embeddings[:, :, 0]) * self.llm_backbone.uni_prompting.ignore_id,  # ignore image embedding\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # eoi\n            (torch.ones(input_ids_mmu.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(input_ids_mmu.device), # sot\n            labels_mmu.to(input_ids_mmu.device)\n            # <eot> is in dataset\n        ], dim=1).long()\n        return input_embeddings_mmu, labels_mmu    \n\n\n    def forward(self, inputs, task='t2i'):\n        if task == 't2i':\n\n            # t2i task\n            # pretokenized data\n            image_ids_t2i = inputs[\"t2i_flow\"]['inputs']\n            caption_ids_t2i = inputs[\"t2i_flow\"]['caption_ids']\n\n            image_embeddings_t2i = self.llm_backbone.mamba.backbone.img_embeddings(image_ids_t2i)\n\n            text_embeddings_t2i = self.llm_backbone.embed_input_ids(caption_ids_t2i)\n            text_embeddings_t2i = self.llm_backbone.mamba.backbone.caption_embed(text_embeddings_t2i, train=True)\n            input_embeddings_t2i = torch.cat((text_embeddings_t2i[:, :-1, :], image_embeddings_t2i, text_embeddings_t2i[:, -1:, :]), dim=1)\n\n\n            labels_t2i = torch.cat([\n                (torch.ones(image_ids_t2i.shape[0], (caption_ids_t2i.shape[1]-1)) * self.llm_backbone.uni_prompting.ignore_id).to(image_embeddings_t2i.device), # class id\n                image_ids_t2i,  # image ids\n                (torch.ones(image_ids_t2i.shape[0], 1) * self.llm_backbone.uni_prompting.ignore_id).to(image_embeddings_t2i.device), # eoi\n            ], dim=1).long()\n\n            pos_embed = self.llm_backbone.mamba.backbone.pos_embed.repeat(input_embeddings_t2i.shape[0], 1, 1)\n            input_embeddings_t2i = input_embeddings_t2i + pos_embed[:, :input_embeddings_t2i.shape[1]]\n            logits, target = self.llm_backbone(input_embeddings_t2i, labels_t2i, cond=None, task='t2i')\n            logits = logits.view(-1, logits.shape[-1])\n            target = target.view(-1)\n\n            loss_t2i = self.loss_fn(logits, target)\n            return loss_t2i\n        elif task == 'mmu':\n            # mmu task\n            pixel_values_mmu, input_ids_mmu, labels_mmu, multimodal_indices = (inputs[\"mmu_flow\"][\"pixel_values\"],\n                                                        inputs[\"mmu_flow\"][\"input_ids\"],\n                                                        inputs[\"mmu_flow\"][\"labels\"],\n                                                        inputs[\"mmu_flow\"][\"multimodal_indices\"])\n    \n            # no text only data\n            if len(multimodal_indices) == input_ids_mmu.shape[0]:\n                input_embeddings_mmu, labels_mmu = self.mmu_multi_ids2embed(pixel_values_mmu, input_ids_mmu, labels_mmu)\n            # all text only data\n            elif len(multimodal_indices) == 0:\n                input_embeddings_mmu, labels_mmu = self.mmu_uni_ids2embed(pixel_values_mmu, input_ids_mmu, labels_mmu)\n            else:\n                all_indices = torch.arange(pixel_values_mmu['dino'].shape[0], device=multimodal_indices.device)\n                uni_indices = all_indices[~torch.isin(all_indices, multimodal_indices)]\n                # multi\n                multi_input_embeddings_mmu, multi_labels_mmu = self.mmu_multi_ids2embed({'dino': pixel_values_mmu['dino'][multimodal_indices], 'siglip': pixel_values_mmu['siglip'][multimodal_indices]}, input_ids_mmu[multimodal_indices], labels_mmu[multimodal_indices])\n                # uni text only data\n                uni_input_embeddings_mmu, uni_labels_mmu = self.mmu_uni_ids2embed(None, input_ids_mmu[uni_indices], labels_mmu[uni_indices])\n                \n                input_embeddings_mmu = torch.cat((multi_input_embeddings_mmu, uni_input_embeddings_mmu), dim=0)\n                labels_mmu = torch.cat((multi_labels_mmu, uni_labels_mmu), dim=0)\n\n            logits, target = self.llm_backbone(input_embeddings_mmu, labels_mmu, cond=None, task='mmu')\n            loss_mmu = self.loss_fn(logits, target)\n            return loss_mmu\n        \n\n\n    def t2i_generate(self, text_ids=None, temperature=1.0, top_k=0, top_p=1.0, fast=True):\n        caption_embeddings = self.llm_backbone.embed_input_ids(text_ids)\n        input_embeddings_t2i = caption_embeddings\n        input_embeddings_t2i = self.llm_backbone.mamba.backbone.caption_embed(input_embeddings_t2i, train=False)\n\n        # position embedding\n        pos_embed = self.llm_backbone.mamba.backbone.pos_embed.repeat(input_embeddings_t2i.shape[0], 1, 1)\n        input_embeddings_t2i = input_embeddings_t2i + pos_embed[:, :input_embeddings_t2i.shape[1]]\n        \n        input_ids_t2i = text_ids\n        max_length = self.llm_backbone.num_tokens + input_embeddings_t2i.shape[1]\n        x = self.llm_backbone.mamba.generate(input_ids=input_ids_t2i,\n                                input_embeddings=input_embeddings_t2i,\n                                cond=None,\n                                max_length=max_length,\n                                temperature=temperature,\n                                top_p=top_p,\n                                top_k=top_k,\n                                cg=fast,\n                                task='t2i',)\n        \n        self.llm_backbone.mamba._decoding_cache = None\n\n        tokens =  x[:text_ids.shape[0], input_embeddings_t2i.shape[1]:]\n\n        imgs = self.llm_backbone.decode_to_img(tokens)\n        return imgs\n"}
{"type": "source_file", "path": "llamagen_tokenizer/vqgan/taming_vqgan_demo.py", "content": "import argparse\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom tokenizer.vqgan.model import VQModel\nfrom tokenizer.vqgan.model import VQGAN_FROM_TAMING\n\n# before running demo, make sure to:\n# (1) download all needed models from https://github.com/CompVis/taming-transformers and put in pretrained_models/\n# (2) pip install pytorch_lightning\n# (3) python3 tools/convert_pytorch_lightning_to_torch.py\n# (4) pip uninstall pytorch_lightning\n\n\ndef main(args):\n    # Setup PyTorch:\n    torch.manual_seed(args.seed)\n    torch.set_grad_enabled(False)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # create and load model\n    cfg, ckpt = VQGAN_FROM_TAMING[args.vqgan]\n    config = OmegaConf.load(cfg)\n    model = VQModel(**config.model.get(\"params\", dict()))\n    model.init_from_ckpt(ckpt)\n    model.to(device)\n    model.eval()\n\n    # load image\n    img_path = args.image_path\n    out_path = args.image_path.replace('.jpg', '_vqgan.jpg').replace('.jpeg', '_vqgan.jpeg').replace('.png', '_vqgan.png')\n    input_size = args.image_size\n    img = Image.open(img_path).convert(\"RGB\")\n\n    # preprocess\n    size_org = img.size\n    img = img.resize((input_size, input_size))\n    img = np.array(img) / 255.\n    x = 2.0 * img - 1.0 # x value is between [-1, 1]\n    x = torch.tensor(x)\n    x = x.unsqueeze(dim=0)\n    x = torch.einsum('nhwc->nchw', x)\n    x_input = x.float().to(\"cuda\")\n\n    # inference\n    with torch.no_grad():\n        latent, _, [_, _, indices] = model.encode(x_input)\n        output = model.decode_code(indices, latent.shape) # output value is between [-1, 1]\n\n    # postprocess\n    output = F.interpolate(output, size=[size_org[1], size_org[0]], mode='bilinear').permute(0, 2, 3, 1)[0]\n    sample = torch.clamp(127.5 * output + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8).numpy()\n\n    # save        \n    Image.fromarray(sample).save(out_path)\n    print(\"Reconstructed image is saved to {}\".format(out_path))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--image-path\", type=str, default=\"assets/example.jpg\")\n    parser.add_argument(\"--vqgan\", type=str, choices=list(VQGAN_FROM_TAMING.keys()), default=\"vqgan_openimage_f8_16384\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512, 1024], default=512)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/vq_train.py", "content": "# Modified from:\n#   fast-DiT: https://github.com/chuanyangjin/fast-DiT/blob/main/train.py\n#   nanoGPT: https://github.com/karpathy/nanoGPT/blob/master/model.py\nimport torch\n# the first flag below was False when we tested this script but True makes A100 training a lot faster:\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\n\nimport os\nimport time\nimport argparse\nfrom glob import glob\nfrom copy import deepcopy\n\nfrom utils.logger import create_logger\nfrom utils.distributed import init_distributed_mode\nfrom utils.ema import update_ema, requires_grad\nfrom dataset.augmentation import random_crop_arr\nfrom dataset.build import build_dataset\nfrom tokenizer.tokenizer_image.vq_model import VQ_models\nfrom tokenizer.tokenizer_image.vq_loss import VQLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#################################################################################\n#                                  Training Loop                                #\n#################################################################################\n\ndef main(args):\n    \"\"\"\n    Trains a new model.\n    \"\"\"\n    assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n    \n    # Setup DDP:\n    init_distributed_mode(args)\n    assert args.global_batch_size % dist.get_world_size() == 0, f\"Batch size must be divisible by world size.\"\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n\n    # Setup an experiment folder:\n    if rank == 0:\n        os.makedirs(args.results_dir, exist_ok=True)  # Make results folder (holds all experiment subfolders)\n        experiment_index = len(glob(f\"{args.results_dir}/*\"))\n        model_string_name = args.vq_model.replace(\"/\", \"-\")\n        experiment_dir = f\"{args.results_dir}/{experiment_index:03d}-{model_string_name}\"  # Create an experiment folder\n        checkpoint_dir = f\"{experiment_dir}/checkpoints\"  # Stores saved model checkpoints\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        logger = create_logger(experiment_dir)\n        logger.info(f\"Experiment directory created at {experiment_dir}\")\n\n        time_record = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n        cloud_results_dir = f\"{args.cloud_save_path}/{time_record}\"\n        cloud_checkpoint_dir = f\"{cloud_results_dir}/{experiment_index:03d}-{model_string_name}/checkpoints\"\n        os.makedirs(cloud_checkpoint_dir, exist_ok=True)\n        logger.info(f\"Experiment directory created in cloud at {cloud_checkpoint_dir}\")\n    \n    else:\n        logger = create_logger(None)\n\n    # training args\n    logger.info(f\"{args}\")\n\n    # training env\n    logger.info(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # create and load model\n    vq_model = VQ_models[args.vq_model](\n        codebook_size=args.codebook_size,\n        codebook_embed_dim=args.codebook_embed_dim,\n        commit_loss_beta=args.commit_loss_beta,\n        entropy_loss_ratio=args.entropy_loss_ratio,\n        dropout_p=args.dropout_p,\n    )\n    logger.info(f\"VQ Model Parameters: {sum(p.numel() for p in vq_model.parameters()):,}\")\n    if args.ema:\n        ema = deepcopy(vq_model).to(device)  # Create an EMA of the model for use after training\n        requires_grad(ema, False)\n        logger.info(f\"VQ Model EMA Parameters: {sum(p.numel() for p in ema.parameters()):,}\")\n    vq_model = vq_model.to(device)\n\n    vq_loss = VQLoss(\n        disc_start=args.disc_start, \n        disc_weight=args.disc_weight,\n        disc_type=args.disc_type,\n        disc_loss=args.disc_loss,\n        gen_adv_loss=args.gen_loss,\n        image_size=args.image_size,\n        perceptual_weight=args.perceptual_weight,\n        reconstruction_weight=args.reconstruction_weight,\n        reconstruction_loss=args.reconstruction_loss,\n        codebook_weight=args.codebook_weight,  \n    ).to(device)\n    logger.info(f\"Discriminator Parameters: {sum(p.numel() for p in vq_loss.discriminator.parameters()):,}\")\n\n    # initialize a GradScaler. If enabled=False scaler is a no-op\n    scaler = torch.cuda.amp.GradScaler(enabled=(args.mixed_precision =='fp16'))\n    scaler_disc = torch.cuda.amp.GradScaler(enabled=(args.mixed_precision =='fp16'))\n    # Setup optimizer\n    optimizer = torch.optim.Adam(vq_model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))\n    optimizer_disc = torch.optim.Adam(vq_loss.discriminator.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: random_crop_arr(pil_image, args.image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n    dataset = build_dataset(args, transform=transform)\n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=True,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=int(args.global_batch_size // dist.get_world_size()),\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=True\n    )\n    logger.info(f\"Dataset contains {len(dataset):,} images ({args.data_path})\")\n    \n\n    # Prepare models for training:\n    if args.vq_ckpt:\n        checkpoint = torch.load(args.vq_ckpt, map_location=\"cpu\")\n        vq_model.load_state_dict(checkpoint[\"model\"])\n        if args.ema:\n            ema.load_state_dict(checkpoint[\"ema\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n        vq_loss.discriminator.load_state_dict(checkpoint[\"discriminator\"])\n        optimizer_disc.load_state_dict(checkpoint[\"optimizer_disc\"])\n        if not args.finetune:\n            train_steps = checkpoint[\"steps\"] if \"steps\" in checkpoint else int(args.vq_ckpt.split('/')[-1].split('.')[0])\n            start_epoch = int(train_steps / int(len(dataset) / args.global_batch_size))\n            train_steps = int(start_epoch * int(len(dataset) / args.global_batch_size))\n        else:\n            train_steps = 0\n            start_epoch = 0           \n        del checkpoint\n        logger.info(f\"Resume training from checkpoint: {args.vq_ckpt}\")\n        logger.info(f\"Initial state: steps={train_steps}, epochs={start_epoch}\")\n    else:\n        train_steps = 0\n        start_epoch = 0\n        if args.ema:\n            update_ema(ema, vq_model, decay=0)  # Ensure EMA is initialized with synced weights\n    \n    if args.compile:\n        logger.info(\"compiling the model... (may take several minutes)\")\n        vq_model = torch.compile(vq_model) # requires PyTorch 2.0        \n    \n    vq_model = DDP(vq_model.to(device), device_ids=[args.gpu])\n    vq_model.train()\n    if args.ema:\n        ema.eval()  # EMA model should always be in eval mode\n    vq_loss = DDP(vq_loss.to(device), device_ids=[args.gpu])\n    vq_loss.train()\n\n    ptdtype = {'none': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16}[args.mixed_precision]\n\n    # Variables for monitoring/logging purposes:\n    log_steps = 0\n    running_loss = 0\n    start_time = time.time()\n\n    logger.info(f\"Training for {args.epochs} epochs...\")\n    for epoch in range(start_epoch, args.epochs):\n        sampler.set_epoch(epoch)\n        logger.info(f\"Beginning epoch {epoch}...\")\n        for x, y in loader:\n            imgs = x.to(device, non_blocking=True)\n\n            # generator training\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast(dtype=ptdtype):  \n                recons_imgs, codebook_loss = vq_model(imgs)\n                loss_gen = vq_loss(codebook_loss, imgs, recons_imgs, optimizer_idx=0, global_step=train_steps+1, \n                                   last_layer=vq_model.module.decoder.last_layer, \n                                   logger=logger, log_every=args.log_every)\n            scaler.scale(loss_gen).backward()\n            if args.max_grad_norm != 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(vq_model.parameters(), args.max_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n            if args.ema:\n                update_ema(ema, vq_model.module._orig_mod if args.compile else vq_model.module)\n\n            # discriminator training            \n            optimizer_disc.zero_grad()\n            with torch.cuda.amp.autocast(dtype=ptdtype):\n                loss_disc = vq_loss(codebook_loss, imgs, recons_imgs, optimizer_idx=1, global_step=train_steps+1,\n                                    logger=logger, log_every=args.log_every)\n            scaler_disc.scale(loss_disc).backward()\n            if args.max_grad_norm != 0.0:\n                scaler_disc.unscale_(optimizer_disc)\n                torch.nn.utils.clip_grad_norm_(vq_loss.module.discriminator.parameters(), args.max_grad_norm)\n            scaler_disc.step(optimizer_disc)\n            scaler_disc.update()\n            \n            # # Log loss values:\n            running_loss += loss_gen.item() + loss_disc.item()\n            \n            log_steps += 1\n            train_steps += 1\n            if train_steps % args.log_every == 0:\n                # Measure training speed:\n                torch.cuda.synchronize()\n                end_time = time.time()\n                steps_per_sec = log_steps / (end_time - start_time)\n                # Reduce loss history over all processes:\n                avg_loss = torch.tensor(running_loss / log_steps, device=device)\n                dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)\n                avg_loss = avg_loss.item() / dist.get_world_size()\n                logger.info(f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\")\n                # Reset monitoring variables:\n                running_loss = 0\n                log_steps = 0\n                start_time = time.time()\n\n            # Save checkpoint:\n            if train_steps % args.ckpt_every == 0 and train_steps > 0:\n                if rank == 0:\n                    if args.compile:\n                        model_weight = vq_model.module._orig_mod.state_dict()\n                    else:\n                        model_weight = vq_model.module.state_dict()  \n                    checkpoint = {\n                        \"model\": model_weight,\n                        \"optimizer\": optimizer.state_dict(),\n                        \"discriminator\": vq_loss.module.discriminator.state_dict(),\n                        \"optimizer_disc\": optimizer_disc.state_dict(),\n                        \"steps\": train_steps,\n                        \"args\": args\n                    }\n                    if args.ema:\n                        checkpoint[\"ema\"] = ema.state_dict()\n                    if not args.no_local_save:\n                        checkpoint_path = f\"{checkpoint_dir}/{train_steps:07d}.pt\"\n                        torch.save(checkpoint, checkpoint_path)\n                        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n                    \n                    cloud_checkpoint_path = f\"{cloud_checkpoint_dir}/{train_steps:07d}.pt\"\n                    torch.save(checkpoint, cloud_checkpoint_path)\n                    logger.info(f\"Saved checkpoint in cloud to {cloud_checkpoint_path}\")\n                dist.barrier()\n\n    vq_model.eval()  # important! This disables randomized embedding dropout\n    # do any sampling/FID calculation/etc. with ema (or model) in eval mode ...\n\n    logger.info(\"Done!\")\n    dist.destroy_process_group()\n\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--data-face-path\", type=str, default=None, help=\"face datasets to improve vq model\")\n    parser.add_argument(\"--cloud-save-path\", type=str, required=True, help='please specify a cloud disk path, if not, local path')\n    parser.add_argument(\"--no-local-save\", action='store_true', help='no save checkpoints to local path for limited disk volume')\n    parser.add_argument(\"--vq-model\", type=str, choices=list(VQ_models.keys()), default=\"VQ-16\")\n    parser.add_argument(\"--vq-ckpt\", type=str, default=None, help=\"ckpt path for resume training\")\n    parser.add_argument(\"--finetune\", action='store_true', help=\"finetune a pre-trained vq model\")\n    parser.add_argument(\"--ema\", action='store_true', help=\"whether using ema training\")\n    parser.add_argument(\"--codebook-size\", type=int, default=16384, help=\"codebook size for vector quantization\")\n    parser.add_argument(\"--codebook-embed-dim\", type=int, default=8, help=\"codebook dimension for vector quantization\")\n    parser.add_argument(\"--codebook-l2-norm\", action='store_true', default=True, help=\"l2 norm codebook\")\n    parser.add_argument(\"--codebook-weight\", type=float, default=1.0, help=\"codebook loss weight for vector quantization\")\n    parser.add_argument(\"--entropy-loss-ratio\", type=float, default=0.0, help=\"entropy loss ratio in codebook loss\")\n    parser.add_argument(\"--commit-loss-beta\", type=float, default=0.25, help=\"commit loss beta in codebook loss\")\n    parser.add_argument(\"--reconstruction-weight\", type=float, default=1.0, help=\"reconstruction loss weight of image pixel\")\n    parser.add_argument(\"--reconstruction-loss\", type=str, default='l2', help=\"reconstruction loss type of image pixel\")\n    parser.add_argument(\"--perceptual-weight\", type=float, default=1.0, help=\"perceptual loss weight of LPIPS\")\n    parser.add_argument(\"--disc-weight\", type=float, default=0.5, help=\"discriminator loss weight for gan training\")\n    parser.add_argument(\"--disc-start\", type=int, default=20000, help=\"iteration to start discriminator training and loss\")\n    parser.add_argument(\"--disc-type\", type=str, choices=['patchgan', 'stylegan'], default='patchgan', help=\"discriminator type\")\n    parser.add_argument(\"--disc-loss\", type=str, choices=['hinge', 'vanilla', 'non-saturating'], default='hinge', help=\"discriminator loss\")\n    parser.add_argument(\"--gen-loss\", type=str, choices=['hinge', 'non-saturating'], default='hinge', help=\"generator loss for gan training\")\n    parser.add_argument(\"--compile\", action='store_true', default=False)\n    parser.add_argument(\"--dropout-p\", type=float, default=0.0, help=\"dropout_p\")\n    parser.add_argument(\"--results-dir\", type=str, default=\"results_tokenizer_image\")\n    parser.add_argument(\"--dataset\", type=str, default='imagenet')\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n    parser.add_argument(\"--epochs\", type=int, default=40)\n    parser.add_argument(\"--lr\", type=float, default=1e-4)\n    parser.add_argument(\"--weight-decay\", type=float, default=5e-2, help=\"Weight decay to use.\")\n    parser.add_argument(\"--beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--beta2\", type=float, default=0.95, help=\"The beta2 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--max-grad-norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\"--global-batch-size\", type=int, default=128)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=16)\n    parser.add_argument(\"--log-every\", type=int, default=100)\n    parser.add_argument(\"--ckpt-every\", type=int, default=5000)\n    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=1)\n    parser.add_argument(\"--mixed-precision\", type=str, default='bf16', choices=[\"none\", \"fp16\", \"bf16\"]) \n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "models/cobra/backbones/vision/__init__.py", "content": "from .base_vision import ImageTransform, VisionBackbone\nfrom .clip_vit import CLIPViTBackbone\nfrom .dinoclip_vit import DinoCLIPViTBackbone\nfrom .dinosiglip_vit import DinoSigLIPViTBackbone\nfrom .dinov2_vit import DinoV2ViTBackbone\nfrom .in1k_vit import IN1KViTBackbone\nfrom .siglip_vit import SigLIPViTBackbone\n"}
{"type": "source_file", "path": "models/stage1/vq_model.py", "content": "# Reference:\n#   VQGAN: https://github.com/CompVis/taming-transformers\n#   MaskGit: https://github.com/google-research/maskgit\n#   LlamaGen: https://github.com/FoundationVision/LlamaGen\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom dataclasses import dataclass, field\nfrom typing import List\n\n\n@dataclass\nclass VQConfig:\n    n_embed: int = 16384\n    embed_dim: int = 8\n    \n    z_channels: int = 256\n    in_channels: int = 3\n    out_channels: int = 3\n    \n    ch: int = 128,\n    ch_mult: List[int] = field(default_factory=lambda: [1, 1, 2, 2, 4])\n    num_res_blocks: int = 2\n    attn_resolutions: List[int] = field(default_factory=lambda: [16])\n\n\nclass VQModel(nn.Module):\n    def __init__(self, config: VQConfig):\n        super().__init__()\n        self.config = config\n        \n        self.encoder = Encoder(ch_mult=config.ch_mult, z_channels=config.z_channels, num_res_blocks=config.num_res_blocks)\n        self.decoder = Decoder(ch_mult=config.ch_mult, z_channels=config.z_channels, num_res_blocks=config.num_res_blocks)\n\n        self.quantize = VectorQuantizer(config.n_embed, config.embed_dim, 0.25, 0.0, True, False)\n        self.quant_conv = nn.Conv2d(config.z_channels, config.embed_dim, 1)\n        self.post_quant_conv = nn.Conv2d(config.embed_dim, config.z_channels, 1)\n        \n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b, shape=None, channel_first=True):\n        quant_b = self.quantize.get_codebook_entry(code_b, shape, channel_first)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input):\n        quant, diff, _ = self.encode(input)\n        dec = self.decode(quant)\n        return dec, diff\n    \n    def from_pretrained(self, path):\n        sd = torch.load(path, map_location=\"cpu\")\n        self.load_state_dict(sd['model'], strict=True)\n        print(f\"Restored from {path}\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels=3, ch=128, ch_mult=(1,1,2,2,4), num_res_blocks=2, \n                 norm_type='group', dropout=0.0, resamp_with_conv=True, z_channels=256):\n        super().__init__()\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n\n        # downsampling\n        in_ch_mult = (1,) + tuple(ch_mult)\n        self.conv_blocks = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            conv_block = nn.Module()\n            # res & attn\n            res_block = nn.ModuleList()\n            attn_block = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for _ in range(self.num_res_blocks):\n                res_block.append(ResnetBlock(block_in, block_out, dropout=dropout, norm_type=norm_type))\n                block_in = block_out\n                if i_level == self.num_resolutions - 1:\n                    attn_block.append(AttnBlock(block_in, norm_type))\n            conv_block.res = res_block\n            conv_block.attn = attn_block\n            # downsample\n            if i_level != self.num_resolutions-1:\n                conv_block.downsample = Downsample(block_in, resamp_with_conv)\n            self.conv_blocks.append(conv_block)\n\n        # middle\n        self.mid = nn.ModuleList()\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n        self.mid.append(AttnBlock(block_in, norm_type=norm_type))\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n\n        # end\n        self.norm_out = Normalize(block_in, norm_type)\n        self.conv_out = nn.Conv2d(block_in, z_channels, kernel_size=3, stride=1, padding=1)\n\n\n    def forward(self, x):\n        h = self.conv_in(x)\n        # downsampling\n        for i_level, block in enumerate(self.conv_blocks):\n            for i_block in range(self.num_res_blocks):\n                h = block.res[i_block](h)\n                if len(block.attn) > 0:\n                    h = block.attn[i_block](h)\n            if i_level != self.num_resolutions - 1:\n                h = block.downsample(h)\n        \n        # middle\n        for mid_block in self.mid:\n            h = mid_block(h)\n        \n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass Decoder(nn.Module):\n    def __init__(self, z_channels=256, ch=128, ch_mult=(1,1,2,2,4), num_res_blocks=2, norm_type=\"group\",\n                 dropout=0.0, resamp_with_conv=True, out_channels=3):\n        super().__init__()\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        # z to block_in\n        self.conv_in = nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n\n       # middle\n        self.mid = nn.ModuleList()\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n        self.mid.append(AttnBlock(block_in, norm_type=norm_type))\n        self.mid.append(ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type))\n\n        # upsampling\n        self.conv_blocks = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            conv_block = nn.Module()\n            # res & attn\n            res_block = nn.ModuleList()\n            attn_block = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for _ in range(self.num_res_blocks + 1):\n                res_block.append(ResnetBlock(block_in, block_out, dropout=dropout, norm_type=norm_type))\n                block_in = block_out\n                if i_level == self.num_resolutions - 1:\n                    attn_block.append(AttnBlock(block_in, norm_type))\n            conv_block.res = res_block\n            conv_block.attn = attn_block\n            # downsample\n            if i_level != 0:\n                conv_block.upsample = Upsample(block_in, resamp_with_conv)\n            self.conv_blocks.append(conv_block)\n\n        # end\n        self.norm_out = Normalize(block_in, norm_type)\n        self.conv_out = nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n\n    @property\n    def last_layer(self):\n        return self.conv_out.weight\n    \n    def forward(self, z):\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        for mid_block in self.mid:\n            h = mid_block(h)\n        \n        # upsampling\n        for i_level, block in enumerate(self.conv_blocks):\n            for i_block in range(self.num_res_blocks + 1):\n                h = block.res[i_block](h)\n                if len(block.attn) > 0:\n                    h = block.attn[i_block](h)\n            if i_level != self.num_resolutions - 1:\n                h = block.upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass VectorQuantizer(nn.Module):\n    def __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage):\n        super().__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n        self.entropy_loss_ratio = entropy_loss_ratio\n        self.l2_norm = l2_norm\n        self.show_usage = show_usage\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n        if self.l2_norm:\n            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=-1)\n        if self.show_usage:\n            self.register_buffer(\"codebook_used\", nn.Parameter(torch.zeros(65536)))\n\n    \n    def forward(self, z):\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = torch.einsum('b c h w -> b h w c', z).contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        if self.l2_norm:\n            z = F.normalize(z, p=2, dim=-1)\n            z_flattened = F.normalize(z_flattened, p=2, dim=-1)\n            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n        else:\n            embedding = self.embedding.weight\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(embedding**2, dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, torch.einsum('n d -> d n', embedding))\n\n        min_encoding_indices = torch.argmin(d, dim=1)\n        z_q = embedding[min_encoding_indices].view(z.shape)\n        perplexity = None\n        min_encodings = None\n        vq_loss = None\n        commit_loss = None\n        entropy_loss = None\n        codebook_usage = 0\n\n        if self.show_usage and self.training:\n            cur_len = min_encoding_indices.shape[0]\n            self.codebook_used[:-cur_len] = self.codebook_used[cur_len:].clone()\n            self.codebook_used[-cur_len:] = min_encoding_indices\n            codebook_usage = len(torch.unique(self.codebook_used)) / self.n_e\n\n        # compute loss for embedding\n        if self.training:\n            vq_loss = torch.mean((z_q - z.detach()) ** 2) \n            commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2) \n            entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        z_q = torch.einsum('b h w c -> b c h w', z_q)\n\n        return z_q, (vq_loss, commit_loss, entropy_loss, codebook_usage), (perplexity, min_encodings, min_encoding_indices)\n\n    def get_codebook_entry(self, indices, shape=None, channel_first=True):\n        # shape = (batch, channel, height, width) if channel_first else (batch, height, width, channel)\n        if self.l2_norm:\n            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)\n        else:\n            embedding = self.embedding.weight\n        z_q = embedding[indices]  # (b*h*w, c)\n\n        if shape is not None:\n            if channel_first:\n                z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[1])\n                # reshape back to match original input shape\n                z_q = z_q.permute(0, 3, 1, 2).contiguous()\n            else:\n                z_q = z_q.view(shape)\n        return z_q\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None, conv_shortcut=False, dropout=0.0, norm_type='group'):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels, norm_type)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.norm2 = Normalize(out_channels, norm_type)\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            else:\n                self.nin_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n        return x+h\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels, norm_type='group'):\n        super().__init__()\n        self.norm = Normalize(in_channels, norm_type)\n        self.q = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.k = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.v = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.proj_out = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w)\n        q = q.permute(0,2,1)   # b,hw,c\n        k = k.reshape(b,c,h*w) # b,c,hw\n        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = F.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, norm_type='group'):\n    assert norm_type in ['group', 'batch']\n    if norm_type == 'group':\n        return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n    elif norm_type == 'batch':\n        return nn.SyncBatchNorm(in_channels)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = F.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = F.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\ndef compute_entropy_loss(affinity, loss_type=\"softmax\", temperature=0.01):\n    flat_affinity = affinity.reshape(-1, affinity.shape[-1])\n    flat_affinity /= temperature\n    probs = F.softmax(flat_affinity, dim=-1)\n    log_probs = F.log_softmax(flat_affinity + 1e-5, dim=-1)\n    if loss_type == \"softmax\":\n        target_probs = probs\n    else:\n        raise ValueError(\"Entropy loss {} not supported\".format(loss_type))\n    avg_probs = torch.mean(target_probs, dim=0)\n    avg_entropy = - torch.sum(avg_probs * torch.log(avg_probs + 1e-5))\n    sample_entropy = - torch.mean(torch.sum(target_probs * log_probs, dim=-1))\n    loss = sample_entropy - avg_entropy\n    return loss\n\n\ndef VQ_f16(**kwargs):\n    return VQModel(VQConfig())\n\n\nVQ_models = {'VQ-f16': VQ_f16}"}
{"type": "source_file", "path": "llamagen_tokenizer/vae/sd_vae_demo.py", "content": "import argparse\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom diffusers.models import AutoencoderKL\n\n\ndef main(args):\n    # Setup PyTorch:\n    torch.manual_seed(args.seed)\n    torch.set_grad_enabled(False)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # create and load model\n    vae = AutoencoderKL.from_pretrained(f\"stabilityai/{args.vae}\").to(device)\n\n    # load image\n    img_path = args.image_path\n    out_path = args.image_path.replace('.jpg', '_vae.jpg').replace('.jpeg', '_vae.jpeg').replace('.png', '_vae.png')\n    input_size = args.image_size\n    img = Image.open(img_path).convert(\"RGB\")\n\n    # preprocess\n    size_org = img.size\n    img = img.resize((input_size, input_size))\n    img = np.array(img) / 255.\n    x = 2.0 * img - 1.0 # x value is between [-1, 1]\n    x = torch.tensor(x)\n    x = x.unsqueeze(dim=0)\n    x = torch.einsum('nhwc->nchw', x)\n    x_input = x.float().to(\"cuda\")\n\n    # inference\n    with torch.no_grad():\n        # Map input images to latent space + normalize latents:\n        latent = vae.encode(x_input).latent_dist.sample().mul_(0.18215)\n        # reconstruct:\n        output = vae.decode(latent / 0.18215).sample # output value is between [-1, 1]\n\n    # postprocess\n    output = F.interpolate(output, size=[size_org[1], size_org[0]], mode='bilinear').permute(0, 2, 3, 1)[0]\n    sample = torch.clamp(127.5 * output + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8).numpy()\n\n    # save        \n    Image.fromarray(sample).save(out_path)\n    print(\"Reconstructed image is saved to {}\".format(out_path))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--image-path\", type=str, default=\"assets/example.jpg\")\n    parser.add_argument(\"--vae\", type=str, choices=[\"sdxl-vae\", \"sd-vae-ft-mse\"], default=\"sd-vae-ft-mse\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512, 1024], default=512)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/vq_model_hf.py", "content": "from huggingface_hub import PyTorchModelHubMixin\n\nfrom tokenizer.tokenizer_image.vq_model import ModelArgs, VQModel\n\nclass VQModelHF(VQModel, PyTorchModelHubMixin, repo_url=\"https://github.com/FoundationVision/LlamaGen\", license=\"mit\", tags=[\"llamagen\", \"text-to-image\"]):\n    pass\n\n#################################################################################\n#                              VQ Model Configs                                 #\n#################################################################################\ndef VQ_8(**kwargs):\n    return VQModelHF(ModelArgs(encoder_ch_mult=[1, 2, 2, 4], decoder_ch_mult=[1, 2, 2, 4], **kwargs))\n\ndef VQ_16(**kwargs):\n    return VQModelHF(ModelArgs(encoder_ch_mult=[1, 1, 2, 2, 4], decoder_ch_mult=[1, 1, 2, 2, 4], **kwargs))\n\nVQ_models_HF = {'VQ-16': VQ_16, 'VQ-8': VQ_8}\n"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/discriminator_stylegan.py", "content": "# Modified from:\n#   stylegan2-pytorch: https://github.com/lucidrains/stylegan2-pytorch/blob/master/stylegan2_pytorch/stylegan2_pytorch.py\n#   stylegan2-pytorch: https://github.com/rosinality/stylegan2-pytorch/blob/master/model.py\n#   maskgit: https://github.com/google-research/maskgit/blob/main/maskgit/nets/discriminator.py\nimport math\nimport torch\nimport torch.nn as nn\ntry:\n    from kornia.filters import filter2d\nexcept:\n    pass\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, channel_multiplier=1, image_size=256):\n        super().__init__()\n        channels = {\n            4: 512,\n            8: 512,\n            16: 512,\n            32: 512,\n            64: 256 * channel_multiplier,\n            128: 128 * channel_multiplier,\n            256: 64 * channel_multiplier,\n            512: 32 * channel_multiplier,\n            1024: 16 * channel_multiplier,\n        }\n        \n        log_size = int(math.log(image_size, 2))\n        in_channel = channels[image_size]\n\n        blocks = [nn.Conv2d(input_nc, in_channel, 3, padding=1), leaky_relu()]\n        for i in range(log_size, 2, -1):\n            out_channel = channels[2 ** (i - 1)]\n            blocks.append(DiscriminatorBlock(in_channel, out_channel))\n            in_channel = out_channel\n        self.blocks = nn.ModuleList(blocks)\n\n        self.final_conv = nn.Sequential(\n            nn.Conv2d(in_channel, channels[4], 3, padding=1),\n            leaky_relu(),\n        )\n        self.final_linear = nn.Sequential(\n            nn.Linear(channels[4] * 4 * 4, channels[4]),\n            leaky_relu(),\n            nn.Linear(channels[4], 1)\n        )\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        x = self.final_conv(x)\n        x = x.view(x.shape[0], -1)\n        x = self.final_linear(x)\n        return x\n\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, input_channels, filters, downsample=True):\n        super().__init__()\n        self.conv_res = nn.Conv2d(input_channels, filters, 1, stride = (2 if downsample else 1))\n\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, filters, 3, padding=1),\n            leaky_relu(),\n            nn.Conv2d(filters, filters, 3, padding=1),\n            leaky_relu()\n        )\n\n        self.downsample = nn.Sequential(\n            Blur(),\n            nn.Conv2d(filters, filters, 3, padding = 1, stride = 2)\n        ) if downsample else None\n\n    def forward(self, x):\n        res = self.conv_res(x)\n        x = self.net(x)\n        if exists(self.downsample):\n            x = self.downsample(x)\n        x = (x + res) * (1 / math.sqrt(2))\n        return x\n\n\n\nclass Blur(nn.Module):\n    def __init__(self):\n        super().__init__()\n        f = torch.Tensor([1, 2, 1])\n        self.register_buffer('f', f)\n    \n    def forward(self, x):\n        f = self.f\n        f = f[None, None, :] * f [None, :, None]\n        return filter2d(x, f, normalized=True)\n\n\ndef leaky_relu(p=0.2):\n    return nn.LeakyReLU(p, inplace=True)\n\n\ndef exists(val):\n    return val is not None\n"}
{"type": "source_file", "path": "models/cobra/backbones/llm/prompting/__init__.py", "content": "from .base_prompter import PromptBuilder, PurePromptBuilder\nfrom .zephyr_prompter import ZephyrChatPromptBuilder\nfrom .mamba_prompter import MambaPromptBuilder\n"}
{"type": "source_file", "path": "models/cobra/materialize.py", "content": "\"\"\"\nmaterialize.py\n\nFactory class for initializing Vision Backbones, LLM Backbones, and VLMs from a set registry; provides and exports\nindividual functions for clear control flow.\n\"\"\"\nfrom typing import Optional, Tuple\n\nfrom transformers import PreTrainedTokenizerBase\nimport torch\nfrom models.cobra.backbones.vision import (\n    CLIPViTBackbone,\n    DinoCLIPViTBackbone,\n    DinoSigLIPViTBackbone,\n    DinoV2ViTBackbone,\n    ImageTransform,\n    IN1KViTBackbone,\n    SigLIPViTBackbone,\n    VisionBackbone,\n)\n\n# === Registries =>> Maps ID --> {cls(), kwargs} :: Different Registries for Vision Backbones, LLM Backbones, VLMs ===\n# fmt: off\n\n# === Vision Backbone Registry ===\nVISION_BACKBONES = {\n    # === 224px Backbones ===\n    \"clip-vit-l\": {\"cls\": CLIPViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n    \"siglip-vit-so400m\": {\"cls\": SigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n    \"dinov2-vit-l\": {\"cls\": DinoV2ViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n    \"in1k-vit-l\": {\"cls\": IN1KViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n\n    # === Assorted CLIP Backbones ===\n    \"clip-vit-b\": {\"cls\": CLIPViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n    \"clip-vit-l-336px\": {\"cls\": CLIPViTBackbone, \"kwargs\": {\"default_image_size\": 336}},\n\n    # === Assorted SigLIP Backbones ===\n    \"siglip-vit-b16-224px\": {\"cls\": SigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 224}},\n    \"siglip-vit-b16-256px\": {\"cls\": SigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 256}},\n    \"siglip-vit-b16-384px\": {\"cls\": SigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 384}},\n    \"siglip-vit-so400m-384px\": {\"cls\": SigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 384}},\n\n    # === Fused Backbones ===\n    \"dinoclip-vit-l-336px\": {\"cls\": DinoCLIPViTBackbone, \"kwargs\": {\"default_image_size\": 336}},\n    \"dinosiglip-vit-so-384px\": {\"cls\": DinoSigLIPViTBackbone, \"kwargs\": {\"default_image_size\": 384}},\n}\n\n\n# fmt: on\n\n\ndef get_vision_backbone_and_transform(\n    vision_backbone_id: str, image_resize_strategy: str = \"resize-naive\"\n) -> Tuple[VisionBackbone, ImageTransform]:\n    \"\"\"Instantiate a Vision Backbone, returning both the nn.Module wrapper class and default Image Transform.\"\"\"\n    if vision_backbone_id in VISION_BACKBONES:\n        vision_cfg = VISION_BACKBONES[vision_backbone_id]\n        vision_backbone: VisionBackbone = vision_cfg[\"cls\"](\n            vision_backbone_id, image_resize_strategy, **vision_cfg[\"kwargs\"]\n        )\n        image_transform = vision_backbone.get_image_transform()\n        return vision_backbone, image_transform\n\n    else:\n        raise ValueError(f\"Vision Backbone `{vision_backbone_id}` is not supported!\")\n\n\n\n"}
{"type": "source_file", "path": "models/cobra/backbones/llm/prompting/mamba_prompter.py", "content": "from typing import Optional\n\nfrom models.cobra.backbones.llm.prompting.base_prompter import PromptBuilder\n\nclass MambaPromptBuilder(PromptBuilder):\n    def __init__(self, model_family: str, system_prompt: Optional[str] = None) -> None:\n        super().__init__(model_family, system_prompt)\n\n        self.bos, self.eos = \"\", \"<|endoftext|>\"\n\n        # Get role-specific \"wrap\" functions\n        self.wrap_human = lambda msg: f\"In: {msg}\\nOut: \"\n        self.wrap_gpt = lambda msg: f\"{msg if msg != '' else ' '}{self.eos}\"\n\n        # === `self.prompt` gets built up over multiple turns ===\n        self.prompt, self.turn_count = \"\", 0\n\n    def add_turn(self, role: str, message: str) -> str:\n        assert (role == \"human\") if (self.turn_count % 2 == 0) else (role == \"gpt\")\n        message = message.replace(\"<image>\", \"\").strip()\n\n        if (self.turn_count % 2) == 0:\n            human_message = self.wrap_human(message)\n            if self.turn_count != 0:\n                human_message = \"\\n\" + human_message\n            wrapped_message = human_message\n        else:\n            gpt_message = self.wrap_gpt(message)\n            wrapped_message = gpt_message\n\n        # Update Prompt\n        self.prompt += wrapped_message\n\n        # Bump Turn Counter\n        self.turn_count += 1\n\n        # Return \"wrapped_message\" (effective string added to context)\n        return wrapped_message\n\n    def get_potential_prompt(self, message: str) -> None:\n        # Assumes that it's always the user's (human's) turn!\n        prompt_copy = str(self.prompt)\n\n        human_message = self.wrap_human(message)\n        prompt_copy += human_message\n\n        return prompt_copy.removeprefix(self.bos)\n\n    def get_prompt(self) -> str:\n        # Remove prefix <bos> (if exists) because it gets auto-inserted by tokenizer!\n        return self.prompt.removeprefix(self.bos)"}
{"type": "source_file", "path": "models/cobra/backbones/vision/dinoclip_vit.py", "content": "\"\"\"\ndinoclip_vit.py\n\nVision backbone that returns concatenated features from both DINOv2 and CLIP.\n\"\"\"\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Callable, Dict, Tuple\n\nimport timm\nimport torch\nfrom PIL import Image\nfrom timm.models.vision_transformer import Block, VisionTransformer\nfrom torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy, transformer_auto_wrap_policy\nfrom torchvision.transforms import Compose, Resize\n\nfrom models.cobra.backbones.vision.base_vision import ImageTransform, LetterboxPad, VisionBackbone, unpack_tuple\n\n# Registry =>> Supported DinoCLIP Pairs (as TIMM identifiers)\nDINOCLIP_VISION_BACKBONES = {\n    \"dinoclip-vit-l-336px\": {\n        \"dino\": \"vit_large_patch14_reg4_dinov2.lvd142m\",\n        \"clip\": \"vit_large_patch14_clip_336.openai\",\n    },\n}\n\n\n@dataclass\nclass DinoCLIPImageTransform:\n    dino_image_transform: ImageTransform\n    clip_image_transform: ImageTransform\n    is_cobra: bool = True\n\n    def __call__(self, img: Image, **kwargs: str) -> Dict[str, torch.Tensor]:\n        return {\"dino\": self.dino_image_transform(img, **kwargs), \"clip\": self.clip_image_transform(img, **kwargs)}\n\n\nclass DinoCLIPViTBackbone(VisionBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(vision_backbone_id, image_resize_strategy, default_image_size=default_image_size)\n        self.dino_timm_path_or_url = DINOCLIP_VISION_BACKBONES[vision_backbone_id][\"dino\"]\n        self.clip_timm_path_or_url = DINOCLIP_VISION_BACKBONES[vision_backbone_id][\"clip\"]\n\n        # Initialize both Featurizers (ViTs) by downloading from HF / TIMM Hub if necessary\n        self.dino_featurizer: VisionTransformer = timm.create_model(\n            self.dino_timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size\n        )\n        self.dino_featurizer.eval()\n\n        self.clip_featurizer: VisionTransformer = timm.create_model(\n            self.clip_timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size\n        )\n        self.clip_featurizer.eval()\n\n        # Monkey-Patch the `forward()` function of the featurizers to ensure FSDP-compatibility\n        #   => Note: By default set `get_intermediate_layers` to return the *SECOND-TO-LAST* layer patches!\n        #   => TODO (siddk) Remove after resolution of https://github.com/pytorch/pytorch/issues/109385\n        self.dino_featurizer.forward = unpack_tuple(\n            partial(self.dino_featurizer.get_intermediate_layers, n={len(self.dino_featurizer.blocks) - 2})\n        )\n        self.clip_featurizer.forward = unpack_tuple(\n            partial(self.clip_featurizer.get_intermediate_layers, n={len(self.clip_featurizer.blocks) - 2})\n        )\n\n        # Get Configs for _both_ Featurizers =>> Note :: Override default image size for larger resolution models\n        self.dino_data_cfg = timm.data.resolve_model_data_config(self.dino_featurizer)\n        self.dino_data_cfg[\"input_size\"] = (3, self.default_image_size, self.default_image_size)\n\n        self.clip_data_cfg = timm.data.resolve_model_data_config(self.clip_featurizer)\n        self.clip_data_cfg[\"input_size\"] = (3, self.default_image_size, self.default_image_size)\n\n        # Initialize *both* Transforms\n        default_dino_transform = timm.data.create_transform(**self.dino_data_cfg, is_training=False)\n        default_clip_transform = timm.data.create_transform(**self.clip_data_cfg, is_training=False)\n        if self.image_resize_strategy == \"resize-naive\":\n            assert isinstance(default_dino_transform, Compose), \"Unexpected `default_dino_image_transform`!\"\n            assert isinstance(default_clip_transform, Compose), \"Unexpected `default_clip_image_transform`!\"\n            assert isinstance(dino_resize_transform := default_dino_transform.transforms[0], Resize)\n            assert isinstance(clip_resize_transform := default_clip_transform.transforms[0], Resize)\n\n            target_size = (self.default_image_size, self.default_image_size)\n            dino_transform = Compose(\n                [\n                    Resize(target_size, interpolation=dino_resize_transform.interpolation),\n                    *default_dino_transform.transforms[1:],\n                ]\n            )\n            clip_transform = Compose(\n                [\n                    Resize(target_size, interpolation=clip_resize_transform.interpolation),\n                    *default_clip_transform.transforms[1:],\n                ]\n            )\n\n            self.image_transform = DinoCLIPImageTransform(dino_transform, clip_transform)\n\n        elif self.image_resize_strategy == \"resize-crop\":\n            self.image_transform = DinoCLIPImageTransform(default_dino_transform, default_clip_transform)\n\n        elif self.image_resize_strategy == \"letterbox\":\n            assert isinstance(default_dino_transform, Compose), \"Unexpected `default_dino_transform`!\"\n            assert isinstance(default_clip_transform, Compose), \"Unexpected `default_clip_transform`!\"\n            assert \"mean\" in self.dino_data_cfg and \"mean\" in self.clip_data_cfg, \"DinoCLIP `data_cfg` missing `mean`!\"\n\n            # Compute Padding Fill Value(s) (rescaled normalization mean if applicable)\n            dino_fill = tuple([int(x * 255) for x in self.dino_data_cfg[\"mean\"]])\n            clip_fill = tuple([int(x * 255) for x in self.clip_data_cfg[\"mean\"]])\n\n            # Build New Transform\n            self.image_transform = DinoCLIPImageTransform(\n                Compose([LetterboxPad(dino_fill), *default_dino_transform.transforms]),\n                Compose([LetterboxPad(clip_fill), *default_clip_transform.transforms]),\n            )\n\n        else:\n            raise ValueError(f\"Image Resize Strategy `{self.image_resize_strategy}` is not supported!\")\n\n    def get_fsdp_wrapping_policy(self) -> Callable:\n        \"\"\"Return a simple FSDP policy that wraps each ViT block and then both of the _entire_ featurizers.\"\"\"\n        vit_wrap_policy = partial(_module_wrap_policy, module_classes={VisionTransformer})\n        transformer_block_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n        return partial(_or_policy, policies=[vit_wrap_policy, transformer_block_policy])\n\n    def forward(self, pixel_values: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"Runs the transformed image/pixel tensors through each vision backbone, returning concatenated patches.\"\"\"\n        dino_patches = self.dino_featurizer(pixel_values[\"dino\"])\n        clip_patches = self.clip_featurizer(pixel_values[\"clip\"])\n\n        return torch.cat([dino_patches, clip_patches], dim=2)\n\n    @property\n    def default_image_resolution(self) -> Tuple[int, int, int]:\n        return self.dino_data_cfg[\"input_size\"]\n\n    @property\n    def embed_dim(self) -> int:\n        return self.dino_featurizer.embed_dim + self.clip_featurizer.embed_dim\n\n    @property\n    def num_patches(self) -> int:\n        assert self.dino_featurizer.patch_embed.num_patches == self.clip_featurizer.patch_embed.num_patches\n        return self.dino_featurizer.patch_embed.num_patches\n\n    @property\n    def half_precision_dtype(self) -> torch.dtype:\n        return torch.bfloat16\n"}
{"type": "source_file", "path": "models/cobra/nn_utils.py", "content": "\"\"\"\nnn_utils.py\n\nUtility functions and PyTorch submodule definitions.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport math\n\n\n# === Definitions for Various Projection Modules, with Signature :: [..., in_dim] --> [..., out_dim] ===\nclass LinearProjector(nn.Module):\n    def __init__(self, vision_dim: int, llm_dim: int) -> None:\n        super().__init__()\n        self.projector = nn.Linear(vision_dim, llm_dim, bias=True)\n\n    def forward(self, img_patches: torch.Tensor) -> torch.Tensor:\n        return self.projector(img_patches)\n\n\nclass MLPProjector(nn.Module):\n    def __init__(self, vision_dim: int, llm_dim: int, mlp_type: str = \"gelu-mlp\") -> None:\n        super().__init__()\n        if mlp_type == \"gelu-mlp\":\n            self.projector = nn.Sequential(\n                nn.Linear(vision_dim, llm_dim, bias=True),\n                nn.GELU(),\n                nn.Linear(llm_dim, llm_dim, bias=True),\n            )\n        else:\n            raise ValueError(f\"Projector with `{mlp_type = }` is not supported!\")\n\n    def forward(self, img_patches: torch.Tensor) -> torch.Tensor:\n        return self.projector(img_patches)\n\n\nclass FusedMLPProjector(nn.Module):\n    def __init__(self, fused_vision_dim: int, llm_dim: int, mlp_type: str = \"fused-gelu-mlp\") -> None:\n        super().__init__()\n        self.initial_projection_dim = fused_vision_dim * 4\n        if mlp_type == \"fused-gelu-mlp\":\n            self.projector = nn.Sequential(\n                nn.Linear(fused_vision_dim, self.initial_projection_dim, bias=True),\n                nn.GELU(),\n                nn.Linear(self.initial_projection_dim, llm_dim, bias=True),\n                nn.GELU(),\n                nn.Linear(llm_dim, llm_dim, bias=True),\n            )\n        else:\n            raise ValueError(f\"Fused Projector with `{mlp_type = }` is not supported!\")\n\n    def forward(self, fused_img_patches: torch.Tensor) -> torch.Tensor:\n        return self.projector(fused_img_patches)\n\n\n# LDPv2 Projector: https://github.com/Meituan-AutoML/MobileVLM/blob/main/mobilevlm/model/vision_projector.py\nclass TokenDownLayer(nn.Module):\n    def __init__(self, shape) -> None:\n        super().__init__()\n        self.dwn = nn.Sequential(\n            nn.AdaptiveAvgPool2d(shape)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, num_tokens, c = x.shape\n        h = int(math.sqrt(num_tokens))\n        assert h * h == num_tokens\n        x = x.permute(0, 2, 1).reshape(b, -1, h, h)\n        x = self.dwn(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass PosInjectLayer(nn.Module):\n    # https://github.com/Meituan-AutoML/Twins/blob/main/gvt.py\n    def __init__(self, in_dim: int, out_dim: int, stride: int = 1) -> None:\n        super().__init__()\n        self.peg = nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, 3, stride, 1, bias=True, groups=out_dim)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, num_tokens, c = x.shape\n        h = int(math.sqrt(num_tokens))\n        assert h * h == num_tokens\n        cnn_feat = x.transpose(1, 2).view(b, c, h, h)\n        x = self.peg(cnn_feat) + cnn_feat\n        x = x.flatten(2).transpose(1, 2)\n        return x\n       \n\nclass LDPProjector(nn.Module):\n    def __init__(self, vision_dim: int, llm_dim: int, mlp_type: str = \"ldpnet\") -> None:\n        super().__init__()\n        if mlp_type == \"ldpnet\":\n            self.projector = nn.Sequential(\n                nn.Linear(vision_dim, llm_dim, bias=True),\n                nn.GELU(),\n                nn.Linear(llm_dim, llm_dim, bias=True),\n                TokenDownLayer((14, 14)),\n                PosInjectLayer(llm_dim, llm_dim, stride=1),\n            )\n        else:\n            raise ValueError(f\"Projector with `{mlp_type = }` is not supported!\")\n\n    def forward(self, img_patches: torch.Tensor) -> torch.Tensor:\n        return self.projector(img_patches)\n    \n        \nclass FusedLDPProjector(nn.Module):\n    def __init__(self, fused_vision_dim: int, llm_dim: int, mlp_type: str = \"fused-ldpnet\") -> None:\n        super().__init__()\n        self.initial_projection_dim = fused_vision_dim * 4\n        if mlp_type == \"fused-ldpnet\":\n            self.projector = nn.Sequential(\n                nn.Linear(fused_vision_dim, self.initial_projection_dim, bias=True), \n                nn.GELU(), \n                nn.Linear(self.initial_projection_dim, llm_dim, bias=True),\n                TokenDownLayer((14, 14)),\n                PosInjectLayer(llm_dim, llm_dim, stride=1),\n            )\n            \n        else:\n            raise ValueError(f\"Fused Projector with `{mlp_type = }` is not supported!\")\n        \n    def forward(self, img_patches: torch.Tensor) -> torch.Tensor:\n        return self.projector(img_patches)\n    "}
{"type": "source_file", "path": "models/cobra/data_utils.py", "content": "\"\"\"\ndata_utils.py\n\nGeneral utilities and classes for facilitating data loading and collation.\n\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Dict, Sequence, Tuple\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\n# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)\nIGNORE_INDEX = -100\n\n\n@dataclass\nclass PaddedCollatorForLanguageModeling:\n    model_max_length: int\n    pad_token_id: int\n    default_image_resolution: Tuple[int, int, int]\n    padding_side: str = \"right\"\n    pixel_values_dtype: torch.dtype = torch.float32\n\n    def __post_init__(self) -> None:\n        self.dummy_pixel_values = torch.zeros(self.default_image_resolution, dtype=self.pixel_values_dtype)\n\n    def __call__(self, instances: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        pixel_values = [instance[\"pixel_values\"] for instance in instances]\n\n        # For now, we only support Tokenizers with `padding_side = \"right\"` during Training (but plan to extend!)\n        #   => Handle padding via RNN Utils => `pad_sequence`\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n\n        # Truncate (if necessary)\n        input_ids, labels = input_ids[:, : self.model_max_length], labels[:, : self.model_max_length]\n\n        # Get `attention_mask` by checking for `pad_token_id`\n        attention_mask = input_ids.ne(self.pad_token_id)\n\n        # === Handle \"unimodal\" (language-only) vs. \"multimodal\" ===\n\n        # Some examples are \"language-only\" --> build a Tensor of `multimodal_indices` that we can slice into easily\n        multimodal_indices = torch.tensor(\n            [idx for idx in range(len(pixel_values)) if pixel_values[idx] is not None], dtype=torch.long\n        )\n\n        # Stack all `pixel_values` --> depending on type (torch.Tensor, or Dict[str, torch.Tensor]) & presence of None\n        if len(multimodal_indices) == 0:\n            pixel_values = torch.stack([self.dummy_pixel_values for _ in range(len(input_ids))])\n        elif isinstance(pv_example := pixel_values[multimodal_indices[0]], torch.Tensor):\n            pixel_values = torch.stack(\n                [\n                    pixel_values[idx] if idx in multimodal_indices else self.dummy_pixel_values\n                    for idx in range(len(input_ids))\n                ]\n            )\n        elif isinstance(pv_example, dict):\n            pixel_values = {\n                k: torch.stack(\n                    [\n                        pixel_values[idx][k] if idx in multimodal_indices else self.dummy_pixel_values\n                        for idx in range(len(input_ids))\n                    ]\n                )\n                for k in pv_example\n            }\n        else:\n            raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n        return dict(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            multimodal_indices=multimodal_indices,\n        )\n"}
{"type": "source_file", "path": "models/cobra/backbones/vision/base_vision.py", "content": "\"\"\"\nbase_vision.py\n\nAbstract class definition of a Vision Backbone (Visual Featurizer), with full annotations of class methods, utility\nfunctions, and initialization logic.\n\nWe also define the generic TimmViTBackbone class here, providing a default interface for loading any TIMM Vision\nTransformer model for feature extraction.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Callable, Dict, Optional, Protocol, Tuple, Union\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TVF\nfrom PIL.Image import Image\nfrom timm.models.vision_transformer import Block, VisionTransformer\nfrom torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy, transformer_auto_wrap_policy\nfrom torchvision.transforms import Compose, Resize\n\n\n# === Utility Functions for Monkey-Patching ===\ndef unpack_tuple(fn: Callable[[Any], Tuple[Any]]) -> Callable[[Any], Any]:\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        result = fn(*args, **kwargs)\n        return result[0] if isinstance(result, tuple) else result\n\n    return wrapper\n\n\n# === Interface for an Image Transform ===\nclass ImageTransform(Protocol):\n    def __call__(self, img: Image, **kwargs: str) -> Union[torch.Tensor, Dict[str, torch.Tensor]]: ...\n\n\n# === Custom Torchvision Image Transforms ===\n@dataclass\nclass LetterboxPad:\n    padding_fill_value: Tuple[int, int, int]\n\n    def __call__(self, image: Image) -> Image:\n        \"\"\"Given a PIL.Image, pad to square by adding a symmetric border around the height/width.\"\"\"\n        (w, h), max_wh = image.size, max(image.size)\n        horizontal_pad, vertical_pad = int((max_wh - w) / 2), int((max_wh - h) / 2)\n        padding = (horizontal_pad, vertical_pad, horizontal_pad, vertical_pad)\n        return TVF.pad(image, padding, fill=self.padding_fill_value, padding_mode=\"constant\")\n\n\n# === Abstract Base Class for arbitrary Vision Backbones ===\nclass VisionBackbone(nn.Module, ABC):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__()\n        self.identifier: str = vision_backbone_id\n        self.image_resize_strategy: str = image_resize_strategy\n        self.default_image_size: int = default_image_size\n\n        # Instance attributes for a Vision Backbone\n        self.featurizer: nn.Module = None\n        self.image_transform: ImageTransform = None\n\n    def get_image_transform(self) -> ImageTransform:\n        return self.image_transform\n\n    @abstractmethod\n    def get_fsdp_wrapping_policy(self) -> Callable: ...\n\n    @abstractmethod\n    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n        \"\"\"Run a forward pass through the featurizer given a set of processed images, returning patch/grid features.\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def default_image_resolution(self) -> Tuple[int, int, int]: ...\n\n    @property\n    @abstractmethod\n    def embed_dim(self) -> int: ...\n\n    @property\n    @abstractmethod\n    def num_patches(self) -> int: ...\n\n    @property\n    @abstractmethod\n    def half_precision_dtype(self) -> torch.dtype: ...\n\n\n# === Abstract Base Class for Arbitrary TIMM Vision Transformer Backbones ===\nclass TimmViTBackbone(VisionBackbone, ABC):\n    def __init__(\n        self,\n        vision_backbone_id: str,\n        timm_path_or_url: str,\n        image_resize_strategy: str,\n        default_image_size: int = 224,\n        pretrained_cfg_overlay: Optional[str] = None,\n        override_act_layer: Optional[str] = None,\n    ) -> None:\n        super().__init__(vision_backbone_id, image_resize_strategy, default_image_size=default_image_size)\n        self.timm_path_or_url = timm_path_or_url\n        self.override_act_layer = override_act_layer\n        self.dtype = torch.bfloat16\n        if pretrained_cfg_overlay is not None:\n            # Initialize Featurizer (ViT) by downloading from HF / TIMM Hub if necessary\n            if self.override_act_layer is None:\n                self.featurizer: VisionTransformer = timm.create_model(\n                    self.timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size,pretrained_cfg_overlay=pretrained_cfg_overlay\n                )\n            else:\n                self.featurizer: VisionTransformer = timm.create_model(\n                    self.timm_path_or_url,\n                    pretrained=True,\n                    num_classes=0,\n                    img_size=self.default_image_size,\n                    pretrained_cfg_overlay=pretrained_cfg_overlay,\n                    act_layer=self.override_act_layer,\n                )\n        else:\n            if self.override_act_layer is None:\n                self.featurizer: VisionTransformer = timm.create_model(\n                    self.timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size,\n                )\n            else:\n                self.featurizer: VisionTransformer = timm.create_model(\n                    self.timm_path_or_url,\n                    pretrained=True,\n                    num_classes=0,\n                    img_size=self.default_image_size,\n                    act_layer=self.override_act_layer,\n                )\n\n        self.featurizer.eval()\n\n        # Monkey-Patch the `forward()` function of the featurizer to ensure FSDP-compatibility\n        #   => Note: By default set `get_intermediate_layers` to return the *SECOND-TO-LAST* layer patches!\n        #   => TODO (siddk) Remove after resolution of https://github.com/pytorch/pytorch/issues/109385\n        self.featurizer.forward = unpack_tuple(\n            partial(self.featurizer.get_intermediate_layers, n={len(self.featurizer.blocks) - 2})\n        )\n\n        # Validation =>> for now, this class *only* supports TIMM Vision Transformers (but can be extended!)\n        assert isinstance(self.featurizer, VisionTransformer), (\n            \"Featurizer is not a TIMM VisionTransformer; if you would like to support a new visual representation, \"\n            \"file an issue or implement the requisite logic (see `cobra/models/backbones/vision/base_vision.py`)!\"\n        )\n\n        # Get Config =>> Note :: Override default image size to ensure correct image transform\n        self.data_cfg = timm.data.resolve_model_data_config(self.featurizer)\n        self.data_cfg[\"input_size\"] = (3, self.default_image_size, self.default_image_size)\n\n        # Initialize Default Image Transform --> Modified by `self.image_resize_strategy`\n        default_image_transform = timm.data.create_transform(**self.data_cfg, is_training=False)\n\n        # Fix =>> SigLIP & IN1K default transforms resize to *larger* than `self.default_image_size` (crops image)!\n        if \"siglip\" in self.timm_path_or_url or \"in1k\" in self.timm_path_or_url:\n            assert isinstance(default_image_transform, Compose), \"Unexpected `default_image_transform`!\"\n            assert isinstance(resize_transform := default_image_transform.transforms[0], Resize)\n            default_image_transform = Compose(\n                [\n                    Resize(self.default_image_size, interpolation=resize_transform.interpolation),\n                    *default_image_transform.transforms[1:],\n                ]\n            )\n\n        # Switch on `image_resize_strategy`\n        if self.image_resize_strategy == \"resize-naive\":\n            assert isinstance(default_image_transform, Compose), \"Unexpected `default_image_transform`!\"\n            assert isinstance(resize_transform := default_image_transform.transforms[0], Resize)\n\n            target_size = (self.default_image_size, self.default_image_size)\n            self.image_transform = Compose(\n                [\n                    Resize(target_size, interpolation=resize_transform.interpolation),\n                    *default_image_transform.transforms[1:],\n                ]\n            )\n\n        elif self.image_resize_strategy == \"resize-crop\":\n            self.image_transform = default_image_transform\n\n        elif self.image_resize_strategy == \"letterbox\":\n            assert isinstance(default_image_transform, Compose), \"Unexpected `default_image_transform`!\"\n            assert \"mean\" in self.data_cfg, \"TIMM `data_cfg` missing image normalization mean!\"\n\n            # Compute Padding Fill Value (rescaled normalization mean if applicable)\n            fill = tuple([int(x * 255) for x in self.data_cfg[\"mean\"]])\n\n            # Build New Transform\n            self.image_transform = Compose([LetterboxPad(fill), *default_image_transform.transforms])\n\n        else:\n            raise ValueError(f\"Image Resize Strategy `{self.image_resize_strategy}` is not supported!\")\n\n    def get_fsdp_wrapping_policy(self) -> Callable:\n        \"\"\"Return a simple FSDP policy that wraps each ViT block and then the _entire_ featurizer.\"\"\"\n        vit_wrap_policy = partial(_module_wrap_policy, module_classes={VisionTransformer})\n        transformer_block_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n        return partial(_or_policy, policies=[vit_wrap_policy, transformer_block_policy])\n\n    def forward(self, pixel_values: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> torch.Tensor:\n        \"\"\"Runs transformed image/pixel tensor through vision backbone, returning _all_ patch features.\"\"\"\n        return self.featurizer(pixel_values)\n\n    @property\n    def default_image_resolution(self) -> Tuple[int, int, int]:\n        return self.data_cfg[\"input_size\"]\n\n    @property\n    def embed_dim(self) -> int:\n        return self.featurizer.embed_dim\n\n    @property\n    def num_patches(self) -> int:\n        return self.featurizer.patch_embed.num_patches\n\n    @property\n    def half_precision_dtype(self) -> torch.dtype:\n        return self.dtype\n"}
{"type": "source_file", "path": "llamagen_tokenizer/vqgan/quantize.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import einsum\nfrom einops import rearrange\n\n\nclass VectorQuantizer(nn.Module):\n    \"\"\"\n    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n    ____________________________________________\n    Discretization bottleneck part of the VQ-VAE.\n    Inputs:\n    - n_e : number of embeddings\n    - e_dim : dimension of embedding\n    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n    _____________________________________________\n    \"\"\"\n\n    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n    # used wherever VectorQuantizer has been used before and is additionally\n    # more efficient.\n    def __init__(self, n_e, e_dim, beta):\n        super(VectorQuantizer, self).__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n    def forward(self, z):\n        \"\"\"\n        Inputs the output of the encoder network z and maps it to a discrete\n        one-hot vector that is the index of the closest embedding vector e_j\n        z (continuous) -> z_q (discrete)\n        z.shape = (batch, channel, height, width)\n        quantization pipeline:\n            1. get encoder input (B,C,H,W)\n            2. flatten input to (B*H*W,C)\n        \"\"\"\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n            torch.matmul(z_flattened, self.embedding.weight.t())\n\n        ## could possible replace this here\n        # #\\start...\n        # find closest encodings\n        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n\n        min_encodings = torch.zeros(\n            min_encoding_indices.shape[0], self.n_e).to(z)\n        min_encodings.scatter_(1, min_encoding_indices, 1)\n\n        # dtype min encodings: torch.float32\n        # min_encodings shape: torch.Size([2048, 512])\n        # min_encoding_indices.shape: torch.Size([2048, 1])\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n        #.........\\end\n\n        # with:\n        # .........\\start\n        #min_encoding_indices = torch.argmin(d, dim=1)\n        #z_q = self.embedding(min_encoding_indices)\n        # ......\\end......... (TODO)\n\n        # compute loss for embedding\n        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n            torch.mean((z_q - z.detach()) ** 2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # perplexity\n        e_mean = torch.mean(min_encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n\n    def get_codebook_entry(self, indices, shape):\n        # shape specifying (batch, height, width, channel)\n        # TODO: check for more easy handling with nn.Embedding\n        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n        min_encodings.scatter_(1, indices[:,None], 1)\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q\n\n\nclass VectorQuantizer2(nn.Module):\n    \"\"\"\n    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n    \"\"\"\n    # NOTE: due to a bug the beta term was applied to the wrong term. for\n    # backwards compatibility we use the buggy version by default, but you can\n    # specify legacy=False to fix it.\n    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\",\n                 sane_index_shape=False, legacy=True):\n        super().__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n        self.legacy = legacy\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n        self.remap = remap\n        if self.remap is not None:\n            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n            self.re_embed = self.used.shape[0]\n            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n            if self.unknown_index == \"extra\":\n                self.unknown_index = self.re_embed\n                self.re_embed = self.re_embed+1\n            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n                  f\"Using {self.unknown_index} for unknown indices.\")\n        else:\n            self.re_embed = n_e\n\n        self.sane_index_shape = sane_index_shape\n\n    def remap_to_used(self, inds):\n        ishape = inds.shape\n        assert len(ishape)>1\n        inds = inds.reshape(ishape[0],-1)\n        used = self.used.to(inds)\n        match = (inds[:,:,None]==used[None,None,...]).long()\n        new = match.argmax(-1)\n        unknown = match.sum(2)<1\n        if self.unknown_index == \"random\":\n            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n        else:\n            new[unknown] = self.unknown_index\n        return new.reshape(ishape)\n\n    def unmap_to_all(self, inds):\n        ishape = inds.shape\n        assert len(ishape)>1\n        inds = inds.reshape(ishape[0],-1)\n        used = self.used.to(inds)\n        if self.re_embed > self.used.shape[0]: # extra token\n            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n        return back.reshape(ishape)\n\n    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n\n        min_encoding_indices = torch.argmin(d, dim=1)\n        z_q = self.embedding(min_encoding_indices).view(z.shape)\n        perplexity = None\n        min_encodings = None\n\n        # compute loss for embedding\n        if not self.legacy:\n            loss = self.beta * torch.mean((z_q.detach()-z)**2) + \\\n                   torch.mean((z_q - z.detach()) ** 2)\n        else:\n            loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n                   torch.mean((z_q - z.detach()) ** 2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n\n        if self.remap is not None:\n            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n\n        if self.sane_index_shape:\n            min_encoding_indices = min_encoding_indices.reshape(\n                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n\n    def get_codebook_entry(self, indices, shape, channel_first=True):\n        # shape = (batch, channel, height, width) if channel_first else (batch, height, width, channel)\n        if self.remap is not None:\n            indices = indices.reshape(shape[0],-1) # add batch axis\n            indices = self.unmap_to_all(indices)\n            indices = indices.reshape(-1) # flatten again\n\n        # get quantized latent vectors\n        z_q = self.embedding(indices)  # (b*h*w, c)\n\n        if shape is not None:\n            if channel_first:\n                z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[1])\n                # reshape back to match original input shape\n                z_q = z_q.permute(0, 3, 1, 2).contiguous()\n            else:\n                z_q = z_q.view(shape)\n\n        return z_q"}
{"type": "source_file", "path": "llamagen_tokenizer/vae/reconstruction_vae_ddp.py", "content": "import torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.distributed as dist\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport os\nimport itertools\nfrom PIL import Image\nimport numpy as np\nimport argparse\nimport random\n\nfrom skimage.metrics import peak_signal_noise_ratio as psnr_loss\nfrom skimage.metrics import structural_similarity as ssim_loss\nfrom diffusers.models import AutoencoderKL\n\n\nclass SingleFolderDataset(Dataset):\n    def __init__(self, directory, transform=None):\n        super().__init__()\n        self.directory = directory\n        self.transform = transform\n        self.image_paths = [os.path.join(directory, file_name) for file_name in os.listdir(directory)\n                            if os.path.isfile(os.path.join(directory, file_name))]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(0)\n\n\ndef create_npz_from_sample_folder(sample_dir, num=50_000):\n    \"\"\"\n    Builds a single .npz file from a folder of .png samples.\n    \"\"\"\n    samples = []\n    for i in tqdm(range(num), desc=\"Building .npz file from samples\"):\n        sample_pil = Image.open(f\"{sample_dir}/{i:06d}.png\")\n        sample_np = np.asarray(sample_pil).astype(np.uint8)\n        samples.append(sample_np)\n    \n    random.shuffle(samples) # This is very important for IS(Inception Score) !!!\n    samples = np.stack(samples)\n    assert samples.shape == (num, samples.shape[1], samples.shape[2], 3)\n    npz_path = f\"{sample_dir}.npz\"\n    np.savez(npz_path, arr_0=samples)\n    print(f\"Saved .npz file to {npz_path} [shape={samples.shape}].\")\n    return npz_path\n\n\ndef center_crop_arr(pil_image, image_size):\n    \"\"\"\n    Center cropping implementation from ADM.\n    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n    \"\"\"\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n\n\ndef main(args):\n    # Setup PyTorch:\n    assert torch.cuda.is_available(), \"Sampling with DDP requires at least one GPU. sample.py supports CPU-only usage\"\n    torch.set_grad_enabled(False)\n\n    # Setup DDP:\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # load vae\n    vae = AutoencoderKL.from_pretrained(f\"stabilityai/{args.vae}\").to(device)\n\n    # Create folder to save samples:\n    folder_name = f\"stabilityai-{args.vae}-{args.dataset}-size-{args.image_size}-seed-{args.global_seed}\"\n    sample_folder_dir = f\"{args.sample_dir}/{folder_name}\"\n    if rank == 0:\n        os.makedirs(sample_folder_dir, exist_ok=True)\n        print(f\"Saving .png samples at {sample_folder_dir}\")\n    dist.barrier()\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n    if args.dataset == 'imagenet':\n        dataset = ImageFolder(args.data_path, transform=transform)\n        num_fid_samples = 50000\n    elif args.dataset == 'coco':\n        dataset = SingleFolderDataset(args.data_path, transform=transform)\n        num_fid_samples = 5000\n    else:\n        raise Exception(\"please check dataset\")\n    \n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=False,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=args.per_proc_batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=False\n    )    \n\n    # Figure out how many samples we need to generate on each GPU and how many iterations we need to run:\n    n = args.per_proc_batch_size\n    global_batch_size = n * dist.get_world_size()\n    \n    psnr_val_rgb = []\n    ssim_val_rgb = []\n    loader = tqdm(loader) if rank == 0 else loader\n    total = 0\n    for x, _ in loader:\n        rgb_gts = x\n        rgb_gts = (rgb_gts.permute(0, 2, 3, 1).to(\"cpu\").numpy() + 1.0) / 2.0 # rgb_gt value is between [0, 1]\n        x = x.to(device)\n        with torch.no_grad():\n            # Map input images to latent space + normalize latents:\n            latent = vae.encode(x).latent_dist.sample().mul_(0.18215)\n            # reconstruct:\n            samples = vae.decode(latent / 0.18215).sample # output value is between [-1, 1]\n        samples = torch.clamp(127.5 * samples + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n        \n        # Save samples to disk as individual .png files\n        for i, (sample, rgb_gt) in enumerate(zip(samples, rgb_gts)):\n            index = i * dist.get_world_size() + rank + total\n            Image.fromarray(sample).save(f\"{sample_folder_dir}/{index:06d}.png\")\n            # metric\n            rgb_restored = sample.astype(np.float32) / 255. # rgb_restored value is between [0, 1]\n            psnr = psnr_loss(rgb_restored, rgb_gt)\n            ssim = ssim_loss(rgb_restored, rgb_gt, multichannel=True, data_range=2.0, channel_axis=-1)\n            psnr_val_rgb.append(psnr)\n            ssim_val_rgb.append(ssim)\n        total += global_batch_size\n\n    # ------------------------------------\n    #       Summary\n    # ------------------------------------\n    # Make sure all processes have finished saving their samples\n    dist.barrier()\n    world_size = dist.get_world_size()\n    gather_psnr_val = [None for _ in range(world_size)]\n    gather_ssim_val = [None for _ in range(world_size)]\n    dist.all_gather_object(gather_psnr_val, psnr_val_rgb)\n    dist.all_gather_object(gather_ssim_val, ssim_val_rgb)\n\n    if rank == 0:\n        gather_psnr_val = list(itertools.chain(*gather_psnr_val))\n        gather_ssim_val = list(itertools.chain(*gather_ssim_val))        \n        psnr_val_rgb = sum(gather_psnr_val) / len(gather_psnr_val)\n        ssim_val_rgb = sum(gather_ssim_val) / len(gather_ssim_val)\n        print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb))\n\n        result_file = f\"{sample_folder_dir}_results.txt\"\n        print(\"writing results to {}\".format(result_file))\n        with open(result_file, 'w') as f:\n            print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb), file=f)\n\n        create_npz_from_sample_folder(sample_folder_dir, num_fid_samples)\n        print(\"Done.\")\n    \n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, choices=['imagenet', 'coco'], default='imagenet')\n    parser.add_argument(\"--vae\", type=str, choices=[\"sdxl-vae\", \"sd-vae-ft-mse\"], default=\"sd-vae-ft-mse\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 512], default=256)\n    parser.add_argument(\"--sample-dir\", type=str, default=\"reconstructions\")\n    parser.add_argument(\"--per-proc-batch-size\", type=int, default=32)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=4)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "llamagen_tokenizer/tokenizer_image/reconstruction_vq_ddp.py", "content": "import torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\nimport numpy as np\nimport argparse\nimport itertools\n\nfrom skimage.metrics import peak_signal_noise_ratio as psnr_loss\nfrom skimage.metrics import structural_similarity as ssim_loss\n\nfrom dataset.augmentation import center_crop_arr\nfrom dataset.build import build_dataset\nfrom tokenizer.tokenizer_image.vq_model import VQ_models\n\n\n\ndef create_npz_from_sample_folder(sample_dir, num=50000):\n    \"\"\"\n    Builds a single .npz file from a folder of .png samples.\n    \"\"\"\n    samples = []\n    for i in tqdm(range(num), desc=\"Building .npz file from samples\"):\n        sample_pil = Image.open(f\"{sample_dir}/{i:06d}.png\")\n        sample_np = np.asarray(sample_pil).astype(np.uint8)\n        samples.append(sample_np)\n    samples = np.stack(samples)\n    assert samples.shape == (num, samples.shape[1], samples.shape[2], 3)\n    npz_path = f\"{sample_dir}.npz\"\n    np.savez(npz_path, arr_0=samples)\n    print(f\"Saved .npz file to {npz_path} [shape={samples.shape}].\")\n    return npz_path\n\n\n\ndef main(args):\n    # Setup PyTorch:\n    assert torch.cuda.is_available(), \"Sampling with DDP requires at least one GPU. sample.py supports CPU-only usage\"\n    torch.set_grad_enabled(False)\n\n    # Setup DDP:\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    device = rank % torch.cuda.device_count()\n    seed = args.global_seed * dist.get_world_size() + rank\n    torch.manual_seed(seed)\n    torch.cuda.set_device(device)\n    print(f\"Starting rank={rank}, seed={seed}, world_size={dist.get_world_size()}.\")\n\n    # create and load model\n    vq_model = VQ_models[args.vq_model](\n        codebook_size=args.codebook_size,\n        codebook_embed_dim=args.codebook_embed_dim)\n    vq_model.to(device)\n    vq_model.eval()\n    checkpoint = torch.load(args.vq_ckpt, map_location=\"cpu\")\n    if \"ema\" in checkpoint:  # ema\n        model_weight = checkpoint[\"ema\"]\n    elif \"model\" in checkpoint:  # ddp\n        model_weight = checkpoint[\"model\"]\n    elif \"state_dict\" in checkpoint:\n        model_weight = checkpoint[\"state_dict\"]\n    else:\n        raise Exception(\"please check model weight\")\n    vq_model.load_state_dict(model_weight)\n    del checkpoint\n\n    # Create folder to save samples:\n    folder_name = (f\"{args.vq_model}-{args.dataset}-size-{args.image_size}-size-{args.image_size_eval}\"\n                  f\"-codebook-size-{args.codebook_size}-dim-{args.codebook_embed_dim}-seed-{args.global_seed}\")\n    sample_folder_dir = f\"{args.sample_dir}/{folder_name}\"\n    if rank == 0:\n        os.makedirs(sample_folder_dir, exist_ok=True)\n        print(f\"Saving .png samples at {sample_folder_dir}\")\n    dist.barrier()\n\n    # Setup data:\n    transform = transforms.Compose([\n        transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n    ])\n\n    if args.dataset == 'imagenet':\n        dataset = build_dataset(args, transform=transform)\n        num_fid_samples = 50000\n    elif args.dataset == 'coco':\n        dataset = build_dataset(args, transform=transform)\n        num_fid_samples = 5000\n    else:\n        raise Exception(\"please check dataset\")\n    \n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=dist.get_world_size(),\n        rank=rank,\n        shuffle=False,\n        seed=args.global_seed\n    )\n    loader = DataLoader(\n        dataset,\n        batch_size=args.per_proc_batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=args.num_workers,\n        pin_memory=True,\n        drop_last=False\n    )    \n\n    # Figure out how many samples we need to generate on each GPU and how many iterations we need to run:\n    n = args.per_proc_batch_size\n    global_batch_size = n * dist.get_world_size()\n    \n    psnr_val_rgb = []\n    ssim_val_rgb = []\n    loader = tqdm(loader) if rank == 0 else loader\n    total = 0\n    for x, _ in loader:\n        if args.image_size_eval != args.image_size:\n            rgb_gts = F.interpolate(x, size=(args.image_size_eval, args.image_size_eval), mode='bicubic')\n        else:\n            rgb_gts = x\n        rgb_gts = (rgb_gts.permute(0, 2, 3, 1).to(\"cpu\").numpy() + 1.0) / 2.0 # rgb_gt value is between [0, 1]\n        x = x.to(device, non_blocking=True)\n        with torch.no_grad():\n            latent, _, [_, _, indices] = vq_model.encode(x)\n            samples = vq_model.decode_code(indices, latent.shape) # output value is between [-1, 1]\n            if args.image_size_eval != args.image_size:\n                samples = F.interpolate(samples, size=(args.image_size_eval, args.image_size_eval), mode='bicubic')\n        samples = torch.clamp(127.5 * samples + 128.0, 0, 255).permute(0, 2, 3, 1).to(\"cpu\", dtype=torch.uint8).numpy()\n\n        # Save samples to disk as individual .png files\n        for i, (sample, rgb_gt) in enumerate(zip(samples, rgb_gts)):\n            index = i * dist.get_world_size() + rank + total\n            Image.fromarray(sample).save(f\"{sample_folder_dir}/{index:06d}.png\")\n            # metric\n            rgb_restored = sample.astype(np.float32) / 255. # rgb_restored value is between [0, 1]\n            psnr = psnr_loss(rgb_restored, rgb_gt)\n            ssim = ssim_loss(rgb_restored, rgb_gt, multichannel=True, data_range=2.0, channel_axis=-1)\n            psnr_val_rgb.append(psnr)\n            ssim_val_rgb.append(ssim)\n            \n        total += global_batch_size\n\n    # ------------------------------------\n    #       Summary\n    # ------------------------------------\n    # Make sure all processes have finished saving their samples\n    dist.barrier()\n    world_size = dist.get_world_size()\n    gather_psnr_val = [None for _ in range(world_size)]\n    gather_ssim_val = [None for _ in range(world_size)]\n    dist.all_gather_object(gather_psnr_val, psnr_val_rgb)\n    dist.all_gather_object(gather_ssim_val, ssim_val_rgb)\n\n    if rank == 0:\n        gather_psnr_val = list(itertools.chain(*gather_psnr_val))\n        gather_ssim_val = list(itertools.chain(*gather_ssim_val))        \n        psnr_val_rgb = sum(gather_psnr_val) / len(gather_psnr_val)\n        ssim_val_rgb = sum(gather_ssim_val) / len(gather_ssim_val)\n        print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb))\n\n        result_file = f\"{sample_folder_dir}_results.txt\"\n        print(\"writing results to {}\".format(result_file))\n        with open(result_file, 'w') as f:\n            print(\"PSNR: %f, SSIM: %f \" % (psnr_val_rgb, ssim_val_rgb), file=f)\n\n        create_npz_from_sample_folder(sample_folder_dir, num_fid_samples)\n        print(\"Done.\")\n    \n    dist.barrier()\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data-path\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, choices=['imagenet', 'coco'], default='imagenet')\n    parser.add_argument(\"--vq-model\", type=str, choices=list(VQ_models.keys()), default=\"VQ-16\")\n    parser.add_argument(\"--vq-ckpt\", type=str, default=None, help=\"ckpt path for vq model\")\n    parser.add_argument(\"--codebook-size\", type=int, default=16384, help=\"codebook size for vector quantization\")\n    parser.add_argument(\"--codebook-embed-dim\", type=int, default=8, help=\"codebook dimension for vector quantization\")\n    parser.add_argument(\"--image-size\", type=int, choices=[256, 384, 512], default=256)\n    parser.add_argument(\"--image-size-eval\", type=int, choices=[256, 384, 512], default=256)\n    parser.add_argument(\"--sample-dir\", type=str, default=\"reconstructions\")\n    parser.add_argument(\"--per-proc-batch-size\", type=int, default=32)\n    parser.add_argument(\"--global-seed\", type=int, default=0)\n    parser.add_argument(\"--num-workers\", type=int, default=4)\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "models/cobra/backbones/vision/dinosiglip_vit.py", "content": "\"\"\"\ndinosiglip_vit.py\n\nVision backbone that returns concatenated features from both DINOv2 and SigLIP.\n\"\"\"\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Callable, Dict, Tuple\n\nimport timm\nimport torch\nfrom PIL import Image\nfrom timm.models.vision_transformer import Block, VisionTransformer\nfrom torch.distributed.fsdp.wrap import _module_wrap_policy, _or_policy, transformer_auto_wrap_policy\nfrom torchvision.transforms import Compose, Resize\n\nfrom models.cobra.backbones.vision.base_vision import ImageTransform, LetterboxPad, VisionBackbone, unpack_tuple\n\n# Registry =>> Supported DinoSigLIP Pairs (as TIMM identifiers)\nDINOSigLIP_VISION_BACKBONES = {\n    \"dinosiglip-vit-so-384px\": {\n        \"dino\": \"vit_large_patch14_reg4_dinov2.lvd142m\",\n        \"siglip\": \"vit_so400m_patch14_siglip_384\",\n    },\n}\n# DINOSigLIP_VISION_BACKBONES = {\n#     \"dinosiglip-vit-so-384px\": {\n#         \"dino\": \"ckpts/dinov2/\",\n#         \"siglip\": \"ckpts/SigLIP/\",\n#     },\n# }\n\n@dataclass\nclass DinoSigLIPImageTransform:\n    dino_image_transform: ImageTransform\n    siglip_image_transform: ImageTransform\n    is_cobra: bool = True\n\n    def __call__(self, img: Image, **kwargs: str) -> Dict[str, torch.Tensor]:\n        return {\"dino\": self.dino_image_transform(img, **kwargs), \"siglip\": self.siglip_image_transform(img, **kwargs)}\n\n\nclass DinoSigLIPViTBackbone(VisionBackbone):\n    def __init__(self, vision_backbone_id: str, image_resize_strategy: str, default_image_size: int = 224) -> None:\n        super().__init__(vision_backbone_id, image_resize_strategy, default_image_size=default_image_size)\n        self.dino_timm_path_or_url = DINOSigLIP_VISION_BACKBONES[vision_backbone_id][\"dino\"]\n        self.siglip_timm_path_or_url = DINOSigLIP_VISION_BACKBONES[vision_backbone_id][\"siglip\"]\n\n        self.siglip_featurizer: VisionTransformer = timm.create_model(\n            self.siglip_timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size, pretrained_cfg_overlay=dict(file=\"ckpts/SigLIP/open_clip_pytorch_model.bin\")\n        )\n\n        self.siglip_featurizer.eval()\n\n\n        self.dino_featurizer: VisionTransformer = timm.create_model(\n            self.dino_timm_path_or_url, pretrained=True, num_classes=0, img_size=self.default_image_size, pretrained_cfg_overlay=dict(file=\"ckpts/dinov2/pytorch_model.bin\")\n        )\n        self.dino_featurizer.eval()\n\n\n        # Monkey-Patch the `forward()` function of the featurizers to ensure FSDP-compatibility\n        #   => Note: By default set `get_intermediate_layers` to return the *SECOND-TO-LAST* layer patches!\n        #   => TODO (siddk) Remove after resolution of https://github.com/pytorch/pytorch/issues/109385\n        self.dino_featurizer.forward = unpack_tuple(\n            partial(self.dino_featurizer.get_intermediate_layers, n={len(self.dino_featurizer.blocks) - 2})\n        )\n        self.siglip_featurizer.forward = unpack_tuple(\n            partial(self.siglip_featurizer.get_intermediate_layers, n={len(self.siglip_featurizer.blocks) - 2})\n        )\n\n        # Get Configs for _both_ Featurizers =>> Note :: Override default image size for larger resolution models\n        self.dino_data_cfg = timm.data.resolve_model_data_config(self.dino_featurizer)\n        self.dino_data_cfg[\"input_size\"] = (3, self.default_image_size, self.default_image_size)\n\n        self.siglip_data_cfg = timm.data.resolve_model_data_config(self.siglip_featurizer)\n        self.siglip_data_cfg[\"input_size\"] = (3, self.default_image_size, self.default_image_size)\n\n        # Initialize *both* Transforms\n        default_dino_transform = timm.data.create_transform(**self.dino_data_cfg, is_training=False)\n        default_siglip_transform = timm.data.create_transform(**self.siglip_data_cfg, is_training=False)\n\n        # Fix =>> SigLIP default transform resizes to *larger* than `self.default_image_size` (crops image)!!\n        assert isinstance(default_siglip_transform, Compose), \"Unexpected `default_image_transform`!\"\n        assert isinstance(sl_resize_transform := default_siglip_transform.transforms[0], Resize)\n        default_siglip_transform = Compose(\n            [\n                Resize(self.default_image_size, interpolation=sl_resize_transform.interpolation),\n                *default_siglip_transform.transforms[1:],\n            ]\n        )\n\n        if self.image_resize_strategy == \"resize-naive\":\n            assert isinstance(default_dino_transform, Compose), \"Unexpected `default_dino_image_transform`!\"\n            assert isinstance(default_siglip_transform, Compose), \"Unexpected `default_siglip_image_transform`!\"\n            assert isinstance(dino_resize_transform := default_dino_transform.transforms[0], Resize)\n            assert isinstance(siglip_resize_transform := default_siglip_transform.transforms[0], Resize)\n\n            target_size = (self.default_image_size, self.default_image_size)\n            dino_transform = Compose(\n                [\n                    Resize(target_size, interpolation=dino_resize_transform.interpolation),\n                    *default_dino_transform.transforms[1:],\n                ]\n            )\n            siglip_transform = Compose(\n                [\n                    Resize(target_size, interpolation=siglip_resize_transform.interpolation),\n                    *default_siglip_transform.transforms[1:],\n                ]\n            )\n\n            self.image_transform = DinoSigLIPImageTransform(dino_transform, siglip_transform)\n\n        elif self.image_resize_strategy == \"resize-crop\":\n            self.image_transform = DinoSigLIPImageTransform(default_dino_transform, default_siglip_transform)\n\n        elif self.image_resize_strategy == \"letterbox\":\n            assert isinstance(default_dino_transform, Compose), \"Unexpected `default_dino_transform`!\"\n            assert isinstance(default_siglip_transform, Compose), \"Unexpected `default_siglip_transform`!\"\n            assert (\n                \"mean\" in self.dino_data_cfg and \"mean\" in self.siglip_data_cfg\n            ), \"DinoSigLIP `data_cfg` missing `mean`!\"\n\n            # Compute Padding Fill Value(s) (rescaled normalization mean if applicable)\n            dino_fill = tuple([int(x * 255) for x in self.dino_data_cfg[\"mean\"]])\n            siglip_fill = tuple([int(x * 255) for x in self.siglip_data_cfg[\"mean\"]])\n\n            # Build New Transform\n            self.image_transform = DinoSigLIPImageTransform(\n                Compose([LetterboxPad(dino_fill), *default_dino_transform.transforms]),\n                Compose([LetterboxPad(siglip_fill), *default_siglip_transform.transforms]),\n            )\n\n        else:\n            raise ValueError(f\"Image Resize Strategy `{self.image_resize_strategy}` is not supported!\")\n\n    def get_fsdp_wrapping_policy(self) -> Callable:\n        \"\"\"Return a simple FSDP policy that wraps each ViT block and then both of the _entire_ featurizers.\"\"\"\n        vit_wrap_policy = partial(_module_wrap_policy, module_classes={VisionTransformer})\n        transformer_block_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n        return partial(_or_policy, policies=[vit_wrap_policy, transformer_block_policy])\n\n    def forward(self, pixel_values: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"Runs the transformed image/pixel tensors through each vision backbone, returning concatenated patches.\"\"\"\n        dino_patches = self.dino_featurizer(pixel_values[\"dino\"]) # b 729 1024\n        siglip_patches = self.siglip_featurizer(pixel_values[\"siglip\"]) # b 729 1152\n\n        return torch.cat([dino_patches, siglip_patches], dim=2)\n\n    @property\n    def default_image_resolution(self) -> Tuple[int, int, int]:\n        return self.dino_data_cfg[\"input_size\"]\n\n    @property\n    def embed_dim(self) -> int:\n        return self.dino_featurizer.embed_dim + self.siglip_featurizer.embed_dim\n\n    @property\n    def num_patches(self) -> int:\n        assert self.dino_featurizer.patch_embed.num_patches == self.siglip_featurizer.patch_embed.num_patches\n        return self.dino_featurizer.patch_embed.num_patches\n\n    @property\n    def half_precision_dtype(self) -> torch.dtype:\n        return torch.bfloat16\n"}
