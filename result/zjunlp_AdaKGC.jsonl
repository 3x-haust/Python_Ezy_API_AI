{"repo_info": {"repo_name": "AdaKGC", "repo_owner": "zjunlp", "repo_url": "https://github.com/zjunlp/AdaKGC"}}
{"type": "source_file", "path": "dataset_construct/scripts/process_nerd.py", "content": "def main():\n    src_path = \"data/Few-NERD/supervised\"\n    tgt_path = \"data/Few-NERD\"\n    for split in [\"train.txt\", \"dev.txt\", \"test.txt\"]:\n        with open(f\"{src_path}/{split}\", 'r') as reader:\n            with open(f\"{tgt_path}/{split}\", 'w') as writer:\n                for line in reader:\n                    if line.strip() != \"\":\n                        n_line = line.strip().split('\\t')\n                        if n_line[-1] != \"O\":\n                            line = f\"{n_line[0]}\\tI-{n_line[-1]}\\n\"\n                    writer.write(line)\n\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "dataset_construct/data_convert.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport os\nimport json\nfrom typing import Dict, List\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom universal_ie.record_schema import RecordSchema\nfrom universal_ie.dataset import Dataset\nfrom universal_ie.ie_format import Sentence\nfrom universal_ie.logger import init_logger\nlogger = None\n\n\n\ndef convert_graph(\n    output_folder: str,\n    datasets: Dict[str, List[Sentence]],\n    label_mapper: Dict = None,\n):\n\n    def get_label_str(label):\n        return label_mapper.get(label.__repr__(), label.__repr__())\n\n    counter = Counter()\n    os.makedirs(output_folder, exist_ok=True)\n\n\n    schema = {}\n    for data_type, instance_list in datasets.items():    \n        with open(os.path.join(output_folder, f\"{data_type}.json\"), \"w\") as output:\n            for instance in tqdm(instance_list):\n                counter.update([f\"{data_type} sent\"])\n\n                for entity in instance.entities:\n                    if get_label_str(entity.label) not in schema:\n                        schema[get_label_str(entity.label)] = set()  \n\n                for relation in instance.relations:\n                    if get_label_str(relation.arg1.label) not in schema:\n                        schema[get_label_str(relation.arg1.label)] = set() \n                    schema[get_label_str(relation.arg1.label)].add(get_label_str(relation.label)) \n\n                for event in instance.events:\n                    if get_label_str(event.label) not in schema:\n                        schema[get_label_str(event.label)] = set() \n                    for arg_role, _ in event.args:\n                        schema[get_label_str(event.label)].add(get_label_str(arg_role))\n\n                output.write(\n                    \"%s\\n\"\n                    % json.dumps(\n                        {\n                            \"text\": ' '.join(instance.tokens),\n                            \"tokens\": instance.tokens,\n                            \"entity\": [\n                                entity.to_offset(label_mapper)\n                                for entity in instance.entities\n                            ],\n                            \"relation\": [\n                                relation.to_offset(\n                                    ent_label_mapper=label_mapper,\n                                    rel_label_mapper=label_mapper,\n                                )\n                                for relation in instance.relations\n                            ],\n                            \"event\": [\n                                event.to_offset(evt_label_mapper=label_mapper)\n                                for event in instance.events\n                            ],\n                        },\n                        ensure_ascii=False,\n                    )\n                )\n\n\n    RecordSchema.output_schema(schema, os.path.join(output_folder, \"schema.json\"))\n    logger.info(f\"Counter: {dict(counter)}\")    \n    print(output_folder)\n    print(\"==========================\")\n\n\n\n\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", dest=\"config\", default=\"../config/data_config/relation\", help=\"Dictory which contains config file used to convert dataset\")\n    parser.add_argument(\"--iter_num\", dest=\"iter_num\", default=7, type=int, help=\"Iteration number\")\n    parser.add_argument(\"--task\", dest=\"task\", default=\"NYT\", choices=['NYT', 'Few-NERD', 'ace05_event'], help=\"Task dataset name\")\n    parser.add_argument(\"--mode\", dest=\"mode\", default=\"V\", choices=['V', 'H', 'M', 'R'], help=\"Segmentation Mode, H: Horizontal segmentation, V: Vertical segmentation, M: Mixed segmentation, R: Replacement segmentation\")\n    options = parser.parse_args()\n\n    global logger\n    logger = init_logger(task_name = f\"{options.task}_{options.mode}\")\n\n    for it in range(1, options.iter_num + 1):\n        filename = f\"{options.config}/{options.task}_{options.mode}{it}.yaml\"\n        logger.info(f\"Filename: {filename}\")  \n        dataset = Dataset.load_yaml_file(filename)\n        datasets = dataset.load_dataset(logger_name=f\"{options.task}_{options.mode}\")    \n        label_mapper = dataset.mapper\n        output_name = f\"../data/{options.task}_{options.mode}/iter_{it}\"  \n\n        convert_graph(\n            output_name,\n            datasets=datasets,\n            label_mapper=label_mapper,\n        )\n\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "inference.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport math\nimport os\nimport argparse\nimport logging\nimport json\nimport time\nimport re\nfrom tqdm import tqdm\nimport sys\n\nimport torch\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration\n\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.sel2record.record import MapConfig\nfrom uie.extraction.scorer import *\nfrom uie.sel2record.sel2record import SEL2Record\nfrom uie.seq2seq.constraint_decoder import get_constraint_decoder\nfrom uie.seq2seq.models import T5Prompt\nfrom uie.extraction.constants import type_start, type_end, span_start, null_span\n\nlogger = logging.getLogger(__name__)\n\n\nsplit_bracket = re.compile(r\"\\s*<extra_id_\\d>\\s*\")\nspecial_to_remove = {'<pad>', '</s>'}\n\ncwd = os.getcwd()\n\n\ndef read_json_file(file_name):\n    return [json.loads(line) for line in open(file_name)]\n\n\ndef schema_to_ssi(schema: RecordSchema):\n    ssi = \"<spot> \" + \"<spot> \".join(sorted(schema.type_list))\n    ssi += \"<asoc> \" + \"<asoc> \".join(sorted(schema.role_list))\n    ssi += \" <extra_id_2> \"\n    return ssi\n\n\ndef schema_to_purssi(schema: RecordSchema):\n    type_ssi = \" \".join(sorted(schema.type_list))\n    role_ssi = \" \".join(sorted(schema.role_list))\n    ssi = \" \".join([type_start, type_end, span_start, null_span])\n    return type_ssi + \" \" + role_ssi + \" \" + ssi\n\n\ndef post_processing(x):\n    for special in special_to_remove:\n        x = x.replace(special, '')\n    return x.strip()\n\n\ndef schema_to_spotasoc(schema: RecordSchema, tokenizer):\n    spots = []\n    asocs = []\n    for spot in sorted(schema.type_list):\n        spots.append(tokenizer.encode(spot, add_special_tokens = False))\n    for asoc in sorted(schema.role_list):\n        asocs.append(tokenizer.encode(asoc, add_special_tokens = False))\n    return spots, asocs\n\n\n\nclass HuggingfacePromptPredictor:\n    def __init__(self, decoding_format = 'spotasoc', source_prefix = '', args = None) -> None:\n        self._tokenizer = T5TokenizerFast.from_pretrained(args.model)\n        logger.info(f\"Tokenizer Length: {len(self._tokenizer)}\")\n        self._device = f\"cuda:{args.cuda}\" if torch.cuda.is_available() else \"cpu\"\n        logger.info(f\"Device: {self._device}\")\n        self._model = T5Prompt(self._tokenizer, args.t5_path, args).to(self._device)\n        self._model.load_state_dict(torch.load(os.path.join(args.model, 'pytorch_model.bin'), map_location=self._device))\n        '''是这样的, 先初始化__init__(slef/encoder/decoder prompt先随机初始化吧), 再load_state_dict取参数(prompt也取)'''\n        self._model.eval()\n\n        self._schema = RecordSchema.read_from_file(os.path.join(args.data_folder, \"record.schema\"))       \n        spots, asocs = schema_to_spotasoc(self._schema, self._tokenizer)\n        self._ssi = schema_to_ssi(self._schema)\n        self._spots = spots\n        self._asocs = asocs\n        logger.info(f\"ssi: {self._ssi}\")\n        logger.info(f\"spots: {self._spots}\")\n        logger.info(f\"asocs: {self._asocs}\")\n        logger.info(f\"use_ssi: {args.use_ssi}\")\n        self._max_source_length = args.max_source_length\n        self._max_target_length = args.max_target_length\n        self._use_ssi = args.use_ssi\n        self._args = {\"num_beams\": args.num_beams, \"do_sample\": args.do_sample, \"top_k\": args.top_k, \"top_p\": args.top_p}\n\n        \n        if args.CD:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self._tokenizer,\n                                                             type_schema = self._schema,\n                                                             decoding_schema = decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = args.task)\n        else:\n            self.constraint_decoder = None\n            \n\n    def predict(self, text):\n        func = None\n        def CD_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n        if self.constraint_decoder is not None:\n            func = CD_fn\n        \n        if self._use_ssi:\n            text = [self._ssi + x for x in text]  \n        inputs = self._tokenizer(text, padding=True, return_tensors='pt').to(self._device)\n        inputs['input_ids'] = inputs['input_ids'][:, :self._max_source_length]\n        inputs['attention_mask'] = inputs['attention_mask'][:, :self._max_source_length] \n\n        result = self._model.generate(\n            input_ids=inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            spot=[self._spots] * inputs[\"input_ids\"].size(0),\n            asoc=[self._asocs] * inputs[\"input_ids\"].size(0),\n            prefix_allowed_tokens_fn=func,\n            **self._args\n        )\n\n        return self._tokenizer.batch_decode(result, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n\n\nclass HuggingfacePredictor:\n    def __init__(self, decoding_format = 'spotasoc', source_prefix = '', args = None) -> None:\n        self._tokenizer = T5TokenizerFast.from_pretrained(args.model)\n        self._model = T5ForConditionalGeneration.from_pretrained(args.model)\n        self._model.cuda(f\"cuda:{args.cuda}\")\n        self._schema = RecordSchema.read_from_file(os.path.join(args.data_folder, \"record.schema\"))\n        self._ssi = schema_to_ssi(self._schema)          \n        self._purssi = list(set(self._tokenizer.encode(schema_to_purssi(self._schema))))\n        self._max_source_length = args.max_source_length\n        self._max_target_length = args.max_target_length\n        self._args = {\"num_beams\": args.num_beams, \"do_sample\": args.do_sample, \"top_k\": args.top_k, \"top_p\": args.top_p}\n        if args.CD:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self._tokenizer,\n                                                             type_schema = self._schema,\n                                                             decoding_schema = decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = args.task)\n        else:\n            self.constraint_decoder = None\n\n    def predict(self, text):\n        func = None\n        def CD_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n        if self.constraint_decoder is not None:\n            func = CD_fn\n\n        text = [self._ssi + x for x in text]          # SSI作前缀\n        inputs = self._tokenizer(text, padding=True, return_tensors='pt').to(self._model.device)\n        inputs['input_ids'] = inputs['input_ids'][:, :self._max_source_length]\n        inputs['attention_mask'] = inputs['attention_mask'][:, :self._max_source_length] \n\n        result = self._model.generate(\n            input_ids=inputs['input_ids'],\n            prefix_allowed_tokens_fn=func,\n            attention_mask=inputs['attention_mask'],\n            max_length=self._max_target_length,\n            **self._args\n        )\n        \n        return self._tokenizer.batch_decode(result, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n\n    \ntask_dict = {\n    'entity': EntityScorer,\n    'relation': RelationScorer,\n    'event': EventScorer,\n}\n\n\ndef do_predict(predictor, output_dir, split_name, batch_num, options, text_list):\n    predicts = list()\n    if os.path.exists(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt')):\n        with open(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt'), 'r') as reader:\n            for line in reader:\n                predicts.append(line.strip())\n        return predicts\n    \n    for index in tqdm(range(batch_num)):\n        start = index * options.batch_size\n        end = index * options.batch_size + options.batch_size\n\n        pred_seq2seq = predictor.predict(text_list[start: end])\n        pred_seq2seq = [post_processing(x) for x in pred_seq2seq]\n\n        predicts += pred_seq2seq\n\n    with open(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt'), 'w') as output:\n        for pred in predicts:\n            output.write(f'{pred}\\n')\n\n    return predicts\n\n\ndef do_sel2record(predicts, sel2record, text_list, token_list, output_dir, split_name):\n    records = list()\n    if os.path.exists(os.path.join(output_dir, f'{split_name}_preds_record.txt')):\n        with open(os.path.join(output_dir, f'{split_name}_preds_record.txt'), 'r') as reader:\n            for line in reader:\n                records.append(json.loads(line.strip()))\n        return records\n\n    for p, text, tokens in zip(predicts, text_list, token_list):\n        r = sel2record.sel2record(pred=p, text=text, tokens=tokens)\n        records += [r]\n\n    with open(os.path.join(output_dir, f'{split_name}_preds_record.txt'), 'w') as output:\n        for record in records:\n            output.write(f'{json.dumps(record)}\\n')\n    \n    return records\n\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataname', default='relation/NYT')\n    parser.add_argument('--model', default='hf_models/mix')\n    parser.add_argument('--task', default='relation')\n    parser.add_argument('--cuda', default='0')\n    parser.add_argument('--mode', default='H')\n    parser.add_argument('--t5_path', default='hf_models/mix', type=str)\n\n    parser.add_argument('--max_source_length', default=256, type=int)\n    parser.add_argument('--max_target_length', default=192, type=int)\n    parser.add_argument('--batch_size', default=256, type=int)\n    parser.add_argument('--config', dest='map_config', help='Offset Re-mapping Config', default='config/offset_map/closest_offset_en.yaml')\n    parser.add_argument('--decoding', default='spotasoc')\n    parser.add_argument('--verbose', action='store_true')\n    parser.add_argument('--match_mode', default='normal', choices=['set', 'normal', 'multimatch'])\n    \n    parser.add_argument('--use_prompt', action='store_true')\n    parser.add_argument('--use_ssi', action='store_true')\n    parser.add_argument('--prompt_len', default=10, type=int)\n    parser.add_argument('--prompt_dim', default=512, type=int)\n    parser.add_argument('--CD', action='store_true')\n\n    parser.add_argument('--do_sample', action='store_true')\n    parser.add_argument('--num_beams', default=None, type=int)\n    parser.add_argument('--top_k', default=None, type=int)\n    parser.add_argument('--top_p', default=None, type=float)\n\n                \n    options = parser.parse_args()\n    if options.task == \"relation\":\n        #tgt = [16, 17, 18, 22, 23, 24, 28, 29, 30, 34, 35, 36]\n        tgt = [18, 24, 30, 36]\n    elif options.task == \"event\":\n        #tgt = [40, 41, 42, 46, 47, 48, 52, 53, 54, 58, 59, 60]\n        tgt = [42, 48, 54, 60]\n    elif options.task == \"entity\":\n        #tgt = [4, 5, 6, 10, 11, 12]\n        tgt = [6, 12]\n\n    options.data_folder = options.dataname\n\n    model_path = '_'.join(options.model.split('/')[1:]).replace('/', '_')\n    if options.num_beams != None:\n            model_path += f'_beam{options.num_beams}'\n    if options.do_sample:\n        if options.top_k != None:\n            model_path += f'_topk{options.top_k}'\n        if options.top_p != None:\n            model_path += f'_topp{options.top_p}'\n    os.makedirs(os.path.join('output_infer', model_path), exist_ok = True)\n    \n\n    data_dir = options.dataname.replace('/', '_')\n    output_dir = os.path.join('output_infer', model_path, data_dir)\n    if options.CD:\n        output_dir += '_CD'\n    if os.path.exists(output_dir) and os.path.exists(os.path.join(output_dir, 'test_results.txt')):\n        cur_time = time.strftime('%m_%d_%H_%M', time.localtime(time.time()))\n        output_dir += cur_time\n    os.makedirs(output_dir, exist_ok = True)\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(funcName)s - %(lineno)d - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(os.path.join(output_dir, 'log.txt'), mode = 'w', encoding = 'utf-8')],\n    )\n\n    logger.setLevel(logging.INFO)\n    logger.info(f\"config: f{vars(options)}\")\n    logger.info(f\"data: {data_dir}\")\n\n\n    if options.use_prompt:\n        predictor = HuggingfacePromptPredictor(args=options) \n    else:\n        predictor = HuggingfacePredictor(args=options) \n\n    map_config = MapConfig.load_from_yaml(options.map_config)\n    schema_dict = SEL2Record.load_schema_dict(options.data_folder)\n    sel2record = SEL2Record(\n        schema_dict=schema_dict,\n        decoding_schema=options.decoding,\n        map_config=map_config,\n    )\n\n    for split, split_name in [('test', 'test')]:\n        gold_filename = os.path.join(options.data_folder, f'{split}.json')\n        text_list = [x['text'] for x in read_json_file(gold_filename)]\n        token_list = [x['tokens'] for x in read_json_file(gold_filename)]\n\n        batch_num = math.ceil(len(text_list) / options.batch_size)\n\n        predicts = do_predict(predictor, output_dir, split_name, batch_num, options, text_list)\n        records = do_sel2record(predicts, sel2record, text_list, token_list, output_dir, split_name)\n\n\n        results = dict()\n        for task, scorer in task_dict.items():\n            gold_list = [x[task] for x in read_json_file(gold_filename)]\n            pred_list = [x[task] for x in records]\n\n            gold_instance_list = scorer.load_gold_list(gold_list)\n            pred_instance_list = scorer.load_pred_list(pred_list)\n\n            sub_results = scorer.eval_instance_list(\n                gold_instance_list=gold_instance_list,\n                pred_instance_list=pred_instance_list,\n                verbose=options.verbose,\n                match_mode=options.match_mode,\n            )\n            results.update(sub_results)\n\n        with open(os.path.join(output_dir, f'{split_name}_results.txt'), 'w') as output:\n            for key, value in results.items():\n                output.write(f'{split_name}_{key}={value}\\n')\n\n                \n        \n        number = []\n        with open(os.path.join(output_dir, f'{split_name}_results.txt'), 'r') as freader:\n            for i, line in enumerate(freader, 1):\n                if i in tgt:\n                    logger.info(f\"{line.strip()}\")\n                    number.append(line.split(\"=\")[-1])\n\n        for num in number:\n            logger.info(f\"{num.strip()}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/logger.py", "content": "import logging\nimport os\n\ndef init_logger(task_name):\n    logger = logging.getLogger(task_name)\n    logger.setLevel(logging.INFO) \n\n    logfile = './log/' + task_name + '_logger.txt'\n    os.makedirs('./log/', exist_ok=True)   \n   \n    fh = logging.FileHandler(logfile, mode = 'a', encoding = 'utf-8')\n    fh.setLevel(logging.INFO)    \n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)  \n\n    formatter = logging.Formatter(\"%(asctime)s: %(message)s\")\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n\n    return logger"}
{"type": "source_file", "path": "uie/extraction/noiser/spot_asoc_noiser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport sys, os\nsys.path.append(os.getcwd())     # ModuleNotFoundError: No module named 'uie'\nfrom uie.extraction import constants\nfrom dataclasses import dataclass\nimport numpy as np\nfrom uie.extraction.utils import *\n\n\n@dataclass\nclass SpotAsocNoiser:\n    spot_noise_ratio: float = 0.1\n    asoc_noise_ratio: float = 0.1\n    null_span: str = constants.null_span\n\n    def random_insert_spot(self, spot_asoc, spot_label_list=None):\n        \"\"\"随机插入 Spot, 类别从 spot_label_list 中自动选择\n\n        Args:\n            spot_asoc ([type]): [description]\n            spot_label_list ([type], optional): [description]. Defaults to None.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        if spot_label_list is None or len(spot_label_list) == 0:\n            return spot_asoc\n        random_num = sum(np.random.binomial(1, self.spot_noise_ratio, len(spot_asoc)))\n        for _ in range(random_num):\n            random_position = np.random.randint(low=0, high=len(spot_asoc))\n            random_label = np.random.choice(spot_label_list)\n            spot_asoc.insert(\n                random_position,\n                {\"span\": self.null_span, \"label\": random_label, 'asoc': list()}\n            )\n        return spot_asoc\n\n    def random_insert_asoc(self, spot_asoc, asoc_label_list=None):\n        \"\"\"随机插入 Asoc，类别从 asoc_label_list 中自动选择\n\n        Args:\n            spot_asoc ([type]): [description]\n            asoc_label_list ([type], optional): [description]. Defaults to None.\n\n        Returns:\n            [type]: [description]\n        \"\"\"\n        if asoc_label_list is None or len(asoc_label_list) == 0:\n            return spot_asoc\n        # asoc_sum = sum([len(x['asoc']) for x in spot_asoc])\n        spot_sum = len(spot_asoc)\n        random_num = sum(np.random.binomial(1, self.asoc_noise_ratio, spot_sum))\n        for _ in range(random_num):\n            random_label = np.random.choice(asoc_label_list)\n            spot_position = np.random.randint(low=0, high=len(spot_asoc))\n            asoc_position = np.random.randint(low=0, high=len(spot_asoc[spot_position]['asoc']) + 1)\n            spot_asoc[spot_position]['asoc'].insert(\n                asoc_position,\n                (random_label, self.null_span)\n            )\n        return spot_asoc\n\n    def add_noise(self, spot_asoc, spot_label_list, asoc_label_list):\n        spot_asoc = self.random_insert_asoc(\n            spot_asoc=spot_asoc,\n            asoc_label_list=asoc_label_list,\n        )\n        spot_asoc = self.random_insert_spot(\n            spot_asoc=spot_asoc,\n            spot_label_list=spot_label_list,\n        )\n        return spot_asoc\n        \n    def add_null(self, spot_list, asoc_list, spot_label_list, asoc_label_list):\n        texts = []\n        for spot in spot_label_list:\n            if spot not in spot_list:\n                texts.append(f\"{constants.type_start} {spot} {constants.span_start} {self.null_span} {constants.type_end}\")\n        for asoc in asoc_label_list:\n            if asoc not in asoc_list:\n                texts.append(f\"{constants.type_start} {asoc} {constants.span_start} {self.null_span} {constants.type_end}\")\n        return \" \".join(texts)\n\n\n\ndef main():\n    from uie.extraction.constants import BaseStructureMarker\n    structure_marker = BaseStructureMarker()\n    spot_asoc = [{\"span\": \"analyzer\", \"label\": \"generic\", \"asoc\": []}, {\"span\": \"`` Amorph ''\", \"label\": \"method\", \"asoc\": []}]\n\n    spot_asoc_noiser = SpotAsocNoiser(\n        spot_noise_ratio=0.5,\n        asoc_noise_ratio=0.5,\n    )\n    spot_asoc_noiser.add_noise(\n        spot_asoc=spot_asoc,\n        spot_label_list=['A', 'B', 'C'],\n        asoc_label_list=['D', 'E', 'F'],\n    )\n    target = convert_spot_asoc(\n        spot_asoc_instance=spot_asoc,\n        structure_maker=structure_marker\n    )\n\n    target = convert_spot_asoc(\n        spot_asoc_instance=spot_asoc,\n        structure_maker=structure_marker\n    )\n\n    replace_map = {\n        '<extra_id_0>': ' ( ',\n        '<extra_id_1>': ' ) ',\n        '<extra_id_5>': ':',\n    }\n    from nltk.tree import Tree\n    for old, new in replace_map.items():\n        target = target.replace(old, new)\n    print(target)\n    Tree.fromstring(target).pretty_print()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/task_format/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.task_format.oneie import OneIEEvent\nfrom universal_ie.task_format.jointer import JointER\nfrom universal_ie.task_format.nerd import NERD\n\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/utils.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List\nimport os\nimport sys\n\n\nglobal_mislabel_log = set()\n\n\ndef tokens_to_str(tokens: List[str], language: str = 'en') -> str:\n    if language == 'en':\n        return ' '.join(tokens)\n    elif language == 'zh':\n        return ''.join(tokens)\n    else:\n        raise NotImplementedError('Language %s not supported' % language)\n        \n\ndef label_format(s):\n    import re\n\n    def uncamelize(s):\n        re_outer = re.compile(r'([^A-Z ])([A-Z])')\n        re_inner = re.compile(r'\\b[A-Z]+(?=[A-Z][a-z])')\n        sub = re_inner.sub(r'\\g<0> ', re_outer.sub(r'\\1 \\2', s)).lower()\n        return sub\n\n    def remove(s):\n        return s.replace(\"_\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n\n    s = remove(uncamelize(s)).split()\n    if len(s) > 1 and s[0] == s[1]:\n        s = s[1:]\n    return \" \".join(s)\n\n\ndef load_dict_ini_file(filename):\n    print(\"Warning: `load_dict_ini_file` is deprecated.\")\n    if not os.path.exists(filename):\n        sys.stderr.write(f'[warning] cannot load label mapper from {filename}\\n')\n        return {}\n    mapper = dict()\n    for line in open(filename):\n        key, value = line.strip().split('=')\n        mapper[key] = label_format(value)\n    return mapper\n\n\ndef change_ptb_token_back(token):\n    \"\"\"将 PTBTokenized 的 Token 转换会原始字符串\n\n    Args:\n        token (str): PTBTokenize 后的 Token 字符串\n\n    Returns:\n        str: 原始 Token 字符串\n    \"\"\"\n    ptb_token_map = {\n        '``': '\"',\n        \"''\": '\"',\n        '-LRB-': '(',\n        '-RRB-': ')',\n        '-LSB-': '[',\n        '-RSB-': ']',\n        '-LCB-': '{',\n        '-RCB-': '}',\n    }\n    for ptb_token, raw_token in ptb_token_map.items():\n        if token == ptb_token:\n            return raw_token\n    return token\n\n\ndef change_name_using_label_mapper(label_name, label_mapper):\n    if label_mapper is None or len(label_mapper) == 0:\n        return label_name\n    if label_name not in label_mapper:\n        print(f\"{label_name} not found in mapper\")\n        global global_mislabel_log\n        if label_name not in global_mislabel_log:\n            global_mislabel_log.add(label_name)\n    return label_mapper.get(label_name, label_name)\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/dataset.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport yaml\nimport os\nfrom typing import Dict, List\n\nfrom universal_ie.utils import label_format\nimport universal_ie.task_format as task_format\n\n\nclass Dataset:\n    def __init__(self, name: str, path: str, data_class: task_format.TaskFormat, split_dict: Dict, language: str, mapper: Dict, delete_list: List, other: Dict = None) -> None:\n        self.name = name\n        self.path = path\n        self.data_class = data_class\n        self.split_dict = split_dict\n        self.language = language\n        self.mapper = mapper\n        self.other = other\n        self.delete_list = delete_list\n\n    def load_dataset(self, logger_name):\n        datasets = {}\n        for split_name, filename in self.split_dict.items():\n            datasets[split_name] = self.data_class.load_from_file(\n                filename=os.path.join(self.path, filename),\n                language=self.language,\n                delete_list=self.delete_list,\n                m=self.mapper,\n                logger_name=logger_name,\n                **self.other,\n            )\n        return datasets\n\n    @staticmethod\n    def load_yaml_file(yaml_file):\n        dataset_config = yaml.load(open(yaml_file), Loader=yaml.FullLoader)\n        if 'mapper' in dataset_config:\n            mapper = dataset_config['mapper']\n            for key in mapper:\n                mapper[key] = label_format(mapper[key])\n        else:\n            print(f\"{dataset_config['name']} without label mapper.\")\n            mapper = None\n\n        return Dataset(\n            name=dataset_config['name'],  \n            path=dataset_config['path'],  \n            data_class=getattr(task_format, dataset_config['data_class']),  \n            split_dict=dataset_config['split'],   \n            language=dataset_config['language'],  \n            mapper=mapper,   \n            delete_list=dataset_config['delete_list'] if dataset_config['delete_list'] is not None else [],\n            other=dataset_config.get('other', {}),\n        )\n"}
{"type": "source_file", "path": "uie/extraction/constants.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nspot_prompt = '<spot>'\nasoc_prompt = '<asoc>'\n\ntype_start = '<extra_id_0>'\ntype_end = '<extra_id_1>'\ntext_start = '<extra_id_2>'\nspan_start = '<extra_id_5>'\nnull_span = '<extra_id_6>'\nnull_label = '<extra_id_7>'\n\nspot_token = '<extra_id_10>'\nasoc_token = '<extra_id_11>'\n\nclass StructureMarker:\n    def __init__(self) -> None:\n        pass\n\n\nclass BaseStructureMarker(StructureMarker):\n    def __init__(self) -> None:\n        super().__init__()\n        self.sent_start = '<extra_id_0>'\n        self.sent_end = '<extra_id_1>'\n        self.record_start = '<extra_id_0>'\n        self.record_end = '<extra_id_1>'\n        self.span_start = '<extra_id_0>'\n        self.span_end = '<extra_id_1>'\n        self.text_start = '<extra_id_2>'\n        self.source_span_start = '<extra_id_3>'\n        self.source_span_end = '<extra_id_4>'\n        self.target_span_start = '<extra_id_5>'\n        self.null_span = '<extra_id_6>'\n        self.null_label = '<extra_id_7>'\n"}
{"type": "source_file", "path": "run_prompt.py", "content": "#!/usr/bin/env python\n# coding=utf-8\nimport logging\nimport os\nimport sys\nimport numpy as np\nfrom datasets import load_dataset\nimport random\n\n\nimport torch\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    HfArgumentParser,\n    default_data_collator,\n    set_seed\n)\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\n\nfrom uie.extraction import constants\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.extraction_metrics import get_extract_metrics\nfrom uie.extraction.noiser.spot_asoc_noiser import SpotAsocNoiser\nfrom uie.extraction.dataset_processer import PrefixGenerator\n\nfrom uie.seq2seq.constrained_seq2seq import ConstraintSeq2SeqTrainingArguments, EMA\nfrom uie.seq2seq.constrained_seq2seq_prompt import (\n    ConstraintSeq2SeqPromptTrainer, \n    ConstraintSeq2SeqPromptSparseTrainer\n)\nfrom uie.seq2seq.data_collator import (\n    PromptForMetaSeq2Seq,\n    PromptSSIGenerator,\n    DynamicSSIGenerator,\n    DataCollatorForMetaSeq2Seq\n)\nfrom uie.seq2seq.features import RecordFeature\nfrom uie.seq2seq.t5_bert_tokenizer import T5BertTokenizer\nfrom uie.seq2seq.trainer_arguments import ModelArguments, DataTrainingArguments, PromptArguments\nfrom uie.seq2seq.models import T5Prompt \n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_negative_samples(l, k):\n    '''\n    prompt中包含现有的spot和asoc(record.schema中存在的), 还预留了一些空间, \n    剩余的这些空间来自于spot和asoc的相似词\n    '''\n    from thefuzz import fuzz\n    from gensim.test.utils import datapath, get_tmpfile\n    from gensim.models import KeyedVectors\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    glove_file = datapath('/zjunlp/ghh/.cache/GloVe/glove.6B.300d.txt')\n    word2vec_glove_file = get_tmpfile(\"glove.6B.300d.word2vec.txt\")\n    glove2word2vec(glove_file, word2vec_glove_file)\n    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n\n    negative_l = []\n    for i in l:\n        try:\n            sim = model.most_similar(i.split()[0])\n        except KeyError:\n            continue\n        cnt = 10\n        for (x, _) in sim:\n            if fuzz.ratio(i, x) < 65:\n                if cnt > 0 and x not in l and x not in negative_l:\n                    negative_l.append(x)\n                    cnt -=1 \n\n    return random.sample(negative_l, k)\n\n\n\ndef seed_torch(seed=42):\n    '''设置随机种子'''\n    seed = int(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n\n\n\ndef main():\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, ConstraintSeq2SeqTrainingArguments, PromptArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, training_args, prompt_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args, prompt_args = parser.parse_args_into_dataclasses()\n\n    '''检查是否继续训练'''\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif last_checkpoint is not None:\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    '''设置logging,既输出到终端,还输出到文件(logging_dir目录下)'''\n    os.makedirs(training_args.logging_dir, exist_ok = True)\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[\n            logging.StreamHandler(sys.stdout), \n            logging.FileHandler(\n                os.path.join(training_args.logging_dir, training_args.output_dir.split('/')[-1]+'.txt'),\n                mode = 'w', encoding = 'utf-8'\n            )\n        ],\n    )\n    logger.setLevel(logging.INFO)\n    logger.info(f\"last_checkpoint: {last_checkpoint}\")\n    logger.info(f\"Options:\\n\\nmodel_args:{model_args}\\n\\ndata_args:{data_args}\\n\\ntraining_args:{training_args}\\n\\nprompt_args:{prompt_args}\")\n\n\n    # Log on each process the small summary:\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\", distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n    logger.info(\"Training/evaluation parameters %s\", training_args)\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n    seed_torch(training_args.seed)\n\n\n    '''加载数据集, json格式, uie_json.py数据集加载脚本(来自于UIE)'''\n    if data_args.dataset_name is not None:\n        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\n    else:\n        data_files = {}\n        if data_args.train_file is not None:\n            data_files[\"train\"] = data_args.train_file\n        if data_args.validation_file is not None:\n            data_files[\"validation\"] = data_args.validation_file\n        if data_args.test_file is not None:\n            data_files[\"test\"] = data_args.test_file\n    datasets = load_dataset(\"uie_json.py\", data_files=data_files)\n    \n    logger.info(datasets)   \n    '''\n    ACE2005_Event\n    {\n        \"text\": \"She would be the first foreign woman to die in the wave of kidnappings in Iraq .\", \n        \"tokens\": [\"She\", \"would\", \"be\", \"the\", \"first\", \"foreign\", \"woman\", \"to\", \"die\", \"in\", \"the\", \"wave\", \"of\", \"kidnappings\", \"in\", \"Iraq\", \".\"], \n        \"record\": \"<extra_id_0> <extra_id_0> die <extra_id_5> die <extra_id_0> victim <extra_id_5> woman <extra_id_1> <extra_id_0> place <extra_id_5> Iraq <extra_id_1> <extra_id_1> <extra_id_1>\", \n        \"entity\": [], \n        \"relation\": [], \n        \"event\": [\n            {\"type\": \"die\", \"offset\": [8], \"text\": \"die\", \"args\": [{\"type\": \"victim\", \"offset\": [6], \"text\": \"woman\"}, \n            {\"type\": \"place\", \"offset\": [15], \"text\": \"Iraq\"}]}\n        ], \n        \"spot\": [\"die\"], \n        \"asoc\": [\"victim\", \"place\"], \n        \"spot_asoc\": [\n            {\"span\": \"die\", \"label\": \"die\", \"asoc\": [[\"victim\", \"woman\"], [\"place\", \"Iraq\"]]}\n        ]\n    }\n    NYT\n    {\n        \"text\": \"Should Turkey face eastward , toward its Muslim neighbors , or westward , toward Europe ?\", \n        \"tokens\": [\"Should\", \"Turkey\", \"face\", \"eastward\", \",\", \"toward\", \"its\", \"Muslim\", \"neighbors\", \",\", \"or\", \"westward\", \",\", \"toward\", \"Europe\", \"?\"], \n        \"record\": \"<extra_id_0> <extra_id_0> location <extra_id_5> Turkey <extra_id_1> <extra_id_0> location <extra_id_5> Europe <extra_id_0> contains <extra_id_5> Turkey <extra_id_1> <extra_id_1> <extra_id_1>\", \n        \"entity\": [\n            {\"type\": \"location\", \"offset\": [14], \"text\": \"Europe\"}, \n            {\"type\": \"location\", \"offset\": [1], \"text\": \"Turkey\"}\n        ], \n        \"relation\": [\n            {\"type\": \"contains\", \"args\": [{\"type\": \"location\", \"offset\": [14], \"text\": \"Europe\"}, \n            {\"type\": \"location\", \"offset\": [1], \"text\": \"Turkey\"}]}\n        ], \n        \"event\": [], \n        \"spot\": [\"location\"], \n        \"asoc\": [\"contains\"], \n        \"spot_asoc\": [\n            {\"span\": \"Turkey\", \"label\": \"location\", \"asoc\": []}, \n            {\"span\": \"Europe\", \"label\": \"location\", \"asoc\": [[\"contains\", \"Turkey\"]]}\n        ]\n    }\n    Few-NERD\n    {\n        \"text\": \"Now Multan is the name of the city in Pakistan .\", \n        \"tokens\": [\"Now\", \"Multan\", \"is\", \"the\", \"name\", \"of\", \"the\", \"city\", \"in\", \"Pakistan\", \".\"], \n        \"record\": \"<extra_id_0> <extra_id_0> geographical social political <extra_id_5> Multan <extra_id_1> <extra_id_0> geographical social political <extra_id_5> Pakistan <extra_id_1> <extra_id_1>\", \n        \"entity\": [\n            {\"type\": \"geographical social political\", \"offset\": [9], \"text\": \"Pakistan\"}, \n            {\"type\": \"geographical social political\", \"offset\": [1], \"text\": \"Multan\"}\n        ], \n        \"relation\": [], \n        \"event\": [], \n        \"spot\": [\"geographical social political\"], \n        \"asoc\": [], \n        \"spot_asoc\": [\n            {\"span\": \"Multan\", \"label\": \"geographical social political\", \"asoc\": []}, \n            {\"span\": \"Pakistan\", \"label\": \"geographical social political\", \"asoc\": []}\n        ]\n    }\n    '''\n\n    ''' 加载config'''\n    config = AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n    )\n    config.max_length = data_args.max_target_length\n\n\n    '''加载tokenizer'''\n    if 'char' in model_args.model_name_or_path:\n        tokenizer = T5BertTokenizer.from_pretrained(model_args.model_name_or_path)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            cache_dir=model_args.cache_dir,\n            use_fast=model_args.use_fast_tokenizer,\n            revision=model_args.model_revision,\n            use_auth_token=True if model_args.use_auth_token else None,\n        )\n\n    '''需要移除的token, 在postprocess_text中有用到'''\n    to_remove_token_list = list()     \n    if tokenizer.bos_token:\n        to_remove_token_list += [tokenizer.bos_token]\n    if tokenizer.eos_token:\n        to_remove_token_list += [tokenizer.eos_token]\n    if tokenizer.pad_token:\n        to_remove_token_list += [tokenizer.pad_token]\n    logger.info(f\"Padding side: {tokenizer.padding_side}\\n\\nTokenizer Length: {len(tokenizer)}\\n\\ntokenizer.bos_token: {tokenizer.bos_token}, tokenizer.bos_token_id: {tokenizer.bos_token_id}\\n\\ntokenizer.eos_token: {tokenizer.eos_token}, tokenizer.eos_token_id: {tokenizer.eos_token_id}\\n\\ntokenizer.pad_token: {tokenizer.pad_token}, tokenizer.pad_token_id: {tokenizer.pad_token_id}\")\n\n\n    '''增加特殊token, 即UIE中提到的spot, asoc等'''\n    if training_args.do_train:\n        to_add_special_token = list()\n        for special_token in [constants.type_start, constants.type_end, constants.text_start, constants.span_start, constants.spot_prompt, constants.asoc_prompt]:\n            if special_token not in tokenizer.get_vocab():\n                to_add_special_token += [special_token]\n        tokenizer.add_special_tokens({\"additional_special_tokens\": tokenizer.special_tokens_map_extended['additional_special_tokens'] + to_add_special_token})\n\n\n    '''加载模型'''\n    model = T5Prompt(           \n        tokenizer,\n        model_args.model_name_or_path,\n        prompt_args,\n    )\n    logger.info(f\"Tokenizer Length: {len(tokenizer)}\")   \n    ema = None\n    if training_args.use_ema:\n        ema = EMA(model, 0.99, training_args.device)\n        ema.register()\n\n    '''\n    只需要关注record_schema即可, 一般来说第一行是spot(实体、事件类型), 第二行是asoc(关系、论元角色)、第三行是spot与asco映射(一般只有事件抽取用到)\n    ACE2005_Event\n    [\"attack\", \"start position\", \"transfer ownership\", \"be born\", \"sentence\", \"die\", \"arrest jail\", \"transport\", \"elect\", \"phone write\", \"end organization\", \"sue\", \"acquit\", \"marry\", \"extradite\"]\n    [\"destination\", \"victim\", \"seller\", \"plaintiff\", \"beneficiary\", \"organization\", \"agent\", \"person\", \"attacker\", \"origin\", \"buyer\", \"vehicle\", \"target\", \"entity\", \"instrument\", \"adjudicator\", \"artifact\", \"place\", \"defendant\"]\n    {\"attack\": [\"target\", \"instrument\", \"place\", \"victim\", \"agent\", \"attacker\"], \"start position\": [\"person\", \"entity\", \"place\"], \"transfer ownership\": [\"artifact\", \"place\", \"seller\", \"beneficiary\", \"buyer\"], \"be born\": [\"person\", \"place\"], \"sentence\": [\"place\", \"adjudicator\", \"defendant\"], \"die\": [\"instrument\", \"place\", \"victim\", \"agent\", \"person\"], \"arrest jail\": [\"person\", \"agent\", \"place\"], \"transport\": [\"destination\", \"artifact\", \"place\", \"victim\", \"agent\", \"origin\", \"vehicle\"], \"elect\": [\"person\", \"entity\", \"place\"], \"phone write\": [\"entity\", \"place\"], \"end organization\": [\"organization\", \"place\"], \"sue\": [\"plaintiff\", \"adjudicator\", \"place\", \"defendant\"], \"acquit\": [\"adjudicator\", \"defendant\"], \"marry\": [\"person\", \"place\"], \"extradite\": [\"person\", \"origin\", \"agent\", \"destination\"]}\n    NYT\n    [\"location\", \"organization\", \"person\"]\n    [\"place of death\", \"industry\", \"profession\", \"contains\", \"place founded\", \"people\", \"advisors\", \"major shareholder of\", \"children\", \"teams\"]\n    {\"location\": [\"place of death\", \"contains\", \"place founded\", \"people\", \"major shareholder of\", \"teams\"], \"organization\": [\"place of death\", \"industry\", \"contains\", \"place founded\", \"people\", \"advisors\", \"children\", \"teams\"], \"person\": [\"place of death\", \"profession\", \"contains\", \"place founded\", \"people\", \"major shareholder of\", \"children\"]}\n    Few-NERD\n    [\"person other\", \"writtenart\", \"director\", \"protest\", \"geographical social political\", \"weapon\", \"scholar\", \"event other\", \"language\", \"film\", \"law\", \"road\", \"soldier\", \"education\", \"library\", \"astronomything\", \"hotel\", \"game\", \"award\", \"theater\", \"disease\", \"election\", \"currency\", \"ship\", \"livingthing\", \"art other\", \"disaster\", \"medical\", \"park\", \"train\"]\n    []\n    {\"person other\": [], \"writtenart\": [], \"director\": [], \"protest\": [], \"geographical social political\": [], \"weapon\": [], \"scholar\": [], \"event other\": [], \"language\": [], \"film\": [], \"law\": [], \"road\": [], \"soldier\": [], \"education\": [], \"library\": [], \"astronomything\": [], \"hotel\": [], \"game\": [], \"award\": [], \"theater\": [], \"disease\": [], \"election\": [], \"currency\": [], \"ship\": [], \"livingthing\": [], \"art other\": [], \"disaster\": [], \"medical\": [], \"park\": [], \"train\": []}\n    '''\n    if data_args.record_schema and os.path.exists(data_args.record_schema):\n        record_schema = RecordSchema.read_from_file(data_args.record_schema)\n    else:\n        record_schema = None\n\n\n    negative_sample = []\n    '''初始化prompt的值'''\n    if prompt_args.init_prompt:\n        logger.info(f\"init_prompt? {prompt_args.init_prompt}\")\n        '''spot_prompt、asoc_prompt是prompt中的分隔符'''\n        spot_prompt_id = tokenizer.encode(constants.spot_prompt, add_special_tokens = False)\n        asoc_prompt_id = tokenizer.encode(constants.asoc_prompt, add_special_tokens = False)\n\n        negative_file = os.path.join('/'.join(data_args.train_file.split('/')[:-1]), 'negative.pt')\n        if os.path.exists(negative_file):\n            logger.info(f\"Load from {negative_file}\")\n            ng = torch.load(negative_file)\n            negative_sample = ng[\"negative_sample\"]\n            negative_sample_ids = ng[\"negative_sample_ids\"]\n            spot_ids = ng[\"spot_ids\"]\n            asoc_ids = ng[\"asoc_ids\"]\n        else:\n            spot_ids = []\n            asoc_ids = []\n            '''选用最后一个迭代的所有spot、asoc的ids, 用于prompt的初始值(仍然有剩余空间, 即neg_len)'''\n            record_schema2 = RecordSchema.read_from_file(prompt_args.record2)\n            for spot in record_schema2.type_list:\n                spot_ids.append(tokenizer.encode(spot, add_special_tokens = False))     \n            for asoc in record_schema2.role_list:\n                asoc_ids.append(tokenizer.encode(asoc, add_special_tokens = False))\n            '''get_negative_samples(选用spot、asoc的近义词)获得剩余空间的prompt'''\n            neg_len = prompt_args.prompt_len - len(record_schema2.type_list) - len(record_schema2.role_list) - 5\n            if data_args.task_name == 'relation':\n                negative_sample = get_negative_samples(record_schema2.role_list, neg_len)\n            else:\n                negative_sample = get_negative_samples(record_schema2.type_list, neg_len)\n            negative_sample_ids = []\n            for it in negative_sample:\n                negative_sample_ids.append(tokenizer.encode(it, add_special_tokens = False))\n            ng = {\n                \"negative_sample\": negative_sample, \n                \"negative_sample_ids\": negative_sample_ids, \n                \"spot_ids\": spot_ids, \n                \"asoc_ids\": asoc_ids\n            }\n            torch.save(ng, negative_file)\n            logger.info(f\"Save to {negative_file}\")\n\n        logger.info(f\"spot_ids: {spot_ids}\\n\\nasoc_ids: {asoc_ids}\\n\\ndata_args.task_name: {data_args.task_name}\\n\\nnegative_sample: {negative_sample}\\n\\nnegative_sample_ids: {negative_sample_ids}\")\n        '''\n        spot_ids: [[414, 1102], [3211], [456, 1102], [5252, 8660], [1567, 16, 12194], [2025, 540], [2025, 7915], [1576, 17782], [36, 2170], [7142], [67], [5970], [10319, 11796], [1855], [11924], [456, 1470], [942], [16, 10609, 15], [951, 1431], [7986, 1470], [15884, 14160], [3689, 3507], [1399], [22664, 75, 17], [414, 1470], [2629, 15], [7759], [3, 9, 75, 10073], [3958], [20111], [12133], [996, 10700], [260, 2029]]\n        asoc_ids: [[16877], [27483], [3954], [3102], [10409], [768, 23, 8717], [428, 52], [1470], [2387], [286], [9123], [8001], [7584], [19181, 76, 4370, 1016], [568], [1689], [23489, 127], [11095], [11819], [5009], [22600], [5233]]\n        data_args.task_name: event\n        negative_sample: ['lawsuits', 'charges', '.', 'daughters', 'dying', 'could', 'been', 'responsible', 'husband', 'designate', 'remarry', 'proclaimed', 'going', 'art', 'embarrass', 'next', 'last', 'began', 'being', 'if']\n        negative_sample_ids: [[9953, 7], [3991], [3, 5], [16649], [13677], [228], [118], [1966], [2553], [408, 342], [3, 60, 1635, 651], [3, 28901], [352], [768], [10960, 10116, 7, 7], [416], [336], [1553], [271], [3, 99]]\n        '''\n        \n        '''初始化prompt值'''\n        model.init_prompt(spot_ids, asoc_ids, negative_sample_ids, spot_prompt_id, asoc_prompt_id, [tokenizer.pad_token_id])\n\n\n    '''默认prefix是空, 可以添加数据集来源'''\n    if data_args.source_prefix is not None:\n        if data_args.source_prefix == 'schema':\n            prefix = PrefixGenerator.get_schema_prefix(schema=record_schema)\n        elif data_args.source_prefix.startswith('meta'):\n            prefix = \"\"\n        else:\n            prefix = data_args.source_prefix\n    else:\n        prefix = \"\"\n    logger.info(f\"Prefix: {prefix}\\n\\nPrefix Length: {len(tokenizer.tokenize(prefix))}\")\n\n\n    if training_args.do_train:\n        column_names = datasets[\"train\"].column_names\n    elif training_args.do_eval:\n        column_names = datasets[\"validation\"].column_names\n    elif training_args.do_predict:\n        column_names = datasets[\"test\"].column_names\n    else:\n        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n        return\n\n\n    text_column = data_args.text_column\n    record_column = data_args.record_column\n    logger.info('Using src: %s and tgt: %s' % (text_column, record_column)) # Using src: text and tgt: record\n    # Temporarily set max_target_length for training.\n    max_target_length = data_args.max_target_length\n    padding = \"max_length\" if data_args.pad_to_max_length else False\n\n    if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n        logger.error(\n            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n        )\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[record_column]\n        inputs = [prefix + inp for inp in inputs]\n        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n\n        # Setup the tokenizer for targets\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n\n        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n        # padding in the loss.\n        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n            labels[\"input_ids\"] = [\n                [(_label if _label != tokenizer.pad_token_id else -100) for _label in label] for label in labels[\"input_ids\"]\n            ]\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n        model_inputs['sample_prompt'] = [False] * len(model_inputs['input_ids'])\n        if data_args.source_prefix is not None and data_args.source_prefix.startswith('meta'):\n            model_inputs['spots'] = examples['spot']\n            model_inputs['asocs'] = examples['asoc']\n            model_inputs['spot_asoc'] = examples['spot_asoc']\n            # sample_prompt=True for Finetune and Pretrain\n            model_inputs['sample_prompt'] = [True] * len(model_inputs['input_ids'])\n        return model_inputs\n\n\n    def preprocess_function_eval(examples):\n        model_inputs = preprocess_function(examples)\n        # sample_prompt=False for evaluation\n        model_inputs['sample_prompt'] = [False] * len(model_inputs['input_ids'])\n        return model_inputs\n    \n\n    def postprocess_text(x_str):\n        # Clean `bos` `eos` `pad` for cleaned text\n        for to_remove_token in to_remove_token_list:\n            x_str = x_str.replace(to_remove_token, '')\n        return x_str.strip()\n\n\n    logger.info(\"Start Data Preprocessing ...\")\n    if training_args.do_train:\n        train_dataset = datasets[\"train\"]\n        if data_args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n        train_dataset = train_dataset.map(\n            preprocess_function,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    if training_args.do_eval:\n        max_target_length = data_args.val_max_target_length\n        eval_dataset = datasets[\"validation\"]\n        if data_args.max_val_samples is not None:\n            eval_dataset = eval_dataset.select(range(data_args.max_val_samples))\n        eval_dataset = eval_dataset.map(\n            preprocess_function_eval,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    if training_args.do_predict:\n        max_target_length = data_args.val_max_target_length\n        test_dataset = datasets[\"test\"]\n        if data_args.max_test_samples is not None:\n            test_dataset = test_dataset.select(range(data_args.max_test_samples))\n        test_dataset = test_dataset.map(\n            preprocess_function_eval,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not data_args.overwrite_cache,\n            features=RecordFeature,\n        )\n\n    logger.info(\"End Data Preprocessing ...\")\n\n    # Data collator\n    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n    if data_args.pad_to_max_length:     # If False, will pad the samples dynamically when batching to the maximum length in the batch\n        data_collator = default_data_collator\n    elif data_args.source_prefix.startswith('meta'):\n        if data_args.spot_noise > 0 or data_args.asoc_noise > 0:\n            if data_args.decoding_format == 'spotasoc':\n                spot_asoc_nosier = SpotAsocNoiser(\n                    spot_noise_ratio=data_args.spot_noise,\n                    asoc_noise_ratio=data_args.asoc_noise,\n                    null_span=constants.null_span,\n                )\n                '''\n                这是负采样器, 可以看到原始数据中的spot、asoc只包含标注(entity、relation、event)中存在的schema, 这些称为正样本\n                其他存在于record.schema中但不存在标注中的称为负样本\n                负采样会从负样本中采样一部分标注中不存在的schema加入到spot、asoc\n\n                \"spot\": [\"geographical social political\"], \n                \"spot_asoc\": [\n                    {\"span\": \"Multan\", \"label\": \"geographical social political\", \"asoc\": []}, \n                    {\"span\": \"Pakistan\", \"label\": \"geographical social political\", \"asoc\": []}\n                ]\n                [\"person other\", \"writtenart\", \"director\", \"protest\", \"geographical social political\", \"weapon\", \"scholar\", \"event other\", \"language\", \"film\", \"law\", \"road\", \"soldier\", \"education\", \"library\", \"astronomything\", \"hotel\", \"game\", \"award\", \"theater\", \"disease\", \"election\", \"currency\", \"ship\", \"livingthing\", \"art other\", \"disaster\", \"medical\", \"park\", \"train\"]\n                上面只有\"geographical social political\"一个schema是正样本, 其他都是负样本,\n                采样两个负样本\"person other\", \"writtenart\"\n                最终得到\"spot\": [\"geographical social political\", \"person other\", \"writtenart\"], \n                '''\n            else:\n                raise NotImplementedError(\n                    f\"decoding_format {data_args.decoding_format} is not implemented.\"\n                )\n        else:\n            spot_asoc_nosier = None\n\n        '''\n        spot_negative、asoc_negative分别是spot、asoc负采样的数量, -1表示负样本都添加, 由negative_ratio参数控制比例\n        '''\n        if data_args.task_name == 'relation':\n            spot_negative = data_args.meta_negative\n            asoc_negative = int(len(record_schema.role_list) * data_args.negative_ratio)\n        else:\n            spot_negative = int(len(record_schema.type_list) * data_args.negative_ratio) \n            asoc_negative = data_args.meta_negative\n        logger.info(f\"len(record_schema.type_list): {len(record_schema.type_list)}\")\n        logger.info(f\"len(record_schema.role_list): {len(record_schema.role_list)}\")   \n        logger.info(f\"data_args.negative_ratio: {data_args.negative_ratio}\")\n        logger.info(f\"data_args.meta_negative: {data_args.meta_negative}\")\n        logger.info(f\"task name: {data_args.task_name}\")   \n        logger.info(f\"spot_negative: {spot_negative}\")\n        logger.info(f\"asoc_negative: {asoc_negative}\")\n        '''\n        len(record_schema.type_list): 15\n        len(record_schema.role_list): 19\n        data_args.negative_ratio: 0.8\n        data_args.meta_negative: -1\n        task name: event\n        spot_negative: 12\n        asoc_negative: -1\n        '''\n\n        data_collator_class = PromptForMetaSeq2Seq if prompt_args.use_prompt else DataCollatorForMetaSeq2Seq\n        negative_sampler_class = PromptSSIGenerator if prompt_args.use_prompt else DynamicSSIGenerator\n        '''具体负采样会用到的数据处理器'''\n        data_collator = data_collator_class(\n            tokenizer,\n            model=model,\n            label_pad_token_id=label_pad_token_id,\n            pad_to_multiple_of=8 if training_args.fp16 else None,\n            max_length=data_args.max_source_length,\n            max_prefix_length=data_args.max_prefix_length,\n            max_target_length=data_args.max_target_length,\n            negative_sampler=negative_sampler_class(\n                tokenizer=tokenizer,\n                schema=record_schema,\n                negative_list=negative_sample,      # 上面的负样本\n                positive_rate=data_args.meta_positive_rate,\n                spot_negative=spot_negative,\n                asoc_negative=asoc_negative,\n                other_ratio=data_args.other_ratio,\n                ordered_prompt=data_args.ordered_prompt,\n                task_name=data_args.task_name, \n            ),\n            spot_asoc_nosier=spot_asoc_nosier,\n            decoding_format=data_args.decoding_format,\n            use_ssi=prompt_args.use_ssi,\n        )\n\n\n    '''在验证集上评估模型输出的F1指标, 通过F1选择更好的模型'''\n    def compute_metrics(eval_preds):\n        preds, labels = eval_preds\n        if isinstance(preds, tuple):\n            preds = preds[0]\n            \n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n        if data_args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n        decoded_preds = [postprocess_text(x) for x in decoded_preds]\n        decoded_labels = [postprocess_text(x) for x in decoded_labels]\n\n        result = get_extract_metrics(\n            pred_lns=decoded_preds,\n            tgt_lns=decoded_labels,\n            label_constraint=record_schema,\n            decoding_format=data_args.decoding_format,\n        )\n\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n        result = {k: round(v, 4) for k, v in result.items()}\n        return result\n\n\n    logger.info(f\"use_ssi? {prompt_args.use_ssi}\")\n    logger.info(f\"use_sparsemax? {training_args.use_sparsemax}\")\n    logger.info(f\"use_ema? {training_args.use_ema}\")\n    logger.info(f\"ema? {ema}\")\n    train_dict = {\"ConstraintSeq2SeqPromptTrainer\": ConstraintSeq2SeqPromptTrainer, \"ConstraintSeq2SeqPromptSparseTrainer\": ConstraintSeq2SeqPromptSparseTrainer}\n    s_sparsemax = \"ConstraintSeq2SeqPromptTrainer\"\n    if training_args.use_sparsemax:\n        s_sparsemax = \"ConstraintSeq2SeqPromptSparseTrainer\"\n    trainer = train_dict[s_sparsemax](\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset if training_args.do_train else None,\n        eval_dataset=eval_dataset if training_args.do_eval else None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n        ema=ema,\n        decoding_type_schema=record_schema,\n        decoding_format=data_args.decoding_format,\n        source_prefix=prefix,\n        task=data_args.task_name,\n    )\n    \n    # Training\n    checkpoint = None\n    if training_args.do_train:\n        if model_args.from_checkpoint:\n            if last_checkpoint is not None:\n                checkpoint = last_checkpoint\n\n        logger.info(f\"checkpoint: {checkpoint}\")\n        print(tokenizer.bos_token_id)\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model()  # Saves the tokenizer too for easy upload\n        # load_best_model_at_end=True，会在训练结束后加载最佳模型，然后save_model到output_dir\n\n        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_train_file, \"w\") as writer:\n                logger.info(\"***** Train results *****\")\n                for key, value in sorted(train_result.metrics.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n\n    # Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n\n        results = trainer.evaluate()\n        results = {k: round(v, 4) for k, v in results.items()}    # 写到 \"eval_results_seq2seq.txt\"\n\n        eval_results = trainer.predict(\n            eval_dataset,\n            metric_key_prefix=\"eval\",\n            max_length=data_args.val_max_target_length,\n            num_beams=data_args.num_beams,\n        )        # 写到 \"eval_preds_seq2seq.txt\"\n\n        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_seq2seq.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_eval_file, \"w\") as writer:\n                logger.info(\"***** Eval results *****\")\n                for key, value in sorted(results.items()):\n                    logger.info(f\"  {key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            if training_args.predict_with_generate:\n                eval_preds = tokenizer.batch_decode(\n                    eval_results.predictions, skip_special_tokens=False, clean_up_tokenization_spaces=False\n                )\n                eval_preds = [postprocess_text(pred) for pred in eval_preds]\n                output_test_preds_file = os.path.join(training_args.output_dir, \"eval_preds_seq2seq.txt\")   \n                # 只生成了preds_seq2seq.txt，没有生成preds_record.txt\n                with open(output_test_preds_file, \"w\") as writer:\n                    writer.write(\"\\n\".join(eval_preds))\n\n    if training_args.do_predict:\n        logger.info(\"*** Test ***\")\n\n        test_results = trainer.predict(\n            test_dataset,\n            metric_key_prefix=\"test\",\n            max_length=data_args.val_max_target_length,\n            num_beams=data_args.num_beams,\n        )\n        test_metrics = test_results.metrics\n        test_metrics[\"test_loss\"] = round(test_metrics[\"test_loss\"], 4)\n\n        output_test_result_file = os.path.join(training_args.output_dir, \"test_results_seq2seq.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_test_result_file, \"w\") as writer:\n                logger.info(\"***** Test results *****\")\n                for key, value in sorted(test_metrics.items()):\n                    logger.info(f\"{key} = {value}\")\n                    writer.write(f\"{key} = {value}\\n\")\n\n            if training_args.predict_with_generate:\n                test_preds = tokenizer.batch_decode(\n                    test_results.predictions, skip_special_tokens=False, clean_up_tokenization_spaces=False\n                )\n                test_preds = [postprocess_text(pred) for pred in test_preds]\n                output_test_preds_file = os.path.join(training_args.output_dir, \"test_preds_seq2seq.txt\")\n                with open(output_test_preds_file, \"w\") as writer:\n                    writer.write(\"\\n\".join(test_preds))\n\n    return results\n  \n\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "inference_mul.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport math\nimport os\nimport argparse\nimport logging\nimport json\nimport time\nimport re\nfrom tqdm import tqdm\nimport sys\n\nimport torch\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration\n\n\nfrom uie.seq2seq.constraint_decoder import get_constraint_decoder\nfrom uie.seq2seq.models import T5Prompt\n\nfrom uie.sel2record.record import MapConfig\nfrom uie.sel2record.sel2record import SEL2Record\n\nfrom uie.extraction.scorer import *\nfrom uie.extraction.record_schema import RecordSchema\n\n\nlogger = logging.getLogger(__name__)\n\nsplit_bracket = re.compile(r\"\\s*<extra_id_\\d>\\s*\")\nspecial_to_remove = {'<pad>', '</s>'}\ncwd = os.getcwd()\n\n\ndef read_json_file(file_name):\n    return [json.loads(line) for line in open(file_name)]\n\n\ndef schema_to_ssi(schema: RecordSchema):\n    ssi = \"<spot> \" + \" <spot> \".join(sorted(schema.type_list))\n    ssi += \"<asoc> \" + \" <asoc> \".join(sorted(schema.role_list))\n    ssi += \" <extra_id_2> \"\n    return ssi\n\n\ndef post_processing(x):\n    for special in special_to_remove:\n        x = x.replace(special, '')\n    return x.strip()\n\n\ndef schema_to_spotasoc(schema: RecordSchema, tokenizer):\n    spots = []\n    asocs = []\n    for spot in sorted(schema.type_list):\n        spots.append(tokenizer.encode(spot, add_special_tokens = False))\n    for asoc in sorted(schema.role_list):\n        asocs.append(tokenizer.encode(asoc, add_special_tokens = False))\n    return spots, asocs\n\n\n\nclass HuggingfacePromptPredictor:\n    def __init__(self, decoding_format = 'spotasoc', source_prefix = '', args = None) -> None:\n        self._tokenizer = T5TokenizerFast.from_pretrained(args.model)\n        logger.info(f\"Tokenizer Length: {len(self._tokenizer)}\")\n        self._device = f\"cuda:{args.cuda}\" if torch.cuda.is_available() else \"cpu\"\n        logger.info(f\"Device: {self._device}\")\n        self._model = T5Prompt(self._tokenizer, args.t5_path, args).to(self._device)\n        self._model.load_state_dict(torch.load(os.path.join(args.model, 'pytorch_model.bin'), map_location=self._device))\n        self._model.eval()\n\n        self._max_source_length = args.max_source_length\n        self._max_target_length = args.max_target_length\n        self._use_ssi = args.use_ssi\n        self._args = {\"num_beams\": args.num_beams, \"do_sample\": args.do_sample, \"top_k\": args.top_k, \"top_p\": args.top_p}\n        self.task_name = args.task\n\n\n    def load_schema(self, record_file, CD):\n        logger.info(f\"record_file: {record_file}\")\n        self._schema = RecordSchema.read_from_file(record_file) \n        spots, asocs = schema_to_spotasoc(self._schema, self._tokenizer)\n        self._ssi = schema_to_ssi(self._schema)\n        self._spots = spots\n        self._asocs = asocs\n        logger.info(f\"ssi: {self._ssi}\")\n        logger.info(f\"spots: {self._spots}\")\n        logger.info(f\"asocs: {self._asocs}\")\n        if CD:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self._tokenizer,\n                                                             type_schema = self._schema,\n                                                             decoding_schema = 'spotasoc',\n                                                             source_prefix = '',\n                                                             task_name = self.task_name)\n        else:\n            self.constraint_decoder = None\n            \n\n    def predict(self, text):\n        func = None\n        def CD_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n        if self.constraint_decoder is not None:\n            func = CD_fn\n        \n        if self._use_ssi:\n            text = [self._ssi + x for x in text]  \n        inputs = self._tokenizer(text, padding=True, return_tensors='pt').to(self._device)\n        inputs['input_ids'] = inputs['input_ids'][:, :self._max_source_length]\n        inputs['attention_mask'] = inputs['attention_mask'][:, :self._max_source_length] \n\n        result = self._model.generate(\n            input_ids=inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            spot=[self._spots] * inputs[\"input_ids\"].size(0),\n            asoc=[self._asocs] * inputs[\"input_ids\"].size(0),\n            prefix_allowed_tokens_fn=func,\n            **self._args\n        )\n\n        return self._tokenizer.batch_decode(result, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n\n\nclass HuggingfacePredictor:\n    def __init__(self, decoding_format = 'spotasoc', source_prefix = '', args = None) -> None:\n        self._tokenizer = T5TokenizerFast.from_pretrained(args.model)\n        self._model = T5ForConditionalGeneration.from_pretrained(args.model)\n        self._model.cuda(f\"cuda:{args.cuda}\")\n                 \n        self._max_source_length = args.max_source_length\n        self._max_target_length = args.max_target_length\n        self._args = {\"num_beams\": args.num_beams, \"do_sample\": args.do_sample, \"top_k\": args.top_k, \"top_p\": args.top_p}\n        self.task_name = args.task\n\n\n    def load_schema(self, record_file, CD): \n        logger.info(f\"record_file: {record_file}\")  \n        self._schema = RecordSchema.read_from_file(record_file) \n        self._ssi = schema_to_ssi(self._schema)\n        logger.info(f\"ssi: {self._ssi}\")\n        if CD:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self._tokenizer,\n                                                             type_schema = self._schema,\n                                                             decoding_schema = 'spotasoc',\n                                                             source_prefix = '',\n                                                             task_name = self.task_name)\n        else:\n            self.constraint_decoder = None\n\n\n    def predict(self, text):\n        func = None\n        def CD_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n        if self.constraint_decoder is not None:\n            func = CD_fn\n\n        text = [self._ssi + x for x in text]          # SSI作前缀\n        inputs = self._tokenizer(text, padding=True, return_tensors='pt').to(self._model.device)\n        inputs['input_ids'] = inputs['input_ids'][:, :self._max_source_length]\n        inputs['attention_mask'] = inputs['attention_mask'][:, :self._max_source_length] \n\n        result = self._model.generate(\n            input_ids=inputs['input_ids'],\n            prefix_allowed_tokens_fn=func,\n            attention_mask=inputs['attention_mask'],\n            max_length=self._max_target_length,\n            **self._args\n        )\n        \n        return self._tokenizer.batch_decode(result, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n\n\n\ntask_dict = {\n    'entity': EntityScorer,\n    'relation': RelationScorer,\n    'event': EventScorer,\n}\n\n\n\ndef do_predict(predictor, output_dir, split_name, batch_num, options, text_list):\n    predicts = list()\n    if os.path.exists(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt')):\n        with open(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt'), 'r') as reader:\n            for line in reader:\n                predicts.append(line.strip())\n        return predicts\n    \n    for index in tqdm(range(batch_num)):\n        start = index * options.batch_size\n        end = index * options.batch_size + options.batch_size\n\n        pred_seq2seq = predictor.predict(text_list[start: end])\n        pred_seq2seq = [post_processing(x) for x in pred_seq2seq]\n\n        predicts += pred_seq2seq\n\n    with open(os.path.join(output_dir, f'{split_name}_preds_seq2seq.txt'), 'w') as output:\n        for pred in predicts:\n            output.write(f'{pred}\\n')\n\n    return predicts\n\n\ndef do_sel2record(predicts, sel2record, text_list, token_list, output_dir, split_name):\n    records = list()\n    if os.path.exists(os.path.join(output_dir, f'{split_name}_preds_record.txt')):\n        with open(os.path.join(output_dir, f'{split_name}_preds_record.txt'), 'r') as reader:\n            for line in reader:\n                records.append(json.loads(line.strip()))\n        return records\n\n    for p, text, tokens in zip(predicts, text_list, token_list):\n        r = sel2record.sel2record(pred=p, text=text, tokens=tokens)\n        records += [r]\n\n    with open(os.path.join(output_dir, f'{split_name}_preds_record.txt'), 'w') as output:\n        for record in records:\n            output.write(f'{json.dumps(record)}\\n')\n    \n    return records\n\n\n \ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataname', default='relation/NYT')\n    parser.add_argument('--model', default='hf_models/mix')\n    parser.add_argument('--task', default='relation')\n    parser.add_argument('--iter_num', default=7, type=int)\n    parser.add_argument('--cuda', default='0')\n    parser.add_argument('--t5_path', default='hf_models/mix', type=str)\n\n    parser.add_argument('--max_source_length', default=256, type=int)\n    parser.add_argument('--max_target_length', default=192, type=int)\n    parser.add_argument('--batch_size', default=256, type=int)\n    parser.add_argument('--config', dest='map_config', help='Offset Re-mapping Config', default='config/offset_map/closest_offset_en.yaml')\n    parser.add_argument('--decoding', default='spotasoc')\n    parser.add_argument('--verbose', action='store_true')\n    parser.add_argument('--match_mode', default='set', choices=['set', 'normal', 'multimatch'])\n    parser.add_argument('--mode', default='H')\n\n    parser.add_argument('--CD', action='store_true')\n    parser.add_argument('--use_prompt', action='store_true')\n    parser.add_argument('--use_prefix', action='store_true')\n    parser.add_argument('--use_ssi', action='store_true')\n    parser.add_argument('--prompt_len', default=10, type=int)\n    parser.add_argument('--prompt_dim', default=512, type=int)\n\n    parser.add_argument('--do_sample', action='store_true')\n    parser.add_argument('--num_beams', default=None, type=int)\n    parser.add_argument('--top_k', default=None, type=int)\n    parser.add_argument('--top_p', default=None, type=float)\n    \n    options = parser.parse_args()\n    map_config = MapConfig.load_from_yaml(options.map_config)\n\n    # only F1 value\n    if options.task == \"relation\":\n        tgt = [18, 24, 30, 36]\n    elif options.task == \"event\":\n        tgt = [42, 48, 54, 60]\n    elif options.task == \"entity\":\n        tgt = [6, 12]\n\n\n    if options.use_prompt:\n        predictor = HuggingfacePromptPredictor(args=options) \n    else:\n        predictor = HuggingfacePredictor(args=options) \n\n\n\n    data_folder = []\n    for it in range(1, options.iter_num + 1):\n        data_folder.append(os.path.join(options.dataname, f\"/iter_{it}\"))\n    data_folder = sorted(data_folder, key=lambda x:x)\n    print(data_folder)\n    \n    number_dict = {}\n    for data in data_folder:\n        options.data_folder = data\n        model_path = '_'.join(options.model.split('/')[1:]).replace('/', '_')\n        if options.num_beams != None:\n            model_path += f'_beam{options.num_beams}'\n        if options.do_sample:\n            if options.top_k != None:\n                model_path += f'_topk{options.top_k}'\n            if options.top_p != None:\n                model_path += f'_topp{options.top_p}'\n        os.makedirs(os.path.join('output_infer', model_path), exist_ok = True)\n        \n        \n        data_dir = data.replace('/', '_')\n        output_dir = os.path.join('output_infer', model_path, data_dir)\n        if options.CD:\n            output_dir += '_CD'\n        if os.path.exists(output_dir) and os.path.exists(os.path.join(output_dir, 'test_results.txt')):\n            cur_time = time.strftime('%m_%d_%H_%M', time.localtime(time.time()))\n            output_dir += cur_time\n        os.makedirs(output_dir, exist_ok = True)\n        \n        logging.basicConfig(\n            format=\"%(message)s\",\n            datefmt=\"%m/%d/%Y %H:%M:%S\",\n            handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(os.path.join('output_infer', 'log.txt'), mode = 'w', encoding = 'utf-8')],\n        )\n        logger.setLevel(logging.INFO)\n        logger.info(f\"config: {vars(options)}\")\n        logger.info(f\"data: {data}\")\n\n\n        predictor.load_schema(os.path.join('output_infer', 'record.schema'), options.CD)   \n        schema_dict = SEL2Record.load_schema_dict(data)\n        sel2record = SEL2Record(\n            schema_dict=schema_dict,\n            decoding_schema=options.decoding,\n            map_config=map_config,\n        )\n\n\n        for split, split_name in [('test', 'test')]:\n            gold_filename = os.path.join(data, f'{split}.json')\n\n            text_list = [x['text'] for x in read_json_file(gold_filename)]\n            token_list = [x['tokens'] for x in read_json_file(gold_filename)]\n\n            batch_num = math.ceil(len(text_list) / options.batch_size)\n\n            predicts = do_predict(predictor, output_dir, split_name, batch_num, options, text_list)\n            records = do_sel2record(predicts, sel2record, text_list, token_list, output_dir, split_name)\n\n            results = dict()\n            for task, scorer in task_dict.items():\n\n                gold_list = [x[task] for x in read_json_file(gold_filename)]\n                pred_list = [x[task] for x in records]\n\n                gold_instance_list = scorer.load_gold_list(gold_list)\n                pred_instance_list = scorer.load_pred_list(pred_list)\n\n                sub_results = scorer.eval_instance_list(\n                    gold_instance_list=gold_instance_list,\n                    pred_instance_list=pred_instance_list,\n                    verbose=options.verbose,\n                    match_mode=options.match_mode,\n                )\n                results.update(sub_results)\n\n\n            with open(os.path.join(output_dir, f'{split_name}_results.txt'), 'w') as output:\n                for key, value in results.items():\n                    output.write(f'{split_name}_{key}={value}\\n')\n\n\n            number = []\n            with open(os.path.join(output_dir, f'{split_name}_results.txt'), 'r') as freader:\n                for i, line in enumerate(freader, 1):\n                    if i in tgt:\n                        logger.info(f\"{line.strip()}\")\n                        number.append(line.strip().split(\"=\")[-1])\n            number_dict[data] = number\n\n            for num in number:\n                logger.info(f\"{num}\")\n\n\n    for key, value in number_dict.items():\n        logger.info(key)\n        for it in value:\n            logger.info(it)\n\n            \n\n    \n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "uie/extraction/label_tree.py", "content": "from typing import Dict\n\n\ndef list_dictionary(d, n_tab=-1):\n    if isinstance(d, list):\n        for i in d:\n            list_dictionary(i, n_tab)\n    elif isinstance(d, dict):\n        n_tab += 1\n        for key, value in d.items():\n            if key == '<end>':\n                print(\"{}{}\".format(\" \" * n_tab, key))\n            else:\n                print(\"{}{}\".format(\" \" * n_tab, key))\n                list_dictionary(value, n_tab)\n    else:\n        print(\"{}{}\".format(\"\\t\" * n_tab, d))\n\n\ndef print_tree(tree):\n    list_dictionary(tree)\n\n\ndef get_label_name_tree(label_name_list, tokenizer, end_symbol='<end>'):\n    '''\n    label_name_list:\n        [\"end position\", \"attack\", \"start position\", \"nominate\", \"charge indict\", \"transfer money\", \"transfer ownership\", \"release parole\", \"born\", \"sentence\", \"die\", \"demonstrate\", \"arrest jail\", \"transport\", \"elect\", \"start organization\", \"meet\", \"injure\", \"phone write\", \"merge organization\", \"declare bankruptcy\", \"trial hearing\", \"fine\", \"convict\", \"end organization\", \"sue\", \"divorce\", \"acquit\", \"appeal\", \"marry\", \"execute\", \"extradite\", \"pardon\"]\n        [\"person\", \"place\", \"destination\", \"giver\", \"plaintiff\", \"instrument\", \"attacker\", \"agent\", \"origin\", \"victim\", \"seller\", \"vehicle\", \"buyer\", \"organization\", \"adjudicator\", \"defendant\", \"beneficiary\", \"artifact\", \"prosecutor\", \"recipient\", \"target\", \"entity\"]\n    '''\n\n    sub_token_tree = dict()\n\n    label_tree = dict()   # {\"end position\":[], \"attack\":[],...}\n    for typename in label_name_list:\n        after_tokenized = tokenizer.encode(typename, add_special_tokens=False)\n        # label_tree[typename] = tokenizer.convert_ids_to_tokens(after_tokenized)\n        label_tree[typename] = after_tokenized\n\n    for _, sub_label_seq in label_tree.items():\n        parent = sub_token_tree\n        for value in sub_label_seq:\n            if value not in parent:\n                parent[value] = dict()\n            parent = parent[value]\n\n        parent[end_symbol] = None\n\n    return sub_token_tree\n\n\ndef get_type_role_tree(type_role_dict, role_tree, tokenizer, end_symbol='<end>'):\n    type_role_tree = dict()\n    for typename, role_list in type_role_dict.items():\n        typetoken = tokenizer.encode(typename, add_special_tokens=False)\n        sub_token_tree = dict()\n        for role in role_list:\n            role_token = tokenizer.encode(role, add_special_tokens=False)\n            sub_token_tree.update({role_token[0]: role_tree[role_token[0]]})\n        type_role_tree[tuple(typetoken)] = sub_token_tree\n    return type_role_tree\n\n\n\nclass PrefixTree:\n    def __init__(self, label_name_list, tokenizer, end_symbol='<end>'):\n        self.label_name_list = label_name_list\n        self._tokenizer = tokenizer\n        self.label_name_tree = get_label_name_tree(label_name_list, tokenizer, end_symbol)\n        self._end_symbol = end_symbol\n\n    def is_end_of_tree(self, tree: Dict):\n        return len(tree) == 1 and self._end_symbol in tree\n"}
{"type": "source_file", "path": "uie/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/task_format/jointer.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nimport logging\nfrom typing import Counter, List, Mapping\n\n\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.utils import tokens_to_str, change_ptb_token_back\nfrom universal_ie.ie_format import Entity, Label, Relation, Sentence, Span\n\n\n\nclass JointER(TaskFormat):\n    \"\"\" Joint Entity Relation Data format at https://github.com/yubowen-ph/JointER\"\"\"\n\n    def __init__(self, sentence_json, language='en'):\n        super().__init__(\n            language=language\n        )\n        self.tokens = sentence_json['tokens']\n        for index in range(len(self.tokens)):\n            self.tokens[index] = change_ptb_token_back(self.tokens[index])\n        if self.tokens is None:\n            print('[sentence without tokens]:', sentence_json)\n            exit(1)\n        self.spo_list = sentence_json['spo_list']\n        self.spo_details = sentence_json['spo_details']\n        self.pos_tags = sentence_json['pos_tags']\n\n\n    def generate_instance(self, delete_list):\n        entities = dict()\n        relations = dict()\n        entity_map = dict()\n        counter_entity = Counter()\n        counter_relation = Counter()\n\n        for spo_index, spo in enumerate(self.spo_details):\n            s_s, s_e, s_t = spo[0], spo[1], spo[2]\n            tokens = self.tokens[s_s: s_e]\n            indexes = list(range(s_s, s_e))\n            if (s_s, s_e, s_t) not in entity_map:\n                entities[(s_s, s_e, s_t)] = Entity(       \n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),  \n                    ),\n                    label=Label(s_t)\n                )\n                counter_entity.update([mapper.get(s_t, s_t)])\n\n            o_s, o_e, o_t = spo[4], spo[5], spo[6]\n            tokens = self.tokens[o_s: o_e]\n            indexes = list(range(o_s, o_e))\n            if (o_s, o_e, o_t) not in entity_map:      \n                entities[(o_s, o_e, o_t)] = Entity(     \n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                    ),\n                    label=Label(o_t)\n                )\n                counter_entity.update([mapper.get(o_t, o_t)])\n            \n            if spo[3] in delete_list:\n                continue\n            relations[spo_index] = Relation(\n                arg1=entities[(s_s, s_e, s_t)],\n                arg2=entities[(o_s, o_e, o_t)],\n                label=Label(spo[3]),   \n            )\n            counter_relation.update([mapper.get(spo[3], spo[3])])\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=entities.values(),\n            relations=relations.values(),\n        ), counter_entity, counter_relation\n\n\n\n    @staticmethod\n    def load_from_file(filename, language='en', delete_list = [], m = None, logger_name='') -> List[Sentence]:\n        global mapper\n        mapper = m\n        logger = logging.getLogger(logger_name)\n        logger.info(f\"Delete Relation: {delete_list}\")\n        \n        sentence_list = list()\n        raw_instance_list = json.load(open(filename))    \n        logger.info(f\"{filename}: {len(raw_instance_list)}\")\n        counter_entitys = Counter()\n        counter_relations = Counter()\n        count_rel = 0\n        \n        for instance in raw_instance_list:\n            instance, counter_entity, counter_relation = JointER(\n                    sentence_json=instance,\n                    language=language\n                ).generate_instance(delete_list)\n            sentence_list += [instance]\n            counter_entitys.update(counter_entity)\n            counter_relations.update(counter_relation)\n            if len(instance.relations) != 0:\n                count_rel += 1\n\n        counter_entitys = dict(counter_entitys)\n        counter_relations = dict(counter_relations)\n        counter_entitys = sorted(counter_entitys.items(), key = lambda x : x[1])\n        counter_relations = sorted(counter_relations.items(), key = lambda x : x[1])\n        logger.info(filename + f\" Entitys: {dict(counter_entitys)}\")\n        logger.info(filename + f\" Relations: {dict(counter_relations)}\")\n        logger.info(filename + f\" Entitys Number: {len(counter_entitys)}\")\n        logger.info(filename + f\" Relations Number: {len(counter_relations)}\")\n        logger.info(filename + f\" Sentence(至少有一个relation) Number: {count_rel}\")\n        return sentence_list\n"}
{"type": "source_file", "path": "uie/sel2record/sel2record.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict, OrderedDict\nimport os\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.predict_parser import get_predict_parser\nfrom uie.sel2record.record import EntityRecord, MapConfig, RelationRecord, EventRecord\nimport logging\n\nlogger = logging.getLogger(\"__main__\")\n\n\ntask_record_map = {\n    'entity': EntityRecord,\n    'relation': RelationRecord,\n    'event': EventRecord,\n}\n\n\n\n\ndef proprocessing_graph_record(graph, schema_dict):\n    \"\"\" Mapping generated spot-asoc result to Entity/Relation/Event\n    将抽取的Spot-Asoc结构, 根据不同的 Schema 转换成 Entity/Relation/Event 结果\n    \"\"\"\n    records = {\n        'entity': list(),\n        'relation': list(),\n        'event': list(),\n    }\n\n    entity_dict = OrderedDict()\n\n    # 根据不同任务的 Schema 将不同的 Spot 对应到不同抽取结果： Entity/Event\n    # Mapping generated spot result to Entity/Event\n    for record in graph['pred_record']:\n\n        if record['type'] in schema_dict['entity'].type_list:\n            records['entity'] += [{\n                'text': record['spot'],\n                'type': record['type']\n            }]\n            entity_dict[record['spot']] = record['type']\n\n        elif record['type'] in schema_dict['event'].type_list:\n            records['event'] += [{\n                'trigger': record['spot'],\n                'type': record['type'],\n                'roles': record['asocs']\n            }]\n\n        else:\n            print(\"Type `%s` invalid.\" % record['type'])\n\n    # 根据不同任务的 Schema 将不同的 Asoc 对应到不同抽取结果： Relation/Argument\n    # Mapping generated asoc result to Relation/Argument\n    for record in graph['pred_record']:\n        if record['type'] in schema_dict['entity'].type_list:\n            for role in record['asocs']:\n                records['relation'] += [{\n                    'type': role[0],\n                    'roles': [\n                        (record['type'], record['spot']),\n                        (entity_dict.get(role[1], record['type']), role[1]),\n                    ]\n                }]\n\n    if len(entity_dict) > 0:\n        for record in records['event']:\n            if record['type'] in schema_dict['event'].type_list:\n                new_role_list = list()\n                for role in record['roles']:\n                    if role[1] in entity_dict:\n                        new_role_list += [role]\n                record['roles'] = new_role_list\n\n    return records\n\n\nclass SEL2Record:\n    def __init__(self, schema_dict, decoding_schema, map_config: MapConfig) -> None:\n        self._schema_dict = schema_dict\n        self._predict_parser = get_predict_parser(\n            decoding_schema=decoding_schema,\n            label_constraint=schema_dict['record']\n        )\n        self._map_config = map_config\n\n    def __repr__(self) -> str:\n        return f\"## {self._map_config}\"\n\n\n\n    def sel2record(self, pred, text, tokens):  # 传入的 pred, text, tokens 都不是 list \n        # Parsing generated SEL to String-level Record\n        # 将生成的结构表达式解析成 String 级别的 Record\n        well_formed_list, counter = self._predict_parser.decode(\n            gold_list=[],\n            pred_list=[pred],\n            text_list=[text],\n        )\n\n        # Convert String-level Record to Entity/Relation/Event\n        # 将抽取的 Spot-Asoc Record 结构\n        # 根据不同的 Schema 转换成 Entity/Relation/Event 结果\n        pred_records = proprocessing_graph_record(      # 上面decode传入的是[pred]，当然只有1个啊\n            well_formed_list[0],\n            self._schema_dict\n        )\n        '''\n        'entity':[{'text': 'John Wilkes Booth', 'type': 'people'}, {'text': 'President Lincoln', 'type': 'people'}]\n        'relation':[{'type':'kill', 'roles':[('people', 'John Wilkes Booth'), ('people', 'President Lincoln')]}, ]\n        '''\n\n        pred = defaultdict(dict)\n        # Mapping String-level record to Offset-level record\n        # 将 String 级别的 Record 回标成 Offset 级别的 Record\n        for task in task_record_map:\n            record_map = task_record_map[task](\n                map_config=self._map_config,\n            )\n\n            pred[task]['offset'] = record_map.to_offset(\n                instance=pred_records.get(task, []),\n                tokens=tokens,\n            )     \n            '''\n            'entity''offset' = [('people', (0, 1, 2)), ('people', (6, 7))]   \n            'relation''offset' = [('kill', 'people', (0, 1, 2), 'people', (6, 7))]\n            '''\n            pred[task]['string'] = record_map.to_string(\n                pred_records.get(task, []),\n            ) \n            '''\n            'entity''string' = [('people', 'John Wilkes Booth'), ('people', 'President Lincoln')]   \n            'relation''string' = [('kill', 'people', 'John Wilkes Booth', 'people', 'President Lincoln')]\n            '''\n\n        return pred\n\n\n\n    @staticmethod\n    def load_schema_dict(schema_folder):\n        schema_dict = dict()\n        for schema_key in ['record', 'entity', 'relation', 'event']:\n            schema_filename = os.path.join(schema_folder, f'{schema_key}.schema')\n            if os.path.exists(schema_filename):\n                schema_dict[schema_key] = RecordSchema.read_from_file(\n                    schema_filename\n                )\n            else:\n                logger.warning(f\"{schema_filename} is empty, ignore.\")\n                schema_dict[schema_key] = RecordSchema.get_empty_schema()\n        return schema_dict\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/record_schema.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\n\nclass RecordSchema:\n    def __init__(self, type_list, role_list, type_role_dict):\n        self.type_list = type_list\n        self.role_list = role_list\n        self.type_role_dict = type_role_dict\n\n    @staticmethod\n    def read_from_file(filename):\n        lines = open(filename).readlines()\n        type_list = json.loads(lines[0])\n        role_list = json.loads(lines[1])\n        type_role_dict = json.loads(lines[2])\n        return RecordSchema(type_list, role_list, type_role_dict)\n\n    def write_to_file(self, filename):\n        with open(filename, 'w') as output:\n            output.write(json.dumps(self.type_list, ensure_ascii=False) + '\\n')\n            output.write(json.dumps(self.role_list, ensure_ascii=False) + '\\n')\n            output.write(json.dumps(self.type_role_dict, ensure_ascii=False) + '\\n')\n\n    @staticmethod\n    def output_schema(record_role_map, filename: str):\n        \"\"\"导出 Schema 文件\n        Args:\n            filename (str): [description]\n        \"\"\"\n        record_list = list(record_role_map.keys())\n        role_set = set()\n        for record in record_role_map:\n            role_set.update(record_role_map[record])\n            record_role_map[record] = list(record_role_map[record])\n        role_list = list(role_set)\n\n        record_schema = RecordSchema(type_list=record_list,\n                                    role_list=role_list,\n                                    type_role_dict=record_role_map)\n        record_schema.write_to_file(filename)"}
{"type": "source_file", "path": "uie/extraction/predict_parser/predict_parser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List, Counter, Tuple\n\n\nclass PredictParser:\n    def __init__(self, label_constraint=None):\n        self.spot_set = label_constraint.type_list if label_constraint else list()\n        self.role_set = label_constraint.role_list if label_constraint else list()\n\n    def decode(self, gold_list, pred_list, text_list=None, raw_list=None) -> Tuple[List, Counter]:\n        pass\n"}
{"type": "source_file", "path": "uie/seq2seq/models.py", "content": "import torch\nfrom torch import nn\nfrom .modeling_t5 import T5ForConditionalGeneration\nimport logging\nlogger = logging.getLogger(\"__main__\")\n\n\nclass T5Prompt(nn.Module):\n    def __init__(self, tokenizer, model_name_or_path, args):\n        super().__init__()\n        self.t5 = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n        '''修改token embeddings大小'''\n        self.t5.resize_token_embeddings(len(tokenizer))\n        logger.info(f\"Model tokenizer length: {len(tokenizer)}\")\n\n        self.config = self.t5.config\n        self.match_n_layer = self.config.num_decoder_layers\n        self.match_n_head = self.config.num_heads\n        self.n_embd = self.config.d_model\n        self.match_n_embd = self.config.d_kv\n        \n        '''\n        prompt参数\n        use_prompt是否使用prompt\n        '''\n        self.prompt_len = args.prompt_len\n        self.prompt_dim = args.prompt_dim\n        self.prompt_inputs = torch.zeros(self.prompt_len).long()    # [0, 1, ..., prompt_len-1]\n        self.use_prompt = args.use_prompt  \n        self.map = {}\n\n        self.wte = nn.Embedding(self.prompt_len, self.n_embd)\n        self.control_trans = nn.Sequential(\n            nn.Linear(self.n_embd, self.prompt_dim),\n            nn.Tanh(),\n            nn.Linear(self.prompt_dim, self.match_n_layer * 2 * self.match_n_head * self.match_n_embd),\n        )\n        \n        self.wte_enc = nn.Embedding(self.prompt_len, self.n_embd)\n        self.control_trans_enc = nn.Sequential(\n            nn.Linear(self.n_embd, self.prompt_dim),\n            nn.Tanh(),\n            nn.Linear(self.prompt_dim, self.match_n_layer * 2 * self.match_n_head * self.match_n_embd),\n        )\n        \n        self.wte_dec = nn.Embedding(self.prompt_len, self.n_embd)\n        self.control_trans_dec = nn.Sequential(\n            nn.Linear(self.n_embd, self.prompt_dim),\n            nn.Tanh(),\n            nn.Linear(self.prompt_dim, self.match_n_layer * 2 * self.match_n_head * self.match_n_embd),\n        )\n        self.dropout = nn.Dropout(0.1)\n\n\n    \n    def get_ids(self, ids):\n        '''\n        获得该ids对应的词向量的平均值\n        '''\n        ids = torch.tensor(ids, dtype=torch.long, device=self.t5.device)\n        if ids.size(0) == 1:\n            return self.t5.shared(ids)\n        else:\n            return torch.mean(self.t5.shared(ids), 0)\n\n\n    def get_prompt_ids(self, ids):\n        if ids in self.map.keys():\n            index = torch.tensor(self.map[ids], dtype=torch.long, device=self.t5.device)\n        else:\n            embed = self.get_ids(ids).squeeze(0)\n            logit = self.wte(torch.arange(self.prompt_len, dtype=torch.long, device=self.t5.device)).matmul(embed)\n            index = torch.topk(torch.log_softmax(logit, dim = 0), k=1)[1]\n        return index\n        \n\n    \n    def init_prompt(self, spot_ids, asoc_ids, negative_sample, spot_prompt_id, asoc_prompt_id, pad_id):\n        token_ids_embedding = torch.randn((self.prompt_len, self.n_embd), dtype = torch.float32, device=self.t5.device)\n        '''\n        用ids对应的词向量初始化prompt的值\n        '''\n        token_ids_embedding[0] = self.get_ids(pad_id)\n        self.map[tuple(pad_id)] = 0\n        token_ids_embedding[3] = self.get_ids(spot_prompt_id)\n        self.map[tuple(spot_prompt_id)] = 3\n        token_ids_embedding[4] = self.get_ids(asoc_prompt_id)\n        self.map[tuple(asoc_prompt_id)] = 4\n        logger.info(f\"Before token_ids_embedding: {token_ids_embedding}\")\n        logger.info(f\"Before wte: {self.wte(self.prompt_inputs)}\")\n        logger.info(f\"Before wte_enc: {self.wte_enc(self.prompt_inputs)}\")\n        logger.info(f\"Before wte_dec: {self.wte_dec(self.prompt_inputs)}\")\n\n        count = 5\n        for spot in spot_ids:\n            self.map[tuple(spot)] = count\n            token_ids_embedding[count] = self.get_ids(spot)\n            count += 1\n\n        for asoc in asoc_ids:\n            self.map[tuple(asoc)] = count\n            token_ids_embedding[count] = self.get_ids(asoc)\n            count += 1\n\n        self.start = count\n        for it in negative_sample:\n            token_ids_embedding[count] = self.get_ids(it)\n            count += 1\n\n        self.wte = self.wte.from_pretrained(token_ids_embedding)\n        self.wte_enc = self.wte_enc.from_pretrained(token_ids_embedding)\n        self.wte_dec = self.wte_dec.from_pretrained(token_ids_embedding)\n\n        logger.info(f\"After token_ids_embedding: {token_ids_embedding}\")\n        logger.info(f\"After wte: {self.wte(self.prompt_inputs)}\")\n        logger.info(f\"After wte_enc: {self.wte_enc(self.prompt_inputs)}\")\n        logger.info(f\"After wte_dec: {self.wte_dec(self.prompt_inputs)}\")\n\n    \n\n    def get_input_tokens(self, bsz, spots, asocs):\n        input_tokens = self.prompt_inputs.unsqueeze(0).expand(bsz, -1).clone().to(self.t5.device)\n        spots_len = len(spots[0])\n\n        input_tokens[:, 0] = 1\n        input_tokens[:, 1] = 2\n        input_tokens[:, 2] = 3\n        start_pos = 3\n        for i, spot in enumerate(spots):\n            for j, s in enumerate(spot, start = start_pos):\n                input_tokens[i, j] = self.get_prompt_ids(tuple(s))\n        input_tokens[:, start_pos + spots_len] = 3\n\n        input_tokens[:, start_pos + 1 + spots_len] = 4\n        start_pos = 5 + spots_len\n        for i, asoc in enumerate(asocs):\n            for j, a in enumerate(asoc, start = start_pos):\n                input_tokens[i, j] = self.get_prompt_ids(tuple(a))\n        input_tokens[:, 5 + spots_len + len(asocs[0])] = 4\n\n        return input_tokens\n\n       \n    def get_prompt(self, bsz, spots, asocs):\n        input_tokens = self.get_input_tokens(bsz, spots, asocs)\n        temp_control = self.wte(input_tokens)\n        past_key_values = self.control_trans(temp_control)  # bsz, seqlen, layer*emb\n\n        bsz, seqlen, _ = past_key_values.shape\n        past_key_values = past_key_values.view(\n            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n        )\n        past_key_values = self.dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n\n        # Cross prefix\n        temp_control_dec = self.wte_dec(input_tokens)\n        past_key_values_dec = self.control_trans_dec(\n            temp_control_dec\n        )  # bsz, seqlen, layer*emb\n\n        bsz, seqlen, _ = past_key_values_dec.shape\n        past_key_values_dec = past_key_values_dec.view(\n            bsz, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n        )\n        past_key_values_dec = self.dropout(past_key_values_dec)\n        past_key_values_dec = past_key_values_dec.permute([2, 0, 3, 1, 4]).split(2)\n\n        # Encoder prefix\n        temp_control_enc = self.wte_enc(input_tokens)\n        past_key_values_enc = self.control_trans_enc(\n            temp_control_enc\n        )  # bsz, seqlen, layer*emb\n\n        bsz_enc, seqlen, _ = past_key_values_enc.shape\n        past_key_values_enc = past_key_values_enc.view(\n            bsz_enc, seqlen, self.match_n_layer * 2, self.match_n_head, self.match_n_embd\n        )\n        past_key_values_enc = self.dropout(past_key_values_enc)\n        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n\n        result = []\n        for i, key_val in enumerate(past_key_values):\n            temp = dict()\n            temp[\"decoder_prompt\"] = {\n                \"prev_key\": key_val[0].contiguous(),\n                \"prev_value\": key_val[1].contiguous(),\n                \"prev_key_padding_mask\": input_tokens.eq(0)\n                    .to(key_val.device)\n                    .bool()\n                # bsz, prompt_len\n            }\n            key_val_dec = past_key_values_dec[i]\n            temp[\"cross_attention_prompt\"] = {\n                \"prev_key\": key_val_dec[0].contiguous(),\n                \"prev_value\": key_val_dec[1].contiguous(),\n                \"prev_key_padding_mask\": input_tokens.eq(0)\n                    .to(key_val_dec.device)\n                    .bool(),\n            }\n            key_val_enc = past_key_values_enc[i]\n            temp[\"encoder_prompt\"] = {\n                \"prev_key\": key_val_enc[0].contiguous(),\n                \"prev_value\": key_val_enc[1].contiguous(),\n                \"prev_key_padding_mask\": input_tokens.eq(0)\n                    .to(key_val_enc.device)\n                    .bool(),\n            }\n            result.append(temp)\n\n        return result\n    \n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        decoder_input_ids=None,\n        return_dict=True,\n        spot=None,\n        asoc=None,\n    ):  \n        '''\n        使用use_prompt才会self.get_prompt获取prompt\n        '''\n        bs = input_ids.size(0)\n        if self.use_prompt:\n            past_prompt = self.get_prompt(bs, spot, asoc)\n        else:\n            past_prompt = None\n        return self.t5(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            decoder_input_ids=decoder_input_ids,\n            return_dict=return_dict,\n            past_prompt=past_prompt,\n        )\n    \n\n    def generate(\n        self,\n        input_ids,\n        attention_mask,\n        spot,\n        asoc,\n        **kwargs,\n    ):\n        bsz = input_ids.shape[0]\n        past_prompt = None\n        if self.use_prompt:\n            past_prompt = self.get_prompt(bsz=bsz, spots=spot, asocs=asoc)\n        else:\n            past_prompt = None\n        \n        generated_ids = self.t5.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_prompt=past_prompt,\n            use_cache=True,\n            **kwargs,\n        )\n\n        return generated_ids"}
{"type": "source_file", "path": "dataset_construct/universal_ie/task_format/task_format.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport abc\nclass TaskFormat:\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def __init__(self, language='en'):\n        self.language = language\n\n    @abc.abstractmethod\n    def generate_instance(self):\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def load_from_file(filename, language='en', name = 'train'):\n        pass\n"}
{"type": "source_file", "path": "uie/extraction/dataset_processer.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.constants import spot_prompt, asoc_prompt, text_start\n\n\nclass TaskConfig:\n    def __init__(self, task_dict) -> None:\n        self.dataset_name = task_dict.get('name', '')\n        self.task_name = task_dict.get('task', '')\n        self.data_path = task_dict.get('path', '')\n        self.decoding_format = task_dict.get('decoding_format', '')\n        self.weight = int(task_dict.get('weight', 0))\n        self.sel2record = task_dict.get('sel2record', '')\n        self.metrics = task_dict.get('metrics', [])\n        self.eval_match_mode = task_dict.get('eval_match_mode', 'normal')\n        self.schema = RecordSchema.read_from_file(f\"{self.data_path}/{self.task_name}.schema\")\n\n    def __repr__(self) -> str:\n        return f\"dataset: {self.dataset_name}\\n\" \\\n               f\"task   : {self.task_name}\\n\" \\\n               f\"format : {self.decoding_format}\\n\" \\\n               f\"path   : {self.data_path}\\n\" \\\n               f\"schema : {self.schema}\\n\" \\\n               f\"metrics: {self.metrics}\\n\" \\\n               f\"eval_match_mode : {self.eval_match_mode}\"\n\n    @staticmethod\n    def load_list_from_yaml(task_config):\n        import yaml\n        configs = yaml.load(open(task_config), Loader=yaml.FullLoader)\n        task_configs = filter(lambda x: x.startswith('T'), configs)\n        for task_config in task_configs:\n            yield TaskConfig(configs[task_config])\n\n\nclass PrefixGenerator:\n    def __init__(self, prefix_dict) -> None:\n        self.type_list = prefix_dict.get('type', 'task dataset').split()\n        self.position = prefix_dict.get('position', 'encoder')\n\n    def __repr__(self) -> str:\n        return f\"Type.   : {self.type_list}\\n\" \\\n               f\"Position: {self.position}\\n\"\n\n    @staticmethod\n    def load_from_yaml(dataset_config):\n        import yaml\n        configs = yaml.load(open(dataset_config), Loader=yaml.FullLoader)\n        return PrefixGenerator(configs['Prefix'])\n\n    @staticmethod\n    def get_schema_prefix(schema: RecordSchema, add_split=True):\n        prefix_list = list()\n        for spot_label in sorted(schema.type_list):\n            prefix_list += [spot_prompt, spot_label]\n        for asoc_label in sorted(schema.role_list):\n            prefix_list += [asoc_prompt, asoc_label]\n        prefix = ' '.join(prefix_list)\n        if add_split:\n            return prefix + f' {text_start} '\n        else:\n            return prefix\n\n    @staticmethod\n    def get_dataset_name_prefix(dataset: TaskConfig, add_split=True):\n        if add_split:\n            return dataset.dataset_name + f' {text_start}'\n        else:\n            return dataset.dataset_name\n\n    @staticmethod\n    def get_task_name_prefix(dataset: TaskConfig, add_split=True):\n        if add_split:\n            return dataset.task_name + f' {text_start}'\n        else:\n            return dataset.task_name\n\n    def get_prefix_by_dataset(self, dataset: TaskConfig):\n        prefix_list = list()\n        for prefix_type in self.type_list:\n            if prefix_type == 'task':\n                prefix = self.get_task_name_prefix(dataset, add_split=False)\n            elif prefix_type == 'dataset':\n                prefix = self.get_dataset_name_prefix(dataset, add_split=False)\n            elif prefix_type == 'schema':\n                prefix = self.get_schema_prefix(dataset.schema, add_split=False)\n            elif prefix_type == 'meta':\n                # Meta 使用 Schema 的 Prefix\n                prefix = self.get_schema_prefix(dataset.schema, add_split=False)\n            else:\n                raise NotImplementedError(\n                    \"Prefix Type %s is not supported\" % prefix_type\n                )\n            prefix_list += [prefix]\n        return ' '.join(prefix_list) + f' {text_start}'\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/task_format/nerd.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nfrom typing import List, Optional, Tuple, Set\nfrom tqdm import tqdm\nimport logging\n\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.ie_format import Entity, Label, Sentence, Span\n\n\n\n\n# https://github.com/allenai/allennlp/blob/main/allennlp/data/dataset_readers/dataset_utils/span_utils.py\ndef _iob1_start_of_chunk(\n    prev_bio_tag: Optional[str],\n    prev_conll_tag: Optional[str],\n    curr_bio_tag: str,\n    curr_conll_tag: str,\n) -> bool:\n    if curr_bio_tag == \"B\":\n        return True\n    if curr_bio_tag == \"I\" and prev_bio_tag == \"O\":\n        return True\n    if curr_bio_tag != \"O\" and prev_conll_tag != curr_conll_tag:\n        return True\n    return False\n\n\n\ndef iob1_tags_to_spans(\n    tag_sequence: List[str], classes_to_ignore: List[str] = None\n) -> List[Tuple[str, Tuple[int, int]]]:\n    \"\"\"\n    Given a sequence corresponding to IOB1 tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded\n    by \"I-LABEL\" or \"B-LABEL\").\n    # Parameters\n    tag_sequence : `List[str]`, required.\n        The integer class labels for a sequence.\n    classes_to_ignore : `List[str]`, optional (default = `None`).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n    # Returns\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"\n    classes_to_ignore = classes_to_ignore or []\n    spans: Set[Tuple[str, Tuple[int, int]]] = set()\n    span_start = 0\n    span_end = 0\n    active_conll_tag = None\n    prev_bio_tag = None\n    prev_conll_tag = None\n    for index, string_tag in enumerate(tag_sequence):\n        curr_bio_tag = string_tag[0]\n        curr_conll_tag = string_tag[2:]\n\n        if curr_bio_tag not in [\"B\", \"I\", \"O\"]:\n            raise RuntimeError('Invalid tag sequence %s' % tag_sequence)\n        if curr_bio_tag == \"O\" or curr_conll_tag in classes_to_ignore:\n            # The span has ended.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = None\n        elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):\n            # We are entering a new span; reset indices\n            # and active tag to new span.\n            if active_conll_tag is not None:\n                spans.add((active_conll_tag, (span_start, span_end)))\n            active_conll_tag = curr_conll_tag\n            span_start = index\n            span_end = index\n        else:\n            # bio_tag == \"I\" and curr_conll_tag == active_conll_tag\n            # We're continuing a span.\n            span_end += 1\n\n        prev_bio_tag = string_tag[0]\n        prev_conll_tag = string_tag[2:]\n    # Last token might have been a part of a valid span.\n    if active_conll_tag is not None:\n        spans.add((active_conll_tag, (span_start, span_end)))\n    return list(spans)\n\n\n\n\n\nclass NERD(TaskFormat):\n    def __init__(self, tokens: List[str], spans:  List[Tuple[Tuple[int, int], str]], language='en', instance_id=None) -> None:\n        super().__init__(\n            language=language\n        )\n        self.instance_id = instance_id\n        self.tokens = tokens\n        self.spans = spans\n\n    @staticmethod\n    def load_from_file(filename, language = 'en', delete_list = [], m = None, logger_name='') -> List[Sentence]:\n        global mapper\n        mapper = m\n        logger = logging.getLogger(logger_name)\n        logger.info(f\"Delete Relation: {delete_list}\")\n        counter_entitys = Counter()\n        count_entitys = 0\n\n        sentence_list = list()\n        for rows in tqdm(NERD.generate_sentence(filename)):\n            if rows[0][0] == '-DOCSTART-':\n                continue\n            tokens = [token[0] for token in rows]\n            ner = [token[1] for token in rows]\n            spans = iob1_tags_to_spans(ner)\n            spans = [\n                {'start': span[1][0], 'end': span[1][1], 'type': span[0]}\n                for span in spans\n            ]\n            sentence, counter_entity = NERD(\n                tokens=tokens,\n                spans=spans,\n                language=language,\n            ).generate_instance(delete_list)\n\n            counter_entitys.update(counter_entity)\n            sentence_list += [sentence]\n            if len(sentence.entities) != 0:\n                count_entitys += 1\n\n        counter_entitys = sorted(counter_entitys.items(), key = lambda x : x[1])\n        logger.info(filename + f\" Entitys: {dict(counter_entitys)}\")\n        logger.info(filename + f\" Entitys Number: {len(counter_entitys)}\")\n        logger.info(filename + f\" Sentence(至少有一个entity) Number: {count_entitys}\")\n        return sentence_list\n\n\n    @staticmethod\n    def generate_sentence(filename):\n        sentence = list()\n        with open(filename) as fin:\n            for line in fin:\n                if line.strip() == '':\n                    if len(sentence) != 0:\n                        yield sentence\n                        sentence = list()\n\n                else:\n                    sentence += [line.strip().split()]       \n\n            if len(sentence) != 0:\n                yield sentence\n\n\n    def generate_instance(self, delete_list):\n        counter_entity = Counter()\n        entities = list()\n        for span_index, span in enumerate(self.spans):\n            tokens = self.tokens[span['start']: span['end'] + 1]\n            indexes = list(range(span['start'], span['end'] + 1))\n            if span['type'] in delete_list:\n                continue\n            entities += [\n                Entity(\n                    span=Span(\n                        tokens=tokens,\n                        indexes=indexes,\n                        text=tokens_to_str(tokens, language=self.language),\n                        text_id=self.instance_id\n                    ),\n                    label=Label(span['type']),\n                    text_id=self.instance_id,\n                    record_id=self.instance_id + \"#%s\" % span_index if self.instance_id else None)\n            ]\n            counter_entity.update([mapper.get(span['type'], span['type'])])\n\n        return Sentence(tokens=self.tokens,\n                        entities=entities,\n                        text_id=self.instance_id), counter_entity\n\n\n\n"}
{"type": "source_file", "path": "uie/extraction/noiser/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/ie_format.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import List, Union, Tuple\nfrom universal_ie.utils import change_name_using_label_mapper\n\n\n\n\n# All Entity Relation Events are structured records.\n# They both have attributes text_id and record_id\nclass Record:\n    def __init__(self,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        self.text_id = text_id\n        self.record_id = record_id\n\n    @abstractmethod\n    def to_offset(self):\n        pass\n\n\n# Text span\n# 连续或者非连续的文本块\nclass Span:\n    def __init__(self,\n                 tokens: List[str],\n                 indexes: List[int],\n                 text: str,\n                 text_id: Union[str, None] = None,\n                 ) -> None:\n        self.tokens = tokens\n        self.indexes = indexes\n        self.text = text\n        self.text_id = text_id\n\n    def __repr__(self) -> str:\n        return \"[%s](%s)\" % (self.text, self.indexes)\n\n    @staticmethod\n    def get_empty_span(text_id: Union[str, None] = None,):\n        return Span(\n            tokens=list(),\n            indexes=list(),\n            text=\"\",\n            text_id=text_id\n        )\n\n    def is_empty_span(self):\n        \"\"\"Check is empty span.\n\n        Returns:\n            bool: True, Empty Span; False Non-Empty Span\n        \"\"\"\n        return len(self.tokens) == 0 and len(self.indexes) == 0\n\n\n# Label Name\nclass Label:\n    def __init__(self, label_name: Union[str, List[str]]) -> None:\n        self.label_name = label_name\n\n    def __repr__(self) -> str:\n        return self.label_name\n\n    def __lt__(self, other):\n        if not isinstance(other, Label):\n            return NotImplemented\n        return self.label_name < other.label_name\n\n\n# Entity, Span\n# 实体，以文本块为核心的一元结构\nclass Entity(Record):\n    def __init__(self,\n                 span: Span,\n                 label: Label,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.span = span\n        self.label = label\n\n    def __lt__(self, other):\n        if not isinstance(other, Entity):\n            return NotImplemented\n        return self.span.indexes < other.span.indexes\n\n    def __repr__(self) -> str:\n        return self.span.__repr__() + self.label.__repr__()\n\n    def to_offset(self, ent_label_mapper=None):\n        if self.span.is_empty_span():\n            # If span is empty, skip entity\n            return {}\n        return {'type': change_name_using_label_mapper(self.label.label_name,    \n                                                       ent_label_mapper),\n                'offset': self.span.indexes,\n                'text': self.span.text}\n\n\n# Relation Span Pair\n# 关系，以文本块对为核心的二元结构\nclass Relation(Record):\n    def __init__(self,\n                 arg1: Entity,\n                 arg2: Entity,\n                 label: Label,\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.arg1 = arg1\n        self.arg2 = arg2\n        self.label = label\n\n    def __repr__(self) -> str:\n        return self.arg1.__repr__() + self.label.__repr__() + self.arg2.__repr__()\n\n    def to_offset(self, rel_label_mapper=None, ent_label_mapper=None):\n        if self.arg1.span.is_empty_span() or self.arg2.span.is_empty_span():\n            # If span is empty, skip relation\n            return {}\n        return {'type': change_name_using_label_mapper(self.label.label_name,\n                                                       rel_label_mapper),\n                'args': [self.arg1.to_offset(ent_label_mapper=ent_label_mapper),\n                         self.arg2.to_offset(ent_label_mapper=ent_label_mapper),\n                         ],\n                }\n\n\n# Event, Trigger-Mult-Argument\n# 事件，以触发词为中心的多元(谓词论元)结构\nclass Event(Record):\n    def __init__(self,\n                 span: Span,\n                 label: Label,\n                 args: List[Tuple[Label, Entity]],\n                 text_id: Union[str, None] = None,\n                 record_id: Union[str, None] = None,\n                 ) -> None:\n        super().__init__(text_id=text_id, record_id=record_id)\n        self.span = span\n        self.label = label\n        self.args = args\n\n    def __repr__(self) -> str:\n        return self.span.__repr__() + self.label.__repr__()\n\n    def to_offset(self, evt_label_mapper=None):\n\n        if self.span.is_empty_span():\n            # If span is empty, skip relation\n            return {}\n\n        args = list()\n        for role, arg in self.args:\n            if arg.span.is_empty_span():\n                continue\n            args += [{\n                    'type': change_name_using_label_mapper(\n                        role.label_name,\n                        evt_label_mapper,\n                    ),\n                    'offset': arg.span.indexes,\n                    'text': arg.span.text\n                }]\n\n        return {'type': change_name_using_label_mapper(self.label.label_name,\n                                                       evt_label_mapper),\n                'offset': self.span.indexes,\n                'text': self.span.text,\n                'args': args}\n\n\nclass Sentence:\n    def __init__(self,\n                 tokens: List[str],\n                 entities: List[Entity] = None,\n                 relations: List[Relation] = None,\n                 events: List[Event] = None,\n                 text_id: Union[str, None] = None,\n                 ) -> None:\n        self.tokens = tokens\n        self.entities = entities or list()\n        self.relations = relations or list()\n        self.events = events or list()\n        self.text_id = text_id\n\n    def count_entity_without_relation(self):\n        entity_set = set()\n        entity_counter = defaultdict(int)\n        for entity in self.entities:\n            entity_set.add((tuple(entity.span.indexes), entity.label.label_name))\n\n        for relation in self.relations:\n            entity1 = (tuple(relation.arg1.span.indexes), relation.arg1.label.label_name)\n            entity2 = (tuple(relation.arg2.span.indexes), relation.arg2.label.label_name)\n            entity_counter[entity1] += 1\n            entity_counter[entity2] += 1\n            entity_set.remove(entity1) if entity1 in entity_set else None\n            entity_set.remove(entity2) if entity2 in entity_set else None\n        overlap_entity = sum([1 if v > 1 else 0 for k, v in entity_counter.items()])\n        return {'entity': len(self.entities),\n                'entity_without_relation': len(entity_set),\n                'overlap_entity': overlap_entity,\n                }\n\n\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/__init__.py", "content": ""}
{"type": "source_file", "path": "uie/extraction/utils.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\ndef convert_spot_asoc(spot_asoc_instance, structure_maker, null_text = \"\"):\n    '''\n    将 spot asoc 结构转换成SEL表达式\n    '''\n    spot_instance_str_rep_list = list()\n    for spot in spot_asoc_instance:\n        spot_str_rep = [\n            spot['label'],\n            structure_maker.target_span_start,\n            spot['span'],\n        ]\n        for asoc_label, asoc_span in spot.get('asoc', list()):\n            asoc_str_rep = [\n                structure_maker.span_start,\n                asoc_label,\n                structure_maker.target_span_start,\n                asoc_span,\n                structure_maker.span_end,\n            ]\n            spot_str_rep += [' '.join(asoc_str_rep)]\n        spot_instance_str_rep_list += [' '.join([\n            structure_maker.record_start,\n            ' '.join(spot_str_rep),\n            structure_maker.record_end,\n        ])]\n    if null_text != \"\":\n        spot_instance_str_rep_list += [null_text]\n    target_text = ' '.join([\n        structure_maker.sent_start,\n        ' '.join(spot_instance_str_rep_list),\n        structure_maker.sent_end,\n    ])\n    return target_text\n\n\ndef convert_spot_asoc_name(spot_asoc_instance, structure_maker):\n    \"\"\"将一个 Spot-Asoc-Name 实例转换成目标字符串\n\n    Args:\n        spot_asoc_instance ([type]): [description]\n        structure_maker ([type]): [description]\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    spot_instance_str_rep_list = list()\n    for spot in spot_asoc_instance:\n        spot_str_rep = [\n            spot['span'],\n            structure_maker.target_span_start,\n            spot['label'],\n        ]\n        for asoc_label, asoc_span in spot.get('asoc', list()):\n            asoc_str_rep = [\n                structure_maker.span_start,\n                asoc_span,\n                structure_maker.target_span_start,\n                asoc_label,\n                structure_maker.span_end,\n            ]\n            spot_str_rep += [' '.join(asoc_str_rep)]\n        spot_instance_str_rep_list += [' '.join([\n            structure_maker.record_start,\n            ' '.join(spot_str_rep),\n            structure_maker.record_end,\n        ])]\n    target_text = ' '.join([\n        structure_maker.sent_start,\n        ' '.join(spot_instance_str_rep_list),\n        structure_maker.sent_end,\n    ])\n    return target_text\n\n\nconvert_to_record_function = {\n    'spotasoc': convert_spot_asoc,\n    'spotasocname': convert_spot_asoc_name,\n}\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.extraction.predict_parser.predict_parser import PredictParser\nfrom uie.extraction.predict_parser.spotasoc_predict_parser import SpotAsocPredictParser\n\n\ndecoding_format_dict = {\n    'spotasoc': SpotAsocPredictParser,\n}\n\n\ndef get_predict_parser(decoding_schema, label_constraint):\n    return decoding_format_dict[decoding_schema](label_constraint=label_constraint)\n"}
{"type": "source_file", "path": "dataset_construct/universal_ie/task_format/oneie.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom typing import Counter, List\nimport logging\n\nfrom universal_ie.utils import tokens_to_str\nfrom universal_ie.task_format.task_format import TaskFormat\nfrom universal_ie.ie_format import Entity, Event, Label, Sentence, Span\n\n\n\nclass OneIEEvent(TaskFormat):\n    def __init__(self, doc_json, language='en'):\n        super().__init__(\n            language=language\n        )\n        self.doc_id = doc_json['doc_id']\n        self.sent_id = doc_json['sent_id']\n        self.tokens = doc_json['tokens']\n        self.entities = doc_json['entity_mentions']\n        self.relations = doc_json['relation_mentions']\n        self.events = doc_json['event_mentions']\n\n    def generate_instance(self, delete_list):\n        events = dict()\n        entities = dict()\n        counter_trigger = Counter()\n        counter_role = Counter()\n\n        for span_index, span in enumerate(self.entities):\n            tokens = self.tokens[span['start']: span['end']]\n            indexes = list(range(span['start'], span['end']))\n            entities[span['id']] = Entity(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id\n                ),\n                label=Label(span['entity_type']),\n                text_id=self.sent_id,\n                record_id=span['id']\n            )\n\n        for event_index, event in enumerate(self.events):\n            if str(event['event_type']) in delete_list:\n              continue\n            start = event['trigger']['start']\n            end = event['trigger']['end']\n            tokens = self.tokens[start:end]\n            indexes = list(range(start, end))\n            events[event['id']] = Event(\n                span=Span(\n                    tokens=tokens,\n                    indexes=indexes,\n                    text=tokens_to_str(tokens, language=self.language),\n                    text_id=self.sent_id\n                ),\n                label=Label(event['event_type']),\n                args=[(Label(x['role']), entities[x['entity_id']])\n                      for x in event['arguments']],\n                text_id=self.sent_id,\n                record_id=event['id']\n            )\n            counter_trigger.update([mapper.get(event['event_type'], event['event_type'])])\n            for x in event['arguments']:\n              counter_role.update([mapper.get(x['role'], x['role'])])\n\n        return Sentence(\n            tokens=self.tokens,\n            entities=list(),\n            relations=list(),\n            events=events.values(),\n            text_id=self.sent_id\n        ), counter_trigger, counter_role\n\n    @staticmethod\n    def load_from_file(filename, language='en', delete_list = [], m = None, logger_name='') -> List[Sentence]:\n        global mapper\n        mapper = m\n        logger = logging.getLogger(logger_name)\n        logger.info(f\"Delete Trigger: {delete_list}\")\n        sentence_list = list()\n        counter_triggers = Counter()\n        counter_roles = Counter()\n        count_sent = 0\n        with open(filename) as fin:\n            for line in fin:\n                instance, counter_trigger, counter_role = OneIEEvent(\n                    json.loads(line.strip()),\n                    language=language\n                ).generate_instance(delete_list)\n                sentence_list += [instance]\n                counter_triggers.update(counter_trigger)\n                counter_roles.update(counter_role)\n                if len(instance.events) != 0:\n                  count_sent += 1\n\n        counter_triggers = sorted(dict(counter_triggers).items(), key = lambda x : x[1])\n        counter_roles = sorted(dict(counter_roles).items(), key = lambda x : x[1])\n        logger.info(filename + f\" Event Trigger: {dict(counter_triggers)}\")\n        logger.info(filename + f\" Event Role: {dict(counter_roles)}\")\n        logger.info(filename + f\" Event Trigger Number: {len(counter_triggers)}\")\n        logger.info(filename + f\" Event Role Number: {len(counter_roles)}\")\n        logger.info(filename + f\" Sentence(至少有一个event) Number: {count_sent}\")\n        return sentence_list\n        \n"}
{"type": "source_file", "path": "uie/extraction/scorer.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom typing import Dict, List\nimport sys\nimport json\n\n\n\ndef tuple_offset(offset):\n    if isinstance(offset, tuple):\n        return offset\n    else:\n        return tuple(offset)\n\n\nclass Metric:\n    \"\"\" Tuple Metric \"\"\"\n    def __init__(self, verbose=False, match_mode='normal'):\n        self.tp = 0.              # 是 self.tp +=1，计算的是总数\n        self.gold_num = 0.        # 是 self.gold_num +=，计算的是总数\n        self.pred_num = 0.        # 是 self.pred_num +=，计算的是总数\n        self.verbose = verbose\n        self.match_mode = match_mode\n        assert self.match_mode in {'set', 'normal', 'multimatch'}\n\n    def __repr__(self) -> str:\n        return f\"tp: {self.tp}, gold: {self.gold_num}, pred: {self.pred_num}\"\n\n    @staticmethod\n    def safe_div(a, b):\n        if b == 0.:\n            return 0.\n        else:\n            return a / b\n\n    def compute_f1(self, prefix=''):\n        tp = self.tp\n        pred_num = self.pred_num\n        gold_num = self.gold_num\n        p, r = self.safe_div(tp, pred_num), self.safe_div(tp, gold_num)\n        return {prefix + 'tp': tp,\n                prefix + 'gold': gold_num,\n                prefix + 'pred': pred_num,\n                prefix + 'P': p * 100,\n                prefix + 'R': r * 100,\n                prefix + 'F1': self.safe_div(2 * p * r, p + r) * 100\n                }\n\n    def count_instance(self, gold_list, pred_list):\n        if self.match_mode == 'set':\n            gold_list = set(gold_list)\n            pred_list = set(pred_list)\n            if self.verbose:\n                print(\"Gold:\", gold_list)\n                print(\"Pred:\", pred_list)\n            self.gold_num += len(gold_list)\n            self.pred_num += len(pred_list)\n            self.tp += len(gold_list & pred_list)\n\n        else:\n            if self.verbose:\n                print(\"Gold:\", gold_list)\n                print(\"Pred:\", pred_list)\n            self.gold_num += len(gold_list)\n            self.pred_num += len(pred_list)\n\n            if len(gold_list) > 0 and len(pred_list) > 0:\n                # guarantee length same\n                assert len(gold_list[0]) == len(pred_list[0]), print(len(gold_list[0]), len(pred_list[0]), gold_list, pred_list)\n\n            dup_gold_list = deepcopy(gold_list)\n            for pred in pred_list:\n                if pred in dup_gold_list:\n                    self.tp += 1\n                    if self.match_mode == 'normal':\n                        # Each Gold Instance can be matched one time\n                        dup_gold_list.remove(pred)\n\n    def count_batch_instance(self, batch_gold_list, batch_pred_list):\n        for gold_list, pred_list in zip(batch_gold_list, batch_pred_list):\n            self.count_instance(gold_list=gold_list, pred_list=pred_list)\n\n\nclass RecordMetric(Metric):\n    \"\"\" 不考虑不同 Role 之间的顺序，例如事件论元\"\"\"\n    @staticmethod\n    def is_equal(gold, pred):\n        if gold['type'] != pred['type']:\n            return False\n        if gold['spot'] != pred['spot']:\n            return False\n        if len(gold['asocs']) != len(pred['asocs']):\n            return False\n        for gold_role, pred_role in zip(sorted(gold['asocs']), sorted(pred['asocs'])):\n            if gold_role != pred_role:\n                return False\n        return True\n\n    def count_instance(self, gold_list, pred_list):\n        if self.match_mode == 'set':\n            raise NotImplementedError(f'{self.__class__.__name__} do not support the match model `set`')\n\n        if self.verbose:\n            print(\"Gold:\", gold_list)\n            print(\"Pred:\", pred_list)\n\n        self.gold_num += len(gold_list)\n        self.pred_num += len(pred_list)\n\n        gold_indexes = list(range(len(gold_list)))\n        non_found = [True] * len(gold_list)\n        for pred in pred_list:\n            for gold_index in gold_indexes:\n                if non_found[gold_index] and self.is_equal(gold_list[gold_index], pred):\n                    self.tp += 1\n                    non_found[gold_index] = False\n                    if self.match_mode == 'normal':\n                        break\n\n\nclass OrderedRecordMetric(RecordMetric):\n    \"\"\" 考虑不同 Role 之间的顺序，例如关系 \"\"\"\n    @staticmethod\n    def is_equal(gold, pred):\n        if gold['type'] != pred['type']:\n            return False\n        if gold['spot'] != pred['spot']:\n            return False\n        if len(gold['asocs']) != len(pred['asocs']):\n            return False\n        for gold_role, pred_role in zip(gold['asocs'], pred['asocs']):\n            if gold_role != pred_role:\n                return False\n        return True\n\n\ndef warning_tp_increment(gold, pred, prefix):\n    sys.stderr.write(f\"{prefix} TP Increment Warning, Gold Offset: {gold['offset']}\\n\")\n    sys.stderr.write(f\"{prefix} TP Increment Warning, Pred Offset: {pred['offset']}\\n\")\n    sys.stderr.write(f\"{prefix} TP Increment Warning, Gold String: {gold['string']}\\n\")\n    sys.stderr.write(f\"{prefix} TP Increment Warning, Pred String: {pred['string']}\\n\")\n    sys.stderr.write(f\"===============\\n\")\n\n\nclass Scorer:\n    @staticmethod\n    def load_gold_list(gold_list, offset_key=None):\n        raise NotImplementedError\n\n    @staticmethod\n    def load_pred_list(pred_list):\n        raise NotImplementedError\n\n    @staticmethod\n    def eval_instance_list(gold_instance_list, pred_instance_list, verbose=False, match_mode='normal'):\n        raise NotImplementedError\n\n\nclass EntityScorer(Scorer):\n    @staticmethod\n    def load_gold_list(gold_list: List[List[Dict]]):\n        \"\"\" Load gold instance to `string` and `offset`\n\n        Args:\n            gold_list (List[List[Dict]]): [description]\n                [\n                    [\n                        {'type': 'Geo-political', 'offset': [7], 'text': 'seattle'},\n                        {'type': 'Location', 'offset': [11], 'text': 'lot'},\n                        {'type': 'Geo-political', 'offset': [14], 'text': 'city'}\n                    ],\n                    [...]\n                ]\n\n        Returns:\n            List[Dict]: each instance has `offset` and `string`\n                [\n                    {\n                        'offset': [('Geo-political', (7,)), ('Location', (11,)), ('Geo-political', (14,))],\n                        'string': [('Geo-political', 'seattle'), ('Location', 'lot'), ('Geo-political', 'city')]\n                    },\n                    {...}, ...\n                ]\n        \"\"\"\n        gold_instance_list = []\n        for gold in gold_list:\n            gold_offset = list()\n            gold_string = list()\n            for span in gold:\n                span_label = span['type']\n                span_offset = span['offset']\n                span_text = span['text']\n                gold_offset += [(span_label, tuple_offset(span_offset))]\n                gold_string += [(span_label, span_text)]\n            gold_instance = {\n                'offset': gold_offset,\n                'string': gold_string,\n            }\n            gold_instance_list += [gold_instance]\n        return gold_instance_list\n\n    @staticmethod\n    def load_pred_list(pred_list: List[Dict]):\n        \"\"\"[summary]\n\n        Args:\n            pred_list (List[Dict]): [description]\n                [\n                    {\n                        'offset': [['Geo-political', [7]], ['Geo-political', [14]]],\n                        'string': [['Geo-political', 'seattle'], ['Geo-political', 'city']]\n                    },\n                    {...},\n                ]\n        Returns:\n            List[Dict] : each relation instance has `offset` and `string`\n                [\n                    {\n                        'offset': [('Geo-political', (7,)), ('Geo-political', (14,))],\n                        'string': [('Geo-political', 'seattle'), ('Geo-political', 'city')]\n                    }\n                ]\n        \"\"\"\n        pred_instance_list = list()\n        for pred in pred_list:\n            for offset_pred in pred['offset']:\n                if not isinstance(offset_pred[1], tuple):\n                    offset_pred[1] = tuple_offset(offset_pred[1])\n            pred['offset'] = [tuple_offset(p) for p in pred['offset']]\n            pred['string'] = [tuple_offset(p) for p in pred['string']]\n            pred_instance_list += [pred]\n        return pred_instance_list\n\n    @staticmethod\n    def eval_instance_list(gold_instance_list: List[Dict], pred_instance_list: List[Dict], verbose=False, match_mode='normal'):\n        \"\"\"[summary]\n\n        Args:\n            gold_instance_list (List[Dict]): [description]\n                [\n                    {\n                        'offset': [('Geo-political', (7,)), ('Location', (11,)), ('Geo-political', (14,))],\n                        'string': [('Geo-political', 'seattle'), ('Location', 'lot'), ('Geo-political', 'city')]\n                    },\n                    {...}, ...\n                ]\n            pred_instance_list (List[Dict]): [description]\n                [\n                    {\n                        'offset': [('Geo-political', (7,)), ('Geo-political', (14,))],\n                        'string': [('Geo-political', 'seattle'), ('Geo-political', 'city')]\n                    }\n                ]\n            verbose (bool, optional): [description]. Defaults to False.\n            match_mode (string, optional): [description]. Defaults to `normal` .\n\n        Returns:\n            Dict: Result of Evaluation\n                (offset, string) X (gold, pred, tp, P, R, F1)\n        \"\"\"\n        metrics = {\n            'string': Metric(verbose=verbose, match_mode=match_mode),\n            'offset': Metric(verbose=verbose, match_mode=match_mode),\n        }\n        for pred, gold in zip(pred_instance_list, gold_instance_list):\n\n            pre_string_tp, pre_offset_tp = metrics['string'].tp, metrics['offset'].tp\n\n            for eval_key in metrics:\n                metrics[eval_key].count_instance(\n                    gold_list=gold.get(eval_key, []),\n                    pred_list=pred.get(eval_key, [])\n                )\n\n            post_string_tp, post_offset_tp = metrics['string'].tp, metrics['offset'].tp\n            if verbose and post_offset_tp - pre_offset_tp != post_string_tp - pre_string_tp:\n                warning_tp_increment(gold=gold, pred=pred, prefix='Entity')\n\n        results = dict()\n        for eval_key in metrics:\n            results.update(metrics[eval_key].compute_f1(prefix=eval_key + '-ent-'))\n\n        return results\n\n\nclass RelationScorer(Scorer):\n    @staticmethod\n    def load_gold_list(gold_list: List[List[Dict]]):\n        \"\"\"[summary]\n\n        Args:\n            gold_list (List[List[Dict]]): List of Sentece, each sentence contains a List of Relation Dict\n                [\n                    [\n                        {\n                            'type': 'Part-whole',\n                            'args': [{'type': 'Location', 'offset': [11], 'text': 'lot'}, {'type': 'Geo-political', 'offset': [14], 'text': 'city'}]\n                        }, ...\n                    ],\n                    [...],\n                ]\n\n        Returns:\n            List[Dict]: List of Sentece, each sentence contains two List (offset, string) of Relation Tuple\n                [\n                    {\n                        'offset': [('Part-whole', 'Geo-political', (0,), 'Geo-political', (2,)), ... ],\n                        'string': [('Part-whole', 'Geo-political', 'MULTAN', 'Geo-political', 'Pakistan'), ...]\n                    }\n                ]\n        \"\"\"\n        gold_instance_list = []\n        for gold in gold_list:\n            gold_instance = defaultdict(list)\n            for record in gold:\n                assert len(record['args']) == 2\n                gold_instance['offset'] += [(\n                    record['type'],\n                    record['args'][0]['type'],\n                    tuple_offset(record['args'][0]['offset']),\n                    record['args'][1]['type'],\n                    tuple_offset(record['args'][1]['offset']),\n                )]\n                gold_instance['string'] += [(\n                    record['type'],\n                    record['args'][0]['type'],\n                    record['args'][0]['text'],\n                    record['args'][1]['type'],\n                    record['args'][1]['text'],\n                )]\n            gold_instance_list += [gold_instance]\n\n        return gold_instance_list\n\n    @staticmethod\n    def load_pred_list(pred_list):\n        \"\"\"[summary]\n\n        Args:\n            pred_list (List[Dict]): List of Sentece, each sentence contains two List (offset, string) of Relation List\n                [\n                    {\n                        'offset': [['Part-whole', 'Geo-political', [0], 'Geo-political', [2]]],\n                        'string': [['Part-whole', 'Geo-political', 'MULTAN', 'Geo-political', 'Pakistan']],\n                    }, ...\n                ]\n        Returns:\n            List[Dict]: List of Sentece, each sentence contains two List (offset, string) of Relation Tuple\n                [\n                    {\n                        'offset': [('Part-whole', 'Geo-political', (0,), 'Geo-political', (2,))],\n                        'string': [('Part-whole', 'Geo-political', 'MULTAN', 'Geo-political', 'Pakistan')]\n                    }, ...\n                ]\n        \"\"\"\n        pred_instance_list = list()\n        for pred in pred_list:\n            for offset_pred in pred['offset']:\n\n                if not isinstance(offset_pred[2], tuple):\n                    offset_pred[2] = tuple_offset(offset_pred[2])\n\n                if not isinstance(offset_pred[4], tuple):\n                    offset_pred[4] = tuple_offset(offset_pred[4])\n\n            pred['offset'] = [tuple_offset(p) for p in pred['offset']]\n            pred['string'] = [tuple_offset(p) for p in pred['string']]\n            pred_instance_list += [pred]\n        return pred_instance_list\n\n    @staticmethod\n    def eval_instance_list(gold_instance_list, pred_instance_list, verbose=False, match_mode='normal'):\n        \"\"\"[summary]\n\n        Args:\n            gold_instance_list (List[Dict]): List of Sentece, each sentence contains two List (offset, string) of Relation Tuple\n                [\n                    {\n                        'offset': [('Part-whole', 'Geo-political', (0,), 'Geo-political', (2,)), ... ],\n                        'string': [('Part-whole', 'Geo-political', 'MULTAN', 'Geo-political', 'Pakistan'), ...]\n                    }\n                ]\n            pred_instance_list ([type]): List of Sentece, each sentence contains two List (offset, string) of Relation Tuple\n                [\n                    {\n                        'offset': [('Part-whole', 'Geo-political', (0,), 'Geo-political', (2,))],\n                        'string': [('Part-whole', 'Geo-political', 'MULTAN', 'Geo-political', 'Pakistan')]\n                    }, ...\n                ]\n            verbose (bool, optional): Defaults to False.\n            match_mode (string, optional): [description]. Defaults to `normal` .\n\n        Returns:\n            Dict: Result of Evaluation\n                (offset, string) X (boundary, strict) X (gold, pred, tp, P, R, F1)\n        \"\"\"\n        # Span Boundary and Type\n        metrics = {\n            'offset': Metric(verbose=verbose, match_mode=match_mode),\n            'string': Metric(verbose=verbose, match_mode=match_mode),\n        }\n        # Span Boundary Only    不看标签是否正确，只看句子中对应的Span是否正确\n        boundary_metrics = {\n            'offset': Metric(verbose=verbose, match_mode=match_mode),\n            'string': Metric(verbose=verbose, match_mode=match_mode),\n        }\n        for pred, gold in zip(pred_instance_list, gold_instance_list):\n\n            pre_string_tp, pre_offset_tp = metrics['string'].tp, metrics['offset'].tp\n\n            for eval_key in metrics:\n                # Span Boundary and Type\n                metrics[eval_key].count_instance(\n                    gold_list=gold.get(eval_key, []),\n                    pred_list=pred.get(eval_key, []),\n                )\n\n            post_string_tp, post_offset_tp = metrics['string'].tp, metrics['offset'].tp\n            if verbose and (post_offset_tp - pre_offset_tp != post_string_tp - pre_string_tp):\n                warning_tp_increment(gold=gold, pred=pred, prefix='Relation Strict')\n\n            pre_string_tp, pre_offset_tp = boundary_metrics['string'].tp, boundary_metrics['offset'].tp\n\n            for eval_key in boundary_metrics:\n                # Span Boundary Only\n                boundary_metrics[eval_key].count_instance(\n                    gold_list=[(x[0], x[2], x[4]) for x in gold.get(eval_key, [])],\n                    pred_list=[(x[0], x[2], x[4]) for x in pred.get(eval_key, [])],\n                )\n            post_string_tp, post_offset_tp = boundary_metrics['string'].tp, boundary_metrics['offset'].tp\n            if verbose and post_offset_tp - pre_offset_tp != post_string_tp - pre_string_tp:\n                warning_tp_increment(gold=gold, pred=pred, prefix='Relation Boundary')\n\n        results = dict()\n        for eval_key in metrics:\n            results.update(metrics[eval_key].compute_f1(prefix=eval_key + '-rel-strict-'))\n        for eval_key in boundary_metrics:\n            results.update(boundary_metrics[eval_key].compute_f1(prefix=eval_key + '-rel-boundary-'))\n        return results\n\n\nclass EventScorer(Scorer):\n    @staticmethod\n    def load_gold_list(gold_list):\n        \"\"\"[summary]\n\n        Args:\n            gold_list (List[List[Dict]]): List of Sentece, each sentence contains a List of Event Dict\n                [\n                    [ # Sentance\n                        { # Event Record\n                            'type': 'Die',\n                            'offset': [16],\n                            'text': 'shot',\n                            'args': [\n                                {'type': 'Victim', 'offset': [17], 'text': 'himself'},\n                                {'type': 'Agent', 'offset': [5, 6], 'text': 'John Joseph'},\n                                {'type': 'Place', 'offset': [23], 'text': 'court'}\n                            ]\n                        },\n                    ]\n                ]\n\n        Returns:\n            List[Dict]: List of Sentece, each sentence contains Four List of Event Tuple\n                [\n                    {\n                        'offset_trigger': [('Die', (16,)), ('Convict', (30,))],\n                        'string_trigger': [('Die', 'shot'), ('Convict', 'convicted')],\n                        'offset_role': [('Die', 'Victim', (17,)), ('Die', 'Agent', (5, 6)), ('Die', 'Place', (23,))],\n                        'string_role': [('Die', 'Victim', 'himself'), ('Die', 'Agent', 'John Joseph'), ('Die', 'Place', 'court')]\n                    },\n                    ...\n                ]\n        \"\"\"\n        gold_instance_list = []\n        for gold in gold_list:\n            gold_instance = defaultdict(list)\n            for record in gold:\n                gold_instance['offset_trigger'] += [(record['type'], tuple_offset(record['offset']))]\n                gold_instance['string_trigger'] += [(record['type'], record['text'])]\n                for arg in record['args']:\n                    gold_instance['offset_role'] += [(record['type'], arg['type'], tuple_offset(arg['offset']))]\n                    gold_instance['string_role'] += [(record['type'], arg['type'], arg['text'])]\n            gold_instance_list += [gold_instance]\n        return gold_instance_list\n\n    @staticmethod\n    def load_pred_list(pred_list):\n        \"\"\"[summary]\n\n        Args:\n            pred_list (List[Dict]): List of Sentece, each sentence contains two List (offset, string) of Event List\n                [\n                    {\n                        'offset': [{'type': 'Attack', 'roles': [['Attacker', [5, 6]], ['Place', [23]], ['Target', [17]]], 'trigger': [16]}],\n                        'string': [{'roles': [['Attacker', 'John Joseph'], ['Place', 'court'], ['Target', 'himself']], 'type': 'Attack', 'trigger': 'shot'}],\n                    },\n                    ...\n                ]\n        Returns:\n            List[Dict]: List of Sentece, each sentence contains four List (offset, string) X (trigger, role) of Event List\n                [\n                    {\n                        'offset_trigger': [('Attack', (16,))],\n                        'offset_role': [('Attack', 'Attacker', (5, 6)), ('Attack', 'Place', (23,)), ('Attack', 'Target', (17,))],\n                        'string_trigger': [('Attack', 'shot')],\n                        'string_role': [('Attack', 'Attacker', 'John Joseph'), ('Attack', 'Place', 'court'), ('Attack', 'Target', 'himself')],\n                    },\n                    ...\n                ]\n        \"\"\"\n        pred_instance_list = list()\n        for pred in pred_list:\n            pred_instance = defaultdict(list)\n\n            for offset_pred in pred['offset']:\n                event_type, trigger_offset = offset_pred['type'], tuple_offset(offset_pred['trigger'])\n                pred_instance['offset_trigger'] += [(event_type, trigger_offset)]\n                for role_type, role_offset in offset_pred['roles']:\n                    pred_instance['offset_role'] += [(event_type, role_type, tuple_offset(role_offset))]\n\n            for string_pred in pred['string']:\n                event_type, trigger_string = string_pred['type'], string_pred['trigger']\n                pred_instance['string_trigger'] += [(event_type, trigger_string)]\n                for role_type, role_string in string_pred['roles']:\n                    pred_instance['string_role'] += [(event_type, role_type, role_string)]\n            pred_instance_list += [pred_instance]\n        return pred_instance_list\n\n    @staticmethod\n    def eval_instance_list(gold_instance_list, pred_instance_list, verbose=False, match_mode='normal'):\n        \"\"\"[summary]\n\n        Args:\n            gold_instance_list (List[Dict]): List of Sentece, each sentence contains Four List of Event Tuple\n                [\n                    {\n                        'offset_trigger': [('Die', (16,)), ('Convict', (30,))],\n                        'string_trigger': [('Die', 'shot'), ('Convict', 'convicted')],\n                        'offset_role': [('Die', 'Victim', (17,)), ('Die', 'Agent', (5, 6)), ('Die', 'Place', (23,))],\n                        'string_role': [('Die', 'Victim', 'himself'), ('Die', 'Agent', 'John Joseph'), ('Die', 'Place', 'court')]\n                    },\n                    ...\n                ]\n            pred_instance_list (List[Dict]): List of Sentece, each sentence contains four List (offset, string) X (trigger, role) of Event List\n                [\n                    {\n                        'offset_trigger': [('Attack', (16,))],\n                        'offset_role': [('Attack', 'Attacker', (5, 6)), ('Attack', 'Place', (23,)), ('Attack', 'Target', (17,))],\n                        'string_trigger': [('Attack', 'shot')],\n                        'string_role': [('Attack', 'Attacker', 'John Joseph'), ('Attack', 'Place', 'court'), ('Attack', 'Target', 'himself')],\n                    },\n                    ...\n                ]\n            verbose (bool, optional): [description]. Defaults to False.\n            match_mode (string, optional): [description]. Defaults to `normal`.\n\n        Returns:\n            Dict: Result of Evaluation\n                (offset, string) X (trigger, role) X (gold, pred, tp, P, R, F1)\n        \"\"\"\n        trigger_metrics = {\n                'offset': Metric(verbose=verbose, match_mode=match_mode),\n                'string': Metric(verbose=verbose, match_mode=match_mode),\n        }\n        role_metrics = {\n                'offset': Metric(verbose=verbose, match_mode=match_mode),\n                'string': Metric(verbose=verbose, match_mode=match_mode),\n        }\n\n        for pred, gold in zip(pred_instance_list, gold_instance_list):\n\n            pre_string_tp, pre_offset_tp = trigger_metrics['string'].tp, trigger_metrics['offset'].tp\n\n            for eval_key in trigger_metrics:\n                trigger_metrics[eval_key].count_instance(\n                    gold_list=gold.get(eval_key + '_trigger', []),\n                    pred_list=pred.get(eval_key + '_trigger', [])\n                )\n\n            post_string_tp, post_offset_tp = trigger_metrics['string'].tp, trigger_metrics['offset'].tp\n            if verbose and post_offset_tp - pre_offset_tp != post_string_tp - pre_string_tp:\n                warning_tp_increment(gold=gold, pred=pred, prefix='Trigger')\n\n            pre_string_tp, pre_offset_tp = role_metrics['string'].tp, role_metrics['offset'].tp\n\n            for eval_key in role_metrics:\n                role_metrics[eval_key].count_instance(\n                    gold_list=gold.get(eval_key + '_role', []),\n                    pred_list=pred.get(eval_key + '_role', [])\n                )\n\n            post_string_tp, post_offset_tp = role_metrics['string'].tp, role_metrics['offset'].tp\n            if verbose and post_offset_tp - pre_offset_tp != post_string_tp - pre_string_tp:\n                warning_tp_increment(gold=gold, pred=pred, prefix='Role')\n\n        results = dict()\n        for eval_key in trigger_metrics:\n            results.update(trigger_metrics[eval_key].compute_f1(prefix=f'{eval_key}-evt-trigger-'))\n        for eval_key in role_metrics:\n            results.update(role_metrics[eval_key].compute_f1(prefix=f'{eval_key}-evt-role-'))\n\n        return results\n\n\n\n"}
{"type": "source_file", "path": "uie/sel2record/__init__.py", "content": ""}
{"type": "source_file", "path": "uie/sel2record/record.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom asyncio.log import logger\nimport sys\nfrom typing import Tuple\nimport numpy\nimport logging\n\nlogger = logging.getLogger(\"__main__\")\n\n\ndef match_sublist(the_list, to_match):\n    \"\"\"\n    :param the_list: [1, 2, 3, 4, 5, 6, 1, 2, 4, 5]\n    :param to_match: [1, 2]\n    :return:\n        [(0, 1), (6, 7)]\n    \"\"\"\n    len_to_match = len(to_match)\n    matched_list = list()\n    for index in range(len(the_list) - len_to_match + 1):\n        if to_match == the_list[index:index + len_to_match]:\n            matched_list += [(index, index + len_to_match - 1)]\n    return matched_list\n\n\ndef check_overlap(x, y):\n    if x[0] > y[1] or y[0] > x[1]:\n        return False\n    else:\n        return True\n\n\ndef get_index_tuple(matched: Tuple[int, int]):\n    return tuple(range(matched[0], matched[1] + 1))\n\n\ndef span_to_token(text, span_to_token_strategy='space'):\n    if span_to_token_strategy == 'space':\n        return text.split(' ')\n    elif span_to_token_strategy == 'list':\n        return list(text)\n    else:\n        raise NotImplementedError(\n            f\"The span to token strategy {span_to_token_strategy} is not implemented.\")\n\n\nclass MapConfig:\n\n    def __init__(self,\n                 map_strategy: str = 'first',\n                 de_duplicate: bool = True,\n                 span_to_token: str = 'space') -> None:\n        self.map_strategy = map_strategy\n        self.de_duplicate = de_duplicate\n        self.span_to_token = span_to_token\n\n    def __repr__(self) -> str:\n        repr_list = [\n            f\"map_strategy: {self.map_strategy}\",\n            f\"de_duplicate: {self.de_duplicate}\",\n            f\"span_to_token: {self.span_to_token}\",\n        ]\n        return ', '.join(repr_list)\n\n    @staticmethod\n    def load_from_yaml(config_file):\n        import yaml\n        with open(config_file) as fin:\n            config = yaml.load(fin, Loader=yaml.FullLoader)\n        return MapConfig(\n            map_strategy=config['map_strategy'],\n            de_duplicate=config['de_duplicate'],\n            span_to_token=config['span_to_token'],\n        )\n\n\nclass Record:\n    def __init__(self, map_config) -> None:\n        self._map_config = map_config\n\n    def span_to_token(self, text):\n        return span_to_token(text, span_to_token_strategy=self._map_config.span_to_token)\n\n\nclass EntityRecord(Record):\n    \"\"\" Record for converting generated string to information record <type, span>\n    \"\"\"\n\n    @staticmethod\n    def to_string(pred_record_list):\n        entity_list = list()\n        for i, pred_record in enumerate(pred_record_list):\n            record_type, record_text = pred_record['type'], pred_record['text']\n            if record_text == \"\":\n                logger.info(f\"Empty Extraction - line {i}: {pred_record}\")\n                continue\n            entity_list += [(record_type, record_text)]\n        return entity_list\n\n    def to_offset(self, instance, tokens):\n        map_strategy_dict = {\n            'first': self.record_to_offset_first_role,\n            'closest': self.record_to_offset_closest_role,\n            'longer_first': self.record_to_offset_longer_first,\n        }\n\n        if self._map_config.map_strategy in map_strategy_dict:\n            map_function = map_strategy_dict[self._map_config.map_strategy]\n            return map_function(\n                instance=instance,\n                token_list=tokens, )\n        else:\n            raise NotImplementedError(\n                f\"The map strategy {self._map_config.map_strategy} in {self.__class__} is not implemented.\"\n            )\n\n    def record_to_offset_closest_role(\n            self,\n            instance,\n            token_list, ):\n        \"\"\"\n        Find Role's offset using closest matched with trigger word.\n        :param instance:\n        :return:\n        \"\"\"\n        return self.record_to_offset_first_role(instance, token_list=token_list)\n\n    def record_to_offset_first_role(self, instance, token_list):\n        \"\"\"\n        Find Entity's offset using first matched in the sentence.\n        :param instance:\n        :return:\n        \"\"\"\n        entity_list = list()\n\n        entity_matched_set = set()\n        for i, pred_record in enumerate(instance):\n            record_type, record_text = pred_record['type'], pred_record['text']\n            if record_text == \"\":\n                logger.info(f\"Empty Extraction - line {i}: {pred_record}\")\n                continue\n            matched_list = match_sublist(token_list, self.span_to_token(record_text))\n            for matched in matched_list:\n                if (record_type, matched) not in entity_matched_set:\n                    entity_list += [(record_type, tuple(range(matched[0], matched[1] + 1)))]\n                    entity_matched_set.add((record_type, matched))\n                    break\n\n        return entity_list\n\n    def record_to_offset_longer_first(self, instance, token_list):\n        \"\"\"\n        Find Entity's offset using first matched in the sentence.\n        :param instance:\n        :return:\n        \"\"\"\n        entity_list = list()\n\n        entity_matched_set = set()\n        for x in instance:\n            x['length'] = len(x['text'])\n        instance.sort(reverse=True, key=lambda x: x['length'])\n\n        for i, pred_record in enumerate(instance):\n            record_type, record_text = pred_record['type'], pred_record['text']\n            if record_text == \"\":\n                logger.info(f\"Empty Extraction - line {i}: {pred_record}\")\n                continue\n\n            matched_list = match_sublist(token_list, self.span_to_token(record_text))\n            for matched in matched_list:\n                flag = False\n                for _, g in entity_matched_set:\n                    if check_overlap(g, matched):\n                        flag = True\n                if flag:\n                    continue\n\n                if (record_type, matched) not in entity_matched_set:\n                    entity_list += [(record_type, tuple(range(matched[0], matched[1] + 1)))]\n                    entity_matched_set.add((record_type, matched))\n                    break\n\n        return entity_list\n\n\nclass RelationRecord(Record):\n    \"\"\" Record for converting generated string to information record\n    <type, arg1_type, arg1_span, arg2_type, arg2_span>\n    \"\"\"\n\n    def to_offset(self, instance, tokens):\n        map_strategy_dict = {\n            'first': self.record_to_offset_first_role,\n            'closest': self.record_to_offset_closest_role,\n            'longer_first': self.record_to_offset_closest_role,\n        }\n        if self._map_config.map_strategy in map_strategy_dict:\n            map_function = map_strategy_dict[self._map_config.map_strategy]\n            return map_function(\n                instance=instance,\n                token_list=tokens, )\n        else:\n            raise NotImplementedError(\n                f\"The map strategy {self._map_config.map_strategy} in {self.__class__} is not implemented.\"\n            )\n\n    @staticmethod\n    def to_string(instance):\n        relation_list = list()\n        for record in instance:\n            relation_type = record['type']\n            relation = [relation_type]\n            if len(record['roles']) < 2:\n                continue\n            for role_type, text_str in record['roles'][:2]:\n                relation += [role_type, text_str]\n            relation_list += [tuple(relation)]\n        return relation_list\n\n    def record_to_offset_first_role(self, instance, token_list):\n        \"\"\"\n        Find Role's offset using first matched in the sentence.\n        :param instance:\n        :return:\n        \"\"\"\n        relation_list = list()\n\n        for i, record in enumerate(instance):\n            relation_type = record['type']\n\n            if len(record['roles']) < 2:\n                continue\n\n            relation = [relation_type]\n            for role_type, text_str in record['roles'][:2]:\n                matched_list = match_sublist(token_list, self.span_to_token(text_str))\n                if len(matched_list) == 0:\n                    logger.info(f\"[Cannot reconstruct] - line {i}: ({relation_type}-{role_type}:{text_str}): {token_list}\")\n                    break\n                relation += [role_type, get_index_tuple(matched_list[0])]\n            if len(relation) != 5 or (self._map_config.de_duplicate and tuple(relation) in relation_list):\n                continue\n            relation_list += [tuple(relation)]\n\n        return relation_list\n\n    def record_to_offset_closest_role(self, instance, token_list):\n        \"\"\"\n        Find Role's offset using closest matched with trigger word.\n        :param instance:\n        :return:\n        \"\"\"\n        relation_list = list()\n\n        for i, record in enumerate(instance):\n            relation_type = record['type']\n\n            if len(record['roles']) < 2:\n                continue\n\n            arg1_type, arg1_text = record['roles'][0]\n            arg2_type, arg2_text = record['roles'][1]\n            arg1_matched_list = match_sublist(token_list, self.span_to_token(arg1_text))\n            arg2_matched_list = match_sublist(token_list, self.span_to_token(arg2_text))\n\n            if len(arg1_matched_list) == 0:\n                logger.info(f\"[Cannot reconstruct] - line {i}: ({relation_type}-{arg1_type}:{arg1_text}) - {token_list}\")\n                break\n            if len(arg2_matched_list) == 0:\n                logger.info(f\"[Cannot reconstruct] - line {i}: ({relation_type}-{arg2_type}:{arg2_text}) - {token_list}\")\n                break\n\n            distance_tuple = list()\n            for arg1_match in arg1_matched_list:\n                for arg2_match in arg2_matched_list:\n                    distance = abs(arg1_match[0] - arg2_match[0])\n                    distance_tuple += [(distance, arg1_match, arg2_match)]\n            distance_tuple.sort()\n\n            relation = [\n                relation_type,\n                arg1_type,\n                get_index_tuple(distance_tuple[0][1]),\n                arg2_type,\n                get_index_tuple(distance_tuple[0][2]),\n            ]\n            if self._map_config.de_duplicate and tuple(\n                    relation) in relation_list:\n                continue\n            relation_list += [tuple(relation)]\n\n        return relation_list\n\n\nclass EventRecord(Record):\n    \"\"\" Record for converting generated string to information record in predicate-arguments\n    {\n        type: pred_type,\n        trigger: predicate_span,\n        args: [(arg_type, arg_span), ...]\n    }\n    \"\"\"\n\n    def to_offset(self, instance, tokens):\n        map_strategy_dict = {\n            'first': self.record_to_offset_first_role,\n            'closest': self.record_to_offset_closest_role,\n            'longer_first': self.record_to_offset_closest_role,\n        }\n        if self._map_config.map_strategy in map_strategy_dict:\n            map_function = map_strategy_dict[self._map_config.map_strategy]\n            return map_function(\n                instance=instance,\n                token_list=tokens, )\n        else:\n            raise NotImplementedError(\n                f\"The map strategy {self._map_config.map_strategy} in {self.__class__} is not implemented.\"\n            )\n\n    @staticmethod\n    def to_string(instance):\n        \"\"\"\n        {'type': 'Justice:Appeal',\n         'trigger': 'appeal',\n         'roles': [\n            ('Adjudicator', 'court'),\n            ('Plaintiff', 'Anwar')\n            ], }\n        \"\"\"\n        return instance\n\n    def record_to_offset_first_role(self, instance, token_list):\n        \"\"\"\n        Find Role's offset using first matched in the sentence.\n        \"\"\"\n        record_list = list()\n\n        trigger_matched_set = set()\n        for i, record in enumerate(instance):\n            event_type = record['type']\n            trigger = record['trigger']\n            matched_list = match_sublist(token_list, self.span_to_token(trigger))\n\n            if len(matched_list) == 0:\n                logger.info(f\"[Cannot reconstruct] - line {i}: ({event_type}:{trigger}): {token_list}\")\n                continue\n\n            trigger_offset = None\n            for matched in matched_list:\n                if matched not in trigger_matched_set:\n                    trigger_offset = get_index_tuple(matched)\n                    trigger_matched_set.add(matched)\n                    break\n\n            # No trigger word, skip the record\n            if trigger_offset is None:\n                break\n\n            pred_record = {\n                'type': event_type,\n                'roles': [],\n                'trigger': trigger_offset\n            }\n\n            for role_type, text_str in record['roles']:\n                matched_list = match_sublist(token_list, self.span_to_token(text_str))\n                if len(matched_list) == 0:\n                    logger.info(f\"[Cannot reconstruct] - line {i}: ({event_type}-{role_type}:{text_str}): {token_list}\")\n                    continue\n                pred_record['roles'] += [(role_type,\n                                          get_index_tuple(matched_list[0]))]\n\n            record_list += [pred_record]\n\n        return record_list\n\n    def record_to_offset_closest_role(self, instance, token_list):\n        \"\"\"\n        Find Role's offset using closest matched with trigger word.\n        \"\"\"\n        record_list = list()\n\n        trigger_matched_set = set()\n        for i, record in enumerate(instance):\n            event_type = record['type']\n            trigger = record['trigger']\n            matched_list = match_sublist(token_list, self.span_to_token(trigger))\n\n            if len(matched_list) == 0:\n                logger.info(f\"[Cannot reconstruct] - line {i}: ({event_type}:{trigger}): {token_list}\")\n                continue\n\n            trigger_offset = None\n            for matched in matched_list:\n                if matched not in trigger_matched_set:\n                    trigger_offset = get_index_tuple(matched)\n                    trigger_matched_set.add(matched)\n                    break\n\n            # No trigger word, skip the record\n            if trigger_offset is None or len(trigger_offset) == 0:\n                break\n\n            pred_record = {\n                'type': event_type,\n                'roles': [],\n                'trigger': trigger_offset\n            }\n\n            for role_type, text_str in record['roles']:\n                matched_list = match_sublist(token_list, self.span_to_token(text_str))\n                if len(matched_list) == 0:\n                    logger.info(f\"[Cannot reconstruct] - line {i}: ({event_type}-{role_type}:{text_str}): {token_list}\")\n                else:\n                    abs_distances = [\n                        abs(match[0] - trigger_offset[0])\n                        for match in matched_list\n                    ]\n                    closest_index = numpy.argmin(abs_distances)\n                    pred_record['roles'] += [(\n                        role_type,\n                        get_index_tuple(matched_list[closest_index]))]\n\n            record_list += [pred_record]\n        return record_list\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/utils.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport re\n\n\ndef fix_unk_from_text(span, text, unk='<unk>'):\n    \"\"\"\n    Find span from the text to fix unk in the generated span\n    从 text 中找到 span, 修复span\n\n    Example:\n    span = \"<unk> colo e Bengo\"\n    text = \"At 159 meters above sea level , Angola International Airport is located at Ícolo e Bengo , part of Luanda Province , in Angola .\"\n\n    span = \"<unk> colo e Bengo\"\n    text = \"Ícolo e Bengo , part of Luanda Province , in Angola .\"\n\n    span = \"Arr<unk> s negre\"\n    text = \"The main ingredients of Arròs negre , which is from Spain , are white rice , cuttlefish or squid , cephalopod ink , cubanelle and cubanelle peppers . Arròs negre is from the Catalonia region .\"\n\n    span = \"colo <unk>\"\n    text = \"At 159 meters above sea level , Angola International Airport is located at e Bengo , part of Luanda Province , in Angola . coloÍ\"\n\n    span = \"Tarō As<unk>\"\n    text = \"The leader of Japan is Tarō Asō .\"\n\n    span = \"Tar<unk> As<unk>\"\n    text = \"The leader of Japan is Tarō Asō .\"\n\n    span = \"<unk>Tar As<unk>\"\n    text = \"The leader of Japan is ōTar Asō .\"\n    \"\"\"\n    if unk not in span:\n        return span\n\n    def clean_wildcard(x):\n        sp = \".*?()[]+\"\n        return re.sub(\"(\"+\"|\".join([f\"\\\\{s}\" for s in sp])+\")\", \"\\\\\\\\\\g<1>\", x)\n\n    match = r'\\s*\\S+\\s*'.join([clean_wildcard(item.strip()) for item in span.split(unk)])\n\n    result = re.search(match, text)\n\n    if not result:\n        return span\n    return result.group().strip()\n\n\ndef test_fix_unk_from_text():\n\n    span_text_list = [\n        (\"<unk> colo e Bengo\",\n         \"At 159 meters above sea level , Angola International Airport is located at Ícolo e Bengo , part of Luanda Province , in Angola .\",\n         \"Ícolo e Bengo\"),\n        (\"<unk> colo e Bengo\",\n         \"Ícolo e Bengo , part of Luanda Province , in Angola .\",\n         \"Ícolo e Bengo\"),\n        (\"Arr<unk> s negre\",\n         \"The main ingredients of Arròs negre , which is from Spain , are white rice , cuttlefish or squid , cephalopod ink , cubanelle and cubanelle peppers . Arròs negre is from the Catalonia region .\",\n         \"Arròs negre\"),\n        (\"colo <unk>\",\n         \"At 159 meters above sea level , Angola International Airport is located at e Bengo , part of Luanda Province , in Angola . coloÍ\",\n         \"coloÍ\"),\n        (\"Tarō As<unk>\", \"The leader of Japan is Tarō Asō .\", \"Tarō Asō\"),\n        (\"Tar<unk> As<unk>\", \"The leader of Japan is Tarō Asō .\", \"Tarō Asō\"),\n        (\"<unk>Tar As<unk>\", \"The leader of Japan is ōTar Asō .\", \"ōTar Asō\"),\n        (\"Atatürk Monument ( <unk> zmir )\",\n         \"The Atatürk Monument ( İzmir ) can be found in Turkey .\",\n         \"Atatürk Monument ( İzmir )\"),\n        (\"The Atatürk Monument [ <unk> zmir ]\",\n         \"The Atatürk Monument [ İzmir ] can be found in Turkey .\",\n         \"The Atatürk Monument [ İzmir ]\")\n    ]\n\n    for span, text, gold in span_text_list:\n        print(span, '|', fix_unk_from_text(span, text))\n        assert fix_unk_from_text(span, text) == gold\n\n\nif __name__ == \"__main__\":\n    test_fix_unk_from_text()\n"}
{"type": "source_file", "path": "uie/extraction/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "uie/seq2seq/constraint_decoder/spotasoc_constraint_decoder.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport os\nfrom typing import List, Dict\nfrom uie.extraction.label_tree import get_label_name_tree, get_type_role_tree\nfrom uie.extraction.constants import (\n    span_start,\n    type_start,\n    type_end,\n    null_span,\n    text_start\n)\nfrom uie.seq2seq.constraint_decoder.constraint_decoder import (\n    ConstraintDecoder,\n    find_bracket_position,\n    generated_search_src_sequence\n)\n\n\ndebug = True if 'DEBUG' in os.environ else False\n\n\n\n\nclass SpotAsocConstraintDecoder(ConstraintDecoder):\n    def __init__(self, tokenizer, type_schema, *args, **kwargs):\n        super().__init__(tokenizer, *args, **kwargs)\n        self.tree_end = self.tokenizer.convert_tokens_to_ids([span_start])[0]        # '<extra_id_5>'\n        self.type_tree = get_label_name_tree(type_schema.type_list, self.tokenizer, end_symbol=self.tree_end)\n        self.role_tree = get_label_name_tree(type_schema.role_list, self.tokenizer, end_symbol=self.tree_end)\n        self.type_role_tree = get_type_role_tree(type_schema.type_role_dict, self.role_tree, self.tokenizer, end_symbol=self.tree_end)\n        self.type_start = self.tokenizer.convert_tokens_to_ids([type_start])[0]       # '<extra_id_0>'\n        self.type_end = self.tokenizer.convert_tokens_to_ids([type_end])[0]           # '<extra_id_1>'\n        self.span_start = self.tokenizer.convert_tokens_to_ids([span_start])[0]       # '<extra_id_5>'\n        self.null_span = self.tokenizer.convert_tokens_to_ids([null_span])[0]         # '<extra_id_6>'\n        self.text_start = self.tokenizer.convert_tokens_to_ids([text_start])[0]       # '<extra_id_2>'\n\n\n    def find_type_token_ids(self, tgt_generated, special_index_token):\n        length = len(special_index_token)\n        for i in range(length - 1, -1, -1):\n            if special_index_token[i][1] == self.type_start:\n                start_number, end_number = 0, 0\n                for j in range(0, i + 1):\n                    if special_index_token[j][1] == self.type_start:\n                        start_number += 1\n                    elif special_index_token[j][1] == self.type_end:\n                        end_number += 1\n                if start_number == end_number + 2:\n                    return tgt_generated[special_index_token[i][0] + 1: special_index_token[i + 1][0]]\n        return None\n\n\n    def check_state(self, tgt_generated):\n        type_token_ids = None\n        if tgt_generated[-1] == self.tokenizer.pad_token_id:       # 当前生成的最后一个token是 <pad>\n            return 'start', -1, type_token_ids\n\n        # special_token_set = {EVENT_TYPE_LEFT, EVENT_TYPE_RIGHT}\n        special_token_set = {self.type_start, self.type_end, self.span_start}   # '<extra_id_0>' '<extra_id_1>' '<extra_id_5>'\n        special_index_token = list(filter(lambda x: x[1] in special_token_set, list(enumerate(tgt_generated))))\n\n        last_special_index, last_special_token = special_index_token[-1]    # 最后一个特殊token对应的索引和token_id\n        if len(special_index_token) == 1:\n            if last_special_token != self.type_start:\n                return 'error', 0, type_token_ids\n\n        bracket_position = find_bracket_position(tgt_generated, _type_start=self.type_start, _type_end=self.type_end)   # 分别统计当前生成中'<extra_id_0>' '<extra_id_1>'的数量\n        start_number, end_number = len(bracket_position[self.type_start]), len(bracket_position[self.type_end])\n\n        if start_number == end_number:    # '<extra_id_0>'='<extra_id_1>', 结束生成\n            return 'end_generate', -1, type_token_ids\n        if start_number == end_number + 1:    # '<extra_id_0>'='<extra_id_1>'+1, 开始生成或生成实体开端\n            state = 'start_first_generation'\n        elif start_number == end_number + 2:  # '<extra_id_0>'='<extra_id_1>'+2, 开始生成spot\n            state = 'generate_trigger'\n            if last_special_token == self.span_start:    # '<extra_id_5>', 开始生成spot的span   \n                state = 'generate_trigger_text'\n        elif start_number == end_number + 3:  # '<extra_id_0>'='<extra_id_1>'+3, 开始生成asoc\n            state = 'generate_role'\n            if last_special_token == self.span_start:    # '<extra_id_5>', 开始生成asoc的span   \n                state = 'generate_role_text'\n            else:\n                type_token_ids = self.find_type_token_ids(tgt_generated, special_index_token)\n        else:\n            state = 'error'\n        return state, last_special_index, type_token_ids\n\n    def search_prefix_tree_and_sequence(self, generated: List[int], prefix_tree: Dict, src_sentence: List[int],\n                                        end_sequence_search_tokens: List[int] = None):\n        \"\"\"\n        Generate Type Name + Text Span\n        :param generated:\n        :param prefix_tree:\n        :param src_sentence:\n        :param end_sequence_search_tokens:\n        :return:\n        \"\"\"\n        tree = prefix_tree\n        for index, token in enumerate(generated):\n            tree = tree[token]\n            is_tree_end = len(tree) == 1 and self.tree_end in tree\n\n            if is_tree_end:\n                valid_token = generated_search_src_sequence(\n                    generated=generated[index + 1:],\n                    src_sentence=src_sentence,\n                    end_sequence_search_tokens=end_sequence_search_tokens,\n                )\n                return valid_token\n\n            if self.tree_end in tree:\n                try:\n                    valid_token = generated_search_src_sequence(\n                        generated=generated[index + 1:],\n                        src_sentence=src_sentence,\n                        end_sequence_search_tokens=end_sequence_search_tokens,\n                    )\n                    return valid_token\n                except IndexError:\n                    # Still search tree\n                    continue\n\n        valid_token = list(tree.keys())\n        return valid_token\n\n    def get_state_valid_tokens(self, src_sentence, tgt_generated):\n        \"\"\"\n\n        :param src_sentence:\n        :param tgt_generated:\n        :return:\n            List[str], valid token list\n        \"\"\"\n        if self.tokenizer.eos_token_id in src_sentence:\n            src_sentence = src_sentence[:src_sentence.index(self.tokenizer.eos_token_id)]        # 将<eos>及之后(<pad>填充)的截取掉\n\n        if self.text_start in src_sentence:\n            src_sentence = src_sentence[src_sentence.index(self.text_start) + 1:]         # 将text_start及之前的截取掉\n        # 最后只剩下原始输入\n        state, index, type_token_ids = self.check_state(tgt_generated)      # 根据当前生成的token, 判断状态 ('start', -1)\n\n        print(\"State: %s\" % state) if debug else None\n\n        if state == 'error':\n            print(\"Decode Error:\")\n            print(\"Src:\", self.tokenizer.convert_ids_to_tokens(src_sentence))\n            print(\"Tgt:\", self.tokenizer.convert_ids_to_tokens(tgt_generated))\n            valid_tokens = [self.tokenizer.eos_token_id]\n\n        elif state == 'start':\n            valid_tokens = [self.type_start]     # 'start'(<pad>即bos)后面一定跟着 type_start即<extra_id_0>\n\n        elif state == 'start_first_generation':\n            valid_tokens = [self.type_start, self.type_end]       # 第一个<extra_id_0>只能对应<extra_id_0>和<extra_id_1>\n\n        elif state == 'generate_trigger':\n\n            if tgt_generated[-1] == self.type_start:    # 当前生成的最后一个token是<extra_id_0>, 返回spot tree\n                # Start Event Label\n                return list(self.type_tree.keys())\n\n            elif tgt_generated[-1] == self.type_end:     # 当前生成的最后一个token是<extra_id_1>, 返回<extra_id_0>和<extra_id_1>\n                # EVENT_TYPE_LEFT: Start a new role\n                # EVENT_TYPE_RIGHT: End this event\n                return [self.type_start, self.type_end]\n            else:\n                valid_tokens = self.search_prefix_tree(\n                    generated=tgt_generated[index + 1:],      # <extra_id_0>后第一个 token\n                    src_sentence=src_sentence,\n                    prefix_tree=self.type_tree,\n                    end_search_tokens=[self.span_start]       # 结束标记是<extra_id_5>\n                )\n\n        elif state == 'generate_trigger_text':\n            generated = tgt_generated[index + 1:]\n\n            if len(generated) > 0 and generated[-1] == self.null_span:\n                return [self.type_end, self.type_start]\n\n            valid_tokens = generated_search_src_sequence(\n                generated=generated,\n                src_sentence=src_sentence + [self.null_span],\n                end_sequence_search_tokens=[self.type_end, self.type_start],\n            )\n\n        elif state == 'generate_role':\n            if tgt_generated[-1] == self.type_start:\n                # Start Role Label\n                return list(self.type_role_tree[tuple(type_token_ids)])\n            generated = tgt_generated[index + 1:]\n            valid_tokens = self.search_type_role_tree(\n                generated=generated,\n                src_sentence=src_sentence,\n                type_token_ids=type_token_ids,\n                prefix_tree=self.type_role_tree,\n                end_search_tokens=[self.span_start]\n            )\n\n        elif state == 'generate_role_text':\n            generated = tgt_generated[index + 1:]\n\n            if len(generated) > 0 and generated[-1] == self.null_span:\n                return [self.type_end]\n\n            valid_tokens = generated_search_src_sequence(\n                generated=generated,\n                src_sentence=src_sentence + [self.null_span],\n                end_sequence_search_tokens=[self.type_end],\n            )\n\n        elif state == 'end_generate':       # 结束生成， 限制下一个token在<eos>\n            valid_tokens = [self.tokenizer.eos_token_id]\n\n        else:\n            raise NotImplementedError('State `%s` for %s is not implemented.' % (state, self.__class__))\n\n        print(\"Valid: %s\" % self.tokenizer.convert_ids_to_tokens(valid_tokens)) if debug else None\n        return valid_tokens\n\n    \n    def search_prefix_tree(self, generated: List[int], src_sentence, prefix_tree: Dict,\n                           end_search_tokens: List[int] = None):\n        \"\"\"\n        Generate Type Name + Text Span\n        :param generated:\n        :param prefix_tree:\n        :param src_sentence:\n        :param end_search_tokens:\n        :return:\n        \"\"\"\n        tree = prefix_tree\n        for _, token in enumerate(generated):\n            try:\n                tree = tree[token]\n            except KeyError:\n                print('search_prefix_tree')\n                print('tree: ', tree)\n                print('generated: ', generated)\n                print('src_sentecne: ', src_sentence)\n\n            is_tree_end = len(tree) == 1 and self.tree_end in tree\n\n            if is_tree_end:\n                return end_search_tokens\n\n        valid_token = list(tree.keys())\n        if self.tree_end in valid_token:\n            valid_token.remove(self.tree_end)\n            valid_token += end_search_tokens\n        return valid_token\n    \n\n    def search_type_role_tree(self, generated: List[int], src_sentence, type_token_ids, prefix_tree: Dict,\n                           end_search_tokens: List[int] = None):\n        \"\"\"\n        Generate Type Name + Text Span\n        :param generated:\n        :param prefix_tree:\n        :param src_sentence:\n        :param end_search_tokens:\n        :return:\n        \"\"\"\n\n        tree = prefix_tree[tuple(type_token_ids)]\n        for _, token in enumerate(generated):\n            try:\n                tree = tree[token]\n            except KeyError:\n                print('search_type_role_tree')\n                print('tree: ', tree)\n                print('generated: ', generated)\n                print('src_sentecne: ',src_sentence)\n            is_tree_end = len(tree) == 1 and self.tree_end in tree\n            if is_tree_end:\n                return end_search_tokens\n\n        valid_token = list(tree.keys())\n        if self.tree_end in valid_token:\n            valid_token.remove(self.tree_end)\n            valid_token += end_search_tokens\n        return valid_token\n\n\n\n\n\nclass SpotConstraintDecoder(SpotAsocConstraintDecoder):\n    def __init__(self, tokenizer, *args, **kwargs):\n        super().__init__(tokenizer, *args, **kwargs)\n\n    def check_state(self, tgt_generated):\n        if tgt_generated[-1] == self.tokenizer.pad_token_id:\n            return 'start', -1\n\n        special_token_set = {self.type_start, self.type_end, self.span_start}\n        special_index_token = list(filter(lambda x: x[1] in special_token_set, list(enumerate(tgt_generated))))\n\n        last_special_index, last_special_token = special_index_token[-1]\n\n        if len(special_index_token) == 1:\n            if last_special_token != self.type_start:\n                return 'error', 0\n\n        bracket_position = find_bracket_position(tgt_generated, _type_start=self.type_start, _type_end=self.type_end)\n        start_number, end_number = len(bracket_position[self.type_start]), len(bracket_position[self.type_end])\n\n        if start_number == end_number:\n            return 'end_generate', -1\n        if start_number == end_number + 1:\n            state = 'start_first_generation'\n        elif start_number == end_number + 2:\n            state = 'generate_span'\n            if last_special_token == self.span_start:\n                state = 'generate_span_text'\n        else:\n            state = 'error'\n        return state, last_special_index\n\n    def get_state_valid_tokens(self, src_sentence, tgt_generated):\n        \"\"\"\n\n        :param src_sentence:\n        :param tgt_generated:\n        :return:\n            List[str], valid token list\n        \"\"\"\n        if self.tokenizer.eos_token_id in src_sentence:\n            src_sentence = src_sentence[:src_sentence.index(self.tokenizer.eos_token_id)]\n\n        if self.text_start in src_sentence:\n            src_sentence = src_sentence[src_sentence.index(self.text_start) + 1:]\n\n        state, index = self.check_state(tgt_generated)\n\n        print(\"State: %s\" % state) if debug else None\n\n        if state == 'error':\n            print(\"Decode Error:\")\n            print(\"Src:\", self.tokenizer.convert_ids_to_tokens(src_sentence))\n            print(\"Tgt:\", self.tokenizer.convert_ids_to_tokens(tgt_generated))\n            valid_tokens = [self.tokenizer.eos_token_id]\n\n        elif state == 'start':\n            valid_tokens = [self.type_start]\n\n        elif state == 'start_first_generation':\n            valid_tokens = [self.type_start, self.type_end]\n\n        elif state == 'generate_span':\n\n            if tgt_generated[-1] == self.type_start:\n                # Start Event Label\n                return list(self.type_tree.keys())\n\n            elif tgt_generated[-1] == self.type_end:\n                raise RuntimeError('Invalid %s in %s' % (self.type_end, tgt_generated))\n\n            else:\n                valid_tokens = self.search_prefix_tree(\n                    generated=tgt_generated[index + 1:],\n                    prefix_tree=self.type_tree,\n                    src_sentence=src_sentence,\n                    end_search_tokens=[self.span_start]\n                )\n\n        elif state == 'generate_span_text':\n            generated = tgt_generated[index + 1:]\n            valid_tokens = generated_search_src_sequence(\n                generated=generated,\n                src_sentence=src_sentence + [self.null_span],\n                end_sequence_search_tokens=[self.type_end],\n            )\n\n        elif state == 'end_generate':\n            valid_tokens = [self.tokenizer.eos_token_id]\n\n        else:\n            raise NotImplementedError('State `%s` for %s is not implemented.' % (state, self.__class__))\n\n        print(\"Valid: %s\" % valid_tokens) if debug else None\n        return valid_tokens\n"}
{"type": "source_file", "path": "uie/seq2seq/constraint_decoder/constraint_decoder.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict\nimport os\nfrom typing import List\n\n\ndef match_sublist(the_list, to_match):   # the_list: input+text, to_match\n    \"\"\"\n\n    :param the_list: [1, 2, 3, 4, 5, 6, 1, 2, 4, 5]\n    :param to_match:\n        [1, 2]\n    :return:\n        [(0, 1), (6, 7)]\n    \"\"\"\n    len_to_match = len(to_match)\n    matched_list = list()\n    for index in range(len(the_list) - len_to_match + 1):\n        if to_match == the_list[index:index + len_to_match]:\n            matched_list += [(index, index + len_to_match - 1)]\n    return matched_list\n\n\ndef find_bracket_position(generated_text, _type_start, _type_end):\n    bracket_position = {_type_start: list(), _type_end: list()}\n    for index, char in enumerate(generated_text):\n        if char in bracket_position:\n            bracket_position[char] += [index]\n    return bracket_position\n\n\ndef build_sentence_tree(sentence):\n    tree = defaultdict(set)\n\n    for prev_token, next_token in zip(sentence[:-1], sentence[1:]):\n        tree[prev_token].add(next_token)\n\n    return tree\n\n\ndef generated_search_prefix_tree(generated, prefix_tree, tokenizer):\n    tree = prefix_tree\n    # Leaf is KEY_VALUE_SPLIT\n    for token in generated:\n\n        if token not in tree:\n            return [tokenizer.eos_token]\n        tree = tree[token]\n\n    return list(tree)\n\n\ndef generated_search_src_sequence(generated, src_sentence, end_sequence_search_tokens=None):\n\n    if len(generated) == 0:    # 已生成的text token为空\n        # All src tokens are valid before generation\n        return src_sentence\n\n    matched_tuples = match_sublist(the_list=src_sentence, to_match=generated)   # 已生成的text token>=1\n\n    valid_token = list()\n    for _, end in matched_tuples:\n        next_index = end + 1\n        if next_index < len(src_sentence):\n            valid_token += [src_sentence[next_index]]\n\n    if end_sequence_search_tokens:\n        valid_token += end_sequence_search_tokens     # 后一个token+<extra_id_0>+<extra_id_1>\n\n    return valid_token\n\n\nclass ConstraintDecoder:\n    def __init__(self, tokenizer, source_prefix):\n        self.tokenizer = tokenizer\n        self.source_prefix = source_prefix\n        self.source_prefix_tokenized = tokenizer.encode(source_prefix,\n                                                        add_special_tokens=False) if source_prefix else []\n\n    def get_state_valid_tokens(self, src_sentence: List[str], tgt_generated: List[str]) -> List[str]:\n        pass\n\n    def constraint_decoding(self, src_sentence, tgt_generated):\n        if self.source_prefix_tokenized:\n            # Remove Source Prefix for Generation\n            src_sentence = src_sentence[len(self.source_prefix_tokenized):]\n\n        valid_token_ids = self.get_state_valid_tokens(src_sentence.tolist(), tgt_generated.tolist())\n\n        return valid_token_ids\n"}
{"type": "source_file", "path": "uie/extraction/predict_parser/spotasoc_predict_parser.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import Counter\nimport logging\nfrom nltk.tree import ParentedTree\nimport re\nfrom typing import Tuple, List, Dict\n\n\nfrom uie.extraction.constants import (\n    null_span,\n    type_start,\n    type_end,\n    span_start,\n)\nfrom uie.extraction.predict_parser.predict_parser import PredictParser\nfrom uie.extraction.predict_parser.utils import fix_unk_from_text\n\nlogger = logging.getLogger(__name__)\n\n\nleft_bracket = '【'\nright_bracket = '】'\nbrackets = left_bracket + right_bracket\n\nsplit_bracket = re.compile(r\"<extra_id_\\d>\")\n\n\ndef add_space(text):\n    \"\"\"\n    add space between special token在 <extra_id_\\d>之间添加 ' '空格\n    <extra_id_0><extra_id_0> location<extra_id_5> Queens<extra_id_0> contains<extra_id_5> Douglaston<extra_id_1><extra_id_1><extra_id_0> location<extra_id_5> Douglaston<extra_id_0> neighborhood of<extra_id_5> Queens<extra_id_1><extra_id_1><extra_id_1> \n    ['<extra_id_0>', '<extra_id_0>', '<extra_id_5>', '<extra_id_0>', '<extra_id_5>', '<extra_id_1>', '<extra_id_1>', '<extra_id_0>', '<extra_id_5>', '<extra_id_0>', '<extra_id_5>', '<extra_id_1>', '<extra_id_1>', '<extra_id_1>'] \n    ['', ' location', ' Queens', ' contains', ' Douglaston', '', '', ' location', ' Douglaston', ' neighborhood of', ' Queens', '', '', ''] \n    findall得到所有的<extra_id_\\d>，split按照<extra_id_\\d>分割，即便两个<extra_id_\\d>直接没有也要分割出''\n    ' '.join(new_text_list)返回：\n    <extra_id_0>  <extra_id_0>  location <extra_id_5>  Queens <extra_id_0>  contains <extra_id_5>  Douglaston <extra_id_1>  <extra_id_1>  <extra_id_0>  location <extra_id_5>  Douglaston <extra_id_0>  neighborhood of <extra_id_5>  Queens <extra_id_1>  <extra_id_1>  <extra_id_1> \n    \"\"\"\n    new_text_list = list()\n    for item in zip(split_bracket.findall(text), split_bracket.split(text)[1:]):  # findall、split 两者 len 相同\n        new_text_list += item\n    return ' '.join(new_text_list) \n\n\ndef convert_bracket(text):\n    '''\n    【【 location<extra_id_5> Queens【 contains<extra_id_5> Douglaston】】【 location<extra_id_5> Douglaston【 neighborhood of<extra_id_5> Queens】】】\n    就是用【、】代替<extra_id_0>、<extra_id_1>\n    '''\n    text = add_space(text)\n    for start in [type_start]:\n        text = text.replace(start, left_bracket)\n    for end in [type_end]:\n        text = text.replace(end, right_bracket)\n    return text \n\n\ndef find_bracket_num(tree_str):\n    \"\"\"\n    Count Bracket Number (num_left - num_right), 0 indicates num_left = num_right\n    \"\"\"\n    count = 0\n    for char in tree_str:\n        if char == left_bracket:\n            count += 1\n        elif char == right_bracket:\n            count -= 1\n        else:\n            pass\n    return count\n\n\ndef check_well_form(tree_str):\n    return find_bracket_num(tree_str) == 0\n\n\ndef clean_text(tree_str):         # 清理多余的【、】(不匹配的)\n    count = 0\n    sum_count = 0\n\n    tree_str_list = tree_str.split()\n\n    for index, char in enumerate(tree_str_list):\n        if char == left_bracket:\n            count += 1\n            sum_count += 1\n        elif char == right_bracket:\n            count -= 1\n            sum_count += 1\n        else:\n            pass\n        if count == 0 and sum_count > 0:\n            return ' '.join(tree_str_list[:index + 1])\n    return ' '.join(tree_str_list)\n\n\ndef resplit_label_span(label, span, split_symbol=span_start):\n    label_span = label + ' ' + span\n\n    if split_symbol in label_span:\n        splited_label_span = label_span.split(split_symbol)\n        if len(splited_label_span) == 2:\n            return splited_label_span[0].strip(), splited_label_span[1].strip()\n\n    return label, span\n\n\ndef add_bracket(tree_str):\n    \"\"\"add right bracket to fix ill-formed expression\n    \"\"\"\n    tree_str_list = tree_str.split()\n    bracket_num = find_bracket_num(tree_str_list)\n    tree_str_list += [right_bracket] * bracket_num\n    return ' '.join(tree_str_list)\n\n\ndef get_tree_str(tree):\n    \"\"\"get str from sel tree\n    \"\"\"\n    str_list = list()\n    for element in tree:\n        if isinstance(element, str):\n            str_list += [element]\n    return ' '.join(str_list)\n\n\ndef rewrite_label_span(label, span, label_set=None, text=None):\n\n    # Invalid Type\n    if label_set and label not in label_set:\n        logger.debug('Invalid Label: %s' % label)\n        return None, None\n\n    # Fix unk using Text\n    if text is not None and '<unk>' in span:\n        span = fix_unk_from_text(span, text, '<unk>')\n\n    # Invalid Text Span\n    if text is not None and span not in text:\n        logger.debug('Invalid Text Span: %s\\n%s\\n' % (span, text))\n        return None, None\n\n    return label, span\n\n\nclass SpotAsocPredictParser(PredictParser):\n    # PredictParser的 __init__(label_constraint) 从 label_constraint 中加载了spot_set(type_list)、role_set(role_list) \n    def decode(self, gold_list, pred_list, text_list=None, raw_list=None) -> Tuple[List[Dict], Counter]:\n        \"\"\"\n        gold_list = []\n        pred_list = [\"<extra_id_0><extra_id_0> people<extra_id_5> John Wilkes Booth<extra_id_0> kill<extra_id_5> President Lincoln<extra_id_1><extra_id_1><extra_id_0> people<extra_id_5> President Lincoln<extra_id_1><extra_id_1>\", ]\n        text_list = ['John Wilkes Booth , who assassinated President Lincoln , was an actor .', ]\n\n        :param gold_list:\n        :param pred_list:\n        :param text_list:\n        :param raw_list:\n        :return:\n            dict:\n                pred_spot -> [(type1, text1), (type2, text2), ...]\n                gold_spot -> [(type1, text1), (type2, text2), ...]\n                pred_asoc -> [(spot type1, asoc type1, text1), (spot type2, asoc type2, text2), ...]\n                gold_asoc -> [(spot type1, asoc type1, text1), (spot type2, asoc type2, text2), ...]\n                pred_record -> [{'type': type1, 'text': text1, 'roles': [(spot type1, asoc type1, text1), ...]},\n                                {'type': type2, 'text': text2, 'roles': [(spot type2, asoc type2, text2), ...]},\n                                ]\n                gold_record -> [{'type': type1, 'text': text1, 'roles': [(spot type1, asoc type1, text1), ...]},\n                                {'type': type2, 'text': text2, 'roles': [(spot type2, asoc type2, text2), ...]},\n                                ]\n            Counter:\n        \"\"\"\n        counter = Counter()\n        well_formed_list = []\n\n        if gold_list is None or len(gold_list) == 0:\n            gold_list = [\"%s%s\" % (type_start, type_end)] * len(pred_list)     # [\"<extra_id_0><extra_id_1>\" * len(pred_list)]\n\n        if text_list is None:\n            text_list = [None] * len(pred_list)\n\n        if raw_list is None:\n            raw_list = [None] * len(pred_list)\n\n        for gold, pred, text, raw_data in zip(gold_list, pred_list, text_list, raw_list):\n            gold = convert_bracket(gold)   \n            pred = convert_bracket(pred)   \n            '''\n            用【、】代替<extra_id_0>、<extra_id_1>\n            '【  【  people <extra_id_5>  John Wilkes Booth 【  kill <extra_id_5>  President Lincoln 】  】  【  people <extra_id_5>  President Lincoln 】  】 '\n            '''\n            pred = clean_text(pred)     # 清理多余的【、】(不匹配的)\n\n            try:\n                gold_tree = ParentedTree.fromstring(gold, brackets=brackets)\n                '''\n                    (people\n                    <extra_id_5>\n                    John\n                    Wilkes\n                    Booth\n                    (kill <extra_id_5> President Lincoln))\n                    (people <extra_id_5> President Lincoln)\n                '''\n            except ValueError:\n                logger.warning(f\"Ill gold: {gold}\")\n                logger.warning(f\"Fix gold: {add_bracket(gold)}\")\n                gold_tree = ParentedTree.fromstring(\n                    add_bracket(gold), brackets=brackets)\n                counter.update(['gold_tree add_bracket'])\n\n            instance = {\n                'gold': gold,\n                'pred': pred,\n                'gold_tree': gold_tree,\n                'text': text,\n                'raw_data': raw_data\n            }\n\n            counter.update(['gold_tree' for _ in gold_tree])\n\n            instance['gold_spot'], instance['gold_asoc'], instance['gold_record'] = self.get_record_list(\n                sel_tree=instance[\"gold_tree\"],\n                text=instance['text']\n            )\n\n            try:\n                if not check_well_form(pred):        # 检查是否有多余未匹配的【】\n                    pred = add_bracket(pred)\n                    counter.update(['fixed'])\n\n                pred_tree = ParentedTree.fromstring(pred, brackets=brackets)\n                counter.update(['pred_tree' for _ in pred_tree])\n\n                instance['pred_tree'] = pred_tree\n                counter.update(['well-formed'])\n\n            except ValueError:\n                counter.update(['ill-formed'])\n                logger.debug('ill-formed', pred)\n                instance['pred_tree'] = ParentedTree.fromstring(      # 'pred_tree':<ParentedTree, len() = 2>\n                    left_bracket + right_bracket,\n                    brackets=brackets\n                )\n\n            instance['pred_spot'], instance['pred_asoc'], instance['pred_record'] = self.get_record_list(\n                sel_tree=instance[\"pred_tree\"],\n                text=instance['text']\n            )  \n            '''\n            'pred_spot':[('people', 'John Wilkes Booth'), ('people', 'President Lincoln')]    \n            'pred_asoc':[('people', 'kill', 'President Lincoln')]   \n            'pred_record':[{'asocs':[('kill', 'President Lincoln')], 'type':'people', 'spot':'John Wilkes Booth'}, {'asocs':[], 'type':'people', 'President Lincoln'}]\n            '''\n            well_formed_list += [instance]\n\n        return well_formed_list, counter\n\n\n\n    def get_record_list(self, sel_tree, text=None):\n        \"\"\" Convert single sel expression to extraction records\n        Args:\n            sel_tree (Tree): sel tree\n            text (str, optional): _description_. Defaults to None.\n        Returns:\n            spot_list: list of (spot_type: str, spot_span: str)\n            asoc_list: list of (spot_type: str, asoc_label: str, asoc_text: str)\n            record_list: list of {'asocs': list(), 'type': spot_type, 'spot': spot_text}\n        \"\"\"\n\n        spot_list = list()\n        asoc_list = list()\n        record_list = list()\n\n        for spot_tree in sel_tree:\n            '''\n            0 (people\n            <extra_id_5>\n            John\n            Wilkes\n            Booth\n            (kill <extra_id_5> President Lincoln))\n            1 (people <extra_id_5> President Lincoln)\n            '''\n\n            # Drop incomplete tree\n            if isinstance(spot_tree, str) or len(spot_tree) == 0:  # 这个很重要\n                continue\n\n            spot_type = spot_tree.label()    # people\n            spot_text = get_tree_str(spot_tree)      # <extra_id_5> John Wilkes Booth\n            spot_type, spot_text = resplit_label_span(\n                spot_type, spot_text)    # people, John Wilkes Booth\n            spot_type, spot_text = rewrite_label_span(\n                label=spot_type,\n                span=spot_text,\n                label_set=self.spot_set,\n                text=text\n            )     # 处理 unk \n\n            # Drop empty generated span\n            if spot_text is None or spot_text == null_span:\n                continue\n            # Drop empty generated type\n            if spot_type is None:\n                continue\n            # Drop invalid spot type\n            if self.spot_set is not None and spot_type not in self.spot_set:\n                continue\n\n            record = {'asocs': list(),\n                      'type': spot_type,\n                      'spot': spot_text}\n\n            for asoc_tree in spot_tree:\n                '''\n                (kill <extra_id_5> President Lincoln)\n                '''\n                if isinstance(asoc_tree, str) or len(asoc_tree) < 1:     # 这个很重要\n                    continue\n\n                asoc_label = asoc_tree.label()\n                asoc_text = get_tree_str(asoc_tree)\n                asoc_label, asoc_text = resplit_label_span(\n                    asoc_label, asoc_text)\n                asoc_label, asoc_text = rewrite_label_span(\n                    label=asoc_label,\n                    span=asoc_text,\n                    label_set=self.role_set,\n                    text=text\n                )\n\n                # Drop empty generated span\n                if asoc_text is None or asoc_text == null_span:\n                    continue\n                # Drop empty generated type\n                if asoc_label is None:\n                    continue\n                # Drop invalid asoc type\n                if self.role_set is not None and asoc_label not in self.role_set:\n                    continue\n\n                asoc_list += [(spot_type, asoc_label, asoc_text)]\n                record['asocs'] += [(asoc_label, asoc_text)]\n\n            spot_list += [(spot_type, spot_text)]\n            record_list += [record]\n\n        return spot_list, asoc_list, record_list\n"}
{"type": "source_file", "path": "uie/seq2seq/constraint_decoder/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom uie.seq2seq.constraint_decoder.spotasoc_constraint_decoder import (\n    SpotAsocConstraintDecoder,\n    SpotConstraintDecoder\n)\n\n\ndef get_constraint_decoder(tokenizer, type_schema, decoding_schema, task_name='event', source_prefix=None):\n    if decoding_schema == 'spotasoc':\n        if len(type_schema.role_list) == 0:\n            task_map = {\n                'entity': SpotConstraintDecoder,\n                'relation': SpotConstraintDecoder,\n                'event': SpotConstraintDecoder,\n                'record': SpotConstraintDecoder,\n            }\n        else:\n            task_map = {\n                'entity': SpotAsocConstraintDecoder,\n                'relation': SpotAsocConstraintDecoder,\n                'event': SpotAsocConstraintDecoder,\n                'record': SpotAsocConstraintDecoder,\n            }\n    else:\n        raise NotImplementedError(\n            f'Type Schema {type_schema}, Decoding Schema {decoding_schema}, Task {task_name} do not map to constraint decoder.'\n        )\n    return task_map[task_name](tokenizer=tokenizer, type_schema=type_schema, source_prefix=source_prefix)\n"}
{"type": "source_file", "path": "uie/seq2seq/constrained_seq2seq_prompt.py", "content": "import torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\nfrom typing import Union, List, Dict, Tuple, Any, Optional\n\nfrom transformers import Seq2SeqTrainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom transformers.trainer import *\nfrom uie.seq2seq.constraint_decoder import get_constraint_decoder\n\n\n\n@dataclass\nclass ConstraintSeq2SeqPromptTrainer(Seq2SeqTrainer):\n    def __init__(self, ema=None, decoding_type_schema=None, task='event', decoding_format='tree', source_prefix=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ema = ema\n        self.decoding_type_schema = decoding_type_schema\n        self.decoding_format = decoding_format\n        \n        # Label smoothing by sum token loss, different from different Label smootheing\n        if self.args.label_smoothing_factor != 0:\n            self.label_smoother = LabelSmoother(epsilon = self.args.label_smoothing_factor)\n            print('Using %s' % self.label_smoother)\n        else:\n            self.label_smoother = None\n\n        if self.args.constraint_decoding:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self.tokenizer,\n                                                             type_schema = self.decoding_type_schema,\n                                                             decoding_schema = self.decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = task)\n        else:\n            self.constraint_decoder = None\n\n        self.oom_batch = 0\n\n\n    def freeze_LM(self):\n        for name, parmas in self.model.named_parameters():\n            if 'encoder' in name or 'decoder' in name or 'shared' in name:\n                parmas.requires_grad = False\n            logger.info(f\"{name} :{parmas.requires_grad}\")\n\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            scaler = self.scaler if self.use_amp else None\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps, scaler=scaler)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        if self.use_amp:\n            with autocast():\n                loss = self.compute_loss(model, inputs)\n        else:\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.use_amp:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            loss = self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()\n\n\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if args.fp16_full_eval and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None:\n            if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n                raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n            logger.info(f\"Loading model from {resume_from_checkpoint}).\")\n\n            if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n                config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n\n            if args.deepspeed:\n                # will be resumed in deepspeed_init\n                pass\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n\n                # release memory\n                del state_dict\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = len(self.train_dataset) * args.num_train_epochs\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = self.sharded_ddp is not None and self.sharded_ddp != ShardedDDPOption.SIMPLE\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # Train!\n        num_examples = (\n            self.num_examples(train_dataloader) if train_dataset_is_sized else total_train_batch_size * args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, \"trainer_state.json\")\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, \"trainer_state.json\"))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n                )\n                if self.is_local_process_zero() and not args.disable_tqdm:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        self.state.trial_params = hp_params(trial) if trial is not None else None\n\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n\n        if args.freeze_LM:\n            self.freeze_LM()\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator) if train_dataset_is_sized else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            for step, inputs in enumerate(epoch_iterator):\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    ((step + 1) % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss += self.training_step(model, inputs)\n                else:\n                    tr_loss += self.training_step(model, inputs)\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.use_amp:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.use_amp:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed:\n                        self.lr_scheduler.step()\n                    \n                    if self.ema is not None:\n                        self.ema.update()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n\n            best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n            if os.path.exists(best_model_path):\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n            else:\n                logger.warn(\n                    f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                    \"on multiple nodes, you should activate `--save_on_each_node`.\"\n                )\n\n            if self.deepspeed:\n                self.deepspeed.load_checkpoint(\n                    self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n                )\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n        self._memory_tracker.stop_and_update_metrics(metrics)\n        self.log(metrics)\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n\n\n\n\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval=None):\n        if self.control.should_log:\n            logs: Dict[str, float] = {}\n            tr_loss_scalar = tr_loss.item()\n            # reset tr_loss to zero\n            tr_loss -= tr_loss\n\n            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n            logs[\"learning_rate\"] = self._get_learning_rate()\n\n            self._total_loss_scalar += tr_loss_scalar\n            self._globalstep_last_logged = self.state.global_step\n\n            self.log(logs)\n\n        if self.args.start_eval_step > 0 and self.state.global_step < self.args.start_eval_step:\n            return\n\n        previous_best_metric = self.state.best_metric\n        metrics = None\n        if self.control.should_evaluate:\n            metrics = self.evaluate()\n            self._report_to_hp_search(trial, epoch, metrics)\n\n        # Only save the checkpoint better than previous_best_metric\n        if self.args.save_better_checkpoint and self.args.metric_for_best_model is not None:\n            if metrics is not None and previous_best_metric is not None:\n                if metrics[self.args.metric_for_best_model] <= previous_best_metric:\n                    return\n\n        if self.control.should_save:\n            self._save_checkpoint(model, trial, metrics=metrics)\n            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n\n\n\n    def prediction_step(\n            self,\n            model: nn.Module,\n            inputs: Dict[str, Union[torch.Tensor, Any]],\n            prediction_loss_only: bool,\n            ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n\n        def prefix_allowed_tokens_fn(batch_id, sent):\n            # print(self.tokenizer.convert_ids_to_tokens(inputs['labels'][batch_id]))\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence=src_sentence,\n                                                               tgt_generated=sent)\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model=model,\n                inputs=inputs,\n                prediction_loss_only=prediction_loss_only,\n                ignore_keys=ignore_keys,\n            )\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        \n        gen_kwargs = {\n            \"max_length\": self._max_length if self._max_length is not None else self.model.config.max_length,\n            \"num_beams\": self._num_beams if self._num_beams is not None else self.model.config.num_beams,\n            \"prefix_allowed_tokens_fn\": prefix_allowed_tokens_fn if self.constraint_decoder else None,\n        }\n\n        generated_tokens = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            spot=inputs[\"spot\"],\n            asoc=inputs[\"asoc\"],\n            **gen_kwargs,\n        )\n\n        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])  # （bs, seqlen)\n        \n        with torch.no_grad():\n            if self.use_amp:\n                with autocast():\n                    outputs = model(**inputs)\n            else:\n                outputs = model(**inputs)\n            if has_labels:\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return loss, None, None\n\n        labels = inputs[\"labels\"]\n        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n\n        return loss, generated_tokens, labels\n\n\n\n\n\n@dataclass\nclass ConstraintSeq2SeqPromptSparseTrainer(Seq2SeqTrainer):\n    def __init__(self, ema=None, decoding_type_schema=None, task='event', decoding_format='tree', source_prefix=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ema = ema\n        self.decoding_type_schema = decoding_type_schema\n        self.decoding_format = decoding_format\n        \n        # Label smoothing by sum token loss, different from different Label smootheing\n        if self.args.label_smoothing_factor != 0:\n            self.label_smoother = LabelSmoother(epsilon = self.args.label_smoothing_factor)\n            print('Using %s' % self.label_smoother)\n        else:\n            self.label_smoother = None\n\n        if self.args.constraint_decoding:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self.tokenizer,\n                                                             type_schema = self.decoding_type_schema,\n                                                             decoding_schema = self.decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = task)\n        else:\n            self.constraint_decoder = None\n\n        self.oom_batch = 0\n\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs[\"labels\"]\n        outputs = model(**inputs)\n        preds = outputs[\"logits\"]\n        preds = preds.log_softmax(dim = -1)\n        _, _, hid_dim = preds.shape\n        \n        preds = preds.reshape(-1, hid_dim)\n        labels = labels.reshape(-1, 1)\n        length, _ = preds.shape\n        mask = labels.eq(-100)\n        labels = labels.masked_fill(mask, 0)\n        mask = mask.squeeze(-1)\n\n        # 负loss\n        topk = preds.topk(k=self.args.k_sparse, dim = 1)[0]\n        neg_loss = torch.logsumexp(topk, dim = 1)\n        neg_loss = neg_loss.masked_fill(mask, 0)\n\n        # 正loss\n        pos_loss = torch.gather(preds, 1, labels)[:, 0]\n        pos_loss = pos_loss.masked_fill(mask, 0)\n        \n        # 总loss\n        loss = (neg_loss - pos_loss).sum() / (length - mask.nonzero().numel())\n        return (loss, outputs) if return_outputs else loss\n\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        oom = False\n        oom_message = \"\"\n        try:\n            loss = super().training_step(model, inputs)\n            return loss\n        except RuntimeError as e:\n            if 'out of memory' in str(e):\n                oom = True\n                oom_message = str(e)\n                logger.warning(f'ran out of memory {self.oom_batch} on {self.args.local_rank}')\n                for k, v in inputs.items():\n                    print(k, v.size())\n            else:\n                raise e\n        if oom:\n            self.oom_batch += 1\n            raise RuntimeError(oom_message)\n\n\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if args.fp16_full_eval and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None:\n            if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n                raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n            logger.info(f\"Loading model from {resume_from_checkpoint}).\")\n\n            if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n                config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n\n            if args.deepspeed:\n                # will be resumed in deepspeed_init\n                pass\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n\n                # release memory\n                del state_dict\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = len(self.train_dataset) * args.num_train_epochs\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = self.sharded_ddp is not None and self.sharded_ddp != ShardedDDPOption.SIMPLE\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # Train!\n        num_examples = (\n            self.num_examples(train_dataloader) if train_dataset_is_sized else total_train_batch_size * args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, \"trainer_state.json\")\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, \"trainer_state.json\"))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n                )\n                if self.is_local_process_zero() and not args.disable_tqdm:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        self.state.trial_params = hp_params(trial) if trial is not None else None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator) if train_dataset_is_sized else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            for step, inputs in enumerate(epoch_iterator):\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    ((step + 1) % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss += self.training_step(model, inputs)\n                else:\n                    tr_loss += self.training_step(model, inputs)\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.use_amp:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.use_amp:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed:\n                        self.lr_scheduler.step()\n                    \n                    if self.ema is not None:\n                        self.ema.update()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n\n            best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n            if os.path.exists(best_model_path):\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n            else:\n                logger.warn(\n                    f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                    \"on multiple nodes, you should activate `--save_on_each_node`.\"\n                )\n\n            if self.deepspeed:\n                self.deepspeed.load_checkpoint(\n                    self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n                )\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n        self._memory_tracker.stop_and_update_metrics(metrics)\n        self.log(metrics)\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n\n\n\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval=None):\n        if self.control.should_log:\n            logs: Dict[str, float] = {}\n            tr_loss_scalar = tr_loss.item()\n            tr_loss -= tr_loss\n\n            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n            logs[\"learning_rate\"] = self._get_learning_rate()\n\n            self._total_loss_scalar += tr_loss_scalar\n            self._globalstep_last_logged = self.state.global_step\n\n            self.log(logs)\n\n        if self.args.start_eval_step > 0 and self.state.global_step < self.args.start_eval_step:\n            return\n\n        previous_best_metric = self.state.best_metric\n        metrics = None\n        if self.ema is not None:\n            self.ema.apply_shadow()\n\n\n        if self.control.should_evaluate:\n            metrics = self.evaluate()\n            self._report_to_hp_search(trial, epoch, metrics)\n\n        if self.args.save_better_checkpoint and self.args.metric_for_best_model is not None:\n            if metrics is not None and previous_best_metric is not None:\n                if metrics[self.args.metric_for_best_model] <= previous_best_metric:\n                    if self.ema is not None:\n                        self.ema.restore()\n                    return\n\n        if self.control.should_save:\n            self._save_checkpoint(model, trial, metrics=metrics)\n            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n        if self.ema is not None:\n            self.ema.restore()\n\n\n\n    def prediction_step(\n            self,\n            model: nn.Module,\n            inputs: Dict[str, Union[torch.Tensor, Any]],\n            prediction_loss_only: bool,\n            ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        def prefix_allowed_tokens_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence=src_sentence,\n                                                               tgt_generated=sent)\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model=model,\n                inputs=inputs,\n                prediction_loss_only=prediction_loss_only,\n                ignore_keys=ignore_keys,\n            )\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n        \n        gen_kwargs = {\n            \"max_length\": self._max_length if self._max_length is not None else self.model.config.max_length,\n            \"num_beams\": self._num_beams if self._num_beams is not None else self.model.config.num_beams,\n            \"prefix_allowed_tokens_fn\": prefix_allowed_tokens_fn if self.constraint_decoder else None,\n        }\n\n\n        generated_tokens = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            spot=inputs[\"spot\"],\n            asoc=inputs[\"asoc\"],\n            **gen_kwargs,\n        )\n\n        # in case the batch is shorter than max length, the output should be padded\n        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n\n        with torch.no_grad():\n            if has_labels:\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    labels = inputs[\"labels\"]\n                    outputs = model(**inputs)\n                    preds = outputs[\"logits\"]\n                    preds = preds.log_softmax(dim = -1)\n                    _, _, hid_dim = preds.shape\n                    \n                    preds = preds.reshape(-1, hid_dim)\n                    labels = labels.reshape(-1, 1)\n                    length, _ = preds.shape\n                    mask = labels.eq(-100)\n                    labels = labels.masked_fill(mask, 0)\n                    mask = mask.squeeze(-1)\n\n                    # 负loss\n                    topk = preds.topk(k=self.args.k_sparse, dim = 1)[0]\n                    neg_loss = torch.logsumexp(topk, dim = 1)\n                    neg_loss = neg_loss.masked_fill(mask, 0)\n\n                    # 正loss\n                    pos_loss = torch.gather(preds, 1, labels)[:, 0]\n                    pos_loss = pos_loss.masked_fill(mask, 0)\n                    \n                    # 总loss\n                    loss = (neg_loss - pos_loss).sum() / (length - mask.nonzero().numel())\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return loss, None, None\n\n        labels = inputs[\"labels\"]\n        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n\n        return loss, generated_tokens, labels"}
{"type": "source_file", "path": "uie/extraction/record_schema.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport json\nfrom collections import defaultdict\nfrom typing import List\n\n\nclass RecordSchema:\n    def __init__(self, type_list, role_list, type_role_dict):\n        self.type_list = type_list\n        self.role_list = role_list\n        self.type_role_dict = type_role_dict\n\n    def __repr__(self) -> str:\n        return f\"Type: {self.type_list}\\n\" \\\n            f\"Role: {self.role_list}\\n\" \\\n            f\"Map: {self.type_role_dict}\"\n\n    @staticmethod\n    def get_empty_schema():\n        return RecordSchema(type_list=list(), role_list=list(), type_role_dict=dict())\n\n    @staticmethod\n    def read_from_file(filename):\n        lines = open(filename).readlines()\n        type_list = json.loads(lines[0])\n        role_list = json.loads(lines[1])\n        type_role_dict = json.loads(lines[2])\n        return RecordSchema(type_list, role_list, type_role_dict)\n\n    def write_to_file(self, filename):\n        with open(filename, 'w') as output:\n            output.write(json.dumps(self.type_list) + '\\n')\n            output.write(json.dumps(self.role_list) + '\\n')\n            output.write(json.dumps(self.type_role_dict) + '\\n')\n\n\ndef merge_schema(schema_list: List[RecordSchema]):\n    type_set = set()\n    role_set = set()\n    type_role_dict = defaultdict(list)\n\n    for schema in schema_list:\n\n        for type_name in schema.type_list:\n            type_set.add(type_name)\n\n        for role_name in schema.role_list:\n            role_set.add(role_name)\n\n        for type_name in schema.type_role_dict:\n            type_role_dict[type_name] += schema.type_role_dict[type_name]\n\n    for type_name in type_role_dict:\n        type_role_dict[type_name] = list(set(type_role_dict[type_name]))\n\n    return RecordSchema(type_list=list(type_set),\n                        role_list=list(role_set),\n                        type_role_dict=type_role_dict,\n                        )\n"}
{"type": "source_file", "path": "uie/seq2seq/features.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom datasets import Features, Value, Sequence\n\nDatasetFeature = Features({\n    'text': Value(dtype='string', id=None),\n    'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'record': Value(dtype='string', id=None),\n    'entity': [{'type': Value(dtype='string', id=None),\n                'offset': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n                'text': Value(dtype='string', id=None)}],\n    'relation': [{'type': Value(dtype='string', id=None),\n                  'args': [{'type': Value(dtype='string', id=None),\n                            'offset': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n                            'text': Value(dtype='string', id=None)}]}],\n    'event': [{'type': Value(dtype='string', id=None),\n               'offset': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n               'text': Value(dtype='string', id=None),\n               'args': [{'type': Value(dtype='string', id=None),\n                         'offset': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n                         'text': Value(dtype='string', id=None)}]}],\n    'spot': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'asoc': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'spot_asoc': [{'span': Value(dtype='string', id=None),\n                   'label': Value(dtype='string', id=None),\n                   'asoc': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}],\n    'task': Value(dtype='string', id=None),\n})\n\n\n_processed_feature = {\n    'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n    'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n    'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n    'spots': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'asocs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'spot_asoc': [\n        {'span': Value(dtype='string', id=None),\n         'label': Value(dtype='string', id=None),\n         'asoc': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\n    ],\n    'task': Value(dtype='string', id=None),\n    'sample_prompt': Value(dtype='bool', id=None)\n}\n\n\nProcessedFeature = Features(_processed_feature)\n\n\nRecordFeature = Features({\n    'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n    'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n    'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n    'spots': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'asocs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n    'spot_asoc': [\n        {'span': Value(dtype='string', id=None),\n         'label': Value(dtype='string', id=None),\n         'asoc': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\n    ],\n    'sample_prompt': Value(dtype='bool', id=None)\n})\n"}
{"type": "source_file", "path": "uie/seq2seq/constrained_seq2seq.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom dataclasses import dataclass, field\nfrom typing import Union, List, Dict, Tuple, Any, Optional\nfrom torch.cuda.amp import autocast\n\nfrom transformers import (\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments, )\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom transformers.trainer import *\nfrom uie.seq2seq.constraint_decoder import get_constraint_decoder\n\n\n\n@dataclass\nclass ConstraintSeq2SeqTrainingArguments(Seq2SeqTrainingArguments):\n    \"\"\"\n    Parameters:\n        constraint_decoding (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether to use Constraint Decoding\n        structure_weight (:obj:`float`, `optional`, defaults to :obj:`None`):\n    \"\"\"\n    constraint_decoding: bool = field(default=False, metadata={\"help\": \"Whether to Constraint Decoding or not.\"})\n    save_better_checkpoint: bool = field(default=False, metadata={\"help\": \"Whether to save better metric checkpoint\"})\n    start_eval_step: int = field(default=0, metadata={\"help\": \"Start Evaluation after Eval Step\"})\n    k_sparse: int = field(default=10, metadata={\"help\": \"k sparse\"})\n    use_ema: bool = field(default=False, metadata={\"help\": \"Whether to use EMA\"})\n    use_sparsemax: bool = field(default=False, metadata={\"help\": \"Whether to use Sparse Max\"}) \n    use_past: bool = field(default=False, metadata={\"help\": \"Whether to use past model\"}) \n    add_null: bool = field(default=False, metadata={\"help\": \"Whether to add null to label\"}) \n    freeze_LM: bool = field(default=False, metadata={\"help\": \"freeze LM\"}) \n\n\n\nclass EMA():\n    def __init__(self, model, decay, device):\n        self.model = model\n        self.decay = decay\n        self.device = device\n        self.shadow = {}\n        self.backup = {}\n\n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone().to(self.device)\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.backup[name] = param.data.to(self.device)\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n\n\n\n\nclass ConstraintSeq2SeqTrainer(Seq2SeqTrainer):\n    def __init__(self, ema=None, decoding_type_schema=None, task='event', decoding_format='tree', source_prefix=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ema = ema\n        self.decoding_type_schema = decoding_type_schema\n        self.decoding_format = decoding_format\n        \n        # Label smoothing by sum token loss, different from different Label smootheing\n        if self.args.label_smoothing_factor != 0:\n            self.label_smoother = LabelSmoother(epsilon = self.args.label_smoothing_factor)\n            print('Using %s' % self.label_smoother)\n        else:\n            self.label_smoother = None\n\n        if self.args.constraint_decoding:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self.tokenizer,\n                                                             type_schema = self.decoding_type_schema,\n                                                             decoding_schema = self.decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = task)\n        else:\n            self.constraint_decoder = None\n\n        self.oom_batch = 0\n\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        oom = False\n        oom_message = \"\"\n        try:\n            loss = super().training_step(model, inputs)\n            return loss\n        except RuntimeError as e:\n            if 'out of memory' in str(e):\n                oom = True\n                oom_message = str(e)\n                logger.warning(f'ran out of memory {self.oom_batch} on {self.args.local_rank}')\n                for k, v in inputs.items():\n                    print(k, v.size())\n            else:\n                raise e\n        if oom:\n            self.oom_batch += 1\n            raise RuntimeError(oom_message)\n\n    \n    def freeze_LM(self):\n        for name, parmas in self.model.named_parameters():\n            if 'encoder' in name or 'decoder' in name or 'shared' in name:\n                parmas.requires_grad = False\n            logger.info(f\"{name} :{parmas.requires_grad}\")\n\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if args.fp16_full_eval and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None:\n            if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n                raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n            logger.info(f\"Loading model from {resume_from_checkpoint}).\")\n\n            if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n                config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n\n            if args.deepspeed:\n                # will be resumed in deepspeed_init\n                pass\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n\n                # release memory\n                del state_dict\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = len(self.train_dataset) * args.num_train_epochs\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = self.sharded_ddp is not None and self.sharded_ddp != ShardedDDPOption.SIMPLE\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # Train!\n        num_examples = (\n            self.num_examples(train_dataloader) if train_dataset_is_sized else total_train_batch_size * args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, \"trainer_state.json\")\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, \"trainer_state.json\"))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n                )\n                if self.is_local_process_zero() and not args.disable_tqdm:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        self.state.trial_params = hp_params(trial) if trial is not None else None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n        if args.freeze_LM:\n            self.freeze_LM()\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator) if train_dataset_is_sized else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            for step, inputs in enumerate(epoch_iterator):\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    ((step + 1) % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss += self.training_step(model, inputs)\n                else:\n                    tr_loss += self.training_step(model, inputs)\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.use_amp:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.use_amp:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed:\n                        self.lr_scheduler.step()\n                    \n                    if self.ema is not None:\n                        self.ema.update()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n\n            best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n            if os.path.exists(best_model_path):\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n            else:\n                logger.warn(\n                    f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                    \"on multiple nodes, you should activate `--save_on_each_node`.\"\n                )\n\n            if self.deepspeed:\n                self.deepspeed.load_checkpoint(\n                    self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n                )\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n        self._memory_tracker.stop_and_update_metrics(metrics)\n        self.log(metrics)\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n\n\n\n\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval=None):\n        if self.control.should_log:\n            logs: Dict[str, float] = {}\n            tr_loss_scalar = tr_loss.item()\n            tr_loss -= tr_loss\n\n            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n            logs[\"learning_rate\"] = self._get_learning_rate()\n\n            self._total_loss_scalar += tr_loss_scalar\n            self._globalstep_last_logged = self.state.global_step\n\n            self.log(logs)\n\n        if self.args.start_eval_step > 0 and self.state.global_step < self.args.start_eval_step:\n            return\n\n        previous_best_metric = self.state.best_metric\n        metrics = None\n        if self.ema is not None:\n            self.ema.apply_shadow()\n\n\n        if self.control.should_evaluate:\n            metrics = self.evaluate()\n            self._report_to_hp_search(trial, epoch, metrics)\n\n        if self.args.save_better_checkpoint and self.args.metric_for_best_model is not None:\n            if metrics is not None and previous_best_metric is not None:\n                if metrics[self.args.metric_for_best_model] <= previous_best_metric:\n                    if self.ema is not None:\n                        self.ema.restore()\n                    return\n\n        if self.control.should_save:\n            self._save_checkpoint(model, trial, metrics=metrics)\n            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n        if self.ema is not None:\n            self.ema.restore()\n\n\n\n    def prediction_step(\n            self,\n            model: nn.Module,\n            inputs: Dict[str, Union[torch.Tensor, Any]],\n            prediction_loss_only: bool,\n            ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n\n        def prefix_allowed_tokens_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model=model,\n                inputs=inputs,\n                prediction_loss_only=prediction_loss_only,\n                ignore_keys=ignore_keys,\n            )\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        gen_kwargs = {\n            \"max_length\": self._max_length if self._max_length is not None else self.model.config.max_length,\n            \"num_beams\": self._num_beams if self._num_beams is not None else self.model.config.num_beams,\n            \"prefix_allowed_tokens_fn\": prefix_allowed_tokens_fn if self.constraint_decoder else None,\n        }\n\n        generated_tokens = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask = inputs[\"attention_mask\"],\n            **gen_kwargs,\n        )\n\n        # in case the batch is shorter than max length, the output should be padded\n        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n\n        with torch.no_grad():\n            if self.use_amp:\n                with autocast():\n                    outputs = model(**inputs)\n            else:\n                outputs = model(**inputs)\n            if has_labels:\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return loss, None, None\n\n        labels = inputs[\"labels\"]\n        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n\n        return loss, generated_tokens, labels\n\n\n\n\nclass ConstraintSparseSeq2SeqTrainer(Seq2SeqTrainer):\n    def __init__(self, ema=None, decoding_type_schema=None, task='event', decoding_format='tree', source_prefix=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ema = ema\n        self.decoding_type_schema = decoding_type_schema\n        self.decoding_format = decoding_format\n        \n        # Label smoothing by sum token loss, different from different Label smootheing\n        if self.args.label_smoothing_factor != 0:\n            self.label_smoother = LabelSmoother(epsilon = self.args.label_smoothing_factor)\n            print('Using %s' % self.label_smoother)\n        else:\n            self.label_smoother = None\n\n        if self.args.constraint_decoding:\n            self.constraint_decoder = get_constraint_decoder(tokenizer = self.tokenizer,\n                                                             type_schema = self.decoding_type_schema,\n                                                             decoding_schema = self.decoding_format,\n                                                             source_prefix = source_prefix,\n                                                             task_name = task)\n        else:\n            self.constraint_decoder = None\n\n        self.oom_batch = 0\n\n\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs[\"labels\"]\n        outputs = model(**inputs)\n        preds = outputs[\"logits\"]\n        preds = preds.log_softmax(dim = -1)\n        bs, seq_len, hid_dim = preds.shape\n        \n        preds = preds.reshape(-1, hid_dim)\n        labels = labels.reshape(-1, 1)\n        length, _ = preds.shape\n        mask = labels.eq(-100)\n        labels = labels.masked_fill(mask, 0)\n        mask = mask.squeeze(-1)\n\n        # 负loss\n        topk = preds.topk(k=self.args.k_sparse, dim = 1)[0]\n        neg_loss = torch.logsumexp(topk, dim = 1)\n        neg_loss = neg_loss.masked_fill(mask, 0)\n\n        # 正loss\n        pos_loss = torch.gather(preds, 1, labels)[:, 0]\n        pos_loss = pos_loss.masked_fill(mask, 0)\n        \n        # 总loss\n        loss = (neg_loss - pos_loss).sum() / (length - mask.nonzero().numel())\n        return (loss, outputs) if return_outputs else loss\n\n\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        oom = False\n        oom_message = \"\"\n        try:\n            loss = super().training_step(model, inputs)\n            return loss\n        except RuntimeError as e:\n            if 'out of memory' in str(e):\n                oom = True\n                oom_message = str(e)\n                logger.warning(f'ran out of memory {self.oom_batch} on {self.args.local_rank}')\n                for k, v in inputs.items():\n                    print(k, v.size())\n            else:\n                raise e\n        if oom:\n            self.oom_batch += 1\n            raise RuntimeError(oom_message)\n\n\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if args.fp16_full_eval and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            set_seed(args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None:\n            if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n                raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n            logger.info(f\"Loading model from {resume_from_checkpoint}).\")\n\n            if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n                config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n\n            if args.deepspeed:\n                # will be resumed in deepspeed_init\n                pass\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n\n                # release memory\n                del state_dict\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        # Keeping track whether we can can len() on the dataset or not\n        train_dataset_is_sized = isinstance(self.train_dataset, collections.abc.Sized)\n\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n        if train_dataset_is_sized:\n            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = len(self.train_dataset) * args.num_train_epochs\n        else:\n            # see __init__. max_steps is set when the dataset has no __len__\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = self.sharded_ddp is not None and self.sharded_ddp != ShardedDDPOption.SIMPLE\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        model = self._wrap_model(self.model_wrapped)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # Train!\n        num_examples = (\n            self.num_examples(train_dataloader) if train_dataset_is_sized else total_train_batch_size * args.max_steps\n        )\n\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, \"trainer_state.json\")\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, \"trainer_state.json\"))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                logger.info(\n                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n                )\n                if self.is_local_process_zero() and not args.disable_tqdm:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n        self.state.trial_params = hp_params(trial) if trial is not None else None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                # We just need to begin an iteration to create the randomization of the sampler.\n                for _ in train_dataloader:\n                    break\n\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator) if train_dataset_is_sized else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            for step, inputs in enumerate(epoch_iterator):\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    ((step + 1) % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss += self.training_step(model, inputs)\n                else:\n                    tr_loss += self.training_step(model, inputs)\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.use_amp:\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        xm.optimizer_step(self.optimizer)\n                    elif self.use_amp:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed:\n                        self.lr_scheduler.step()\n                    \n                    if self.ema is not None:\n                        self.ema.update()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n            logger.info(\n                f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\"\n            )\n\n            best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n            if os.path.exists(best_model_path):\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                # If the model is on the GPU, it still works!\n                self._load_state_dict_in_model(state_dict)\n            else:\n                logger.warn(\n                    f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                    \"on multiple nodes, you should activate `--save_on_each_node`.\"\n                )\n\n            if self.deepspeed:\n                self.deepspeed.load_checkpoint(\n                    self.state.best_model_checkpoint, load_optimizer_states=False, load_lr_scheduler_states=False\n                )\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n        self._memory_tracker.stop_and_update_metrics(metrics)\n        self.log(metrics)\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n\n\n\n\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval=None):\n        if self.control.should_log:\n            logs: Dict[str, float] = {}\n            tr_loss_scalar = tr_loss.item()\n            tr_loss -= tr_loss\n\n            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n            logs[\"learning_rate\"] = self._get_learning_rate()\n\n            self._total_loss_scalar += tr_loss_scalar\n            self._globalstep_last_logged = self.state.global_step\n\n            self.log(logs)\n\n        if self.args.start_eval_step > 0 and self.state.global_step < self.args.start_eval_step:\n            return\n\n        previous_best_metric = self.state.best_metric\n        metrics = None\n        if self.ema is not None:\n            self.ema.apply_shadow()\n\n\n        if self.control.should_evaluate:\n            metrics = self.evaluate()\n            self._report_to_hp_search(trial, epoch, metrics)\n\n        if self.args.save_better_checkpoint and self.args.metric_for_best_model is not None:\n            if metrics is not None and previous_best_metric is not None:\n                if metrics[self.args.metric_for_best_model] <= previous_best_metric:\n                    if self.ema is not None:\n                        self.ema.restore()\n                    return\n\n        if self.control.should_save:\n            self._save_checkpoint(model, trial, metrics=metrics)\n            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n        if self.ema is not None:\n            self.ema.restore()\n\n\n\n    def prediction_step(\n            self,\n            model: nn.Module,\n            inputs: Dict[str, Union[torch.Tensor, Any]],\n            prediction_loss_only: bool,\n            ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n\n        def prefix_allowed_tokens_fn(batch_id, sent):\n            src_sentence = inputs['input_ids'][batch_id]\n            return self.constraint_decoder.constraint_decoding(src_sentence = src_sentence, tgt_generated = sent)\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model=model,\n                inputs=inputs,\n                prediction_loss_only=prediction_loss_only,\n                ignore_keys=ignore_keys,\n            )\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        gen_kwargs = {\n            \"max_length\": self._max_length if self._max_length is not None else self.model.config.max_length,\n            \"num_beams\": self._num_beams if self._num_beams is not None else self.model.config.num_beams,\n            \"prefix_allowed_tokens_fn\": prefix_allowed_tokens_fn if self.constraint_decoder else None,\n        }\n\n        generated_tokens = self.model.generate(\n            inputs[\"input_ids\"],\n            attention_mask = inputs[\"attention_mask\"],\n            **gen_kwargs,\n        )\n\n        # in case the batch is shorter than max length, the output should be padded\n        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n\n        with torch.no_grad():\n            if has_labels:\n                if self.label_smoother is not None:\n                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n                else:\n                    labels = inputs[\"labels\"]\n                    outputs = model(**inputs)\n                    preds = outputs[\"logits\"]\n                    preds = preds.log_softmax(dim = -1)\n                    _, _, hid_dim = preds.shape\n                    \n                    preds = preds.reshape(-1, hid_dim)\n                    labels = labels.reshape(-1, 1)\n                    length, _ = preds.shape\n                    mask = labels.eq(-100)\n                    labels = labels.masked_fill(mask, 0)\n                    mask = mask.squeeze(-1)\n\n                    # 负loss\n                    topk = preds.topk(k=self.args.k_sparse, dim = 1)[0]\n                    neg_loss = torch.logsumexp(topk, dim = 1)\n                    neg_loss = neg_loss.masked_fill(mask, 0)\n\n                    # 正loss\n                    pos_loss = torch.gather(preds, 1, labels)[:, 0]\n                    pos_loss = pos_loss.masked_fill(mask, 0)\n                    \n                    # 总loss\n                    loss = (neg_loss - pos_loss).sum() / (length - mask.nonzero().numel())\n            else:\n                loss = None\n\n        if self.args.prediction_loss_only:\n            return loss, None, None\n\n        labels = inputs[\"labels\"]\n        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n\n        return loss, generated_tokens, labels"}
{"type": "source_file", "path": "uie/seq2seq/modeling_t5.py", "content": "# coding=utf-8\n# Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch T5 model with prefix tuning. \"\"\"\n\n\nimport copy\nimport math\nimport os\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.checkpoint import checkpoint\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    DUMMY_INPUTS,\n    DUMMY_MASK,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_torch_fx_proxy,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import logging\nfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\nfrom transformers.models.t5.configuration_t5 import T5Config\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"T5Config\"\n_TOKENIZER_FOR_DOC = \"T5Tokenizer\"\n\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n\n\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch\n# More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n####################################################\ndef load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    tf_weights = {}\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        tf_weights[name] = array\n\n    for txt_name in names:\n        name = txt_name.split(\"/\")\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(\n                n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n                for n in name\n        ):\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            tf_weights.pop(txt_name, None)\n            continue\n        if \"_slot_\" in name[-1]:\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            tf_weights.pop(txt_name, None)\n            continue\n        pointer = model\n        array = tf_weights[txt_name]\n\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] in [\"kernel\", \"scale\", \"embedding\"]:\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"self_attention\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[0]\n            elif scope_names[0] == \"enc_dec_attention\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[1]\n            elif scope_names[0] == \"dense_relu_dense\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[2]\n            elif scope_names[0] == \"rms_norm\":\n                if hasattr(pointer, \"layer_norm\"):\n                    pointer = getattr(pointer, \"layer_norm\")\n                elif hasattr(pointer, \"final_layer_norm\"):\n                    pointer = getattr(pointer, \"final_layer_norm\")\n            elif scope_names[0] == \"scale\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            elif scope_names[0] == \"decoder\" and name[1] == \"logits\":\n                continue\n            elif scope_names[0] == \"logits\":\n                pointer = getattr(pointer, \"lm_head\")\n            elif scope_names[0] == \"wi\" and len(scope_names) > 1 and scope_names[1].isdigit():\n                pointer = getattr(pointer, f\"wi_{scope_names[1]}\")\n                continue\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(f\"Skipping {'/'.join(name)}\")\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        if scope_names[0] not in [\"kernel\", \"scale\", \"embedding\"]:\n            pointer = getattr(pointer, \"weight\")\n        if scope_names[0] != \"embedding\":\n            logger.info(f\"Transposing numpy weight of shape {array.shape} for {name}\")\n            array = np.transpose(array)\n        try:\n            assert (\n                    pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(f\"Initialize PyTorch weight {name}\")\n        pointer.data = torch.from_numpy(array.astype(np.float32))\n        tf_weights.pop(txt_name, None)\n\n    logger.info(f\"Weights not copied to PyTorch model: {', '.join(tf_weights.keys())}.\")\n    return model\n\n\n####################################################\n# PyTorch Models are constructed by sub-classing\n# - torch.nn.Module for the layers and\n# - PreTrainedModel for the models (it-self a sub-class of nn.Module)\n####################################################\nPARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n    Args:\n        device_map (:obj:`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:\n                - t5-small: 6\n                - t5-base: 12\n                - t5-large: 24\n                - t5-3b: 24\n                - t5-11b: 24\n    Example::\n            # Here is an example of a device map on a machine with 4 GPUs using t5-3b, which has a total of 24 attention modules:\n            model = T5ForConditionalGeneration.from_pretrained('t5-3b')\n            device_map = {0: [0, 1, 2],\n                         1: [3, 4, 5, 6, 7, 8, 9],\n                         2: [10, 11, 12, 13, 14, 15, 16],\n                         3: [17, 18, 19, 20, 21, 22, 23]}\n            model.parallelize(device_map)\n\"\"\"\nDEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n    Example::\n        # On a 4 GPU machine with t5-3b:\n        model = T5ForConditionalGeneration.from_pretrained('t5-3b')\n        device_map = {0: [0, 1, 2],\n                     1: [3, 4, 5, 6, 7, 8, 9],\n                     2: [10, 11, 12, 13, 14, 15, 16],\n                     3: [17, 18, 19, 20, 21, 22, 23]}\n        model.parallelize(device_map) # Splits the model across several devices\n        model.deparallelize() # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n\"\"\"\n\n\nclass T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # layer norm should always be calculated in float32\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into float16 if necessary\n        if self.weight.dtype == torch.float16:\n            hidden_states = hidden_states.to(torch.float16)\n        return self.weight * hidden_states\n\n\nclass T5DenseReluDense(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = nn.functional.relu(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass T5DenseGatedGeluDense(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.gelu_act = ACT2FN[\"gelu_new\"]\n\n    def forward(self, hidden_states):\n        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\n        hidden_linear = self.wi_1(hidden_states)\n        hidden_states = hidden_gelu * hidden_linear\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass T5LayerFF(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.feed_forward_proj == \"relu\":\n            self.DenseReluDense = T5DenseReluDense(config)\n        elif config.feed_forward_proj == \"gated-gelu\":\n            self.DenseReluDense = T5DenseGatedGeluDense(config)\n        else:\n            raise ValueError(\n                f\"{self.config.feed_forward_proj} is not supported. Choose between `relu` and `gated-gelu`\"\n            )\n\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, hidden_states):\n        forwarded_states = self.layer_norm(hidden_states)\n        forwarded_states = self.DenseReluDense(forwarded_states)\n        hidden_states = hidden_states + self.dropout(forwarded_states)\n        return hidden_states\n\n\nclass T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv\n        self.n_heads = config.num_heads\n        self.dropout = config.dropout_rate\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)\n\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n        self.pruned_heads = set()\n        self.gradient_checkpointing = getattr(config, \"gradient_checkpointing\", False)\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n        )\n        # Prune linear layers\n        self.q = prune_linear_layer(self.q, index)\n        self.k = prune_linear_layer(self.k, index)\n        self.v = prune_linear_layer(self.v, index)\n        self.o = prune_linear_layer(self.o, index, dim=1)\n        # Update hyper params\n        self.n_heads = self.n_heads - len(heads)\n        self.inner_dim = self.key_value_proj_dim * self.n_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n        \"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n            relative_position = torch.abs(relative_position)\n        else:\n            relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position < max_exact\n\n        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n        relative_postion_if_large = max_exact + (\n                torch.log(relative_position.float() / max_exact)\n                / math.log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n        ).to(torch.long)\n        relative_postion_if_large = torch.min(\n            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n        )\n\n        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n        relative_position = memory_position - context_position  # shape (query_length, key_length)\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,  # shape (query_length, key_length)\n            bidirectional=(not self.is_decoder),\n            num_buckets=self.relative_attention_num_buckets,\n        )\n        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n        values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)\n        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n        return values\n\n    def forward(\n            self,\n            hidden_states,\n            mask=None,\n            key_value_states=None,\n            position_bias=None,\n            past_key_value=None,\n            layer_head_mask=None,\n            query_length=None,\n            use_cache=False,\n            output_attentions=False,\n            prefix=None,  # TODO: Chen\n    ):\n        \"\"\"\n        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n        \"\"\"\n        # Input is (batch_size, seq_length, dim)\n        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        int_seq_length = int(seq_length)\n\n        real_seq_length = seq_length\n\n        if past_key_value is not None:\n            assert (\n                    len(past_key_value) == 2\n            ), f\"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states\"\n            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n\n        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n\n        def shape(states):\n            \"\"\"projection\"\"\"\n            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n\n        def unshape(states):\n            \"\"\"reshape\"\"\"\n            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n\n        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n            \"\"\"projects hidden states correctly to key/query states\"\"\"\n            if key_value_states is None:\n                # self-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(hidden_states))\n            elif past_key_value is None:\n                # cross-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(key_value_states))\n\n            if past_key_value is not None:\n                if key_value_states is None:\n                    # self-attn\n                    # (batch_size, n_heads, key_length, dim_per_head)\n                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n                else:\n                    # cross-attn\n                    hidden_states = past_key_value\n            return hidden_states\n\n        # get query states\n        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n\n        # get key/value states\n        key_states = project(\n            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n        )\n        value_states = project(\n            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n        )\n\n        # Prefix Concatenation should be done AFTER present_key_value_state is saved.  # TODO: Chen\n        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # TODO: Chen\n\n        # Concatenate prefix to key-value states.  # TODO: Chen\n        if prefix is not None:\n            key_states = torch.cat([prefix[\"prev_key\"], key_states], dim=2)\n            value_states = torch.cat([prefix[\"prev_value\"], value_states], dim=2)  # TODO: Chen\n\n        # compute scores\n        scores = torch.matmul(\n            query_states, key_states.transpose(3, 2)\n        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n\n        if position_bias is None:\n            if not self.has_relative_attention_bias:\n                position_bias = torch.zeros(\n                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n                )\n                if self.training and self.gradient_checkpointing:\n                    position_bias.requires_grad = True\n            else:\n                position_bias = self.compute_bias(real_seq_length, key_length)\n\n            # if key and values are already calculated\n            # we want only the last query position bias\n            if past_key_value is not None:\n                position_bias = position_bias[:, :, -int_seq_length:, :]\n\n            # Handle position_bias for prefix.  # TODO: Chen\n            if prefix is not None:\n                position_bias = torch.cat([\n                    torch.zeros((1, self.n_heads, int_seq_length, key_states.shape[2] - key_length)).to(position_bias.device),\n                    position_bias\n                ], dim=3)  # TODO: Chen\n\n            if mask is not None:\n                # Handle attention masks for prefix.  # TODO: Chen\n                if prefix is not None:\n                    assert key_states.shape[2] > mask.shape[3]  # TODO: Chen\n                    assert prefix[\"prev_key_padding_mask\"].shape[1] == key_states.shape[2] - key_length  # TODO: Chen\n                    assert mask.shape[3] == key_length\n                    mask = torch.cat([\n                        #torch.zeros((batch_size, 1, mask.shape[2], key_states.shape[2] - key_length)).to(mask.device),\n                        prefix[\"prev_key_padding_mask\"].float().unsqueeze(1).unsqueeze(2).expand(-1, -1, mask.shape[2], -1) * -10000.0,\n                        mask\n                    ], dim=3)  # TODO: Chen\n                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length + prefix_length)  # TODO: Chen\n\n        scores += position_bias\n        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n            scores\n        )  # (batch_size, n_heads, seq_length, key_length)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.dropout, training=self.training\n        )  # (batch_size, n_heads, seq_length, key_length)\n\n        # Mask heads if we want to\n        if layer_head_mask is not None:\n            attn_weights = attn_weights * layer_head_mask\n\n        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n        attn_output = self.o(attn_output)\n\n        # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # TODO: Chen\n        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n        return outputs\n\n\nclass T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_bias=None,\n            layer_head_mask=None,\n            past_key_value=None,\n            use_cache=False,\n            output_attentions=False,\n            prefix=None,  # TODO: Chen\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.SelfAttention(\n            normed_hidden_states,\n            mask=attention_mask,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=past_key_value,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            prefix=prefix,  # TODO: Chen\n        )\n        hidden_states = hidden_states + self.dropout(attention_output[0])\n        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n        return outputs\n\n\nclass T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(\n            self,\n            hidden_states,\n            key_value_states,\n            attention_mask=None,\n            position_bias=None,\n            layer_head_mask=None,\n            past_key_value=None,\n            use_cache=False,\n            query_length=None,\n            output_attentions=False,\n            prefix=None,  # TODO: Chen\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.EncDecAttention(\n            normed_hidden_states,\n            mask=attention_mask,\n            key_value_states=key_value_states,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=past_key_value,\n            use_cache=use_cache,\n            query_length=query_length,\n            output_attentions=output_attentions,\n            prefix=prefix,  # TODO: Chen\n        )\n        layer_output = hidden_states + self.dropout(attention_output[0])\n        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n        return outputs\n\n\nclass T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n        if self.is_decoder:\n            self.layer.append(T5LayerCrossAttention(config))\n\n        self.layer.append(T5LayerFF(config))\n\n    def forward(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_bias=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            encoder_decoder_position_bias=None,\n            layer_head_mask=None,\n            cross_attn_layer_head_mask=None,\n            past_key_value=None,\n            use_cache=False,\n            output_attentions=False,\n            return_dict=True,\n            encoder_prefix=None,  # TODO: Chen\n            decoder_prefix=None,  # TODO: Chen\n            cross_attn_prefix=None,  # TODO: Chen\n    ):\n\n        if past_key_value is not None:\n            assert self.is_decoder, \"Only decoder can use `past_key_values`\"\n            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n\n            if len(past_key_value) != expected_num_past_key_values:\n                raise ValueError(\n                    f\"There should be {expected_num_past_key_values} past states. \"\n                    f\"{'2 (past / key) for cross attention' if expected_num_past_key_values == 4 else ''}.\"\n                    f\"Got {len(past_key_value)} past key / value states\"\n                )\n\n            self_attn_past_key_value = past_key_value[:2]\n            cross_attn_past_key_value = past_key_value[2:]\n        else:\n            self_attn_past_key_value, cross_attn_past_key_value = None, None\n\n        self_attention_outputs = self.layer[0](\n            hidden_states,\n            attention_mask=attention_mask,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=self_attn_past_key_value,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            prefix=decoder_prefix if self.is_decoder else encoder_prefix   # TODO: Chen\n        )\n        hidden_states, present_key_value_state = self_attention_outputs[:2]\n        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n\n        # clamp inf values to enable fp16 training\n        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n        if do_cross_attention:\n            # the actual query length is unknown for cross attention\n            # if using past key value states. Need to inject it here\n            if present_key_value_state is not None:\n                query_length = present_key_value_state[0].shape[2]\n            else:\n                query_length = None\n\n            cross_attention_outputs = self.layer[1](\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                position_bias=encoder_decoder_position_bias,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n                query_length=query_length,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                prefix=cross_attn_prefix  # TODO: Chen\n            )\n            hidden_states = cross_attention_outputs[0]\n\n            # clamp inf values to enable fp16 training\n            if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n            # Combine self attn and cross attn key value states\n            if present_key_value_state is not None:\n                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n\n            # Keep cross-attention outputs and relative position weights\n            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n\n        # Apply Feed Forward layer\n        hidden_states = self.layer[-1](hidden_states)\n\n        # clamp inf values to enable fp16 training\n        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        outputs = (hidden_states,)\n\n        if use_cache:\n            outputs = outputs + (present_key_value_state,) + attention_outputs\n        else:\n            outputs = outputs + attention_outputs\n\n        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n\n\nclass T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n\n    @property\n    def dummy_inputs(self):\n        input_ids = torch.tensor(DUMMY_INPUTS)\n        input_mask = torch.tensor(DUMMY_MASK)\n        dummy_inputs = {\n            \"decoder_input_ids\": input_ids,\n            \"input_ids\": input_ids,\n            \"decoder_attention_mask\": input_mask,\n        }\n        return dummy_inputs\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        factor = self.config.initializer_factor  # Used for testing weights initialization\n        if isinstance(module, T5LayerNorm):\n            module.weight.data.fill_(factor * 1.0)\n        elif isinstance(module, (T5Model, T5ForConditionalGeneration, T5EncoderModel)):\n            # Mesh TensorFlow embeddings initialization\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n        elif isinstance(module, T5DenseReluDense):\n            # Mesh TensorFlow FF initialization\n            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n            module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n                module.wi.bias.data.zero_()\n            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5DenseGatedGeluDense):\n            module.wi_0.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n            if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n                module.wi_0.bias.data.zero_()\n            module.wi_1.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n            if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n                module.wi_1.bias.data.zero_()\n            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5Attention):\n            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n            d_model = self.config.d_model\n            key_value_proj_dim = self.config.d_kv\n            n_heads = self.config.num_heads\n            module.q.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n            module.o.weight.data.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n            if module.has_relative_attention_bias:\n                module.relative_attention_bias.weight.data.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n\n    def _shift_right(self, input_ids):\n        decoder_start_token_id = self.config.decoder_start_token_id\n        pad_token_id = self.config.pad_token_id\n\n        assert (\n                decoder_start_token_id is not None\n        ), \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n\n        # shift inputs to the right\n        if is_torch_fx_proxy(input_ids):\n            # Item assignment is not supported natively for proxies.\n            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n        else:\n            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n            shifted_input_ids[..., 0] = decoder_start_token_id\n\n        assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n        # replace possible -100 values in labels by `pad_token_id`\n        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"\n\n        return shifted_input_ids\n\n\nclass T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n\n        self.block = nn.ModuleList(\n            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n        self.init_weights()\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        # Check validity of device_map\n        self.device_map = (\n            get_device_map(len(self.block), range(torch.cuda.device_count())) if device_map is None else device_map\n        )\n        assert_device_map(self.device_map, len(self.block))\n        self.model_parallel = True\n        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n        # Load onto devices\n        for k, v in self.device_map.items():\n            for layer in v:\n                cuda_device = \"cuda:\" + str(k)\n                self.block[layer] = self.block[layer].to(cuda_device)\n\n        # Set embed_tokens to first layer\n        self.embed_tokens = self.embed_tokens.to(self.first_device)\n        # Set final layer norm to last device\n        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.model_parallel = False\n        self.device_map = None\n        self.first_device = \"cpu\"\n        self.last_device = \"cpu\"\n        for i in range(len(self.block)):\n            self.block[i] = self.block[i].to(\"cpu\")\n        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, new_embeddings):\n        self.embed_tokens = new_embeddings\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            inputs_embeds=None,\n            head_mask=None,\n            cross_attn_head_mask=None,\n            past_key_values=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            past_prompt=None,  # TODO: Chen\n    ):\n        # Model parallel\n        if self.model_parallel:\n            torch.cuda.set_device(self.first_device)\n            self.embed_tokens = self.embed_tokens.to(self.first_device)\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_ids is not None and inputs_embeds is not None:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(\n                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n\n        if inputs_embeds is None:\n            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        batch_size, seq_length = input_shape\n\n        # required mask seq length can be calculated via length of past\n        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n\n        if use_cache is True:\n            assert self.is_decoder, f\":obj:`use_cache` can only be set to `True` if {self} is used as a decoder\"\n\n        if attention_mask is None:\n            attention_mask = torch.ones(batch_size, mask_seq_length).to(inputs_embeds.device)\n        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n            encoder_seq_length = encoder_hidden_states.shape[1]\n            encoder_attention_mask = torch.ones(\n                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n            )\n\n        # initialize past_key_values with `None` if past does not exist\n        if past_key_values is None:\n            past_key_values = [None] * len(self.block)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, inputs_embeds.device)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n        cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n        present_key_value_states = () if use_cache else None\n        all_hidden_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n        position_bias = None\n        encoder_decoder_position_bias = None\n\n        hidden_states = self.dropout(inputs_embeds)\n\n        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n            layer_head_mask = head_mask[i]\n            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n\n            encoder_prefix = past_prompt[i]['encoder_prompt'] if past_prompt else None  # TODO: Chen\n            decoder_prefix = past_prompt[i]['decoder_prompt'] if past_prompt else None  # TODO: Chen\n            cross_attn_prefix = past_prompt[i]['cross_attention_prompt'] if past_prompt else None  # TODO: Chen\n\n            # Model parallel\n            if self.model_parallel:\n                torch.cuda.set_device(hidden_states.device)\n                # Ensure that attention_mask is always on the same device as hidden_states\n                if attention_mask is not None:\n                    attention_mask = attention_mask.to(hidden_states.device)\n                if position_bias is not None:\n                    position_bias = position_bias.to(hidden_states.device)\n                if encoder_hidden_states is not None:\n                    encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n                if encoder_extended_attention_mask is not None:\n                    encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n                if encoder_decoder_position_bias is not None:\n                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n                if layer_head_mask is not None:\n                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n                if cross_attn_layer_head_mask is not None:\n                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n                        \"`use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return tuple(module(*inputs, use_cache, output_attentions))\n\n                    return custom_forward\n\n                layer_outputs = checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    extended_attention_mask,\n                    position_bias,\n                    encoder_hidden_states,\n                    encoder_extended_attention_mask,\n                    encoder_decoder_position_bias,\n                    layer_head_mask,\n                    cross_attn_layer_head_mask,\n                    None,  # past_key_value is always None with gradient checkpointing\n                )\n\n                raise NotImplementedError()  # TODO: Chen\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask=extended_attention_mask,\n                    position_bias=position_bias,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_extended_attention_mask,\n                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n                    layer_head_mask=layer_head_mask,\n                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                    past_key_value=past_key_value,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    encoder_prefix=encoder_prefix,  # TODO: Chen\n                    decoder_prefix=decoder_prefix,  # TODO: Chen\n                    cross_attn_prefix=cross_attn_prefix,  # TODO: Chen\n                )\n\n            # layer_outputs is a tuple with:\n            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n            if use_cache is False:\n                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n\n            hidden_states, present_key_value_state = layer_outputs[:2]\n\n            # We share the position biases between the layers - the first layer store them\n            # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n            # (cross-attention position bias), (cross-attention weights)\n            position_bias = layer_outputs[2]\n            if self.is_decoder and encoder_hidden_states is not None:\n                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n            # append next layer key value states\n            if use_cache:\n                present_key_value_states = present_key_value_states + (present_key_value_state,)\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[3],)\n                if self.is_decoder:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n\n            # Model Parallel: If it's the last layer for that device, put things on the next device\n            if self.model_parallel:\n                for k, v in self.device_map.items():\n                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        # Add last layer\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    present_key_value_states,\n                    all_hidden_states,\n                    all_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=present_key_value_states,\n            hidden_states=all_hidden_states,\n            attentions=all_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nT5_START_DOCSTRING = r\"\"\"\n    The T5 model was proposed in `Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n    <https://arxiv.org/abs/1910.10683>`__ by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\n    Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a text-to-text\n    denoising generative setting.\n    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)\n    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n    general usage and behavior.\n    Parameters:\n        config (:class:`~transformers.T5Config`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n            weights.\n\"\"\"\n\nT5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using :class:`~transformers.T5Tokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            detail.\n            `What are input IDs? <../glossary.html#input-ids>`__\n            To know more on how to prepare :obj:`input_ids` for pretraining take a look a `T5 Training\n            <./t5.html#training>`__.\n        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n            Indices of decoder input sequence tokens in the vocabulary.\n            Indices can be obtained using :class:`~transformers.T5Tokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n            `What are decoder input IDs? <../glossary.html#decoder-input-ids>`__\n            T5 uses the :obj:`pad_token_id` as the starting token for :obj:`decoder_input_ids` generation. If\n            :obj:`past_key_values` is used, optionally only the last :obj:`decoder_input_ids` have to be input (see\n            :obj:`past_key_values`).\n            To know more on how to prepare :obj:`decoder_input_ids` for pretraining take a look at `T5 Training\n            <./t5.html#training>`__.\n        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n            also be used by default.\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules in the encoder. Mask values selected in ``[0,\n            1]``:\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        decoder_head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in ``[0,\n            1]``:\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        cross_attn_head_mask (:obj:`torch.Tensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n                Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in\n                ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):\n            Tuple consists of (:obj:`last_hidden_state`, :obj:`optional`: `hidden_states`, :obj:`optional`:\n            `attentions`) :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)` is a\n            sequence of hidden states at the output of the last layer of the encoder. Used in the cross-attention of\n            the decoder.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n            vectors than the model's internal embedding lookup matrix.\n        decoder_inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, target_sequence_length, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`decoder_input_ids` you can choose to directly pass an embedded\n            representation. If :obj:`past_key_values` is used, optionally only the last :obj:`decoder_inputs_embeds`\n            have to be input (see :obj:`past_key_values`). This is useful if you want more control over how to convert\n            :obj:`decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n            If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both unset, :obj:`decoder_inputs_embeds`\n            takes the value of :obj:`inputs_embeds`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\nT5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using :class:`~transformers.T5Tokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            detail.\n            To know more on how to prepare :obj:`input_ids` for pretraining take a look a `T5 Training\n            <./t5.html#training>`__.\n        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n            vectors than the model's internal embedding lookup matrix.\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n# Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states\" \"without any specific head on top.\",\n    T5_START_DOCSTRING,\n)\nclass T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder\\.embed_tokens\\.weight\",\n        r\"decoder\\.embed_tokens\\.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n    ]\n\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = config.num_decoder_layers\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        self.init_weights()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.decoder.parallelize(self.device_map)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.decoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.decoder = self.decoder.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            head_mask=None,\n            decoder_head_mask=None,\n            cross_attn_head_mask=None,\n            encoder_outputs=None,\n            past_key_values=None,\n            inputs_embeds=None,\n            decoder_inputs_embeds=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        r\"\"\"\n        Returns:\n        Example::\n            >>> from transformers import T5Tokenizer, T5Model\n            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n            >>> model = T5Model.from_pretrained('t5-small')\n            >>> input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n        if head_mask is not None and decoder_head_mask is None:\n            if self.config.num_layers == self.config.num_decoder_layers:\n                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n                decoder_head_mask = head_mask\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING)\nclass T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder\\.embed_tokens\\.weight\",\n        r\"decoder\\.embed_tokens\\.weight\",\n        r\"lm_head\\.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n    ]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model_dim = config.d_model\n\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = config.num_decoder_layers\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n        self.init_weights()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.decoder.parallelize(self.device_map)\n        self.lm_head = self.lm_head.to(self.decoder.first_device)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.decoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.decoder = self.decoder.to(\"cpu\")\n        self.lm_head = self.lm_head.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            head_mask=None,\n            decoder_head_mask=None,\n            cross_attn_head_mask=None,\n            encoder_outputs=None,\n            past_key_values=None,\n            inputs_embeds=None,\n            decoder_inputs_embeds=None,\n            labels=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            past_prompt=None,  # TODO: Chen\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[-100, 0, ...,\n            config.vocab_size - 1]`. All labels set to ``-100`` are ignored (masked), the loss is only computed for\n            labels in ``[0, ..., config.vocab_size]``\n        Returns:\n        Examples::\n            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small')\n            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='pt').input_ids\n            >>> labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='pt').input_ids\n            >>> outputs = model(input_ids=input_ids, labels=labels)\n            >>> loss = outputs.loss\n            >>> logits = outputs.logits\n            >>> input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you \", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> outputs = model.generate(input_ids)\n        \"\"\"\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n        if head_mask is not None and decoder_head_mask is None:\n            if self.config.num_layers == self.config.num_decoder_layers:\n                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n                decoder_head_mask = head_mask\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            # Convert encoder inputs in embeddings if needed\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                past_prompt=past_prompt,  # TODO: Chen\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n\n        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n            # get decoder inputs from shifting lm labels to the right\n            decoder_input_ids = self._shift_right(labels)\n\n        # If decoding with past key value states, only the last tokens\n        # should be given as an input\n        if past_key_values is not None:\n            assert labels is None, \"Decoder should not use cached key value states when training.\"\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids[:, -1:]\n            if decoder_inputs_embeds is not None:\n                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            past_prompt=past_prompt,  # TODO: Chen\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.encoder.first_device)\n            self.lm_head = self.lm_head.to(self.encoder.first_device)\n            sequence_output = sequence_output.to(self.lm_head.weight.device)\n\n        if self.config.tie_word_embeddings:\n            # Rescale output before projecting on vocab\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n            sequence_output = sequence_output * (self.model_dim ** -0.5)\n\n        lm_logits = self.lm_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n\n        if not return_dict:\n            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n            return ((loss,) + output) if loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n            self,\n            input_ids,\n            past=None,\n            attention_mask=None,\n            head_mask=None,\n            decoder_head_mask=None,\n            cross_attn_head_mask=None,\n            use_cache=None,\n            encoder_outputs=None,\n            **kwargs\n    ):\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"decoder_input_ids\": input_ids,\n            \"past_key_values\": past,\n            \"encoder_outputs\": encoder_outputs,\n            \"attention_mask\": attention_mask,\n            \"head_mask\": head_mask,\n            \"decoder_head_mask\": decoder_head_mask,\n            \"cross_attn_head_mask\": cross_attn_head_mask,\n            \"use_cache\": use_cache,\n            \"past_prompt\": kwargs['past_prompt'],  # TODO: Chen\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return self._shift_right(labels)\n\n    def _reorder_cache(self, past, beam_idx):\n        # if decoder past is not included in output\n        # speedy decoding is disabled and no need to reorder\n        if past is None:\n            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n            return past\n\n        reordered_decoder_past = ()\n        for layer_past_states in past:\n            # get the correct batch idx from layer past batch dim\n            # batch dim of `past` is at 2nd position\n            reordered_layer_past_states = ()\n            for layer_past_state in layer_past_states:\n                # need to set correct `past` for each of the four key / value states\n                reordered_layer_past_states = reordered_layer_past_states + (\n                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n                )\n\n            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n            assert len(reordered_layer_past_states) == len(layer_past_states)\n\n            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n        return reordered_decoder_past\n\n\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting encoder's raw hidden-states\" \"without any specific head on top.\",\n    T5_START_DOCSTRING,\n)\nclass T5EncoderModel(T5PreTrainedModel):\n    authorized_missing_keys = [\n        r\"encoder\\.embed_tokens\\.weight\",\n    ]\n\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        self.init_weights()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n\n    def get_encoder(self):\n        return self.encoder\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(T5_ENCODER_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            head_mask=None,\n            inputs_embeds=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        r\"\"\"\n        Returns:\n        Example::\n            >>> from transformers import T5Tokenizer, T5EncoderModel\n            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n            >>> model = T5EncoderModel.from_pretrained('t5-small')\n            >>> input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> outputs = model(input_ids=input_ids)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        return "}
{"type": "source_file", "path": "uie/seq2seq/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n"}
{"type": "source_file", "path": "uie/sel2record/text2spotasoc.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom collections import defaultdict\nfrom uie.extraction.constants import BaseStructureMarker\n\n\ndef convert_spot_asoc(spot_asoc_instance, structure_maker):\n    spot_instance_str_rep_list = list()\n    for spot in spot_asoc_instance:\n        spot_str_rep = [\n            spot['label'],\n            structure_maker.target_span_start,\n            spot['span'],\n        ]     \n        for asoc_label, asoc_span in spot.get('asoc', list()):\n            asoc_str_rep = [\n                structure_maker.span_start,\n                asoc_label,\n                structure_maker.target_span_start,\n                asoc_span,\n                structure_maker.span_end,\n            ]      \n            spot_str_rep += [' '.join(asoc_str_rep)]\n        spot_instance_str_rep_list += [' '.join([\n            structure_maker.record_start,\n            ' '.join(spot_str_rep),\n            structure_maker.record_end,\n        ])]  \n    target_text = ' '.join([\n        structure_maker.sent_start,\n        ' '.join(spot_instance_str_rep_list),\n        structure_maker.sent_end,\n    ])   \n    return target_text\n\n\n\n\n\ndef text2spotasoc(entities, relations, events):\n    \"\"\"Convert Entity Relation Event to Spot-Asoc\n    \"\"\"\n    spot_dict = dict()\n    asoc_dict = defaultdict(list)\n\n    def add_spot(spot):\n        spot_key = (tuple(spot[\"offset\"]), spot[\"type\"]) \n        spot_dict[spot_key] = spot  \n\n    def add_asoc(spot, asoc, tail):\n        spot_key = (tuple(spot[\"offset\"]), spot[\"type\"])\n        asoc_dict[spot_key] += [(tail[\"offset\"], tail, asoc)]   \n        \n\n    for entity in entities:\n        add_spot(spot=entity)\n\n    for relation in relations:\n        add_spot(spot=relation[\"args\"][0])\n        add_asoc(spot=relation[\"args\"][0], asoc=relation[\"type\"], tail=relation[\"args\"][1])\n\n    for event in events:\n        add_spot(spot=event)\n        for arg in event[\"args\"]:\n            add_asoc(spot=event, asoc=arg[\"type\"], tail=arg)\n\n    spot_asoc_instance = list()\n    for spot_key in sorted(spot_dict.keys()):\n        _, label = spot_key\n\n        if spot_dict[spot_key][\"text\"] == \"\":\n            continue\n\n        spot_instance = {'span': spot_dict[spot_key][\"text\"],\n                            'label': label,\n                            'asoc': list(),\n                        }\n\n        for _, tail, asoc in asoc_dict.get(spot_key, []):\n            if tail[\"text\"] == \"\":\n                continue\n            spot_instance['asoc'] += [(asoc, tail[\"text\"])]\n        spot_asoc_instance += [spot_instance]\n\n    target_text = convert_spot_asoc(\n        spot_asoc_instance,\n        structure_maker=BaseStructureMarker(),\n    )\n\n    spot_labels = set([label for _, label in spot_dict.keys()])\n    asoc_labels = set()\n    for _, asoc_list in asoc_dict.items():\n        for _, _, asoc in asoc_list:\n            asoc_labels.add(asoc)\n            \n    return target_text, list(spot_labels), list(asoc_labels), spot_asoc_instance\n\n"}
{"type": "source_file", "path": "uie_json.py", "content": "# coding=utf-8\n\nimport json\nfrom dataclasses import dataclass\nfrom io import BytesIO\nimport os\nfrom typing import Optional\n\nimport pyarrow as pa\nimport pyarrow.json as paj\n\nimport datasets\n\n\n@dataclass\nclass JsonConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for JSON.\"\"\"\n\n    features: Optional[datasets.Features] = None\n    field: Optional[str] = None\n    use_threads: bool = True\n    block_size: Optional[int] = None\n    newlines_in_values: Optional[bool] = None\n\n    @property\n    def pa_read_options(self):\n        return paj.ReadOptions(use_threads=self.use_threads, block_size=self.block_size)\n\n    @property\n    def pa_parse_options(self):\n        import pickle\n        table_schema = pickle.load(open('etc/record.dataload.schema', 'rb'))\n        print(table_schema)\n        return paj.ParseOptions(explicit_schema=table_schema, newlines_in_values=self.newlines_in_values)\n\n    @property\n    def schema(self):\n        return pa.schema(self.features.type) if self.features is not None else None\n\n\nclass Json(datasets.ArrowBasedBuilder):\n    BUILDER_CONFIG_CLASS = JsonConfig\n\n    def _info(self):\n        return datasets.DatasetInfo(features=self.config.features)\n\n    def _split_generators(self, dl_manager):\n        \"\"\"We handle string, list and dicts in datafiles\"\"\"\n        if not self.config.data_files:\n            raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\n        data_files = dl_manager.download_and_extract(self.config.data_files)\n        if isinstance(data_files, (str, list, tuple)):\n            files = data_files\n            if isinstance(files, str):\n                files = [files]\n            return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"files\": files})]\n        splits = []\n        for split_name, files in data_files.items():\n            if isinstance(files, str):\n                files = [files]\n            splits.append(datasets.SplitGenerator(name=split_name, gen_kwargs={\"files\": files}))\n        return splits\n\n    def _generate_tables(self, files):\n        for i, file in enumerate(files):\n            if self.config.field is not None:\n                with open(file, encoding=\"utf-8\") as f:\n                    dataset = json.load(f)\n\n                # We keep only the field we are interested in\n                dataset = dataset[self.config.field]\n\n                # We accept two format: a list of dicts or a dict of lists\n                if isinstance(dataset, (list, tuple)):\n                    pa_table = paj.read_json(\n                        BytesIO(\"\\n\".join(json.dumps(row) for row in dataset).encode(\"utf-8\")),\n                        read_options=self.config.pa_read_options,\n                        parse_options=self.config.pa_parse_options,\n                    )\n                else:\n                    pa_table = pa.Table.from_pydict(mapping=dataset)\n            else:\n                try:\n                    pa_table = paj.read_json(\n                        file,\n                        read_options=self.config.pa_read_options,\n                        parse_options=self.config.pa_parse_options,\n                    )\n                except pa.ArrowInvalid:\n                    with open(file, encoding=\"utf-8\") as f:\n                        dataset = json.load(f)\n                    raise ValueError(\n                        f\"Not able to read records in the JSON file at {file}. \"\n                        f\"You should probably indicate the field of the JSON file containing your records. \"\n                        f\"This JSON file contain the following fields: {str(list(dataset.keys()))}. \"\n                        f\"Select the correct one and provide it as `field='XXX'` to the dataset loading method. \"\n                    )\n            if self.config.features:\n                # Encode column if ClassLabel\n                for i, col in enumerate(self.config.features.keys()):\n                    if isinstance(self.config.features[col], datasets.ClassLabel):\n                        pa_table = pa_table.set_column(\n                            i, self.config.schema.field(col), [self.config.features[col].str2int(pa_table[col])]\n                        )\n                # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\n                # Before casting, rearrange JSON field names to match passed features schema field names order\n                pa_table = pa.Table.from_arrays(\n                    [pa_table[name] for name in self.config.features], schema=self.config.schema\n                )\n            yield i, pa_table\n"}
{"type": "source_file", "path": "uie/seq2seq/trainer_arguments.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n\n@dataclass\nclass PromptArguments:\n    src_seq_ratio: float = field(\n        default=0, metadata={\"help\": \"src seq ratio.\"}\n    )\n    length_penalty: bool = field(\n        default=True,\n        metadata={\"help\": \"length penalty.\"},\n    )\n    use_prompt: bool = field(\n        default=True,\n        metadata={\"help\": \"use prompt.\"},\n    )\n    use_ssi: bool = field(\n        default=True,\n        metadata={\"help\": \"use SSI.\"},\n    )\n    freeze_plm: bool = field(\n        default=True,\n        metadata={\"help\": \"freeze plm.\"},\n    )\n    learn_weights: bool = field(\n        default=True,\n        metadata={\"help\": \"learn weights\"},\n    )\n    prompt_len: int = field(\n        default=80,\n        metadata={\"help\": \"prompt len.\"},\n    )\n    prompt_dim: int = field(\n        default=800,\n        metadata={\"help\": \"prompt dim.\"},\n    )\n    init_prompt: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether init prompt with spot asoc tokens.\"},\n    )\n    record2: str = field(\n        default=None,\n        metadata={\"help\": \"record2\"},\n    )\n    \n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n                    \"with private models).\"\n        },\n    )\n    from_checkpoint: bool = field(\n        default=False, metadata={\"help\": \"Whether load from checkpoint to continue learning\"}\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    task: str = field(\n        default=\"summarization\",\n        metadata={\n            \"help\": \"The name of the task, should be summarization (or summarization_{dataset} for evaluating \"\n                    \"pegasus) or translation (or translation_{xx}_to_{yy}).\"\n        },\n    )\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    text_column: Optional[str] = field(\n        default='text',\n        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n    )\n    cached_trie: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"cached trie  cached_trie.pkl\"\n        },\n    )\n    record_column: Optional[str] = field(\n        default='record',\n        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"An optional input evaluation data file to evaluate the metrics (rouge/sacreblue) on \"\n                    \"(a jsonlines or csv file).\"\n        },\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"An optional input test data file to evaluate the metrics (rouge/sacreblue) on \"\n                    \"(a jsonlines or csv file).\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    preprocess: bool = field(\n        default=True,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    preprocessed_folder: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Folder to preprocessed data\"\n        },\n    )\n    max_source_length: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    max_target_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    max_prefix_length: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"The maximum prefix length.\"\n        },\n    )\n    val_max_target_length: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n                    \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n                    \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n                    \"during ``evaluate`` and ``predict``.\"\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n                    \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n                    \"efficient on GPU but very bad for TPU.\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_val_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n                    \"value if set.\"\n        },\n    )\n    max_test_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of test examples to this \"\n                    \"value if set.\"\n        },\n    )\n    num_beams: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n                    \"which is used during ``evaluate`` and ``predict``.\"\n        },\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n        },\n    )\n    source_prefix: Optional[str] = field(\n        default=\"meta\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n    )\n    meta_negative: int = field(\n        default=-1, metadata={\"help\": \"Negative Schema Number in Training.\"}\n    )\n    ordered_prompt: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to sort the spot prompt and asoc prompt or not.\"\n        },\n    )\n\n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n        if self.val_max_target_length is None:\n            self.val_max_target_length = self.max_target_length\n\n    decoding_format: str = field(\n        default='spotasoc',\n        metadata={\"help\": \"Decoding Format\"}\n    )\n    record_schema: str = field(\n        default=None, metadata={\"help\": \"The input event schema file.\"}\n    )\n    spot_noise: float = field(\n        default=0., metadata={\"help\": \"The noise rate of null spot.\"}\n    )\n    asoc_noise: float = field(\n        default=0., metadata={\"help\": \"The noise rate of null asoc.\"}\n    )\n    other_ratio: float = field(\n        default=0., metadata={\"help\": \"The noise rate of null asoc.\"}\n    )\n    meta_positive_rate: float = field(\n        default=1., metadata={\"help\": \"The keep rate of positive spot.\"}\n    )\n    negative_ratio: float = field(\n        default=0.7, metadata={\"help\": \"The keep rate of negative spot or asoc.\"}\n    )\n    task_name: str = field(\n        default='',\n    )\n"}
{"type": "source_file", "path": "uie/seq2seq/t5_bert_tokenizer.py", "content": "#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\nimport logging\nfrom typing import Optional, List\n\nfrom transformers import BertTokenizer\n\nlogger = logging.getLogger(__name__)\n\n\nclass T5BertTokenizer(BertTokenizer):\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(self,\n                 vocab_file,\n                 do_lower_case=False,\n                 do_basic_tokenize=True,\n                 never_split=None,\n                 unk_token=\"<unk>\",\n                 sep_token=None,\n                 pad_token=\"<pad>\",\n                 cls_token=None,\n                 mask_token=None,\n                 space_token=\"<space>\",\n                 tokenize_chinese_chars=True,\n                 strip_accents=None,\n                 **kwargs):\n        super().__init__(\n            vocab_file=vocab_file,\n            do_lower_case=do_lower_case,\n            do_basic_tokenize=do_basic_tokenize,\n            never_split=never_split,\n            unk_token=unk_token,\n            sep_token=sep_token,\n            pad_token=pad_token,\n            cls_token=cls_token,\n            mask_token=mask_token,\n            tokenize_chinese_chars=tokenize_chinese_chars,\n            strip_accents=strip_accents,\n            **kwargs, )\n\n        self._space_token = space_token\n\n    def get_vocab(self):\n        vocab = {\n            self.convert_ids_to_tokens(i): i\n            for i in range(self.vocab_size)\n        }\n        vocab.update(self.added_tokens_encoder)\n        return vocab\n\n    def tokenize(self, text):\n        import re\n        # Remove space between <extra_id_*> <spot> <asoc>\n        split_bracket = re.compile(\n            r\"\\s*<extra_id_\\d>\\s*|\\s*<spot>\\s*|\\s*<asoc>\\s*\")\n\n        if len(split_bracket.split(text)) > 1:\n            new_text_list = [split_bracket.split(text)[0]]\n            for item in zip(\n                    split_bracket.findall(text), split_bracket.split(text)[1:]):\n                new_text_list += [item[0].strip(), item[1]]\n            text = \"\".join(new_text_list)\n        text = text.replace(' ', self._space_token)\n        return super().tokenize(text)\n\n    def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:\n        \"\"\"Do not add eos again if user already added it.\"\"\"\n        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:\n            logging.warn(\n                f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n            )\n            return token_ids\n        else:\n            return token_ids + [self.eos_token_id]\n\n    def build_inputs_with_special_tokens(\n            self, token_ids_0: List[int],\n            token_ids_1: Optional[List[int]]=None) -> List[int]:\n        \"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n        adding special tokens. A sequence has the following format:\n\n        - single sequence: ``X </s>``\n        - pair of sequences: ``A </s> B </s>``\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"\n        token_ids_0 = self._add_eos_if_not_present(token_ids_0)\n        if token_ids_1 is None:\n            return token_ids_0\n        else:\n            token_ids_1 = self._add_eos_if_not_present(token_ids_1)\n            return token_ids_0 + token_ids_1\n\n    def _decode(self,\n                token_ids: List[int],\n                skip_special_tokens: bool = False,\n                **kwargs) -> str:\n        tokens = self.convert_ids_to_tokens(\n            token_ids, skip_special_tokens=skip_special_tokens)\n\n        # Fix '##' subtoken\n        tokens = [x.lstrip('#') if x.startswith(\"##\") else x for x in tokens]\n\n        x_str = \"\".join(tokens)\n        x_str = x_str.replace(' ', '')\n        x_str = x_str.replace(self._space_token, ' ')\n        return x_str\n"}
{"type": "source_file", "path": "uie/extraction/extraction_metrics.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom typing import List\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.predict_parser import get_predict_parser, PredictParser\nfrom uie.extraction.scorer import Metric, RecordMetric, OrderedRecordMetric\n\n\ndef eval_pred(predict_parser: PredictParser, gold_list, pred_list, text_list=None, raw_list=None):\n    well_formed_list, counter = predict_parser.decode(\n        gold_list, pred_list, text_list, raw_list\n    )\n\n    spot_metric = Metric()\n    asoc_metric = Metric()\n    record_metric = RecordMetric()\n    ordered_record_metric = OrderedRecordMetric()\n\n    for instance in well_formed_list:\n        spot_metric.count_instance(instance['gold_spot'], instance['pred_spot'])\n        asoc_metric.count_instance(instance['gold_asoc'], instance['pred_asoc'])\n        record_metric.count_instance(instance['gold_record'], instance['pred_record'])\n        ordered_record_metric.count_instance(instance['gold_record'], instance['pred_record'])\n\n    spot_result = spot_metric.compute_f1(prefix='spot-')\n    asoc_result = asoc_metric.compute_f1(prefix='asoc-')\n    record_result = record_metric.compute_f1(prefix='record-')\n    ordered_record_result = ordered_record_metric.compute_f1(prefix='ordered-record-')\n\n    overall_f1 = spot_result.get('spot-F1', 0.) + asoc_result.get('asoc-F1', 0.)\n    # print(counter)\n    result = {'overall-F1': overall_f1}\n    result.update(spot_result)\n    result.update(asoc_result)\n    result.update(record_result)\n    result.update(ordered_record_result)\n    result.update(counter)\n    return result\n\n\ndef get_extract_metrics(pred_lns: List[str], tgt_lns: List[str], label_constraint: RecordSchema, decoding_format='tree'):\n    predict_parser = get_predict_parser(decoding_schema=decoding_format, label_constraint=label_constraint)\n    return eval_pred(\n        predict_parser=predict_parser,\n        gold_list=tgt_lns,\n        pred_list=pred_lns\n    )\n"}
{"type": "source_file", "path": "uie/seq2seq/data_collator/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n\nfrom uie.seq2seq.data_collator.meta_data_collator import (\n    DataCollatorForMetaSeq2Seq,\n    DynamicSSIGenerator,\n    PromptForMetaSeq2Seq,\n    PromptSSIGenerator,\n)\n\n\n\n__all__ = [\n    'DataCollatorForMetaSeq2Seq',\n    'DynamicSSIGenerator',\n    'HybirdDataCollator',\n    'DataCollatorForT5MLM',\n    'PromptForMetaSeq2Seq',\n    'PromptSSIGenerator',\n]\n"}
{"type": "source_file", "path": "uie/seq2seq/data_collator/meta_data_collator.py", "content": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nfrom itertools import count\nimport os\nfrom dataclasses import dataclass\nimport torch\nimport logging\nimport random\nimport math\nfrom typing import Optional, Union\nfrom collections import OrderedDict\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\nfrom transformers.file_utils import PaddingStrategy\n\nfrom uie.extraction.record_schema import RecordSchema\nfrom uie.extraction.constants import BaseStructureMarker, text_start, type_start, type_end, spot_prompt, asoc_prompt, span_start, null_span\nfrom uie.extraction.utils import convert_to_record_function\nfrom uie.extraction.noiser.spot_asoc_noiser import SpotAsocNoiser\n\n\nlogger = logging.getLogger(\"__main__\")\n\n\nclass DynamicSSIGenerator():\n    \"\"\"\n    Sample negative spot and asoc to construct SSI, meta schema需要添加一些negative spot和asoc\n    \"\"\"\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, schema: RecordSchema, negative_list=[], positive_rate=1, spot_negative=5, asoc_negative=5, other_ratio=0.3, ordered_prompt=False, task_name=None) -> None:\n        self.spot_dict = self.get_ordered_dict(schema.type_list, tokenizer)    # {'aspect':[2663], ...}\n        self.asoc_dict = self.get_ordered_dict(schema.role_list, tokenizer)    # {'positive':[1465], ...}\n        self.spot_list = list(self.spot_dict.keys())\n        self.asoc_list = list(self.asoc_dict.keys())\n        self.spot_prompt = tokenizer.get_vocab()[spot_prompt]    # spot_prompt 是 '<spot>'，从constants里导入的\n        self.asoc_prompt = tokenizer.get_vocab()[asoc_prompt]\n        self.text_start = tokenizer.get_vocab()[text_start]\n        self.positive_rate = positive_rate if positive_rate > 0 and positive_rate < 1 else 1\n        self.spot_negative = spot_negative\n        self.asoc_negative = asoc_negative\n        self.ordered_prompt = ordered_prompt\n        self.task_name = task_name\n\n    @staticmethod\n    def get_ordered_dict(schema_name_list, tokenizer):\n        schema_ordered_dict = OrderedDict()\n        for name in schema_name_list:\n            schema_ordered_dict[name] = tokenizer.encode(name, add_special_tokens=False)\n        return schema_ordered_dict\n\n    @staticmethod\n    def sample_negative(postive, candidates, k=5):\n        if k < 0:\n            k = len(candidates)\n        negative_set = set()\n        for index in torch.randperm(len(candidates))[:k].tolist():\n            negative = candidates[index]\n            if negative not in postive:\n                negative_set.add(negative)\n        return list(negative_set)\n\n    def sample_spot(self, positive):\n        \"\"\" Sample spot\n        \"\"\"\n        if self.task_name == 'relation':\n            negative_spot = self.sample_negative(postive=positive, candidates=self.spot_list, k=self.spot_negative)\n        else:\n            negative_spot = self.sample_negative(postive=positive, candidates=self.spot_list, k=random.randint(self.spot_negative,len(self.spot_list)))\n        prefix_spot_candidates = positive + negative_spot\n\n        converted_spot_prefix = self.convert_prefix(\n            candidates=prefix_spot_candidates,\n            prompt=self.spot_prompt,\n            mapper=self.spot_dict,\n            ordered_prompt=self.ordered_prompt,\n        )\n\n        return converted_spot_prefix, positive, negative_spot\n\n    def sample_asoc(self, positive):\n        \"\"\" Sample Asoc\n        \"\"\"\n        if self.task_name == 'relation':\n            negative_asoc = self.sample_negative(postive=positive, candidates=self.asoc_list, k=random.randint(self.asoc_negative,len(self.asoc_list)))\n        else:\n            negative_asoc = self.sample_negative(postive=positive, candidates=self.asoc_list, k=self.asoc_negative)\n        prefix_asoc_candidates = positive + negative_asoc\n        converted_asoc_prefix = self.convert_prefix(\n            candidates=prefix_asoc_candidates,\n            prompt=self.asoc_prompt,\n            mapper=self.asoc_dict,\n            ordered_prompt=self.ordered_prompt,\n        )\n        return converted_asoc_prefix, positive, negative_asoc\n\n    def full_spot(self, shuffle=False):\n        # Random Prompt + Shuffle\n        if not self.ordered_prompt and shuffle:\n            ordered_prompt = False\n        else:\n            ordered_prompt = True\n        return self.convert_prefix(\n            candidates=self.spot_list,\n            prompt=self.spot_prompt,\n            mapper=self.spot_dict,\n            ordered_prompt=ordered_prompt,\n        )\n\n    def full_asoc(self, shuffle=False):\n        # Random Prompt + Shuffle\n        if not self.ordered_prompt and shuffle:\n            ordered_prompt = False\n        else:\n            ordered_prompt = True\n        return self.convert_prefix(\n            candidates=self.asoc_list,\n            prompt=self.asoc_prompt,\n            mapper=self.asoc_dict,\n            ordered_prompt=ordered_prompt,\n        )\n\n    def full_null(self, negative_spot, negative_asoc):\n        full_null_text = []\n        if self.task_name == 'relation':\n            for asoc in negative_asoc:\n                asoc_str_rep = ' '.join([\n                    type_start,\n                    asoc,\n                    span_start,\n                    null_span,\n                    type_end,\n                ])\n                full_null_text.append(asoc_str_rep)\n        else:\n            for spot in negative_spot:\n                spot_str_rep = ' '.join([\n                    type_start,\n                    spot,\n                    span_start,\n                    null_span,\n                    type_end,\n                ])\n                full_null_text.append(spot_str_rep)    \n        return ' '.join(full_null_text)\n\n\n\n\n\n    @staticmethod\n    def convert_prefix(candidates, prompt, mapper, ordered_prompt=True):\n        prefix = list()\n        if ordered_prompt:\n            candidate_sorted = sorted([(candidate, index) for index, candidate in enumerate(candidates)])\n            index_list = [index for _, index in candidate_sorted]\n        else:\n            index_list = torch.randperm(len(candidates)).tolist()\n\n        for index in index_list:\n            prefix += [prompt]\n            prefix += mapper[candidates[index]]\n        return prefix\n\n\n\n\n@dataclass\nclass DataCollatorForMetaSeq2Seq:\n    tokenizer: PreTrainedTokenizerBase\n    negative_sampler: DynamicSSIGenerator\n    model: Optional[PreTrainedModel] = None\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    max_target_length: Optional[int] = None\n    max_prefix_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    spot_asoc_nosier: SpotAsocNoiser = None\n    decoding_format: str = 'spotasoc'\n    add_null: bool = False\n    use_ssi: bool = True\n    count : int = 10\n\n    def __call__(self, features):\n        \"\"\" Make Meta Schema Batch\n        Args:\n            features (Dict): [description]\n                - sample_prompt: indicates sample_prompt example, need pop after call\n                - spots (List[str]): List of spots in this sentence, need pop after call\n                - asocs (List[str]): List of asocs in this sentence, need pop after call\n                - input_ids\n                - attention_mask\n                - labels\n\n        Returns:\n        \"\"\"\n        for feature in features:\n\n            sample_prompt = feature['sample_prompt']\n            if self.count > 0:\n                logger.info(f\"{self.count}/10\")\n                logger.info(f\"feature['input_ids']: {self.tokenizer.convert_ids_to_tokens(feature['input_ids'])}\")   # feature['input_ids']: Parts of the Pennsylvania Turnpike were closed ; so too was the Baseball Hall of Fame and Museum in Cooperstown, N.Y. Then there was the house that was spotted drifting down the Susquehanna River in New York -- on fire for a while, it seemed. \"</s>\n                logger.info(f\"feature['labels']: {self.tokenizer.convert_ids_to_tokens(feature['labels'])}\")\n                logger.info(f\"feature['input_ids']: {feature['input_ids']}\")   # feature['input_ids']: Parts of the Pennsylvania Turnpike were closed ; so too was the Baseball Hall of Fame and Museum in Cooperstown, N.Y. Then there was the house that was spotted drifting down the Susquehanna River in New York -- on fire for a while, it seemed. \"</s>\n                logger.info(f\"feature['labels']: {feature['labels']}\")         # feature['labels']: <extra_id_0><extra_id_0> location<extra_id_5> Cooperstown<extra_id_1><extra_id_0> location<extra_id_5> Susquehanna River<extra_id_1><extra_id_0> location<extra_id_5> New York<extra_id_0> contains<extra_id_5> Cooperstown<extra_id_1><extra_id_0> contains<extra_id_5> Susquehanna River<extra_id_1><extra_id_1><extra_id_1></s>\n\n            if not sample_prompt:\n                # Evaluation using Ordered SSI\n                converted_spot_prefix = self.negative_sampler.full_spot(shuffle=self.model.training)\n                converted_asoc_prefix = self.negative_sampler.full_asoc(shuffle=self.model.training)\n            else:\n                # Sample SSI，采样negtive shema\n                converted_spot_prefix, positive_spot, negative_spot = self.negative_sampler.sample_spot(positive=feature.get('spots', []))\n                converted_asoc_prefix, positive_asoc, negative_asoc = self.negative_sampler.sample_asoc(positive=feature.get('asocs', []))\n                # 传入的 positive=feature.get('asocs'/'spots')是该feature的真实label\n                if self.count > 0:\n                    logger.info(f\"Converted_Spot_Prefix: {self.tokenizer.decode(converted_spot_prefix)}\") \n                    logger.info(f\"Converted_Asoc_Prefix: {self.tokenizer.decode(converted_asoc_prefix)}\") \n                    logger.info(f\"Positive_Spot Len: {len(positive_spot)} \\t {positive_spot}\")  \n                    logger.info(f\"Positive_Asoc Len: {len(positive_asoc)} \\t {positive_asoc}\")\n                    logger.info(f\"Negative_Spot Len: {len(negative_spot)} \\t {negative_spot}\")  \n                    logger.info(f\"Negative_Asoc Len: {len(negative_asoc)} \\t {negative_asoc}\")\n                # Dynamic generating spot-asoc during training，evaluating时也有\n                if 'spot_asoc' in feature:\n                    # Deleted positive example Spot in Target that was not sampled by Prefix\n                    feature['spot_asoc'] = [spot_asoc for spot_asoc in feature['spot_asoc'] if spot_asoc[\"label\"] in positive_spot]\n                    if self.add_null:\n                        null_text = self.negative_sampler.full_null(negative_spot, negative_asoc)\n                        record = convert_to_record_function[self.decoding_format](\n                            spot_asoc_instance = feature[\"spot_asoc\"],\n                            structure_maker = BaseStructureMarker(),\n                            null_text = null_text,\n                        )\n                    else:\n                        feature['spot_asoc'] = self.spot_asoc_nosier.add_noise(\n                            feature['spot_asoc'],\n                            spot_label_list=negative_spot,\n                            asoc_label_list=negative_asoc,\n                        )\n                        record = convert_to_record_function[self.decoding_format](\n                            spot_asoc_instance = feature[\"spot_asoc\"],\n                            structure_maker = BaseStructureMarker(),\n                        )\n                    feature[\"labels\"] = self.tokenizer.encode(record)\n\n                    if self.count > 0:\n                        logger.info(f\"Record: {record}\")   # Record: <extra_id_0> <extra_id_0> organization <extra_id_5> <extra_id_6> <extra_id_1> <extra_id_0> location <extra_id_5> Cooperstown <extra_id_1> <extra_id_0> location <extra_id_5> Susquehanna River <extra_id_1> <extra_id_0> location <extra_id_5> New York <extra_id_0> contains <extra_id_5> Cooperstown <extra_id_1> <extra_id_0> contains <extra_id_5> Susquehanna River <extra_id_1> <extra_id_1> <extra_id_1>\n                        logger.info(f\"feature['labels']: {self.tokenizer.convert_ids_to_tokens(feature['labels'])}\")   # 同Record不过是encode后的\n                        logger.info(f\"feature['labels']: {feature['labels']}\")   # 同Record不过是encode后的\n\n\n            feature.pop('sample_prompt') if 'sample_prompt' in feature else None\n            feature.pop('spot_asoc') if 'spot_asoc' in feature else None\n            feature.pop('spots') if 'spots' in feature else None\n            feature.pop('asocs') if 'asocs' in feature else None\n\n            prefix = converted_spot_prefix + converted_asoc_prefix\n            # truncate `prefix` to max length\n            if self.max_prefix_length is not None and self.max_prefix_length >= 0:\n                prefix = prefix[:self.max_prefix_length]\n\n            feature['input_ids'] = prefix + [self.negative_sampler.text_start] + feature['input_ids']  # <text>分隔符\n            # truncate `input_ids` to max length\n\n            if self.count > 0:\n                logger.info(f\"Prefix: {self.tokenizer.convert_ids_to_tokens(prefix)}\")     # <spot> organization<spot> location<spot> person<asoc> major shareholder of<asoc> people<asoc> profession<asoc> children<asoc> place of death<asoc> advisors<asoc> teams<asoc> contains<asoc> industry<asoc> place founded\n                logger.info(f\"feature['input_ids']: {self.tokenizer.convert_ids_to_tokens(feature['input_ids'])}\")    # feature['input_ids']：<spot> organization<spot> location<spot> person<asoc> major shareholder of<asoc> people<asoc> profession<asoc> children<asoc> place of death<asoc> advisors<asoc> teams<asoc> contains<asoc> industry<asoc> place founded<extra_id_2> Parts of the Pennsylvania Turnpike were closed ; so too was the Baseball Hall of Fame and Museum in Cooperstown, N.Y. Then there was the house that was spotted drifting down the Susquehanna River in New York -- on fire for a while, it seemed. \"</s>\n                logger.info(f\"Prefix: {prefix}\")     # <spot> organization<spot> location<spot> person<asoc> major shareholder of<asoc> people<asoc> profession<asoc> children<asoc> place of death<asoc> advisors<asoc> teams<asoc> contains<asoc> industry<asoc> place founded\n                logger.info(f\"feature['input_ids']: {feature['input_ids']}\")    # feature['input_ids']：<spot> organization<spot> location<spot> person<asoc> major shareholder of<asoc> people<asoc> profession<asoc> children<asoc> place of death<asoc> advisors<asoc> teams<asoc> contains<asoc> industry<asoc> place founded<extra_id_2> Parts of the Pennsylvania Turnpike were closed ; so too was the Baseball Hall of Fame and Museum in Cooperstown, N.Y. Then there was the house that was spotted drifting down the Susquehanna River in New York -- on fire for a while, it seemed. \"</s>\n                # Prefix取了所有label，'input_ids'在这里添加前缀\n\n            if self.max_length:\n                feature['input_ids'] = feature['input_ids'][:self.max_length]\n            if self.max_target_length and 'labels' in feature:\n                feature['labels'] = feature['labels'][:self.max_target_length]\n\n            feature['attention_mask'] = [1] * len(feature['input_ids'])\n\n            if self.count > 0:\n                self.count -= 1\n\n        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n        if labels is not None:\n            max_label_length = max(len(_label) for _label in labels)\n            padding_side = self.tokenizer.padding_side\n            for feature in features:\n                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n                feature[\"labels\"] = (\n                    feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n                )\n\n        features = self.tokenizer.pad(\n            features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\"\n        )\n\n        if self.model is not None and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\"):\n            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n            features[\"decoder_input_ids\"] = decoder_input_ids\n        \n        return features\n\n\n\n\nclass PromptSSIGenerator():\n    def __init__(self, tokenizer, schema: RecordSchema, negative_list=[], positive_rate=1, spot_negative=5, asoc_negative=5, ordered_prompt=False, other_ratio=0.3, task_name='') -> None:\n        self.spot_dict = self.get_ordered_dict(schema.type_list, tokenizer)    # {'aspect':[2663], ...}\n        self.asoc_dict = self.get_ordered_dict(schema.role_list, tokenizer)    # {'positive':[1465], ...}\n        self.spot_list = list(self.spot_dict.keys())\n        self.asoc_list = list(self.asoc_dict.keys())\n        self.spot_prompt = tokenizer.get_vocab()[spot_prompt]    \n        self.asoc_prompt = tokenizer.get_vocab()[asoc_prompt]\n        self.text_start = tokenizer.get_vocab()[text_start]\n        self.negative_list = negative_list\n        self.other_rate = other_ratio\n        self.positive_rate = positive_rate if positive_rate > 0 and positive_rate < 1 else 1\n        self.spot_negative = spot_negative\n        self.asoc_negative = asoc_negative\n        self.ordered_prompt = ordered_prompt\n        self.task_name = task_name\n\n    @staticmethod\n    def get_ordered_dict(schema_name_list, tokenizer):\n        schema_ordered_dict = OrderedDict()\n        for name in schema_name_list:\n            schema_ordered_dict[name] = tokenizer.encode(name, add_special_tokens=False)\n        return schema_ordered_dict\n\n\n    @staticmethod\n    def sample_negative(postive, candidates, k = 5):\n        if k < 0:\n            k = len(candidates)\n        negative_set = set()\n        for index in torch.randperm(len(candidates))[: k].tolist():\n            negative = candidates[index]\n            if negative not in postive:\n                negative_set.add(negative)\n        return list(negative_set)\n\n\n    def sample_spot(self, positive):\n        if self.task_name != 'relation':\n            negative_spot = self.sample_negative(postive = positive, candidates = self.spot_list + self.negative_list[: int(self.other_rate * len(self.spot_list))], k = self.spot_negative)\n        else:\n            negative_spot = self.sample_negative(postive = positive, candidates = self.spot_list, k = self.spot_negative)\n        prefix_spot_candidates = positive + negative_spot\n\n        converted_spot_prefix = self.convert_prefix(\n            candidates=prefix_spot_candidates,\n            prompt=self.spot_prompt,\n            mapper=self.spot_dict,\n            ordered_prompt=self.ordered_prompt,\n        )\n        if self.ordered_prompt:\n            prefix_spot_candidates = sorted(prefix_spot_candidates)\n\n        return prefix_spot_candidates, converted_spot_prefix, positive, negative_spot\n\n\n    def sample_asoc(self, positive):\n        if self.task_name == 'relation':\n            negative_asoc = self.sample_negative(postive = positive, candidates = self.asoc_list + self.negative_list[: int(self.other_rate * len(self.asoc_list))], k = self.asoc_negative)\n        else:\n            negative_asoc = self.sample_negative(postive = positive, candidates = self.asoc_list, k = self.asoc_negative)\n        prefix_asoc_candidates = positive + negative_asoc\n\n        converted_asoc_prefix = self.convert_prefix(\n            candidates=prefix_asoc_candidates,\n            prompt=self.asoc_prompt,\n            mapper=self.asoc_dict,\n            ordered_prompt=self.ordered_prompt,\n        )\n        if self.ordered_prompt:\n            prefix_asoc_candidates = sorted(prefix_asoc_candidates)\n\n        return prefix_asoc_candidates, converted_asoc_prefix, positive, negative_asoc\n\n\n    def full_spot(self, shuffle=False):\n        if not self.ordered_prompt and shuffle:\n            ordered_prompt = False\n        else:\n            ordered_prompt = True\n\n        spot_list = self.spot_list\n        if ordered_prompt:\n            spot_list = sorted(self.spot_list)\n\n        convert_spot = self.convert_prefix(\n            candidates=self.spot_list,\n            prompt=self.spot_prompt,\n            mapper=self.spot_dict,\n            ordered_prompt=ordered_prompt,\n        )\n            \n        return spot_list, convert_spot\n\n\n    def full_asoc(self, shuffle=False):\n        if not self.ordered_prompt and shuffle:\n            ordered_prompt = False\n        else:\n            ordered_prompt = True\n\n        asoc_list = self.asoc_list\n        if ordered_prompt:\n            asoc_list = sorted(self.asoc_list)\n\n        convert_asoc = self.convert_prefix(\n            candidates=self.asoc_list,\n            prompt=self.asoc_prompt,\n            mapper=self.asoc_dict,\n            ordered_prompt=ordered_prompt,\n        )\n\n        return asoc_list, convert_asoc\n\n\n    def full_null(self, negative_spot, negative_asoc):\n        full_null_text = []\n        if self.task_name == 'relation':\n            for asoc in negative_asoc:\n                asoc_str_rep = ' '.join([\n                    type_start,\n                    asoc,\n                    span_start,\n                    null_span,\n                    type_end,\n                ])\n                full_null_text.append(asoc_str_rep)\n        else:\n            for spot in negative_spot:\n                spot_str_rep = ' '.join([\n                    type_start,\n                    spot,\n                    span_start,\n                    null_span,\n                    type_end,\n                ])\n                full_null_text.append(spot_str_rep)    \n        return ' '.join(full_null_text)\n\n\n    @staticmethod\n    def convert_prefix(candidates, prompt, mapper, ordered_prompt=True):\n        prefix = list()\n        if ordered_prompt:\n            candidate_sorted = sorted([(candidate, index) for index, candidate in enumerate(candidates)])\n            index_list = [index for _, index in candidate_sorted]\n        else:\n            index_list = torch.randperm(len(candidates)).tolist()\n\n        for index in index_list:\n            prefix += [prompt]\n            prefix += mapper[candidates[index]]\n        return prefix\n\n\n\n@dataclass\nclass PromptForMetaSeq2Seq:\n    tokenizer: PreTrainedTokenizerBase\n    negative_sampler: PromptSSIGenerator\n    model: Optional[PreTrainedModel] = None\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    max_target_length: Optional[int] = None\n    max_prefix_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    label_pad_token_id: int = -100\n    spot_asoc_nosier: SpotAsocNoiser = None\n    decoding_format: str = 'spotasoc'\n    use_ssi: bool = True\n    add_null: bool = False\n    count : int = 10\n\n    def __call__(self, features):\n        for feature in features:\n            sample_prompt = feature['sample_prompt']\n            if self.count > 0:\n                logger.info(f\"{self.count}/10\")\n                logger.info(f\"feature['input_ids']: {self.tokenizer.convert_ids_to_tokens(feature['input_ids'])}\")   # feature['input_ids']: Parts of the Pennsylvania Turnpike were closed ; so too was the Baseball Hall of Fame and Museum in Cooperstown, N.Y. Then there was the house that was spotted drifting down the Susquehanna River in New York -- on fire for a while, it seemed. \"</s>\n                logger.info(f\"feature['labels']: {self.tokenizer.convert_ids_to_tokens(feature['labels'])}\")         # feature['labels']: <extra_id_0><extra_id_0> location<extra_id_5> Cooperstown<extra_id_1><extra_id_0> location<extra_id_5> Susquehanna River<extra_id_1><extra_id_0> location<extra_id_5> New York<extra_id_0> contains<extra_id_5> Cooperstown<extra_id_1><extra_id_0> contains<extra_id_5> Susquehanna River<extra_id_1><extra_id_1><extra_id_1></s>\n\n            '''评估(test)所有的spot、asoc都要用到, 不存在正负样本'''\n            if not sample_prompt:\n                spot_prefix, convert_spot = self.negative_sampler.full_spot(shuffle=self.model.training)\n                asoc_prefix, convert_asoc = self.negative_sampler.full_asoc(shuffle=self.model.training)\n            else:\n                '''\n                获得正样本(positive_spot)和负样本(negative_spot), \n                spot_prefix是正spot+负spot,\n                convert_spot是在每个spot之间添加了特殊分隔符(<spot>)\n                '''\n                spot_prefix, convert_spot, positive_spot, negative_spot = self.negative_sampler.sample_spot(positive=feature.get('spots', []))\n                asoc_prefix, convert_asoc, positive_asoc, negative_asoc = self.negative_sampler.sample_asoc(positive=feature.get('asocs', []))\n                if self.count > 0:\n                    logger.info(f\"Spot_Prefix: {spot_prefix}\") \n                    logger.info(f\"Asoc_Prefix: {asoc_prefix}\") \n                    logger.info(f\"Positive_Spot Len: {len(positive_spot)} \\t {positive_spot}\")  \n                    logger.info(f\"Positive_Asoc Len: {len(positive_asoc)} \\t {positive_asoc}\")\n                    logger.info(f\"Negative_Spot Len: {len(negative_spot)} \\t {negative_spot}\")  \n                    logger.info(f\"Negative_Asoc Len: {len(negative_asoc)} \\t {negative_asoc}\")\n                '''\n                feature['input_ids']: ['▁So', '▁to', '▁me', '▁', ',', '▁the', '▁key', '▁thing', '▁is', '▁that', '▁we', '▁ought', '▁to', '▁be', '▁taking', '▁care', '▁of', '▁the', '▁military', '▁and', '▁that', '▁', \"'\", '▁', 's', '▁what', '▁we', '▁should', '▁do', '▁', '.', '</s>']\n                feature['labels']: ['<extra_id_0>', '<extra_id_1>', '</s>']\n                Positive_Spot Len: 0 \t []\n                Positive_Asoc Len: 0 \t []\n                Negative_Spot Len: 12 \t ['sentence', 'marry', 'phone write', 'sue', 'be born', 'acquit', 'elect', 'transfer ownership', 'attack', 'extradite', 'end organization', 'arrest jail']\n                Negative_Asoc Len: 19 \t ['adjudicator', 'beneficiary', 'place', 'instrument', 'destination', 'organization', 'buyer', 'plaintiff', 'defendant', 'target', 'victim', 'artifact', 'person', 'attacker', 'entity', 'seller', 'agent', 'origin', 'vehicle']\n                上面的数据中可以看到label中的正样本数是0, 负采样后得到12、19个负样本\n                '''\n\n                # Dynamic generating spot-asoc during training，evaluating时也有\n                if 'spot_asoc' in feature:\n                    feature['spot_asoc'] = [spot_asoc for spot_asoc in feature['spot_asoc'] if spot_asoc[\"label\"] in positive_spot]\n                    if self.add_null:\n                        null_text = self.negative_sampler.full_null(negative_spot, negative_asoc)\n                        record = convert_to_record_function[self.decoding_format](\n                            spot_asoc_instance = feature[\"spot_asoc\"],\n                            structure_maker = BaseStructureMarker(),\n                            null_text = null_text,\n                        )\n                    else:\n                        feature['spot_asoc'] = self.spot_asoc_nosier.add_noise(\n                            feature['spot_asoc'],\n                            spot_label_list=negative_spot,\n                            asoc_label_list=negative_asoc,\n                        )\n                        record = convert_to_record_function[self.decoding_format](\n                            spot_asoc_instance = feature[\"spot_asoc\"],\n                            structure_maker = BaseStructureMarker(),\n                        )\n                    feature[\"labels\"] = self.tokenizer.encode(record)\n\n                    if self.count > 0:\n                        logger.info(f\"Record: {record}\")   \n                        logger.info(f\"feature['labels']: {self.tokenizer.convert_ids_to_tokens(feature['labels'])}\")  \n\n\n            feature.pop('sample_prompt') if 'sample_prompt' in feature else None\n            feature.pop('spot_asoc') if 'spot_asoc' in feature else None\n            '''\n            spot、asoc由原先的正样本变为负采样后的正加负\n            \"spot\": [\"geographical social political\"], \n            采样两个负样本\"person other\", \"writtenart\"\n            最终得到\"spot\": [\"geographical social political\", \"person other\", \"writtenart\"], \n            feature['spot']、feature['asoc']在获得prompt的时候用到\n            '''\n            feature['spot'] = [self.tokenizer.encode(s, add_special_tokens = False) for s in spot_prefix]\n            feature['asoc'] = [self.tokenizer.encode(a, add_special_tokens = False) for a in asoc_prefix]\n\n            if self.use_ssi:\n                prefix = convert_spot + convert_asoc\n                if self.max_prefix_length is not None and self.max_prefix_length >= 0:\n                    prefix = prefix[:self.max_prefix_length]\n                feature['input_ids'] = prefix + [self.negative_sampler.text_start] + feature['input_ids']  # <text>分隔符\n                if self.count > 0:\n                    logger.info(f\"Prefix: {self.tokenizer.convert_ids_to_tokens(prefix)}\")     # <spot> organization<spot> location<spot> person<asoc> major shareholder of<asoc> people<asoc> profession<asoc> children<asoc> place of death<asoc> advisors<asoc> teams<asoc> contains<asoc> industry<asoc> place founded\n                    logger.info(f\"feature['input_ids']: {self.tokenizer.convert_ids_to_tokens(feature['input_ids'])}\") \n                    '''\n                    Prefix: ['<spot>', '▁', 'a', 'c', 'quit', '<spot>', '▁arrest', '▁jail', '<spot>', '▁attack', '<spot>', '▁be', '▁born', '<spot>', '▁elect', '<spot>', '▁end', '▁organization', '<spot>', '▁extra', 'dite', '<spot>', '▁marry', '<spot>', '▁phone', '▁write', '<spot>', '▁sentence', '<spot>', '▁su', 'e', '<spot>', '▁transfer', '▁ownership', '<asoc>', '▁adj', 'u', 'dic', 'ator', '<asoc>', '▁agent', '<asoc>', '▁art', 'i', 'fact', '<asoc>', '▁attacker', '<asoc>', '▁beneficiary', '<asoc>', '▁buyer', '<asoc>', '▁defendant', '<asoc>', '▁destination', '<asoc>', '▁entity', '<asoc>', '▁instrument', '<asoc>', '▁organization', '<asoc>', '▁origin', '<asoc>', '▁person', '<asoc>', '▁place', '<asoc>', '▁plaintiff', '<asoc>', '▁seller', '<asoc>', '▁target', '<asoc>', '▁vehicle', '<asoc>', '▁victim']\n                    feature['input_ids']: ['<spot>', '▁', 'a', 'c', 'quit', '<spot>', '▁arrest', '▁jail', '<spot>', '▁attack', '<spot>', '▁be', '▁born', '<spot>', '▁elect', '<spot>', '▁end', '▁organization', '<spot>', '▁extra', 'dite', '<spot>', '▁marry', '<spot>', '▁phone', '▁write', '<spot>', '▁sentence', '<spot>', '▁su', 'e', '<spot>', '▁transfer', '▁ownership', '<asoc>', '▁adj', 'u', 'dic', 'ator', '<asoc>', '▁agent', '<asoc>', '▁art', 'i', 'fact', '<asoc>', '▁attacker', '<asoc>', '▁beneficiary', '<asoc>', '▁buyer', '<asoc>', '▁defendant', '<asoc>', '▁destination', '<asoc>', '▁entity', '<asoc>', '▁instrument', '<asoc>', '▁organization', '<asoc>', '▁origin', '<asoc>', '▁person', '<asoc>', '▁place', '<asoc>', '▁plaintiff', '<asoc>', '▁seller', '<asoc>', '▁target', '<asoc>', '▁vehicle', '<asoc>', '▁victim', '<extra_id_2>', '▁So', '▁to', '▁me', '▁', ',', '▁the', '▁key', '▁thing', '▁is', '▁that', '▁we', '▁ought', '▁to', '▁be', '▁taking', '▁care', '▁of', '▁the', '▁military', '▁and', '▁that', '▁', \"'\", '▁', 's', '▁what', '▁we', '▁should', '▁do', '▁', '.', '</s>']\n                    Prefix是spot、asoc之间添加了特殊分隔符(<spot>、<asoc>)的格式\n                    如果use_ssi==True, 会在模型的输入'input_ids'的前面添加prefix\n                    '''\n\n            if self.max_length:\n                feature['input_ids'] = feature['input_ids'][:self.max_length]\n            if self.max_target_length and 'labels' in feature:\n                feature['labels'] = feature['labels'][:self.max_target_length]\n            \n            feature['attention_mask'] = [1] * len(feature['input_ids'])\n            if self.max_length:\n                feature['input_ids'] = feature['input_ids'] + [self.tokenizer.pad_token_id] * (self.max_length - len(feature['input_ids']))\n                feature['attention_mask'] = feature['attention_mask'] + [0] * (self.max_length - len(feature['attention_mask']))\n\n            if self.count > 0:\n                self.count -= 1\n\n\n        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n        if labels is not None:\n            max_label_length = max(len(_label) for _label in labels)\n            padding_side = self.tokenizer.padding_side\n            for feature in features:\n                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n                feature[\"labels\"] = (\n                    feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n                )\n                feature['labels'] = torch.tensor(feature['labels'], dtype = torch.long).unsqueeze(0)\n\n        for feature in features:\n            feature['input_ids'] = torch.tensor(feature['input_ids'], dtype = torch.long).unsqueeze(0)\n            feature['attention_mask'] = torch.tensor(feature['attention_mask'], dtype = torch.long).unsqueeze(0)\n\n        max_spot_len = max(len(feature[\"spot\"]) for feature in features)\n        max_asoc_len = max(len(feature[\"asoc\"]) for feature in features)\n\n        for feature in features:\n            feature['spot'] = feature['spot'] + [(max_spot_len - len(feature['spot'])) * [0]]\n            feature['asoc'] = feature['asoc'] + [(max_asoc_len - len(feature['asoc'])) * [0]]\n\n        examples = {}\n        for feature in features:\n            if 'input_ids' not in examples.keys():\n                examples['input_ids'] = feature['input_ids']\n                examples['attention_mask'] = feature['attention_mask']\n                if 'labels' in feature.keys():\n                    examples['labels'] = feature['labels']\n                examples['spot'] = [feature['spot'], ]\n                examples['asoc'] = [feature['asoc'], ]\n            else:\n                examples['input_ids'] = torch.cat([examples['input_ids'], feature['input_ids']], dim=0)\n                examples['attention_mask'] = torch.cat([examples['attention_mask'], feature['attention_mask']], dim=0)\n                if 'labels' in feature.keys():\n                    examples['labels'] = torch.cat([examples['labels'], feature['labels']], dim=0)\n                examples['spot'].append(feature['spot'])\n                examples['asoc'].append(feature['asoc'])\n\n        # prepare decoder_input_idsf\n        if self.model is not None and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\"):\n            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n            examples[\"decoder_input_ids\"] = decoder_input_ids\n        \n        return examples        # 返回的examples应该是字典dict{tuple()}, features是字典元组tuple(dict{})\n\n        "}
