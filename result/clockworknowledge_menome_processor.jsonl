{"repo_info": {"repo_name": "menome_processor", "repo_owner": "clockworknowledge", "repo_url": "https://github.com/clockworknowledge/menome_processor"}}
{"type": "source_file", "path": "app/routers/utils.py", "content": "# utils.py\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt, JWTError\nfrom datetime import datetime\nfrom neo4j import GraphDatabase\nfrom config import AppConfig\nfrom models import User, UserIn\n\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.docstore.document import Document\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# Initialize the Neo4j driver\ndriver = GraphDatabase.driver(AppConfig.NEO4J_URI, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))\n\n# Define the dependency function to get the current user\nasync def get_current_user(token: str = Depends(oauth2_scheme)) -> User:\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, AppConfig.SECRET_KEY, algorithms=[AppConfig.ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        # Here you should get the user from your Neo4j database using the username\n        # and convert it to the UserInDB model.\n    \n        user = get_user_from_db(username)\n        if user is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    return user\n\n# Helper function to convert Neo4j datetime to Python datetime\ndef neo4j_datetime_to_python_datetime(neo4j_dt_str: str) -> datetime:\n    truncated_str = neo4j_dt_str[:26] + neo4j_dt_str[29:]\n    return datetime.fromisoformat(truncated_str)\n\n# Get user from database\ndef get_user_from_db(username: str):\n    with driver.session() as session:\n        result = session.run(\"MATCH (u:User {username: $username}) RETURN u\", username=username)\n        user_data = result.single()\n        if user_data:\n            date_created_str = str(user_data['u']['datecreated'])\n            date_created = neo4j_datetime_to_python_datetime(date_created_str)\n            return UserIn(\n                username=user_data['u']['username'], \n                password=user_data['u']['password'], \n                uuid=user_data['u']['uuid'], \n                email=user_data['u']['email'], \n                name=user_data['u']['name'], \n                disabled=user_data['u']['disabled'],\n                datecreated=date_created\n            )\n    return None\n\n\n## fetches node properties by uuid\ndef fetch_node_properties_by_uuid(driver, uuids: list):\n    output = []\n    with driver.session() as session:\n        # Query to fetch specific node properties based on UUIDs\n        query = \"\"\"\n            MATCH (d:Document)-[]-(p:Page)-[]-(c:Child)\n            WHERE c.uuid IN $uuids\n            OPTIONAL MATCH (p)-[]-(q:Question)\n            OPTIONAL MATCH (p)-[]-(s:Summary)\n\n            WITH d, p, \n                collect(DISTINCT {uuid: q.uuid, name: q.name, text: q.text}) AS questions,\n                collect(DISTINCT {uuid: s.uuid, name: s.name, text: s.text}) AS summaries,\n                collect(DISTINCT {uuid: c.uuid, name: c.name, text: c.text}) AS children\n            ORDER BY d.name, toInteger(replace(p.name, 'Page ', ''))\n\n            WITH d,\n                collect({\n                    uuid: p.uuid, \n                    name: p.name, \n                    summaries: summaries, \n                    questions: questions, \n                    children: children\n                }) AS pages\n            RETURN \n                d.uuid AS doc_uuid, d.name AS doc_name, d.addeddate AS doc_addeddate, \n                d.imageurl AS doc_imageurl, d.publisher AS doc_publisher, \n                d.thumbnail AS doc_thumbnail, d.url AS doc_url, d.wordcount AS doc_wordcount,\n                pages\n\n        \"\"\"\n        results = session.run(query, uuids=uuids)\n            \n        \n        for record in results:\n            # Structuring the output\n            document_data = {\n                \"uuid\": record[\"doc_uuid\"],\n                \"name\": record[\"doc_name\"],\n                \"addeddate\": record[\"doc_addeddate\"],\n                \"imageurl\": record[\"doc_imageurl\"],\n                \"publisher\": record[\"doc_publisher\"],\n                \"thumbnail\": record[\"doc_thumbnail\"],\n                \"url\": record[\"doc_url\"],\n                \"wordcount\": record[\"doc_wordcount\"]\n            }\n\n            pages_data = record[\"pages\"]\n\n            output.append({\n                \"document\": document_data,\n                \"pages\": pages_data\n            })\n\n\n        # Return the output as a single JSON-formatted object\n        return output\n\n\n## generates a summary of text being returned\ndef summarize_text_with_openai(text: str) -> str:\n    \"\"\"Generate a summary for the given text using OpenAI.\"\"\"\n    llm = ChatOpenAI(temperature=0, model_name=AppConfig.OPENAI_CHAT_MODEL)\n    #loader=langchain.document_loaders.TextLoader(text)\n    docs = [Document(page_content=text)]\n    # Run the summarization chain and get the summary\n    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n    summary = chain.run(docs)\n    return summary\n\n\n## checks if an index exists in the neo4j graph \ndef index_exists(driver, index_name: str) -> bool:\n    \"\"\"Check if a given index exists in the Neo4j database.\"\"\"\n    query = f\"\"\"\n    SHOW INDEXES\n    YIELD name\n    WHERE name = '{index_name}'\n    RETURN name\n    \"\"\"\n    with driver.session() as session:\n        result = session.run(query).single()\n        return bool(result)\n    \n## Sets up the graph database index\ndef setup_graph_db(driver, index_name, node_label=\"Child\", property_name=\"embedding\"):\n\n    query = f\"\"\"\n    CALL db.index.vector.createNodeIndex(\n      '{index_name}',      // index name\n      '{node_label},             // node label\n      '{property_name}',         // node property\n      1536,                // vector size\n      'cosine'             // similarity metric\n    )\n    \"\"\"\n\n    with driver.session() as session:\n        result=session.run(query)\n    return True\n\n# Define functions for adding documents, pages, and chunks to the graph\ndef add_document(tx, properties):\n    query = (\n        \"CREATE (d:Document {uuid: $uuid, name: $name, title: $title, url: $url, \"\n        \"sourceurl: $sourceurl, thumbnailurl: $thumbnailurl, text: $text}) \"\n        \"RETURN d\"\n    )\n    return tx.run(query, properties).single()[\"d\"]\n\ndef add_page(tx, doc_uuid, properties):\n    query = (\n        \"MATCH (d:Document {Uuid: $doc_uuid}) \"\n        \"CREATE (p:Page {uuid: $uuid, name: $name, text: $text}) \"\n        \"MERGE (d)-[:HAS_PAGE]->(p) \"\n        \"RETURN p\"\n    )\n    return tx.run(query, {\"doc_uuid\": doc_uuid, **properties}).single()[\"p\"]\n\ndef add_chunk(tx, parent_uuid, properties):\n    query = (\n        \"MATCH (p {uuid: $parent_uuid}) \"\n        \"CREATE (cu:Child {uuid: $uuid, name: $name,  text: $text}) \"\n        \"MERGE (p)-[:HAS_CHILD]->(cu) \"\n        \"with cu CALL db.create.setVectorProperty(cu, 'embedding', $embedding) YIELD node as c \"\n        \"RETURN c\"\n    )\n    return tx.run(query, {\"parent_uuid\": parent_uuid, **properties}).single()[\"c\"]\n\ndef setupSourceChunks(tx):\n    query1=\"\"\"match (c:Chunk)-[]-(p:Page)-[]-(d:Document) where  c.source is null\n        set c.source=p.Name + ', PDF ' + d.SourceUrl\"\"\"\n    \n    query2=\"\"\"match (c:Chunk)-[]-(p:Page)-[]-(d:Document) \n        set c.source=p.Uuid\"\"\"\n    return tx.run(query2)\n\n\n"}
{"type": "source_file", "path": "config.py", "content": "import os\nfrom decouple import Config, RepositoryEnv\n\nclass AppConfig:\n    DOTENV_FILE = '/code/config/.env'\n    config = Config(RepositoryEnv(DOTENV_FILE))\n\n    # Database Configurations\n    NEO4J_URI = config.get('NEO4J_URI', default='bolt://localhost:7687')\n    NEO4J_USER = config.get('NEO4J_USER', default='neo4j')\n    NEO4J_PASSWORD = config('NEO4J_PASSWORD')\n    NEO4J_INDEX_NAME = config('NEO4J_INDEX_NAME', default='typical_rag')\n    NEO4J_CHUNK_LABEL = config('NEO4J_CHUNK_LABEL', default='Child')\n    NEO4J_CHUNK_TEXT_PROPERTY = config('NEO4J_CHUNK_TEXT_PROPERTY', default='text')\n    NEO4J_CHUNK_EMBEDDING_PROPERTY = config('NEO4J_CHUNK_EMBEDDING_PROPERTY', default='embedding')\n\n    # DEFAULT USER:\n    DEFAULT_USER_UUID = config('DEFAULT_USER_UUID', default='00000000-0000-0000-0000-000000000000')\n    DEFAULT_USER_USERNAME = config('DEFAULT_USER_USERNAME', default='admin')\n    DEFAULT_USER_EMAIL = config('DEFAULT_USER_EMAIL',\n                                default='test@test.com')\n    DEFAULT_USER_NAME = config('DEFAULT_USER_NAME', default='Admin')\n    DEFAULT_USER_PASSWORD = config('DEFAULT_USER_PASSWORD', default='test')\n\n    # OpenAI Configuration\n    OPENAI_API_KEY = config('OPENAI_API_KEY')\n    EMBEDDING_DIMENSION = config('EMBEDDING_DIMENSION', cast=int, default=1536)\n    OPENAI_CHAT_MODEL = config('OPENAI_CHAT_MODEL', default='gpt-4-1106-preview')\n\n    RABBITMQ_HOST = config('RABBMITMQ_HOST', default='localhost')\n    RABBITMQ_PORT = config('RABBMITMQ_PORT', cast=int, default=5672)\n    RABBITMQ_USER = config('RABBITMQ_USER', default='admin')\n    RABBITMQ_PASSWORD = config('RABBITMQ_PASSWORD', default='brier23glrefy!')\n\n    # Celery Configuration\n    CELERY_BROKER_URL = config.get('CELERY_BROKER_URL', default='amqp://guest:guest@rabbit:5672//')\n    CELERY_RESULT_BACKEND_URL = config.get('CELERY_RESULT_BACKEND_URL', default='rpc://')\n\n\n    # State processing messages:\n    # Task states and celery configuration \n    PROCESSING_DOCUMENT = 'PROCESSING_DOCUMENT'\n    PROCESSING_QUESTIONS = 'PROCESSING_QUESTIONS'\n    PROCESSING_SUMMARY = 'PROCESSING_SUMMARY'\n    PROCESSING_DONE = 'PROCESSING_DONE'\n    PROCESSING_FAILED = 'PROCESSING_FAILED'\n    PROCESSING_PAGES = 'PROCESSING_PAGES'\n\n    # Other Configurations\n    MAX_QUESTIONS_PER_PAGE = config('MAX_QUESTIONS_PER_PAGE', cast=int, default=2)\n    SECRET_KEY=config('SECRET_KEY')\n    ALGORITHM=config('ALGORITHM')\n    TEST_DOCUMENT_URL = \"https://en.wikipedia.org/wiki/As_We_May_Think\"\n    ACCESS_TOKEN_EXPIRE_MINUTES = config('ACCESS_TOKEN_EXPIRE_MINUTES', cast=int, default=30)\n\n    @staticmethod\n    def initialize_environment_variables():\n        os.environ[\"OPENAI_API_KEY\"] = AppConfig.OPENAI_API_KEY\n        os.environ[\"NEO4J_URI\"] = AppConfig.NEO4J_URI\n        os.environ[\"NEO4J_USERNAME\"] = AppConfig.NEO4J_USER\n        os.environ[\"NEO4J_PASSWORD\"] = AppConfig.NEO4J_PASSWORD\n"}
{"type": "source_file", "path": "worker/__init__.py", "content": ""}
{"type": "source_file", "path": "models.py", "content": "from pydantic import BaseModel\nfrom pydantic.networks import HttpUrl\nfrom datetime import datetime\nfrom typing import Optional\nfrom enum import Enum\nfrom typing import List\nfrom langchain.pydantic_v1 import Field\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n\n# Models\nclass UserIn(BaseModel):\n    uuid: Optional[str] = None\n    username: str\n    email: str\n    name: str\n    disabled: Optional[bool] = None\n    password: str\n    datecreated: Optional[datetime] = None\n\nclass User(BaseModel):\n    uuid: str\n    username: str\n    email: str\n    name: str\n    disabled: Optional[bool] = None\n    datecreated: Optional[datetime] = None\n\nclass DocumentRequest(BaseModel):\n    url: str\n    note: Optional[str] = None\n\nclass DocumentResponse(BaseModel):\n    uuid: str\n    name: str\n    url: str\n    text: str\n    imageurl: str\n    publisher: str\n    addeddate: str\n    thumbnail: str\n    wordcount: int\n\n# Add typing for input\nclass Question(BaseModel):\n    question: str\n\nclass DefaultIcons:\n    ARTICLE_ICON_SVG=\"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" class=\"w-6 h-6\">\n                        <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z\" />\n                        </svg>\"\"\""}
{"type": "source_file", "path": "worker/processing_functions.py", "content": "import logging\nimport uuid\nfrom typing import List\n\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain.chains.openai_functions import create_structured_output_chain\nfrom langchain.chains.openai_functions import create_structured_output_chain\nfrom langchain.prompts import ChatPromptTemplate\n\nfrom config import AppConfig\nfrom  models import Question\n\n\n# internal classes\n#TODO: refactor this to a separate file\nclass Questions(BaseModel):\n    \"\"\"Generating hypothetical questions about text.\"\"\"\n\n    questions: List[str] = Field(\n        ...,\n        description=(\n            \"Generated hypothetical questions based on \" \"the information from the text\"\n        ),\n    )\n\n# Initialize environment variables if needed\nAppConfig.initialize_environment_variables()\n\ndef generate_questions(self,llm, parent_documents, documentId, embeddings, driver):\n\n    # Generate Questions for page node \n    logging.info(f\"Generating questions for document {documentId}\")\n    questions_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                (\n                    \"You are generating hypothetical questions based on the information \"\n                    \"found in the text. Make sure to provide full context in the generated \"\n                    \"questions.\"\n                ),\n            ),\n            (\n                \"human\",\n                (\n                    \"Use the given format to generate hypothetical questions from the \"\n                    \"following input: {input}\"\n                ),\n            ),\n        ]\n    )\n    \n    logging.info(f\"LLM type: {type(llm)}, Prompt: {questions_prompt}\")\n    \n    question_chain = create_structured_output_chain(Questions, llm, questions_prompt)\n\n    for i, parent in enumerate(parent_documents):\n        self.update_state(state=AppConfig.PROCESSING_QUESTIONS, meta={\"page\": i+1, \"total_pages\": len(parent_documents), \"documentId\": documentId})\n        logging.info(f\"Generating questions for page {i+1} of {len(parent_documents)} for document {documentId}\")\n        generated_questions = question_chain.run(parent.page_content).questions\n        limited_questions = generated_questions[:AppConfig.MAX_QUESTIONS_PER_PAGE]  # Limit the number of questions\n\n        params = {\n            \"parent_id\": f\"Page {i+1}\",\n            \"document_uuid\": documentId,\n            \"questions\": [\n                {\n                    \"text\": q, \n                    \"uuid\": str(uuid.uuid4()), \n                    \"name\": f\"{i+1}-{iq+1}\", \n                    \"embedding\": embeddings.embed_query(q)\n                }\n                for iq, q in enumerate(limited_questions) if q  # Iterate over limited questions\n            ],\n        }\n        with driver.session() as session :\n            session.run(\n                \"\"\"\n            match (d:Document)-[]-(p:Page) where d.uuid=$document_uuid and p.name=$parent_id\n            WITH p\n            UNWIND $questions AS question\n            CREATE (q:Question {uuid: question.uuid})\n            SET q.text = question.text, q.name = question.name, q.datecreated= datetime(), q.source=p.uuid\n            MERGE (q)<-[:HAS_QUESTION]-(p)\n            WITH q, question\n            CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n            YIELD node\n            RETURN count(*)\n            \"\"\",\n            params,\n        )\n        \n\ndef generate_summaries(self,llm, parent_documents, documentId, embeddings, driver):\n    # Code for generating summaries\n       \n    # Ingest summaries\n\n    summary_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                (\n                    \"You are generating concise and accurate summaries based on the \"\n                    \"information found in the text.\"\n                ),\n            ),\n            (\n                \"human\",\n                (\"Generate a summary of the following input: {question}\\n\" \"Summary:\"),\n            ),\n        ]\n    )\n\n    summary_chain = summary_prompt | llm\n\n    for i, parent in enumerate(parent_documents):\n        self.update_state(state=AppConfig.PROCESSING_SUMMARY, meta={\"page\": i+1, \"total_pages\": len(parent_documents), \"documentId\": documentId})\n        logging.info(f\"Generating summary for page {i+1} of {len(parent_documents)} for document {documentId}\")\n        \n        summary = summary_chain.invoke({\"question\": parent.page_content}).content\n        params = {\n            \"parent_id\": f\"Page {i+1}\",\n            \"uuid\": str(uuid.uuid4()),\n            \"summary\": summary,\n            \"embedding\": embeddings.embed_query(summary),\n            \"document_uuid\": documentId\n        }\n        with driver.session() as session :\n            session.run(\n                \"\"\"\n            match (d:Document)-[]-(p:Page) where d.uuid=$document_uuid and p.name=$parent_id\n            with p\n            MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n            SET s.text = $summary, s.datecreated= datetime(), s.uuid= $uuid, s.source=p.uuid\n            WITH s\n            CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n            YIELD node\n            RETURN count(*)\n            \"\"\",\n                params,\n            )"}
{"type": "source_file", "path": "app/routers/processing.py", "content": "from fastapi import APIRouter, Query, Depends\nfrom fastapi import Depends, HTTPException, status, APIRouter\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nfrom pydantic import BaseModel\nfrom models import User,UserIn  # Import your User model\nfrom neo4j import GraphDatabase  # Import Neo4j driver\nimport logging\nfrom datetime import datetime,  timedelta\n\nfrom worker.tasks import process_text_task, get_task_info, purge_celery_queue\n\nfrom config import AppConfig\nfrom dotenv import load_dotenv\n\n# Assuming your .env file is in /code/config/.env inside the container\ndotenv_path = '/code/config/.env'\nload_dotenv(dotenv_path)\n\n# Initialize environment variables if needed\nAppConfig.initialize_environment_variables()\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\n# Password hashing\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n# Verify hashed password\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\n# Create an access token\ndef create_access_token(data: dict, expires_delta: timedelta = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, AppConfig.SECRET_KEY, algorithm=AppConfig.ALGORITHM)\n    return encoded_jwt\n\n# Authenticate a user (This function might need the get_user_from_db function, which should be imported from user_management.py)\ndef authenticate_user(username: str, password: str):\n    user = get_user_from_db(username)\n    if user and verify_password(password, user.password):\n        return user\n    return None\n\n# Helper function to convert Neo4j datetime to Python datetime\ndef neo4j_datetime_to_python_datetime(neo4j_dt_str: str) -> datetime:\n    truncated_str = neo4j_dt_str[:26] + neo4j_dt_str[29:]\n    return datetime.fromisoformat(truncated_str)\n\n# Get user from database\ndef get_user_from_db(username: str):\n    driver = GraphDatabase.driver(AppConfig.NEO4J_URI, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))\n    with driver.session() as session:\n        result = session.run(\"MATCH (u:User {username: $username}) RETURN u\", username=username)\n        user_data = result.single()\n        if user_data:\n            date_created_str = str(user_data['u']['datecreated'])\n            date_created = neo4j_datetime_to_python_datetime(date_created_str)\n            return UserIn(\n                username=user_data['u']['username'], \n                password=user_data['u']['password'], \n                uuid=user_data['u']['uuid'], \n                email=user_data['u']['email'], \n                name=user_data['u']['name'], \n                disabled=user_data['u']['disabled'],\n                datecreated=date_created\n            )\n    session.close()\n    driver.close()\n    return None\n\n\n# Define the dependency function to get the current user\nasync def get_current_user(token: str = Depends(oauth2_scheme)) -> User:\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, AppConfig.SECRET_KEY, algorithms=[AppConfig.ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        # Here you should get the user from your Neo4j database using the username\n        # and convert it to the UserInDB model.  \n        user = get_user_from_db(username)\n        if user is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    return user\n\n## Endpoints\nrouter = APIRouter()\n\n## Process documents in the database using workers\n@router.post(\"/process-documents\", tags=[ \"Process\"]\n             , description=\"Process documents in the database\"\n             , summary=\"Process documents in the database that have not had pages, questions or summaries created. Helps illustrate async processing\")\nasync def process_documents(\n    document_limit: int = Query(default=None, description=\"Limit on number of documents to process\"),\n    generateQuestions: bool = Query(default=False, description=\"Flag to generate questions\"),\n    generateSummaries: bool = Query(default=False, description=\"Flag to generate summaries\"),\n    current_user: User = Depends(get_current_user)):\n    logging.basicConfig(level=logging.INFO)\n    \n    # Setup neo4j driver\n    driver = GraphDatabase.driver(AppConfig.NEO4J_URI, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))\n\n    query = \"MATCH (a:Document) WHERE NOT (a)-[:HAS_PAGE]->(:Page) and a.text <> '' and a.process=True RETURN a.uuid as uuid\"\n    if document_limit is not None:\n        query += f\" LIMIT {document_limit}\"\n    \n    logging.info(\"Querying for documents to process.\")\n    document_ids = []\n    with driver.session() as session:\n        result = session.run(query)\n        document_ids = [record['uuid'] for record in result]\n    session.close()\n\n    logging.info(f\"Found {len(document_ids)} documents to process.\")\n    \n    task_ids = []\n    for document_id in document_ids:\n        try:\n            logging.info(f\"Queueing document {document_id} for processing.\")\n            with driver.session() as session:\n                result = session.run(\"MATCH (a:Document {uuid: $uuid}) RETURN a\", {\"uuid\": document_id})\n                document_data = result.single().value()\n                text = document_data['text']\n                # Pass the generateQuestions and generateSummaries flags to the task\n                task = process_text_task.delay(text, document_id, generateQuestions, generateSummaries)\n                task_ids.append(task.id)\n                logging.info(f\"Queued document {document_id} with task ID {task.id}\")\n        except Exception as e:\n            logging.error(f\"Failed to queue document {document_id}: {e}\")\n        finally:\n            session.close()\n            driver.close()\n\n    return {\n        \"message\": f\"Processing started for {len(document_ids)} documents\",\n        \"task_ids\": task_ids\n    }\n\n## Returns an access token based on username and password\n@router.post(\"/token\", response_model=Token, description=\"Returns an access token\", summary=\"Returns an access token\", tags=[\"Users\"])\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=AppConfig.ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user.username}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n## Basic health check\n@router.post(\"/divide\")\nasync def divide(x: int, y: int):\n    from worker.tasks import divide\n    result = divide.delay(x, y)\n    return {\"task_id\": result.id}\n\n\n## Returns the status of the submitted Task\n@router.get(\"/task/{task_id}\")\nasync def get_task_status(task_id: str): \n    \"\"\"\n    Return the status of the submitted Task\n    \"\"\"\n    return get_task_info(task_id)\n\n\n## Purge all tasks in the Celery queue\n@router.post(\"/purge-queue\", tags=[\"Queue Management\"], description=\"Purge all tasks in the Celery queue\", summary=\"Purge all tasks in the Celery queue\")\nasync def purge_queue(current_user: User = Depends(get_current_user)):\n    \"\"\"\n    Purge all tasks in the Celery queue.\n    Only accessible to authenticated users.\n    \"\"\"\n    if not current_user:  # Add your own authentication checks\n        raise HTTPException(status_code=403, detail=\"Not authorized to purge queue\")\n\n    try:\n        purge_celery_queue()\n        return {\"status\": \"success\", \"message\": \"Celery queue purged successfully\"}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}"}
{"type": "source_file", "path": "worker/tasks.py", "content": "from celery import Celery\nfrom celery.result import AsyncResult\n\nfrom neo4j import GraphDatabase\nfrom neo4j.exceptions import Neo4jError\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import TokenTextSplitter\nfrom langchain.document_loaders import telegram\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain.chat_models import ChatOpenAI\n\nimport uuid\nimport logging \nfrom typing import List\nfrom dotenv import load_dotenv\nimport time\nfrom kombu.exceptions import OperationalError\nimport threading\n\nfrom config import AppConfig\nfrom .processing_functions import generate_questions, generate_summaries\n\n\n# Initialize environment variables if needed\nAppConfig.initialize_environment_variables()\nlogging.info(f\"Starting worker\")\n\n# Assuming you have a global variable to track the number of active tasks\nactive_tasks_lock = threading.Lock()\nactive_tasks_count = 0\nMAX_CONCURRENT_TASKS = 2  # Set your maximum concurrent tasks\n\n\ndef create_celery_app(broker_url, result_backend, max_retries=5, wait_seconds=5):\n    \"\"\"\n    Create and configure a Celery app, ensuring that the broker is available.\n    \"\"\"\n    # Wait for the RabbitMQ broker to be ready\n    for _ in range(max_retries):\n        try:\n            # Try to establish a connection to the broker\n            celery_temp = Celery(broker=broker_url)\n            celery_temp.connection().ensure_connection(max_retries=1)\n            print(\"Successfully connected to the broker!\")\n            break\n        except OperationalError:\n            print(f\"Broker connection failed. Retrying in {wait_seconds} seconds...\")\n            time.sleep(wait_seconds)\n    else:\n        raise Exception(\"Failed to connect to the broker after several attempts.\")\n\n    # Create the Celery app\n    celery_app = Celery(\"worker\", broker=broker_url, result_backend=result_backend)\n    celery_app.conf.task_routes = {\"celery_worker.test_celery\": \"celery\"}\n    celery_app.conf.update(task_track_started=True)\n\n    return celery_app\n\n# Define your broker and result backend URLs\nbroker_url = AppConfig.CELERY_BROKER_URL\nresult_backend_url = AppConfig.CELERY_RESULT_BACKEND_URL\n\n\n# Create the Celery app\ncelery_app = create_celery_app(broker_url, result_backend_url)\n# Set a lower acknowledgment timeout, for example, 300 seconds (5 minutes)\ncelery_app.conf.broker_transport_options = {'confirm_publish': True, 'acknowledgement_timeout': 300}\n# Set heartbeat interval and prefetch count\ncelery_app.conf.broker_heartbeat = 10  # seconds\ncelery_app.conf.worker_prefetch_multiplier = 1\n\n\nfrom celery.result import AsyncResult\nfrom celery.exceptions import TimeoutError, CeleryError\n\nfrom celery.app.control import Inspect\n\ndef purge_celery_queue():\n    i = Inspect(app=celery_app)\n    active_queues = i.active_queues()\n    if active_queues:\n        for queue in active_queues.keys():\n            celery_app.control.purge()\n\n\ndef get_task_info(task_id: str):\n    try:\n        task = AsyncResult(task_id)\n\n        # Task Not Ready\n        if not task.ready():\n            return {\"task_id\": str(task_id), \"status\": task.status}\n\n        # Task done: return the value\n        task_result = task.get(timeout=10)  # Set a timeout for task.get() if needed\n        return {\n            \"task_id\": str(task_id),\n            \"result\": task_result,\n            \"status\": task.status\n        }\n\n    except TimeoutError:\n        # Handle timeout exceptions\n        return {\n            \"task_id\": str(task_id),\n            \"error\": \"Timeout while retrieving the task result\",\n            \"status\": \"TIMEOUT\"\n        }\n\n    except CeleryError as e:\n        # Handle general Celery errors\n        return {\n            \"task_id\": str(task_id),\n            \"error\": str(e),\n            \"status\": \"ERROR\"\n        }\n\n    except Exception as e:\n        # Handle other exceptions\n        return {\n            \"task_id\": str(task_id),\n            \"error\": f\"An error occurred: {e}\",\n            \"status\": \"FAILURE\"\n        }\n\n# Set up Neo4j driver (replace with your actual connection details)\ndriver = GraphDatabase.driver(AppConfig.NEO4J_URI, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))\nllm = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n\n## Worker tasks\n\n@celery_app.task(name=\"celery_worker.test_celery\")\ndef divide(x, y):\n    import time\n    print(\"Starting divide task\")\n    time.sleep(5)\n    return x / y\n\n# Celery task for processing text\n@celery_app.task(bind=True, rate_limit=\"1/m\", name=\"celery_worker.process_text_task\")\ndef process_text_task(self, textToProcess: str, documentId: str, generateQuestions: bool, generateSummaries: bool):\n    logging.info(f\"Starting process for document {documentId}\")\n    self.update_state(state=AppConfig.PROCESSING_DOCUMENT, meta={\"documentId\": documentId})\n \n    global active_tasks_count\n\n    # Task Concurrency Management\n    with active_tasks_lock:\n        # Check if the maximum number of concurrent tasks has been reached\n        if active_tasks_count >= MAX_CONCURRENT_TASKS:\n            # Requeue or delay the task\n            raise self.retry(countdown=60)  # Retry after 60 seconds\n\n        # Increment the count of active tasks\n        active_tasks_count += 1\n\n    try: \n        doc = telegram.text_to_docs(textToProcess)  \n\n        # Setup splitters \n        parent_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n        child_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=24)\n        parent_documents = parent_splitter.split_documents(doc)\n\n        # Setup embeddings\n        embeddings = OpenAIEmbeddings(openai_api_key=AppConfig.OPENAI_API_KEY, embedding_dimension=AppConfig.EMBEDDING_DIMENSION)\n\n        # Iterate through parent and child chunks for document and generate structure\n        for i, parent in enumerate(parent_documents):\n\n            self.update_state(state=AppConfig.PROCESSING_PAGES, meta={\"page\": i+1, \"total_pages\": len(parent_documents), \"documentId\": documentId})\n            logging.info(f\"processing chunk {i+1} of {len(parent_documents)} for document {documentId}\")\n            \n            child_documents = child_splitter.split_documents([parent])\n            params = {\n                \"document_uuid\": documentId,\n                \"parent_uuid\": str(uuid.uuid4()),\n                \"name\": f\"Page {i+1}\",\n                \"parent_text\": parent.page_content,\n                \"parent_id\": i,\n                \"parent_embedding\": embeddings.embed_query(parent.page_content),\n                \"children\": [\n                    {\n                        \"text\": c.page_content,\n                        \"id\": str(uuid.uuid4()),\n                        \"name\": f\"{i}-{ic+1}\",\n                        \"embedding\": embeddings.embed_query(c.page_content),\n                    }\n                    for ic, c in enumerate(child_documents)\n                ],\n            }\n\n            try:\n                # Ingest data\n                with driver.session() as session :\n                    session.run(\n                    \"\"\"\n                        MERGE (p:Page {uuid: $parent_uuid})\n                        SET p.text = $parent_text,\n                        p.name = $name,\n                        p.type = \"Page\",\n                        p.datecreated= datetime(),\n                        p.source=$parent_uuid\n                        WITH p\n                        CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding) YIELD node\n                        WITH p\n                            MATCH (d:Document {uuid: $document_uuid})\n                            MERGE (d)-[:HAS_PAGE]->(p)\n                        WITH p \n                        UNWIND $children AS child\n                            MERGE (c:Child {uuid: child.id})\n                            SET \n                                c.text = child.text,\n                                c.name = child.name,\n                                c.source=child.id\n                            MERGE (c)<-[:HAS_CHILD]-(p)\n                            WITH c, child       \n                                CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n                            YIELD node\n                            RETURN count(*)\n                        \"\"\",\n                            params,\n                        )\n            except Neo4jError as e:\n                logging.error(f\"Neo4j error in document {documentId}, chunk {i+1}: {e}\")\n                raise    \n        \n        if generateQuestions:\n            generate_questions(self,llm, parent_documents, documentId, embeddings, driver)\n\n            \n        if generateSummaries:\n            generate_summaries(self, llm, parent_documents, documentId, embeddings, driver) \n\n    except Exception as e:\n        logging.error(f\"Failed to process document {documentId}: {e}\")\n        self.update_state(state=AppConfig.PROCESSING_FAILED, meta={\"documentId\": documentId})\n        return {\"message\": \"Failed\", \"error\": str(e), \"task_id\": self.request.id}\n    \n    finally:\n        with active_tasks_lock:\n            # Decrement the count of active tasks\n            active_tasks_count -= 1\n\n    self.update_state(state=AppConfig.PROCESSING_DONE, meta={\"documentId\": documentId})\n    logging.info(f\"Successfully processed document {documentId}\")\n    return {\"message\": \"Success\", \"uuid\": documentId, \"task_id\": self.request.id}\n\n\n"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/routers/document.py", "content": "from fastapi import APIRouter, Depends, HTTPException, status\nfrom bs4 import BeautifulSoup, Comment\nfrom typing import List\nfrom jose import JWTError, jwt\n\nimport uuid\nfrom datetime import datetime\nfrom urllib.parse import urlparse\nimport httpx\n\nfrom config import AppConfig\nfrom models import User, DocumentRequest, DefaultIcons, UserIn\nfrom worker.tasks import process_text_task, get_task_info, purge_celery_queue\nfrom app.routers.utils import get_current_user, get_user_from_db, neo4j_datetime_to_python_datetime\n\n\nfrom langchain.chat_models import ChatOpenAI\nfrom fastapi.security import OAuth2PasswordBearer\nfrom neo4j import GraphDatabase, Transaction  # Import Neo4j driver\n\nimport logging\n\n# Global Variables\nrouter = APIRouter()\nllm = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n# Initialize environment variables if needed\nAppConfig.initialize_environment_variables()\n\n\n# Inside documents.py\ndef extract_title(soup: BeautifulSoup, document_id: str) -> str:\n    title = soup.title.string if soup.title else None\n    if not title:\n        title = f\"Untitled Document {document_id}\"\n        meta_title = soup.find('meta', attrs={'property': 'og:title'})\n        if meta_title:\n            title = meta_title.get('content', title)\n    return title\n\n\ndef extract_primary_image(soup: BeautifulSoup) -> str:\n    default_image_url = DefaultIcons.ARTICLE_ICON_SVG\n    image = soup.find('meta', property='og:image')\n    if image and image.get('content'):\n        return image['content']\n    image = soup.find('img')\n    if image and image.get('src'):\n        return image['src']\n    return default_image_url  # Return a default image URL if no image is found\n\n\ndef extract_publisher(soup: BeautifulSoup, url: str) -> str:\n    publisher = soup.find('meta', property='og:site_name')\n    if publisher and publisher.get('content'):\n        return publisher['content']\n    domain = urlparse(url).netloc\n    if domain:\n        return domain.replace(\"www.\", \"\")\n    return ''\n\ndef extract_full_text(soup: BeautifulSoup) -> str:\n    # Remove unwanted tags:\n    for tag in soup.find_all(['script', 'style', 'meta', 'noscript']):\n        tag.extract()\n    \n    # Attempt to find the main document element based on common HTML structures.\n    # You may need to adjust the tag name and class name based on the specific HTML structure of the pages you're working with.\n    document_element = soup.find('div', {'class': 'document-content'})\n    if document_element:\n        return document_element.get_text(' ', strip=True)  # Use a space as the separator for text in different elements, and strip leading/trailing whitespace.\n\n    # If the main document element wasn't found, fall back to extracting all text.\n    return soup.get_text(' ', strip=True)\n\ndef normalize_whitespace(text: str) -> str:\n    return ' '.join(text.split())\n\ndef tag_visible(element):\n    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n        return False\n    if isinstance(element, Comment):\n        return False\n    return True\n\ndef remove_html_tags(text):\n    return BeautifulSoup(text, \"html.parser\").get_text()\n\ndef remove_non_ascii(text):\n    return ''.join(character for character in text if ord(character) < 128)\n\ndef clean_text(text: str) -> str:\n    text = remove_html_tags(text)\n    text = normalize_whitespace(text)\n    text = remove_non_ascii(text)\n    return text\n\n\ndef extract_thumbnail(soup: BeautifulSoup) -> str:\n    thumb = soup.find('meta', attrs={'name': 'thumbnail'})\n\n    if thumb:\n        return thumb.get('content', '')\n    # If no thumbnail meta tag is found, try fetching the primary image as a fallback\n    return extract_primary_image(soup)\n\n\n# ------------------------------------------------------------------------------------------------\n# REST Endpoints for documents\n@router.post(\"/add-document\",\n             summary=\"Allows for adding an document to the graph from specified URL\",\n             description=\"Take the specified uri and add the document to the graph using beautiful soup to extract the content\",\n             tags=[\"Documents\"]\n            )\nasync def add_document(request: DocumentRequest, current_user: User = Depends(get_current_user)):\n    \n    headers = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\"\n}\n    # Assuming you have a Neo4j driver instance\n    driver = GraphDatabase.driver(AppConfig.NEO4J_URI, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))\n\n    logging.basicConfig(level=logging.INFO)\n\n    logging.info(f\"Fetching document from {request.url}\")\n    url_str = str(request.url)\n    logging.info(f\"Fetching document from {url_str}\")\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url_str, headers=headers)\n        if response.status_code != 200:\n            logging.error(f\"Failed to fetch document from {url_str}: Status {response.status_code}\")\n            raise HTTPException(status_code=400, detail=f\"Could not fetch document from {url_str}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n\n    documentId=str(uuid.uuid4())\n    title = extract_title(soup, documentId)\n    text = extract_full_text(soup)\n    imageurl = extract_primary_image(soup)\n    publisher = extract_publisher(soup, url_str)\n    thumbnail = extract_thumbnail(soup)\n    wordcount = len(text.split())\n    note=request.note\n    logging.info(f\"Document {documentId} has {wordcount} words\")\n    utc_now = datetime.utcnow().strftime('%Y-%m-%dT%H:%M') + 'Z'  # No seconds or microseconds, append 'Z' for UTC\n    \n    query = \"\"\"\n    CREATE (d:Document {\n        uuid: $uuid,\n        name: $name,\n        url: $url,\n        text: $text,\n        note: $note,\n        imageurl: $imageurl,\n        publisher: $publisher,\n        addeddate: $addeddate,\n        thumbnail: $thumbnail,\n        wordcount: $wordcount,\n        type: \"Document\"\n    })\n    with d\n    MATCH (u:User {uuid: $useruuid})\n    MERGE (ua:UserAction {useruuid: u.uuid}) \n    ON CREATE SET ua.name = u.username, ua.uuid=randomUUID()\n    MERGE (u)-[r:HAS_ACTION]->(ua)\n    MERGE (ua)-[:ADDED]-(d) set r.dateadded= datetime()\n    \"\"\"\n    with driver.session() as session:\n        def create_document(tx: Transaction):\n            return tx.run(query, {\n                \"uuid\": documentId,\n                \"name\": title,\n                \"url\": request.url,\n                \"text\": text,\n                \"note\": note,\n                \"imageurl\": imageurl,\n                \"publisher\": publisher,\n                \"addeddate\": utc_now,\n                \"thumbnail\": thumbnail,\n                \"wordcount\": wordcount,\n                \"useruuid\": current_user.uuid\n            }).consume()\n        session.write_transaction(create_document)\n\n    try:\n        task_ids = []\n        logging.info(f\"Queueing document {documentId} for processing.\")\n        with driver.session() as session:\n            result = session.run(\"MATCH (a:Document {uuid: $uuid}) RETURN a\", {\"uuid\": documentId})\n            document_data = result.single().value()\n            text = document_data['text']\n            # Pass the generateQuestions and generateSummaries flags to the task\n            task = process_text_task.delay(text, documentId, True, True)\n            task_ids.append(task.id)\n            logging.info(f\"Queued document {documentId} with task ID {task.id}\")\n    except Exception as e:\n        logging.error(f\"Failed to queue document {documentId}: {e}\")\n    finally:\n        session.close()\n        driver.close()\n        return {\n            \"message\": f\"Processing started for {len(documentId)} documents\",\n            \"task_ids\": task_ids\n        }\n\n"}
{"type": "source_file", "path": "app/main.py", "content": "from fastapi import FastAPI\nfrom .routers import processing  \nfrom .routers import document  \nfrom .routers import chat  \n\napp = FastAPI()\n\n# Include the router from the processing module\napp.include_router(processing.router)\napp.include_router(document.router)\napp.include_router(chat.router)\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Welcome to Menome Processor API!\"}\n"}
{"type": "source_file", "path": "app/routers/chat.py", "content": "import os\nimport sys\nfrom fastapi import APIRouter, Body\nfrom config import AppConfig\nfrom app.routers.utils import fetch_node_properties_by_uuid, setup_graph_db\nfrom pydantic import BaseModel\nimport json\n\nfrom langchain.vectorstores import Neo4jVector\nfrom neo4j import GraphDatabase\nfrom neo4j.exceptions import ServiceUnavailable\n\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.chat_models import ChatOpenAI\nimport openai\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.pydantic_v1 import BaseModel\nfrom models import User\n\nfrom fastapi import Depends\nfrom app.routers.utils import get_current_user, get_user_from_db, neo4j_datetime_to_python_datetime\n\nimport logging \nimport time\n\nfrom fastapi import Query\n\nrouter = APIRouter()\n\n# Inside chat.py\nprint(\"Chat routes imported\")\n\n\n# Set the logging level\nlogging.basicConfig(level=logging.INFO)\n# Enable logging for Langchain\nlogging.getLogger(\"langchain\").setLevel(logging.INFO)\n\n\n# Initialze environment:\nos.environ[\"OPENAI_API_KEY\"] =AppConfig.OPENAI_API_KEY\nopenai.api_key =  AppConfig.OPENAI_API_KEY\n\n# Initialize Neo4j driver (do this once, e.g., at the top of your file or in another module)\nuri = AppConfig.NEO4J_URI\n\n# Add typing for input\nclass Question(BaseModel):\n    question: str\n\n\n\n# Try initializing the parent_retriever vector store\ntry:\n    # setup Parent retriever for advanced RAG pattern\n    parent_query = \"\"\"\n    MATCH (node)<-[:HAS_CHILD]-(parent)\n    WITH parent, max(score) AS score \n    RETURN parent.uuid as uuid, parent.uuid as source, parent.text AS text, score, {} AS metadata LIMIT 5\n    \"\"\"\n\n    parent_vectorstore = Neo4jVector.from_existing_index(\n        OpenAIEmbeddings(openai_api_key=AppConfig.OPENAI_API_KEY),\n        index_name=\"parent_document\",\n        url=AppConfig.NEO4J_URI,\n        username=AppConfig.NEO4J_USER,\n        password=AppConfig.NEO4J_PASSWORD,\n        retrieval_query=parent_query,\n        #text_node_property=\"text\",\n        #node_label=\"Child\",\n    )\nexcept ServiceUnavailable as e:\n    if \"Index not found\" in str(e):  # Replace with the appropriate error message for your setup\n        setup_graph_db()  # If the index does not exist, set it up\n    else:\n        raise e  # If the error is due to another reason, raise the exception\n\n\n# Try initializing the vector store\ntry:\n    # setup Parent retriever for advanced RAG pattern\n    parent_query = \"\"\"\n    MATCH (node)<-[:HAS_CHILD]-(parent)\n    WITH parent, max(score) AS score \n    RETURN parent.uuid as uuid, parent.uuid as source, parent.text AS text, score, {} AS metadata LIMIT 5\n    \"\"\"\n\n    typical_vectorstore = Neo4jVector.from_existing_index(\n        OpenAIEmbeddings(openai_api_key=AppConfig.OPENAI_API_KEY),\n        index_name=\"typical_rag\",\n        url=AppConfig.NEO4J_URI,\n        username=AppConfig.NEO4J_USER,\n        password=AppConfig.NEO4J_PASSWORD,\n        #retrieval_query=parent_query,\n        text_node_property=\"text\",\n        node_label=\"Child\",\n        \n    )\nexcept ServiceUnavailable as e:\n    if \"Index not found\" in str(e):  # Replace with the appropriate error message for your setup\n        setup_graph_db()  # If the index does not exist, set it up\n    else:\n        raise e  # If the error is due to another reason, raise the exception\n\n\nclass ChatRequest(BaseModel):\n    question: str\n\n\n@router.get(\"/chatSources\",\n             summary=\"Chat with source references\",\n             description=\"This endpoint provides a chat response along with sources of information. It uses the ChatOpenAI model for generating responses.\",\n             tags=[\"Chat\", \"Sources\"])\ndef chatSourcesquestion(question: str = Query(..., description=\"The question to be processed\"), current_user: User = Depends(get_current_user)):\n    driver = GraphDatabase.driver(uri, auth=(AppConfig.NEO4J_USER, AppConfig.NEO4J_PASSWORD))  \n\n    # Start measuring time\n    start_time = time.time()\n    request_payload = json.dumps({\"question\": question}).encode('utf-8')\n    request_payload_size = sys.getsizeof(request_payload)\n\n    # Generate a response in chatGPT style based on the user's question\n    chain = RetrievalQAWithSourcesChain.from_chain_type(\n        ChatOpenAI(temperature=1, max_tokens=4000, model_name=\"gpt-4-1106-preview\", openai_api_key=AppConfig.OPENAI_API_KEY),\n        chain_type=\"stuff\",\n        retriever=typical_vectorstore.as_retriever(search_kwargs={\"k\": 5, 'score_threshold': 0.5})\n    )\n\n    # Measure time after setting up the chain\n    setup_time = time.time()\n\n    langchain_response = chain({\"question\": question}, return_only_outputs=False)\n\n\n    # Measure time after getting the response\n    langchain_response_time = time.time()\n    langchain_response_payload = json.dumps(langchain_response).encode('utf-8')\n    langchain_response_payload_size = sys.getsizeof(langchain_response_payload)\n\n    # Extract 'answer' and 'sources' (UUIDs) from the chain response\n    answer = langchain_response.get('answer', '')\n    uuids = langchain_response.get('sources', [])\n\n    if isinstance(uuids, str):\n        uuids = [uuid.strip() for uuid in uuids.split(\",\")]\n\n    # Fetch node properties from Neo4j based on UUIDs\n    nodes_data = fetch_node_properties_by_uuid(driver, uuids)\n    nodes_data_payload = json.dumps(nodes_data).encode('utf-8')\n    nodes_data_payload_size = sys.getsizeof(nodes_data_payload)\n\n    # Measure time after fetching data from Neo4j\n    neo4j_fetch_time = time.time()\n\n    # Calculate elapsed times\n    setup_duration = setup_time - start_time\n    response_duration = langchain_response_time - setup_time\n    fetch_duration = neo4j_fetch_time - langchain_response_time\n    total_duration = neo4j_fetch_time - start_time\n    \n    driver.close()\n\n    return {\n        \"answer\": answer,\n        \"sources\": nodes_data,\n        \"timings\": {\n            \"setup_duration\": setup_duration,\n            \"langchain_response_duration\": response_duration,\n            \"neo4j_fetch_duration\": fetch_duration,\n            \"total_duration\": total_duration\n        },\n        \"payload_sizes\": {\n            \"request_size\": request_payload_size,\n            \"response_size\": langchain_response_payload_size,\n            \"db_response_size\": nodes_data_payload_size\n        }\n    }\n\n\n"}
