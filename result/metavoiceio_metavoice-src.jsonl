{"repo_info": {"repo_name": "metavoice-src", "repo_owner": "metavoiceio", "repo_url": "https://github.com/metavoiceio/metavoice-src"}}
{"type": "test_file", "path": "tests/llm/loaders/test_dataloader.py", "content": "import itertools\nfrom pathlib import Path\n\nimport pytest\nimport torch\nfrom huggingface_hub import snapshot_download\nfrom torch.utils.data import DataLoader\n\nfrom fam.llm.config.finetune_params import audio_token_mode as atm\nfrom fam.llm.config.finetune_params import num_max_audio_tokens_timesteps\nfrom fam.llm.loaders.training_data import DynamicComputeDataset\nfrom fam.llm.preprocessing.audio_token_mode import get_params_for_mode\n\n\n@pytest.mark.parametrize(\"dataset\", [\"tests/resources/datasets/sample_dataset.csv\"])\n@pytest.mark.skip(reason=\"Requires ckpt download, not feasible as test suite\")\ndef test_dataset_preprocess_e2e(dataset):\n    model_name = \"metavoiceio/metavoice-1B-v0.1\"\n    device = \"cuda\"\n    mode_params = get_params_for_mode(atm, num_max_audio_tokens_timesteps=num_max_audio_tokens_timesteps)\n\n    _model_dir = snapshot_download(repo_id=model_name)\n    checkpoint_path = Path(f\"{_model_dir}/first_stage.pt\")\n    spk_emb_ckpt_path = Path(f\"{_model_dir}/speaker_encoder.pt\")\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=False)\n    tokenizer_info = checkpoint.get(\"meta\", {}).get(\"tokenizer\", {})\n\n    dataset = DynamicComputeDataset.from_meta(\n        tokenizer_info,\n        mode_params[\"combine_func\"],\n        spk_emb_ckpt_path,\n        dataset,\n        mode_params[\"pad_token\"],\n        mode_params[\"ctx_window\"],\n        device\n    )\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n    result = next(iter(dataloader))\n\n    # TODO: better assertions based on sample input dims\n    assert len(result) == 2\n"}
{"type": "source_file", "path": "fam/llm/adapters/flattened_encodec.py", "content": "from fam.llm.adapters.base import BaseDataAdapter\n\n\nclass FlattenedInterleavedEncodec2Codebook(BaseDataAdapter):\n    def __init__(self, end_of_audio_token):\n        self._end_of_audio_token = end_of_audio_token\n\n    def decode(self, tokens: list[list[int]]) -> tuple[list[int], list[list[int]]]:\n        assert len(tokens) == 1\n        tokens = tokens[0]\n\n        text_ids = []\n        extracted_audio_ids = [[], []]\n\n        for t in tokens:\n            if t < self._end_of_audio_token:\n                extracted_audio_ids[0].append(t)\n            elif t >= self._end_of_audio_token and t < 2 * self._end_of_audio_token:\n                extracted_audio_ids[1].append(t - self._end_of_audio_token)\n            # We ignore t = 2 * self._end_of_audio_token, as it is the end of audio token\n            elif t > 2 * self._end_of_audio_token:\n                text_ids.append(t)\n\n        if len(set([len(x) for x in extracted_audio_ids])) != 1:\n            min_len = min([len(x) for x in extracted_audio_ids])\n            max_len = max([len(x) for x in extracted_audio_ids])\n            print(\"WARNING: Number of tokens at each hierarchy must be of the same length!\")\n            print(f\"Truncating to min length of {min_len} tokens from {max_len} max.\")\n            print([len(x) for x in extracted_audio_ids])\n            extracted_audio_ids = [x[:min_len] for x in extracted_audio_ids]\n\n        return text_ids[:-1], extracted_audio_ids\n\n    def encode(self, text_tokens: list[int], audio_tokens: list[list[int]]):\n        \"\"\"\n        Performs the required combination and padding as needed.\n        \"\"\"\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "fam/llm/mixins/causal.py", "content": "from typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom torch.nn import functional as F\n\n\ndef top_p_sample(prob_dist: torch.Tensor, top_p: float):\n    sorted_probs, sorted_indices = torch.sort(prob_dist, descending=True, dim=-1)\n    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)  # (b, vocab_size)\n\n    sorted_indices_to_remove = cum_sum_probs > top_p\n\n    # Shift the indices to the right to keep also the first token above the threshold\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0\n    sorted_indices_to_remove = sorted_indices_to_remove.bool()\n\n    # replace probs to be removed with 0 in the sorted_probs\n    sorted_probs[sorted_indices_to_remove] = 0\n\n    # reverse the sorting process\n    reversed_indices = torch.argsort(sorted_indices)\n    prob_dist = torch.gather(sorted_probs, -1, reversed_indices)\n\n    # normalize\n    prob_dist = prob_dist / prob_dist.sum(dim=-1, keepdim=True)\n\n    return prob_dist\n\n\nclass CausalInferenceMixin:\n    \"\"\"\n    Mixin class for performing inference in a causal language model.\n\n    This mixin provides methods for predicting the next token in a sequence, sampling from the model,\n    and applying token prediction masks.\n\n    Attributes:\n        None\n\n    Methods:\n        _sample_next_token: Predicts the next token in the sequence.\n        _create_token_pred_mask: Creates a token prediction mask based on sequence lengths.\n        _apply_token_pred_mask: Applies a token prediction mask to the next token predictions.\n        _sample_batch: Samples a batch of tokens from the model.\n        _sort_for_batching: Sorts the input sequences for efficient batching.\n        _causal_sample: Generates a sequence of tokens using causal sampling.\n\n    \"\"\"\n\n    @torch.no_grad()\n    def _sample_next_token(\n        self,\n        *,\n        idx: torch.Tensor,\n        speaker_embs: Optional[torch.Tensor],\n        temperature: float,\n        top_k: Optional[int],\n        top_p: Optional[float],\n        guidance_scale: Optional[Tuple[float, float]],\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict the next token in the sequence.\n\n        Args:\n            idx (torch.Tensor): Initial sequence indices of shape (batch, num_hierarchies, time).\n            speaker_embs (Optional[torch.Tensor]): Speaker embeddings. Set to `None` if using an unconditional model.\n            temperature (float): Sampling temperature.\n            top_k (Optional[int]): Top-k filtering threshold. Set to `None` to disable top-k filtering.\n            top_p (Optional[float]): Nucleus sampling threshold. Set to `None` to disable it.\n            guidance_scale (Optional[float]): Scale factor for the guidance loss. Set to `None` to disable guidance.\n\n        Returns:\n            torch.Tensor: Next index in the sequence after sampling. Shape: (batch, num_hierarchies).\n        \"\"\"\n        if top_k is not None and top_p is not None:\n            raise ValueError(\"Only one of top_k and top_p can be set\")\n\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = idx if idx.size(-1) <= self.config.block_size else idx[:, :, -self.config.block_size :]\n\n        # forward the model to get the logits for the index in the sequence\n        list_logits, _ = self(\n            idx_cond, speaker_embs=speaker_embs\n        )  # list with len num_hierarchies of (b,1,vocab_size) tensors\n\n        if guidance_scale is not None:\n            spkemb_guidance_scale, prompt_guidance_scale = guidance_scale\n            assert spkemb_guidance_scale >= 1\n            assert prompt_guidance_scale >= 1\n            base_scale = spkemb_guidance_scale + prompt_guidance_scale - 1\n\n            for i, logits in enumerate(list_logits):\n                if prompt_guidance_scale > 1:\n                    logits_cond, logits_uncond_spkemb, logits_uncond_prompt = logits.split(logits.shape[0] // 3, dim=0)\n                else:\n                    logits_cond, logits_uncond_spkemb = logits.split(logits.shape[0] // 2, dim=0)\n                    logits_uncond_prompt = 0\n                list_logits[i] = (\n                    (base_scale) * logits_cond\n                    + (1 - spkemb_guidance_scale) * logits_uncond_spkemb\n                    + (1 - prompt_guidance_scale) * logits_uncond_prompt\n                )\n\n        # pluck the logits at the final step and scale by desired temperature\n        list_logits = [\n            logits[:, -1, :] / temperature for logits in list_logits\n        ]  # list with len num_hierarchies of (b,vocab_size) tensors\n\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            for i in range(len(list_logits)):\n                logits = list_logits[i]\n                v, _ = torch.topk(\n                    logits, min(top_k, logits.size(-1))\n                )  # returns a descending sorted list of values and indices of top_k values\n                logits[logits < v[:, [-1]]] = -float(\"Inf\")  # set all logits below the smallest top_k value to -Inf\n                list_logits[i] = logits\n\n        # apply softmax to convert logits to (normalized) probabilities\n        probs = [\n            F.softmax(logits, dim=-1) for logits in list_logits\n        ]  # list of len num_hierarchies of (b,vocab_size) tensors\n\n        if top_p is not None:\n            for i in range(len(probs)):\n                probs[i] = top_p_sample(probs[i], top_p)\n\n        # sample from the distribution\n        idx_next = [\n            torch.multinomial(prob, num_samples=1) for prob in probs\n        ]  # list of len num_hierarchies of (b,1) tensors\n        idx_next = torch.cat(idx_next, dim=-1)  # (b, num_hierarchies) tensor\n\n        return idx_next  # (b, num_hierarchies) tensor\n\n    @torch.no_grad()\n    def _create_token_pred_mask(self, idx: torch.Tensor, seq_lens: list[int]) -> torch.Tensor:\n        \"\"\"\n        Creates a token prediction mask based on sequence lengths.\n\n        Args:\n            idx (torch.Tensor): Initial sequence indices of shape (batch, num_hierarchies, time).\n            seq_lens (list[int]): List of sequence lengths for each sequence in idx.\n\n        Returns:\n            torch.Tensor: Token prediction mask of shape (batch, time).\n        \"\"\"\n        token_pred_mask = torch.zeros((idx.shape[0], idx.shape[-1]), dtype=torch.bool, device=idx.device)\n        for i in range(len(seq_lens)):\n            token_pred_mask[i, : seq_lens[i]] = True\n\n        assert (token_pred_mask[:, : min(seq_lens)] == 1).all()\n\n        return token_pred_mask\n\n    @torch.no_grad()\n    def _apply_token_pred_mask(\n        self, *, idx_next: torch.Tensor, orig_input_at_t: torch.Tensor, token_pred_mask_at_t: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Applies a token prediction mask to the next token predictions.\n\n        Args:\n            idx_next (torch.Tensor): Next token predictions of shape (batch, num_hierarchies).\n            orig_input_at_t (torch.Tensor): Original input at time step t of shape (batch, num_hierarchies).\n            token_pred_mask_at_t (torch.Tensor): Token prediction mask at time step t of shape (batch, 1).\n\n        Returns:\n            torch.Tensor: Updated next token predictions after applying the token prediction mask.\n        \"\"\"\n        idx_next = idx_next * (~token_pred_mask_at_t) + orig_input_at_t * token_pred_mask_at_t\n\n        return idx_next\n\n    @torch.no_grad()\n    def _sample_batch(\n        self,\n        *,\n        idx: torch.Tensor,\n        max_new_tokens: int,\n        seq_lens: list[int],\n        temperature: float,\n        top_k: Optional[int],\n        top_p: Optional[float],\n        speaker_embs: Optional[torch.Tensor],\n        guidance_scale: Optional[Tuple[float, float]],\n        end_of_audio_token: int,\n        end_of_text_token: int,\n    ):\n        \"\"\"\n        Samples a batch of tokens from the model.\n\n        Args:\n            idx (torch.Tensor): Initial sequence indices of shape (batch, num_hierarchies, time).\n            max_new_tokens (int): Maximum number of NEW tokens to generate (in addition to largest sequence in idx).\n            seq_lens (list[int]): List of sequence lengths for each sequence in idx.\n            temperature (float): Sampling temperature.\n            top_k (Optional[int]): Top-k filtering threshold. Set to `None` to disable top-k filtering.\n            top_p (Optional[float]): Nucleus sampling threshold. Set to `None` to disable it.\n            speaker_embs (Optional[torch.Tensor]): Speaker embeddings. Set to `None` if using an unconditional model.\n            guidance_scale (Optional[float]): Scale factor for the guidance loss. Set to `None` to disable guidance.\n\n        Returns:\n            torch.Tensor: Generated sequence indices of shape (batch, num_hierarchies, time).\n        \"\"\"\n        assert max(seq_lens) <= idx.shape[-1]\n        token_pred_mask = self._create_token_pred_mask(idx, seq_lens)\n        input = torch.clone(idx)\n\n        min_seq_lens = min(seq_lens)\n        idx = idx[:, :, :min_seq_lens]\n        idx_out = torch.full(\n            (idx.shape[0], idx.shape[1], idx.shape[2] + max_new_tokens),\n            end_of_audio_token,\n            dtype=idx.dtype,\n            device=idx.device,\n        )\n        idx_out[:, :, :min_seq_lens] = idx\n        terminated = idx.new_zeros(idx.shape[0], dtype=torch.bool)\n\n        if guidance_scale is not None:\n            _, prompt_guidance_scale = guidance_scale\n            if speaker_embs is None:\n                raise Exception(\"Guidance is only supported for conditional models\")\n\n            # create speaker embeddings equivalent to the batch size, filling with None\n            # for second half to do unconditional generation.\n            speaker_embs = (\n                list(speaker_embs)\n                + [None] * (speaker_embs.shape[0])\n                + (list(speaker_embs) if prompt_guidance_scale > 1 else [])\n            )\n\n        for timestep in tqdm.tqdm(range(min_seq_lens, min_seq_lens + max_new_tokens), desc=\"tokens: \"):\n            if terminated.all():\n                break\n            if (self.kv_cache_enabled is True) and (timestep > min_seq_lens):\n                idx_input = idx_out[:, :, [timestep - 1]]\n            else:\n                idx_input = idx_out[:, :, :timestep]\n\n            if guidance_scale is not None:\n                _, prompt_guidance_scale = guidance_scale\n                # TODO: fix: will cause a problem with kv-caching as it's not expecting larger batch-size.\n                if timestep == min_seq_lens:\n                    print(\"[hack!!!!] Guidance is on, so we're doubling/tripling batch size!\")\n\n                # replicate idx in the batch dimension\n                idx_input = (\n                    idx_input.unsqueeze(0)\n                    .repeat(3 if prompt_guidance_scale > 1 else 2, 1, 1, 1)\n                    .reshape(-1, idx_input.shape[1], idx_input.shape[2])\n                )\n\n                if prompt_guidance_scale > 1:\n                    idx_input_uncond = idx_input[idx_input.shape[0] // 3 * 2 :]\n                    idx_input_uncond = idx_input_uncond.view(-1)\n                    # Replace all text tokens with endoftext token\n                    idx_input_uncond[idx_input_uncond > end_of_audio_token] = end_of_text_token\n\n            idx_next = self._sample_next_token(\n                idx=idx_input,\n                speaker_embs=speaker_embs,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                guidance_scale=guidance_scale,\n            )  # (b, num_hierarchies)\n\n            assert idx_next.shape[0] == idx.shape[0]\n\n            if timestep < token_pred_mask.shape[-1]:\n                idx_next = self._apply_token_pred_mask(\n                    idx_next=idx_next,\n                    orig_input_at_t=input[:, :, timestep],\n                    token_pred_mask_at_t=token_pred_mask[:, [timestep]],\n                )\n            is_endofaudio = (idx_next == end_of_audio_token).any(dim=-1)  # shape: b\n            terminated = terminated | is_endofaudio\n            idx_next[terminated] = end_of_audio_token\n            # append sampled index to the running sequence and continue\n            idx_out[:, :, timestep] = idx_next\n\n        return idx_out\n\n    @torch.no_grad()\n    def _sort_for_batching(\n        self,\n        *,\n        idx: torch.Tensor,\n        seq_lens: list[int],\n        speaker_embs: Optional[torch.Tensor],\n        batch_size: int,\n        max_new_tokens: int,\n    ) -> Tuple[list[int], list[int], torch.Tensor, list[int], Optional[torch.Tensor], int]:\n        \"\"\"\n        Sorts the input sequences for efficient batching.\n\n        Args:\n            idx (torch.Tensor): Initial sequence indices of shape (batch, num_hierarchies, time).\n            seq_lens (list[int]): List of sequence lengths for each sequence in idx.\n            speaker_embs (Optional[torch.Tensor]): Speaker embeddings. Set to `None` if using an unconditional model.\n            batch_size (int): Batch size for sampling. idx is split into batches of this size for sampling.\n            max_new_tokens (int): Maximum number of NEW tokens to generate (in addition to largest sequence in idx).\n\n        Returns:\n            Tuple[list[int], list[int], torch.Tensor, list[int], Optional[torch.Tensor], int]:\n                - sorted_indices (list[int]): List of indices of the input sequences that transform it into sorted order.\n                - invert_sorted_indices (list[int]): List of indices to invert the sorted sequences back to the original order.\n                - idx (torch.Tensor): Input sequence indices in sorted order.\n                - seq_lens (list[int]): Sequence lengths in sorted order.\n                - speaker_embs (Optional[torch.Tensor]): speaker embeddings in sorted order.\n                - max_token_len (int): Effective maximum number of tokens to generate.\n        \"\"\"\n        assert len(seq_lens) == idx.shape[0]\n        assert max(seq_lens) <= idx.shape[-1]\n\n        sorted_indices = np.argsort(seq_lens)\n        inverted_sorted_indices = np.zeros(len(seq_lens), dtype=np.int32)\n        inverted_sorted_indices[sorted_indices] = np.arange(len(seq_lens), dtype=np.int32)\n\n        idx = idx[sorted_indices]\n        seq_lens = [seq_lens[i] for i in sorted_indices]\n        speaker_embs = speaker_embs[sorted_indices] if speaker_embs is not None else None\n        max_token_len = 0\n\n        # figure out effective max_tokens to generate\n        for start_index in range(0, len(seq_lens), batch_size):\n            end_index = min(start_index + batch_size, len(seq_lens))\n            batch_seq_lens = seq_lens[start_index:end_index]\n            # random heuristic...\n            # # TODO: fix!\n            max_token_len = max(max_token_len, min(batch_seq_lens) + max_new_tokens)\n\n        return sorted_indices, inverted_sorted_indices, idx, seq_lens, speaker_embs, max_token_len\n\n    @torch.no_grad()\n    def _causal_sample(\n        self,\n        *,\n        idx: torch.Tensor,\n        max_new_tokens: int,\n        seq_lens: list[int],\n        temperature: float,\n        top_k: Optional[int],\n        top_p: Optional[float],\n        speaker_embs: Optional[torch.Tensor],\n        batch_size: int,\n        guidance_scale: Optional[Tuple[float, float]] = None,\n        dtype: torch.dtype = torch.bfloat16,\n        end_of_audio_token: int,\n        end_of_text_token: int,\n    ) -> torch.Tensor:\n        \"\"\"\n        Generates a sequence of tokens using causal sampling.\n\n        Args:\n            idx (torch.Tensor): Initial sequence indices of shape (batch, num_hierarchies, time).\n            max_new_tokens (int): Maximum number of NEW tokens to generate (in addition to largest sequence in idx).\n            seq_lens (list[int]): List of sequence lengths for each sequence in idx.\n            temperature (float): Sampling temperature.\n            top_k (Optional[int]): Top-k filtering threshold. Set to `None` to disable top-k filtering.\n            top_p (Optional[float]): Nucleus sampling threshold. Set to `None` to disable it.\n            speaker_embs (Optional[torch.Tensor]): Speaker embeddings. Set to `None` if using an unconditional model.\n            batch_size (int): Batch size for sampling. idx is split into batches of this size for sampling.\n            guidance_scale (Optional[float]): Scale factor for the guidance loss. Set to `None` to disable guidance.\n\n        Returns:\n            torch.Tensor: Generated sequence indices of shape (batch, num_hierarchies, time).\n        \"\"\"\n        (\n            _,\n            invert_sorted_indices,\n            idx,\n            seq_lens,\n            speaker_embs,\n            max_token_len,\n        ) = self._sort_for_batching(\n            idx=idx, seq_lens=seq_lens, speaker_embs=speaker_embs, batch_size=batch_size, max_new_tokens=max_new_tokens\n        )\n\n        return_idx = torch.zeros((len(seq_lens), idx.size(1), max_token_len), dtype=torch.long, device=idx.device)\n\n        for start_index in tqdm.tqdm(range(0, len(seq_lens), batch_size), desc=\"batch: \"):\n            end_index = min(start_index + batch_size, len(seq_lens))\n\n            kv_batch_size = end_index - start_index\n            if guidance_scale is not None:\n                if guidance_scale[1] > 1:\n                    kv_batch_size = 3 * kv_batch_size\n                else:\n                    kv_batch_size = 2 * kv_batch_size\n\n            if self.kv_cache_enabled:\n                self.empty_kv_cache(\n                    batch_size=kv_batch_size,\n                    kv_cache_maxlen=self.config.block_size,\n                    dtype=dtype,\n                )\n\n            batch_seq_lens = seq_lens[start_index:end_index]\n            batch_max_new_tokens = max_token_len - min(batch_seq_lens)\n\n            batch_idx = idx[start_index:end_index]\n            batch_speaker_embs = speaker_embs[start_index:end_index] if speaker_embs is not None else None\n\n            batch_idx = self._sample_batch(\n                idx=batch_idx,\n                max_new_tokens=batch_max_new_tokens,\n                seq_lens=batch_seq_lens,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                speaker_embs=batch_speaker_embs,\n                guidance_scale=guidance_scale,\n                end_of_audio_token=end_of_audio_token,\n                end_of_text_token=end_of_text_token,\n            )\n            return_idx[start_index:end_index] = batch_idx\n\n        return return_idx[invert_sorted_indices]\n\n    def empty_kv_cache(self, *, batch_size: int, kv_cache_maxlen: int, dtype: torch.dtype):\n        \"\"\"\n        Empties key-value (KV) cache for causal attention.\n\n        Args:\n            batch_size (int): The batch size.\n            kv_cache_maxlen (int): The maximum length of the KV cache.\n            dtype (torch.dtype): The data type of the KV cache.\n\n        Raises:\n            Exception: If KV cache is enabled for non-causal attention.\n\n        \"\"\"\n        if self.kv_cache_enabled is False:\n            raise Exception(\"KV cache is not enabled\")\n        if self.config.causal is False:\n            raise Exception(\"KV cache is not supported for non-causal attention\")\n\n        self.kv_pos = 0\n        for block in self.transformer.h:\n            block.attn.empty_kv_cache(batch_size=batch_size, kv_cache_maxlen=kv_cache_maxlen, dtype=dtype)\n\n    def enable_kv_cache(self):\n        \"\"\"\n        Enables key-value (KV) cache for causal attention.\n\n        Raises:\n            Exception: If KV cache is enabled for non-causal attention.\n\n        \"\"\"\n        if self.config.causal is False:\n            raise Exception(\"KV cache is not supported for non-causal attention\")\n\n        self.kv_cache_enabled = True\n        for block in self.transformer.h:\n            block.attn.kv_cache_enabled = True\n\n    def disable_kv_cache(self):\n        \"\"\"\n        Disables the key-value cache for the transformer and all its blocks.\n        \"\"\"\n        self.kv_cache_enabled = False\n        for block in self.transformer.h:\n            block.attn.kv_cache_enabled = False\n            block.attn.kv_cache = None\n            block.attn.kv_cache_first_empty_index = 0\n\n    @torch.no_grad()\n    def _slow_causal_sampling_loop(\n        self,\n        idx: torch.Tensor,\n        max_new_tokens: int,\n        temperature: float = 1.0,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        speaker_embs: Optional[torch.Tensor] = None,\n        guidance_scale: Optional[float] = None,\n    ):\n        \"\"\"\n        Old non-batched version of causal sampling. Kept for testing / reference.\n\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,n_head,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        assert idx.dim() == 3, \"idx must be a batch of sequences of hierarchical tokens\"\n        assert idx.size(0) == 1, \"can only do one sequence at a time for now\"\n        assert top_p is None, \"nucleus sampling not supported yet with _slow_causal_sampling_loop\"\n\n        if self.config.causal is not True:\n            raise Exception(\"Causal sampling is only supported for causal models\")\n\n        if self.kv_cache_enabled:\n            print(\"!!!! USING KV-CACHING ASSUMED TORCH.BFLOAT16\")\n            self.empty_kv_cache(\n                batch_size=1,\n                kv_cache_maxlen=self.config.block_size,\n                dtype=torch.bfloat16,\n            )\n\n        for i in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(-1) <= self.config.block_size else idx[:, -self.config.block_size :]\n\n            if self.kv_cache_enabled:\n                if i > 0:\n                    idx_cond = idx_cond[:, :, -1:]\n\n            # forward the model to get the logits for the index in the sequence\n            list_logits, _ = self(idx_cond, speaker_embs=speaker_embs)\n\n            if guidance_scale is not None:\n                # we've already checked that kv-caching is not switched on\n                # so this should be ok.\n                list_logits_uncond, _ = self(idx_cond, speaker_embs=None)\n                list_logits = [\n                    (guidance_scale) * logits + (1 - guidance_scale) * logits_uncond\n                    for logits, logits_uncond in zip(list_logits, list_logits_uncond)\n                ]\n\n            # pluck the logits at the final step and scale by desired temperature\n            list_logits = [logits[:, -1, :] / temperature for logits in list_logits]\n\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                for i in range(len(list_logits)):\n                    logits = list_logits[i]\n                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                    logits[logits < v[:, [-1]]] = -float(\"Inf\")\n                    list_logits[i] = logits\n\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = [F.softmax(logits, dim=-1) for logits in list_logits]\n            # sample from the distribution\n            idx_next = torch.tensor(\n                [torch.multinomial(prob, num_samples=1) for prob in probs], device=idx.device\n            )  # (c, 1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next.unsqueeze(0).unsqueeze(-1)), dim=2)\n\n        return idx\n"}
{"type": "source_file", "path": "fam/llm/adapters/tilted_encodec.py", "content": "from fam.llm.adapters.base import BaseDataAdapter\n\n\nclass TiltedEncodec(BaseDataAdapter):\n    def __init__(self, end_of_audio_token):\n        self._end_of_audio_token = end_of_audio_token\n\n    def decode(self, tokens: list[list[int]]) -> tuple[list[int], list[list[int]]]:\n        assert len(tokens) > 1\n\n        text_ids = []\n        extracted_audio_ids = []\n\n        extracted_audio_ids.append([])\n        # Handle first hierarchy as special case as it contains text tokens as well\n        # TODO: maybe it doesn't need special case, and can be handled on it's own :)\n        for t in tokens[0]:\n            if t > self._end_of_audio_token:\n                text_ids.append(t)\n            elif t < self._end_of_audio_token:\n                extracted_audio_ids[0].append(t)\n\n        # Handle the rest of the hierarchies\n        for i in range(1, len(tokens)):\n            token_hierarchy_ids = tokens[i]\n            extracted_audio_ids.append([])\n            for t in token_hierarchy_ids:\n                if t < self._end_of_audio_token:\n                    extracted_audio_ids[i].append(t)\n\n        if len(set([len(x) for x in extracted_audio_ids])) != 1:\n            min_len = min([len(x) for x in extracted_audio_ids])\n            max_len = max([len(x) for x in extracted_audio_ids])\n            print(\"WARNING: Number of tokens at each hierarchy must be of the same length!\")\n            print(f\"Truncating to min length of {min_len} tokens from {max_len} max.\")\n            print([len(x) for x in extracted_audio_ids])\n            extracted_audio_ids = [x[:min_len] for x in extracted_audio_ids]\n\n        return text_ids[:-1], extracted_audio_ids\n\n    def encode(self, text_tokens: list[int], audio_tokens: list[list[int]]):\n        \"\"\"\n        Performs the required combination and padding as needed.\n        \"\"\"\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "fam/llm/fast_inference_utils.py", "content": "# Copyright (c) MetaVoice Labs Inc., Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted\n# provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this list of\n# conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice, this\n# list of conditions and the following disclaimer in the documentation and/or other\n# materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its contributors\n# may be used to endorse or promote products derived from this software without\n# specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR\n# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nimport itertools\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Literal, Optional, Tuple\n\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nimport tqdm\n\nfrom fam.llm.fast_quantize import WeightOnlyInt4QuantHandler, WeightOnlyInt8QuantHandler\n\n\ndef device_sync(device):\n    if \"cuda\" in device:\n        torch.cuda.synchronize()\n    elif \"cpu\" in device:\n        pass\n    else:\n        print(f\"device={device} is not yet suppported\")\n\n\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\ntorch._inductor.config.fx_graph_cache = (\n    True  # Experimental feature to reduce compilation times, will be on by default in future\n)\n\n# imports need to happen after setting above flags\nfrom fam.llm.fast_model import Transformer\nfrom fam.quantiser.audio.speaker_encoder.model import SpeakerEncoder\nfrom fam.quantiser.text.tokenise import TrainedBPETokeniser\n\n\ndef multinomial_sample_one_no_sync(\n    probs_sort,\n):  # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\n\ndef top_p_sample(logits: torch.Tensor, top_p: torch.Tensor):\n    # ref: huggingface/transformers\n\n    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n\n    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n    # Keep at least min_tokens_to_keep\n    sorted_indices_to_remove[-1:] = 0\n\n    # scatter sorted tensors to original indexing\n    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n    scores = logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n    return scores\n\n\ndef logits_to_probs(\n    logits,\n    *,\n    temperature: torch.Tensor,\n    top_p: Optional[torch.Tensor] = None,\n    top_k: Optional[torch.Tensor] = None,\n):\n    logits = logits / torch.max(temperature, 1e-5 * torch.ones_like(temperature))\n\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        pivot = v.select(-1, -1).unsqueeze(-1)\n        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n\n    if top_p is not None:\n        logits = top_p_sample(logits, top_p)\n\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n\n    return probs\n\n\ndef sample(\n    logits,\n    guidance_scale: torch.Tensor,\n    temperature: torch.Tensor,\n    top_p: Optional[torch.Tensor] = None,\n    top_k: Optional[torch.Tensor] = None,\n):\n    # (b, t, vocab_size)\n    logits = logits[:, -1]\n    logits_cond, logits_uncond_spkemb = logits.split(logits.size(0) // 2, dim=0)\n    logits = guidance_scale * logits_cond + (1 - guidance_scale) * logits_uncond_spkemb\n    probs = logits_to_probs(logits[0], temperature=temperature, top_p=top_p, top_k=top_k)\n    idx_next = multinomial_sample_one_no_sync(probs)\n    return idx_next, probs\n\n\ndef prefill(\n    model: Transformer,\n    x: torch.Tensor,\n    spk_emb: torch.Tensor,\n    input_pos: torch.Tensor,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    # input_pos: [B, S]\n    logits = model(x, spk_emb, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\n\ndef decode_one_token(\n    model: Transformer,\n    x: torch.Tensor,\n    spk_emb: torch.Tensor,\n    input_pos: torch.Tensor,\n    **sampling_kwargs,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    logits = model(x, spk_emb, input_pos)\n    return sample(logits, **sampling_kwargs)\n\n\ndef decode_n_tokens(\n    model: Transformer,\n    cur_token: torch.Tensor,\n    spk_emb: torch.Tensor,\n    input_pos: torch.Tensor,\n    num_new_tokens: int,\n    callback=lambda _: _,\n    return_probs: bool = False,\n    end_of_audio_token: int = 2048,\n    **sampling_kwargs,\n):\n    new_tokens, new_probs = [], []\n    for i in tqdm.tqdm(range(num_new_tokens)):\n        if (cur_token == end_of_audio_token).any():\n            break\n        with torch.backends.cuda.sdp_kernel(\n            enable_flash=False, enable_mem_efficient=False, enable_math=True\n        ):  # Actually better for Inductor to codegen attention here\n            next_token, next_prob = decode_one_token(model, cur_token, spk_emb, input_pos, **sampling_kwargs)\n            input_pos += 1\n            new_tokens.append(next_token.clone())\n            callback(new_tokens[-1])\n            if return_probs:\n                new_probs.append(next_prob.clone())\n            cur_token = next_token.view(1, -1).repeat(2, 1)\n\n    return new_tokens, new_probs\n\n\ndef model_forward(model, x, spk_emb, input_pos):\n    return model(x, spk_emb, input_pos)\n\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    spk_emb: torch.Tensor,\n    *,\n    max_new_tokens: Optional[int] = None,\n    callback=lambda x: x,\n    end_of_audio_token: int = 2048,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(0)\n    if max_new_tokens is None:\n        max_seq_length = model.config.block_size\n    else:\n        max_seq_length = T + max_new_tokens\n        max_seq_length = min(max_seq_length, model.config.block_size)\n    max_new_tokens = max_seq_length - T\n    if max_new_tokens <= 0:\n        raise ValueError(\"Prompt is too long to generate more tokens\")\n\n    device, dtype = prompt.device, prompt.dtype\n\n    seq = torch.clone(prompt)\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt.view(1, -1).repeat(2, 1), spk_emb, input_pos, **sampling_kwargs)\n    seq = torch.cat([seq, next_token.view(1)])\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n\n    generated_tokens, _ = decode_n_tokens(\n        model,\n        next_token.view(1, -1).repeat(2, 1),\n        spk_emb,\n        input_pos,\n        max_new_tokens - 1,\n        callback=callback,\n        end_of_audio_token=end_of_audio_token,\n        **sampling_kwargs,\n    )\n    seq = torch.cat([seq, torch.cat(generated_tokens)])\n\n    return seq\n\n\ndef encode_tokens(tokenizer: TrainedBPETokeniser, text: str, device=\"cuda\") -> torch.Tensor:\n    tokens = tokenizer.encode(text)\n    return torch.tensor(tokens, dtype=torch.int, device=device)\n\n\ndef _load_model(\n    checkpoint_path, spk_emb_ckpt_path, device, precision, quantisation_mode: Optional[Literal[\"int4\", \"int8\"]] = None\n):\n    ##### MODEL\n    with torch.device(\"meta\"):\n        model = Transformer.from_name(\"metavoice-1B\")\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=False)\n    state_dict = checkpoint[\"model\"]\n    # convert MetaVoice-1B model weights naming to gptfast naming\n    unwanted_prefix = \"_orig_mod.\"\n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n    state_dict[\"tok_embeddings.weight\"] = state_dict.pop(\"transformer.wtes.0.weight\")\n    state_dict[\"pos_embeddings.weight\"] = state_dict.pop(\"transformer.wpe.weight\")\n    state_dict[\"output.weight\"] = state_dict.pop(\"lm_heads.0.weight\")\n    state_dict[\"norm.weight\"] = state_dict.pop(\"transformer.ln_f.weight\")\n    for k, v in list(state_dict.items()):\n        if k.startswith(\"transformer.h.\"):\n            state_dict[k.replace(\"transformer.h.\", \"layers.\")] = state_dict.pop(k)\n            k = k.replace(\"transformer.h.\", \"layers.\")\n        if \".attn.c_attn.\" in k:\n            state_dict[k.replace(\".attn.c_attn.\", \".attention.wqkv.\")] = state_dict.pop(k)\n            k = k.replace(\".attn.c_attn.\", \".attention.wqkv.\")\n        if \".attn.c_proj.\" in k:\n            state_dict[k.replace(\".attn.c_proj.\", \".attention.wo.\")] = state_dict.pop(k)\n            k = k.replace(\".attn.c_proj.\", \".attention.wo.\")\n        if \".mlp.swiglu.w1.\" in k:\n            state_dict[k.replace(\".mlp.swiglu.w1.\", \".feed_forward.swiglu.w1.\")] = state_dict.pop(k)\n            k = k.replace(\".mlp.swiglu.w1.\", \".feed_forward.swiglu.w1.\")\n        if \".mlp.swiglu.w3.\" in k:\n            state_dict[k.replace(\".mlp.swiglu.w3.\", \".feed_forward.swiglu.w3.\")] = state_dict.pop(k)\n            k = k.replace(\".mlp.swiglu.w3.\", \".feed_forward.swiglu.w3.\")\n        if \".ln_1.\" in k:\n            state_dict[k.replace(\".ln_1.\", \".attention_norm.\")] = state_dict.pop(k)\n            k = k.replace(\".ln_1.\", \".attention_norm.\")\n        if \".ln_2.\" in k:\n            state_dict[k.replace(\".ln_2.\", \".ffn_norm.\")] = state_dict.pop(k)\n            k = k.replace(\".ln_2.\", \".ffn_norm.\")\n        if \".mlp.c_proj.\" in k:\n            state_dict[k.replace(\".mlp.c_proj.\", \".feed_forward.w2.\")] = state_dict.pop(k)\n            k = k.replace(\".mlp.c_proj.\", \".feed_forward.w2.\")\n\n    model.load_state_dict(state_dict, assign=True)\n    model = model.to(device=device, dtype=torch.bfloat16)\n\n    if quantisation_mode == \"int8\":\n        warnings.warn(\n            \"int8 quantisation is slower than bf16/fp16 for undebugged reasons! Please set optimisation_mode to `None` or to `int4`.\"\n        )\n        warnings.warn(\n            \"quantisation will degrade the quality of the audio! Please set optimisation_mode to `None` for best quality.\"\n        )\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        quantized_state_dict = simple_quantizer.create_quantized_state_dict()\n        model = simple_quantizer.convert_for_runtime()\n        model.load_state_dict(quantized_state_dict, assign=True)\n        model = model.to(device=device, dtype=torch.bfloat16)\n        # TODO: int8/int4 doesn't decrease VRAM usage substantially... fix that (might be linked to kv-cache)\n        torch.cuda.empty_cache()\n    elif quantisation_mode == \"int4\":\n        warnings.warn(\n            \"quantisation will degrade the quality of the audio! Please set optimisation_mode to `None` for best quality.\"\n        )\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize=128)\n        quantized_state_dict = simple_quantizer.create_quantized_state_dict()\n        model = simple_quantizer.convert_for_runtime(use_cuda=True)\n        model.load_state_dict(quantized_state_dict, assign=True)\n        model = model.to(device=device, dtype=torch.bfloat16)\n        torch.cuda.empty_cache()\n    elif quantisation_mode is not None:\n        raise Exception(f\"Invalid quantisation mode {quantisation_mode}! Must be either 'int4' or 'int8'!\")\n\n    ###### TOKENIZER\n    tokenizer_info = checkpoint.get(\"meta\", {}).get(\"tokenizer\", {})\n    tokenizer = TrainedBPETokeniser(**tokenizer_info)\n\n    ###### SPEAKER EMBEDDER\n    smodel = SpeakerEncoder(\n        weights_fpath=spk_emb_ckpt_path,\n        device=device,\n        eval=True,\n        verbose=False,\n    )\n    return model.eval(), tokenizer, smodel\n\n\ndef build_model(\n    *,\n    precision: torch.dtype,\n    checkpoint_path: Path = Path(\"\"),\n    spk_emb_ckpt_path: Path = Path(\"\"),\n    compile_prefill: bool = False,\n    compile: bool = True,\n    device: str = \"cuda\",\n    quantisation_mode: Optional[Literal[\"int4\", \"int8\"]] = None,\n):\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    print(f\"Using device={device}\")\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model, tokenizer, smodel = _load_model(\n        checkpoint_path, spk_emb_ckpt_path, device, precision, quantisation_mode=quantisation_mode\n    )\n\n    device_sync(device=device)  # MKG\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    torch.manual_seed(1234)\n    model_size = sum([p.numel() * p.dtype.itemsize for p in itertools.chain(model.parameters(), model.buffers())])\n\n    with torch.device(device):\n        model.setup_spk_cond_mask()\n        model.setup_caches(max_batch_size=2, max_seq_length=model.config.block_size)\n\n    if compile:\n        print(\"Compiling...Can take up to 2 mins.\")\n        global decode_one_token, prefill\n        decode_one_token = torch.compile(\n            decode_one_token,\n            mode=\"max-autotune\",\n            fullgraph=True,\n        )\n\n        if compile_prefill:\n            prefill = torch.compile(\n                prefill,\n                fullgraph=True,\n                dynamic=True,\n            )\n\n    encoded = encode_tokens(tokenizer, \"Hello, what's up?\", device=device)\n    spk_emb = torch.randn((1, 256), device=device, dtype=precision)\n\n    device_sync(device=device)  # MKG\n    t0 = time.perf_counter()\n    y = generate(\n        model,\n        encoded,\n        spk_emb,\n        max_new_tokens=200,\n        callback=lambda x: x,\n        temperature=torch.tensor(1.0, device=device, dtype=precision),\n        top_k=None,\n        top_p=torch.tensor(0.95, device=device, dtype=precision),\n        guidance_scale=torch.tensor(3.0, device=device, dtype=precision),\n        end_of_audio_token=9999,  # don't end early for compilation stage.\n    )\n\n    device_sync(device=device)  # MKG\n\n    print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n\n    return model, tokenizer, smodel, model_size\n\n\ndef main(\n    *,\n    model,\n    tokenizer,\n    model_size,\n    prompt: str,\n    guidance_scale: torch.Tensor,\n    temperature: torch.Tensor,\n    spk_emb: torch.Tensor,\n    top_k: Optional[torch.Tensor] = None,\n    top_p: Optional[torch.Tensor] = None,\n    device: str = \"cuda\",\n) -> list:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\"\"\"\n\n    encoded = encode_tokens(tokenizer, prompt, device=device)\n    prompt_length = encoded.size(0)\n\n    aggregate_metrics: dict = {\n        \"tokens_per_sec\": [],\n    }\n\n    device_sync(device=device)  # MKG\n\n    if True:\n        callback = lambda x: x\n    t0 = time.perf_counter()\n\n    y = generate(\n        model,\n        encoded,\n        spk_emb,\n        callback=callback,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        guidance_scale=guidance_scale,\n    )\n\n    device_sync(device=device)  # MKG\n    t = time.perf_counter() - t0\n\n    tokens_generated = y.size(0) - prompt_length\n    tokens_sec = tokens_generated / t\n    aggregate_metrics[\"tokens_per_sec\"].append(tokens_sec)\n    print(f\"Time for 1st stage LLM inference: {t:.02f} sec total, {tokens_sec:.02f} tokens/sec\")\n    print(f\"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s\")\n    # print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\\n\")\n\n    return y.tolist()\n"}
{"type": "source_file", "path": "fam/llm/layers/attn.py", "content": "import warnings\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, config):\n        \"\"\"\n        Initializes the SelfAttention module.\n\n        Args:\n            config: An object containing the configuration parameters for the SelfAttention module.\n        \"\"\"\n        super().__init__()\n        self._validate_config(config)\n        self._initialize_parameters(config)\n\n    def empty_kv_cache(self, batch_size: int, kv_cache_maxlen: int, dtype: torch.dtype):\n        \"\"\"\n        Empties the key-value cache.\n\n        Args:\n            batch_size: The batch size.\n            kv_cache_maxlen: The maximum length of the key-value cache.\n            dtype: The data type of the cache.\n\n        Raises:\n            Exception: If trying to empty the KV cache when it is disabled.\n        \"\"\"\n        if self.kv_cache_enabled is False:\n            raise Exception(\"Trying to empty KV cache when it is disabled\")\n\n        # register so that the cache moves devices along with the module\n        # TODO: get rid of re-allocation.\n        self.register_buffer(\n            \"kv_cache\",\n            torch.zeros(\n                2,\n                batch_size,\n                kv_cache_maxlen,\n                self.n_head,\n                self.n_embd // self.n_head,\n                dtype=dtype,\n                device=self.c_attn.weight.device,\n            ),\n            persistent=False,\n        )\n\n        self.kv_cache_first_empty_index = 0\n\n    def _initialize_parameters(self, config):\n        \"\"\"\n        Initializes the parameters of the SelfAttention module.\n\n        Args:\n            config: An object containing the configuration parameters for the SelfAttention module.\n        \"\"\"\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # regularization\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        self.causal = config.causal\n        self.attn_kernel_type = config.attn_kernel_type\n        self.attn_dropout = nn.Dropout(config.dropout)\n\n        self.kv_cache_enabled = False\n\n    def _validate_config(self, config):\n        \"\"\"\n        Validates the configuration parameters.\n\n        Args:\n            config: An object containing the configuration parameters for the SelfAttention module.\n\n        Raises:\n            AssertionError: If the embedding dimension is not divisible by the number of heads.\n        \"\"\"\n        assert config.n_embd % config.n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n\n    def _update_kv_cache(self, q, k, v):\n        \"\"\"\n        Updates the key-value cache.\n\n        Args:\n            q: The query tensor.\n            k: The key tensor.\n            v: The value tensor.\n\n        Returns:\n            The updated key and value tensors.\n\n        Raises:\n            AssertionError: If the dimensions of the query, key, and value tensors are not compatible.\n        \"\"\"\n        q_time, k_time, v_time = q.shape[1], k.shape[1], v.shape[1]\n\n        if self.kv_cache_first_empty_index == 0:\n            assert q_time == k_time and q_time == v_time\n        else:\n            assert (\n                q_time == 1\n            ), f\"Only one query at a time is supported, but got q_time={q_time} for kv_cache_first_empty_index={self.kv_cache_first_empty_index}\"\n\n        self.kv_cache[0, :, self.kv_cache_first_empty_index : self.kv_cache_first_empty_index + q_time] = k\n        self.kv_cache[1, :, self.kv_cache_first_empty_index : self.kv_cache_first_empty_index + q_time] = v\n        self.kv_cache_first_empty_index += q_time\n\n        k = self.kv_cache[0, :, : self.kv_cache_first_empty_index]\n        v = self.kv_cache[1, :, : self.kv_cache_first_empty_index]\n\n        return k, v\n\n    def _torch_attn(self, c_x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs attention using the torch.nn.functional.scaled_dot_product_attention function.\n\n        Args:\n            c_x: The input tensor.\n\n        Returns:\n            The output tensor.\n        \"\"\"\n        q, k, v = c_x.split(1, dim=2)  # q, k, v of shape (B, T, 1, nh, hs)\n        q = q.squeeze(2)  # (B, T, nh, hs)\n        k = k.squeeze(2)  # (B, T, nh, hs)\n        v = v.squeeze(2)  # (B, T, nh, hs)\n\n        # if kv-caching and causal, for the \"prefill\" stage, we need to use a causal mask, and\n        # use no mask for the \"one time step\" parts.\n        # calculate this before updating kv_caching so we have the right value for kv_cache_first_empty_index\n        is_causal_attn_mask = self.causal and (not self.kv_cache_enabled or self.kv_cache_first_empty_index == 0)\n\n        if self.kv_cache_enabled:\n            k, v = self._update_kv_cache(q, k, v)\n\n        q = q.transpose(1, 2)  # (B, nh, T, hs)\n        k = k.transpose(1, 2)  # (B, nh, T, hs)\n        v = v.transpose(1, 2)  # (B, nh, T, hs)\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=None,\n            dropout_p=self.dropout if self.training else 0,\n            is_causal=is_causal_attn_mask,\n        ).transpose(\n            1, 2\n        )  # (B, nh, T, hs) -> (B, T, nh, hs)\n\n        return y\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the SelfAttention module.\n\n        Args:\n            x: The input tensor.\n\n        Returns:\n            The output tensor.\n        \"\"\"\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        c_x = self.c_attn(x).view(B, T, 3, self.n_head, C // self.n_head)  # (B, T, 3, nh, hs)\n\n        # causal self-attention;\n        if self.attn_kernel_type == \"torch_attn\":\n            y = self._torch_attn(c_x)\n        else:\n            raise Exception(f\"Unknown attention kernel type: {self.attn_kernel_type}\")\n\n        y = y.contiguous().view(B, T, C)  # re-assemble all head outputs side by side: (B, T, nh, hs) -> (B, T, hs * nh)\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n"}
{"type": "source_file", "path": "fam/llm/inference.py", "content": "\"\"\"\nCommand: python fam/llm/inference.py --spk_cond_path=\"assets/bria.mp3\" --text=\"This is a demo of text to speech by MetaVoice-1B, an open-source foundational audio model.\"\n\"\"\"\n\nimport dataclasses\nimport hashlib\nimport json\nimport os\nimport pathlib\nimport shutil\nimport subprocess\nimport tempfile\nimport time\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass\nfrom typing import List, Literal, Optional, Tuple, Type, Union\n\nimport torch\nimport tqdm\nimport tqdm.contrib.concurrent\nimport tyro\nfrom huggingface_hub import snapshot_download\n\nfrom fam.llm.adapters import FlattenedInterleavedEncodec2Codebook, TiltedEncodec\nfrom fam.llm.decoders import Decoder, EncodecDecoder\nfrom fam.llm.enhancers import BaseEnhancer, get_enhancer\nfrom fam.llm.model import GPT, GPTConfig\nfrom fam.llm.utils import check_audio_file, get_default_dtype, normalize_text\nfrom fam.quantiser.audio.speaker_encoder.model import SpeakerEncoder\nfrom fam.quantiser.text.tokenise import TrainedBPETokeniser\n\n\n@dataclass\nclass InferenceConfig:\n    ckpt_path: str  # path to checkpoint\n    output_dir: str\n    num_samples: int = 10  # number of samples to draw\n    seed: int = 1337  # random seed\n    device: str = \"cuda\"\n    dtype: str = \"bfloat16\"\n    compile: bool = False\n    init_from: str = \"resume\"  # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n\n    def __str__(self):\n        field_strs = []\n        for field in dataclasses.fields(self):\n            value = getattr(self, field.name)\n            field_strs.append(f\"  {field.name}: {value}\")\n\n        return \"InferenceConfig:\\n\" + \"\\n\".join(field_strs)\n\n\nclass Model:\n    def __init__(\n        self,\n        config: InferenceConfig,\n        tokenizer_cls: Type[TrainedBPETokeniser],\n        decoder_cls: Type[Decoder],\n        data_adapter_fn,\n        use_kv_cache: Optional[Literal[\"vanilla\"]] = None,\n    ):\n        # TODO: disentangle the encodec stuff and numbers etc with rest of this code (esp at encoder-only / second stage model inference)\n        # TODO: remove magic number\n        self._encodec_codes_pad_token = 1024\n        self._num_encodec_codebooks = 8\n        self.config = config\n        self.use_kv_cache = use_kv_cache\n\n        torch.manual_seed(config.seed)\n        torch.cuda.manual_seed(config.seed)\n        torch.backends.cuda.matmul.allow_tf32 = True if config.dtype != \"float32\" else False  # allow tf32 on matmul\n        torch.backends.cudnn.allow_tf32 = True if config.dtype != \"float32\" else False  # allow tf32 on cudnn\n        device_type = \"cuda\" if \"cuda\" in config.device else \"cpu\"  # for later use in torch.autocast\n        self.ptdtype = {\n            \"float32\": torch.float32,\n            \"tfloat32\": torch.float32,\n            \"bfloat16\": torch.bfloat16,\n            \"float16\": torch.float16,\n        }[config.dtype]\n        self._ctx = (\n            nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type=device_type, dtype=self.ptdtype)\n        )\n\n        self.use_bpe_tokenizer = False\n        self.load_meta = None\n        self.speaker_cond = None\n        self.meta = None\n        self.model = None\n        self.checkpoint_config = None\n        self.vocab_sizes = None\n        self.smodel = None\n\n        self._init_model()\n\n        self.tokenizer = tokenizer_cls(**self.meta[\"tokenizer\"])\n        self.decoder = decoder_cls(\n            tokeniser_decode_fn=self.tokenizer.decode,\n            output_dir=self.config.output_dir,\n            data_adapter_fn=data_adapter_fn,\n        )\n\n    def _init_model(self):\n        if self.config.init_from == \"resume\":\n            # init from a model saved in a specific directory\n            checkpoint = torch.load(self.config.ckpt_path, map_location=self.config.device)\n            self.vocab_sizes = checkpoint[\"model_args\"][\"vocab_sizes\"]\n\n            self.load_meta = False\n            self.speaker_cond = False\n\n            if \"config\" in checkpoint:\n                self.checkpoint_config = checkpoint[\"config\"]\n\n                self.meta = checkpoint[\"meta\"]\n                self.load_meta = True\n\n            if self.load_meta:\n                self.use_bpe_tokenizer = \"stoi\" not in self.meta or \"itos\" not in self.meta\n                self.speaker_cond = self.meta.get(\"speaker_cond\")\n\n            if self.speaker_cond:\n                speaker_emb_size = self.meta[\"speaker_emb_size\"]\n\n            model_args = checkpoint[\"model_args\"]\n            if \"causal\" in self.checkpoint_config and self.checkpoint_config[\"causal\"] is False:\n                self._encodec_ctx_window = model_args[\"block_size\"]\n\n            gptconf = GPTConfig(**model_args)\n\n            # TODO: rename `speaker_emb_dim` to `speaker_emb_size`.\n            self.model = GPT(gptconf, speaker_emb_dim=speaker_emb_size if self.speaker_cond else None)\n            state_dict = checkpoint[\"model\"]\n            unwanted_prefix = \"_orig_mod.\"\n            for k, v in list(state_dict.items()):\n                if k.startswith(unwanted_prefix):\n                    state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n            self.model.load_state_dict(state_dict)\n\n        # model\n        self.model.eval()\n        self.model.to(self.config.device)\n\n        if self.config.compile:\n            from einops._torch_specific import allow_ops_in_compiled_graph\n\n            allow_ops_in_compiled_graph()\n            self.model = torch.compile(self.model)  # type: ignore\n\n        if self.use_kv_cache is not None:\n            if \"causal\" in self.checkpoint_config and self.checkpoint_config[\"causal\"] is False:\n                raise Exception(\"kv_cache not supported for non-causal models!\")\n\n            if self.use_kv_cache == \"vanilla\":\n                self.model.enable_kv_cache()\n            else:\n                raise NotImplementedError(f\"kv_cache type {self.use_kv_cache} not implemented!\")\n\n    def causal_sample(\n        self,\n        *,\n        texts: list[str],\n        batch_size: int,\n        max_new_tokens: int,\n        temperature: Optional[float],\n        top_k: Optional[int],\n        top_p: Optional[float],\n        speaker_embs: Optional[torch.Tensor] = None,\n        guidance_scale: Optional[float] = None,\n    ) -> list[torch.Tensor]:\n        \"\"\"\n        Returns list of torch.Tensors of tokens. Each tensor is of shape (1, c, t) where c is the number of codebooks.\n        Any flattening / inteleaving / tilting gets reversed before the output is returned.\n        \"\"\"\n        if speaker_embs is not None:\n            assert len(texts) == len(speaker_embs)\n\n        encoded_texts = [self.tokenizer.encode(text) for text in texts]\n\n        ## create multiple hierarchies and get seq_lens\n        seq_lens = []\n        xs = []\n        for i, encoded_text in enumerate(encoded_texts):\n            encoded_text = torch.tensor([encoded_text], dtype=torch.long, device=self.config.device)\n            # TODO: remove magic number\n            xs.append(\n                torch.cat(\n                    # [1st hierarchy of text, *remaining hierarchies of padded tokens]\n                    # TODO: self.vocab_sizes should be from the model config?\n                    [encoded_text, *[torch.ones_like(encoded_text) * 1024] * (len(self.vocab_sizes) - 1)],\n                    dim=0,\n                ).unsqueeze(0)\n            )  # b x [(b=1, c, t)]\n            seq_lens.append(xs[-1].shape[-1])\n        max_len = max(seq_lens)\n        assert len(xs) == len(seq_lens)\n\n        ## equalise the shapes in the batch. we can use torch.zeros as tokens > seq_lens will be masked out.\n        x = torch.zeros((len(encoded_texts), xs[0].shape[1], max_len), dtype=torch.long, device=self.config.device)\n        for i, _xs in enumerate(xs):\n            assert _xs.shape[-1] == seq_lens[i]\n            x[i, :, : seq_lens[i]] = _xs\n\n        ## check that the input is correct\n        for i in range(x.shape[0]):\n            assert x[i, 0, : seq_lens[i]].tolist() == encoded_texts[i]\n\n            # TODO: remove magic number\n            if x.shape[1] > 1:\n                assert set(x[i, 1, : seq_lens[i]].tolist()) == set([1024])\n\n        assert x.shape[0] == speaker_embs.shape[0] if speaker_embs is not None else True\n\n        if self.speaker_cond is False:\n            speaker_embs = None\n\n        # run sampling loop\n        with torch.no_grad():\n            with self._ctx:  # type: ignore\n                to_return = []\n                for k in range(self.config.num_samples):\n                    assert seq_lens is not None\n                    assert batch_size is not None\n\n                    if max(seq_lens) + max_new_tokens >= self.model.config.block_size:\n                        raise Exception(\n                            f\"max_new_tokens {max_new_tokens} too large! Choose {self.model.config.block_size - max(seq_lens) - 1} instead.\"\n                        )\n\n                    y = self.model.generate(\n                        x,\n                        max_new_tokens,\n                        seq_lens=seq_lens,\n                        temperature=temperature,\n                        top_k=top_k,\n                        top_p=top_p,\n                        speaker_embs=speaker_embs,\n                        batch_size=batch_size,\n                        guidance_scale=guidance_scale,\n                        dtype=self.ptdtype,\n                        end_of_audio_token=self.tokenizer.offset - 1,\n                        end_of_text_token=self.tokenizer.eot_token,\n                    )\n                    for i in range(len(y)):\n                        to_return.append(self.decoder.decode(tokens=y[i].tolist(), causal=True))\n\n                return to_return\n\n    def non_causal_sample(\n        self,\n        *,\n        texts: list[str],\n        encodec_tokens: list[torch.Tensor],\n        batch_size: int,\n        top_k: Optional[int],\n        temperature: Optional[float],\n        speaker_embs: Optional[torch.Tensor] = None,\n    ) -> list[str]:\n        \"\"\"\n        Returns paths to saved audio files.\n        \"\"\"\n        if speaker_embs is not None:\n            assert len(texts) == len(speaker_embs)\n\n        encoded_texts = [self.tokenizer.encode(text) for text in texts]\n\n        # setup input\n        # TODO: same code is used during data prep. refactor\n        padded_hierarchies_inputs = []\n        for encoded_text, encodec_token in zip(encoded_texts, encodec_tokens):\n            x = torch.tensor(encoded_text, dtype=torch.long, device=self.config.device)[\n                None, None, ...\n            ]  # (b=1, c=1, t)\n\n            # TODO: should only happen if decoder is encodecdeocder?\n            assert encodec_token.shape[0] == 1\n            encodec_token = encodec_token[0].tolist()  # (b=1, c, t) -> (c, t)\n            assert len(encodec_token) >= 1 and len(encodec_token) <= self._num_encodec_codebooks\n\n            ## setup hierarchies of tokens\n            # TODO: refactor and merge with code in processing.py\n            text_tokens = encoded_text  # (t,)\n\n            hierarchies_in = []\n            hierarchies_in.append(text_tokens + encodec_token[0] + [self._encodec_codes_pad_token])\n            hierarchies_in.append(\n                [self._encodec_codes_pad_token] * len(text_tokens) + encodec_token[1] + [self._encodec_codes_pad_token]\n            )\n\n            ## adding padding / cutting to the right size as needed\n            # TODO: refactor and merge with code in processing.py\n            padded_hierarchies_input = []\n            for _, t_hierarchy in enumerate(hierarchies_in):\n                assert len(t_hierarchy) == len(hierarchies_in[0])\n                if len(t_hierarchy) < self._encodec_ctx_window:\n                    padded_hierarchies_input.append(\n                        t_hierarchy + [self._encodec_codes_pad_token] * (self._encodec_ctx_window - len(t_hierarchy))\n                    )\n                elif len(t_hierarchy) > self._encodec_ctx_window:\n                    padded_hierarchies_input.append(t_hierarchy[: self._encodec_ctx_window])\n                else:\n                    padded_hierarchies_input.append(t_hierarchy)\n\n            padded_hierarchies_inputs.append(padded_hierarchies_input)\n\n        ## check that the input is correct\n        in_x = torch.tensor(padded_hierarchies_inputs, dtype=torch.long, device=self.config.device)\n        assert in_x.shape[0] == speaker_embs.shape[0] if speaker_embs is not None else True\n\n        if self.speaker_cond is False:\n            speaker_embs = None\n\n        # run sampling loop\n        with torch.no_grad():\n            with self._ctx:  # type: ignore\n                to_return = []\n                for k in range(self.config.num_samples):\n                    y = self.model.generate(\n                        in_x,\n                        None,\n                        temperature=temperature,\n                        top_k=top_k,\n                        # TODO: handle separate top_p for this model explicitly\n                        top_p=None,\n                        speaker_embs=speaker_embs,\n                        batch_size=batch_size,\n                        guidance_scale=None,\n                    )\n\n                    b_tokens = torch.cat([in_x, y], dim=1)\n                    for tokens in b_tokens:\n                        try:\n                            to_return.append(self.decoder.decode(tokens=tokens.tolist(), causal=False))\n                        except Exception as e:\n                            print(\"failed to run MBD.\")\n                            print(f\"reason: {str(e)}\")\n                            to_return.append(None)\n\n                return to_return\n\n    def __call__(\n        self,\n        *,\n        texts: list[str],\n        batch_size: int,\n        max_new_tokens: Optional[int],\n        top_k: Optional[int],\n        top_p: Optional[float],\n        temperature: Optional[float],\n        encodec_tokens: Optional[list[torch.Tensor]] = None,\n        speaker_embs: Optional[torch.Tensor] = None,\n        guidance_scale: Optional[float] = None,\n    ):\n        if self.checkpoint_config.get(\"causal\", True):\n            return self.causal_sample(\n                texts=texts,\n                batch_size=batch_size,\n                speaker_embs=speaker_embs,\n                guidance_scale=guidance_scale,\n                max_new_tokens=max_new_tokens,\n                top_k=top_k,\n                top_p=top_p,\n                temperature=temperature,\n            )\n        else:\n            assert encodec_tokens is not None\n            assert guidance_scale is None\n            assert max_new_tokens is None\n            assert top_p is None\n\n            return self.non_causal_sample(\n                texts=texts,\n                encodec_tokens=encodec_tokens,\n                batch_size=batch_size,\n                speaker_embs=speaker_embs,\n                top_k=top_k,\n                temperature=temperature,\n            )\n\n\ndef save_result_metadata(wav_path, ref_path, text, first_stage_ckpt_path, second_stage_ckpt_path):\n    if first_stage_ckpt_path is None or second_stage_ckpt_path is None:\n        return\n    json.dump(\n        {\n            \"speaker\": ref_path,\n            \"text\": text,\n        },\n        pathlib.Path(str(wav_path) + \".json\").open(\"w\"),\n    )\n\n\ndef get_cached_file(file_or_uri: str):\n    \"\"\"\n    If it's an s3 file, download it to a local temporary file and return that path.\n    Otherwise return the path as is.\n    \"\"\"\n    is_uri = file_or_uri.startswith(\"http\")\n\n    cache_path = None\n    if is_uri:\n        ext = pathlib.Path(file_or_uri).suffix\n        # hash the file path to get the cache name\n        _cache_name = \"audio_\" + hashlib.md5(file_or_uri.encode(\"utf-8\")).hexdigest() + ext\n\n        os.makedirs(os.path.expanduser(\"~/.cache/metavoice/\"), exist_ok=True)\n        cache_path = os.path.expanduser(f\"~/.cache/metavoice/{_cache_name}\")\n\n        if not os.path.exists(cache_path):\n            command = f\"curl -o {cache_path} {file_or_uri}\"\n            subprocess.run(command, shell=True, check=True)\n    else:\n        if os.path.exists(file_or_uri):\n            cache_path = file_or_uri\n        else:\n            raise FileNotFoundError(f\"File {file_or_uri} not found!\")\n    return cache_path\n\n\ndef get_cached_embedding(local_file_path: str, spkemb_model):\n    if not os.path.exists(local_file_path):\n        raise FileNotFoundError(f\"File {local_file_path} not found!\")\n\n    # hash the file path to get the cache name\n    _cache_name = \"embedding_\" + hashlib.md5(local_file_path.encode(\"utf-8\")).hexdigest() + \".pt\"\n\n    os.makedirs(os.path.expanduser(\"~/.cache/fam/\"), exist_ok=True)\n    cache_path = os.path.expanduser(f\"~/.cache/fam/{_cache_name}\")\n\n    if not os.path.exists(cache_path):\n        spk_emb = spkemb_model.embed_utterance_from_file(local_file_path, numpy=False).unsqueeze(0)  # (b=1, c)\n        torch.save(spk_emb, cache_path)\n    else:\n        spk_emb = torch.load(cache_path)\n\n    return spk_emb\n\n\ndef _sample_utterance_batch(\n    texts: list[str],\n    spk_cond_paths: list[Optional[str]],\n    spkemb_model,\n    first_stage_model,\n    second_stage_model,\n    enhancer: Optional[Union[Literal[\"df\"], BaseEnhancer]],\n    first_stage_ckpt_path: str,\n    second_stage_ckpt_path: str,\n    guidance_scale: Optional[Tuple[float, float]],\n    max_new_tokens: int,\n    top_k: Optional[int],\n    top_p: Optional[float],\n    temperature: Optional[float],\n    batch_size: int = 128,\n) -> List[str]:\n\n    speaker_embs = []\n    refs = spk_cond_paths.copy()\n\n    # multithreaded loop to cache all the files\n    spk_cond_paths = tqdm.contrib.concurrent.thread_map(\n        get_cached_file, spk_cond_paths, desc=\"getting cached speaker ref files\"\n    )\n\n    for i, (text, spk_cond_path) in tqdm.tqdm(\n        enumerate(zip(texts, spk_cond_paths)), total=len(texts), desc=\"calculating speaker embeddings\"\n    ):\n        texts[i] = normalize_text(text)\n        speaker_embs.append(get_cached_embedding(spk_cond_path, spkemb_model) if spk_cond_path else None)\n\n    b_speaker_embs = torch.cat(speaker_embs, dim=0)\n\n    start = time.time()\n    b_tokens = first_stage_model(\n        texts=texts,\n        speaker_embs=b_speaker_embs,\n        batch_size=batch_size,\n        guidance_scale=guidance_scale,\n        top_p=top_p,\n        top_k=top_k,\n        temperature=temperature,\n        max_new_tokens=max_new_tokens,\n    )\n\n    # TODO: set batch size for second stage model!\n    wav_files = second_stage_model(\n        texts=texts,\n        encodec_tokens=b_tokens,\n        speaker_embs=b_speaker_embs,\n        batch_size=batch_size,\n        guidance_scale=None,\n        top_p=None,\n        top_k=top_k,\n        temperature=temperature,\n        max_new_tokens=None,\n    )\n\n    for text, tokens, speaker_embs, ref_name, wav_file in zip(texts, b_tokens, b_speaker_embs, refs, wav_files):\n        if wav_file is None:\n            continue\n\n        with tempfile.NamedTemporaryFile(suffix=\".wav\") as enhanced_tmp:\n            if enhancer is not None:\n                enhancer = get_enhancer(enhancer) if isinstance(enhancer, str) else enhancer\n                enhancer(str(wav_file) + \".wav\", enhanced_tmp.name)\n                # copy enhanced_tmp.name back to wav_file\n                print(f\"copying enhanced file from {enhanced_tmp.name} to {str(wav_file) + '.wav'}.\")\n                shutil.copy2(enhanced_tmp.name, str(wav_file) + \".wav\")\n\n            save_result_metadata(\n                wav_file,\n                ref_name,\n                text,\n                first_stage_ckpt_path,\n                second_stage_ckpt_path,\n            )\n\n    print(f\"time_to_synth_s: {time.time() - start}\")\n    return [str(w) + \".wav\" if not str(w).endswith(\".wav\") else str(w) for w in wav_files]\n\n\ndef sample_utterance(\n    text: str,\n    spk_cond_path: Optional[str],\n    spkemb_model,\n    first_stage_model,\n    second_stage_model,\n    enhancer: Optional[Union[Literal[\"df\"], BaseEnhancer]],\n    first_stage_ckpt_path: str,\n    second_stage_ckpt_path: str,\n    guidance_scale: Optional[Tuple[float, float]],\n    max_new_tokens: int,\n    top_k: Optional[int],\n    top_p: Optional[float],\n    temperature: Optional[float],\n) -> str:\n    # NOTE: supports max. 220 characters atm.\n    # Long form synthesis coming soon...\n    MAX_CHARS = 220\n    if len(text) > MAX_CHARS:\n        print(\n            f\"\\n***WARNING: Max {MAX_CHARS} characters supported. Provided: {len(text)}. Truncating and generating speech...Can lead to unpredictable speech at the end.***\"\n        )\n\n    return _sample_utterance_batch(\n        texts=[text],\n        spk_cond_paths=[spk_cond_path],\n        spkemb_model=spkemb_model,\n        first_stage_model=first_stage_model,\n        second_stage_model=second_stage_model,\n        enhancer=enhancer,\n        first_stage_ckpt_path=first_stage_ckpt_path,\n        second_stage_ckpt_path=second_stage_ckpt_path,\n        batch_size=1,\n        guidance_scale=guidance_scale,\n        max_new_tokens=max_new_tokens,\n        top_k=top_k,\n        top_p=top_p,\n        temperature=temperature,\n    )[0]\n\n\ndef build_models(config_first_stage, config_second_stage, model_dir, device, use_kv_cache):\n    smodel = SpeakerEncoder(\n        weights_fpath=os.path.join(model_dir, \"speaker_encoder.pt\"), device=device, eval=True, verbose=False\n    )\n    data_adapter = FlattenedInterleavedEncodec2Codebook(end_of_audio_token=1024)\n    llm_first_stage = Model(\n        config_first_stage,\n        TrainedBPETokeniser,\n        EncodecDecoder,\n        data_adapter_fn=data_adapter.decode,\n        use_kv_cache=use_kv_cache,\n    )\n    data_adapter_second_stage = TiltedEncodec(end_of_audio_token=1024)\n    llm_second_stage = Model(\n        config_second_stage, TrainedBPETokeniser, EncodecDecoder, data_adapter_fn=data_adapter_second_stage.decode\n    )\n    return smodel, llm_first_stage, llm_second_stage\n\n\ndef get_first_stage_path(model_dir: str):\n    \"\"\"Absolute path to checkpoint for the first stage model.\"\"\"\n    return os.path.join(os.path.expanduser(model_dir), \"first_stage.pt\")\n\n\ndef get_second_stage_path(model_dir: str):\n    \"\"\"Absolute path to checkpoint for the second stage model.\"\"\"\n    return os.path.join(os.path.expanduser(model_dir), \"second_stage.pt\")\n\n\n@dataclass\nclass SamplingControllerConfig:\n    \"\"\"\n    Sample from a trained model.\n    \"\"\"\n\n    spk_cond_path: str\n    \"\"\"Path to speaker reference file. Min. 30s of audio required. Supports both local paths & public URIs. Audio formats: wav, flac & mp3\"\"\"\n\n    huggingface_repo_id: str = \"metavoiceio/metavoice-1B-v0.1\"\n    \"\"\"Absolute path to the model directory.\"\"\"\n\n    text: str = (\n        \"This is a demo of text to speech by MetaVoice-1B, an open-source foundational audio model by MetaVoice.\"\n    )\n    \"\"\"Text to synthesise.\"\"\"\n\n    num_samples: int = 1\n    \"\"\"Number of samples to generate from each model.\"\"\"\n\n    max_new_tokens: int = 864\n    \"\"\"Maximum number of new tokens to generate from the first stage model.\"\"\"\n\n    temperature: float = 1.0\n    \"\"\"Temperature for sampling applied to both models.\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Top k for sampling applied to both models.\"\"\"\n\n    top_p: Optional[float] = 0.95\n    \"\"\"Top p for sampling applied to first-stage model.\"\"\"\n\n    seed: int = 1337\n    \"\"\"Random seed for sampling.\"\"\"\n\n    device: Literal[\"cuda\", \"cpu\"] = \"cuda\"\n    \"\"\"Device to use for sampling.\"\"\"\n\n    dtype: Literal[\"bfloat16\", \"float16\", \"float32\", \"tfloat32\"] = get_default_dtype()\n    \"\"\"Data type to use for sampling.\"\"\"\n\n    compile: bool = False\n    \"\"\"Whether to compile the model using PyTorch 2.0.\"\"\"\n\n    enhancer: Optional[Literal[\"df\"]] = \"df\"\n    \"\"\"Enhancer to use for post-processing.\"\"\"\n\n    init_from: str = \"resume\"\n    \"\"\"Either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl').\"\"\"\n\n    use_kv_cache: Optional[Literal[\"vanilla\"]] = \"vanilla\"\n    \"\"\"Type of kv caching to use for inference: 1) [none] no kv caching, 2) [vanilla] use torch attention with hand implemented kv-cache.\"\"\"\n\n    output_dir: str = \"samples/\"\n    \"\"\"Relative path to output directory\"\"\"\n\n    guidance_scale: Optional[Tuple[float, float]] = (3.0, 1.0)\n    \"\"\"Guidance scale for sampling: (speaker conditioning guidance_scale, prompt conditioning guidance scale).\"\"\"\n\n    batch_size: int = 128\n    \"\"\"Batch size to use for sampling. Note that the batch size gets doubled when guidance is used. For H100, and 1B model,\n    1 w/ guidance and 1 w/o guidance work well (without kv-caching). With kv-caching, 128 (w/o guidance) and\n    64 (w/ guidance) works well.\"\"\"\n\n\nif __name__ == \"__main__\":\n    # TODO: add support for batch sampling via CLI. Function has been implemented above.\n    sampling_config = tyro.cli(SamplingControllerConfig, use_underscores=True)\n\n    check_audio_file(sampling_config.spk_cond_path)\n\n    model_dir = snapshot_download(repo_id=sampling_config.huggingface_repo_id)\n    first_stage_ckpt_path = get_first_stage_path(model_dir)\n    second_stage_ckpt_path = get_second_stage_path(model_dir)\n\n    config_first_stage = InferenceConfig(\n        ckpt_path=first_stage_ckpt_path,\n        num_samples=sampling_config.num_samples,\n        seed=sampling_config.seed,\n        device=sampling_config.device,\n        dtype=sampling_config.dtype,\n        compile=sampling_config.compile,\n        init_from=sampling_config.init_from,\n        output_dir=sampling_config.output_dir,\n    )\n\n    config_second_stage = InferenceConfig(\n        ckpt_path=second_stage_ckpt_path,\n        num_samples=sampling_config.num_samples,\n        seed=sampling_config.seed,\n        device=sampling_config.device,\n        dtype=sampling_config.dtype,\n        compile=sampling_config.compile,\n        init_from=sampling_config.init_from,\n        output_dir=sampling_config.output_dir,\n    )\n\n    sampling_config.max_new_tokens *= (\n        2  # deal with max_new_tokens for flattened interleaving! (should scale with num_codebooks?)\n    )\n\n    # define models\n    smodel, llm_first_stage, llm_second_stage = build_models(\n        config_first_stage,\n        config_second_stage,\n        model_dir=model_dir,\n        device=sampling_config.device,\n        use_kv_cache=sampling_config.use_kv_cache,\n    )\n\n    sample_utterance(\n        sampling_config.text,\n        os.path.expanduser(sampling_config.spk_cond_path),\n        smodel,\n        llm_first_stage,\n        llm_second_stage,\n        sampling_config.enhancer,\n        first_stage_ckpt_path,\n        second_stage_ckpt_path,\n        sampling_config.guidance_scale,\n        max_new_tokens=sampling_config.max_new_tokens,\n        top_k=sampling_config.top_k,\n        top_p=sampling_config.top_p,\n        temperature=sampling_config.temperature,\n    )\n"}
{"type": "source_file", "path": "fam/__init__.py", "content": ""}
{"type": "source_file", "path": "app.py", "content": "import os\nimport sys\n\nproject_root = os.path.dirname(os.path.abspath(__file__))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n\nimport gradio as gr\nimport tyro\n\nfrom fam.llm.fast_inference import TTS\nfrom fam.llm.utils import check_audio_file\n\n#### setup model\nTTS_MODEL = tyro.cli(TTS, args=[\"--telemetry_origin\", \"webapp\"])\n\n#### setup interface\nRADIO_CHOICES = [\"Preset voices\", \"Upload target voice (atleast 30s)\"]\nMAX_CHARS = 220\nPRESET_VOICES = {\n    # female\n    \"Bria\": \"https://cdn.themetavoice.xyz/speakers/bria.mp3\",\n    # male\n    \"Alex\": \"https://cdn.themetavoice.xyz/speakers/alex.mp3\",\n    \"Jacob\": \"https://cdn.themetavoice.xyz/speakers/jacob.wav\",\n}\n\n\ndef denormalise_top_p(top_p):\n    # returns top_p in the range [0.9, 1.0]\n    return round(0.9 + top_p / 100, 2)\n\n\ndef denormalise_guidance(guidance):\n    # returns guidance in the range [1.0, 3.0]\n    return 1 + ((guidance - 1) * (3 - 1)) / (5 - 1)\n\n\ndef _check_file_size(path):\n    if not path:\n        return\n    filesize = os.path.getsize(path)\n    filesize_mb = filesize / 1024 / 1024\n    if filesize_mb >= 50:\n        raise gr.Error(f\"Please upload a sample less than 20MB for voice cloning. Provided: {round(filesize_mb)} MB\")\n\n\ndef _handle_edge_cases(to_say, upload_target):\n    if not to_say:\n        raise gr.Error(\"Please provide text to synthesise\")\n\n    if len(to_say) > MAX_CHARS:\n        gr.Warning(\n            f\"Max {MAX_CHARS} characters allowed. Provided: {len(to_say)} characters. Truncating and generating speech...Result at the end can be unstable as a result.\"\n        )\n\n    if not upload_target:\n        return\n\n    check_audio_file(upload_target)  # check file duration to be atleast 30s\n    _check_file_size(upload_target)\n\n\ndef tts(to_say, top_p, guidance, toggle, preset_dropdown, upload_target):\n    try:\n        d_top_p = denormalise_top_p(top_p)\n        d_guidance = denormalise_guidance(guidance)\n\n        _handle_edge_cases(to_say, upload_target)\n\n        to_say = to_say if len(to_say) < MAX_CHARS else to_say[:MAX_CHARS]\n\n        return TTS_MODEL.synthesise(\n            text=to_say,\n            spk_ref_path=PRESET_VOICES[preset_dropdown] if toggle == RADIO_CHOICES[0] else upload_target,\n            top_p=d_top_p,\n            guidance_scale=d_guidance,\n        )\n    except Exception as e:\n        raise gr.Error(f\"Something went wrong. Reason: {str(e)}\")\n\n\ndef change_voice_selection_layout(choice):\n    if choice == RADIO_CHOICES[0]:\n        return [gr.update(visible=True), gr.update(visible=False)]\n\n    return [gr.update(visible=False), gr.update(visible=True)]\n\n\ntitle = \"\"\"\n<picture>\n  <source srcset=\"https://cdn.themetavoice.xyz/banner_light_transparent.png\" media=\"(prefers-color-scheme: dark)\" />\n  <img alt=\"MetaVoice logo\" src=\"https://cdn.themetavoice.xyz/banner_light_transparent.png\" style=\"width: 20%; margin: 0 auto;\" />\n</picture>\n\n\\n# TTS by MetaVoice-1B\n\"\"\"\n\ndescription = \"\"\"\n<strong>MetaVoice-1B</strong> is a 1.2B parameter base model for TTS (text-to-speech). It has been built with the following priorities:\n\\n\n* <strong>Emotional speech rhythm and tone</strong> in English.\n* <strong>Zero-shot cloning for American & British voices</strong>, with 30s reference audio.\n* Support for <strong>voice cloning with finetuning</strong>.\n  * We have had success with as little as 1 minute training data for Indian speakers.\n* Support for <strong>long-form synthesis</strong>.\n\nWe are releasing the model under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). See [Github](https://github.com/metavoiceio/metavoice-src) for details and to contribute.\n\"\"\"\n\nwith gr.Blocks(title=\"TTS by MetaVoice\") as demo:\n    gr.Markdown(title)\n\n    with gr.Row():\n        gr.Markdown(description)\n\n    with gr.Row():\n        with gr.Column():\n            to_say = gr.TextArea(\n                label=f\"What should I say!? (max {MAX_CHARS} characters).\",\n                lines=4,\n                value=\"This is a demo of text to speech by MetaVoice-1B, an open-source foundational audio model by MetaVoice.\",\n            )\n            with gr.Row(), gr.Column():\n                # voice settings\n                top_p = gr.Slider(\n                    value=5.0,\n                    minimum=0.0,\n                    maximum=10.0,\n                    step=1.0,\n                    label=\"Speech Stability - improves text following for a challenging speaker\",\n                )\n                guidance = gr.Slider(\n                    value=5.0,\n                    minimum=1.0,\n                    maximum=5.0,\n                    step=1.0,\n                    label=\"Speaker similarity - How closely to match speaker identity and speech style.\",\n                )\n\n                # voice select\n                toggle = gr.Radio(choices=RADIO_CHOICES, label=\"Choose voice\", value=RADIO_CHOICES[0])\n\n            with gr.Row(visible=True) as row_1:\n                preset_dropdown = gr.Dropdown(\n                    PRESET_VOICES.keys(), label=\"Preset voices\", value=list(PRESET_VOICES.keys())[0]\n                )\n                with gr.Accordion(\"Preview: Preset voices\", open=False):\n                    for label, path in PRESET_VOICES.items():\n                        gr.Audio(value=path, label=label)\n\n            with gr.Row(visible=False) as row_2:\n                upload_target = gr.Audio(\n                    sources=[\"upload\"],\n                    type=\"filepath\",\n                    label=\"Upload a clean sample to clone. Sample should contain 1 speaker, be between 30-90 seconds and not contain background noise.\",\n                )\n\n            toggle.change(\n                change_voice_selection_layout,\n                inputs=toggle,\n                outputs=[row_1, row_2],\n            )\n\n        with gr.Column():\n            speech = gr.Audio(\n                type=\"filepath\",\n                label=\"MetaVoice-1B says...\",\n            )\n\n    submit = gr.Button(\"Generate Speech\")\n    submit.click(\n        fn=tts,\n        inputs=[to_say, top_p, guidance, toggle, preset_dropdown, upload_target],\n        outputs=speech,\n    )\n\n\ndemo.queue()\ndemo.launch(\n    favicon_path=os.path.join(os.path.dirname(os.path.abspath(__file__)), \"assets/favicon.ico\"),\n    server_name=\"0.0.0.0\",\n    server_port=7861,\n)\n"}
{"type": "source_file", "path": "fam/llm/enhancers.py", "content": "import os\nfrom abc import ABC\nfrom typing import Literal, Optional\n\nfrom df.enhance import enhance, init_df, load_audio, save_audio\nfrom pydub import AudioSegment\n\n\ndef convert_to_wav(input_file: str, output_file: str):\n    \"\"\"Convert an audio file to WAV format\n\n    Args:\n        input_file (str): path to input audio file\n        output_file (str): path to output WAV file\n\n    \"\"\"\n    # Detect the format of the input file\n    format = input_file.split(\".\")[-1].lower()\n\n    # Read the audio file\n    audio = AudioSegment.from_file(input_file, format=format)\n\n    # Export as WAV\n    audio.export(output_file, format=\"wav\")\n\n\ndef make_output_file_path(audio_file: str, tag: str, ext: Optional[str] = None) -> str:\n    \"\"\"Generate the output file path\n\n    Args:\n        audio_file (str): path to input audio file\n        tag (str): tag to append to the output file name\n        ext (str, optional): extension of the output file. Defaults to None.\n\n    Returns:\n        str: path to output file\n    \"\"\"\n\n    directory = \"./enhanced\"\n    # Get the name of the input file\n    filename = os.path.basename(audio_file)\n\n    # Get the name of the input file without the extension\n    filename_without_extension = os.path.splitext(filename)[0]\n\n    # Get the extension of the input file\n    extension = ext or os.path.splitext(filename)[1]\n\n    # Generate the output file path\n    output_file = os.path.join(directory, filename_without_extension + tag + extension)\n\n    return output_file\n\n\nclass BaseEnhancer(ABC):\n    \"\"\"Base class for audio enhancers\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def __call__(self, audio_file: str, output_file: Optional[str] = None) -> str:\n        raise NotImplementedError\n\n    def get_output_file(self, audio_file: str, tag: str, ext: Optional[str] = None) -> str:\n        output_file = make_output_file_path(audio_file, tag, ext=ext)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        return output_file\n\n\nclass DFEnhancer(BaseEnhancer):\n    def __init__(self, *args, **kwargs):\n        self.model, self.df_state, _ = init_df()\n\n    def __call__(self, audio_file: str, output_file: Optional[str] = None) -> str:\n        output_file = output_file or self.get_output_file(audio_file, \"_df\")\n\n        audio, _ = load_audio(audio_file, sr=self.df_state.sr())\n\n        enhanced = enhance(self.model, self.df_state, audio)\n\n        save_audio(output_file, enhanced, self.df_state.sr())\n\n        return output_file\n\n\ndef get_enhancer(enhancer_name: Literal[\"df\"]) -> BaseEnhancer:\n    \"\"\"Get an audio enhancer\n\n    Args:\n        enhancer_name (Literal[\"df\"]): name of the audio enhancer\n\n    Raises:\n        ValueError: if the enhancer name is not recognised\n\n    Returns:\n        BaseEnhancer: audio enhancer\n    \"\"\"\n\n    if enhancer_name == \"df\":\n        import warnings\n\n        warnings.filterwarnings(\n            \"ignore\",\n            message='\"sinc_interpolation\" resampling method name is being deprecated and replaced by \"sinc_interp_hann\" in the next release. The default behavior remains unchanged.',\n        )\n        return DFEnhancer()\n    else:\n        raise ValueError(f\"Unknown enhancer name: {enhancer_name}\")\n"}
{"type": "source_file", "path": "fam/llm/adapters/base.py", "content": "from abc import ABC\n\n\nclass BaseDataAdapter(ABC):\n    pass\n"}
{"type": "source_file", "path": "fam/llm/loaders/training_data.py", "content": "from pathlib import Path\nfrom typing import Any, Mapping\n\nimport julius\nimport torch\nimport math\nimport numpy as np\nimport pandas as pd\nfrom audiocraft.data.audio import audio_read\nfrom encodec import EncodecModel\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom fam.llm.fast_inference_utils import encode_tokens\nfrom fam.llm.preprocessing.audio_token_mode import CombinerFuncT, CombinerFuncT\nfrom fam.llm.preprocessing.data_pipeline import pad_tokens\nfrom fam.llm.utils import normalize_text\nfrom fam.quantiser.audio.speaker_encoder.model import SpeakerEncoder\nfrom fam.quantiser.text.tokenise import TrainedBPETokeniser\n\nMBD_SAMPLE_RATE = 24000\nENCODEC_BANDWIDTH = 6\n\n\nclass DynamicComputeDataset(Dataset):\n    def __init__(\n        self,\n        dataset_dir: Path | str,\n        encodec_model: EncodecModel,\n        tokenizer: TrainedBPETokeniser,\n        spkemb_model: SpeakerEncoder,\n        combiner: CombinerFuncT,\n        pad_token: int,\n        ctx_window: int,\n        device: str,\n    ):\n        self.dataset_dir = dataset_dir\n        self.encodec_model = encodec_model\n        self.tokenizer = tokenizer\n        self.spkemb_model = spkemb_model\n        self.device = device\n        self.combiner = combiner\n        self.pad_token = pad_token\n        self.ctx_window = ctx_window\n        self.df = pd.read_csv(dataset_dir, delimiter=\"|\", index_col=False)\n\n    @classmethod\n    def from_meta(\n        cls,\n        tokenizer_info: Mapping[str, Any],\n        combiner: CombinerFuncT,\n        speaker_embedding_ckpt_path: Path | str,\n        dataset_dir: Path | str,\n        pad_token: int,\n        ctx_window: int,\n        device: str\n    ):\n        encodec = EncodecModel.encodec_model_24khz().to(device)\n        encodec.set_target_bandwidth(ENCODEC_BANDWIDTH)\n        smodel = SpeakerEncoder(\n            weights_fpath=str(speaker_embedding_ckpt_path),\n            eval=True,\n            device=device,\n            verbose=False,\n        )\n        tokeniser = TrainedBPETokeniser(**tokenizer_info)\n\n        return cls(\n            dataset_dir,\n            encodec,\n            tokeniser,\n            smodel,\n            combiner,\n            pad_token,\n            ctx_window,\n            device\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_path, text = self.df.iloc[idx].values.tolist()\n        with torch.no_grad():\n            text_tokens = self._extract_text_tokens(text)\n            encodec_tokens = self._extract_encodec_tokens(audio_path)\n            speaker_embedding = self._extract_speaker_embedding(audio_path)\n            combined = self.combiner(encodec_tokens, text_tokens)\n        padded_combined_tokens = pad_tokens(combined, self.ctx_window, self.pad_token)\n\n        return {\"tokens\": padded_combined_tokens, \"spkemb\": speaker_embedding}\n\n    def _extract_text_tokens(self, text: str):\n        _text = normalize_text(text)\n        _tokens = encode_tokens(self.tokenizer, _text, self.device)\n\n        return _tokens.detach().cpu().numpy()\n\n    def _extract_encodec_tokens(self, audio_path: str):\n        wav, sr = audio_read(audio_path)\n        if sr != MBD_SAMPLE_RATE:\n            wav = julius.resample_frac(wav, sr, MBD_SAMPLE_RATE)\n\n        # Convert to mono and fix dimensionality\n        if wav.ndim == 2:\n            wav = wav.mean(axis=0, keepdims=True)\n        wav = wav.unsqueeze(0)  # Add batch dimension\n\n        wav = wav.to(self.device)\n        tokens = self.encodec_model.encode(wav)\n        _tokens = tokens[0][0][0].detach().cpu().numpy()  # shape = [8, T]\n\n        return _tokens\n\n    def _extract_speaker_embedding(self, audio_path: str):\n        emb = self.spkemb_model.embed_utterance_from_file(audio_path, numpy=False)  # shape = [256,]\n        return emb.unsqueeze(0).detach()\n"}
{"type": "source_file", "path": "fam/llm/adapters/__init__.py", "content": "from fam.llm.adapters.flattened_encodec import FlattenedInterleavedEncodec2Codebook\nfrom fam.llm.adapters.tilted_encodec import TiltedEncodec\n"}
{"type": "source_file", "path": "fam/llm/fast_quantize.py", "content": "# Copyright (c) MetaVoice Labs Inc., Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted\n# provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this list of\n# conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice, this\n# list of conditions and the following disclaimer in the documentation and/or other\n# materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its contributors\n# may be used to endorse or promote products derived from this software without\n# specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR\n# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndefault_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n##### Quantization Primitives ######\n\n\ndef dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    # assumes symmetric quantization\n    # assumes axis == 0\n    # assumes dense memory format\n    # TODO(future): relax ^ as needed\n\n    # default setup for affine quantization of activations\n    eps = torch.finfo(torch.float32).eps\n\n    # get min and max\n    min_val, max_val = torch.aminmax(x, dim=1)\n\n    # calculate scales and zero_points based on min and max\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    # ensure scales is the same dtype as the original tensor\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n\n    # quantize based on qmin/qmax/scales/zp\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n\n    return quant, scales, zero_points\n\n\ndef get_group_qparams(w, n_bit=4, groupsize=128):\n    # needed for GPTQ with padding\n    if groupsize > w.shape[-1]:\n        groupsize = w.shape[-1]\n    assert groupsize > 1\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    max_val = to_quant.amax(dim=1, keepdim=True)\n    min_val = to_quant.amin(dim=1, keepdim=True)\n    max_int = 2**n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-6) / max_int\n    zeros = min_val + scales * (2 ** (n_bit - 1))\n    return scales.to(torch.bfloat16).reshape(w.shape[0], -1), zeros.to(torch.bfloat16).reshape(w.shape[0], -1)\n\n\ndef pack_scales_and_zeros(scales, zeros):\n    assert scales.shape == zeros.shape\n    assert scales.dtype == torch.bfloat16\n    assert zeros.dtype == torch.bfloat16\n    return (\n        torch.cat(\n            [\n                scales.reshape(scales.size(0), scales.size(1), 1),\n                zeros.reshape(zeros.size(0), zeros.size(1), 1),\n            ],\n            2,\n        )\n        .transpose(0, 1)\n        .contiguous()\n    )\n\n\ndef group_quantize_tensor_from_qparams(w, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column quantize\n    if groupsize > w.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w.shape[-1]\n\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n    min_val = zeros - scales * (2 ** (n_bit - 1))\n    max_int = 2**n_bit - 1\n    min_int = 0\n    w_int32 = to_quant.sub(min_val).div(scales).round().clamp_(min_int, max_int).to(torch.int32).reshape_as(w)\n\n    return w_int32\n\n\ndef group_quantize_tensor(w, n_bit=4, groupsize=128):\n    scales, zeros = get_group_qparams(w, n_bit, groupsize)\n    w_int32 = group_quantize_tensor_from_qparams(w, scales, zeros, n_bit, groupsize)\n    scales_and_zeros = pack_scales_and_zeros(scales, zeros)\n    return w_int32, scales_and_zeros\n\n\ndef group_dequantize_tensor_from_qparams(w_int32, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column dequantize\n    if groupsize > w_int32.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w_int32.shape[-1]\n    assert w_int32.shape[-1] % groupsize == 0\n    assert w_int32.dim() == 2\n\n    w_int32_grouped = w_int32.reshape(-1, groupsize)\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n\n    w_dq = w_int32_grouped.sub(2 ** (n_bit - 1)).mul(scales).add(zeros).reshape_as(w_int32)\n    return w_dq\n\n\n##### Weight-only int8 per-channel quantized code ######\n\n\ndef replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(module, name, WeightOnlyInt8Linear(child.in_features, child.out_features))\n        else:\n            replace_linear_weight_only_int8_per_channel(child)\n\n\nclass WeightOnlyInt8QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            # TODO: quantise RMSNorm as well.\n            if isinstance(mod, torch.nn.Linear):\n                int8_weight, scales, _ = dynamically_quantize_per_channel(mod.weight.float(), -128, 127, torch.int8)\n                cur_state_dict[f\"{fqn}.weight\"] = int8_weight.to(\"cpu\")\n                cur_state_dict[f\"{fqn}.scales\"] = scales.to(mod.weight.dtype).to(\"cpu\")\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_weight_only_int8_per_channel(self.mod)\n        return self.mod\n\n\nclass WeightOnlyInt8Linear(torch.nn.Module):\n    __constants__ = [\"in_features\", \"out_features\"]\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.register_buffer(\"weight\", torch.empty((out_features, in_features), dtype=torch.int8))\n        self.register_buffer(\"scales\", torch.ones(out_features, dtype=torch.bfloat16))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.to(dtype=input.dtype)) * self.scales\n\n\n##### weight only int4 per channel groupwise quantized code ######\n\n\ndef prepare_int4_weight_and_scales_and_zeros(weight_bf16, groupsize, inner_k_tiles):\n    weight_int32, scales_and_zeros = group_quantize_tensor(weight_bf16, n_bit=4, groupsize=groupsize)\n    weight_int4pack = torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)\n    return weight_int4pack, scales_and_zeros\n\n\ndef linear_forward_int4(x, weight_int4pack, scales_and_zeros, out_features, groupsize):\n    origin_x_size = x.size()\n    x = x.reshape(-1, origin_x_size[-1])\n    c = torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)\n    new_shape = origin_x_size[:-1] + (out_features,)\n    c = c.reshape(new_shape)\n    return c\n\n\ndef _check_linear_int4_k(k, groupsize=1, inner_k_tiles=1):\n    return k % groupsize == 0 and k % (inner_k_tiles * 16) == 0\n\n\ndef replace_linear_int4(module, groupsize, inner_k_tiles, padding, use_cuda):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and child.out_features % 8 == 0:\n            if _check_linear_int4_k(child.in_features, groupsize, inner_k_tiles):\n                setattr(\n                    module,\n                    name,\n                    WeightOnlyInt4Linear(\n                        child.in_features,\n                        child.out_features,\n                        bias=False,\n                        groupsize=groupsize,\n                        inner_k_tiles=inner_k_tiles,\n                        padding=False,\n                        use_cuda=use_cuda,\n                    ),\n                )\n            elif padding:\n                setattr(\n                    module,\n                    name,\n                    WeightOnlyInt4Linear(\n                        child.in_features,\n                        child.out_features,\n                        bias=False,\n                        groupsize=groupsize,\n                        inner_k_tiles=inner_k_tiles,\n                        padding=True,\n                        use_cuda=use_cuda,\n                    ),\n                )\n        else:\n            replace_linear_int4(child, groupsize, inner_k_tiles, padding, use_cuda)\n\n\nclass WeightOnlyInt4QuantHandler:\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        assert groupsize in [32, 64, 128, 256]\n        assert inner_k_tiles in [2, 4, 8]\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                assert not mod.bias\n                out_features = mod.out_features\n                in_features = mod.in_features\n                if out_features % 8 != 0:\n                    continue\n                assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n                print(f\"linear: {fqn}, in={in_features}, out={out_features}\")\n\n                weight = mod.weight.data\n                if not _check_linear_int4_k(in_features, self.groupsize, self.inner_k_tiles):\n                    if self.padding:\n                        import torch.nn.functional as F\n                        from model import find_multiple\n\n                        print(f\"warning: {fqn} is padded to satisfy in_features % 1024 == 0\")\n                        padded_in_features = find_multiple(in_features, 1024)\n                        weight = F.pad(weight, pad=(0, padded_in_features - in_features))\n                    else:\n                        print(\n                            f\"warning: {fqn} is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, \"\n                            + \"and that groupsize and inner_k_tiles*16 evenly divide into it\"\n                        )\n                        continue\n                weight_int4pack, scales_and_zeros = prepare_int4_weight_and_scales_and_zeros(\n                    weight.to(torch.bfloat16), self.groupsize, self.inner_k_tiles\n                )\n                cur_state_dict[f\"{fqn}.weight\"] = weight_int4pack.to(\"cpu\")\n                cur_state_dict[f\"{fqn}.scales_and_zeros\"] = scales_and_zeros.to(\"cpu\")\n\n        return cur_state_dict\n\n    def convert_for_runtime(self, use_cuda):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding, use_cuda)\n        return self.mod\n\n\nclass WeightOnlyInt4Linear(torch.nn.Module):\n    __constants__ = [\"in_features\", \"out_features\"]\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias=True,\n        device=None,\n        dtype=None,\n        groupsize: int = 128,\n        inner_k_tiles: int = 8,\n        padding: bool = True,\n        use_cuda=True,\n    ) -> None:\n        super().__init__()\n        self.padding = padding\n        if padding:\n            from model import find_multiple\n\n            self.origin_in_features = in_features\n            in_features = find_multiple(in_features, 1024)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias, \"require bias=False\"\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n\n        assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n        assert in_features % (inner_k_tiles * 16) == 0, \"require in_features % (innerKTiles * 16) == 0\"\n        if use_cuda:\n            self.register_buffer(\n                \"weight\",\n                torch.empty(\n                    (out_features // 8, in_features // (inner_k_tiles * 16), 32, inner_k_tiles // 2), dtype=torch.int32\n                ),\n            )\n        else:\n            self.register_buffer(\"weight\", torch.empty((out_features, in_features // 2), dtype=torch.uint8))\n        self.register_buffer(\n            \"scales_and_zeros\", torch.empty((in_features // groupsize, out_features, 2), dtype=torch.bfloat16)\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        input = input.to(torch.bfloat16)\n        if self.padding:\n            import torch.nn.functional as F\n\n            input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))\n        return linear_forward_int4(input, self.weight, self.scales_and_zeros, self.out_features, self.groupsize)\n"}
{"type": "source_file", "path": "fam/llm/preprocessing/__init__.py", "content": ""}
{"type": "source_file", "path": "fam/llm/utils.py", "content": "import hashlib\nimport json\nimport os\nimport re\nimport subprocess\nimport tempfile\n\nimport librosa\nimport torch\n\n\ndef normalize_text(text: str) -> str:\n    unicode_conversion = {\n        8175: \"'\",\n        8189: \"'\",\n        8190: \"'\",\n        8208: \"-\",\n        8209: \"-\",\n        8210: \"-\",\n        8211: \"-\",\n        8212: \"-\",\n        8213: \"-\",\n        8214: \"||\",\n        8216: \"'\",\n        8217: \"'\",\n        8218: \",\",\n        8219: \"`\",\n        8220: '\"',\n        8221: '\"',\n        8222: \",,\",\n        8223: '\"',\n        8228: \".\",\n        8229: \"..\",\n        8230: \"...\",\n        8242: \"'\",\n        8243: '\"',\n        8245: \"'\",\n        8246: '\"',\n        180: \"'\",\n        2122: \"TM\",  # Trademark\n    }\n\n    text = text.translate(unicode_conversion)\n\n    non_bpe_chars = set([c for c in list(text) if ord(c) >= 256])\n    if len(non_bpe_chars) > 0:\n        non_bpe_points = [(c, ord(c)) for c in non_bpe_chars]\n        raise ValueError(f\"Non-supported character found: {non_bpe_points}\")\n\n    text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"*\", \" \").strip()\n    text = re.sub(\"\\s\\s+\", \" \", text)  # remove multiple spaces\n    return text\n\n\ndef check_audio_file(path_or_uri, threshold_s=30):\n    if \"http\" in path_or_uri:\n        temp_fd, filepath = tempfile.mkstemp()\n        os.close(temp_fd)  # Close the file descriptor, curl will create a new connection\n        curl_command = [\"curl\", \"-L\", path_or_uri, \"-o\", filepath]\n        subprocess.run(curl_command, check=True)\n\n    else:\n        filepath = path_or_uri\n\n    audio, sr = librosa.load(filepath)\n    duration_s = librosa.get_duration(y=audio, sr=sr)\n    if duration_s < threshold_s:\n        raise Exception(\n            f\"The audio file is too short. Please provide an audio file that is at least {threshold_s} seconds long to proceed.\"\n        )\n\n    # Clean up the temporary file if it was created\n    if \"http\" in path_or_uri:\n        os.remove(filepath)\n\n\ndef get_default_dtype() -> str:\n    \"\"\"Compute default 'dtype' based on GPU architecture\"\"\"\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            device_properties = torch.cuda.get_device_properties(i)\n            dtype = \"float16\" if device_properties.major <= 7 else \"bfloat16\"  # tesla and turing architectures\n    else:\n        dtype = \"float16\"\n\n    print(f\"using dtype={dtype}\")\n    return dtype\n\n\ndef get_device() -> str:\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef hash_dictionary(d: dict):\n    # Serialize the dictionary into JSON with sorted keys to ensure consistency\n    serialized = json.dumps(d, sort_keys=True)\n    # Encode the serialized string to bytes\n    encoded = serialized.encode()\n    # Create a hash object (you can also use sha1, sha512, etc.)\n    hash_object = hashlib.sha256(encoded)\n    # Get the hexadecimal digest of the hash\n    hash_digest = hash_object.hexdigest()\n    return hash_digest\n"}
{"type": "source_file", "path": "fam/llm/preprocessing/data_pipeline.py", "content": "from typing import Any, Dict, Optional, Tuple\n\nimport torch\nimport numpy as np\n\n\ndef pad_tokens(tokens: np.ndarray, context_window: int, pad_token: int) -> np.ndarray:\n    \"\"\"Pads or truncates a single example to the context_window + 1 size.\n\n    tokens: (..., example_length)\n    \"\"\"\n    example_length = tokens.shape[-1]\n    if example_length > context_window + 1:\n        # Truncate\n        tokens = tokens[..., : context_window + 1]\n    elif example_length < context_window + 1:\n        # Pad\n        padding = np.full(tokens.shape[:-1] + (context_window + 1 - example_length,), pad_token)\n        tokens = np.concatenate([tokens, padding], axis=-1)\n    assert tokens.shape[-1] == context_window + 1\n    return tokens\n\n\ndef get_training_tuple(\n    batch: Dict[str, Any],\n    causal: bool,\n    num_codebooks: Optional[int],\n    speaker_cond: bool,\n    device: torch.device,\n) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n    # batch contains combined tokens as specified by audio_token_mode\n    if causal:\n        num_codebooks = batch[\"tokens\"].shape[1] if num_codebooks is None else num_codebooks\n        x = batch[\"tokens\"][:, :num_codebooks, :-1]\n        y = batch[\"tokens\"][:, :num_codebooks, 1:]\n\n    se = batch[\"spkemb\"]\n\n    x = x.to(device, non_blocking=True)\n    y = y.to(device, non_blocking=True)\n    se = se.to(device, non_blocking=True) if speaker_cond else None\n\n    return x, y, se\n\n\ndef pad_with_values(tensor, batch_size, value):\n    \"\"\"Pads the tensor up to batch_size with values.\"\"\"\n    if tensor.shape[0] < batch_size:\n        return torch.cat(\n            [\n                tensor,\n                torch.full(\n                    (batch_size - tensor.shape[0], *tensor.shape[1:]), value, dtype=tensor.dtype, device=tensor.device\n                ),\n            ]\n        )\n    else:\n        return tensor\n"}
{"type": "source_file", "path": "fam/llm/config/finetune_params.py", "content": "from contextlib import nullcontext\nimport os\nimport uuid\nimport pathlib\nfrom typing import Literal, Optional\nimport torch\n\nbatch_size = 2\ndataset_size: int = 400\nbatched_ds_size = dataset_size // batch_size\nval_train_ratio = 0.2\n\nepochs: int = 2\nmax_iters = batched_ds_size * epochs\nlearning_rate = 3e-5\nlast_n_blocks_to_finetune = 1\ndecay_lr = False\nlr_decay_iters = 0 # decay learning rate after this many iterations\nmin_lr = 3e-6\n\neval_interval = batched_ds_size\neval_iters = int(batched_ds_size*val_train_ratio)\neval_only: bool = False # if True, script exits right after the first eval\nlog_interval = batched_ds_size # don't print too too often\nsave_interval: int = batched_ds_size * (epochs//2) # save a checkpoint every this many iterations\nassert save_interval % eval_interval == 0, \"save_interval must be divisible by eval_interval.\"\nseed = 1337\ngrad_clip: float = 1.0  # clip gradients at this value, or disable if == 0.0\n\nwandb_log = False\nwandb_project = \"project-name\"\nwandb_run_name = \"run-name\"\nwandb_tags = [\"tag1\", \"tag2\"]\n\ngradient_accumulation_steps = 1\nblock_size = 2_048\naudio_token_mode = \"flattened_interleaved\"\nnum_max_audio_tokens_timesteps = 1_024\n\nn_layer = 24\nn_head = 16\nn_embd = 2048\ndropout = 0.1\n\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\n\nwarmup_iters: int = 0 # how many steps to warm up for\nout_dir = f\"finetune-{epochs=}-{learning_rate=}-{batch_size=}-{last_n_blocks_to_finetune=}-{dropout=}-{uuid.uuid4()}\"\n\ncompile = True\nnum_codebooks = None\nnorm_type = \"rmsnorm\"\nrmsnorm_eps = 1e-5\nnonlinearity_type = \"swiglu\"\nswiglu_multiple_of = 256\nattn_kernel_type = \"torch_attn\"\nmeta_target_vocab_sizes: Optional[list[int]] = None\nspeaker_emb_size: int = 256\nspeaker_cond = True\n\n# always running finetuning on a single GPU\nmaster_process = True\ndevice: str = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\nddp = False\nddp_world_size = 1\ntokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n\ncausal = True\nbias: bool = False  # do we use bias inside LayerNorm and Linear layers?\nspk_emb_on_text: bool = True  # whether to add speaker embedding conditioning to text tokens or not\n"}
{"type": "source_file", "path": "fam/llm/model.py", "content": "import inspect\nimport math\nfrom dataclasses import dataclass, field\nfrom typing import Literal, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport tqdm\nfrom einops import rearrange\nfrom torch.nn import functional as F\n\nfrom fam.llm.layers import Block, LayerNorm, RMSNorm\nfrom fam.llm.mixins import CausalInferenceMixin, NonCausalInferenceMixin\n\nEND_OF_TEXT_TOKEN = 1537\n\n\ndef _select_spkemb(spkemb, mask):\n    _, examples, _ = spkemb.shape\n    mask = torch.nn.functional.one_hot(mask.long(), num_classes=examples).to(spkemb)  # shape: (batch, time, examples)\n    spkemb = spkemb.transpose(1, 2)  # b ex c -> b c ex\n    mask = mask.transpose(1, 2)  # b t ex -> b ex t\n    return torch.bmm(spkemb, mask).transpose(1, 2)  # b c t -> b t c\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_sizes: list = field(default_factory=list)\n    target_vocab_sizes: Optional[list] = None\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    spkemb_dropout: float = 0.0\n    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n    causal: bool = (\n        True  # auto-regressive or not, i.e. whether to have attention mask that prevents attending to future tokens\n    )\n    spk_emb_on_text: bool = True  # whether to add speaker embedding conditioning to text tokens or not\n    norm_type: str = \"layernorm\"  # \"rmsnorm\" or \"layernorm\n    rmsnorm_eps: Optional[float] = None  # only used for rmsnorm\n    nonlinearity_type: str = \"gelu\"  # \"gelu\" or \"swiglu\"\n    swiglu_multiple_of: Optional[int] = None  # MLP hidden layer (using SwiGLU) will be multiple of this\n    attn_kernel_type: Literal[\"torch_attn\"] = \"torch_attn\"\n    kv_cache_enabled: bool = False  # whether to use key-value cache for attention\n\n\ndef _check_speaker_emb_dims(\n    speaker_embs: Union[list, torch.Tensor], expected_speaker_emb_dim: int, expected_batch_size: int\n) -> Union[torch.Tensor, list]:\n    \"\"\"\n    Checks that the speaker embedding dimensions are correct, and reshapes them if necessary.\n    \"\"\"\n    if type(speaker_embs) == list:\n        b_se = len(speaker_embs)\n        for i, s in enumerate(speaker_embs):\n            if s is not None:\n                emb_dim = s.shape[-1]\n                if s.ndim == 1:\n                    speaker_embs[i] = speaker_embs[i].unsqueeze(0)\n    else:\n        if speaker_embs.ndim == 2:\n            # if we have a single speaker embedding for the whole sequence,\n            # add a dummy dimension for backwards compatibility\n            speaker_embs = speaker_embs[:, None, :]\n\n        # num_examples is the number of utterances packed into this sequence\n        b_se, num_examples, emb_dim = speaker_embs.size()\n\n    assert b_se == expected_batch_size, f\"Batch size mismatch: {b_se} != {expected_batch_size}\"\n    assert (\n        emb_dim == expected_speaker_emb_dim\n    ), f\"Speaker embedding dimension mismatch: {emb_dim} != {expected_speaker_emb_dim}\"\n\n    return speaker_embs\n\n\nclass GPT(nn.Module, NonCausalInferenceMixin, CausalInferenceMixin):\n    def __init__(self, config: GPTConfig, speaker_emb_dim: Optional[int] = None):\n        \"\"\"\n        Initialize the GPT model.\n\n        Args:\n            config (GPTConfig): Configuration object for the model.\n            speaker_emb_dim (Optional[int]): Dimension of the speaker embedding. Default is None.\n        \"\"\"\n        super().__init__()\n        assert config.vocab_sizes is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.kv_cache_enabled = False  # disabled by default\n        self.kv_pos = 0\n\n        self.speaker_emb_dim = speaker_emb_dim\n        self.spk_emb_on_text = config.spk_emb_on_text\n        if self.config.causal is True and self.spk_emb_on_text is False:\n            print(\"!!!!!!!!!!!!!!!!!!\")\n            print(\n                f\"!!!!!!!! Using DEFAULT of {END_OF_TEXT_TOKEN} as end of text token to find speaker cond masking!! You likely need to change this.\"\n            )\n            print(\"!!!!!!!!!!!!!!!!!!\")\n        if self.config.causal is False and self.spk_emb_on_text is False:\n            raise Exception(\n                \"Cannot use speaker embedding masking with non-causal model. This is unexpected. Check for relevant changes required in code before proceeding.\"\n            )\n\n        if config.norm_type == \"rmsnorm\":\n            if config.rmsnorm_eps is None:\n                raise Exception(\"RMSNorm requires rmsnorm_eps to be set\")\n            ln_f = RMSNorm(config.n_embd, eps=config.rmsnorm_eps)\n        elif config.norm_type == \"layernorm\":\n            ln_f = LayerNorm(config.n_embd, bias=config.bias)\n        else:\n            raise Exception(f\"Unknown norm type: {config.norm_type}\")\n\n        self.transformer = nn.ModuleDict(\n            dict(\n                wtes=nn.ModuleList([nn.Embedding(vsize, config.n_embd) for vsize in config.vocab_sizes]),\n                wpe=nn.Embedding(config.block_size, config.n_embd),\n                drop=nn.Dropout(config.dropout),\n                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n                ln_f=ln_f,\n            )\n        )\n        if speaker_emb_dim is not None:\n            self.speaker_cond_pos = nn.Linear(speaker_emb_dim, config.n_embd, bias=False)\n\n        self.lm_heads = nn.ModuleList()\n        if config.target_vocab_sizes is not None:\n            assert config.causal is False\n        else:\n            assert config.causal is True\n\n        for vsize in config.vocab_sizes if config.target_vocab_sizes is None else config.target_vocab_sizes:\n            self.lm_heads.append(nn.Linear(config.n_embd, vsize, bias=False))\n\n        if config.target_vocab_sizes is None:\n            for i in range(len(config.vocab_sizes)):\n                # TODO: do we not need to take the transpose here?\n                # https://paperswithcode.com/method/weight-tying\n                self.lm_heads[i].weight = self.transformer.wtes[i].weight  # type: ignore\n            assert len(self.lm_heads) == len(\n                self.transformer.wtes  # type: ignore\n            ), f\"Number of heads ({len(self.lm_heads)}) must match number of one-hot embedding matrics ({len(self.transformer.wtes)}).\"  # type: ignore\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith(\"c_proj.weight\"):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def _mask_spk_emb_on_text(self, idx: torch.Tensor, spk_emb: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        This is in a separate function so we can test it easily.\n        \"\"\"\n        # find index of end of text token in each sequence, then generate a binary mask\n        # of shape (b, 1, t) to mask out the speaker embedding for all tokens before the end of text token.\n        # Note: this does NOT mask the <end_of_text_token> token. This is important so that the first audio token predicted\n        # has speaker information to use.\n\n        # Check in channel dimension 0 as this is usually the first hierarchy where we put the text tokens.\n        is_end_of_text = idx[:, 0, :] == END_OF_TEXT_TOKEN\n        # use > 0, in case end_of_text_token is repeated for any reason.\n        mask = (torch.cumsum(is_end_of_text, dim=-1) > 0).float()\n        spk_emb = spk_emb * mask[:, :, None]\n\n        return spk_emb\n\n    def forward(\n        self,\n        idx,\n        targets=None,\n        speaker_embs=None,\n        speaker_emb_mask=None,\n        loss_reduce: Literal[\"mean\", \"none\"] = \"mean\",\n    ):\n        device = idx.device\n        b, num_hierarchies, t = idx.size()\n\n        if speaker_embs is not None:\n            speaker_embs = _check_speaker_emb_dims(\n                speaker_embs=speaker_embs, expected_speaker_emb_dim=self.speaker_emb_dim, expected_batch_size=b\n            )\n\n        assert (\n            t <= self.config.block_size\n        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\n        if self.kv_cache_enabled:\n            if self.kv_pos == 0:\n                pos = torch.arange(0, t, dtype=torch.long, device=device)\n                self.kv_pos += t\n            else:\n                assert t == 1, \"KV cache is only supported for single token inputs\"\n                pos = torch.tensor([self.kv_pos], dtype=torch.long, device=device)  # shape (1)\n                self.kv_pos += 1\n        else:\n            pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n\n        # forward the GPT model itself\n        assert num_hierarchies == len(\n            self.transformer.wtes\n        ), f\"Input tensor has {num_hierarchies} hierarchies, but model has {len(self.transformer.wtes)} set of input embeddings.\"\n\n        # embed the tokens, positional encoding, and speaker embedding\n        tok_emb = torch.zeros((b, t, self.config.n_embd), device=device)\n        # ends up swapping (B, num_hierarchies, t) tokens -> (B, t, c) embeddings.\n        for i, wte in enumerate(self.transformer.wtes):\n            tok_emb += wte(idx[:, i, :])\n        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n\n        spk_emb = 0.0\n        if speaker_embs is not None:\n            if type(speaker_embs) == list:\n                assert speaker_emb_mask is None\n                assert self.training is False\n                assert self.spk_emb_on_text is True\n\n                spk_emb = []\n                for speaker_emb_row in speaker_embs:\n                    if speaker_emb_row is not None:\n                        spk_emb.append(self.speaker_cond_pos(speaker_emb_row.unsqueeze(0)))\n                        assert spk_emb[-1].shape == (1, 1, self.config.n_embd), f\"spk_emb[-1].shape={spk_emb[-1].shape}\"\n                    else:\n                        spk_emb.append(torch.zeros((1, 1, self.config.n_embd), device=device, dtype=pos_emb.dtype))\n                spk_emb = torch.cat(spk_emb, dim=0)\n\n                assert (\n                    spk_emb.ndim == 3 and spk_emb.shape[1] == 1 and spk_emb.shape[0] == b\n                ), f\"spk_emb.ndim={spk_emb.ndim}, spk_emb.shape={spk_emb.shape}, len(speaker_embs)={len(speaker_embs)}\"\n            else:\n                speakers_embedded = self.speaker_cond_pos(speaker_embs)  # shape (b, num_examples, c)\n\n                if speaker_emb_mask is not None:\n                    spk_emb = _select_spkemb(speakers_embedded, speaker_emb_mask)\n                    assert spk_emb.shape == (b, t, self.config.n_embd)\n                else:\n                    spk_emb = speakers_embedded\n                    # if we don't have a mask, we assume that the speaker embedding is the same for all tokens\n                    # then num_examples dimension just becomes the time dimension\n                    assert spk_emb.ndim == 3 and spk_emb.shape[1] == 1\n\n                if self.training and self.config.spkemb_dropout > 0.0:\n                    # Remove speaker conditioning at random.\n                    dropout = torch.ones_like(speakers_embedded) * (\n                        torch.rand(speakers_embedded.shape[0], 1, 1, device=device) >= self.config.spkemb_dropout\n                    )\n                    spk_emb = torch.where(dropout == 0, torch.zeros_like(speakers_embedded), speakers_embedded)\n\n            if self.spk_emb_on_text is False:\n                assert speaker_emb_mask is None, \"Not implemented for spk_emb_on_text=False\"\n                spk_emb = self._mask_spk_emb_on_text(idx, spk_emb)\n\n        x = self.transformer.drop(tok_emb + pos_emb + spk_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            list_logits = [lm_head(x) for lm_head in self.lm_heads]\n\n            losses = [\n                F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    targets[:, i, :].contiguous().view(-1),\n                    ignore_index=-1,\n                    reduction=loss_reduce,\n                )\n                for i, logits in enumerate(list_logits)\n            ]\n            # TODO: should we do this better without stack somehow?\n            losses = torch.stack(losses)\n            if loss_reduce == \"mean\":\n                losses = losses.mean()\n            else:\n                losses = rearrange(losses, \"h (b t) -> b h t\", h=len(self.lm_heads), b=b, t=t)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            if self.config.causal:\n                list_logits = [\n                    lm_head(x[:, [-1], :]) for lm_head in self.lm_heads\n                ]  # note: using list [-1] to preserve the time dim\n            else:\n                list_logits = [lm_head(x) for lm_head in self.lm_heads]\n            losses = None\n\n        return list_logits, losses\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {\"params\": decay_params, \"weight_decay\": weight_decay},\n            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == \"cuda\"\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    @torch.no_grad()\n    def generate(\n        self,\n        idx: torch.Tensor,\n        max_new_tokens: int,\n        seq_lens: Optional[list] = None,\n        temperature: float = 1.0,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        speaker_embs: Optional[torch.Tensor] = None,\n        batch_size: Optional[int] = None,\n        guidance_scale: Optional[Tuple[float, float]] = None,\n        dtype: torch.dtype = torch.bfloat16,\n        end_of_audio_token: int = 99999,  # Dummy values will disable early termination / guidance features.\n        end_of_text_token: int = 99999,\n    ):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,num_hierarchies,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        assert idx.dim() == 3, \"idx must be a batch of sequences of hierarchical tokens\"\n\n        if self.config.causal:\n            if seq_lens is None or batch_size is None:\n                raise Exception(\"seq_lens and batch_size must be provided for causal sampling\")\n\n            return self._causal_sample(\n                idx=idx,\n                max_new_tokens=max_new_tokens,\n                seq_lens=seq_lens,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                speaker_embs=speaker_embs,\n                batch_size=batch_size,\n                guidance_scale=guidance_scale,\n                dtype=dtype,\n                end_of_audio_token=end_of_audio_token,\n                end_of_text_token=end_of_text_token,\n            )\n\n        else:\n            if seq_lens is not None:\n                raise Exception(\"seq_lens is not supported yet for non-causal sampling\")\n\n            if batch_size is None:\n                raise Exception(\"batch_size must be provided for non-causal sampling\")\n\n            if guidance_scale is not None:\n                raise Exception(\"guidance_scale is not supported for non-causal sampling\")\n\n            if top_p is not None:\n                raise Exception(\"top_p is not supported for non-causal sampling\")\n\n            out = []\n            for start_index in tqdm.tqdm(range(0, idx.shape[0], batch_size), desc=\"Non-causal batching\"):\n                end_index = min(start_index + batch_size, idx.shape[0])\n                out.append(\n                    self._non_causal_sample(\n                        idx=idx[start_index:end_index],\n                        speaker_embs=speaker_embs[start_index:end_index] if speaker_embs is not None else None,\n                        temperature=temperature,\n                        top_k=top_k,\n                    )\n                )\n            return torch.cat(out, dim=0)\n"}
{"type": "source_file", "path": "fam/llm/fast_model.py", "content": "# Copyright (c) MetaVoice Labs Inc., Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted\n# provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this list of\n# conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice, this\n# list of conditions and the following disclaimer in the documentation and/or other\n# materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its contributors\n# may be used to endorse or promote products derived from this software without\n# specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR\n# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nfrom dataclasses import dataclass\nfrom functools import reduce\nfrom math import gcd\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.nn import functional as F\n\nfrom fam.llm.utils import get_default_dtype\n\nimport logging\n\n# Adjust the logging level\nlogger = logging.getLogger(\"torch\")\nlogger.setLevel(logging.ERROR)\n\n\ndef find_multiple(n: int, *args: Tuple[int]) -> int:\n    k = reduce(lambda x, y: x * y // gcd(x, y), args + (1,))\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n\n@dataclass\nclass ModelArgs:\n    block_size: int = 2048\n    vocab_size: int = 32000\n    n_layer: int = 32\n    n_head: int = 32\n    dim: int = 4096\n    speaker_emb_dim: int = 256\n    intermediate_size: int = None\n    n_local_heads: int = -1\n    head_dim: int = 64\n    norm_eps: float = 1e-5\n    dtype: torch.dtype = torch.bfloat16\n\n    def __post_init__(self):\n        if self.n_local_heads == -1:\n            self.n_local_heads = self.n_head\n        if self.intermediate_size is None:\n            hidden_dim = 4 * self.dim\n            n_hidden = int(2 * hidden_dim / 3)\n            self.intermediate_size = find_multiple(n_hidden, 256)\n        self.head_dim = self.dim // self.n_head\n\n        self.dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16}[get_default_dtype()]\n\n    @classmethod\n    def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        # fuzzy search\n        config = [config for config in transformer_configs if config in str(name).upper() or config in str(name)]\n        assert len(config) == 1, name\n        return cls(**transformer_configs[config[0]])\n\n\ntransformer_configs = {\n    \"metavoice-1B\": dict(\n        n_layer=24,\n        n_head=16,\n        dim=2048,\n        vocab_size=2562,\n    ),\n}\n\n\nclass KVCache(nn.Module):\n    def __init__(self, max_batch_size, max_seq_length, n_heads, head_dim, dtype):\n        super().__init__()\n        cache_shape = (max_batch_size, n_heads, max_seq_length, head_dim)\n        self.register_buffer(\"k_cache\", torch.zeros(cache_shape, dtype=dtype))\n        self.register_buffer(\"v_cache\", torch.zeros(cache_shape, dtype=dtype))\n\n    def update(self, input_pos, k_val, v_val):\n        # input_pos: [S], k_val: [B, H, S, D]\n        assert input_pos.shape[0] == k_val.shape[2]\n\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n\n        return k_out, v_out\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.config = config\n\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n        self.pos_embeddings = nn.Embedding(config.block_size, config.dim)\n        self.speaker_cond_pos = nn.Linear(config.speaker_emb_dim, config.dim, bias=False)\n        self.layers = nn.ModuleList(TransformerBlock(config) for _ in range(config.n_layer))\n        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n        self.mask_cache: Optional[Tensor] = None\n        self.max_batch_size = -1\n        self.max_seq_length = -1\n\n    def setup_spk_cond_mask(self):\n        self.spk_cond_mask = torch.zeros((2, 1, self.config.dim), dtype=torch.bool)\n        self.spk_cond_mask[0] = 1\n\n    def setup_caches(self, max_batch_size, max_seq_length):\n        if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(\n                max_batch_size, max_seq_length, self.config.n_local_heads, head_dim, dtype=self.config.dtype\n            )\n\n        self.causal_mask = torch.tril(torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool))\n\n    def forward(self, idx: Tensor, spk_emb: Tensor, input_pos: Tensor) -> Tensor:\n        mask = self.causal_mask[None, None, input_pos]\n        x = (\n            self.tok_embeddings(idx)\n            + self.pos_embeddings(input_pos)\n            # masking for speaker condition free guidance\n            + self.speaker_cond_pos(spk_emb) * self.spk_cond_mask\n        )\n\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.attention = Attention(config)\n        self.feed_forward = FeedForward(config)\n        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n\n    def forward(self, x: Tensor, input_pos: Tensor, mask: Tensor) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), mask, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        assert config.dim % config.n_head == 0\n\n        total_head_dim = (config.n_head + 2 * config.n_local_heads) * config.head_dim\n        # key, query, value projections for all heads, but in a batch\n        self.wqkv = nn.Linear(config.dim, total_head_dim, bias=False)\n        self.wo = nn.Linear(config.dim, config.dim, bias=False)\n        self.kv_cache = None\n\n        self.n_head = config.n_head\n        self.head_dim = config.head_dim\n        self.n_local_heads = config.n_local_heads\n        self.dim = config.dim\n\n    def forward(\n        self,\n        x: Tensor,\n        mask: Tensor,\n        input_pos: Optional[Tensor] = None,\n    ) -> Tensor:\n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n\n        k = k.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        v = v.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n\n        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)\n\n        y = self.wo(y)\n        return y\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w3 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return F.silu(self.w1(x)) * self.w3(x)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.swiglu = SwiGLU(config)\n        self.w2 = nn.Linear(config.intermediate_size, config.dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.w2(self.swiglu(x))\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n"}
{"type": "source_file", "path": "fam/llm/decoders.py", "content": "from datetime import datetime\nimport os\nimport pathlib\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Optional, Union\n\nimport julius\nimport torch\nfrom audiocraft.data.audio import audio_read, audio_write\nfrom audiocraft.models import MultiBandDiffusion  # type: ignore\n\nmbd = MultiBandDiffusion.get_mbd_24khz(bw=6)  # 1.5\n\n\nclass Decoder(ABC):\n    @abstractmethod\n    def decode(self, tokens: list[int], ref_audio_path: Optional[str] = None, causal: Optional[bool] = None):\n        raise NotImplementedError\n\n\nclass EncodecDecoder(Decoder):\n    def __init__(\n        self,\n        tokeniser_decode_fn: Callable[[list[int]], str],\n        data_adapter_fn: Callable[[list[list[int]]], tuple[list[int], list[list[int]]]],\n        output_dir: str,\n    ):\n        self._mbd_sample_rate = 24_000\n        self._end_of_audio_token = 1024\n        self._num_codebooks = 8\n        self.mbd = mbd\n\n        self.tokeniser_decode_fn = tokeniser_decode_fn\n        self._data_adapter_fn = data_adapter_fn\n\n        self.output_dir = pathlib.Path(output_dir).resolve()\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def _save_audio(self, name: str, wav: torch.Tensor):\n        audio_write(\n            name,\n            wav.squeeze(0).cpu(),\n            self._mbd_sample_rate,\n            strategy=\"loudness\",\n            loudness_compressor=True,\n        )\n\n    def get_tokens(self, audio_path: str) -> list[list[int]]:\n        \"\"\"\n        Utility method to get tokens from audio. Useful when you want to test reconstruction in some form (e.g.\n        limited codebook reconstruction or sampling from second stage model only).\n        \"\"\"\n        pass\n        wav, sr = audio_read(audio_path)\n        if sr != self._mbd_sample_rate:\n            wav = julius.resample_frac(wav, sr, self._mbd_sample_rate)\n        if wav.ndim == 2:\n            wav = wav.unsqueeze(1)\n        wav = wav.to(\"cuda\")\n        tokens = self.mbd.codec_model.encode(wav)\n        tokens = tokens[0][0]\n\n        return tokens.tolist()\n\n    def decode(\n        self, tokens: list[list[int]], causal: bool = True, ref_audio_path: Optional[str] = None\n    ) -> Union[str, torch.Tensor]:\n        # TODO: this has strange behaviour -- if causal is True, it returns tokens. if causal is False, it SAVES the audio file.\n        text_ids, extracted_audio_ids = self._data_adapter_fn(tokens)\n        text = self.tokeniser_decode_fn(text_ids)\n        # print(f\"Text: {text}\")\n\n        tokens = torch.tensor(extracted_audio_ids, device=\"cuda\").unsqueeze(0)\n\n        if tokens.shape[1] < self._num_codebooks:\n            tokens = torch.cat(\n                [tokens, *[torch.ones_like(tokens[0:1, 0:1]) * 0] * (self._num_codebooks - tokens.shape[1])], dim=1\n            )\n\n        if causal:\n            return tokens\n        else:\n            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32):\n                wav = self.mbd.tokens_to_wav(tokens)\n            # NOTE: we couldn't just return wav here as it goes through loudness compression etc :)\n\n        if wav.shape[-1] < 9600:\n            # this causes problem for the code below, and is also odd :)\n            # first happened for tokens (1, 8, 28) -> wav (1, 1, 8960) (~320x factor in time dimension!)\n            raise Exception(\"wav predicted is shorter than 400ms!\")\n\n        try:\n            wav_file_name = self.output_dir / f\"synth_{datetime.now().strftime('%y-%m-%d--%H-%M-%S')}_{text.replace(' ', '_')[:25]}_{uuid.uuid4()}\"\n            self._save_audio(wav_file_name, wav)\n            return wav_file_name\n        except Exception as e:\n            print(f\"Failed to save audio! Reason: {e}\")\n\n            wav_file_name = self.output_dir / f\"synth_{datetime.now().strftime('%y-%m-%d--%H-%M-%S')}_{uuid.uuid4()}\"\n            self._save_audio(wav_file_name, wav)\n            return wav_file_name\n"}
{"type": "source_file", "path": "fam/llm/layers/layers.py", "content": "import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass LayerNorm(nn.Module):\n    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, ndim: int, eps: float):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(ndim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        return self._norm(x) * self.weight\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_dim, out_dim, bias) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(in_dim, out_dim, bias=bias)\n        self.w3 = nn.Linear(in_dim, out_dim, bias=bias)\n\n    def forward(self, x):\n        return F.silu(self.w1(x)) * self.w3(x)\n\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.non_linearity = config.nonlinearity_type\n        hidden_dim = 4 * config.n_embd\n        if config.nonlinearity_type == \"gelu\":\n            self.c_fc = nn.Linear(config.n_embd, hidden_dim, bias=config.bias)\n            self.gelu = nn.GELU()\n            self.c_proj = nn.Linear(hidden_dim, config.n_embd, bias=config.bias)\n        elif config.nonlinearity_type == \"swiglu\":\n            if config.swiglu_multiple_of is None:\n                raise Exception(\"SwiGLU requires swiglu_multiple_of to be set\")\n            hidden_dim = int(2 * hidden_dim / 3)\n            hidden_dim = config.swiglu_multiple_of * math.ceil(hidden_dim / config.swiglu_multiple_of)\n            # set name to `c_proj` so that the right initialisation gets applied to it in GPT.__init__()\n            self.swiglu = SwiGLU(config.n_embd, hidden_dim, bias=config.bias)\n            self.c_proj = nn.Linear(hidden_dim, config.n_embd, bias=config.bias)\n        else:\n            raise Exception(f\"Unknown nonlinearity type: {config.nonlinearity_type}\")\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        if self.non_linearity == \"gelu\":\n            x = self.c_fc(x)\n            x = self.gelu(x)\n        elif self.non_linearity == \"swiglu\":\n            x = self.swiglu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n"}
{"type": "source_file", "path": "fam/llm/mixins/non_causal.py", "content": "from typing import Optional\n\nimport torch\nfrom torch.nn import functional as F\n\n\nclass NonCausalInferenceMixin:\n    \"\"\"\n    Mixin class for non-causal inference in a language model.\n\n    This class provides methods for performing non-causal sampling using a language model.\n    \"\"\"\n\n    @torch.no_grad()\n    def _non_causal_sample(\n        self, *, idx: torch.Tensor, speaker_embs: Optional[torch.Tensor], temperature: float, top_k: int\n    ):\n        \"\"\"\n        Perform non-causal sampling.\n\n        Args:\n            idx (torch.Tensor): Input tensor of shape (batch_size, num_in_hierarchies, sequence_length).\n            speaker_embs (Optional[torch.Tensor]): Speaker embeddings tensor of shape (batch_size, embedding_size).\n            temperature (float): Temperature parameter for scaling the logits.\n            top_k (int): Number of top options to consider.\n\n        Returns:\n            torch.Tensor: Sampled output tensor of shape (batch_size, num_out_hierarchies, sequence_length).\n        \"\"\"\n        b, c, t = idx.size()\n        assert t == self.config.block_size, f\"input size {t} != config.block_size {self.config.block_size}\"\n        # forward the model to get the logits for the index in the sequence\n        list_logits, _ = self(idx, speaker_embs=speaker_embs)  # c x (b, t, vocab_size)\n\n        # scale by desired temperature\n        list_logits = [logits / temperature for logits in list_logits]  # c x (b, t, vocab_size)\n\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            for i in range(len(list_logits)):\n                logits = list_logits[i]  # (b, t, vocab_size)\n\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))  # (b, t, top_k)\n                logits[logits < v[:, :, [-1]]] = -float(\"Inf\")\n                list_logits[i] = logits  # (b, t, vocab_size)\n                assert logits.shape[0] == b and logits.shape[1] == t\n\n        # apply softmax to convert logits to (normalized) probabilities\n        # TODO: check shapes here!\n        probs = [F.softmax(logits, dim=-1) for logits in list_logits]  # c x (b, t, top_k)\n        assert probs[0].shape[0] == b and probs[0].shape[1] == t\n\n        # TODO: output shape is as expected\n        outs = []\n        for b_prob in probs:  # c x (b, t, top_k) -> (b, t, top_k)\n            out = [\n                torch.multinomial(prob, num_samples=1).transpose(0, 1).unsqueeze(0) for prob in b_prob\n            ]  # b x (t, top_k) -> b x (t, 1) -> b x (1, t) -> b x (1, 1, t)\n            assert len(out) == b and out[0].shape[0] == 1 and out[0].shape[1] == 1 and out[0].shape[2] == t\n            out = torch.cat(out, dim=0)  # (b, 1, t)\n            assert out.shape[0] == b and out.shape[1] == 1 and out.shape[2] == t\n            outs.append(out)\n\n        out = torch.cat(outs, dim=1)  # (b, c, t)\n        assert out.shape[0] == b and out.shape[2] == t\n\n        return out\n"}
{"type": "source_file", "path": "fam/llm/config/__init__.py", "content": ""}
{"type": "source_file", "path": "fam/llm/mixins/__init__.py", "content": "from fam.llm.mixins.causal import CausalInferenceMixin\nfrom fam.llm.mixins.non_causal import NonCausalInferenceMixin\n"}
{"type": "source_file", "path": "fam/llm/__init__.py", "content": ""}
{"type": "source_file", "path": "fam/llm/layers/__init__.py", "content": "from fam.llm.layers.attn import SelfAttention\nfrom fam.llm.layers.combined import Block\nfrom fam.llm.layers.layers import MLP, LayerNorm, RMSNorm, SwiGLU\n"}
{"type": "source_file", "path": "fam/llm/finetune.py", "content": "\"\"\"\nModule responsible for finetuning the first stage LLM.\n\"\"\"\n\nimport itertools\nimport math\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nimport click\nimport torch\nfrom huggingface_hub import snapshot_download\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom fam.llm.config.finetune_params import *\nfrom fam.llm.loaders.training_data import DynamicComputeDataset\nfrom fam.llm.model import GPT, GPTConfig\nfrom fam.llm.preprocessing.audio_token_mode import get_params_for_mode\nfrom fam.llm.preprocessing.data_pipeline import get_training_tuple\nfrom fam.llm.utils import hash_dictionary\nfrom fam.telemetry import TelemetryEvent\nfrom fam.telemetry.posthog import PosthogClient\n\n# see fam/telemetry/README.md for more information\nposthog = PosthogClient()\n\ndtype: Literal[\"bfloat16\", \"float16\", \"tfloat32\", \"float32\"] = (\n    \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\"\n)  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nseed_offset = 0\n\ntorch.manual_seed(seed + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True if dtype != \"float32\" else False\ntorch.backends.cudnn.allow_tf32 = True if dtype != \"float32\" else False\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\nptdtype = {\"float32\": torch.float32, \"tfloat32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[\n    dtype\n]\nctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\nprint(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n\nckpts_base_dir = pathlib.Path(__file__).resolve().parent / \"ckpts\"\nif not os.path.exists(ckpts_base_dir) and master_process:\n    print(\"Checkpoints directory didn't exist, creating...\")\n    ckpts_base_dir.mkdir(parents=True)\n\nif master_process:\n    if \"/\" in out_dir:\n        raise Exception(\"out_dir should be just a name, not a path with slashes\")\n\n    ckpts_save_dir = ckpts_base_dir / out_dir\n    os.makedirs(ckpts_save_dir, exist_ok=True)\n\n\ndef get_globals_state():\n    \"\"\"Return entirety of configuration global state which can be used for logging.\"\"\"\n    config_keys = [k for k, v in globals().items() if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))]\n    return {k: globals()[k] for k in config_keys}  # will be useful for logging\n\n\nmodel_args: dict = dict(\n    n_layer=n_layer,\n    n_head=n_head,\n    n_embd=n_embd,\n    block_size=block_size,\n    bias=bias,\n    vocab_sizes=None,\n    dropout=dropout,\n    causal=causal,\n    norm_type=norm_type,\n    rmsnorm_eps=rmsnorm_eps,\n    nonlinearity_type=nonlinearity_type,\n    spk_emb_on_text=spk_emb_on_text,\n    attn_kernel_type=attn_kernel_type,\n    swiglu_multiple_of=swiglu_multiple_of,\n)  # start with model_args from command line\n\n\ndef strip_prefix(state_dict: Dict[str, Any], unwanted_prefix: str):\n    # TODO: this also appears in fast_inference_utils._load_model, it should be moved to a common place.\n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n    return state_dict\n\n\ndef force_ckpt_args(model_args, checkpoint_model_args) -> None:\n    # force these config attributes to be equal otherwise we can't even resume training\n    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_sizes\", \"causal\"]:\n        model_args[k] = checkpoint_model_args[k]\n    # this enables backward compatability with previously saved checkpoints.\n    for k in [\n        \"target_vocab_sizes\",\n        \"norm_type\",\n        \"rmsnorm_eps\",\n        \"nonlinearity_type\",\n        \"attn_kernel_type\",\n        \"spk_emb_on_text\",\n        \"swiglu_multiple_of\",\n    ]:\n        if k in checkpoint_model_args:\n            model_args[k] = checkpoint_model_args[k]\n    if attn_kernel_type != model_args[\"attn_kernel_type\"]:\n        print(\n            f'Found {model_args[\"attn_kernel_type\"]} kernel type inside model,',\n            f\"but expected {attn_kernel_type}. Manually replacing it.\",\n        )\n        model_args[\"attn_kernel_type\"] = attn_kernel_type\n\n\n@click.command()\n@click.option(\"--train\", type=click.Path(exists=True, path_type=Path), required=True)\n@click.option(\"--val\", type=click.Path(exists=True, path_type=Path), required=True)\n@click.option(\"--model-id\", type=str, required=False, default=\"metavoiceio/metavoice-1B-v0.1\")\n@click.option(\"--ckpt\", type=click.Path(exists=True, path_type=Path))\n@click.option(\"--spk-emb-ckpt\", type=click.Path(exists=True, path_type=Path))\ndef main(train: Path, val: Path, model_id: str, ckpt: Optional[Path], spk_emb_ckpt: Optional[Path]):\n    if ckpt and spk_emb_ckpt:\n        checkpoint_path, spk_emb_ckpt_path = ckpt, spk_emb_ckpt\n    else:\n        _model_dir = snapshot_download(repo_id=model_id)\n        checkpoint_path = Path(f\"{_model_dir}/first_stage.pt\")\n        spk_emb_ckpt_path = Path(f\"{_model_dir}/speaker_encoder.pt\")\n\n    mode_params = get_params_for_mode(audio_token_mode, num_max_audio_tokens_timesteps=num_max_audio_tokens_timesteps)\n    config = get_globals_state()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, map_location=device)\n    iter_num = checkpoint.get(\"iter_num\", 0)\n    best_val_loss = checkpoint.get(\"best_val_loss\", 1e9)\n    checkpoint_model_args = checkpoint[\"model_args\"]\n    tokenizer_info = checkpoint.get(\"meta\", {}).get(\"tokenizer\", {})\n    force_ckpt_args(model_args, checkpoint_model_args)\n    gptconf = GPTConfig(**model_args)  # type: ignore\n    model = GPT(gptconf, speaker_emb_dim=speaker_emb_size if speaker_cond else None)\n\n    # removing torch.compile module prefixes for pre-compile loading\n    state_dict = strip_prefix(checkpoint[\"model\"], \"_orig_mod.\")\n    model.load_state_dict(state_dict)\n    model.to(device)\n    # initialize a GradScaler. If enabled=False scaler is a no-op\n    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n    if compile:\n        print(\"Compiling the model... (takes a ~minute)\")\n        # requires PyTorch 2.0\n        from einops._torch_specific import allow_ops_in_compiled_graph\n\n        allow_ops_in_compiled_graph()\n        model = torch.compile(model)  # type: ignore\n\n    def estimate_loss(dataset, iters: int = eval_iters):\n        \"\"\"Estimate loss on a dataset by running on `iters` batches.\"\"\"\n        if dataset is None:\n            return torch.nan\n        losses = []\n        for _, batch in zip(tqdm(range(iters)), dataset):\n            X, Y, SE = get_training_tuple(batch, causal, num_codebooks, speaker_cond, device)\n            with ctx:\n                _, loss = model(X, Y, speaker_embs=SE, speaker_emb_mask=None)\n            losses.append(loss.item())\n        return torch.tensor(losses).mean()\n\n    # learning rate decay scheduler (cosine with warmup)\n    def get_lr(it):\n        # 1) linear warmup for warmup_iters steps\n        if it < warmup_iters:\n            return learning_rate * it / warmup_iters\n        # 2) if it > lr_decay_iters, return min learning rate\n        if it > lr_decay_iters:\n            return min_lr\n        # 3) in between, use cosine decay down to min learning rate\n        decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n        assert 0 <= decay_ratio <= 1\n        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n        return min_lr + coeff * (learning_rate - min_lr)\n\n    if wandb_log and master_process:\n        import wandb\n\n        if os.environ.get(\"WANDB_RUN_ID\", None) is not None:\n            resume = \"must\"\n        else:\n            resume = None\n\n        wandb.init(project=wandb_project, name=wandb_run_name, tags=wandb_tags, config=config, resume=resume)\n\n    train_dataset = DynamicComputeDataset.from_meta(\n        tokenizer_info,\n        mode_params[\"combine_func\"],\n        spk_emb_ckpt_path,\n        train,\n        mode_params[\"pad_token\"],\n        mode_params[\"ctx_window\"],\n        device,\n    )\n    val_dataset = DynamicComputeDataset.from_meta(\n        tokenizer_info,\n        mode_params[\"combine_func\"],\n        spk_emb_ckpt_path,\n        val,\n        mode_params[\"pad_token\"],\n        mode_params[\"ctx_window\"],\n        device,\n    )\n    train_dataloader = itertools.cycle(DataLoader(train_dataset, batch_size, shuffle=True))\n    train_data = iter(train_dataloader)\n    # we do not perform any explicit checks for dataset overlap & leave it to the user\n    # to handle this\n    eval_val_data = DataLoader(val_dataset, batch_size, shuffle=True)\n    # we can use the same Dataset object given it is a mapped dataset & not an iterable\n    # one that can be exhausted. This implies we will be needlessly recomputing, fine\n    # for now.\n    eval_train_data = DataLoader(train_dataset, batch_size, shuffle=True)\n\n    batch = next(train_data)\n    X, Y, SE = get_training_tuple(batch, causal, num_codebooks, speaker_cond, device)\n\n    t0 = time.time()\n    local_iter_num = 0  # number of iterations in the lifetime of this process\n    raw_model = model.module if ddp else model  # unwrap DDP container if needed\n    running_mfu = -1.0\n    total_norm = 0.0\n    save_checkpoint = False\n    if master_process:\n        progress = tqdm(total=max_iters, desc=\"Training\", initial=iter_num)\n    else:\n        progress = None\n\n    # finetune last X transformer blocks and the ln_f layer\n    trainable_count = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Before layer freezing {trainable_count(model)=}...\")\n    for param in model.parameters():\n        param.requires_grad = False\n    for param in itertools.chain(\n        model.transformer.ln_f.parameters(), model.transformer.h[last_n_blocks_to_finetune * -1 :].parameters()\n    ):\n        param.requires_grad = True\n    print(f\"After freezing excl. last {last_n_blocks_to_finetune} transformer blocks: {trainable_count(model)=}...\")\n\n    # log start of finetuning event\n    properties = {\n        **config,\n        **model_args,\n        \"train\": str(train),\n        \"val\": str(val),\n        \"model_id\": model_id,\n        \"ckpt\": ckpt,\n        \"spk_emb_ckpt\": spk_emb_ckpt,\n    }\n    finetune_jobid = hash_dictionary(properties)\n    posthog.capture(\n        TelemetryEvent(\n            name=\"user_started_finetuning\",\n            properties={\"finetune_jobid\": finetune_jobid, **properties},\n        )\n    )\n\n    while True:\n        lr = get_lr(iter_num) if decay_lr else learning_rate\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n        if master_process:\n            if iter_num % eval_interval == 0 and master_process:\n                ckpt_save_name = f\"ckpt_{iter_num:07d}.pt\"\n                with torch.no_grad():\n                    model.eval()\n                    losses = {\n                        \"train\": estimate_loss(eval_train_data),\n                        \"val\": estimate_loss(eval_val_data),\n                    }\n                    model.train()\n                print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n                if wandb_log:\n                    wandb.log(\n                        {\n                            \"iter\": iter_num,\n                            \"train/loss\": losses[\"train\"],\n                            \"val/loss\": losses[\"val\"],\n                            \"lr\": lr,\n                            \"mfu\": running_mfu * 100,  # convert to percentage\n                            \"stats/total_norm\": total_norm,\n                        }\n                    )\n                if losses[\"val\"] < best_val_loss:\n                    best_val_loss = losses[\"val\"]\n                    if iter_num > 0:\n                        ckpt_save_name = ckpt_save_name.replace(\n                            \".pt\", f\"_bestval_{best_val_loss}\".replace(\".\", \"_\") + \".pt\"\n                        )\n                        save_checkpoint = True\n\n                save_checkpoint = save_checkpoint or iter_num % save_interval == 0\n                if save_checkpoint and iter_num > 0:\n                    checkpoint = {\n                        \"model\": raw_model.state_dict(),  # type: ignore\n                        \"optimizer\": optimizer.state_dict(),\n                        \"model_args\": model_args,\n                        \"iter_num\": iter_num,\n                        \"best_val_loss\": best_val_loss,\n                        \"config\": config,\n                        \"meta\": {\n                            \"speaker_cond\": speaker_cond,\n                            \"speaker_emb_size\": speaker_emb_size,\n                            \"tokenizer\": tokenizer_info,\n                        },\n                    }\n                    torch.save(checkpoint, os.path.join(ckpts_save_dir, ckpt_save_name))\n                    print(f\"saving checkpoint to {ckpts_save_dir}\")\n                    save_checkpoint = False\n            if iter_num == 0 and eval_only:\n                break\n            # forward backward update, with optional gradient accumulation to simulate larger batch size\n            # and using the GradScaler if data type is float16\n            for micro_step in range(gradient_accumulation_steps):\n                if ddp:\n                    # in DDP training we only need to sync gradients at the last micro step.\n                    # the official way to do this is with model.no_sync() context manager, but\n                    # I really dislike that this bloats the code and forces us to repeat code\n                    # looking at the source of that context manager, it just toggles this variable\n                    model.require_backward_grad_sync = micro_step == gradient_accumulation_steps - 1  # type: ignore\n                with ctx:  # type: ignore\n                    logits, loss = model(X, Y, speaker_embs=SE, speaker_emb_mask=None)\n                    loss = loss / gradient_accumulation_steps  # scale the loss to account for gradient accumulation\n                # immediately async prefetch next batch while model is doing the forward pass on the GPU\n                batch = next(train_data)\n                X, Y, SE = get_training_tuple(\n                    batch,\n                    causal,\n                    num_codebooks,\n                    speaker_cond,\n                    device,\n                )\n                # backward pass, with gradient scaling if training in fp16\n                scaler.scale(loss).backward()\n            # clip the gradient\n            if grad_clip != 0.0:\n                scaler.unscale_(optimizer)\n                total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            # step the optimizer and scaler if training in fp16\n            scaler.step(optimizer)\n            scaler.update()\n            # flush the gradients as soon as we can, no need for this memory anymore\n            optimizer.zero_grad(set_to_none=True)\n\n            # timing and logging\n            t1 = time.time()\n            dt = t1 - t0\n            t0 = t1\n            if master_process:\n                # get loss as float. note: this is a CPU-GPU sync point\n                # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n                lossf = loss.item() * gradient_accumulation_steps\n                progress.update(1)\n                progress.set_description(f\"Training: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n                if iter_num % log_interval == 0:\n                    print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n\n            iter_num += 1\n            local_iter_num += 1\n\n            # termination conditions\n            if iter_num > max_iters:\n                # log end of finetuning event\n                posthog.capture(\n                    TelemetryEvent(\n                        name=\"user_completed_finetuning\",\n                        properties={\"finetune_jobid\": finetune_jobid, \"loss\": round(lossf, 4)},\n                    )\n                )\n                break\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "fam/llm/preprocessing/audio_token_mode.py", "content": "from functools import partial\nfrom typing import Any, Callable, Literal, Optional\n\nimport numpy as np\n\nAudioTokenModeT = Literal[\"flattened_interleaved\"]\nCombinerWithOffsetFuncT = Callable[[np.ndarray, np.ndarray, int], np.ndarray]\nCombinerFuncT = Callable[[np.ndarray, np.ndarray], np.ndarray]\n\n\ndef combine_tokens_flattened_interleaved(\n    audio_tokens: np.ndarray, text_tokens: np.ndarray, second_hierarchy_flattening_offset: int\n) -> np.ndarray:\n    \"\"\"\n    Flattens & interleaves first 2 of the audio token hierarchies. Note that the tokens for the second hierarchy\n    are also offset by second_hierarchy_flattening_offset as part of this transform to avoid conflict with values for the\n    first hierarchy.\n    \"\"\"\n    assert np.issubdtype(audio_tokens.dtype, np.integer)\n    assert np.issubdtype(text_tokens.dtype, np.integer)\n\n    num_hierarchies = audio_tokens.shape[0]\n    assert num_hierarchies >= 2, f\"Unexpected number of hierarchies: {num_hierarchies}. Expected at least 2.\"\n\n    # choosing -5 so that we can't get error!\n    interleaved_audio_tokens = np.full((len(audio_tokens[0]) + len(audio_tokens[1]),), -5)\n    interleaved_audio_tokens[::2] = audio_tokens[0]\n    interleaved_audio_tokens[1::2] = audio_tokens[1] + second_hierarchy_flattening_offset\n\n    tokens = np.concatenate([text_tokens, interleaved_audio_tokens])\n\n    return np.expand_dims(tokens, axis=0)\n\n\ndef get_params_for_mode(\n    audio_token_mode: AudioTokenModeT, num_max_audio_tokens_timesteps: Optional[int] = None\n) -> dict[str, Any]:\n    if audio_token_mode == \"flattened_interleaved\":\n        return {\n            \"text_tokenisation_offset\": 1024 * 2 + 1,\n            \"pad_token\": 1024 * 2,\n            \"ctx_window\": num_max_audio_tokens_timesteps * 2 if num_max_audio_tokens_timesteps else None,\n            \"second_hierarchy_flattening_offset\": 1024,\n            # TODO: fix the repeat of `second_hierarchy_flattening_offset`\n            \"combine_func\": partial(\n                combine_tokens_flattened_interleaved,\n                second_hierarchy_flattening_offset=1024,\n            ),\n        }\n    else:\n        raise Exception(f\"Unknown mode {audio_token_mode}\")\n"}
{"type": "source_file", "path": "fam/llm/loaders/__init__.py", "content": ""}
{"type": "source_file", "path": "fam/llm/fast_inference.py", "content": "import os\nimport shutil\nimport tempfile\nimport time\nfrom pathlib import Path\nfrom typing import Literal, Optional\n\nimport librosa\nimport torch\nimport tyro\nfrom huggingface_hub import snapshot_download\n\nfrom fam.llm.adapters import FlattenedInterleavedEncodec2Codebook\nfrom fam.llm.decoders import EncodecDecoder\nfrom fam.llm.fast_inference_utils import build_model, main\nfrom fam.llm.inference import (\n    EncodecDecoder,\n    InferenceConfig,\n    Model,\n    TiltedEncodec,\n    TrainedBPETokeniser,\n    get_cached_embedding,\n    get_cached_file,\n    get_enhancer,\n)\nfrom fam.llm.utils import (\n    check_audio_file,\n    get_default_dtype,\n    get_device,\n    normalize_text,\n)\nfrom fam.telemetry import TelemetryEvent\nfrom fam.telemetry.posthog import PosthogClient\n\nposthog = PosthogClient()  # see fam/telemetry/README.md for more information\n\n\nclass TTS:\n    END_OF_AUDIO_TOKEN = 1024\n\n    def __init__(\n        self,\n        model_name: str = \"metavoiceio/metavoice-1B-v0.1\",\n        *,\n        seed: int = 1337,\n        output_dir: str = \"outputs\",\n        quantisation_mode: Optional[Literal[\"int4\", \"int8\"]] = None,\n        first_stage_path: Optional[str] = None,\n        telemetry_origin: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialise the TTS model.\n\n        Args:\n            model_name: refers to the model identifier from the Hugging Face Model Hub (https://huggingface.co/metavoiceio)\n            seed: random seed for reproducibility\n            output_dir: directory to save output files\n            quantisation_mode: quantisation mode for first-stage LLM.\n                Options:\n                - None for no quantisation (bf16 or fp16 based on device),\n                - int4 for int4 weight-only quantisation,\n                - int8 for int8 weight-only quantisation.\n            first_stage_path: path to first-stage LLM checkpoint. If provided, this will override the one grabbed from Hugging Face via `model_name`.\n            telemetry_origin: A string identifier that specifies the origin of the telemetry data sent to PostHog.\n        \"\"\"\n\n        # NOTE: this needs to come first so that we don't change global state when we want to use\n        # the torch.compiled-model.\n        self._dtype = get_default_dtype()\n        self._device = get_device()\n        self._model_dir = snapshot_download(repo_id=model_name)\n        self.first_stage_adapter = FlattenedInterleavedEncodec2Codebook(end_of_audio_token=self.END_OF_AUDIO_TOKEN)\n        self.output_dir = output_dir\n        os.makedirs(self.output_dir, exist_ok=True)\n        if first_stage_path:\n            print(f\"Overriding first stage checkpoint via provided model: {first_stage_path}\")\n        self._first_stage_ckpt = first_stage_path or f\"{self._model_dir}/first_stage.pt\"\n\n        second_stage_ckpt_path = f\"{self._model_dir}/second_stage.pt\"\n        config_second_stage = InferenceConfig(\n            ckpt_path=second_stage_ckpt_path,\n            num_samples=1,\n            seed=seed,\n            device=self._device,\n            dtype=self._dtype,\n            compile=False,\n            init_from=\"resume\",\n            output_dir=self.output_dir,\n        )\n        data_adapter_second_stage = TiltedEncodec(end_of_audio_token=self.END_OF_AUDIO_TOKEN)\n        self.llm_second_stage = Model(\n            config_second_stage, TrainedBPETokeniser, EncodecDecoder, data_adapter_fn=data_adapter_second_stage.decode\n        )\n        self.enhancer = get_enhancer(\"df\")\n\n        self.precision = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16}[self._dtype]\n        self.model, self.tokenizer, self.smodel, self.model_size = build_model(\n            precision=self.precision,\n            checkpoint_path=Path(self._first_stage_ckpt),\n            spk_emb_ckpt_path=Path(f\"{self._model_dir}/speaker_encoder.pt\"),\n            device=self._device,\n            compile=True,\n            compile_prefill=True,\n            quantisation_mode=quantisation_mode,\n        )\n        self._seed = seed\n        self._quantisation_mode = quantisation_mode\n        self._model_name = model_name\n        self._telemetry_origin = telemetry_origin\n\n    def synthesise(self, text: str, spk_ref_path: str, top_p=0.95, guidance_scale=3.0, temperature=1.0) -> str:\n        \"\"\"\n        text: Text to speak\n        spk_ref_path: Path to speaker reference file. Min. 30s of audio required. Supports both local paths & public URIs. Audio formats: wav, flac & mp3\n        top_p: Top p for sampling applied to first-stage model. Range [0.9, 1.0] are good. This is a measure of speech stability - improves text following for a challenging speaker\n        guidance_scale: Guidance scale [1.0, 3.0] for sampling. This is a measure of speaker similarity - how closely to match speaker identity and speech style.\n        temperature: Temperature for sampling applied to both LLMs (first & second stage)\n\n        returns: path to speech .wav file\n        \"\"\"\n        text = normalize_text(text)\n        spk_ref_path = get_cached_file(spk_ref_path)\n        check_audio_file(spk_ref_path)\n        spk_emb = get_cached_embedding(\n            spk_ref_path,\n            self.smodel,\n        ).to(device=self._device, dtype=self.precision)\n\n        start = time.time()\n        # first stage LLM\n        tokens = main(\n            model=self.model,\n            tokenizer=self.tokenizer,\n            model_size=self.model_size,\n            prompt=text,\n            spk_emb=spk_emb,\n            top_p=torch.tensor(top_p, device=self._device, dtype=self.precision),\n            guidance_scale=torch.tensor(guidance_scale, device=self._device, dtype=self.precision),\n            temperature=torch.tensor(temperature, device=self._device, dtype=self.precision),\n        )\n        _, extracted_audio_ids = self.first_stage_adapter.decode([tokens])\n\n        b_speaker_embs = spk_emb.unsqueeze(0)\n\n        # second stage LLM + multi-band diffusion model\n        wav_files = self.llm_second_stage(\n            texts=[text],\n            encodec_tokens=[torch.tensor(extracted_audio_ids, dtype=torch.int32, device=self._device).unsqueeze(0)],\n            speaker_embs=b_speaker_embs,\n            batch_size=1,\n            guidance_scale=None,\n            top_p=None,\n            top_k=200,\n            temperature=1.0,\n            max_new_tokens=None,\n        )\n\n        # enhance using deepfilternet\n        wav_file = wav_files[0]\n        with tempfile.NamedTemporaryFile(suffix=\".wav\") as enhanced_tmp:\n            self.enhancer(str(wav_file) + \".wav\", enhanced_tmp.name)\n            shutil.copy2(enhanced_tmp.name, str(wav_file) + \".wav\")\n            print(f\"\\nSaved audio to {wav_file}.wav\")\n\n        # calculating real-time factor (RTF)\n        time_to_synth_s = time.time() - start\n        audio, sr = librosa.load(str(wav_file) + \".wav\")\n        duration_s = librosa.get_duration(y=audio, sr=sr)\n        real_time_factor = time_to_synth_s / duration_s\n        print(f\"\\nTotal time to synth (s): {time_to_synth_s}\")\n        print(f\"Real-time factor: {real_time_factor:.2f}\")\n\n        posthog.capture(\n            TelemetryEvent(\n                name=\"user_ran_tts\",\n                properties={\n                    \"model_name\": self._model_name,\n                    \"text\": text,\n                    \"temperature\": temperature,\n                    \"guidance_scale\": guidance_scale,\n                    \"top_p\": top_p,\n                    \"spk_ref_path\": spk_ref_path,\n                    \"speech_duration_s\": duration_s,\n                    \"time_to_synth_s\": time_to_synth_s,\n                    \"real_time_factor\": round(real_time_factor, 2),\n                    \"quantisation_mode\": self._quantisation_mode,\n                    \"seed\": self._seed,\n                    \"first_stage_ckpt\": self._first_stage_ckpt,\n                    \"gpu\": torch.cuda.get_device_name(0),\n                    \"telemetry_origin\": self._telemetry_origin,\n                },\n            )\n        )\n\n        return str(wav_file) + \".wav\"\n\n\nif __name__ == \"__main__\":\n    tts = tyro.cli(TTS)\n"}
{"type": "source_file", "path": "fam/llm/layers/combined.py", "content": "import torch.nn as nn\n\nfrom fam.llm.layers.attn import SelfAttention\nfrom fam.llm.layers.layers import MLP, LayerNorm, RMSNorm\n\n\nclass Block(nn.Module):\n    \"\"\"\n    Block class represents a single block in the model.\n\n    Args:\n        config (object): Configuration object containing parameters for the block.\n\n    Attributes:\n        ln_1 (object): Layer normalization for the attention layer.\n        ln_2 (object): Layer normalization for the feed-forward layer.\n        attn (object): Self-attention layer.\n        mlp (object): Multi-layer perceptron layer.\n\n    Methods:\n        forward(x): Performs forward pass through the block.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        if config.norm_type == \"rmsnorm\":\n            if config.rmsnorm_eps is None:\n                raise Exception(\"RMSNorm requires rmsnorm_eps to be set\")\n            self.ln_1 = RMSNorm(config.n_embd, eps=config.rmsnorm_eps)  # attn norm\n            self.ln_2 = RMSNorm(config.n_embd, eps=config.rmsnorm_eps)  # ffn norm\n        elif config.norm_type == \"layernorm\":\n            self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)  # attn norm\n            self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)  # ffn norm\n        else:\n            raise Exception(f\"Unknown norm type: {config.norm_type}\")\n        self.attn = SelfAttention(config)\n\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        \"\"\"\n        Performs forward pass through the block.\n\n        Args:\n            x (tensor): Input tensor.\n\n        Returns:\n            tensor: Output tensor after passing through the block.\n        \"\"\"\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n"}
{"type": "source_file", "path": "fam/quantiser/text/__init__.py", "content": "\n"}
{"type": "source_file", "path": "fam/quantiser/audio/__init__.py", "content": ""}
{"type": "source_file", "path": "fam/quantiser/audio/speaker_encoder/audio.py", "content": "import librosa\nimport numpy as np\n\nmel_window_length = 25\nmel_window_step = 10\nmel_n_channels = 40\nsampling_rate = 16000\n\n\ndef wav_to_mel_spectrogram(wav):\n    \"\"\"\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    \"\"\"\n    frames = librosa.feature.melspectrogram(\n        y=wav,\n        sr=sampling_rate,\n        n_fft=int(sampling_rate * mel_window_length / 1000),\n        hop_length=int(sampling_rate * mel_window_step / 1000),\n        n_mels=mel_n_channels,\n    )\n    return frames.astype(np.float32).T\n"}
{"type": "source_file", "path": "fam/quantiser/text/tokenise.py", "content": "import tiktoken\n\n\nclass TrainedBPETokeniser:\n    def __init__(self, name, pat_str, mergeable_ranks, special_tokens, offset=None) -> None:\n        self.tokenizer = tiktoken.Encoding(\n            name=name,\n            pat_str=pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=special_tokens,\n        )\n        self.offset = offset\n\n    def encode(self, text: str) -> list[int]:\n        # note: we add a end of text token!\n        tokens = self.tokenizer.encode(text) + [self.tokenizer.eot_token]\n        if self.offset is not None:\n            tokens = [x + self.offset for x in tokens]\n\n        return tokens\n\n    def decode(self, tokens: list[int]):\n        if self.offset is not None:\n            tokens = [x - self.offset for x in tokens]\n        return self.tokenizer.decode(tokens)\n\n    @property\n    def eot_token(self):\n        if self.offset is not None:\n            return self.tokenizer.eot_token + self.offset\n        else:\n            return self.tokenizer.eot_token\n"}
{"type": "source_file", "path": "fam/telemetry/posthog.py", "content": "import logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom posthog import Posthog\n\nfrom fam.telemetry import TelemetryClient, TelemetryEvent\n\nload_dotenv()\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout), logging.StreamHandler(sys.stderr)])\n\n\nclass PosthogClient(TelemetryClient):\n    def __init__(self):\n        self._posthog = Posthog(\n            project_api_key=\"phc_tk7IUlV7Q7lEa9LNbXxyC1sMWlCqiW6DkHyhJrbWMCS\", host=\"https://eu.posthog.com\"\n        )\n\n        if not bool(os.getenv(\"ANONYMIZED_TELEMETRY\", True)) or \"pytest\" in sys.modules:\n            self._posthog.disabled = True\n            logger.info(\"Anonymized telemetry disabled. See fam/telemetry/README.md for more information.\")\n        else:\n            logger.info(\"Anonymized telemetry enabled. See fam/telemetry/README.md for more information.\")\n\n        posthog_logger = logging.getLogger(\"posthog\")\n        posthog_logger.disabled = True  # Silence posthog's logging\n\n        super().__init__()\n\n    def capture(self, event: TelemetryEvent) -> None:\n        try:\n            self._posthog.capture(\n                self.user_id,\n                event.name,\n                {**event.properties},\n            )\n        except Exception as e:\n            logger.error(f\"Failed to send telemetry event {event.name}: {e}\")\n"}
{"type": "source_file", "path": "fam/telemetry/__init__.py", "content": "import abc\nimport os\nimport uuid\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n\n@dataclass(frozen=True)\nclass TelemetryEvent:\n    name: str\n    properties: dict\n\n\nclass TelemetryClient(abc.ABC):\n    USER_ID_PATH = str(Path.home() / \".cache\" / \"metavoice\" / \"telemetry_user_id\")\n    UNKNOWN_USER_ID = \"UNKNOWN\"\n    _curr_user_id = None\n\n    @abstractmethod\n    def capture(self, event: TelemetryEvent) -> None:\n        pass\n\n    @property\n    def user_id(self) -> str:\n        if self._curr_user_id:\n            return self._curr_user_id\n\n        # File access may fail due to permissions or other reasons. We don't want to\n        # crash so we catch all exceptions.\n        try:\n            if not os.path.exists(self.USER_ID_PATH):\n                os.makedirs(os.path.dirname(self.USER_ID_PATH), exist_ok=True)\n                with open(self.USER_ID_PATH, \"w\") as f:\n                    new_user_id = str(uuid.uuid4())\n                    f.write(new_user_id)\n                self._curr_user_id = new_user_id\n            else:\n                with open(self.USER_ID_PATH, \"r\") as f:\n                    self._curr_user_id = f.read()\n        except Exception:\n            self._curr_user_id = self.UNKNOWN_USER_ID\n        return self._curr_user_id\n"}
{"type": "source_file", "path": "fam/quantiser/audio/speaker_encoder/model.py", "content": "import os\nfrom time import perf_counter as timer\nfrom typing import List, Optional, Union\n\nimport librosa\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom fam.quantiser.audio.speaker_encoder import audio\n\nmel_window_step = 10\nmel_n_channels = 40\nsampling_rate = 16000\npartials_n_frames = 160\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3\n\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(\n        self,\n        weights_fpath: Optional[str] = None,\n        device: Optional[Union[str, torch.device]] = None,\n        verbose: bool = True,\n        eval: bool = False,\n    ):\n        super().__init__()\n\n        # Define the network\n        self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n        self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n        self.relu = nn.ReLU()\n\n        # Get the target device\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        elif isinstance(device, str):\n            device = torch.device(device)\n        self.device = device\n\n        start = timer()\n\n        checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n        self.load_state_dict(checkpoint[\"model_state\"], strict=False)\n        self.to(device)\n\n        if eval:\n            self.eval()\n\n        if verbose:\n            print(\"Loaded the speaker embedding model on %s in %.2f seconds.\" % (device.type, timer() - start))\n\n    def forward(self, mels: torch.FloatTensor):\n        _, (hidden, _) = self.lstm(mels)\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        return embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n\n    @staticmethod\n    def compute_partial_slices(n_samples: int, rate, min_coverage):\n        # Compute how many frames separate two partial utterances\n        samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n        n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n        frame_step = int(np.round((sampling_rate / rate) / samples_per_frame))\n\n        # Compute the slices\n        wav_slices, mel_slices = [], []\n        steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n        for i in range(0, steps, frame_step):\n            mel_range = np.array([i, i + partials_n_frames])\n            wav_range = mel_range * samples_per_frame\n            mel_slices.append(slice(*mel_range))\n            wav_slices.append(slice(*wav_range))\n\n        # Evaluate whether extra padding is warranted or not\n        last_wav_range = wav_slices[-1]\n        coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n        if coverage < min_coverage and len(mel_slices) > 1:\n            mel_slices = mel_slices[:-1]\n            wav_slices = wav_slices[:-1]\n\n        return wav_slices, mel_slices\n\n    def embed_utterance(self, wav: np.ndarray, return_partials=False, rate=1.3, min_coverage=0.75, numpy: bool = True):\n        wav_slices, mel_slices = self.compute_partial_slices(len(wav), rate, min_coverage)\n        max_wave_length = wav_slices[-1].stop\n        if max_wave_length >= len(wav):\n            wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n\n        mel = audio.wav_to_mel_spectrogram(wav)\n        mels = np.array([mel[s] for s in mel_slices])\n        mels = torch.from_numpy(mels).to(self.device)  # type: ignore\n        with torch.no_grad():\n            partial_embeds = self(mels)\n\n        if numpy:\n            raw_embed = np.mean(partial_embeds.cpu().numpy(), axis=0)\n            embed = raw_embed / np.linalg.norm(raw_embed, 2)\n        else:\n            raw_embed = partial_embeds.mean(dim=0)\n            embed = raw_embed / torch.linalg.norm(raw_embed, 2)\n\n        if return_partials:\n            return embed, partial_embeds, wav_slices\n        return embed\n\n    def embed_speaker(self, wavs: List[np.ndarray], **kwargs):\n        raw_embed = np.mean([self.embed_utterance(wav, return_partials=False, **kwargs) for wav in wavs], axis=0)\n        return raw_embed / np.linalg.norm(raw_embed, 2)\n\n    def embed_utterance_from_file(self, fpath: str, numpy: bool) -> torch.Tensor:\n        wav_tgt, _ = librosa.load(fpath, sr=sampling_rate)\n        wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n\n        embedding = self.embed_utterance(wav_tgt, numpy=numpy)\n        return embedding\n"}
{"type": "source_file", "path": "fam/quantiser/audio/speaker_encoder/__init__.py", "content": ""}
{"type": "source_file", "path": "serving.py", "content": "# curl -X POST http://127.0.0.1:58003/tts -F \"text=Testing this inference server.\" -F \"speaker_ref_path=https://cdn.themetavoice.xyz/speakers/bria.mp3\" -F \"guidance=3.0\" -F \"top_p=0.95\" --output out.wav\n\nimport logging\nimport shlex\nimport subprocess\nimport tempfile\nimport warnings\nfrom pathlib import Path\nfrom typing import Literal, Optional\n\nimport fastapi\nimport fastapi.middleware.cors\nimport tyro\nimport uvicorn\nfrom attr import dataclass\nfrom fastapi import File, Form, HTTPException, UploadFile, status\nfrom fastapi.responses import Response\n\nfrom fam.llm.fast_inference import TTS\nfrom fam.llm.utils import check_audio_file\n\nlogger = logging.getLogger(__name__)\n\n\n## Setup FastAPI server.\napp = fastapi.FastAPI()\n\n\n@dataclass\nclass ServingConfig:\n    huggingface_repo_id: str = \"metavoiceio/metavoice-1B-v0.1\"\n    \"\"\"Absolute path to the model directory.\"\"\"\n\n    temperature: float = 1.0\n    \"\"\"Temperature for sampling applied to both models.\"\"\"\n\n    seed: int = 1337\n    \"\"\"Random seed for sampling.\"\"\"\n\n    port: int = 58003\n\n    quantisation_mode: Optional[Literal[\"int4\", \"int8\"]] = None\n\n\n# Singleton\nclass _GlobalState:\n    config: ServingConfig\n    tts: TTS\n\n\nGlobalState = _GlobalState()\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\"}\n\n\n@app.post(\"/tts\", response_class=Response)\nasync def text_to_speech(\n    text: str = Form(..., description=\"Text to convert to speech.\"),\n    speaker_ref_path: Optional[str] = Form(None, description=\"Optional URL to an audio file of a reference speaker. Provide either this URL or audio data through 'audiodata'.\"),\n    audiodata: Optional[UploadFile] = File(None, description=\"Optional audio data of a reference speaker. Provide either this file or a URL through 'speaker_ref_path'.\"),\n    guidance: float = Form(3.0, description=\"Control speaker similarity - how closely to match speaker identity and speech style, range: 0.0 to 5.0.\", ge=0.0, le=5.0),\n    top_p: float = Form(0.95, description=\"Controls speech stability - improves text following for a challenging speaker, range: 0.0 to 1.0.\", ge=0.0, le=1.0),\n):\n    # Ensure at least one of speaker_ref_path or audiodata is provided\n    if not audiodata and not speaker_ref_path:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Either an audio file or a speaker reference path must be provided.\",\n        )\n\n    wav_out_path = None\n\n    try:\n        with tempfile.NamedTemporaryFile(suffix=\".wav\") as wav_tmp:\n            if speaker_ref_path is None:\n                wav_path = _convert_audiodata_to_wav_path(audiodata, wav_tmp)\n                check_audio_file(wav_path)\n            else:\n                # TODO: fix\n                wav_path = speaker_ref_path\n\n            if wav_path is None:\n                warnings.warn(\"Running without speaker reference\")\n                assert guidance is None\n\n            wav_out_path = GlobalState.tts.synthesise(\n                text=text,\n                spk_ref_path=wav_path,\n                top_p=top_p,\n                guidance_scale=guidance,\n            )\n\n        with open(wav_out_path, \"rb\") as f:\n            return Response(content=f.read(), media_type=\"audio/wav\")\n    except Exception as e:\n        # traceback_str = \"\".join(traceback.format_tb(e.__traceback__))\n        logger.exception(\n            f\"Error processing request. text: {text}, speaker_ref_path: {speaker_ref_path}, guidance: {guidance}, top_p: {top_p}\"\n        )\n        return Response(\n            content=\"Something went wrong. Please try again in a few mins or contact us on Discord\",\n            status_code=500,\n        )\n    finally:\n        if wav_out_path is not None:\n            Path(wav_out_path).unlink(missing_ok=True)\n\n\ndef _convert_audiodata_to_wav_path(audiodata: UploadFile, wav_tmp):\n    with tempfile.NamedTemporaryFile() as unknown_format_tmp:\n        if unknown_format_tmp.write(audiodata.read()) == 0:\n            return None\n        unknown_format_tmp.flush()\n\n        subprocess.check_output(\n            # arbitrary 2 minute cutoff\n            shlex.split(f\"ffmpeg -t 120 -y -i {unknown_format_tmp.name} -f wav {wav_tmp.name}\")\n        )\n\n        return wav_tmp.name\n\n\nif __name__ == \"__main__\":\n    for name in logging.root.manager.loggerDict:\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.INFO)\n    logging.root.setLevel(logging.INFO)\n\n    GlobalState.config = tyro.cli(ServingConfig)\n    GlobalState.tts = TTS(\n        seed=GlobalState.config.seed,\n        quantisation_mode=GlobalState.config.quantisation_mode,\n        telemetry_origin=\"api_server\",\n    )\n\n    app.add_middleware(\n        fastapi.middleware.cors.CORSMiddleware,\n        allow_origins=[\"*\", f\"http://localhost:{GlobalState.config.port}\", \"http://localhost:3000\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=GlobalState.config.port,\n        log_level=\"info\",\n    )\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import find_packages, setup  # type: ignore\n\nsetup(\n    name=\"fam\",\n    packages=find_packages(\".\", exclude=[\"tests\"]),\n)\n"}
{"type": "source_file", "path": "fam/quantiser/__init__.py", "content": ""}
