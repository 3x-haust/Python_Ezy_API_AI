{"repo_info": {"repo_name": "OpenManus", "repo_owner": "henryalps", "repo_url": "https://github.com/henryalps/OpenManus"}}
{"type": "test_file", "path": "tests/integration/test_workflow.py", "content": "import pytest\nfrom src.workflow.graph import build_graph\n\ndef test_workflow_initialization():\n    \"\"\"Test that the LangGraph workflow can be initialized.\"\"\"\n    graph = build_graph()\n    assert graph is not None"}
{"type": "source_file", "path": "src/agents/nodes/planner_node.py", "content": "import logging\nimport json\nimport json_repair\nfrom copy import deepcopy\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\nfrom src.prompts.template import OpenManusPromptTemplate\nfrom src.tools.search import bing_tool\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State # Import State type\n\nlogger = logging.getLogger(__name__)\n\ndef planner_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Planner node that generate the full plan.\"\"\"\n    logger.info(\"Planner generating full plan\")\n    messages = OpenManusPromptTemplate.apply_prompt_template(\"planner\", state)\n    # whether to enable deep thinking mode\n    llm = get_llm_by_type(AGENT_LLM_MAP[\"planner\"]) # Changed to planner LLM\n    if state.get(\"deep_thinking_mode\"):\n        llm = get_llm_by_type(\"reasoning\")\n    if state.get(\"search_before_planning\"):\n        searched_content = bing_tool.invoke({\"query\": state[\"messages\"][-1].content})\n        messages = deepcopy(messages)\n        messages[\n            -1\n        ].content += \"\\\\n\\\\n# Relative Search Results\\\\n\\\\n\" + json.dumps([{'title': elem['title'], 'content': elem['content']} for elem in searched_content], ensure_ascii=False)\n    stream = llm.stream(messages)\n    full_response = \"\"\n    for chunk in stream:\n        full_response += chunk.content\n    logger.debug(f\"Current state messages: {state['messages']}\")\n    logger.debug(f\"Planner response: {full_response}\")\n\n    if full_response.startswith(\"```json\"):\n        full_response = full_response.removeprefix(\"```json\")\n\n    if full_response.endswith(\"```\"):\n        full_response = full_response.removesuffix(\"```\")\n\n    goto = \"supervisor\"\n    try:\n        repaired_response = json_repair.loads(full_response)\n        full_response = json.dumps(repaired_response)\n    except json.JSONDecodeError:\n        logger.warning(\"Planner response is not a valid JSON\")\n        goto = \"__end__\"\n\n    return Command(\n        update={\n            \"messages\": [HumanMessage(content=full_response, name=\"planner\")],\n            \"full_plan\": full_response,\n        },\n        goto=goto,\n    )"}
{"type": "source_file", "path": "src/agents/coordinator.py", "content": "class TaskCoordinator:\n    \"\"\"Coordinates tasks between multiple agents and tools.\"\"\"\n\n    def __init__(self):\n        self.agents = {}  # Will store initialized agents\n        self.tools = {}   # Will store available tools\n        self._initialize_system()\n\n    def _initialize_system(self):\n        \"\"\"Initialize the multi-agent system and tools.\"\"\"\n        self.agents['planner'] = PlannerAgent()\n        self.agents['executor'] = ExecutionAgent()\n        self.agents['tool'] = ToolAgent() # Generic tool agent for now\n        self.tools = self._initialize_tools()\n\n    def _initialize_tools(self):\n        \"\"\"Initialize and return available tools.\"\"\"\n        from src.tools.web_browser import WebBrowserTool\n        from src.tools.code_executor import CodeExecutorTool\n        from src.tools.data_retriever import DataRetrieverTool\n        return {\n            'web_browser': WebBrowserTool(),\n            'code_executor': CodeExecutorTool(),\n            'data_retriever': DataRetrieverTool(),\n        }\n\n    def execute_task(self, task_description):\n        \"\"\"\n        Execute a task using the multi-agent system.\n\n        Args:\n            task_description (str): Natural language description of the task\n\n        Returns:\n            dict: Result of the task execution\n        \"\"\"\n        plan = self.agents['planner'].plan_task(task_description)\n        result = self.agents['executor'].execute_plan(plan, self.agents, self.tools)\n        return {\n            \"status\": \"success\",\n            \"result\": result\n        }\n\n\nclass PlannerAgent:\n    \"\"\"Agent responsible for planning tasks.\"\"\"\n    def plan_task(self, task_description):\n        \"\"\"Generates a task execution plan.\"\"\"\n        # Placeholder plan: Use web_browser tool\n        return {\n            \"steps\": [\n                {\"agent\": \"tool\", \"action\": \"use_tool\", \"tool_name\": \"web_browser\", \"tool_args\": {\"url\": \"https://www.example.com\"}}\n            ]\n        }\n\nclass ExecutionAgent:\n    \"\"\"Agent responsible for executing task plans.\"\"\"\n    def execute_plan(self, plan, agents, tools):\n        \"\"\"Executes a given task plan.\"\"\"\n        results = []\n        for step in plan['steps']:\n            agent_name = step['agent']\n            action = step['action']\n            if agent_name == 'tool' and action == 'use_tool':\n                tool_name = step['tool_name']\n                tool_args = step['tool_args']\n                tool_result = agents['tool'].use_tool(tool_name, tool_args, tools)\n                results.append(f\"Tool '{tool_name}' used with args {tool_args}. Result: {tool_result}\")\n            else:\n                results.append(f\"Unknown step: {step}\")\n        return \"\\\\n\".join(results)\n\nclass ToolAgent:\n    \"\"\"Agent responsible for using tools.\"\"\"\n    def use_tool(self, tool_name, tool_args, tools):\n        \"\"\"Uses a specific tool to perform an action.\"\"\"\n        if tool_name in tools:\n            tool = tools[tool_name]\n            if tool_name == 'web_browser':\n                return tool.browse_web(**tool_args)\n            elif tool_name == 'code_executor':\n                return tool.execute_code(**tool_args)\n            elif tool_name == 'data_retriever':\n                return tool.retrieve_data(**tool_args)\n            else:\n                return f\"Tool '{tool_name}' not yet fully implemented.\"\n        else:\n            return f\"Tool '{tool_name}' not found.\""}
{"type": "source_file", "path": "src/agents/nodes/coordinator_node.py", "content": "import logging\nimport json_repair\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\nfrom src.prompts.template import OpenManusPromptTemplate\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State, Router # Import State and Router types\n\nlogger = logging.getLogger(__name__)\n\ndef coordinator_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Coordinator node that communicate with customers.\"\"\"\n    logger.info(\"Coordinator talking.\")\n    messages = OpenManusPromptTemplate.apply_prompt_template(\"coordinator\", state)\n    response = get_llm_by_type(AGENT_LLM_MAP[\"coordinator\"]).invoke(messages)\n    logger.debug(f\"Current state messages: {state['messages']}\")\n    response_content = response.content\n    # Attempt to repair potential JSON output\n    response_content = repair_json_output(response_content)\n    logger.debug(f\"Coordinator response: {response_content}\")\n\n    goto = \"__end__\"\n    if \"handoff_to_planner\" in response_content:\n        goto = \"planner\"\n\n    # Update response.content with repaired content\n    response.content = response_content\n\n    return Command(\n        goto=goto,\n    )"}
{"type": "source_file", "path": "src/agents/nodes/reporter_node.py", "content": "import logging\nimport json_repair\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.types import Command\n\nfrom src.agents import reporter_agent # Import reporter agent\nfrom src.utils.json_utils import repair_json_output\nfrom src.prompts.template import OpenManusPromptTemplate\nfrom .types import State # Import State type\n\nlogger = logging.getLogger(__name__)\n\ndef reporter_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Reporter node that write a final report.\"\"\"\n    logger.info(\"Reporter writing final report\")\n    messages = OpenManusPromptTemplate.apply_prompt_template(\"reporter\", state)\n    response = reporter_agent.invoke(state)\n    logger.debug(f\"Current state messages: {state['messages']}\")\n    response_content = response.content\n    response_content = repair_json_output(response_content)\n    logger.debug(f\"Reporter agent response: {response_content}\")\n\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(\n                    content=response_content,\n                    name=\"reporter\",\n                )\n            ]\n        },\n        goto=\"supervisor\", # Go back to supervisor to decide next step\n    )"}
{"type": "source_file", "path": "src/agents/research_agent.py", "content": "from typing import List\nfrom langchain_core.messages import BaseMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\n\nclass ResearchAgent:\n    \"\"\"Research agent that handles research and information gathering tasks.\"\"\"\n    \n    def invoke(self, messages: List[BaseMessage]) -> BaseMessage:\n        \"\"\"Process the messages and return a response.\"\"\"\n        llm = get_llm_by_type(AGENT_LLM_MAP[\"researcher\"])\n        return llm.invoke(messages)\n\nresearch_agent = ResearchAgent()"}
{"type": "source_file", "path": "src/agents/nodes/supervisor_node.py", "content": "import logging\nimport json_repair\nfrom copy import deepcopy\nfrom typing import Literal, Dict, Any\n\nfrom langchain_core.messages import HumanMessage, BaseMessage\nfrom langgraph.types import Command\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config import TEAM_MEMBERS\nfrom src.config.agents import AGENT_LLM_MAP\nfrom src.prompts.template import OpenManusPromptTemplate\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State, Router # Import State and Router types\n\nlogger = logging.getLogger(__name__)\n\nRESPONSE_FORMAT = \"Response from {}:\\n\\n<response>\\n{}\\n</response>\\n\\n*Please execute the next step.*\"\n\ndef supervisor_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Supervisor node that decides which agent should act next.\"\"\"\n    logger.info(\"Supervisor evaluating next action\")\n    messages = OpenManusPromptTemplate.apply_prompt_template(\"supervisor\", state)\n    # preprocess messages to make supervisor execute better.\n    messages = deepcopy(messages)\n    for message in messages:\n        if isinstance(message, BaseMessage) and message.name in TEAM_MEMBERS:\n            message.content = RESPONSE_FORMAT.format(message.name, message.content)\n    response = (\n        get_llm_by_type(AGENT_LLM_MAP[\"supervisor\"])\n        .with_structured_output(schema=Router, method=\"json_mode\")\n        .invoke(messages)\n    )\n    goto = response[\"next\"]\n    logger.debug(f\"Current state messages: {state['messages']}\")\n    logger.debug(f\"Supervisor response: {response}\")\n\n    if goto == \"FINISH\":\n        goto = \"__end__\"\n        logger.info(\"Workflow completed\")\n    else:\n        logger.info(f\"Supervisor delegating to: {goto}\")\n\n    return Command(goto=goto, update={\"next\": goto})"}
{"type": "source_file", "path": "src/agents/nodes/researcher_node.py", "content": "import logging\nimport json_repair\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.types import Command\n\nfrom src.agents import research_agent # Import researcher agent\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State # Import State type\n\nlogger = logging.getLogger(__name__)\n\ndef researcher_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Node for the researcher agent that performs research tasks.\"\"\"\n    logger.info(\"Research agent starting task\")\n    result = research_agent.invoke(state)\n    logger.info(\"Research agent completed task\")\n    response_content = result[\"messages\"][-1].content\n    response_content = repair_json_output(response_content)\n    logger.debug(f\"Research agent response: {response_content}\")\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(\n                    content=response_content,\n                    name=\"researcher\",\n                )\n            ]\n        },\n        goto=\"supervisor\",\n    )"}
{"type": "source_file", "path": "src/agents/browser_agent.py", "content": "from typing import List\nfrom langchain_core.messages import BaseMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\n\nclass BrowserAgent:\n    \"\"\"Browser agent that handles web browsing tasks.\"\"\"\n    \n    def invoke(self, messages: List[BaseMessage]) -> BaseMessage:\n        \"\"\"Process the messages and return a response.\"\"\"\n        llm = get_llm_by_type(AGENT_LLM_MAP[\"browser\"])\n        return llm.invoke(messages)\n\nbrowser_agent = BrowserAgent()"}
{"type": "source_file", "path": "src/llms/__init__.py", "content": "from .llm import PlaceholderLLM, get_llm_by_type\n\n__all__ = ['PlaceholderLLM', 'get_llm_by_type']"}
{"type": "source_file", "path": "src/agents/coder_agent.py", "content": "from typing import List\nfrom langchain_core.messages import BaseMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\n\nclass CoderAgent:\n    \"\"\"Coder agent that handles code-related tasks.\"\"\"\n    \n    def invoke(self, messages: List[BaseMessage]) -> BaseMessage:\n        \"\"\"Process the messages and return a response.\"\"\"\n        llm = get_llm_by_type(AGENT_LLM_MAP[\"coder\"])\n        return llm.invoke(messages)\n\ncoder_agent = CoderAgent()"}
{"type": "source_file", "path": "src/config/agents.py", "content": "from typing import Literal\n\n# Define available LLM types\nLLMType = Literal[\"basic\", \"reasoning\", \"vision\"]\n\n# Define agent-LLM mapping\nAGENT_LLM_MAP: dict[str, LLMType] = {\n    \"coordinator\": \"basic\",  # 协调默认使用basic llm\n    \"planner\": \"reasoning\",  # 计划默认使用basic llm\n    \"supervisor\": \"basic\",  # 决策使用basic llm\n    \"researcher\": \"basic\",  # 简单搜索任务使用basic llm\n    \"coder\": \"basic\",  # 编程任务使用basic llm\n    \"browser\": \"vision\",  # 浏览器操作使用vision llm\n    \"reporter\": \"basic\",  # 编写报告使用basic llm\n}"}
{"type": "source_file", "path": "src/utils/json_utils.py", "content": "\"\"\"Utility functions for JSON handling in OpenManus.\"\"\"\n\nimport json\nimport json_repair\n\ndef repair_json_output(json_str: str) -> str:\n    \"\"\"Repair malformed JSON strings.\n    \n    Args:\n        json_str: The potentially malformed JSON string to repair.\n        \n    Returns:\n        str: A valid JSON string.\n    \"\"\"\n    try:\n        # First try to parse as-is\n        json.loads(json_str)\n        return json_str\n    except json.JSONDecodeError:\n        # If parsing fails, attempt repair\n        try:\n            return json_repair.repair_json(json_str)\n        except Exception as e:\n            # If repair fails, return original string\n            return json_str"}
{"type": "source_file", "path": "src/agents/reporter_agent.py", "content": "from typing import List\nfrom langchain_core.messages import BaseMessage\n\nfrom src.llms.llm import get_llm_by_type\nfrom src.config.agents import AGENT_LLM_MAP\n\nclass ReporterAgent:\n    \"\"\"Reporter agent that handles report generation tasks.\"\"\"\n    \n    def invoke(self, messages: List[BaseMessage]) -> BaseMessage:\n        \"\"\"Process the messages and return a response.\"\"\"\n        llm = get_llm_by_type(AGENT_LLM_MAP[\"reporter\"])\n        return llm.invoke(messages)\n\nreporter_agent = ReporterAgent()"}
{"type": "source_file", "path": "src/agents/nodes/coder_node.py", "content": "import logging\nimport json_repair\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.types import Command\n\nfrom src.agents import coder_agent # Import coder agent\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State # Import State type\n\nlogger = logging.getLogger(__name__)\n\ndef coder_node(state: State) -> Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Node for the coder agent that executes Python code.\"\"\"\n    logger.info(\"Code agent starting task\")\n    result = coder_agent.invoke(state)\n    logger.info(\"Code agent completed task\")\n    response_content = result[\"messages\"][-1].content\n    response_content = repair_json_output(response_content)\n    logger.debug(f\"Code agent response: {response_content}\")\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(\n                    content=response_content,\n                    name=\"coder\",\n                )\n            ]\n        },\n        goto=\"supervisor\",\n    )"}
{"type": "source_file", "path": "src/tools/bash_tool.py", "content": "import logging\nimport subprocess\nfrom typing import Annotated\nfrom langchain_core.tools import tool\nfrom .decorators import log_io\n\n# Initialize logger\nlogger = logging.getLogger(__name__)\n\n\n@tool\n@log_io\ndef bash_tool(\n    cmd: Annotated[str, \"The bash command to be executed.\"],\n):\n    \"\"\"Use this to execute bash command and do necessary operations.\"\"\"\n    logger.info(f\"Executing Bash Command: {cmd}\")\n    try:\n        # Execute the command and capture output\n        result = subprocess.run(\n            cmd, shell=True, check=True, text=True, capture_output=True\n        )\n        # Return stdout as the result\n        return result.stdout\n    except subprocess.CalledProcessError as e:\n        # If command fails, return error information\n        error_message = f\"Command failed with exit code {e.returncode}.\\nStdout: {e.stdout}\\nStderr: {e.stderr}\"\n        logger.error(error_message)\n        return error_message\n    except Exception as e:\n        # Catch any other exceptions\n        error_message = f\"Error executing command: {str(e)}\"\n        logger.error(error_message)\n        return error_message\n\n\nif __name__ == \"__main__\":\n    print(bash_tool.invoke(\"ls -all\"))"}
{"type": "source_file", "path": "src/tools/file_management.py", "content": "import logging\nfrom langchain_community.tools.file_management import WriteFileTool\nfrom .decorators import create_logged_tool\n\nlogger = logging.getLogger(__name__)\n\n# Initialize file management tool with logging\nLoggedWriteFile = create_logged_tool(WriteFileTool)\nwrite_file_tool = LoggedWriteFile()"}
{"type": "source_file", "path": "src/tools/server.py", "content": "from flask import Flask, request, jsonify\nimport logging\n\napp = Flask(__name__)\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({'status': 'ok'})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5001)"}
{"type": "source_file", "path": "src/llms/llm.py", "content": "import os\nfrom typing import Literal, Type\n\nfrom src.config.agents import LLMType\nfrom src.config.env import (\n    BASIC_API_KEY,\n    BASIC_BASE_URL,\n    BASIC_MODEL,\n    REASONING_API_KEY,\n    REASONING_BASE_URL,\n    REASONING_MODEL,\n    VL_API_KEY,\n    VL_BASE_URL,\n    VL_MODEL,\n    AZURE_API_BASE,\n    AZURE_API_KEY,\n    AZURE_API_VERSION,\n    BASIC_AZURE_DEPLOYMENT,\n    VL_AZURE_DEPLOYMENT,\n    REASONING_AZURE_DEPLOYMENT\n)\n\nclass PlaceholderLLM:\n    def __init__(self, model_name):\n        self.model_name = model_name\n\n    def invoke(self, messages):\n        return f\"Placeholder LLM: {self.model_name} invoked with messages: {messages}\"\n\ndef get_llm_by_type(llm_type: LLMType):\n    if llm_type == \"reasoning\":\n        model_name = REASONING_MODEL\n        base_url = REASONING_BASE_URL\n        api_key = REASONING_API_KEY\n        azure_deployment = REASONING_AZURE_DEPLOYMENT\n    elif llm_type == \"vision\":\n        model_name = VL_MODEL\n        base_url = VL_BASE_URL\n        api_key = VL_API_KEY\n        azure_deployment = VL_AZURE_DEPLOYMENT\n    elif llm_type == \"basic\":\n        model_name = BASIC_MODEL\n        base_url = BASIC_BASE_URL\n        api_key = BASIC_API_KEY\n        azure_deployment = BASIC_AZURE_DEPLOYMENT\n    else:\n        raise ValueError(f\"Unknown LLM type: {llm_type}\")\n\n    return PlaceholderLLM(model_name)"}
{"type": "source_file", "path": "src/config/env.py", "content": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Reasoning LLM configuration (for complex reasoning tasks)\nREASONING_MODEL = os.getenv(\"REASONING_MODEL\", \"deepseek-r1\")\nREASONING_BASE_URL = os.getenv(\"REASONING_BASE_URL\")\nREASONING_API_KEY = os.getenv(\"REASONING_API_KEY\")\n\n# Non-reasoning LLM configuration (for straightforward tasks)\nBASIC_MODEL = os.getenv(\"BASIC_MODEL\", \"gemini-2.0-flash\")\nBASIC_BASE_URL = os.getenv(\"BASIC_BASE_URL\")\nBASIC_API_KEY = os.getenv(\"BASIC_API_KEY\")\n\n# Azure OpenAI配置（按LLM类型区分）\nAZURE_API_BASE = os.getenv(\"AZURE_API_BASE\")\nAZURE_API_KEY = os.getenv(\"AZURE_API_KEY\")\nAZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n# 各类型专用部署名称\nBASIC_AZURE_DEPLOYMENT = os.getenv(\"BASIC_AZURE_DEPLOYMENT\")\nVL_AZURE_DEPLOYMENT = os.getenv(\"VL_AZURE_DEPLOYMENT\")\nREASONING_AZURE_DEPLOYMENT = os.getenv(\"REASONING_AZURE_DEPLOYMENT\")\n\n# Vision-language LLM configuration (for tasks requiring visual understanding)\nVL_MODEL = os.getenv(\"VL_MODEL\", \"gemini-2.0-flash\")\nVL_BASE_URL = os.getenv(\"VL_BASE_URL\")\nVL_API_KEY = os.getenv(\"VL_API_KEY\")\n\n# Chrome Instance configuration\nCHROME_INSTANCE_PATH = os.getenv(\"CHROME_INSTANCE_PATH\")\nCHROME_HEADLESS = os.getenv(\"CHROME_HEADLESS\", \"False\") == \"True\"\nCHROME_PROXY_SERVER = os.getenv(\"CHROME_PROXY_SERVER\")\nCHROME_PROXY_USERNAME = os.getenv(\"CHROME_PROXY_USERNAME\")\nCHROME_PROXY_PASSWORD = os.getenv(\"CHROME_PROXY_PASSWORD\")"}
{"type": "source_file", "path": "src/tools/crawl.py", "content": "import logging\nfrom typing import Annotated\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nfrom .decorators import log_io\n\nfrom src.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\n@tool\n@log_io\ndef crawl_tool(\n    url: Annotated[str, \"The url to crawl.\"],\n) -> HumanMessage:\n    \"\"\"Use this to crawl a url and get a readable content in markdown format.\"\"\"\n    try:\n        crawler = Crawler()\n        article = crawler.crawl(url)\n        return {\"role\": \"user\", \"content\": article.to_message()}\n    except BaseException as e:\n        error_msg = f\"Failed to crawl. Error: {repr(e)}\"\n        logger.error(error_msg)\n        return error_msg"}
{"type": "source_file", "path": "src/utils/__init__.py", "content": "\"\"\"Utility functions for OpenManus.\"\"\"\n\nfrom .json_utils import repair_json_output\n\n__all__ = ['repair_json_output']"}
{"type": "source_file", "path": "src/agents/nodes/browser_node.py", "content": "import logging\nimport json_repair\nfrom typing import Literal, Dict, Any\nfrom langchain_core.messages import HumanMessage\n\nfrom src.agents import browser_agent # Import browser agent\nfrom src.utils.json_utils import repair_json_output\nfrom .types import State # Import State type\n\nlogger = logging.getLogger(__name__)\n\ndef browser_node(state: State) ->  Dict[str, Any]: # Modified return type to Dict\n    \"\"\"Node for the browser agent that performs web browsing tasks.\"\"\"\n    logger.info(\"Browser agent starting task\")\n    result = browser_agent.invoke(state)\n    logger.info(\"Browser agent completed task\")\n    response_content = result[\"messages\"][-1].content\n    response_content = repair_json_output(response_content)\n    logger.debug(f\"Browser agent response: {response_content}\")\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(\n                    content=response_content,\n                    name=\"browser\",\n                )\n            ]\n        },\n        goto=\"supervisor\",\n    )"}
{"type": "source_file", "path": "src/prompts/__init__.py", "content": "__all__ = [\n    \"apply_prompt_template\",\n    \"get_prompt_template\",\n]"}
{"type": "source_file", "path": "src/workflow/graph.py", "content": "from langgraph.graph import StateGraph, START\nfrom src.graph.types import State # Import State class\nfrom src.agents.nodes import ( # Import agent nodes\n    coordinator_node,\n    planner_node,\n    supervisor_node,\n    researcher_node,\n    coder_node,\n    browser_node,\n    reporter_node,\n)\n\ndef build_graph():\n    \"\"\"Build and return the agent workflow graph.\"\"\"\n    builder = StateGraph(State)\n\n    # Define nodes\n    builder.add_node(\"coordinator\", coordinator_node)\n    builder.add_node(\"planner\", planner_node)\n    builder.add_node(\"supervisor\", supervisor_node)\n    builder.add_node(\"researcher\", researcher_node)\n    builder.add_node(\"coder\", coder_node)\n    builder.add_node(\"browser\", browser_node)\n    builder.add_node(\"reporter\", reporter_node)\n\n    # Define edges\n    builder.add_edge(START, \"coordinator\")\n    builder.add_edge(\"coordinator\", \"planner\") # Coordinator -> Planner\n    builder.add_edge(\"planner\", \"supervisor\") # Planner -> Supervisor\n    builder.add_edge(\"supervisor\", \"researcher\", condition=lambda state: state['next'] == \"researcher\") # Supervisor -> Researcher if next agent is researcher\n    builder.add_edge(\"supervisor\", \"coder\", condition=lambda state: state['next'] == \"coder\") # Supervisor -> Coder if next agent is coder\n    builder.add_edge(\"supervisor\", \"browser\", condition=lambda state: state['next'] == \"browser\") # Supervisor -> Browser if next agent is browser\n    builder.add_edge(\"supervisor\", \"reporter\", condition=lambda state: state['next'] == \"reporter\") # Supervisor -> Reporter if next agent is reporter\n    builder.add_edge(\"supervisor\", \"__end__\", condition=lambda state: state['next'] == \"__end__\") # Supervisor -> END if next agent is FINISH\n    builder.add_edge(\"researcher\", \"supervisor\") # Researcher -> Supervisor\n    builder.add_edge(\"coder\", \"supervisor\") # Coder -> Supervisor\n    builder.add_edge(\"browser\", \"supervisor\") # Browser -> Supervisor\n    builder.add_edge(\"reporter\", \"supervisor\") # Reporter -> Supervisor\n\n    builder.set_entry_point(\"coordinator\")\n    builder.set_conditional_edge(\"supervisor\", supervisor_node) # Conditional edge for supervisor node\n    builder.add_end_point(\"__end__\")\n\n    return builder.compile()"}
{"type": "source_file", "path": "src/agents/__init__.py", "content": "# Initialize agents package\n\nfrom .browser_agent import browser_agent\nfrom .coder_agent import coder_agent\nfrom .research_agent import research_agent\nfrom .reporter_agent import reporter_agent\n\n__all__ = [\n    'browser_agent',\n    'coder_agent',\n    'research_agent',\n    'reporter_agent',\n]"}
{"type": "source_file", "path": "src/service/__init__.py", "content": "\"\"\"Service package for OpenManus workflow management.\"\"\""}
{"type": "source_file", "path": "src/config/tools.py", "content": "# Tool configuration\nBROWSER_HISTORY_DIR = \"static/browser_history\""}
{"type": "source_file", "path": "src/agents/nodes/types.py", "content": "from typing import TypedDict, Literal, List, Dict, Any\nfrom langchain_core.messages import BaseMessage\n\nclass State(TypedDict):\n    \"\"\"Type definition for the workflow state.\"\"\"\n    messages: List[BaseMessage]\n    full_plan: str\n    next: str\n    deep_thinking_mode: bool\n    search_before_planning: bool\n\nclass Router(TypedDict):\n    \"\"\"Type definition for the supervisor's routing decision.\"\"\"\n    next: Literal['coordinator', 'planner', 'supervisor', 'researcher', 'coder', 'browser', 'reporter', 'FINISH']"}
{"type": "source_file", "path": "src/tools/python_repl.py", "content": "import logging\nfrom typing import Annotated\nfrom langchain_core.tools import tool\nfrom langchain_experimental.utilities import PythonREPL\nfrom .decorators import log_io\n\n# Initialize REPL and logger\nrepl = PythonREPL()\nlogger = logging.getLogger(__name__)\n\n\n@tool\n@log_io\ndef python_repl_tool(\n    code: Annotated[\n        str, \"The python code to execute to do further analysis or calculation.\"\n    ],\n):\n    \"\"\"Use this to execute python code and do data analysis or calculation. If you want to see the output of a value,\n    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n    logger.info(\"Executing Python code\")\n    try:\n        result = repl.run(code)\n        logger.info(\"Code execution successful\")\n    except BaseException as e:\n        error_msg = f\"Failed to execute. Error: {repr(e)}\"\n        logger.error(error_msg)\n        return error_msg\n    result_str = f\"Successfully executed:\\\\n```python\\\\n{code}\\\\n```\\\\nStdout: {result}\"\n    return result_str"}
{"type": "source_file", "path": "src/config/__init__.py", "content": "from .env import (\n    # AZURE Config\n    AZURE_API_BASE,\n    AZURE_API_KEY,\n    AZURE_API_VERSION,\n    # Reasoning LLM\n    REASONING_MODEL,\n    REASONING_BASE_URL,\n    REASONING_API_KEY,\n    REASONING_AZURE_DEPLOYMENT,\n    # Basic LLM\n    BASIC_MODEL,\n    BASIC_BASE_URL,\n    BASIC_API_KEY,\n    BASIC_AZURE_DEPLOYMENT,\n    # Vision-language LLM\n    VL_MODEL,\n    VL_BASE_URL,\n    VL_API_KEY,\n    VL_AZURE_DEPLOYMENT,\n    # Other configurations\n    CHROME_INSTANCE_PATH,\n    CHROME_HEADLESS,\n    CHROME_PROXY_SERVER,\n    CHROME_PROXY_USERNAME,\n    CHROME_PROXY_PASSWORD,\n)\nfrom .tools import BROWSER_HISTORY_DIR\n\n# Team configuration\nTEAM_MEMBERS = [\"researcher\", \"coder\", \"browser\", \"reporter\"]\n\n__all__ = [\n    # Reasoning LLM\n    \"REASONING_MODEL\",\n    \"REASONING_BASE_URL\",\n    \"REASONING_API_KEY\",\n    # Basic LLM\n    \"BASIC_MODEL\",\n    \"BASIC_BASE_URL\",\n    \"BASIC_API_KEY\",\n    # Vision-language LLM\n    \"VL_MODEL\",\n    \"VL_BASE_URL\",\n    \"VL_API_KEY\",\n    # Other configurations\n    \"TEAM_MEMBERS\",\n    \"CHROME_INSTANCE_PATH\",\n    \"CHROME_HEADLESS\",\n    \"CHROME_PROXY_SERVER\",\n    \"CHROME_PROXY_USERNAME\",\n    \"CHROME_PROXY_PASSWORD\",\n    \"BROWSER_HISTORY_DIR\",\n]"}
{"type": "source_file", "path": "src/tools/data_retriever.py", "content": "class DataRetrieverTool:\n    def retrieve_data(self, query):\n        return f\"Placeholder: Data retrieved for query: {query}\""}
{"type": "source_file", "path": "src/tools/search.py", "content": "import logging\nimport logging\nfrom src.tools.browser import browser_tool  # Import the existing browser tool\nfrom .decorators import create_logged_tool\n\nlogger = logging.getLogger(__name__)\n\n# Initialize Bing search tool using browser tool with logging\nLoggedBrowserSearch = create_logged_tool(browser_tool.__class__)  # Use the class of browser_tool\nbing_tool = LoggedBrowserSearch(name=\"bing_search\", description=\"Search Bing using a headless browser.\") # Updated name and description"}
{"type": "source_file", "path": "src/tools/web_browser.py", "content": "class WebBrowserTool:\n    def browse_web(self, url):\n         return f\"Placeholder: Web content from {url}\""}
{"type": "source_file", "path": "src/server.py", "content": "import asyncio\nimport json\nimport logging\nfrom typing import List, Optional, Union\n\nfrom fastapi import FastAPI, HTTPException, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"OpenManus API\",\n    description=\"API for OpenManus LangGraph-based agent workflow\",\n    version=\"0.1.0\",\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Allows all origins\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # Allows all methods\n    allow_headers=[\"*\"],  # Allows all headers\n)\n\n\nclass ChatMessage(BaseModel):\n    role: str = Field(\n        ..., description=\"The role of the message sender (user or assistant)\"\n    )\n    content: str = Field(..., description=\"The content of the message\")\n\n\nclass ChatRequest(BaseModel):\n    messages: List[ChatMessage] = Field(..., description=\"The conversation history\")\n    debug: Optional[bool] = Field(False, description=\"Whether to enable debug logging\")\n\nfrom src.service.workflow_service import run_agent_workflow\n\n@app.post(\"/api/chat/stream\")\nasync def chat_stream_endpoint(request: ChatRequest, req: Request):\n    \"\"\"Chat endpoint for LangGraph invoke.\n\n    Args:\n        request: The chat request\n        req: The FastAPI request object for connection state checking\n\n    Returns:\n        The streamed response\n    \"\"\"\n    try:\n        async def event_generator():\n            async for event in run_agent_workflow(\n                request.messages, request.debug\n            ):\n                # Check if client is still connected\n                if await req.is_disconnected():\n                    logger.info(\"Client disconnected, stopping workflow\")\n                    break\n                yield {\n                    \"event\": event[\"event\"],\n                    \"data\": json.dumps(event[\"data\"], ensure_ascii=False),\n                }\n        return EventSourceResponse(\n            event_generator(), media_type=\"text/event-stream\"\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\", reload=True)"}
{"type": "source_file", "path": "src/tools/browser.py", "content": "from langchain.tools import BaseTool\n\nclass BrowserTool(BaseTool):\n    name: str = \"browser\"\n    description: str = \"Placeholder browser tool\"\n\n    def _run(self, instruction: str) -> str:\n        return \"Browser tool placeholder\"\n\n    async def _arun(self, instruction: str) -> str:\n        return \"Browser tool placeholder\"\n\nbrowser_tool = BrowserTool()"}
{"type": "source_file", "path": "src/tools/decorators.py", "content": "import logging\nimport functools\nfrom typing import Any, Callable, Type, TypeVar\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\ndef log_io(func: Callable) -> Callable:\n    @functools.wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        return func(*args, **kwargs)\n    return wrapper\n\nclass LoggedToolMixin:\n    def _run(self, *args: Any, **kwargs: Any) -> Any:\n        return super()._run(*args, **kwargs)\n\ndef create_logged_tool(base_tool_class: Type[T]) -> Type[T]:\n    class LoggedTool(LoggedToolMixin, base_tool_class):\n        pass\n    return LoggedTool"}
{"type": "source_file", "path": "src/graph/types.py", "content": "from typing import Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import MessagesState\n\nfrom src.config import TEAM_MEMBERS\n\n# Define routing options\nOPTIONS = TEAM_MEMBERS + [\"FINISH\"]\n\n\nclass Router(TypedDict):\n    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n\n    next: Literal[OPTIONS[0], OPTIONS[1], OPTIONS[2], OPTIONS[3], \"FINISH\"]\n\n\nclass State(MessagesState):\n    \"\"\"State for the agent system, extends MessagesState with next field.\"\"\"\n\n    # Constants\n    TEAM_MEMBERS: list[str]\n\n    # Runtime Variables\n    next: str\n    full_plan: str\n    deep_thinking_mode: bool\n    search_before_planning: bool"}
{"type": "source_file", "path": "src/client.py", "content": "import argparse\nimport requests\nimport json\n\ndef submit_task(task, host=\"http://localhost:5000\"):\n    \"\"\"Submit a task to the agent server.\"\"\"\n    try:\n        response = requests.post(f\"{host}/task\", json={\"task\": task})\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        return {\"error\": str(e)}\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"OpenManus CLI Client\")\n    parser.add_argument(\"--task\", required=True, help=\"Task description\")\n    parser.add_argument(\"--host\", default=\"http://localhost:5000\", help=\"Agent server host\")\n    \n    args = parser.parse_args()\n    \n    result = submit_task(args.task, args.host)\n    print(json.dumps(result, indent=2))\n\nif __name__ == \"__main__\":\n    main() "}
{"type": "source_file", "path": "src/tools/code_executor.py", "content": "class CodeExecutorTool:\n    def execute_code(self, code, language):\n        return f\"Placeholder: Code executed: {language}\\\\nOutput:\\\\n{code}\""}
{"type": "source_file", "path": "src/prompts/template.py", "content": "import os\nimport re\nfrom datetime import datetime\nfrom typing import Dict, List\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\n\nclass OpenManusPromptTemplate:\n    \"\"\"OpenManus prompt template manager for handling agent-specific prompts.\"\"\"\n\n    @staticmethod\n    def get_prompt_template(prompt_name: str) -> str:\n        \"\"\"Load and process a prompt template from file.\n\n        Args:\n            prompt_name: Name of the prompt template file (without .md extension)\n\n        Returns:\n            Processed template string with variable placeholders\n        \"\"\"\n        template_path = os.path.join(os.path.dirname(__file__), f\"{prompt_name}.md\")\n        with open(template_path, 'r', encoding='utf-8') as f:\n            template = f.read()\n\n        # Escape curly braces for string formatting\n        template = template.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        # Convert <<VAR>> to {VAR} format\n        template = re.sub(r\"<<([^>>]+)>>\", r\"{1}\", template)\n        return template\n\n    @staticmethod\n    def apply_prompt_template(prompt_name: str, state: AgentState) -> List[Dict[str, str]]:\n        \"\"\"Apply a prompt template with current state variables.\n\n        Args:\n            prompt_name: Name of the prompt template to apply\n            state: Current agent state containing variables and messages\n\n        Returns:\n            List of message dictionaries with system prompt and state messages\n        \"\"\"\n        # Format current time in a consistent format\n        current_time = datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\")\n\n        # Create and format the system prompt\n        system_prompt = PromptTemplate(\n            input_variables=[\"CURRENT_TIME\"],\n            template=OpenManusPromptTemplate.get_prompt_template(prompt_name),\n        ).format(CURRENT_TIME=current_time, **state)\n\n        # Combine system prompt with existing messages\n        return [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]"}
{"type": "source_file", "path": "src/service/workflow_service.py", "content": "\"\"\"Workflow service for managing agent workflow execution.\"\"\"\n\nimport asyncio\nfrom typing import AsyncGenerator, Dict, List\n\nfrom src.workflow.graph import build_graph\nfrom src.prompts.template import OpenManusPromptTemplate\n\n\nasync def run_agent_workflow(\n    messages: List[Dict[str, str]], debug: bool = False\n) -> AsyncGenerator[Dict[str, str], None]:\n    \"\"\"Run the agent workflow with the given messages.\n\n    Args:\n        messages: List of chat messages\n        debug: Whether to enable debug logging\n\n    Yields:\n        Event data for SSE streaming\n    \"\"\"\n    # Initialize workflow graph\n    workflow = build_graph()\n\n    # Format messages with system prompt\n    formatted_messages = OpenManusPromptTemplate.apply_prompt_template(\n        \"coordinator\", {\"messages\": messages}\n    )\n\n    # Run workflow\n    async for event in workflow.astream({\"messages\": formatted_messages}):\n        yield {\n            \"event\": \"message\",\n            \"data\": {\"content\": event.get(\"content\", \"\"), \"role\": \"assistant\"}\n        }\n        # Small delay to avoid overwhelming the client\n        await asyncio.sleep(0.1)"}
