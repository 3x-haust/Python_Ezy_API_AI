{"repo_info": {"repo_name": "gamechanger-ml", "repo_owner": "dod-advana", "repo_url": "https://github.com/dod-advana/gamechanger-ml"}}
{"type": "test_file", "path": "gamechangerml/api/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/api/tests/api_tests.py", "content": "import requests\nimport logging\nimport pytest\nimport os\nimport json\nimport sys\nimport time\n\nfrom requests.packages.urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom http.client import HTTPConnection  # py3\n\nfrom gamechangerml.src.search.query_expansion.utils import remove_original_kw\nfrom gamechangerml.src.text_handling.process import preprocess\nfrom gamechangerml.src.utilities.text_utils import (\n    has_many_short_tokens,\n    has_many_repeating,\n    has_extralong_tokens,\n    is_a_toc,\n    check_quality_paragraph,\n)\n\n# from gamechangerml import DATA_PATH\n\nfrom .test_examples import TestSet\n\nlogger = logging.getLogger()\nGC_ML_HOST = os.environ.get(\"GC_ML_HOST\", default=\"localhost\")\nAPI_URL = f\"{GC_ML_HOST}:5000\" if \"http\" in GC_ML_HOST else f\"http://{GC_ML_HOST}:5000\"\nQA_TIMEOUT = 30\n\n\nretries = Retry(total=10, backoff_factor=1)\nadapter = HTTPAdapter(max_retries=retries)\nhttp = requests.Session()\n\n\ndef test_conn():\n    resp = http.get(API_URL)\n    assert resp.ok == True\n\n\ndef test_expand_terms():\n    test_data = {\"termsList\": [\"artificial intelligence\"]}\n    resp = http.post(API_URL + \"/expandTerms\", json=test_data)\n    verified = {\n        \"qexp\": {\n            \"artificial intelligence\": [\n                '\"employ artificial intelligence\"',\n                '\"developing artificial intelligence\"',\n            ]\n        },\n        \"wordsim\": {\"artificial\": [\"artifical\"], \"intelligence\": [\"intellegence\"]},\n    }\n    assert resp.json() == verified\n\n\ndef test_expand_terms_proper_fields_qexp():\n    test_data = {\"termsList\": [\"artificial intelligence\"]}\n    resp = http.post(API_URL + \"/expandTerms\", json=test_data)\n    print(resp)\n    data = resp.json()\n    if \"qexp\" in data.keys():\n        assert True\n    else:\n        assert False\n\n\ndef test_expand_terms_proper_fields_wordsim():\n    test_data = {\"termsList\": [\"artificial intelligence\"]}\n    resp = http.post(API_URL + \"/expandTerms\", json=test_data)\n    data = resp.json()\n    if \"wordsim\" in data.keys():\n        assert True\n    else:\n        assert False\n\n\n# this is in here because it is based off of api function flow not specifically qe\ndef test_remove_kw_1():\n    test_term = \"network\"\n    test_list = [\"network connection\", \"communications network\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"connection\", \"communications\"]\n    assert terms == verified\n\n\ndef test_remove_kw_2():\n    test_term = \"animal\"\n    test_list = [\"animals\", \"animal cruelty\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"animals\", \"cruelty\"]\n    assert terms == verified\n\n\ndef test_remove_kw_3():\n    test_term = \"american navy\"\n    test_list = [\"british navy\", \"navy washington\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"british navy\", \"navy washington\"]\n    assert terms == verified\n\n\ndef test_remove_kw_4():\n    test_term = \"weapons\"\n    test_list = [\"enemy weapons\", \"weapons explosives\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"enemy\", \"explosives\"]\n    assert terms == verified\n\n\ndef test_get_transformer_list():\n    resp = http.get(API_URL + \"/getModelsList\")\n    verified = TestSet.transformer_list_expect\n    response = resp.json()\n    trans = len(list(response[\"transformers\"].keys()))\n    assert trans > 0\n    return verified\n\n\ndef get_current_trans():\n    resp = http.get(API_URL + \"/getCurrentTransformer\")\n    return resp.json()\n\n\n# Sent Index Processing Tests\n\n\ndef test_has_many_short_tokens():\n    test_pars = TestSet.sent_index_processing_pars\n    results = []\n    for x in test_pars.keys():\n        text = test_pars[x]\n        tokens = preprocess(text)\n        check = has_many_short_tokens(tokens, threshold=4.0)\n        results.append(check)\n    assert results == TestSet.sent_index_processing_results[\"has_many_short_tokens\"]\n\n\ndef test_has_many_repeating():\n    test_pars = TestSet.sent_index_processing_pars\n    results = []\n    for x in test_pars.keys():\n        text = test_pars[x]\n        tokens = preprocess(text)\n        check = has_many_repeating(text, tokens, threshold=0.6)\n        results.append(check)\n    assert results == TestSet.sent_index_processing_results[\"has_many_repeating\"]\n\n\ndef test_has_extralong_tokens():\n    test_pars = TestSet.sent_index_processing_pars\n    results = []\n    for x in test_pars.keys():\n        text = test_pars[x]\n        check = has_extralong_tokens(text, threshold=25)\n        results.append(check)\n    assert results == TestSet.sent_index_processing_results[\"has_extralong_tokens\"]\n\n\ndef test_is_a_toc():\n    test_pars = TestSet.sent_index_processing_pars\n    results = []\n    for x in test_pars.keys():\n        text = test_pars[x]\n        check = is_a_toc(text)\n        results.append(check)\n    assert results == TestSet.sent_index_processing_results[\"is_a_toc\"]\n\n\ndef test_check_quality_paragraph():\n    test_pars = TestSet.sent_index_processing_pars\n    results = []\n    for x in test_pars.keys():\n        text = test_pars[x]\n        tokens = preprocess(text)\n        check = check_quality_paragraph(tokens, text)\n        results.append(check)\n    assert results == TestSet.sent_index_processing_results[\"check_quality\"]\n\n\n# def test_changeModels():\n\n#     test_index = \"sent_index_20210715\"\n#     model_dict = {\"sentence\": test_index}\n#     resp = http.post(API_URL + \"/reloadModels\", json=model_dict)\n#     time.sleep(20)\n#     curr = getCurrentTrans()\n#     assert curr[\"sentence_index\"] == \"gamechangerml/models/sent_index_20210715\"\n\n# Search Tests\n\n\ndef test_post_sent_search():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n\n    # assert [{'id':resp['id'],'text':resp['text']} for resp in resp.json()] == [{'id':resp['id'],'text':resp['text']} for resp in verified]\n    # for i in range(0,len(verified)):\n    #     assert abs(resp.json()[i]['score'] - verified[i]['score']) < .01\n    assert len(resp.json()) > 5\n\n\ndef test_sent_search_dupes():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n    resp_data = resp.json()\n    num_results = len(resp_data)\n    print(num_results)\n    unique = set([x[\"id\"] for x in resp_data])\n    if len(unique) == num_results:\n        assert True\n    else:\n        assert False\n\n\ndef test_sent_search_proper_fields_id():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n    resp_data = resp.json()\n    if \"id\" in resp_data[0].keys():\n        assert True\n\n\ndef test_sent_search_proper_fields_text():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n    resp_data = resp.json()\n    if \"text\" in resp_data[0].keys():\n        assert True\n\n\ndef test_sentSearch_properFields_threshold():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n    resp_data = resp.json()\n    if \"threshold\" in resp_data[0].keys():\n        assert True\n\n\ndef test_sent_search_has_text():\n    test_data = TestSet.sentence_test_data\n    verified = TestSet.sentence_search_expect\n\n    resp = http.post(API_URL + \"/transSentenceSearch\", json=test_data)\n    resp_data = resp.json()\n    allHaveText = True\n    for doc in resp_data:\n        if len(doc[\"text\"]) < 1:\n            allHaveText = False\n    assert allHaveText\n\n\ndef test_sent_search_threshold():\n    test_data = TestSet.sentence_test_data\n    # threshold = \"0.6\"\n    resp = http.post(\n        API_URL + \"/transSentenceSearch?threshold=0.5\", json=test_data)\n    resp_data = resp.json()\n    for i in resp_data:\n        if float(i[\"score\"]) >= 0.5:\n            assert int(i[\"passing_result\"]) == 1\n        else:\n            assert int(i[\"passing_result\"]) == 0\n\n\ndef test_recommender():\n    test_data = TestSet.recommender_data\n    expected = TestSet.recommender_results\n\n    resp = http.post(API_URL + \"/recommender\", json=test_data)\n    data = resp.json()\n    print(data)\n    assert len(data[\"results\"]) == 5\n    assert len(set(expected[\"results\"]).intersection(data[\"results\"])) > 0\n\n\n# QA Tests\n\n\ndef send_qa(query, context):\n\n    start = time.perf_counter()\n    post = {\"query\": query, \"search_context\": context}\n    data = json.dumps(post).encode(\"utf-8\")\n    headers = {\"Content-Type\": \"application/json\"}\n    response = http.post(API_URL + \"/questionAnswer\",\n                         data=data, headers=headers)\n\n    end = time.perf_counter()\n    took = float(f\"{end-start:0.4f}\")\n\n    return response.json(), took\n\n\ndef test_qa_regular():\n    query = \"when is marijuana legalized\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_one_question():\n    query = \"when is marijuana legalized?\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_multiple_question():\n    query = \"when is marijuana legalized???\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_allcaps():\n    query = \"WHEN IS MARIJUANA LEGALIZED\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_apostrophe():\n    query = \"when's marijuana legalized\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_past_tense():\n    query = \"when was marijuana legalized?\"\n    expected = \"Wednesday\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_future_tense():\n    query = \"when will marijuana be legal?\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_specific():\n    query = \"when will marijuana be legal in Virginia?\"\n    expected = \"it will be legal to grow up to four marijuana plants beginning July 1\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_qa_outside_scope():\n    query = \"what is the capital of Assyria?\"\n    expected = \"\"\n    resp, took = send_qa(query, TestSet.qa_test_context_1)\n    top_answer = resp[\"answers\"][0][\"text\"]\n    scores = [i[\"null_score_diff\"] for i in resp[\"answers\"]]\n    print(\n        \"\\nQUESTION: \", query, \"\\nANSWER: \", top_answer, f\"\\n (took {took} seconds)\\n\"\n    )\n    assert top_answer == expected  # assert response is right\n    # assert took < QA_TIMEOUT # assert time\n    assert resp[\"answers\"][0][\"null_score_diff\"] == min(\n        scores\n    )  # assert is best scoring answer\n\n\ndef test_get_current_models():\n    expected = \"\"\n    resp = http.get(API_URL + \"/getLoadedModels\")\n    resp_data = resp.json()\n    assert type(resp_data) == dict\n\n\ndef test_get_current_models_not_empty():\n    expected = \"\"\n    resp = http.get(API_URL + \"/getLoadedModels\")\n    resp_data = resp.json()\n    assert len(resp_data.keys()) > 0\n\n\ndef test_data_dir():\n    resp = http.get(API_URL + \"/getDataList\")\n    resp_data = resp.json()\n    assert type(resp_data) == dict\n\n\ndef test_data_dir_not_empty():\n    resp = http.get(API_URL + \"/getDataList\")\n    resp_data = resp.json()\n    if resp_data[\"dirs\"]:\n        assert True\n\n\n# Train Model tests\n\n# def test_trainModel_sentence():\n#     model_dict = {\n#         \"build_type\": \"sentence\",\n#         \"corpus\": os.path.join(DATA_PATH, \"test_data\"), # should have 3 test docs\n#         \"encoder_model\": \"msmarco-distilbert-base-v2\",\n#         \"gpu\": False,\n#         \"upload\": False,\n#         \"version\": \"TEST\"\n#     }\n#     resp = http.post(API_URL + \"/trainModel\", json=model_dict)\n#     assert resp.ok == True\n\n# def test_trainModel_eval_squad():\n#     model_dict = {\n#         \"build_type\": \"eval\",\n#         \"model_name\": \"bert-base-cased-squad2\",\n#         \"eval_type\": \"original\",\n#         \"sample_limit\": 10,\n#         \"validation_data\": \"latest\"\n#     }\n#     resp = http.post(API_URL + \"/trainModel\", json=model_dict)\n#     assert resp.ok == True\n\n# def test_trainModel_eval_msmarco():\n#     model_dict = {\n#         \"build_type\": \"eval\",\n#         \"model_name\": \"msmarco-distilbert-base-v2\",\n#         \"eval_type\": \"original\",\n#         \"sample_limit\": 10,\n#         \"validation_data\": \"latest\"\n#     }\n#     resp = http.post(API_URL + \"/trainModel\", json=model_dict)\n#     assert resp.ok == True\n\n# def test_trainModel_eval_nli():\n#     model_dict = {\n#         \"build_type\": \"eval\",\n#         \"model_name\": \"distilbart-mnli-12-3\",\n#         \"eval_type\": \"original\",\n#         \"sample_limit\": 10,\n#         \"validation_data\": \"latest\"\n#     }\n#     resp = http.post(API_URL + \"/trainModel\", json=model_dict)\n#     assert resp.ok == True\n\n# def test_TrainModel_meta():\n#     model_dict = {\n#         \"build_type\": \"meta\",\n#     }\n#     resp = http.post(API_URL + \"/trainModel\", json=model_dict)\n#     assert resp.ok == True\n"}
{"type": "test_file", "path": "gamechangerml/src/featurization/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/src/search/embed_reader/test/test_ebr_query_json.py", "content": "#  Basic test\nfrom gamechangerml.src.search.embed_reader.sparse import SparseReader\nfrom pprint import pformat\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_json_in(query_results_ten):\n    assert \"query\" in query_results_ten\n    assert \"documents\" in query_results_ten\n    assert len(query_results_ten[\"documents\"]) == 10\n\n\ndef test_query_in(query_results_ten, model_name):\n    query = query_results_ten[\"query\"]\n    logger.info(\"query : {}\".format(query))\n    sparse_reader = SparseReader(model_name=model_name)\n    assert sparse_reader\n\n    resp = sparse_reader.predict(query_results_ten)\n    logger.info(pformat(resp))\n    assert resp[\"query\"] == query\n    assert len(resp[\"answers\"]) == 10\n\n\ndef test_bad_query(bad_query_results, model_name):\n    query = bad_query_results[\"query\"]\n    logger.info(\"query : {}\".format(query))\n    sparse_reader = SparseReader(model_name=model_name)\n    assert sparse_reader\n\n    resp = sparse_reader.predict(bad_query_results)\n    logger.info(pformat(resp))\n    assert resp[\"query\"] == query\n    assert len(resp[\"answers\"]) == 10\n"}
{"type": "test_file", "path": "gamechangerml/src/search/query_expansion/tests/test_qe_exceptions.py", "content": "import logging\n\nimport pytest\n\nfrom gamechangerml.src.search.query_expansion.build_ann_cli.build_qe_model import (  # noqa\n    main,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_qe_except_build():\n    fake_path = \"foo\"\n    with pytest.raises(FileNotFoundError):\n        main(fake_path, fake_path)\n"}
{"type": "test_file", "path": "gamechangerml/src/search/ranking/tests/__init__.py", "content": "\n"}
{"type": "test_file", "path": "gamechangerml/src/search/sent_transformer/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/api/tests/test_examples.py", "content": "class TestSet:\n    qa_test_data = {\"text\": \"How manysides does a pentagon have?\"}\n    qa_expect = {\n        \"answers\": [\"five\"],\n        \"question\": \"How many sides does a pentagon have?\",\n    }\n    qa_test_context_1 = [\"Virginia'''s Democratic-controlled Legislature passed a bill legalizing the possession of small amounts of marijuana on Wednesday, making it the 16th state to take the step. Under Virginia'''s new law, adults ages 21 and over can possess an ounce or less of marijuana beginning on July 1, rather than Jan. 1, 2024. Gov. Ralph Northam, a Democrat, proposed moving up the date, arguing it would be a mistake to continue to penalize people for possessing a drug that would soon be legal. Lt. Gov. Justin Fairfax, also a Democrat, broke a 20-20 vote tie in Virginia'''s Senate to pass the bill. No Republicans supported the measure. Democratic House of Delegates Speaker Eileen Filler-Corn hailed the plan. Today, with the Governor'''s amendments, we will have made tremendous progress in ending the targeting of Black and brown Virginians through selective enforcement of marijuana prohibition by this summer she said in a statement. Republicans voiced a number of objections to what they characterized as an unwieldy, nearly 300-page bill. Several criticized measures that would grant licensing preferences to people and groups who'''ve been affected by the war on drugs and make it easier for workers in the industry to unionize. Senate Minority Leader Tommy Norment also questioned Northam'''s motives.\",\n    \"We have a governor who wants to contribute to the resurrection of his legacy, Norment said, referring to the 2019 discovery of a racist photo in Northam'''s 1984 medical school yearbook. The accelerated timeline sets Virginia cannabis consumers in an unusual predicament. While it will be legal to grow up to four marijuana plants beginning July 1, it could be several years before the state begins licensing recreational marijuana retailers. And unlike other states, the law won'''t allow the commonwealth'''s existing medical dispensaries to begin selling to all adults immediately. Jenn Michelle Pedini, executive director of Virginia NORML, called legalization an incredible victory but said the group would continue to push to allow retail sales to begin sooner.\",\n    \"In the interest of public and consumer safety, Virginians 21 and older should be able to purchase retail cannabis products at the already operational dispensaries in 2021, not in 2024, Pedini said in a statement. Such a delay will only exacerbate the divide for equity applicants and embolden illicit activity. Northam and other Democrats pitched marijuana legalization as a way to address the historic harms of the war on drugs. One state study found Black Virginians were 3.5 times more likely to be arrested on marijuana charges compared with white people. Those trends persisted even after Virginia reduced penalties for possession to a $25 civil fine. New York and New Jersey also focused on addressing those patterns when governors in those states signed laws to legalize recreational marijuana this year. Northam'''s proposal sets aside 30% of funds to go to communities affected by the war on drugs, compared with 70% in New Jersey. Another 40% of Virginia'''s revenue will go toward early childhood education, with the remainder funding public health programs and substance abuse treatment.\",\n    \"Those plans, and much of the bill'''s regulatory framework, are still tentative; Virginia lawmakers will have to approve them again during their general session next year. Some criminal justice advocates say lawmakers should also revisit language that creates a penalty for driving with an open container of marijuana. In the absence of retail sales, some members of law enforcement said it'''s not clear what a container of marijuana will be. The bill specifies a category of social equity applicants, such as people who'''ve been charged with marijuana-related offenses or who graduated from historically Black colleges and universities. Those entrepreneurs will be given preference when the state grants licensing. Mike Thomas, a Black hemp cultivator based in Richmond who served jail time for marijuana possession, said those entrepreneurs deserved special attention. Thomas said he looked forward to offering his own line of organic, craft cannabis. Being that the arrest rate wasn'''t the same for everyone, I don'''t think the business opportunities should be the same for everyone\",\n]\n\n    text_extract_test_data = {\n        \"text\": \"In a major policy revision intended to encourage more schools to welcome children back to in-person instruction, federal health officials on Friday relaxed the six-foot distancing rule for elementary school students, saying they need only remain three feet apart in classrooms as long as everyone is wearing a mask. The three-foot rule also now applies to students in middle schools and high schools, as long as community transmission is not high, officials said. When transmission is high, however, these students must be at least six feet apart, unless they are taught in cohorts, or small groups that are kept separate from others. The six-foot rule still applies in the community at large, officials emphasized, and for teachers and other adults who work in schools, who must maintain that distance from other adults and from students. Most schools are already operating at least partially in person, and evidence suggests they are doing so relatively safely. Research shows in-school spread can be mitigated with simple safety measures such as masking, distancing, hand-washing and open windows. EDUCATION BRIEFING: The pandemic is upending education. Get the latest news and tips.\"\n    }\n    summary_expect = {\n        \"extractType\": \"summary\",\n        \"extracted\": \"In a major policy revision intended to encourage more schools to welcome children back to in-person instruction, federal health officials on Friday relaxed the six-foot distancing rule for elementary school students, saying they need only remain three feet apart in classrooms as long as everyone is wearing a mask.\",\n    }\n    topics_expect = {\n        \"extractType\": \"topics\",\n        \"extracted\": [\n            [0.44866187988155737, \"distancing\"],\n            [0.30738175379466876, \"schools\"],\n            [0.3028274099264987, \"upending\"],\n            [0.26273395468924415, \"students\"],\n            [0.23815691706519543, \"adults\"],\n        ],\n    }\n    keywords_expect = {\n        \"extractType\": \"keywords\",\n        \"extracted\": [\"six-foot rule\", \"three-foot rule\"],\n    }\n    sentence_test_data = {\"text\": \"naval command\"}\n    sentence_search_expect = [\n        {\n            \"id\": \"OPNAVNOTE 5430.1032.pdf_36\",\n            \"text\": \"naval forces central command comusnavcent commander u s naval forces southern command comnavso and commander u s naval forces europe commander u s naval forces africa comusnaveur comusnavaf\",\n            \"text_length\": 0.2,\n            \"score\": 0.9124890685081481,\n        },\n        {\n            \"id\": \"OPNAVINST 3440.18.pdf_124\",\n            \"text\": \"c commander u s naval forces europe africa for ports in the u s european command and the u s africa command area of responsibility\",\n            \"text_length\": 0.11060606060606061,\n            \"score\": 0.7812968355236631,\n        },\n        {\n            \"id\": \"OPNAVINST 3006.1 w CH-2.pdf_178\",\n            \"text\": \"enclosure naval forces africa commander u s naval forces central command commander u s naval forces southern command shall\",\n            \"text_length\": 0.09848484848484848,\n            \"score\": 0.775530730233048,\n        },\n        {\n            \"id\": \"MILPERSMAN 1001-021.pdf_10\",\n            \"text\": \"major shore commands e g office of the chief of naval operations navy personnel command commander navy reserve forces command etc\",\n            \"text_length\": 0.10909090909090909,\n            \"score\": 0.7683667984875766,\n        },\n        {\n            \"id\": \"OPNAVINST 3440.18.pdf_125\",\n            \"text\": \"d commander u s naval forces central command for ports in the u s central command area of responsibility and\",\n            \"text_length\": 0.07727272727272727,\n            \"score\": 0.7664882681586526,\n        },\n        {\n            \"id\": \"OPNAVINST 8120.1A.pdf_64\",\n            \"text\": \"j commander naval sea systems command comnavseasyscom comnavseasyscom is the echelon supporting flag officer to\",\n            \"text_length\": 0.08181818181818182,\n            \"score\": 0.764475125616247,\n        },\n        {\n            \"id\": \"DoDD 4500.56 CH 5.pdf_157\",\n            \"text\": \"m commander u s naval forces europe and commander u s naval forces africa\",\n            \"text_length\": 0.024242424242424242,\n            \"score\": 0.7282583944725268,\n        },\n        {\n            \"id\": \"OPNAVINST 3111.17B.pdf_224\",\n            \"text\": \"commander u s naval forces europe u s naval forces africa\",\n            \"text_length\": 0.0,\n            \"score\": 0.716657280921936,\n        },\n        {\n            \"id\": \"MARINE CORPS MANUAL CH 1-3.pdf_690\",\n            \"text\": \"navy personnel under the military command of the commandant of the marine corps\",\n            \"text_length\": 0.03333333333333333,\n            \"score\": 0.6932793577512105,\n        },\n        {\n            \"id\": \"SECNAVINST 4200.36B.pdf_28\",\n            \"text\": \"naval regional commanders and the commandant of the marine corps shall\",\n            \"text_length\": 0.019696969696969695,\n            \"score\": 0.6766319462747284,\n        },\n    ]\n\n    word_sim_data = {\"text\": \"naval command\"}\n    word_sim_except = {\"naval\": [\"navy\", \"maritime\"], \"command\": []}\n\n    recommender_data = {\"filenames\": [\"Title 10\"]}\n    recommender_results = {\n        \"filenames\": [\"Title 10\"],\n        \"results\": [\n            \"Title 50\",\n            \"AACP 02.1\",\n            \"DoDD 5143.01 CH 2\",\n            \"DoDD S-5230.28\",\n            \"DoDI 5000.89\",\n        ],\n    }\n\n    # extraction_data = {\"text\": \"Carbon emissions trading is poised to go global, and billions of dollars — maybe even trillions — could be at stake. That's thanks to last month's U.N. climate summit in Glasgow Scotland, which approved a new international trading system where companies pay for cuts in greenhouse gas emissions somewhere else, rather than doing it themselves.\"}\n    # extraction_keywords_expect = {\n    #     \"extractType\": \"keywords\",\n    #     \"extracted\": [\n    #         \"climate summit\",\n    #         \"glasgow scotland\"\n    #     ]\n    # }\n    # extraction_topic_except = {\n    #     \"extractType\": \"topics\",\n    #     \"extracted\": [\n    #         [\n    #             0.402564416275499,\n    #             \"trillions\"\n    #         ],\n    #         [\n    #             0.35468207783971445,\n    #             \"trading\"\n    #         ],\n    #         [\n    #             0.34311022758576537,\n    #             \"carbon_emissions\"\n    #         ],\n    #         [\n    #             0.2798555740973044,\n    #             \"greenhouse_emissions\"\n    #         ],\n    #         [\n    #             0.2722433559706402,\n    #             \"glasgow\"\n    #         ]\n    #     ]\n    # }\n    transformer_test_data = {\n        \"query\": \"chemical agents\",\n        \"documents\": [\n            {\n                \"text\": \"a . The Do D chemical agent facility commander or director and contractor laboratories that are provided Do D chemical agents will develop a reliable security system and process that provide the capability to detect , assess , deter , communicate , delay , and respond to unauthorized attempts to access chemical agents .\",\n                \"id\": \"DoDI 5210.65 CH 2.pdf_2\",\n            },\n            {\n                \"text\": \"b . Entities approved to receive ultra dilute chemical agents from Do D will assume liability , accountability , custody , and ownership upon accepting transfer of the agents .The entity will provide Do D with an authenticated list of officials and facilities authorized to accept shipment of ultra dilute chemical agents\",\n                \"id\": \"DoDI 5210.65 CH 2.pdf_37\",\n            },\n        ],\n    }\n    transformer_search_expect = {\n        \"query\": \"chemical agents\",\n        \"answers\": [\n            {\n                \"answer\": \"Do D chemical agent facility commander\",\n                \"context\": \"a . The Do D chemical agent facility commander or director and contractor laboratories that are provided Do D chemical agents will develop a reliable security system and process that provide the c\",\n                \"id\": \"DoDI 5210.65 CH 2.pdf_2\",\n                \"text\": \"a . The Do D chemical agent facility commander or director and contractor laboratories that are provided Do D chemical agents will develop a reliable security system and process that provide the capability to detect , assess , deter , communicate , delay , and respond to unauthorized attempts to access chemical agents .\",\n            },\n            {\n                \"answer\": \"shipment of ultra dilute chemical agents\",\n                \"context\": \"rship upon accepting transfer of the agents .The entity will provide Do D with an authenticated list of officials and facilities authorized to accept shipment of ultra dilute chemical agents\",\n                \"id\": \"DoDI 5210.65 CH 2.pdf_37\",\n                \"text\": \"b . Entities approved to receive ultra dilute chemical agents from Do D will assume liability , accountability , custody , and ownership upon accepting transfer of the agents .The entity will provide Do D with an authenticated list of officials and facilities authorized to accept shipment of ultra dilute chemical agents\",\n            },\n        ],\n    }\n    transformer_list_expect = {\n        \"bert-base-cased-squad2\",\n        \"distilbart-mnli-12-3\",\n        \"distilbert-base-uncased-distilled-squad\",\n        \"distilroberta-base\",\n        \"msmarco-distilbert-base-v2\",\n        \"msmarco-distilbert-base-v2_20220105\"\n        # 'msmarco-distilbert-base-v2_2021-10-17',\n        # 'msmarco-distilbert-base-v2_20211210',\n    }\n\n    sent_index_processing_pars = {\n        \"good\": \"6. U.S. Army Corps of Engineers (USACE). USACE is involved with waterways dredging, flood prevention, permitting obstructions within U.S. waters, and the construction, maintenance, and operation of waterway projects, such as locks, dams, and reservoirs, etc. USACE also enforces the Refuse Act (33 U.S.C. 407). The Coast Guard has been designated to assist in the enforcement of certain specific provisions of law and regulations administered by USACE. Comman...\",\n        \"bad_acronyms\": \"EPA/625/11-91/002, 1992 (ar) 40 CFR 268 (as) 40 CFR 240 (at) 42 U.S.C. 7401 (au) 40 CFR 61 (av) 40 CFR 230 (aw) 33 CFR 320 (ax) 33 CFR 321 (ay) 33 CFR 322 (az) 33 CFR 323 (ba) 33 CFR 325 (bb) 33 CFR 330 (bc) 40 CFR 233 (bd) 16 U.S.C. §§1451-1464 (be) 42 U.S.C. 4321 (bf) 40 CFR 220 (bg) 40 CFR 221 (bh) 40 CFR 222 (bi) 40 CFR 227 (bj) 40 CFR 224 (bk) 40 CFR 228 (bl) 40 CFR 223 (bm) 40 CFR 225 (bn) 40 CFR 226 (bo) 40 CFR 229 (bp) 33 U.S.C. 1401 (bq) 40 CFR 255 (br) 33 CFR 324 (bs) 15 CFR 930\",\n        \"bad_pages\": \"Page D4-1 – D4-6 Page D4-1 – D4-6 Page D4-9 – D4-10 Page D4-9 – D4-10 Page D4-13 – D4-16 Page D4-13 – D4-16 Page D4-19 – D4-20 Page D4-18a – D4-20 Page E3-1 – E3-25 Page E3-1 – E3-36\",\n        \"bad_long_tokens\": \"OPLANOPORDPAPACEPCCPCIPDSPHAPMCSPMIPOIPSGPZRTDSBSPOSQDLDRSSASTANAGTACEVACTACSOPTAPTASKORGTB MEDTCTCCCTEWLSTFCTLTLPTMTOETTPUAoperation planoperation orderphysician assistantprimary, alternate, contingency, and emergencypre-combat checkpre-combat inspectionpatient decontamination siteperiodic health assessmentpreventive maintainence checks and servicepatient movement itempoint of injuryplatoon sergeantpickup zonereturn to dutysupply bulletinsupport ope\",\n        \"bad_toc\": \"DoDM 4140.68 March 5 2020 TABLE OF CONTENTS 2 TABLE OF CONTENTS SECTION 1 GENERAL ISSUANCE INFORMATION .............................................................................. 4 1.1. Applicability. .................................................................................................................... 4 SECTION 2 RESPONSIBILITIES ......................................................................................................... 5 2.1. Assistant Secretary of Defense for Sustainment. .............................................................. 5 2.2. DLA. ................................................................................................................................. 5 2.3. DoD Component Heads. ................................................................................................... 5 2.4. Secretaries of the Military Departments. .......................................................................... 6 2.5. Commander United States Special Operations Command USSOCOM. ...................... 7 2.6. Administrators of Participating U.S. Government Civil Agencies. .................................. 7 SECTION 3 GENERAL PROCEDURES ................................................................................................ 8 3.1. NIMSCs. ........................................................................................................................... 8 3.2. PICA. .............................................................................................................................. 17 3.3. SICA. .............................................................................................................................. 18 3.4. Exceptions for SOP Items.............................................................................................. 19 3.5. NIMSC Designation........................................................................................................ 20 SECTION 4 SUPPLY AND DEPOT MAINTENANCE OPERATIONS PROCEDURES ................................ 24 4.1. Procedures for NIMSC 1 2 3 4 5 6 7 8 or 0 items. ................................................. 24 4.2. Provisioning. ................................................................................................................... 24 4.3. PICA Assignment. .......................................................................................................... 25 4.4. IMC Changes and PICA or SICA Reassignment Requests. ........................................... 26 4.5. Item Adoption. ................................................................................................................ 28 4.6. Procurement. ................................................................................................................... 28 4.7. Cataloging. ...................................................................................................................... 29 4.8. Depot Maintenance. ........................................................................................................ 31 4.9. Disposition. ..................................................................................................................... 31 4.10. Inactive Items. ............................................................................................................... 32 4.11. Standardization. ............................................................................................................ 33 SECTION 5 ITEM REVIEW PROCEDURES FOR MIGRATION TO NIMSC 5 OR NIMSC 6 .................. 34 5.1. Review Items for Migration to NIMSC 5 or NIMSC 6. ................................................. 34 5.2. Single Submitter of Procurement Specifications and Depotlevel Repair Specifications. ................................................................................................................... 35 SECTION 6 NIMSC MIGRATION PROCEDURES ............................................................................. 37 6.1. Migration to NIMSC 5 or NIMSC 6. .............................................................................. 37 6.2. NIMSC Migration or PICA Reassignment. .................................................................... 37 6.3. PreETD TimePeriod. .................................................................................................... 40 6.4. ETD TimePeriod............................................................................................................ 42 6.5. PostETD Timeperiod.................................................................................................... 42 SECTION 7 SUPPLY OPERATIONS PROCEDURES FOR NIMSC 5 AND NIMSC 6 ITEMS ................. 44 7.1. Item Stockage.................................................................................................................. 44 7.2. Requirements Computation and Methodology. .............................................................. 44 7.3. Item Distribution. ............................................................................................................ 44 a. Item Transfer Actions. ................................................................................................. 44 b. Requisition Processing. ................................................................................................ 45 \",\n    }\n    sent_index_processing_results = {\n        \"has_many_short_tokens\": [False, True, True, False, False],\n        \"has_many_repeating\": [False, True, True, False, True],\n        \"has_extralong_tokens\": [False, False, False, True, True],\n        \"is_a_toc\": [False, False, False, False, True],\n        \"check_quality\": [True, False, False, False, False],\n    }\n"}
{"type": "test_file", "path": "gamechangerml/src/featurization/test_hf_ner.py", "content": "import logging\nfrom transformers import pipeline\nfrom gamechangerml.src.utilities.text_generators import gen_json, child_doc_gen\nfrom gamechangerml.src.utilities.text_utils import simple_clean\nimport os\nimport glob\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport torch\n\n# TODO: format output\n# TODO: add timing to get time of running HF vs spacy\n# TODO: write tests to see how good both NERs are\n\n\ndef load_text(corpus_dir):\n    if not os.path.isdir(corpus_dir):\n        raise FileNotFoundError(\n            \"directory not found; got {}\".format(corpus_dir)\n        )\n    return ner_dir(corpus_dir)\n\n\n# TODO: update\ndef ner_dir(corpus_dir):\n    doc_gen = gen_json(corpus_dir)\n    for text, f_name in child_doc_gen(doc_gen):\n        text = simple_clean(text)\n        yield text\n\n\nif __name__ == \"__main__\":\n    from collections import defaultdict\n\n    model = AutoModelForTokenClassification.from_pretrained(\n        \"dbmdz/bert-large-cased-finetuned-conll03-english\", use_cdn=False\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_cdn=False)\n    nlp = pipeline(\n        \"ner\", model=model, tokenizer=tokenizer, grouped_entities=True\n    )\n\n    log_fmt = (\n        \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n        + \"%(funcName)s()], %(message)s\"\n    )\n    logging.basicConfig(level=logging.INFO, format=log_fmt)\n\n    here = os.path.dirname(os.path.abspath(__file__))\n    c_dir = os.path.join(here, \"test_data\")\n\n    ner_dict = defaultdict(set)\n    for text in ner_dir(c_dir):\n        print(nlp(text))\n"}
{"type": "test_file", "path": "gamechangerml/src/featurization/keywords/qe_mlm/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/src/search/ranking/tests/test_rank.py", "content": "import json\nimport logging\nimport os\n\nimport pytest\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.fixture\ndef rank_obj():\n    from gamechangerml.src.featurization.rank_features.rank import Rank\n\n    return Rank()\n\n\n# # TODO If ICMP is enabled, try using `os.system(\"ping -c 1 \" + host)`\n# @pytest.fixture\n# def search_data():\n#     import requests\n\n#     data = {\"searchText\": \"environmental policy\", \"index\": \"game_changer\", \"limit\": 100}\n#     r = None\n#     endpt = f\"http://{os.environ.get('ML_API_HOST', 'localhost')}:9346/v2/data/documentSearch\"\n#     try:\n#         r = requests.post(endpt, json=data, timeout=2)\n#     except requests.HTTPError:\n#         logger.exception(\"host not reachable\")\n#     return r\n\n\n@pytest.fixture\ndef search_data_sem():\n    here = os.path.dirname(os.path.realpath(__file__))\n    test_data = os.path.join(here, \"sem_test.json\")\n\n    with open(test_data) as f:\n        resp = json.load(f)\n    return resp\n\n\ndef test_rank_func_sem(search_data_sem, rank_obj):\n\n    resp = search_data_sem[\"docs\"]\n    assert rank_obj.rerank(resp)\n\n\n# def test_rank_func_kw(search_data, rank_obj):\n#     r = search_data\n#     if r is None:\n#         assert False\n#     resp = r.json()[\"docs\"]\n#     assert rank_obj.rerank(resp)\n\n\ndef test_rank_func_empty(rank_obj):\n    with pytest.raises(ValueError):\n        rank_obj.rerank({})\n\n\ndef test_rank_func_alpha(rank_obj):\n    with pytest.raises(ValueError):\n        rank_obj.rerank({}, alpha=0.6)\n"}
{"type": "test_file", "path": "gamechangerml/src/search/query_expansion/tests/test_qe.py", "content": "import logging\nfrom gamechangerml.src.search.query_expansion.utils import remove_original_kw\n\nimport pytest\n\nlogger = logging.getLogger(__name__)\n\n\ndef check(expanded, exp_len):\n    return 1 <= len(expanded) <= exp_len\n\n\n# this is in here because it is based off of api function flow not specifically qe\ndef test_remove_kw_1():\n    test_term = \"network\"\n    test_list = [\"network connection\", \"communications network\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"connection\", \"communications\"]\n    assert terms == verified\n\n\ndef test_remove_kw_2():\n    test_term = \"animal\"\n    test_list = [\"animals\", \"animal cruelty\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"animals\", \"cruelty\"]\n    assert terms == verified\n\n\ndef test_remove_kw_3():\n    test_term = \"american navy\"\n    test_list = [\"british navy\", \"navy washington\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"british navy\", \"navy washington\"]\n    assert terms == verified\n\n\ndef test_remove_kw_4():\n    test_term = \"weapons\"\n    test_list = [\"enemy weapons\", \"weapons explosives\"]\n    terms = remove_original_kw(test_list, test_term)\n    verified = [\"enemy\", \"explosives\"]\n    assert terms == verified\n\n\n# @pytest.mark.parametrize(\n#     \"args\",\n#     [\n#         [\"passport\", []],\n#         [\n#             \"Find a book, painting, or work of art created in Santa Monica or on the west coast\",\n#             [\"sculpture\", \"piece\"],\n#         ],  # noqa\n#         [\"telework policy for remote work\", []],\n#         [\"telework policy work\", [\"public\"]],\n#     ],\n# )\n# def test_qe_mlm(topn, qe_mlm_obj, args):\n#     query, expected = args\n#     actual = qe_mlm_obj.expand(query, topn=topn, threshold=0.2, min_tokens=3)\n#     assert actual == expected\n"}
{"type": "test_file", "path": "gamechangerml/src/search/query_expansion/tests/model_conftest.py", "content": "# flake8: noqa\n# pylint: skip-file\n\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pytest\n\nfrom gamechangerml.src.search.query_expansion.build_ann_cli.build_qe_model import (  # noqa\n    main,\n)\nfrom gamechangerml.src.search.query_expansion.qe import QE\nfrom gamechangerml.configs import QexpConfig\nfrom gamechangerml.api.fastapi.settings import QEXP_MODEL_NAME\nlog_fmt = (\n    \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n    + \"%(funcName)s()], %(message)s\"\n)\nlogging.basicConfig(level=logging.DEBUG, format=log_fmt)\nlogger = logging.getLogger(__name__)\n\ntry:\n    here = os.path.dirname(os.path.realpath(__file__))\n    p = Path(here)\n    test_data_dir = os.path.join(p.parents[3], \"data\", \"test_data\")\n    aux_path = os.path.join(p.parents[3], \"data\", \"features\")\n    word_wt = os.path.join(aux_path, \"enwiki_vocab_min200.txt\")\n    assert os.path.isfile(word_wt)\nexcept (AttributeError, FileExistsError) as e:\n    logger.exception(\"{}: {}\".format(type(e), str(e)), exc_info=True)\n\n\n@pytest.fixture(scope=\"session\")\ndef ann_index_dir(tmpdir_factory):\n    fn = tmpdir_factory.mktemp(\"data\")\n    return str(fn)\n\n\n@pytest.fixture(scope=\"session\")\ndef qe_obj(ann_index_dir):\n    # main(test_data_dir, ann_index_dir, weight_file=word_wt)\n    return QE(\n        QEXP_MODEL_NAME.value, **QexpConfig.INIT_ARGS\n    )\n\n\n# @pytest.fixture(scope=\"session\")\n# def qe_mlm_obj():\n#     return QE(QEXP_MODEL_NAME.value, QexpConfig.INIT_ARGS[\"qe_files_dir\"], \"mlm\")\n\n\n@pytest.fixture(scope=\"session\")\ndef topn():\n    return 2\n"}
{"type": "test_file", "path": "gamechangerml/src/search/sent_transformer/tests/model_conftest.py", "content": "# flake8: noqa\n# pylint: skip-file\n\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pytest\n\nfrom gamechangerml.src.search.sent_transformer.model import *\nfrom gamechangerml import REPO_PATH\nfrom gamechangerml.configs import EmbedderConfig\nfrom gamechangerml.api.fastapi.settings import LOCAL_TRANSFORMERS_DIR\n\nlog_fmt = (\n    \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n    + \"%(funcName)s()], %(message)s\"\n)\nlogging.basicConfig(level=logging.DEBUG, format=log_fmt)\nlogger = logging.getLogger(__name__)\n\ntry:\n    here = os.path.dirname(os.path.realpath(__file__))\n    p = Path(here)\n    gc_path = REPO_PATH\n    test_data_dir = os.path.join(str(p), \"test_data\")\n    test_data_2_dir = os.path.join(str(p), \"test_data_2\")\n    test_index_dir = os.path.join(str(p), \"test_index\")\n\n    encoder_model_path = os.path.join(\n        str(gc_path), \"gamechangerml/models/transformers/msmarco-distilbert-base-v2\"\n    )\n    assert os.path.isdir(test_data_dir)\n    assert os.path.isdir(test_index_dir)\nexcept (AttributeError, FileExistsError) as e:\n    logger.exception(\"{}: {}\".format(type(e), str(e)), exc_info=True)\n\n\n@pytest.fixture(scope=\"session\")\ndef sent_dirs():\n    return test_data_dir, test_data_2_dir, test_index_dir\n\n\n@pytest.fixture(scope=\"session\")\ndef sent_encoder():\n    return SentenceEncoder(\n        encoder_model_name=EmbedderConfig.BASE_MODEL,\n        transformer_path=LOCAL_TRANSFORMERS_DIR.value,\n        **EmbedderConfig.MODEL_ARGS\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef sent_searcher():\n    return SentenceSearcher(test_index_dir)\n\n\n@pytest.fixture(scope=\"session\")\ndef topn():\n    return 10\n\n\n@pytest.fixture(scope=\"session\")\ndef index_files():\n    return [\"config\", \"data.csv\", \"doc_ids.txt\", \"embeddings\", \"embeddings.npy\"]\n"}
{"type": "test_file", "path": "gamechangerml/src/search/evaluation/tests/conftest.py", "content": "import logging\nimport os\nimport json\nfrom pathlib import Path\n\nfrom gamechangerml.src.search.evaluation.ablation import AblationStudy\nfrom gamechangerml.src.search.evaluation.evaltool import EvalTool\n\nimport pytest\n\nlog_fmt = (\n    \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n    + \"%(funcName)s()], %(message)s\"\n)\nlogging.basicConfig(level=logging.DEBUG, format=log_fmt)\nlogger = logging.getLogger(__name__)\n\nhere = os.path.dirname(os.path.realpath(__file__))\np = Path(here)\n\ntest_data_dir = os.path.join(p.parents[0], \"tests\", \"test_data\")\nmodel_a_pred_path = os.path.join(test_data_dir, \"model_a_predictions.json\")\nmodel_b_pred_path = os.path.join(test_data_dir, \"model_b_predictions.json\")\nground_truth_path = os.path.join(test_data_dir, \"ground_truth.json\")\n\n\neval_tool_pred = os.path.join(test_data_dir, \"predictions.json\")\neval_tool_true = os.path.join(test_data_dir, \"relations.json\")\n\nexpected_scores_path = os.path.join(test_data_dir, \"expected_score.json\")\nexpected_ranks_path = os.path.join(test_data_dir, \"expected_ranks.json\")\n\nassert os.path.isdir(test_data_dir), test_data_dir\n\ndef load_json(fpath):\n    with open(fpath, \"r\") as fp:\n        data = json.load(fp)\n    return data\n\n@pytest.fixture(scope=\"session\")\ndef ablation_model():\n    ablation = AblationStudy(\n        model_a_answer_path = model_a_pred_path,\n        model_b_answer_path = model_b_pred_path,\n        ground_truth_path = ground_truth_path,\n    )\n    return ablation\n\n@pytest.fixture(scope=\"session\")\ndef evaltool():\n    test_k_values = [5, 10, 20, 50, 100]\n    ev = EvalTool(\n        eval_tool_pred,\n        eval_tool_true,\n        test_k_values\n    )\n    return ev\n\n@pytest.fixture(scope=\"session\")\ndef expected_values():\n    expected_scores = load_json(expected_scores_path)\n    expected_ranks = load_json(expected_ranks_path)\n\n    return expected_scores, expected_ranks"}
{"type": "test_file", "path": "gamechangerml/src/text_classif/tests/concat_df.py", "content": "import logging\nimport os\nimport fnmatch\nimport pandas as pd\nimport gamechangerml.src.text_classif.utils.log_init as li\n\nlogger = logging.getLogger(__name__)\n\n# TODO Turn this into a proper CLI if it's still useful\nif __name__ == \"__main__\":\n    li.initialize_logger(to_file=False, log_name=\"none\")\n    failed_data_path = \"\"\n    train_data_path = \"\"\n    output_csv = \"\"\n    if not failed_data_path or not train_data_path or output_csv:\n        msg = \"Please update `failed_data_path`, `train_data_path` \"\n        msg += \"or `output_csv` with appropriate file names.\"\n        raise NotImplementedError(msg)\n\n    file_list = [\n        f for f in os.listdir(failed_data_path) if fnmatch.fnmatch(f, \"*.csv\")\n    ]\n    new_df = pd.DataFrame(columns=[\"src\", \"label\", \"sentence\"])\n\n    for f in sorted(file_list):\n        df = pd.read_csv(os.path.join(failed_data_path, f))\n        df = df.drop([\"prob\", \"label\"], axis=1)\n        df.rename(columns={\"top_class\": \"label\"}, inplace=True)\n        new_df = new_df.append(df, ignore_index=True)\n        logger.info(\"adding {:>5,d} : {:>5,d}\".format(len(df), len(new_df)))\n\n    df = pd.read_csv(train_data_path)\n    logger.info(\"training data size : {:>6,d}\".format(len(df)))\n    combined = df.append(new_df, ignore_index=True)\n    logger.info(\" new training size : {:>6,d}\".format(len(combined)))\n\n    # update this name as required!\n    df.to_csv(output_csv, header=False)\n"}
{"type": "test_file", "path": "gamechangerml/src/search/evaluation/tests/test_evaltool.py", "content": "import os\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef test_evaltool(evaltool, expected_values):\n    metrics_at_k = evaltool.evaluate()\n    score, _ = expected_values\n\n    all_pass = True\n    for k in metrics_at_k.keys():\n        for metric in [\"precision\", \"recall\"]:\n            predicted_val = round(metrics_at_k[k][metric], 6)\n            expected_val = round(score[str(k)][metric], 6)\n            if predicted_val != expected_val:\n                logger.error(f\"{k} {metric}\")\n                logger.error(f\"Expectected: {predicted_val}\")\n                logger.error(f\"Actual:      {expected_val}\")\n                logger.error(\"-\" * 20)\n                all_pass = False\n\n    assert all_pass\n"}
{"type": "test_file", "path": "gamechangerml/src/search/query_expansion/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/src/featurization/tests/test_abbs.py", "content": "import logging\nimport gamechangerml.src.featurization.abbreviation as abbreviation\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_expansion_dict():\n    check_str = \"Article 15 of the Uniform Code of Military Justice UCMJ or pending discharge based on action under the UCMJ are temporarily nondeployable.\"\n    abb_dict = [\n        {\"abbr_s\": \"Uniform Code of Military Justice\", \"description_s\": \"ucmj\"}\n    ]\n    text, list = abbreviation.expand_abbreviations(check_str)\n    assert abb_dict == list\n\n\ndef test_expansion_text():\n    check_str = \"Article 15 of the Uniform Code of Military Justice UCMJ or pending discharge based on action under the UCMJ are temporarily nondeployable.\"\n    new_str = \"Article 15 of the Uniform Code of Military Justice or pending discharge based on action under the Uniform Code of Military Justice are temporarily nondeployable.\"\n    text, list = abbreviation.expand_abbreviations(check_str)\n    assert new_str == text\n\n\ndef test_expansion_no_context_1():\n    check_str = \"DD\"\n    expansion = [\"Department of Defense\"]\n    result = abbreviation.expand_abbreviations_no_context(check_str)\n    assert expansion == result\n\n\ndef test_expansion_no_context_2():\n    check_str = \"Dd\"\n    expansion = [\"Department of Defense\"]\n    result = abbreviation.expand_abbreviations_no_context(check_str)\n    assert expansion == result\n\n\ndef test_expansion_no_context_3():\n    check_str = \"CI\"\n    expansion = [\"configuration item\"]\n    result = abbreviation.expand_abbreviations_no_context(check_str)\n    assert expansion == result\n"}
{"type": "test_file", "path": "gamechangerml/src/featurization/keywords/qe_mlm/tests/test_qe_mlm.py", "content": "import logging\nimport pytest\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(\n    \"args\",\n    [\n        [\"passport\", []],\n        [\n            \"Find a book, painting, or work of art created in Santa Monica or on the west coast\",  # noqa\n            [\"sculpture\", \"piece\"],\n        ],\n        [\"telework policy for remote work\", []],\n        [\"telework policy work\", [\"public\"]],\n    ],\n)\ndef test_qe_mlm(qe_mlm, args):\n    q_str, expected = args\n    actual = qe_mlm.predict(q_str, threshold=0.2, top_n=2)\n    assert actual == expected\n"}
{"type": "test_file", "path": "gamechangerml/src/search/sent_transformer/tests/model_test_search.py", "content": "import logging\nimport os\n\n\nfrom gamechangerml.src.search.sent_transformer.model import *\nfrom gamechangerml.src.utilities.utils import get_local_model_prefix\nfrom gamechangerml import REPO_PATH, MODEL_PATH\n\nfrom gamechangerml.configs import PathConfig, SimilarityConfig\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_sent_search(sent_dirs, topn):\n    \"\"\"\n    Test for performing a search\n    \"\"\"\n    test_data_dir, test_data_2_dir, test_index_dir = sent_dirs\n\n    sent_searcher = SentenceSearcher(\n        sim_model_name=SimilarityConfig.BASE_MODEL,\n        index_path=os.path.join(\n            MODEL_PATH, get_local_model_prefix(\"sent_index\")[0]),\n        transformer_path=PathConfig.TRANSFORMER_PATH,\n    )\n\n    queries = [\"regulation\", \"Major Automated Information System\"]\n    for query in queries:\n        results = sent_searcher.search(query, num_results=topn)\n        assert len(results) == topn\n"}
{"type": "test_file", "path": "gamechangerml/src/utilities/numpy_encoder/tests/test_encoder.py", "content": "import json\n\nimport numpy as np\nimport os\nimport pytest\n\nfrom gamechangerml.src.utilities.numpy_encoder import NumpyEncoder, ndarray_hook\n\ntest_vector = np.array([1.0, 2.0, 3.0])\ntest_mat = np.eye(3)\nhere = os.path.dirname(os.path.realpath(__file__))\n\n\ndef write_encoded(obj, tdir):\n    fp = tdir.mkdir(\"sub\").join(\"obj.json\")\n    fp.write(obj)\n    return fp\n\n\ndef read_encoded(fp, hook=None):\n    with open(fp) as f:\n        return json.load(f, object_hook=hook)\n\n\n@pytest.mark.parametrize(\n    \"np_dtype\",\n    [\n        np.int_,\n        np.intc,\n        np.intp,\n        np.int8,\n        np.int16,\n        np.int32,\n        np.int64,\n        np.uint8,\n        np.uint16,\n        np.uint32,\n        np.uint64,\n        np.float_,\n        np.float16,\n        np.float32,\n        np.float64,\n    ],\n)\ndef test_number_cast(np_dtype, tmpdir):\n    num = 42\n    num_cast = np_dtype(num)\n    encoded = json.dumps(num_cast, cls=NumpyEncoder)\n\n    fp = write_encoded(encoded, tmpdir)\n    rw_num = read_encoded(fp)\n\n    rw_num = np_dtype(rw_num)\n    assert rw_num == num_cast, num_cast\n\n\n@pytest.mark.parametrize(\n    \"np_dtype\",\n    [\n        np.int_,\n        np.intc,\n        np.intp,\n        np.int8,\n        np.int16,\n        np.int32,\n        np.int64,\n        np.uint8,\n        np.uint16,\n        np.uint32,\n        np.uint64,\n        np.float_,\n        np.float16,\n        np.float32,\n        np.float64,\n    ],\n)\ndef test_array(np_dtype, tmpdir):\n    test_cast = np_dtype(test_mat)\n    encoded = json.dumps(test_cast, cls=NumpyEncoder)\n\n    fp = write_encoded(encoded, tmpdir)\n    mat = read_encoded(fp, hook=ndarray_hook)\n\n    assert mat.dtype == test_cast.dtype\n    assert mat.shape == test_cast.shape\n    assert np.allclose(test_cast, mat)\n\n\n@pytest.mark.parametrize(\n    \"metadata\",\n    [\"D\", \"ms\"],\n)\ndef test_single_date(tmpdir, metadata):\n    date_in = np.datetime64(\"2020-04\", metadata)\n    encoded = json.dumps(date_in, cls=NumpyEncoder)\n\n    fp = write_encoded(encoded, tmpdir)\n    date_out = np.datetime64(read_encoded(fp, hook=None))\n\n    assert date_in == date_out\n    assert isinstance(date_out, np.datetime64)\n    assert str(date_in.dtype) == str(date_out.dtype)\n\n\ndef test_date_array(tmpdir):\n    date_array = np.arange(\"2020-01-01\", \"2020-02-01\", dtype=np.datetime64)\n    encoded = json.dumps(date_array, cls=NumpyEncoder)\n    fp = write_encoded(encoded, tmpdir)\n    dates_out = read_encoded(fp, hook=ndarray_hook)\n    assert (dates_out == date_array).all()\n\n\ndef test_array_as_list():\n    with open(os.path.join(here, \"ndarray_as_list.json\")) as fp:\n        float_array = np.array(json.load(fp, object_hook=ndarray_hook))\n    assert isinstance(float_array, np.ndarray)\n"}
{"type": "test_file", "path": "gamechangerml/src/search/query_expansion/tests/model_test_qe.py", "content": "import logging\nfrom gamechangerml.src.search.query_expansion.utils import remove_original_kw\n# flake8: noqa\n# pylint: skip-file\n\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pytest\n\nfrom gamechangerml.src.search.query_expansion.build_ann_cli.build_qe_model import (  # noqa\n    main,\n)\nfrom gamechangerml.src.search.query_expansion.qe import QE\nfrom gamechangerml.configs import QexpConfig\nfrom gamechangerml.api.fastapi.settings import QEXP_MODEL_NAME\nlog_fmt = (\n    \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n    + \"%(funcName)s()], %(message)s\"\n)\nlogging.basicConfig(level=logging.DEBUG, format=log_fmt)\nlogger = logging.getLogger(__name__)\n\ntry:\n    here = os.path.dirname(os.path.realpath(__file__))\n    p = Path(here)\n    test_data_dir = os.path.join(p.parents[3], \"data\", \"test_data\")\n    aux_path = os.path.join(p.parents[3], \"data\", \"features\")\n    word_wt = os.path.join(aux_path, \"enwiki_vocab_min200.txt\")\n    assert os.path.isfile(word_wt)\nexcept (AttributeError, FileExistsError) as e:\n    logger.exception(\"{}: {}\".format(type(e), str(e)), exc_info=True)\n\n\n@pytest.fixture(scope=\"session\")\ndef ann_index_dir(tmpdir_factory):\n    fn = tmpdir_factory.mktemp(\"data\")\n    return str(fn)\n\n\n@pytest.fixture(scope=\"session\")\ndef qe_obj(ann_index_dir):\n    # main(test_data_dir, ann_index_dir, weight_file=word_wt)\n    return QE(\n        QEXP_MODEL_NAME.value, **QexpConfig.INIT_ARGS\n    )\n\n\n# @pytest.fixture(scope=\"session\")\n# def qe_mlm_obj():\n#     return QE(QEXP_MODEL_NAME.value, QexpConfig.INIT_ARGS[\"qe_files_dir\"], \"mlm\")\n\n\n@pytest.fixture(scope=\"session\")\ndef topn():\n    return 2\n\nimport pytest\n\nlogger = logging.getLogger(__name__)\n\n\ndef check(expanded, exp_len):\n    return 1 <= len(expanded) <= exp_len\n\n\ndef test_qe_emb_expand(qe_obj, topn):\n    q_str = \"security clearance\"\n    exp = qe_obj.expand(q_str, topn=topn, threshold=0.2, min_tokens=3)\n    logger.info(exp)\n    assert check(exp, topn)\n\n\ndef test_qe_emb_empty(qe_obj, topn):\n    q_str = \"\"\n    exp = qe_obj.expand(q_str, topn=topn, threshold=0.2, min_tokens=3)\n    assert len(exp) == 0\n\n\ndef test_qe_emb_oov_1(qe_obj, topn):\n    q_str = \"kljljfalj\"\n    exp = qe_obj.expand(q_str, topn=topn, threshold=0.2, min_tokens=3)\n    assert len(exp) == 0\n\n\ndef test_qe_emb_iv_2(qe_obj, topn):\n    q_str = \"financial reporting\"\n    exp = qe_obj.expand(q_str, topn=topn, threshold=0.2, min_tokens=3)\n    assert check(exp, topn)\n\n\n# @pytest.mark.parametrize(\n#     \"args\",\n#     [\n#         [\"passport\", []],\n#         [\n#             \"Find a book, painting, or work of art created in Santa Monica or on the west coast\",\n#             [\"sculpture\", \"piece\"],\n#         ],  # noqa\n#         [\"telework policy for remote work\", []],\n#         [\"telework policy work\", [\"public\"]],\n#     ],\n# )\n# def test_qe_mlm(topn, qe_mlm_obj, args):\n#     query, expected = args\n#     actual = qe_mlm_obj.expand(query, topn=topn, threshold=0.2, min_tokens=3)\n#     assert actual == expected\n"}
{"type": "test_file", "path": "gamechangerml/src/text_classif/tests/conftest.py", "content": "# flake8: noqa\n# pylint: skip-file\n\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport pytest\n\nfrom gamechangerml.src.text_classif.bert_classifier import BertClassifier\nfrom gamechangerml.src.text_classif.roberta_classifier import RobertaClassifier\nfrom gamechangerml.src.text_classif.utils.log_init import initialize_logger\n\ninitialize_logger(to_file=True, log_name=\"cola-test\")\nlogger = logging.getLogger(__name__)\n\ntry:\n    here = os.path.dirname(os.path.realpath(__file__))\n    p = Path(here)\n    test_data_path = os.path.join(p, \"test_data\")\n    test_data_dir = os.path.join(test_data_path, \"cola_public\", \"raw\")\n    assert os.path.isdir(test_data_dir)\nexcept (AttributeError, FileExistsError) as e:\n    logger.exception(\"{}: {}\".format(type(e), str(e)), exc_info=True)\n\n\n@pytest.fixture(scope=\"session\")\ndef cola_train_data():\n    df = pd.read_csv(\n        os.path.join(test_data_dir, \"in_domain_train.tsv\"),\n        delimiter=\"\\t\",\n        header=None,\n        names=[\"src\", \"label\", \"l_notes\", \"sentence\"],\n    )\n    sents = df.sentence.values\n    labels = df.label.values\n    return sents, labels\n\n\n@pytest.fixture(scope=\"session\")\ndef cola_train_small(cola_train_data):\n    sents, labels = cola_train_data\n    return sents[:1000], labels[:1000]\n\n\n@pytest.fixture(scope=\"session\")\ndef small_epochs():\n    return 2\n\n\n@pytest.fixture(scope=\"session\")\ndef cola_val_data():\n    df = pd.read_csv(\n        os.path.join(test_data_dir, \"in_domain_dev.tsv\"),\n        delimiter=\"\\t\",\n        header=None,\n        names=[\"src\", \"label\", \"l_notes\", \"sentence\"],\n    )\n    df.sample(frac=1).reset_index(drop=True)\n    sents = df.sentence.values\n    labels = df.label.values\n    return sents, labels\n\n\n@pytest.fixture(scope=\"session\")\ndef good_roberta_config():\n    return os.path.join(test_data_path, \"test_roberta_cola.yml\")\n\n\n@pytest.fixture(scope=\"session\")\ndef roberta_config_4e():\n    return os.path.join(test_data_path, \"test_roberta_cola_4.yml\")\n\n\n@pytest.fixture(scope=\"session\")\ndef bert_config_2e():\n    return os.path.join(test_data_path, \"test_bert_cola.yml\")\n\n\n@pytest.fixture(scope=\"session\")\ndef bad_config():\n    return os.path.join(test_data_path, \"test_roberta_bad.yml\")\n\n\n@pytest.fixture(scope=\"session\")\ndef bert_obj(bert_config_2e):\n    return BertClassifier(bert_config_2e)\n\n\n@pytest.fixture(scope=\"session\")\ndef roberta_obj(good_roberta_config):\n    return RobertaClassifier(good_roberta_config)\n\n\n@pytest.fixture(scope=\"session\")\ndef roberta_obj_4e(roberta_config_4e):\n    return RobertaClassifier(roberta_config_4e)\n"}
{"type": "test_file", "path": "gamechangerml/src/utilities/test_utils.py", "content": "import os\nimport re\nimport json\nimport pandas as pd\nimport math\nimport numpy as np\nfrom dateutil import parser\nfrom datetime import date, datetime\nimport signal\nimport torch\nimport random\nimport logging\n\nfrom gamechangerml.configs import ValidationConfig\n\nMATAMO_DIR = ValidationConfig.DATA_ARGS['matamo_dir']\nSEARCH_HIST = ValidationConfig.DATA_ARGS['search_hist_dir']\n\nMATAMO_TEST_FILE = \"gamechangerml/data/test_data/MatamoFeedback_TEST.csv\"\nSEARCH_TEST_FILE = \"gamechangerml/data/test_data/SearchPDFMapping_TEST.csv\"\nlogger = logging.getLogger(__name__)\n\n\n# https://stackoverflow.com/questions/25027122/break-the-function-after-certain-time/25027182\nclass TimeoutException(Exception):   # Custom exception class\n    pass\n\n\ndef init_timer():\n    '''Creates a timer using signal'''\n    # https://stackoverflow.com/questions/25027122/break-the-function-after-certain-time/25027182\n    def timeout_handler(signum, frame):   # Custom signal handler\n        raise TimeoutException\n    signal.signal(signal.SIGALRM, timeout_handler)\n    logger.info(\"Created timer.\")\n\n    return\n\n\ndef check_file_size(filename, path):\n    '''Returns the filesize (in bytes) of a file'''\n    return os.path.getsize(os.path.join(path, filename))\n\n# from create_embeddings.py\n\n\ndef get_user(logger):\n    '''Gets user or sets value to 'unknown' (from create_embeddings.py)'''\n    try:\n        user = os.environ.get(\"GC_USER\", default=\"root\")\n        if (user == \"root\"):\n            user = str(os.getlogin())\n    except Exception as e:\n        user = \"unknown\"\n        logger.info(\"Could not get system user\")\n        logger.info(e)\n\n\ndef save_json(filename, path, data):\n    '''Saved a json file'''\n    filepath = os.path.join(path, filename)\n    with open(filepath, \"w\") as outfile:\n        return json.dump(data, outfile, cls=NumpyJSONEncoder)\n\n\ndef open_json(filename, path):\n    '''Opens a json file'''\n    with open(os.path.join(path, filename)) as f:\n        return json.load(f)\n\n\ndef open_jsonl(filename, path):\n    '''Opens a jsonl file'''\n    with open(os.path.join(path, filename), 'r') as json_file:\n        json_list = list(json_file)\n\n    data = []\n    for json_str in json_list:\n        result = json.loads(json_str)\n        data.append(result)\n\n    return data\n\n\ndef open_txt(filepath):\n    '''Opens a txt file'''\n    with open(filepath, \"r\") as fp:\n        return fp.readlines()\n\n\ndef get_index_size(sent_index_path):\n    '''Checks the size of a sentence index by # of doc ids.'''\n    doc_ids = open_txt(os.path.join(sent_index_path, 'doc_ids.txt'))\n    return len(doc_ids)\n\n\ndef timestamp_filename(filename, extension):\n    '''Makes a filename that include a %Y-%m-%d timestamp'''\n    today = date.today()\n    formatted = '_'.join([filename, today.strftime(\"%Y%m%d\")])\n    return formatted + extension\n\n\ndef check_directory(directory):\n    '''Checks if a directory exists, if it does not makes the directory'''\n    if not os.path.exists(directory):\n        logger.info(\"Creating new directory {}\".format(directory))\n        os.makedirs(directory)\n\n    return directory\n\n\ndef make_timestamp_directory(base_dir):\n\n    now = datetime.now()\n    new_dir = os.path.join(base_dir, now.strftime(\"%Y-%m-%d_%H%M%S\"))\n    if not os.path.exists(new_dir):\n        logger.info(\"Creating new directory {}\".format(new_dir))\n        os.makedirs(new_dir)\n    else:\n        logger.info(\"Directory {} already exists.\".format(new_dir))\n\n    return new_dir\n\n# stackoverflow\n# https://stackoverflow.com/questions/50916422/python-typeerror-object-of-type-int64-is-not-json-serializable\n\n\nclass NumpyJSONEncoder(json.JSONEncoder):\n    \"\"\" Custom encoder for numpy data types \"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n                            np.int16, np.int32, np.int64, np.uint8,\n                            np.uint16, np.uint32, np.uint64)):\n\n            return int(obj)\n\n        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n            return float(obj)\n\n        elif isinstance(obj, (np.complex_, np.complex64, np.complex128)):\n            return {'real': obj.real, 'imag': obj.imag}\n\n        elif isinstance(obj, (np.ndarray,)):\n            return obj.tolist()\n\n        elif isinstance(obj, (np.bool_)):\n            return bool(obj)\n\n        elif isinstance(obj, (np.void)):\n            return None\n\n        return json.JSONEncoder.default(self, obj)\n\n\ndef clean_nans(value):\n    '''Replaces null value with 0'''\n    if value == None or math.isnan(value):\n        return 0\n    else:\n        return value\n\n# Evaluation utility functions\n\n\ndef get_most_recent_eval(directory):\n    '''Gets the most recent eval json from a directory'''\n    files = [f for f in os.listdir(directory) if os.path.isfile(\n        os.path.join(directory, f))]\n    evals = [f for f in files if f.split('.')[-1] == 'json']\n    if evals:\n        evals.sort(key=lambda x: int(\n            x.split('_')[-1].split('.')[0].replace('-', '')))\n        return evals[-1]\n    else:\n        return ''\n\n\ndef collect_evals(directory):\n    '''Checks if a model directory has any evaluations'''\n    sub_dirs = [d for d in os.listdir(\n        directory) if os.path.isdir(os.path.join(directory, d))]\n    eval_dirs = [os.path.join(directory, d)\n                 for d in sub_dirs if d.split('_')[0] == 'evals']\n    if not eval_dirs:\n        return {}\n    else:\n        evaldict = {}\n        for i in eval_dirs:\n            name = i.split('_')[1]\n            file = get_most_recent_eval(i)\n            if file != '':\n                evaldict[name] = open_json(file, i)\n            else:\n                evaldict[name] = {}\n        return evaldict\n\n\ndef collect_sent_evals_gc(index_path):\n    '''gets evals for index'''\n    eval_dict = {}\n    subdict = {}\n    evals_path = os.path.join(index_path, 'evals_gc')\n    logger.info(f\"evals path: {evals_path}\")\n    for level in ['gold', 'silver']:\n        fullpath = os.path.join(evals_path, level)\n        file = get_most_recent_eval(fullpath)\n        logger.info(f\"file: {file}\")\n        if file != '':\n            subdict[level] = open_json(file, fullpath)\n        else:\n            subdict[level] = ''\n\n    eval_dict[\"gc\"] = subdict\n    return eval_dict\n\n\ndef handle_sent_evals(index_path):\n    try:\n        return collect_sent_evals_gc(index_path)\n    except Exception as e:\n        logger.warning(e)\n        return collect_evals(index_path)\n\n# from sentence_transformers==2.0.0\n# https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py\n\n\ndef cos_sim(a, b):\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(a)\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(b)\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n    return torch.mm(a_norm, b_norm.transpose(0, 1))\n\n\ndef update_dictionary(old_dict, new_additions, prefix):\n    '''Update master dictionary of unique queries'''\n\n    def make_ids(new_additions, last_count, prefix):\n        '''Make UUIDs for new queries/docs'''\n\n        new_dict = {}\n        for i in new_additions:\n            if i not in old_dict.values():\n                last_count += 1\n                myid = str(last_count)\n                add = str(0) * (7 - len(myid))\n                myid = prefix + add + myid\n                new_dict[myid] = i\n\n        return new_dict\n\n    if old_dict != {}:\n        last_count = [re.sub(r'[A-Z]', '', i) for i in old_dict.keys()][-1]\n    else:\n        last_count = -1\n    new_dict = make_ids(new_additions, last_count, prefix)\n\n    return {**old_dict, **new_dict}\n\n\ndef map_ids(iddict, df, mapcol, idcol):\n    '''Map IDs back to df'''\n\n    reverse = {iddict[k]: k for k in iddict.keys()}\n    col = 'ID_' + idcol\n    df[col] = df[mapcol].map(reverse)\n\n    return df\n\n\ndef update_meta_relations(metadata, df, query_col, return_col):\n    '''Update dict with relations and metadata about each match'''\n\n    df = df.sort_values(\n        by=['date'], ascending=False).sort_values(by=['ID_key'])\n\n    for x in df['ID_key'].unique():\n        subset = df[df['ID_key'] == x].copy()\n        for i in subset['ID_value'].unique():\n            subsubset = subset[subset['ID_value'] == i]\n            exact_matches = []\n            for k in subsubset.index:\n                em = {}\n                em['exact_query'] = subsubset.loc[k, query_col]\n                em['exact_result'] = subsubset.loc[k, return_col]\n                em['source'] = subsubset.loc[k, 'source']\n                em['date'] = subsubset.loc[k, 'date']\n                exact_matches.append(em)\n\n            if x in metadata.keys() and i in metadata[x]:\n                metadata[x][i]['exact_matches'].extend(exact_matches)\n            else:\n                matchdict = {}\n                matchdict['correct_match'] = subset['correct_match'].all()\n                matchdict['last_match_date'] = list(subset['date'])[0]\n                matchdict['exact_matches'] = exact_matches\n\n            if x in metadata.keys():\n                metadata[x][i] = matchdict\n            else:\n                searchdict = {}\n                searchdict[i] = matchdict\n                metadata[x] = searchdict\n\n            metadata[x][i]['times_matched'] = len(\n                metadata[x][i]['exact_matches'])\n\n    return metadata\n\n\ndef filter_rels(metadata, min_correct_matches, max_results):\n    '''Filter relations by criteria'''\n\n    correct_rels = {}\n    incorrect_rels = {}\n    logger.info(\n        f\"Generating data for {str(len(metadata))} queries with {str(max_results)} max results and {str(min_correct_matches)} min correct matches\")\n    for key in metadata:\n        acceptable_positive_results = []\n        negative_results = []\n        # if we have more than n max results, skip this match\n        if max_results and len(metadata[key]) > max_results:\n            logger.info(\n                f\"Skipping {key}: has {str(len(metadata[key]))} unique matches\")\n            continue\n        for match in metadata[key]:\n            result = metadata[key][match]\n            sources = [i['source'] for i in result['exact_matches']]\n            if result['correct_match'] == True:\n                if 'matamo' in sources:  # we trust matamo data\n                    acceptable_positive_results.append(match)\n                # only pull history matches occurring more than x times\n                elif result['times_matched'] >= min_correct_matches:\n                    acceptable_positive_results.append(match)\n                else:\n                    logger.info(\n                        f\"Skipping {key}, {match}: matched {str(result['times_matched'])} times\")\n            elif result['correct_match'] == False:\n                negative_results.append(match)\n\n        if acceptable_positive_results != []:\n            correct_rels[key] = acceptable_positive_results\n        if negative_results != []:\n            incorrect_rels[key] = negative_results\n\n    logger.info(f\"Generated {str(len(correct_rels))} correct queries\")\n    logger.info(f\"Generated {str(len(incorrect_rels))} incorrect queries\")\n\n    return correct_rels, incorrect_rels\n\n\ndef convert_timestamp_to_datetime(timestamp):\n    return pd.to_datetime(parser.parse(timestamp).strftime(\"%Y-%m-%d\"))\n\n# filter users and dates when csv read in\n\n\ndef filter_date_range(df, start_date, end_date):\n    if 'createdAt' in df.columns:\n        timecol = 'createdAt'\n    elif 'searchtime' in df.columns:\n        timecol = 'searchtime'\n    df['dt'] = df[timecol].apply(lambda x: convert_timestamp_to_datetime(x))\n    logger.info(\n        f\"Available date range: {str(min(df['dt']))} - {str(max(df['dt']))}\")\n    subset = df.copy()\n    if start_date:\n        subset = subset[subset['dt'] >= pd.to_datetime(start_date)]\n    if end_date:\n        subset = subset[subset['dt'] <= pd.to_datetime(end_date)]\n    logger.info(\n        f\"New date range: {str(min(subset['dt']))} - {str(max(subset['dt']))}\")\n    return subset\n\n\ndef concat_csvs(directory):\n    '''Combines csvs in directory into one df; drops entirely null columns'''\n    df = pd.DataFrame()\n    logger.info(str(directory))\n    csvs = [i for i in os.listdir(directory) if i.split('.')[-1] == 'csv']\n    csvs = [i for i in csvs if i[:2] != '._']\n    logger.info(f\"Combining csvs: {str(csvs)}\")\n    for i in csvs:\n        try:\n            f = pd.read_csv(os.path.join(directory, i))\n            df = pd.concat([df, f])\n        except Exception as e:\n            logger.warning(e)\n            pass\n    return df\n\n\ndef concat_matamo(testing_only=False):\n    if testing_only:\n        return pd.read_csv(MATAMO_TEST_FILE)\n    else:\n        return concat_csvs(MATAMO_DIR)\n\n\ndef concat_search_hist(testing_only=False):\n    if testing_only:\n        return pd.read_csv(SEARCH_TEST_FILE)\n    else:\n        return concat_csvs(SEARCH_HIST)\n\n\ndef get_most_recent_dir(parent_dir):\n\n    subdirs = [os.path.join(parent_dir, d) for d in os.listdir(\n        parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]\n    if len(subdirs) > 0:\n        return max(subdirs, key=os.path.getctime)\n    else:\n        logger.error(\n            \"There are no subdirectories to retrieve most recent data from\")\n        return None\n\n\ndef make_test_corpus(\n    corpus_dir,  # main corpus dir\n    save_dir,  # where to save the test corpus\n    percent_random,  # float from 0-1 percentage of index to make from random docs\n    max_size=1000,  # max size of the index (to save on time building)\n    include_ids=None,  # if any IDs need to be in the test, pass as list\n    max_file_size=100000  # max size of random files to add to the test corpus\n):\n    '''Makes a small test corpus for checking validation'''\n    all_files = [f.split('.json')[0] + '.json' for f in os.listdir(corpus_dir)\n                 if os.path.isfile(os.path.join(corpus_dir, f))]\n    if percent_random > 1:\n        percent_random = percent_random / 100\n    if include_ids:\n        logger.info(f\"{str(len(include_ids))} ids required in test corpus\")\n        # make sure json at end of filenames\n        include_ids = [f.split('.json')[0] + '.json' for f in include_ids]\n        # only get ids in the main corpus\n        subset = list(set(all_files).intersection(include_ids))\n        if len(subset) < len(include_ids):\n            logger.info(\n                f\"Did not find all required ids in the main corpus dir.\")\n            logger.info(\n                f\"Found {str(len(subset))} / {str(len(include_ids))} ids\")\n        other = [i for i in all_files if i not in include_ids]\n        if percent_random > 0:\n            num_add = round(len(subset)/percent_random - len(subset))\n        else:\n            num_add = 0\n    else:\n        subset = []\n        other = all_files\n        num_add = max_size\n\n    # add random docs\n    for i in range(num_add):\n        filesize = 1000000\n        while filesize > max_file_size:  # as we iterate, skip large files\n            random_index = random.randint(0, len(other)-1)\n            file = other[random_index]  # pick a random file\n            # if filesize is smaller than max, break loop\n            filesize = check_file_size(file, corpus_dir)\n        subset.append(file)\n        subset = list(set(subset))  # remove duplicates\n\n    logger.info(f\"Collected {str(len(subset))} jsons\")\n    return subset\n"}
{"type": "test_file", "path": "gamechangerml/src/featurization/keywords/qe_mlm/tests/conftest.py", "content": "import spacy\nimport pytest\n\nfrom gamechangerml.src.featurization.keywords.qe_mlm.qe import QeMLM\n\n\n@pytest.fixture(scope=\"session\")\ndef qe_mlm():\n    nlp = spacy.load(\"en_core_web_md\")\n    qe = QeMLM(nlp, model_path=\"bert-base-uncased\")\n    return qe\n"}
{"type": "test_file", "path": "gamechangerml/src/search/evaluation/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "gamechangerml/src/search/evaluation/tests/test_ablation.py", "content": "import os\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef test_ablation(ablation_model,\n                  expected_values):\n    answer = ablation_model.model_scores\n    _, expected_ranks = expected_values\n\n    assert answer == expected_ranks"}
{"type": "test_file", "path": "gamechangerml/src/search/ranking/test_ltr.py", "content": "from gamechangerml.src.search.ranking.ltr import LTR\nfrom gamechangerml.src.utilities.user_utils import normalize\nimport pandas as pd\nimport numpy as np\n\nltr = LTR()\njudgements = None\n\n\ndef test_read_mappings():\n    mappings = ltr.read_mappings()\n    assert isinstance(mappings, pd.DataFrame)\n\n\ndef test_generate_judgement_exists():\n    judgements = ltr.generate_judgement(ltr.mappings[:10])\n    assert isinstance(judgements, pd.DataFrame)\n\n\ndef test_construct_query():\n    query = ltr.construct_query(\"AFI 36-2110.pdf\", \"veteran\")\n    actual = {\n        \"_source\": [\"filename\", \"fields\"],\n        \"query\": {\n            \"bool\": {\n                \"filter\": [\n                    {\"terms\": {\"filename\": [\"AFI 36-2110.pdf\"]}},\n                    {\n                        \"sltr\": {\n                            \"_name\": \"logged_featureset\",\n                            \"featureset\": \"doc_features\",\n                            \"params\": {\"keywords\": \"veteran\"},\n                        }\n                    },\n                ]\n            }\n        },\n        \"ext\": {\n            \"ltr_log\": {\n                \"log_specs\": {\"name\": \"log_entry1\", \"named_query\": \"logged_featureset\"}\n            }\n        },\n    }\n    assert query == actual\n\n\ndef test_process_ltr_log():\n    log = [\n        [\n            {\n                \"_index\": \"gamechanger_20211014\",\n                \"_type\": \"_doc\",\n                \"_id\": \"faa5c0e8e4d9dca2f4e05838775c31959e144c0dbfc97b6eaa0c6edc206515d7\",\n                \"_score\": 0.0,\n                \"_source\": {\"filename\": \"AFI 17-130.pdf\"},\n                \"fields\": {\n                    \"_ltrlog\": [\n                        {\n                            \"log_entry1\": [\n                                {\"name\": \"title\", \"value\": 2.0},\n                                {\"name\": \"keyw_5\"},\n                                {\"name\": \"textlength\", \"value\": 21.0},\n                                {\"name\": \"paragraph\", \"value\": 6.8133063},\n                            ]\n                        }\n                    ]\n                },\n                \"matched_queries\": [\"logged_featureset\"],\n            }\n        ],\n        [\n            {\n                \"_index\": \"gamechanger_20211014\",\n                \"_type\": \"_doc\",\n                \"_id\": \"7ef50e434c0fb42da31d7b720c13614e47640ed0e6e6c59607e2e805bf66b087\",\n                \"_score\": 0.0,\n                \"_source\": {\"filename\": \"AFI 99-103.pdf\"},\n                \"fields\": {\n                    \"_ltrlog\": [\n                        {\n                            \"log_entry1\": [\n                                {\"name\": \"title\"},\n                                {\"name\": \"keyw_5\"},\n                                {\"name\": \"textlength\", \"value\": 123.0},\n                                {\"name\": \"paragraph\", \"value\": 4.470147},\n                            ]\n                        }\n                    ]\n                },\n                \"matched_queries\": [\"logged_featureset\"],\n            }\n        ],\n        [\n            {\n                \"_index\": \"gamechanger_20211014\",\n                \"_type\": \"_doc\",\n                \"_id\": \"b8cc78f957169bf7c3595c171a53aa6fccb1c108b5972a5ee39abe334043bec5\",\n                \"_score\": 0.0,\n                \"_source\": {\"filename\": \"AR 25-2.pdf\"},\n                \"fields\": {\n                    \"_ltrlog\": [\n                        {\n                            \"log_entry1\": [\n                                {\"name\": \"title\", \"value\": 2.0},\n                                {\"name\": \"keyw_5\"},\n                                {\"name\": \"textlength\", \"value\": 57.0},\n                                {\"name\": \"paragraph\", \"value\": 7.5755434},\n                            ]\n                        }\n                    ]\n                },\n                \"matched_queries\": [\"logged_featureset\"],\n            }\n        ],\n        [\n            {\n                \"_index\": \"gamechanger_20211014\",\n                \"_type\": \"_doc\",\n                \"_id\": \"43d5a144894034d8beb8a83e3d1809310de1c8bcf8a1446838637d2e27ee707c\",\n                \"_score\": 0.0,\n                \"_source\": {\"filename\": \"ATP 6-01.1.pdf\"},\n                \"fields\": {\n                    \"_ltrlog\": [\n                        {\n                            \"log_entry1\": [\n                                {\"name\": \"title\"},\n                                {\"name\": \"keyw_5\"},\n                                {\"name\": \"textlength\", \"value\": 146.0},\n                                {\"name\": \"paragraph\", \"value\": 2.7437596},\n                            ]\n                        }\n                    ]\n                },\n                \"matched_queries\": [\"logged_featureset\"],\n            }\n        ],\n    ]\n    log = ltr.process_ltr_log(log)\n    print(log)\n    expected = [\n        [2.0, 0, 21.0, 6.8133063],\n        [0, 0, 123.0, 4.470147],\n        [2.0, 0, 57.0, 7.5755434],\n        [0, 0, 146.0, 2.7437596],\n    ]\n\n    assert np.array_equal(expected, log)\n\n\ndef test_normalize():\n    norm = normalize(np.array([1, 3, 4, 5]))\n    assert np.array_equal(\n        norm.tolist(), [0.0, 2.7304247779439415, 3.4454124645871445, 4.0]\n    )\n"}
{"type": "test_file", "path": "gamechangerml/src/search/sent_transformer/tests/model_test_encode.py", "content": "import logging\nimport os\nimport pytest\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_sent_encode(sent_encoder, sent_dirs, index_files):\n    \"\"\"\n    Test for encoding a corpus folder\n    \"\"\"\n    data_dir, data_dir_2, index_dir = sent_dirs\n    sent_encoder.index_documents(data_dir, index_dir)\n\n    for file in index_files:\n        fpath = os.path.join(index_dir, file)\n        assert os.path.isfile(fpath)\n\n    embedder_ids = sent_encoder.embedder.config[\"ids\"]\n\n    assert len(embedder_ids) == 82\n\n\ndef test_sent_merge(sent_encoder, sent_dirs, index_files):\n    \"\"\"\n    Test for encoding new documents\n    \"\"\"\n    data_dir, data_dir_2, index_dir = sent_dirs\n    sent_encoder.index_documents(data_dir_2, index_dir)\n\n    for file in index_files:\n        fpath = os.path.join(index_dir, file)\n        assert os.path.isfile(fpath)\n\n    embedder_ids = sent_encoder.embedder.config[\"ids\"]\n\n    assert len(embedder_ids) == 79\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/redisdriver.py", "content": "import os\nimport redis\nimport json\n\nfrom gamechangerml.api.utils.logger import logger\n\nREDIS_HOST = os.environ.get(\"REDIS_HOST\", default=\"localhost\")\nREDIS_PORT = os.environ.get(\"REDIS_PORT\", default=\"6379\")\nif REDIS_HOST == \"\":\n    REDIS_HOST = \"localhost\"\nif REDIS_PORT == \"\":\n    REDIS_PORT = 6379\n\n# An easy variable interface for redis.\n# Takes in a string key and an optional boolean hash which point\n# to an index in redis and declar if it is a dictionary or not.\n# Once initialized use get and set .value with and equals sign.\n# Eg: latest_intel_model_sent.value = \"foo\"\n\n\n\n\n# A singleton class that creates a connection pool with redis.\n# All cache variables use this one connection pool.\nclass RedisPool:\n    __pool = None\n\n    @staticmethod\n    def getPool():\n        \"\"\"Static access method.\"\"\"\n        if RedisPool.__pool == None:\n            RedisPool()\n        return RedisPool.__pool\n\n    def __init__(self):\n        \"\"\"Virtually private constructor.\"\"\"\n        if RedisPool.__pool != None:\n            logger.info(\"Using redis pool singleton\")\n        else:\n            try:\n                RedisPool.__pool = redis.ConnectionPool(\n                    host=REDIS_HOST, port=int(REDIS_PORT), db=0, decode_responses=True\n                )\n            except Exception as e:\n                logger.error(\n                    \" *** Unable to connect to redis {REDIS_HOST} {REDIS_PORT}***\"\n                )\n                logger.error(e)\n\nredisConnection = redis.Redis(connection_pool=RedisPool().getPool())\n\nclass CacheVariable:\n    def __init__(self, key, encode=False):\n        self._connection = redisConnection\n        self._key = key\n        self._encode = encode\n        self.test_value = None\n\n    # Default get method, checks if the key is in redis and gets\n    # the value whether it is a list, dict or standard type\n    def get_value(self):\n        try:\n            if self._connection.exists(self._key):\n                result = self._connection.get(self._key)\n                if self._encode:\n                    result = json.loads(result)\n                return result\n            return None\n        except Exception as e:\n            print(e)\n            return self.test_value\n\n    # Default set method, sets values for dicts and standard types.\n    # Note: Should use push if using a list.\n    def set_value(self, value, expire=None):\n        try:\n            if self._encode:\n                value = json.dumps(value)\n            if expire:\n                self._connection.set(self._key, value)\n                self._connection.expireat(self._key, expire)\n            else:\n                self._connection.set(self._key, value)\n        except Exception as e:\n            print(e)\n            self.test_value = value\n\n    # Default delete method, removes key from redis\n    def del_value(self):\n        return self._connection.delete(self._key)\n\n    value = property(get_value, set_value, del_value)"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/__init__.py", "content": "from pathlib import Path\n\nPACKAGE_ROOT = Path(__file__).parent\nREPO_ROOT = PACKAGE_ROOT.parent.parent\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/model_config.py", "content": "import os\nfrom . import REPO_ROOT\n\nclass Config:\n    # ML MODEL PATH\n    LOCAL_PACKAGED_MODELS_DIR = os.environ.get(\n        \"GC_ML_API_MODEL_PARENT_DIR\", default=REPO_ROOT.joinpath(\"models\")\n    )\n    S3_MODELS_DIR = \"models/v3/\"\n"}
{"type": "source_file", "path": "gamechangerml/api/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/__init__.py", "content": "import os\n\n# abs path to this package\nPACKAGE_PATH: str = os.path.dirname(os.path.abspath(__file__))\nREPO_PATH: str = os.path.abspath(os.path.join(PACKAGE_PATH, \"..\"))\nDATA_PATH: str = os.path.join(PACKAGE_PATH, \"data\")\nMODEL_PATH: str = os.path.join(PACKAGE_PATH, \"models\")\nCORPUS_PATH: str = os.environ.get(\n    \"LOCAL_CORPUS_PATH\", default=os.path.join(PACKAGE_PATH, \"corpus\")\n)\nNLTK_DATA_PATH: str = os.path.join(DATA_PATH, \"nltk_data\")\nAGENCY_DATA_PATH: str = os.path.join(DATA_PATH, \"agencies\")\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/mlapp.py", "content": "import faulthandler\nfrom fastapi import FastAPI\n\nfrom gamechangerml.api.fastapi.routers import startup, search, controls\nfrom gamechangerml.debug.debug_connector import debug_if_flagged\n\n# start debugger if flagged\ndebug_if_flagged()\n\n# start API\napp = FastAPI()\nfaulthandler.enable()\n\napp.include_router(\n    startup.router\n)\napp.include_router(\n    search.router,\n    tags=[\"Search\"]\n)\napp.include_router(\n    controls.router,\n    tags=[\"API Controls\"]\n)\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/model_loader.py", "content": "import os\nfrom gamechangerml.src.search.QA.QAReader import DocumentReader as QAReader\nfrom gamechangerml.configs import (\n    QAConfig,\n    EmbedderConfig,\n    DocCompareEmbedderConfig,\n    SimilarityConfig,\n    DocCompareSimilarityConfig,\n    QexpConfig,\n    TopicsConfig,\n)\nfrom gamechangerml.src.search.query_expansion import qe\nfrom gamechangerml.src.search.sent_transformer.model import (\n    SentenceSearcher,\n    SentenceEncoder,\n)\nfrom gamechangerml.src.search.doc_compare import (\n    DocCompareSentenceEncoder,\n    DocCompareSentenceSearcher,\n)\nfrom gamechangerml.src.recommender.recommend import Recommender\nfrom gamechangerml.src.search.embed_reader import sparse\nfrom gamechangerml.api.fastapi.settings import (\n    logger,\n    TOPICS_MODEL,\n    MODEL_LOAD_FLAG,\n    QEXP_JBOOK_MODEL_NAME,\n    QEXP_MODEL_NAME,\n    WORD_SIM_MODEL,\n    LOCAL_TRANSFORMERS_DIR,\n    SENT_INDEX_PATH,\n    DOC_COMPARE_SENT_INDEX_PATH,\n    latest_intel_model_encoder,\n    latest_intel_model_sim,\n    latest_intel_model_trans,\n    latest_doc_compare_sim,\n    latest_doc_compare_encoder,\n    QA_MODEL,\n)\nfrom gamechangerml.src.featurization.word_sim import WordSim\nfrom gamechangerml.src.featurization.topic_modeling import Topics\nfrom gamechangerml.api.utils import processmanager\n\n# A singleton class that loads all of the models.\n# All variables and methods are static so you\n# reference them by ModelLoader().example_method()\n\n\nclass ModelLoader:\n    # private model variables\n    def __init__(self):\n        __qa_model = None\n        __sentence_searcher = None\n        __sentence_encoder = None\n        __query_expander = None\n        __query_expander_jbook = None\n        __word_sim = None\n        __sparse_reader = None\n        __topic_model = None\n        __recommender = None\n        __document_compare_searcher = None\n        __document_compare_encoder = None\n\n    # Get methods for the models. If they don't exist try initializing them.\n    def getQA(self):\n        if ModelLoader.__qa_model == None:\n            logger.warning(\n                \"qa_model was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initQA()\n        return ModelLoader.__qa_model\n\n    def getQE(self):\n        if ModelLoader.__query_expander == None:\n            logger.warning(\n                \"query_expander was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initQE()\n        return ModelLoader.__query_expander\n\n    def getQEJbook(self):\n        if ModelLoader.__query_expander_jbook == None:\n            logger.warning(\n                \"query_expander was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initQEJBook()\n        return ModelLoader.__query_expander_jbook\n\n    def getWordSim(self):\n        if ModelLoader.__word_sim == None:\n            logger.warning(\n                \"word_sim was not set and was attempted to be used. Running init\"\n            )\n            # ModelLoader.initWordSim()\n        return ModelLoader.__word_sim\n\n    def getSentence_searcher(self):\n        if ModelLoader.__sentence_searcher == None:\n            logger.warning(\n                \"sentence_searcher was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initSentenceSearcher()\n        return ModelLoader.__sentence_searcher\n\n    def getSentence_encoder(self):\n        if ModelLoader.__sentence_encoder == None:\n            logger.warning(\n                \"sentence_encoder was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initSentenceEncoder()\n        return ModelLoader.__sentence_encoder\n\n    def getDocumentCompareSearcher(self):\n        if ModelLoader.__document_compare_searcher == None:\n            logger.warning(\n                \"document_compare_searcher was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initDocumentCompareSearcher()\n        return ModelLoader.__document_compare_searcher\n\n    def getDocumentCompareEncoder(self):\n        if ModelLoader.__document_compare_encoder == None:\n            logger.warning(\n                \"document_compare_encoder was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initDocumentCompareEncoder()\n        return ModelLoader.__document_compare_encoder\n\n    def getSparse(self):\n        return ModelLoader.__sparse_reader\n\n    def getTopicModel(self):\n        if ModelLoader.__topic_model is None:\n            logger.warning(\n                \"topic_model was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initTopics()\n        return ModelLoader.__topic_model\n\n    def getRecommender(self):\n        if ModelLoader.__recommender is None:\n            logger.warning(\n                \"recommender was not set and was attempted to be used. Running init\"\n            )\n            ModelLoader.initRecommender()\n        return ModelLoader.__recommender\n\n    def set_error(self):\n        logger.error(\"Models cannot be directly set. Must use init methods.\")\n\n    # Static variables that use these custom getters defined above.\n    # So when ModelLoader().qa_model is referenced getQA is called.\n    qa_model = property(getQA, set_error)\n    query_expander = property(getQE, set_error)\n    query_expander_jbook = property(getQEJbook, set_error)\n    sparse_reader = property(getSparse, set_error)\n    sentence_searcher = property(getSentence_searcher, set_error)\n    sentence_encoder = property(getSentence_encoder, set_error)\n    word_sim = property(getWordSim, set_error)\n    topic_model = property(getTopicModel, set_error)\n    recommender = property(getRecommender, set_error)\n    document_compare_searcher = property(getDocumentCompareSearcher, set_error)\n    document_compare_encoder = property(getDocumentCompareEncoder, set_error)\n\n    @staticmethod\n    def initQA(qa_model_name=QA_MODEL.value):\n        \"\"\"initQA - loads transformer model on start\n        Args:\n        Returns:\n        \"\"\"\n        try:\n            logger.info(\"Starting QA pipeline\")\n            ModelLoader.__qa_model = QAReader(\n                transformer_path=LOCAL_TRANSFORMERS_DIR.value,\n                use_gpu=True,\n                model_name=qa_model_name,\n                **QAConfig.MODEL_ARGS,\n            )\n            # set cache variable defined in settings.py\n            QA_MODEL.value = ModelLoader.__qa_model.READER_PATH\n            logger.info(\"Finished loading QA Reader\")\n        except OSError:\n            logger.error(f\"Could not load Question Answer Model\")\n\n    @staticmethod\n    def initQE(qexp_model_path=QEXP_MODEL_NAME.value):\n        \"\"\"initQE - loads QE model on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(f\"Loading Pretrained Vector from {qexp_model_path}\")\n        try:\n            ModelLoader.__query_expander = qe.QE(\n                qexp_model_path, **QexpConfig.INIT_ARGS\n            )\n            logger.info(\"** Loaded Query Expansion Model\")\n        except Exception as e:\n            logger.warning(\"** Could not load QE model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initQEJBook(qexp_jbook_model_path=QEXP_JBOOK_MODEL_NAME.value):\n        \"\"\"initQE - loads JBOOK QE model on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(f\"Loading Pretrained Vector from {qexp_jbook_model_path}\")\n        try:\n            ModelLoader.__query_expander_jbook = qe.QE(\n                qexp_jbook_model_path, **QexpConfig.INIT_ARGS\n            )\n            logger.info(\"** Loaded JBOOK Query Expansion Model\")\n        except Exception as e:\n            logger.warning(\"** Could not load JBOOK QE model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initWordSim(model_path=WORD_SIM_MODEL.value):\n        \"\"\"initQE - loads QE model on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(f\"Loading Word Sim Model from {model_path}\")\n        try:\n            ModelLoader.__word_sim = WordSim(model_path)\n            logger.info(\"** Loaded Word Sim Model\")\n        except Exception as e:\n            logger.warning(\"** Could not load Word Sim model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initSentenceSearcher(\n        index_path=SENT_INDEX_PATH.value, transformer_path=LOCAL_TRANSFORMERS_DIR.value\n    ):\n        \"\"\"\n        initSentenceSearcher - loads SentenceSearcher class on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(\n            f\"Loading Sentence Searcher with sent index path: {index_path}\")\n        try:\n\n            ModelLoader.__sentence_searcher = SentenceSearcher(\n                sim_model_name=SimilarityConfig.BASE_MODEL,\n                index_path=index_path,\n                transformer_path=transformer_path,\n            )\n\n            sim_model = ModelLoader.__sentence_searcher.similarity\n            # set cache variable defined in settings.py\n            latest_intel_model_sim.value = sim_model.sim_model\n            logger.info(\n                f\"** Loaded Similarity Model from {sim_model.sim_model} and sent index from {index_path}\"\n            )\n\n        except Exception as e:\n            logger.warning(\"** Could not load Similarity model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initSentenceEncoder(transformer_path=LOCAL_TRANSFORMERS_DIR.value):\n        \"\"\"\n        initSentenceEncoder - loads Sentence Encoder on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(f\"Loading encoder model\")\n        try:\n            ModelLoader.__sentence_encoder = SentenceEncoder(\n                encoder_model_name=EmbedderConfig.BASE_MODEL,\n                transformer_path=transformer_path,\n                processmanager=processmanager,\n                **EmbedderConfig.MODEL_ARGS,\n            )\n            encoder_model = ModelLoader.__sentence_encoder.encoder_model\n            # set cache variable defined in settings.py\n            latest_intel_model_encoder.value = encoder_model\n            logger.info(f\"** Loaded Encoder Model from {encoder_model}\")\n\n        except Exception as e:\n            logger.warning(\"** Could not load Encoder model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initDocumentCompareSearcher(\n        index_path=DOC_COMPARE_SENT_INDEX_PATH.value,\n        transformer_path=LOCAL_TRANSFORMERS_DIR.value,\n    ):\n        \"\"\"\n        initDocumentCompareSearcher - loads SentenceSearcher class on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(\n            f\"Loading Document Compare Searcher with index path: {index_path}\")\n        try:\n            ModelLoader.__document_compare_searcher = DocCompareSentenceSearcher(\n                sim_model_name=DocCompareSimilarityConfig.BASE_MODEL,\n                index_path=index_path,\n                transformer_path=transformer_path,\n            )\n            sim_model = ModelLoader.__document_compare_searcher.similarity\n            # set cache variable defined in settings.py\n            latest_doc_compare_sim.value = sim_model.sim_model\n            logger.info(\n                f\"** Loaded Doc Compare Similarity model from {sim_model.sim_model} and sent index from {index_path}\"\n            )\n\n        except Exception as e:\n            logger.warning(\"** Could not load Doc Compare Similarity model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initDocumentCompareEncoder(transformer_path=LOCAL_TRANSFORMERS_DIR.value):\n        \"\"\"\n        initDocumentCompareEncoder - loads Document Compare Encoder on start\n        Args:\n        Returns:\n        \"\"\"\n        logger.info(f\"Loading document compare encoder model\")\n        try:\n            ModelLoader.__document_compare_encoder = DocCompareSentenceEncoder(\n                encoder_model_name=DocCompareEmbedderConfig.BASE_MODEL,\n                transformer_path=transformer_path,\n                processmanager=processmanager,\n                **DocCompareEmbedderConfig.MODEL_ARGS,\n            )\n            encoder_model = ModelLoader.__document_compare_encoder.encoder_model\n            # set cache variable defined in settings.py\n            latest_doc_compare_encoder.value = encoder_model\n            logger.info(\n                f\"** Loaded Doc Compare Encoder Model from {encoder_model}\")\n\n        except Exception as e:\n            logger.warning(\"** Could not load Doc Compare Encoder model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initSparse(model_name=latest_intel_model_trans.value):\n        try:\n            ModelLoader.__sparse_reader = sparse.SparseReader(\n                model_name=model_name)\n            logger.info(f\"Sparse Reader: {model_name} loaded\")\n        except Exception as e:\n            logger.warning(\"** Could not load Sparse Reader\")\n            logger.warning(e)\n\n    @staticmethod\n    def initTopics(model_path=TOPICS_MODEL.value) -> None:\n        \"\"\"initTopics - load topics model on start\n        Args:\n        Returns:\n        \"\"\"\n        try:\n            logger.info(f\"Loading topic model {model_path}\")\n            logger.info(TopicsConfig.DATA_ARGS)\n            ModelLoader.__topic_model = Topics(directory=model_path)\n            logger.info(\"Finished loading Topic Model\")\n        except Exception as e:\n            logger.warning(\"** Could not load Topic model\")\n            logger.warning(e)\n\n    @staticmethod\n    def initRecommender():\n        \"\"\"initRecommender - loads recommender class on start\n        Args:\n        Returns:\n        \"\"\"\n        try:\n            logger.info(\"Starting Recommender pipeline\")\n            ModelLoader.__recommender = Recommender()\n            logger.info(\"Finished loading Recommender\")\n        except OSError:\n            logger.error(f\"** Could not load Recommender\")\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/routers/search.py", "content": "from fastapi import APIRouter, Response, status\nimport time\nimport requests\nimport base64\nimport hashlib\nimport datetime\n\n# must import sklearn first or you get an import error\nfrom gamechangerml.src.search.query_expansion.utils import remove_original_kw\nfrom gamechangerml.src.featurization.keywords.extract_keywords import get_keywords\nfrom gamechangerml.src.text_handling.process import preprocess\nfrom gamechangerml.api.fastapi.version import __version__\nfrom gamechangerml.src.utilities import gc_web_api\nfrom gamechangerml.api.utils.redisdriver import CacheVariable\n\n# from gamechangerml.models.topic_models.tfidf import bigrams, tfidf_model\n# from gamechangerml.src.featurization.summary import GensimSumm\nfrom gamechangerml.api.fastapi.settings import CACHE_EXPIRE_DAYS\nfrom gamechangerml.api.utils.logger import logger\nfrom gamechangerml.api.fastapi.model_loader import ModelLoader\n\nfrom gamechangerml.configs import QexpConfig\n\nrouter = APIRouter()\nMODELS = ModelLoader()\n\n\n@router.post(\"/transformerSearch\", status_code=200)\nasync def transformer_infer(body: dict, response: Response) -> dict:\n    \"\"\"transformer_infer - endpoint for transformer inference\n    Args:\n        body: dict; json format of query\n            {\"query\": \"test\", \"documents\": [{\"text\": \"...\", \"id\": \"xxx\"}, ...]\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        results: dict; results of inference\n    \"\"\"\n    logger.info(\"TRANSFORMER - predicting query: \" + str(body))\n    results = {}\n    try:\n        results = MODELS.sparse_reader.predict(body)\n        logger.info(results)\n    except Exception:\n        logger.error(f\"Unable to get results from transformer for {body}\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        raise\n    return results\n\n\n@router.post(\"/textExtractions\", status_code=200)\nasync def text_extract_infer(body: dict, extractType: str, response: Response) -> dict:\n    \"\"\"textExtract_infer - endpoint for sentence transformer inference\n    Args:\n        body: dict; json format of query\n            {\"text\": \"i am text\"}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n        extractType: url query string; one of topics, keywords, or summary\n    Returns:\n        results: dict; results of inference\n    \"\"\"\n    results = {}\n    try:\n        query_text = body[\"text\"]\n        results[\"extractType\"] = extractType\n        if extractType == \"topics\":\n            logger.debug(\"TOPICS - predicting query: \" + str(body))\n            topics = MODELS.topic_model.get_topics_from_text(query_text)\n            logger.info(topics)\n            results[\"extracted\"] = topics\n        elif extractType == \"summary\":\n            # gensim upgrade breaks GensimSumm class\n            # summary = GensimSumm(\n            #     query_text, long_doc=False, word_count=30\n            # ).make_summary()\n            # results[\"extracted\"] = summary\n            results[\"extracted\"] = \"Summary is not supported at this time\"\n        elif extractType == \"keywords\":\n            logger.debug(\"keywords - predicting query: \" + str(body))\n            results[\"extracted\"] = get_keywords(query_text)\n\n    except Exception:\n        logger.error(f\"Unable to get extract text for {body}\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        raise\n    return results\n\n\n@router.post(\"/transSentenceSearch\", status_code=200)\nasync def trans_sentence_infer(\n    body: dict,\n    response: Response,\n    num_results: int = 10,\n    process: bool = True,\n    externalSim: bool = False,\n    threshold=\"auto\",\n) -> dict:\n    \"\"\"trans_sentence_infer - endpoint for sentence transformer inference\n    Args:\n        body: dict; json format of query\n            {\"text\": \"i am text\"}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        results: dict; results of inference\n    \"\"\"\n    logger.info(\"SENTENCE TRANSFORMER - predicting query: \" + str(body))\n    results = {}\n    try:\n        query_text = body[\"text\"]\n        cache = CacheVariable(f\"search: {query_text}\", True)\n        cached_value = cache.get_value()\n        if cached_value:\n            logger.info(\"Searched was found in cache\")\n            results = cached_value\n        else:\n            results = MODELS.sentence_searcher.search(\n                query_text,\n                num_results,\n                process=process,\n                externalSim=False,\n                threshold=threshold,\n            )\n            cache.set_value(\n                results,\n                expire=int(\n                    (\n                        datetime.datetime.utcnow()\n                        + datetime.timedelta(days=CACHE_EXPIRE_DAYS)\n                    ).timestamp()\n                ),\n            )\n        logger.info(results)\n    except Exception:\n        logger.error(f\"Unable to get results from sentence transformer for {body}\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        raise\n    return results\n\n\n@router.post(\"/questionAnswer\", status_code=200)\nasync def qa_infer(body: dict, response: Response) -> dict:\n    \"\"\"qa_infer - endpoint for sentence transformer inference\n    Args:\n        body: dict; json format of query, text must be concatenated string\n            {\"query\": \"what is the navy\",\n            \"search_context\":[\"pargraph 1\", \"xyz\"]}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        results: dict; results of inference\n    \"\"\"\n    logger.info(\"QUESTION ANSWER - predicting query: \" + str(body[\"query\"]))\n    results = {}\n\n    try:\n        query_text = body[\"query\"]\n        query_context = body[\"search_context\"]\n        start = time.perf_counter()\n        answers = MODELS.qa_model.answer(query_text, query_context)\n        end = time.perf_counter()\n        logger.info(answers)\n        logger.info(f\"time: {end - start:0.4f} seconds\")\n        results[\"answers\"] = answers\n        results[\"question\"] = query_text\n\n    except Exception:\n        logger.error(f\"Unable to get results from QA model for {body}\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        raise\n    return results\n\n\n@router.post(\"/expandTerms\", status_code=200)\nasync def post_expand_query_terms(body: dict, response: Response) -> dict:\n    \"\"\"post_expand_query_terms - endpoint for expand query terms\n    Args:\n        body: dict; json format of query\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        expansion_dict: dict; expanded dictionary of terms\n    \"\"\"\n\n    terms_string = \" \".join(body[\"termsList\"])\n    # terms = preprocess(terms_string, remove_stopwords=True)\n    expansion_dict = {}\n    # logger.info(\"[{}] expanded: {}\".format(user, termsList))\n\n    logger.info(f\"Query Expansion on: {body}\")\n    query_expander = (\n        MODELS.query_expander\n        if body.get(\"qe_model\", \"gc_core\") != \"jbook\"\n        or MODELS.query_expander_jbook is None\n        else MODELS.query_expander_jbook\n    )\n    try:\n        terms_string = unquoted(terms_string)\n        expansion_list = query_expander.expand(\n            terms_string, **QexpConfig.EXPANSION_ARGS\n        )\n        # Pass entire query from frontend to query expansion model and return topn.\n        # Removes original word from the return terms unless it is combined with another word\n        logger.info(f\"original expanded terms: {expansion_list}\")\n        final_terms = remove_original_kw(expansion_list, terms_string)\n        expansion_dict[terms_string] = ['\"{}\"'.format(exp) for exp in final_terms]\n        logger.info(f\"-- Expanded {terms_string} to \\n {final_terms}\")\n        # Perform word similarity\n        logger.info(f\"Finding similiar words for: {terms_string}\")\n        sim_words_dict = MODELS.word_sim.most_similiar_tokens(terms_string)\n        logger.info(f\"-- Expanded {terms_string} to \\n {sim_words_dict}\")\n        # Construct return payload\n        expanded_words = {}\n        expanded_words[\"qexp\"] = expansion_dict\n        expanded_words[\"wordsim\"] = sim_words_dict\n        return expanded_words\n    except Exception as e:\n        logger.error(f\"Error with query expansion on {body}\")\n        logger.error(e)\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n\n\n@router.post(\"/wordSimilarity\", status_code=200)\nasync def post_word_sim(body: dict, response: Response) -> dict:\n    \"\"\"post_word_sim - endpoint for getting similar words\n    Args:\n        body: dict; json format of query\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        expansion_dict: dict; expanded dictionary of terms\n    \"\"\"\n    # logger.info(\"[{}] expanded: {}\".format(user, termsList))\n    terms = body[\"text\"]\n    logger.info(f\"Finding similiar words for: {terms}\")\n    try:\n        sim_words_dict = MODELS.word_sim.most_similiar_tokens(terms)\n        logger.info(f\"-- Expanded {terms} to \\n {sim_words_dict}\")\n        return sim_words_dict\n    except Exception as e:\n        logger.error(f\"Error with query expansion on {terms}\")\n        logger.error(f\"{e}\")\n\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n\n\n@router.post(\"/recommender\", status_code=200)\nasync def post_recommender(body: dict, response: Response) -> dict:\n    results = {}\n    sample = False\n    try:\n        filenames = body[\"filenames\"]\n        if not filenames:\n            if body[\"sample\"]:\n                sample = body[\"sample\"]\n        logger.info(f\"Recommending similar documents to {filenames}\")\n        results = MODELS.recommender.get_recs(filenames=filenames, sample=sample)\n        if results[\"results\"] != []:\n            logger.info(f\"Found similar docs: \\n {str(results)}\")\n        else:\n            logger.info(\"Did not find any similar docs\")\n    except Exception as e:\n        logger.warning(f\"Could not get similar docs for {filenames}\")\n        logger.warning(e)\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n\n    return results\n\n\n@router.post(\"/documentCompare\", status_code=200)\nasync def document_compare_infer(\n    body: dict,\n    response: Response,\n    num_results: int = 10,\n    process: bool = True,\n) -> dict:\n    \"\"\"document_compare_infer - endpoint for document compare inference\n    Args:\n        body: dict; json format of query\n            {\n                <str> \"text\": \"i am text\",\n                <?array[[threshold, display]] \"confidences\": optional array of 2 tuples (threshold, display) where score > threshold -> display :: default [[0.8, \"High\"], [0.5, \"Medium\"], [0.4, \"Low\"]]\n                <?float> \"cutoff\": optional cutoff to filter result scores by\n            }\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        results: dict; results of inference\n    \"\"\"\n    logger.debug(\"DOCUMENT COMPARE INFER - predicting query: \" + str(body))\n    results = {}\n    try:\n        query_text = body[\"text\"]\n        results = MODELS.document_compare_searcher.search(\n            query_text, num_results, body, process=process, externalSim=False\n        )\n        logger.info(results)\n    except Exception:\n        logger.error(\n            f\"Unable to get results from doc compare sentence transformer for {body}\"\n        )\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        raise\n    return results\n\n\ndef unquoted(term):\n    \"\"\"unquoted - unquotes string\n    Args:\n        term: string\n    Returns:\n        term: without quotes\n    \"\"\"\n    if term[0] in [\"'\", '\"'] and term[-1] in [\"'\", '\"']:\n        return term[1:-1]\n    else:\n        return term\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/mlscheduler.py", "content": "\"\"\"Utility functions for scheduling ml builds based on events\n\nAlso see gamechangerml.src.services.s3_service.py\n\"\"\"\n\nfrom threading import current_thread\nfrom os import makedirs\nfrom os.path import join, exists, basename\nfrom datetime import datetime, timezone\nfrom gamechangerml.src.services.s3_service import S3Service\nfrom gamechangerml.src.utilities import configure_logger\nfrom gamechangerml.configs import S3Config\nfrom gamechangerml.api.utils import processmanager\nfrom gamechangerml.api.fastapi.routers.controls import (\n    train_qexp,\n    train_sentence,\n)\nfrom gamechangerml.api.utils.threaddriver import MlThread\nimport os\nfrom queue import Queue\nfrom gamechangerml.src.data_transfer import download_corpus_s3\n\nfrom fastapi import APIRouter, Response\n\nfrom gamechangerml.api.fastapi.settings import (\n    CORPUS_DIR,\n    S3_CORPUS_PATH,\n    CORPUS_EVENT_TRIGGER_VAL,\n    latest_intel_model_encoder,\n)\n\n\nasync def corpus_update_event(\n    s3_corpus_dir: str,\n    corpus_dir: str = \"gamechangerml/corpus\",\n    bucket=None,\n    logger=None,\n) -> bool:\n    if logger is None:\n        logger = configure_logger()\n\n    if bucket is None:\n        bucket = S3Service.connect_to_bucket(S3Config.BUCKET_NAME, logger)\n\n    process = processmanager.ml_change_event\n\n    try:\n        logger.info(\"ML EVENT - Checking corpus staleness\")\n\n        s3_filter = bucket.objects.filter(Prefix=f\"{s3_corpus_dir}/\")\n        last_mod_list = []\n        if os.path.isdir(corpus_dir):\n            local_corpus_size = len(os.listdir(corpus_dir))\n            if local_corpus_size > 0:\n                local_corpus_last_updated = datetime.fromtimestamp(\n                    os.stat(corpus_dir).st_mtime\n                ).astimezone(timezone.utc)\n                for obj in s3_filter:\n                    last_mod_list.append(obj.last_modified)\n\n                last_mod_list = [\n                    dates\n                    for dates in last_mod_list\n                    if dates > local_corpus_last_updated\n                ]\n                ratio = len(last_mod_list) / local_corpus_size\n            else:\n                ratio = 1\n        else:\n            ratio = 1\n        if ratio > CORPUS_EVENT_TRIGGER_VAL:\n            logger.info(\"ML EVENT - Corpus is stale - downloading data\")\n            # trigger a thread to update corpus and build selected models\n            logger.info(\"Attempting to download corpus from S3\")\n\n            thread_args = {\n                \"args\": {\n                    \"logger\": logger,\n                    \"s3_args\": {\n                        \"s3_corpus_dir\": s3_corpus_dir,\n                        \"output_dir\": CORPUS_DIR,\n                        \"logger\": logger,\n                    },\n                    \"qexp_model_dict\": {\n                        \"build_type\": \"qexp\",\n                        \"upload\": True,\n                        \"version\": datetime.today().strftime(\"%Y%m%d\"),\n                    },\n                    \"sent_model_dict\": {\n                        \"build_type\": \"sentence\",\n                        \"upload\": True,\n                        \"version\": datetime.today().strftime(\"%Y%m%d\"),\n                        \"encoder_model\": str(latest_intel_model_encoder.value).split(\n                            \"/\"\n                        )[-1],\n                        \"gpu\": True,\n                    },\n                }\n            }\n\n            logger.info(thread_args)\n            ml_event_thread = MlThread(run_update, thread_args)\n            ml_event_thread.start()\n            processmanager.running_threads[ml_event_thread.ident] = ml_event_thread\n            processmanager.update_status(\n                processmanager.ml_change_event, 0, 3, thread_id=ml_event_thread.ident\n            )\n\n    except Exception:\n        logger.exception(\"Failed to update corpus or train models\")\n        processmanager.update_status(\n            process, failed=True, thread_id=current_thread().ident\n        )\n\n\ndef run_update(args):\n    logger = args[\"logger\"]\n    logger.info(\"Attempting to download corpus from S3\")\n    download_corpus_s3(**args[\"s3_args\"])\n    processmanager.update_status(\n        processmanager.ml_change_event,\n        1,\n        3,\n        thread_id=current_thread().ident,\n    )\n    logger.info(\"Attempting to build Qexp\")\n    model_dict = args[\"qexp_model_dict\"]\n    train_qexp(model_dict)\n    processmanager.update_status(\n        processmanager.ml_change_event,\n        2,\n        3,\n        thread_id=current_thread().ident,\n    )\n    logger.info(\"Attempting to build Sentence Index\")\n    model_dict = args[\"sent_model_dict\"]\n\n    train_sentence(model_dict)\n    processmanager.update_status(\n        processmanager.ml_change_event,\n        3,\n        3,\n        thread_id=current_thread().ident,\n    )\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/pathselect.py", "content": "import os\nimport logging\nfrom gamechangerml.api.fastapi.model_config import Config\n\nlogger = logging.getLogger()\n\n\ndef get_model_paths():\n    model_dict = {}\n    # QEXP MODEL\n    try:\n        qexp_names = [\n            f\n            for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR)\n            if (\"qexp_\" in f) and (all(substr not in f for substr in [\"tar\", \"jbook\"]))\n        ]\n        qexp_names.sort(reverse=True)\n        if len(qexp_names) > 0:\n            QEXP_MODEL_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, qexp_names[0]\n            )\n        else:\n            print(\"defaulting INDEX_PATH to qexp\")\n            QEXP_MODEL_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, \"qexp_20201217\"\n            )\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get QEXP model path\")\n        QEXP_MODEL_PATH = \"gamechangerml/models/\"\n\n    # QEXP JBOOK MODEL\n    try:\n        qexp_jbook_names = [\n            f\n            for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR)\n            if (all(substr in f for substr in [\"qexp_\", \"jbook\"])) and (\"tar\" not in f)\n        ]\n        qexp_jbook_names.sort(reverse=True)\n        if len(qexp_jbook_names) > 0:\n            QEXP_JBOOK_MODEL_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, qexp_jbook_names[0]\n            )\n        else:\n            print(\"defaulting INDEX_PATH to qexp\")\n            QEXP_JBOOK_MODEL_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, \"jbook_qexp_20220131\"\n            )\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get QEXP JBOOK model path\")\n        QEXP_JBOOK_MODEL_PATH = \"gamechangerml/models/\"\n\n    # TRANSFORMER MODEL PATH\n    try:\n        LOCAL_TRANSFORMERS_DIR = os.path.join(\n            Config.LOCAL_PACKAGED_MODELS_DIR, \"transformers\"\n        )\n    except Exception as e:\n        logger.error(e)\n\n        logger.info(\"Cannot get TRANSFORMER model path\")\n    # WORK SIM MODEL PATH\n    try:\n        WORD_SIM_MODEL_PATH = os.path.join(\n            LOCAL_TRANSFORMERS_DIR, \"wiki-news-300d-1M.bin\"\n        )\n    except Exception as e:\n        logger.error(e)\n\n        logger.info(\"Cannot get word sim model path\")\n\n    # SENTENCE INDEX AND DOC COMPARE INDEX\n    # get largest file name with sent_index prefix (by date)\n    try:\n        sent_index_name = [\n            f\n            for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR)\n            if (\"sent_index\" in f) and (\"tar\" not in f)\n        ]\n        sent_index_name = [\n            f\n            for f in sent_index_name\n            if os.path.isfile(\n                os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, f, \"config\")\n            )\n        ]\n        sent_index_name.sort(reverse=True)\n        if len(sent_index_name) > 0:\n            INDEX_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, sent_index_name[0]\n            )\n            DOC_COMPARE_INDEX_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, sent_index_name[0]\n            )\n        else:\n            print(\"defaulting INDEX_PATH to sent_index\")\n            INDEX_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, \"sent_index\")\n            DOC_COMPARE_INDEX_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, \"sent_index\")\n    except Exception as e:\n        logger.error(e)\n        INDEX_PATH = \"gamechangerml/models/\"\n        DOC_COMPARE_INDEX_PATH = INDEX_PATH\n        logger.info(f\"Cannot get Sentence Index model path {e}\",)\n\n    # TOPICS\n    try:\n\n        topic_model_dirs = [\n            name\n            for name in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR)\n            if \"topic_model_\" in name\n            and os.path.isdir(os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, name))\n        ]\n        topic_model_dirs.sort(reverse=True)\n\n        if len(topic_model_dirs) > 0:\n            TOPICS_PATH = os.path.join(\n                Config.LOCAL_PACKAGED_MODELS_DIR, topic_model_dirs[0]\n            )\n        else:\n            raise ValueError(\n                f\"No topic_model_<date> folders in {Config.LOCAL_PACKAGED_MODELS_DIR}\"\n            )\n\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get Topics model path\")\n        TOPICS_PATH = \"gamechangerml/models/\"\n\n    model_dict = {\n        \"transformers\": LOCAL_TRANSFORMERS_DIR,\n        \"sentence\": INDEX_PATH,\n        \"qexp\": QEXP_MODEL_PATH,\n        \"qexp_jbook\": QEXP_JBOOK_MODEL_PATH,\n        \"word_sim\": WORD_SIM_MODEL_PATH,\n        \"topics\": TOPICS_PATH,\n        \"doc_compare\": DOC_COMPARE_INDEX_PATH,\n    }\n    return model_dict\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/routers/controls.py", "content": "from concurrent.futures import thread\nfrom fastapi import APIRouter, Response, status\nimport subprocess\nimport os\nimport json\nimport tarfile\nimport shutil\nimport threading\nimport pandas as pd\nimport redis\n\nfrom datetime import datetime\nfrom gamechangerml import DATA_PATH\nfrom gamechangerml.configs.s3_config import S3Config\nfrom gamechangerml.src.utilities import utils\nfrom gamechangerml.src.utilities.es_utils import ESUtils\nfrom gamechangerml.src.services import S3Service\nfrom gamechangerml.api.fastapi.model_config import Config\nfrom gamechangerml.api.fastapi.version import __version__\n\nfrom gamechangerml.api.fastapi.settings import (\n    logger,\n    TOPICS_MODEL,\n    CORPUS_DIR,\n    QEXP_JBOOK_MODEL_NAME,\n    QEXP_MODEL_NAME,\n    WORD_SIM_MODEL,\n    LOCAL_TRANSFORMERS_DIR,\n    SENT_INDEX_PATH,\n    latest_intel_model_encoder,\n    latest_intel_model_sim,\n    latest_doc_compare_encoder,\n    latest_doc_compare_sim,\n    DOC_COMPARE_SENT_INDEX_PATH,\n    S3_CORPUS_PATH,\n    QA_MODEL,\n    ignore_files,\n)\nfrom gamechangerml.src.data_transfer import download_corpus_s3\nfrom gamechangerml.api.utils.threaddriver import MlThread\nfrom gamechangerml.api.utils.redisdriver import RedisPool\n\nfrom gamechangerml.train.pipeline import Pipeline\nfrom gamechangerml.api.utils import processmanager\nfrom gamechangerml.api.fastapi.model_loader import ModelLoader\nfrom gamechangerml.src.utilities.test_utils import (\n    collect_evals,\n    handle_sent_evals,\n)\nfrom gamechangerml import MODEL_PATH\nfrom gamechangerml.src.utilities import gc_web_api\n\nrouter = APIRouter()\nMODELS = ModelLoader()\ngcClient = gc_web_api.GCWebClient()\n## Get Methods ##\n\npipeline = Pipeline()\nes = ESUtils()\n\n\n@router.get(\"/\")\nasync def api_information():\n    return {\n        \"API_Name\": \"GAMECHANGER ML API\",\n        \"Version\": __version__,\n        \"Elasticsearch_Host\": es.root_url,\n        \"Elasticsearch_Status\": get_es_status(),\n    }\n\n\ndef get_es_status():\n    status = \"red\"\n    try:\n        res = es.get(es.root_url + \"_cluster/health\", timeout=5)\n        cont = json.loads(res.content)\n        status = cont[\"status\"]\n    except Exception as e:\n        logger.warning(e)\n\n    return status\n\n\n@router.get(\"/getProcessStatus\")\nasync def get_process_status():\n    return {\n        \"process_status\": processmanager.PROCESS_STATUS.value,\n        \"completed_process\": processmanager.COMPLETED_PROCESS.value,\n    }\n\n\n@router.post(\"/clearCache\")\nasync def clear_cache(body: dict, response: Response):\n    _connection = redis.Redis(connection_pool=RedisPool().getPool())\n\n    if body[\"clear\"]:\n        for key in body[\"clear\"]:\n            _connection.delete(f\"search: {key}\")\n    else:\n        for key in _connection.scan_iter(\"search:*\"):\n            # delete the key\n            _connection.delete(key)\n\n\n@router.get(\"/getCache\")\nasync def get_cache():\n    _connection = redis.Redis(connection_pool=RedisPool().getPool())    \n    return [key.split('search: ')[1] for key in list(_connection.scan_iter(\"search:*\"))]\n\n@router.get(\"/getDataList\")\ndef get_downloaded_data_list():\n    \"\"\"\n    Gets a list of the data in the local data folder\n    Args:\n    Returns: dict {\"dirs\":[ array of dicts {\"name\":(name of file):\"path\":(base directory), \"files\":(arr of files in directory),\"subdirectories\":(arr of subdirectories)}]}\n    \"\"\"\n    files = []\n    dir_arr = []\n    logger.info(DATA_PATH)\n\n    for dir in os.listdir(DATA_PATH):\n        temp_path = os.path.join(DATA_PATH, dir)\n        if os.path.isdir(temp_path):\n            for dirpath, dirnames, filenames in os.walk(temp_path):\n                dir_arr.append(\n                    {\n                        \"name\": dirpath.replace(temp_path, \"\"),\n                        \"path\": dir,\n                        \"files\": filenames,\n                        \"subdirectories\": dirnames,\n                    }\n                )\n\n    return {\"dirs\": dir_arr}\n\n\n@router.get(\"/getModelsList\")\ndef get_downloaded_models_list():\n    \"\"\"\n    Gets a list of the models in the local model folder\n    Args:\n    Returns:{\n        \"transformers\": (list of transformers),\n        \"sentence\": (list of sentence indexes),\n        \"qexp\": (list of query expansion indexes),\n        \"ltr\": (list of learn to rank),\n    }\n    \"\"\"\n    qexp_list = {}\n    jbook_qexp_list = {}\n    sent_index_list = {}\n    transformer_list = {}\n    topic_models = {}\n    ltr_list = {}\n    # QEXP MODEL PATH\n    try:\n        for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR):\n            if (\"qexp_\" in f) and (\"tar\" not in f):\n                qexp_list[f] = {}\n                meta_path = os.path.join(\n                    Config.LOCAL_PACKAGED_MODELS_DIR, f, \"metadata.json\"\n                )\n                if os.path.isfile(meta_path):\n                    meta_file = open(meta_path)\n                    qexp_list[f] = json.load(meta_file)\n                    qexp_list[f][\"evaluation\"] = {}\n                    qexp_list[f][\"evaluation\"] = collect_evals(\n                        os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, f)\n                    )\n                    meta_file.close()\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get QEXP model path\")\n    # JBOOK QEXP\n    try:\n        for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR):\n            if (\"jbook_qexp_\" in f) and (\"tar\" not in f):\n                jbook_qexp_list[f] = {}\n                meta_path = os.path.join(\n                    Config.LOCAL_PACKAGED_MODELS_DIR, f, \"metadata.json\"\n                )\n                if os.path.isfile(meta_path):\n                    meta_file = open(meta_path)\n                    jbook_qexp_list[f] = json.load(meta_file)\n                    jbook_qexp_list[f][\"evaluation\"] = {}\n                    jbook_qexp_list[f][\"evaluation\"] = collect_evals(\n                        os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, f)\n                    )\n                    meta_file.close()\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get Jbook QEXP model path\")\n\n    # TRANSFORMER MODEL PATH\n    try:\n        for trans in os.listdir(LOCAL_TRANSFORMERS_DIR.value):\n            if trans not in ignore_files and \".\" not in trans:\n                transformer_list[trans] = {}\n                config_path = os.path.join(\n                    LOCAL_TRANSFORMERS_DIR.value, trans, \"config.json\"\n                )\n                if os.path.isfile(config_path):\n                    config_file = open(config_path)\n                    transformer_list[trans] = json.load(config_file)\n                    transformer_list[trans][\"evaluation\"] = {}\n                    transformer_list[trans][\"evaluation\"] = handle_sent_evals(\n                        os.path.join(LOCAL_TRANSFORMERS_DIR.value, trans)\n                    )\n                    config_file.close()\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get transformer model path\")\n    # SENTENCE INDEX\n    # get largest file name with sent_index prefix (by date)\n    try:\n        for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR):\n            if (\"sent_index\" in f) and (\"tar\" not in f):\n                logger.info(f\"sent indices: {str(f)}\")\n                sent_index_list[f] = {}\n                meta_path = os.path.join(\n                    Config.LOCAL_PACKAGED_MODELS_DIR, f, \"metadata.json\"\n                )\n                if os.path.isfile(meta_path):\n                    meta_file = open(meta_path)\n                    sent_index_list[f] = json.load(meta_file)\n                    sent_index_list[f][\"evaluation\"] = {}\n\n                    sent_index_list[f][\"evaluation\"] = handle_sent_evals(\n                        os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, f)\n                    )\n                    meta_file.close()\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get Sentence Index model path\")\n\n    # TOPICS MODELS\n    try:\n\n        topic_dirs = [\n            name\n            for name in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR)\n            if os.path.isdir(os.path.join(Config.LOCAL_PACKAGED_MODELS_DIR, name))\n            and \"topic_model_\" in name\n        ]\n        for topic_model_name in topic_dirs:\n            topic_models[topic_model_name] = {}\n            try:\n                with open(\n                    os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        topic_model_name,\n                        \"metadata.json\",\n                    )\n                ) as mf:\n                    topic_models[topic_model_name] = json.load(mf)\n            except Exception as e:\n                logger.error(e)\n                topic_models[topic_model_name] = {\n                    \"Error\": \"Failed to load metadata file for this model\"\n                }\n\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get Topic model path\")\n\n    # LTR\n    try:\n        for f in os.listdir(Config.LOCAL_PACKAGED_MODELS_DIR):\n            if (\"ltr\" in f) and (\"tar\" not in f):\n                logger.info(f\"LTR: {str(f)}\")\n                ltr_list[f] = {}\n                meta_path = os.path.join(\n                    Config.LOCAL_PACKAGED_MODELS_DIR, f, \"metadata.json\"\n                )\n                if os.path.isfile(meta_path):\n                    meta_file = open(meta_path)\n                    ltr_list[f] = json.load(meta_file)\n                    meta_file.close()\n    except Exception as e:\n        logger.error(e)\n        logger.info(\"Cannot get LTR model path\")\n\n    model_list = {\n        \"transformers\": transformer_list,\n        \"sentence\": sent_index_list,\n        \"qexp\": qexp_list,\n        \"jbook_qexp\": jbook_qexp_list,\n        \"topic_models\": topic_models,\n        \"ltr\": ltr_list,\n    }\n    return model_list\n\n\n@router.post(\"/deleteLocalModel\")\nasync def delete_local_model(model: dict, response: Response):\n    \"\"\"\n    Delete a model from the local model folder\n    Args: model: dict; {\"model\":(model you want to delete), \"type\":(type of model being deleted)}\n    Returns: process statuses\n    \"\"\"\n\n    def removeDirectory(dir):\n        try:\n            logger.info(f'Removing directory {os.path.join(dir,model[\"model\"])}')\n            shutil.rmtree(os.path.join(dir, model[\"model\"]))\n        except OSError as e:\n            logger.error(e)\n\n    def removeFiles(dir):\n        for f in os.listdir(dir):\n            if model[\"model\"] in f:\n                logger.info(f\"Removing file {f}\")\n                try:\n                    os.remove(os.path.join(dir, f))\n                except OSError as e:\n                    logger.error(e)\n\n    logger.info(model)\n    if model[\"type\"] == \"transformers\":\n        removeDirectory(LOCAL_TRANSFORMERS_DIR.value)\n    elif model[\"type\"] in (\"sentence\", \"qexp\", \"doc_compare_sentence\"):\n        removeDirectory(Config.LOCAL_PACKAGED_MODELS_DIR)\n        removeFiles(Config.LOCAL_PACKAGED_MODELS_DIR)\n\n    return await get_process_status()\n\n\n@router.get(\"/LTR/initLTR\", status_code=200)\nasync def initLTR(response: Response):\n    \"\"\"generate judgement - checks how many files are in the corpus directory\n    Args:\n    Returns: integer\n    \"\"\"\n    number_files = 0\n    resp = None\n    try:\n        pipeline.init_ltr()\n    except Exception as e:\n        logger.warning(\"Could not init LTR\")\n    return resp\n\n\n@router.get(\"/LTR/createModel\", status_code=200)\nasync def create_LTR_model(response: Response):\n    \"\"\"generate judgement - checks how many files are in the corpus directory\n    Args:\n    Returns: integer\n    \"\"\"\n    number_files = 0\n    resp = None\n    model = []\n\n    def ltr_process():\n        try:\n\n            pipeline.create_ltr()\n            processmanager.update_status(\n                processmanager.ltr_creation,\n                1,\n                1,\n                thread_id=threading.current_thread().ident,\n            )\n        except Exception as e:\n            logger.warning(e)\n            logger.warning(f\"There is an issue with LTR creation\")\n            response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n            processmanager.update_status(\n                processmanager.ltr_creation,\n                failed=True,\n                thread_id=threading.current_thread().ident,\n            )\n\n    ltr_thread = MlThread(ltr_process)\n    ltr_thread.start()\n    processmanager.running_threads[ltr_thread.ident] = ltr_thread\n    processmanager.update_status(\n        processmanager.ltr_creation, 0, 1, thread_id=ltr_thread.ident\n    )\n\n    return response.status_code\n\n\n@router.get(\"/getFilesInCorpus\", status_code=200)\nasync def files_in_corpus(response: Response):\n    \"\"\"files_in_corpus - checks how many files are in the corpus directory\n    Args:\n    Returns: integer\n    \"\"\"\n    number_files = 0\n    try:\n        logger.info(\"Reading files from local corpus\")\n        number_files = len(\n            [\n                name\n                for name in os.listdir(CORPUS_DIR)\n                if os.path.isfile(os.path.join(CORPUS_DIR, name))\n            ]\n        )\n    except:\n        logger.warning(f\"Could not get files in corpus\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n    return json.dumps(number_files)\n\n\n@router.get(\"/getLoadedModels\")\nasync def get_current_models():\n    \"\"\"get_current_models - endpoint for current models\n    Args:\n    Returns:\n        dict of model name\n    \"\"\"\n    # sent_model = latest_intel_model_sent.value\n    return {\n        \"sim_model\": latest_intel_model_sim.value,\n        \"encoder_model\": latest_intel_model_encoder.value,\n        \"sentence_index\": SENT_INDEX_PATH.value,\n        \"qexp_model\": QEXP_MODEL_NAME.value,\n        \"jbook_model\": QEXP_JBOOK_MODEL_NAME.value,\n        \"topic_model\": TOPICS_MODEL.value,\n        \"wordsim_model\": WORD_SIM_MODEL.value,\n        \"qa_model\": QA_MODEL.value,\n        \"doc_compare_sim_model\": latest_doc_compare_sim.value,\n        \"doc_compare_encoder_model\": latest_doc_compare_encoder.value,\n        \"doc_compare_sentence_index\": DOC_COMPARE_SENT_INDEX_PATH.value,\n    }\n\n\n@router.get(\"/download\", status_code=200)\nasync def download(response: Response):\n    \"\"\"download - downloads dependencies from s3\n    Args:\n    Returns:\n    \"\"\"\n\n    def download_s3_thread():\n        try:\n            logger.info(\"Attempting to download dependencies from S3\")\n            output = subprocess.call([\"gamechangerml/scripts/download_dependencies.sh\"])\n            # get_transformers(overwrite=False)\n            # get_sentence_index(overwrite=False)\n            processmanager.update_status(\n                processmanager.s3_dependency,\n                1,\n                1,\n                thread_id=threading.current_thread().ident,\n            )\n        except:\n\n            logger.warning(f\"Could not get dependencies from S3\")\n            response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n            processmanager.update_status(\n                processmanager.s3_dependency,\n                failed=True,\n                thread_id=threading.current_thread().ident,\n            )\n\n    thread = MlThread(download_s3_thread)\n    thread.start()\n    processmanager.running_threads[thread.ident] = thread\n    processmanager.update_status(\n        processmanager.s3_dependency, 0, 1, thread_id=thread.ident\n    )\n    return await get_process_status()\n\n\n@router.post(\"/downloadS3File\", status_code=200)\nasync def download_s3_file(file_dict: dict, response: Response):\n    \"\"\"\n    download a s3 file from the given path. If folder is given download all files recursively from the folder and untar all .tar files\n    Args:file_dict - dict {\"file\":(file or folder path),\"type\":\"whether from ml-data or models)}\n    Returns: process status\n    \"\"\"\n\n    def download_s3_thread():\n        logger.info(f'downloading file {file_dict[\"file\"]}')\n        try:\n\n            path = (\n                \"gamechangerml/models/\"\n                if file_dict[\"type\"] == \"models\"\n                else \"gamechangerml/\"\n            )\n            bucket = S3Service.connect_to_bucket(S3Config.BUCKET_NAME, logger)\n            downloaded_files = S3Service.download(\n                bucket,\n                os.path.join(\n                    \"bronze/gamechanger\", file_dict[\"type\"], file_dict[\"file\"]\n                ),\n                path,\n                logger,\n            )\n            logger.info(downloaded_files)\n\n            if len(downloaded_files) == 0:\n                processmanager.update_status(\n                    f's3: {file_dict[\"file\"]}',\n                    failed=True,\n                    message=\"No files found\",\n                    thread_id=threading.current_thread().ident,\n                )\n                return\n\n            processmanager.update_status(\n                f's3: {file_dict[\"file\"]}',\n                0,\n                len(downloaded_files),\n                thread_id=threading.current_thread().ident,\n            )\n            i = 0\n            for f in downloaded_files:\n                i += 1\n                processmanager.update_status(\n                    f's3: {file_dict[\"file\"]}',\n                    0,\n                    i,\n                    thread_id=threading.current_thread().ident,\n                )\n                logger.info(f)\n                if \".tar\" in f:\n                    tar = tarfile.open(f)\n                    if tar.getmembers()[0].name == \".\":\n                        if \"sentence_index\" in file_dict[\"file\"]:\n                            path += \"sent_index_\"\n                        elif \"jbook_qexp_model\" in file_dict[\"file\"]:\n                            path += \"jbook_qexp_\"\n                        elif \"qexp_model\" in file_dict[\"file\"]:\n                            path += \"qexp_\"\n                        elif \"topic_model\" in file_dict[\"file\"]:\n                            path += \"topic_models\"\n\n                        path += f.split(\"/\")[-1].split(\".\")[0]\n\n                    logger.info(f\"Extracting {f} to {path}\")\n                    tar.extractall(\n                        path=path,\n                        members=[\n                            member\n                            for member in tar.getmembers()\n                            if (\n                                \".git\" not in member.name\n                                and \".DS_Store\" not in member.name\n                            )\n                        ],\n                    )\n                    tar.close()\n\n            processmanager.update_status(\n                f's3: {file_dict[\"file\"]}',\n                len(downloaded_files),\n                len(downloaded_files),\n                thread_id=threading.current_thread().ident,\n            )\n\n        except PermissionError:\n            failedExtracts = []\n            for member in tar.getmembers():\n                try:\n                    tar.extract(member, path=path)\n                except Exception as e:\n                    failedExtracts.append(member.name)\n\n            logger.warning(f\"Could not extract {failedExtracts} with permission errors\")\n            processmanager.update_status(\n                f's3: {file_dict[\"file\"]}',\n                failed=True,\n                message=\"Permission error not all files extracted\",\n                thread_id=threading.current_thread().ident,\n            )\n\n        except Exception as e:\n            logger.warning(e)\n            logger.warning(f\"Could download {file_dict['file']} from S3\")\n            response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n            processmanager.update_status(\n                f's3: {file_dict[\"file\"]}',\n                failed=True,\n                message=e,\n                thread_id=threading.current_thread().ident,\n            )\n\n    thread = MlThread(download_s3_thread)\n    thread.start()\n    processmanager.running_threads[thread.ident] = thread\n    processmanager.update_status(\n        f's3: {file_dict[\"file\"]}', 0, 1, thread_id=thread.ident\n    )\n\n    return await get_process_status()\n\n\n@router.get(\"/s3\", status_code=200)\nasync def s3_func(function, response: Response):\n    \"\"\"s3_func - s3 functionality for model managment\n    Args:\n        function: str\n    Returns:\n    \"\"\"\n    models = []\n    try:\n        logger.info(\"Retrieving model list from s3::\")\n        if function == \"models\":\n            s3_path = \"bronze/gamechanger/models/\"\n        elif function == \"data\":\n            s3_path = \"bronze/gamechanger/ml-data/\"\n\n        bucket = S3Service.connect_to_bucket(S3Config.BUCKET_NAME, logger)\n        start_char = len(s3_path)\n        models = [\n            (obj.key[start_char:], obj.last_modified)\n            for obj in bucket.objects.filter(Prefix=s3_path)\n        ]\n\n    except:\n        logger.warning(f\"Could not get model list from s3\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n    return models\n\n\n## Post Methods ##\n\n\n@router.post(\"/reloadModels\", status_code=200)\nasync def reload_models(model_dict: dict, response: Response):\n    \"\"\"load_latest_models - endpoint for updating the transformer model\n    Args:\n        model_dict: dict; {\"sentence\": \"bert...\",\n            \"qexp\": \"bert...\", \"transformer\": \"bert...\"}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n    \"\"\"\n    try:\n        total = len(model_dict)\n\n        # put the reload process on a thread\n        def reload_thread(model_dict):\n            try:\n                progress = 0\n                thread_name = processmanager.reloading + \" \".join(\n                    [key for key in model_dict]\n                )\n                logger.info(thread_name)\n                if \"sentence\" in model_dict:\n                    sentence_path = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        model_dict[\"sentence\"],\n                    )\n                    # uses SENT_INDEX_PATH by default\n                    logger.info(\"Attempting to load Sentence Transformer\")\n                    MODELS.initSentenceSearcher(sentence_path)\n                    SENT_INDEX_PATH.value = sentence_path\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n                if \"doc_compare_sentence\" in model_dict:\n                    doc_compare_sentence_path = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        model_dict[\"doc_compare_sentence\"],\n                    )\n                    # uses DOC_COMPARE_SENT_INDEX_PATH by default\n                    logger.info(\"Attempting to load Doc Compare Sentence Transformer\")\n                    MODELS.initDocumentCompareSearcher(doc_compare_sentence_path)\n                    DOC_COMPARE_SENT_INDEX_PATH.value = doc_compare_sentence_path\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n                if \"qexp\" in model_dict:\n                    qexp_name = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR, model_dict[\"qexp\"]\n                    )\n                    # uses QEXP_MODEL_NAME by default\n                    logger.info(\"Attempting to load QE\")\n                    MODELS.initQE(qexp_name)\n                    QEXP_MODEL_NAME.value = qexp_name\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n                if \"jbook_qexp\" in model_dict:\n                    jbook_qexp_name = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        model_dict[\"jbook_qexp\"],\n                    )\n                    # uses QEXP_MODEL_NAME by default\n                    logger.info(\"Attempting to load Jbook QE\")\n                    MODELS.initQEJBook(jbook_qexp_name)\n                    QEXP_JBOOK_MODEL_NAME.value = jbook_qexp_name\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n\n                if \"topic_models\" in model_dict:\n                    topics_name = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        model_dict[\"topic_models\"],\n                    )\n\n                    logger.info(\"Attempting to load Topics\")\n                    MODELS.initTopics(topics_name)\n                    TOPICS_MODEL.value = topics_name\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n                if \"qa_model\" in model_dict:\n                    qa_model_name = os.path.join(\n                        Config.LOCAL_PACKAGED_MODELS_DIR,\n                        model_dict[\"qa_model\"],\n                    )\n\n                    logger.info(\"Attempting to load QA model\")\n                    qa_model_name = qa_model_name.split(\"/\")[-1]\n                    MODELS.initQA(qa_model_name)\n                    QA_MODEL.value = qa_model_name\n                    progress += 1\n                    processmanager.update_status(\n                        thread_name,\n                        progress,\n                        total,\n                        thread_id=threading.current_thread().ident,\n                    )\n            except Exception as e:\n                logger.warning(e)\n                processmanager.update_status(\n                    f\"{processmanager.reloading}\",\n                    failed=True,\n                    thread_id=threading.current_thread().ident,\n                )\n\n        args = {\"model_dict\": model_dict}\n        thread = MlThread(reload_thread, args)\n        thread.start()\n        processmanager.running_threads[thread.ident] = thread\n        thread_name = processmanager.reloading + \" \".join([key for key in model_dict])\n        processmanager.update_status(thread_name, 0, total, thread_id=thread.ident)\n    except Exception as e:\n        logger.warning(e)\n\n    return await get_process_status()\n\n\n@router.post(\"/downloadCorpus\", status_code=200)\nasync def download_corpus(corpus_dict: dict, response: Response):\n    \"\"\"download_corpus - endpoint for downloading corpus\n    Args:\n        corpus_dict: dict; {\"corpus\": \"bronze/gamechanger/json\"}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n    \"\"\"\n    try:\n        logger.info(\"Attempting to download corpus from S3\")\n        # grabs the s3 path to the corpus from the post in \"corpus\"\n        # then passes in where to dowload the corpus locally.\n\n        s3_corpus_dir = corpus_dict.get(\"corpus\", S3_CORPUS_PATH)\n        args = {\n            \"s3_corpus_dir\": s3_corpus_dir,\n            \"output_dir\": CORPUS_DIR,\n            \"logger\": logger,\n        }\n\n        logger.info(args)\n        corpus_thread = MlThread(download_corpus_s3, args)\n        corpus_thread.start()\n        processmanager.running_threads[corpus_thread.ident] = corpus_thread\n        processmanager.update_status(\n            processmanager.corpus_download, 0, 1, thread_id=corpus_thread.ident\n        )\n    except Exception as e:\n        logger.exception(\"Could not get corpus from S3\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        processmanager.update_status(\n            processmanager.corpus_download,\n            failed=True,\n            message=e,\n            thread_id=corpus_thread.ident,\n        )\n\n    return await get_process_status()\n\n\n# Create a mapping between the training methods and input from the api\n# Methods for all the different models we can train\n# Defined outside the function so they arent recreated each time its called\n\n# Methods for all the different models we can train\n\n\ndef update_metadata(model_dict):\n    logger.info(\"Attempting to update feature metadata\")\n    pipeline = Pipeline()\n    model_dict[\"build_type\"] = \"meta\"\n    try:\n        corpus_dir = model_dict[\"corpus_dir\"]\n    except:\n        corpus_dir = CORPUS_DIR\n    try:\n        retriever = MODELS.sentence_searcher\n        logger.info(\"Using pre-loaded SentenceSearcher\")\n    except:\n        retriever = None\n        logger.info(\"Setting SentenceSearcher to None\")\n    try:\n        meta_steps = model_dict[\"meta_steps\"]\n    except:\n        meta_steps = [\n            \"pop_docs\",\n            \"combined_ents\",\n            \"rank_features\",\n            \"update_sent_data\",\n        ]\n    try:\n        index_path = model_dict[\"index_path\"]\n    except:\n        index_path = os.path.join(\n            Config.LOCAL_PACKAGED_MODELS_DIR, model_dict[\"sentence\"]\n        )\n    try:\n        update_eval_data = model_dict[\"update_eval_data\"]\n    except:\n        update_eval_data = False\n    try:\n        testing_only = model_dict[\"testing_only\"]\n    except:\n        testing_only = False\n    try:\n        upload = model_dict[\"upload\"]\n    except:\n        upload = True\n\n    logger.info(f\"Testing only is set to: {testing_only}\")\n\n    args = {\n        \"meta_steps\": meta_steps,\n        \"corpus_dir\": corpus_dir,\n        \"retriever\": retriever,\n        \"index_path\": index_path,\n        \"update_eval_data\": update_eval_data,\n        \"testing_only\": testing_only,\n        \"upload\": upload,\n    }\n\n    pipeline.run(\n        build_type=model_dict[\"build_type\"],\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\ndef finetune_sentence(model_dict):\n    logger.info(\"Attempting to finetune the sentence transformer\")\n    try:\n        testing_only = model_dict[\"testing_only\"]\n    except:\n        testing_only = False\n    try:\n        remake_train_data = model_dict[\"remake_train_data\"]\n    except:\n        remake_train_data = False\n    try:\n        model = model_dict[\"model\"]\n    except:\n        model = None\n    args = {\n        \"batch_size\": 8,\n        \"epochs\": int(model_dict[\"epochs\"]),\n        \"warmup_steps\": int(model_dict[\"warmup_steps\"]),\n        \"testing_only\": bool(testing_only),\n        \"remake_train_data\": bool(remake_train_data),\n        \"retriever\": MODELS.sentence_searcher,\n        \"model\": model,\n    }\n    pipeline.run(\n        build_type=\"sent_finetune\",\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\ndef train_sentence(model_dict):\n\n    build_type = model_dict[\"build_type\"]\n    logger.info(f\"Attempting to start {build_type} pipeline\")\n\n    corpus_dir = model_dict.get(\"corpus_dir\", CORPUS_DIR)\n    if not os.path.exists(corpus_dir):\n        logger.warning(f\"Corpus is not in local directory {str(corpus_dir)}\")\n        raise Exception(\"Corpus is not in local directory\")\n    args = {\n        \"corpus\": corpus_dir,\n        \"encoder_model\": model_dict[\"encoder_model\"],\n        \"gpu\": bool(model_dict[\"gpu\"]),\n        \"upload\": bool(model_dict[\"upload\"]),\n        \"version\": model_dict[\"version\"],\n    }\n    logger.info(args)\n    pipeline.run(\n        build_type=model_dict[\"build_type\"],\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\ndef train_qexp(model_dict):\n    logger.info(\"Attempting to start qexp pipeline\")\n    args = {\n        \"upload\": bool(model_dict[\"upload\"]),\n        \"version\": model_dict[\"version\"],\n    }\n    pipeline.run(\n        build_type=model_dict[\"build_type\"],\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\ndef run_evals(model_dict):\n    logger.info(\"Attempting to run evaluation\")\n    try:\n        sample_limit = int(model_dict[\"sample_limit\"])\n    except:\n        sample_limit = 15000\n    if \"sent_index\" in model_dict[\"model_name\"]:\n        retriever = MODELS.sentence_searcher\n    else:\n        retriever = None\n    args = {\n        \"model_name\": model_dict[\"model_name\"],\n        \"eval_type\": model_dict[\"eval_type\"],\n        \"sample_limit\": sample_limit,\n        \"validation_data\": model_dict[\"validation_data\"],\n        \"retriever\": retriever,\n    }\n    pipeline.run(\n        build_type=model_dict[\"build_type\"],\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\ndef train_topics(model_dict):\n    logger.info(\"Attempting to train topic model\")\n    logger.info(model_dict)\n    args = {\n        \"sample_rate\": model_dict[\"sample_rate\"],\n        \"upload\": model_dict[\"upload\"],\n    }\n    pipeline.run(\n        build_type=model_dict[\"build_type\"],\n        run_name=datetime.now().strftime(\"%Y%m%d\"),\n        params=args,\n    )\n\n\n@router.post(\"/trainModel\", status_code=200)\nasync def train_model(model_dict: dict, response: Response):\n    \"\"\"load_latest_models - endpoint for updating the transformer model\n    Args:\n        model_dict: dict; {\"encoder_model\":\"msmarco-distilbert-base-v2\", \"gpu\":true, \"upload\":false,\"version\": \"v5\"}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n    \"\"\"\n    try:\n        # Create a mapping between the training methods and input from the api\n        training_switch = {\n            \"sentence\": train_sentence,\n            \"qexp\": train_qexp,\n            \"sent_finetune\": finetune_sentence,\n            \"eval\": run_evals,\n            \"meta\": update_metadata,\n            \"topics\": train_topics,\n        }\n\n        # Set the training method to be loaded onto the thread\n        if \"build_type\" in model_dict and model_dict[\"build_type\"] in training_switch:\n            training_method = training_switch[model_dict[\"build_type\"]]\n        else:  # PLACEHOLDER\n            logger.warn(\"No build type specified in model_dict, defaulting to sentence\")\n            model_dict[\"build_type\"] = \"sentence\"\n            training_method = training_switch[model_dict[\"build_type\"]]\n\n        build_type = model_dict.get(\"build_type\")\n        training_method = training_switch.get(build_type)\n\n        if not training_method:\n            raise Exception(f\"No training method mapped for build type {build_type}\")\n\n        # Set the training method to be loaded onto the thread\n        training_thread = MlThread(training_method, args={\"model_dict\": model_dict})\n        training_thread.start()\n        processmanager.running_threads[training_thread.ident] = training_thread\n        processmanager.update_status(\n            processmanager.training, 0, 1, thread_id=training_thread.ident\n        )\n    except:\n        logger.warning(f\"Could not train/evaluate the model\")\n        response.status_code = status.HTTP_500_INTERNAL_SERVER_ERROR\n        processmanager.update_status(\n            processmanager.training,\n            failed=True,\n            thread_id=training_thread.ident,\n        )\n\n    return await get_process_status()\n\n\n@router.post(\"/stopProcess\")\nasync def stop_process(thread_dict: dict, response: Response):\n    \"\"\"stop_process - endpoint for stopping a process in a thread\n    Args:\n        thread_dict: dict; {\"thread_id\":(int of thread id), \"process\":(name of the process so we can also update it in redis)}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        Stopped thread id\n    \"\"\"\n    logger.info(processmanager.running_threads)\n    thread_id = int(thread_dict[\"thread_id\"])\n    with processmanager.thread_lock:\n        if thread_id in processmanager.running_threads:\n            processmanager.running_threads[thread_id].kill()\n            del processmanager.running_threads[thread_id]\n    processmanager.update_status(\n        thread_dict[\"process\"],\n        failed=True,\n        message=\"Killed by user\",\n        thread_id=thread_id,\n    )\n\n    return {\"stopped\": thread_id}\n\n\n@router.post(\"/sendUserAggregations\")\nasync def get_user_data(data_dict: dict, response: Response):\n    \"\"\"get_user_data - Get user aggregation data for selected date and write to data folder\n    Args:\n        date_dict: dict; {data}\n        Response: Response class; for status codes(apart of fastapi do not need to pass param)\n    Returns:\n        confirmation of data download\n    \"\"\"\n\n    userData = data_dict[\"params\"][\"userData\"]\n    GC_USER_DATA = os.path.join(\n        DATA_PATH, \"user_data\", \"search_history\", \"UserAggregations.json\"\n    )\n    with open(GC_USER_DATA, \"w\") as f:\n        json.dump(userData, f)\n\n    searchData = data_dict[\"params\"][\"searchData\"]\n    df = pd.DataFrame(searchData)\n    GC_SEARCH_DATA = os.path.join(\n        DATA_PATH, \"user_data\", \"search_history\", \"SearchPdfMapping.csv\"\n    )\n    df.to_csv(GC_SEARCH_DATA)\n\n    return f\"wrote {len(userData)} user data and searches to file\"\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/routers/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/api/fastapi/routers/startup.py", "content": "from fastapi import APIRouter\nfrom fastapi_utils.tasks import repeat_every\nimport os\nfrom typing import Tuple\n\nfrom gamechangerml.api.fastapi.settings import (\n    DOC_COMPARE_SENT_INDEX_PATH,\n    logger,\n    TOPICS_MODEL,\n    MODEL_LOAD_FLAG,\n    QEXP_JBOOK_MODEL_NAME,\n    QEXP_MODEL_NAME,\n    LOCAL_TRANSFORMERS_DIR,\n    SENT_INDEX_PATH,\n    latest_intel_model_encoder,\n    latest_intel_model_sim,\n    latest_intel_model_sent,\n    latest_doc_compare_sim,\n    latest_doc_compare_encoder,\n    MEMORY_LOAD_LIMIT,\n    CORPUS_EVENT_TRIGGER,\n)\nfrom gamechangerml.api.fastapi.model_loader import ModelLoader\nfrom gamechangerml.api.utils.mlscheduler import corpus_update_event\nfrom gamechangerml.api.utils.threaddriver import MlThread\nfrom gamechangerml.api.utils import processmanager\nfrom gamechangerml.api.fastapi.routers.controls import get_process_status\nimport psutil\n\nrouter = APIRouter()\nMODELS = ModelLoader()\nmodel_functions = [\n    MODELS.initQA,\n    MODELS.initQE,\n    MODELS.initQEJBook,\n    MODELS.initSentenceSearcher,\n    MODELS.initWordSim,\n    MODELS.initTopics,\n    MODELS.initRecommender,\n    MODELS.initDocumentCompareSearcher,\n]\n\n\n@router.on_event(\"startup\")\nasync def load_models():\n\n    if MODEL_LOAD_FLAG:\n        count = 0\n        for f in model_functions:\n            f()\n            ram_used, surpassed, cpu_usage = get_hw_usage()\n            count += 1\n            if surpassed:\n                logger.warning(\n                    f\" ---- WARNING: RAM used is {ram_used}%, which is passed the threshold, will not load any other models\"\n                )\n                models_not_loaded = model_functions[:count]\n                logger.warning(f\"---- Did not load: {models_not_loaded}\")\n                break\n        logger.info(\"LOADED MODELS\")\n    else:\n        logger.info(\"MODEL_LOAD_FLAG set to False, no models loaded\")\n\n\n@router.on_event(\"startup\")\n@repeat_every(seconds=120, wait_first=True)\nasync def check_health():\n    \"\"\"check_health - periodically checks redis for a new model for workers, checks access to end points\n    Args:\n    Returns:\n    \"\"\"\n    logger.info(\"API Health Check\")\n    if check_dep_exist:\n        good_health = True\n    else:\n        good_health = False\n    if good_health:\n        logger.info(\"Model Health: GOOD\")\n    else:\n        logger.info(\"Model Health: POOR\")\n\n    # logger.info(f\"CPU usage: {cpu_usage}\")\n    # logger.info(f\"RAM % used: {ram_used}\")\n\n\n# @router.on_event(\"startup\")\n# @repeat_every(seconds=60 * 60, wait_first=False)\n# async def corpus_event_trigger():\n#     if CORPUS_EVENT_TRIGGER:\n#         logger.info(\"Checking Corpus Staleness\")\n#         args = {\n#             \"s3_corpus_dir\": \"bronze/gamechanger/json\",\n#             \"logger\": logger,\n#         }\n#         # await corpus_update_event(**args)\n\n\ndef get_hw_usage(threshold: int = MEMORY_LOAD_LIMIT) -> Tuple[float, bool, float]:\n    surpassed = False\n    ram_used = psutil.virtual_memory()[2]\n    if threshold:\n        if ram_used > threshold:\n            surpassed = True\n    cpu_usage = psutil.cpu_percent(4)\n    return ram_used, surpassed, cpu_usage\n\n\ndef check_dep_exist():\n    healthy = True\n    if not os.path.isdir(LOCAL_TRANSFORMERS_DIR.value):\n        logger.warning(f\"{LOCAL_TRANSFORMERS_DIR.value} does NOT exist\")\n        healthy = False\n\n    if not os.path.isdir(SENT_INDEX_PATH.value):\n        logger.warning(f\"{SENT_INDEX_PATH.value} does NOT exist\")\n        healthy = False\n\n    if not os.path.isdir(DOC_COMPARE_SENT_INDEX_PATH.value):\n        logger.warning(f\"{DOC_COMPARE_SENT_INDEX_PATH.value} does NOT exist\")\n        healthy = False\n\n    if not os.path.isdir(QEXP_MODEL_NAME.value):\n        logger.warning(f\"{QEXP_MODEL_NAME.value} does NOT exist\")\n        healthy = False\n\n    if not os.path.isdir(TOPICS_MODEL.value):\n        logger.warning(f\"{TOPICS_MODEL.value} does NOT exist\")\n        healthy = False\n\n    if not os.path.isdir(QEXP_JBOOK_MODEL_NAME.value):\n        logger.warning(f\"{QEXP_JBOOK_MODEL_NAME.value} does NOT exist\")\n        healthy = False\n\n    return healthy\n"}
{"type": "source_file", "path": "__init__.py", "content": "import os\n\n# abs path to this package\nPACKAGE_PATH: str = os.path.dirname(os.path.abspath(__file__))\nDATA_PATH: str = os.path.join(PACKAGE_PATH, 'data')\nNLTK_DATA_PATH: str = os.path.join(DATA_PATH, 'nltk_data')\nAGENCY_DATA_PATH: str = os.path.join(DATA_PATH, 'agencies')\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/api/fastapi/version.py", "content": "__version__ = \"1.10\"\n"}
{"type": "source_file", "path": "gamechangerml/api/fastapi/settings.py", "content": "import os\n\nfrom gamechangerml.api.utils.pathselect import get_model_paths\nfrom gamechangerml.api.utils.logger import logger\nfrom gamechangerml.api.utils.redisdriver import CacheVariable, REDIS_HOST, REDIS_PORT\nfrom gamechangerml import CORPUS_PATH\nfrom gamechangerml.configs import QAConfig\n\n# get environ vars\nGC_ML_HOST = os.environ.get(\"GC_ML_HOST\", default=\"localhost\")\nML_WEB_TOKEN = os.environ.get(\"ML_WEB_TOKEN\", default=\"\")\nMEMORY_LOAD_LIMIT = os.environ.get(\"MEMORY_LOAD_LIMIT\", default=None)\nif MEMORY_LOAD_LIMIT:\n    MEMORY_LOAD_LIMIT = int(MEMORY_LOAD_LIMIT)\nMODEL_LOAD_FLAG = os.environ.get(\"MODEL_LOAD\", default=True)\nif MODEL_LOAD_FLAG in [\"False\", \"false\", \"0\"]:\n    MODEL_LOAD_FLAG = False\nelse:\n    MODEL_LOAD_FLAG = True\nCACHE_EXPIRE_DAYS = 15\nif GC_ML_HOST == \"\":\n    GC_ML_HOST = \"localhost\"\nignore_files = [\"._.DS_Store\", \".DS_Store\", \"index\"]\n\nCORPUS_DIR = CORPUS_PATH\nS3_CORPUS_PATH = os.environ.get(\"S3_CORPUS_PATH\")\nCORPUS_EVENT_TRIGGER_VAL = 0.5\nCORPUS_EVENT_TRIGGER = bool(os.environ.get(\"CORPUS_EVENT_TRIGGER\", default=True))\n\n# Redis Cache Variables\nlatest_intel_model_sent = CacheVariable(\"latest_intel_model_sent\", True)\nlatest_intel_model_sim = CacheVariable(\n    \"latest sentence searcher (similarity model + sent index)\", True\n)\nlatest_intel_model_encoder = CacheVariable(\"latest encoder model\", True)\nlatest_intel_model_trans = CacheVariable(\"latest_intel_model_trans\")\nlatest_doc_compare_encoder = CacheVariable(\"latest doc compare encoder model\", True)\nlatest_doc_compare_sim = CacheVariable(\n    \"latest doc compare searcher (similarity model + sent index)\", True\n)\n\nLOCAL_TRANSFORMERS_DIR = CacheVariable(\"LOCAL_TRANSFORMERS_DIR\")\nSENT_INDEX_PATH = CacheVariable(\"SENT_INDEX_PATH\")\nQEXP_MODEL_NAME = CacheVariable(\"QEXP_MODEL_NAME\")\nQEXP_JBOOK_MODEL_NAME = CacheVariable(\"QEXP_JBOOK_MODEL_NAME\")\nWORD_SIM_MODEL = CacheVariable(\"WORD_SIM_MODEL\")\nTOPICS_MODEL = CacheVariable(\"TOPICS_MODEL\")\nQA_MODEL = CacheVariable(\"QA_MODEL\")\nDOC_COMPARE_SENT_INDEX_PATH = CacheVariable(\"DOC_COMPARE_SENT_INDEX_PATH\")\n\n\nmodel_path_dict = get_model_paths()\nLOCAL_TRANSFORMERS_DIR.value = model_path_dict[\"transformers\"]\nSENT_INDEX_PATH.value = model_path_dict[\"sentence\"]\nQEXP_MODEL_NAME.value = model_path_dict[\"qexp\"]\nQEXP_JBOOK_MODEL_NAME.value = model_path_dict[\"qexp_jbook\"]\nWORD_SIM_MODEL.value = model_path_dict[\"word_sim\"]\nTOPICS_MODEL.value = model_path_dict[\"topics\"]\nQA_MODEL.value = QAConfig.BASE_MODEL\nDOC_COMPARE_SENT_INDEX_PATH.value = model_path_dict[\"doc_compare\"]\n\nt_list = []\ntry:\n    t_list = [\n        trans for trans in os.listdir(LOCAL_TRANSFORMERS_DIR.value) if \".\" not in trans\n    ]\nexcept Exception as e:\n    logger.warning(\"No transformers folder\")\n    logger.warning(e)\nlogger.info(f\"API AVAILABLE TRANSFORMERS are: {t_list}\")\n\n\n# validate correct configurations\nlogger.info(f\"API TRANSFORMERS DIRECTORY is: {LOCAL_TRANSFORMERS_DIR.value}\")\nlogger.info(f\"API INDEX PATH is: {SENT_INDEX_PATH.value}\")\nlogger.info(f\"API DOC COMPARE INDEX PATH is: {DOC_COMPARE_SENT_INDEX_PATH.value}\")\nlogger.info(f\"API REDIS HOST is: {REDIS_HOST}\")\nlogger.info(f\"API REDIS PORT is: {REDIS_PORT}\")\n"}
{"type": "source_file", "path": "gamechangerml/api/getInitModels.py", "content": "\"\"\"THIS FILE INTENDED TO BE RUN BY startProd/Dev.sh AS A MODULE\"\"\"\n\nfrom threading import Thread\nfrom datetime import datetime\nfrom time import sleep\nfrom os import environ, makedirs\nfrom os.path import join, exists\n\nfrom gamechangerml.api.fastapi.model_config import Config\nfrom gamechangerml.src.data_transfer import get_latest_model_name\nfrom gamechangerml.src.services import S3Service\nfrom gamechangerml.configs import S3Config\nfrom gamechangerml.src.utilities import configure_logger\n\n\ndef poll_alive():\n    while True:\n        sleep(3)\n        print(\"{} getInitModels.py still running...\".format(datetime.now()))\n\ndef start_thread():\n    thread = Thread(target=poll_alive)\n    thread.daemon = True\n    thread.start()\n\ndef verify_env_var(value, name):\n    \"\"\"Verify that an environment variable is not None.\n\n    Args:\n        value (any): Check that this value is not None\n        name (str): Name of the environment variable\n\n    Raises:\n        RuntimeError: If value is None.\n    \"\"\"\n    if value is None:\n        raise RuntimeError(f\"{name} cannot be None. Verify env setup.\")\n    \ndef run(pull_type, logger):\n    \"\"\"Main function to run for getInitModels.py.\n\n    Args:\n        pull_type (str): The name of the model to pull. Or, \n            \"all\": to pull all models\n            \"latest\": to pull the latest model\n\n    Raises:\n        Exception: If S3 download fails\n\n    Returns:\n        (list of str, list of str): A tuple of length 2 with:\n            downloaded_paths: S3 paths of files successfully downloaded\n            existing_paths: local paths for files that were not downloaded \n                because they already exist\n    \"\"\"\n    start_thread()\n\n    S3_MODELS_DIR = Config.S3_MODELS_DIR\n    LOCAL_MODELS_DIR = Config.LOCAL_PACKAGED_MODELS_DIR\n    verify_env_var(LOCAL_MODELS_DIR, \"LOCAL_MODELS_DIR\")\n\n    bucket = S3Service.connect_to_bucket(S3Config.BUCKET_NAME, logger)\n    \n    logger.info(\"pull_type:\", pull_type)\n    if pull_type == \"all\":\n        model_names = S3Service.get_object_names(bucket, S3_MODELS_DIR, \"dir\")\n    elif pull_type == \"latest\":\n        model_names = get_latest_model_name(S3_MODELS_DIR, bucket)\n        if model_names is None:\n            logger.error(\"No latest model detected.\")\n            exit(1)\n        model_names = [model_names]\n    else:\n        model_names = [pull_type]\n\n    downloaded_paths = []\n    existing_paths = []\n\n    for model_pkg in model_names:\n        local_dir = join(LOCAL_MODELS_DIR, model_pkg)\n        makedirs(local_dir, exist_ok=True)\n        prefix = join(S3_MODELS_DIR, model_pkg)\n        s3_file_names = S3Service.get_object_names(bucket, prefix, \"filename\")\n\n        for fn in s3_file_names:\n            s3_path = join(prefix, fn)\n            try:\n                local_file = join(local_dir, fn)\n                if exists(local_file):\n                    existing_paths.append(local_file)\n                    logger.info(\n                        f\"{fn} already exists locally in {local_dir}. \"\n                        \"Skipping download.\"\n                    )\n                else:\n                    logger.info(f\"Downloading from S3: {s3_path}\")\n                    S3Service.download(bucket, s3_path, local_dir, logger)\n                    downloaded_paths.append(s3_path)\n            except Exception as e:\n                logger.exception(\n                    f\"Failed to download from S3: {s3_path}.\"\n                )\n                raise e\n    \n    return downloaded_paths, existing_paths\n\n    \nif __name__ == \"__main__\":\n    logger = configure_logger(min_level=\"INFO\")\n    logger.info(\"Running getInitModels.py\")\n    \n    pull_type = environ.get(\"PULL_MODELS\")\n\n    try:\n        downloaded, existing = run(pull_type, logger)\n    except Exception:\n        logger.exception(\"----- FATAL ERROR ON INIT, FETCHING MODELS FAILED.\")\n        exit(1)\n    else:\n        logger.info(\"++++ Model retrieval success\")\n        logger.info(f\"Downloaded: {downloaded}.\")\n        logger.info(f\"Not downloaded (already exist): {existing}.\")\n        exit(0)\n    \n        \n   "}
{"type": "source_file", "path": "gamechangerml/api/utils/processmanager.py", "content": "import threading\nfrom datetime import datetime\nfrom gamechangerml.api.utils.redisdriver import CacheVariable\n\n# from gamechangerml.api.fastapi.settings import logger   # commenting out because of API calls failing for gamechanger-data\n# Process Keys\nclear_corpus = \"corpus: corpus_download\"\ncorpus_download = \"corpus: corpus_download\"\ndelete_corpus = \"corpus: delete_corpus\"\ns3_file_download = \"s3: file_download\"\ns3_dependency = \"s3: all_dependency_download\"\nloading_corpus = \"training: load_corpus\"\nloading_data = \"training: load_data\"\ntraining = \"training: train_model\"\nreloading = \"models: reloading_models \"\nltr_creation = \"training: ltr_creation\"\ntopics_creation = \"models: topics_creation\"\nml_change_event = \"training: corpus_download_training_models\"\n\nrunning_threads = {}\n\n# the dictionary that holds all the progress values\ntry:\n    PROCESS_STATUS = CacheVariable(\"process_status\", True)\n    COMPLETED_PROCESS = CacheVariable(\"completed_process\", True)\n    thread_lock = threading.Lock()\n    default_flags = {\n        corpus_download: False,\n        clear_corpus: False,\n        training: False,\n        loading_corpus: False,\n        reloading: False,\n        ltr_creation: False,\n        topics_creation: False,\n        s3_file_download: False,\n        s3_dependency: False,\n        loading_data: False,\n        ml_change_event: False,\n    }\n\nexcept Exception as e:\n    print(e)\n\n# adding try-catch around process_status in case calling the API for the CacheVariable fails\ntry:\n    if PROCESS_STATUS.value == None:\n        PROCESS_STATUS.value = {\"flags\": default_flags}\n    if COMPLETED_PROCESS.value == None:\n        COMPLETED_PROCESS.value = []\nexcept Exception as e:\n    print(e)\n\n\ndef update_status(\n    name,\n    progress=0,\n    total=100,\n    message=\"\",\n    failed=False,\n    thread_id=\"\",\n    completed_max=20,\n):\n\n    thread_id = str(thread_id)\n    try:\n        if progress == total or failed:\n            date = datetime.now()\n            date_string = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n            completed = {\n                \"process\": name,\n                \"total\": total,\n                \"message\": message,\n                \"date\": date_string,\n            }\n            with thread_lock:\n                if thread_id in PROCESS_STATUS.value:\n                    temp = PROCESS_STATUS.value\n                    temp.pop(thread_id, None)\n                    if name in temp[\"flags\"]:\n                        temp[\"flags\"][name] = True\n                    PROCESS_STATUS.value = temp\n                    if thread_id in running_threads:\n                        del running_threads[thread_id]\n                    if failed:\n                        completed[\"date\"] = \"Failed\"\n\n                completed_list = COMPLETED_PROCESS.value\n                completed_list.append(completed)\n\n                if len(completed_list) >= completed_max:\n                    completed_list = completed_list[-completed_max:]\n\n                COMPLETED_PROCESS.value = completed_list\n        else:\n            status = {\"progress\": progress, \"total\": total}\n            with thread_lock:\n                status_dict = PROCESS_STATUS.value\n\n                if thread_id not in status_dict:\n                    status[\"process\"] = name\n                    status[\"category\"] = name.split(\":\")[0]\n                    status_dict[thread_id] = status\n                else:\n                    status_dict[thread_id].update(status)\n\n                if name in status_dict[\"flags\"]:\n                    status_dict[\"flags\"][name] = False\n                PROCESS_STATUS.value = status_dict\n    except Exception as e:\n        print(e)\n\n\ndef set_flags(key, value):\n    with thread_lock:\n        status_dict = PROCESS_STATUS.values\n        status_dict[\"flags\"][key] = value\n        PROCESS_STATUS.value = status_dict\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/logger.py", "content": "import logging\nfrom logging import handlers\nimport sys\nfrom gamechangerml.src.utilities import configure_logger\n\n# set loggers\nlogger = configure_logger()\nglogger = logging.getLogger(\"gunicorn.error\")\n\ntry:\n    # glogger.addHandler(ch)\n    log_file_path = \"gamechangerml/api/logs/gc_ml_logs.txt\"\n    fh = logging.handlers.RotatingFileHandler(\n        log_file_path, maxBytes=2000000, backupCount=1, mode=\"a\"\n    )\n    logger.info(f\"ML API is logging to {log_file_path}\")\n\n    # fh.setFormatter(log_formatter)\n    logger.addHandler(fh)\n    glogger.addHandler(fh)\nexcept Exception as e:\n    logger.warn(e)\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/threaddriver.py", "content": "import threading\nimport json\nimport sys\nfrom gamechangerml.api.utils.logger import logger\nfrom gamechangerml.debug.debug_connector import check_debug_flagged\n\n# A class that takes in a function and a dictionary of arguments.\n# The keys in args have to match the parameters in the function.\n\n\nclass MlThread(threading.Thread):\n    def __init__(self, function, args={}):\n        super(MlThread, self).__init__()\n        self.function = function\n        self.args = args\n        self.killed = False\n\n    def run(self):\n        try:\n            if check_debug_flagged():\n                logger.info(\n                    \"Debugger from debugpy package is not compatible with sys.settrace, so globaltrace not activated for MlThread\")\n            else:\n                sys.settrace(self.globaltrace)\n\n            self.function(**self.args)\n        except Exception as e:\n            logger.error(e)\n            logger.info(\"Thread errored out attempting \" + self.function.__name__ +\n                        \" with parameters: \" + json.dumps(self.args))\n\n    def globaltrace(self, frame, why, arg):\n        if why == 'call':\n            return self.localtrace\n        else:\n            return None\n\n    def localtrace(self, frame, why, arg):\n        if self.killed:\n            if why == 'line':\n                raise SystemExit()\n        return self.localtrace\n\n    def kill(self):\n        logger.info(f'killing {self.function}')\n        self.killed = True\n\n# Pass in a function and args which is an array of dicts\n# A way to load mulitple jobs and run them on threads.\n# join is set to false unless we need to collect the results immediately.\n\n\ndef run_threads(function_list, args_list=[], join=False):\n    threads = []\n    for i, function in enumerate(function_list):\n        args = {}\n        if i < len(args_list):\n            args = args_list[i]\n        thread = MlThread(function, args)\n        threads.append(thread)\n        thread.start()\n    # If we join the threads the function will wait until they have all completed.\n    if join:\n        for thread in threads:\n            thread.join()\n"}
{"type": "source_file", "path": "gamechangerml/models/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/configs/doc_compare_similarity_config.py", "content": "class DocCompareSimilarityConfig:\n    \"\"\"Configurations for the Document Compare Searcher model.\"\"\"\n\n    \"\"\"Base model name.\"\"\"\n    BASE_MODEL = \"distilbart-mnli-12-3\"\n"}
{"type": "source_file", "path": "gamechangerml/configs/path_config.py", "content": "from os.path import join\nfrom gamechangerml import DATA_PATH, MODEL_PATH\n\n\nclass PathConfig:\n    \"\"\"Configurations for repository paths.\"\"\"\n    \n    DATA_DIR = DATA_PATH\n    LOCAL_MODEL_DIR = MODEL_PATH\n    TRANSFORMER_PATH = join(MODEL_PATH, \"transformers\")\n"}
{"type": "source_file", "path": "gamechangerml/configs/doc_compare_embedder_config.py", "content": "class DocCompareEmbedderConfig:\n    \"\"\"Configurations for the Document Comparison Encoder model.\"\"\"\n\n    \"\"\"Base model name.\"\"\"\n    BASE_MODEL = \"msmarco-distilbert-base-v2\"\n\n    \"\"\"Arguments used to load the model.\n\n        min_token_len\n        verbose: for creating LocalCorpus\n        return_id: LocalCorpus\n    \"\"\"\n    MODEL_ARGS = {\n        \"min_token_len\": 25,\n        \"verbose\": True,\n        \"return_id\": True,\n    }\n\n    FINETUNE = {\n        \"shuffle\": True,\n        \"batch_size\": 32,\n        \"epochs\": 3,\n        \"warmup_steps\": 100,\n    }\n"}
{"type": "source_file", "path": "gamechangerml/configs/similarity_config.py", "content": "class SimilarityConfig:\n    \"\"\"Configurations for the Sentence Searcher/ Similarity model.\"\"\"\n\n    \"\"\"Base model name.\"\"\"\n    BASE_MODEL = \"distilbart-mnli-12-3\"\n"}
{"type": "source_file", "path": "gamechangerml/configs/validation_config.py", "content": "from os.path import join\nfrom gamechangerml import DATA_PATH\n\n\nclass ValidationConfig:\n    DATA_ARGS = {\n        # need to have validation data in here\n        \"validation_dir\": join(DATA_PATH, \"validation\"),\n        \"evaluation_dir\": join(DATA_PATH, \"evaluation\"),\n        \"user_dir\": join(DATA_PATH, \"user_data\"),\n        \"test_corpus_dir\": \"gamechangerml/test_corpus\",\n        \"squad\": {\n            \"dev\": \"original/squad2.0/dev-v2.0.json\",\n            \"train\": \"original/squad2.0/train-v2.0.json\",\n        },\n        \"nli\": {\n            \"matched\": \"original/multinli_1.0/multinli_1.0_dev_matched.jsonl\",\n            \"mismatched\": \"original/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\",\n        },\n        \"msmarco\": {\n            \"collection\": \"original/msmarco_1k/collection.json\",\n            \"queries\": \"original/msmarco_1k/queries.json\",\n            \"relations\": \"original/msmarco_1k/relations.json\",\n            \"metadata\": \"original/msmarco_1k/metadata.json\",\n        },\n        \"question_gc\": {\n            \"queries\": \"domain/question_answer/QA_domain_data.json\"\n        },\n        \"retriever_gc\": {\"gold_standard\": \"gold_standard.csv\"},\n        \"matamo_dir\": join(DATA_PATH, \"user_data\", \"matamo_feedback\"),\n        \"search_hist_dir\": join(DATA_PATH, \"user_data\", \"search_history\"),\n        \"qe_gc\": \"domain/query_expansion/QE_domain.json\",\n    }\n\n    TRAINING_ARGS = {\n        \"start_date\": \"2020-12-01\",  # earliest date to include search hist/feedback data from\n        \"end_date\": \"2025-12-01\",  # last date to include search hist/feedback data from\n        \"exclude_searches\": [\"pizza\", \"shark\"],\n        \"min_correct_matches\": {\"gold\": 3, \"silver\": 2, \"any\": 0},\n        \"max_results\": {\"gold\": 7, \"silver\": 10, \"any\": 100},\n    }\n\n"}
{"type": "source_file", "path": "gamechangerml/configs/training_config.py", "content": "from os.path import join\nfrom gamechangerml import DATA_PATH\n\n\nclass TrainingConfig:\n    \"\"\"Configurations for model training.\"\"\"\n\n    TRAINING_DATA_DIR = join(DATA_PATH, \"training\")\n  \n    TRAIN_TEST_SPLIT_RATIO = 0.8\n  \n"}
{"type": "source_file", "path": "gamechangerml/configs/bert_summary_config.py", "content": "# for Bert Extractive Summarizer (https://pypi.org/project/bert-extractive-summarizer/)\nclass BertSummConfig:\n    \"\"\"Configurations for Bert Summary model.\"\"\"\n    \n    MODEL_ARGS = {\n        \"initialize\": {\n            # This gets used by the hugging face bert library to load the model, you can supply a custom trained model here\n            \"model\": \"bert-base-uncased\",\n            # If you have a pre-trained model, you can add the model class here.\n            \"custom_model\": None,\n            # If you have a custom tokenizer, you can add the tokenizer here.\n            \"custom_tokenizer\": None,\n            # Needs to be negative, but allows you to pick which layer you want the embeddings to come from.\n            \"hidden\": -2,\n            # It can be \"mean\", \"median\", or \"max\". This reduces the embedding layer for pooling.\n            \"reduce_option\": \"mean\",\n        },\n        \"fit\": {\n            \"ratio\": None,  # The ratio of sentences that you want for the final summary\n            # Parameter to specify to remove sentences that are less than 40 characters\n            \"min_length\": 40,\n            \"max_length\": 600,  # Parameter to specify to remove sentences greater than the max length\n            # Number of sentences to use. Overrides ratio if supplied.\n            \"num_sentences\": 2,\n        },\n        \"coreference\": {\"greedyness\": 0.4},\n        \"doc_limit\": 100000,\n    }\n"}
{"type": "source_file", "path": "gamechangerml/configs/d2v_config.py", "content": "class D2VConfig:\n    \"\"\"Configurations for Doc2Vec model.\"\"\"\n    \n    MODEL_ARGS = {\n        \"dm\": 1,\n        \"dbow_words\": 1,\n        \"vector_size\": 50,\n        \"window\": 5,\n        \"min_count\": 5,\n        \"sample\": 1e-5,\n        \"epochs\": 20,\n        \"alpha\": 0.020,\n        \"min_alpha\": 0.005,\n    }\n"}
{"type": "source_file", "path": "gamechangerml/configs/topics_config.py", "content": "from gamechangerml import MODEL_PATH\n\n\nclass TopicsConfig:\n    \"\"\"Configurations for the Topic Model.\"\"\"\n\n    \"\"\"Topic models should be in the folder \n    gamechangerml/models/topic_model_<date>\n    and should contain bigrams.phr, tfidf.model, and tfidf_dictionary.dic.\n    \"\"\"\n    DATA_ARGS = {\"LOCAL_MODEL_DIR\": MODEL_PATH}\n\n"}
{"type": "source_file", "path": "gamechangerml/configs/qa_config.py", "content": "class QAConfig:\n    \"\"\"Configurations for the QA model.\"\"\"\n\n    \"\"\"Base model name.\"\"\"\n    BASE_MODEL = \"bert-base-cased-squad2\"\n\n    \"\"\"Arguments used to load the model.\n    \n        qa_type: options are [\"scored_answer\", \"simple_answer\"]\n        nbest: number of answers to retrieve from each context for comparison.\n            If diff between the answer score and null answer score is greater \n            than this threshold, don't return answer.\n        null_threshold\n    \"\"\"\n    MODEL_ARGS = {\n        \"qa_type\": \"scored_answer\",\n        \"nbest\": 1,\n        \"null_threshold\": -3,\n    }\n"}
{"type": "source_file", "path": "gamechangerml/configs/__init__.py", "content": "from .s3_config import S3Config\nfrom .qa_config import QAConfig\nfrom .embedder_config import EmbedderConfig\nfrom .doc_compare_embedder_config import DocCompareEmbedderConfig\nfrom .similarity_config import SimilarityConfig\nfrom .doc_compare_similarity_config import DocCompareSimilarityConfig\nfrom .qexp_config import QexpConfig\nfrom .training_config import TrainingConfig\nfrom .d2v_config import D2VConfig\nfrom .validation_config import ValidationConfig\nfrom .bert_summary_config import BertSummConfig\nfrom .topics_config import TopicsConfig\nfrom .path_config import PathConfig\n"}
{"type": "source_file", "path": "gamechangerml/configs/embedder_config.py", "content": "class EmbedderConfig:\n    \"\"\"Configurations for the Sentence Encoder model.\"\"\"\n\n    \"\"\"Base model name\"\"\"\n    BASE_MODEL = \"msmarco-distilbert-base-v2\"\n\n    \"\"\"Arguments used to load the model.\n    \n        min_token_len\n        verbose: for creating LocalCorpus\n        return_id: for creating LocalCorpus\n    \"\"\"\n    MODEL_ARGS = {\n        \"min_token_len\": 25,\n        \"verbose\": True,\n        \"return_id\": True,\n    }\n\n    \"\"\"Arguments for STFinetuner.\"\"\"\n    FINETUNE = {\n        \"shuffle\": True,\n        \"batch_size\": 32,\n        \"epochs\": 3,\n        \"warmup_steps\": 100,\n    }\n\n    \"\"\"If no threshold is recommended in evaluations, this is the default \n    minimum score for the sentence index.\n    \"\"\"\n    DEFAULT_THRESHOLD = 0.7\n\n    \"\"\"Makes the default threshold less strict. To use exact default, set to 1.\"\"\"\n    THRESHOLD_MULTIPLIER = 0.8\n\n    SENT_INDEX = \"sent_index_20210715\"\n"}
{"type": "source_file", "path": "gamechangerml/scripts/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/scripts/data_transfer/download_corpus.py", "content": "\"\"\"Script to download the data corpus from S3.\n\nUsage Example:\n    python gamechangerml/scripts/data_transfer/download_corpus.py -c \"corpus_20200909\"\n\nOptions:\n    -c: Directory in S3 that contains the corpus\n\"\"\"\n\nfrom gamechangerml.src.data_transfer import download_corpus_s3\nfrom argparse import ArgumentParser\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Downloads Corpus\")\n    parser.add_argument(\"-c\", dest=\"corpus\", help=\"S3 corpus location\")\n    args = parser.parse_args()\n    corpus = args.corpus\n    download_corpus_s3(corpus)\n"}
{"type": "source_file", "path": "gamechangerml/scripts/combine_entities.py", "content": "import pandas as pd\nfrom gamechangerml import DATA_PATH\nimport os\n\n# simple script to combine agencies (orgs) and topics for ingestion\ntopics_path = os.path.join(DATA_PATH, \"features\", \"topics_wiki.csv\")\nout_path = os.path.join(DATA_PATH, \"features\", \"combined_entities.csv\")\norg_path = os.path.join(DATA_PATH, \"features\", \"agencies.csv\")\ntopics = pd.read_csv(topics_path)\norgs = pd.read_csv(org_path)\norgs.drop(columns=[\"Unnamed: 0\"], inplace=True)\ntopics.rename(columns={\"name\": \"entity_name\",\n              \"type\": \"entity_type\"}, inplace=True)\norgs.rename(columns={\"Agency_Name\": \"entity_name\"}, inplace=True)\norgs[\"entity_type\"] = \"org\"\n\ncombined_ents = orgs.append(topics)\ncombined_ents.to_csv(out_path, index=False)\n"}
{"type": "source_file", "path": "gamechangerml/scripts/data_transfer/download_eval_data.py", "content": "\"\"\"Script to download evaluation data. Requires command-line input.\"\"\"\n\nfrom gamechangerml.src.services import S3Service\nfrom gamechangerml.configs import S3Config\nfrom gamechangerml.src.data_transfer import download_eval_data\nfrom gamechangerml.src.utilities import configure_logger\n\n\nif __name__ == \"__main__\":\n    logger = configure_logger()\n    bucket = S3Service.connect_to_bucket(S3Config.BUCKET_NAME, logger)\n    available_datasets = [\n        \"/\".join(x.split(\"/\")[2:])\n        for x in\n        S3Service.get_object_names(bucket, S3Config.EVAL_DATA_DIR, \"path\")\n    ]\n    logger.info(f\"Available datasets are: {available_datasets}.\")\n\n    # Prompt the user to enter the dataset name and where to save it.\n    dataset = input(\"Which dataset do you want?\\n\")\n    save_dir = input(\"Where should the dataset be saved?\\n\")\n    logger.info(f\"Downloading [{dataset}]...\")\n    \n    if not any([x == \"\" for x in [dataset, save_dir]]):\n        download_eval_data(dataset, save_dir)\n"}
{"type": "source_file", "path": "gamechangerml/configs/qexp_config.py", "content": "from os.path import join\nfrom gamechangerml import REPO_PATH\n\n\nclass QexpConfig:\n    \"\"\"Configurations for the Query Expansion (QE) model.\"\"\"\n\n    \"\"\"Arguments for loading the QE object.\"\"\"\n    INIT_ARGS = {\n        \"qe_files_dir\": join(\n            REPO_PATH, \"gamechangerml\", \"src\", \"search\", \"query_expansion\"\n        ),\n        \"method\": \"emb\",\n    }\n\n    \"\"\"Arguments for getting expanded terms.\"\"\"\n    EXPANSION_ARGS = {\n        \"topn\": 2,\n        \"threshold\": 0.2,\n        \"min_tokens\": 3,\n    }\n\n    \"\"\"Arguments for building the QE model.\"\"\"\n    BUILD_ARGS = {\n        \"num_trees\": 125,\n        \"num_keywords\": 2,\n        \"ngram\": (1, 3),\n        \"abbrv_file\": None,\n        \"merge_word_sim\": True,\n    }\n"}
{"type": "source_file", "path": "gamechangerml/configs/s3_config.py", "content": "from os import getenv\n\n\nclass S3Config:\n    \"\"\"Configurations for S3.\"\"\"\n\n    \"\"\"S3 directory that holds ML models.\"\"\"\n    S3_MODELS_DIR = \"models/v3/\"\n\n    \"\"\"Name of the S3 Bucket to connect to.\"\"\"\n    BUCKET_NAME = getenv(\"AWS_BUCKET_NAME\", default=\"advana-data-zone\")\n\n    \"\"\"S3 directory that holds evaluation data.\"\"\"\n    EVAL_DATA_DIR = \"eval_data/\"\n"}
{"type": "source_file", "path": "gamechangerml/api/utils/status_updater.py", "content": "import logging\nimport threading\nimport typing as t\nfrom gamechangerml.api.utils import processmanager\n\nlogger = logging.getLogger(\"gamechanger\")\n\n\nclass StatusUpdater:\n    \"\"\"\n        Updates process manager for key given in constructor\n        each step takes a message\n        nsteps needs to be known at start\n        will not stop updating if steps > nsteps but the output will be nonsense, e.g. 'step 6 of 4'\n    \"\"\"\n\n    def __init__(self, process_key: str, nsteps: int, log_messages=True):\n        self.key = process_key\n        self.current_step = 1\n        self.nsteps = nsteps\n        self.last_message = None\n        self.log_messages = log_messages\n\n    def next_step(self, message: str = \"\") -> None:\n        try:\n            if self.log_messages:\n                logger.info(message)\n\n            processmanager.update_status(\n                self.key,\n                progress=self.current_step,\n                total=self.nsteps,\n                message=message,\n                thread_id=threading.current_thread().ident\n            )\n            if self.current_step > self.nsteps:\n                logger.warn(f\"StatusUpdater current step larger than nsteps\")\n\n        except Exception as e:\n            logger.warn(\n                f\"StatusUpdater {self.key} failed to update status: {message} \\n{e}\"\n            )\n        finally:\n            self.current_step += 1\n            self.last_message = message\n\n    def current(self) -> dict:\n        return {\n            \"key\": self.key,\n            \"current_step\": self.current_step,\n            \"nsteps\": self.nsteps,\n            \"last_message\": self.last_message,\n        }\n"}
{"type": "source_file", "path": "gamechangerml/scripts/entity_extraction_example.py", "content": "\"\"\"\nusage: entity_extraction_example.py [-h] -m {spacy,hf} [-c CORPUS_DIR]\n\nExample Named Entity Extraction (NER)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m {spacy,hf}, --method {spacy,hf}\n  -c CORPUS_DIR, --corpus-dir CORPUS_DIR\n                        corpus directory\n\n\"\"\"\nimport logging\nimport os\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom pprint import pformat\n\nfrom gamechangerml.src.featurization.ner import NER\nfrom gamechangerml.src.utilities.arg_parser import LocalParser\nfrom gamechangerml.src.utilities.timer import Timer\n\nlogger = logging.getLogger(__name__)\n\nre_list = list()\nre_list.append(re.compile(\"^##\\\\S+\"))\nre_list.append(re.compile(\"^(?:title|section|chapter)\\\\s?\\\\d+?$\", re.I))\n\n\ndef _num_words(entity):\n    return entity.count(\" \")\n\n\ndef _keep_entity(entity):\n    if _num_words(entity.strip()):\n        for regex in re_list:\n            match_obj = re.search(regex, entity)\n            if match_obj is not None:\n                return False\n        return True\n    return False\n\n\ndef _spacy_extract(corpus_dir, ner, entity_dict):\n    with Timer():\n        logger.info(\"finding entities using spaCy NER\")\n        for lbl, ent, file_name, doc_id in ner.extract(corpus_dir):\n            if _keep_entity(ent):\n                k = \"~\".join((lbl, doc_id))\n                entity_dict[k].add(ent)\n\n\ndef _hf_extract(corpus_dir, ner, entity_dict):\n    with Timer():\n        logger.info(\"finding entities with HF NER\")\n        for lbl, ent, file_name, doc_id in ner.extract(corpus_dir):\n            if _keep_entity(ent):\n                k = \"~\".join((lbl, doc_id))\n                entity_dict[k].add(ent)\n\n\ndef main(method, corpus_dir):\n    ner = None\n    if method == \"hf\":\n        ner = NER(model_type=\"hf-transformer\", entity_types=[\"I-ORG\", \"B-ORG\"])\n    elif method == \"spacy\":\n        ner = NER(model_type=\"spacy-large\", entity_types=[\"ORG\", \"LAW\"])\n\n    entity_dict = defaultdict(set)\n    if method == \"spacy\":\n        _spacy_extract(corpus_dir, ner, entity_dict)\n    else:\n        _hf_extract(corpus_dir, ner, entity_dict)\n\n    for k in sorted(entity_dict.keys()):\n        lbl, doc_id = k.split(\"~\")\n        logger.info(\"{}: {}\".format(lbl, doc_id))\n        entity_list = sorted(list(entity_dict[k]))\n        logger.info(pformat(entity_list))\n\n\nif __name__ == \"__main__\":\n    import sys\n\n    log_fmt = (\n        \"[%(asctime)s %(levelname)-8s], [%(filename)s:%(lineno)s - \"\n        + \"%(funcName)s()], %(message)s\"\n    )\n    logging.basicConfig(level=logging.DEBUG, format=log_fmt)\n\n    here = os.path.dirname(os.path.abspath(__file__))\n    p = Path(here)\n\n    c_dir = os.path.join(p.parent, \"data\", \"test_data\")\n\n    parser = LocalParser(description=\"Example Named Entity Extraction (NER)\")\n    parser.add_argument(\n        \"-m\",\n        \"--method\",\n        choices=[\"spacy\", \"hf\"],\n        dest=\"method\",\n        type=str,\n        required=True,\n        default=\"spacy\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--corpus-dir\",\n        dest=\"corpus_dir\",\n        type=str,\n        default=c_dir,\n        help=\"corpus directory\",\n    )\n\n    args = parser.parse_args()\n    sys.exit(main(args.method, c_dir))\n"}
{"type": "source_file", "path": "gamechangerml/scripts/profile_corpus.py", "content": "import json\nimport argparse\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport time\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport functools\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.utils import simple_preprocess\nfrom transformers import BertTokenizer\n#from gamechangerml.src.text_handling.process import preprocess, bert_tokenizer\nfrom gamechangerml import REPO_PATH\n\ncolumns = [\n    \"filename\",  # filename\n    \"doc_type\",  # doc type\n    \"char_len\",  # doc character length\n    \"num_par\",  # number of paragraphs\n    \"doc_token_gensim\",  # number of tokens (using Gensim tokenizer)\n    \"doc_token_bert\",  # number of tokens (using Bert tokenizer)\n    \"par_token_gensim\",  # number of Gensim tokens per paragraph\n    \"par_token_bert\",  # number of Bert tokens per paragraph\n    \"par_char_len\",  # paragraph character length\n]\n\ncolumn_names = \",\".join(columns)\n\n## copied from gamechangerml.src.text_handling.process\nclass bert_tokenizer(object):\n    def __init__(self,\n                 vocab_file):\n        \n        self.vocab_file = vocab_file\n        self.tokenizer = BertTokenizer(vocab_file = self.vocab_file, do_lower_case = True)\n\n    def tokenize(self, text):\n        tokens = self.tokenizer.tokenize(text)\n\n        return tokens, len(tokens)\n\n## copied from gamechangerml.src.text_handling.process\ndef preprocess(\n    text,\n    min_len=2,\n    phrase_detector=None,\n    remove_stopwords=False,\n    additional_stopwords=None,\n):\n    \"\"\"\n        preprocess - standard text processing (possibly break out more if complex preprocessing needed\n        Args:\n            text (str)\n            min_len (int): optional Minimum length of token (inclusive). Shorter tokens are discarded.\n            remove_stopwords (bool)\n            additional_stopwords (list of strings)\n        Returns:\n            tokens (list of strings)\n    \"\"\"\n    tokens = simple_preprocess(text, min_len=min_len, max_len=20)\n\n    if phrase_detector != None:\n        tokens = phrase_detector.apply(tokens)\n\n    if remove_stopwords:\n        if additional_stopwords != None:\n            stopwords_list = STOPWORDS.union(set(additional_stopwords))\n        else:\n            stopwords_list = STOPWORDS\n        tokens = [word for word in tokens if word not in stopwords_list]\n\n    return tokens\n\ndef name_outputs(corpus_files):\n\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    name = \"./corpus_stats_\" + today + \"_\" + str(len(corpus_files))\n\n    return name + \".csv\", name + \".txt\"\n\n\ndef open_file(filename, path):\n    with open(join(path, filename)) as f:\n        return json.load(f)\n\n\ndef get_doc_stats(file):\n\n    doc = open_file(file, corpus_dir)\n    text = doc[\"text\"]\n\n    p_gensim_tokens = []\n    p_bert_tokens = []\n    p_char = []\n\n    for i in range(len(doc[\"paragraphs\"])):\n        p = doc[\"paragraphs\"][i][\"par_raw_text_t\"]\n        p_char.append(str(len(p)))\n        p_gensim_tokens.append(str(len(preprocess(p))))\n        p_bert_tokens.append(str(bert_token.tokenize(p)[1]))\n\n    p_gensim_tokens = \"; \".join(p_gensim_tokens)\n    p_bert_tokens = \"; \".join(p_bert_tokens)\n    p_char = \"; \".join(p_char)\n\n    doc_stats = [\n        file,  # filename\n        doc[\"doc_type\"],  # doc type\n        len(doc[\"text\"]),  # length of text in char\n        len(doc[\"paragraphs\"]),  # number of paragraphs\n        len(preprocess(text)),  # length gensim tokenized\n        bert_token.tokenize(text)[1],  # length bert tokenized\n        p_gensim_tokens,  # list of gensim tokenized paragraph lengths\n        p_bert_tokens,  # list of bert tokenized paragraph lengths\n        p_char,  # list of paragraph lengths (characters)\n    ]\n\n    doc_stats = [str(i) for i in doc_stats]\n\n    return \"\\n\" + \",\".join(doc_stats)\n\n\ndef get_paragraph_stats(df):\n\n    gensim_pars = []\n    for x in df[\"par_token_gensim\"]:\n        if type(x) == str:\n            x = x.lstrip(\"[\").strip(\"]\").split(\";\")\n            x = [int(i.strip().lstrip()) for i in x]\n        gensim_pars.extend(x)\n\n    gensim_pars.sort()\n\n    bert_pars = []\n    for x in df[\"par_token_bert\"]:\n        if type(x) == str:\n            x = x.lstrip(\"[\").strip(\"]\").split(\";\")\n            x = [int(i.strip().lstrip()) for i in x]\n        bert_pars.extend(x)\n\n    bert_pars.sort()\n\n    return gensim_pars, bert_pars\n\n\ndef get_vocab(df):\n\n    fulltext = \"\"\n    vocab_dict = {}\n    srcs = list(df[\"source\"].unique())\n    for i in srcs:\n\n        sub = df[df[\"source\"] == i]\n        subtext = \"\"\n        for j in sub[\"filename\"]:\n\n            text = open_file(j, corpus_dir)[\"text\"]\n\n            subtext += \" \" + text\n            fulltext += \" \" + text\n\n        sub_tokens = len(set(preprocess(subtext)))\n\n        vocab_dict[i] = sub_tokens\n\n    full_tokens = len(set(preprocess(fulltext)))\n    full_tokens_stop = len(set(preprocess(fulltext, remove_stopwords=True)))\n\n    return vocab_dict, full_tokens, full_tokens_stop\n\n\ndef get_agg_df(df, vocab_dict):\n\n    agg_dfs = [\n        df.groupby(\"source\")\n        .agg({\"doc_type\": \"count\"})\n        .reset_index()[[\"source\", \"doc_type\"]],\n        df.groupby(\"source\").count().reset_index()[[\"source\", \"filename\"]],\n        df.groupby(\"source\")\n        .agg({\"doc_token_gensim\": \"sum\"})\n        .reset_index()[[\"source\", \"doc_token_gensim\"]],\n        df.groupby(\"source\")\n        .agg({\"doc_token_gensim\": \"mean\"})\n        .reset_index()[[\"source\", \"doc_token_gensim\"]],\n        df.groupby(\"source\")\n        .agg({\"doc_token_gensim\": \"median\"})\n        .reset_index()[[\"source\", \"doc_token_gensim\"]],\n        df.groupby(\"source\")\n        .agg({\"doc_token_bert\": \"median\"})\n        .reset_index()[[\"source\", \"doc_token_bert\"]],\n        df.groupby(\"source\")\n        .agg({\"doc_token_gensim\": \"std\"})\n        .reset_index()[[\"source\", \"doc_token_gensim\"]],\n        df.groupby(\"source\")\n        .agg({\"num_par\": \"sum\"})\n        .reset_index()[[\"source\", \"num_par\"]],\n        df.groupby(\"source\")\n        .agg({\"num_par\": \"mean\"})\n        .reset_index()[[\"source\", \"num_par\"]],\n        df.groupby(\"source\")\n        .agg({\"num_par\": \"median\"})\n        .reset_index()[[\"source\", \"num_par\"]],\n        df.groupby(\"source\")\n        .agg({\"num_par\": \"std\"})\n        .reset_index()[[\"source\", \"num_par\"]],\n    ]\n\n    agg = functools.reduce(\n        lambda left, right: pd.merge(left, right, on=\"source\", how=\"outer\"), agg_dfs\n    )\n\n    agg.columns = [\n        \"source\",\n        \"doc_type\",\n        \"number_docs\",\n        \"total_tokens\",\n        \"mean_tokens\",\n        \"median_tokens_gensim\",\n        \"median_tokens_bert\",\n        \"std_tokens\",\n        \"total_paragraphs\",\n        \"mean_paragraphs\",\n        \"median_paragraphs\",\n        \"std_paragraphs\",\n    ]\n\n    agg[\"unique_vocab\"] = agg[\"source\"].map(vocab_dict)\n\n    agg = agg.sort_values(by=\"number_docs\", ascending=False)\n\n    return agg\n\n\ndef join_df(corpus_profile: pd.DataFrame, sources: pd.DataFrame) -> pd.DataFrame:\n\n    df = corpus_profile.merge(sources, on=\"doc_type\", how=\"inner\")\n    df[\"group\"].fillna(\"Misc.\", inplace=True)\n\n    return df\n\n\ndef format_counts(sub, sources):\n\n    count = sub.groupby(\"doc_type\").count().reset_index()\n    count = count[[\"doc_type\", \"filename\"]]\n    count = count.merge(sources, on=\"doc_type\")\n    count.sort_values(by=\"filename\", ascending=False, inplace=True)\n    count.rename(\n        columns={\"doc_type\": \"Doc Type\", \"filename\": \"# Documents\", \"source\": \"Source\"},\n        inplace=True,\n    )\n\n    return count[[\"Doc Type\", \"Source\", \"# Documents\", \"Description\"]]\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(description=\"Profile Corpus\")\n\n    parser.add_argument(\"--corpus\", \"-c\", dest=\"corpus\", help=\"local corpus dir\")\n    parser.add_argument(\"--save_dir\", \"-d\", dest=\"save_dir\", help=\"directory to save outputs\")\n\n    args = parser.parse_args()\n\n    corpus_dir = args.corpus\n    save_dir = args.save_dir\n    bert_vocab = os.path.join(REPO_PATH, \"gamechangerml/src/text_handling/assets/bert_vocab.txt\")\n    sources_path = os.path.join(REPO_PATH, \"gamechangerml/scripts/corpus_doctypes.csv\")\n    #save_dir = os.path.join(REPO_PATH, \"gamechangerml/scripts/stats_output/\")\n\n    if not os.path.exists(save_dir):  # make dir to save files\n        os.makedirs(save_dir)\n\n    bert_token = bert_tokenizer(bert_vocab)\n    corpus_files = [f for f in listdir(corpus_dir) if isfile(join(corpus_dir, f))]\n\n    print(\"\\n|--------------Generating stats csv---------------|\\n\")\n\n    start = time.time()\n    doc_count = 0\n    csv_path, txt_path = name_outputs(corpus_files)\n    stats_csv = open(join(save_dir, csv_path), \"w\")\n    stats_csv.write(column_names)\n\n    for i in corpus_files:\n\n        doc_count += 1\n        print(doc_count, i)\n        row = get_doc_stats(i)\n        stats_csv.write(row)\n\n    stats_csv.close()\n\n    end = time.time()\n    print(\"Time to process csv: \", (end - start) / 60, \" minutes\")\n\n    print(\"\\n|--------------Getting vocabularies---------------|\\n\")\n\n    start = time.time()\n\n    dtypes = pd.read_csv(sources_path)\n    profile = pd.read_csv(join(save_dir, csv_path))\n    profile = join_df(profile, dtypes)\n\n    groups = list(profile[\"group\"].unique())\n    sources = list(profile[\"source\"].unique())\n\n    vocab_dict, full_tokens, full_tokens_stop = get_vocab(profile)\n\n    end = time.time()\n    print(\"Time to get vocab by source: \", (end - start) / 60, \" minutes\")\n\n    print(\"\\n|---------------Saving Corpus Stats---------------|\\n\")\n\n    start = time.time()\n\n    gensim_pars, bert_pars = get_paragraph_stats(profile)\n\n    gen_pars_max = gensim_pars[-1:]\n    gen_pars_min = gensim_pars[0]\n    gen_pars_5 = len([i for i in gensim_pars if i <= 5])\n    bert_pars_max = bert_pars[-1:]\n    bert_pars_min = bert_pars[0]\n    bert_pars_5 = len([i for i in bert_pars if i <= 5])\n\n    stats_txt = open(join(save_dir, txt_path), \"w\")\n\n    stats_txt.write(\"\\n\\nNumber of documents in corpus: {}\".format(profile.shape[0]))\n    stats_txt.write(\"\\n\\nMax tokens in a document: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(profile[\"doc_token_gensim\"].max())\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(profile[\"doc_token_bert\"].max())\n    )\n    stats_txt.write(\n        \"\\n\\nMin tokens in a document: {}\".format(profile[\"doc_token_gensim\"].min())\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Number empty docs: {}\".format(\n            profile[profile[\"doc_token_bert\"] == 0].shape[0]\n        )\n    )\n    stats_txt.write(\"\\n\\nMean tokens in a document: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(\n            int(round(profile[\"doc_token_gensim\"].mean()))\n        )\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(\n            int(round(profile[\"doc_token_bert\"].mean()))\n        )\n    )\n    stats_txt.write(\"\\n\\nMedian tokens in a document: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(\n            int(round(profile[\"doc_token_gensim\"].median()))\n        )\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(\n            int(round(profile[\"doc_token_bert\"].median()))\n        )\n    )\n    stats_txt.write(\"\\n\\nSTD tokens in a document: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(\n            int(round(profile[\"doc_token_gensim\"].std()))\n        )\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(\n            int(round(profile[\"doc_token_bert\"].std()))\n        )\n    )\n    stats_txt.write(\"\\n\\nUntrimmed vocabulary size: {}\".format(full_tokens))\n    stats_txt.write(\n        \"\\nTrimmed vocabulary size (stopwords removed): {}\".format(full_tokens_stop)\n    )\n    stats_txt.write(\"\\n\\nNumber of paragraphs in corpus: {}\".format(len(gensim_pars)))\n    stats_txt.write(\"\\n\\nMin tokens in a Paragraph: \")\n    stats_txt.write(\"\\n\\u2022 Gensim tokenization: {}\".format(gen_pars_min))\n    stats_txt.write(\"\\n\\u2022 Bert tokenization: {}\".format(bert_pars_min))\n    stats_txt.write(\"\\n\\nMax tokens in a Paragraph: \")\n    stats_txt.write(\"\\n\\u2022 Gensim tokenization: {}\".format(*gen_pars_max))\n    stats_txt.write(\"\\n\\u2022 Bert tokenization: {}\".format(*bert_pars_max))\n    stats_txt.write(\"\\n\\nParagraphs with <= 5 tokens: \")\n    stats_txt.write(\"\\n\\u2022 Gensim tokenization: {}\".format(gen_pars_5))\n    stats_txt.write(\"\\n\\u2022 Bert tokenization: {}\".format(bert_pars_5))\n    stats_txt.write(\"\\n\\nMean tokens per paragraph: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(int(round(np.mean(gensim_pars))))\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(int(round(np.mean(bert_pars))))\n    )\n    stats_txt.write(\"\\n\\nMedian tokens per paragraph: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(int(round(np.median(gensim_pars))))\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(int(round(np.median(bert_pars))))\n    )\n    stats_txt.write(\"\\n\\nSTD tokens in paragraphs: \")\n    stats_txt.write(\n        \"\\n\\u2022 Gensim tokenization: {}\".format(int(round(np.std(gensim_pars))))\n    )\n    stats_txt.write(\n        \"\\n\\u2022 Bert tokenization: {}\".format(int(round(np.std(bert_pars))))\n    )\n\n    stats_txt.close()\n\n    end = time.time()\n    print(\"Time to save stats: \", (end - start) / 60, \" minutes\")"}
{"type": "source_file", "path": "gamechangerml/scripts/get_wiki_descriptions.py", "content": "## pip install wikipedia\nimport wikipedia\nfrom datetime import date\nimport pandas as pd\nimport argparse\nimport os\nfrom gamechangerml import DATA_PATH\n\ndef lookup_wiki_summary(query):\n    try:\n        return wikipedia.summary(query).replace(\"\\n\", \"\")\n    except:\n        print(f\"Could not retrieve description for {query}\")\n        return \"\"\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description=\"Getting entity descriptions\")\n    \n    parser.add_argument(\"--filepath\", \"-p\", dest=\"filepath\", help=\"path to csv with entities\")\n\n    args = parser.parse_args()\n\n    if args.filepath:\n        entities_filepath = args.filepath\n    else:\n        entities_filepath = os.path.join(DATA_PATH, \"features\", \"combined_entities.csv\")\n    df = pd.read_csv(entities_filepath)\n    df['information'] = df['entity_name'].apply(lambda x: lookup_wiki_summary(x))\n    df['information_source'] = \"Wikipedia\"\n    df['information_retrieved'] = date.today().strftime(\"%Y-%m-%d\")\n    df.to_csv(entities_filepath)\n\n    print(f\"Saved csv to {entities_filepath}\")"}
{"type": "source_file", "path": "gamechangerml/scripts/make_training_data.py", "content": "import random\nimport pandas as pd\nimport os\nimport json\nfrom datetime import date\nfrom typing import List, Union, Dict, Tuple\nimport logging\n\nfrom gamechangerml.configs import (\n    SimilarityConfig,\n    TrainingConfig,\n    ValidationConfig,\n)\nfrom gamechangerml.src.search.sent_transformer.model import SentenceSearcher\nfrom gamechangerml.src.model_testing.query_es import *\nfrom gamechangerml.src.utilities.text_utils import normalize_query\nfrom gamechangerml.src.utilities.test_utils import *\nfrom gamechangerml.api.utils.pathselect import get_model_paths\nfrom gamechangerml.scripts.update_eval_data import make_tiered_eval_data\nfrom gensim.utils import simple_preprocess\nfrom gamechangerml import DATA_PATH, CORPUS_PATH\nfrom gamechangerml.src.utilities import gc_web_api, es_utils\n\nlogger = logging.getLogger(__name__)\n\nmodel_path_dict = get_model_paths()\nrandom.seed(42)\n\nLOCAL_TRANSFORMERS_DIR = model_path_dict[\"transformers\"]\nSIM_MODEL = SimilarityConfig.BASE_MODEL\ntraining_dir = os.path.join(DATA_PATH, \"training\", \"sent_transformer\")\ntts_ratio = TrainingConfig.TRAIN_TEST_SPLIT_RATIO\ngold_standard_path = os.path.join(\n    \"gamechangerml/data/user_data\",\n    ValidationConfig.DATA_ARGS[\"retriever_gc\"][\"gold_standard\"],\n)\n\ncorpus_docs = []\ntry:\n    corpus_docs = [\n        i.split(\".json\")[0]\n        for i in os.listdir(CORPUS_PATH)\n        if os.path.isfile(os.path.join(CORPUS_PATH, i))\n    ]\nexcept Exception as e:\n    logger.error(e)\n\n\nscores = {\n    \"strong_match\": 0.95,\n    \"weak_match\": 0.75,\n    \"neutral\": 0.5,\n    \"negative\": -0.95\n}\n\ngcClient = gc_web_api.GCWebClient()\nesu = es_utils.ESUtils()\n\ndef clean_id(id_1: str) -> str:\n    \"\"\"Normalizes doc ids to compare\"\"\"\n    return id_1.split('.pdf')[0].upper().strip().lstrip()\n\ndef get_matching_es_result(query, doc):\n\n    try:\n        docid = doc + \".pdf_0\"\n        search_query = make_query_one_doc(query, docid)\n        r = esu.client.search(index=esu.es_index, body=dict(search_query))\n        return r\n    except Exception as e:\n        logger.warning(\"Failed to get ES results\")\n        logger.warning(e)\n\ndef get_any_es_result(query):\n\n    try:\n        search_query = make_query(query)\n        r = esu.client.search(index=esu.es_index, body=dict(search_query))\n        return r\n    except Exception as e:\n        logger.warning(\"Failed to get ES results\")\n        logger.warning(e)\n\ndef get_paragraph_results(resp):\n    \"\"\"Get list of paragraph texts for each search result\"\"\"\n\n    texts = []\n    if resp[\"hits\"][\"total\"][\"value\"] > 0:\n        docs = resp[\"hits\"][\"hits\"]\n        for doc in docs:\n            doc_id = \"_\".join(doc[\"fields\"][\"id\"][0].split(\"_\")[:-1])\n            hits = doc[\"inner_hits\"][\"paragraphs\"][\"hits\"][\"hits\"]\n            for par in hits:\n                par_id = doc_id + \"_\" + str(par[\"_nested\"][\"offset\"])\n                par_text = par[\"fields\"][\"paragraphs.par_raw_text_t\"][0]\n                processed = ' '.join(simple_preprocess(par_text, min_len=2, max_len=100))\n                texts.append({\"par_id\": par_id, \"par_text\": processed})\n\n    return texts\n\ndef format_matching_paragraphs(query, doc, uid, score):\n    \"\"\"Retrieve & format matching positive/negative paragraphs from ES\"\"\"\n    found = {}\n    not_found = {}\n    try:\n        matches = get_matching_es_result(query, doc)\n        results = get_paragraph_results(matches)\n        for r in results:\n            offset = r['par_id'].split('_')[-1]\n            uid = uid + \"_\" + offset\n            found[uid] = {\n                \"query\": query,\n                \"doc\": r['par_id'],\n                \"paragraph\": r['par_text'],\n                \"label\": score\n            }\n\n    except Exception as e:\n        logger.error(f\"Could not get results for {query} / {doc}\")\n        logger.error(e)\n        not_found[uid] = {\"query\": query, \"doc\": doc, \"label\": score}\n    \n    return found, not_found\n\ndef format_nonmatching_paragraphs(query, matching_docs, single_matching_docs, par_count):\n    found = {}\n    try:\n        non_matches = get_any_es_result(query)\n        results = get_paragraph_results(non_matches)\n        previous_text = ''\n        logger.info(f\"Positive num: {par_count}\")\n        for r in results:\n            if len(found) >= (par_count * 4): # stop getting negatives after 1:4 ratio\n                logger.info(\"Exceeded balance, stop retrieving paragraphs\")\n                break\n            else:\n                doc_id = r['par_id'].split('.pdf_')[0]\n                if doc_id in matching_docs:\n                    logger.info(\"Paragraph comes from a matching doc, skipping\")\n                else:\n                    if doc_id in single_matching_docs:\n                        label = scores[\"weak_match\"]\n                    else:\n                        label = scores[\"neutral\"]\n                    if r['par_text'] != previous_text: # skip duplicate paragraphs\n                        uid = query + \"_|_\" + r['par_id']\n                        resultdict = {\n                            \"query\": query,\n                            \"doc\": r['par_id'],\n                            \"paragraph\": r['par_text'],\n                            \"label\": label,\n                        }\n                        found[uid] = resultdict\n                        previous_text = r['par_text']\n    except Exception as e:\n        logger.warning(f\"Could not get non-matching results from ES for {query}\")\n    \n    return found\n\ndef get_any_matches(any_matches, matching_docs, query):\n    \"\"\"Collect docs that were clicked on at all for this query (so we can adjust their score)\"\"\"\n    try:\n        single_matching_docs = [clean_id(i) for i in any_matches[query] if clean_id(i) not in matching_docs]\n        logger.info(f\"Found {str(len(single_matching_docs))} other docs opened for this query.\")\n        return single_matching_docs\n    except:\n        return []\n\ndef collect_paragraphs_es(correct, incorrect, queries, collection, any_matches):\n    \"\"\"Query ES for search/doc matches and negative samples and add them to query results with a label\"\"\"\n\n    all_found = {}\n    all_not_found = {}\n    fullcount = 0\n    total = len(correct.keys())\n    for i in correct.keys():\n        found = {}\n        notfound = {}\n        logger.info(f\"{str(fullcount)} / {str(total)}\")\n        fullcount += 1\n        query = queries[i]\n        matching_docs = []\n        par_count = 0\n        for k in correct[i]: # for each possible match, collect positive samples\n            doc = collection[k] # get the docid\n            uid = query + \"_|_\" + doc\n            matching_docs.append(doc)\n            logger.info(f\" *** Querying ES: {query} / {doc} (POS)***\")\n            p_found, p_not_found = format_matching_paragraphs(query, doc, uid, score=scores['strong_match'])\n            found.update(p_found)\n            notfound.update(p_not_found)\n            par_count += len(p_found)\n\n        # check for negative samples\n        if i in list(incorrect.keys()):\n            for n in incorrect[i]:\n                doc = collection[n] # get the docid\n                uid = query + \"_|_\" + doc\n                matching_docs.append(doc)\n                logger.info(f\" *** Querying ES: {query} / {doc} (NEG)***\")\n                n_found, n_not_found = format_matching_paragraphs(query, doc, uid, score=scores['negative'])\n                found.update(n_found)\n                notfound.update(n_not_found)\n\n        if par_count > 0:\n            single_matching_docs = get_any_matches(any_matches, matching_docs, query)\n            neutral_found = format_nonmatching_paragraphs(query, matching_docs, single_matching_docs, par_count)\n            if len(neutral_found) > 0:\n                found.update(neutral_found)\n                all_found.update(found)\n                all_not_found.update(notfound)\n            else:\n                logger.info(f\"\\n**** No non-matching results retrieved for {query}\")\n        else:\n            logger.info(f\"\\n**** No matching results retrieved for {query}\")\n    \n    return all_found, all_not_found\n\n\ndef add_gold_standard(\n    intel: Dict[str, str], gold_standard_path: Union[str, os.PathLike]\n) -> Dict[str, str]:\n    \"\"\"Adds original gold standard data to the intel training data.\n    Args:\n        intel [Dict[str,str]: intelligent search evaluation data\n        gold_standard_path [Union[str, os.PathLike]]: path to load in the manually curated gold_standard.csv\n    Returns:\n        intel [Dict[str,str]: intelligent search evaluation data with manual entries added\n    \"\"\"\n    gold_original = pd.read_csv(gold_standard_path, names=['query', 'document'])\n    logger.info(f\"Reading in {gold_original.shape[0]} queries from the Gold Standard data\")\n\n    def add_extra_queries(intel: Dict[str,str]) -> Dict[str,str]:\n        '''Multiply query/doc pairs to add by using title/filename/id as queries'''\n        extra_queries = []\n        docs = []\n        for doc_id in intel['collection'].values():\n            try:\n                json = open_json(doc_id + '.json', CORPUS_PATH)\n                extra_queries.append(json['display_title_s'])\n                docs.append(doc_id)\n                logger.info(f\"Added extra queries for {doc_id}\")\n            except:\n                logger.warning(f\"Could not add extra queries for {doc_id}\")\n        \n        df = pd.DataFrame()\n        df['query'] = extra_queries\n        df['document'] = docs\n        return df\n    \n    extra_queries_df = add_extra_queries(intel)\n    gold = pd.concat([gold_original, extra_queries_df])\n    gold.reset_index(inplace = True)\n    logger.info(f\"Added {extra_queries_df.shape[0]} extra queries to the Gold Standard\")\n    \n    gold['query_clean'] = gold['query'].apply(lambda x: normalize_query(x))\n    gold['docs_split'] = gold['document'].apply(lambda x: x.split(';'))\n    all_docs = list(set([a for b in gold['docs_split'].tolist() for a in b]))\n\n    def add_key(mydict: Dict[str, str]) -> str:\n        \"\"\"Adds new key to queries/collections dictionaries\"\"\"\n        last_key = sorted([*mydict.keys()])[-1]\n        key_len = len(last_key) - 1\n        last_prefix = last_key[0]\n        last_num = int(last_key[1:])\n        new_num = str(last_num + 1)\n\n        return last_prefix + str(str(0) * (key_len - len(new_num)) + new_num)\n\n    # check if queries already in dict, if not add\n    for i in gold[\"query_clean\"]:\n        if i in intel[\"queries\"].values():\n            logger.info(f\"'{i}' already in intel queries\")\n            continue\n        else:\n            logger.info(f\"adding '{i}' to intel queries\")\n            new_key = add_key(intel[\"queries\"])\n            intel[\"queries\"][new_key] = i\n\n    # check if docs already in dict, if not add\n    for i in all_docs:\n        if i in intel[\"collection\"].values():\n            logger.info(f\"'{i}' already in intel collection\")\n            continue\n        else:\n            logger.info(f\"adding '{i}' to intel collection\")\n            new_key = add_key(intel[\"collection\"])\n            intel[\"collection\"][new_key] = i\n\n    # check if rels already in intel, if not add\n    reverse_q = {v: k for k, v in intel[\"queries\"].items()}\n    reverse_d = {v: k for k, v in intel[\"collection\"].items()}\n    for i in gold.index:\n        q = gold.loc[i, \"query_clean\"]\n        docs = gold.loc[i, \"docs_split\"]\n        for j in docs:\n            q_id = reverse_q[q]\n            d_id = reverse_d[j]\n            if q_id in intel[\"correct\"]:  # if query in rels, add new docs\n                if d_id in intel[\"correct\"][q_id]:\n                    continue\n                else:\n                    intel[\"correct\"][q_id] += [d_id]\n            else:\n                intel[\"correct\"][q_id] = [d_id]\n\n    return intel\n\n\ndef train_test_split(data: Dict[str, str], tts_ratio: float) -> Tuple[Dict[str, str]]:\n    \"\"\"Splits a dictionary into train/test set based on split ratio\"\"\"\n\n    queries = list(set([data[i][\"query\"] for i in data]))\n\n    # split the data into positive and negative examples grouped by query\n    neg_passing = {}\n    pos_passing = {}\n    for q in queries:\n        subset = {i:data[i] for i in data.keys() if data[i]['query']==q}\n        pos_sample = [i for i in subset.keys() if subset[i]['label']==0.95]\n        neg_sample = [i for i in subset.keys() if subset[i]['label']==-0.5]\n        if len(neg_sample)>0: #since we have so few negative samples, add to neg list if it has a negative ex\n            neg_passing[q] = subset\n        elif (\n            len(pos_sample) > 0\n        ):  # only add the other samples if they have a positive matching sample\n            pos_passing[q] = subset\n\n    pos_train_size = round(len(pos_passing.keys()) * tts_ratio)\n    neg_train_size = round(len(neg_passing.keys()) * tts_ratio)\n\n    pos_train_keys = random.sample(pos_passing.keys(), pos_train_size)\n    neg_train_keys = random.sample(neg_passing.keys(), neg_train_size)\n\n    pos_test_keys = [i for i in pos_passing.keys() if i not in pos_train_keys]\n    neg_test_keys = [i for i in neg_passing.keys() if i not in neg_train_keys]\n\n    train_keys = []\n    test_keys = []\n    for x in pos_train_keys:\n        train_keys.extend(pos_passing[x])\n    for x in pos_test_keys:\n        test_keys.extend(pos_passing[x])\n    for x in neg_train_keys:\n        train_keys.extend(neg_passing[x])\n    for x in neg_test_keys:\n        test_keys.extend(neg_passing[x])\n\n    train = {i: data[i] for i in train_keys}\n    test = {i: data[i] for i in test_keys}\n\n    metadata = {\n        \"date_created\": str(date.today()),\n        \"n_queries\": f\"{str(len(pos_train_keys))} train queries / {str(len(pos_test_keys))} test queries\",\n        \"total_train_samples_size\": len(train),\n        \"total_test_samples_size\": len(test),\n        \"split_ratio\": tts_ratio\n    }\n\n    return train, test, metadata\n\ndef get_all_single_matches(validation_dir):\n    directory = os.path.join(validation_dir, \"any\")\n    any_matches = {}\n    try:\n        f = open_json(\"intelligent_search_data.json\", directory)\n        intel = json.loads(f)\n        for x in intel[\"correct\"].keys():\n            query = intel[\"queries\"][x]\n            doc_keys = intel[\"correct\"][x]\n            docs = [intel[\"collection\"][k] for k in doc_keys]\n            any_matches[query] = docs\n    except Exception as e:\n        logger.warning(\"Could not load all validation data\")\n        logger.warning(e)\n\n    return any_matches\n\ndef make_training_data_csv(data, label):\n    \n    df = pd.DataFrame(data).T\n    df['match'] = df['label'].apply(lambda x: 1 if x >= 0.95 else 0)\n    matches = df[df['match']==1]\n    non_matches = df[df['match']==0]\n    \n    \n    def get_docs(mylist):\n        try:\n            return [i.split('.pdf')[0] for i in mylist]\n        except:\n            return []\n\n    def count_unique(mylist):\n    \n        return len(set(get_docs(mylist)))\n    \n    agg_match = pd.DataFrame(matches.groupby('query')['doc'].apply(list))\n    agg_match.rename(columns = {'doc': 'matching_paragraphs'}, inplace = True)\n    agg_match['num_matching_paragraphs'] = agg_match['matching_paragraphs'].apply(lambda x: len(x))\n    agg_match['num_matching_docs'] = agg_match['matching_paragraphs'].apply(lambda x: count_unique(x))\n\n    agg_nonmatch = pd.DataFrame(non_matches.groupby('query')['doc'].apply(list))\n    agg_nonmatch.rename(columns = {'doc': 'nonmatching_paragraphs'}, inplace = True)\n    agg_nonmatch['num_nonmatching_paragraphs'] = agg_nonmatch['nonmatching_paragraphs'].apply(lambda x: len(x))\n    agg_nonmatch['num_nonmatching_docs'] = agg_nonmatch['nonmatching_paragraphs'].apply(lambda x: count_unique(x))\n\n    combined = agg_match.merge(agg_nonmatch, on='query', how = 'outer')\n    combined['label'] = label\n        \n    def check_overlap(list1, list2):\n        return len(set(get_docs(list1)).intersection(get_docs(list2)))\n        \n    combined['overlap'] = [check_overlap(x, y) for x, y in zip(combined['matching_paragraphs'], combined['nonmatching_paragraphs'])]\n    combined['par_balance'] = combined['num_matching_paragraphs'] / combined['num_nonmatching_paragraphs']\n    combined['doc_balance'] = combined['num_matching_docs'] / combined['num_nonmatching_docs']\n\n    combined.fillna(0, inplace = True)\n    \n    return combined\n    \n\ndef make_training_data(\n    index_path: Union[str, os.PathLike],\n    level: str, \n    update_eval_data: bool, \n    testing_only: bool=False,\n    gold_standard_path: Union[str,os.PathLike]=gold_standard_path,\n    tts_ratio: float=tts_ratio,\n    training_dir: Union[str,os.PathLike]=training_dir) -> Tuple[Dict[str,str]]:\n    \"\"\"Makes training data based on new user search history data\n    Args:\n        index_path [str|os.PathLike]: path to the sent index for retrieving the training data (should be most recent index)\n        level [str]: level of eval tier to use for training data (options: ['all', 'silver', 'gold'])\n        update_eval_data [bool]: whether or not to update the eval data before making training data\n        gold_standard_path [Union[str,os.PathLike]]: path to load in the manually curated gold_standard.csv\n        tts_ratio [float]: train/test split ratio, float from 0-1\n        training_dir [Union[str,os.PathLike]]: directory for saving training data\n    Returns:\n        [Tuple[Dict[str,str]]]: training data and training metadata dictionaries\n    \"\"\"\n    # open json files\n    if (\n        not os.path.exists(\n            os.path.join(DATA_PATH, \"validation\", \"domain\", \"sent_transformer\")\n        )\n        or update_eval_data\n    ):\n        logger.info(\"****    Updating the evaluation data\")\n        make_tiered_eval_data(index_path, testing_only)\n\n    validation_dir = get_most_recent_dir(\n        os.path.join(DATA_PATH, \"validation\", \"domain\", \"sent_transformer\")\n    )\n    directory = os.path.join(validation_dir, level)\n    logger.info(\n        f\"****    Loading in intelligent search data from {str(directory)}\")\n    try:\n        f = open_json(\"intelligent_search_data.json\", directory)\n        intel = json.loads(f)\n    except Exception as e:\n        logger.warning(\"Could not load intelligent search data\")\n        logger.warning(e)\n        intel = {}\n\n    # make save_dir\n    timestamp = str(validation_dir).split('/')[-1]\n    save_dir = os.path.join(training_dir, timestamp)\n    os.makedirs(save_dir)\n    logger.info(f\"Created training data save directory {str(save_dir)}\")\n    \n    ## gather all possible matches\n    any_matches = get_all_single_matches(validation_dir)\n\n    # add gold standard samples\n    logger.info(\"****   Adding gold standard examples\")\n    intel = add_gold_standard(intel, gold_standard_path)\n\n    try:\n        found, notfound = collect_paragraphs_es(\n            correct=intel['correct'], \n            incorrect=intel['incorrect'], \n            queries=intel['queries'], \n            collection=intel['collection'],\n            any_matches=any_matches)\n        logger.info(f\"---Number of correct query/result pairs that were not found: {str(len(notfound))}\")\n    except Exception as e:\n        logger.warning(e)\n        logger.warning(\"\\nCould not retrieve positive matches from ES\\n\")\n\n    ## train/test split  \n    train, test, metadata = train_test_split(found, tts_ratio)\n    metadata[\"sent_index_used\"] = index_path\n    metadata[\"validation_data_used\"] = validation_dir\n    metadata[\"not_found_search_pairs\"] = str(len(notfound))\n\n    data = {\"train\": train, \"test\": test}\n\n    logger.info(f\"**** Generated training data: \\n {metadata}\")\n\n    ## Make summary csv of training data\n    train_df = make_training_data_csv(train, \"train\")\n    test_df = make_training_data_csv(test, \"test\")\n    fulldf = pd.concat([train_df, test_df])\n    csv_path = os.path.join(save_dir, \"retrieved_paragraphs.csv\")\n    fulldf.to_csv(csv_path)\n\n    ## save data and metadata files\n    save_json(\"training_data.json\", save_dir, data)\n    save_json(\"training_metadata.json\", save_dir, metadata)\n    save_json(\"not_found_search_pairs.json\", save_dir, notfound)\n\n    logger.info(f\"Finished saving training data files to {save_dir}\")\n\n    return\n"}
{"type": "source_file", "path": "gamechangerml/scripts/ingest_wiki.py", "content": "from datetime import datetime\nfrom elasticsearch import Elasticsearch\n\nfrom gensim.corpora.wikicorpus import WikiCorpus\nwiki_dump = 'simplewiki-20170820-pages-meta-current.xml.bz2'\n\ncorpus = WikiCorpus(wiki_dump)\n\nes = Elasticsearch()\nfor i in corpus.get_texts():\n    print(i)\n    wiki_doc = \" \".join(i)\n    doc = {\"text\": wiki_doc, \"timestamp\": datetime.now()}\n    es.index(index=\"simple-wiki\", body=doc)\n"}
{"type": "source_file", "path": "gamechangerml/scripts/finetune_sentence_retriever.py", "content": "from gamechangerml.src.search.sent_transformer.finetune import STFinetuner\nfrom gamechangerml.configs import EmbedderConfig\nfrom gamechangerml.api.utils.pathselect import get_model_paths\nimport argparse\nimport os\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nmodel_path_dict = get_model_paths()\n\nLOCAL_TRANSFORMERS_DIR = model_path_dict[\"transformers\"]\nBASE_MODEL_NAME = EmbedderConfig.BASE_MODEL\n\ndef main(data_path, model_load_path, model_save_path):\n\n    tuner = STFinetuner(model_load_path=model_load_path, model_save_path=model_save_path, **EmbedderConfig.FINETUNE)\n    return tuner.retrain(data_path)\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(description=\"Finetuning the sentence transformer model\")\n    \n    parser.add_argument(\n        \"--data-path\", \"-d\", \n        dest=\"data_path\", \n        required=True,\n        help=\"path to csv with finetuning data\"\n        )\n\n    parser.add_argument(\n        \"--model-load-path\", \"-m\", \n        dest=\"model_load_path\", \n        required=False,\n        help=\"path to load model for fine-tuning\"\n        )\n\n    parser.add_argument(\n        \"--model-save-path\", \"-s\", \n        dest=\"model_save_path\", \n        required=False,\n        help=\"path to save model after fine-tuning\"\n        )\n\n    args = parser.parse_args()\n\n    ## getting default paths\n    if args.model_load_path:\n        model_load_path = args.model_load_path\n    else:\n        model_load_path = os.path.join(LOCAL_TRANSFORMERS_DIR, BASE_MODEL_NAME)\n\n    if args.model_save_path:\n        model_save_path = args.model_save_path\n    else:\n        model_save_path = model_load_path + str(datetime.now().strftime(\"%Y%m%d\"))\n\n    data_path = args.data_path\n\n    logger.info(\"\\n|---------------------Beginning to finetune model-----------------------|\")\n    \n    main(data_path, model_load_path, model_save_path)\n\n    logger.info(\"|------------------------Done finetuning model--------------------------|\\n\")\n"}
{"type": "source_file", "path": "gamechangerml/data/features/generated_files/__init__.py", "content": "import os\n\n# abs path to this package\nPACKAGE_PATH: str = os.path.dirname(os.path.abspath(__file__))\nGENERATED_FILES_PATH: str = os.path.join(PACKAGE_PATH, 'generated_files')"}
{"type": "source_file", "path": "gamechangerml/data/__init__.py", "content": ""}
{"type": "source_file", "path": "gamechangerml/debug/debug_connector.py", "content": "import os\nfrom gamechangerml.api.fastapi.settings import logger\n\nenv_flag = \"ENABLE_DEBUGGER\"\n\n\ndef check_debug_flagged():\n    flag_str = os.getenv(env_flag, \"false\")\n    return flag_str == 'true'\n\n\ndef debug_if_flagged():\n\n    if check_debug_flagged():\n        try:\n            import debugpy\n            debugger_port = 5678\n            debugpy.listen(('0.0.0.0', debugger_port))\n            logger.info(f\"\\n Debugger listening on {debugger_port}  🥾🦟 \\n\")\n\n            # debugpy.wait_for_client()\n            # debugpy.breakpoint()\n        except Exception as e:\n            import time\n            logger.warning(\"ERROR STARTING DEBUGGER CONNECTION\")\n            time.sleep(3)\n            logger.warning(e)\n            time.sleep(3)\n            logger.info(\n                f\"Debugging can be turned off by removing env variable {env_flag}\")\n    else:\n        logger.info(\"ENABLE_DEBUGGER not set, debugger not started\")\n"}
