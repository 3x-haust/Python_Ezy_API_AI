{"repo_info": {"repo_name": "octogen", "repo_owner": "dbpunk-labs", "repo_url": "https://github.com/dbpunk-labs/octogen"}}
{"type": "test_file", "path": "agent/tests/agent_api_tests.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport os\nimport pytest\nimport asyncio\nimport logging\nimport json\nimport random\nimport logging\nfrom tempfile import gettempdir\nfrom pathlib import Path\nfrom og_sdk.agent_sdk import AgentProxySDK\nfrom og_sdk.utils import random_str\nfrom og_agent import agent_api_server\n\nlogger = logging.getLogger(__name__)\napi_base = \"127.0.0.1:9528\"\napi_key = \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFUH\"\n\n\n@pytest.fixture\ndef agent_sdk():\n    sdk = AgentProxySDK(api_base)\n    sdk.connect()\n    yield sdk\n\n\n@pytest.mark.asyncio\nasync def test_helloworld_test(agent_sdk):\n    await agent_sdk.add_kernel(api_key, \"127.0.0.1:9527\", api_key)\n    agent_api_server.agent_sdk = agent_sdk\n    request = agent_api_server.TaskRequest(\n        prompt=\"hello\", token_limit=0, llm_model_name=\"\", input_files=[], context_id=\"\"\n    )\n    responds = []\n    async for respond in agent_api_server.run_task(request, api_key):\n        json_data = respond[6:]\n        responds.append(json.loads(respond[6:]))\n    logger.debug(f\"{responds}\")\n    assert len(responds) > 0, \"no responds for the prompt\"\n    assert (\n        responds[len(responds) - 1][\"step_type\"]\n        == agent_api_server.StepResponseType.OnFinalAnswer\n    )\n    assert (\n        responds[len(responds) - 1][\"final_answer\"][\"answer\"]\n        == \"how can I help you today?\"\n    )\n\n\n@pytest.mark.asyncio\nasync def test_run_code_test(agent_sdk):\n    agent_api_server.agent_sdk = agent_sdk\n    sdk = agent_sdk\n    await sdk.add_kernel(api_key, \"127.0.0.1:9527\", api_key)\n    request = agent_api_server.TaskRequest(\n        prompt=\"write a hello world in python\",\n        token_limit=0,\n        llm_model_name=\"\",\n        input_files=[],\n        context_id=\"\",\n    )\n    responds = []\n    async for respond in agent_api_server.run_task(request, api_key):\n        responds.append(json.loads(respond[6:]))\n    logger.debug(f\"{responds}\")\n    assert len(responds) > 0, \"no responds for the prompt\"\n    assert (\n        responds[len(responds) - 1][\"step_type\"]\n        == agent_api_server.StepResponseType.OnFinalAnswer\n    )\n    assert (\n        responds[len(responds) - 1][\"final_answer\"][\"answer\"]\n        == \"this code prints 'hello world'\"\n    )\n"}
{"type": "test_file", "path": "agent/tests/openai_agent_tests.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport json\nimport logging\nimport pytest\nfrom og_sdk.kernel_sdk import KernelSDK\nfrom og_agent import openai_agent\nfrom og_proto.agent_server_pb2 import ProcessOptions, TaskResponse, ProcessTaskRequest\nfrom openai.openai_object import OpenAIObject\nimport asyncio\nimport pytest_asyncio\n\napi_base = \"127.0.0.1:9528\"\napi_key = \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFUH\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass PayloadStream:\n\n    def __init__(self, payload):\n        self.payload = payload\n\n    def __aiter__(self):\n        # create an iterator of the input keys\n        self.iter_keys = iter(self.payload)\n        return self\n\n    async def __anext__(self):\n        try:\n            k = next(self.iter_keys)\n            obj = OpenAIObject()\n            delta = OpenAIObject()\n            content = OpenAIObject()\n            content.content = k\n            delta.delta = content\n            obj.choices = [delta]\n            return obj\n        except StopIteration:\n            # raise stopasynciteration at the end of iterator\n            raise StopAsyncIteration\n\n\nclass FunctionCallPayloadStream:\n\n    def __init__(self, name, arguments):\n        self.name = name\n        self.arguments = arguments\n\n    def __aiter__(self):\n        # create an iterator of the input keys\n        self.iter_keys = iter(self.arguments)\n        return self\n\n    async def __anext__(self):\n        try:\n            k = next(self.iter_keys)\n            obj = OpenAIObject()\n            delta = OpenAIObject()\n            function_para = OpenAIObject()\n            function_para.name = self.name\n            function_para.arguments = k\n            function_call = OpenAIObject()\n            function_call.function_call = function_para\n            delta.delta = function_call\n            obj.choices = [delta]\n            return obj\n        except StopIteration:\n            # raise stopasynciteration at the end of iterator\n            raise StopAsyncIteration\n\n\nclass MockContext:\n\n    def done(self):\n        return False\n\n\nclass MultiCallMock:\n\n    def __init__(self, responses):\n        self.responses = responses\n        self.index = 0\n\n    def call(self, *args, **kwargs):\n        if self.index >= len(self.responses):\n            raise Exception(\"no more response\")\n        self.index += 1\n        logger.debug(\"call index %d\", self.index)\n        return self.responses[self.index - 1]\n\n\n@pytest.fixture\ndef kernel_sdk():\n    endpoint = (\n        \"localhost:9527\"  # Replace with the actual endpoint of your test gRPC server\n    )\n    return KernelSDK(endpoint, \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFUH\")\n\n\n@pytest.mark.asyncio\nasync def test_openai_agent_call_execute_bash_code(mocker, kernel_sdk):\n    kernel_sdk.connect()\n    arguments = {\n        \"explanation\": \"the hello world in bash\",\n        \"code\": \"echo 'hello world'\",\n        \"saved_filenames\": [],\n        \"language\": \"bash\",\n    }\n    stream1 = FunctionCallPayloadStream(\"execute\", json.dumps(arguments))\n    sentence = \"The output 'hello world' is the result\"\n    stream2 = PayloadStream(sentence)\n    call_mock = MultiCallMock([stream1, stream2])\n    with mocker.patch(\n        \"og_agent.openai_agent.openai.ChatCompletion.acreate\",\n        side_effect=call_mock.call,\n    ) as mock_openai:\n        agent = openai_agent.OpenaiAgent(\"gpt4\", kernel_sdk, is_azure=False)\n        queue = asyncio.Queue()\n        task_opt = ProcessOptions(\n            streaming=True,\n            llm_name=\"gpt4\",\n            input_token_limit=100000,\n            output_token_limit=100000,\n            timeout=5,\n        )\n        request = ProcessTaskRequest(\n            input_files=[],\n            task=\"write a hello world in bash\",\n            context_id=\"\",\n            options=task_opt,\n        )\n        await agent.arun(request, queue, MockContext(), task_opt)\n        responses = []\n        while True:\n            try:\n                response = await queue.get()\n                if not response:\n                    break\n                responses.append(response)\n            except asyncio.QueueEmpty:\n                break\n        logger.info(responses)\n        console_output = list(\n            filter(\n                lambda x: x.response_type == TaskResponse.OnStepActionStreamStdout,\n                responses,\n            )\n        )\n        assert len(console_output) == 1, \"bad console output count\"\n        assert console_output[0].console_stdout == \"hello world\\n\", \"bad console output\"\n\n\n@pytest.mark.asyncio\nasync def test_openai_agent_call_execute_python_code(mocker, kernel_sdk):\n    kernel_sdk.connect()\n    arguments = {\n        \"explanation\": \"the hello world in python\",\n        \"code\": \"print('hello world')\",\n        \"language\": \"python\",\n        \"saved_filenames\": [],\n    }\n    stream1 = FunctionCallPayloadStream(\"execute\", json.dumps(arguments))\n    sentence = \"The output 'hello world' is the result\"\n    stream2 = PayloadStream(sentence)\n    call_mock = MultiCallMock([stream1, stream2])\n    with mocker.patch(\n        \"og_agent.openai_agent.openai.ChatCompletion.acreate\",\n        side_effect=call_mock.call,\n    ) as mock_openai:\n        agent = openai_agent.OpenaiAgent(\"gpt4\", kernel_sdk, is_azure=False)\n        queue = asyncio.Queue()\n        task_opt = ProcessOptions(\n            streaming=True,\n            llm_name=\"gpt4\",\n            input_token_limit=100000,\n            output_token_limit=100000,\n            timeout=5,\n        )\n        request = ProcessTaskRequest(\n            input_files=[],\n            task=\"write a hello world in python\",\n            context_id=\"\",\n            options=task_opt,\n        )\n        await agent.arun(request, queue, MockContext(), task_opt)\n        responses = []\n        while True:\n            try:\n                response = await queue.get()\n                if not response:\n                    break\n                responses.append(response)\n            except asyncio.QueueEmpty:\n                break\n        logger.info(responses)\n        console_output = list(\n            filter(\n                lambda x: x.response_type == TaskResponse.OnStepActionStreamStdout,\n                responses,\n            )\n        )\n        assert len(console_output) == 1, \"bad console output count\"\n        assert console_output[0].console_stdout == \"hello world\\n\", \"bad console output\"\n\n\n@pytest.mark.asyncio\nasync def test_openai_agent_smoke_test(mocker, kernel_sdk):\n    sentence = \"Hello, how can I help you?\"\n    stream = PayloadStream(sentence)\n    with mocker.patch(\n        \"og_agent.openai_agent.openai.ChatCompletion.acreate\", return_value=stream\n    ) as mock_openai:\n        agent = openai_agent.OpenaiAgent(\"gpt4\", kernel_sdk, is_azure=False)\n        queue = asyncio.Queue()\n        task_opt = ProcessOptions(\n            streaming=True,\n            llm_name=\"gpt4\",\n            input_token_limit=100000,\n            output_token_limit=100000,\n            timeout=5,\n        )\n        request = ProcessTaskRequest(\n            input_files=[], task=\"hello\", context_id=\"\", options=task_opt\n        )\n        await agent.arun(request, queue, MockContext(), task_opt)\n        responses = []\n        while True:\n            try:\n                response = await queue.get()\n                if not response:\n                    break\n                responses.append(response)\n            except asyncio.QueueEmpty:\n                break\n        logger.info(responses)\n        assert len(responses) == len(sentence) + 1, \"bad response count\"\n        assert (\n            responses[-1].response_type == TaskResponse.OnFinalAnswer\n        ), \"bad response type\"\n        assert responses[-1].state.input_token_count == 153\n        assert responses[-1].state.output_token_count == 8\n"}
{"type": "test_file", "path": "agent/tests/tokenizer_test.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport logging\nimport io\nfrom og_agent.tokenizer import tokenize\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_parse_explanation():\n    arguments = \"\"\"{\"function_call\":\"execute\", \"arguments\": {\"explanation\":\"h\"\"\"\n    for token_state, token in tokenize(io.StringIO(arguments)):\n        logger.info(f\"token_state: {token_state}, token: {token}\")\n"}
{"type": "test_file", "path": "chat/tests/test_chat_function.py", "content": "#! /usr/bin/env python3\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom og_terminal.terminal_chat import parse_numbers\nfrom og_terminal.terminal_chat import handle_action_end\nfrom og_terminal.terminal_chat import handle_action_output\nfrom og_terminal.terminal_chat import handle_final_answer\nfrom og_terminal.terminal_chat import handle_typing\nfrom og_terminal.ui_block import TaskBlocks\nfrom og_proto import agent_server_pb2\n\n\ndef test_parse_number():\n    test_text = \"/cc0\"\n    numbers = parse_numbers(test_text)\n    assert numbers\n    assert numbers[0] == \"0\"\n\n\ndef test_handle_final_answer_smoke_test():\n    images = []\n    values = []\n    task_state = agent_server_pb2.ContextState(\n        output_token_count=10,\n        llm_name=\"mock\",\n        total_duration=1,\n        input_token_count=10,\n        llm_response_duration=1000,\n    )\n    respond_content = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnModelTypeText,\n        typing_content=agent_server_pb2.TypingContent(\n            content=\"hello world!\", language=\"text\"\n        ),\n    )\n    respond_final = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnFinalAnswer,\n        final_answer=agent_server_pb2.FinalAnswer(answer=\"\"),\n    )\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n    handle_typing(task_blocks, respond_content)\n    handle_final_answer(task_blocks, respond_final)\n    segments = list(task_blocks.render())\n    assert len(segments) == 1, \"bad segment count\"\n    assert segments[0][1] == \"🧠\"\n    assert values[0] == \"hello world!\"\n\n\ndef test_handle_action_end_boundary_test():\n    # Setup\n    images = []\n    values = []\n    task_state = agent_server_pb2.ContextState(\n        output_token_count=10,\n        llm_name=\"mock\",\n        total_duration=1,\n        input_token_count=10,\n        llm_response_duration=1000,\n    )\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n\n    # Create a response with a large number of output files\n    respond = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnStepActionEnd,\n        on_step_action_end=agent_server_pb2.OnStepActionEnd(\n            output=\"\", output_files=[\"test.png\"] * 1000, has_error=False\n        ),\n    )\n\n    # Call the function\n    handle_action_end(task_blocks, respond, images)\n\n    # Check the results\n    assert len(images) == 1000\n    assert all(image == \"test.png\" for image in images)\n\n\ndef test_handle_action_end_smoke_test():\n    images = []\n    values = []\n    task_state = agent_server_pb2.ContextState(\n        output_token_count=10,\n        llm_name=\"mock\",\n        total_duration=1,\n        input_token_count=10,\n        llm_response_duration=1000,\n    )\n\n    respond_stdout = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnStepActionStreamStdout,\n        console_stdout=\"hello world!\",\n    )\n\n    respond = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnStepActionEnd,\n        on_step_action_end=agent_server_pb2.OnStepActionEnd(\n            output=\"\", output_files=[\"test.png\"], has_error=False\n        ),\n    )\n\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n    handle_action_output(task_blocks, respond_stdout)\n    handle_action_end(task_blocks, respond, images)\n    segments = list(task_blocks.render())\n    assert len(segments) == 2, \"bad segment count\"\n    assert segments[0][1] == \"✅\"\n    assert images[0] == \"test.png\"\n    assert values[0] == \"hello world!\"\n\n\ndef test_error_handle_action_end():\n    images = []\n    values = []\n    task_state = agent_server_pb2.ContextState(\n        output_token_count=10,\n        llm_name=\"mock\",\n        total_duration=1,\n        input_token_count=10,\n        llm_response_duration=1000,\n    )\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n\n    respond_stderr = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnStepActionStreamStderr,\n        console_stderr=\"error\",\n    )\n\n    respond = agent_server_pb2.TaskResponse(\n        state=task_state,\n        response_type=agent_server_pb2.TaskResponse.OnStepActionEnd,\n        on_step_action_end=agent_server_pb2.OnStepActionEnd(\n            output=\"\", output_files=[\"test.png\"], has_error=True\n        ),\n    )\n    handle_action_output(task_blocks, respond_stderr)\n    handle_action_end(task_blocks, respond, images)\n    segments = list(task_blocks.render())\n    assert len(segments) == 2, \"bad segment count\"\n    assert segments[0][1] == \"❌\"\n    assert len(images) == 0\n    assert values[0] == \"\\nerror\"\n\n\ndef test_handle_action_end_performance_test():\n    # Setup\n    images = []\n    values = []\n    task_state = agent_server_pb2.ContextState(\n        output_token_count=10,\n        llm_name=\"mock\",\n        total_duration=1,\n        input_token_count=10,\n        llm_response_duration=1000,\n    )\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n\n    # Create a large number of responses\n    responses = [\n        agent_server_pb2.TaskResponse(\n            state=task_state,\n            response_type=agent_server_pb2.TaskResponse.OnStepActionEnd,\n            on_step_action_end=agent_server_pb2.OnStepActionEnd(\n                output=\"\",\n                output_files=[\n                    f\"test{i}.png\"\n                ],  # Modify this line to create unique filenames\n                has_error=False,\n            ),\n        )\n        for i in range(1000)\n    ]\n\n    # Call the function with each response\n    for respond in responses:\n        handle_action_end(task_blocks, respond, images)\n\n    # Check the results\n    assert len(images) == 1000\n    assert all(image == f\"test{i}.png\" for i, image in enumerate(images))\n"}
{"type": "test_file", "path": "chat/tests/test_parse_files.py", "content": "#! /usr/bin/env python3\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nfrom og_terminal.utils import parse_file_path\n\n\ndef test_parse_file_path():\n    prompt = \"convert the file /up /home/test.pdf to text\"\n    paths = parse_file_path(prompt)\n    assert len(paths) == 1, \"bad file path count\"\n    assert paths[0] == \"/home/test.pdf\", \"bad file path \"\n"}
{"type": "test_file", "path": "kernel/tests/kernel_client_tests.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport json\nimport os\nimport asyncio\nimport random\nimport pytest\nimport logging\nfrom og_kernel.kernel.kernel_mgr import KernelManager\nfrom og_kernel.kernel.kernel_client import KernelClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass MockContext:\n    \"\"\"\n    Mock the grpc request context\n    \"\"\"\n\n    def done(self):\n        return False\n\n\n@pytest.fixture\ndef kernel_manager():\n    config_path = os.path.join(\"/tmp\", str(random.randint(1, 100000)))\n    workspace = os.path.join(\"/tmp\", str(random.randint(1, 100000)))\n    kernel_manager = KernelManager(config_path, workspace)\n    kernel_manager.start()\n    yield kernel_manager\n    kernel_manager.stop()\n\n\n@pytest.fixture\ndef ts_kernel_manager():\n    config_path = os.path.join(\"/tmp\", str(random.randint(1, 100000)))\n    workspace = os.path.join(\"/tmp\", str(random.randint(1, 100000)))\n    kernel_manager = KernelManager(config_path, workspace, \"tslab\")\n    kernel_manager.start()\n    yield kernel_manager\n    kernel_manager.stop()\n\n\n@pytest.mark.asyncio\nasync def test_watching(kernel_manager):\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n\n    async def on_message_fn(msg):\n        if \"text\" in msg:\n            assert msg[\"text\"] == \"Hello, world!\"\n\n    await kernel_client.watching(on_message_fn)\n    for i in range(1):\n        kernel_client.execute(\"print('Hello, world!')\")\n        await asyncio.sleep(1)\n    await asyncio.sleep(10)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n\n\n@pytest.mark.asyncio\nasync def test_result_occurs(kernel_manager):\n    \"\"\"Test stdout occurs\"\"\"\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n    code = \"\"\"\n5\n\"\"\"\n    kernel_client.execute(code)\n    messages = []\n    context = MockContext()\n    async for msg in kernel_client.read_response(context):\n        if not msg:\n            break\n        messages.append(msg)\n    logger.info(f\"{messages}\")\n    filtered = list(filter(lambda x: x[\"msg_type\"] == \"execute_result\", messages))\n    assert len(filtered) > 0\n    await asyncio.sleep(2)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n\n\n@pytest.mark.asyncio\nasync def test_stderr_occurs(kernel_manager):\n    \"\"\"Test stderr occurs\"\"\"\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n    code = \"\"\"\nimport sys\nprint('Hello world', file=sys.stderr)\n\"\"\"\n    kernel_client.execute(code)\n    messages = []\n\n    context = MockContext()\n    async for msg in kernel_client.read_response(context):\n        if not msg:\n            break\n        messages.append(msg)\n    filtered = list(filter(lambda x: x[\"msg_type\"] == \"stream\", messages))\n    assert len(filtered) > 0\n    assert filtered[0][\"content\"][\"name\"] == \"stderr\"\n    await asyncio.sleep(2)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n\n\n@pytest.mark.asyncio\nasync def test_stdout_occurs(kernel_manager):\n    \"\"\"Test stdout occurs\"\"\"\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n    code = \"\"\"\nprint(\"hello world!\")\n\"\"\"\n    kernel_client.execute(code)\n    messages = []\n\n    context = MockContext()\n    async for msg in kernel_client.read_response(context):\n        if not msg:\n            break\n        messages.append(msg)\n    filtered = list(filter(lambda x: x[\"msg_type\"] == \"stream\", messages))\n    assert len(filtered) > 0\n    assert filtered[0][\"content\"][\"name\"] == \"stdout\"\n    await asyncio.sleep(2)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n\n\n@pytest.mark.asyncio\nasync def test_syntax_exception_occurs(kernel_manager):\n    \"\"\"Test exception occurs\"\"\"\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n    code = \"\"\"\na = 10\nb = 20\nif (a < b)\n    print('a is less than b')\n\"\"\"\n    kernel_client.execute(code)\n    messages = []\n\n    context = MockContext()\n    async for msg in kernel_client.read_response(context):\n        if not msg:\n            break\n        messages.append(msg)\n    assert len(list(filter(lambda x: x[\"msg_type\"] == \"error\", messages))) > 0\n    await asyncio.sleep(2)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n\n\n@pytest.mark.asyncio\nasync def test_generate_pie_chart(kernel_manager):\n    \"\"\"Test generate pie output\"\"\"\n    kernel_client = KernelClient(kernel_manager.config_path)\n    await kernel_client.start_client()\n    logger.info(\"is alive %s\", await kernel_client.is_alive())\n\n    code = \"\"\"\nimport matplotlib.pyplot as plt \nimport numpy as np\n\n# Create a pie chart\ndata = np.array([10, 20, 30, 40])\nlabels = ['Category 1', 'Category 2', 'Category 3', 'Category 4']\n\nplt.pie(data, labels=labels, autopct='%1.1f%%')\nplt.title('Pie Chart')\nplt.show() \n\"\"\"\n    kernel_client.execute(code)\n    messages = []\n\n    context = MockContext()\n    async for msg in kernel_client.read_response(context):\n        if msg:\n            logger.debug(f\"{msg}\")\n            messages.append(msg)\n\n    logger.info(f\"{messages}\")\n    assert len(list(filter(lambda x: x[\"msg_type\"] == \"display_data\", messages))) > 0\n    await asyncio.sleep(2)\n    await kernel_client.stop_watch()\n    kernel_client.stop_client()\n"}
{"type": "test_file", "path": "kernel/tests/kernel_mgr_tests.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport pytest\n\nfrom og_kernel.kernel.kernel_mgr import KernelManager\n\n\n@pytest.mark.parametrize(\n    \"config_path, workspace\",\n    [\n        (\"\", \"\"),\n        (\"kernel_connection_file.json\", \"\"),\n        (\"\", \"/tmp/workspace1\"),\n    ],\n)\ndef test_init_with_invalid_args(config_path, workspace):\n    with pytest.raises(ValueError):\n        KernelManager(config_path, workspace)\n\n\ndef test_init_with_valid_args():\n    km = KernelManager(\n        config_path=\"/tmp/kernel_connection_file.json\",\n        workspace=\"/tmp/workspace1\",\n    )\n    assert km.config_path == \"/tmp/kernel_connection_file.json\"\n    assert km.workspace == \"/tmp/workspace1\"\n    assert km.process is None\n    assert not km.is_running\n\n\ndef test_start_kernel():\n    km = KernelManager(\n        config_path=\"/tmp/kernel_connection_file1.json\",\n        workspace=\"/tmp/workspace1\",\n    )\n    km.start()\n    assert km.is_running\n    assert km.process is not None\n    km.stop()\n\n\ndef test_stop_kernel():\n    km = KernelManager(\n        config_path=\"/tmp/kernel_connection_file2.json\",\n        workspace=\"/tmp/workspace2\",\n    )\n    km.start()\n    km.stop()\n    assert not km.is_running\n    assert km.process is None\n"}
{"type": "test_file", "path": "memory/tests/memory_tests.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\"\n\n\"\"\"\nimport json\nfrom og_memory.memory import agent_memory_to_context, AgentMemoryOption\nfrom og_proto.memory_pb2 import AgentMemory, ChatMessage, GuideMemory, Feedback\nfrom og_proto.prompt_pb2 import AgentPrompt, ActionDesc\n# defina a logger variable\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef test_agent_memory_to_context_smoke_test():\n    \"\"\"\n    test the gent_memory_to_contex for smoke test\n    \"\"\"\n\n    action = ActionDesc(name=\"execute_python_code\", desc=\"run python code\", parameters=json.dumps({\n            \"type\": \"object\",\n            \"properties\": {\n                \"explanation\": {\n                    \"type\": \"string\",\n                    \"description\": \"the explanation about the bash code\",\n                },\n                \"code\": {\n                    \"type\": \"string\",\n                    \"description\": \"the bash code to be executed\",\n                },\n                \"language\": {\n                    \"type\": \"string\",\n                    \"description\": \"the language of the code\",\n                },\n                \"saved_filenames\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"string\"},\n                    \"description\": \"A list of filenames that were created by the code\",\n                },\n            },\n            \"required\": [\"explanation\", \"code\", \"language\"],\n        }))   \n    rules = [\"To complete the goal, write a plan and execute it step-by-step, limiting the number of steps to five. the following are examples\", \"rule2\"]\n    prompt = AgentPrompt(actions=[action, action], rules=rules, \n                         role=\"You are the QA engineer\",\n                         role_name=\"Kitty\", output_format=\"\")\n    context = agent_memory_to_context(prompt, [], AgentMemoryOption(show_function_instruction=True))\n    expected_context=\"\"\"You are the QA engineer\nFollow the rules\n1.To complete the goal, write a plan and execute it step-by-step, limiting the number of steps to five. the following are examples\n2.rule2\nUse the following actions to help you finishing your task\n\n1.execute_python_code: run python code, the following are parameters\n    explanation(string):the explanation about the bash code\n    code(string):the bash code to be executed\n    language(string):the language of the code\n    saved_filenames(array):A list of filenames that were created by the code\n    \n2.execute_python_code: run python code, the following are parameters\n    explanation(string):the explanation about the bash code\n    code(string):the bash code to be executed\n    language(string):the language of the code\n    saved_filenames(array):A list of filenames that were created by the code\n    \"\"\"\n    assert context == expected_context, \"context is not expected\"\n"}
{"type": "test_file", "path": "sdk/tests/agent_sdk_tests.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport os\nimport pytest\nimport asyncio\nimport logging\nimport json\nimport random\nimport logging\nfrom tempfile import gettempdir\nfrom pathlib import Path\nfrom og_sdk.agent_sdk import AgentSDK, AgentSyncSDK\nfrom og_sdk.utils import random_str\nfrom og_proto.agent_server_pb2 import TaskResponse\nimport pytest_asyncio\n\nlogger = logging.getLogger(__name__)\napi_base = \"127.0.0.1:9528\"\napi_key = \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFUH\"\n\n\n@pytest.fixture\ndef agent_sync_sdk():\n    sdk = AgentSyncSDK(api_base, api_key)\n    sdk.connect()\n    yield sdk\n    sdk.close()\n\n\n@pytest_asyncio.fixture\nasync def agent_sdk():\n    sdk = AgentSDK(api_base, api_key)\n    sdk.connect()\n    yield sdk\n    await sdk.close()\n\n\ndef test_connect_bad_endpoint():\n    try:\n        sdk = AgentSDK(\"xxx\", api_key)\n        sdk.connect()\n        assert 0, \"should not go here\"\n    except Exception as ex:\n        assert 1\n\n\ndef test_connect_bad_endpoint_for_sync_sdk():\n    try:\n        sdk = AgentSyncSDK(\"xxx\", api_key)\n        sdk.connect()\n        assert 0, \"should not go here\"\n    except Exception as ex:\n        assert 1\n\n\ndef test_connect_bad_kernel_api_key_for_sync_sdk(agent_sync_sdk):\n    try:\n        agent_sync_sdk.add_kernel(\"bad_kernel_api_key\", \"127.0.0.1:9527\")\n        agent_sdk.ping()\n        assert 0, \"should not go here\"\n    except Exception as ex:\n        assert 1\n\n\ndef test_ping_test_for_sync_sdk(agent_sync_sdk):\n    try:\n        agent_sync_sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n        response = agent_sync_sdk.ping()\n        assert response.code == 0\n    except Exception as ex:\n        assert 0, str(ex)\n\n\n@pytest.mark.asyncio\nasync def test_ping_test_with_bad_kernel_api_key(agent_sdk):\n    \"\"\"\n    the ping method will throw an exception if the kernel api key is not valid\n    \"\"\"\n    try:\n        await agent_sdk.add_kernel(\"bad_kernel_api_key\", \"127.0.0.1:9527\")\n        response = await agent_sdk.ping()\n        assert 0, \"should not go here\"\n    except Exception as ex:\n        assert 1\n\n\n@pytest.mark.asyncio\nasync def test_ping_test(agent_sdk):\n    try:\n        await agent_sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n        response = await agent_sdk.ping()\n        assert response.code == 0\n    except Exception as ex:\n        assert 0, str(ex)\n\n\n@pytest.mark.asyncio\nasync def test_upload_and_download_test(agent_sdk):\n    sdk = agent_sdk\n    await sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n    path = os.path.abspath(__file__)\n    # upload file\n    uploaded = await sdk.upload_file(path, \"agent_sdk_tests.py\")\n    assert uploaded\n    file_stats = os.stat(path)\n    assert file_stats.st_size == uploaded.length, \"bad upload_file size\"\n    # download file\n    tmp_dir = gettempdir()\n    fullpath = \"%s%s%s\" % (tmp_dir, os.sep, \"agent_sdk_tests.py\")\n    await sdk.download_file(\"agent_sdk_tests.py\", tmp_dir)\n    file_stats2 = os.stat(fullpath)\n    assert file_stats.st_size == file_stats2.st_size, \"bad download_file size\"\n\n\n@pytest.mark.asyncio\nasync def test_prompt_smoke_test(agent_sdk):\n    sdk = agent_sdk\n    await sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n    try:\n        responds = []\n        async for respond in sdk.prompt(\"hello\"):\n            responds.append(respond)\n        logger.debug(f\"{responds}\")\n        assert len(responds) > 0, \"no responds for the prompt\"\n        assert responds[len(responds) - 1].response_type == TaskResponse.OnFinalAnswer\n        assert (\n            responds[len(responds) - 1].final_answer.answer\n            == \"how can I help you today?\"\n        )\n    except Exception as ex:\n        assert 0, str(ex)\n\n\n@pytest.mark.asyncio\nasync def test_run_code_test(agent_sdk):\n    sdk = agent_sdk\n    await sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n    try:\n        responds = []\n        async for respond in sdk.prompt(\"write a hello world in python\"):\n            responds.append(respond)\n        logger.debug(f\"{responds}\")\n        assert len(responds) > 0, \"no responds for the prompt\"\n        assert responds[len(responds) - 1].response_type == TaskResponse.OnFinalAnswer\n        assert (\n            responds[len(responds) - 1].final_answer.answer\n            == \"this code prints 'hello world'\"\n        )\n    except Exception as ex:\n        assert 0, str(ex)\n\n\n@pytest.mark.asyncio\nasync def test_run_code_with_error(agent_sdk):\n    sdk = agent_sdk\n    await sdk.add_kernel(api_key, \"127.0.0.1:9527\")\n    try:\n        responds = []\n        async for respond in sdk.prompt(\"error function\"):\n            responds.append(respond)\n        logger.debug(f\"{responds}\")\n        assert len(responds) > 0, \"no responds for the prompt\"\n        assert responds[len(responds) - 3].response_type == TaskResponse.OnStepActionEnd\n        assert responds[\n            len(responds) - 3\n        ].on_step_action_end.has_error, \"bad has error result\"\n        assert responds[len(responds) - 1].response_type == TaskResponse.OnFinalAnswer\n        assert (\n            responds[len(responds) - 1].final_answer.answer\n            == \"this code prints 'hello world'\"\n        )\n    except Exception as ex:\n        assert 0, str(ex)\n"}
{"type": "test_file", "path": "sdk/tests/kernel_sdk_tests.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport os\nimport asyncio\nimport pytest\nimport logging\nimport json\nfrom og_sdk.kernel_sdk import KernelSDK\nfrom og_sdk.utils import generate_async_chunk\nfrom og_proto.kernel_server_pb2 import ExecuteResponse\nimport aiofiles\nfrom typing import AsyncIterable\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.fixture\ndef kernel_sdk():\n    endpoint = (\n        \"localhost:9527\"  # Replace with the actual endpoint of your test gRPC server\n    )\n    return KernelSDK(endpoint, \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFUH\")\n\n\n@pytest.fixture\ndef bad_kernel_sdk():\n    endpoint = (\n        \"localhost:9527\"  # Replace with the actual endpoint of your test gRPC server\n    )\n    return KernelSDK(endpoint, \"ZCeI9cYtOCyLISoi488BgZHeBkHWuFU\")\n\n\n@pytest.mark.asyncio\nasync def test_bad_sdk(bad_kernel_sdk):\n    try:\n        kernel_sdk.connect()\n        assert kernel_sdk.stub is not None  # Check that stub is initialized\n        await kernel_sdk.start()\n        assert False\n    except Exception as e:\n        assert True\n\n\n@pytest.mark.asyncio\nasync def test_upload_and_download_smoke_test(kernel_sdk):\n    kernel_sdk.connect()\n    path = os.path.abspath(__file__)\n    response = await kernel_sdk.upload_binary(\n        generate_async_chunk(path, \"kernel_sdk_tests.py\")\n    )\n    assert response\n    file_stats = os.stat(path)\n    assert response.length == file_stats.st_size, \"bad upload file size\"\n    length = 0\n    async for chunk in kernel_sdk.download_file(\"kernel_sdk_tests.py\"):\n        length += len(chunk.buffer)\n    assert length == file_stats.st_size, \"bad upload file size\"\n\n\n@pytest.mark.asyncio\nasync def test_stop_kernel(kernel_sdk):\n    kernel_sdk.connect()\n    assert kernel_sdk.stub is not None  # Check that stub is initialized\n    if not await kernel_sdk.is_alive():\n        await kernel_sdk.start()\n    assert await kernel_sdk.is_alive()\n    response = await kernel_sdk.stop()\n    assert response.code == 0\n    assert not await kernel_sdk.is_alive()\n\n\n@pytest.mark.asyncio\nasync def test_sdk_smoke_test(kernel_sdk):\n    kernel_sdk.connect()\n    assert kernel_sdk.stub is not None  # Check that stub is initialized\n    if not await kernel_sdk.is_alive():\n        await kernel_sdk.start()\n    code = \"\"\"print('hello world!')\"\"\"\n    responds = []\n    async for respond in kernel_sdk.execute(code):\n        responds.append(respond)\n    await kernel_sdk.stop()\n    assert len(responds) == 1\n    assert responds[0].output_type == ExecuteResponse.StdoutType\n    assert json.loads(responds[0].output)[\"text\"] == \"hello world!\\n\"\n\n\n@pytest.mark.asyncio\nasync def test_sdk_result_test(kernel_sdk):\n    kernel_sdk.connect()\n    assert kernel_sdk.stub is not None  # Check that stub is initialized\n    if not await kernel_sdk.is_alive():\n        await kernel_sdk.start()\n    code = \"\"\"print('hello world!')\n5\"\"\"\n    responds = []\n    async for respond in kernel_sdk.execute(code):\n        responds.append(respond)\n    await kernel_sdk.stop()\n    assert len(responds) == 2\n    assert responds[0].output_type == ExecuteResponse.StdoutType\n    assert responds[1].output_type == ExecuteResponse.ResultType\n    assert json.loads(responds[0].output)[\"text\"] == \"hello world!\\n\"\n    assert json.loads(responds[1].output)[\"text/plain\"] == \"5\"\n"}
{"type": "test_file", "path": "sdk/tests/utils_test.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport pytest\nimport json\nfrom og_sdk.utils import process_char_stream\n\n\ndef test_process_char_stream_case2():\n    stream1 = \"\\rt:   0%|          | 0/518 [00:00<?, ?it/s, now=None]\"\n    output1 = process_char_stream(stream1)\n    stream2 = \"\\rt:   3%|▎         | 15/518 [00:00<00:03, 137.85it/s, now=None]\"\n    output2 = process_char_stream(output1 + stream2)\n    assert output2 == \"t:   3%|▎         | 15/518 [00:00<00:03, 137.85it/s, now=None]\"\n\n\ndef test_process_char_stream():\n    stream0 = \"  Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\\r\\n\\x1b[?25l     \\x1b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.1 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m\"\n    stream1 = \"\\r\\x1b[2K     \\x1b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.1 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m\"\n    output1 = process_char_stream(stream0 + stream1)\n    output2 = process_char_stream(output1 + stream1)\n    assert output1 == output2\n    final_stream = \"\\r1.1 MB 100%\\r\\n\"\n    output3 = process_char_stream(output2 + final_stream)\n    final_ouput_expected = (\n        \"  Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\\n1.1 MB 100%\\n\"\n    )\n    assert final_ouput_expected == output3\n\n\ndef test_empty_string():\n    assert process_char_stream(\"\") == \"\"\n\n\ndef test_single_character():\n    assert process_char_stream(\"a\") == \"a\"\n\n\ndef test_multiple_characters():\n    assert process_char_stream(\"abc\") == \"abc\"\n\n\ndef test_backspace():\n    assert process_char_stream(\"ab\\b\") == \"a\"\n\n\ndef test_carriage_return():\n    assert process_char_stream(\"ab\\r\") == \"ab\"\n\n\ndef test_carriage_return_with_newline():\n    assert process_char_stream(\"ab\\r\\n\") == \"ab\\n\"\n\n\ndef test_backspace_and_carriage_return():\n    assert process_char_stream(\"ab\\b\\r\") == \"a\"\n\n\ndef test_mixed_escape_characters_and_regular_characters():\n    assert process_char_stream(\"ab\\b\\r\\ncde\") == \"a\\ncde\"\n\n\ndef test_special_characters():\n    assert (\n        process_char_stream(\"ab!@#$%^&*()_+{}|:\\\";'<>,.?/`~\")\n        == \"ab!@#$%^&*()_+{}|:\\\";'<>,.?/`~\"\n    )\n"}
{"type": "test_file", "path": "up/tests/up_tests.py", "content": "#! /usr/bin/env python3\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport os\nimport sys\nimport pytest\nimport tempfile\nimport logging\nfrom rich.live import Live\nfrom rich.console import Console\nfrom og_up.up import run_with_realtime_print\nfrom og_up.up import download_model\nfrom og_up.up import load_docker_image\nfrom og_up.up import get_latest_release_version\nfrom og_up.up import start_octogen_for_codellama\nfrom og_up.up import start_octogen_for_azure_openai\nfrom og_up.up import start_octogen_for_openai\nfrom og_up.up import random_str\nfrom og_up.up import generate_agent_common, generate_agent_azure_openai, generate_agent_openai, generate_agent_codellama\nfrom og_up.up import generate_kernel_env\nfrom og_up.up import check_the_env\nfrom og_up.up import run_install_cli\nfrom rich.console import Group\nfrom dotenv import dotenv_values\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_random_dir():\n    fullpath = os.getcwd() + \"/\" + random_str(32)\n    os.makedirs(fullpath, exist_ok=True)\n    return fullpath\n\n\ndef test_generate_kernel_env():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n        rpc_key = \"rpc_key\"\n        generate_kernel_env(live, segments, temp_dir, rpc_key)\n        fullpath = f\"{temp_dir}/kernel/.env\"\n        config = dotenv_values(fullpath)\n        assert config[\"rpc_key\"] == rpc_key, \"bad rpc key\"\n        assert config[\"rpc_port\"] == \"9527\", \"bad rpc port\"\n\n\ndef test_generate_agent_codellama():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n        admin_key = \"admin_key\"\n        generate_agent_codellama(live, segments, temp_dir, admin_key)\n        fullpath = f\"{temp_dir}/agent/.env\"\n        config = dotenv_values(fullpath)\n        assert config[\"llm_key\"] == \"codellama\", \"bad llm key\"\n        assert (\n            config[\"llama_api_base\"] == \"http://127.0.0.1:8080\"\n        ), \"bad codellama server endpoint\"\n        assert config[\"admin_key\"] == admin_key, \"bad admin key\"\n\n\ndef test_generate_agent_env_openai():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n        admin_key = \"admin_key\"\n        openai_key = \"openai_key\"\n        model = \"gpt-4-0613\"\n        generate_agent_openai(live, segments, temp_dir, admin_key, openai_key, model)\n        fullpath = f\"{temp_dir}/agent/.env\"\n        config = dotenv_values(fullpath)\n        assert config[\"llm_key\"] == \"openai\", \"bad llm key\"\n        assert config[\"openai_api_key\"] == openai_key, \"bad api key\"\n        assert config[\"openai_api_model\"] == model, \"bad model\"\n        assert config[\"admin_key\"] == admin_key, \"bad admin key\"\n\n\ndef test_generate_agent_env_azure_openai():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n        admin_key = \"admin_key\"\n        openai_key = \"openai_key\"\n        deployment = \"octogen\"\n        api_base = \"azure\"\n        generate_agent_azure_openai(\n            live, segments, temp_dir, admin_key, openai_key, deployment, api_base\n        )\n        fullpath = f\"{temp_dir}/agent/.env\"\n        config = dotenv_values(fullpath)\n        assert config[\"llm_key\"] == \"azure_openai\", \"bad llm key\"\n        assert config[\"openai_api_base\"] == api_base, \"bad api base\"\n        assert config[\"openai_api_key\"] == openai_key, \"bad api key\"\n        assert config[\"openai_api_deployment\"] == deployment, \"bad deployment\"\n        assert config[\"admin_key\"] == admin_key, \"bad admin key\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_check_the_env():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        result, msg = check_the_env(live, segments)\n        assert result\n\n\ndef test_check_the_env_win():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        result, msg = check_the_env(live, segments, need_container=False)\n        assert result\n\n\ndef test_run_print():\n    use_dir = os.path.expanduser(\"~\")\n    command = [\"ls\", use_dir]\n    result_code = 0\n    for code, output in run_with_realtime_print(command):\n        result_code = code\n    assert code == 0, \"bad return code\"\n\n\ndef test_download_model():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        result_code = download_model(\n            live,\n            segments,\n            repo=\"TheBloke/CodeLlama-7B-Instruct-GGUF\",\n            filename=\"codellama-7b-instruct.Q2_K.gguf\",\n        )\n        assert result_code == 0, \"fail to download model\"\n\n\ndef test_get_version():\n    repo = \"imotai/test_repos\"\n    console = Console()\n    segments = []\n    temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n    with Live(Group(*segments), console=console) as live:\n        version = get_latest_release_version(repo, live, segments)\n        assert \"v0.1.0\" == version\n\n\ndef test_install_cli():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        assert run_install_cli(live, segments)\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_start_azure_openai_smoketest():\n    console = Console()\n    segments = []\n    install_dir = create_random_dir()\n    cli_install_dir = create_random_dir()\n    admin_key = random_str(32)\n    kernel_key = random_str(32)\n    with Live(Group(*segments), console=console) as live:\n        generate_kernel_env(live, segments, install_dir, kernel_key)\n        code = load_docker_image(\"v0.5.2\", \"dbpunk/octogen\", live, segments)\n        assert code == 0, \"bad result code of loading docker image\"\n        result = start_octogen_for_azure_openai(\n            live,\n            segments,\n            install_dir,\n            cli_install_dir,\n            admin_key,\n            kernel_key,\n            \"dbpunk/octogen\",\n            \"v0.5.2\",\n            \"azure_open_api_key\",\n            \"test_deployment\",\n            \"https://azure_base\",\n        )\n        assert result\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_start_openai_smoketest():\n    console = Console()\n    segments = []\n    install_dir = create_random_dir()\n    cli_install_dir = create_random_dir()\n\n    admin_key = random_str(32)\n    kernel_key = random_str(32)\n    with Live(Group(*segments), console=console) as live:\n        generate_kernel_env(live, segments, install_dir, kernel_key)\n        code = load_docker_image(\"v0.5.2\", \"dbpunk/octogen\", live, segments)\n        assert code == 0, \"bad result code of loading docker image\"\n        result = start_octogen_for_openai(\n            live,\n            segments,\n            install_dir,\n            cli_install_dir,\n            admin_key,\n            kernel_key,\n            \"dbpunk/octogen\",\n            \"v0.5.2\",\n            \"openai_api_key\",\n            \"gpt-3.5-turbo\",\n        )\n        assert result\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_start_codellama_smoketest():\n    console = Console()\n    segments = []\n    install_dir = create_random_dir()\n    cli_install_dir = create_random_dir()\n    admin_key = random_str(32)\n    kernel_key = random_str(32)\n    with Live(Group(*segments), console=console) as live:\n        generate_kernel_env(live, segments, install_dir, kernel_key)\n        code = load_docker_image(\"v0.5.2\", \"dbpunk/octogen\", live, segments)\n        assert code == 0, \"bad result code of loading docker image\"\n        result = start_octogen_for_codellama(\n            live,\n            segments,\n            \"TheBloke/CodeLlama-7B-Instruct-GGUF\",\n            \"codellama-7b-instruct.Q2_K.gguf\",\n            install_dir,\n            cli_install_dir,\n            admin_key,\n            kernel_key,\n            \"dbpunk/octogen\",\n            \"v0.5.2\",\n        )\n        assert result\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_load_bad_docker_image():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        code = load_docker_image(\"xxx\", \"xxxx\", live, segments)\n        assert code != 0, \"loading image should be failed\"\n\n\n@pytest.mark.skipif(sys.platform.startswith(\"win\"), reason=\"skip on windows\")\ndef test_load_valid_docker_image():\n    console = Console()\n    segments = []\n    with Live(Group(*segments), console=console) as live:\n        code = load_docker_image(\"v0.5.2\", \"dbpunk/octogen\", live, segments)\n        assert code == 0, \"loading image should be ok\"\n"}
{"type": "source_file", "path": "agent/setup.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_agent\",\n    version=\"0.3.6\",\n    description=\"Open source code interpreter agent\",\n    author=\"imotai\",\n    author_email=\"wangtaize@dbpunk.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    packages=[\n        \"og_agent\",\n    ],\n    package_dir={\n        \"og_agent\": \"src/og_agent\",\n    },\n    install_requires=[\n        \"og_proto\",\n        \"og_kernel\",\n        \"og_sdk\",\n        \"grpcio-tools>=1.57.0\",\n        \"grpc-google-iam-v1>=0.12.6\",\n        \"aiofiles\",\n        \"orm[sqlite]\",\n        \"python-dotenv\",\n        \"openai\",\n        \"aiohttp>=3.8.5\",\n        \"pydantic\",\n        \"tiktoken\",\n        \"fastapi\",\n        \"uvicorn\",\n    ],\n    package_data={\"og_agent\": [\"*.bnf\"]},\n    entry_points={\n        \"console_scripts\": [\n            \"og_agent_rpc_server = og_agent.agent_server:server_main\",\n            \"og_agent_setup = og_agent.agent_setup:setup\",\n            \"og_agent_http_server = og_agent.agent_api_server:run_app\",\n        ]\n    },\n)\n"}
{"type": "source_file", "path": "agent/src/og_agent/__init__.py", "content": ""}
{"type": "source_file", "path": "agent/src/og_agent/agent_builder.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport json\nfrom .llama_agent import LlamaAgent\nfrom .openai_agent import OpenaiAgent\nfrom .llama_client import LlamaClient\nfrom .mock_agent import MockAgent\n\n\ndef build_llama_agent(endpoint, key, sdk, grammer_path):\n    \"\"\"\n    build llama agent\n    \"\"\"\n    with open(grammer_path, \"r\") as fd:\n        grammar = fd.read()\n    client = LlamaClient(endpoint, key, grammar)\n    # init the agent\n    return LlamaAgent(client, sdk)\n\n\ndef build_openai_agent(sdk, model_name, is_azure=True):\n    \"\"\"build openai function call agent\"\"\"\n    # TODO a data dir per user\n    # init the agent\n\n    agent = OpenaiAgent(model_name, sdk, is_azure=is_azure)\n    return agent\n\n\ndef build_mock_agent(sdk, cases_path):\n    \"\"\"\n    build the mock agent for testing\n    \"\"\"\n    with open(cases_path, \"r\") as fd:\n        messages = json.load(fd)\n    agent = MockAgent(messages, sdk)\n    return agent\n"}
{"type": "source_file", "path": "agent/src/og_agent/agent_api_server.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport sys\nimport asyncio\nimport uvicorn\nimport json\nimport logging\nfrom typing import List\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI, status, Response\nfrom og_sdk.agent_sdk import AgentProxySDK\nfrom og_proto import agent_server_pb2\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.param_functions import Header, Annotated\nfrom dotenv import dotenv_values\n\n# the api server config\nconfig = dotenv_values(\".env\")\n\nLOG_LEVEL = (\n    logging.DEBUG if config.get(\"log_level\", \"info\") == \"debug\" else logging.INFO\n)\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(__name__)\n\napp = FastAPI()\n# the agent endpoint\nlisten_addr = \"%s:%s\" % (\n    config.get(\"rpc_host\", \"127.0.0.1\"),\n    config.get(\"rpc_port\", \"9528\"),\n)\nif config.get(\"rpc_host\", \"\") == \"0.0.0.0\":\n    listen_addr = \"127.0.0.1:%s\" % config.get(\"rpc_port\", \"9528\")\nagent_sdk = AgentProxySDK(listen_addr)\n\n\nclass StepResponseType(str, Enum):\n    OnStepActionStart = \"OnStepActionStart\"\n    OnStepTextTyping = \"OnStepTextTyping\"\n    OnStepCodeTyping = \"OnStepCodeTyping\"\n    OnStepActionStdout = \"OnStepActionStdout\"\n    OnStepActionStderr = \"OnStepActionStderr\"\n    OnStepActionEnd = \"OnStepActionEnd\"\n    OnFinalAnswer = \"OnFinalAnswer\"\n\n\nclass ContextState(BaseModel):\n    output_token_count: int\n    llm_name: str\n    total_duration: int\n    output_token_count: int\n    llm_response_duration: int\n    context_id: str | None = None\n\n    @classmethod\n    def new_from(cls, state):\n        return cls(\n            output_token_count=state.output_token_count,\n            llm_name=state.llm_name,\n            total_duration=state.total_duration,\n            input_token_count=state.input_token_count,\n            llm_response_duration=state.llm_response_duration,\n        )\n\n\nclass StepActionEnd(BaseModel):\n    output: str\n    output_files: List[str]\n    has_error: bool\n\n    @classmethod\n    def new_from(cls, step_action_end: agent_server_pb2.OnStepActionEnd):\n        return cls(\n            output=step_action_end.output,\n            output_files=step_action_end.output_files,\n            has_error=step_action_end.has_error,\n        )\n\n\nclass FinalAnswer(BaseModel):\n    answer: str\n\n    @classmethod\n    def new_from(cls, final_answer: agent_server_pb2.FinalAnswer):\n        return cls(answer=final_answer.answer)\n\n\nclass StepActionStart(BaseModel):\n    input: str\n    tool: str\n\n    @classmethod\n    def new_from(cls, step_action_start: agent_server_pb2.OnStepActionStart):\n        return cls(input=step_action_start.input, tool=step_action_start.tool)\n\n\nclass StepResponse(BaseModel):\n    step_type: StepResponseType\n    step_state: ContextState\n    typing_content: str | None = None\n    step_action_stdout: str | None = None\n    step_action_stderr: str | None = None\n    step_action_start: StepActionStart | None = None\n    step_action_end: StepActionEnd | None = None\n    final_answer: FinalAnswer | None = None\n\n    @classmethod\n    def new_from(cls, response: agent_server_pb2.TaskResponse):\n        if response.response_type == agent_server_pb2.TaskResponse.OnStepActionStart:\n            return cls(\n                step_type=StepResponseType.OnStepActionStart,\n                step_state=ContextState.new_from(response.state),\n                step_action_start=StepActionStart.new_from(\n                    response.on_step_action_start\n                ),\n            )\n        elif response.response_type == agent_server_pb2.TaskResponse.OnModelTypeCode:\n            return cls(\n                step_type=StepResponseType.OnStepCodeTyping,\n                step_state=ContextState.new_from(response.state),\n                typing_content=response.typing_content.content,\n            )\n\n        elif response.response_type == agent_server_pb2.TaskResponse.OnModelTypeText:\n            return cls(\n                step_type=StepResponseType.OnStepTextTyping,\n                step_state=ContextState.new_from(response.state),\n                typing_content=response.typing_content.content,\n            )\n        elif (\n            response.response_type\n            == agent_server_pb2.TaskResponse.OnStepActionStreamStdout\n        ):\n            return cls(\n                step_type=StepResponseType.OnStepActionStdout,\n                step_state=ContextState.new_from(response.state),\n                step_action_stdout=response.console_stdout,\n            )\n        elif (\n            response.response_type\n            == agent_server_pb2.TaskResponse.OnStepActionStreamStderr\n        ):\n            return cls(\n                step_type=StepResponseType.OnStepActionStderr,\n                step_state=ContextState.new_from(response.state),\n                step_action_stderr=response.console_stderr,\n            )\n        elif response.response_type == agent_server_pb2.TaskResponse.OnStepActionEnd:\n            return cls(\n                step_type=StepResponseType.OnStepActionEnd,\n                step_state=ContextState.new_from(response.state),\n                step_action_end=StepActionEnd.new_from(response.on_step_action_end),\n            )\n        elif response.response_type == agent_server_pb2.TaskResponse.OnFinalAnswer:\n            return cls(\n                step_type=StepResponseType.OnFinalAnswer,\n                step_state=ContextState.new_from(response.state),\n                final_answer=FinalAnswer.new_from(response.final_answer),\n            )\n\n\nclass TaskRequest(BaseModel):\n    prompt: str\n    token_limit: int\n    llm_model_name: str\n    input_files: List[str]\n    context_id: str\n\n\nasync def run_task(task: TaskRequest, key):\n    async for respond in agent_sdk.prompt(\n        task.prompt, key, files=task.input_files, context_id=task.context_id\n    ):\n        response = StepResponse.new_from(respond).model_dump(exclude_none=True)\n        yield \"data: %s\\n\" % json.dumps(response)\n\n\n@app.post(\"/process\")\nasync def process_task(\n    task: TaskRequest,\n    response: Response,\n    api_token: Annotated[str | None, Header()] = None,\n):\n    if api_token is None:\n        response.status_code = status.HTTP_401_UNAUTHORIZED\n        return\n    response.status_code = status.HTTP_200_OK\n    response.media_type = \"text/event-stream\"\n    agent_sdk.connect()\n    return StreamingResponse(run_task(task, api_token))\n\n\nasync def run_server():\n    logger.info(f\"connect the agent server at {listen_addr}\")\n    port = int(config.get(\"rpc_port\", \"9528\")) + 1\n    server_config = uvicorn.Config(\n        app, host=config.get(\"rpc_host\", \"127.0.0.1\"), port=port\n    )\n    server = uvicorn.Server(server_config)\n    await server.serve()\n\n\ndef run_app():\n    asyncio.run(run_server())\n"}
{"type": "source_file", "path": "agent/src/og_agent/agent_server.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport asyncio\nimport logging\nimport sys\nimport os\nimport pathlib\nimport hashlib\nimport grpc\nimport json\nfrom grpc.aio import AioRpcError\nfrom og_proto.agent_server_pb2_grpc import AgentServerServicer\nfrom og_proto.agent_server_pb2_grpc import add_AgentServerServicer_to_server\nfrom og_proto import agent_server_pb2\nfrom og_proto import common_pb2\nfrom og_proto import kernel_server_pb2\nfrom dotenv import dotenv_values\nfrom typing import AsyncIterable, Any, Dict, List, Optional, Sequence, Union, Type\nfrom tempfile import gettempdir\nfrom grpc.aio import ServicerContext, server\nfrom og_sdk.kernel_sdk import KernelSDK\nfrom og_sdk.utils import parse_image_filename\nfrom .agent_llm import LLMManager\nfrom .agent_builder import build_mock_agent, build_openai_agent, build_llama_agent\nimport databases\nimport orm\nfrom datetime import datetime\n\nconfig = dotenv_values(\".env\")\n\nLOG_LEVEL = (\n    logging.DEBUG if config.get(\"log_level\", \"info\") == \"debug\" else logging.INFO\n)\n\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(__name__)\n# the database instance for agent\ndatabase = databases.Database(\"sqlite:///%s\" % config[\"db_path\"])\nmodels = orm.ModelRegistry(database=database)\n\n\nclass LiteApp(orm.Model):\n    tablename = \"lite_app\"\n    registry = models\n    fields = {\n        \"id\": orm.Integer(primary_key=True),\n        \"key_hash\": orm.String(max_length=64, index=True),\n        \"name\": orm.String(max_length=20, index=True),\n        \"language\": orm.String(max_length=20, allow_null=False),\n        \"code\": orm.Text(),\n        \"time\": orm.DateTime(),\n        \"desc\": orm.String(max_length=100, allow_null=True),\n        \"saved_filenames\": orm.String(max_length=512, allow_null=True),\n    }\n\n\nclass AgentRpcServer(AgentServerServicer):\n\n    def __init__(self):\n        self.agents = {}\n        self.max_file_size = int(config[\"max_file_size\"])\n        self.verbose = config.get(\"verbose\", False)\n        self.llm_manager = LLMManager(config)\n        self.llm = self.llm_manager.get_llm()\n\n    async def ping(\n        self, request: agent_server_pb2.PingRequest, context: ServicerContext\n    ) -> agent_server_pb2.PongResponse:\n        metadata = dict(context.invocation_metadata())\n        if (\n            \"api_key\" not in metadata\n            or metadata[\"api_key\"] not in self.agents\n            or not self.agents[metadata[\"api_key\"]]\n        ):\n            return agent_server_pb2.PongResponse(\n                code=-1, msg=\"Your API Key is invalid!\"\n            )\n        else:\n            return agent_server_pb2.PongResponse(\n                code=0, msg=\"Connect to Octopus Agent Ok!\"\n            )\n\n    async def add_kernel(\n        self, request: agent_server_pb2.AddKernelRequest, context: ServicerContext\n    ) -> agent_server_pb2.AddKernelResponse:\n        \"\"\"Create a token, only the admin can call this method\"\"\"\n        metadata = dict(context.invocation_metadata())\n        if \"api_key\" not in metadata or metadata[\"api_key\"] != config[\"admin_key\"]:\n            await context.abort(10, \"You are not the admin\")\n        if request.key in self.agents and self.agents[request.key]:\n            return agent_server_pb2.AddKernelResponse(code=0, msg=\"ok\")\n        # init the sdk\n        sdk = KernelSDK(request.endpoint, request.key)\n        try:\n            sdk.connect()\n            await sdk.is_alive()\n        except Exception as ex:\n            await context.abort(10, f\"Connecting to kernel {request.endpoint} failed\")\n        if config[\"llm_key\"] == \"azure_openai\" or config[\"llm_key\"] == \"openai\":\n            logger.info(f\"create a openai agent {request.endpoint}\")\n            agent = build_openai_agent(\n                sdk,\n                config[\"openai_api_model\"],\n                is_azure=True if config[\"llm_key\"] == \"azure_openai\" else False,\n            )\n            self.agents[request.key] = {\"sdk\": sdk, \"agent\": agent}\n        elif config[\"llm_key\"] == \"mock\":\n            logger.info(f\"create a mock agent to kernel {request.endpoint}\")\n            agent = build_mock_agent(sdk, config[\"cases_path\"])\n            self.agents[request.key] = {\"sdk\": sdk, \"agent\": agent}\n        elif config[\"llm_key\"] == \"codellama\":\n            logger.info(f\"create a llama agent {request.endpoint}\")\n            grammer_path = os.path.join(\n                pathlib.Path(__file__).parent.resolve(), \"grammar.bnf\"\n            )\n            agent = build_llama_agent(\n                config[\"llama_api_base\"], config[\"llama_api_key\"], sdk, grammer_path\n            )\n            self.agents[request.key] = {\"sdk\": sdk, \"agent\": agent}\n        return agent_server_pb2.AddKernelResponse(code=0, msg=\"ok\")\n\n    async def process_task(\n        self, request: agent_server_pb2.ProcessTaskRequest, context\n    ) -> AsyncIterable[agent_server_pb2.TaskResponse]:\n        \"\"\"\n        process the task from the client\n        \"\"\"\n        metadata = dict(context.invocation_metadata())\n        if (\n            \"api_key\" not in metadata\n            or metadata[\"api_key\"] not in self.agents\n            or not self.agents[metadata[\"api_key\"]]\n        ):\n            logger.debug(\"invalid api key\")\n            await context.abort(10, \"invalid api key\")\n\n        logger.debug(\"receive the task %s \", request.task)\n        agent = self.agents[metadata[\"api_key\"]][\"agent\"]\n        sdk = self.agents[metadata[\"api_key\"]][\"sdk\"]\n        queue = asyncio.Queue()\n\n        async def worker(request, agent, queue, context, task_opt):\n            return await agent.arun(request, queue, context, task_opt)\n\n        options = (\n            request.options\n            if request.options\n            and request.options.input_token_limit != 0\n            and request.options.output_token_limit != 0\n            else agent_server_pb2.ProcessOptions(\n                streaming=True,\n                llm_name=\"\",\n                input_token_limit=4000\n                if not request.options.input_token_limit\n                else request.options.input_token_limit,\n                output_token_limit=4000\n                if not request.options.output_token_limit\n                else request.options.output_token_limit,\n                timeout=10,\n            )\n        )\n\n        logger.debug(\"create the agent task\")\n        task = asyncio.create_task(worker(request, agent, queue, context, options))\n        while True:\n            logger.debug(\"start wait the queue message\")\n            # TODO add timeout\n            respond = await queue.get()\n            if not respond:\n                logger.debug(\"exit the queue\")\n                break\n            logger.debug(f\"respond {respond}\")\n            queue.task_done()\n            yield respond\n        await task\n\n    async def download(\n        self, request: common_pb2.DownloadRequest, context: ServicerContext\n    ) -> AsyncIterable[common_pb2.FileChunk]:\n        \"\"\"\n        download file from kernel and send it to client\n        \"\"\"\n        metadata = dict(context.invocation_metadata())\n        if (\n            \"api_key\" not in metadata\n            or metadata[\"api_key\"] not in self.agents\n            or not self.agents[metadata[\"api_key\"]]\n        ):\n            await context.abort(10, \"invalid api key\")\n        sdk = self.agents[metadata[\"api_key\"]][\"sdk\"]\n        async for chunk in sdk.download_file(request.filename):\n            yield chunk\n\n    async def upload(\n        self,\n        request: AsyncIterable[common_pb2.FileChunk],\n        context: ServicerContext,\n    ) -> common_pb2.FileUploaded:\n        \"\"\"\n        upload file to the kernel workspace\n        \"\"\"\n        metadata = dict(context.invocation_metadata())\n        if (\n            \"api_key\" not in metadata\n            or metadata[\"api_key\"] not in self.agents\n            or not self.agents[metadata[\"api_key\"]]\n        ):\n            await context.abort(10, \"invalid arguments\")\n        sdk = self.agents[metadata[\"api_key\"]][\"sdk\"]\n\n        async def generate_chunk(proxy_request, context, limit):\n            length = 0\n            async for chunk in proxy_request:\n                if length + len(chunk.buffer) > limit:\n                    await context.abort(\n                        grpc.StatusCode.INVALID_ARGUMENT.value[0],\n                        \"exceed the max file limit\",\n                    )\n                length += len(chunk.buffer)\n                yield chunk\n\n        return await sdk.upload_binary(\n            generate_chunk(request, context, self.max_file_size)\n        )\n\n\nasync def serve() -> None:\n    logger.info(\n        \"start agent rpc server with host %s and port %s\",\n        config[\"rpc_host\"],\n        config[\"rpc_port\"],\n    )\n    await models.create_all()\n    serv = server()\n    add_AgentServerServicer_to_server(AgentRpcServer(), serv)\n    listen_addr = \"%s:%s\" % (config[\"rpc_host\"], config[\"rpc_port\"])\n    serv.add_insecure_port(listen_addr)\n    await serv.start()\n    await serv.wait_for_termination()\n\n\ndef server_main():\n    asyncio.run(serve())\n"}
{"type": "source_file", "path": "agent/src/og_agent/agent_llm.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport logging\nimport openai\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMManager:\n\n    def __init__(self, config):\n        \"\"\"\n        llm_key=xxxx\n        # the other optional\n        \"\"\"\n        self.config = config\n        self.llms = {}\n        self.llm_key = self.config[\"llm_key\"]\n        if self.config[\"llm_key\"] == \"azure_openai\":\n            self._build_azure_openai()\n        elif self.config[\"llm_key\"] == \"openai\":\n            self._build_openai()\n\n    def get_llm(self):\n        return self.llms.get(self.llm_key, None)\n\n    def get_llm_by_key(self, llm_key):\n        \"\"\"\n        get llm with a key, the supported keys are 'mock', 'openai', 'azure_openai', 'codellama'\n        \"\"\"\n        return self.llms.get(self.llm_key, None)\n\n    def _no_empty_value_required(self, keys):\n        for key in keys:\n            if not self.config.get(key, None):\n                raise Exception(f\"the value of required {key} is empty\")\n\n    def _build_openai(self):\n        self._no_empty_value_required([\n            \"openai_api_key\",\n            \"openai_api_model\",\n        ])\n        if self.config.get(\"openai_api_base\", None):\n            openai.api_base = self.config.get(\"openai_api_base\", None)\n        openai.api_key = self.config[\"openai_api_key\"]\n\n    def _build_azure_openai(self):\n        \"\"\"\n        build azure openai client from config\n        \"\"\"\n        self._no_empty_value_required([\n            \"openai_api_base\",\n            \"openai_api_version\",\n            \"openai_api_key\",\n            \"openai_api_type\",\n            \"openai_api_deployment\",\n        ])\n        openai.api_base = self.config[\"openai_api_base\"]\n        openai.api_version = self.config[\"openai_api_version\"]\n        openai.api_type = self.config[\"openai_api_type\"]\n        openai.api_key = self.config[\"openai_api_key\"]\n        self.config[\"openai_api_model\"] = self.config[\"openai_api_deployment\"]\n"}
{"type": "source_file", "path": "agent/src/og_agent/agent_setup.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport click\nimport asyncio\nfrom og_sdk.agent_sdk import AgentSDK\n\n\nasync def add_kernel(endpoint, api_key, kernel_endpoint, kernel_api_key):\n    sdk = AgentSDK(endpoint, api_key)\n    sdk.connect()\n    try:\n        await sdk.add_kernel(kernel_api_key, kernel_endpoint)\n        print(\"add kernel %s done\" % kernel_endpoint)\n    except Exception as ex:\n        print(\"add kernel %s failed %s\" % (kernel_endpoint, ex))\n\n\n@click.command()\n@click.option(\"--kernel_endpoint\", help=\"the endpoint of kernel\")\n@click.option(\"--kernel_api_key\", help=\"the api key of kernel\")\n@click.option(\"--agent_endpoint\", help=\"the endpoint of agent\")\n@click.option(\"--admin_key\", help=\"the admin key of agent\")\ndef setup(kernel_endpoint, kernel_api_key, agent_endpoint, admin_key):\n    if not kernel_endpoint or not kernel_api_key or not admin_key or not agent_endpoint:\n        print(\"kernel_endpoint or kernel_api_key or admin key is empty\")\n        return\n    asyncio.run(add_kernel(agent_endpoint, admin_key, kernel_endpoint, kernel_api_key))\n"}
{"type": "source_file", "path": "agent/src/og_agent/base_agent.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport uuid\nimport json\nimport io\nimport logging\nimport time\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom og_proto.kernel_server_pb2 import ExecuteResponse\nfrom og_proto.agent_server_pb2 import TaskResponse, ContextState\nfrom og_sdk.utils import parse_image_filename, process_char_stream\nfrom og_proto.agent_server_pb2 import OnStepActionStart, TaskResponse, OnStepActionEnd, FinalAnswer, TypingContent\nfrom og_proto.prompt_pb2 import AgentPrompt\nfrom .tokenizer import tokenize\nfrom .prompt import ROLE, RULES, ACTIONS, OUTPUT_FORMAT\nfrom og_memory.memory import MemoryAgentMemory\nimport tiktoken\n\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\nlogger = logging.getLogger(__name__)\n\n\nclass FunctionResult(BaseModel):\n    console_stderr: str = \"\"\n    console_stdout: str = \"\"\n    saved_filenames: List[str] = Field(\n        description=\"A list of filenames that were created by the code\", default=[]\n    )\n    has_result: bool = False\n    has_error: bool = False\n\n\nclass TaskContext(BaseModel):\n    start_time: float = 0\n    output_token_count: int = 0\n    input_token_count: int = 0\n    llm_name: str = \"\"\n    llm_response_duration: int = 0\n    context_id: str = \"\"\n\n    def to_context_state_proto(self):\n        # in ms\n        total_duration = int((time.time() - self.start_time) * 1000)\n        return ContextState(\n            output_token_count=self.output_token_count,\n            input_token_count=self.input_token_count,\n            llm_name=self.llm_name,\n            total_duration=total_duration,\n            llm_response_duration=self.llm_response_duration,\n        )\n\n\nclass TypingState:\n    START = 0\n    EXPLANATION = 1\n    CODE = 2\n    LANGUAGE = 3\n    MESSAGE = 4\n    OTHER = 5\n\n\nclass BaseAgent:\n\n    def __init__(self, sdk):\n        self.kernel_sdk = sdk\n        self.model_name = \"\"\n        self.agent_memories = {}\n\n    def create_new_memory_with_default_prompt(\n        self, user_name, user_id, actions=ACTIONS\n    ):\n        \"\"\"\n        create a new memory for the user\n        \"\"\"\n        # generate a uuid\n        memory_id = str(uuid.uuid4())\n        agent_prompt = AgentPrompt(\n            role=ROLE,\n            rules=RULES,\n            actions=actions,\n            output_format=OUTPUT_FORMAT,\n        )\n        agent_memory = MemoryAgentMemory(memory_id, user_name, user_id)\n        agent_memory.swap_instruction(agent_prompt)\n        self.agent_memories[memory_id] = agent_memory\n        logger.info(f\"create a new memory {memory_id} for user {user_name}\")\n        return memory_id\n\n    def reset_memory(self, memory_id):\n        \"\"\"\n        reset the memory\n        \"\"\"\n        if memory_id in self.agent_memories:\n            self.agent_memories[memory_id].reset_memory()\n            logger.info(f\"reset the memory {memory_id}\")\n        else:\n            logger.info(f\"the memory {memory_id} does not exist\")\n\n    def _merge_delta_for_function_call(self, message, delta):\n        if len(message.keys()) == 0:\n            message.update(delta)\n            return\n        if \"function_call\" not in message:\n            message[\"function_call\"] = delta[\"function_call\"]\n            return\n        old_arguments = message[\"function_call\"].get(\"arguments\", \"\")\n        if delta[\"function_call\"][\"arguments\"]:\n            message[\"function_call\"][\"arguments\"] = (\n                old_arguments + delta[\"function_call\"][\"arguments\"]\n            )\n\n    def _merge_delta_for_content(self, message, delta):\n        if not delta:\n            return\n        content = message.get(\"content\", \"\")\n        if delta.get(\"content\"):\n            message[\"content\"] = content + delta[\"content\"]\n\n    def _parse_arguments(\n        self,\n        arguments,\n        is_code=False,\n    ):\n        \"\"\"\n        parse the partial key with string value from json\n        \"\"\"\n        if is_code:\n            return TypingState.CODE, \"\", arguments, \"python\", \"\"\n        state = TypingState.START\n        explanation_str = \"\"\n        code_str = \"\"\n        language_str = \"\"\n        message_str = \"\"\n        logger.debug(f\"the arguments {arguments}\")\n        for token_state, token in tokenize(io.StringIO(arguments)):\n            if token_state == None:\n                if state == TypingState.EXPLANATION and token[0] == 1:\n                    explanation_str = token[1]\n                    state = TypingState.START\n                if state == TypingState.CODE and token[0] == 1:\n                    code_str = token[1]\n                    state = TypingState.START\n                if state == TypingState.LANGUAGE and token[0] == 1:\n                    language_str = token[1]\n                    state = TypingState.START\n                if state == TypingState.MESSAGE and token[0] == 1:\n                    message_str = token[1]\n                    state = TypingState.START\n                if state == TypingState.OTHER and token[0] == 1:\n                    state = TypingState.START\n\n                if token[1] == \"explanation\":\n                    state = TypingState.EXPLANATION\n                if token[1] == \"code\":\n                    state = TypingState.CODE\n                if token[1] == \"language\":\n                    state = TypingState.LANGUAGE\n                if token[1] == \"message\":\n                    state = TypingState.MESSAGE\n                if token[1] == \"saved_filenames\":\n                    state = TypingState.OTHER\n            else:\n                # String\n                if token_state == 9 and state == TypingState.EXPLANATION:\n                    explanation_str = \"\".join(token)\n                elif token_state == 9 and state == TypingState.CODE:\n                    code_str = \"\".join(token)\n                elif token_state == 9 and state == TypingState.LANGUAGE:\n                    language_str = \"\".join(token)\n                elif token_state == 9 and state == TypingState.MESSAGE:\n                    message_str = \"\".join(token)\n        return (state, explanation_str, code_str, language_str, message_str)\n\n    def _get_message_token_count(self, message):\n        response_token_count = 0\n        if \"function_call\" in message and message[\"function_call\"]:\n            arguments = message[\"function_call\"].get(\"arguments\", \"\")\n            response_token_count += len(encoding.encode(arguments))\n        if \"content\" in message and message[\"content\"]:\n            response_token_count += len(encoding.encode(message.get(\"content\")))\n        return response_token_count\n\n    async def _read_function_call_message(\n        self,\n        message,\n        queue,\n        old_text_content,\n        old_code_content,\n        old_message_str,\n        language_str,\n        task_context,\n        task_opt,\n    ):\n        typing_language = \"text\"\n        is_code = False\n        if message[\"function_call\"].get(\"name\", \"\") == \"python\":\n            is_code = True\n        arguments = message[\"function_call\"].get(\"arguments\", \"\")\n        return await self._send_typing_message(\n            arguments,\n            queue,\n            old_text_content,\n            old_code_content,\n            old_message_str,\n            language_str,\n            task_context,\n            task_opt,\n            is_code=is_code,\n        )\n\n    async def _read_json_message(\n        self,\n        message,\n        queue,\n        old_text_content,\n        old_code_content,\n        old_message_str,\n        old_language_str,\n        task_context,\n        task_opt,\n    ):\n        return await self._send_typing_message(\n            message.get(\"content\", \"\"),\n            queue,\n            old_text_content,\n            old_code_content,\n            old_message_str,\n            old_language_str,\n            task_context,\n            task_opt,\n        )\n\n    async def _send_typing_message(\n        self,\n        arguments,\n        queue,\n        old_text_content,\n        old_code_content,\n        old_message_str,\n        old_language_str,\n        task_context,\n        task_opt,\n        is_code=False,\n    ):\n        \"\"\"\n        send the typing message to the client\n        \"\"\"\n        (\n            state,\n            explanation_str,\n            code_str,\n            language_str,\n            message_str,\n        ) = self._parse_arguments(arguments, is_code)\n\n        logger.debug(\n            f\"argument explanation:{explanation_str} code:{code_str} language_str:{language_str} text_content:{old_text_content} old_message_str:{old_message_str}\"\n        )\n\n        if explanation_str and old_text_content != explanation_str:\n            typed_chars = explanation_str[len(old_text_content) :]\n            new_text_content = explanation_str\n            if task_opt.streaming and len(typed_chars) > 0:\n                task_response = TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnModelTypeText,\n                    typing_content=TypingContent(content=typed_chars, language=\"text\"),\n                    context_id=task_context.context_id,\n                )\n                await queue.put(task_response)\n            return new_text_content, old_code_content, old_language_str, old_message_str\n        if code_str and old_code_content != code_str:\n            typed_chars = code_str[len(old_code_content) :]\n            code_content = code_str\n            if task_opt.streaming and len(typed_chars) > 0:\n                await queue.put(\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnModelTypeCode,\n                        typing_content=TypingContent(\n                            content=typed_chars, language=\"text\"\n                        ),\n                        context_id=task_context.context_id,\n                    )\n                )\n            return old_text_content, code_content, old_language_str, old_message_str\n        if language_str and old_language_str != language_str:\n            typed_chars = language_str[len(old_language_str) :]\n            if task_opt.streaming and len(typed_chars) > 0:\n                await queue.put(\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnModelTypeCode,\n                        typing_content=TypingContent(content=\"\", language=language_str),\n                        context_id=task_context.context_id,\n                    )\n                )\n            return old_text_content, old_code_content, language_str, old_message_str\n        if message_str and old_message_str != message_str:\n            typed_chars = message_str[len(old_message_str) :]\n            if task_opt.streaming and len(typed_chars) > 0:\n                await queue.put(\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnModelTypeText,\n                        typing_content=TypingContent(\n                            content=typed_chars, language=\"text\"\n                        ),\n                        context_id=task_context.context_id,\n                    )\n                )\n\n            return old_text_content, old_code_content, old_language_str, message_str\n        return old_text_content, old_code_content, old_language_str, old_message_str\n\n    async def extract_message(\n        self,\n        response_generator,\n        queue,\n        rpc_context,\n        task_context,\n        task_opt,\n        start_time,\n        is_json_format=False,\n    ):\n        \"\"\"\n        extract the chunk from the response generator\n        \"\"\"\n        message = {}\n        text_content = \"\"\n        code_content = \"\"\n        language_str = \"\"\n        message_str = \"\"\n        context_output_token_count = task_context.output_token_count\n        start_time = time.time()\n        async for chunk in response_generator:\n            if rpc_context.done():\n                logger.debug(\"the client has cancelled the request\")\n                break\n            if not chunk[\"choices\"]:\n                continue\n            logger.debug(f\"the chunk {chunk}\")\n            task_context.llm_name = chunk.get(\"model\", \"\")\n            self.model_name = chunk.get(\"model\", \"\")\n            delta = chunk[\"choices\"][0][\"delta\"]\n            if \"function_call\" in delta:\n                self._merge_delta_for_function_call(message, delta)\n                response_token_count = self._get_message_token_count(message)\n                task_context.output_token_count = (\n                    response_token_count + context_output_token_count\n                )\n                task_context.llm_response_duration += int(\n                    (time.time() - start_time) * 1000\n                )\n                start_time = time.time()\n                (\n                    new_text_content,\n                    new_code_content,\n                    new_language_str,\n                    new_message_str,\n                ) = await self._read_function_call_message(\n                    message,\n                    queue,\n                    text_content,\n                    code_content,\n                    message_str,\n                    language_str,\n                    task_context,\n                    task_opt,\n                )\n                text_content = new_text_content\n                code_content = new_code_content\n                message_str = new_message_str\n                language_str = new_language_str\n            else:\n                self._merge_delta_for_content(message, delta)\n                task_context.llm_response_duration += int(\n                    (time.time() - start_time) * 1000\n                )\n                start_time = time.time()\n                if message.get(\"content\") != None:\n                    response_token_count = self._get_message_token_count(message)\n                    task_context.output_token_count = (\n                        response_token_count + context_output_token_count\n                    )\n                    if is_json_format:\n                        (\n                            new_text_content,\n                            new_code_content,\n                            new_language_str,\n                            new_message_str,\n                        ) = await self._read_json_message(\n                            message,\n                            queue,\n                            text_content,\n                            code_content,\n                            message_str,\n                            language_str,\n                            task_context,\n                            task_opt,\n                        )\n                        text_content = new_text_content\n                        code_content = new_code_content\n                        message_str = new_message_str\n                        language_str = new_language_str\n\n                    elif task_opt.streaming and delta.get(\"content\"):\n                        await queue.put(\n                            TaskResponse(\n                                state=task_context.to_context_state_proto(),\n                                response_type=TaskResponse.OnModelTypeText,\n                                typing_content=TypingContent(\n                                    content=delta[\"content\"], language=\"text\"\n                                ),\n                                context_id=task_context.context_id,\n                            )\n                        )\n        logger.info(\n            f\"call the {self.model_name} with input token {task_context.input_token_count} and output token count {task_context.output_token_count}\"\n        )\n        return message\n\n    async def call_function(self, code, context, task_context):\n        \"\"\"\n        run code with kernel\n        \"\"\"\n        console_stdout = \"\"\n        console_stderr = \"\"\n        has_result = False\n        has_error = False\n        is_alive = await self.kernel_sdk.is_alive()\n        if not is_alive:\n            await self.kernel_sdk.start(kernel_name=\"python3\")\n        async for kernel_respond in self.kernel_sdk.execute(code=code):\n            if context.done():\n                logger.debug(\n                    \"the context is not active and the client cancelled the request\"\n                )\n                break\n            # process the stdout\n            if kernel_respond.output_type == ExecuteResponse.StdoutType:\n                kernel_output = json.loads(kernel_respond.output)[\"text\"]\n                console_stdout += kernel_output\n                console_stdout = process_char_stream(console_stdout)\n                logger.debug(f\"the new stdout {console_stdout}\")\n                yield (\n                    None,\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnStepActionStreamStdout,\n                        console_stdout=kernel_output,\n                        context_id=task_context.context_id,\n                    ),\n                )\n            # process the stderr\n            elif kernel_respond.output_type == ExecuteResponse.StderrType:\n                kernel_err = json.loads(kernel_respond.output)[\"text\"]\n                console_stderr += kernel_err\n                console_stderr = process_char_stream(console_stderr)\n                logger.debug(f\"the new stderr {console_stderr}\")\n                yield (\n                    None,\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnStepActionStreamStderr,\n                        console_stderr=kernel_err,\n                        context_id=task_context.context_id,\n                    ),\n                )\n            elif kernel_respond.output_type == ExecuteResponse.TracebackType:\n                traceback = json.loads(kernel_respond.output)[\"traceback\"]\n                console_stderr += traceback\n                logger.debug(f\"the new traceback {console_stderr}\")\n                has_error = True\n                yield (\n                    None,\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnStepActionStreamStderr,\n                        console_stderr=traceback,\n                        context_id=task_context.context_id,\n                    ),\n                )\n            else:\n                has_result = True\n                result = json.loads(kernel_respond.output)\n                logger.debug(f\"the result {result}\")\n                if \"image/gif\" in result:\n                    console_stdout = result[\"image/gif\"]\n                elif \"image/png\" in result:\n                    console_stdout = result[\"image/png\"]\n                elif \"text/plain\" in result:\n                    console_stdout = result[\"text/plain\"]\n                    console_stdout = bytes(console_stdout, \"utf-8\").decode(\n                        \"unicode_escape\"\n                    )\n                    if console_stdout.startswith(\"'\") and console_stdout.endswith(\"'\"):\n                        console_stdout = console_stdout[1 : len(console_stdout) - 1]\n                yield (\n                    None,\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnStepActionStreamStdout,\n                        console_stdout=console_stdout,\n                        context_id=task_context.context_id,\n                    ),\n                )\n        output_files = []\n        filename = parse_image_filename(console_stdout)\n        if filename:\n            output_files.append(filename)\n        yield (\n            FunctionResult(\n                console_stderr=console_stderr,\n                console_stdout=console_stdout,\n                saved_filenames=output_files,\n                has_error=has_error,\n                has_result=has_result,\n            ),\n            None,\n        )\n"}
{"type": "source_file", "path": "agent/src/og_agent/base_stream_client.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport json\nimport aiohttp\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseStreamClient:\n\n    def __init__(self, endpoint, key):\n        self.endpoint = endpoint\n        self.key = key\n\n    async def arun(self, request_data):\n        logging.debug(f\"{request_data}\")\n        headers = {\"Authorization\": self.key}\n        async with aiohttp.ClientSession(\n            headers=headers, raise_for_status=True\n        ) as session:\n            async with session.post(self.endpoint, json=request_data) as r:\n                async for line in r.content:\n                    if line:\n                        yield line\n"}
{"type": "source_file", "path": "agent/src/og_agent/llama_agent.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport json\nimport logging\nimport io\nimport time\nfrom .llama_client import LlamaClient\nfrom og_proto.agent_server_pb2 import OnStepActionStart, TaskResponse, OnStepActionEnd, FinalAnswer, TypingContent\nfrom .base_agent import BaseAgent, TypingState, TaskContext\nfrom og_memory.memory import AgentMemoryOption\nfrom .prompt import FUNCTION_DIRECT_MESSAGE, FUNCTION_EXECUTE\nfrom .tokenizer import tokenize\nimport tiktoken\n\nlogger = logging.getLogger(__name__)\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n\nclass LlamaAgent(BaseAgent):\n\n    def __init__(self, client, kernel_sdk):\n        super().__init__(kernel_sdk)\n        self.client = client\n        self.memory_option = AgentMemoryOption(\n            show_function_instruction=True, disable_output_format=False\n        )\n\n    def _output_exception(self):\n        return (\n            \"Sorry, the LLM did return nothing, You can use a better performance model\"\n        )\n\n    async def handle_bash_code(\n        self, json_response, queue, context, task_context, task_opt\n    ):\n        commands = json_response[\"code\"]\n        code = f\"%%bash\\n {commands}\"\n        explanation = json_response[\"explanation\"]\n        saved_filenames = json_response.get(\"saved_filenames\", [])\n        tool_input = json.dumps({\n            \"code\": commands,\n            \"explanation\": explanation,\n            \"saved_filenames\": saved_filenames,\n            \"language\": json_response.get(\"language\"),\n        })\n        await queue.put(\n            TaskResponse(\n                state=task_context.to_context_state_proto(),\n                response_type=TaskResponse.OnStepActionStart,\n                on_step_action_start=OnStepActionStart(\n                    input=tool_input, tool=\"execute\"\n                ),\n            )\n        )\n        function_result = None\n        async for (result, respond) in self.call_function(code, context, task_context):\n            if context.done():\n                logger.debug(\"the client has cancelled the request\")\n                break\n            function_result = result\n            if respond and task_opt.streaming:\n                await queue.put(respond)\n        return function_result\n\n    async def handle_python_function(\n        self, json_response, queue, context, task_context, task_opt\n    ):\n        code = json_response[\"code\"]\n        explanation = json_response[\"explanation\"]\n        saved_filenames = json_response.get(\"saved_filenames\", [])\n        tool_input = json.dumps({\n            \"code\": code,\n            \"explanation\": explanation,\n            \"saved_filenames\": saved_filenames,\n            \"language\": json_response.get(\"language\"),\n        })\n        await queue.put(\n            TaskResponse(\n                state=task_context.to_context_state_proto(),\n                response_type=TaskResponse.OnStepActionStart,\n                on_step_action_start=OnStepActionStart(\n                    input=tool_input, tool=\"execute\"\n                ),\n            )\n        )\n        function_result = None\n        async for (result, respond) in self.call_function(code, context, task_context):\n            if context.done():\n                logger.debug(\"the client has cancelled the request\")\n                break\n            function_result = result\n            if respond and task_opt.streaming:\n                await queue.put(respond)\n        return function_result\n\n    async def call_llama(self, agent_memory, queue, context, task_context, task_opt):\n        \"\"\"\n        call llama api\n        \"\"\"\n        input_token_count = 0\n        messages = agent_memory.to_messages()\n        for message in messages:\n            if not message[\"content\"]:\n                continue\n            input_token_count += len(encoding.encode(message[\"content\"]))\n        task_context.input_token_count += input_token_count\n        start_time = time.time()\n        response = self.client.chat(messages, \"llama\", max_tokens=2048)\n        message = await self.extract_message(\n            response,\n            queue,\n            context,\n            task_context,\n            task_opt,\n            start_time,\n            is_json_format=True,\n        )\n        return message\n\n    async def arun(self, request, queue, context, task_opt):\n        \"\"\"\n        run the agent\n        \"\"\"\n        question = request.task\n        context_id = (\n            request.context_id\n            if request.context_id\n            else self.create_new_memory_with_default_prompt(\n                \"\", \"\", actions=[FUNCTION_EXECUTE, FUNCTION_DIRECT_MESSAGE]\n            )\n        )\n        if context_id not in self.agent_memories:\n            await queue.put(\n                TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnSystemError,\n                    error_msg=\"invalid context id\",\n                    context_id=context_id,\n                )\n            )\n            return\n        agent_memory = self.agent_memories[context_id]\n        agent_memory.update_options(self.memory_option)\n        agent_memory.append_chat_message(\n            {\"role\": \"user\", \"content\": question},\n        )\n        task_context = TaskContext(\n            start_time=time.time(),\n            output_token_count=0,\n            input_token_count=0,\n            llm_name=\"llama\",\n            llm_respond_duration=0,\n        )\n        try:\n            while not context.done():\n                if task_context.input_token_count >= task_opt.input_token_limit:\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnInputTokenLimitExceed,\n                            error_msg=f\"input token limit reached {task_opt.input_token_limit}\",\n                        )\n                    )\n                    break\n                if task_context.output_token_count >= task_opt.output_token_limit:\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnOutputTokenLimitExceed,\n                            error_msg=f\"output token limit reached {task_opt.output_token_limit}\",\n                        )\n                    )\n                    break\n                message = await self.call_llama(\n                    agent_memory,\n                    queue,\n                    context,\n                    task_context,\n                    task_opt,\n                )\n                try:\n                    json_response = json.loads(message[\"content\"])\n                    if not json_response:\n                        await queue.put(\n                            TaskResponse(\n                                state=task_context.to_context_state_proto(),\n                                response_type=TaskResponse.OnModelOutputError,\n                                error_msg=self._output_exception(),\n                            )\n                        )\n                        break\n                except Exception as ex:\n                    logger.exception(f\"fail to load message the message is {message}\")\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnModelOutputError,\n                            error_msg=str(ex),\n                        )\n                    )\n                    break\n                logger.debug(f\" llama response {json_response}\")\n                if (\n                    \"function_call\" in json_response\n                    and json_response[\"function_call\"] == \"execute\"\n                ):\n                    agent_memory.append_chat_message(message)\n                    tools_mapping = {\n                        \"python\": self.handle_python_function,\n                        \"bash\": self.handle_bash_code,\n                    }\n\n                    function_result = await tools_mapping[\n                        json_response[\"arguments\"][\"language\"]\n                    ](\n                        json_response[\"arguments\"],\n                        queue,\n                        context,\n                        task_context,\n                        task_opt,\n                    )\n\n                    logger.debug(f\"the function result {function_result}\")\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnStepActionEnd,\n                            on_step_action_end=OnStepActionEnd(\n                                output=\"\"\n                                if task_opt.streaming\n                                else function_result.console_stderr\n                                + function_result.console_stdout,\n                                output_files=function_result.saved_filenames,\n                                has_error=function_result.has_error,\n                            ),\n                        )\n                    )\n                    action_output = \"the output of %s:\" % json_response[\"function_call\"]\n                    current_question = \"Give me the final answer summary if the above output of action  meets the goal Otherwise try a new step\"\n                    if function_result.has_result:\n                        agent_memory.append_chat_message({\n                            \"role\": \"user\",\n                            \"content\": f\"{action_output} \\n {function_result.console_stdout}\",\n                        })\n                        agent_memory.append_chat_message(\n                            {\"role\": \"user\", \"content\": current_question}\n                        )\n                    elif function_result.has_error:\n                        agent_memory.append_chat_message({\n                            \"role\": \"user\",\n                            \"content\": f\"{action_output} \\n {function_result.console_stderr}\",\n                        })\n                        current_question = f\"Generate a new step to fix the above error\"\n                        agent_memory.append_chat_message(\n                            {\"role\": \"user\", \"content\": current_question}\n                        )\n                    else:\n                        agent_memory.append_chat_message({\n                            \"role\": \"user\",\n                            \"content\": f\"{action_output} \\n {function_result.console_stdout}\",\n                        })\n                        agent_memory.append_chat_message(\n                            {\"role\": \"user\", \"content\": current_question}\n                        )\n                elif (\n                    \"function_call\" in json_response\n                    and json_response[\"function_call\"] == \"direct_message\"\n                ):\n                    message = json_response[\"arguments\"][\"message\"]\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnFinalAnswer,\n                            final_answer=FinalAnswer(\n                                answer=message if not task_opt.streaming else \"\"\n                            ),\n                        )\n                    )\n                    break\n        except Exception as ex:\n            logger.exception(\"fail to run the agent\")\n            response = TaskResponse(\n                response_type=TaskResponse.OnSystemError,\n                error_msg=str(ex),\n            )\n            await queue.put(response)\n        finally:\n            await queue.put(None)\n"}
{"type": "source_file", "path": "agent/src/og_agent/llama_client.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport json\nimport aiohttp\nimport logging\nfrom .base_stream_client import BaseStreamClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass LlamaClient(BaseStreamClient):\n\n    def __init__(self, endpoint, key, grammar):\n        super().__init__(endpoint + \"/v1/chat/completions\", key)\n        self.grammar = grammar\n\n    async def chat(self, messages, model, temperature=0, max_tokens=1024, stop=[\"\\n\"]):\n        data = {\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"grammar\": self.grammar,\n            \"stream\": True,\n            \"model\": model,\n            \"max_tokens\": max_tokens,\n            \"top_p\": 0.9,\n        }\n        if stop:\n            data[\"stop\"] = stop\n        async for line in self.arun(data):\n            if len(line) < 6:\n                continue\n            try:\n                content = line[6:]\n                logger.debug(f\"llama response content: {content}\")\n                message = json.loads(content)\n                yield message\n            except Exception as e:\n                logger.error(\"error: %s, content: %s\", e, content)\n                continue\n"}
{"type": "source_file", "path": "agent/src/og_agent/openai_agent.py", "content": "# SPDX-FileCopyrightText: 2023 ghf5t565698```\\\\\\\\\\\\\\\\\\-=[-[9oi86y53e12motai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport openai\nimport io\nimport json\nimport logging\nimport time\nfrom pydantic import BaseModel, Field\nfrom og_proto.agent_server_pb2 import OnStepActionStart, TaskResponse, OnStepActionEnd, FinalAnswer, TypingContent\nfrom .base_agent import BaseAgent, TypingState, TaskContext\nfrom .tokenizer import tokenize\nfrom og_memory.memory import AgentMemoryOption\nimport tiktoken\n\nlogger = logging.getLogger(__name__)\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n\nclass OpenaiAgent(BaseAgent):\n\n    def __init__(self, model, sdk, is_azure=True):\n        super().__init__(sdk)\n        self.model = model\n        logger.info(f\"use openai model {model} is_azure {is_azure}\")\n        self.is_azure = is_azure\n        self.model_name = model if not is_azure else \"\"\n        self.memory_option = AgentMemoryOption(\n            show_function_instruction=False, disable_output_format=True\n        )\n\n    async def call_openai(self, agent_memory, queue, context, task_context, task_opt):\n        \"\"\"\n        call the openai api\n        \"\"\"\n        input_token_count = 0\n        messages = agent_memory.to_messages()\n        logger.debug(f\"call openai with messages {messages}\")\n        for message in messages:\n            if not message[\"content\"]:\n                continue\n            input_token_count += len(encoding.encode(message[\"content\"]))\n        task_context.input_token_count += input_token_count\n        start_time = time.time()\n        if self.is_azure:\n            response = await openai.ChatCompletion.acreate(\n                engine=self.model,\n                messages=messages,\n                temperature=0,\n                functions=agent_memory.get_functions(),\n                function_call=\"auto\",\n                stream=True,\n            )\n        else:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model,\n                messages=messages,\n                temperature=0,\n                functions=agent_memory.get_functions(),\n                function_call=\"auto\",\n                stream=True,\n            )\n        message = await self.extract_message(\n            response, queue, context, task_context, task_opt, start_time\n        )\n        return message\n\n    async def handle_function(self, message, queue, context, task_context, task_opt):\n        if \"function_call\" in message:\n            if context.done():\n                logging.debug(\"the client has cancelled the request\")\n                return\n            function_name = message[\"function_call\"][\"name\"]\n            raw_code = \"\"\n            code = \"\"\n            explanation = \"\"\n            saved_filenames = []\n            language = \"python\"\n            if function_name == \"direct_message\":\n                arguments = json.loads(message[\"function_call\"][\"arguments\"])\n                await queue.put(\n                    TaskResponse(\n                        state=task_context.to_context_state_proto(),\n                        response_type=TaskResponse.OnFinalAnswer,\n                        final_answer=FinalAnswer(\n                            answer=arguments[\"message\"]\n                            if not task_opt.streaming\n                            else \"\"\n                        ),\n                        context_id=task_context.context_id,\n                    )\n                )\n                return None\n            if function_name == \"python\":\n                raw_code = message[\"function_call\"][\"arguments\"]\n                logger.debug(f\"call function {function_name} with args {code}\")\n                code = raw_code\n            else:\n                arguments = json.loads(message[\"function_call\"][\"arguments\"])\n                raw_code = arguments[\"code\"]\n                code = raw_code\n                explanation = arguments[\"explanation\"]\n                saved_filenames = arguments.get(\"saved_filenames\", [])\n                language = arguments.get(\"language\")\n            if language == \"bash\":\n                code = f\"%%bash\\n{raw_code}\"\n\n            tool_input = json.dumps({\n                \"code\": raw_code,\n                \"explanation\": explanation,\n                \"saved_filenames\": saved_filenames,\n                \"language\": language,\n            })\n            # send the respond to client\n            await queue.put(\n                TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnStepActionStart,\n                    on_step_action_start=OnStepActionStart(\n                        input=tool_input, tool=function_name\n                    ),\n                    context_id=task_context.context_id,\n                )\n            )\n            function_result = None\n            async for (result, respond) in self.call_function(\n                code, context, task_context\n            ):\n                if context.done():\n                    logger.debug(\"the client has cancelled the request\")\n                    break\n                function_result = result\n                if respond and task_opt.streaming:\n                    await queue.put(respond)\n            return function_result\n        else:\n            raise Exception(\"bad message, function message expected\")\n\n    async def arun(self, request, queue, context, task_opt):\n        \"\"\"\n        process the task\n        \"\"\"\n        task = request.task\n        context_id = (\n            request.context_id\n            if request.context_id\n            else self.create_new_memory_with_default_prompt(\"\", \"\")\n        )\n        task_context = TaskContext(\n            start_time=time.time(),\n            output_token_count=0,\n            input_token_count=0,\n            llm_name=self.model_name,\n            llm_respond_duration=0,\n            context_id=context_id,\n        )\n        if context_id not in self.agent_memories:\n            await queue.put(\n                TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnSystemError,\n                    error_msg=\"invalid context id\",\n                    context_id=context_id,\n                )\n            )\n            return\n        agent_memory = self.agent_memories[context_id]\n        agent_memory.update_options(self.memory_option)\n        agent_memory.append_chat_message(\n            {\"role\": \"user\", \"content\": task},\n        )\n        try:\n            while not context.done():\n                if task_context.input_token_count >= task_opt.input_token_limit:\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnInputTokenLimitExceed,\n                            error_msg=\"input token limit reached\",\n                            context_id=context_id,\n                        )\n                    )\n                    break\n                if task_context.output_token_count >= task_opt.output_token_limit:\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnOutputTokenLimitExceed,\n                            error_msg=\"output token limit reached\",\n                            context_id=context_id,\n                        )\n                    )\n                    break\n                chat_message = await self.call_openai(\n                    agent_memory, queue, context, task_context, task_opt\n                )\n                logger.debug(f\"the response {chat_message}\")\n                if \"function_call\" in chat_message:\n                    if \"content\" not in chat_message:\n                        chat_message[\"content\"] = None\n                    if \"role\" not in chat_message:\n                        chat_message[\"role\"] = \"assistant\"\n                    agent_memory.append_chat_message(chat_message)\n                    function_result = await self.handle_function(\n                        chat_message, queue, context, task_context, task_opt\n                    )\n                    if not function_result:\n                        break\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnStepActionEnd,\n                            on_step_action_end=OnStepActionEnd(\n                                output=\"\"\n                                if task_opt.streaming\n                                else function_result.console_stderr\n                                + function_result.console_stdout,\n                                output_files=function_result.saved_filenames,\n                                has_error=function_result.has_error,\n                            ),\n                            context_id=context_id,\n                        )\n                    )\n                    function_name = chat_message[\"function_call\"][\"name\"]\n                    # TODO optimize the token limitation\n                    if function_result.has_result:\n                        agent_memory.append_chat_message({\n                            \"role\": \"function\",\n                            \"name\": function_name,\n                            \"content\": function_result.console_stdout[0:500],\n                        })\n                    elif function_result.has_error:\n                        agent_memory.append_chat_message({\n                            \"role\": \"function\",\n                            \"name\": function_name,\n                            \"content\": function_result.console_stderr[0:500],\n                        })\n                    else:\n                        agent_memory.append_chat_message({\n                            \"role\": \"function\",\n                            \"name\": function_name,\n                            \"content\": function_result.console_stdout[0:500],\n                        })\n                else:\n                    # end task\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnFinalAnswer,\n                            final_answer=FinalAnswer(\n                                answer=chat_message[\"content\"]\n                                if not task_opt.streaming\n                                else \"\"\n                            ),\n                            context_id=context_id,\n                        )\n                    )\n                    break\n        except Exception as ex:\n            logging.exception(\"fail process task\")\n            response = TaskResponse(\n                response_type=TaskResponse.OnSystemError,\n                error_msg=str(ex),\n                context_id=context_id,\n            )\n            await queue.put(response)\n        finally:\n            await queue.put(None)\n"}
{"type": "source_file", "path": "agent/src/og_agent/mock_agent.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport json\nimport time\nimport logging\nfrom .base_agent import BaseAgent, TypingState, TaskContext\nfrom og_proto.agent_server_pb2 import OnStepActionStart, TaskResponse, OnStepActionEnd, FinalAnswer, TypingContent\nfrom .tokenizer import tokenize\n\nlogger = logging.getLogger(__name__)\n\n\nclass MockAgent(BaseAgent):\n    \"\"\"\n    a test agent for octogen\n    \"\"\"\n\n    def __init__(self, messages, sdk):\n        \"\"\"\n        the messages are the cases\n        \"\"\"\n        super().__init__(sdk)\n        self.messages = messages\n\n    async def call_ai(self, prompt, queue, iteration, task_context):\n        message = self.messages.get(prompt)[iteration]\n        if message.get(\"explanation\", None):\n            await queue.put(\n                TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnModelTypeText,\n                    typing_content=TypingContent(\n                        content=message[\"explanation\"], language=\"text\"\n                    ),\n                )\n            )\n        if message.get(\"code\", None):\n            await queue.put(\n                TaskResponse(\n                    state=task_context.to_context_state_proto(),\n                    response_type=TaskResponse.OnModelTypeCode,\n                    typing_content=TypingContent(\n                        content=message[\"code\"], language=\"python\"\n                    ),\n                )\n            )\n        return message\n\n    async def handle_call_function(\n        self, code, queue, explanation, context, task_context, saved_filenames=[]\n    ):\n        tool_input = json.dumps({\n            \"code\": code,\n            \"explanation\": explanation,\n            \"saved_filenames\": saved_filenames,\n            \"language\": \"python\",\n        })\n        await queue.put(\n            TaskResponse(\n                state=task_context.to_context_state_proto(),\n                response_type=TaskResponse.OnStepActionStart,\n                on_step_action_start=OnStepActionStart(\n                    input=tool_input, tool=\"execute\"\n                ),\n            )\n        )\n        function_result = None\n        async for (result, respond) in self.call_function(code, context, task_context):\n            function_result = result\n            if respond:\n                await queue.put(respond)\n        return function_result\n\n    async def arun(self, request, queue, context, task_opt):\n        \"\"\"\n        run the agent\n\n        \"\"\"\n        task = request.task\n        task_context = TaskContext(\n            start_time=time.time(),\n            output_token_count=10,\n            input_token_count=10,\n            llm_name=\"mock\",\n            llm_respond_duration=1000,\n        )\n        iteration = 0\n        try:\n            while iteration <= 10:\n                message = await self.call_ai(task, queue, iteration, task_context)\n                iteration = iteration + 1\n                if message.get(\"code\", None):\n                    function_result = await self.handle_call_function(\n                        message[\"code\"],\n                        queue,\n                        message[\"explanation\"],\n                        context,\n                        task_context,\n                        message.get(\"saved_filenames\", []),\n                    )\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnStepActionEnd,\n                            on_step_action_end=OnStepActionEnd(\n                                output=\"\",\n                                output_files=function_result.saved_filenames,\n                                has_error=function_result.has_error,\n                            ),\n                        )\n                    )\n                else:\n                    await queue.put(\n                        TaskResponse(\n                            state=task_context.to_context_state_proto(),\n                            response_type=TaskResponse.OnFinalAnswer,\n                            final_answer=FinalAnswer(answer=message[\"explanation\"]),\n                        )\n                    )\n                    break\n        finally:\n            await queue.put(None)\n"}
{"type": "source_file", "path": "chat/src/og_discord/__init__.py", "content": ""}
{"type": "source_file", "path": "agent/src/og_agent/prompt.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport json\nfrom og_proto.prompt_pb2 import ActionDesc\n\nROLE = \"\"\"You are the Programming Copilot called Octogen, a world-class programmer to complete any goal by executing code\"\"\"\n\nRULES = [\n    \"To complete the goal, write a plan and execute it step-by-step, limiting the number of steps to five\",\n    \"Every step must include the explanation and the code block. if the code block has any display data, save it as a file and add it to saved_filenames field\",\n    \"You have a fully controlled programming environment to execute code with internet connection but sudo is not allowed\",\n    \"You must try to correct your code when you get errors from the output\",\n    \"You can install new package with pip\",\n    \"Use `execute` action to execute any code and `direct_message` action to send message to user\",\n]\n\nFUNCTION_EXECUTE = ActionDesc(\n    name=\"execute\",\n    desc=\"This action executes code in your programming environment and returns the output\",\n    parameters=json.dumps({\n        \"type\": \"object\",\n        \"properties\": {\n            \"explanation\": {\n                \"type\": \"string\",\n                \"description\": \"the explanation about the code parameters\",\n            },\n            \"code\": {\n                \"type\": \"string\",\n                \"description\": \"the bash code to be executed\",\n            },\n            \"language\": {\n                \"type\": \"string\",\n                \"description\": \"the language of the code, only python and bash are supported\",\n            },\n            \"saved_filenames\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"A list of filenames that were created by the code\",\n            },\n        },\n        \"required\": [\"explanation\", \"code\", \"language\"],\n    }),\n)\n\nFUNCTION_DIRECT_MESSAGE = ActionDesc(\n    name=\"direct_message\",\n    desc=\"This action sends a direct message to user.\",\n    parameters=json.dumps({\n        \"type\": \"object\",\n        \"properties\": {\n            \"message\": {\n                \"type\": \"string\",\n                \"description\": \"the message will be sent to user\",\n            },\n        },\n        \"required\": [\"message\"],\n    }),\n)\n\nACTIONS = [FUNCTION_EXECUTE]\n\nOUTPUT_FORMAT = \"\"\"The output format must be a JSON format with the following fields:\n* function_call: The name of the action\n* arguments: The arguments of the action\n\"\"\"\n\nOCTOGEN_CODELLAMA_MID_INS = \"\"\"The above output of the %s determines whether the execution is successful. \nIf successful, go to the next step. If the current step is the final step, summarize the entire plan. If not, adjust the input and try again\"\"\"\n\nOCTOGEN_CODELLAMA_MID_ERROR_INS = \"\"\"Adjust the action input and try again for the above output of %s showing the error message\"\"\"\n"}
{"type": "source_file", "path": "chat/src/og_terminal/terminal_chat.py", "content": "#! /usr/bin/env python3\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n# vim:fenc=utf-8\n#\n\n\"\"\" \"\"\"\nimport sys\nimport os\nimport re\nimport time\nimport json\nimport asyncio\nimport click\nimport glob\nimport random\nfrom datetime import datetime\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.syntax import Syntax\nfrom rich.console import Group\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.text import Text\nfrom rich.progress import Progress\nfrom rich.rule import Rule\nfrom rich.live import Live\nfrom rich.spinner import Spinner\nfrom rich import box\nfrom rich.style import Style\nfrom prompt_toolkit.auto_suggest import AutoSuggestFromHistory\nfrom prompt_toolkit.shortcuts import CompleteStyle, clear\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit import PromptSession\nfrom og_sdk.agent_sdk import AgentSyncSDK\nfrom og_sdk.utils import process_char_stream\nfrom og_proto import agent_server_pb2\nfrom dotenv import dotenv_values\nfrom prompt_toolkit.completion import Completer, Completion\nfrom .utils import parse_file_path\nfrom .markdown import CodeBlock as SyntaxBlock\nfrom .ui_block import TaskBlocks, TerminalBlock\nimport clipboard\n\nMarkdown.elements[\"fence\"] = SyntaxBlock\nMarkdown.elements[\"code_block\"] = SyntaxBlock\n\nUSER_TITLE = \"✍[bold yellow]User\"\nOCTOGEN_TITLE = \"🤖[bold green]Octogen\"\nSYSTEM_TITLE = \"🔨[bold red]System\"\n\n\ndef show_welcome(console):\n    welcome = \"\"\"\nWelcome to use octogen❤️ . To ask a programming question, simply type your question and press [bold yellow]esc + enter[/]\nUse [bold yellow]/help[/] for help\n\"\"\"\n    console.print(welcome)\n\n\ndef prompt_continuation(width, line_number, is_soft_wrap):\n    return \".\" * width\n\n\ndef show_help(console):\n    help = \"\"\"\n### Keyboard Shortcut:\n- **`ESC + ENTER`**: Submit your question to Octogen or execute your command.\n\n### Commands:\n\n- **`/clear`**: Clears the screen.\n- **`/cc{number}`**: Copies the output of Octogen to your clipboard.\n- **`/exit`**: Exits the Octogen CLI.\n- **`/up`**: Uploads files from your local machine; useful for including in your questions.\n\n### Need Help?\n\n1. Create an issue on our GitHub page: [Octogen GitHub Issues](https://github.com/dbpunk-labs/octogen/issues)\n2. Alternatively, you can email us at [codego.me@gmail.com](mailto:codego.me@gmail.com).\n\"\"\"\n    mk = Markdown(help, justify=\"left\")\n    console.print(mk)\n\n\ndef parse_numbers(text):\n    \"\"\"Parses numbers from a string.\n\n    Args:\n    text: The string to parse.\n\n    Returns:\n    A list of numbers found in the string.\n    \"\"\"\n    pattern = r\"\\d+\\.\\d+|\\d+\"\n    numbers = re.findall(pattern, text)\n    return numbers\n\n\nclass OctogenCompleter(Completer):\n\n    def __init__(self, values):\n        Completer.__init__(self)\n        self.values = values\n\n    def get_completions(self, document, complete_event):\n        index = document.current_line_before_cursor.find(\"/up \")\n        if index >= 0:\n            word_before_cursor = document.current_line_before_cursor[\n                index + 3 :\n            ].strip()\n            if word_before_cursor:\n                for comp in glob.glob(word_before_cursor + \"*\"):\n                    yield Completion(comp, -len(word_before_cursor))\n\n\ndef check_parameter(octopus_config, console):\n    \"\"\"\n    check the parameter\n    \"\"\"\n    if \"api_key\" not in octopus_config or not octopus_config[\"api_key\"]:\n        content = \"\"\"No api key was found, you can get api key from the following ways\n* You can use your [self-hosted](https://github.com/dbpunk-labs) agent server api key\n* You can apply the api key from [octogen waitlist](https://octogen.dev) directly\"\"\"\n        mk = Markdown(content)\n        console.print(mk)\n        return False\n    return True\n\n\ndef clean_code(code: str):\n    start_tag = \"```python\"\n    end_tag = \"```\"\n    index = code.find(start_tag)\n    if index >= 0:\n        last = code.rfind(end_tag)\n        return code[index + len(start_tag) : last]\n    return code\n\n\ndef refresh(live, task_blocks, title=OCTOGEN_TITLE, task_state=None):\n    speed = (\n        task_state.output_token_count\n        / ((task_state.llm_response_duration + 1) / 1000.0)\n        if task_state\n        else 0\n    )\n    table = Table.grid(padding=1, pad_edge=True)\n    table.add_column(\"Index\", no_wrap=True, justify=\"center\")\n    table.add_column(\"Status\", no_wrap=True, justify=\"center\")\n    table.add_column(\"Content\")\n    count = 0\n    for index, status, block in task_blocks.render():\n        table.add_row(f\"{index}\", status, block)\n        count += 1\n    if count:\n        live.update(\n            Panel(\n                table,\n                title=title,\n                title_align=\"left\",\n                subtitle=\"[gray] Speed:%.1ft/s Input:%d Output:%d Model:%s\"\n                % (\n                    speed,\n                    task_state.input_token_count,\n                    task_state.output_token_count,\n                    task_state.llm_name,\n                )\n                if task_state\n                else \"\",\n                subtitle_align=\"left\",\n            )\n        )\n        live.refresh()\n    else:\n        live.update(Group(*[]))\n        live.refresh()\n\n\ndef handle_action_output(task_blocks, respond):\n    if respond.response_type not in [\n        agent_server_pb2.TaskResponse.OnStepActionStreamStdout,\n        agent_server_pb2.TaskResponse.OnStepActionStreamStderr,\n    ]:\n        return\n    if respond.response_type == agent_server_pb2.TaskResponse.OnStepActionStreamStdout:\n        task_blocks.add_terminal(respond.console_stdout, \"\")\n    elif (\n        respond.response_type == agent_server_pb2.TaskResponse.OnStepActionStreamStderr\n    ):\n        task_blocks.add_terminal(\"\", respond.console_stderr)\n\n\ndef handle_action_end(task_blocks, respond, images):\n    \"\"\"\n    Handles the end of an agent action.\n\n    Args:\n      segments: A list of segments in the current turn.\n      respond: The response from the agent.\n      images: A list of images to be displayed.\n\n    Returns:\n      None.\n    \"\"\"\n    if respond.response_type != agent_server_pb2.TaskResponse.OnStepActionEnd:\n        return\n    has_error = respond.on_step_action_end.has_error\n    if not has_error:\n        images.extend(respond.on_step_action_end.output_files)\n    if isinstance(task_blocks.get_last_block(), TerminalBlock):\n        task_blocks.get_last_block().finish(has_error)\n    task_blocks.finish_current_all_blocks()\n    # wait for the next steps\n    task_blocks.add_loading()\n\n\ndef handle_typing(task_blocks, respond):\n    if respond.response_type not in [\n        agent_server_pb2.TaskResponse.OnModelTypeText,\n        agent_server_pb2.TaskResponse.OnModelTypeCode,\n    ]:\n        return\n    if respond.response_type == agent_server_pb2.TaskResponse.OnModelTypeText:\n        task_blocks.add_markdown(respond.typing_content.content)\n    else:\n        task_blocks.add_code(\n            respond.typing_content.content, respond.typing_content.language\n        )\n\n\ndef handle_action_start(task_blocks, respond, images):\n    \"\"\"start to execute the action\"\"\"\n    if respond.response_type != agent_server_pb2.TaskResponse.OnStepActionStart:\n        return\n    action = respond.on_step_action_start\n    if not action.input:\n        return\n    arguments = json.loads(action.input)\n    images.extend(arguments.get(\"saved_filenames\", []))\n    task_blocks.finish_current_all_blocks()\n    # wait for the action to be finished\n    task_blocks.add_loading()\n\n\ndef extract_the_code(content, task_blocks):\n    start_index = 0\n    while start_index < len(content):\n        first_pos = content.find(\"```\", start_index)\n        if first_pos >= 0:\n            second_pos = content.find(\"```\", first_pos + 1)\n            if second_pos >= 0:\n                sub_content = content[start_index:first_pos]\n                task_blocks.add_markdown(sub_content)\n                task_blocks.get_last_block().finish()\n                start_index = first_pos\n                code_content = content[first_pos : second_pos + 3]\n                clean_code_content = clean_code(code_content)\n                task_blocks.add_code(clean_code_content, \"python\")\n                task_blocks.get_last_block().finish()\n                # TODO parse language\n                start_index = second_pos + 3\n        else:\n            break\n    if start_index < len(content):\n        sub_content = content[start_index:]\n        task_blocks.add_markdown(sub_content)\n        task_blocks.get_last_block().finish()\n\n\ndef handle_final_answer(task_blocks, respond):\n    if respond.response_type != agent_server_pb2.TaskResponse.OnFinalAnswer:\n        return\n    answer = respond.final_answer.answer\n    task_blocks.finish_current_all_blocks()\n    if not answer:\n        return\n    extract_the_code(answer, task_blocks)\n\n\ndef render_image(images, sdk, image_dir, console):\n    try:\n        from PIL import Image\n        from term_image.image import AutoImage\n\n        image_set = set(images)\n        for image in image_set:\n            if image.endswith(\"jpg\") or image.endswith(\"png\") or image.endswith(\"gif\"):\n                try:\n                    sdk.download_file(image, image_dir)\n                    fullpath = \"%s/%s\" % (image_dir, image)\n                    pil_image = Image.open(fullpath)\n                    auto_image = AutoImage(\n                        image=pil_image, width=int(pil_image.size[0] / 15)\n                    )\n                    print(f\"{auto_image:1.1#}\")\n                    return True\n                except Exception as ex:\n                    return False\n    except Exception as ex:\n        return False\n\n\ndef upload_file(prompt, console, history_prompt, sdk, values):\n    filepaths = parse_file_path(prompt)\n    if not filepaths:\n        return prompt\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n    real_prompt = prompt.replace(\"/up\", \"\")\n    with Live(Group(*[]), console=console) as live:\n        mk = \"\"\"The following files will be uploaded\n\"\"\"\n        task_blocks.add_markdown(mk)\n        refresh(live, task_blocks, title=SYSTEM_TITLE)\n        for file in filepaths:\n            filename = file.split(\"/\")[-1]\n            sdk.upload_file(file, filename)\n            mk = \"* ✅%s\\n\" % file\n            task_blocks.add_markdown(mk)\n            real_prompt = real_prompt.replace(file, \"uploaded %s\" % filename)\n            refresh(live, task_blocks, title=SYSTEM_TITLE)\n        task_blocks.get_last_block().finish()\n        refresh(live, task_blocks, title=SYSTEM_TITLE)\n    history_prompt.append_string(real_prompt)\n    return real_prompt\n\n\ndef run_chat(prompt, session, console, values, filedir=None):\n    \"\"\"\n    run the chat\n    \"\"\"\n    task_blocks = TaskBlocks(values)\n    task_blocks.begin()\n    images = []\n    error_responses = []\n    with Live(Group(*[]), console=console) as live:\n        refresh(live, task_blocks)\n        task_state = None\n        for respond in session.prompt(prompt):\n            if not respond:\n                break\n            if respond.response_type in [\n                agent_server_pb2.TaskResponse.OnSystemError,\n                agent_server_pb2.TaskResponse.OnInputTokenLimitExceed,\n                agent_server_pb2.TaskResponse.OnOutputTokenLimitExceed,\n            ]:\n                error_responses.append(respond)\n                task_blocks.finish_current_all_blocks()\n                break\n            handle_typing(task_blocks, respond)\n            handle_action_start(task_blocks, respond, images)\n            handle_action_output(task_blocks, respond)\n            handle_action_end(task_blocks, respond, images)\n            handle_final_answer(task_blocks, respond)\n            task_state = respond.state\n            refresh(live, task_blocks, task_state=respond.state)\n        refresh(live, task_blocks, task_state=task_state)\n    if error_responses:\n        task_blocks = TaskBlocks(values)\n        task_blocks.begin()\n        with Live(Group(*[]), console=console) as live:\n            for respond in error_responses:\n                task_blocks.add_markdown(respond.error_msg)\n                task_blocks.get_last_block().finish()\n            refresh(live, task_blocks, title=SYSTEM_TITLE)\n    # display the images\n    # render_image(images, sdk, filedir, console)\n\n\n@click.command()\n@click.option(\"--octogen_dir\", default=\"~/.octogen\", help=\"the root path of octogen\")\ndef app(octogen_dir):\n    console = Console()\n    if octogen_dir.find(\"~\") == 0:\n        real_octogen_dir = octogen_dir.replace(\"~\", os.path.expanduser(\"~\"))\n    else:\n        real_octogen_dir = octogen_dir\n    os.makedirs(real_octogen_dir, exist_ok=True)\n    octopus_config = dotenv_values(real_octogen_dir + \"/config\")\n    if not check_parameter(octopus_config, console):\n        return\n    filedir = real_octogen_dir + \"/data\"\n    os.makedirs(filedir, exist_ok=True)\n    sdk = AgentSyncSDK(octopus_config[\"endpoint\"], octopus_config[\"api_key\"])\n    sdk.connect()\n    with sdk.create_session() as agent_session:\n        history = FileHistory(real_octogen_dir + \"/history\")\n        values = []\n        completer = OctogenCompleter(values)\n        session = PromptSession(\n            history=history,\n            completer=completer,\n            complete_in_thread=True,\n            complete_while_typing=True,\n            complete_style=CompleteStyle.MULTI_COLUMN,\n        )\n        index = 0\n        show_welcome(console)\n        while True:\n            index = index + 1\n            real_prompt = session.prompt(\n                \"[%s]%s>\" % (index, \"🎧\"),\n                multiline=True,\n                prompt_continuation=prompt_continuation,\n            )\n            if not \"\".join(real_prompt.strip().split(\"\\n\")):\n                continue\n            if real_prompt.find(\"/help\") >= 0:\n                show_help(console)\n                continue\n            if real_prompt.find(\"/exit\") >= 0:\n                console.print(\"👋👋!\")\n                return\n            if real_prompt.find(\"/clear\") >= 0:\n                clear()\n                continue\n            if real_prompt.find(\"/cc\") >= 0:\n                # handle copy\n                for number in parse_numbers(real_prompt):\n                    num = int(number)\n                    if num < len(values):\n                        clipboard.copy(values[num])\n                        console.print(f\"👍 /cc{number} has been copied to clipboard!\")\n                        break\n                    else:\n                        console.print(f\"❌ /cc{number} was not found!\")\n                continue\n            # try to upload first⌛⏳❌\n            real_prompt = upload_file(real_prompt, console, history, sdk, values)\n            run_chat(\n                real_prompt,\n                agent_session,\n                console,\n                values,\n                filedir=filedir,\n            )\n"}
{"type": "source_file", "path": "kernel/setup.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_kernel\",\n    version=\"0.3.6\",\n    description=\"Open source code interpreter agent for LLM\",\n    author=\"imotai\",\n    author_email=\"codego.me@gmail.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    packages=[\n        \"og_kernel\",\n        \"og_kernel.kernel\",\n        \"og_kernel.server\",\n    ],\n    package_dir={\n        \"og_kernel\": \"src/og_kernel\",\n        \"og_kernel.kernel\": \"src/og_kernel/kernel\",\n        \"og_kernel.server\": \"src/og_kernel/server\",\n    },\n    install_requires=[\n        \"og_proto\",\n        \"grpc-google-iam-v1>=0.12.6\",\n        \"grpcio-tools>=1.57.0\",\n        \"ipykernel>=6.25.1\",\n        \"jupyter_client>=8.3.0\",\n        \"matplotlib>=3.7.2\",\n        \"pandas\",\n        \"numpy\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"og_kernel_rpc_server = og_kernel.server.kernel_rpc_server:server_main\",\n            \"og_kernel_app = og_kernel.kernel.kernel_app:run_app\",\n        ]\n    },\n)\n"}
{"type": "source_file", "path": "chat/src/og_terminal/__init__.py", "content": ""}
{"type": "source_file", "path": "docs/source/conf.py", "content": "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'Octogen'\ncopyright = '2023 octogem.dev'\nauthor = 'imotai'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = []\n\ntemplates_path = ['_templates']\nexclude_patterns = []\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"alabaster\"\nhtml_static_path = ['_static']\n"}
{"type": "source_file", "path": "kernel/cases/pie.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a pie chart\ndata = np.array([10, 20, 30, 40])\nlabels = [\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"]\n\nplt.pie(data, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Pie Chart\")\nplt.show()\n"}
{"type": "source_file", "path": "chat/src/og_terminal/markdown.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nfrom rich.markdown import TextElement, Markdown\nfrom rich.syntax import Syntax\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom markdown_it.token import Token\n\n\nclass CodeBlock(TextElement):\n    \"\"\"A code block with syntax highlighting.\"\"\"\n\n    style_name = \"markdown.code_block\"\n\n    @classmethod\n    def create(cls, markdown: \"Markdown\", token: Token) -> \"CodeBlock\":\n        node_info = token.info or \"\"\n        lexer_name = node_info.partition(\" \")[0]\n        return cls(lexer_name or \"default\", markdown.code_theme)\n\n    def __init__(self, lexer_name: str, theme: str) -> None:\n        self.lexer_name = lexer_name\n        self.theme = theme\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        code = str(self.text).rstrip()\n        syntax = Syntax(\n            code,\n            self.lexer_name,\n            # background_color=\"default\",\n            line_numbers=True,\n            theme=self.theme,\n            word_wrap=True,\n            padding=1,\n        )\n        yield syntax\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/config.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nfrom dotenv import dotenv_values\n\nconfig = dotenv_values(\".env\")\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/__init__.py", "content": ""}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/kernel_client.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport os\nimport logging\nimport inspect\nimport asyncio\nimport queue\nimport json\nfrom jupyter_client import AsyncKernelClient\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"\nthe kernel client for watching the output of kernel\n\"\"\"\n\n\nclass KernelClient:\n\n    def __init__(self, connection_file):\n        if not connection_file:\n            raise ValueError(f\"connection_file={connection_file} is empty\")\n        if not os.path.exists(connection_file):\n            raise ValueError(f\"connection_file={connection_file} do not exist\")\n        logger.info(\n            \"create a new kernel client with connection_file %s\", connection_file\n        )\n        self.client = None\n        self.is_running = False\n        self.task = None\n        self.connection_file = connection_file\n\n    async def is_alive(self):\n        return await self.client.is_alive()\n\n    async def start_client(self):\n        self.client = AsyncKernelClient(connection_file=self.connection_file)\n        self.client.load_connection_file()\n        self.client.start_channels()\n        await self.client.wait_for_ready()\n\n    async def _loop(self, on_message_fn):\n        logger.debug(\"start loop the kernel message\")\n        try:\n            while self.is_running and self.client:\n                try:\n                    logger.debug(\"start wait message\")\n                    msg = await self.client.get_iopub_msg(timeout=1)\n                    logger.debug(\"msg %s\", msg)\n                    try:\n                        await on_message_fn(msg)\n                    except Exception as e:\n                        logger.error(\"fail to call on message function for error %s\", e)\n                        continue\n                except queue.Empty:\n                    logger.debug(\"empty message\")\n                    continue\n                except (ValueError, IndexError):\n                    # get_iopub_msg suffers from message fetch errors\n                    logger.error(\"fail to get message\")\n                    break\n                except Exception as e:\n                    logger.error(\"fail to wait for message %s\", e)\n                    break\n        except Exception as e:\n            logger.error(\"loop exception\", e)\n\n    async def watching(self, on_message_fn):\n        \"\"\"\n        Watch the message from kernel, when a new message arrived , the `on_message_fn` will be\n        called\n\n        Arguments\n        on_message_fn - when a new message arrived the function will be called\n        \"\"\"\n        if self.is_running:\n            raise ValueError(f\"the watch is running, do not watch it again\")\n        if not on_message_fn or not self.client:\n            raise ValueError(f\"on_message_fn or clent is None\")\n        if not inspect.iscoroutinefunction(on_message_fn):\n            raise ValueError(f\"on_message_fn must be async function\")\n        self.is_running = True\n        self.task = asyncio.create_task(self._loop(on_message_fn))\n\n    async def read_response(self, context, tries=1):\n        try:\n            hit_empty = 0\n            while self.client:\n                try:\n                    msg = await self.client.get_iopub_msg(timeout=1)\n                    if context.done():\n                        logger.debug(\"the client  has cancelled the request\")\n                        break\n                    logger.debug(f\"{msg}\")\n                    yield msg\n                except queue.Empty:\n                    hit_empty += 1\n                    if hit_empty >= tries:\n                        break\n                except (ValueError, IndexError):\n                    # get_iopub_msg suffers from message fetch errors\n                    logger.error(\"fail to get message\")\n                    break\n                except Exception as e:\n                    logger.error(\"fail to wait for message %s\", e)\n                    break\n            yield None\n        except Exception as e:\n            logger.error(\"loop exception\", e)\n            yield None\n\n    def execute(self, code):\n        \"\"\"\n        Execute the python code\n        \"\"\"\n        if not self.client:\n            raise ValueError(f\"no client is avaliable\")\n        msg_id = self.client.execute(code)\n        logger.debug(\"the execute msg id %s\", msg_id)\n        return msg_id\n\n    async def stop_watch(self):\n        if self.task and self.is_running:\n            self.is_running = False\n            logger.info(\n                \"stop the kernel client for connection_file %s\", self.connection_file\n            )\n            try:\n                self.task.cancel()\n                await self.task\n            except:\n                pass\n\n    def stop_client(self):\n        if self.client:\n            self.client.stop_channels()\n            self.client = None\n"}
{"type": "source_file", "path": "memory/src/og_memory/__init__.py", "content": ""}
{"type": "source_file", "path": "proto/setup.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_proto\",\n    version=\"0.3.6\",\n    description=\"Open source code interpreter agent for LLM\",\n    author=\"imotai\",\n    author_email=\"codego.me@gmail.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description= open('README.md').read(),\n    long_description_content_type='text/markdown',\n\n    packages=[\n        \"og_proto\",\n    ],\n\n    package_dir={\n        \"og_proto\": \"src/og_proto\",\n    },\n\n    package_data={\"og_proto\": [\"*.pyi\"]},\n\n    install_requires=[\n        \"grpc-google-iam-v1>=0.12.0\",\n        \"grpcio-tools>=1.40.0\",\n    ],\n\n    entry_points={\n    },\n\n)\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/server/__init__.py", "content": ""}
{"type": "source_file", "path": "memory/src/og_memory/memory.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\"\n\n\"\"\"\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom pydantic import BaseModel, Field\nfrom og_proto.memory_pb2 import AgentMemory as AgentMemoryProto\nfrom jinja2 import Environment\nfrom jinja2.loaders import PackageLoader\nimport tiktoken\nlogger = logging.getLogger(__name__)\n\nenv = Environment(loader=PackageLoader(\"og_memory\", \"template\"))\nenv.filters['from_json'] = lambda s: json.loads(s)\ncontext_tpl = env.get_template(\"agent.jinja\")\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n\ndef agent_memory_to_context(instruction, guide_memory, options):\n    \"\"\"\n    Convert the agent memory to context\n    :param instruction: the instruction\n    :param guide_memory: the guide memory\n    :return: string context for llm\n    \"\"\"\n    return context_tpl.render(prompt=instruction, guides=guide_memory, options=options)\n\nclass BaseAgentMemory(ABC):\n    \"\"\"\n    Base class for agent memory\n    \"\"\"\n    @abstractmethod\n    def append_chat_message(self, message):\n        \"\"\"\n        Append chat message to the memory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def append_guide(self, guide):\n        \"\"\"\n        Append guide to the memory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_options(self, options):\n        \"\"\"\n        Update the options\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def swap_instruction(self, instruction):\n        \"\"\"\n        Swap the instruction\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def to_messages(self):\n        \"\"\"\n        Convert the memory to messages\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset_memory(self):\n        \"\"\"\n        Reset the memory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_functions(self):\n        \"\"\"\n        return the function definitions for model that supports the function_call\n        \"\"\"\n        pass\n\n\nclass AgentMemoryOption(BaseModel):\n    \"\"\"\n    The agent memory option\n    \"\"\"\n    show_function_instruction: bool = Field(False, description=\"Show the function instruction\")\n    disable_output_format: bool = Field(False, description=\"Disable the output format\")\n\nclass MemoryAgentMemory(BaseAgentMemory):\n    \"\"\"\n    The agent memory based on memory\n    \"\"\"\n    def __init__(self, memory_id, user_name, user_id):\n        self.memory_id = memory_id\n        self.user_name = user_name\n        self.user_id = user_id\n        self.guide_memory = []\n        self.chat_memory = []\n        self.instruction = None\n        self.options = AgentMemoryOption(show_function_instruction=True)\n\n    def update_options(self, options):\n        self.options = options\n\n    def reset_memory(self):\n        self.guide_memory = []\n        self.chat_memory = []\n\n    def append_guide(self, guide):\n        self.guide_memory.append(guide)\n\n    def append_chat_message(self, message):\n        self.chat_memory.append(message)\n\n    def swap_instruction(self, instruction):\n        self.instruction = instruction\n\n    def get_functions(self):\n        return [{\"name\": action.name, \"description\": action.desc, \"parameters\":\n          json.loads(action.parameters)} for action in self.instruction.actions]\n\n    def to_messages(self):\n        system_message = {\n          \"role\":\"system\",\n          \"content\":agent_memory_to_context(self.instruction, self.guide_memory, options = self.options)\n        }\n        logging.debug(f\"system message: {system_message}\")\n        return [system_message] + self.chat_memory\n\n\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/kernel_mgr.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport os\nimport logging\nimport subprocess\nimport json\nimport sys\nimport pathlib\nfrom time import sleep\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"\nThe jupytor kernel manager used for\n- create kernel instance\n- start kernel instance\n- stop kernel instance\n\nTypical usage example:\n    config_path = \"kernel_connection_file.json\"\n    workspace = \"/mnt/workspace1\"\n    km = KernelManager(config_path, workspace)\n    # start the kernel\n    km.start()\n\"\"\"\n\n\nclass KernelManager:\n\n    def __init__(self, config_path: str, workspace: str, kernel: str = \"python3\"):\n        if not config_path or not workspace:\n            raise ValueError(\n                f\"config path={config_path} or workspace={workspace} is empty\"\n            )\n        self.config_path = config_path\n        self.workspace = workspace\n        self.process = None\n        self.is_running = False\n        logger.info(\n            \"new kernel manager with config path %s and worksapce %s\",\n            config_path,\n            workspace,\n        )\n        self.kernel = kernel\n\n    def start(self):\n        \"\"\"\n        Start a kernel instance and generate the kernel connection file\n        \"\"\"\n        self.is_running = True\n\n        os.makedirs(self.workspace, exist_ok=True)\n        launch_kernel_script_path = os.path.join(\n            pathlib.Path(__file__).parent.resolve(), \"launch_kernel.py\"\n        )\n        self.process = subprocess.Popen(\n            [\n                sys.executable,\n                launch_kernel_script_path,\n                \"--connection_file=\" + self.config_path,\n                \"--kernel=\" + self.kernel,\n            ],\n            cwd=self.workspace,\n        )\n        logger.info(\"Start the kernel with process id %s\", str(self.process.pid))\n        while True:\n            if not os.path.isfile(self.config_path):\n                sleep(1)\n            else:\n                try:\n                    with open(self.config_path, \"r\") as fp:\n                        logger.info(\"connection file content %s\", json.load(fp))\n                    break\n                except json.JSONDecodeError:\n                    pass\n\n    def stop(self):\n        \"\"\"\n        stop the kernel instance\n        \"\"\"\n        self.is_running = False\n        logger.info(\"stop the kernel with process id %s\", str(self.process.pid))\n        if self.process:\n            self.process.terminate()\n            self.process.wait()\n            self.process = None\n\n    def __str__(self):\n        return f'KernelManager(config_path=\"{self.config_path}\", workspace=\"{self.workspace}\")'\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/kernel_app.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\"\"\"An application to launch a kernel by name in a local subprocess.\"\"\"\nimport os\nimport signal\nimport uuid\n\nfrom jupyter_core.application import JupyterApp, base_flags\nfrom tornado.ioloop import IOLoop\nfrom traitlets import Unicode\nfrom jupyter_client.kernelspec import NATIVE_KERNEL_NAME, KernelSpecManager\nfrom jupyter_client.manager import KernelManager\n\n\nclass KernelApp(JupyterApp):\n    \"\"\"Launch a kernel by name in a local subprocess.\"\"\"\n\n    description = \"Run a kernel locally in a subprocess\"\n    classes = [KernelManager, KernelSpecManager]\n    aliases = {\n        \"kernel\": \"KernelApp.kernel_name\",\n        \"ip\": \"KernelManager.ip\",\n        \"connection_file\": \"KernelApp.connection_file\",\n    }\n    flags = {\"debug\": base_flags[\"debug\"]}\n    kernel_name = Unicode(\n        NATIVE_KERNEL_NAME, help=\"The name of a kernel type to start\"\n    ).tag(config=True)\n    connection_file = Unicode(\"\", help=\"The connection file path of the kernel\").tag(\n        config=True\n    )\n\n    def initialize(self, argv=None):\n        \"\"\"Initialize the application.\"\"\"\n        super().initialize(argv)\n        cf_basename = (\n            self.connection_file\n            if self.connection_file\n            else \"kernel-%s.json\" % uuid.uuid4()\n        )\n        self.config.setdefault(\"KernelManager\", {}).setdefault(\n            \"connection_file\", os.path.join(self.runtime_dir, cf_basename)\n        )\n        self.km = KernelManager(kernel_name=self.kernel_name, config=self.config)\n\n        self.loop = IOLoop.current()\n        self.loop.add_callback(self._record_started)\n\n    def setup_signals(self) -> None:\n        \"\"\"Shutdown on SIGTERM or SIGINT (Ctrl-C)\"\"\"\n        if os.name == \"nt\":\n            return\n\n        def shutdown_handler(signo, frame):\n            self.loop.add_callback_from_signal(self.shutdown, signo)\n\n        for sig in [signal.SIGTERM, signal.SIGINT]:\n            signal.signal(sig, shutdown_handler)\n\n    def shutdown(self, signo: int) -> None:\n        \"\"\"Shut down the application.\"\"\"\n        self.log.info(\"Shutting down on signal %d\", signo)\n        self.km.shutdown_kernel()\n        self.loop.stop()\n\n    def log_connection_info(self) -> None:\n        \"\"\"Log the connection info for the kernel.\"\"\"\n        cf = self.km.connection_file\n        self.log.info(\"Connection file: %s\", cf)\n        self.log.info(\"To connect a client: --existing %s\", os.path.basename(cf))\n\n    def _record_started(self) -> None:\n        \"\"\"For tests, create a file to indicate that we've started\n\n        Do not rely on this except in our own tests!\n        \"\"\"\n        fn = os.environ.get(\"JUPYTER_CLIENT_TEST_RECORD_STARTUP_PRIVATE\")\n        if fn is not None:\n            with open(fn, \"wb\"):\n                pass\n\n    def start(self) -> None:\n        \"\"\"Start the application.\"\"\"\n        self.log.info(\"Starting kernel %r\", self.kernel_name)\n        try:\n            self.km.start_kernel()\n            self.log_connection_info()\n            self.setup_signals()\n            self.loop.start()\n        finally:\n            self.km.cleanup_resources()\n\n\ndef run_app():\n    KernelApp.launch_instance()\n"}
{"type": "source_file", "path": "chat/src/og_terminal/ping.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport sys\nimport os\nimport click\nfrom og_sdk.agent_sdk import AgentSyncSDK\nfrom rich.console import Console\nfrom dotenv import dotenv_values\n\n\n@click.command()\n@click.option(\"--octogen_dir\", default=\"~/.octogen\", help=\"the root path of octogen\")\ndef app(octogen_dir):\n    console = Console()\n    if octogen_dir.find(\"~\") == 0:\n        real_octogen_dir = octogen_dir.replace(\"~\", os.path.expanduser(\"~\"))\n    else:\n        real_octogen_dir = octogen_dir\n    if not os.path.exists(real_octogen_dir):\n        os.mkdir(real_octopus_dir)\n    octogen_config = dotenv_values(real_octogen_dir + \"/config\")\n    console = Console()\n    try:\n        if \"api_key\" not in octogen_config or \"endpoint\" not in octogen_config:\n            console.print(\n                f\"❌ api key and endpoint are required! please check your config {octogen_dir}/config\"\n            )\n            sys.exit(1)\n        sdk = AgentSyncSDK(octogen_config[\"endpoint\"], octogen_config[\"api_key\"])\n        sdk.connect()\n        response = sdk.ping()\n        if response.code == 0:\n            console.print(f\"👍 {response.msg}\")\n            sys.exit(0)\n        else:\n            console.print(f\"❌ {response.msg}\")\n            sys.exit(1)\n    except Exception as ex:\n        console.print(\n            f\"❌ please check your config {octogen_dir}/config with error {ex}\"\n        )\n        sys.exit(1)\n"}
{"type": "source_file", "path": "memory/setup.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_memory\",\n    version=\"0.3.6\",\n    description=\"Open source code interpreter agent\",\n    author=\"imotai\",\n    author_email=\"wangtaize@dbpunk.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n\n    packages=[\n        \"og_memory\",\n        \"og_memory.template\",\n    ],\n\n    package_dir={\n        \"og_memory\": \"src/og_memory\",\n        \"og_memory.template\": \"src/og_memory/template\",\n    },\n    install_requires=[\n        \"og_proto\",\n        \"Jinja2\",\n    ],\n    package_data={\"og_memory.template\": [\"*.jinja\"]},\n)\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/chainlit/chainlit_ui.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport json\nimport chainlit as cl\nimport aiohttp\nfrom chainlit.input_widget import TextInput\nfrom og_sdk.utils import process_char_stream\n\n\n@cl.on_chat_start\nasync def start():\n    settings = await cl.ChatSettings([\n        TextInput(\n            id=\"Endpoint\", label=\"Octogen Endpoint\", initial=\"http://127.0.0.1:9529\"\n        ),\n        TextInput(id=\"API_KEY\", label=\"Octogen KEY\", initial=\"\"),\n    ]).send()\n    await setup_agent(settings)\n\n\n@cl.on_settings_update\nasync def setup_agent(settings):\n    cl.user_session.set(\"Endpoint\", settings[\"Endpoint\"])\n    cl.user_session.set(\"Key\", settings[\"API_KEY\"])\n\n\n@cl.on_message\nasync def main(message):\n    last_msg = cl.Message(\n        content=\"\",\n        author=\"Octogen\",\n    )\n    request = {\n        \"prompt\": message,\n        \"token_limit\": 0,\n        \"llm_model_name\": \"string\",\n        \"input_files\": [],\n        \"context_id\": \"string\",\n    }\n    headers = {\"api-token\": cl.user_session.get(\"Key\")}\n    last_type = None\n    async with aiohttp.ClientSession(headers=headers, raise_for_status=True) as session:\n        async with session.post(\n            cl.user_session.get(\"Endpoint\") + \"/process\", json=request\n        ) as r:\n            async for line in r.content:\n                if line:\n                    text = str(line, encoding=\"utf-8\")\n                    response = json.loads(text)\n                    if response[\"step_type\"] == \"OnStepTextTyping\":\n                        if last_type and last_type != \"OnStepTextTyping\":\n                            if last_msg:\n                                await last_msg.send()\n                            new_msg = cl.Message(author=\"Octogen\", content=\"\")\n                            last_msg = new_msg\n                        await last_msg.stream_token(response[\"typing_content\"])\n                        last_type = \"OnStepTextTyping\"\n                    elif response[\"step_type\"] == \"OnStepCodeTyping\":\n                        if last_type != \"OnStepCodeTypeing\":\n                            await last_msg.send()\n                            new_msg = cl.Message(\n                                author=\"Octogen\", language=\"text\", content=\"\"\n                            )\n                            last_msg = new_msg\n                        await last_msg.stream_token(response[\"typing_content\"])\n                        last_type = \"OnStepCodeTypeing\"\n                    elif response[\"step_type\"] == \"OnStepActionStart\":\n                        parent_id = last_msg.parent_id\n                        await last_msg.remove()\n                        tool = response[\"step_action_start\"][\"tool\"]\n                        if tool in [\"execute_python_code\", \"show_sample_code\"]:\n                            tool_input = json.loads(\n                                response[\"step_action_start\"][\"input\"]\n                            )\n                            new_msg = cl.Message(\n                                author=\"Octogen\",\n                                language=tool_input.get(\"language\", \"text\"),\n                                content=\"\",\n                            )\n                            last_msg = new_msg\n                            await last_msg.stream_token(tool_input[\"code\"])\n                        last_type = \"OnStepActionStart\"\n                    elif response[\"step_type\"] == \"OnStepActionStdout\":\n                        if last_type not in [\n                            \"OnStepActionStdout\",\n                            \"OnStepActionStderr\",\n                        ]:\n                            await last_msg.send()\n                            new_msg = cl.Message(\n                                author=\"Octogen\",\n                                language=\"text\",\n                                content=\"\",\n                            )\n                            last_msg = new_msg\n                        last_type = \"OnStepActionStdout\"\n                        temp_content = last_msg.content + response[\"step_action_stdout\"]\n                        new_content = process_char_stream(temp_content)\n                        last_msg.content = new_content\n                        await last_msg.update()\n                    elif response[\"step_type\"] == \"OnStepActionStderr\":\n                        if last_type not in [\n                            \"OnStepActionStdout\",\n                            \"OnStepActionStderr\",\n                        ]:\n                            await last_msg.send()\n                            new_msg = cl.Message(\n                                author=\"Octogen\", language=\"text\", content=\"\"\n                            )\n                            last_msg = new_msg\n                        last_type = \"OnStepActionStderr\"\n                        temp_content = last_msg.content + response[\"step_action_stderr\"]\n                        new_content = process_char_stream(temp_content)\n                        last_msg.content = new_content\n                        await last_msg.update()\n                    elif response[\"step_type\"] == \"OnStepActionEnd\":\n                        await last_msg.send()\n                        last_msg = None\n                        last_type = \"OnStepActionEnd\"\n                    elif response[\"step_type\"] == \"OnFinalAnswer\":\n                        await last_msg.send()\n                        last_msg = None\n                        last_type = \"OnFinalAnswer\"\n"}
{"type": "source_file", "path": "roles/src/og_roles/code_interpreter.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\"\n\n\"\"\"\nimport json\nfrom og_proto.prompt_pb2 import ActionDesc\n\n\nROLE = f\"\"\"You are the Programming Copilot, a world-class programmer to complete any goal by executing code\"\"\"\nRULES = [\n    \"To complete the goal, write a plan and execute it step-by-step, limiting the number of steps to five\",\n    \"Every step must include the explanation and the code block. if the code block has any display data, save it as a file and add it to saved_filenames field\",\n    \"You have a fully controlled programming environment to execute code with internet connection but sudo is not allowed\",\n    \"You must try to correct your code when you get errors from the output\",\n    \"You can install new package with pip\",\n    \"Use `execute` action to execute any code and `direct_message` action to send message to user\",\n]\nFUNCTION_EXECUTE= ActionDesc(\n        name=\"execute\",\n        desc=\"This action executes code in your programming environment and returns the output\",\n        parameters=json.dumps({\n            \"type\": \"object\",\n            \"properties\": {\n                \"explanation\": {\n                    \"type\": \"string\",\n                    \"description\": \"the explanation about the code parameters\",\n                },\n                \"code\": {\n                    \"type\": \"string\",\n                    \"description\": \"the bash code to be executed\",\n                },\n                \"language\": {\n                    \"type\": \"string\",\n                    \"description\": \"the language of the code, only python and bash are supported\",\n                },\n                \"saved_filenames\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"string\"},\n                    \"description\": \"A list of filenames that were created by the code\",\n                },\n            },\n            \"required\": [\"explanation\", \"code\", \"language\"],\n        }),\n    )\n\nFUNCTION_DIRECT_MESSAGE= ActionDesc(\n        name=\"direct_message\",\n        desc=\"This action sends a direct message to user.\",\n        parameters=json.dumps({\n            \"type\": \"object\",\n            \"properties\": {\n                \"message\": {\n                    \"type\": \"string\",\n                    \"description\": \"the message will be sent to user\",\n                },\n            },\n            \"required\": [\"message\"],\n        }),\n)\nACTIONS = [\n    FUNCTION_EXECUTE\n]\nOUTPUT_FORMAT = \"\"\"The output format must be a JSON format with the following fields:\n* function_call: The name of the action\n* arguments: The arguments of the action\n\"\"\"\n\nOCTOGEN_CODELLAMA_MID_INS = \"\"\"The above output of the %s determines whether the execution is successful. \nIf successful, go to the next step. If the current step is the final step, summarize the entire plan. If not, adjust the input and try again\"\"\"\n\nOCTOGEN_CODELLAMA_MID_ERROR_INS = \"\"\"Adjust the action input and try again for the above output of %s showing the error message\"\"\"\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/kernel/launch_kernel.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom og_kernel.kernel.kernel_app import run_app\n\nif __name__ == \"__main__\":\n    run_app()\n"}
{"type": "source_file", "path": "roles/src/og_roles/__init__.py", "content": ""}
{"type": "source_file", "path": "chat/setup.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_chat\",\n    version=\"0.3.6\",\n    description=\"the chat client for open source code interpreter octogen\",\n    author=\"imotai\",\n    author_email=\"codego.me@gmail.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    packages=[\n        \"og_discord\",\n        \"og_terminal\",\n    ],\n    package_dir={\n        \"og_discord\": \"src/og_discord\",\n        \"og_terminal\": \"src/og_terminal\",\n    },\n    install_requires=[\n        \"og_sdk>=0.1.0\",\n        \"rich>=13.5.2\",\n        \"prompt_toolkit>=3.0.0\",\n        \"click>=8.0.0\",\n        \"discord.py>=2.3.2\",\n        \"clipboard>=0.0.4\",\n        \"term-image>=0.7.0\",\n        \"python-dotenv\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"og = og_terminal.terminal_chat:app\",\n            \"og_ping = og_terminal.ping:app\",\n            \"og_discord_bot = og_discord.discord_chat:run_app\",\n        ]\n    },\n)\n"}
{"type": "source_file", "path": "roles/setup.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_roles\",\n    version=\"0.3.6\",\n    description=\"Open source llm agent service\",\n    author=\"imotai\",\n    author_email=\"wangtaize@dbpunk.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    packages=[\n        \"og_roles\",\n    ],\n    package_dir={\n        \"og_roles\": \"src/og_roles\",\n    },\n    package_data={},\n)\n"}
{"type": "source_file", "path": "chat/src/og_discord/discord_chat.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport discord\nimport asyncio\nimport logging\nimport json\nimport sys\nimport os\nimport click\nfrom datetime import datetime\nfrom dotenv import dotenv_values\nfrom og_proto import common_pb2\nfrom og_sdk.agent_sdk import AgentSDK\n\nLOG_LEVEL = logging.INFO\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(__name__)\n\n\nclass OctogenDiscordBot(discord.Client):\n\n    def __init__(self, octogen_sdk, filedir, **kwargs):\n        discord.Client.__init__(self, **kwargs)\n        self.octogen_sdk = octogen_sdk\n        self.filedir = filedir\n\n    def handle_action_start(self, respond, saved_images):\n        \"\"\"Run on agent action.\"\"\"\n        segments = []\n        if not respond.on_agent_action:\n            return segments\n        action = respond.on_agent_action\n        if not action.input:\n            return segments\n        logger.info(\"handle action start return\")\n        arguments = json.loads(action.input)\n        if action.tool == \"execute_python_code\" and action.input:\n            explanation = arguments[\"explanation\"]\n            code = arguments[\"code\"]\n            saved_images.extend(arguments.get(\"saved_filenames\", []))\n            mk = f\"\"\"{explanation}\\n\n```python\n{code}\n```\"\"\"\n            segments.append(mk)\n        return segments\n\n    def handle_final_answer(self, respond):\n        segments = []\n        if not respond.final_respond:\n            return segments\n        answer = respond.final_respond.answer\n        if not answer:\n            return segments\n        state = \"token:%s iteration:%s model:%s\" % (\n            respond.token_usage,\n            respond.iteration,\n            respond.model_name,\n        )\n        segments.append(\"%s\\n%s\" % (answer, state))\n        return segments\n\n    def handle_action_output(self, respond, saved_images):\n        segments = []\n        if not respond.on_agent_action_end:\n            return segments\n        mk = respond.on_agent_action_end.output\n        if not mk:\n            return segments\n        saved_images.extend(respond.on_agent_action_end.output_files)\n        segments.append(mk)\n        return segments\n\n    async def download_files(self, images):\n        for image in images:\n            await self.octogen_sdk.download_file(image, self.filedir)\n\n    async def on_ready(self):\n        logger.info(f\"Logged in as {self.user} (ID: {self.user.id})\")\n\n    async def run_app(self, name, message):\n        saved_images = []\n        async for respond in self.octogen_sdk.run(name):\n            if not respond:\n                break\n            if respond.on_agent_action_end:\n                segments = self.handle_action_output(respond, saved_images)\n                msg = \"\".join(segments)\n                if msg:\n                    await message.channel.send(msg)\n            if respond.on_agent_action:\n                segments = self.handle_action_start(respond, saved_images)\n                msg = \"\".join(segments)\n                if msg:\n                    await message.channel.send(msg)\n        saved_images = list(set(saved_images))\n        if saved_images:\n            await self.download_files(saved_images)\n            for filename in saved_images:\n                fullpath = \"%s/%s\" % (self.filedir, filename)\n                await message.channel.send(\"\", file=discord.File(fullpath))\n                break\n\n    async def show_apps(self):\n        header = \"\"\"Apps\n\"\"\"\n        rows = []\n        apps = await self.octogen_sdk.query_apps()\n        for index, app in enumerate(apps.apps):\n            ctime = datetime.fromtimestamp(app.ctime).strftime(\"%m/%d/%Y\")\n            rows.append(f\"{index+1}.{app.name}\")\n        table = header + \"\\n\".join(rows)\n        return table\n\n    async def on_message(self, message):\n        # we do not want the bot to reply to itself\n        try:\n            if message.author.id == self.user.id:\n                return\n            if message.content.find(\"/apps\") >= 0:\n                apps = await self.show_apps()\n                await message.channel.send(apps)\n                return\n            content = message.content\n            if content.find(\"/run\") >= 0:\n                name = content.split(\" \")[1]\n                await self.run_app(name, message)\n                return\n            await message.channel.send(\"working...\")\n            files = []\n            for att in message.attachments:\n\n                async def generate_chunk(att):\n                    # TODO split\n                    chunk = await att.read()\n                    yield common_pb2.FileChunk(buffer=chunk, filename=att.filename)\n\n                await sdk.upload_binary(generate_chunk(att), att.filename)\n                files.append(\"uploaded \" + att.filename)\n            if files:\n                prompt = message.content + \"\\n\" + \"\\n\".join(files)\n            else:\n                prompt = message.content\n            try:\n                async for respond in self.octogen_sdk.prompt(prompt):\n                    if not respond:\n                        break\n                    logger.info(f\"{respond}\")\n                    if respond.on_agent_action_end:\n                        saved_images = []\n                        segments = self.handle_action_output(respond, saved_images)\n                        msg = \"\".join(segments)\n                        logger.info(f\"action output {msg}\")\n                        if msg:\n                            if saved_images:\n                                await self.download_files(saved_images)\n                                for filename in saved_images:\n                                    fullpath = \"%s/%s\" % (self.filedir, filename)\n                                    await message.channel.send(\n                                        msg, file=discord.File(fullpath)\n                                    )\n                                    break\n                            else:\n                                await message.channel.send(msg)\n                    if respond.on_agent_action:\n                        saved_images = []\n                        segments = self.handle_action_start(respond, saved_images)\n                        msg = \"\".join(segments)\n                        logger.info(f\"action start {msg}\")\n                        if msg:\n                            await message.channel.send(msg)\n                    if respond.final_respond:\n                        segments = self.handle_final_answer(respond)\n                        msg = \"\".join(segments)\n                        logger.info(f\"final answer {msg}\")\n                        if msg:\n                            await message.channel.send(msg)\n            except Exception as ex:\n                logger.error(f\"fail to get file {ex}\")\n                await message.channel.send(\"I am sorry for the internal error\")\n        except Exception as ex:\n            logging.exception(ex)\n\n\nasync def app():\n    octogen_discord_bot_dir = \"~/.octogen_discord_bot\"\n    if octogen_discord_bot_dir.find(\"~\") == 0:\n        real_octogen_dir = octogen_discord_bot_dir.replace(\"~\", os.path.expanduser(\"~\"))\n    else:\n        real_octogen_dir = octogen_discord_bot_dir\n    if not os.path.exists(real_octogen_dir):\n        os.mkdir(real_octogen_dir)\n    octogen_config = dotenv_values(real_octogen_dir + \"/config\")\n    filedir = real_octogen_dir + \"/data\"\n    if not os.path.exists(filedir):\n        os.mkdir(filedir)\n    sdk = AgentSDK(octogen_config[\"endpoint\"], octogen_config[\"api_key\"])\n    sdk.connect()\n    intents = discord.Intents.default()\n    intents.message_content = True\n    client = OctogenDiscordBot(sdk, filedir, intents=intents)\n    await client.start(octogen_config[\"discord_bot_token\"])\n\n\ndef run_app():\n    asyncio.run(app())\n"}
{"type": "source_file", "path": "kernel/src/og_kernel/server/kernel_rpc_server.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\"\nthe websocket server for the kernel\n\"\"\"\nimport os\nimport json\nimport sys\nimport logging\nimport asyncio\nimport functools\nimport grpc\nimport re\nimport uuid\nimport random\nimport string\nimport base64\nimport tempfile\nfrom pathlib import Path\nfrom typing import Awaitable, Callable, Optional, AsyncIterable\nfrom grpc.aio import ServicerContext, server, ServerInterceptor\nfrom google.rpc import status_pb2\nfrom dotenv import dotenv_values\nfrom ..kernel.kernel_mgr import KernelManager\nfrom og_proto.kernel_server_pb2_grpc import KernelServerNodeServicer\nfrom og_proto.kernel_server_pb2_grpc import add_KernelServerNodeServicer_to_server\nfrom og_proto import kernel_server_pb2\nfrom og_proto import common_pb2\nfrom ..kernel.kernel_client import KernelClient\nimport aiofiles\nfrom aiofiles import os as aio_os\n\nconfig = dotenv_values(\".env\")\n\nLOG_LEVEL = (\n    logging.DEBUG if config.get(\"log_level\", \"info\") == \"debug\" else logging.INFO\n)\n\nlogging.basicConfig(\n    level=LOG_LEVEL,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(__name__)\nansi_escape = re.compile(r\"(?:\\x1B[@-_]|[\\x80-\\x9F])[0-?]*[ -/]*[@-~]\")\n\n\ndef _unary_unary_rpc_terminator(code, details):\n    async def terminate(ignored_request, context):\n        await context.abort(code, details)\n\n    return grpc.unary_unary_rpc_method_handler(terminate)\n\n\nclass ApiKeyInterceptor(ServerInterceptor):\n    \"\"\"\n    the api key interceptor\n    \"\"\"\n\n    def __init__(self, header, value, code, error):\n        self._header = header\n        self._value = value\n        self._terminator = _unary_unary_rpc_terminator(code, error)\n\n    async def intercept_service(\n        self,\n        continuation: Callable[\n            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]\n        ],\n        handler_call_details: grpc.HandlerCallDetails,\n    ) -> grpc.RpcMethodHandler:\n        if (\n            self._header,\n            self._value,\n        ) in handler_call_details.invocation_metadata:\n            return await continuation(handler_call_details)\n        else:\n            return self._terminator\n\n\nclass KernelRpcServer(KernelServerNodeServicer):\n\n    def __init__(self):\n        self.kms = {}\n        self.kcs = {}\n        self.auth_failed_status = status_pb2.Status(\n            code=grpc.StatusCode.INVALID_ARGUMENT.value[0],\n            message=\"api key is required\",\n            details=[],\n        )\n        os.makedirs(config[\"config_root_path\"], exist_ok=True)\n        os.makedirs(config[\"workspace\"], exist_ok=True)\n        config_root_path = config[\"config_root_path\"]\n        workspace = config[\"workspace\"]\n        logger.info(\n            f\"start kernel rpc with config root path {config_root_path} and workspace {workspace}\"\n        )\n\n    async def stop(\n        self, request: kernel_server_pb2.StopKernelRequest, context: ServicerContext\n    ) -> kernel_server_pb2.StopKernelResponse:\n        \"\"\"\n        Stop the kernel\n        \"\"\"\n\n        kernel_name = request.kernel_name if request.kernel_name else \"python3\"\n        if kernel_name not in self.kms or not self.kms[kernel_name]:\n            logger.warning(\"no started kernel\")\n            return kernel_server_pb2.StopKernelResponse(\n                key=\"k\", code=1, msg=\"no started kernel\"\n            )\n        self.kms[kernel_name].stop()\n        self.kcs[kernel_name].stop_client()\n        self.kms[kernel_name] = None\n        self.kcs[kernel_name] = None\n        return kernel_server_pb2.StopKernelResponse(code=0, msg=\"ok\")\n\n    async def get_status(\n        self, request: kernel_server_pb2.GetStatusRequest, context: ServicerContext\n    ) -> kernel_server_pb2.GetStatusResponse:\n        kernel_name = request.kernel_name if request.kernel_name else \"python3\"\n        logger.debug(\"check the kernel %s status\", kernel_name)\n        if kernel_name not in self.kms or not self.kms[kernel_name]:\n            return kernel_server_pb2.GetStatusResponse(is_alive=False, code=0, msg=\"ok\")\n        is_alive = await self.kcs[kernel_name].is_alive()\n        return kernel_server_pb2.GetStatusResponse(is_alive=is_alive, code=0, msg=\"ok\")\n\n    async def start(\n        self, request: kernel_server_pb2.StartKernelRequest, context: ServicerContext\n    ) -> kernel_server_pb2.StartKernelResponse:\n        \"\"\"\n        Start the kernel\n        \"\"\"\n        kernel_name = request.kernel_name if request.kernel_name else \"python3\"\n        if kernel_name in self.kms and self.kms[kernel_name]:\n            logger.warning(\n                \"the request will be ignored for that the kernel has been started\"\n            )\n            return kernel_server_pb2.StartKernelResponse(\n                code=0, msg=\"the kernel has been started\"\n            )\n        logging.info(\"create a new kernel with kernel_name %s\" % kernel_name)\n        connection_file = \"%s/kernel-%s.json\" % (\n            config[\"config_root_path\"],\n            uuid.uuid4(),\n        )\n        km = KernelManager(connection_file, config[\"workspace\"], kernel=kernel_name)\n        km.start()\n        kc = KernelClient(connection_file)\n        await kc.start_client()\n        self.kms[kernel_name] = km\n        self.kcs[kernel_name] = kc\n        return kernel_server_pb2.StartKernelResponse(code=0, msg=\"ok\")\n\n    async def download(\n        self, request: common_pb2.DownloadRequest, context: ServicerContext\n    ) -> AsyncIterable[common_pb2.FileChunk]:\n        \"\"\"\n        download file\n        \"\"\"\n        filename = request.filename\n        workspace = config[\"workspace\"]\n        target_filename = f\"{workspace}{os.sep}{filename}\"\n        if not await aio_os.path.exists(target_filename):\n            await context.abort(10, \"%s filename do not exist\" % request.filename)\n        async with aiofiles.open(target_filename, \"rb\") as afp:\n            while True:\n                chunk = await afp.read(1024 * 128)\n                if not chunk:\n                    break\n                yield common_pb2.FileChunk(buffer=chunk, filename=request.filename)\n\n    async def upload(\n        self,\n        request: AsyncIterable[common_pb2.FileChunk],\n        context: ServicerContext,\n    ) -> common_pb2.FileUploaded:\n        \"\"\"\n        upload file\n        \"\"\"\n        temp_dir = tempfile.mkdtemp(prefix=\"octogen\")\n        filename = \"\".join(random.choices(string.ascii_lowercase, k=16))\n        tmp_full_path = f\"{temp_dir}{os.sep}{filename}\"\n        target_filename = None\n        logger.info(f\"upload file to temp file {tmp_full_path}\")\n        length = 0\n        async with aiofiles.open(tmp_full_path, \"wb\") as afp:\n            async for chunk in request:\n                length = length + await afp.write(chunk.buffer)\n                logger.debug(f\"write the {tmp_full_path} with {length}\")\n                if not target_filename:\n                    target_filename = \"%s/%s\" % (config[\"workspace\"], chunk.filename)\n        if length != 0:\n            logging.info(f\"move file from {tmp_full_path} to  {target_filename}\")\n            await aio_os.rename(tmp_full_path, target_filename)\n        else:\n            logging.warning(\"empty file\")\n        await aio_os.rmdir(temp_dir)\n        return common_pb2.FileUploaded(length=length)\n\n    async def execute(\n        self, request: kernel_server_pb2.ExecuteRequest, context: ServicerContext\n    ) -> AsyncIterable[kernel_server_pb2.ExecuteResponse]:\n        \"\"\"\n        Execute the python code and return a stream response\n        \"\"\"\n        kernel_name = request.kernel_name if request.kernel_name else \"python3\"\n        if not request.code:\n            raise grpc.RpcError(grpc.StatusCode.INVALID_ARGUMENT, \"Invalid argument\")\n        if (\n            kernel_name not in self.kms\n            or not self.kms[kernel_name]\n            or not self.kcs[kernel_name]\n        ):\n            logger.warning(\n                \"no started kernel for executing code for kernel name %s\" % kernel_name\n            )\n            raise grpc.RpcError(grpc.StatusCode.INVALID_ARGUMENT, \"Invalid argument\")\n        logger.debug(\"the code %s with kernel %s\", request.code, kernel_name)\n        # TODO check the busy status\n        msg_id = self.kcs[kernel_name].execute(request.code)\n        async for msg in self.kcs[kernel_name].read_response(context, 5):\n            try:\n                if context.cancelled() or not msg:\n                    break\n                if msg[\"parent_header\"][\"msg_id\"] != msg_id:\n                    continue\n                if msg[\"msg_type\"] in [\"status\", \"execute_input\"]:\n                    continue\n                respond = self._build_payload(msg, config[\"workspace\"])\n                yield respond\n            except Exception as ex:\n                logger.exception(\"fail to handle the result\")\n\n    def _build_payload(self, msg, workspace) -> kernel_server_pb2.ExecuteResponse:\n        if msg[\"msg_type\"] == \"display_data\":\n            if \"image/png\" in msg[\"content\"][\"data\"]:\n                filename = \"octopus_%s.png\" % uuid.uuid4().hex\n                fullpath = \"%s/%s\" % (workspace, filename)\n                with open(fullpath, \"wb+\") as fd:\n                    data = msg[\"content\"][\"data\"][\"image/png\"].encode(\"ascii\")\n                    buffer = base64.b64decode(data)\n                    fd.write(buffer)\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.ResultType,\n                    output=json.dumps({\"image/png\": filename}),\n                )\n            elif \"image/gif\" in msg[\"content\"][\"data\"]:\n                filename = \"octopus_%s.gif\" % uuid.uuid4().hex\n                fullpath = \"%s/%s\" % (workspace, filename)\n                with open(fullpath, \"wb+\") as fd:\n                    data = msg[\"content\"][\"data\"][\"image/gif\"].encode(\"ascii\")\n                    buffer = base64.b64decode(data)\n                    fd.write(buffer)\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.ResultType,\n                    output=json.dumps({\"image/gif\": filename}),\n                )\n            elif \"text/plain\" in msg[\"content\"][\"data\"]:\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.ResultType,\n                    output=json.dumps(\n                        {\"text/plain\": msg[\"content\"][\"data\"][\"text/plain\"]}\n                    ),\n                )\n            else:\n                logger.warning(f\" unsupported display_data {msg}\")\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.ResultType,\n                    output=json.dumps({}),\n                )\n                # keys = \",\".join(msg[\"content\"][\"data\"].keys())\n                # raise Exception(\n                #    f\"unsupported display data type {keys} for the result {msg}\"\n                # )\n\n        if msg[\"msg_type\"] == \"execute_result\":\n            logger.debug(\"result data %s\", msg[\"content\"][\"data\"][\"text/plain\"])\n            return kernel_server_pb2.ExecuteResponse(\n                output_type=kernel_server_pb2.ExecuteResponse.ResultType,\n                output=json.dumps({\"text/plain\": msg[\"content\"][\"data\"][\"text/plain\"]}),\n            )\n        elif msg[\"msg_type\"] == \"stream\":\n            if msg[\"content\"][\"name\"] == \"stdout\":\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.StdoutType,\n                    output=json.dumps({\"text\": msg[\"content\"][\"text\"]}),\n                )\n            else:\n                return kernel_server_pb2.ExecuteResponse(\n                    output_type=kernel_server_pb2.ExecuteResponse.StderrType,\n                    output=json.dumps({\"text\": msg[\"content\"][\"text\"]}),\n                )\n        elif msg[\"msg_type\"] == \"error\":\n            if len(msg[\"content\"][\"traceback\"]) > 6:\n                traceback = \"\\n\".join(msg[\"content\"][\"traceback\"][:3])\n                traceback = traceback + \"\\n\".join(msg[\"content\"][\"traceback\"][-3:])\n            else:\n                traceback = \"\\n\".join(msg[\"content\"][\"traceback\"])\n            return kernel_server_pb2.ExecuteResponse(\n                output_type=kernel_server_pb2.ExecuteResponse.TracebackType,\n                output=json.dumps({\"traceback\": ansi_escape.sub(\"\", traceback)}),\n            )\n        raise Exception(f\"unsupported msg type {msg}\")\n\n\nasync def serve() -> None:\n    logger.info(\n        \"start kernel rpc server with host %s and port %s\",\n        config[\"rpc_host\"],\n        config[\"rpc_port\"],\n    )\n    interceptors = [\n        ApiKeyInterceptor(\n            \"api_key\",\n            config[\"rpc_key\"],\n            grpc.StatusCode.ABORTED.value[0],\n            \"api key is required or invalid\",\n        )\n    ]\n    serv = server(interceptors=interceptors)\n    add_KernelServerNodeServicer_to_server(KernelRpcServer(), serv)\n    listen_addr = \"%s:%s\" % (config[\"rpc_host\"], config[\"rpc_port\"])\n    serv.add_insecure_port(listen_addr)\n    await serv.start()\n    await serv.wait_for_termination()\n\n\ndef server_main():\n    asyncio.run(serve())\n"}
{"type": "source_file", "path": "chat/src/og_terminal/utils.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\n\ndef parse_file_path(real_prompt):\n    \"\"\"\n    parse the file path from the prompt\n    \"\"\"\n    filepaths = []\n    position = 0\n    while position < len(real_prompt):\n        first_pos = real_prompt.find(\"/up\", position)\n        # break the loop if no file to upload\n        if first_pos == -1 or len(real_prompt) - first_pos <= 4:\n            break\n        #\n        if real_prompt[first_pos + 3] != \" \":\n            position = first_pos + 4\n            continue\n        end_pos = real_prompt.find(\"\\n\", first_pos + 4)\n        end_pos = end_pos if end_pos >= 0 else len(real_prompt)\n        blank_pos = real_prompt.find(\" \", first_pos + 4, end_pos)\n        if blank_pos == -1:\n            filepath = real_prompt[first_pos + 4 : end_pos]\n            position = len(real_prompt)\n        else:\n            filepath = real_prompt[first_pos + 4 : blank_pos]\n            position = blank_pos\n        if filepath:\n            filepaths.append(filepath)\n    return filepaths\n"}
{"type": "source_file", "path": "sdk/src/og_sdk/utils.py", "content": "# Copyright (C) 2023 dbpunk.com Author imotai <codego.me@gmail.com>\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\nimport re\nimport string\nimport random\nimport aiofiles\nimport logging\nfrom og_proto import agent_server_pb2, common_pb2\nfrom typing import AsyncIterable\n\nlogger = logging.getLogger(__name__)\n\n\ndef generate_chunk(filepath, filename) -> common_pb2.FileChunk:\n    try:\n        with open(filepath, \"rb\") as fp:\n            while True:\n                chunk = fp.read(1024 * 128)\n                if not chunk:\n                    break\n                yield common_pb2.FileChunk(buffer=chunk, filename=filename)\n    except Exception as ex:\n        logger.error(\"fail to read file %s\" % ex)\n\n\nasync def generate_async_chunk(\n    filepath, filename\n) -> AsyncIterable[common_pb2.FileChunk]:\n    try:\n        async with aiofiles.open(filepath, \"rb\") as afp:\n            while True:\n                chunk = await afp.read(1024 * 128)\n                if not chunk:\n                    break\n                yield common_pb2.FileChunk(buffer=chunk, filename=filename)\n    except Exception as ex:\n        logger.error(\"fail to read file %s\", ex)\n\n\ndef process_char_stream(stream):\n    buffer = []\n    i = 0\n\n    def carriage_return(buf):\n        pop_buf = []\n        if \"\\n\" in buf:\n            for _ in range(buf[::-1].index(\"\\n\")):\n                pop_buf.append(buf.pop())\n            return pop_buf[::-1]\n        else:\n            pop_buf.extend(buf)\n            buf.clear()\n            return pop_buf\n\n    last_pop_buf = []\n    while i < len(stream):\n        c = stream[i]\n        if c == \"\\b\":\n            if buffer:\n                buffer.pop()\n            last_pop_buf = []\n        elif c == \"\\r\":\n            last_pop_buf = carriage_return(buffer)\n        elif c == \"\\n\":\n            if last_pop_buf:\n                buffer.extend(last_pop_buf)\n                last_pop_buf = []\n            buffer.append(c)\n        else:\n            last_pop_buf = []\n            buffer.append(c)\n        i += 1\n    if last_pop_buf:\n        buffer.extend(last_pop_buf)\n    return \"\".join(buffer)\n\n\ndef clean_code(code: str):\n    start_tag = \"```\"\n    end_tag = \"```\"\n    index = code.find(start_tag)\n    if index >= 0:\n        last = code.rfind(end_tag)\n        return code[index + len(start_tag) : last]\n    return code\n\n\ndef parse_link(text):\n    \"\"\"Parses a link from markdown text.\n\n    Args:\n    text: The markdown text.\n\n    Returns:\n    The link text and href, or None if no link is found.\n    \"\"\"\n    link_regex = r\"\\[(.+?)\\]\\((.+?)\\)\"\n    match = re.search(link_regex, text)\n    if match:\n        return match.groups()\n    else:\n        return None, None\n\n\ndef parse_image_filename(string):\n    \"\"\"Parses the image filename from a string.\n\n    Args:\n      string: A string containing the image filename.\n\n    Returns:\n      The image filename, or None if the filename is not valid.\n    \"\"\"\n\n    pattern = r\"octopus_\\w+\\.(jpg|png|gif)\"\n    match = re.search(pattern, string)\n    if match:\n        return match.group()\n    else:\n        return None\n\n\ndef random_str(n):\n    # using random.choices()\n    # generating random strings\n    res = \"\".join(random.choices(string.ascii_uppercase + string.digits, k=n))\n    return str(res)\n"}
{"type": "source_file", "path": "sdk/setup.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"og_sdk\",\n    version=\"0.3.6\",\n    description=\"Open source code interpreter agent for LLM\",\n    author=\"imotai\",\n    author_email=\"codego.me@gmail.com\",\n    url=\"https://github.com/dbpunk-labs/octogen\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    packages=[\n        \"og_sdk\",\n    ],\n    package_dir={\n        \"og_sdk\": \"src/og_sdk\",\n    },\n    install_requires=[\n        \"og_proto\",\n        \"aiofiles\",\n    ],\n)\n"}
{"type": "source_file", "path": "sdk/src/og_sdk/kernel_sdk.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nimport logging\nimport grpc\nfrom grpc import aio\nfrom og_proto import kernel_server_pb2\nfrom og_proto import common_pb2\nfrom og_proto.kernel_server_pb2_grpc import KernelServerNodeStub\nfrom typing import AsyncIterable\n\nlogger = logging.getLogger(__name__)\n\n\nclass KernelSDK:\n\n    def __init__(self, endpoint, api_key):\n        self.endpoint = endpoint\n        self.stub = None\n        self.metadata = aio.Metadata(\n            (\"api_key\", api_key),\n        )\n\n    def connect(self):\n        \"\"\"\n        Connect the remote kernel instance\n        \"\"\"\n        channel = aio.insecure_channel(self.endpoint)\n        self.channel = channel\n        self.stub = KernelServerNodeStub(channel)\n\n    async def stop(self, kernel_name=None):\n        \"\"\"\n        Stop the kernel\n        \"\"\"\n        request = kernel_server_pb2.StopKernelRequest(kernel_name=kernel_name)\n        response = await self.stub.stop(request, metadata=self.metadata)\n        return response\n\n    async def is_alive(self, kernel_name=None):\n        request = kernel_server_pb2.GetStatusRequest(kernel_name=kernel_name)\n        response = await self.stub.get_status(request, metadata=self.metadata)\n        return response.is_alive\n\n    async def download_file(self, filename):\n        request = common_pb2.DownloadRequest(filename=filename)\n        async for chunk in self.stub.download(request, metadata=self.metadata):\n            yield chunk\n\n    async def upload_binary(self, chunks: AsyncIterable[common_pb2.FileChunk]):\n        try:\n            return await self.stub.upload(chunks, metadata=self.metadata)\n        except Exception as ex:\n            logger.error(\"upload file ex %s\" % ex)\n\n    async def start(self, kernel_name=None):\n        \"\"\"\n        Start the kernel\n        \"\"\"\n        request = kernel_server_pb2.StartKernelRequest(kernel_name=kernel_name)\n        response = await self.stub.start(request, metadata=self.metadata)\n        return response\n\n    async def execute(self, code, kernel_name=None):\n        \"\"\"\n        Execute the python code\n        \"\"\"\n        request = kernel_server_pb2.ExecuteRequest(code=code, kernel_name=kernel_name)\n        async for respond in self.stub.execute(request, metadata=self.metadata):\n            yield respond\n\n    async def close(self):\n        if self.channel:\n            self.channel.close()\n            self.channel = None\n"}
{"type": "source_file", "path": "memory/src/og_memory/template/__init__.py", "content": ""}
{"type": "source_file", "path": "agent/src/og_agent/tokenizer.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\"\nTaken from the NAYA project\n\nhttps://github.com/danielyule/naya\n\nCopyright (c) 2019 Daniel Yule\n\"\"\"\nimport io\nimport unicodedata\n\nSURROGATE = \"Cs\"\n\n\nclass TokenType:\n    OPERATOR = 0\n    STRING = 1\n    NUMBER = 2\n    BOOLEAN = 3\n    NULL = 4\n\n\nclass State:\n    WHITESPACE = 0\n    INTEGER_0 = 1\n    INTEGER_SIGN = 2\n    INTEGER = 3\n    INTEGER_EXP = 4\n    INTEGER_EXP_0 = 5\n    FLOATING_POINT_0 = 6\n    FLOATING_POINT = 8\n    STRING = 9\n    STRING_ESCAPE = 10\n    STRING_END = 11\n    TRUE_1 = 12\n    TRUE_2 = 13\n    TRUE_3 = 14\n    FALSE_1 = 15\n    FALSE_2 = 16\n    FALSE_3 = 17\n    FALSE_4 = 18\n    NULL_1 = 19\n    NULL_2 = 20\n    NULL_3 = 21\n    UNICODE = 22\n    UNICODE_SURROGATE_START = 23\n    UNICODE_SURROGATE_STRING_ESCAPE = 24\n    UNICODE_SURROGATE = 25\n\n\nclass SpecialChar:\n    # Kind of a hack but simple: if we used the empty string \"\" to represent\n    # EOF, expressions like `char in \"0123456789\"` would be true for EOF, which\n    # is confusing. If we used a non-string, they would result in TypeErrors.\n    # By using the string \"EOF\", they work as expected. The only thing we have\n    # to be careful about is to not ever use \"EOF\" in any such strings used for\n    # char membership checking, which we have no reason to do anyway.\n    EOF = \"EOF\"\n\n\nclass UnCompletedException(Exception):\n\n    def __init_(self, token):\n        super().__init__(\"\")\n        self.token = token\n\n\ndef _guess_encoding(stream):\n    # if it looks like a urllib response, get the charset from the headers (if any)\n    try:\n        encoding = stream.headers.get_content_charset()\n    except:  # noqa\n        encoding = None\n    if encoding is None:\n        # JSON is supposed to be UTF-8\n        # https://tools.ietf.org/id/draft-ietf-json-rfc4627bis-09.html#:~:text=The%20default%20encoding%20is%20UTF,16%20and%20UTF%2D32).\n        encoding = \"utf-8\"\n    return encoding\n\n\ndef _ensure_text(stream):\n    data = stream.read(0)\n    if isinstance(data, bytes):\n        encoding = _guess_encoding(stream)\n        return io.TextIOWrapper(stream, encoding=encoding)\n    return stream\n\n\ndef tokenize(stream):\n    stream = _ensure_text(stream)\n\n    def is_delimiter(char):\n        return char.isspace() or char in \"{}[]:,\" or char == SpecialChar.EOF\n\n    token = []\n    unicode_buffer = \"\"\n    completed = False\n    now_token = \"\"\n\n    def process_char(char):\n        nonlocal token, completed, now_token, unicode_buffer\n        advance = True\n        add_char = False\n        next_state = state\n        if state == State.WHITESPACE:\n            if char == \"{\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \"{\")\n            elif char == \"}\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \"}\")\n            elif char == \"[\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \"[\")\n            elif char == \"]\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \"]\")\n            elif char == \",\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \",\")\n            elif char == \":\":\n                completed = True\n                now_token = (TokenType.OPERATOR, \":\")\n            elif char == '\"':\n                next_state = State.STRING\n            elif char in \"123456789\":\n                next_state = State.INTEGER\n                add_char = True\n            elif char == \"0\":\n                next_state = State.INTEGER_0\n                add_char = True\n            elif char == \"-\":\n                next_state = State.INTEGER_SIGN\n                add_char = True\n            elif char == \"f\":\n                next_state = State.FALSE_1\n            elif char == \"t\":\n                next_state = State.TRUE_1\n            elif char == \"n\":\n                next_state = State.NULL_1\n            elif not char.isspace() and not char == SpecialChar.EOF:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.INTEGER:\n            if char in \"0123456789\":\n                add_char = True\n            elif char == \".\":\n                next_state = State.FLOATING_POINT_0\n                add_char = True\n            elif char == \"e\" or char == \"E\":\n                next_state = State.INTEGER_EXP_0\n                add_char = True\n            elif is_delimiter(char):\n                next_state = State.WHITESPACE\n                completed = True\n                now_token = (TokenType.NUMBER, int(\"\".join(token)))\n                advance = False\n            else:\n                raise ValueError(\n                    \"A number must contain only digits.  Got '{}'\".format(char)\n                )\n        elif state == State.INTEGER_0:\n            if char == \".\":\n                next_state = State.FLOATING_POINT_0\n                add_char = True\n            elif char == \"e\" or char == \"E\":\n                next_state = State.INTEGER_EXP_0\n                add_char = True\n            elif is_delimiter(char):\n                next_state = State.WHITESPACE\n                completed = True\n                now_token = (TokenType.NUMBER, 0)\n                advance = False\n            else:\n                raise ValueError(\n                    \"A 0 must be followed by a '.' or a 'e'.  Got '{0}'\".format(char)\n                )\n        elif state == State.INTEGER_SIGN:\n            if char == \"0\":\n                next_state = State.INTEGER_0\n                add_char = True\n            elif char in \"123456789\":\n                next_state = State.INTEGER\n                add_char = True\n            else:\n                raise ValueError(\n                    \"A - must be followed by a digit.  Got '{0}'\".format(char)\n                )\n        elif state == State.INTEGER_EXP_0:\n            if char == \"+\" or char == \"-\" or char in \"0123456789\":\n                next_state = State.INTEGER_EXP\n                add_char = True\n            else:\n                raise ValueError(\n                    \"An e in a number must be followed by a '+', '-' or digit.  Got '{0}'\".format(\n                        char\n                    )\n                )\n        elif state == State.INTEGER_EXP:\n            if char in \"0123456789\":\n                add_char = True\n            elif is_delimiter(char):\n                completed = True\n                now_token = (TokenType.NUMBER, float(\"\".join(token)))\n                next_state = State.WHITESPACE\n                advance = False\n            else:\n                raise ValueError(\n                    \"A number exponent must consist only of digits.  Got '{}'\".format(\n                        char\n                    )\n                )\n        elif state == State.FLOATING_POINT:\n            if char in \"0123456789\":\n                add_char = True\n            elif char == \"e\" or char == \"E\":\n                next_state = State.INTEGER_EXP_0\n                add_char = True\n            elif is_delimiter(char):\n                completed = True\n                now_token = (TokenType.NUMBER, float(\"\".join(token)))\n                next_state = State.WHITESPACE\n                advance = False\n            else:\n                raise ValueError(\"A number must include only digits\")\n        elif state == State.FLOATING_POINT_0:\n            if char in \"0123456789\":\n                next_state = State.FLOATING_POINT\n                add_char = True\n            else:\n                raise ValueError(\n                    \"A number with a decimal point must be followed by a fractional part\"\n                )\n        elif state == State.FALSE_1:\n            if char == \"a\":\n                next_state = State.FALSE_2\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.FALSE_2:\n            if char == \"l\":\n                next_state = State.FALSE_3\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.FALSE_3:\n            if char == \"s\":\n                next_state = State.FALSE_4\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.FALSE_4:\n            if char == \"e\":\n                next_state = State.WHITESPACE\n                completed = True\n                now_token = (TokenType.BOOLEAN, False)\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.TRUE_1:\n            if char == \"r\":\n                next_state = State.TRUE_2\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.TRUE_2:\n            if char == \"u\":\n                next_state = State.TRUE_3\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.TRUE_3:\n            if char == \"e\":\n                next_state = State.WHITESPACE\n                completed = True\n                now_token = (TokenType.BOOLEAN, True)\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.NULL_1:\n            if char == \"u\":\n                next_state = State.NULL_2\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.NULL_2:\n            if char == \"l\":\n                next_state = State.NULL_3\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.NULL_3:\n            if char == \"l\":\n                next_state = State.WHITESPACE\n                completed = True\n                now_token = (TokenType.NULL, None)\n            else:\n                raise ValueError(\"Invalid JSON character: '{0}'\".format(char))\n        elif state == State.STRING:\n            if char == '\"':\n                completed = True\n                now_token = (TokenType.STRING, \"\".join(token))\n                next_state = State.STRING_END\n            elif char == \"\\\\\":\n                next_state = State.STRING_ESCAPE\n            elif char == SpecialChar.EOF:\n                raise ValueError(\"Unterminated string at end of file\")\n            else:\n                add_char = True\n        elif state == State.STRING_END:\n            if is_delimiter(char):\n                advance = False\n                next_state = State.WHITESPACE\n            else:\n                raise ValueError(\n                    \"Expected whitespace or an operator after string.  Got '{}'\".format(\n                        char\n                    )\n                )\n        elif state == State.STRING_ESCAPE:\n            next_state = State.STRING\n            if char == \"\\\\\" or char == '\"':\n                add_char = True\n            elif char == \"b\":\n                char = \"\\b\"\n                add_char = True\n            elif char == \"f\":\n                char = \"\\f\"\n                add_char = True\n            elif char == \"n\":\n                char = \"\\n\"\n                add_char = True\n            elif char == \"t\":\n                char = \"\\t\"\n                add_char = True\n            elif char == \"r\":\n                char = \"\\r\"\n                add_char = True\n            elif char == \"/\":\n                char = \"/\"\n                add_char = True\n            elif char == \"u\":\n                next_state = State.UNICODE\n                unicode_buffer = \"\"\n            else:\n                raise ValueError(\"Invalid string escape: {}\".format(char))\n        elif state == State.UNICODE:\n            if char == SpecialChar.EOF:\n                raise ValueError(\"Unterminated unicode literal at end of file\")\n            unicode_buffer += char\n            if len(unicode_buffer) == 4:\n                try:\n                    code_point = int(unicode_buffer, 16)\n                except ValueError:\n                    raise ValueError(f\"Invalid unicode literal: \\\\u{unicode_buffer}\")\n                char = chr(code_point)\n                if unicodedata.category(char) == SURROGATE:\n                    next_state = State.UNICODE_SURROGATE_START\n                else:\n                    next_state = State.STRING\n                    add_char = True\n        elif state == State.UNICODE_SURROGATE_START:\n            if char == \"\\\\\":\n                next_state = State.UNICODE_SURROGATE_STRING_ESCAPE\n            elif char == SpecialChar.EOF:\n                raise ValueError(\"Unpaired UTF-16 surrogate at end of file\")\n            else:\n                raise ValueError(f\"Unpaired UTF-16 surrogate\")\n\n        elif state == State.UNICODE_SURROGATE_STRING_ESCAPE:\n            if char == \"u\":\n                next_state = State.UNICODE_SURROGATE\n            elif char == SpecialChar.EOF:\n                raise ValueError(\"Unpaired UTF-16 surrogate at end of file\")\n            else:\n                raise ValueError(f\"Unpaired UTF-16 surrogate\")\n\n        elif state == State.UNICODE_SURROGATE:\n            if char == SpecialChar.EOF:\n                raise ValueError(\"Unterminated unicode literal at end of file\")\n            unicode_buffer += char\n            if len(unicode_buffer) == 8:\n                code_point_1 = int(unicode_buffer[:4], 16)\n                try:\n                    code_point_2 = int(unicode_buffer[4:], 16)\n                except ValueError:\n                    raise ValueError(\n                        f\"Invalid unicode literal: \\\\u{unicode_buffer[4:]}\"\n                    )\n                char = chr(code_point_2)\n                if unicodedata.category(char) != SURROGATE:\n                    raise ValueError(\n                        f\"Second half of UTF-16 surrogate pair is not a surrogate!\"\n                    )\n                try:\n                    pair = int.to_bytes(code_point_1, 2, \"little\") + int.to_bytes(\n                        code_point_2, 2, \"little\"\n                    )\n                    char = pair.decode(\"utf-16-le\")\n                except ValueError:\n                    raise ValueError(\n                        f\"Error decoding UTF-16 surrogate pair \\\\u{unicode_buffer[:4]}\\\\u{unicode_buffer[4:]}\"\n                    )\n                next_state = State.STRING\n                add_char = True\n\n        if add_char:\n            token.append(char)\n\n        return advance, next_state\n\n    state = State.WHITESPACE\n    c = stream.read(1)\n    index = 0\n    while c:\n        try:\n            advance, state = process_char(c)\n        except Exception as e:\n            yield (state, token)\n            break\n        if completed:\n            completed = False\n            token = []\n            yield (None, now_token)\n        if advance:\n            c = stream.read(1)\n            index += 1\n    try:\n        process_char(SpecialChar.EOF)\n        if completed:\n            yield (None, now_token)\n    except Exception as e:\n        yield (state, token)\n"}
{"type": "source_file", "path": "chat/src/og_terminal/ui_block.py", "content": "# vim:fenc=utf-8\n\n# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\n\nfrom rich.markdown import Markdown\nfrom og_sdk.utils import process_char_stream\nfrom rich.spinner import Spinner\nfrom rich.syntax import Syntax\n\n\nclass BaseBlock:\n\n    def __init__(self, index):\n        self.index = index\n        self.finished = False\n        self.has_error = False\n        self.emoji = \"\"\n\n    def is_finished(self):\n        return self.finished\n\n    def get_index(self):\n        return self.index\n\n    def finish(self, has_error=False):\n        self.finished = True\n        self.has_error = has_error\n\n    def get_status(self):\n        if self.has_error:\n            return \"❌\"\n        if self.finished:\n            return self.emoji\n        else:\n            return Spinner(\"dots\", style=\"status.spinner\", speed=1.0, text=\"\")\n\n    def set_emoji(self, emoji):\n        self.emoji = emoji\n\n\nclass StreamingBlock(BaseBlock):\n\n    def __init__(self, index, content):\n        super().__init__(index)\n        self.content = content\n\n    def append(self, new_content):\n        if self.finished:\n            return\n        tmp_content = self.content + new_content\n        self.content = process_char_stream(tmp_content)\n\n\nclass MarkdownBlock(StreamingBlock):\n\n    def __init__(self, index, content):\n        super().__init__(index, content)\n        self.set_emoji(\"🧠\")\n\n    def render(self):\n        if self.finished:\n            return Markdown(self.content)\n        else:\n            return Markdown(self.content + \"█\")\n\n\nclass TerminalBlock(StreamingBlock):\n\n    def __init__(self, index):\n        super().__init__(index, \"\")\n        self.set_emoji(\"✅\")\n        self.terminal_stdout = \"\"\n        self.terminal_stderr = \"\"\n\n    def render(self):\n        output = self.terminal_stdout\n        if self.terminal_stderr:\n            output += \"\\n\" + self.terminal_stderr\n        if self.finished:\n            return Syntax(output, \"text\", line_numbers=True)\n        else:\n            return Syntax(output + \"█\", \"text\", line_numbers=True)\n\n    def write(self, terminal_stdout, terminal_stderr):\n        if self.finished:\n            return\n        if terminal_stdout:\n            tmp_content = self.terminal_stdout + terminal_stdout\n            self.terminal_stdout = process_char_stream(tmp_content)\n        if terminal_stderr:\n            tmp_content = self.terminal_stderr + terminal_stderr\n            self.terminal_stderr = process_char_stream(tmp_content)\n        output = self.terminal_stdout\n        if self.terminal_stderr:\n            output += \"\\n\" + self.terminal_stderr\n        self.content = output\n\n\nclass CodeBlock(StreamingBlock):\n\n    def __init__(self, index, content, language):\n        super().__init__(index, content)\n        self.language = language\n        self.set_emoji(\"📖\")\n\n    def render(self):\n        if self.finished:\n            return Syntax(self.content, self.language, line_numbers=True)\n        else:\n            return Syntax(self.content + \"█\", self.language, line_numbers=True)\n\n\nclass LoadingBlock(BaseBlock):\n\n    def __init__(self, index):\n        super().__init__(index)\n\n    def render(self):\n        return \"\"\n\n\nclass UploadFilesBlock(BaseBlock):\n\n    def __init__(self, index, filenames):\n        super().__init__(index)\n        self.filenames = filenames\n        self.file_states = {}\n\n    def update_progress(self, filename, uploaded, total):\n        self.file_states[filename] = (uploaded, total)\n\n\nclass TaskBlocks:\n\n    def __init__(self, values):\n        self.blocks = []\n        self.values = values\n\n    def begin(self):\n        self.blocks.append(LoadingBlock(0))\n\n    def add_terminal(self, terminal_stdout, terminal_stderr):\n        last_block = self.blocks[-1]\n        if isinstance(last_block, LoadingBlock):\n            self.blocks.pop()\n            block = TerminalBlock(len(self.values))\n            block.write(terminal_stdout, terminal_stderr)\n            self.blocks.append(block)\n            self.values.append(block.content)\n        elif isinstance(last_block, TerminalBlock):\n            if last_block.is_finished():\n                block = TerminalBlock(len(self.values))\n                block.write(terminal_stdout, terminal_stderr)\n                self.blocks.append(block)\n                self.values.append(block.content)\n            else:\n                last_block.write(terminal_stdout, terminal_stderr)\n                self.values[last_block.get_index()] = last_block.content\n        else:\n            last_block.finish()\n            block = TerminalBlock(len(self.values))\n            block.write(terminal_stdout, terminal_stderr)\n            self.blocks.append(block)\n            self.values.append(block.content)\n\n    def add_markdown(self, content):\n        last_block = self.blocks[-1]\n        if isinstance(last_block, LoadingBlock):\n            self.blocks.pop()\n            self.blocks.append(MarkdownBlock(len(self.values), content))\n            self.values.append(content)\n\n        elif isinstance(last_block, MarkdownBlock):\n            if last_block.is_finished():\n                self.blocks.append(MarkdownBlock(len(self.values), content))\n                self.values.append(content)\n            else:\n                last_block.append(content)\n                self.values[last_block.get_index()] = last_block.content\n        else:\n            last_block.finish()\n            self.blocks.append(MarkdownBlock(len(self.values), content))\n            self.values.append(content)\n\n    def add_loading(self):\n        last_block = self.blocks[-1]\n        if isinstance(last_block, LoadingBlock) and not last_block.is_finished():\n            return\n        self.blocks.append(LoadingBlock(0))\n\n    def finish_current_all_blocks(self):\n        for block in self.blocks:\n            if block.is_finished():\n                continue\n            block.finish()\n\n    def get_last_block(self):\n        return self.blocks[-1]\n\n    def add_code(self, code, language):\n        last_block = self.blocks[-1]\n        if isinstance(last_block, LoadingBlock):\n            self.blocks.pop()\n            self.blocks.append(CodeBlock(len(self.values), code, language))\n            self.values.append(code)\n        elif isinstance(last_block, CodeBlock):\n            if last_block.is_finished():\n                self.blocks.append(CodeBlock(len(self.values), code, language))\n                self.values.append(code)\n            else:\n                last_block.append(code)\n                last_block.language = language\n                self.values[last_block.get_index()] = last_block.content\n        else:\n            last_block.finish()\n            self.blocks.append(CodeBlock(len(self.values), code, language))\n            self.values.append(code)\n\n    def render(self):\n        for block in self.blocks:\n            if isinstance(block, LoadingBlock) and block.is_finished():\n                continue\n            yield (block.get_index(), block.get_status(), block.render())\n"}
{"type": "source_file", "path": "sdk/src/og_sdk/__init__.py", "content": ""}
{"type": "source_file", "path": "sdk/src/og_sdk/agent_sdk.py", "content": "# SPDX-FileCopyrightText: 2023 imotai <jackwang@octogen.dev>\n# SPDX-FileContributor: imotai\n#\n# SPDX-License-Identifier: Elastic-2.0\n\n\"\"\" \"\"\"\nimport logging\nimport grpc\nfrom grpc import aio\nfrom og_proto import agent_server_pb2, common_pb2\nfrom og_proto.agent_server_pb2_grpc import AgentServerStub\nimport aiofiles\nfrom typing import AsyncIterable\nfrom .utils import generate_chunk, generate_async_chunk\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgentSyncSession:\n\n    def __init__(self, agent_sdk):\n        self.context_id = None\n        self.agent_sdk = agent_sdk\n\n    def prompt(self, prompt, files=[]):\n        \"\"\"\n        ask the ai with prompt and  uploaded files\n        \"\"\"\n        for respond in self.agent_sdk.prompt(prompt, files, self.context_id):\n            if respond.context_id:\n                self.context_id = respond.context_id\n            yield respond\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.context_id = None\n\n\nclass AgentAsyncSession:\n\n    def __init__(self, agent_sdk):\n        self.context_id = None\n        self.agent_sdk = agent_sdk\n\n    async def prompt(self, prompt, files=[]):\n        \"\"\"\n        ask the ai with prompt and  uploaded files\n        \"\"\"\n        async for respond in self.agent_sdk.prompt(prompt, files, self.context_id):\n            if respond.context_id:\n                self.context_id = respond.context_id\n            yield respond\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.context_id = None\n\n\nclass AgentBaseSDK:\n\n    def __init__(self, endpoint):\n        self.endpoint = endpoint\n        self.stub = None\n        self.channel = None\n\n    def connect_sync(self):\n        \"\"\"\n        Connect the agent service with sync mode\n        \"\"\"\n        if self.channel:\n            return\n        if self.endpoint.startswith(\"https\"):\n            channel_credential = grpc.ssl_channel_credentials()\n            self.channel = grpc.secure_channel(\n                self.endpoint.replace(\"https://\", \"\"), channel_credential\n            )\n        else:\n            self.channel = grpc.insecure_channel(self.endpoint)\n        self.stub = AgentServerStub(self.channel)\n\n    def connect_async(self):\n        \"\"\"\n        Connect the agent service with async mode\n        \"\"\"\n        if self.channel:\n            return\n        if self.endpoint.startswith(\"https\"):\n            channel_credential = grpc.ssl_channel_credentials()\n            self.channel = aio.secure_channel(\n                self.endpoint.replace(\"https://\", \"\"), channel_credential\n            )\n        else:\n            self.channel = aio.insecure_channel(self.endpoint)\n        self.stub = AgentServerStub(self.channel)\n\n\nclass AgentSyncSDK(AgentBaseSDK):\n\n    def __init__(self, endpoint, api_key):\n        super().__init__(endpoint)\n        self.metadata = aio.Metadata(\n            (\"api_key\", api_key),\n        )\n\n    def connect(self):\n        self.connect_sync()\n\n    def create_session(self):\n        \"\"\"\n        create a session for the agent\n        \"\"\"\n        return AgentSyncSession(self)\n\n    def add_kernel(self, key, endpoint):\n        \"\"\"\n        add kernel instance to the agent and only admin can call this method\n        \"\"\"\n        request = agent_server_pb2.AddKernelRequest(endpoint=endpoint, key=key)\n        response = self.stub.add_kernel(request, metadata=self.metadata)\n        return response\n\n    def ping(self):\n        request = agent_server_pb2.PingRequest()\n        response = self.stub.ping(request, metadata=self.metadata)\n        return response\n\n    def download_file(self, filename, parent_path):\n        request = common_pb2.DownloadRequest(filename=filename)\n        fullpath = \"%s/%s\" % (parent_path, filename)\n        with open(fullpath, \"wb+\") as fd:\n            for chunk in self.stub.download(request, metadata=self.metadata):\n                fd.write(chunk.buffer)\n\n    def upload_file(self, filepath, filename):\n        \"\"\"\n        upload file to agent\n        \"\"\"\n\n        # TODO limit the file size\n\n        return self.stub.upload(\n            generate_chunk(filepath, filename), metadata=self.metadata\n        )\n\n    def prompt(self, prompt, files=[], context_id=None):\n        \"\"\"\n        ask the ai with prompt and  uploaded files\n        \"\"\"\n        request = agent_server_pb2.ProcessTaskRequest(\n            task=prompt, input_files=files, context_id=context_id\n        )\n        for respond in self.stub.process_task(request, metadata=self.metadata):\n            yield respond\n\n    def close(self):\n        if self.channel:\n            self.channel.close()\n            self.channel = None\n\n\nclass AgentProxySDK(AgentBaseSDK):\n\n    def __init__(self, endpoint):\n        super().__init__(endpoint)\n        self.endpoint = endpoint\n\n    def connect(self):\n        self.connect_async()\n\n    async def add_kernel(self, key, endpoint, api_key):\n        \"\"\"\n        add kernel instance to the agent and only admin can call this method\n        \"\"\"\n        metadata = aio.Metadata(\n            (\"api_key\", api_key),\n        )\n        request = agent_server_pb2.AddKernelRequest(endpoint=endpoint, key=key)\n        response = await self.stub.add_kernel(request, metadata=metadata)\n        return response\n\n    async def prompt(self, prompt, api_key, files=[], context_id=None):\n        metadata = aio.Metadata(\n            (\"api_key\", api_key),\n        )\n        request = agent_server_pb2.ProcessTaskRequest(\n            task=prompt, input_files=files, context_id=context_id\n        )\n        async for respond in self.stub.process_task(request, metadata=metadata):\n            yield respond\n\n    async def close(self):\n        if self.channel:\n            await self.channel.close()\n            self.channel = None\n\n\nclass AgentSDK(AgentBaseSDK):\n\n    def __init__(self, endpoint, api_key):\n        super().__init__(endpoint)\n        self.metadata = aio.Metadata(\n            (\"api_key\", api_key),\n        )\n\n    def connect(self):\n        self.connect_async()\n\n    async def ping(self):\n        request = agent_server_pb2.PingRequest()\n        response = await self.stub.ping(request, metadata=self.metadata)\n        return response\n\n    async def add_kernel(self, key, endpoint):\n        \"\"\"\n        add kernel instance to the agent and only admin can call this method\n        \"\"\"\n        request = agent_server_pb2.AddKernelRequest(endpoint=endpoint, key=key)\n        response = await self.stub.add_kernel(request, metadata=self.metadata)\n        return response\n\n    def create_session(self):\n        \"\"\"\n        create a session for the agent\n        \"\"\"\n        return AgentAsyncSession(self)\n\n    async def prompt(self, prompt, files=[], context_id=None):\n        \"\"\"\n        ask the ai with prompt and  uploaded files\n        \"\"\"\n        request = agent_server_pb2.ProcessTaskRequest(\n            task=prompt, input_files=files, context_id=context_id\n        )\n        async for respond in self.stub.process_task(request, metadata=self.metadata):\n            yield respond\n\n    async def download_file(self, filename, parent_path):\n        request = common_pb2.DownloadRequest(filename=filename)\n        fullpath = \"%s/%s\" % (parent_path, filename)\n        async with aiofiles.open(fullpath, \"wb+\") as afd:\n            async for chunk in self.stub.download(request, metadata=self.metadata):\n                await afd.write(chunk.buffer)\n\n    async def upload_binary(self, chunks: AsyncIterable[common_pb2.FileChunk]):\n        try:\n            return await self.stub.upload(chunks, metadata=self.metadata)\n        except Exception as ex:\n            logger.error(\"upload file ex %s\", ex)\n\n    async def upload_file(self, filepath, filename):\n        \"\"\"\n        upload file to agent\n        \"\"\"\n        # TODO limit the file size\n        return await self.upload_binary(generate_async_chunk(filepath, filename))\n\n    async def close(self):\n        if self.channel:\n            await self.channel.close()\n            self.channel = None\n"}
