{"repo_info": {"repo_name": "MACD", "repo_owner": "wenwenmin", "repo_url": "https://github.com/wenwenmin/MACD"}}
{"type": "source_file", "path": "MACD_github/Baselines/Cell2location/main_code.py", "content": "import sys\r\nimport scanpy as sc\r\nimport anndata\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\nimport subprocess\r\n\r\nimport os\r\n\r\nimport cell2location\r\nimport scvi\r\n\r\nfrom matplotlib import rcParams\r\nrcParams['pdf.fonttype'] = 42 # enables correct plotting of text\r\nimport seaborn as sns\r\nfrom scipy.sparse import csr_matrix\r\nfrom cell2location.utils.filtering import filter_genes\r\ndef main(a,b,cell_key):\r\n    for i in range(a,b):\r\n        sc_file_path = 'Datasets/preproced_data\\dataset' + str(i)+ '\\Scdata_filter.h5ad'\r\n        spatial_file_path = 'Datasets/preproced_data\\dataset' + str(i)+ '\\Real_STdata_filter.h5ad'\r\n        celltype_key =cell_key\r\n        output_file_path ='Baselines/Cell2location\\Result\\dataset' + str(i)\r\n        if not os.path.exists(output_file_path):\r\n            os.makedirs(output_file_path)\r\n        adata_snrna_raw = sc.read_h5ad(sc_file_path)\r\n        adata_vis = sc.read_h5ad(spatial_file_path)\r\n        adata_snrna_raw.X = csr_matrix(adata_snrna_raw.X)\r\n        adata_vis.X = csr_matrix(adata_vis.X)\r\n        print(adata_snrna_raw)\r\n\r\n        adata_snrna_raw = adata_snrna_raw[~adata_snrna_raw.obs[celltype_key].isin(np.array(\r\n            adata_snrna_raw.obs[celltype_key].value_counts()[\r\n                adata_snrna_raw.obs[celltype_key].value_counts() <= 1].index))]\r\n\r\n        # remove cells and genes with 0 counts everywhere\r\n        sc.pp.filter_genes(adata_snrna_raw, min_cells=1)\r\n        sc.pp.filter_cells(adata_snrna_raw, min_genes=1)\r\n\r\n        adata_snrna_raw.obs[celltype_key] = pd.Categorical(adata_snrna_raw.obs[celltype_key])\r\n        adata_snrna_raw = adata_snrna_raw[~adata_snrna_raw.obs[celltype_key].isna(), :]\r\n\r\n        selected = filter_genes(adata_snrna_raw, cell_count_cutoff=5, cell_percentage_cutoff2=0.03,\r\n                                nonz_mean_cutoff=1.12)\r\n\r\n        # filter the object\r\n        adata_snrna_raw = adata_snrna_raw[:, selected].copy()\r\n\r\n        # scvi.data.setup_anndata(adata=adata_snrna_raw, labels_key=celltype_key)\r\n        cell2location.models.RegressionModel.setup_anndata(adata=adata_snrna_raw, labels_key=celltype_key)\r\n\r\n        # create and train the regression model\r\n        from cell2location.models import RegressionModel\r\n        mod = RegressionModel(adata_snrna_raw)\r\n\r\n        # Use all data for training (validation not implemented yet, train_size=1)\r\n        mod.train(max_epochs=150, batch_size=2500, train_size=1, lr=0.002, use_gpu=True)\r\n\r\n        # plot ELBO loss history during training, removing first 20 epochs from the plot\r\n        # mod.plot_history(20)\r\n\r\n        # In this section, we export the estimated cell abundance (summary of the posterior distribution).\r\n        adata_snrna_raw = mod.export_posterior(\r\n            adata_snrna_raw, sample_kwargs={'num_samples': 1000, 'batch_size': 2500, 'use_gpu': True}\r\n        )\r\n\r\n        # export estimated expression in each cluster\r\n        if 'means_per_cluster_mu_fg' in adata_snrna_raw.varm.keys():\r\n            inf_aver = adata_snrna_raw.varm['means_per_cluster_mu_fg'][[f'means_per_cluster_mu_fg_{i}'\r\n                                                                        for i in adata_snrna_raw.uns['mod'][\r\n                                                                            'factor_names']]].copy()\r\n        else:\r\n            inf_aver = adata_snrna_raw.var[[f'means_per_cluster_mu_fg_{i}'\r\n                                            for i in adata_snrna_raw.uns['mod']['factor_names']]].copy()\r\n        inf_aver.columns = adata_snrna_raw.uns['mod']['factor_names']\r\n        inf_aver.iloc[0:5, 0:5]\r\n\r\n        intersect = np.intersect1d(adata_vis.var_names, inf_aver.index)\r\n        adata_vis = adata_vis[:, intersect].copy()\r\n        inf_aver = inf_aver.loc[intersect, :].copy()\r\n\r\n        cell2location.models.Cell2location.setup_anndata(adata=adata_vis)\r\n        # scvi.data.view_anndata_setup(adata_vis)\r\n        # create and train the model\r\n        mod = cell2location.models.Cell2location(\r\n            adata_vis, cell_state_df=inf_aver,\r\n            # the expected average cell abundance: tissue-dependent\r\n            # hyper-prior which can be estimated from paired histology:\r\n            N_cells_per_location=30,\r\n            # hyperparameter controlling normalisation of\r\n            # within-experiment variation in RNA detection (using default here):\r\n            detection_alpha=200\r\n        )\r\n\r\n        mod.train(max_epochs=600,\r\n                  # train using full data (batch_size=None)\r\n                  batch_size=None,\r\n                  # use all data points in training because\r\n                  # we need to estimate cell abundance at all locations\r\n                  train_size=1,\r\n                  use_gpu=True)\r\n\r\n        # plot ELBO loss history during training, removing first 100 epochs from the plot\r\n        # mod.plot_history(1000)\r\n        # plt.legend(labels=['full data training'])\r\n\r\n        adata_vis = mod.export_posterior(\r\n            adata_vis, sample_kwargs={'num_samples': 200, 'batch_size': 200, 'use_gpu': True}\r\n        )\r\n        # print(adata_vis)\r\n        adata_vis.obsm['q05_cell_abundance_w_sf'].to_csv(output_file_path + '/Cell2location_result.csv')\r\n\r\n\r\n\r\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/main_code.py", "content": "\"\"\"读取数据\"\"\"\r\nimport copy\r\nimport multiprocessing as mp\r\nfrom Baselines.Spoint.model import init_model\r\nimport anndata as ad\r\nimport torch\r\nimport os\r\nimport Baselines.Spoint.data_utils\r\nsm_model='Result'\r\n\r\ndef main(a,b,cell_key):\r\n    for i in range(a, b):\r\n        print('第'+str(i)+'个切片')\r\n\r\n        sc_file = 'D:\\Python_Projects\\MACD_github\\Datasets\\Simulated_datasets\\dataset4\\scRNA.h5ad'\r\n        st_file = 'D:\\Python_Projects\\MACD_github\\Datasets\\Simulated_datasets\\dataset4\\Spatial.h5ad'\r\n        st_ad1 = ad.read_h5ad(st_file)\r\n        sc_data1 = ad.read_h5ad(sc_file)\r\n        st_ad = copy.deepcopy(st_ad1)\r\n        sc_data = copy.deepcopy(sc_data1)\r\n        output_path = 'Baselines/Spoint\\Result\\dataset' + str(i)\r\n        if not os.path.exists(output_path):\r\n            os.makedirs(output_path)\r\n        model = init_model(sc_ad=sc_data,\r\n                                   st_ad=st_ad,\r\n                                   celltype_key=cell_key,\r\n                                   n_top_markers=500,\r\n                                   n_top_hvg=3000)\r\n        # model.train\r\n        model.model_train(sm_lr=0.01,\r\n                    st_lr=0.01)\r\n\r\n        pre = model.deconv_spatial()\r\n\r\n        pre.to_csv(output_path + \"/proportion.csv\")\r\n\r\n\r\n# if __name__ == '__main__':\r\n#     main(4,5,'celltype_final')\r\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/model.py", "content": "from . import data_utils\nfrom . import base_model\nfrom . import data_downsample\nfrom . import data_augmentation\nfrom . import spatial_simulation\nimport numpy as np\nimport torch\nfrom scipy.sparse import csr_matrix\nimport numba\nimport logging\nimport random\n\ndef init_model(\n    sc_ad,\n    st_ad,\n    celltype_key,\n    sc_genes=None,\n    st_genes=None,\n    used_genes=None,\n    deg_method:str='wilcoxon',\n    n_top_markers:int=200,\n    n_top_hvg:int=None,\n    log2fc_min=0.5,\n    pval_cutoff=0.01,\n    pct_diff=None, \n    pct_min=0.1,\n    use_rep='scvi',\n    st_batch_key=None,\n    sm_size:int=10000,\n    cell_counts=None,\n    clusters_mean=None,\n    cells_mean=10,\n    cells_min=1,\n    cells_max=20,\n    cell_sample_counts=None,\n    cluster_sample_counts=None,\n    ncell_sample_list=None,\n    cluster_sample_list=None,\n    scvi_layers=2,\n    scvi_latent=128,\n    scvi_gene_likelihood='zinb',\n    scvi_dispersion='gene-batch',\n    latent_dims=128, \n    hidden_dims=512,\n    infer_losses=['kl','cos'],\n    n_threads=1,\n    seed=42,\n    use_gpu=None\n):\n    \"\"\"Initialize Spoint model.\n    \n    Given specific data and parameters to initialize Spoint model.\n\n    Args:\n        sc_ad: An AnnData object representing single cell reference.\n        st_ad: An AnnData object representing spatial transcriptomic data.\n        celltype_key: A string representing cell types annotation columns in obs of single cell reference.\n        sc_genes: A sequence of strings containing genes of single cell reference used in Spoint model. Only used when ``used_genes`` is None.\n        st_genes: A sequence of strings containing genes of spatial transcriptomic data used in Spoint model. Only used when ``used_genes`` is None.\n        used_genes: A sequence of strings containing genes used in Spoint model.\n        deg_method: A string passed to method parameter of scanpy.tl.rank_genes_groups.\n        n_top_markers: The number of differential expressed genes in each cell type of single cell reference used in Spoint model.\n        n_top_hvg: The number of highly variable genes of spatial transcriptomic data used in Spoint model.\n        log2fc_min: The threshold of log2 fold-change used for filtering differential expressed genes of single cell reference.\n        pval_cutoff: The threshold of p-value used for filtering differential expressed genes of single cell reference.\n        pct_min: The threshold of precentage of expressed cells used for filtering differential expressed genes of single cell reference.\n        st_batch_key: A column name in obs of spatial transcriptomic data representing batch groups of spatial transcriptomic data.\n        sm_size: The number of simulated spots.\n        hiddem_dims: The number of nodes of hidden layers in Spoint model.\n        n_threads: The number of cpu core used for parallel.\n    \n    Returns:\n        A ``SpointModel`` object.\n    \"\"\"\n    \n    print('Setting global seed:', seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    spatial_simulation.numba_set_seed(seed)\n    numba.set_num_threads(n_threads)\n\n    sc_ad = data_utils.normalize_adata(sc_ad,target_sum=1e4)\n    st_ad = data_utils.normalize_adata(st_ad,target_sum=1e4)\n    sc_ad, st_ad = data_utils.filter_model_genes(\n        sc_ad,\n        st_ad,\n        celltype_key=celltype_key,\n        deg_method=deg_method,\n        n_top_markers=n_top_markers,\n        n_top_hvg=n_top_hvg,\n        used_genes=used_genes,\n        sc_genes=sc_genes,\n        st_genes=st_genes,\n        log2fc_min=log2fc_min, \n        pval_cutoff=pval_cutoff, \n        pct_diff=pct_diff, \n        pct_min=pct_min\n    )\n    sm_ad = data_utils.generate_sm_adata(sc_ad,num_sample=sm_size,celltype_key=celltype_key,n_threads=n_threads,cell_counts=cell_counts,clusters_mean=clusters_mean,cells_mean=cells_mean,cells_min=cells_min,cells_max=cells_max,cell_sample_counts=cell_sample_counts,cluster_sample_counts=cluster_sample_counts,ncell_sample_list=ncell_sample_list,cluster_sample_list=cluster_sample_list)\n    data_utils.downsample_sm_spot_counts(sm_ad,st_ad,n_threads=n_threads)\n    model = base_model.SpointModel(\n        st_ad,\n        sm_ad,\n        clusters = np.array(sm_ad.obsm['label'].columns),\n        spot_names = np.array(st_ad.obs_names),\n        used_genes = np.array(st_ad.var_names),\n        use_rep=use_rep,\n        st_batch_key=st_batch_key,\n        scvi_layers=scvi_layers,\n        scvi_latent=scvi_latent,\n        scvi_gene_likelihood=scvi_gene_likelihood,\n        scvi_dispersion=scvi_dispersion,\n        latent_dims=latent_dims, \n        hidden_dims=hidden_dims,\n        infer_losses=infer_losses,\n        use_gpu=use_gpu,\n        seed=seed\n    )\n    return model"}
{"type": "source_file", "path": "MACD_github/Baselines/DestVi/main_code.py", "content": "\r\n\"\"\"https://github.com/QuKunLab/SpatialBenchmarking/blob/main/Codes/Deconvolution/DestVI_pipeline.py\"\"\"\r\nimport copy\r\n\r\nimport scanpy as sc\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport scvi\r\nfrom scvi.model import CondSCVI, DestVI\r\nimport anndata as ad\r\nimport sys\r\nimport os\r\ndef main(a,b,cell_key):\r\n    for i in range(a, b):\r\n        sc_file = 'Datasets/preproced_data\\dataset' + str(i) + '\\Scdata_filter.h5ad'\r\n        st_file = 'Datasets/preproced_data\\dataset' + str(i) + '\\Real_STdata_filter.h5ad'\r\n        st_adata1 = ad.read_h5ad(st_file)\r\n        sc_adata1 = ad.read_h5ad(sc_file)\r\n        sc_adata = copy.deepcopy(sc_adata1)\r\n        st_adata = copy.deepcopy(st_adata1)\r\n        outpath = 'Baselines/DestVi\\Result/' + 'dataset' + str(i)\r\n        if not os.path.exists(outpath):\r\n            os.makedirs(outpath)\r\n        intersect = np.intersect1d(sc_adata.var_names, st_adata.var_names)\r\n        st_adata = st_adata[:, intersect].copy()\r\n        sc_adata = sc_adata[:, intersect].copy()\r\n        # let us filter some genes\r\n        G = len(intersect)\r\n        sc.pp.filter_genes(sc_adata, min_counts=10)\r\n\r\n        sc_adata.layers[\"counts\"] = sc_adata.X.copy()\r\n\r\n        sc.pp.highly_variable_genes(\r\n            sc_adata,\r\n            n_top_genes=G,\r\n            subset=True,\r\n            layer=\"counts\",\r\n            flavor=\"seurat_v3\"\r\n        )\r\n\r\n        sc.pp.normalize_total(sc_adata, target_sum=10e4)\r\n        sc.pp.log1p(sc_adata)\r\n        sc_adata.raw = sc_adata\r\n        st_adata.layers[\"counts\"] = st_adata.X.copy()\r\n        sc.pp.normalize_total(st_adata, target_sum=10e4)\r\n        sc.pp.log1p(st_adata)\r\n        st_adata.raw = st_adata\r\n        # filter genes to be the same on the spatial data\r\n        intersect = np.intersect1d(sc_adata.var_names, st_adata.var_names)\r\n        st_adata = st_adata[:, intersect].copy()\r\n        sc_adata = sc_adata[:, intersect].copy()\r\n        print(st_adata)\r\n        CondSCVI.setup_anndata(sc_adata, layer=\"counts\", labels_key=cell_key)\r\n\r\n        sc_model = CondSCVI(sc_adata, weight_obs=True)\r\n        sc_model.train(max_epochs=250, lr=0.0001)\r\n        sc_model.history[\"elbo_train\"].plot()\r\n        # CondSCVI.setup_anndata(st_adata, layer=\"counts\")\r\n        # st_model = DestVI.from_rna_model(st_adata, sc_model)\r\n\r\n\r\n\r\n        if hasattr(st_adata.X, 'toarray'):\r\n            dense_matrix = st_adata.X.toarray()\r\n        else:\r\n            dense_matrix = st_adata.X\r\n\r\n        # 确保数据类型为浮点数\r\n        dense_matrix = dense_matrix.astype(np.float64)\r\n\r\n        # 检查是否有NaN值\r\n        row_sums = np.sum(dense_matrix, axis=1)\r\n\r\n        # 创建一个布尔索引，表示哪些行不全为零\r\n        non_zero_rows = row_sums != 0\r\n\r\n        # 使用布尔索引过滤掉全为零的行\r\n        stadata_filtered = st_adata[non_zero_rows, :].copy()\r\n        filtered_file = outpath+'\\Spatial.h5ad'\r\n        stadata_filtered.write_h5ad(filtered_file)\r\n        scvi.model.DestVI.setup_anndata(stadata_filtered)\r\n        st_model = DestVI.from_rna_model(stadata_filtered, sc_model)\r\n        st_model.train(max_epochs=1000)\r\n        st_model.history[\"elbo_train\"].plot()\r\n        st_model.get_proportions().to_csv(outpath + '/DestVI_result.csv')\r\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/base_model.py", "content": "from . import model\nfrom . import data_utils\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport scvi\nimport anndata\nfrom . import metrics\nimport matplotlib.pyplot as plt\nimport os\nimport tempfile\nfrom copy import deepcopy\nimport logging\nimport itertools\nfrom functools import partial\nfrom tqdm import tqdm\nfrom time import strftime, localtime\nfrom scipy.sparse import issparse\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, BatchSampler\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\nfrom torch.utils.data.dataloader import default_collate\n\ndef compute_kernel(x, y):\n    x_size = x.size(0)\n    y_size = y.size(0)\n    dim = x.size(1)\n\n    tiled_x = x.unsqueeze(1).expand(x_size, y_size, dim)\n    tiled_y = y.unsqueeze(0).expand(x_size, y_size, dim)\n\n    kernel = torch.exp(-torch.square(tiled_x - tiled_y).mean(dim=2) / dim)\n    return kernel\n\ndef compute_mmd(x, y):\n    x_kernel = compute_kernel(x, x)\n    y_kernel = compute_kernel(y, y)\n    xy_kernel = compute_kernel(x, y)\n\n    mmd = x_kernel.mean() + y_kernel.mean() - 2 * xy_kernel.mean()\n    return mmd\n\nclass PredictionModel(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        latent_dims,\n        hidden_dims,\n        celltype_dims,\n        dropout\n    ):\n        super(PredictionModel, self).__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_dims, hidden_dims),\n            nn.LeakyReLU(),\n            nn.LayerNorm(hidden_dims),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dims, latent_dims),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(celltype_dims, hidden_dims),\n            nn.LeakyReLU(),\n            nn.LayerNorm(hidden_dims),\n            nn.Linear(hidden_dims, hidden_dims),\n            nn.LeakyReLU(),\n            nn.LayerNorm(hidden_dims),\n            nn.Linear(hidden_dims, hidden_dims),\n            nn.LeakyReLU(),\n            nn.LayerNorm(hidden_dims),\n            nn.Linear(hidden_dims, input_dims)\n        )\n        self.pred = nn.Sequential(\n            nn.Linear(latent_dims, hidden_dims),\n            nn.LeakyReLU(),\n            nn.LayerNorm(hidden_dims),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dims, celltype_dims),\n            nn.Softmax(dim=1),\n        )\n                \n        nn.init.kaiming_normal_(self.encoder[0].weight)\n        nn.init.kaiming_normal_(self.encoder[4].weight)\n        nn.init.kaiming_normal_(self.decoder[0].weight)\n        nn.init.kaiming_normal_(self.decoder[3].weight)\n        nn.init.kaiming_normal_(self.decoder[6].weight)\n        nn.init.xavier_uniform_(self.decoder[-1].weight)\n        nn.init.kaiming_normal_(self.pred[0].weight)\n        nn.init.xavier_uniform_(self.pred[4].weight)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        pred = self.pred(z)\n        decoded = self.decoder(pred)\n        return z, pred, decoded\n\nclass SpointModel():\n    def __init__(\n        self, \n        st_ad, \n        sm_ad, \n        clusters, \n        used_genes, \n        spot_names, \n        use_rep,\n        st_batch_key=None,\n        scvi_layers=2,\n        scvi_latent=64,\n        scvi_gene_likelihood='zinb',\n        scvi_dispersion='gene-batch',\n        latent_dims=32, \n        hidden_dims=512,\n        infer_losses=['kl','cos'],\n        l1=0.01,\n        l2=0.01,\n        sm_lr=3e-4,\n        st_lr=3e-5,\n        use_gpu=None,\n        seed=42\n    ):\n        if ((use_gpu is None) or (use_gpu is True)) and (torch.cuda.is_available()):\n            self.device = 'cuda'\n        else:\n            self.device = 'cpu'\n        self.use_gpu = use_gpu\n        \n        self.st_ad = st_ad\n        self.sm_ad = sm_ad\n        self.scvi_dims=64\n        self.spot_names = spot_names\n        self.used_genes = used_genes\n        self.clusters = clusters\n        self.st_batch_key = st_batch_key\n        self.scvi_layers = scvi_layers\n        self.scvi_latent = scvi_latent\n        self.scvi_gene_likelihood = scvi_gene_likelihood\n        self.scvi_dispersion = scvi_dispersion\n        self.kl_infer_loss_func = partial(self.kl_divergence, dim=1)\n        self.kl_rec_loss_func = partial(self.kl_divergence, dim=1)\n        self.cosine_infer_loss_func = partial(F.cosine_similarity, dim=1)\n        self.cosine_rec_loss_func = partial(F.cosine_similarity, dim=1)\n        self.rmse_loss_func = self.rmse\n        self.infer_losses = infer_losses\n        self.mmd_loss = compute_mmd\n        self.l1 = l1\n        self.l2 = l2\n        self.use_rep = use_rep\n        if use_rep == 'scvi':\n            self.feature_dims = scvi_latent\n        elif use_rep == 'X':\n            self.feature_dims = st_ad.shape[1]\n        elif use_rep == 'pca':\n            self.feature_dims = 50\n        else:\n            raise ValueError('use_rep must be one of scvi, pca and X.')\n        self.latent_dims = latent_dims\n        self.hidden_dims = hidden_dims\n        self.sm_lr = sm_lr\n        self.st_lr = st_lr\n        self.init_model()\n        self.st_data = None\n        self.sm_data = None\n        self.sm_labels = None\n        self.best_path = None\n        self.history = pd.DataFrame(columns = ['sm_train_rec_loss','sm_train_infer_loss','sm_test_rec_loss','sm_test_infer_loss','st_train_rec_loss','st_test_rec_loss','st_train_mmd_loss','st_test_mmd_loss','is_best'])\n        self.batch_size = None\n        self.seed = seed\n\n    @staticmethod\n    def rmse(y_true, y_pred):\n        mse = F.mse_loss(y_pred, y_true)\n        rmse = torch.sqrt(mse)\n        return rmse\n        \n    @staticmethod\n    def kl_divergence(y_true, y_pred, dim=0):\n        y_pred = torch.clip(y_pred, torch.finfo(torch.float32).eps)\n        y_true = y_true.to(y_pred.dtype)\n        y_true = torch.nan_to_num(torch.div(y_true, y_true.sum(dim, keepdims=True)),0)\n        y_pred = torch.nan_to_num(torch.div(y_pred, y_pred.sum(dim, keepdims=True)),0)\n        y_true = torch.clip(y_true, torch.finfo(torch.float32).eps, 1)\n        y_pred = torch.clip(y_pred, torch.finfo(torch.float32).eps, 1)\n        return torch.mul(y_true, torch.log(torch.nan_to_num(torch.div(y_true, y_pred)))).mean(dim)\n    \n    def init_model(self):\n        self.model = PredictionModel(self.feature_dims,self.latent_dims,self.hidden_dims,len(self.clusters),0.8).to(self.device)\n        self.sm_optimizer = optim.Adam(list(self.model.encoder.parameters())+list(self.model.pred.parameters()),lr=self.sm_lr)\n        self.st_optimizer = optim.Adam(list(self.model.encoder.parameters())+list(self.model.decoder.parameters()),lr=self.st_lr)\n        \n    def get_scvi_latent(\n        self,\n        n_layers=None,\n        n_latent=None,\n        gene_likelihood=None,\n        dispersion=None,\n        max_epochs=100,\n        early_stopping=True,\n        batch_size=4096,\n    ):\n        if self.st_batch_key is not None:\n            if 'simulated' in self.st_ad.obs[self.st_batch_key]:\n                raise ValueError(f'obs[{self.st_batch_key}] cannot include \"real\".')\n            self.st_ad.obs[\"batch\"] = self.st_ad.obs[self.st_batch_key].astype(str)\n            self.sm_ad.obs[\"batch\"] = 'simulated'\n        else:\n            self.st_ad.obs[\"batch\"] = 'real'\n            self.sm_ad.obs[\"batch\"] = 'simulated'\n\n        adata = sc.concat([self.st_ad,self.sm_ad])\n        adata.layers[\"counts\"] = adata.X.copy()\n\n        scvi.model.SCVI.setup_anndata(\n            adata,\n            layer=\"counts\",\n            batch_key=\"batch\"\n        )\n        if n_layers is None:\n            n_layers = self.scvi_layers\n        if n_latent is None:\n            n_latent = self.scvi_latent\n        if gene_likelihood is None:\n            gene_likelihood = self.scvi_gene_likelihood\n        if dispersion is None:\n            dispersion = self.scvi_dispersion\n        vae = scvi.model.SCVI(adata, n_layers=n_layers, n_latent=n_latent, gene_likelihood=gene_likelihood,dispersion=dispersion)\n        vae.train(max_epochs=max_epochs,early_stopping=early_stopping,batch_size=batch_size,use_gpu=self.use_gpu)\n        adata.obsm[\"X_scVI\"] = vae.get_latent_representation()\n\n        st_scvi_ad = anndata.AnnData(adata[adata.obs['batch'] != 'simulated'].obsm[\"X_scVI\"])\n        sm_scvi_ad = anndata.AnnData(adata[adata.obs['batch'] == 'simulated'].obsm[\"X_scVI\"])\n\n        st_scvi_ad.obs = self.st_ad.obs\n        st_scvi_ad.obsm = self.st_ad.obsm\n\n        sm_scvi_ad.obs = self.sm_ad.obs\n        sm_scvi_ad.obsm = self.sm_ad.obsm\n        \n        sm_scvi_ad = data_utils.check_data_type(sm_scvi_ad)\n        st_scvi_ad = data_utils.check_data_type(st_scvi_ad)\n\n        self.sm_data = sm_scvi_ad.X\n        self.sm_labels = sm_scvi_ad.obsm['label'].values\n        self.st_data = st_scvi_ad.X\n        \n        return sm_scvi_ad,st_scvi_ad\n\n    \n    def build_dataset(self, batch_size, device=None):\n        if device is None:\n            device = self.device\n        x_train,y_train,x_test,y_test = data_utils.split_shuffle_data(np.array(self.sm_data,dtype=np.float32),np.array(self.sm_labels,dtype=np.float32))\n        \n        x_train = torch.tensor(x_train).to(device)\n        y_train = torch.tensor(y_train).to(device)\n        x_test = torch.tensor(x_test).to(device)\n        y_test = torch.tensor(y_test).to(device)\n        st_data = torch.tensor(self.st_data).to(device)\n        \n        self.sm_train_ds = TensorDataset(x_train, y_train)\n        self.sm_test_ds = TensorDataset(x_test,y_test)\n        self.st_ds = TensorDataset(st_data)\n        \n        self.sm_train_batch_size = min(len(self.sm_train_ds), batch_size)\n        self.sm_test_batch_size = min(len(self.sm_test_ds), batch_size)\n        self.st_batch_size = min(len(self.st_ds), batch_size)\n        \n        g = torch.Generator()\n        g.manual_seed(self.seed)\n        self.sm_train_sampler = BatchSampler(RandomSampler(self.sm_train_ds, generator=g), batch_size=self.sm_train_batch_size, drop_last=True)\n        self.sm_test_sampler = BatchSampler(RandomSampler(self.sm_test_ds, generator=g), batch_size=self.sm_test_batch_size, drop_last=True)\n        self.st_sampler = BatchSampler(RandomSampler(self.st_ds, generator=g), batch_size=self.st_batch_size, drop_last=True)\n    \n    def train_st(self, sm_data, st_data, rec_w=1, m_w=1):\n        self.model.train()\n        self.st_optimizer.zero_grad()\n        sm_latent, sm_predictions, sm_rec_data = self.model(sm_data)\n        st_latent, _, st_rec_data = self.model(st_data)\n        sm_rec_loss = self.kl_rec_loss_func(sm_data, sm_rec_data).mean() - self.cosine_rec_loss_func(sm_data, sm_rec_data).mean()\n        st_rec_loss = self.kl_rec_loss_func(st_data, st_rec_data).mean() - self.cosine_rec_loss_func(st_data, st_rec_data).mean()\n        mmd_loss = self.mmd_loss(sm_latent, st_latent)\n        loss = rec_w*sm_rec_loss + rec_w*st_rec_loss + m_w*mmd_loss\n        loss.backward()\n        self.st_optimizer.step()\n        return loss, sm_rec_loss, st_rec_loss, mmd_loss\n\n    def train_sm(self, sm_data, sm_labels, infer_w=1):\n        self.model.train()\n        self.sm_optimizer.zero_grad()\n        sm_latent, sm_predictions, sm_rec_data = self.model(sm_data)\n        infer_loss = 0\n        for loss in self.infer_losses:\n            if loss == 'kl':\n                infer_loss += self.kl_infer_loss_func(sm_labels, sm_predictions).mean()\n            elif loss == 'cos':\n                infer_loss -= self.cosine_infer_loss_func(sm_labels, sm_predictions).mean()\n            elif loss == 'rmse':\n                infer_loss += self.rmse_loss_func(sm_labels, sm_predictions)\n        loss = infer_w*infer_loss\n        loss.backward()\n        self.sm_optimizer.step()\n        return loss, infer_loss\n        \n    def test_st(self, sm_data, st_data, rec_w=1, m_w=1):\n        self.model.eval()\n        sm_latent, sm_predictions, sm_rec_data = self.model(sm_data)\n        st_latent, _, st_rec_data = self.model(st_data)\n        sm_rec_loss = self.kl_rec_loss_func(sm_data, sm_rec_data).mean() - self.cosine_rec_loss_func(sm_data, sm_rec_data).mean()\n        st_rec_loss = self.kl_rec_loss_func(st_data, st_rec_data).mean() - self.cosine_rec_loss_func(st_data, st_rec_data).mean()\n        mmd_loss = self.mmd_loss(sm_latent, st_latent)\n        loss = rec_w*sm_rec_loss + rec_w*st_rec_loss + m_w*mmd_loss\n        return loss, sm_rec_loss, st_rec_loss, mmd_loss\n        \n    def test_sm(self, sm_data, sm_labels, infer_w=1):\n        self.model.eval()\n        sm_latent, sm_predictions, sm_rec_data = self.model(sm_data)\n        infer_loss = 0\n        for loss in self.infer_losses:\n            if loss == 'kl':\n                infer_loss += self.kl_infer_loss_func(sm_labels, sm_predictions).mean()\n            elif loss == 'cos':\n                infer_loss -= self.cosine_infer_loss_func(sm_labels, sm_predictions).mean()\n            elif loss == 'rmse':\n                infer_loss += self.rmse_loss_func(sm_labels, sm_predictions)\n        loss = infer_w*infer_loss\n        return loss, infer_loss\n    \n    def train_model_by_step(\n        self,\n        max_steps=5000,\n        save_mode='all',\n        save_path=None,\n        prefix=None,\n        sm_step=10,\n        st_step=10,\n        test_step_gap=1,\n        convergence=0.001,\n        early_stop=True,\n        early_stop_max=2000,\n        sm_lr=None,\n        st_lr=None,\n        rec_w=1, \n        infer_w=1,\n        m_w=1,\n    ):\n        if len(self.history) > 0:\n            best_ind = np.where(self.history['is_best'] == 'True')[0][-1]\n            best_loss = self.history['sm_test_infer_loss'][best_ind]\n            best_rec_loss = self.history['st_test_rec_loss'][best_ind]\n        else:\n            best_loss = np.inf\n            best_rec_loss = np.inf\n        early_stop_count = 0\n        if sm_lr is not None:\n            for g in self.sm_optimizer.param_groups:\n                g['lr'] = sm_lr\n        if st_lr is not None:\n            for g in self.st_optimizer.param_groups:\n                g['lr'] = st_lr\n\n        pbar = tqdm(range(max_steps))\n        sm_trainr_iter = itertools.cycle(self.sm_train_sampler)\n        sm_test_iter = itertools.cycle(self.sm_test_sampler)\n        st_iter = itertools.cycle(self.st_sampler)\n        sm_train_shuffle_step = max(int(len(self.sm_train_ds)/(self.sm_train_batch_size*sm_step)),1)\n        sm_test_shuffle_step = max(int(len(self.sm_test_ds)/(self.sm_test_batch_size*sm_step)),1)\n        st_shuffle_step = max(int(len(self.st_ds)/(self.st_batch_size*st_step)),1)\n        for step in pbar:\n            if step % sm_train_shuffle_step == 0:\n                sm_train_iter = itertools.cycle(self.sm_train_sampler)\n            if step % sm_test_shuffle_step == 0:\n                sm_test_iter = itertools.cycle(self.sm_test_sampler)\n            if step % st_shuffle_step == 0:\n                st_iter = itertools.cycle(self.st_sampler)\n\n            st_exp = self.st_ds[next(st_iter)][0]\n            sm_exp, sm_proportion = self.sm_train_ds[next(sm_train_iter)]\n            for i in range(st_step):\n                st_train_total_loss, sm_train_rec_loss, st_train_rec_loss, st_train_mmd_loss = self.train_st(sm_exp, st_exp, rec_w=rec_w, m_w=m_w)\n            for i in range(sm_step):\n                sm_train_total_loss, sm_train_infer_loss = self.train_sm(sm_exp, sm_proportion, infer_w=infer_w)\n            \n            if step % test_step_gap == 0:\n                sm_test_exp, sm_test_proportion = self.sm_test_ds[next(sm_test_iter)]\n                st_test_total_loss, sm_test_rec_loss, st_test_rec_loss, st_test_mmd_loss = self.test_st(sm_test_exp, st_exp, rec_w=rec_w, m_w=m_w)\n                sm_test_total_loss, sm_test_infer_loss = self.test_sm(sm_test_exp, sm_test_proportion, infer_w=infer_w)\n                \n                current_infer_loss = sm_test_infer_loss.item()\n\n                best_flag='False'\n                if best_loss - current_infer_loss > convergence:\n                    if best_loss > current_infer_loss:\n                        best_loss = current_infer_loss\n                    best_flag='True'\n                    # print('### Update best model')\n                    early_stop_count = 0\n                    old_best_path = self.best_path\n                    if prefix is not None:\n                        self.best_path = os.path.join(save_path,prefix+'_'+f'celleagle_weights_step{step}.h5')\n                    else:\n                        self.best_path = os.path.join(save_path,f'celleagle_weights_step{step}.h5')\n                    if save_mode == 'best':\n                        if old_best_path is not None:\n                            if os.path.exists(old_best_path):\n                                os.remove(old_best_path)\n                        torch.save(self.model.state_dict(), self.best_path)\n                else:\n                    early_stop_count += 1\n                    \n                if save_mode == 'all':\n                    if prefix is not None:\n                        self.best_path = os.path.join(save_path,prefix+'_'+f'celleagle_weights_step{step}.h5')\n                    else:\n                        self.best_path = os.path.join(save_path,f'celleagle_weights_step{step}.h5')\n                    torch.save(self.model.state_dict(), self.best_path)\n                \n                self.history = pd.concat([\n                    self.history,\n                    pd.DataFrame({\n                        'sm_train_infer_loss':sm_train_infer_loss.item(),\n                        'sm_train_rec_loss':sm_train_infer_loss.item(),\n                        'sm_test_rec_loss':sm_test_rec_loss.item(),\n                        'sm_test_infer_loss':sm_test_infer_loss.item(),\n                        'st_train_rec_loss':st_train_rec_loss.item(),\n                        'st_test_rec_loss':st_test_rec_loss.item(),\n                        'st_train_mmd_loss':st_train_rec_loss.item(),\n                        'st_test_mmd_loss':st_test_rec_loss.item(),\n                        'is_best':best_flag\n                    },index=[0])\n                ]).reset_index(drop=True)\n\n                pbar.set_description(f\"Step {step + 1}: Test inference loss={sm_test_infer_loss.item():.3f}\",refresh=True)\n                \n                if (early_stop_count > early_stop_max) and early_stop:\n                    print('Stop trainning because of loss convergence')\n                    break\n    \n    def train_model(\n        self,\n        max_steps=5000,\n        save_mode='all',\n        save_path=None,\n        prefix=None,\n        sm_step=10,\n        st_step=10,\n        test_step_gap=1,\n        convergence=0.001,\n        early_stop=False,\n        early_stop_max=2000,\n        sm_lr=None,\n        st_lr=None,\n        batch_size=1024,\n        rec_w=1, \n        infer_w=1,\n        m_w=1,\n    ):\n        \"\"\"Training Spoint model.\n        \n        Training Spoint model.\n\n        Args:\n            max_steps: The max step of training. The training process will be stop when achive max step.\n            save_mode: A string determinates how the model is saved. It must be one of 'best' and 'all'.\n            save_path: A string representing the path directory where the model is saved.\n            prefix: A string added to the prefix of file name of saved model.\n            convergence: The threshold of early stop.\n            early_stop: If True, turn on early stop.\n            early_stop_max: The max steps of loss difference less than convergence.\n            sm_lr: Learning rate for simulated data.\n            st_lr: Learning rate for spatial transcriptomic data.\n            disc_lr: Learning rate of discriminator.\n            batch_size: Batch size of the data be feeded in model once.\n            rec_w: The weight of reconstruction loss.\n            infer_w: The weig ht of inference loss.\n            m_w: The weight of MMD loss.\n        \n        Returns:\n            ``None``\n        \"\"\"\n        self.init_model()\n        self.train_model_by_step(\n            max_steps=max_steps,\n            save_mode=save_mode,\n            save_path=save_path,\n            prefix=prefix,\n            sm_step=sm_step,\n            st_step=st_step,\n            test_step_gap=test_step_gap,\n            convergence=convergence,\n            early_stop=early_stop,\n            early_stop_max=early_stop_max,\n            sm_lr=sm_lr,\n            st_lr=st_lr,\n            rec_w=rec_w, \n            infer_w=infer_w,\n            m_w=m_w\n        )\n                        \n    def model_train(\n        self,\n        max_steps=5000,\n        save_mode='all',\n        save_path=None,\n        prefix=None,\n        sm_step=10,\n        st_step=10,\n        test_step_gap=1,\n        convergence=0.001,\n        early_stop=False,\n        early_stop_max=2000,\n        sm_lr=None,\n        st_lr=None,\n        batch_size=1024,\n        rec_w=1, \n        infer_w=1,\n        m_w=1,\n        scvi_max_epochs=100,\n        scvi_early_stopping=True,\n        scvi_batch_size=4096,\n    ):\n        \"\"\"Training Spoint model.\n        \n        Obtain latent feature from scVI then feed in Spoint model for training.\n\n        Args:\n            max_steps: The max step of training. The training process will be stop when achive max step.\n            save_mode: A string determinates how the model is saved. It must be one of 'best' and 'all'.\n            save_path: A string representing the path directory where the model is saved.\n            prefix: A string added to the prefix of file name of saved model.\n            convergence: The threshold of early stop.\n            early_stop: If True, turn on early stop.\n            early_stop_max: The max steps of loss difference less than convergence.\n            sm_lr: Learning rate for simulated data.\n            st_lr: Learning rate for spatial transcriptomic data.\n            batch_size: Batch size of the data be feeded in model once.\n            rec_w: The weight of reconstruction loss.\n            infer_w: The weig ht of inference loss.\n            m_w: The weight of MMD loss.\n            scvi_max_epochs: The max epoch of scVI.\n            scvi_batch_size: The batch size of scVI.\n        Returns:\n            ``None``\n        \"\"\"\n        if save_path is None:\n            save_path = os.path.join(tempfile.gettempdir() ,'Spoint_models_'+strftime(\"%Y%m%d%H%M%S\",localtime()))\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n        self.get_scvi_latent(max_epochs=scvi_max_epochs, early_stopping=scvi_early_stopping, batch_size=scvi_batch_size)\n        self.build_dataset(batch_size)\n        self.train_model(\n            max_steps=max_steps,\n            save_mode=save_mode,\n            save_path=save_path,\n            prefix=prefix,\n            sm_step=sm_step,\n            st_step=st_step,\n            test_step_gap=test_step_gap,\n            convergence=convergence,\n            early_stop=early_stop,\n            early_stop_max=early_stop_max,\n            sm_lr=sm_lr,\n            st_lr=st_lr,\n            rec_w=rec_w, \n            infer_w=infer_w,\n            m_w=m_w\n        )\n    \n    def eval_model(self,model_path=None,use_best_model=True,batch_size=4096,metric='pcc'):\n        if metric=='pcc':\n            metric_name = 'PCC'\n            func = metrics.pcc\n        if metric=='spcc':\n            metric_name = 'SPCC'\n            func = metrics.spcc\n        if metric=='mae':\n            metric_name = 'MAE'\n            func = metrics.mae\n        if metric=='js':\n            metric_name = 'JS'\n            func = metrics.js\n        if metric=='rmse':\n            metric_name = 'RMSE'\n            func = metrics.rmse\n        if metric=='ssim':\n            metric_name = 'SSIM'\n            func = metrics.ssim\n        \n        if model_path is not None:\n            self.model.load_state_dict(torch.load(model_path))\n        elif use_best_model:\n            self.model.load_state_dict(torch.load(self.best_path))\n        model.eval()\n        pre = []\n        prop = []\n        for exp_batch, prop_batch in self.sm_test_dataloader:\n            latent_tmp, pre_tmp, _ = self.model(exp_batch)\n            pre.extend(pre_tmp.cpu().detach().numpy())\n            prop.extend(prop_batch.cpu().detach().numpy())\n        pre = np.array(pre)\n        prop = np.array(prop)\n        metric_list = []\n        for i,c in enumerate(self.clusters):\n            metric_list.append(func(pre[:,i],prop[:,i]))\n        print('### Evaluate model with simulation data')\n        for i in range(len(metric_list)):\n            print(f'{metric_name} of {self.clusters[i]}, {metric_list[i]}')\n            \n    def plot_training_history(self,save=None,return_fig=False,show=True,dpi=300):\n        if len(self.history) > 0:\n            fig, ax = plt.subplots()\n            plt.plot(np.arange(len(self.history)), self.history['sm_test_infer_loss'], label='sm_test_infer_loss')\n            plt.plot(np.arange(len(self.history)), self.history['st_test_rec_loss'], label='st_test_rec_loss')\n            plt.xlabel('Epochs')\n            plt.ylabel('Losses')\n            plt.title('Training history')\n            plt.legend()\n            if save is not None:\n                plt.savefig(save,bbox_inches='tight',dpi=dpi)\n            if show:\n                plt.show()\n            plt.close()\n            if return_fig:\n                return fig\n        else:\n            print('History is empty, training model first')\n            \n\n    def deconv_spatial(self,st_data=None,min_prop=0.01,model_path=None,use_best_model=True,add_obs=True,add_uns=True):\n        \"\"\"Deconvolute spatial transcriptomic data.\n        \n        Using well-trained Spoint model to predict the cell type porportion of spots in spatial transcriptomic data.\n        \n        Args:\n            st_data: An AnnData object of spatial transcriptomic data to be deconvolute.\n            min_prop: A threshold value below which the predicted value will be set to 0. \n            model_path: A string representing the path of saved model file.\n            use_best_model: If True, the model with the least loss will be used, otherwise, the last trained model will be used.\n            add_obs: If True, the predicted results will be writen to the obs of input AnnData object of spatial transcriptomic data.\n            add_uns: If True, the name of predicted cell types will be writen to the uns of input AnnData object of spatial transcriptomic data\n        \n        Returns:\n            A ``DataFrame`` contained deconvoluted results. Each row representing a spot, and each column representing a cell type.\n        \"\"\"\n        if st_data is None:\n            st_data = self.st_data\n        st_data = torch.tensor(st_data).to(self.device)\n        if model_path is not None:\n            self.model.load_state_dict(torch.load(model_path))\n        elif use_best_model:\n            self.model.load_state_dict(torch.load(self.best_path))\n        self.model.to(self.device)\n        self.model.eval()\n        latent, pre, _ = self.model(st_data)\n        pre = pre.cpu().detach().numpy()\n        pre[pre < min_prop] = 0\n        pre = pd.DataFrame(pre,columns=self.clusters,index=self.st_ad.obs_names)\n        self.st_ad.obs[pre.columns] = pre.values\n        self.st_ad.uns['celltypes'] = list(pre.columns)\n        return pre"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/data_downsample.py", "content": "import numba\nimport numpy as np\nimport multiprocessing as mp\n\nimport random\n\n# Cite from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=True)\ndef get_bin_edges(a, bins):\n    bin_edges = np.zeros((bins+1,), dtype=np.float32)\n    a_min = a.min()\n    a_max = a.max()\n    delta = (a_max - a_min) / bins\n    for i in numba.prange(bin_edges.shape[0]):\n        bin_edges[i] = a_min + i * delta\n\n    bin_edges[-1] = a_max  # Avoid roundoff error on last point\n    return bin_edges\n\n# Modified from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=False)\ndef compute_bin(x, bin_edges):\n    # assuming uniform bins for now\n    n = bin_edges.shape[0] - 1\n    a_max = bin_edges[-1]\n    # special case to mirror NumPy behavior for last bin\n    if x == a_max:\n        return n - 1 # a_max always in last bin\n    bin = np.searchsorted(bin_edges, x)-1\n    if bin < 0 or bin >= n:\n        return None\n    else:\n        return bin\n\n# Modified from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=False)\ndef numba_histogram(a, bin_edges):\n    hist = np.zeros((bin_edges.shape[0] - 1,), dtype=np.intp)\n    for x in a.flat:\n        bin = compute_bin(x, bin_edges)\n        if bin is not None:\n            hist[int(bin)] += 1\n    return hist, bin_edges\n\n\n# Modified from https://rdrr.io/bioc/scRecover/src/R/countsSampling.R\n# Downsample cell reads to a fraction\n@numba.jit(nopython=True, parallel=True)\ndef downsample_cell(cell_counts,fraction):\n    n = np.floor(np.sum(cell_counts) * fraction)\n    readsGet = np.sort(np.random.choice(np.arange(np.sum(cell_counts)), np.intp(n), replace=False))\n    cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n    counts_new = numba_histogram(readsGet,cumCounts)[0]\n    counts_new = counts_new.astype(np.float32)\n    return counts_new\n\ndef downsample_cell_python(cell_counts,fraction):\n    n = np.floor(np.sum(cell_counts) * fraction)\n    readsGet = np.sort(random.sample(range(np.intp(np.sum(cell_counts))), np.intp(n)))\n    cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n    counts_new = numba_histogram(readsGet,cumCounts)[0]\n    counts_new = counts_new.astype(np.float32)\n    return counts_new\n\n@numba.jit(nopython=True, parallel=True)\ndef downsample_per_cell(cell_counts,new_cell_counts):\n    n = new_cell_counts\n    if n < np.sum(cell_counts):\n        readsGet = np.sort(np.random.choice(np.arange(np.sum(cell_counts)), np.intp(n), replace=False))\n        cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n        counts_new = numba_histogram(readsGet,cumCounts)[0]\n        counts_new = counts_new.astype(np.float32)\n        return counts_new\n    else:\n        return cell_counts.astype(np.float32)\n\ndef downsample_per_cell_python(param):\n    cell_counts,new_cell_counts = param[0],param[1]\n    n = new_cell_counts\n    if n < np.sum(cell_counts):\n        readsGet = np.sort(random.sample(range(np.intp(np.sum(cell_counts))), np.intp(n)))\n        cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n        counts_new = numba_histogram(readsGet,cumCounts)[0]\n        counts_new = counts_new.astype(np.float32)\n        return counts_new\n    else:\n        return cell_counts.astype(np.float32)\n\ndef downsample_matrix_by_cell(matrix,per_cell_counts,n_cpus=None,numba_end=True):\n    if numba_end:\n        downsample_func = downsample_per_cell\n    else:\n        downsample_func = downsample_per_cell_python\n    if n_cpus is not None:\n        with mp.Pool(n_cpus) as p:\n            matrix_ds = p.map(downsample_func, zip(matrix,per_cell_counts))\n    else:\n        matrix_ds = [downsample_func(c,per_cell_counts[i]) for i,c in enumerate(matrix)]\n    return np.array(matrix_ds)\n\n# ps. slow speed.\ndef downsample_matrix_total(matrix,fraction):\n    matrix_flat = matrix.reshape(-1)\n    matrix_flat_ds = downsample_cell(matrix_flat,fraction)\n    matrix_ds = matrix_flat_ds.reshape(matrix.shape)\n    return matrix_ds\n\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/data_utils.py", "content": "import pandas as pd\nimport numpy as np\nimport scanpy as sc\nimport anndata\nfrom scipy.sparse import issparse,csr_matrix\nfrom sklearn.preprocessing import normalize\nfrom . import data_downsample\n\nfrom . import spatial_simulation\n\n\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects as robjects\n\ndef normalize_adata(ad,target_sum=None):\n    ad_norm = sc.pp.normalize_total(ad,inplace=False,target_sum=1e4)\n    ad_norm  = sc.pp.log1p(ad_norm['X'])\n    # ad_norm  = sc.pp.scale(ad_norm)\n    # ad_norm = normalize(ad_norm,axis=1)\n    ad.layers['norm'] = ad_norm\n    return ad\n\ndef normalize_mtx(mtx,target_sum):\n    mtx = mtx[mtx.sum(1)!=0,:]\n    mtx = np.nan_to_num(np.log1p((mtx.T*target_sum/mtx.sum(axis=1)).T))\n    mtx = (mtx-mtx.min(axis=1,keepdims=True))/(mtx.max(axis=1,keepdims=True)-mtx.min(axis=1,keepdims=True))\n    mtx = normalize(mtx,axis=1)\n    return mtx\n    \n# 计算单细胞亚群差异基因\ndef find_sc_markers(sc_ad, celltype_key, layer='norm', deg_method=None, log2fc_min=0.5, pval_cutoff=0.01, n_top_markers=200, pct_diff=None, pct_min=0.1):\n    print('### Finding marker genes...')\n    \n    # filter celltype contain only one sample.\n    filtered_celltypes = list(sc_ad.obs[celltype_key].value_counts()[(sc_ad.obs[celltype_key].value_counts() == 1).values].index)\n    if len(filtered_celltypes) > 0:\n        sc_ad = sc_ad[sc_ad.obs[~(sc_ad.obs[celltype_key].isin(filtered_celltypes))].index,:].copy()\n        print(f'### Filter cluster contain only one sample: {filtered_celltypes}')\n\n    sc.tl.rank_genes_groups(sc_ad, groupby=celltype_key, pts=True, layer=layer, use_raw=False, method=deg_method)\n    marker_genes_dfs = []\n    for c in np.unique(sc_ad.obs[celltype_key]):\n        tmp_marker_gene_df = sc.get.rank_genes_groups_df(sc_ad, group=c, pval_cutoff=pval_cutoff, log2fc_min=log2fc_min)\n        if (tmp_marker_gene_df.empty is not True):\n            tmp_marker_gene_df.index = tmp_marker_gene_df.names\n            tmp_marker_gene_df.loc[:,celltype_key] = c\n            if pct_diff is not None:\n                pct_diff_genes = sc_ad.var_names[np.where((sc_ad.uns['rank_genes_groups']['pts'][c]-sc_ad.uns['rank_genes_groups']['pts_rest'][c]) > pct_diff)]\n                tmp_marker_gene_df = tmp_marker_gene_df.loc[np.intersect1d(pct_diff_genes, tmp_marker_gene_df.index),:]\n            if pct_min is not None:\n                # pct_min_genes = sc_ad.var_names[np.where((sc_ad.uns['rank_genes_groups']['pts'][c]) > pct_min)]\n                tmp_marker_gene_df = tmp_marker_gene_df[tmp_marker_gene_df['pct_nz_group'] > pct_min]\n            if n_top_markers is not None:\n                tmp_marker_gene_df = tmp_marker_gene_df.sort_values('logfoldchanges',ascending=False)\n                tmp_marker_gene_df = tmp_marker_gene_df.iloc[:n_top_markers,:]\n            marker_genes_dfs.append(tmp_marker_gene_df)\n    marker_gene_df = pd.concat(marker_genes_dfs,axis=0)\n    print(marker_gene_df[celltype_key].value_counts())\n    all_marker_genes = np.unique(marker_gene_df.names)\n    return all_marker_genes\n\n# 计算空间HVG\ndef find_st_hvg(st_ad,n_top_hvg=None):\n    print('### Finding HVG in spatial...')\n    sc.pp.highly_variable_genes(st_ad,n_top_genes=n_top_hvg,flavor='seurat_v3')\n    return st_ad.var_names[st_ad.var['highly_variable'] == True]\n\n# \ndef filter_model_genes(\n    sc_ad, \n    st_ad, \n    celltype_key=None, \n    used_genes=None,\n    sc_genes=None,\n    st_genes=None,\n    layer='norm',\n    deg_method=None,\n    log2fc_min=0.5, \n    pval_cutoff=0.01, \n    n_top_markers:int=200, \n    n_top_hvg=None,\n    pct_diff=None, \n    pct_min=0.1,\n):\n    overlaped_genes = np.intersect1d(sc_ad.var_names,st_ad.var_names)\n    sc_ad = sc_ad[:,overlaped_genes].copy()\n    st_ad = st_ad[:,overlaped_genes].copy()\n    if used_genes is None:\n        if st_genes is None:\n            if n_top_hvg is None:\n                st_genes = st_ad.var_names\n            else:\n                st_genes = find_st_hvg(st_ad, n_top_hvg)\n        if sc_genes is None:\n            sc_ad = sc_ad[:, st_genes].copy()\n            sc_genes = find_sc_markers(sc_ad, celltype_key, layer, deg_method, log2fc_min, pval_cutoff, n_top_markers, pct_diff, pct_min)\n        used_genes = np.intersect1d(sc_genes,st_genes)\n    sc_ad = sc_ad[:,used_genes].copy()\n    st_ad = st_ad[:,used_genes].copy()\n    sc.pp.filter_cells(sc_ad,min_genes=1)\n    sc.pp.filter_cells(st_ad,min_genes=1)\n    print(f'### Used gene numbers: {len(used_genes)}')\n    return sc_ad, st_ad\n\ndef check_data_type(ad):\n    if issparse(ad.X):\n        ad.X = ad.X.toarray()\n    if ad.X.dtype != np.float32:\n        ad.X =ad.X.astype(np.float32)\n    return ad\n\ndef generate_sm_adata(sc_ad,num_sample,celltype_key,n_threads,cell_counts,clusters_mean,cells_mean,cells_min,cells_max,cell_sample_counts,cluster_sample_counts,ncell_sample_list,cluster_sample_list):\n    sm_data,sm_labels = spatial_simulation.generate_simulation_data(sc_ad,num_sample=num_sample,celltype_key=celltype_key,downsample_fraction=None,\n    data_augmentation=False,n_cpus=n_threads,cell_counts=cell_counts,clusters_mean=clusters_mean,cells_mean=cells_mean,cells_min=cells_min,cells_max=cells_max,cell_sample_counts=cell_sample_counts,cluster_sample_counts=cluster_sample_counts,ncell_sample_list=ncell_sample_list,cluster_sample_list=cluster_sample_list)\n    sm_data_mtx = csr_matrix(sm_data)\n    sm_ad = anndata.AnnData(sm_data_mtx)\n    sm_ad.var.index = sc_ad.var_names\n    sm_labels = (sm_labels.T/sm_labels.sum(axis=1)).T\n    sm_ad.obsm['label'] = pd.DataFrame(sm_labels,columns=np.array(sc_ad.obs[celltype_key].value_counts().index.values),index=sm_ad.obs_names)\n    return sm_ad\n\ndef downsample_sm_spot_counts(sm_ad,st_ad,n_threads):\n    fitdistrplus = importr('fitdistrplus')\n    lib_sizes = robjects.FloatVector(np.array(st_ad.X.sum(1)).reshape(-1))\n    res = fitdistrplus.fitdist(lib_sizes,'lnorm')\n    loc = res[0][0]\n    scale = res[0][1]\n\n    sm_mtx_count = sm_ad.X.toarray()\n    sample_cell_counts = np.random.lognormal(loc,scale,sm_ad.shape[0])\n    sm_mtx_count_lb = data_downsample.downsample_matrix_by_cell(sm_mtx_count,sample_cell_counts.astype(np.int64), n_cpus=n_threads, numba_end=False)\n    print(\"111\")\n    sm_ad.X = csr_matrix(sm_mtx_count_lb)\n    \ndef split_shuffle_data(X,Y,shuffle=True,proportion=0.8):\n    if shuffle:\n        reind = np.random.permutation(len(X))\n        X = X[reind]\n        Y = Y[reind]\n    X_train = X[:int(len(X)*proportion)]\n    Y_train = Y[:int(len(Y)*proportion)]\n    X_test = X[int(len(X)*proportion):]\n    Y_test = Y[int(len(Y)*proportion):]\n    return X_train,Y_train,X_test,Y_test\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/__init__.py", "content": "from .data_downsample import *\nfrom .model import *\nfrom .data_utils import *\nfrom .spatial_simulation import *\nfrom .base_model import *\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/metrics.py", "content": "from scipy.stats import pearsonr, entropy, spearmanr\nfrom scipy.spatial.distance import jensenshannon\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef pcc(x1,x2):\n    return pearsonr(x1,x2)[0]\n\ndef spcc(x1,x2):\n    return spearmanr(x1,x2)[0]\n\ndef rmse(x1,x2):\n    return mean_squared_error(x1,x2,squared=False)\n\ndef mae(x1,x2):\n    return np.mean(np.abs(x1-x2))\n\ndef js(x1,x2):\n    return jensenshannon(x1,x2)\n\ndef kl(x1,x2):\n    entropy(x1, x2)\n\ndef ssim(im1,im2,M=1):\n    im1, im2 = im1/im1.max(), im2/im2.max()\n    mu1 = im1.mean()\n    mu2 = im2.mean()\n    sigma1 = np.sqrt(((im1 - mu1) ** 2).mean())\n    sigma2 = np.sqrt(((im2 - mu2) ** 2).mean())\n    sigma12 = ((im1 - mu1) * (im2 - mu2)).mean()\n    k1, k2, L = 0.01, 0.03, M\n    C1 = (k1*L) ** 2\n    C2 = (k2*L) ** 2\n    C3 = C2/2\n    l12 = (2*mu1*mu2 + C1)/(mu1 ** 2 + mu2 ** 2 + C1)\n    c12 = (2*sigma1*sigma2 + C2)/(sigma1 ** 2 + sigma2 ** 2 + C2)\n    s12 = (sigma12 + C3)/(sigma1*sigma2 + C3)\n    ssim = l12 * c12 * s12\n    return ssim"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/data_augmentation.py", "content": "import numba\nimport numpy as np\n\n@numba.njit\ndef random_dropout(cell_expr,max_rate):\n    non_zero_mask = np.where(cell_expr!=0)[0]\n    zero_mask = np.random.choice(non_zero_mask,int(len(non_zero_mask)*np.float32(np.random.uniform(0,max_rate))))\n    cell_expr[zero_mask] = 0\n    return cell_expr\n\n@numba.njit\ndef random_scale(cell_expr,max_val):\n    scale_factor = np.float32(1+np.random.uniform(-max_val,max_val))\n    cell_expr = cell_expr*scale_factor\n    return cell_expr\n\n@numba.njit\ndef random_shift(cell_expr,kth):\n    shift_value = np.random.choice(np.array([1,0,-1]),1)[0]*np.unique(cell_expr)[int(np.random.uniform(0,kth)*len(np.unique(cell_expr)))]\n    cell_expr[cell_expr != 0] = cell_expr[cell_expr != 0]+shift_value\n    cell_expr[cell_expr < 0] = 0\n    return cell_expr\n\n@numba.njit(parallel=True)\ndef random_augment(mtx,max_rate=0.8,max_val=0.8,kth=0.2):\n    for i in numba.prange(mtx.shape[0]):\n        random_dropout(mtx[i,:],max_rate=max_rate)\n        random_scale(mtx[i,:],max_val=max_val)\n        random_shift(mtx[i,:],kth=kth)\n    return mtx\n\n@numba.njit\ndef random_augmentation_cell(cell_expr,max_rate=0.8,max_val=0.8,kth=0.2):\n    cell_expr = random_dropout(cell_expr,max_rate=max_rate)\n    cell_expr = random_scale(cell_expr,max_val=max_val)\n    cell_expr = random_shift(cell_expr,kth=kth)\n    return cell_expr"}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/model/DANN_model.py", "content": "import os\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data as Data\nimport random\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom Baselines.scpDeconv.scpDeconv_main.model.utils import *\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, do_rates):\n        super(EncoderBlock, self).__init__()\n        self.layer = nn.Sequential(nn.Linear(in_dim, out_dim),\n                                   nn.LeakyReLU(0.2, inplace=True),\n                                   nn.Dropout(p=do_rates, inplace=False))\n    def forward(self, x):\n        out = self.layer(x)\n        return out\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, do_rates):\n        super(DecoderBlock, self).__init__()\n        self.layer = nn.Sequential(nn.Linear(in_dim, out_dim),\n                                   nn.LeakyReLU(0.2, inplace=True),\n                                   nn.Dropout(p=do_rates, inplace=False))\n    def forward(self, x):\n        out = self.layer(x)\n        return out\n\nclass DANN(object):\n    def __init__(self, celltype_num,outdirfile):\n        self.num_epochs =150\n        self.batch_size = 100\n        self.target_type = \"real\"\n        self.learning_rate = 0.0001\n        self.celltype_num = celltype_num\n        self.labels = None\n        self.used_features = None\n        #self.seed = 2021,200,1200\n        self.seed = 2021\n        self.outdir = outdirfile\n\n        cudnn.deterministic = True\n        torch.cuda.manual_seed_all(self.seed)\n        torch.manual_seed(self.seed)\n        random.seed(self.seed)\n\n    def DANN_model(self, celltype_num):\n        feature_num = len(self.used_features)\n        \n        self.encoder_da = nn.Sequential(EncoderBlock(feature_num, 512, 0), \n                                        EncoderBlock(512, 256, 0.3))\n\n        self.predictor_da = nn.Sequential(EncoderBlock(256, 128, 0.2), \n                                          nn.Linear(128, celltype_num), \n                                          nn.Softmax(dim=1))\n        \n        self.discriminator_da = nn.Sequential(EncoderBlock(256, 128, 0.2), \n                                              nn.Linear(128, 1), \n                                              nn.Sigmoid())\n\n        model_da = nn.ModuleList([])\n        model_da.append(self.encoder_da)\n        model_da.append(self.predictor_da)\n        model_da.append(self.discriminator_da)\n        return model_da\n\n    def prepare_dataloader(self, sm_data, sm_label,st_data, batch_size):\n        ### Prepare data loader for training ###\n        # Source dataset\n        # source_ratios = [source_data.obs[ctype] for ctype in source_data.uns['cell_types']]\n        # self.source_data_x = source_data.X.astype(np.float32)\n        # self.source_data_y = np.array(source_ratios, dtype=np.float32).transpose()\n        self.source_data_x = sm_data.values.astype(np.float32)\n        self.source_data_y = sm_label.values.astype(np.float32)\n        \n        tr_data = torch.FloatTensor(self.source_data_x)\n        tr_labels = torch.FloatTensor(self.source_data_y)\n        source_dataset = Data.TensorDataset(tr_data, tr_labels)\n        self.train_source_loader = Data.DataLoader(dataset=source_dataset, batch_size=batch_size, shuffle=True)\n\n        # Extract celltype and feature info\n        # self.labels = source_data.uns['cell_types']\n        # self.celltype_num = len(self.labels)\n        # self.used_features = list(source_data.var_names)\n        self.used_features = list(sm_data.columns)\n\n        # Target dataset\n        # self.target_data_x = target_data.X.astype(np.float32)\n        self.target_data_x = torch.from_numpy(st_data.values.astype(np.float32))\n        if self.target_type == \"simulated\":\n            target_ratios = [self.target_data_y.obs[ctype] for ctype in self.labels]\n            self.target_data_y = np.array(target_ratios, dtype=np.float32).transpose()\n        elif self.target_type == \"real\":\n            self.target_data_y = np.random.rand(self.target_data_x.shape[0], self.celltype_num)\n\n        te_data = torch.FloatTensor(self.target_data_x)\n        te_labels = torch.FloatTensor(self.target_data_y)\n        target_dataset = Data.TensorDataset(te_data, te_labels)\n        self.train_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=True)\n        self.test_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=False)\n\n    def train(self, sm_data, sm_label,st_data):\n        ### prepare model structure ###\n        self.prepare_dataloader(sm_data, sm_label,st_data, self.batch_size)\n        self.model_da = self.DANN_model(self.celltype_num).cuda()\n\n        ### setup optimizer ###\n        optimizer_da1 = torch.optim.Adam([{'params': self.encoder_da.parameters()},\n                                          {'params': self.predictor_da.parameters()},\n                                          {'params': self.discriminator_da.parameters()}], lr=self.learning_rate)\n        optimizer_da2 = torch.optim.Adam([{'params': self.encoder_da.parameters()},\n                                          {'params': self.discriminator_da.parameters()}], lr=self.learning_rate)\n        \n        criterion_da = nn.BCELoss().cuda()\n        source_label = torch.ones(self.batch_size).unsqueeze(1).cuda()   # 定义source domain label为1\n        target_label = torch.zeros(self.batch_size).unsqueeze(1).cuda()  # 定义target domain label为0\n        \n        metric_logger = defaultdict(list) \n\n        for epoch in range(self.num_epochs):\n            self.model_da.train()\n            train_target_iterator = iter(self.train_target_loader)\n            pred_loss_epoch, disc_loss_epoch, disc_loss_DA_epoch = 0., 0., 0.\n            for batch_idx, (source_x, source_y) in enumerate(self.train_source_loader):\n                # get batch item of target\n                try:\n                    target_x, _ = next(train_target_iterator)\n                except StopIteration:\n                    train_target_iterator = iter(self.train_target_loader)\n                    target_x, _ = next(train_target_iterator)\n\n                embedding_source = self.encoder_da(source_x.cuda())\n                embedding_target = self.encoder_da(target_x.cuda())\n                frac_pred = self.predictor_da(embedding_source)\n                domain_pred_source = self.discriminator_da(embedding_source)\n                domain_pred_target = self.discriminator_da(embedding_target)\n\n                # caculate loss \n                pred_loss = L1_loss(frac_pred, source_y.cuda())       \n                pred_loss_epoch += pred_loss.data.item()\n                disc_loss = criterion_da(domain_pred_source, source_label[0:domain_pred_source.shape[0],]) + criterion_da(domain_pred_target, target_label[0:domain_pred_target.shape[0],])\n                disc_loss_epoch += disc_loss.data.item()\n                loss = pred_loss + disc_loss\n\n                # update weights\n                optimizer_da1.zero_grad()\n                loss.backward(retain_graph=True)\n                optimizer_da1.step()\n\n                embedding_source = self.encoder_da(source_x.cuda())\n                embedding_target = self.encoder_da(target_x.cuda())\n                domain_pred_source = self.discriminator_da(embedding_source)\n                domain_pred_target = self.discriminator_da(embedding_target)\n\n                # caculate loss \n                disc_loss_DA = criterion_da(domain_pred_target, source_label[0:domain_pred_target.shape[0],]) + criterion_da(domain_pred_source, target_label[0:domain_pred_source.shape[0],]) \n                disc_loss_DA_epoch += disc_loss_DA.data.item()\n\n                # update weights\n                optimizer_da2.zero_grad()\n                disc_loss_DA.backward(retain_graph=True)\n                optimizer_da2.step()\n\n            pred_loss_epoch = pred_loss_epoch/(batch_idx + 1)\n            metric_logger['pred_loss'].append(pred_loss_epoch)\n            disc_loss_epoch = disc_loss_epoch/(batch_idx + 1)\n            metric_logger['disc_loss'].append(disc_loss_epoch)\n            disc_loss_DA_epoch = disc_loss_DA_epoch/(batch_idx + 1)\n            metric_logger['disc_loss_DA'].append(disc_loss_DA_epoch)\n        \n            if (epoch+1) % 50 == 0:\n                print('============= Epoch {:02d}/{:02d} in stage3 ============='.format(epoch + 1, self.num_epochs))\n                print(\"pred_loss=%f, disc_loss=%f, disc_loss_DA=%f\" % (pred_loss_epoch, disc_loss_epoch, disc_loss_DA_epoch))\n                if self.target_type == \"simulated\":\n                    ### model validation on target data ###\n                    target_preds, ground_truth = self.prediction()\n                    epoch_ccc, epoch_rmse, epoch_corr = compute_metrics(target_preds, ground_truth)\n                    metric_logger['target_ccc'].append(epoch_ccc)\n                    metric_logger['target_rmse'].append(epoch_rmse)\n                    metric_logger['target_corr'].append(epoch_corr)\n\n        if self.target_type == \"simulated\":\n            SaveLossPlot(self.outdir, metric_logger, loss_type = ['pred_loss','disc_loss','disc_loss_DA','target_ccc','target_rmse','target_corr'], output_prex = 'Loss_metric_plot_stage3')\n        elif self.target_type == \"real\":\n            SaveLossPlot(self.outdir, metric_logger, loss_type = ['pred_loss','disc_loss','disc_loss_DA'], output_prex = 'Loss_metric_plot_stage3')\n            \n    def prediction(self):\n        self.model_da.eval()\n        preds, gt = None, None\n        for batch_idx, (x, y) in enumerate(self.test_target_loader):\n            logits = self.predictor_da(self.encoder_da(x.cuda())).detach().cpu().numpy()\n            frac = y.detach().cpu().numpy()\n            preds = logits if preds is None else np.concatenate((preds, logits), axis=0)\n            gt = frac if gt is None else np.concatenate((gt, frac), axis=0)\n\n        target_preds = pd.DataFrame(preds, columns=self.labels)\n        ground_truth = pd.DataFrame(gt, columns=self.labels)\n        return target_preds\n    "}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/main.py", "content": "import copy\nimport os\nimport sys\n\nfrom Baselines.scpDeconv.scpDeconv_main.model.DANN_model import *\nfrom Baselines.scpDeconv.scpDeconv_main.model.utils import *\nimport anndata\nfrom collections import Counter\ndef main(a,b,cell_key):\n\tfor i in range(a,b):\n\t\tsc_file = 'Datasets/preproced_data\\dataset' + str(i) + '\\Scdata_filter.h5ad'\n\t\tsm_labelfile = 'Datasets/preproced_data\\dataset' + str(i) + '\\Sm_STdata_filter.h5ad'\n\t\tst_datafile = 'Datasets/preproced_data\\dataset' + str(i) + '\\Real_STdata_filter.h5ad'\n\n\n\t\tst_adata1 = anndata.read_h5ad(st_datafile)\n\t\treal_sc_adata1 = anndata.read_h5ad(sc_file)\n\t\tsm_labelad1 = anndata.read_h5ad(sm_labelfile)\n\t\t\"\"\"copy:保证数据不改变\"\"\"\n\t\tst_adata=copy.deepcopy(st_adata1)\n\t\treal_sc_adata = copy.deepcopy(real_sc_adata1)\n\t\tsm_labelad = copy.deepcopy(sm_labelad1)\n\t\tsc_adata = copy.deepcopy(sm_labelad1)\n\t\t# sm_data=pd.DataFrame(data=sc_adata.X.toarray(),columns=sc_adata.var_names)\n\t\tsm_data = pd.DataFrame(data=sc_adata.X.toarray(), columns=sc_adata.var_names)\n\t\tsm_lable = sm_labelad.obsm['label']\n\n\t\tst_data = pd.DataFrame(data=st_adata.X, columns=st_adata.var_names, index=st_adata.obs_names)\n\n\n\t\tcount_ct_dict = Counter(list(real_sc_adata.obs[cell_key]))\n\t\tcelltypenum = len(count_ct_dict)\n\t\t\"\"\"保存途径\"\"\"\n\t\toutfile = 'Baselines/scpDeconv\\Result' + '/dataset' + str(i) + '/'\n\t\tif not os.path.exists(outfile):\n\t\t\tos.makedirs(outfile)\n\t\t### Run Stage 3 ###\n\t\tprint(\"------Start Running Stage 3 : Training DANN model------\")\n\t\tmodel_da = DANN( celltype_num=celltypenum,outdirfile=outfile)\n\t\tprint(st_data.shape,sm_data.shape,sm_lable.shape)\n\t\tmodel_da.train(st_data=st_data, sm_data=sm_data, sm_label=sm_lable)\n\t\tprint(\"Stage 3 : DANN model training finished!\")\n\n\t\t### Run Stage 4 ###\n\t\tprint(\"------Start Running Stage 4 : Inference for target data------\")\n\n\t\tfinal_preds_target = model_da.prediction()\n\t\t# print(final_preds_target)\n\t\tfinal_preds_target.to_csv(outfile+ '/final_preds1.csv')\n\t\tfinal_preds_target.columns = sm_lable.columns.tolist()\n\t\tpd.DataFrame(data=final_preds_target).to_csv(outfile + '/final_preds.csv')\n\t\tprint(\"Stage 4 : Inference for target data finished!\")\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Stereoscope/main_code.py", "content": "\"\"\"https://github.com/QuKunLab/SpatialBenchmarking/blob/main/Codes/Deconvolution/Stereoscope_pipeline.py\"\"\"\r\nimport copy\r\nimport torch\r\nimport scanpy as sc\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport scvi\r\nfrom scvi.external import RNAStereoscope, SpatialStereoscope\r\n\r\nimport sys\r\n\r\nimport os\r\ndef check_and_convert_to_integers(tensor):\r\n    if not torch.all(tensor == tensor.to(torch.int)):\r\n        print(\"输入数据包含浮点数，正在转换为整数...\")\r\n        tensor = tensor.to(torch.int)\r\n    return tensor\r\ndef main(a,b,cell_key):\r\n    for i in range(a, b):\r\n        sc_file = 'Datasets/preproced_data\\dataset' + str(i) + '\\Scdata_filter.h5ad'\r\n        st_file = 'Datasets/preproced_data\\dataset' + str(i)+ '\\Real_STdata_filter.h5ad'\r\n\r\n        celltype_key = cell_key\r\n        output_path = 'Baselines/Stereoscope\\Result\\dataset' + str(i)\r\n        if not os.path.exists(output_path):\r\n            os.makedirs(output_path)\r\n        sc_adata1 = sc.read_h5ad(sc_file)\r\n        st_adata1 = sc.read_h5ad(st_file)\r\n\r\n        sc_adata = copy.deepcopy(sc_adata1)\r\n        st_adata = copy.deepcopy(st_adata1)\r\n\r\n        sc.pp.filter_genes(sc_adata, min_counts=10)\r\n\r\n        non_mito_genes_list = [name for name in sc_adata.var_names if not name.startswith('MT-')]\r\n        sc_adata = sc_adata[:, non_mito_genes_list]\r\n\r\n        sc_adata.layers[\"counts\"] = sc_adata.X.copy()\r\n        sc.pp.normalize_total(sc_adata, target_sum=1e5)\r\n        sc.pp.log1p(sc_adata)\r\n        sc_adata.raw = sc_adata\r\n\r\n        sc.pp.highly_variable_genes(\r\n            sc_adata,\r\n            n_top_genes=7000,\r\n            subset=True,\r\n            layer=\"counts\",\r\n            flavor=\"seurat_v3\",\r\n            span=1\r\n        )\r\n        if len(set(st_adata.var_names)) != len(st_adata.var_names):\r\n            print(\"Removing duplicate genes from st_ad\")\r\n            # Create a boolean mask where True indicates the first occurrence of each gene\r\n            mask = ~st_adata.var_names.duplicated()\r\n            st_adata = st_adata[:, mask].copy()\r\n        intersect = np.intersect1d(sc_adata.var_names, st_adata.var_names)\r\n        st_adata = st_adata[:, intersect].copy()\r\n        sc_adata = sc_adata[:, intersect].copy()\r\n\r\n        # scvi.data.setup_anndata(sc_adata, layer=\"counts\", labels_key=celltype_key)\r\n        RNAStereoscope.setup_anndata(sc_adata, layer=\"counts\", labels_key=celltype_key)\r\n\r\n        print(sc_adata1)\r\n\r\n        stereo_sc_model = RNAStereoscope(sc_adata)\r\n        stereo_sc_model.train(max_epochs=50)\r\n        # stereo_sc_model.history[\"elbo_train\"][10:].plot()\r\n\r\n        st_adata.layers[\"counts\"] = st_adata.X.copy()\r\n        # scvi.data.setup_anndata(st_adata, layer=\"counts\")\r\n        scvi.external.SpatialStereoscope.setup_anndata(st_adata, layer=\"counts\")\r\n        spatial_model = SpatialStereoscope.from_rna_model(st_adata, stereo_sc_model)\r\n        spatial_model.train(max_epochs=350)\r\n        # spatial_model.history[\"elbo_train\"][10:].plot()\r\n        spatial_model.get_proportions().to_csv(output_path + '/Stereoscope_result.csv')"}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/model/__init__.py", "content": "\nfrom Baselines.scpDeconv.scpDeconv_main.model.AEimpute_model import *\nfrom Baselines.scpDeconv.scpDeconv_main.model.DANN_model import *\nfrom Baselines.scpDeconv.scpDeconv_main.model.utils import *\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Tangram/main_code.py", "content": "\"\"\"https://github.com/QuKunLab/SpatialBenchmarking/blob/main/Codes/Deconvolution/Tangram_pipeline.py#L9\"\"\"\r\nimport copy\r\nimport os\r\n\r\nimport tangram\r\nimport anndata as ad\r\nimport anndata\r\nimport pandas as pd\r\nimport scanpy as sc\r\nimport numpy as np\r\nimport tangram as tg\r\n\r\ndef main(a,b,cell_key):\r\n    for i in range(a, b):\r\n        sc_file = 'Datasets/preproced_data\\dataset' + str(i) + '\\Scdata_filter.h5ad'\r\n        st_file = 'Datasets/preproced_data\\dataset' + str(i)+ '\\Real_STdata_filter.h5ad'\r\n        output_file_path='Baselines/Tangram\\Result/' + 'dataset' + str(i)\r\n        if not os.path.exists(output_file_path):\r\n            os.makedirs(output_file_path)\r\n\r\n        celltype_key = cell_key\r\n        ad_sc1 = sc.read_h5ad(sc_file)\r\n        ad_sp1 = sc.read_h5ad(st_file)\r\n        ad_sc = copy.deepcopy(ad_sc1)\r\n        ad_sp = copy.deepcopy(ad_sp1)\r\n        # use raw count both of scrna and spatial\r\n        sc.pp.normalize_total(ad_sc)\r\n        celltype_counts = ad_sc.obs[celltype_key].value_counts()\r\n        celltype_drop = celltype_counts.index[celltype_counts < 2]\r\n        print(f'Drop celltype {list(celltype_drop)} contain less 2 sample')\r\n        ad_sc = ad_sc[~ad_sc.obs[celltype_key].isin(celltype_drop),].copy()\r\n        sc.tl.rank_genes_groups(ad_sc, groupby=celltype_key, use_raw=False)\r\n        markers_df = pd.DataFrame(ad_sc.uns[\"rank_genes_groups\"][\"names\"]).iloc[0:300, :]\r\n        # print(markers_df)\r\n        genes_sc = np.unique(markers_df.melt().value.values)\r\n        # print(genes_sc)\r\n        genes_st = ad_sp.var_names.values\r\n        genes = list(set(genes_sc).intersection(set(genes_st)))\r\n        tg.pp_adatas(ad_sc, ad_sp, genes=genes)\r\n        ad_map = tg.map_cells_to_space(\r\n            ad_sc,\r\n            ad_sp,\r\n            num_epochs=800,\r\n            mode='clusters',\r\n            cluster_label=celltype_key)\r\n        tg.project_cell_annotations(ad_map, ad_sp, annotation=celltype_key)\r\n        celltype_density = ad_sp.obsm['tangram_ct_pred']\r\n        celltype_density = (celltype_density.T / celltype_density.sum(axis=1)).T\r\n\r\n        celltype_density.to_csv(output_file_path + '/Tangram_result.csv')\r\n\r\n\r\n\r\n\r\n"}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/model/refer_mixup.py", "content": "import os\nimport sys\nimport pandas as pd\nimport anndata as ad\nimport numpy as np\nimport scipy\nimport scanpy as sc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom Baselines.scpDeconv.scpDeconv_main.model.utils import *\n\nclass ReferMixup(object):\n    def __init__(self, option_list):  \n        self.data_path = option_list['data_dir']\n        self.ref_dataset_name = option_list['ref_dataset_name']\n        self.ref_metadata_name = option_list['ref_metadata_name']\n        self.target_dataset_name = option_list['target_dataset_name']\n        self.target_metadata_name = option_list['target_metadata_name']\n        self.random_type = option_list['random_type']\n        self.type_list = option_list['type_list']\n        self.train_sample_num = option_list['ref_sample_num']\n        self.sample_size = option_list['sample_size']\n        self.HVP_num = option_list['HVP_num']\n        self.target_type = option_list['target_type']\n        self.target_sample_num = option_list['target_sample_num']\n        self.outdir = option_list['SaveResultsDir']\n        self.normalize = 'min_max'\n\n    def mixup(self):\n\n        # mixup reference datasets and simulate pseudo-bulk train data\n        train_data_x, train_data_y = self.mixup_dataset(self.ref_dataset_name, self.ref_metadata_name, self.train_sample_num)\n\n        # mixup to simulate pseudo target data or get real target data\n        if self.target_type == \"simulated\":\n            target_data_x, target_data_y = self.mixup_dataset(self.target_dataset_name, self.target_metadata_name, self.target_sample_num)\n            target_data = ad.AnnData(X=target_data_x.to_numpy(), obs=target_data_y)\n            target_data.var_names = target_data_x.columns\n\n        elif self.target_type == \"real\":\n            target_data = self.load_real_data(self.target_dataset_name)\n\n        # find protein list as used features by integrating train and target \n        used_features = self.align_features(train_data_x, target_data)\n\n        # prepare train data and target data with aligned features\n        train_data = self.align_dataset(train_data_x, train_data_y, used_features)\n        target_data = target_data[:,used_features]\n\n        # SavetSNEPlot(self.outdir, train_data, output_prex='Pseudo_Bulk_Source_'+str(self.train_sample_num))\n        # SavetSNEPlot(self.outdir, target_data, output_prex='Pseudo_Bulk_Target_'+str(self.target_sample_num))\n            \n        return train_data, target_data\n\n    def align_features(self, train_data_x, target_data):\n\n        used_features = set(train_data_x.columns.tolist()).intersection(set(target_data.var_names.tolist())) # overlapped features between reference and target\n        \n        if self.HVP_num == 0:\n            used_features = list(used_features)\n\n        elif self.HVP_num > 0:\n            sc.pp.highly_variable_genes(target_data, n_top_genes=self.HVP_num)\n            HVPs = set(target_data.var[target_data.var.highly_variable].index)\n            used_features = list(used_features.union(HVPs))\n\n        return used_features\n\n    def align_dataset(self, sim_data_x, sim_data_y, used_features):\n\n        missing_features = [feature for feature in used_features if feature not in list(sim_data_x.columns)]   \n\n        if len(missing_features) > 0:\n            missing_data_x = pd.DataFrame(np.zeros((sim_data_x.shape[0],len(missing_features))), columns=missing_features, index=sim_data_x.index)\n            sim_data_x = pd.concat([sim_data_x, missing_data_x], axis=1)\n\n        sim_data_x = sim_data_x[used_features]\n        \n        sim_data = ad.AnnData(\n            X=sim_data_x.to_numpy(),\n            obs=sim_data_y\n        )\n        sim_data.uns[\"cell_types\"] = self.type_list\n        sim_data.var_names = used_features\n\n        return sim_data\n\n    def mixup_dataset(self, dataset, metadata, sample_num):\n\n        sim_data_x = []\n        sim_data_y = []\n\n        ref_data_x, ref_data_y = self.load_ref_dataset(dataset, metadata)\n\n        for i in range(int(sample_num)):\n            sample, label = self.mixup_cells(ref_data_x, ref_data_y, self.type_list)\n            sim_data_x.append(sample)\n            sim_data_y.append(label)\n\n        sim_data_x = pd.concat(sim_data_x, axis=1).T\n        sim_data_y = pd.DataFrame(sim_data_y, columns=self.type_list)\n\n        # Scale pseudo-bulk data\n        if self.normalize:\n            sim_data_x_scale = sample_normalize(sim_data_x, normalize_method=self.normalize)\n            sim_data_x_scale = pd.DataFrame(sim_data_x_scale, columns=sim_data_x.columns)\n            sim_data_x = sim_data_x_scale\n\n        return sim_data_x, sim_data_y\n\n    def load_ref_dataset(self, dataset, metadata):\n        \n        if \".h5ad\" in dataset:\n            filename = os.path.join(self.data_path, dataset)\n\n            try:\n                data_h5ad = ad.read_h5ad(filename)\n                # Extract celltypes\n                if self.type_list == None:\n                    self.type_list = list(set(data_h5ad.obs[self.random_type].tolist()))\n                data_h5ad = data_h5ad[data_h5ad.obs[self.random_type].isin(self.type_list)]\n            except FileNotFoundError as e:\n                print(f\"No such h5ad file found for [cyan]{dataset}\")\n                sys.exit(e)\n\n            try:\n                data_y = pd.DataFrame(data_h5ad.obs[self.random_type])\n                data_y.reset_index(inplace=True, drop=True)\n            except Exception as e:\n                print(f\"Celltype attribute not found for [cyan]{dataset}\")\n                sys.exit(e)\n\n            if scipy.sparse.issparse(data_h5ad.X):\n                data_x = pd.DataFrame(data_h5ad.X.todense())\n            else:\n                data_x = pd.DataFrame(data_h5ad.X)\n            \n            data_x = data_x.fillna(0) # fill na with 0    \n            data_x.index = data_h5ad.obs_names\n            data_x.columns = data_h5ad.var_names\n\n            return data_x, data_y\n\n        elif \".csv\" in dataset:\n            filename = os.path.join(self.data_path, dataset)\n\n            try:\n                data_x = pd.read_csv(filename, header=0, index_col=0)\n            except FileNotFoundError as e:\n                print(f\"No such expression csv file found for [cyan]{dataset}\")\n                sys.exit(e)\n        \n            data_x = data_x.fillna(0) # fill na with 0    \n            \n            if metadata is not None:\n                metadata_filename = os.path.join(self.data_path, metadata)\n                try:\n                    data_y = pd.read_csv(metadata_filename, header=0, index_col=0)\n                except Exception as e:\n                    print(f\"Celltype attribute not found for [cyan]{dataset}\")\n                    sys.exit(e)\n            else:\n                print(f\"Metadata file is not provided for [cyan]{dataset}\")\n                sys.exit(1)\n\n            return data_x, data_y\n\n    def load_real_data(self, dataset):\n        \n        if \".h5ad\" in dataset:\n            filename = os.path.join(self.data_path, dataset)\n\n            try:\n                data_h5ad = ad.read_h5ad(filename)\n            except FileNotFoundError as e:\n                print(f\"No such h5ad file found for [cyan]{dataset}.\")\n                sys.exit(e)\n            \n            if scipy.sparse.issparse(data_h5ad.X):\n                data_h5ad.X = pd.DataFrame(data_h5ad.X.todense()).fillna(0)\n            else:\n                data_h5ad.X = pd.DataFrame(data_h5ad.X).fillna(0)\n        \n            if self.normalize:\n                data_h5ad.X = sample_normalize(data_h5ad.X, normalize_method=self.normalize)\n\n            return data_h5ad\n\n        elif \".csv\" in dataset:\n            filename = os.path.join(self.data_path, dataset)\n\n            try:\n                data_x = pd.read_csv(filename, header=0, index_col=0)\n            except FileNotFoundError as e:\n                print(f\"No such target expression csv file found for [cyan]{dataset}.\")\n                sys.exit(e)\n            \n            data_x = data_x.fillna(0) # fill na with 0    \n            \n            data_h5ad = ad.AnnData(X=data_x)\n            data_h5ad.var_names = data_x.columns\n\n            if self.normalize:\n                data_h5ad.X = sample_normalize(data_h5ad.X, normalize_method=self.normalize)\n\n            return data_h5ad\n\n    def mixup_fraction(self, celltype_num):\n\n        fracs = np.random.rand(celltype_num)\n        fracs_sum = np.sum(fracs)\n        fracs = np.divide(fracs, fracs_sum)\n\n        return fracs\n\n    def mixup_cells(self, x, y, celltypes):\n\n        available_celltypes = celltypes\n        \n        celltype_num = len(available_celltypes)\n\n        # Create fractions for available celltypes\n        fracs = self.mixup_fraction(celltype_num)\n\n        samp_fracs = np.multiply(fracs, self.sample_size)\n        samp_fracs = list(map(round, samp_fracs))\n        \n        # Make complete fracions\n        fracs_complete = [0] * len(celltypes)\n\n        for i, act in enumerate(available_celltypes):\n            idx = celltypes.index(act)\n            fracs_complete[idx] = fracs[i]\n\n        artificial_samples = []\n\n        for i in range(celltype_num):\n            ct = available_celltypes[i]\n            cells_sub = x.loc[np.array(y[self.random_type] == ct), :]\n            cells_fraction = np.random.randint(0, cells_sub.shape[0], samp_fracs[i])\n            cells_sub = cells_sub.iloc[cells_fraction, :]\n            artificial_samples.append(cells_sub)\n\n        df_samp = pd.concat(artificial_samples, axis=0)\n        df_samp = df_samp.sum(axis=0)\n\n        return df_samp, fracs_complete\n\n    "}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/model/utils.py", "content": "import os\nfrom sklearn import preprocessing as pp\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport scanpy as sc\nimport torch\nimport torch.nn as nn\n\n### loss function ###\ndef L1_loss(preds, gt):\n    loss = torch.mean(torch.reshape(torch.square(preds - gt), (-1,)))\n    return loss\n\ndef Recon_loss(recon_data, input_data):\n    loss_rec_fn = nn.MSELoss().cuda()\n    loss= loss_rec_fn(recon_data, input_data)\n    return loss\n\n### evaluate metrics ###\ndef ccc(preds, gt):\n    numerator = 2 * np.corrcoef(gt, preds)[0][1] * np.std(gt) * np.std(preds)\n    denominator = np.var(gt) + np.var(preds) + (np.mean(gt) - np.mean(preds)) ** 2\n    ccc_value = numerator / denominator\n    return ccc_value\n\ndef compute_metrics(preds, gt):\n    gt = gt[preds.columns] # Align pred order and gt order  \n    x = pd.melt(preds)['value']\n    y = pd.melt(gt)['value']\n    CCC = ccc(x, y)\n    RMSE = sqrt(mean_squared_error(x, y))\n    Corr = pearsonr(x, y)[0]\n    return CCC, RMSE, Corr\n\ndef sample_normalize(data, normalize_method = 'min_max'):\n    # Normalize data\n    mm = pp.MinMaxScaler(feature_range=(0, 1), copy=True)\n    if normalize_method == 'min_max':\n        # it scales features so transpose is needed\n        data = mm.fit_transform(data.T).T   \n    elif normalize_method == 'z_score':\n        # Z score normalization\n        data = (data - data.mean(0))/(data.std(0)+(1e-10))\n    return data\n\ndef SaveLossPlot(SavePath, metric_logger, loss_type, output_prex):\n    if not os.path.exists(SavePath):\n        os.mkdir(SavePath)\n    for i in range(len(loss_type)):\n        plt.subplot(2, 3, i+1)\n        plt.plot(metric_logger[loss_type[i]])\n        plt.title(loss_type[i], x = 0.5, y = 0.5)\n    imgName = os.path.join(SavePath, output_prex +'.png')\n    plt.savefig(imgName)\n    plt.close()\n\ndef SavePredPlot(SavePath, target_preds, ground_truth):\n    if not os.path.exists(SavePath):\n        os.mkdir(SavePath)\n    celltypes = list(target_preds.columns)\n\n    plt.figure(figsize=(5*(len(celltypes)+1), 5)) \n\n    eval_metric = []\n    x = pd.melt(target_preds)['value']\n    y = pd.melt(ground_truth)['value']\n    eval_metric.append(ccc(x, y))\n    eval_metric.append(sqrt(mean_squared_error(x, y)))\n    eval_metric.append(pearsonr(x, y)[0])\n    plt.subplot(1, len(celltypes)+1, 1)\n    plt.xlim(0, max(y))\n    plt.ylim(0, max(y))\n    plt.scatter(x, y, s=2)\n    z = np.polyfit(x, y, 1)\n    p = np.poly1d(z)\n    plt.plot(x,p(x),\"r--\")\n    text = f\"$CCC = {eval_metric[0]:0.3f}$\\n$RMSE = {eval_metric[1]:0.3f}$\\n$Corr = {eval_metric[2]:0.3f}$\"\n    plt.text(0.05, max(y)-0.05, text, fontsize=8, verticalalignment='top')\n    plt.title('All samples')\n    plt.xlabel('Prediction')\n    plt.ylabel('Ground Truth')\n\n    for i in range(len(celltypes)):\n        eval_metric = []\n        x = target_preds[celltypes[i]]\n        y = ground_truth[celltypes[i]]\n        eval_metric.append(ccc(x, y))\n        eval_metric.append(sqrt(mean_squared_error(x, y)))\n        eval_metric.append(pearsonr(x, y)[0])\n        plt.subplot(1, len(celltypes)+1, i+2)\n        plt.xlim(0, max(y))\n        plt.ylim(0, max(y))\n        plt.scatter(x, y, s=2)\n        z = np.polyfit(x, y, 1)\n        p = np.poly1d(z)\n        plt.plot(x,p(x),\"r--\")\n        text = f\"$CCC = {eval_metric[0]:0.3f}$\\n$RMSE = {eval_metric[1]:0.3f}$\\n$Corr = {eval_metric[2]:0.3f}$\"\n        plt.text(0.05, max(y)-0.05, text, fontsize=8, verticalalignment='top')\n        plt.title(celltypes[i])\n        plt.xlabel('Prediction')\n        plt.ylabel('Ground Truth')\n    imgName = os.path.join(SavePath, 'pred_fraction_target_scatter.jpg')\n    plt.savefig(imgName)\n    plt.close()\n\ndef SavetSNEPlot(SavePath, ann_data, output_prex):\n    if not os.path.exists(SavePath):\n        os.mkdir(SavePath)\n    sc.tl.pca(ann_data, svd_solver='arpack')\n    sc.pp.neighbors(ann_data, n_neighbors=10, n_pcs=20)\n    sc.tl.tsne(ann_data)\n    with plt.rc_context({'figure.figsize': (8, 8)}):\n        sc.pl.tsne(ann_data, color=list(ann_data.obs.columns), color_map='viridis',frameon=False)\n        plt.tight_layout()\n    plt.savefig(os.path.join(SavePath, output_prex + \"_tSNE_plot.jpg\"))\n    ann_data.write(os.path.join(SavePath, output_prex + \".h5ad\"))\n    plt.close()\n    "}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/options.py", "content": "from collections import defaultdict\n\noption_list = defaultdict(list)\n\ndef get_option_list(dataset):\n\t\n\tif dataset == 'human_breast_atlas_PP':\n\t\t# Input parameters\n\t\toption_list['data_dir']='./data/human_breast_atlas_PP/'\n\t\toption_list['ref_dataset_name'] = 'Human_Breast_Atlas_scProteome_normed_aligned_individual1.h5ad'\n\t\toption_list['ref_metadata_name'] = None\n\t\toption_list['target_dataset_name'] = 'Human_Breast_Atlas_scProteome_normed_aligned_individual3.h5ad'\n\t\toption_list['target_metadata_name'] = None\n\t\toption_list['random_type']=\"cell_type\"\n\t\toption_list['type_list']=None\n\t\t# Training parameters\n\t\toption_list['ref_sample_num'] = 4000\n\t\toption_list['sample_size'] = 50\n\t\toption_list['HVP_num'] = 0\n\t\toption_list['target_type'] = \"simulated\"\n\t\toption_list['target_sample_num'] = 1000\t\n\t\toption_list['batch_size'] = 50\n\t\toption_list['epochs'] = 30\n\t\toption_list['learning_rate'] = 0.0001\n\t\t# Output parameters\n\t\toption_list['SaveResultsDir'] = \"./Result/human_breast_atlas_PP/\"\n\n\telif dataset == 'murine_cellline':\n\t\t# Input parameters\n\t\toption_list['data_dir']='./data/murine_cellline/'\n\t\toption_list['ref_dataset_name'] = 'murine_N2_SCP_exp.csv'\n\t\toption_list['ref_metadata_name'] = 'murine_N2_SCP_meta.csv'\n\t\toption_list['target_dataset_name'] = 'murine_nanoPOTS_SCP_exp.csv'\n\t\toption_list['target_metadata_name'] = 'murine_nanoPOTS_SCP_meta.csv'\n\t\toption_list['random_type']=\"CellType\"\n\t\toption_list['type_list']=['C10','SVEC','RAW']\n\t\t# Training parameters\n\t\toption_list['ref_sample_num'] = 4000\n\t\toption_list['sample_size'] = 15\n\t\toption_list['HVP_num'] = 500\n\t\toption_list['target_type'] = \"simulated\"\n\t\toption_list['target_sample_num'] = 1000\t\n\t\toption_list['batch_size'] = 50\n\t\toption_list['epochs'] = 30\n\t\toption_list['learning_rate'] = 0.0001\n\t\t# Output parameters\n\t\toption_list['SaveResultsDir'] = \"./Result/murine_cellline/\"\n\n\telif dataset == 'human_cellline':\n\t\t# Input parameters\n\t\toption_list['data_dir']='./data/human_cellline/'\n\t\toption_list['ref_dataset_name'] = 'pSCoPE_Huffman_PDAC+pSCoPE_Leduc+SCoPE2_Leduc_integrated_SCP.h5ad'\n\t\toption_list['ref_metadata_name'] = None\n\t\toption_list['target_dataset_name'] = 'T-SCP+plexDIA_integrated_SCP.h5ad'\n\t\toption_list['target_metadata_name'] = None\n\t\toption_list['random_type']=\"cell_type\"\n\t\toption_list['type_list']=None\n\t\t# Training parameters\n\t\toption_list['ref_sample_num'] = 4000\n\t\toption_list['sample_size'] = 50\n\t\toption_list['HVP_num'] = 500\n\t\toption_list['target_type'] = \"simulated\"\n\t\toption_list['target_sample_num'] = 1000\t\n\t\toption_list['batch_size'] = 50\n\t\toption_list['epochs'] = 30\n\t\toption_list['learning_rate'] = 0.0001\n\t\t# Output parameters\n\t\toption_list['SaveResultsDir'] = \"./Result/human_cellline/\"\n\n\telif dataset == 'simulated_data':\n\t\t# Input parameters\n\t\toption_list['data_dir']='./data/human_cellline/'\n\t\toption_list['ref_dataset_name'] = 'pSCoPE_Huffman_PDAC+pSCoPE_Leduc+SCoPE2_Leduc_integrated_SCP.h5ad'\n\t\toption_list['ref_metadata_name'] = None\n\t\toption_list['target_dataset_name'] = 'T-SCP+plexDIA_integrated_SCP.h5ad'\n\t\toption_list['target_metadata_name'] = None\n\t\toption_list['random_type']=\"cell_type\"\n\t\toption_list['type_list']=None\n\t\t# Training parameters\n\t\toption_list['ref_sample_num'] = 4000\n\t\toption_list['sample_size'] = 50\n\t\toption_list['HVP_num'] = 500\n\t\toption_list['target_type'] = \"simulated\"\n\t\toption_list['target_sample_num'] = 1000\n\t\toption_list['batch_size'] = 50\n\t\toption_list['epochs'] = 30\n\t\toption_list['learning_rate'] = 0.0001\n\t\t# Output parameters\n\t\toption_list['SaveResultsDir'] = \"./Result/human_cellline/\"\n\n\treturn option_list\n"}
{"type": "source_file", "path": "MACD_github/Baselines/Spoint/spatial_simulation.py", "content": "import numpy as np\nimport pandas as pd\nimport numba as nb\nfrom numba import jit\nimport collections\nimport random\nfrom .data_downsample import downsample_cell,downsample_matrix_by_cell\nfrom .data_augmentation import random_augment,random_augmentation_cell\nimport logging\n# logging.basicConfig(level=print,\n#                     format='%(asctime)s %(levelname)s %(message)s',\n#                     datefmt='%m-%d %H:%M')\n# logging.getLogger().setLevel(print)\n\n# 汇总每个spot的细胞数，统计细胞数的分布\ndef count_cell_counts(cell_counts):\n    cell_counts = np.array(cell_counts.values,dtype=int).reshape(-1)\n    counts_list = np.array(np.histogram(cell_counts,range=[0,np.max(cell_counts)+1],bins=np.max(cell_counts)+1)[0],dtype=int)\n    counts_index = np.array((np.histogram(cell_counts,range=[0,np.max(cell_counts)+1],bins=np.max(cell_counts)+1)[1][:-1]),dtype=int)\n    counts_df = pd.DataFrame(counts_list,index=counts_index,columns=['count'],dtype=np.int32)\n    counts_df = counts_df[(counts_df['count'] != 0) & (counts_df.index != 0)]\n    count_sum = 0\n    for i in np.arange(len(counts_df)):\n        count_sum += counts_df.iloc[i].values\n        if count_sum > counts_df.values.sum()*0.99:\n            counts_df_filtered = counts_df.iloc[:i+1,:]\n            break\n    return counts_df_filtered\n\n\n@nb.njit\ndef numba_set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n\n# 对某个axis调用numpy函数(numba版本)\n@nb.njit\ndef np_apply_along_axis(func1d, axis, arr):\n    assert arr.ndim == 2\n    assert axis in [0, 1]\n    if axis == 0:\n        result = np.empty(arr.shape[1], dtype=arr.dtype)\n        for i in range(len(result)):\n            result[i] = func1d(arr[:, i])\n    else:\n        result = np.empty(arr.shape[0], dtype=arr.dtype)\n        for i in range(len(result)):\n            result[i] = func1d(arr[i, :])\n    return result\n\n# 对某个axis计算均值(numba版本)\n@nb.njit\ndef np_mean(array, axis):\n    return np_apply_along_axis(np.mean, axis, array)\n\n# 对某个axis计算加和(numba版本)\n@nb.njit\ndef np_sum(array, axis):\n    return np_apply_along_axis(np.sum, axis, array)\n\n# 根据参数采样单细胞数据，生成模拟spot(numba版本)\n@jit(nopython=True,parallel=True)\ndef sample_cell(param_list,cluster_p,clusters,cluster_id,sample_exp,sample_cluster,cell_p_balanced,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(param_list), sample_exp.shape[1]),dtype=np.float32)\n    density = np.empty((len(param_list), sample_cluster.shape[1]),dtype=np.float32)\n\n    for i in nb.prange(len(param_list)):\n        params = param_list[i]\n        num_cell = params[0]\n        num_cluster = params[1]\n        used_clusters = clusters[np.searchsorted(np.cumsum(cluster_p), np.random.rand(num_cluster), side=\"right\")]\n        cluster_mask = np.array([False]*len(cluster_id))\n        for c in used_clusters:\n            cluster_mask = (cluster_id==c)|(cluster_mask)\n        # print('cluster_mask',cluster_mask)\n        # print('used_clusters',used_clusters)\n        used_cell_ind = np.where(cluster_mask)[0]\n        used_cell_p = cell_p_balanced[cluster_mask]\n        used_cell_p = used_cell_p/used_cell_p.sum()\n        sampled_cells = used_cell_ind[np.searchsorted(np.cumsum(used_cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp = np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float32)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        combined_clusters = np_sum(sample_cluster[cluster_id[sampled_cells]],axis=0).astype(np.float32)\n        exp[i,:] = combined_exp\n        density[i,:] = combined_clusters\n    return exp,density\n\n@jit(nopython=True,parallel=True)\ndef sample_cell_from_clusters(cluster_sample_list,ncell_sample_list,cluster_id,sample_exp,sample_cluster,cell_p_balanced,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(cluster_sample_list), sample_exp.shape[1]),dtype=np.float32)\n    density = np.empty((len(cluster_sample_list), sample_cluster.shape[1]),dtype=np.float32)\n    for i in nb.prange(len(cluster_sample_list)):\n        used_clusters = np.where(cluster_sample_list[i] == 1)[0]\n        num_cell = ncell_sample_list[i]\n        cluster_mask = np.array([False]*len(cluster_id))\n        for c in used_clusters:\n            cluster_mask = (cluster_id==c)|(cluster_mask)\n        used_cell_ind = np.where(cluster_mask)[0]\n        used_cell_p = cell_p_balanced[cluster_mask]\n        used_cell_p = used_cell_p/used_cell_p.sum()\n        sampled_cells = used_cell_ind[np.searchsorted(np.cumsum(used_cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp = np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float32)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        combined_clusters = np_sum(sample_cluster[cluster_id[sampled_cells]],axis=0).astype(np.float32)\n        exp[i,:] = combined_exp\n        density[i,:] = combined_clusters\n    return exp,density\n\ndef init_sample_prob(sc_ad,celltype_key):\n    print('### Initializing sample probability')\n    sc_ad.uns['celltype2num'] = pd.DataFrame(\n        np.arange(len(sc_ad.obs[celltype_key].value_counts())).T,\n        index=sc_ad.obs[celltype_key].value_counts().index.values,\n        columns=['celltype_num']\n    )\n    sc_ad.obs['celltype_num'] = [sc_ad.uns['celltype2num'].loc[c,'celltype_num'] for c in sc_ad.obs[celltype_key]]\n    cluster_p_unbalance = sc_ad.obs['celltype_num'].value_counts()/sc_ad.obs['celltype_num'].value_counts().sum()\n    cluster_p_sqrt = np.sqrt(sc_ad.obs['celltype_num'].value_counts())/np.sqrt(sc_ad.obs['celltype_num'].value_counts()).sum()\n    cluster_p_balance = pd.Series(\n        np.ones(len(sc_ad.obs['celltype_num'].value_counts()))/len(sc_ad.obs['celltype_num'].value_counts()), \n        index=sc_ad.obs['celltype_num'].value_counts().index\n    )\n#     cluster_p_balance = np.ones(len(sc_ad.obs['celltype_num'].value_counts()))/len(sc_ad.obs['celltype_num'].value_counts())\n    cell_p_balanced = [1/cluster_p_unbalance[c] for c in sc_ad.obs['celltype_num']]\n    cell_p_balanced = np.array(cell_p_balanced)/np.array(cell_p_balanced).sum()\n    sc_ad.obs['cell_p_balanced'] = cell_p_balanced\n    sc_ad.uns['cluster_p_balance'] = cluster_p_balance\n    sc_ad.uns['cluster_p_sqrt'] = cluster_p_sqrt\n    sc_ad.uns['cluster_p_unbalance'] = cluster_p_unbalance\n    return sc_ad\n\n# 将表达矩阵转化成array\ndef generate_sample_array(sc_ad, used_genes):\n    if used_genes is not None:\n        sc_df = sc_ad.to_df().loc[:,used_genes]\n    else:\n        sc_df = sc_ad.to_df()\n    return sc_df.values\n\n# 从均匀分布中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_uniform(num_sample,cells_min=None,cells_max=None,clusters_min=None,clusters_max=None):\n\n    cell_count = np.asarray(np.ceil(np.random.uniform(int(cells_min),int(cells_max),size=num_sample)),dtype=int)\n    cluster_count = np.asarray(np.ceil(np.clip(np.random.uniform(clusters_min,clusters_max,size=num_sample),1,cell_count)),dtype=int)\n    return cell_count, cluster_count\n\n# 从高斯分布中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_gaussian(num_sample,cells_min=None,cells_max=None,cells_mean=None,cells_std=None,clusters_mean=None,clusters_std=None):\n\n    cell_count = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    cluster_count = np.asarray(np.ceil(np.clip(np.random.normal(clusters_mean,clusters_std,size=num_sample),1,cell_count)),dtype=int)\n    return cell_count,cluster_count\n\n# 从用空间数据估计的cell counts中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_cell_counts(\n    num_sample,\n    cell_counts,\n    cluster_sample_mode='gaussian',\n    cells_min=None,cells_max=None,\n    cells_mean=None,cells_std=None,\n    clusters_mean=None,clusters_std=None,\n    clusters_min=None,clusters_max=None\n):\n    cell_count = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    if cluster_sample_mode == 'gaussian':\n        cluster_count = np.asarray(np.ceil(np.clip(np.random.normal(clusters_mean,clusters_std,size=num_sample),1,cell_count)),dtype=int)\n    elif cluster_sample_mode == 'uniform':\n        cluster_count = np.asarray(np.ceil(np.clip(np.random.uniform(clusters_min,clusters_max,size=num_sample),1,cell_count)),dtype=int)\n    else:\n        raise TypeError('Not correct sample method.')\n    return cell_count,cluster_count\n\n# 获取每个cluster的采样概率\ndef get_cluster_sample_prob(sc_ad,mode):\n    if mode == 'unbalance':\n        cluster_p = sc_ad.uns['cluster_p_unbalance'].values\n    elif mode == 'balance':\n        cluster_p = sc_ad.uns['cluster_p_balance'].values\n    elif mode == 'sqrt':\n        cluster_p = sc_ad.uns['cluster_p_sqrt'].values\n    else:\n        raise TypeError('Balance argument must be one of [ None, banlance, sqrt ].')\n    return cluster_p\n\ndef cal_downsample_fraction(sc_ad,st_ad,celltype_key=None):\n    st_counts_median = np.median(st_ad.X.sum(axis=1))\n    simulated_st_data, simulated_st_labels = generate_simulation_data(sc_ad,num_sample=10000,celltype_key=celltype_key,balance_mode=['unbalance'])\n    simulated_st_counts_median = np.median(simulated_st_data.sum(axis=1))\n    if st_counts_median < simulated_st_counts_median:\n        fraction = st_counts_median / simulated_st_counts_median\n        print(f'### Simulated data downsample fraction: {fraction}')\n        return fraction\n    else:\n        return None\n\n# 生成模拟数据\ndef generate_simulation_data(\n    sc_ad,\n    celltype_key,\n    num_sample: int, \n    used_genes=None,\n    balance_mode=['unbalance','sqrt','balance'],\n    cell_sample_method='gaussian',\n    cluster_sample_method='gaussian',\n    cell_counts=None,\n    downsample_fraction=None,\n    data_augmentation=True,\n    max_rate=0.8,max_val=0.8,kth=0.2,\n    cells_min=1,cells_max=20,\n    cells_mean=10,cells_std=5,\n    clusters_mean=None,clusters_std=None,\n    clusters_min=None,clusters_max=None,\n    cell_sample_counts=None,cluster_sample_counts=None,\n    ncell_sample_list=None,\n    cluster_sample_list=None,\n    n_cpus=None\n):\n    if not 'cluster_p_unbalance' in sc_ad.uns:\n        sc_ad = init_sample_prob(sc_ad,celltype_key)\n    num_sample_per_mode = num_sample//len(balance_mode)\n    cluster_ordered = np.array(sc_ad.obs['celltype_num'].value_counts().index)\n    cluster_num = len(cluster_ordered)\n    cluster_id = sc_ad.obs['celltype_num'].values\n    cluster_mask = np.eye(cluster_num)\n    if (cell_sample_counts is None) or (cluster_sample_counts is None):\n        if cell_counts is not None:\n            cells_mean = np.mean(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n            cells_std = np.std(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n            cells_min = int(np.min(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n            cells_max = int(np.max(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n        if clusters_mean is None:\n            clusters_mean = cells_mean/2\n        if clusters_std is None:\n            clusters_std = cells_std/2\n        if clusters_min is None:\n            clusters_min = cells_min\n        if clusters_max is None:\n            clusters_max = np.min((cells_max//2,cluster_num))\n\n        if cell_counts is not None:\n            cell_sample_counts, cluster_sample_counts = get_param_from_cell_counts(num_sample_per_mode,cell_counts,cluster_sample_method,cells_mean=cells_mean,cells_std=cells_std,cells_max=cells_max,cells_min=cells_min,clusters_mean=clusters_mean,clusters_std=clusters_std,clusters_min=clusters_min,clusters_max=clusters_max)\n        elif cell_sample_method == 'gaussian':\n            cell_sample_counts, cluster_sample_counts = get_param_from_gaussian(num_sample_per_mode,cells_mean=cells_mean,cells_std=cells_std,cells_max=cells_max,cells_min=cells_min,clusters_mean=clusters_mean,clusters_std=clusters_std)\n        elif cell_sample_method == 'uniform':\n            cell_sample_counts, cluster_sample_counts = get_param_from_uniform(num_sample_per_mode,cells_max=cells_max,cells_min=cells_min,clusters_min=clusters_min,clusters_max=clusters_max)\n        else:\n            raise TypeError('Not correct sample method.')\n    if cluster_sample_list is None or ncell_sample_list is None:\n        params = np.array(list(zip(cell_sample_counts, cluster_sample_counts)))\n\n        sample_data_list = []\n        sample_labels_list = []\n        for b in balance_mode:\n            print(f'### Genetating simulated spatial data using(no) scRNA data with mode: {b}')\n            cluster_p = get_cluster_sample_prob(sc_ad,b)\n            if downsample_fraction is not None:\n                if downsample_fraction > 0.035:\n                    sample_data,sample_labels = sample_cell(\n                        param_list=params,\n                        cluster_p=cluster_p,\n                        clusters=cluster_ordered,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        downsample_fraction=downsample_fraction,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                else:\n                    sample_data,sample_labels = sample_cell(\n                        param_list=params,\n                        cluster_p=cluster_p,\n                        clusters=cluster_ordered,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                    # logging.warning('### Downsample data with python backend')\n                    sample_data = downsample_matrix_by_cell(sample_data, downsample_fraction, n_cpus=n_cpus, numba_end=False)\n            else:\n                sample_data,sample_labels = sample_cell(\n                    param_list=params,\n                    cluster_p=cluster_p,\n                    clusters=cluster_ordered,\n                    cluster_id=cluster_id,\n                    sample_exp=generate_sample_array(sc_ad,used_genes),\n                    sample_cluster=cluster_mask,\n                    cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                    data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                )\n    #         if data_augmentation:\n    #             sample_data = random_augment(sample_data)\n            sample_data_list.append(sample_data)\n            sample_labels_list.append(sample_labels)\n    else:\n        sample_data_list = []\n        sample_labels_list = []\n        for b in balance_mode:\n            print(f'### Genetating simulated spatial data using scRNA data with mode: {b}')\n            cluster_p = get_cluster_sample_prob(sc_ad,b)\n            if downsample_fraction is not None:\n                if downsample_fraction > 0.035:\n                    sample_data,sample_labels = sample_cell_from_clusters(\n                        cluster_sample_list=cluster_sample_list,\n                        ncell_sample_list=ncell_sample_list,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        downsample_fraction=downsample_fraction,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                else:\n                    sample_data,sample_labels = sample_cell_from_clusters(\n                        cluster_sample_list=cluster_sample_list,\n                        ncell_sample_list=ncell_sample_list,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                    # logging.warning('### Downsample data with python backend')\n                    sample_data = downsample_matrix_by_cell(sample_data, downsample_fraction, n_cpus=n_cpus, numba_end=False)\n            else:\n                sample_data,sample_labels = sample_cell_from_clusters(\n                    cluster_sample_list=cluster_sample_list,\n                    ncell_sample_list=ncell_sample_list,\n                    cluster_id=cluster_id,\n                    sample_exp=generate_sample_array(sc_ad,used_genes),\n                    sample_cluster=cluster_mask,\n                    cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                    data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                )\n            sample_data_list.append(sample_data)\n            sample_labels_list.append(sample_labels)\n\n    return np.concatenate(sample_data_list), np.concatenate(sample_labels_list)\n\n@jit(nopython=True,parallel=True)\ndef sample_cell_exp(cell_counts,sample_exp,cell_p,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(cell_counts), sample_exp.shape[1]),dtype=np.float32)\n    ind = np.zeros((len(cell_counts), np.max(cell_counts)),dtype=np.int32)\n    cell_ind = np.arange(sample_exp.shape[0])\n    for i in nb.prange(len(cell_counts)):\n        num_cell = cell_counts[i]\n        sampled_cells=cell_ind[np.searchsorted(np.cumsum(cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp=np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float64)\n#         print(combined_exp.dtype)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        exp[i,:] = combined_exp\n        ind[i,:cell_counts[i]] = sampled_cells + 1\n    return exp,ind\n\ndef generate_simulation_st_data(\n    st_ad,\n    num_sample: int, \n    used_genes=None,\n    balance_mode=['unbalance'],\n    cell_sample_method='gaussian',\n    cell_counts=None,\n    downsample_fraction=None,\n    data_augmentation=True,\n    max_rate=0.8,max_val=0.8,kth=0.2,\n    cells_min=1,cells_max=10,\n    cells_mean=5,cells_std=3,\n):\n    print('### Genetating simulated data using spatial data')\n    cell_p = np.ones(len(st_ad))/len(st_ad)\n    if cell_counts is not None:\n        cells_mean = np.mean(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n        cells_std = np.std(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n        cells_min = int(np.min(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n        cells_max = int(np.max(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n    elif cell_sample_method == 'gaussian':\n        cell_counts = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    elif cell_sample_method == 'uniform':\n        cell_counts = np.asarray(np.ceil(np.random.uniform(int(cells_min),int(cells_max),size=num_sample)),dtype=int)\n    else:\n        raise TypeError('Not correct sample method.')\n\n    sample_data,sample_ind = sample_cell_exp(\n        cell_counts=cell_counts,\n        sample_exp=generate_sample_array(st_ad,used_genes),\n        cell_p=cell_p,\n        downsample_fraction=downsample_fraction,\n        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n    )\n#     if data_augmentation:\n#         sample_data = random_augment(sample_data)\n    return sample_data,sample_ind\n"}
{"type": "source_file", "path": "MACD_github/Baselines/scpDeconv/scpDeconv_main/model/AEimpute_model.py", "content": "import os\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data as Data\nimport random\nimport numpy as np\nimport pandas as pd\nimport anndata as ad\nimport scanpy as sc\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom Baselines.scpDeconv.scpDeconv_main.model.utils import *\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(EncoderBlock, self).__init__()\n        self.layer = nn.Sequential(nn.Linear(in_dim, out_dim),\n                                   nn.BatchNorm1d(out_dim),\n                                   nn.LeakyReLU(0.2, inplace=True))\n    def forward(self, x):\n        out = self.layer(x)\n        return out\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(DecoderBlock, self).__init__()\n        self.layer = nn.Sequential(nn.Linear(in_dim, out_dim),\n                                   nn.BatchNorm1d(out_dim),\n                                   nn.LeakyReLU(0.2, inplace=True))\n    def forward(self, x):\n        out = self.layer(x)\n        return out\n      \nclass AEimpute(object):\n    def __init__(self, option_list):\n        self.num_epochs = 200\n        self.batch_size = option_list['batch_size']\n        self.learning_rate = option_list['learning_rate']\n        self.celltype_num = None\n        self.labels = None\n        self.used_features = None\n        self.seed = 2021\n        self.outdir = option_list['SaveResultsDir']\n\n        cudnn.deterministic = True\n        torch.cuda.manual_seed_all(self.seed)\n        torch.manual_seed(self.seed)\n        random.seed(self.seed)\n\n    def AEimpute_model(self, celltype_num):\n        feature_num = len(self.used_features)\n\n        self.encoder_im = nn.Sequential(EncoderBlock(feature_num, 512), \n                                        EncoderBlock(512, 256))\n\n        self.predictor_im = nn.Sequential(nn.Linear(256, celltype_num), \n                                          nn.Softmax(dim=-1))\n\n        self.decoder_im = nn.Sequential(DecoderBlock(256, 512), \n                                        DecoderBlock(512, feature_num))\n\n        model_im = nn.ModuleList([])\n        model_im.append(self.encoder_im)\n        model_im.append(self.predictor_im)\n        model_im.append(self.decoder_im)\n        return model_im\n\n    def prepare_dataloader(self, ref_data, target_data, batch_size):\n        ### Prepare data loader for training ###\n        # ref dataset\n        ref_ratios = [ref_data.obs[ctype] for ctype in ref_data.uns['cell_types']]\n        self.ref_data_x = ref_data.X.astype(np.float32)\n        self.ref_data_y = np.array(ref_ratios, dtype=np.float32).transpose()\n\n        tr_data = torch.FloatTensor(self.ref_data_x)\n        tr_labels = torch.FloatTensor(self.ref_data_y)\n        ref_dataset = Data.TensorDataset(tr_data, tr_labels)\n        self.train_ref_loader = Data.DataLoader(dataset=ref_dataset, batch_size=batch_size, shuffle=True)\n        self.test_ref_loader = Data.DataLoader(dataset=ref_dataset, batch_size=batch_size, shuffle=False)\n\n        # Extract celltype and feature info\n        self.labels = ref_data.uns['cell_types']\n        self.celltype_num = len(self.labels)\n        self.used_features = list(ref_data.var_names)\n\n        # Target dataset\n        self.target_data_x = target_data.X.astype(np.float32)\n        self.target_data_y = np.random.rand(target_data.shape[0], self.celltype_num)\n        te_data = torch.FloatTensor(self.target_data_x)\n        te_labels = torch.FloatTensor(self.target_data_y)\n        target_dataset = Data.TensorDataset(te_data, te_labels)\n        self.train_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=True)\n        self.test_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=False)\n\n    def train(self, ref_data, target_data):\n        ### prepare model structure ###\n        self.prepare_dataloader(ref_data, target_data, self.batch_size)\n        self.model_im = self.AEimpute_model(self.celltype_num).cuda()\n\n        ### setup optimizer ###\n        optimizer_im = torch.optim.Adam([{'params': self.encoder_im.parameters()},\n                                         {'params': self.predictor_im.parameters()},\n                                         {'params': self.decoder_im.parameters()}], lr=self.learning_rate)\n\n        metric_logger = defaultdict(list)\n\n        for epoch in range(self.num_epochs):\n            self.model_im.train()\n\n            train_target_iterator = iter(self.train_target_loader)\n            loss_epoch, pred_loss_epoch, recon_loss_epoch = 0., 0., 0.\n            for batch_idx, (ref_x, ref_y) in enumerate(self.train_ref_loader):\n                # get batch item of target\n                try:\n                    target_x, _ = next(train_target_iterator)\n                except StopIteration:\n                    train_target_iterator = iter(self.train_target_loader)\n                    target_x, _ = next(train_target_iterator)\n\n                X = torch.cat((ref_x, target_x))\n\n                embedding = self.encoder_im(X.cuda())\n                frac_pred = self.predictor_im(embedding)\n                recon_X = self.decoder_im(embedding)\n\n                # caculate loss \n                pred_loss = L1_loss(frac_pred[range(self.batch_size),], ref_y.cuda()) \n                pred_loss_epoch += pred_loss\n                rec_loss = Recon_loss(recon_X, X.cuda())\n                recon_loss_epoch += rec_loss\n                loss = rec_loss + pred_loss\n                loss_epoch += loss   \n\n                # update weights\n                optimizer_im.zero_grad()\n                loss.backward()\n                optimizer_im.step()\n\n            loss_epoch = loss_epoch/(batch_idx + 1)\n            metric_logger['cAE_loss'].append(loss_epoch)\n            pred_loss_epoch = pred_loss_epoch/(batch_idx + 1)\n            metric_logger['pred_loss'].append(pred_loss_epoch)\n            recon_loss_epoch = recon_loss_epoch/(batch_idx + 1)\n            metric_logger['recon_loss'].append(recon_loss_epoch)\n            if (epoch+1) % 10 == 0:\n                print('============= Epoch {:02d}/{:02d} in stage2 ============='.format(epoch + 1, self.num_epochs))\n                print(\"cAE_loss=%f, pred_loss=%f, recon_loss=%f\" % (loss_epoch, pred_loss_epoch, recon_loss_epoch))\n\n        ### Plot loss ###\n        SaveLossPlot(self.outdir, metric_logger, loss_type = ['cAE_loss','pred_loss','recon_loss'], output_prex = 'Loss_plot_stage2')\n\n        ### Save reconstruction data of ref and target ###\n        ref_recon_data = self.write_recon(ref_data)\n\n        return ref_recon_data\n\n    def write_recon(self, ref_data):\n        \n        self.model_im.eval()\n        \n        ref_recon, ref_label = None, None\n        for batch_idx, (x, y) in enumerate(self.test_ref_loader):\n            x_embedding = self.encoder_im(x.cuda())\n            x_prediction = self.predictor_im(x_embedding)\n            x_recon = self.decoder_im(x_embedding).detach().cpu().numpy()\n            labels = y.detach().cpu().numpy()\n            ref_recon = x_recon if ref_recon is None else np.concatenate((ref_recon, x_recon), axis=0)\n            ref_label = labels if ref_label is None else np.concatenate((ref_label, labels), axis=0)\n        ref_recon = pd.DataFrame(ref_recon, columns=self.used_features)\n        ref_label = pd.DataFrame(ref_label, columns=self.labels)\n\n        ref_recon_data = ad.AnnData(X=ref_recon.to_numpy(), obs=ref_label)\n        ref_recon_data.uns['cell_types'] = self.labels\n        ref_recon_data.var_names = self.used_features\n\n        ### Plot recon ref TSNE plot ###\n        # SavetSNEPlot(self.outdir, ref_recon_data, output_prex='AE_Recon_ref')\n        ### Plot recon ref TSNE plot using missing features ###\n        # sc.pp.filter_genes(ref_data, min_cells=0)\n        # missing_features = list(ref_data.var[ref_data.var['n_cells']==0].index)\n        # if len(missing_features) > 0:\n        #     Recon_ref_data_new = ref_recon_data[:,missing_features]\n        #     SavetSNEPlot(self.outdir, Recon_ref_data_new, output_prex='AE_Recon_ref_missingfeature')\n\n        return ref_recon_data\n"}
{"type": "source_file", "path": "MACD_github/MACD/data_augmentation.py", "content": "import numba\nimport numpy as np\n\n@numba.njit\ndef random_dropout(cell_expr,max_rate):\n    non_zero_mask = np.where(cell_expr!=0)[0]\n    zero_mask = np.random.choice(non_zero_mask,int(len(non_zero_mask)*np.float32(np.random.uniform(0,max_rate))))\n    cell_expr[zero_mask] = 0\n    return cell_expr\n\n@numba.njit\ndef random_scale(cell_expr,max_val):\n    scale_factor = np.float32(1+np.random.uniform(-max_val,max_val))\n    cell_expr = cell_expr*scale_factor\n    return cell_expr\n\n@numba.njit\ndef random_shift(cell_expr,kth):\n    shift_value = np.random.choice(np.array([1,0,-1]),1)[0]*np.unique(cell_expr)[int(np.random.uniform(0,kth)*len(np.unique(cell_expr)))]\n    cell_expr[cell_expr != 0] = cell_expr[cell_expr != 0]+shift_value\n    cell_expr[cell_expr < 0] = 0\n    return cell_expr\n\n@numba.njit(parallel=True)\ndef random_augment(mtx,max_rate=0.8,max_val=0.8,kth=0.2):\n    for i in numba.prange(mtx.shape[0]):\n        random_dropout(mtx[i,:],max_rate=max_rate)\n        random_scale(mtx[i,:],max_val=max_val)\n        random_shift(mtx[i,:],kth=kth)\n    return mtx\n\n@numba.njit\ndef random_augmentation_cell(cell_expr,max_rate=0.8,max_val=0.8,kth=0.2):\n    cell_expr = random_dropout(cell_expr,max_rate=max_rate)\n    cell_expr = random_scale(cell_expr,max_val=max_val)\n    cell_expr = random_shift(cell_expr,kth=kth)\n    return cell_expr"}
{"type": "source_file", "path": "MACD_github/MACD/main_code.py", "content": "from collections import Counter\r\n\r\nimport anndata\r\nimport anndata as ad\r\nimport pandas as pd\r\n\r\nfrom MACD.MCCD_model import MDCD\r\nfrom MACD.data_prepare import *\r\nimport copy\r\ndef main(a,b,cell_key):\r\n    for i in range(a, b):\r\n        st_file = 'Datasets/Simulated_datasets\\dataset' + str(i) + '\\Spatial.h5ad'\r\n        sc_file = 'Datasets/Simulated_datasets\\dataset' + str(i) + '\\scRNA.h5ad'\r\n        st_data1 = ad.read_h5ad(st_file)\r\n        sc_data1 = ad.read_h5ad(sc_file)\r\n        sc_data=copy.deepcopy(sc_data1)\r\n        st_data=copy.deepcopy(st_data1)\r\n        outfile = 'MACD\\Result\\dataset' + str(i)\r\n        datafile='Datasets/preproced_data\\dataset' + str(i)\r\n        if not os.path.exists(outfile):\r\n            os.makedirs(outfile)\r\n        if not os.path.exists(datafile):\r\n            os.makedirs(datafile)\r\n        \"\"\"数据处理\"\"\"\r\n        data_prepare(sc_ad=sc_data,st_ad=st_data,celltype_key=cell_key,h5ad_file_path=outfile,data_file_path=datafile,n_layers=2,n_latent=2048)\r\n        sc_adata = anndata.read_h5ad(outfile + '\\sm_scvi_ad.h5ad')\r\n        st_adata = anndata.read_h5ad(outfile + '\\st_scvi_ad.h5ad')\r\n        real_sc_adata = anndata.read_h5ad(datafile + '\\Scdata_filter.h5ad')\r\n        sm_labelad = anndata.read_h5ad(datafile + '\\Sm_STdata_filter.h5ad')\r\n        # sm_data=pd.DataFrame(data=sc_adata.X.toarray(),columns=sc_adata.var_names)\r\n        sm_data = pd.DataFrame(data=sc_adata.X, columns=sc_adata.var_names)\r\n        sm_lable = sm_labelad.obsm['label']\r\n        st_data = pd.DataFrame(data=st_adata.X, columns=st_adata.var_names)\r\n        print(st_data.shape,sm_data.shape,sm_lable.shape)\r\n        count_ct_dict = Counter(list(real_sc_adata.obs[cell_key]))\r\n        celltypenum = len(count_ct_dict)\r\n\r\n\r\n        print(\"------Start Running Stage------\")\r\n        model_da = MDCD(celltypenum, outdirfile=outfile, used_features=list(sm_data.columns),num_epochs=200)\r\n        # model_da.d\r\n        model_da.double_train(sm_data=sm_data, st_data=st_data, sm_label=sm_lable)\r\n        final_preds_target = model_da.prediction()\r\n        final_preds_target.to_csv(outfile + '/final_pro1.csv')\r\n        final_preds_target.columns = sm_lable.columns.tolist()\r\n        pd.DataFrame(data=final_preds_target).to_csv(outfile + '/final_pro.csv')\r\n"}
{"type": "source_file", "path": "MACD_github/MACD/data_prepare.py", "content": "import os\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom scipy.sparse import csr_matrix\r\nimport numba\r\nimport logging\r\nimport random\r\nimport MACD.simulation_st\r\nfrom MACD.utils import *\r\nimport scvi\r\n\r\ndef get_scvi_latent(\r\n        st_ad,\r\n        sm_ad,\r\n        n_layers,\r\n        n_latent,\r\n        gene_likelihood='zinb',\r\n        dispersion='gene-batch',\r\n        max_epochs=1000,\r\n        early_stopping=True,\r\n        batch_size=4096,\r\n):\r\n\r\n\r\n    st_ad.obs[\"batch\"] = 'real'\r\n    sm_ad.obs[\"batch\"] = 'simulated'\r\n\r\n    adata = sc.concat([st_ad,sm_ad])\r\n    adata.layers[\"counts\"] = adata.X.copy()\r\n\r\n    scvi.model.SCVI.setup_anndata(\r\n        adata,\r\n        layer=\"counts\",\r\n        batch_key=\"batch\"\r\n    )\r\n\r\n    vae = scvi.model.SCVI(adata, n_layers=n_layers, n_latent=n_latent, gene_likelihood=gene_likelihood,\r\n                          dispersion=dispersion)\r\n    vae.train(max_epochs=max_epochs, early_stopping=early_stopping, batch_size=batch_size, use_gpu=True)\r\n    adata.obsm[\"X_scVI\"] = vae.get_latent_representation()\r\n    # print(\"现在的adata.shape\", adata.shape)\r\n    st_scvi_ad = anndata.AnnData(adata[adata.obs['batch'] != 'simulated'].obsm[\"X_scVI\"])\r\n    sm_scvi_ad = anndata.AnnData(adata[adata.obs['batch'] == 'simulated'].obsm[\"X_scVI\"])\r\n\r\n    st_scvi_ad.obs = st_ad.obs\r\n    st_scvi_ad.obsm = st_ad.obsm\r\n\r\n    sm_scvi_ad.obs = sm_ad.obs\r\n    sm_scvi_ad.obsm = sm_ad.obsm\r\n\r\n    sm_scvi_ad = check_data_type(sm_scvi_ad)\r\n    st_scvi_ad = check_data_type(st_scvi_ad)\r\n\r\n    sm_data = sm_scvi_ad.X\r\n    sm_labels = sm_scvi_ad.obsm['label'].values\r\n    st_data = st_scvi_ad.X\r\n    return sm_scvi_ad, st_scvi_ad\r\n\r\n    return sm_scvi_ad, st_scvi_ad\r\ndef data_prepare(\r\n    sc_ad,\r\n    st_ad,\r\n    celltype_key,\r\n    h5ad_file_path,\r\n    data_file_path,\r\n    n_layers,\r\n    n_latent,\r\n    deg_method:str='wilcoxon',\r\n    n_top_markers:int=200,\r\n    n_top_hvg:int=None,\r\n    log2fc_min=0.5,\r\n    pval_cutoff=0.01,\r\n    pct_diff=None,\r\n    pct_min=0.1,\r\n    sm_size:int=10000,\r\n    cell_counts=None,\r\n    clusters_mean=None,\r\n    cells_mean=10,\r\n    cells_min=1,\r\n    cells_max=20,\r\n    cell_sample_counts=None,\r\n    cluster_sample_counts=None,\r\n    ncell_sample_list=None,\r\n    cluster_sample_list=None,\r\n    n_threads=4,\r\n        #seed=42,82\r\n    seed=42,\r\n\r\n):\r\n    print('Setting global seed:', seed)\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n    torch.backends.cudnn.benchmark = False\r\n    MACD.simulation_st.numba_set_seed(seed)\r\n    numba.set_num_threads(n_threads)\r\n\r\n    sc_ad = normalized_data(sc_ad, target_sum=1e4)\r\n    st_ad = normalized_data(st_ad, target_sum=1e4)\r\n    print(\"原始st的形状：\",st_ad.shape)\r\n    sc_ad, st_ad = filter_model_genes(\r\n        sc_ad,\r\n        st_ad,\r\n        celltype_key=celltype_key,\r\n        deg_method=deg_method,\r\n        n_top_markers=n_top_markers,\r\n        n_top_hvg=n_top_hvg,\r\n        log2fc_min=log2fc_min,\r\n        pval_cutoff=pval_cutoff,\r\n        pct_diff=pct_diff,\r\n        pct_min=pct_min\r\n    )\r\n    print(\"筛选后st的形状：\", st_ad.shape)\r\n    \"\"\"产生模拟数据并进行下采样\"\"\"\r\n    sm_ad =generate_sm_stdata(sc_ad,num_sample=sm_size,celltype_key=celltype_key,n_threads=n_threads,cell_counts=cell_counts,clusters_mean=clusters_mean,cells_mean=cells_mean,\r\n                             cells_min=cells_min,cells_max=cells_max,cell_sample_counts=cell_sample_counts,cluster_sample_counts=cluster_sample_counts,\r\n                             ncell_sample_list=ncell_sample_list,cluster_sample_list=cluster_sample_list)\r\n\r\n    downsample_sm_spot_counts(sm_ad,st_ad,n_threads=n_threads)\r\n    os.makedirs(h5ad_file_path, exist_ok=True)\r\n    sc_adcopy = sc_ad\r\n    if 'cluster_p_balance' in sc_adcopy.uns:\r\n        del sc_adcopy.uns['cluster_p_balance']\r\n        del sc_adcopy.uns['cluster_p_sqrt']\r\n        del sc_adcopy.uns['cluster_p_unbalance']\r\n    \"\"\"将scdata和stdata进行预处理后存储，以便其他Baselines使用\"\"\"\r\n    sc_adcopy.write_h5ad(data_file_path + '\\Scdata_filter.h5ad')\r\n    sm_ad.write_h5ad(data_file_path + '\\Sm_STdata_filter.h5ad')\r\n    st_ad.write_h5ad(data_file_path + '\\Real_STdata_filter.h5ad')\r\n    \"\"\"对scdata和stdata利用scvi工具进行降维\"\"\"\r\n    sm_scvi_ad, st_scvi_ad = get_scvi_latent(st_ad,sm_ad,n_layers,n_latent)\r\n    sm_scvi_ad.write_h5ad(h5ad_file_path + '\\sm_scvi_ad.h5ad')\r\n    st_scvi_ad.write_h5ad(h5ad_file_path + '\\st_scvi_ad.h5ad')\r\n\r\n\r\n"}
{"type": "source_file", "path": "MACD_github/MACD/data_downsample.py", "content": "import numba\nimport numpy as np\nimport multiprocessing as mp\nfrom functools import partial\nimport random\n\n# Cite from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=True)\ndef get_bin_edges(a, bins):\n    bin_edges = np.zeros((bins+1,), dtype=np.float32)\n    a_min = a.min()\n    a_max = a.max()\n    delta = (a_max - a_min) / bins\n    for i in numba.prange(bin_edges.shape[0]):\n        bin_edges[i] = a_min + i * delta\n\n    bin_edges[-1] = a_max  # Avoid roundoff error on last point\n    return bin_edges\n\n# Modified from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=False)\ndef compute_bin(x, bin_edges):\n    # assuming uniform bins for now\n    n = bin_edges.shape[0] - 1\n    a_max = bin_edges[-1]\n    # special case to mirror NumPy behavior for last bin\n    if x == a_max:\n        return n - 1 # a_max always in last bin\n    bin = np.searchsorted(bin_edges, x)-1\n    if bin < 0 or bin >= n:\n        return None\n    else:\n        return bin\n\n# Modified from https://github.com/numba/numba-examples\n@numba.jit(nopython=True, parallel=False)\ndef numba_histogram(a, bin_edges):\n    hist = np.zeros((bin_edges.shape[0] - 1,), dtype=np.intp)\n    for x in a.flat:\n        bin = compute_bin(x, bin_edges)\n        if bin is not None:\n            hist[int(bin)] += 1\n    return hist, bin_edges\n\n\n# Modified from https://rdrr.io/bioc/scRecover/src/R/countsSampling.R\n# Downsample cell reads to a fraction\n@numba.jit(nopython=True, parallel=True)\ndef downsample_cell(cell_counts,fraction):\n    n = np.floor(np.sum(cell_counts) * fraction)\n    readsGet = np.sort(np.random.choice(np.arange(np.sum(cell_counts)), np.intp(n), replace=False))\n    cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n    counts_new = numba_histogram(readsGet,cumCounts)[0]\n    counts_new = counts_new.astype(np.float32)\n    return counts_new\n\ndef downsample_cell_python(cell_counts,fraction):\n    n = np.floor(np.sum(cell_counts) * fraction)\n    readsGet = np.sort(random.sample(range(np.intp(np.sum(cell_counts))), np.intp(n)))\n    cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n    counts_new = numba_histogram(readsGet,cumCounts)[0]\n    counts_new = counts_new.astype(np.float32)\n    return counts_new\n\n@numba.jit(nopython=True, parallel=True)\ndef downsample_per_cell(cell_counts,new_cell_counts):\n    n = new_cell_counts\n    if n < np.sum(cell_counts):\n        readsGet = np.sort(np.random.choice(np.arange(np.sum(cell_counts)), np.intp(n), replace=False))\n        cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n        counts_new = numba_histogram(readsGet,cumCounts)[0]\n        counts_new = counts_new.astype(np.float32)\n        return counts_new\n    else:\n        return cell_counts.astype(np.float32)\n\ndef downsample_per_cell_python(param):\n    cell_counts,new_cell_counts = param[0],param[1]\n    n = new_cell_counts\n    if n < np.sum(cell_counts):\n        readsGet = np.sort(random.sample(range(np.intp(np.sum(cell_counts))), np.intp(n)))\n        cumCounts = np.concatenate((np.array([0]),np.cumsum(cell_counts)))\n        counts_new = numba_histogram(readsGet,cumCounts)[0]\n        counts_new = counts_new.astype(np.float32)\n        return counts_new\n    else:\n        return cell_counts.astype(np.float32)\n\ndef downsample_matrix_by_cell(matrix,per_cell_counts,n_cpus=None,numba_end=True):\n    print('n_cpus:',n_cpus)\n    if numba_end:\n        downsample_func = downsample_per_cell\n    else:\n        downsample_func = downsample_per_cell_python\n    if n_cpus is not None:\n        # 使用单线程处理\n        matrix_ds = [downsample_func(x) for x in zip(matrix, per_cell_counts)]\n\n        # with mp.Pool(n_cpus) as p:\n        #     matrix_ds = p.map(downsample_func, zip(matrix,per_cell_counts))\n    else:\n        matrix_ds = [downsample_func(c,per_cell_counts[i]) for i,c in enumerate(matrix)]\n    return np.array(matrix_ds)\n\n# ps. slow speed.\ndef downsample_matrix_total(matrix,fraction):\n    matrix_flat = matrix.reshape(-1)\n    matrix_flat_ds = downsample_cell(matrix_flat,fraction)\n    matrix_ds = matrix_flat_ds.reshape(matrix.shape)\n    return matrix_ds\n\n"}
{"type": "source_file", "path": "MACD_github/MACD/utils.py", "content": "\"\"\"对scdata和stdata数据进行过滤\"\"\"\r\nimport os\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport scanpy as sc\r\nfrom matplotlib import pyplot as plt\r\n\r\nimport MACD.simulation_st as simulation_st\r\nimport anndata\r\n\r\nfrom scipy.sparse import issparse,csr_matrix\r\nfrom sklearn.preprocessing import normalize\r\nfrom rpy2.robjects.packages import importr\r\nimport rpy2.robjects as robjects\r\n\r\nfrom MACD import data_downsample\r\n\r\n\r\ndef normalized_data(ad,target_sum=None):\r\n    ad_norm = sc.pp.normalize_total(ad,inplace=False,target_sum=1e4)\r\n    ad_norm  = sc.pp.log1p(ad_norm['X'])\r\n    # ad_norm  = sc.pp.scale(ad_norm)\r\n    # ad_norm = normalize(ad_norm,axis=1)\r\n    ad.layers['norm'] = ad_norm\r\n    return ad\r\ndef filter_model_genes(\r\n    sc_ad,\r\n    st_ad,\r\n    celltype_key,\r\n    layer='norm',\r\n    deg_method=None,\r\n    log2fc_min=0.5,\r\n    pval_cutoff=0.01,\r\n    n_top_markers=500,\r\n    n_top_hvg=None,\r\n    pct_diff=None,\r\n    pct_min=0.1,\r\n):\r\n    # Remove duplicate genes from st_ad\r\n    if len(set(st_ad.var_names)) != len(st_ad.var_names):\r\n        print(\"Removing duplicate genes from st_ad\")\r\n        # Create a boolean mask where True indicates the first occurrence of each gene\r\n        mask = ~st_ad.var_names.duplicated()\r\n        st_ad = st_ad[:, mask].copy()\r\n\r\n    # Compute overlapping genes\r\n    overlaped_genes = np.intersect1d(sc_ad.var_names, st_ad.var_names)\r\n\r\n    sc_ad = sc_ad[:,overlaped_genes].copy()\r\n    st_ad = st_ad[:,overlaped_genes].copy()\r\n\r\n    if n_top_hvg is None:\r\n        st_genes = st_ad.var_names\r\n    else:\r\n        sc.pp.highly_variable_genes(st_ad, n_top_genes=n_top_hvg, flavor='seurat_v3')\r\n        st_genes = st_ad.var_names[st_ad.var['highly_variable'] == True]\r\n\r\n    sc_ad = sc_ad[:, st_genes].copy()\r\n    sc_genes = find_sc_markers(sc_ad, celltype_key, layer, deg_method, log2fc_min, pval_cutoff, n_top_markers, pct_diff, pct_min)\r\n    used_genes = np.intersect1d(sc_genes,st_genes)\r\n    sc_ad = sc_ad[:,used_genes].copy()\r\n    st_ad = st_ad[:,used_genes].copy()\r\n    sc.pp.filter_cells(sc_ad, min_genes=1)\r\n    sc.pp.filter_cells(st_ad,min_genes=1)\r\n    # print(f'### This Sample Used gene numbers is: {len(used_genes)}')\r\n    print(f'### This Sample Used gene numbers is: {len(used_genes)}')\r\n    return sc_ad, st_ad\r\n\r\n\r\ndef find_sc_markers(sc_ad, celltype_key, layer='norm', deg_method=None, log2fc_min=0.5, pval_cutoff=0.01,\r\n                    n_top_markers=500, pct_diff=None, pct_min=0.1):\r\n    print('### Finding marker genes...')\r\n    # filter celltype contain only one sample.\r\n    filtered_celltypes = list(\r\n        sc_ad.obs[celltype_key].value_counts()[(sc_ad.obs[celltype_key].value_counts() == 1).values].index)\r\n    if len(filtered_celltypes) > 0:\r\n        sc_ad = sc_ad[sc_ad.obs[~(sc_ad.obs[celltype_key].isin(filtered_celltypes))].index, :].copy()\r\n    sc.tl.rank_genes_groups(sc_ad, groupby=celltype_key, pts=True, layer=layer, use_raw=False, method=deg_method)\r\n    marker_genes_dfs = []\r\n    for c in np.unique(sc_ad.obs[celltype_key]):\r\n        tmp_marker_gene_df = sc.get.rank_genes_groups_df(sc_ad, group=c, pval_cutoff=pval_cutoff, log2fc_min=log2fc_min)\r\n        if (tmp_marker_gene_df.empty is not True):\r\n            tmp_marker_gene_df.index = tmp_marker_gene_df.names\r\n            tmp_marker_gene_df.loc[:, celltype_key] = c\r\n            if pct_diff is not None:\r\n                pct_diff_genes = sc_ad.var_names[np.where((sc_ad.uns['rank_genes_groups']['pts'][c] -\r\n                                                           sc_ad.uns['rank_genes_groups']['pts_rest'][c]) > pct_diff)]\r\n                tmp_marker_gene_df = tmp_marker_gene_df.loc[np.intersect1d(pct_diff_genes, tmp_marker_gene_df.index), :]\r\n            if pct_min is not None:\r\n                # pct_min_genes = sc_ad.var_names[np.where((sc_ad.uns['rank_genes_groups']['pts'][c]) > pct_min)]\r\n                tmp_marker_gene_df = tmp_marker_gene_df[tmp_marker_gene_df['pct_nz_group'] > pct_min]\r\n            if n_top_markers is not None:\r\n                tmp_marker_gene_df = tmp_marker_gene_df.sort_values('logfoldchanges', ascending=False)\r\n                tmp_marker_gene_df = tmp_marker_gene_df.iloc[:n_top_markers, :]\r\n            marker_genes_dfs.append(tmp_marker_gene_df)\r\n    marker_gene_df = pd.concat(marker_genes_dfs, axis=0)\r\n    print(marker_gene_df[celltype_key].value_counts())\r\n    all_marker_genes = np.unique(marker_gene_df.names)\r\n    return all_marker_genes\r\n\r\ndef generate_sm_stdata(sc_ad,num_sample,celltype_key,n_threads,cell_counts,clusters_mean,cells_mean,cells_min,cells_max,cell_sample_counts,cluster_sample_counts,ncell_sample_list,cluster_sample_list):\r\n    sm_data,sm_labels = simulation_st.generate_simulation_data(sc_ad,num_sample=num_sample,celltype_key=celltype_key,downsample_fraction=None,data_augmentation=False,n_cpus=n_threads,\r\n                                                               cell_counts=cell_counts,clusters_mean=clusters_mean,cells_mean=cells_mean,cells_min=cells_min,\r\n                                                               cells_max=cells_max,cell_sample_counts=cell_sample_counts,cluster_sample_counts=cluster_sample_counts,\r\n                                                               ncell_sample_list=ncell_sample_list,cluster_sample_list=cluster_sample_list)\r\n    sm_data_mtx = csr_matrix(sm_data)\r\n    sm_ad = anndata.AnnData(sm_data_mtx)\r\n    sm_ad.var.index = sc_ad.var_names\r\n    sm_labels = (sm_labels.T/sm_labels.sum(axis=1)).T\r\n    sm_ad.obsm['label'] = pd.DataFrame(sm_labels,columns=np.array(sc_ad.obs[celltype_key].value_counts().index.values),index=sm_ad.obs_names)\r\n    return sm_ad\r\n\r\ndef downsample_sm_spot_counts(sm_ad,st_ad,n_threads):\r\n    fitdistrplus = importr('fitdistrplus')\r\n    lib_sizes = robjects.FloatVector(np.array(st_ad.X.sum(1)).reshape(-1))\r\n    res = fitdistrplus.fitdist(lib_sizes,'lnorm')\r\n    loc = res[0][0]\r\n    scale = res[0][1]\r\n    sm_mtx_count = sm_ad.X.toarray()\r\n    sample_cell_counts = np.random.lognormal(loc,scale,sm_ad.shape[0])\r\n    sm_mtx_count_lb = data_downsample.downsample_matrix_by_cell(sm_mtx_count,sample_cell_counts.astype(np.int64), n_cpus=n_threads, numba_end=False)\r\n    sm_ad.X = csr_matrix(sm_mtx_count_lb)\r\n\r\n\r\ndef check_data_type(ad):\r\n    if issparse(ad.X):\r\n        ad.X = ad.X.toarray()\r\n    if ad.X.dtype != np.float32:\r\n        ad.X =ad.X.astype(np.float32)\r\n    return ad\r\n\r\ndef SaveLossPlot(SavePath, metric_logger, loss_type, output_prex):\r\n    if not os.path.exists(SavePath):\r\n        os.mkdir(SavePath)\r\n    for i in range(len(loss_type)):\r\n        plt.subplot(3, 3, i+1)\r\n        plt.plot(metric_logger[loss_type[i]])\r\n        plt.title(loss_type[i], x = 0.5, y = 0.5)\r\n    imgName = os.path.join(SavePath, output_prex +'.png')\r\n    plt.savefig(imgName)\r\n    plt.close()"}
{"type": "source_file", "path": "MACD_github/MACD/MCCD_model.py", "content": "\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.utils.data as Data\r\nimport random\r\nimport torch.nn as nn\r\nfrom collections import defaultdict\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\nfrom pytorch_revgrad import RevGrad\r\nfrom MACD.utils import *\r\nimport torch\r\ntorch.autograd.set_detect_anomaly(True)\r\n\r\n\r\n\r\n\r\nclass Encoder(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Encoder, self).__init__()\r\n        in_dim, h_dim,out_dim=dim\r\n        self.layer = nn.Sequential(nn.Linear(in_dim, h_dim),\r\n                                   nn.LeakyReLU(0.2, inplace=True),\r\n                                   nn.LayerNorm(h_dim),\r\n                                   nn.Linear(h_dim, out_dim),\r\n                                   nn.LeakyReLU(0.2, inplace=True),\r\n                                   nn.LayerNorm(out_dim),\r\n                                   )\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n            for m in self.modules():\r\n                if isinstance(m, nn.Linear):\r\n                    nn.init.xavier_normal_(m.weight.data)\r\n                    if m.bias is not None:\r\n                        m.bias.data.zero_()\r\n\r\n    def forward(self, x):\r\n        out = self.layer(x.cuda())\r\n        return out\r\n\r\n\r\nclass Decoder(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Decoder, self).__init__()\r\n        in_dim, h_dim, out_dim = dim\r\n        self.layer = nn.Sequential(\r\n            nn.Linear(in_dim, h_dim),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.LayerNorm(h_dim),\r\n            nn.Linear(h_dim, h_dim),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.LayerNorm(h_dim),\r\n            nn.Linear(h_dim, out_dim),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.LayerNorm(out_dim),\r\n                                   )\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n            for m in self.modules():\r\n                if isinstance(m, nn.Linear):\r\n                    nn.init.xavier_normal_(m.weight.data)\r\n                    if m.bias is not None:\r\n                        m.bias.data.zero_()\r\n\r\n    def forward(self, x):\r\n        out = self.layer(x)\r\n        return out\r\n\r\nclass Predictor(nn.Module):\r\n    def __init__(self,in_dim, out_dim):\r\n        super(Predictor, self).__init__()\r\n        self.layer = nn.Sequential(\r\n            nn.Linear(in_dim, 256),\r\n            nn.LeakyReLU(),\r\n            nn.LayerNorm(256),\r\n            nn.Linear(256, out_dim),\r\n            nn.Softmax(dim=1)\r\n        )\r\n        self.init_weights()\r\n    def init_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Linear):\r\n                nn.init.xavier_normal_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\nclass Discriminator(nn.Module):\r\n    def __init__(self,dim):\r\n        super(Discriminator, self).__init__()\r\n        in_dim, h_dim,out_dim=dim\r\n        self.layer = nn.Sequential(\r\n            nn.Linear(in_dim, h_dim),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.LayerNorm(h_dim),\r\n\r\n            nn.Linear(h_dim, out_dim),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.LayerNorm(out_dim),\r\n            nn.Sigmoid()\r\n                                   )\r\n        self.init_weights()\r\n\r\n\r\n\r\n    def init_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Linear):\r\n                nn.init.xavier_normal_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n    def forward(self, x):\r\n        out = self.layer(x)\r\n        return out\r\n\r\n\r\nclass Classifier(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Classifier, self).__init__()\r\n        in_dim, h_dim, out_dim = dim\r\n        self.layer = nn.Sequential(nn.Linear(in_dim, h_dim),\r\n                                   nn.LeakyReLU(0.2, inplace=True),\r\n                                   nn.LayerNorm(h_dim),\r\n                                   nn.Linear(h_dim, out_dim),\r\n                                   nn.LeakyReLU(0.2, inplace=True),\r\n                                   nn.LayerNorm(out_dim),\r\n                                   nn.Sigmoid()\r\n                                   )\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Linear):\r\n                nn.init.xavier_normal_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n    def forward(self, x):\r\n        out = self.layer(x)\r\n        return out\r\nclass MDCD(nn.Module):\r\n    def __init__(self, celltype_num, outdirfile, used_features,num_epochs):\r\n        super(MDCD, self).__init__()\r\n        self.num_epochs_new =num_epochs\r\n        self.batch_size = 2048\r\n        self.target_type = \"real\"\r\n        self.learning_rate = 0.01\r\n        self.celltype_num = celltype_num\r\n        self.labels = None\r\n        self.used_features = used_features\r\n        # self.seed = 2021,150,1700,3000\r\n        self.seed = 2021\r\n        self.outdir = outdirfile\r\n        cudnn.deterministic = True\r\n        torch.cuda.manual_seed_all(self.seed)\r\n        torch.manual_seed(self.seed)\r\n        random.seed(self.seed)\r\n        feature_num = len(self.used_features)\r\n        dim=[feature_num, 1024,512]\r\n        self.encoder_da = Encoder(dim).cuda()\r\n        dim1=[512,1024,feature_num]\r\n        self.decoder_da = Decoder(dim1).cuda()\r\n        self.predictor_da =Predictor(512,celltype_num).cuda()\r\n        dim2=[256,64,1]\r\n        self.Discriminator = Discriminator(dim2).cuda()\r\n        dim3=[256,128,1]\r\n        self.Classifier = Classifier(dim3).cuda()\r\n    def forward(self, x, lamda=1):\r\n        self.revgrad = RevGrad(lamda).cuda()\r\n        x = x.cuda()\r\n        embedding_source = self.encoder_da(x)\r\n        con_source = self.decoder_da(embedding_source)\r\n        pro = self.predictor_da(embedding_source)\r\n        znoise = embedding_source[:, :256]\r\n        zbio = embedding_source[:, 256:]\r\n        clas_out = self.Classifier(zbio)\r\n        disc_out = self.Discriminator(self.revgrad(znoise))\r\n        return embedding_source, con_source, pro, clas_out, disc_out\r\n\r\n    def prepare_dataloader(self, sm_data, sm_label, st_data, batch_size):\r\n        self.source_data_x = sm_data.values.astype(np.float32)\r\n        self.source_data_y = sm_label.values.astype(np.float32)\r\n        tr_data = torch.FloatTensor(self.source_data_x)\r\n        tr_labels = torch.FloatTensor(self.source_data_y)\r\n        source_dataset = Data.TensorDataset(tr_data, tr_labels)\r\n        self.train_source_loader = Data.DataLoader(dataset=source_dataset, batch_size=batch_size, shuffle=True)\r\n        self.used_features = list(sm_data.columns)\r\n        self.target_data_x = torch.from_numpy(st_data.values.astype(np.float32))\r\n        if self.target_type == \"real\":\r\n            target_ratios = self.target_data_y = np.random.rand(st_data.shape[0], sm_label.shape[1])\r\n            self.target_data_y = np.array(target_ratios, dtype=np.float32)\r\n        else:\r\n            print(\"target_type类型错误\")\r\n        te_data = torch.FloatTensor(self.target_data_x)\r\n        te_labels = torch.FloatTensor(self.target_data_y)\r\n        target_dataset = Data.TensorDataset(te_data, te_labels)\r\n        self.train_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=True)\r\n        self.test_target_loader = Data.DataLoader(dataset=target_dataset, batch_size=batch_size, shuffle=False)\r\n\r\n    def mask_features(self, X, mask_ratio):\r\n        if isinstance(X, np.ndarray):\r\n            mask = np.random.choice([True, False], size=X.shape, p=[mask_ratio, 1 - mask_ratio])\r\n        elif isinstance(X, torch.Tensor):\r\n            mask = torch.rand(X.shape) < mask_ratio\r\n        else:\r\n            raise TypeError(\"type error!\")\r\n        use_x = X.clone()  # 使用 X 的副本以避免改变原始数据\r\n        use_x[mask] = 0\r\n        return use_x, mask\r\n\r\n    def prediction(self):\r\n        self.eval()\r\n        preds = None\r\n        for batch_idx, (x, y) in enumerate(self.test_target_loader):\r\n            x = x.cuda()\r\n            embedding_source, con_source, pro, clas_out, disc_out = self.forward(x)\r\n            logits = pro.detach().cpu().numpy()\r\n            preds = logits if preds is None else np.concatenate((preds, logits), axis=0)\r\n        target_preds = pd.DataFrame(preds, columns=self.labels)\r\n        model_and_settings = {\r\n            'model': self.state_dict(),\r\n            'seed': self.seed\r\n        }\r\n        torch.save(model_and_settings, self.outdir + '/model_with_settings.pth')\r\n        return target_preds\r\n\r\n    def double_train(self, sm_data, sm_label, st_data):\r\n        self.train()\r\n        self.prepare_dataloader(sm_data, sm_label, st_data, self.batch_size)\r\n        self.optim = torch.optim.Adam([{'params': self.encoder_da.parameters()},\r\n                                          {'params': self.decoder_da.parameters()},], lr=self.learning_rate)\r\n        self.optim1 = torch.optim.Adam([{'params': self.encoder_da.parameters()},\r\n                                          {'params': self.predictor_da.parameters()},], lr=self.learning_rate)\r\n        self.optim_discriminator = torch.optim.Adam([{'params': self.encoder_da.parameters()},\r\n                                          {'params': self.Discriminator.parameters()},] lr=0.005)  # 判别器的学习率\r\n        self.optim_classifier = torch.optim.Adam([{'params': self.encoder_da.parameters()},\r\n                                          {'params': self.Classifier.parameters()},], lr=0.01)  # 分类器的学习率\r\n        criterion_da = nn.MSELoss().cuda()\r\n        metric_logger = defaultdict(list)\r\n        epsilon=0.01\r\n        for epoch in range(self.num_epochs_new):\r\n            train_target_iterator = iter(self.train_target_loader)\r\n            pred_loss_epoch, con_loss_epoch = 0., 0.\r\n            dis_loss_epoch_y, dis_loss_epoch = 0.0, 0.0\r\n            class_loss_epoch, class_loss_epoch_y = 0.0, 0.0\r\n            all_loss_epoch=0.0\r\n            for i in range(1):\r\n                for batch_idx, (source_x, source_y) in enumerate(self.train_source_loader):\r\n                    try:\r\n                        target_x, _ = next(train_target_iterator)\r\n                    except StopIteration:\r\n                        train_target_iterator = iter(self.train_target_loader)\r\n                        target_x, _ = next(train_target_iterator)\r\n\r\n                    use_x, mask = self.mask_features(source_x.cuda(), 0.3)\r\n                    source_x = source_x.cuda()  # 将source_x移动到CUDA设备\r\n\r\n                    embedding_source, con_source, pro, clas_out, disc_out = self.forward(source_x * (~mask.cuda()))\r\n                    embedding_source_y, con_source_y, pro_y, clas_out_y, disc_out_y = self.forward(target_x.cuda())\r\n\r\n                    con_loss = criterion_da(source_x * (~mask.cuda()), con_source * (~mask.cuda()))\r\n                    con_loss_epoch += con_loss.data.item()\r\n\r\n\r\n                    source_label = torch.ones(disc_out.shape[0]).unsqueeze(1).cuda()  # 定义source domain label为1\r\n                    source_label1 = source_label * (1 - epsilon) + (epsilon / 2)\r\n\r\n                    target_label_y = torch.zeros(clas_out_y.shape[0]).unsqueeze(1).cuda()  # 定义target domain label为0\r\n                    target_label_y1 = target_label_y * (1 - epsilon) + (epsilon / 2)\r\n                    clas_loss = nn.BCELoss()(clas_out, source_label1)\r\n                    dis_loss = nn.BCELoss()(disc_out, source_label1)\r\n                    clas_loss_y = nn.BCELoss()(clas_out_y, target_label_y1)\r\n                    dis_loss_y = nn.BCELoss()(disc_out_y, target_label_y1)\r\n\r\n                    dis_loss_epoch += dis_loss.data.item()\r\n                    dis_loss_epoch_y += dis_loss_y.data.item()\r\n                    class_loss_epoch += clas_loss.data.item()\r\n                    class_loss_epoch_y += clas_loss_y.data.item()\r\n                    # loss = 10 * con_loss + dis_loss + 1000 * dis_loss_y + clas_loss + 1000 * clas_loss_y\r\n                    loss = con_loss  + (dis_loss + dis_loss_y +clas_loss +clas_loss_y)\r\n                    all_loss_epoch += loss.data.item()\r\n                    self.optim.zero_grad()\r\n                    self.optim_discriminator.zero_grad()\r\n                    self.optim_classifier.zero_grad()\r\n                    loss.backward()\r\n                    self.optim.step()\r\n                    self.optim_discriminator.step()\r\n                    self.optim_classifier.step()\r\n                    torch.cuda.empty_cache()\r\n\r\n\r\n            for i in range(1):\r\n                for batch_idx, (source_x, source_y) in enumerate(self.train_source_loader):\r\n                    try:\r\n                        target_x, _ = next(train_target_iterator)\r\n                    except StopIteration:\r\n                        train_target_iterator = iter(self.train_target_loader)\r\n                        target_x, _ = next(train_target_iterator)\r\n\r\n                    source_x = source_x.cuda()  # 将source_x移动到CUDA设备\r\n\r\n                    embedding_source, con_source, pro, clas_out, disc_out = self.forward(source_x)\r\n                    pred_loss = criterion_da(source_y.cuda(), pro)\r\n                    pred_loss_epoch += pred_loss.data.item()\r\n                    # loss1=1000*pred_loss\r\n                    loss1 = pred_loss\r\n                    self.optim1.zero_grad()\r\n                    loss1.backward()\r\n                    self.optim1.step()\r\n                    torch.cuda.empty_cache()\r\n\r\n            pred_loss_epoch = pred_loss_epoch / (batch_idx + 1)\r\n            con_loss_epoch = con_loss_epoch / (batch_idx + 1)\r\n            dis_loss_epoch_y = dis_loss_epoch_y / (batch_idx + 1)\r\n            dis_loss_epoch = dis_loss_epoch / (batch_idx + 1)\r\n            all_loss_epoch = all_loss_epoch / (batch_idx + 1)\r\n            class_loss_epoch = class_loss_epoch / (batch_idx + 1)\r\n            class_loss_epoch_y = class_loss_epoch_y / (batch_idx + 1)\r\n            if epoch>0:\r\n                metric_logger['pre_loss'].append(pred_loss_epoch)\r\n\r\n\r\n                metric_logger['con_loss'].append(con_loss_epoch)\r\n\r\n\r\n                metric_logger['dis_loss_y'].append(dis_loss_epoch_y)\r\n\r\n\r\n                metric_logger['dis_loss'].append(dis_loss_epoch)\r\n\r\n                metric_logger['all_loss'].append(all_loss_epoch)\r\n\r\n\r\n                metric_logger['class_loss'].append(class_loss_epoch)\r\n\r\n\r\n                metric_logger['class_loss_y'].append(class_loss_epoch_y)\r\n\r\n\r\n            if (epoch+1) % 50== 0:\r\n                print(\r\n                    '============= Epoch {:02d}/{:02d} in stage ============='.format(epoch + 1, self.num_epochs_new))\r\n                print(\r\n                    \"pre_loss=%f, con_loss=%f, dis_loss_y=%f,dis_loss=%f,class_loss_y=%f, class_loss=%f,total_loss_DA=%f\" % (\r\n                        pred_loss_epoch, con_loss_epoch, dis_loss_epoch_y, dis_loss_epoch, class_loss_epoch_y,\r\n                        class_loss_epoch, all_loss_epoch))\r\n\r\n        if self.target_type == \"simulated\":\r\n            SaveLossPlot(self.outdir, metric_logger,\r\n                         loss_type=['pred_loss', 'disc_loss', 'disc_loss_DA', 'target_ccc', 'target_rmse',\r\n                                    'target_corr'], output_prex='Loss_metric_plot_stage3')\r\n        elif self.target_type == \"real\":\r\n            SaveLossPlot(self.outdir, metric_logger,\r\n                         loss_type=['pre_loss', 'con_loss', 'dis_loss_y', 'dis_loss', 'class_loss_y', 'class_loss',\r\n                                    'all_loss'],\r\n                         output_prex='Loss_metric_plot_stage')\r\n\r\n"}
{"type": "source_file", "path": "MACD_github/MACD/simulation_st.py", "content": "import numpy as np\nimport pandas as pd\nimport numba as nb\nfrom numba import jit\nimport collections\nimport random\nfrom .data_downsample import downsample_cell,downsample_matrix_by_cell\nfrom .data_augmentation import random_augment,random_augmentation_cell\nimport logging\n# logging.basicConfig(level=print,\n#                     format='%(asctime)s %(levelname)s %(message)s',\n#                     datefmt='%m-%d %H:%M')\n# logging.getLogger().setLevel(print)\n\n# 汇总每个spot的细胞数，统计细胞数的分布\ndef count_cell_counts(cell_counts):\n    cell_counts = np.array(cell_counts.values,dtype=int).reshape(-1)\n    counts_list = np.array(np.histogram(cell_counts,range=[0,np.max(cell_counts)+1],bins=np.max(cell_counts)+1)[0],dtype=int)\n    counts_index = np.array((np.histogram(cell_counts,range=[0,np.max(cell_counts)+1],bins=np.max(cell_counts)+1)[1][:-1]),dtype=int)\n    counts_df = pd.DataFrame(counts_list,index=counts_index,columns=['count'],dtype=np.int32)\n    counts_df = counts_df[(counts_df['count'] != 0) & (counts_df.index != 0)]\n    count_sum = 0\n    for i in np.arange(len(counts_df)):\n        count_sum += counts_df.iloc[i].values\n        if count_sum > counts_df.values.sum()*0.99:\n            counts_df_filtered = counts_df.iloc[:i+1,:]\n            break\n    return counts_df_filtered\n\n\n@nb.njit\ndef numba_set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n\n# 对某个axis调用numpy函数(numba版本)\n@nb.njit\ndef np_apply_along_axis(func1d, axis, arr):\n    assert arr.ndim == 2\n    assert axis in [0, 1]\n    if axis == 0:\n        result = np.empty(arr.shape[1], dtype=arr.dtype)\n        for i in range(len(result)):\n            result[i] = func1d(arr[:, i])\n    else:\n        result = np.empty(arr.shape[0], dtype=arr.dtype)\n        for i in range(len(result)):\n            result[i] = func1d(arr[i, :])\n    return result\n\n# 对某个axis计算均值(numba版本)\n@nb.njit\ndef np_mean(array, axis):\n    return np_apply_along_axis(np.mean, axis, array)\n\n# 对某个axis计算加和(numba版本)\n@nb.njit\ndef np_sum(array, axis):\n    return np_apply_along_axis(np.sum, axis, array)\n\n# 根据参数采样单细胞数据，生成模拟spot(numba版本)\n@jit(nopython=True,parallel=True)\ndef sample_cell(param_list,cluster_p,clusters,cluster_id,sample_exp,sample_cluster,cell_p_balanced,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(param_list), sample_exp.shape[1]),dtype=np.float32)\n    density = np.empty((len(param_list), sample_cluster.shape[1]),dtype=np.float32)\n\n    for i in nb.prange(len(param_list)):\n        params = param_list[i]\n        num_cell = params[0]\n        num_cluster = params[1]\n        used_clusters = clusters[np.searchsorted(np.cumsum(cluster_p), np.random.rand(num_cluster), side=\"right\")]\n        cluster_mask = np.array([False]*len(cluster_id))\n        for c in used_clusters:\n            cluster_mask = (cluster_id==c)|(cluster_mask)\n        # print('cluster_mask',cluster_mask)\n        # print('used_clusters',used_clusters)\n        used_cell_ind = np.where(cluster_mask)[0]\n        used_cell_p = cell_p_balanced[cluster_mask]\n        used_cell_p = used_cell_p/used_cell_p.sum()\n        sampled_cells = used_cell_ind[np.searchsorted(np.cumsum(used_cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp = np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float32)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        combined_clusters = np_sum(sample_cluster[cluster_id[sampled_cells]],axis=0).astype(np.float32)\n        exp[i,:] = combined_exp\n        density[i,:] = combined_clusters\n    return exp,density\n\n@jit(nopython=True,parallel=True)\ndef sample_cell_from_clusters(cluster_sample_list,ncell_sample_list,cluster_id,sample_exp,sample_cluster,cell_p_balanced,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(cluster_sample_list), sample_exp.shape[1]),dtype=np.float32)\n    density = np.empty((len(cluster_sample_list), sample_cluster.shape[1]),dtype=np.float32)\n    for i in nb.prange(len(cluster_sample_list)):\n        used_clusters = np.where(cluster_sample_list[i] == 1)[0]\n        num_cell = ncell_sample_list[i]\n        cluster_mask = np.array([False]*len(cluster_id))\n        for c in used_clusters:\n            cluster_mask = (cluster_id==c)|(cluster_mask)\n        used_cell_ind = np.where(cluster_mask)[0]\n        used_cell_p = cell_p_balanced[cluster_mask]\n        used_cell_p = used_cell_p/used_cell_p.sum()\n        sampled_cells = used_cell_ind[np.searchsorted(np.cumsum(used_cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp = np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float32)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        combined_clusters = np_sum(sample_cluster[cluster_id[sampled_cells]],axis=0).astype(np.float32)\n        exp[i,:] = combined_exp\n        density[i,:] = combined_clusters\n    return exp,density\n\ndef init_sample_prob(sc_ad,celltype_key):\n    print('### Initializing sample probability')\n    sc_ad.uns['celltype2num'] = pd.DataFrame(\n        np.arange(len(sc_ad.obs[celltype_key].value_counts())).T,\n        index=sc_ad.obs[celltype_key].value_counts().index.values,\n        columns=['celltype_num']\n    )\n    sc_ad.obs['celltype_num'] = [sc_ad.uns['celltype2num'].loc[c,'celltype_num'] for c in sc_ad.obs[celltype_key]]\n    cluster_p_unbalance = sc_ad.obs['celltype_num'].value_counts()/sc_ad.obs['celltype_num'].value_counts().sum()\n    cluster_p_sqrt = np.sqrt(sc_ad.obs['celltype_num'].value_counts())/np.sqrt(sc_ad.obs['celltype_num'].value_counts()).sum()\n    cluster_p_balance = pd.Series(\n        np.ones(len(sc_ad.obs['celltype_num'].value_counts()))/len(sc_ad.obs['celltype_num'].value_counts()), \n        index=sc_ad.obs['celltype_num'].value_counts().index\n    )\n#     cluster_p_balance = np.ones(len(sc_ad.obs['celltype_num'].value_counts()))/len(sc_ad.obs['celltype_num'].value_counts())\n    cell_p_balanced = [1/cluster_p_unbalance[c] for c in sc_ad.obs['celltype_num']]\n    cell_p_balanced = np.array(cell_p_balanced)/np.array(cell_p_balanced).sum()\n    sc_ad.obs['cell_p_balanced'] = cell_p_balanced\n    sc_ad.uns['cluster_p_balance'] = cluster_p_balance\n    sc_ad.uns['cluster_p_sqrt'] = cluster_p_sqrt\n    sc_ad.uns['cluster_p_unbalance'] = cluster_p_unbalance\n    return sc_ad\n\n# 将表达矩阵转化成array\ndef generate_sample_array(sc_ad, used_genes):\n    if used_genes is not None:\n        sc_df = sc_ad.to_df().loc[:,used_genes]\n    else:\n        sc_df = sc_ad.to_df()\n    return sc_df.values\n\n# 从均匀分布中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_uniform(num_sample,cells_min=None,cells_max=None,clusters_min=None,clusters_max=None):\n\n    cell_count = np.asarray(np.ceil(np.random.uniform(int(cells_min),int(cells_max),size=num_sample)),dtype=int)\n    cluster_count = np.asarray(np.ceil(np.clip(np.random.uniform(clusters_min,clusters_max,size=num_sample),1,cell_count)),dtype=int)\n    return cell_count, cluster_count\n\n# 从高斯分布中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_gaussian(num_sample,cells_min=None,cells_max=None,cells_mean=None,cells_std=None,clusters_mean=None,clusters_std=None):\n\n    cell_count = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    cluster_count = np.asarray(np.ceil(np.clip(np.random.normal(clusters_mean,clusters_std,size=num_sample),1,cell_count)),dtype=int)\n    return cell_count,cluster_count\n\n# 从用空间数据估计的cell counts中获取每个spot采样的细胞数和细胞类型数\ndef get_param_from_cell_counts(\n    num_sample,\n    cell_counts,\n    cluster_sample_mode='gaussian',\n    cells_min=None,cells_max=None,\n    cells_mean=None,cells_std=None,\n    clusters_mean=None,clusters_std=None,\n    clusters_min=None,clusters_max=None\n):\n    cell_count = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    if cluster_sample_mode == 'gaussian':\n        cluster_count = np.asarray(np.ceil(np.clip(np.random.normal(clusters_mean,clusters_std,size=num_sample),1,cell_count)),dtype=int)\n    elif cluster_sample_mode == 'uniform':\n        cluster_count = np.asarray(np.ceil(np.clip(np.random.uniform(clusters_min,clusters_max,size=num_sample),1,cell_count)),dtype=int)\n    else:\n        raise TypeError('Not correct sample method.')\n    return cell_count,cluster_count\n\n# 获取每个cluster的采样概率\ndef get_cluster_sample_prob(sc_ad,mode):\n    if mode == 'unbalance':\n        cluster_p = sc_ad.uns['cluster_p_unbalance'].values\n    elif mode == 'balance':\n        cluster_p = sc_ad.uns['cluster_p_balance'].values\n    elif mode == 'sqrt':\n        cluster_p = sc_ad.uns['cluster_p_sqrt'].values\n    else:\n        raise TypeError('Balance argument must be one of [ None, banlance, sqrt ].')\n    return cluster_p\n\ndef cal_downsample_fraction(sc_ad,st_ad,celltype_key=None):\n    st_counts_median = np.median(st_ad.X.sum(axis=1))\n    simulated_st_data, simulated_st_labels = generate_simulation_data(sc_ad,num_sample=10000,celltype_key=celltype_key,balance_mode=['unbalance'])\n    simulated_st_counts_median = np.median(simulated_st_data.sum(axis=1))\n    if st_counts_median < simulated_st_counts_median:\n        fraction = st_counts_median / simulated_st_counts_median\n        print(f'### Simulated data downsample fraction: {fraction}')\n        return fraction\n    else:\n        return None\n\n# 生成模拟数据\ndef generate_simulation_data(\n    sc_ad,\n    celltype_key,\n    num_sample: int, \n    used_genes=None,\n    balance_mode=['unbalance','sqrt','balance'],\n    cell_sample_method='gaussian',\n    cluster_sample_method='gaussian',\n    cell_counts=None,\n    downsample_fraction=None,\n    data_augmentation=True,\n    max_rate=0.8,max_val=0.8,kth=0.2,\n    cells_min=1,cells_max=20,\n    cells_mean=10,cells_std=5,\n    clusters_mean=None,clusters_std=None,\n    clusters_min=None,clusters_max=None,\n    cell_sample_counts=None,cluster_sample_counts=None,\n    ncell_sample_list=None,\n    cluster_sample_list=None,\n    n_cpus=None\n):\n    if not 'cluster_p_unbalance' in sc_ad.uns:\n        sc_ad = init_sample_prob(sc_ad,celltype_key)\n    num_sample_per_mode = num_sample//len(balance_mode)\n    cluster_ordered = np.array(sc_ad.obs['celltype_num'].value_counts().index)\n    cluster_num = len(cluster_ordered)\n    cluster_id = sc_ad.obs['celltype_num'].values\n    cluster_mask = np.eye(cluster_num)\n    if (cell_sample_counts is None) or (cluster_sample_counts is None):\n        if cell_counts is not None:\n            cells_mean = np.mean(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n            cells_std = np.std(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n            cells_min = int(np.min(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n            cells_max = int(np.max(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n        if clusters_mean is None:\n            clusters_mean = cells_mean/2\n        if clusters_std is None:\n            clusters_std = cells_std/2\n        if clusters_min is None:\n            clusters_min = cells_min\n        if clusters_max is None:\n            clusters_max = np.min((cells_max//2,cluster_num))\n\n        if cell_counts is not None:\n            cell_sample_counts, cluster_sample_counts = get_param_from_cell_counts(num_sample_per_mode,cell_counts,cluster_sample_method,cells_mean=cells_mean,cells_std=cells_std,cells_max=cells_max,cells_min=cells_min,clusters_mean=clusters_mean,clusters_std=clusters_std,clusters_min=clusters_min,clusters_max=clusters_max)\n        elif cell_sample_method == 'gaussian':\n            cell_sample_counts, cluster_sample_counts = get_param_from_gaussian(num_sample_per_mode,cells_mean=cells_mean,cells_std=cells_std,cells_max=cells_max,cells_min=cells_min,clusters_mean=clusters_mean,clusters_std=clusters_std)\n        elif cell_sample_method == 'uniform':\n            cell_sample_counts, cluster_sample_counts = get_param_from_uniform(num_sample_per_mode,cells_max=cells_max,cells_min=cells_min,clusters_min=clusters_min,clusters_max=clusters_max)\n        else:\n            raise TypeError('Not correct sample method.')\n    if cluster_sample_list is None or ncell_sample_list is None:\n        params = np.array(list(zip(cell_sample_counts, cluster_sample_counts)))\n\n        sample_data_list = []\n        sample_labels_list = []\n        for b in balance_mode:\n            print(f'### Genetating simulated spatial data using scRNA data with mode: {b}')\n            cluster_p = get_cluster_sample_prob(sc_ad,b)\n            if downsample_fraction is not None:\n                if downsample_fraction > 0.035:\n                    sample_data,sample_labels = sample_cell(\n                        param_list=params,\n                        cluster_p=cluster_p,\n                        clusters=cluster_ordered,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        downsample_fraction=downsample_fraction,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                else:\n                    sample_data,sample_labels = sample_cell(\n                        param_list=params,\n                        cluster_p=cluster_p,\n                        clusters=cluster_ordered,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                    # logging.warning('### Downsample data with python backend')\n                    sample_data = downsample_matrix_by_cell(sample_data, downsample_fraction, n_cpus=n_cpus, numba_end=False)\n            else:\n                sample_data,sample_labels = sample_cell(\n                    param_list=params,\n                    cluster_p=cluster_p,\n                    clusters=cluster_ordered,\n                    cluster_id=cluster_id,\n                    sample_exp=generate_sample_array(sc_ad,used_genes),\n                    sample_cluster=cluster_mask,\n                    cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                    data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                )\n    #         if data_augmentation:\n    #             sample_data = random_augment(sample_data)\n            sample_data_list.append(sample_data)\n            sample_labels_list.append(sample_labels)\n    else:\n        sample_data_list = []\n        sample_labels_list = []\n        for b in balance_mode:\n            print(f'### Genetating simulated spatial data using scRNA data with mode: {b}')\n            cluster_p = get_cluster_sample_prob(sc_ad,b)\n            if downsample_fraction is not None:\n                if downsample_fraction > 0.035:\n                    sample_data,sample_labels = sample_cell_from_clusters(\n                        cluster_sample_list=cluster_sample_list,\n                        ncell_sample_list=ncell_sample_list,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        downsample_fraction=downsample_fraction,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                else:\n                    sample_data,sample_labels = sample_cell_from_clusters(\n                        cluster_sample_list=cluster_sample_list,\n                        ncell_sample_list=ncell_sample_list,\n                        cluster_id=cluster_id,\n                        sample_exp=generate_sample_array(sc_ad,used_genes),\n                        sample_cluster=cluster_mask,\n                        cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                    )\n                    # logging.warning('### Downsample data with python backend')\n                    sample_data = downsample_matrix_by_cell(sample_data, downsample_fraction, n_cpus=n_cpus, numba_end=False)\n            else:\n                sample_data,sample_labels = sample_cell_from_clusters(\n                    cluster_sample_list=cluster_sample_list,\n                    ncell_sample_list=ncell_sample_list,\n                    cluster_id=cluster_id,\n                    sample_exp=generate_sample_array(sc_ad,used_genes),\n                    sample_cluster=cluster_mask,\n                    cell_p_balanced=sc_ad.obs['cell_p_balanced'].values,\n                    data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n                )\n            sample_data_list.append(sample_data)\n            sample_labels_list.append(sample_labels)\n    return np.concatenate(sample_data_list), np.concatenate(sample_labels_list)\n\n@jit(nopython=True,parallel=True)\ndef sample_cell_exp(cell_counts,sample_exp,cell_p,downsample_fraction=None,data_augmentation=True,max_rate=0.8,max_val=0.8,kth=0.2):\n    exp = np.empty((len(cell_counts), sample_exp.shape[1]),dtype=np.float32)\n    ind = np.zeros((len(cell_counts), np.max(cell_counts)),dtype=np.int32)\n    cell_ind = np.arange(sample_exp.shape[0])\n    for i in nb.prange(len(cell_counts)):\n        num_cell = cell_counts[i]\n        sampled_cells=cell_ind[np.searchsorted(np.cumsum(cell_p), np.random.rand(num_cell), side=\"right\")]\n        combined_exp=np_sum(sample_exp[sampled_cells,:],axis=0).astype(np.float64)\n#         print(combined_exp.dtype)\n        if downsample_fraction is not None:\n            combined_exp = downsample_cell(combined_exp, downsample_fraction)\n        if data_augmentation:\n            combined_exp = random_augmentation_cell(combined_exp,max_rate=max_rate,max_val=max_val,kth=kth)\n        exp[i,:] = combined_exp\n        ind[i,:cell_counts[i]] = sampled_cells + 1\n    return exp,ind\n\ndef generate_simulation_st_data(\n    st_ad,\n    num_sample: int, \n    used_genes=None,\n    balance_mode=['unbalance'],\n    cell_sample_method='gaussian',\n    cell_counts=None,\n    downsample_fraction=None,\n    data_augmentation=True,\n    max_rate=0.8,max_val=0.8,kth=0.2,\n    cells_min=1,cells_max=10,\n    cells_mean=5,cells_std=3,\n):\n    print('### Genetating simulated data using spatial data')\n    cell_p = np.ones(len(st_ad))/len(st_ad)\n    if cell_counts is not None:\n        cells_mean = np.mean(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n        cells_std = np.std(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)])\n        cells_min = int(np.min(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n        cells_max = int(np.max(np.sort(cell_counts)[int(len(cell_counts)*0.05):int(len(cell_counts)*0.95)]))\n    elif cell_sample_method == 'gaussian':\n        cell_counts = np.asarray(np.ceil(np.clip(np.random.normal(cells_mean,cells_std,size=num_sample),int(cells_min),int(cells_max))),dtype=int)\n    elif cell_sample_method == 'uniform':\n        cell_counts = np.asarray(np.ceil(np.random.uniform(int(cells_min),int(cells_max),size=num_sample)),dtype=int)\n    else:\n        raise TypeError('Not correct sample method.')\n\n    sample_data,sample_ind = sample_cell_exp(\n        cell_counts=cell_counts,\n        sample_exp=generate_sample_array(st_ad,used_genes),\n        cell_p=cell_p,\n        downsample_fraction=downsample_fraction,\n        data_augmentation=data_augmentation,max_rate=max_rate,max_val=max_val,kth=kth,\n    )\n#     if data_augmentation:\n#         sample_data = random_augment(sample_data)\n    return sample_data,sample_ind\n"}
{"type": "source_file", "path": "MACD_github/main.py", "content": "import gc\r\nimport MACD.main_code as MACD\r\nimport Baselines.scpDeconv.scpDeconv_main.main as scp\r\nimport Baselines.Cell2location.main_code as cell2location\r\nimport Baselines.DestVi.main_code as DestVi\r\nimport Baselines.Stereoscope.main_code as Stereoscope\r\nimport Baselines.Tangram.main_code as Tangram\r\nimport Baselines.Spoint.main_code as Spoint\r\ndef train_Tangram_model(i,cell_key):\r\n    print(\"-------------Tangram模型开始-----------------\")\r\n    Tangram.main(i, i + 1, cell_key)\r\n    clear_memory()\r\n\r\ndef train_Spoint_model(i,cell_key):\r\n    print(\"-------------Spoint模型开始-----------------\")\r\n    Spoint.main(i, i + 1, cell_key)\r\n    clear_memory()\r\ndef train_DestVi_model(i,cell_key):\r\n    print(\"-------------DestVi模型开始-----------------\")\r\n    DestVi.main(i, i + 1, cell_key)\r\n    clear_memory()\r\ndef train_Stereoscope_model(i,cell_key):\r\n    print(\"-------------Stereoscope模型开始-----------------\")\r\n    Stereoscope.main(i, i + 1, cell_key)\r\n    clear_memory()\r\ndef train_cell2location_model(i,cell_key):\r\n    print(\"-------------cell2location模型开始-----------------\")\r\n    cell2location.main(i, i + 1, cell_key)\r\n    clear_memory()\r\ndef train_macm_model(i,cell_key):\r\n    print(\"-------------MACM模型开始-----------------\")\r\n    MACD.main(i, i + 1, cell_key)\r\n    clear_memory()\r\n\r\ndef train_scp_model(i,cell_key):\r\n    print(\"-------------scpDeconv模型开始-----------------\")\r\n    scp.main(i, i + 1, cell_key)\r\n    clear_memory()\r\ndef clear_memory():\r\n    # 删除所有不再需要的变量\r\n    for name in dir():\r\n        if not name.startswith('_') and name not in ['gc', 'train_scp_model', 'train_macm_model', 'clear_memory', 'i', 'number_of_datasets', 'scp', 'MACD']:\r\n            del globals()[name]\r\n    # 强制进行垃圾回收\r\n    gc.collect()\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    cell_key='celltype_final'\r\n    for i in range(4, 5):\r\n        print(\"——————————第\" + str(i) + \"个数据——————————\")\r\n        train_macm_model(i, cell_key)\r\n        train_scp_model(i, cell_key)\r\n        train_Tangram_model(i, cell_key)\r\n        train_DestVi_model(i, cell_key)\r\n        train_Stereoscope_model(i, cell_key)\r\n        train_cell2location_model(i, cell_key)\r\n        train_Spoint_model(i,cell_key)\r\n\r\n\r\n\r\n"}
