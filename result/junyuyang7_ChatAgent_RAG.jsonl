{"repo_info": {"repo_name": "ChatAgent_RAG", "repo_owner": "junyuyang7", "repo_url": "https://github.com/junyuyang7/ChatAgent_RAG"}}
{"type": "test_file", "path": "server/chat/test/test_cases.py", "content": "import os \nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\nsys.path.append(os.path.dirname(__file__))\nsys.path.append('/home/yangjy/Study/ChatAgent_RAG/')\n\nimport pytest\nfrom fastapi import HTTPException\nfrom server.knowledge_base.kb_doc_api import search_docs_query_fusion\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.knowledge_base.model.kb_document_model import DocumentWithVSId, Document\nfrom unittest.mock import patch, MagicMock\n\n# Mocking dependencies\n@pytest.fixture\ndef mock_kb_service():\n    with patch.object(KBServiceFactory, 'get_service_by_name') as mock:\n        yield mock\n\n@pytest.fixture\ndef mock_document():\n    return MagicMock(spec=Document)\n\n# Happy path tests with various realistic test values\n@pytest.mark.parametrize(\"queries, knowledge_base_name, top_k, score_threshold, file_name, metadata, expected_ids\", [\n    ([\"你好\"], \"samples\", 5, 0.5, \"\", {}, [\"id1\", \"id2\"]), # Test ID: HP-1\n    ([\"查询\"], \"samples\", 3, 0.7, \"\", {}, [\"id1\", \"id2\"]), # Test ID: HP-2\n    ([], \"samples\", 5, 0.5, \"file_name\", {\"key\": \"value\"}, [\"id1\", \"id2\", 'value']), # Test ID: HP-3\n], ids=[\"HP-1\", \"HP-2\", \"HP-3\"])\ndef test_search_docs_query_fusion_happy_path(mock_kb_service, mock_document, queries, knowledge_base_name, top_k, score_threshold, file_name, metadata, expected_ids):\n    # Arrange\n    mock_service = MagicMock()\n    mock_kb_service.return_value = mock_service\n    mock_service.search_docs.return_value = [(mock_document, 0.9)]\n    mock_service.list_docs.return_value = [mock_document]\n    mock_document.page_content = ''\n    mock_document.metadata = {'id': [\"id1\", \"id2\"]}  # Set the 'metadata' attribute\n    \n    # Act\n    result = search_docs_query_fusion(queries=queries, knowledge_base_name=knowledge_base_name, top_k=top_k, score_threshold=score_threshold, file_name=file_name, metadata=metadata)\n    \n    # Assert\n    print([doc.id for doc in result])\n    assert [doc.id for doc in result] == expected_ids\n\n# Edge cases\n@pytest.mark.parametrize(\"queries, knowledge_base_name, top_k, score_threshold, file_name, metadata, expected_exception\", [\n    (None, \"samples\", 5, 0.5, \"\", {}, ValueError), # Test ID: EC-1\n    ([\"\"], \"\", 5, 0.5, \"\", {}, HTTPException), # Test ID: EC-2\n], ids=[\"EC-1\", \"EC-2\"])\ndef test_search_docs_query_fusion_edge_cases(queries, knowledge_base_name, top_k, score_threshold, file_name, metadata, expected_exception):\n    # Act & Assert\n    with pytest.raises(expected_exception):\n        search_docs_query_fusion(queries=queries, knowledge_base_name=knowledge_base_name, top_k=top_k, score_threshold=score_threshold, file_name=file_name, metadata=metadata)\n\n# Error cases\n@pytest.mark.parametrize(\"knowledge_base_name, expected_exception\", [\n    (\"invalid_kb\", ValueError), # Test ID: ERR-1\n], ids=[\"ERR-1\"])\ndef test_search_docs_query_fusion_error_cases(mock_kb_service, knowledge_base_name, expected_exception):\n    # Arrange\n    mock_kb_service.return_value = None\n    \n    # Act & Assert\n    with pytest.raises(expected_exception):\n        search_docs_query_fusion(knowledge_base_name=knowledge_base_name)\n\n\nimport pytest\nfrom fastapi import FastAPI\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\nfrom unittest.mock import AsyncMock, patch\nfrom server.chat.knowledge_base_chat import knowledge_base_chat\nfrom server.utils import BaseResponse\nfrom configs import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, TEMPERATURE, LLM_MODELS\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.chat.utils import History\n\n# Setup FastAPI app for testing\napp = FastAPI()\napp.post(\"/knowledge_base_chat\")(knowledge_base_chat)\nclient = TestClient(app)\n\n@pytest.mark.parametrize(\n    \"query, knowledge_base_name, top_k, score_threshold, history, stream, model_name, temperature, max_tokens, prompt_name, expected_status, expected_response_contains\",\n    [\n        # Happy path test cases\n        (\"你好\", \"samples\", VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, [], False, LLM_MODELS[0], TEMPERATURE, None, \"default\", 200, \"answer\"),\n        (\"历史查询\", \"history\", 5, 0.5, [{\"role\": \"user\", \"content\": \"历史\"}], False, LLM_MODELS[0], 0.7, 100, \"default\", 200, \"answer\"),\n        # Edge case: Empty query\n        (\"\", \"samples\", VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, [], False, LLM_MODELS[0], TEMPERATURE, None, \"default\", 200, \"未找到知识库\"),\n        # Error case: Non-existent knowledge base\n        (\"你好\", \"nonexistent\", VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, [], False, LLM_MODELS[0], TEMPERATURE, None, \"default\", 404, \"未找到知识库\"),\n    ],\n    ids=[\"happy-path\", \"happy-path-with-history\", \"edge-case-empty-query\", \"error-nonexistent-kb\"]\n)\n@pytest.mark.asyncio\nasync def test_knowledge_base_chat(query, knowledge_base_name, top_k, score_threshold, history, stream, model_name, temperature, max_tokens, prompt_name, expected_status, expected_response_contains):\n    # Arrange\n    with patch.object(KBServiceFactory, 'get_service_by_name', return_value=AsyncMock()) as mock_kb_service:\n        mock_kb_service.return_value.search_docs = AsyncMock(return_value=[])\n        mock_kb_service.return_value.search_docs_query_fusion = AsyncMock(return_value=[])\n\n    # Act\n    async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n        response = await ac.post(\"/knowledge_base_chat\", json={\n            \"query\": query,\n            \"knowledge_base_name\": knowledge_base_name,\n            \"top_k\": top_k,\n            \"score_threshold\": score_threshold,\n            \"history\": history,\n            \"stream\": stream,\n            \"model_name\": model_name,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"prompt_name\": prompt_name\n        })\n\n    # Assert\n    assert response.status_code == expected_status\n    assert expected_response_contains in response.text\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api_thread.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\n\ndef knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    result = []\n    response = requests.post(url, headers=headers, json=data, stream=True)\n\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line[6:])\n        result.append(data)\n    \n    return result\n\n\ndef test_thread():\n    threads = []\n    times = []\n    pool = ThreadPoolExecutor()\n    start = time.time()\n    for i in range(10):\n        t = pool.submit(knowledge_chat)\n        threads.append(t)\n    \n    for r in as_completed(threads):\n        end = time.time()\n        times.append(end - start)\n        print(\"\\nResult:\\n\")\n        pprint(r.result())\n\n    print(\"\\nTime used:\\n\")\n    for x in times:\n        print(f\"{x}\")\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_faiss_kb.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\n\nkbService = FaissKBService(\"test\")\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\n\n\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n\ndef test_delete_db():\n    assert kbService.drop_kb()\n"}
{"type": "test_file", "path": "tests/api/test_server_state_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\n\nfrom webui_pages.utils import ApiRequest\n\nimport pytest\nfrom pprint import pprint\nfrom typing import List\n\n\napi = ApiRequest()\n\n\ndef test_get_default_llm():\n    llm = api.get_default_llm_model()\n    \n    print(llm)\n    assert isinstance(llm, tuple)\n    assert isinstance(llm[0], str) and isinstance(llm[1], bool)\n\n\ndef test_server_configs():\n    configs = api.get_server_configs()\n    pprint(configs, depth=2)\n\n    assert isinstance(configs, dict)\n    assert len(configs) > 0\n\n\ndef test_list_search_engines():\n    engines = api.list_search_engines()\n    pprint(engines)\n\n    assert isinstance(engines, list)\n    assert len(engines) > 0\n\n\n@pytest.mark.parametrize(\"type\", [\"llm_chat\", \"agent_chat\"])\ndef test_get_prompt_template(type):\n    print(f\"prompt template for: {type}\")\n    template = api.get_prompt_template(type=type)\n\n    print(template)\n    assert isinstance(template, str)\n    assert len(template) > 0\n"}
{"type": "test_file", "path": "tests/custom_splitter/test_different_splitter.py", "content": "import os\n\nfrom transformers import AutoTokenizer\nimport sys\n\nsys.path.append(\"../..\")\nfrom configs import (\n    CHUNK_SIZE,\n    OVERLAP_SIZE\n)\n\nfrom server.knowledge_base.utils import make_text_splitter\n\ndef text(splitter_name):\n    from langchain import document_loaders\n\n    # 使用DocumentLoader读取文件\n    filepath = \"../../knowledge_base/samples/content/test.txt\"\n    loader = document_loaders.UnstructuredFileLoader(filepath, autodetect_encoding=True)\n    docs = loader.load()\n    text_splitter = make_text_splitter(splitter_name, CHUNK_SIZE, OVERLAP_SIZE)\n    if splitter_name == \"MarkdownHeaderTextSplitter\":\n        docs = text_splitter.split_text(docs[0].page_content)\n        for doc in docs:\n            if doc.metadata:\n                doc.metadata[\"source\"] = os.path.basename(filepath)\n    else:\n        docs = text_splitter.split_documents(docs)\n    for doc in docs:\n        print(doc)\n    return docs\n\n\n\n\nimport pytest\nfrom langchain.docstore.document import Document\n\n@pytest.mark.parametrize(\"splitter_name\",\n                         [\n                             \"ChineseRecursiveTextSplitter\",\n                             \"SpacyTextSplitter\",\n                             \"RecursiveCharacterTextSplitter\",\n                             \"MarkdownHeaderTextSplitter\"\n                         ])\ndef test_different_splitter(splitter_name):\n    try:\n        docs = text(splitter_name)\n        assert isinstance(docs, list)\n        if len(docs)>0:\n            assert isinstance(docs[0], Document)\n    except Exception as e:\n        pytest.fail(f\"test_different_splitter failed with {splitter_name}, error: {str(e)}\")\n"}
{"type": "test_file", "path": "tests/api/test_kb_summary_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\n\napi_base_url = api_address()\n\nkb = \"samples\"\nfile_name = \"/media/gpt4-pdf-chatbot-langchain/langchain-ChatGLM/knowledge_base/samples/content/llm/大模型技术栈-实战与应用.md\"\ndoc_ids = [\n    \"357d580f-fdf7-495c-b58b-595a398284e8\",\n    \"c7338773-2e83-4671-b237-1ad20335b0f0\",\n    \"6da613d1-327d-466f-8c1a-b32e6f461f47\"\n]\n\n\ndef test_summary_file_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_file_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"file_name\": file_name\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk[6:])\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n\ndef test_summary_doc_ids_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"doc_ids\": doc_ids\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk[6:])\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data)\n"}
{"type": "test_file", "path": "tests/api/test_kb_api_request.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\nfrom webui_pages.utils import ApiRequest\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\napi: ApiRequest = ApiRequest(api_base_url)\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"FAQ.MD\": str(root_path / \"docs\" / \"FAQ.MD\"),\n    \"README.MD\": str(root_path / \"README.MD\"),\n    \"test.txt\": get_file_path(\"samples\", \"test.txt\"),\n}\n\nprint(\"\\n\\nApiRquest调用\\n\")\n\n\ndef test_delete_kb_before():\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb():\n    print(f\"\\n尝试用空名称创建知识库：\")\n    data = api.create_knowledge_base(\" \")\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs():\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb in data\n\n\ndef test_upload_docs():\n    files = list(test_files.values())\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": docs}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files():\n    print(\"\\n获取知识库中文件列表：\")\n    data = api.list_kb_docs(knowledge_base_name=kb)\n    pprint(data)\n    assert isinstance(data, list)\n    for name in test_files:\n        assert name in data\n\n\ndef test_search_docs():\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_docs():\n    print(f\"\\n更新知识文件\")\n    data = api.update_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs():\n    print(f\"\\n删除知识文件\")\n    data = api.delete_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs():\n    print(\"\\n重建知识库：\")\n    r = api.recreate_vector_store(kb)\n    for data in r:\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after():\n    print(\"\\n删除知识库\")\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n\n    # check kb not exists anymore\n    print(\"\\n获取知识库列表：\")\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb not in data\n"}
{"type": "test_file", "path": "tests/kb_vector_db/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api/test_llm_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom configs.server_config import FSCHAT_MODEL_WORKERS\nfrom server.utils import api_address, get_model_worker_config\n\nfrom pprint import pprint\nimport random\nfrom typing import List\n\n\ndef get_configured_models() -> List[str]:\n    model_workers = list(FSCHAT_MODEL_WORKERS)\n    if \"default\" in model_workers:\n        model_workers.remove(\"default\")\n    return model_workers\n\n\napi_base_url = api_address()\n\n\ndef get_running_models(api=\"/llm_model/list_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    if r.status_code == 200:\n        return r.json()[\"data\"]\n    return []\n\n\ndef test_running_models(api=\"/llm_model/list_running_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    assert r.status_code == 200\n    print(\"\\n获取当前正在运行的模型列表：\")\n    pprint(r.json())\n    assert isinstance(r.json()[\"data\"], list)\n    assert len(r.json()[\"data\"]) > 0\n\n\n# 不建议使用stop_model功能。按现在的实现，停止了就只能手动再启动\n# def test_stop_model(api=\"/llm_model/stop\"):\n#     url = api_base_url + api\n#     r = requests.post(url, json={\"\"})\n\n\ndef test_change_model(api=\"/llm_model/change_model\"):\n    url = api_base_url + api\n\n    running_models = get_running_models()\n    assert len(running_models) > 0\n\n    model_workers = get_configured_models()\n\n    availabel_new_models = list(set(model_workers) - set(running_models))\n    assert len(availabel_new_models) > 0\n    print(availabel_new_models)\n\n    local_models = [x for x in running_models if not get_model_worker_config(x).get(\"online_api\")]\n    model_name = random.choice(local_models)\n    new_model_name = random.choice(availabel_new_models)\n    print(f\"\\n尝试将模型从 {model_name} 切换到 {new_model_name}\")\n    r = requests.post(url, json={\"model_name\": model_name, \"new_model_name\": new_model_name})\n    assert r.status_code == 200\n\n    running_models = get_running_models()\n    assert new_model_name in running_models\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"query\": \"请用100字左右的文字介绍自己\",\n    \"history\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"你好，我是人工智能大模型\"\n        }\n    ],\n    \"stream\": True,\n    \"temperature\": 0.7,\n}\n\n\ndef test_chat_chat(api=\"/chat/chat\"):\n    url = f\"{api_base_url}{api}\"\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    dump_output(response, api)\n    assert response.status_code == 200\n\n\ndef test_knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    print(\"\\n\")\n    print(\"=\" * 30 + api + \"  output\" + \"=\"*30)\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line[6:])\n        if \"answer\" in data:\n            print(data[\"answer\"], end=\"\", flush=True)\n    pprint(data)\n    assert \"docs\" in data and len(data[\"docs\"]) > 0\n    assert response.status_code == 200\n\n\ndef test_search_engine_chat(api=\"/chat/search_engine_chat\"):\n    global data\n\n    data[\"query\"] = \"室温超导最新进展是什么样？\"\n\n    url = f\"{api_base_url}{api}\"\n    for se in [\"bing\", \"duckduckgo\"]:\n        data[\"search_engine_name\"] = se\n        dump_input(data, api + f\" by {se}\")\n        response = requests.post(url, json=data, stream=True)\n        if se == \"bing\" and not BING_SUBSCRIPTION_KEY:\n            data = response.json()\n            assert data[\"code\"] == 404\n            assert data[\"msg\"] == f\"要使用Bing搜索引擎，需要设置 `BING_SUBSCRIPTION_KEY`\"\n\n        print(\"\\n\")\n        print(\"=\" * 30 + api + f\" by {se}  output\" + \"=\"*30)\n        for line in response.iter_content(None, decode_unicode=True):\n            data = json.loads(line[6:])\n            if \"answer\" in data:\n                print(data[\"answer\"], end=\"\", flush=True)\n        assert \"docs\" in data and len(data[\"docs\"]) > 0\n        pprint(data[\"docs\"])\n        assert response.status_code == 200\n\n"}
{"type": "test_file", "path": "tests/document_loader/test_pdfloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.pdf\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.pdf\"),\n}\n\ndef test_rapidocrpdfloader():\n    pdf_path = test_files[\"ocr_test.pdf\"]\n    from document_loaders import RapidOCRPDFLoader\n\n    loader = RapidOCRPDFLoader(pdf_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "test_file", "path": "tests/test_migrate.py", "content": "from pathlib import Path\nfrom pprint import pprint\nimport os\nimport shutil\nimport sys\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.knowledge_base.utils import get_kb_path, get_doc_path, KnowledgeFile\nfrom server.knowledge_base.migrate import folder2db, prune_db_docs, prune_folder_files\n\n\n# setup test knowledge base\nkb_name = \"test_kb_for_migrate\"\ntest_files = {\n    \"readme.md\": str(root_path / \"readme.md\"),\n}\n\n\nkb_path = get_kb_path(kb_name)\ndoc_path = get_doc_path(kb_name)\n\nif not os.path.isdir(doc_path):\n    os.makedirs(doc_path)\n\nfor k, v in test_files.items():\n    shutil.copy(v, os.path.join(doc_path, k))\n\n\ndef test_recreate_vs():\n    folder2db([kb_name], \"recreate_vs\")\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb and kb.exists()\n\n    files = kb.list_files()\n    print(files)\n    for name in test_files:\n        assert name in files\n        path = os.path.join(doc_path, name)\n\n        # list docs based on file name\n        docs = kb.list_docs(file_name=name)\n        assert len(docs) > 0\n        pprint(docs[0])\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n        # list docs base on metadata\n        docs = kb.list_docs(metadata={\"source\": name})\n        assert len(docs) > 0\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n\ndef test_increment():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.clear_vs()\n    assert kb.list_files() == []\n    assert kb.list_docs() == []\n\n    folder2db([kb_name], \"increment\")\n\n    files = kb.list_files()\n    print(files)\n    for f in test_files:\n        assert f in files\n\n        docs = kb.list_docs(file_name=f)\n        assert len(docs) > 0\n        pprint(docs[0])\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == f\n\n\ndef test_prune_db():\n    del_file, keep_file = list(test_files)[:2]\n    os.remove(os.path.join(doc_path, del_file))\n\n    prune_db_docs([kb_name])\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n    pprint(docs[0])\n\n    shutil.copy(test_files[del_file], os.path.join(doc_path, del_file))\n\n\ndef test_prune_folder():\n    del_file, keep_file = list(test_files)[:2]\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n\n    # delete docs for file\n    kb.delete_doc(KnowledgeFile(del_file, kb_name))\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    assert os.path.isfile(os.path.join(doc_path, del_file))\n\n    # prune folder\n    prune_folder_files([kb_name])\n\n    # check result\n    assert not os.path.isfile(os.path.join(doc_path, del_file))\n    assert os.path.isfile(os.path.join(doc_path, keep_file))\n\n\ndef test_drop_kb():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.drop_kb()\n    assert not kb.exists()\n    assert not os.path.isdir(kb_path)\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb is None\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_pg_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = PGKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/document_loader/test_imgloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.jpg\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.jpg\"),\n}\n\ndef test_rapidocrloader():\n    img_path = test_files[\"ocr_test.jpg\"]\n    from document_loaders import RapidOCRLoader\n\n    loader = RapidOCRLoader(img_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "test_file", "path": "tests/test_online_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom configs import ONLINE_LLM_MODEL\nfrom server.model_workers.base import *\nfrom server.utils import get_model_worker_config, list_config_llm_models\nfrom pprint import pprint\nimport pytest\n\n\nworkers = []\nfor x in list_config_llm_models()[\"online\"]:\n    if x in ONLINE_LLM_MODEL and x not in workers:\n        workers.append(x)\nprint(f\"all workers to test: {workers}\")\n\n# workers = [\"fangzhou-api\"]\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_chat(worker):\n    params = ApiChatParams(\n        messages = [\n            {\"role\": \"user\", \"content\": \"你是谁\"},\n        ],\n    )\n    print(f\"\\nchat with {worker} \\n\")\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        for x in worker_class().do_chat(params):\n            pprint(x)\n            assert isinstance(x, dict)\n            assert x[\"error_code\"] == 0\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_embeddings(worker):\n    params = ApiEmbeddingsParams(\n        texts = [\n            \"ChatAgent_RAG (原 Langchain-ChatGLM): 基于 Langchain 与 ChatGLM 等大语言模型的本地知识库问答应用实现。\",\n            \"一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。\",\n        ]\n    )\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        if worker_class.can_embedding():\n            print(f\"\\embeddings with {worker} \\n\")\n            resp = worker_class().do_embeddings(params)\n\n            pprint(resp, depth=2)\n            assert resp[\"code\"] == 200\n            assert \"data\" in resp\n            embeddings = resp[\"data\"]\n            assert isinstance(embeddings, list) and len(embeddings) > 0\n            assert isinstance(embeddings[0], list) and len(embeddings[0]) > 0\n            assert isinstance(embeddings[0][0], float)\n            print(\"向量长度：\", len(embeddings[0]))\n\n\n# @pytest.mark.parametrize(\"worker\", workers)\n# def test_completion(worker):\n#     params = ApiCompletionParams(prompt=\"五十六个民族\")\n    \n#     print(f\"\\completion with {worker} \\n\")\n\n#     worker_class = get_model_worker_config(worker)[\"worker_class\"]\n#     resp = worker_class().do_completion(params)\n#     pprint(resp)\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_milvus_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.milvus_kb_service import MilvusKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = MilvusKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/api/test_kb_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"wiki/Home.MD\": get_file_path(\"samples\", \"wiki/Home.md\"),\n    \"wiki/开发环境部署.MD\": get_file_path(\"samples\", \"wiki/开发环境部署.md\"),\n    \"test_files/test.txt\": get_file_path(\"samples\", \"test_files/test.txt\"),\n}\n\nprint(\"\\n\\n直接url访问\\n\")\n\n\ndef test_delete_kb_before(api=\"/knowledge_base/delete_knowledge_base\"):\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    url = api_base_url + api\n    print(\"\\n测试知识库存在，需要删除\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb(api=\"/knowledge_base/create_knowledge_base\"):\n    url = api_base_url + api\n\n    print(f\"\\n尝试用空名称创建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": \" \"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs(api=\"/knowledge_base/list_knowledge_bases\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb in data[\"data\"]\n\n\ndef test_upload_docs(api=\"/knowledge_base/upload_docs\"):\n    url = api_base_url + api\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": json.dumps(docs)}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files(api=\"/knowledge_base/list_files\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库中文件列表：\")\n    r = requests.get(url, params={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list)\n    for name in test_files:\n        assert name in data[\"data\"]\n\n\ndef test_search_docs(api=\"/knowledge_base/search_docs\"):\n    url = api_base_url + api\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_info(api=\"/knowledge_base/update_info\"):\n    url = api_base_url + api\n    print(\"\\n更新知识库介绍\")\n    r = requests.post(url, json={\"knowledge_base_name\": \"samples\", \"kb_info\": \"你好\"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n\ndef test_update_docs(api=\"/knowledge_base/update_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n更新知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs(api=\"/knowledge_base/delete_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n删除知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs(api=\"/knowledge_base/recreate_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n重建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb}, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk[6:])\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after(api=\"/knowledge_base/delete_knowledge_base\"):\n    url = api_base_url + api\n    print(\"\\n删除知识库\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n"}
{"type": "source_file", "path": "configs/basic_config.py", "content": "import logging\nimport os\nimport langchain\nimport tempfile\nimport shutil\n\n\n# 是否显示详细日志\nlog_verbose = False\nlangchain.verbose = False\n\n# 通常情况下不需要更改以下内容\n\n# 日志格式\nLOG_FORMAT = \"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\"\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nlogging.basicConfig(format=LOG_FORMAT)\n\n\n# 日志存储路径\nLOG_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"logs\")\nif not os.path.exists(LOG_PATH):\n    os.mkdir(LOG_PATH)\n\n# 临时文件目录，主要用于文件对话\nBASE_TEMP_DIR = os.path.join(tempfile.gettempdir(), \"chatchat\")\ntry:\n    shutil.rmtree(BASE_TEMP_DIR)\nexcept Exception:\n    pass\nos.makedirs(BASE_TEMP_DIR, exist_ok=True)\n"}
{"type": "source_file", "path": "configs/server_config.py", "content": "import sys\nfrom configs.model_config import LLM_DEVICE\n\n# httpx 请求默认超时时间（秒）。如果加载模型或对话较慢，出现超时错误，可以适当加大该值。\nHTTPX_DEFAULT_TIMEOUT = 300.0\n\n# API 是否开启跨域，默认为False，如果需要开启，请设置为True\n# is open cross domain\nOPEN_CROSS_DOMAIN = False\n\n# 各服务器默认绑定host。如改为\"0.0.0.0\"需要修改下方所有XX_SERVER的host\nDEFAULT_BIND_HOST = \"0.0.0.0\" if sys.platform != \"win32\" else \"127.0.0.1\"\n\n# webui.py server\nWEBUI_SERVER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 8501,\n}\n\n# api.py server\nAPI_SERVER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 7861,\n}\n\n# fastchat openai_api server\nFSCHAT_OPENAI_API = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 20000,\n}\n\n# fastchat model_worker server\n# 这些模型必须是在model_config.MODEL_PATH或ONLINE_MODEL中正确配置的。\n# 在启动startup.py时，可用通过`--model-name xxxx yyyy`指定模型，不指定则为LLM_MODELS\nFSCHAT_MODEL_WORKERS = {\n    # 所有模型共用的默认配置，可在模型专项配置中进行覆盖。\n    \"default\": {\n        \"host\": DEFAULT_BIND_HOST,\n        \"port\": 20002,\n        \"device\": LLM_DEVICE,\n        # TODO: 多卡加载\n        \"gpus\": '2,3',\n        # False,'vllm',使用的推理加速框架,使用vllm如果出现HuggingFace通信问题，参见doc/FAQ\n        # vllm对一些模型支持还不成熟，暂时默认关闭\n        \"infer_turbo\": False,\n\n        # model_worker多卡加载需要配置的参数\n        # \"gpus\": None, # 使用的GPU，以str的格式指定，如\"0,1\"，如失效请使用CUDA_VISIBLE_DEVICES=\"0,1\"等形式指定\n        # \"num_gpus\": 1, # 使用GPU的数量\n        # \"max_gpu_memory\": \"20GiB\", # 每个GPU占用的最大显存\n\n        # 以下为model_worker非常用参数，可根据需要配置\n        # \"load_8bit\": False, # 开启8bit量化\n        # \"cpu_offloading\": None,\n        # \"gptq_ckpt\": None,\n        # \"gptq_wbits\": 16,\n        # \"gptq_groupsize\": -1,\n        # \"gptq_act_order\": False,\n        # \"awq_ckpt\": None,\n        # \"awq_wbits\": 16,\n        # \"awq_groupsize\": -1,\n        # \"model_names\": LLM_MODELS,\n        # \"conv_template\": None,\n        # \"limit_worker_concurrency\": 5,\n        # \"stream_interval\": 2,\n        # \"no_register\": False,\n        # \"embed_in_truncate\": False,\n\n        # 以下为vllm_worker配置参数,注意使用vllm必须有gpu，仅在Linux测试通过\n\n        # tokenizer = model_path # 如果tokenizer与model_path不一致在此处添加\n        # 'tokenizer_mode':'auto',\n        # 'trust_remote_code':True,\n        # 'download_dir':None,\n        # 'load_format':'auto',\n        # 'dtype':'auto',\n        # 'seed':0,\n        # 'worker_use_ray':False,\n        # 'pipeline_parallel_size':1,\n        # 'tensor_parallel_size':1,\n        # 'block_size':16,\n        # 'swap_space':4 , # GiB\n        # 'gpu_memory_utilization':0.90,\n        # 'max_num_batched_tokens':2560,\n        # 'max_num_seqs':256,\n        # 'disable_log_stats':False,\n        # 'conv_template':None,\n        # 'limit_worker_concurrency':5,\n        # 'no_register':False,\n        # 'num_gpus': 1\n        # 'engine_use_ray': False,\n        # 'disable_log_requests': False\n\n    },\n    \"chatglm3-6b\": {\n        \"device\": \"cuda\",\n    },\n    \"Qwen1.5-0.5B-Chat\": {\n        \"device\": \"cuda\",\n    },\n    # 以下配置可以不用修改，在model_config中设置启动的模型\n    \"zhipu-api\": {\n        \"port\": 21001,\n    },\n    \"minimax-api\": {\n        \"port\": 21002,\n    },\n    \"xinghuo-api\": {\n        \"port\": 21003,\n    },\n    \"qianfan-api\": {\n        \"port\": 21004,\n    },\n    \"fangzhou-api\": {\n        \"port\": 21005,\n    },\n    \"qwen-api\": {\n        \"port\": 21006,\n    },\n    \"baichuan-api\": {\n        \"port\": 21007,\n    },\n    \"azure-api\": {\n        \"port\": 21008,\n    },\n    \"tiangong-api\": {\n        \"port\": 21009,\n    },\n    \"gemini-api\": {\n        \"port\": 21010,\n    },\n}\n\nFSCHAT_CONTROLLER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 20001,\n    \"dispatch_method\": \"shortest_queue\",\n}\n"}
{"type": "source_file", "path": "server/chat/__init__.py", "content": ""}
{"type": "source_file", "path": "document_loaders/mypdfloader.py", "content": "import os \nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom configs import PDF_OCR_THRESHOLD\nfrom document_loaders.ocr import get_ocr\nimport tqdm\n\n\nclass RapidOCRPDFLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def rotate_img(img, angle):\n            '''\n            img   --image\n            angle --rotation angle\n            return--rotated img\n            '''\n            \n            h, w = img.shape[:2]\n            rotate_center = (w/2, h/2)\n            #获取旋转矩阵\n            # 参数1为旋转中心点;\n            # 参数2为旋转角度,正值-逆时针旋转;负值-顺时针旋转\n            # 参数3为各向同性的比例因子,1.0原图，2.0变成原来的2倍，0.5变成原来的0.5倍\n            M = cv2.getRotationMatrix2D(rotate_center, angle, 1.0)\n            #计算图像新边界\n            new_w = int(h * np.abs(M[0, 1]) + w * np.abs(M[0, 0]))\n            new_h = int(h * np.abs(M[0, 0]) + w * np.abs(M[0, 1]))\n            #调整旋转矩阵以考虑平移\n            M[0, 2] += (new_w - w) / 2\n            M[1, 2] += (new_h - h) / 2\n\n            rotated_img = cv2.warpAffine(img, M, (new_w, new_h))\n            return rotated_img\n        \n        def pdf2text(filepath):\n            import fitz # pyMuPDF里面的fitz包，不要与pip install fitz混淆\n            import numpy as np\n\n            ocr = get_ocr()\n            doc = fitz.open(filepath)\n            resp = \"\"\n            \n            b_unit = tqdm.tqdm(total=doc.page_count, desc=\"RapidOCRPDFLoader context page index: 0\")\n            for i, page in enumerate(doc):\n                b_unit.set_description(\"RapidOCRPDFLoader context page index: {}\".format(i))\n                b_unit.refresh()\n                text = page.get_text(\"\")\n                resp += text + \"\\n\"\n                table_list = page.find_tables()\n                img_list = page.get_image_info(xrefs=True)\n\n                # 处理图片\n                for img in img_list:\n                    if xref := img.get(\"xref\"):\n                        bbox = img[\"bbox\"]\n                        # 检查图片尺寸是否超过设定的阈值\n                        if ((bbox[2] - bbox[0]) / (page.rect.width) < PDF_OCR_THRESHOLD[0]\n                            or (bbox[3] - bbox[1]) / (page.rect.height) < PDF_OCR_THRESHOLD[1]):\n                            continue\n                        pix = fitz.Pixmap(doc, xref)\n                        samples = pix.samples\n                        if int(page.rotation)!=0:  #如果Page有旋转角度，则旋转图片\n                            img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, -1)\n                            tmp_img = Image.fromarray(img_array);\n                            ori_img = cv2.cvtColor(np.array(tmp_img),cv2.COLOR_RGB2BGR)\n                            rot_img = rotate_img(img=ori_img, angle=360-page.rotation)\n                            img_array = cv2.cvtColor(rot_img, cv2.COLOR_RGB2BGR)\n                        else:\n                            img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, -1)\n\n                        result, _ = ocr(img_array)\n                        if result:\n                            ocr_result = [line[1] for line in result]\n                            resp += \"\\n\".join(ocr_result)\n\n                # 更新进度\n                b_unit.update(1)\n            return resp\n\n        text = pdf2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == \"__main__\":\n    loader = RapidOCRPDFLoader(file_path=\"test.pdf\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "server/api.py", "content": "import nltk\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom configs import VERSION\nfrom configs.model_config import NLTK_DATA_PATH\nfrom configs.server_config import OPEN_CROSS_DOMAIN\n\nimport argparse\nimport uvicorn\nfrom fastapi import Body\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.responses import RedirectResponse\n\nfrom server.chat.chat import chat\nfrom server.chat.search_engine_chat import search_engine_chat\nfrom server.chat.completion import completion\nfrom server.chat.feedback import chat_feedback\nfrom server.embeddings_api import embed_texts_endpoint\nfrom server.llm_api import (list_running_models, list_config_models,\n                            change_llm_model, stop_llm_model,\n                            get_model_config, list_search_engines)\nfrom server.utils import (BaseResponse, ListResponse, FastAPI, MakeFastAPIOffline,\n                          get_server_configs, get_prompt_template)\n\nfrom typing import List, Literal\n\nnltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\n\nasync def document():\n    return RedirectResponse(url=\"/docs\")\n\ndef create_app(run_mode: str=None):\n    app = FastAPI(\n        title=\"My RAG ChatAgent\",\n        version=VERSION\n    )\n    MakeFastAPIOffline(app)\n    # Add CORS middleware to allow all origins\n    # 在config.py中设置OPEN_DOMAIN=True，允许跨域\n    # set OPEN_DOMAIN=True in config.py to allow cross-domain\n    if OPEN_CROSS_DOMAIN:\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n    \n    mount_app_routes(app, run_mode=run_mode)\n    return app\n\ndef mount_app_routes(app: FastAPI, run_mode: str = None):\n    app.get(\"/\",\n            response_model=BaseResponse,\n            summary=\"swagger 文档\")(document)\n    \n    # Tag: Chat\n    app.post(\"/chat/chat\",\n             tags=[\"Chat\"],\n             summary=\"与llm模型对话(通过LLMChain)\",\n             )(chat)\n\n    app.post(\"/chat/search_engine_chat\",\n             tags=[\"Chat\"],\n             summary=\"与搜索引擎对话\",\n             )(search_engine_chat)\n\n    app.post(\"/chat/feedback\",\n             tags=[\"Chat\"],\n             summary=\"返回llm模型对话评分\",\n             )(chat_feedback)\n\n    # 知识库相关接口\n    mount_knowledge_routes(app)\n\n    # 摘要相关接口\n    mount_filename_summary_routes(app)\n\n    # LLM模型相关接口\n    app.post(\"/llm_model/list_running_models\",\n             tags=[\"LLM Model Management\"],\n             summary=\"列出当前已加载的模型\",\n             )(list_running_models)\n\n    app.post(\"/llm_model/list_config_models\",\n             tags=[\"LLM Model Management\"],\n             summary=\"列出configs已配置的模型\",\n             )(list_config_models)\n\n    app.post(\"/llm_model/get_model_config\",\n             tags=[\"LLM Model Management\"],\n             summary=\"获取模型配置（合并后）\",\n             )(get_model_config)\n\n    app.post(\"/llm_model/stop\",\n             tags=[\"LLM Model Management\"],\n             summary=\"停止指定的LLM模型（Model Worker)\",\n             )(stop_llm_model)\n\n    app.post(\"/llm_model/change\",\n             tags=[\"LLM Model Management\"],\n             summary=\"切换指定的LLM模型（Model Worker)\",\n             )(change_llm_model)\n    \n    # 服务器相关接口\n    app.post(\"/server/configs\",\n             tags=[\"Server State\"],\n             summary=\"获取服务器原始配置信息\",\n             )(get_server_configs)\n\n    app.post(\"/server/list_search_engines\",\n             tags=[\"Server State\"],\n             summary=\"获取服务器支持的搜索引擎\",\n             )(list_search_engines)\n\n    @app.post(\"/server/get_prompt_template\",\n             tags=[\"Server State\"],\n             summary=\"获取服务区配置的 prompt 模板\")\n    def get_server_prompt_template(\n        type: Literal[\"llm_chat\", \"knowledge_base_chat\", \"search_engine_chat\", \"agent_chat\"]=Body(\"llm_chat\", description=\"模板类型，可选值：llm_chat，knowledge_base_chat，search_engine_chat，agent_chat\"),\n        name: str = Body(\"default\", description=\"模板名称\"),\n    ) -> str:\n        return get_prompt_template(type=type, name=name)\n    \n    # 其它接口\n    app.post(\"/other/completion\",\n             tags=[\"Other\"],\n             summary=\"要求llm模型补全(通过LLMChain)\",\n             )(completion)\n\n    app.post(\"/other/embed_texts\",\n            tags=[\"Other\"],\n            summary=\"将文本向量化，支持本地模型和在线模型\",\n            )(embed_texts_endpoint)\n\n    \ndef mount_knowledge_routes(app: FastAPI):\n    from server.chat.knowledge_base_chat import knowledge_base_chat\n    from server.chat.file_chat import upload_temp_docs, file_chat\n    # from server.chat.agent_chat import agent_chat\n    from server.knowledge_base.kb_api import list_kbs, create_kb, delete_kb\n    from server.knowledge_base.kb_doc_api import (list_files, upload_docs, delete_docs,\n                                                update_docs, download_doc, recreate_vector_store,\n                                                search_docs, DocumentWithVSId, update_info,\n                                                update_docs_by_id,)\n    app.post(\"/chat/knowledge_base_chat\",\n             tags=['Chat'],\n             summary=\"与知识库对话\")(knowledge_base_chat)\n    \n    app.post(\"/chat/file_chat\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"文件对话\"\n             )(file_chat)\n\n    # app.post(\"/chat/agent_chat\",\n    #          tags=[\"Chat\"],\n    #          summary=\"与agent对话\")(agent_chat)\n    \n    # 知识库管理\n    app.get(\"/knowledge_base/list_knowledge_bases\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=ListResponse,\n            summary=\"获取知识库列表\")(list_kbs)\n\n    app.post(\"/knowledge_base/create_knowledge_base\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"创建知识库\"\n             )(create_kb)\n\n    app.post(\"/knowledge_base/delete_knowledge_base\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"删除知识库\"\n             )(delete_kb)\n\n    app.get(\"/knowledge_base/list_files\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=ListResponse,\n            summary=\"获取知识库内的文件列表\"\n            )(list_files)\n\n    app.post(\"/knowledge_base/search_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=List[DocumentWithVSId],\n             summary=\"搜索知识库\"\n             )(search_docs)\n\n    app.post(\"/knowledge_base/update_docs_by_id\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"直接更新知识库文档\"\n             )(update_docs_by_id)\n\n    app.post(\"/knowledge_base/upload_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"上传文件到知识库，并/或进行向量化\"\n             )(upload_docs)\n\n    app.post(\"/knowledge_base/delete_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"删除知识库内指定文件\"\n             )(delete_docs)\n\n    app.post(\"/knowledge_base/update_info\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"更新知识库介绍\"\n             )(update_info)\n    \n    app.post(\"/knowledge_base/update_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"更新现有文件到知识库\"\n             )(update_docs)\n\n    app.get(\"/knowledge_base/download_doc\",\n            tags=[\"Knowledge Base Management\"],\n            summary=\"下载对应的知识文件\")(download_doc)\n\n    app.post(\"/knowledge_base/recreate_vector_store\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"根据content中文档重建向量库，流式输出处理进度。\"\n             )(recreate_vector_store)\n\n    app.post(\"/knowledge_base/upload_temp_docs\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"上传文件到临时目录，用于文件对话。\"\n             )(upload_temp_docs)\n    \ndef mount_filename_summary_routes(app: FastAPI):\n    from server.knowledge_base.kb_summary_api import (summary_file_to_vector_store, recreate_summary_vector_store,\n                                                      summary_doc_ids_to_vector_store)\n\n    app.post(\"/knowledge_base/kb_summary_api/summary_file_to_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"单个知识库根据文件名称摘要\"\n             )(summary_file_to_vector_store)\n    app.post(\"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"单个知识库根据doc_ids摘要\",\n             response_model=BaseResponse,\n             )(summary_doc_ids_to_vector_store)\n    app.post(\"/knowledge_base/kb_summary_api/recreate_summary_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"重建单个知识库文件摘要\"\n             )(recreate_summary_vector_store)\n\ndef run_api(host, port, **kwargs):\n    # 服务端证书和公钥\n    if kwargs.get('ssl_keyfile') and kwargs.get('ssl_certfile'):\n        uvicorn.run(app,\n                    host=host,\n                    port=port,\n                    ssl_keyfile=kwargs.get(\"ssl_keyfile\"),\n                    ssl_certfile=kwargs.get('ssl_certfile'))\n    else:\n        uvicorn.run(app, host=host, port=port)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(prog='langchain-ChatGLM',\n                                     description='About langchain-ChatGLM, local knowledge based ChatGLM with langchain'\n                                                 ' ｜ 基于本地知识库的 ChatGLM 问答')\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int, default=7861)\n    parser.add_argument(\"--ssl_keyfile\", type=str)\n    parser.add_argument(\"--ssl_certfile\", type=str)\n    # 初始化消息\n    args = parser.parse_args()\n    args_dict = vars(args)\n\n    app = create_app()\n\n    run_api(host=args.host,\n            port=args.port,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            )\n    "}
{"type": "source_file", "path": "configs/kb_config.py", "content": "import os\n\n# 默认使用的知识库\nDEFAULT_KNOWLEDGE_BASE = \"samples\"\n\n# 默认向量库/全文检索引擎类型。可选：faiss, milvus(离线) & zilliz(在线), pgvector, chromadb 全文检索引擎es\nDEFAULT_VS_TYPE = \"faiss\"\n\n# 缓存向量库数量（针对FAISS）\nCACHED_VS_NUM = 1\n\n# 缓存临时向量库数量（针对FAISS），用于文件对话\nCACHED_MEMO_VS_NUM = 10\n\n# 知识库中单段文本长度(不适用MarkdownHeaderTextSplitter)\nCHUNK_SIZE = 250\n\n# 知识库中相邻文本重合长度(不适用MarkdownHeaderTextSplitter)\nOVERLAP_SIZE = 25\n\n# 知识库匹配向量数量\nVECTOR_SEARCH_TOP_K = 5\n\n# 知识库匹配的距离阈值，一般取值范围在0-1之间，SCORE越小，距离越小从而相关度越高。\n# 但有用户报告遇到过匹配分值超过1的情况，为了兼容性默认设为1，在WEBUI中调整范围为0-2\nSCORE_THRESHOLD = 1.0\n\n# 默认搜索引擎。可选：bing, duckduckgo, metaphor\nDEFAULT_SEARCH_ENGINE = \"duckduckgo\"\n\n# 搜索引擎匹配结题数量\nSEARCH_ENGINE_TOP_K = 3\n\n\n# Bing 搜索必备变量\n# 使用 Bing 搜索需要使用 Bing Subscription Key,需要在azure port中申请试用bing search\n# 具体申请方式请见\n# https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource\n# 使用python创建bing api 搜索实例详见:\n# https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/quickstarts/rest/python\nBING_SEARCH_URL = \"https://api.bing.microsoft.com/v7.0/search\"\n# 注意不是bing Webmaster Tools的api key，\n\n# 此外，如果是在服务器上，报Failed to establish a new connection: [Errno 110] Connection timed out\n# 是因为服务器加了防火墙，需要联系管理员加白名单，如果公司的服务器的话，就别想了GG\nBING_SUBSCRIPTION_KEY = \"\"\n\n# metaphor搜索需要KEY\nMETAPHOR_API_KEY = \"\"\n\n# 心知天气 API KEY，用于天气Agent。申请：https://www.seniverse.com/\nSENIVERSE_API_KEY = \"\"\n\n# 是否开启中文标题加强，以及标题增强的相关配置\n# 通过增加标题判断，判断哪些文本为标题，并在metadata中进行标记；\n# 然后将文本与往上一级的标题进行拼合，实现文本信息的增强。\nZH_TITLE_ENHANCE = False\n\n# PDF OCR 控制：只对宽高超过页面一定比例（图片宽/页面宽，图片高/页面高）的图片进行 OCR。\n# 这样可以避免 PDF 中一些小图片的干扰，提高非扫描版 PDF 处理速度\nPDF_OCR_THRESHOLD = (0.6, 0.6)\n\n# 每个知识库的初始化介绍，用于在初始化知识库时显示和Agent调用，没写则没有介绍，不会被Agent调用。\nKB_INFO = {\n    \"知识库名称\": \"知识库介绍\",\n    \"samples\": \"关于本项目issue的解答\",\n}\n\n\n# 通常情况下不需要更改以下内容\n\n# 知识库默认存储路径\nKB_ROOT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"knowledge_base\")\nif not os.path.exists(KB_ROOT_PATH):\n    os.mkdir(KB_ROOT_PATH)\n# 数据库默认存储路径。\n# 如果使用sqlite，可以直接修改DB_ROOT_PATH；如果使用其它数据库，请直接修改SQLALCHEMY_DATABASE_URI。\nDB_ROOT_PATH = os.path.join(KB_ROOT_PATH, \"info.db\")\nSQLALCHEMY_DATABASE_URI = f\"sqlite:///{DB_ROOT_PATH}\"\n\n# 可选向量库类型及对应配置\nkbs_config = {\n    \"faiss\": {\n    },\n    \"milvus\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n        \"user\": \"\",\n        \"password\": \"\",\n        \"secure\": False,\n    },\n    \"zilliz\": {\n        \"host\": \"in01-a7ce524e41e3935.ali-cn-hangzhou.vectordb.zilliz.com.cn\",\n        \"port\": \"19530\",\n        \"user\": \"\",\n        \"password\": \"\",\n        \"secure\": True,\n        },\n    \"pg\": {\n        \"connection_uri\": \"postgresql://postgres:postgres@127.0.0.1:5432/langchain_chatchat\",\n    },\n\n    \"es\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"9200\",\n        \"index_name\": \"test_index\",\n        \"user\": \"\",\n        \"password\": \"\"\n    },\n    \"milvus_kwargs\":{\n        \"search_params\":{\"metric_type\": \"L2\"}, #在此处增加search_params\n        \"index_params\":{\"metric_type\": \"L2\",\"index_type\": \"HNSW\"} # 在此处增加index_params\n    },\n    \"chromadb\": {}\n}\n\n# TextSplitter配置项，如果你不明白其中的含义，就不要修改。\ntext_splitter_dict = {\n    \"ChineseRecursiveTextSplitter\": {\n        \"source\": \"huggingface\",   # 选择tiktoken则使用openai的方法\n        \"tokenizer_name_or_path\": \"\",\n    },\n    \"SpacyTextSplitter\": {\n        \"source\": \"huggingface\",\n        \"tokenizer_name_or_path\": \"gpt2\",\n    },\n    \"RecursiveCharacterTextSplitter\": {\n        \"source\": \"tiktoken\",\n        \"tokenizer_name_or_path\": \"cl100k_base\",\n    },\n    \"MarkdownHeaderTextSplitter\": {\n        \"headers_to_split_on\":\n            [\n                (\"#\", \"head1\"),\n                (\"##\", \"head2\"),\n                (\"###\", \"head3\"),\n                (\"####\", \"head4\"),\n            ]\n    },\n    \"SemanticTextSplitter\": {\n        \"source\": \"huggingface\",\n        \"embedding_name_or_path\": \"bge-large-zh\",\n        'breakpoint_threshold_type': 'percentile',  # [\"percentile\", \"standard_deviation\", \"interquartile\"]\n        'breakpoint_threshold_amount': 0.4\n    },\n}\n\n# TEXT_SPLITTER 名称\nTEXT_SPLITTER_NAME = \"SemanticTextSplitter\"\n\n# Embedding模型定制词语的词表文件\nEMBEDDING_KEYWORD_FILE = \"embedding_keywords.txt\"\n"}
{"type": "source_file", "path": "document_loaders/mydocloader.py", "content": "from langchain.document_loaders.unstructured import UnstructuredFileLoader\nfrom typing import List\nimport tqdm\n\nclass RapidOCRDocLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def doc2text(filepath):\n            from docx.table import _Cell, Table\n            from docx.oxml.table import CT_Tbl\n            from docx.oxml.text.paragraph import CT_P\n            from docx.text.paragraph import Paragraph\n            from docx import Document, ImagePart\n            from PIL import Image\n            from io import BytesIO\n            import numpy as np\n            from rapidocr_onnxruntime import RapidOCR\n            ocr = RapidOCR()\n            doc = Document(filepath)\n            resp = \"\"\n\n            def iter_block_items(parent):\n                from docx.document import Document\n                if isinstance(parent, Document):\n                    parent_elm = parent.element.body\n                elif isinstance(parent, _Cell):\n                    parent_elm = parent._tc\n                else:\n                    raise ValueError(\"RapidOCRDocLoader parse fail\")\n\n                for child in parent_elm.iterchildren():\n                    if isinstance(child, CT_P):\n                        yield Paragraph(child, parent)\n                    elif isinstance(child, CT_Tbl):\n                        yield Table(child, parent)\n\n            b_unit = tqdm.tqdm(total=len(doc.paragraphs)+len(doc.tables),\n                               desc=\"RapidOCRDocLoader block index: 0\")\n            for i, block in enumerate(iter_block_items(doc)):\n                b_unit.set_description(\n                    \"RapidOCRDocLoader  block index: {}\".format(i))\n                b_unit.refresh()\n                if isinstance(block, Paragraph):\n                    resp += block.text.strip() + \"\\n\"\n                    images = block._element.xpath('.//pic:pic')  # 获取所有图片\n                    for image in images:\n                        for img_id in image.xpath('.//a:blip/@r:embed'):  # 获取图片id\n                            part = doc.part.related_parts[img_id]  # 根据图片id获取对应的图片\n                            if isinstance(part, ImagePart):\n                                image = Image.open(BytesIO(part._blob))\n                                result, _ = ocr(np.array(image))\n                                if result:\n                                    ocr_result = [line[1] for line in result]\n                                    resp += \"\\n\".join(ocr_result)\n                elif isinstance(block, Table):\n                    for row in block.rows:\n                        for cell in row.cells:\n                            for paragraph in cell.paragraphs:\n                                resp += paragraph.text.strip() + \"\\n\"\n                b_unit.update(1)\n            return resp\n\n        text = doc2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == '__main__':\n    loader = RapidOCRDocLoader(file_path=\"../tests/samples/ocr_test.docx\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "server/chat/chat.py", "content": "from fastapi import Body\nfrom sse_starlette.sse import EventSourceResponse\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import wrap_done, get_ChatOpenAI\nfrom langchain.chains.llm import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, List, Optional, Union\nimport asyncio\nimport json\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom server.chat.utils import History\nfrom langchain.prompts import PromptTemplate\nfrom server.utils import get_prompt_template\nfrom server.memory.conversation_db_buffer_memory import ConversationBufferDBMemory\nfrom server.db.repository import add_message_to_db\nfrom server.callback_handler.conversation_callback_handler import ConversationCallbackHandler\n\n\nasync def chat(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n               conversation_id: str = Body(\"\", description=\"对话框ID\"),\n               history_len: int = Body(-1, description=\"从数据库中取历史消息的数量\"),\n               history: Union[int, List[History]] = Body([],\n                                                         description=\"历史对话，设为一个整数可以从数据库中读取历史消息\",\n                                                         examples=[[\n                                                             {\"role\": \"user\",\n                                                              \"content\": \"老虎和狮子打架谁会赢\"},\n                                                             {\"role\": \"assistant\", \"content\": \"人类会赢\"}]]\n                                                         ),\n               stream: bool = Body(False, description=\"流式输出\"),\n               model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n               temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=2.0),\n               max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n               # top_p: float = Body(TOP_P, description=\"LLM 核采样。勿与temperature同时设置\", gt=0.0, lt=1.0),\n               prompt_name: str = Body(\"default\", description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n               ):\n    async def chat_iterator() -> AsyncIterable[str]:\n        nonlocal history, max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        callbacks = [callback]\n        memory = None\n\n        # 负责保存llm response到message db\n        message_id = add_message_to_db(chat_type=\"llm_chat\", query=query, conversation_id=conversation_id)\n        conversation_callback = ConversationCallbackHandler(conversation_id=conversation_id, message_id=message_id,\n                                                            chat_type=\"llm_chat\",\n                                                            query=query)\n        callbacks.append(conversation_callback)\n\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=callbacks,\n        )\n\n        if history: # 优先使用前端传入的历史消息\n            history = [History.from_data(h) for h in history]\n            prompt_template = get_prompt_template(\"llm_chat\", prompt_name)\n            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n            chat_prompt = ChatPromptTemplate.from_messages(\n                [i.to_msg_template() for i in history] + [input_msg])\n        elif conversation_id and history_len > 0: # 前端要求从数据库取历史消息\n            # 使用memory 时必须 prompt 必须含有memory.memory_key 对应的变量\n            prompt = get_prompt_template(\"llm_chat\", \"with_history\")\n            chat_prompt = PromptTemplate.from_template(prompt)\n            # 根据conversation_id 获取message 列表进而拼凑 memory\n            memory = ConversationBufferDBMemory(conversation_id=conversation_id,\n                                                llm=model,\n                                                message_limit=history_len)\n        else:\n            prompt_template = get_prompt_template(\"llm_chat\", prompt_name)\n            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n            chat_prompt = ChatPromptTemplate.from_messages([input_msg])\n\n        chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"input\": query}),\n            callback.done),\n        )\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps(\n                    {\"text\": token, \"message_id\": message_id},\n                    ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps(\n                {\"text\": answer, \"message_id\": message_id},\n                ensure_ascii=False)\n\n        await task\n\n    # 在函数内部，通过生成器 event_generator 生成事件数据，并使用 EventSourceResponse 将其作为响应返回给客户端。客户端就可以通过 SSE 连接接收到实时事件数据，从而实现实时更新的效果。\n    return EventSourceResponse(chat_iterator())\n"}
{"type": "source_file", "path": "document_loaders/FilteredCSVloader.py", "content": "## 指定制定列的csv文件加载器\n\nfrom langchain_community.document_loaders import CSVLoader\nimport csv\nfrom io import TextIOWrapper\nfrom typing import Dict, List, Optional\nfrom langchain.docstore.document import Document\nfrom langchain.document_loaders.helpers import detect_file_encodings\n\n\nclass FilteredCSVLoader(CSVLoader):\n    def __init__(\n            self,\n            file_path: str, \n            columns_to_read: List[str], \n            source_column: Optional[str] = None, \n            metadata_columns: List[str] = [], \n            csv_args: Optional[Dict] = None,\n            encoding: Optional[str] = None,\n            autodetect_encoding: bool = False,\n    ):\n        super().__init__(\n            file_path, \n            source_column, \n            metadata_columns, \n            csv_args, \n            encoding, \n            autodetect_encoding\n        )\n        self.columns_to_read = columns_to_read\n    \n    def load(self) -> List[Document]:\n        \"\"\"Load data into document objects.\"\"\"\n        \n        docs = []\n        try:\n            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n                docs = self.__read_file(csvfile)\n        except UnicodeDecodeError as e:\n            if self.autodetect_encoding:\n                detected_encodings = detect_file_encodings(self.file_path)\n                for encoding in detected_encodings:\n                    try:\n                        with open(\n                            self.file_path, newline=\"\", encoding=encoding.encoding\n                        ) as csvfile:\n                            docs = self.__read_file(csvfile)\n                            break\n                    except UnicodeDecodeError:\n                        continue\n            else:\n                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n\n        return docs\n    \n    def __read_file(self, csvfile: TextIOWrapper) -> List[Document]:\n        docs = []\n        csv_reader = csv.DictReader(csvfile, **self.csv_args)  # type: ignore\n        for i, row in enumerate(csv_reader):\n            if self.columns_to_read[0] in row:\n                content = row[self.columns_to_read[0]]\n                # Extract the source if available\n                source = (\n                    row.get(self.source_column, None)\n                    if self.source_column is not None\n                    else self.file_path\n                )\n                metadata = {\"source\": source, \"row\": i}\n\n                for col in self.metadata_columns:\n                    if col in row:\n                        metadata[col] = row[col]\n\n                doc = Document(page_content=content, metadata=metadata)\n                docs.append(doc)\n            else:\n                raise ValueError(f\"Column '{self.columns_to_read[0]}' not found in CSV file.\")\n\n        return docs\n"}
{"type": "source_file", "path": "document_loaders/ocr.py", "content": "from typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    try:\n        from rapidocr_paddle import RapidOCR\n    except ImportError:\n        from rapidocr_onnxruntime import RapidOCR\n\n\ndef get_ocr(use_cuda: bool = True) -> \"RapidOCR\":\n    try:\n        from rapidocr_paddle import RapidOCR\n        ocr = RapidOCR(det_use_cuda=use_cuda, cls_use_cuda=use_cuda, rec_use_cuda=use_cuda)\n    except ImportError:\n        from rapidocr_onnxruntime import RapidOCR\n        ocr = RapidOCR()\n    return ocr\n"}
{"type": "source_file", "path": "configs/prompt_config.py", "content": "# prompt模板使用Jinja2语法，简单点就是用双大括号代替f-string的单大括号\n# 本配置文件支持热加载，修改prompt模板后无需重启服务。\n\n# LLM对话支持的变量：\n#   - input: 用户输入内容\n\n# 知识库和搜索引擎对话支持的变量：\n#   - context: 从检索结果拼接的知识文本\n#   - question: 用户提出的问题\n\n# Agent对话支持的变量：\n\n#   - tools: 可用的工具列表\n#   - tool_names: 可用的工具名称列表\n#   - history: 用户和Agent的对话历史\n#   - input: 用户输入内容\n#   - agent_scratchpad: Agent的思维记录\n\nPROMPT_TEMPLATES = {\n    \"rag-fusion\": {\n        \"default\":\n            '{original_query}',\n        \n        \"fusion_en\": \n            \"You are a helpful assistant that generates multiple search queries based on a single input query.\"\n            \"Please Generate multiple search queries related to: {original_query}\"\n            \"OUTPUT ({k_query} queries): \\n\\n\"\n\n    },\n\n    \"llm_chat\": {\n        \"default\":\n            '{{ input }}',\n\n        \"with_history\":\n            'The following is a friendly conversation between a human and an AI. '\n            'The AI is talkative and provides lots of specific details from its context. '\n            'If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\n'\n            'Current conversation:\\n'\n            '{history}\\n'\n            'Human: {input}\\n'\n            'AI:',\n\n        \"py\":\n            '你是一个聪明的代码助手，请你给我写出简单的py代码。 \\n'\n            '{{ input }}',\n    },\n\n\n    \"knowledge_base_chat\": {\n        \"default\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，'\n            '不允许在答案中添加编造成分，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"text\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"empty\":  # 搜不到知识库的时候使用\n            '请你回答我的问题:\\n'\n            '{{ question }}\\n\\n',\n    },\n\n\n    \"search_engine_chat\": {\n        \"default\":\n            '<指令>这是我搜索到的互联网信息，请你根据这些信息进行提取并有调理，简洁的回答问题。'\n            '如果无法从中得到答案，请说 “无法搜索到能回答问题的内容”。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"search\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n    },\n\n\n    \"agent_chat\": {\n        \"default\":\n            'Answer the following questions as best you can. If it is in order, you can use some tools appropriately. '\n            'You have access to the following tools:\\n\\n'\n            '{tools}\\n\\n'\n            'Use the following format:\\n'\n            'Question: the input question you must answer1\\n'\n            'Thought: you should always think about what to do and what tools to use.\\n'\n            'Action: the action to take, should be one of [{tool_names}]\\n'\n            'Action Input: the input to the action\\n'\n            'Observation: the result of the action\\n'\n            '... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\\n'\n            'Thought: I now know the final answer\\n'\n            'Final Answer: the final answer to the original input question\\n'\n            'Begin!\\n\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}\\n',\n\n        \"ChatGLM3\":\n            'You can answer using the tools, or answer directly using your knowledge without using the tools. '\n            'Respond to the human as helpfully and accurately as possible.\\n'\n            'You have access to the following tools:\\n'\n            '{tools}\\n'\n            'Use a json blob to specify a tool by providing an action key (tool name) '\n            'and an action_input key (tool input).\\n'\n            'Valid \"action\" values: \"Final Answer\" or  [{tool_names}]'\n            'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n            '```\\n'\n            '{{{{\\n'\n            '  \"action\": $TOOL_NAME,\\n'\n            '  \"action_input\": $INPUT\\n'\n            '}}}}\\n'\n            '```\\n\\n'\n            'Follow this format:\\n\\n'\n            'Question: input question to answer\\n'\n            'Thought: consider previous and subsequent steps\\n'\n            'Action:\\n'\n            '```\\n'\n            '$JSON_BLOB\\n'\n            '```\\n'\n            'Observation: action result\\n'\n            '... (repeat Thought/Action/Observation N times)\\n'\n            'Thought: I know what to respond\\n'\n            'Action:\\n'\n            '```\\n'\n            '{{{{\\n'\n            '  \"action\": \"Final Answer\",\\n'\n            '  \"action_input\": \"Final response to human\"\\n'\n            '}}}}\\n'\n            'Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. '\n            'Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}',\n    }\n}\n"}
{"type": "source_file", "path": "document_loaders/mypptloader.py", "content": "from langchain.document_loaders.unstructured import UnstructuredFileLoader\nfrom typing import List\nimport tqdm\n\n\nclass RapidOCRPPTLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def ppt2text(filepath):\n            from pptx import Presentation\n            from PIL import Image\n            import numpy as np\n            from io import BytesIO\n            from rapidocr_onnxruntime import RapidOCR\n            ocr = RapidOCR()\n            prs = Presentation(filepath)\n            resp = \"\"\n\n            def extract_text(shape):\n                nonlocal resp\n                if shape.has_text_frame:\n                    resp += shape.text.strip() + \"\\n\"\n                if shape.has_table:\n                    for row in shape.table.rows:\n                        for cell in row.cells:\n                            for paragraph in cell.text_frame.paragraphs:\n                                resp += paragraph.text.strip() + \"\\n\"\n                if shape.shape_type == 13:  # 13 表示图片\n                    image = Image.open(BytesIO(shape.image.blob))\n                    result, _ = ocr(np.array(image))\n                    if result:\n                        ocr_result = [line[1] for line in result]\n                        resp += \"\\n\".join(ocr_result)\n                elif shape.shape_type == 6:  # 6 表示组合\n                    for child_shape in shape.shapes:\n                        extract_text(child_shape)\n\n            b_unit = tqdm.tqdm(total=len(prs.slides),\n                               desc=\"RapidOCRPPTLoader slide index: 1\")\n            # 遍历所有幻灯片\n            for slide_number, slide in enumerate(prs.slides, start=1):\n                b_unit.set_description(\n                    \"RapidOCRPPTLoader slide index: {}\".format(slide_number))\n                b_unit.refresh()\n                sorted_shapes = sorted(slide.shapes,\n                                       key=lambda x: (x.top, x.left))  # 从上到下、从左到右遍历\n                for shape in sorted_shapes:\n                    extract_text(shape)\n                b_unit.update(1)\n            return resp\n\n        text = ppt2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == '__main__':\n    loader = RapidOCRPPTLoader(file_path=\"../tests/samples/ocr_test.pptx\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "server/db/models/__init__.py", "content": ""}
{"type": "source_file", "path": "server/chat/feedback.py", "content": "from fastapi import Body\nfrom configs import logger, log_verbose\nfrom server.utils import BaseResponse\nfrom server.db.repository import feedback_message_to_db\n\ndef chat_feedback(message_id: str = Body(\"\", max_length=32, description=\"聊天记录id\"),\n            score: int = Body(0, max=100, description=\"用户评分，满分100，越大表示评价越高\"),\n            reason: str = Body(\"\", description=\"用户评分理由，比如不符合事实等\")\n            ):\n    try:\n        feedback_message_to_db(message_id=message_id,\n                               feedback_score=score,\n                               feedback_reason=reason)\n    except Exception as e:\n        msg = f\"反馈聊天记录出错：{e}\"\n        logger.error(f'{e.__class__.__name__}: {msg}',\n                     exc_info=e if log_verbose else None)\n        return BaseResponse(code=500, msg=msg)\n\n    return BaseResponse(code=200, msg=f\"已反馈聊天记录 {message_id}\")\n\n\n"}
{"type": "source_file", "path": "document_loaders/myimgloader.py", "content": "from typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\nfrom document_loaders.ocr import get_ocr\n\n\nclass RapidOCRLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def img2text(filepath):\n            resp = \"\"\n            ocr = get_ocr()\n            result, _ = ocr(filepath)\n            if result:\n                ocr_result = [line[1] for line in result]\n                resp += \"\\n\".join(ocr_result)\n            return resp\n\n        text = img2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == \"__main__\":\n    loader = RapidOCRLoader(file_path=\"../tests/samples/ocr_test.jpg\")\n    docs = loader.load()\n    print(docs)"}
{"type": "source_file", "path": "server/callback_handler/conversation_callback_handler.py", "content": "from typing import Any, Dict, List\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import LLMResult\nfrom server.db.repository import update_message\n\nclass ConversationCallbackHandler(BaseCallbackHandler):\n    raise_error: bool = True\n\n    def __init__(self, conversation_id: str, message_id: str, chat_type: str, query: str):\n        self.conversation_id = conversation_id\n        self.message_id = message_id\n        self.chat_type = chat_type\n        self.query = query\n        self.start_at = None\n\n    @property\n    def always_verbose(self) -> bool:\n        \"\"\"Whether to call verbose callbacks even if verbose is False.\"\"\"\n        return True\n    \n    def on_llm_start(\n            self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        '''如果想存更多信息，则prompts 也需要持久化'''\n        pass \n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        answer = response.generations[0][0].text\n        update_message(self.message_id, answer)\n"}
{"type": "source_file", "path": "server/chat/completion.py", "content": "from fastapi import Body\nfrom sse_starlette.sse import EventSourceResponse\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import wrap_done, get_OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, Optional\nimport asyncio\nfrom langchain.prompts import PromptTemplate\n\nfrom server.utils import get_prompt_template\n\n\nasync def completion(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n                     stream: bool = Body(False, description=\"流式输出\"),\n                     echo: bool = Body(False, description=\"除了输出之外，还回显输入\"),\n                     model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                     temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                     max_tokens: Optional[int] = Body(1024, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                     # top_p: float = Body(TOP_P, description=\"LLM 核采样。勿与temperature同时设置\", gt=0.0, lt=1.0),\n                     prompt_name: str = Body(\"default\",\n                                             description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                     ):\n\n    # todo 因ApiModelWorker 默认是按chat处理的，会对params[\"prompt\"] 解析为messages，因此ApiModelWorker 使用时需要有相应处理\n    async def completion_iterator(query: str,\n                                  model_name: str = LLM_MODELS[0],\n                                  prompt_name: str = prompt_name,\n                                  echo: bool = echo,\n                                  ) -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_OpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n            echo=echo\n        )\n\n        prompt_template = get_prompt_template(\"completion\", prompt_name)\n        prompt = PromptTemplate.from_template(prompt_template)\n        chain = LLMChain(prompt=prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"input\": query}),\n            callback.done),\n        )\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield token\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield answer\n\n        await task\n\n    return EventSourceResponse(completion_iterator(query=query,\n                                                 model_name=model_name,\n                                                 prompt_name=prompt_name),\n                             )\n"}
{"type": "source_file", "path": "server/db/base.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.ext.declarative import declarative_base, DeclarativeMeta\n# declarative_base()用于创建基本的类，这些类可以映射到数据库中的表格，使得通过对象来进行数据库操作更加方便。\n# DeclarativeMeta是declarative_base()返回的类型 Base = declarative_base()\nfrom sqlalchemy.orm import sessionmaker\n\nfrom configs import SQLALCHEMY_DATABASE_URI\nimport json\n\n# json.dumps 函数作用是将对象 obj 转换为 JSON 字符串。\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URI,\n    json_serializer=lambda obj: json.dumps(obj, ensure_ascii=False),\n)\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase: DeclarativeMeta = declarative_base()\n\nif __name__ == \"__main__\":\n    # 测试能否连接数据库\n    try:\n        session = SessionLocal()\n        result = session.execute(text(\"SELECT 1\"))\n        print(result.scalar())\n        session.close()\n        print(\"Database connection test successful!\")\n    except Exception as e:\n        print(\"Database connection test failed:\", e)"}
{"type": "source_file", "path": "server/chat/rag_fusion.py", "content": "# RAG utils\nimport os\nimport sys\nimport re\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport random\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Pipeline\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom configs import PROMPT_TEMPLATES, FUSION_K, EMBEDDING_DEVICE\nfrom server.utils import embedding_device\n\n# 定义一个函数来去除字符串开头的数字和句点\ndef remove_number_and_dot(text):\n    if re.match(r'^\\d+\\.\\s*', text):\n        return re.sub(r'^\\d+\\.\\s*', '', text)\n    else:\n        return text\n\n# Function to generate queries using OpenAI's ChatGPT\ndef generate_queries(original_query, pipe: Pipeline):\n    # get HuggingFacePipeline\n    local_llm = HuggingFacePipeline(pipeline=pipe)\n    # get llm-chain \n    template = PROMPT_TEMPLATES['rag-fusion']['fusion_en']\n    template = \"You are a helpful assistant that generates multiple search queries based on a single input query. Please Generate multiple search queries related to: {original_query}. OUTPUT ({k_query} queries): \\n\\n\"\n    prompt = PromptTemplate.from_template(template=template)\n\n    llm_chain = LLMChain(prompt=prompt,\n                        llm=local_llm\n                        )\n    input_list = [\n        {'original_query': original_query, 'k_query': str(FUSION_K)}\n    ]\n    response = llm_chain.apply(input_list)\n\n    generated_queries = response[0]['text'].split(template[-6:])[-1].split(\"\\n\")\n    generated_queries = [remove_number_and_dot(query) for query in generated_queries]\n    \n    return generated_queries\n\n# Mock function to simulate vector search, returning random scores\ndef vector_search(query, all_documents):\n    available_docs = list(all_documents.keys())\n    random.shuffle(available_docs)\n    selected_docs = available_docs[:random.randint(2, 5)]\n    scores = {doc: round(random.uniform(0.7, 0.9), 2) for doc in selected_docs}\n    return {doc: score for doc, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)}\n\n# Reciprocal Rank Fusion algorithm\ndef reciprocal_rank_fusion(search_results_dict, k=60) -> dict:\n    fused_scores = {}\n    print(\"Initial individual search result ranks:\")\n    for query, doc_scores in search_results_dict.items():\n        print(f\"For query '{query}': {doc_scores}\")\n        \n    for query, doc_scores in search_results_dict.items():\n        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n            if doc not in fused_scores:\n                fused_scores[doc] = 0\n            previous_score = fused_scores[doc]\n            fused_scores[doc] += 1 / (rank + k)\n            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n\n    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n    print(\"Final reranked results:\", reranked_results)\n    return reranked_results\n\n# Dummy function to simulate generative output\ndef generate_output(reranked_results, queries):\n    return f\"Final output based on {queries} and reranked documents: {list(reranked_results.keys())}\"\n\n\n# Predefined set of documents (usually these would be from your search database)\nall_documents = {\n    \"doc1\": \"Climate change and economic impact.\",\n    \"doc2\": \"Public health concerns due to climate change.\",\n    \"doc3\": \"Climate change: A social perspective.\",\n    \"doc4\": \"Technological solutions to climate change.\",\n    \"doc5\": \"Policy changes needed to combat climate change.\",\n    \"doc6\": \"Climate change and its impact on biodiversity.\",\n    \"doc7\": \"Climate change: The science and models.\",\n    \"doc8\": \"Global warming: A subset of climate change.\",\n    \"doc9\": \"How climate change affects daily weather.\",\n    \"doc10\": \"The history of climate change activism.\"\n}\n\n# Test Code\nif __name__ == \"__main__\":\n    # get Model\n    tokenizer = AutoTokenizer.from_pretrained('llm_models/chatglm3-6b', trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained('llm_models/chatglm3-6b', trust_remote_code=True)\n    pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, \n                    max_length=256, temperature=0.6, top_p=0.95, repetition_penalty=1.2, device='cuda:1')\n\n\n    original_query = \"impact of climate change\"\n    generated_queries = generate_queries(original_query, pipe)\n    \n    # all_results = {}\n    # for query in generated_queries:\n    #     search_results = vector_search(query, all_documents)\n    #     all_results[query] = search_results\n    \n    # reranked_results = reciprocal_rank_fusion(all_results)\n    \n    # final_output = generate_output(reranked_results, generated_queries)\n    \n    # print(final_output)\n\n"}
{"type": "source_file", "path": "embeddings/add_embedding_keywords.py", "content": "'''\r\n该功能是为了将关键词加入到embedding模型中，以便于在embedding模型中进行关键词的embedding\r\n该功能的实现是通过修改embedding模型的tokenizer来实现的\r\n该功能仅仅对EMBEDDING_MODEL参数对应的的模型有效，输出后的模型保存在原本模型\r\n感谢@CharlesJu1和@charlesyju的贡献提出了想法和最基础的PR\r\n\r\n保存的模型的位置位于原本嵌入模型的目录下，模型的名称为原模型名称+Merge_Keywords_时间戳\r\n'''\r\nimport sys\r\n\r\nsys.path.append(\"..\")\r\nimport os\r\nimport torch\r\n\r\nfrom datetime import datetime\r\nfrom configs import (\r\n    MODEL_PATH,\r\n    EMBEDDING_MODEL,\r\n    EMBEDDING_KEYWORD_FILE,\r\n)\r\n\r\nfrom safetensors.torch import save_model\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom langchain_core._api import deprecated\r\n\r\n\r\ndef get_keyword_embedding(bert_model, tokenizer, key_words):\r\n    tokenizer_output = tokenizer(key_words, return_tensors=\"pt\", padding=True, truncation=True)\r\n    input_ids = tokenizer_output['input_ids']\r\n    input_ids = input_ids[:, 1:-1]\r\n\r\n    keyword_embedding = bert_model.embeddings.word_embeddings(input_ids)\r\n    keyword_embedding = torch.mean(keyword_embedding, 1)\r\n    return keyword_embedding\r\n\r\n\r\ndef add_keyword_to_model(model_name=EMBEDDING_MODEL, keyword_file: str = \"\", output_model_path: str = None):\r\n    key_words = []\r\n    with open(keyword_file, \"r\") as f:\r\n        for line in f:\r\n            key_words.append(line.strip())\r\n\r\n    st_model = SentenceTransformer(model_name)\r\n    key_words_len = len(key_words)\r\n    word_embedding_model = st_model._first_module()\r\n    bert_model = word_embedding_model.auto_model\r\n    tokenizer = word_embedding_model.tokenizer\r\n    key_words_embedding = get_keyword_embedding(bert_model, tokenizer, key_words)\r\n\r\n    embedding_weight = bert_model.embeddings.word_embeddings.weight\r\n    embedding_weight_len = len(embedding_weight)\r\n    tokenizer.add_tokens(key_words)\r\n    bert_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=32)\r\n    embedding_weight = bert_model.embeddings.word_embeddings.weight\r\n    with torch.no_grad():\r\n        embedding_weight[embedding_weight_len:embedding_weight_len + key_words_len, :] = key_words_embedding\r\n\r\n    if output_model_path:\r\n        os.makedirs(output_model_path, exist_ok=True)\r\n        word_embedding_model.save(output_model_path)\r\n        safetensors_file = os.path.join(output_model_path, \"model.safetensors\")\r\n        metadata = {'format': 'pt'}\r\n        save_model(bert_model, safetensors_file, metadata)\r\n        print(\"save model to {}\".format(output_model_path))\r\n\r\n\r\ndef add_keyword_to_embedding_model(path: str = EMBEDDING_KEYWORD_FILE):\r\n    keyword_file = os.path.join(path)\r\n    model_name = MODEL_PATH[\"embed_model\"][EMBEDDING_MODEL]\r\n    model_parent_directory = os.path.dirname(model_name)\r\n    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\r\n    output_model_name = \"{}_Merge_Keywords_{}\".format(EMBEDDING_MODEL, current_time)\r\n    output_model_path = os.path.join(model_parent_directory, output_model_name)\r\n    add_keyword_to_model(model_name, keyword_file, output_model_path)\r\n"}
{"type": "source_file", "path": "server/db/__init__.py", "content": ""}
{"type": "source_file", "path": "document_loaders/__init__.py", "content": "from .mypdfloader import RapidOCRPDFLoader\nfrom .myimgloader import RapidOCRLoader\nfrom .mydocloader import RapidOCRDocLoader\nfrom .mypptloader import RapidOCRPPTLoader"}
{"type": "source_file", "path": "server/db/models/base.py", "content": "from datetime import datetime\nfrom sqlalchemy import Column, DateTime, String, Integer\n\n\nclass BaseModel:\n    '''\n    数据库基础模型\n    '''\n    id = Column(Integer, primary_key=True, index=True, comment=\"主键ID\")\n    create_time = Column(DateTime, default=datetime.utcnow, comment=\"创建时间\")\n    update_time = Column(DateTime, default=None, onupdate=datetime.utcnow, comment=\"更新时间\")\n    create_by = Column(String, default=None, comment=\"创建者\")\n    update_by = Column(String, default=None, comment=\"更新者\")"}
{"type": "source_file", "path": "server/db/models/knowledge_base_model.py", "content": "from sqlalchemy import Column, Integer, String, DateTime, func\nfrom server.db.base import Base\n\nclass KnowledgeBaseModel(Base):\n    \"\"\"\n    知识库模型\n    \"\"\"\n    __tablename__ = 'knowledge_base'\n    id = Column(Integer, primary_key=True, autoincrement=True, comment='知识库ID')\n    kb_name = Column(String(50), comment='知识库名称')\n    kb_info = Column(String(200), comment='知识库简介(用于Agent)')\n    vs_type = Column(String(50), comment='向量库类型')\n    embed_model = Column(String(50), comment='嵌入模型名称')\n    file_count = Column(Integer, default=0, comment='文件数量')\n    create_time = Column(DateTime, default=func.now(), comment='创建时间')\n\n    def __repr__(self):\n        return f\"<KnowledgeBase(id='{self.id}', kb_name='{self.kb_name}',kb_intro='{self.kb_info} vs_type='{self.vs_type}', embed_model='{self.embed_model}', file_count='{self.file_count}', create_time='{self.create_time}')>\"\n"}
{"type": "source_file", "path": "configs/__init__.py", "content": "from .basic_config import *\nfrom .model_config import *\nfrom .kb_config import *\nfrom .server_config import *\nfrom .prompt_config import *\n\n\nVERSION = \"v0.2.10\"\n"}
{"type": "source_file", "path": "configs/model_config.py", "content": "import os\n\n# 可以指定一个绝对路径，统一存放所有的Embedding和LLM模型。\n# 每个模型可以是一个单独的目录，也可以是某个目录下的二级子目录。\n# 如果模型目录名称和 MODEL_PATH 中的 key 或 value 相同，程序会自动检测加载，无需修改 MODEL_PATH 中的路径。\nMODEL_ROOT_PATH = \"llm_models/\"\n# model_file = 'llm_models/'\n\n# 选用的 Embedding 名称\nEMBEDDING_MODEL = \"bge-large-zh\"\n\n# Embedding 模型运行设备。设为 \"auto\" 会自动检测(会有警告)，也可手动设定为 \"cuda\",\"mps\",\"cpu\",\"xpu\" 其中之一。\nEMBEDDING_DEVICE = \"auto\"\n\n# 选用的reranker模型\nRERANKER_MODEL = \"bge-reranker-large\"\n\n# 是否启用reranker模型\nUSE_RERANKER = True\nRERANKER_MAX_LENGTH = 1024\n\n# 是否启用query-fusion\nUSE_QUERY_FUSION = True\n\n# 如果需要在 EMBEDDING_MODEL 中增加自定义的关键字时配置\nEMBEDDING_KEYWORD_FILE = \"keywords.txt\"\nEMBEDDING_MODEL_OUTPUT_PATH = \"output\"\n\n# 要运行的 LLM 名称，可以包括本地模型和在线模型。列表中本地模型将在启动项目时全部加载。\n# 列表中第一个模型将作为 API 和 WEBUI 的默认模型。\n# 在这里，我们使用目前主流的两个离线模型，其中，chatglm3-6b 为默认加载模型。\n# 如果你的显存不足，可使用 Qwen-1_8B-Chat, 该模型 FP16 仅需 3.8G显存。\n\nLLM_MODELS = [\"chatglm3-6b\"]\n# LLM_MODELS = [\"chatglm3-6b\", \"qwen-1.5-7b\"]\nAgent_MODEL = None\n\n# LLM 模型运行设备。设为\"auto\"会自动检测(会有警告)，也可手动设定为 \"cuda\",\"mps\",\"cpu\",\"xpu\" 其中之一。\nLLM_DEVICE = \"cuda\"\n\nHISTORY_LEN = 4\n\nMAX_TOKENS = 2048\n\nTEMPERATURE = 0.7\n\n# RAG-fusion产生的用户问询\nFUSION_K = 3\n\nONLINE_LLM_MODEL = {\n    \"openai-api\": {\n        \"model_name\": \"gpt-4\",\n        \"api_base_url\": \"https://api.openai.com/v1\",\n        \"api_key\": \"\",\n        \"openai_proxy\": \"\",\n    },\n\n    # 智谱AI API,具体注册及api key获取请前往 http://open.bigmodel.cn\n    \"zhipu-api\": {\n        \"api_key\": \"\",\n        \"version\": \"glm-4\",\n        \"provider\": \"ChatGLMWorker\", # 即server model_worker中的类\n    },\n\n    # 具体注册及api key获取请前往 https://api.minimax.chat/\n    \"minimax-api\": {\n        \"group_id\": \"\",\n        \"api_key\": \"\",\n        \"is_pro\": False,\n        \"provider\": \"MiniMaxWorker\",\n    },\n\n    # 具体注册及api key获取请前往 https://xinghuo.xfyun.cn/\n    \"xinghuo-api\": {\n        \"APPID\": \"\",\n        \"APISecret\": \"\",\n        \"api_key\": \"\",\n        \"version\": \"v3.5\", # 你使用的讯飞星火大模型版本，可选包括 \"v3.5\",\"v3.0\", \"v2.0\", \"v1.5\"\n        \"provider\": \"XingHuoWorker\",\n    },\n\n    # 百度千帆 API，申请方式请参考 https://cloud.baidu.com/doc/WENXINWORKSHOP/s/4lilb2lpf\n    \"qianfan-api\": {\n        \"version\": \"ERNIE-Bot\",  # 注意大小写。当前支持 \"ERNIE-Bot\" 或 \"ERNIE-Bot-turbo\"， 更多的见官方文档。\n        \"version_url\": \"\",  # 也可以不填写version，直接填写在千帆申请模型发布的API地址\n        \"api_key\": \"\",\n        \"secret_key\": \"\",\n        \"provider\": \"QianFanWorker\",\n    },\n\n    # 火山方舟 API，文档参考 https://www.volcengine.com/docs/82379\n    \"fangzhou-api\": {\n        \"version\": \"chatglm-6b-model\",\n        \"version_url\": \"\",\n        \"api_key\": \"\",\n        \"secret_key\": \"\",\n        \"provider\": \"FangZhouWorker\",\n    },\n\n    # 阿里云通义千问 API，文档参考 https://help.aliyun.com/zh/dashscope/developer-reference/api-details\n    \"qwen-api\": {\n        \"version\": \"qwen-max\",\n        \"api_key\": \"\",\n        \"provider\": \"QwenWorker\",\n        \"embed_model\": \"text-embedding-v1\"  # embedding 模型名称\n    },\n\n    # 百川 API，申请方式请参考 https://www.baichuan-ai.com/home#api-enter\n    \"baichuan-api\": {\n        \"version\": \"Baichuan2-53B\",\n        \"api_key\": \"\",\n        \"secret_key\": \"\",\n        \"provider\": \"BaiChuanWorker\",\n    },\n\n    # Azure API\n    \"azure-api\": {\n        \"deployment_name\": \"\",  # 部署容器的名字\n        \"resource_name\": \"\",  # https://{resource_name}.openai.azure.com/openai/ 填写resource_name的部分，其他部分不要填写\n        \"api_version\": \"\",  # API的版本，不是模型版本\n        \"api_key\": \"\",\n        \"provider\": \"AzureWorker\",\n    },\n\n    # 昆仑万维天工 API https://model-platform.tiangong.cn/\n    \"tiangong-api\": {\n        \"version\": \"SkyChat-MegaVerse\",\n        \"api_key\": \"\",\n        \"secret_key\": \"\",\n        \"provider\": \"TianGongWorker\",\n    },\n    # Gemini API https://makersuite.google.com/app/apikey\n    \"gemini-api\": {\n        \"api_key\": \"\",\n        \"provider\": \"GeminiWorker\",\n    }\n\n}\n\n# 在以下字典中修改属性值，以指定本地embedding模型存储位置。支持3种设置方法：\n# 1、将对应的值修改为模型绝对路径\n# 2、不修改此处的值（以 text2vec 为例）：\n#       2.1 如果{MODEL_ROOT_PATH}下存在如下任一子目录：\n#           - text2vec\n#           - GanymedeNil/text2vec-large-chinese\n#           - text2vec-large-chinese\n#       2.2 如果以上本地路径不存在，则使用huggingface模型\n\nMODEL_PATH = {\n    \"embed_model\": {\n        \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n        \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n        \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n        \"text2vec\": \"GanymedeNil/text2vec-large-chinese\",\n        \"text2vec-paraphrase\": \"shibing624/text2vec-base-chinese-paraphrase\",\n        \"text2vec-sentence\": \"shibing624/text2vec-base-chinese-sentence\",\n        \"text2vec-multilingual\": \"shibing624/text2vec-base-multilingual\",\n        \"text2vec-bge-large-chinese\": \"shibing624/text2vec-bge-large-chinese\",\n        \"m3e-small\": \"moka-ai/m3e-small\",\n        \"m3e-base\": \"moka-ai/m3e-base\",\n        \"m3e-large\": \"moka-ai/m3e-large\",\n\n        \"bge-small-zh\": \"BAAI/bge-small-zh\",\n        \"bge-base-zh\": \"BAAI/bge-base-zh\",\n        \"bge-large-zh\": \"BAAI/bge-large-zh\",\n        \"bge-large-zh-noinstruct\": \"BAAI/bge-large-zh-noinstruct\",\n        \"bge-base-zh-v1.5\": \"BAAI/bge-base-zh-v1.5\",\n        \"bge-large-zh-v1.5\": \"BAAI/bge-large-zh-v1.5\",\n\n        \"bge-m3\": \"BAAI/bge-m3\",\n\n        \"piccolo-base-zh\": \"sensenova/piccolo-base-zh\",\n        \"piccolo-large-zh\": \"sensenova/piccolo-large-zh\",\n        \"nlp_gte_sentence-embedding_chinese-large\": \"damo/nlp_gte_sentence-embedding_chinese-large\",\n        \"text-embedding-ada-002\": \"your OPENAI_API_KEY\",\n    },\n\n    \"llm_model\": {\n        \"chatglm2-6b\": \"THUDM/chatglm2-6b\",\n        \"chatglm2-6b-32k\": \"THUDM/chatglm2-6b-32k\",\n        \"chatglm3-6b\": \"THUDM/chatglm3-6b\",\n        \"chatglm3-6b-32k\": \"THUDM/chatglm3-6b-32k\",\n\n        \"Orion-14B-Chat\": \"OrionStarAI/Orion-14B-Chat\",\n        \"Orion-14B-Chat-Plugin\": \"OrionStarAI/Orion-14B-Chat-Plugin\",\n        \"Orion-14B-LongChat\": \"OrionStarAI/Orion-14B-LongChat\",\n\n        \"Llama-2-7b-chat-hf\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"Llama-2-13b-chat-hf\": \"meta-llama/Llama-2-13b-chat-hf\",\n        \"Llama-2-70b-chat-hf\": \"meta-llama/Llama-2-70b-chat-hf\",\n\n        \"Qwen-1_8B-Chat\": \"Qwen/Qwen-1_8B-Chat\",\n        \"Qwen-7B-Chat\": \"Qwen/Qwen-7B-Chat\",\n        \"Qwen-14B-Chat\": \"Qwen/Qwen-14B-Chat\",\n        \"Qwen-72B-Chat\": \"Qwen/Qwen-72B-Chat\",\n\n        # Qwen1.5 模型 VLLM可能出现问题\n        \"Qwen1.5-0.5B-Chat\": \"Qwen/Qwen1.5-0.5B-Chat\",\n        \"Qwen1.5-1.8B-Chat\": \"Qwen/Qwen1.5-1.8B-Chat\",\n        \"Qwen1.5-4B-Chat\": \"Qwen/Qwen1.5-4B-Chat\",\n        \"Qwen1.5-7B-Chat\": \"Qwen/Qwen1.5-7B-Chat\",\n        \"Qwen1.5-14B-Chat\": \"Qwen/Qwen1.5-14B-Chat\",\n        \"Qwen1.5-72B-Chat\": \"Qwen/Qwen1.5-72B-Chat\",\n\n        \"baichuan-7b-chat\": \"baichuan-inc/Baichuan-7B-Chat\",\n        \"baichuan-13b-chat\": \"baichuan-inc/Baichuan-13B-Chat\",\n        \"baichuan2-7b-chat\": \"baichuan-inc/Baichuan2-7B-Chat\",\n        \"baichuan2-13b-chat\": \"baichuan-inc/Baichuan2-13B-Chat\",\n\n        \"internlm-7b\": \"internlm/internlm-7b\",\n        \"internlm-chat-7b\": \"internlm/internlm-chat-7b\",\n        \"internlm2-chat-7b\": \"internlm/internlm2-chat-7b\",\n        \"internlm2-chat-20b\": \"internlm/internlm2-chat-20b\",\n\n        \"BlueLM-7B-Chat\": \"vivo-ai/BlueLM-7B-Chat\",\n        \"BlueLM-7B-Chat-32k\": \"vivo-ai/BlueLM-7B-Chat-32k\",\n\n        \"Yi-34B-Chat\": \"https://huggingface.co/01-ai/Yi-34B-Chat\",\n\n        \"agentlm-7b\": \"THUDM/agentlm-7b\",\n        \"agentlm-13b\": \"THUDM/agentlm-13b\",\n        \"agentlm-70b\": \"THUDM/agentlm-70b\",\n\n        \"falcon-7b\": \"tiiuae/falcon-7b\",\n        \"falcon-40b\": \"tiiuae/falcon-40b\",\n        \"falcon-rw-7b\": \"tiiuae/falcon-rw-7b\",\n\n        \"aquila-7b\": \"BAAI/Aquila-7B\",\n        \"aquilachat-7b\": \"BAAI/AquilaChat-7B\",\n        \"open_llama_13b\": \"openlm-research/open_llama_13b\",\n        \"vicuna-13b-v1.5\": \"lmsys/vicuna-13b-v1.5\",\n        \"koala\": \"young-geng/koala\",\n        \"mpt-7b\": \"mosaicml/mpt-7b\",\n        \"mpt-7b-storywriter\": \"mosaicml/mpt-7b-storywriter\",\n        \"mpt-30b\": \"mosaicml/mpt-30b\",\n        \"opt-66b\": \"facebook/opt-66b\",\n        \"opt-iml-max-30b\": \"facebook/opt-iml-max-30b\",\n        \"gpt2\": \"gpt2\",\n        \"gpt2-xl\": \"gpt2-xl\",\n        \"gpt-j-6b\": \"EleutherAI/gpt-j-6b\",\n        \"gpt4all-j\": \"nomic-ai/gpt4all-j\",\n        \"gpt-neox-20b\": \"EleutherAI/gpt-neox-20b\",\n        \"pythia-12b\": \"EleutherAI/pythia-12b\",\n        \"oasst-sft-4-pythia-12b-epoch-3.5\": \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n        \"dolly-v2-12b\": \"databricks/dolly-v2-12b\",\n        \"stablelm-tuned-alpha-7b\": \"stabilityai/stablelm-tuned-alpha-7b\",\n    },\n\n    \"reranker\": {\n        \"bge-reranker-large\": \"BAAI/bge-reranker-large\",\n        \"bge-reranker-base\": \"BAAI/bge-reranker-base\",\n    }\n}\n\n# 通常情况下不需要更改以下内容\n\n# nltk 模型存储路径\nNLTK_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"nltk_data\")\n\n# 使用VLLM可能导致模型推理能力下降，无法完成Agent任务\nVLLM_MODEL_DICT = {\n    \"chatglm2-6b\": \"THUDM/chatglm2-6b\",\n    \"chatglm2-6b-32k\": \"THUDM/chatglm2-6b-32k\",\n    \"chatglm3-6b\": \"THUDM/chatglm3-6b\",\n    \"chatglm3-6b-32k\": \"THUDM/chatglm3-6b-32k\",\n\n    \"Llama-2-7b-chat-hf\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"Llama-2-13b-chat-hf\": \"meta-llama/Llama-2-13b-chat-hf\",\n    \"Llama-2-70b-chat-hf\": \"meta-llama/Llama-2-70b-chat-hf\",\n\n    \"Qwen-1_8B-Chat\": \"Qwen/Qwen-1_8B-Chat\",\n    \"Qwen-7B-Chat\": \"Qwen/Qwen-7B-Chat\",\n    \"Qwen-14B-Chat\": \"Qwen/Qwen-14B-Chat\",\n    \"Qwen-72B-Chat\": \"Qwen/Qwen-72B-Chat\",\n\n    \"baichuan-7b-chat\": \"baichuan-inc/Baichuan-7B-Chat\",\n    \"baichuan-13b-chat\": \"baichuan-inc/Baichuan-13B-Chat\",\n    \"baichuan2-7b-chat\": \"baichuan-inc/Baichuan-7B-Chat\",\n    \"baichuan2-13b-chat\": \"baichuan-inc/Baichuan-13B-Chat\",\n\n    \"BlueLM-7B-Chat\": \"vivo-ai/BlueLM-7B-Chat\",\n    \"BlueLM-7B-Chat-32k\": \"vivo-ai/BlueLM-7B-Chat-32k\",\n\n    \"internlm-7b\": \"internlm/internlm-7b\",\n    \"internlm-chat-7b\": \"internlm/internlm-chat-7b\",\n    \"internlm2-chat-7b\": \"internlm/Models/internlm2-chat-7b\",\n    \"internlm2-chat-20b\": \"internlm/Models/internlm2-chat-20b\",\n\n    \"aquila-7b\": \"BAAI/Aquila-7B\",\n    \"aquilachat-7b\": \"BAAI/AquilaChat-7B\",\n\n    \"falcon-7b\": \"tiiuae/falcon-7b\",\n    \"falcon-40b\": \"tiiuae/falcon-40b\",\n    \"falcon-rw-7b\": \"tiiuae/falcon-rw-7b\",\n    \"gpt2\": \"gpt2\",\n    \"gpt2-xl\": \"gpt2-xl\",\n    \"gpt-j-6b\": \"EleutherAI/gpt-j-6b\",\n    \"gpt4all-j\": \"nomic-ai/gpt4all-j\",\n    \"gpt-neox-20b\": \"EleutherAI/gpt-neox-20b\",\n    \"pythia-12b\": \"EleutherAI/pythia-12b\",\n    \"oasst-sft-4-pythia-12b-epoch-3.5\": \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n    \"dolly-v2-12b\": \"databricks/dolly-v2-12b\",\n    \"stablelm-tuned-alpha-7b\": \"stabilityai/stablelm-tuned-alpha-7b\",\n    \"open_llama_13b\": \"openlm-research/open_llama_13b\",\n    \"vicuna-13b-v1.3\": \"lmsys/vicuna-13b-v1.3\",\n    \"koala\": \"young-geng/koala\",\n    \"mpt-7b\": \"mosaicml/mpt-7b\",\n    \"mpt-7b-storywriter\": \"mosaicml/mpt-7b-storywriter\",\n    \"mpt-30b\": \"mosaicml/mpt-30b\",\n    \"opt-66b\": \"facebook/opt-66b\",\n    \"opt-iml-max-30b\": \"facebook/opt-iml-max-30b\",\n\n}\n\nSUPPORT_AGENT_MODEL = [\n    \"openai-api\",  # GPT4 模型\n    \"qwen-api\",  # Qwen Max模型\n    \"zhipu-api\",  # 智谱AI GLM4模型\n    \"Qwen\",  # 所有Qwen系列本地模型\n    \"chatglm3-6b\",\n    \"internlm2-chat-20b\",\n    \"Orion-14B-Chat-Plugin\",\n]\n"}
{"type": "source_file", "path": "server/db/models/conversation_model.py", "content": "from sqlalchemy import Column, Integer, String, DateTime, JSON, func\nfrom server.db.base import Base\n\n\nclass ConversationModel(Base):\n    \"\"\"\n    聊天记录模型\n    \"\"\"\n    __tablename__ = 'conversation'\n    id = Column(String(32), primary_key=True, comment='对话框ID')\n    name = Column(String(50), comment='对话框名称')\n    # chat/agent_chat等\n    chat_type = Column(String(50), comment='聊天类型')\n    create_time = Column(DateTime, default=func.now(), comment='创建时间')\n\n    def __repr__(self):\n        return f\"<Conversation(id='{self.id}', name='{self.name}', chat_type='{self.chat_type}', create_time='{self.create_time}')>\"\n"}
{"type": "source_file", "path": "init_database.py", "content": "import sys\nsys.path.append(\".\")\nfrom server.knowledge_base.migrate import (create_tables, reset_tables, import_from_db,\n                                           folder2db, prune_db_docs, prune_folder_files)\nfrom configs.model_config import NLTK_DATA_PATH, EMBEDDING_MODEL\nimport nltk\nnltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\nfrom datetime import datetime\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"please specify only one operate method once time.\")\n\n    parser.add_argument(\n        \"-r\",\n        \"--recreate-vs\",\n        action=\"store_true\",\n        help=('''\n            recreate vector store.\n            use this option if you have copied document files to the content folder, but vector store has not been populated or DEFAUL_VS_TYPE/EMBEDDING_MODEL changed.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--create-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables if not existed\")\n    )\n    parser.add_argument(\n        \"--clear-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables, or drop the database tables before recreate vector stores\")\n    )\n    parser.add_argument(\n        \"--import-db\",\n        help=\"import tables from specified sqlite database\"\n    )\n    parser.add_argument(\n        \"-u\",\n        \"--update-in-db\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in database.\n            use this option if you want to recreate vectors for files exist in db and skip files exist in local folder only.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--increment\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in local folder and not exist in database.\n            use this option if you want to create vectors incrementally.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--prune-db\",\n        action=\"store_true\",\n        help=('''\n            delete docs in database that not existed in local folder.\n            it is used to delete database docs after user deleted some doc files in file browser\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--prune-folder\",\n        action=\"store_true\",\n        help=('''\n            delete doc files in local folder that not existed in database.\n            is is used to free local disk space by delete unused doc files.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--kb-name\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=(\"specify knowledge base names to operate on. default is all folders exist in KB_ROOT_PATH.\")\n    )\n    parser.add_argument(\n        \"-e\",\n        \"--embed-model\",\n        type=str,\n        default=EMBEDDING_MODEL,\n        help=(\"specify embeddings model.\")\n    )\n\n    args = parser.parse_args()\n    start_time = datetime.now()\n\n    if args.create_tables:\n        create_tables() # confirm tables exist\n\n    if args.clear_tables:\n        reset_tables()\n        print(\"database tables reset\")\n\n    if args.recreate_vs:\n        create_tables()\n        print(\"recreating all vector stores\")\n        folder2db(kb_names=args.kb_name, mode=\"recreate_vs\", embed_model=args.embed_model)\n    elif args.import_db:\n        import_from_db(args.import_db)\n    elif args.update_in_db:\n        folder2db(kb_names=args.kb_name, mode=\"update_in_db\", embed_model=args.embed_model)\n    elif args.increment:\n        folder2db(kb_names=args.kb_name, mode=\"increment\", embed_model=args.embed_model)\n    elif args.prune_db:\n        prune_db_docs(args.kb_name)\n    elif args.prune_folder:\n        prune_folder_files(args.kb_name)\n\n    end_time = datetime.now()\n    print(f\"总计用时： {end_time-start_time}\")\n"}
{"type": "source_file", "path": "embeddings/__init__.py", "content": ""}
{"type": "source_file", "path": "server/chat/utils.py", "content": "from pydantic import BaseModel, Field\nfrom langchain.prompts.chat import ChatMessagePromptTemplate\nfrom configs import logger, log_verbose\nfrom typing import List, Tuple, Dict, Union\n\n\nclass History(BaseModel):\n    '''\n    对话历史\n    可从dict生成，如\n    h = History(**{\"role\":\"user\",\"content\":\"你好\"})\n    也可转换为tuple，如\n    h.to_msy_tuple = (\"human\", \"你好\")\n    '''\n    role: str = Field(...)\n    content: str = Field(...)\n\n    def to_msg_tuple(self):\n        '''转化为元组'''\n        return 'ai' if self.role == 'assistant' else 'human', self.content\n    \n    def to_msg_template(self, is_raw=True) -> ChatMessagePromptTemplate:\n        role_maps = {\n            'ai': 'assistant',\n            'human': 'user',\n        }\n        role = role_maps.get(self.role, self.role)\n        if is_raw: # 当前默认历史消息都是没有input_variable的文本。\n            content = '{% raw %}' + self.content + '{% endraw %}' #  Jinja2 模板引擎中的语法标记\n        else:\n            content = self.content\n\n        return ChatMessagePromptTemplate.from_template(\n            template=content,\n            template_format=\"jinja2\",\n            role=role,\n        )\n    \n    @classmethod\n    def from_data(cls, h: Union[List, Tuple, Dict]) -> \"History\":\n        if isinstance(h, (list, tuple)) and len(h) >= 2:\n            h = cls(role=h[0], content=h[1])\n        elif isinstance(h, dict):\n            h = cls(**h)\n\n        return h\n\n        \n\n"}
{"type": "source_file", "path": "server/chat/agent_chat.py", "content": "# import json\n# import asyncio\n\n# from fastapi import Body\n# from sse_starlette.sse import EventSourceResponse\n# from configs import LLM_MODELS, TEMPERATURE, HISTORY_LEN, Agent_MODEL\n\n# from langchain.chains import LLMChain\n# from langchain.memory import ConversationBufferWindowMemory\n# from langchain.agents import LLMSingleActionAgent, AgentExecutor\n# from typing import AsyncIterable, Optional, List\n\n# from server.utils import wrap_done, get_ChatOpenAI, get_prompt_template\n# from server.knowledge_base.kb_service.base import get_kb_details\n# from server.agent.custom_agent.ChatGLM3Agent import initialize_glm3_agent\n# from server.agent.tools_select import tools, tool_names\n# from server.agent.callbacks import CustomAsyncIteratorCallbackHandler, Status\n# from server.chat.utils import History\n# from server.agent import model_container\n# from server.agent.custom_template import CustomOutputParser, CustomPromptTemplate\n\n\n# async def agent_chat(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n#                      history: List[History] = Body([],\n#                                                    description=\"历史对话\",\n#                                                    examples=[[\n#                                                        {\"role\": \"user\", \"content\": \"请使用知识库工具查询今天北京天气\"},\n#                                                        {\"role\": \"assistant\",\n#                                                         \"content\": \"使用天气查询工具查询到今天北京多云，10-14摄氏度，东北风2级，易感冒\"}]]\n#                                                    ),\n#                      stream: bool = Body(False, description=\"流式输出\"),\n#                      model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n#                      temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n#                      max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n#                      prompt_name: str = Body(\"default\",\n#                                              description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n#                      ):\n#     history = [History.from_data(h) for h in history]\n\n#     async def agent_chat_iterator(\n#             query: str,\n#             history: Optional[List[History]],\n#             model_name: str = LLM_MODELS[0],\n#             prompt_name: str = prompt_name,\n#     ) -> AsyncIterable[str]:\n#         nonlocal max_tokens\n#         callback = CustomAsyncIteratorCallbackHandler()\n#         if isinstance(max_tokens, int) and max_tokens <= 0:\n#             max_tokens = None\n\n#         model = get_ChatOpenAI(\n#             model_name=model_name,\n#             temperature=temperature,\n#             max_tokens=max_tokens,\n#             callbacks=[callback],\n#         )\n\n#         kb_list = {x[\"kb_name\"]: x for x in get_kb_details()}\n#         model_container.DATABASE = {name: details['kb_info'] for name, details in kb_list.items()}\n\n#         if Agent_MODEL:\n#             model_agent = get_ChatOpenAI(\n#                 model_name=Agent_MODEL,\n#                 temperature=temperature,\n#                 max_tokens=max_tokens,\n#                 callbacks=[callback],\n#             )\n#             model_container.MODEL = model_agent\n#         else:\n#             model_container.MODEL = model\n\n#         prompt_template = get_prompt_template(\"agent_chat\", prompt_name)\n#         prompt_template_agent = CustomPromptTemplate(\n#             template=prompt_template,\n#             tools=tools,\n#             input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n#         )\n#         output_parser = CustomOutputParser()\n#         llm_chain = LLMChain(llm=model, prompt=prompt_template_agent)\n#         memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)\n#         for message in history:\n#             if message.role == 'user':\n#                 memory.chat_memory.add_user_message(message.content)\n#             else:\n#                 memory.chat_memory.add_ai_message(message.content)\n#         if \"chatglm3\" in model_container.MODEL.model_name or \"zhipu-api\" in model_container.MODEL.model_name:\n#             agent_executor = initialize_glm3_agent(\n#                 llm=model,\n#                 tools=tools,\n#                 callback_manager=None,\n#                 prompt=prompt_template,\n#                 input_variables=[\"input\", \"intermediate_steps\", \"history\"],\n#                 memory=memory,\n#                 verbose=True,\n#             )\n#         else:\n#             agent = LLMSingleActionAgent(\n#                 llm_chain=llm_chain,\n#                 output_parser=output_parser,\n#                 stop=[\"\\nObservation:\", \"Observation\"],\n#                 allowed_tools=tool_names,\n#             )\n#             agent_executor = AgentExecutor.from_agent_and_tools(agent=agent,\n#                                                                 tools=tools,\n#                                                                 verbose=True,\n#                                                                 memory=memory,\n#                                                                 )\n#         while True:\n#             try:\n#                 task = asyncio.create_task(wrap_done(\n#                     agent_executor.acall(query, callbacks=[callback], include_run_info=True),\n#                     callback.done))\n#                 break\n#             except:\n#                 pass\n\n#         if stream:\n#             async for chunk in callback.aiter():\n#                 tools_use = []\n#                 # Use server-sent-events to stream the response\n#                 data = json.loads(chunk)\n#                 if data[\"status\"] == Status.start or data[\"status\"] == Status.complete:\n#                     continue\n#                 elif data[\"status\"] == Status.error:\n#                     tools_use.append(\"\\n```\\n\")\n#                     tools_use.append(\"工具名称: \" + data[\"tool_name\"])\n#                     tools_use.append(\"工具状态: \" + \"调用失败\")\n#                     tools_use.append(\"错误信息: \" + data[\"error\"])\n#                     tools_use.append(\"重新开始尝试\")\n#                     tools_use.append(\"\\n```\\n\")\n#                     yield json.dumps({\"tools\": tools_use}, ensure_ascii=False)\n#                 elif data[\"status\"] == Status.tool_finish:\n#                     tools_use.append(\"\\n```\\n\")\n#                     tools_use.append(\"工具名称: \" + data[\"tool_name\"])\n#                     tools_use.append(\"工具状态: \" + \"调用成功\")\n#                     tools_use.append(\"工具输入: \" + data[\"input_str\"])\n#                     tools_use.append(\"工具输出: \" + data[\"output_str\"])\n#                     tools_use.append(\"\\n```\\n\")\n#                     yield json.dumps({\"tools\": tools_use}, ensure_ascii=False)\n#                 elif data[\"status\"] == Status.agent_finish:\n#                     yield json.dumps({\"final_answer\": data[\"final_answer\"]}, ensure_ascii=False)\n#                 else:\n#                     yield json.dumps({\"answer\": data[\"llm_token\"]}, ensure_ascii=False)\n\n\n#         else:\n#             answer = \"\"\n#             final_answer = \"\"\n#             async for chunk in callback.aiter():\n#                 data = json.loads(chunk)\n#                 if data[\"status\"] == Status.start or data[\"status\"] == Status.complete:\n#                     continue\n#                 if data[\"status\"] == Status.error:\n#                     answer += \"\\n```\\n\"\n#                     answer += \"工具名称: \" + data[\"tool_name\"] + \"\\n\"\n#                     answer += \"工具状态: \" + \"调用失败\" + \"\\n\"\n#                     answer += \"错误信息: \" + data[\"error\"] + \"\\n\"\n#                     answer += \"\\n```\\n\"\n#                 if data[\"status\"] == Status.tool_finish:\n#                     answer += \"\\n```\\n\"\n#                     answer += \"工具名称: \" + data[\"tool_name\"] + \"\\n\"\n#                     answer += \"工具状态: \" + \"调用成功\" + \"\\n\"\n#                     answer += \"工具输入: \" + data[\"input_str\"] + \"\\n\"\n#                     answer += \"工具输出: \" + data[\"output_str\"] + \"\\n\"\n#                     answer += \"\\n```\\n\"\n#                 if data[\"status\"] == Status.agent_finish:\n#                     final_answer = data[\"final_answer\"]\n#                 else:\n#                     answer += data[\"llm_token\"]\n\n#             yield json.dumps({\"answer\": answer, \"final_answer\": final_answer}, ensure_ascii=False)\n#         await task\n\n#     return EventSourceResponse(agent_chat_iterator(query=query,\n#                                                    history=history,\n#                                                    model_name=model_name,\n#                                                    prompt_name=prompt_name),\n#                                )\n"}
{"type": "source_file", "path": "server/chat/knowledge_base_chat.py", "content": "import os \nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nfrom fastapi import Body, Request\nfrom sse_starlette.sse import EventSourceResponse\nfrom fastapi.concurrency import run_in_threadpool\nfrom configs import (LLM_MODELS,\n                     VECTOR_SEARCH_TOP_K,\n                     FUSION_K,\n                     SCORE_THRESHOLD,\n                     TEMPERATURE,\n                     USE_RERANKER,\n                     USE_QUERY_FUSION,\n                     RERANKER_MAX_LENGTH,\n                     RERANKER_MODEL,\n                     MODEL_PATH,\n                     MODEL_ROOT_PATH)\nfrom server.utils import wrap_done, get_ChatOpenAI\nfrom server.utils import BaseResponse, get_prompt_template\nfrom langchain.chains.llm import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, List, Optional\nimport asyncio\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom server.chat.utils import History\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nimport json\nfrom urllib.parse import urlencode\nfrom server.knowledge_base.kb_doc_api import search_docs, search_docs_query_fusion\nfrom server.reranker.reranker import LangchainReranker\nfrom server.utils import embedding_device\nfrom server.chat.rag_fusion import generate_queries\nfrom transformers import Pipeline, pipeline, AutoTokenizer, AutoModelForCausalLM\n\nasync def knowledge_base_chat(query: str = Body(..., description=\"用户输入\", examples=[\"你好\"]),\n                              knowledge_base_name: str = Body(..., description=\"知识库名称\", examples=[\"samples\"]),\n                              top_k: int = Body(VECTOR_SEARCH_TOP_K, description=\"匹配向量数\"),\n                              score_threshold: float = Body(\n                                  SCORE_THRESHOLD,\n                                  description=\"知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右\",\n                                  ge=0,\n                                  le=2\n                              ),\n                              history: List[History] = Body(\n                                  [],\n                                  description=\"历史对话\",\n                                  examples=[[\n                                      {\"role\": \"user\",\n                                       \"content\": \"我们来玩成语接龙，我先来，生龙活虎\"},\n                                      {\"role\": \"assistant\",\n                                       \"content\": \"虎头虎脑\"}]]\n                              ),\n                              stream: bool = Body(False, description=\"流式输出\"),\n                              model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                              temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                              max_tokens: Optional[int] = Body(\n                                  None,\n                                  description=\"限制LLM生成Token数量，默认None代表模型最大值\"\n                              ),\n                              prompt_name: str = Body(\n                                  \"default\",\n                                  description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"\n                              ),\n                              request: Request = None,\n                              ):\n    kb = KBServiceFactory.get_service_by_name(knowledge_base_name)\n    if kb is None:\n        return BaseResponse(code=404, msg=f\"未找到知识库 {knowledge_base_name}\")\n\n    history = [History.from_data(h) for h in history]\n\n    async def knowledge_base_chat_iterator(\n            query: str,\n            top_k: int,\n            history: Optional[List[History]],\n            model_name: str = model_name,\n            prompt_name: str = prompt_name,\n    ) -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n        )\n\n        if USE_QUERY_FUSION:\n            tokenizer = AutoTokenizer.from_pretrained(MODEL_ROOT_PATH + model_name, trust_remote_code=True)\n            transformer_model = AutoModelForCausalLM.from_pretrained(MODEL_ROOT_PATH + model_name, trust_remote_code=True)\n            pipe = pipeline('text-generation', model=transformer_model, tokenizer=tokenizer, \\\n                            max_length=256, temperature=0.6, top_p=0.95, repetition_penalty=1.2, device=embedding_device())\n            # 使用RAG-fusion\n            queries = generate_queries(original_query=query, pipe=pipe)\n            # 将query修改成queries\n            docs = await run_in_threadpool(search_docs_query_fusion,\n                                        queries=queries,\n                                        knowledge_base_name=knowledge_base_name,\n                                        top_k=top_k,\n                                        score_threshold=score_threshold)\n        else:\n            docs = await run_in_threadpool(search_docs,\n                                        query=query,\n                                        knowledge_base_name=knowledge_base_name,\n                                        top_k=top_k,\n                                        score_threshold=score_threshold)\n\n        # 加入reranker\n        if USE_RERANKER:\n            reranker_model_path = MODEL_PATH[\"reranker\"].get(RERANKER_MODEL,\"BAAI/bge-reranker-large\")\n            print(\"-----------------model path------------------\")\n            print(reranker_model_path)\n            reranker_model = LangchainReranker(top_n=top_k,\n                                            device=embedding_device(),\n                                            max_length=RERANKER_MAX_LENGTH,\n                                            model_name_or_path=reranker_model_path\n                                            )\n            print(docs)\n            docs = reranker_model.compress_documents(documents=docs,\n                                                     query=query)\n            print(\"---------after rerank------------------\")\n            print(docs)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n        # Retieval检索 -----------------------------------------------------\n\n        # Augmented增强（生成提示模板 / 设计生成摘要） -----------------------\n        if len(docs) == 0:  # 如果没有找到相关文档，使用empty模板\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", \"empty\")\n        else:\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", prompt_name)\n        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n        chat_prompt = ChatPromptTemplate.from_messages(\n            [i.to_msg_template() for i in history] + [input_msg])\n        \n        source_documents = []\n        for inum, doc in enumerate(docs):\n            filename = doc.metadata.get(\"source\")\n            parameters = urlencode({\"knowledge_base_name\": knowledge_base_name, \"file_name\": filename})\n            base_url = request.base_url\n            url = f\"{base_url}knowledge_base/download_doc?\" + parameters\n            text = f\"\"\"出处 [{inum + 1}] [{filename}]({url}) \\n\\n{doc.page_content}\\n\\n\"\"\"\n            source_documents.append(text)\n\n        if len(source_documents) == 0:  # 没有找到相关文档\n            source_documents.append(f\"<span style='color:red'>未找到相关文档,该回答为大模型自身能力解答！</span>\")\n\n        # Augmented增强（生成提示模板 / 设计生成摘要） -----------------------\n\n        # Generate生成 -----------------------------------------------------\n        chain = LLMChain(prompt=chat_prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"context\": context, \"question\": query}),\n            callback.done),\n        )\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps({\"answer\": token}, ensure_ascii=False)\n            yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps({\"answer\": answer,\n                              \"docs\": source_documents},\n                             ensure_ascii=False)\n        await task\n        # Generate生成 -----------------------------------------------------\n\n    return EventSourceResponse(knowledge_base_chat_iterator(query, top_k, history,model_name,prompt_name))\n"}
{"type": "source_file", "path": "server/chat/file_chat.py", "content": "from fastapi import Body, File, Form, UploadFile\nfrom sse_starlette.sse import EventSourceResponse\nfrom configs import (LLM_MODELS, VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, TEMPERATURE,\n                     CHUNK_SIZE, OVERLAP_SIZE, ZH_TITLE_ENHANCE)\nfrom server.utils import (wrap_done, get_ChatOpenAI,\n                        BaseResponse, get_prompt_template, get_temp_dir, run_in_thread_pool)\nfrom server.knowledge_base.kb_cache.faiss_cache import memo_faiss_pool\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, List, Optional\nimport asyncio\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom server.chat.utils import History\nfrom server.knowledge_base.kb_service.base import EmbeddingsFunAdapter\nfrom server.knowledge_base.utils import KnowledgeFile\nimport json\nimport os\n\ndef _parse_files_in_thread(\n    files: List[UploadFile],\n    dir: str,\n    zh_title_enhance: bool,\n    chunk_size: int,\n    chunk_overlap: int,\n):\n    \"\"\"\n    通过多线程将上传的文件保存到对应目录内。\n    生成器返回保存结果：[success or error, filename, msg, docs]\n    \"\"\"\n    def parse_file(file: UploadFile) -> dict:\n        '''\n        保存单个文件。\n        '''\n        try:\n            filename = file.filename\n            file_path = os.path.join(dir, filename)\n            file_content = file.file.read()  # 读取上传文件的内容\n\n            if not os.path.isdir(os.path.dirname(file_path)):\n                os.makedirs(os.path.dirname(file_path))\n            with open(file_path, \"wb\") as f:\n                f.write(file_content)\n            kb_file = KnowledgeFile(filename=filename, knowledge_base_name=\"temp\")\n            kb_file.filepath = file_path\n            docs = kb_file.file2text(zh_title_enhance=zh_title_enhance,\n                                     chunk_size=chunk_size,\n                                     chunk_overlap=chunk_overlap)\n            return True, filename, f\"成功上传文件 {filename}\", docs\n        except Exception as e:\n            msg = f\"{filename} 文件上传失败，报错信息为: {e}\"\n            return False, filename, msg, []\n\n    params = [{\"file\": file} for file in files]\n    for result in run_in_thread_pool(parse_file, params=params):\n        yield result\n\ndef upload_temp_docs(\n    files: List[UploadFile] = File(..., description=\"上传文件，支持多文件\"),\n    prev_id: str = Form(None, description=\"前知识库ID\"),\n    chunk_size: int = Form(CHUNK_SIZE, description=\"知识库中单段文本最大长度\"),\n    chunk_overlap: int = Form(OVERLAP_SIZE, description=\"知识库中相邻文本重合长度\"),\n    zh_title_enhance: bool = Form(ZH_TITLE_ENHANCE, description=\"是否开启中文标题加强\"),\n) -> BaseResponse:\n    '''\n    将文件保存到临时目录，并进行向量化。\n    返回临时目录名称作为ID，同时也是临时向量库的ID。\n    '''\n    if prev_id is not None:\n        memo_faiss_pool.pop(prev_id)\n\n    failed_files = []\n    documents = []\n    path, id = get_temp_dir(prev_id)\n    for success, file, msg, docs in _parse_files_in_thread(files=files,\n                                                        dir=path,\n                                                        zh_title_enhance=zh_title_enhance,\n                                                        chunk_size=chunk_size,\n                                                        chunk_overlap=chunk_overlap):\n        if success:\n            documents += docs\n        else:\n            failed_files.append({file: msg})\n\n    with memo_faiss_pool.load_vector_store(id).acquire() as vs:\n        vs.add_documents(documents)\n    return BaseResponse(data={\"id\": id, \"failed_files\": failed_files})\n\nasync def file_chat(query: str = Body(..., description=\"用户输入\", examples=[\"你好\"]),\n                    knowledge_id: str = Body(..., description=\"临时知识库ID\"),\n                    top_k: int = Body(VECTOR_SEARCH_TOP_K, description=\"匹配向量数\"),\n                    score_threshold: float = Body(SCORE_THRESHOLD, description=\"知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右\", ge=0, le=2),\n                    history: List[History] = Body([],\n                                                description=\"历史对话\",\n                                                examples=[[\n                                                    {\"role\": \"user\",\n                                                    \"content\": \"我们来玩成语接龙，我先来，生龙活虎\"},\n                                                    {\"role\": \"assistant\",\n                                                    \"content\": \"虎头虎脑\"}]]\n                                                ),\n                    stream: bool = Body(False, description=\"流式输出\"),\n                    model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                    temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                    max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                    prompt_name: str = Body(\"default\", description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                ):\n    '''自行上传文件建立临时知识库进行问答'''\n    if knowledge_id not in memo_faiss_pool.keys():\n        return BaseResponse(code=404, msg=f\"未找到临时知识库 {knowledge_id}，请先上传文件\")\n\n    history = [History.from_data(h) for h in history]\n\n    async def knowledge_base_chat_iterator() -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n        )\n        embed_func = EmbeddingsFunAdapter()\n        embeddings = await embed_func.aembed_query(query)\n        with memo_faiss_pool.acquire(knowledge_id) as vs:\n            docs = vs.similarity_search_with_score_by_vector(embeddings, k=top_k, score_threshold=score_threshold)\n            docs = [x[0] for x in docs]\n\n        context = \"\\n\".join([doc.page_content for doc in docs])\n        if len(docs) == 0: ## 如果没有找到相关文档，使用Empty模板\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", \"empty\")\n        else:\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", prompt_name)\n        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n        chat_prompt = ChatPromptTemplate.from_messages(\n            [i.to_msg_template() for i in history] + [input_msg])\n\n        chain = LLMChain(prompt=chat_prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"context\": context, \"question\": query}),\n            callback.done),\n        )\n\n        source_documents = []\n        for inum, doc in enumerate(docs):\n            filename = doc.metadata.get(\"source\")\n            text = f\"\"\"出处 [{inum + 1}] [{filename}] \\n\\n{doc.page_content}\\n\\n\"\"\"\n            source_documents.append(text)\n\n        if len(source_documents) == 0: # 没有找到相关文档\n            source_documents.append(f\"\"\"<span style='color:red'>未找到相关文档,该回答为大模型自身能力解答！</span>\"\"\")\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps({\"answer\": token}, ensure_ascii=False)\n            yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps({\"answer\": answer,\n                              \"docs\": source_documents},\n                             ensure_ascii=False)\n        await task\n\n    return EventSourceResponse(knowledge_base_chat_iterator())\n"}
{"type": "source_file", "path": "server/chat/search_engine_chat.py", "content": "from langchain.utilities.bing_search import BingSearchAPIWrapper\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\nfrom configs import (BING_SEARCH_URL, BING_SUBSCRIPTION_KEY, METAPHOR_API_KEY,\n                     LLM_MODELS, SEARCH_ENGINE_TOP_K, TEMPERATURE, OVERLAP_SIZE)\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\n\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom fastapi import Body\nfrom fastapi.concurrency import run_in_threadpool\nfrom sse_starlette.sse import EventSourceResponse\nfrom server.utils import wrap_done, get_ChatOpenAI\nfrom server.utils import BaseResponse, get_prompt_template\nfrom server.chat.utils import History\nfrom typing import AsyncIterable, List, Optional, Dict\nimport asyncio\nimport json\nfrom strsimpy.normalized_levenshtein import NormalizedLevenshtein\nfrom markdownify import markdownify\n\n\ndef bing_search(text, result_len=SEARCH_ENGINE_TOP_K, **kwargs):\n    '''调用langchain的接口进行必应搜索'''\n    if not (BING_SEARCH_URL and BING_SUBSCRIPTION_KEY):\n        return [{\"snippet\": \"please set BING_SUBSCRIPTION_KEY and BING_SEARCH_URL in os ENV\",\n                 \"title\": \"env info is not found\",\n                 \"link\": \"https://python.langchain.com/en/latest/modules/agents/tools/examples/bing_search.html\"}]\n    search = BingSearchAPIWrapper(bing_subscription_key=BING_SUBSCRIPTION_KEY,\n                                  bing_search_url=BING_SEARCH_URL)\n    return search.results(text, result_len)\n\ndef duckduckgo_search(text, result_len=SEARCH_ENGINE_TOP_K, **kwargs):\n    '''轻量搜索引擎'''\n    search = DuckDuckGoSearchAPIWrapper()\n    return search.results(text, result_len)\n\ndef metaphor_search(\n        text: str,\n        result_len: int = SEARCH_ENGINE_TOP_K,\n        split_result: bool = False,\n        chunk_size: int = 500,\n        chunk_overlap: int = OVERLAP_SIZE,\n) -> List[Dict]:\n    '''使用Metaphor的神经搜索算法，为AI提供来自互联网的最高质量的信息的东西。'''\n    from metaphor_python import Metaphor\n\n    if not METAPHOR_API_KEY:\n        return []\n\n    client = Metaphor(METAPHOR_API_KEY)\n    search = client.search(text, num_results=result_len, use_autoprompt=True)\n    contents = search.get_contents().contents\n    for x in contents:\n        x.extract = markdownify(x.extract)\n\n    # metaphor 返回的内容都是长文本，需要分词再检索\n    if split_result:\n        docs = [Document(page_content=x.extract,\n                         metadata={\"link\": x.url, \"title\": x.title})\n                for x in contents]\n        text_splitter = RecursiveCharacterTextSplitter([\"\\n\\n\", \"\\n\", \".\", \" \"],\n                                                       chunk_size=chunk_size,\n                                                       chunk_overlap=chunk_overlap)\n        splitted_docs = text_splitter.split_documents(docs)\n\n        # 将切分好的文档放入临时向量库，重新筛选出TOP_K个文档\n        if len(splitted_docs) > result_len:\n            normal = NormalizedLevenshtein()\n            for x in splitted_docs:\n                x.metadata[\"score\"] = normal.similarity(text, x.page_content)\n            splitted_docs.sort(key=lambda x: x.metadata[\"score\"], reverse=True)\n            splitted_docs = splitted_docs[:result_len]\n\n        docs = [{\"snippet\": x.page_content,\n                 \"link\": x.metadata[\"link\"],\n                 \"title\": x.metadata[\"title\"]}\n                for x in splitted_docs]\n    else:\n        docs = [{\"snippet\": x.extract,\n                 \"link\": x.url,\n                 \"title\": x.title}\n                for x in contents]\n\n    return docs\n\n\nSEARCH_ENGINES = {\"bing\": bing_search,\n                  \"duckduckgo\": duckduckgo_search,\n                  \"metaphor\": metaphor_search,\n                  }\n\ndef search_result2docs(search_results):\n    '''将搜索结果转化为数据库存的文档形式 page_content, metadata'''\n    docs = []\n    for result in search_results:\n        doc = Document(page_content=result[\"snippet\"] if \"snippet\" in result.keys() else \"\",\n                       metadata={\"source\": result[\"link\"] if \"link\" in result.keys() else \"\",\n                                 \"filename\": result[\"title\"] if \"title\" in result.keys() else \"\"})\n        docs.append(doc)\n    return docs\n\nasync def lookup_search_engine(\n        query: str,\n        search_engine_name: str,\n        top_k: int = SEARCH_ENGINE_TOP_K,\n        split_result: bool = False,\n):\n    '''利用浏览器检索'''\n    search_engine = SEARCH_ENGINES[search_engine_name]\n    results = await run_in_threadpool(search_engine, query, result_len=top_k, split_result=split_result)\n    docs = search_result2docs(results)\n    return docs\n\nasync def search_engine_chat(query: str = Body(..., description=\"用户输入\", examples=[\"你好\"]),\n                             search_engine_name: str = Body(..., description=\"搜索引擎名称\", examples=[\"duckduckgo\"]),\n                             top_k: int = Body(SEARCH_ENGINE_TOP_K, description=\"检索结果数量\"),\n                             history: List[History] = Body([],\n                                                           description=\"历史对话\",\n                                                           examples=[[\n                                                               {\"role\": \"user\",\n                                                                \"content\": \"我们来玩成语接龙，我先来，生龙活虎\"},\n                                                               {\"role\": \"assistant\",\n                                                                \"content\": \"虎头虎脑\"}]]\n                                                           ),\n                             stream: bool = Body(False, description=\"流式输出\"),\n                             model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                             temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                             max_tokens: Optional[int] = Body(None,\n                                                              description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                             prompt_name: str = Body(\"default\",\n                                                     description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                             split_result: bool = Body(False,\n                                                       description=\"是否对搜索结果进行拆分（主要用于metaphor搜索引擎）\")\n                             ):\n    if search_engine_name not in SEARCH_ENGINES.keys():\n        return BaseResponse(code=404, msg=f\"未支持搜索引擎 {search_engine_name}\")\n\n    if search_engine_name == \"bing\" and not BING_SUBSCRIPTION_KEY:\n        return BaseResponse(code=404, msg=f\"要使用Bing搜索引擎，需要设置 `BING_SUBSCRIPTION_KEY`\")\n\n    history = [History.from_data(h) for h in history]\n\n    async def search_engine_chat_iterator(query: str,\n                                          search_engine_name: str,\n                                          top_k: int,\n                                          history: Optional[List[History]],\n                                          model_name: str = LLM_MODELS[0],\n                                          prompt_name: str = prompt_name,\n                                          ) -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n        )\n\n        docs = await lookup_search_engine(query, search_engine_name, top_k, split_result=split_result)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n\n        prompt_template = get_prompt_template(\"search_engine_chat\", prompt_name)\n        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n        chat_prompt = ChatPromptTemplate.from_messages(\n            [i.to_msg_template() for i in history] + [input_msg])\n        \n        chain = LLMChain(prompt=chat_prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"context\": context, \"question\": query}),\n            callback.done),\n        )\n\n        source_documents = [\n            f\"\"\"出处 [{inum + 1}] [{doc.metadata[\"source\"]}]({doc.metadata[\"source\"]}) \\n\\n{doc.page_content}\\n\\n\"\"\"\n            for inum, doc in enumerate(docs)\n        ]\n\n        if len(source_documents) == 0:  # 没有找到相关资料（不太可能）\n            source_documents.append(f\"\"\"<span style='color:red'>未找到相关文档,该回答为大模型自身能力解答！</span>\"\"\")\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps({\"answer\": token}, ensure_ascii=False)\n            yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps({\"answer\": answer,\n                              \"docs\": source_documents},\n                             ensure_ascii=False)\n        await task\n\n    return EventSourceResponse(search_engine_chat_iterator(query=query,\n                                                           search_engine_name=search_engine_name,\n                                                           top_k=top_k,\n                                                           history=history,\n                                                           model_name=model_name,\n                                                           prompt_name=prompt_name),\n                               )\n\n"}
{"type": "source_file", "path": "server/db/models/knowledge_file_model.py", "content": "from sqlalchemy import Column, Integer, String, DateTime, Float, Boolean, JSON, func\nfrom server.db.base import Base\n\nclass KnowledgeFileModel(Base):\n    \"\"\"\n    知识文件模型\n    \"\"\"\n    __tablename__ = 'knowledge_file'\n    id = Column(Integer, primary_key=True, autoincrement=True, comment='知识文件ID')\n    file_name = Column(String(255), comment='文件名')\n    file_ext = Column(String(10), comment='文件扩展名')\n    kb_name = Column(String(50), comment='所属知识库名称')\n    document_loader_name = Column(String(50), comment='文档加载器名称')\n    text_splitter_name = Column(String(50), comment='文本分割器名称')\n    file_version = Column(Integer, default=1, comment='文件版本')\n    file_mtime = Column(Float, default=0.0, comment=\"文件修改时间\")\n    file_size = Column(Integer, default=0, comment=\"文件大小\")\n    custom_docs = Column(Boolean, default=False, comment=\"是否自定义docs\")\n    docs_count = Column(Integer, default=0, comment=\"切分文档数量\")\n    create_time = Column(DateTime, default=func.now(), comment='创建时间')\n\n    def __repr__(self):\n        return f\"<KnowledgeFile(id='{self.id}', file_name='{self.file_name}', file_ext='{self.file_ext}', kb_name='{self.kb_name}', document_loader_name='{self.document_loader_name}', text_splitter_name='{self.text_splitter_name}', file_version='{self.file_version}', create_time='{self.create_time}')>\"\n\n\nclass FileDocModel(Base):\n    \"\"\"\n    文件-向量库文档模型\n    \"\"\"\n    __tablename__ = 'file_doc'\n    id = Column(Integer, primary_key=True, autoincrement=True, comment='ID')\n    kb_name = Column(String(50), comment='知识库名称')\n    file_name = Column(String(255), comment='文件名称')\n    doc_id = Column(String(50), comment=\"向量库文档ID\")\n    meta_data = Column(JSON, default={})\n\n    def __repr__(self):\n        return f\"<FileDoc(id='{self.id}', kb_name='{self.kb_name}', file_name='{self.file_name}', doc_id='{self.doc_id}', metadata='{self.meta_data}')>\"\n"}
{"type": "source_file", "path": "server/db/repository/knowledge_file_repository.py", "content": "from server.db.models.knowledge_base_model import KnowledgeBaseModel\nfrom server.db.models.knowledge_file_model import KnowledgeFileModel, FileDocModel\nfrom server.db.session import with_session\nfrom server.knowledge_base.utils import KnowledgeFile\nfrom typing import List, Dict\n\n@with_session\ndef list_file_num_docs_id_by_kb_name_and_file_name(session,\n                                                   kb_name: str,\n                                                   file_name: str,\n                                                   ) -> List[int]:\n    '''\n    列出某知识库某文件对应的所有Document的id。\n    返回形式：[str, ...]\n    '''\n    doc_ids = session.query(FileDocModel.doc_id).filter_by(kb_name=kb_name, file_name=file_name).all()\n    return [int(_id[0]) for _id in doc_ids]\n\n@with_session\ndef list_docs_from_db(session,\n                      kb_name: str,\n                      file_name: str = None,\n                      metadata: Dict = {}\n                      ) -> List[Dict]:\n    '''\n    列出某知识库某文件对应的所有Document。\n    返回形式：[{\"id\": str, \"metadata\": dict}, ...]\n    '''\n    docs = session.query(FileDocModel).filter(FileDocModel.kb_name.ilike(kb_name))\n    if file_name:\n        docs = docs.filter(FileDocModel.file_name.ilike(file_name))\n    for k, v in metadata.items():\n        docs = docs.filter(FileDocModel.meta_data[k].as_string() == str(v))\n        \n    return [{'id': x.doc_id, 'metadata': x.metadata} for x in docs.all()]\n\n@with_session\ndef delete_docs_from_db(session,\n                        kb_name: str,\n                        file_name: str = None,\n                        ) -> List[Dict]:\n    '''\n    删除某知识库某文件对应的所有Document，并返回被删除的Document。\n    返回形式：[{\"id\": str, \"metadata\": dict}, ...]\n    '''\n    docs = list_docs_from_db(kb_name=kb_name, file_name=file_name)\n    query = session.query(FileDocModel).filter(FileDocModel.kb_name.ilike(kb_name))\n    if file_name:\n        query = query.filter(FileDocModel.file_name.ilike(file_name))\n    query.delete(synchronize_session=False)\n    session.commit()\n    return docs\n\n@with_session\ndef add_docs_to_db(session,\n                   kb_name: str,\n                   file_name: str,\n                   doc_infos: List[Dict]) -> bool:\n    '''\n    将某知识库某文件对应的所有Document信息添加到数据库。\n    doc_infos形式：[{\"id\": str, \"metadata\": dict}, ...]\n    '''\n    if doc_infos is None:\n        print(\"输入的server.db.repository.knowledge_file_repository.add_docs_to_db的doc_infos参数为None\")\n        return False\n    for d in doc_infos:\n        obj = FileDocModel(\n            kb_name=kb_name,\n            file_name=file_name,\n            doc_id=d['id'],\n            meta_data=d['metadata'],\n        )\n        session.add(obj)\n    return True\n\n@with_session\ndef count_files_from_db(session, kb_name: str) -> int:\n    '''统计db中文件数量'''\n    return session.query(KnowledgeFileModel).filter(KnowledgeFileModel.kb_name.ilike(kb_name)).count()\n\n@with_session\ndef list_files_from_db(session, kb_name: str):\n    '''列举出db kb_name数据库中的文件'''\n    files = session.query(KnowledgeFileModel).filter(KnowledgeFileModel.kb_name.ilike(kb_name)).all()\n    docs = [f.file_name for f in files]\n    return docs\n\n@with_session\ndef add_file_to_db(session,\n                   kb_file: KnowledgeFile,\n                   docs_count: int = 0,\n                   custom_docs: bool = False,\n                   doc_infos: List[Dict] = []):\n    '''添加文件'''\n    kb = session.query(KnowledgeBaseModel).filter_by(kb_name=kb_file.kb_name).first()\n    if kb:\n        # 如果存在文件则进行更新即可\n        existing_file: KnowledgeFileModel = (session.query(KnowledgeFileModel).filter(\n            KnowledgeFileModel.kb_name.ilike(kb_file.kb_name),\n            KnowledgeFileModel.file_name.ilike(kb_file.filename)\n        ).first())\n        mtime = kb_file.get_mtime()\n        size = kb_file.get_size()\n\n        if existing_file:\n            existing_file.file_mtime = mtime\n            existing_file.file_size = size\n            existing_file.docs_count = docs_count\n            existing_file.custom_docs = custom_docs\n            existing_file.file_version += 1\n        # 否则添加新文件\n        else:\n            new_file = KnowledgeFileModel(\n                file_name=kb_file.filename,\n                file_ext=kb_file.ext,\n                kb_name=kb_file.kb_name,\n                document_loader_name=kb_file.document_loader_name,\n                text_splitter_name=kb_file.text_splitter_name or \"SpacyTextSplitter\",\n                file_mtime=mtime,\n                file_size=size,\n                docs_count=docs_count,\n                custom_docs=custom_docs,\n            )\n            kb.file_count += 1\n            session.add(new_file)\n\n        add_docs_to_db(kb_name=kb_file.kb_name, file_name=kb_file.filename, doc_infos=doc_infos)\n    \n    return True\n\n@with_session\ndef delete_file_from_db(session, kb_file: KnowledgeFile):\n    '''删除文件'''\n    existing_file = session.query(KnowledgeFileModel).filter(\n        KnowledgeFileModel.file_name.ilike(kb_file.filename),\n        KnowledgeFileModel.kb_name.ilike(kb_file.kb_name)\n    ).first()\n    if existing_file:\n        # 删除文件\n        session.delete(existing_file)\n        delete_docs_from_db(kb_name=kb_file.kb_name, file_name=kb_file.filename)\n        session.commit()\n\n        # 对应知识库文件的数量 -1\n        kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_file.kb_name)).first()\n        if kb:\n            kb.file_count -= 1\n            session.commit()\n    return True\n\n@with_session\ndef delete_files_from_db(session, kb_name: str):\n    '''删除对应知识库的所有文件'''\n    session.query(KnowledgeFileModel).filter(KnowledgeFileModel.kb_name.ilike(kb_name)).delete(\n        synchronize_session=False)\n    session.query(FileDocModel).filter(FileDocModel.kb_name.ilike(kb_name)).delete(\n        synchronize_session=False)\n    kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    if kb:\n        kb.file_count = 0\n\n    session.commit()\n    return True\n\n@with_session\ndef file_exists_in_db(session, kb_file: KnowledgeFile):\n    '''判断文件存在不'''\n    existing_file = (session.query(KnowledgeFileModel)\n                     .filter(KnowledgeFileModel.file_name.ilike(kb_file.filename),\n                             KnowledgeFileModel.kb_name.ilike(kb_file.kb_name))\n                     .first())\n    return True if existing_file else False\n\n@with_session\ndef get_file_detail(session, kb_name: str, filename: str) -> dict:\n    '''获取文件详细信息'''\n    file: KnowledgeFileModel = (session.query(KnowledgeFileModel)\n                                .filter(KnowledgeFileModel.file_name.ilike(filename),\n                                        KnowledgeFileModel.kb_name.ilike(kb_name))\n                                .first())\n    if file:\n        return {\n            \"kb_name\": file.kb_name,\n            \"file_name\": file.file_name,\n            \"file_ext\": file.file_ext,\n            \"file_version\": file.file_version,\n            \"document_loader\": file.document_loader_name,\n            \"text_splitter\": file.text_splitter_name,\n            \"create_time\": file.create_time,\n            \"file_mtime\": file.file_mtime,\n            \"file_size\": file.file_size,\n            \"custom_docs\": file.custom_docs,\n            \"docs_count\": file.docs_count,\n        }\n    else:\n        return {}\n"}
{"type": "source_file", "path": "server/db/repository/knowledge_metadata_repository.py", "content": "from server.db.models.knowledge_metadata_model import SummaryChunkModel\nfrom server.db.session import with_session\nfrom typing import List, Dict\n\n@with_session\ndef list_summary_from_db(session,\n                         kb_name: str,\n                         metadata: Dict = {}\n                         ) -> List[Dict]:\n    '''\n    列出某知识库chunk summary。\n    返回形式：[{\"id\": str, \"summary_context\": str, \"doc_ids\": str}, ...]\n    '''\n    docs = session.query(SummaryChunkModel).filter(SummaryChunkModel.kb_name.ilike(kb_name))\n    for k, v in metadata.items():\n        docs = docs.filter(SummaryChunkModel.meta_data[k].as_string() == str(v))\n    \n    return [{\"id\": x.id,\n             \"summary_context\": x.summary_context,\n             \"summary_id\": x.summary_id,\n             \"doc_ids\": x.doc_ids,\n             \"metadata\": x.metadata} for x in docs.all()]\n\n@with_session\ndef delete_summary_from_db(session,\n                           kb_name: str\n                           ) -> List[Dict]:\n    '''\n    删除知识库chunk summary，并返回被删除的Dchunk summary。\n    返回形式：[{\"id\": str, \"summary_context\": str, \"doc_ids\": str}, ...]\n    '''\n    docs = list_summary_from_db(kb_name=kb_name)\n    query = session.query(SummaryChunkModel).filter(SummaryChunkModel.kb_name.ilike(kb_name))\n    query.delete(synchronize_session=False)\n    session.commit()\n    return docs\n\n@with_session\ndef add_summary_to_db(session,\n                      kb_name: str,\n                      summary_infos: List[Dict]):\n    '''\n    将总结信息添加到数据库。\n    summary_infos形式：[{\"summary_context\": str, \"doc_ids\": str}, ...]\n    '''\n    for summary in summary_infos:\n        obj = SummaryChunkModel(\n            kb_name=kb_name,\n            summary_context=summary[\"summary_context\"],\n            summary_id=summary[\"summary_id\"],\n            doc_ids=summary[\"doc_ids\"],\n            meta_data=summary[\"metadata\"],\n        )\n        session.add(obj)\n\n    session.commit()\n    return True\n\n@with_session\ndef count_summary_from_db(session, kb_name: str) -> int:\n    return session.query(SummaryChunkModel).filter(SummaryChunkModel.kb_name.ilike(kb_name)).count()\n"}
{"type": "source_file", "path": "server/db/models/message_model.py", "content": "from sqlalchemy import Column, Integer, String, DateTime, JSON, func\nfrom server.db.base import Base\n\nclass MessageModel(Base):\n    \"\"\"\n    聊天记录模型\n    \"\"\"\n    __tablename__ = 'message'\n    id = Column(String(32), primary_key=True, comment='聊天记录ID')\n    conversation_id = Column(String(32), default=None, index=True, comment='对话框ID')\n    # chat/agent_chat等\n    chat_type = Column(String(50), comment='聊天类型')\n    query = Column(String(4096), comment='用户问题')\n    response = Column(String(4096), comment='模型回答')\n    # 记录知识库id等，以便后续扩展\n    meta_data = Column(JSON, default={})\n    # 满分100 越高表示评价越好\n    feedback_score = Column(Integer, default=-1, comment='用户评分')\n    feedback_reason = Column(String(255), default=\"\", comment='用户评分理由')\n    create_time = Column(DateTime, default=func.now(), comment='创建时间')\n\n    def __repr__(self):\n        return f\"<message(id='{self.id}', conversation_id='{self.conversation_id}', chat_type='{self.chat_type}', query='{self.query}', response='{self.response}',meta_data='{self.meta_data}',feedback_score='{self.feedback_score}',feedback_reason='{self.feedback_reason}', create_time='{self.create_time}')>\"\n"}
{"type": "source_file", "path": "server/db/repository/__init__.py", "content": "from .conversation_repository import *\nfrom .message_repository import *\nfrom .knowledge_metadata_repository import *\nfrom .knowledge_base_repository import *\n"}
{"type": "source_file", "path": "server/db/repository/knowledge_base_repository.py", "content": "from server.db.models.knowledge_base_model import KnowledgeBaseModel\nfrom server.db.session import with_session\nfrom typing import List\n\n@with_session\ndef add_kb_to_db(session, kb_name, kb_info, vs_type, embed_model):\n    '''创建/更新知识库实例加入数据库'''\n    kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    if not kb:\n        kb = KnowledgeBaseModel(kb_name=kb_name, kb_info=kb_info, vs_type=vs_type, embed_model=embed_model)\n        session.add(kb)\n    else: # 如果已经存在就进行更新即可\n        kb.vs_type = vs_type\n        kb.kb_info = kb_info\n        kb.embed_model = embed_model\n    return True\n\n@with_session\ndef list_kbs_from_db(session, min_file_count: int = -1) -> List:\n    '''列出数据库中含有的知识库'''\n    kbs = session.query(KnowledgeBaseModel.kb_name).filter(KnowledgeBaseModel.file_count > min_file_count).all()\n    kbs = [kb[0] for kb in kbs]\n    return kbs\n\n@with_session\ndef kb_exists(session, kb_name):\n    '''判断知识库存不存在'''\n    kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    status = True if kb else False\n    return status\n\n@with_session\ndef load_kb_from_db(session, kb_name):\n    '''从数据库中加载对应知识库'''\n    kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    if kb:\n        kb_name, vs_type, embed_model = kb.kb_name, kb.vs_type, kb.embed_model\n    else:\n        kb_name, vs_type, embed_model = None, None, None\n    return kb_name, vs_type, embed_model\n\n@with_session\ndef delete_kb_from_db(session, kb_name):\n    '''从数据库中删除对应知识库'''\n    kb = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    if kb:\n        session.delete(kb)\n    return True\n\n@with_session\ndef get_kb_detail(session, kb_name: str) -> dict:\n    '''获取知识库的详细信息'''\n    kb: KnowledgeBaseModel = session.query(KnowledgeBaseModel).filter(KnowledgeBaseModel.kb_name.ilike(kb_name)).first()\n    if kb:\n        return {\n            \"kb_name\": kb.kb_name,\n            \"kb_info\": kb.kb_info,\n            \"vs_type\": kb.vs_type,\n            \"embed_model\": kb.embed_model,\n            \"file_count\": kb.file_count,\n            \"create_time\": kb.create_time,\n        }\n    else:\n        return {}\n"}
{"type": "source_file", "path": "server/db/repository/conversation_repository.py", "content": "from server.db.session import with_session\nimport uuid\nfrom server.db.models.conversation_model import ConversationModel\n\n@with_session\ndef add_conversation_to_db(session, chat_type, name=\"\", conversation_id=None):\n    '''新增聊天记录'''\n    if not conversation_id:\n        conversation_id = uuid.uuid4().hex # Python 中生成 UUID（Universally Unique Identifier，通用唯一标识符）的一种方式。\n    c = ConversationModel(id=conversation_id, name=name, chat_type=chat_type)\n\n    # add()方法用于将对象添加到会话中，表示将对象添加到数据库会话的待处理队列中。这意味着对象被暂时标记为“待插入”状态，但还没有真正被插入到数据库中。\n    session.add(c)\n    return c.id\n"}
{"type": "source_file", "path": "server/db/repository/message_repository.py", "content": "from server.db.session import with_session\nfrom typing import Dict, List\nimport uuid\nfrom server.db.models.message_model import MessageModel\n\n@with_session\ndef add_message_to_db(session, \n                      conversation_id: str, \n                      chat_type, \n                      query, \n                      response=\"\", \n                      message_id=None,\n                      metadata: Dict = {}):\n    '''\n    新增聊天记录\n    '''\n    if not message_id:\n        message_id = uuid.uuid4().hex\n    m = MessageModel(id=message_id, chat_type=chat_type,\n                     query=query, response=response,\n                     conversation_id=conversation_id,\n                     meta_data=metadata)\n    session.add(m)\n    session.commit()\n    return m.id\n\n@with_session\ndef update_message(session, message_id, response: str=None, metadata: Dict = None):\n    '''\n    更新现有的聊天记录\n    '''\n    m = get_message_by_id(message_id)\n    if m is not None:\n        if response is not None:\n            m.respnse = response\n        if isinstance(metadata, dict):\n            m.meta_data = metadata\n        session.add(m)\n        session.commit()\n        return m.id\n\n@with_session\ndef get_message_by_id(session, message_id) -> MessageModel:\n    '''\n    查询聊天记录\n    '''\n    m = session.query(MessageModel).filter_by(id=message_id).first()\n    return m\n\n@with_session\ndef feedback_message_to_db(session, message_id, feedback_score, feedback_reason):\n    '''\n    反馈聊天记录\n    '''\n    m = get_message_by_id(message_id)\n    if m:\n        m.feedback_score = feedback_score\n        m.feedback_reason = feedback_reason\n    session.commit()\n    return m.id\n\n@with_session\ndef filter_message(session, conversation_id: str, limit: int=10):\n    # 用户最新的query 也会插入到db，忽略这个message record\n    messages = (session.query(MessageModel).filter_by(conversation_id=conversation_id).filter(\n        MessageModel.response != ''\n    ).order_by(MessageModel.create_time.desc()).limit(limit).all())\n    data = []\n    for m in messages:\n        data.append({'query': m.query, 'response': m.response})\n    return data\n"}
{"type": "source_file", "path": "server/embeddings_api.py", "content": "import os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\nfrom langchain.docstore.document import Document\nfrom configs import EMBEDDING_MODEL, logger\nfrom server.utils import BaseResponse, get_model_worker_config, list_embed_models, list_online_embed_models\nfrom server.model_workers.base import ApiEmbeddingsParams\nfrom fastapi import Body\nfrom fastapi.concurrency import run_in_threadpool\nfrom typing import Dict, List\n\nonline_embed_models = list_online_embed_models()\n\ndef embed_texts(\n        texts: List[str],\n        embed_model: str = EMBEDDING_MODEL,\n        to_query: bool = False,\n) -> BaseResponse:\n    '''\n    对文本进行向量化。返回数据格式：BaseResponse(data=List[List[float]])\n    '''\n    try:\n        if embed_model in list_embed_models():  # 使用本地Embeddings模型\n            from server.utils import load_local_embeddings\n\n            embeddings = load_local_embeddings(model=embed_model)\n            return BaseResponse(data=embeddings.embed_documents(texts))\n        \n        if embed_model in list_online_embed_models: # 使用在线API\n            config = get_model_worker_config(embed_model)\n            worker_class = config.get('worker_class')\n            embed_model = config.get('embed_model')\n            worker = worker_class()\n            if worker_class.can_embedding():\n                params = ApiEmbeddingsParams(texts=texts, to_query=to_query, embed_model=embed_model)\n                resp = worker.do_embeddings(params)\n                return BaseResponse(**resp)\n            \n        return BaseResponse(code=500, msg=f\"指定的模型 {embed_model} 不支持 Embeddings 功能。\")\n    except Exception as e:\n        logger.error(e)\n        return BaseResponse(code=500, msg=f\"文本向量化过程中出现错误：{e}\")\n    \nasync def aembed_texts(\n    texts: List[str],\n    embed_model: str = EMBEDDING_MODEL,\n    to_query: bool = False,\n) -> BaseResponse:\n    '''异步实现：对文本向量化'''\n    try:\n        if embed_model in list_embed_models(): # 使用本地Embeddings模型\n            from server.utils import load_local_embeddings\n\n            embeddings = load_local_embeddings(model=embed_model)\n            return BaseResponse(data=await embeddings.aembed_documents(texts))\n\n        if embed_model in list_online_embed_models(): # 使用在线API\n            return await run_in_threadpool(embed_texts,\n                                           texts=texts,\n                                           embed_model=embed_model,\n                                           to_query=to_query)\n    except Exception as e:\n        logger.error(e)\n        return BaseResponse(code=500, msg=f\"文本向量化过程中出现错误：{e}\")\n\ndef embed_texts_endpoint(\n        texts: List[str] = Body(..., description=\"要嵌入的文本列表\", examples=[[\"hello\", \"world\"]]),\n        embed_model: str = Body(EMBEDDING_MODEL,\n                                description=f\"使用的嵌入模型，除了本地部署的Embedding模型，也支持在线API({online_embed_models})提供的嵌入服务。\"),\n        to_query: bool = Body(False, description=\"向量是否用于查询。有些模型如Minimax对存储/查询的向量进行了区分优化。\"),\n) -> BaseResponse:\n    '''\n    对文本进行向量化，返回 BaseResponse(data=List[List[float]])\n    '''\n    return embed_texts(texts=texts, embed_model=embed_model, to_query=to_query)\n\ndef embed_documents(\n        docs: List[Document],\n        embed_model: str = EMBEDDING_MODEL,\n        to_query: bool = False,\n) -> Dict:\n    \"\"\"\n    将 List[Document] 向量化，转化为 VectorStore.add_embeddings 可以接受的参数\n    \"\"\"\n    texts = [x.page_content for x in docs]\n    metadatas = [x.metadata for x in docs]\n    embeddings = embed_texts(texts=texts, embed_model=embed_model, to_query=to_query).data\n    if embeddings is not None:\n        return {\n            \"texts\": texts,\n            \"embeddings\": embeddings,\n            \"metadatas\": metadatas,\n        }\n    \n    "}
{"type": "source_file", "path": "server/knowledge_base/__init__.py", "content": ""}
{"type": "source_file", "path": "server/db/models/knowledge_metadata_model.py", "content": "from sqlalchemy import Column, Integer, String, DateTime, Float, Boolean, JSON, func\nfrom server.db.base import Base\n\nclass SummaryChunkModel(Base):\n    \"\"\"\n    chunk summary模型，用于存储file_doc中每个doc_id的chunk 片段，\n    数据来源:\n        用户输入: 用户上传文件，可填写文件的描述，生成的file_doc中的doc_id，存入summary_chunk中\n        程序自动切分 对file_doc表meta_data字段信息中存储的页码信息，按每页的页码切分，自定义prompt生成总结文本，将对应页码关联的doc_id存入summary_chunk中\n    后续任务:\n        矢量库构建: 对数据库表summary_chunk中summary_context创建索引，构建矢量库，meta_data为矢量库的元数据（doc_ids）\n        语义关联： 通过用户输入的描述，自动切分的总结文本，计算\n        语义相似度\n\n    \"\"\"\n    __tablename__ = 'summary_chunk'\n    id = Column(Integer, primary_key=True, autoincrement=True, comment='ID')\n    kb_name = Column(String(50), comment='知识库名称')\n    summary_context = Column(String(255), comment='总结文本')\n    summary_id = Column(String(255), comment='总结矢量id')\n    doc_ids = Column(String(1024), comment=\"向量库id关联列表\")\n    meta_data = Column(JSON, default={})\n\n    def __repr__(self):\n        return (f\"<SummaryChunk(id='{self.id}', kb_name='{self.kb_name}', summary_context='{self.summary_context}',\"\n                f\" doc_ids='{self.doc_ids}', metadata='{self.metadata}')>\")\n"}
{"type": "source_file", "path": "server/knowledge_base/kb_api.py", "content": "import urllib\nfrom server.utils import BaseResponse, ListResponse\nfrom server.knowledge_base.utils import validate_kb_name\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.db.repository.knowledge_base_repository import list_kbs_from_db\nfrom configs import EMBEDDING_MODEL, logger, log_verbose\nfrom fastapi import Body\n\ndef list_kbs():\n    '''获得数据库中的知识库列表'''\n    return ListResponse(data=list_kbs_from_db())\n\ndef create_kb(knowledge_base_name: str = Body(..., examples=[\"samples\"]),\n              vector_store_type: str = Body(\"faiss\"),\n              embed_model: str = Body(EMBEDDING_MODEL),\n              ) -> BaseResponse:\n    '''根据knowledge_base_name创建对应知识库'''\n    if not validate_kb_name(knowledge_base_name):\n        return BaseResponse(code=403, msg=\"The kb_name is illegal\")\n    if knowledge_base_name is None or knowledge_base_name.strip() == \"\":\n        return BaseResponse(code=404, msg=\"知识库名称不能为空，请重新填写知识库名称\")\n    \n    # 获得具体的数据库\n    kb = KBServiceFactory.get_service_by_name(knowledge_base_name)\n    if kb is not None:\n        return BaseResponse(code=404, msg=f\"已存在同名知识库 {knowledge_base_name}\")\n\n    kb = KBServiceFactory.get_service(knowledge_base_name, vector_store_type, embed_model)\n    try:\n        kb.create_kb()\n    except Exception as e:\n        msg = f\"创建知识库出错： {e}\"\n        logger.error(f'{e.__class__.__name__}: {msg}',\n                     exc_info=e if log_verbose else None)\n        return BaseResponse(code=500, msg=msg)\n\n    return BaseResponse(code=200, msg=f\"已新增知识库 {knowledge_base_name}\")\n\ndef delete_kb(\n        knowledge_base_name: str = Body(..., examples=[\"samples\"])\n) -> BaseResponse:\n    '''根据knowledge_base_name删除对应知识库'''\n    if not validate_kb_name(knowledge_base_name):\n        return BaseResponse(code=403, msg=\"Don't attack me\")\n    knowledge_base_name = urllib.parse.unquote(knowledge_base_name)\n\n    kb = KBServiceFactory.get_service_by_name(knowledge_base_name)\n\n    if kb is None:\n        return BaseResponse(code=404, msg=f\"未找到知识库 {knowledge_base_name}\")\n\n    try:\n        status = kb.clear_vs()\n        status = kb.drop_kb()\n        if status:\n            return BaseResponse(code=200, msg=f\"成功删除知识库 {knowledge_base_name}\")\n    except Exception as e:\n        msg = f\"删除知识库时出现意外： {e}\"\n        logger.error(f'{e.__class__.__name__}: {msg}',\n                     exc_info=e if log_verbose else None)\n        return BaseResponse(code=500, msg=msg)\n\n    return BaseResponse(code=500, msg=f\"删除知识库失败 {knowledge_base_name}\")"}
{"type": "source_file", "path": "server/db/session.py", "content": "from functools import wraps\nfrom contextlib import contextmanager\nfrom server.db.base import SessionLocal\nfrom sqlalchemy.orm import Session\n\n# 上下文管理器是指在一段代码执行之前执行一段代码，用于一些预处理工作；执行之后再执行一段代码，用于一些清理工作。\n# 装饰器contextmanager。该装饰器将一个函数中yield语句之前的代码当做__enter__方法执行，yield语句之后的代码当做__exit__方法执行。同时yield返回值赋值给as后的变量。\n@contextmanager\ndef session_scope() -> Session: # type: ignore\n    '''上下文管理器用于自动获取 Session, 避免发生异常'''\n    session = SessionLocal()\n    try:\n        yield session\n        session.commit()\n    except:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n\n# 定义一个与数据库交互的装饰器，减少代码冗余\ndef with_session(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        with session_scope() as session:\n            try:\n                result = f(session, *args, **kwargs)\n                session.commit()\n                return result\n            except:\n                session.rollback()\n                raise\n    \n    return wrapper\n\n\ndef get_db() -> SessionLocal: # type: ignore\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n"}
{"type": "source_file", "path": "server/knowledge_base/kb_cache/base.py", "content": "import os\nos.environ[\"HF-ENDPOINT\"] = 'https://hf-mirror.com'\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.vectorstores.faiss import FAISS\nimport threading\nfrom configs import (EMBEDDING_MODEL, CHUNK_SIZE,\n                     logger, log_verbose)\nfrom server.utils import embedding_device, get_model_path, list_online_embed_models\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nfrom typing import List, Any, Union, Tuple\n\n\nclass ThreadSafeObject:\n    '''线程安全的对象包装器，用于在多线程环境中安全地访问和修改对象。'''\n    def __init__(self, key: Union[str, Tuple], obj: Any = None, pool: \"CachePool\" = None):\n        self._obj = obj\n        self._key = key\n        self._pool = pool\n        self._lock = threading.RLock()\n        self._loaded = threading.Event()\n\n    def __repr__(self) -> str:\n        ''' 类可以通过定义 __repr__() 方法来控制此函数为它的实例所返回的内容。'''\n        cls = type(self).__name__\n        return f\"<{cls}: key: {self.key}, obj: {self._obj}>\"\n\n    @property\n    def key(self):\n        return self._key\n\n    @contextmanager\n    def acquire(self, owner: str = \"\", msg: str = \"\") -> FAISS: # type: ignore\n        '''获取对象的锁，用于访问和修改对象。'''\n        owner = owner or f\"thread {threading.get_native_id()}\"\n        try:\n            self._lock.acquire()\n            if self._pool is not None:\n                self._pool._cache.move_to_end(self.key)\n            if log_verbose:\n                logger.info(f\"{owner} 开始操作：{self.key}。{msg}\")\n            yield self._obj\n        finally:\n            if log_verbose:\n                logger.info(f\"{owner} 结束操作：{self.key}。{msg}\")\n            self._lock.release()\n\n    def start_loading(self):\n        '''标记对象开始加载'''\n        self._loaded.clear()\n\n    def finish_loading(self):\n        '''标记对象加载完成'''\n        self._loaded.set()\n\n    def wait_for_loading(self):\n        '''等待对象加载完成'''\n        self._loaded.wait()\n\n    @property\n    def obj(self):\n        return self._obj\n\n    @obj.setter\n    def obj(self, val: Any):\n        self._obj = val\n\n\nclass CachePool:\n    '''缓存池，用于管理多个对象的缓存'''\n    def __init__(self, cache_num: int = -1):\n        self._cache_num = cache_num\n        self._cache = OrderedDict()\n        self.atomic = threading.RLock()\n\n    def keys(self) -> List[str]:\n        return list(self._cache.keys())\n\n    def _check_count(self):\n        if isinstance(self._cache_num, int) and self._cache_num > 0:\n            while len(self._cache) > self._cache_num:\n                self._cache.popitem(last=False)\n\n    def get(self, key: str) -> ThreadSafeObject:\n        if cache := self._cache.get(key):\n            cache.wait_for_loading()\n            return cache\n\n    def set(self, key: str, obj: ThreadSafeObject) -> ThreadSafeObject:\n        self._cache[key] = obj\n        self._check_count()\n        return obj\n\n    def pop(self, key: str = None) -> ThreadSafeObject:\n        if key is None:\n            return self._cache.popitem(last=False)\n        else:\n            return self._cache.pop(key, None)\n\n    def acquire(self, key: Union[str, Tuple], owner: str = \"\", msg: str = \"\"):\n        cache = self.get(key)\n        if cache is None:\n            raise RuntimeError(f\"请求的资源 {key} 不存在\")\n        elif isinstance(cache, ThreadSafeObject):\n            self._cache.move_to_end(key)\n            return cache.acquire(owner=owner, msg=msg)\n        else:\n            return cache\n\n    def load_kb_embeddings(\n            self,\n            kb_name: str,\n            embed_device: str = embedding_device(),\n            default_embed_model: str = EMBEDDING_MODEL,\n    ) -> Embeddings:\n        from server.db.repository.knowledge_base_repository import get_kb_detail\n        from server.knowledge_base.kb_service.base import EmbeddingsFunAdapter\n\n        kb_detail = get_kb_detail(kb_name)\n        embed_model = kb_detail.get(\"embed_model\", default_embed_model)\n\n        if embed_model in list_online_embed_models():\n            return EmbeddingsFunAdapter(embed_model)\n        else:\n            return embeddings_pool.load_embeddings(model=embed_model, device=embed_device)\n\n\nclass EmbeddingsPool(CachePool):\n    def load_embeddings(self, model: str = None, device: str = None) -> Embeddings:\n        self.atomic.acquire()\n        model = model or EMBEDDING_MODEL\n        device = embedding_device()\n        key = (model, device)\n        if not self.get(key):\n            item = ThreadSafeObject(key, pool=self)\n            self.set(key, item)\n            with item.acquire(msg=\"初始化\"):\n                self.atomic.release()\n                if model == \"text-embedding-ada-002\":  # openai text-embedding-ada-002\n                    from langchain.embeddings.openai import OpenAIEmbeddings\n                    embeddings = OpenAIEmbeddings(model=model,\n                                                  openai_api_key=get_model_path(model),\n                                                  chunk_size=CHUNK_SIZE)\n                elif 'bge-' in model:\n                    from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n                    if 'zh' in model:\n                        # for chinese model\n                        query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n                    elif 'en' in model:\n                        # for english model\n                        query_instruction = \"Represent this sentence for searching relevant passages:\"\n                    else:\n                        # maybe ReRanker or else, just use empty string instead\n                        query_instruction = \"\"\n                    embeddings = HuggingFaceBgeEmbeddings(model_name=get_model_path(model),\n                                                          model_kwargs={'device': device},\n                                                          query_instruction=query_instruction)\n                    if model == \"bge-large-zh-noinstruct\":  # bge large -noinstruct embedding\n                        embeddings.query_instruction = \"\"\n                else:\n                    from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n                    embeddings = HuggingFaceEmbeddings(model_name=get_model_path(model),\n                                                       model_kwargs={'device': device})\n                item.obj = embeddings\n                item.finish_loading()\n        else:\n            self.atomic.release()\n        return self.get(key).obj\n\n\nembeddings_pool = EmbeddingsPool(cache_num=1)\n"}
{"type": "source_file", "path": "server/knowledge_base/kb_cache/faiss_cache.py", "content": "import sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))\n\nfrom configs import CACHED_VS_NUM, CACHED_MEMO_VS_NUM\nfrom server.knowledge_base.kb_cache.base import *\nfrom server.knowledge_base.kb_service.base import EmbeddingsFunAdapter\nfrom server.utils import load_local_embeddings\nfrom server.knowledge_base.utils import get_vs_path\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\n# patch FAISS to include doc id in Document.metadata\ndef _new_ds_search(self, search: str) -> Union[str, Document]:\n    if search not in self._dict:\n        return f\"ID {search} not found.\"\n    else:\n        doc = self._dict[search]\n        if isinstance(doc, Document):\n            doc.metadata[\"id\"] = search\n        return doc\n    \n# 修改的目的可能是为了在搜索结果中包含文档的 ID 信息，以便后续处理中可以更方便地使用文档的 ID。\nInMemoryDocstore.search = _new_ds_search\n\n\nclass ThreadSafeFaiss(ThreadSafeObject):\n    '''继承自 ThreadSafeObject，并对其进行了扩展，增加了一些针对 Faiss 的操作方法。'''\n    def __repr__(self) -> str:\n        cls = type(self).__name__\n        return f\"<{cls}: key: {self.key}, obj: {self._obj}, docs_count: {self.docs_count()}>\"\n\n    def docs_count(self) -> int:\n        return len(self._obj.docstore._dict)\n\n    def save(self, path: str, create_path: bool = True):\n        with self.acquire():\n            if not os.path.isdir(path) and create_path:\n                os.makedirs(path)\n            ret = self._obj.save_local(path)\n            logger.info(f\"已将向量库 {self.key} 保存到磁盘\")\n        return ret\n\n    def clear(self):\n        ret = []\n        with self.acquire():\n            ids = list(self._obj.docstore._dict.keys())\n            if ids:\n                ret = self._obj.delete(ids)\n                assert len(self._obj.docstore._dict) == 0\n            logger.info(f\"已将向量库 {self.key} 清空\")\n        return ret\n\n\nclass _FaissPool(CachePool):\n    '''继承自 CachePool，并提供了管理 Faiss 向量库的功能。'''\n    def new_vector_store(\n        self,\n        embed_model: str = EMBEDDING_MODEL,\n        embed_device: str = embedding_device(),\n    ) -> FAISS:\n        embeddings = EmbeddingsFunAdapter(embed_model)\n        doc = Document(page_content=\"init\", metadata={})\n        vector_store = FAISS.from_documents([doc], embeddings, normalize_L2=True,distance_strategy=\"METRIC_INNER_PRODUCT\")\n        ids = list(vector_store.docstore._dict.keys())\n        vector_store.delete(ids)\n        return vector_store\n\n    def save_vector_store(self, kb_name: str, path: str=None):\n        if cache := self.get(kb_name):\n            return cache.save(path)\n\n    def unload_vector_store(self, kb_name: str):\n        if cache := self.get(kb_name):\n            self.pop(kb_name)\n            logger.info(f\"成功释放向量库：{kb_name}\")\n\n\nclass KBFaissPool(_FaissPool):\n    '''继承自 _FaissPool 类，并扩展了从磁盘中加载 Faiss 向量库的功能。load_vector_store 方法用于加载指定名称的 Faiss 向量库。'''\n    def load_vector_store(\n            self,\n            kb_name: str,\n            vector_name: str = None,\n            create: bool = True,\n            embed_model: str = EMBEDDING_MODEL,\n            embed_device: str = embedding_device(),\n    ) -> ThreadSafeFaiss:\n        self.atomic.acquire()\n        vector_name = vector_name or embed_model\n        cache = self.get((kb_name, vector_name)) # 用元组比拼接字符串好一些\n        if cache is None:\n            item = ThreadSafeFaiss((kb_name, vector_name), pool=self)\n            self.set((kb_name, vector_name), item)\n            with item.acquire(msg=\"初始化\"):\n                self.atomic.release()\n                logger.info(f\"loading vector store in '{kb_name}/vector_store/{vector_name}' from disk.\")\n                vs_path = get_vs_path(kb_name, vector_name)\n\n                if os.path.isfile(os.path.join(vs_path, \"index.faiss\")):\n                    embeddings = self.load_kb_embeddings(kb_name=kb_name, embed_device=embed_device, default_embed_model=embed_model)\n                    vector_store = FAISS.load_local(vs_path, embeddings, normalize_L2=True, distance_strategy=\"METRIC_INNER_PRODUCT\", allow_dangerous_deserialization=True)\n                elif create:\n                    # create an empty vector store\n                    if not os.path.exists(vs_path):\n                        os.makedirs(vs_path)\n                    vector_store = self.new_vector_store(embed_model=embed_model, embed_device=embed_device)\n                    vector_store.save_local(vs_path)\n                else:\n                    raise RuntimeError(f\"knowledge base {kb_name} not exist.\")\n                item.obj = vector_store\n                item.finish_loading()\n        else:\n            self.atomic.release()\n        return self.get((kb_name, vector_name))\n    \n\nclass MemoFaissPool(_FaissPool):\n    '''继承自 _FaissPool，用于管理内存中的 Faiss 向量库。'''\n    def load_vector_store(\n        self,\n        kb_name: str,\n        embed_model: str = EMBEDDING_MODEL,\n        embed_device: str = embedding_device(),\n    ) -> ThreadSafeFaiss:\n        '''方法用于加载指定知识库的 Faiss 向量库到内存中。'''\n        self.atomic.acquire()\n        cache = self.get(kb_name)\n        if cache is None:\n            item = ThreadSafeFaiss(kb_name, pool=self)\n            self.set(kb_name, item)\n            with item.acquire(msg=\"初始化\"):\n                self.atomic.release()\n                logger.info(f\"loading vector store in '{kb_name}' to memory.\")\n                # create an empty vector store\n                vector_store = self.new_vector_store(embed_model=embed_model, embed_device=embed_device)\n                item.obj = vector_store\n                item.finish_loading()\n        else:\n            self.atomic.release()\n        return self.get(kb_name)\n\n\nkb_faiss_pool = KBFaissPool(cache_num=CACHED_VS_NUM)\nmemo_faiss_pool = MemoFaissPool(cache_num=CACHED_MEMO_VS_NUM)\n\n\nif __name__ == \"__main__\":\n    import time, random\n    from pprint import pprint\n\n    kb_names = [\"vs1\", \"vs2\", \"vs3\"]\n    # for name in kb_names:\n    #     memo_faiss_pool.load_vector_store(name)\n\n    def worker(vs_name: str, name: str):\n        vs_name = \"samples\"\n        time.sleep(random.randint(1, 5))\n        embeddings = load_local_embeddings()\n        r = random.randint(1, 3)\n\n        with kb_faiss_pool.load_vector_store(vs_name).acquire(name) as vs:\n            if r == 1: # add docs\n                ids = vs.add_texts([f\"text added by {name}\"], embeddings=embeddings)\n                pprint(ids)\n            elif r == 2: # search docs\n                docs = vs.similarity_search_with_score(f\"{name}\", k=3, score_threshold=1.0)\n                pprint(docs)\n        if r == 3: # delete docs\n            logger.warning(f\"清除 {vs_name} by {name}\")\n            try:\n                kb_faiss_pool.get(vs_name).clear()\n            except:\n                print(\"vector store已经空了，没法清除了\")\n\n    threads = []\n    for n in range(1, 30):\n        t = threading.Thread(target=worker,\n                             kwargs={\"vs_name\": random.choice(kb_names), \"name\": f\"worker {n}\"},\n                             daemon=True)\n        t.start()\n        threads.append(t)\n\n    for t in threads:\n        t.join()"}
{"type": "source_file", "path": "server/knowledge_base/kb_cache/__init__.py", "content": ""}
