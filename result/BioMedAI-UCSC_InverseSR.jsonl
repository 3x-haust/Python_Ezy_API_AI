{"repo_info": {"repo_name": "InverseSR", "repo_owner": "BioMedAI-UCSC", "repo_url": "https://github.com/BioMedAI-UCSC/InverseSR"}}
{"type": "source_file", "path": "project/BRGM_ddim.py", "content": "# Code is adpated from: https://huggingface.co/spaces/Warvito/diffusion_brain/blob/main/app.py and\n# https://colab.research.google.com/drive/1xJAor6_Ky36gxIk6ICNP--NMBjSlYKil?usp=sharing#scrollTo=4XDeCy-Vj59b\n# A lot of thanks to the author of the code\n\n# Reference:\n# [1] Pinaya, W. H., et al. (2022). \"Brain Imaging Generation with Latent Diffusion Models.\" arXiv preprint arXiv:2209.07162.\n# [2] Marinescu, R., et al. (2020). Bayesian Image Reconstruction using Deep Generative Models.\n\nimport math\nfrom argparse import ArgumentParser, Namespace\nfrom pathlib import Path\nfrom time import perf_counter\nfrom typing import Any, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport csv\nfrom torch.utils.tensorboard import SummaryWriter\nfrom monai.transforms import apply_transform\nfrom skimage.metrics import mean_squared_error as mse\nfrom skimage.metrics import normalized_root_mse as nmse\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\n\nfrom models.BRGM.forward_models import (\n    ForwardDownsample,\n    ForwardFillMask,\n    ForwardAbstract,\n)\nfrom models.ddim import DDIMSampler\nfrom utils.transorms import get_preprocessing\nfrom utils.plot import draw_corrupted_images, draw_images, draw_img\nfrom utils.add_argument import add_argument\nfrom utils.utils import (\n    setup_noise_inputs,\n    load_target_image,\n    load_pre_trained_model,\n    create_corruption_function,\n    sampling_from_ddim,\n    getVggFeatures,\n)\nfrom utils.const import (\n    INPUT_FOLDER,\n    PRETRAINED_MODEL_VGG_PATH,\n    OUTPUT_FOLDER,\n)\n\n\ndef transform_img(\n    img_path: Path,\n    device: torch.device,\n) -> Any:\n    data = {\"image\": img_path}\n    data = apply_transform(get_preprocessing(device), data)\n    return data[\"image\"]\n\n\ndef return_true_conditional_variable(subject_id: str) -> Tuple[int, int]:\n    csv_path = INPUT_FOLDER / \"oasis_cross-sectional.csv\"\n    df = pd.read_csv(csv_path)\n    df = df[df[\"ID\"] == f\"{subject_id}_MR1\"]\n    gender = 0 if df[\"M/F\"].values[0] == \"F\" else 1\n    age = df[\"Age\"].values[0]\n    return gender, age.item()\n\n\ndef create_corruption_imgs(\n    img_tensor: torch.Tensor, hparams: Namespace, device: torch.device\n) -> Tuple[ForwardAbstract, torch.Tensor]:\n    if hparams.corruption == \"downsample\":\n        forward = ForwardDownsample(factor=hparams.downsample_factor)\n        # mask = skimage.io.imread(maskFile)\n        # mask = mask[:, :, 0] == np.min(mask[:, :, 0])  # need to block black color\n        # mask = np.reshape(mask, (1, 1, mask.shape[0], mask.shape[1]))\n\n        # Original image for now\n        corrupted_img = forward(\n            img_tensor\n        )  # pass through forward model to generate corrupted image\n    elif hparams.corruption == \"None\":\n        forward = ForwardFillMask(device=device)\n        corrupted_img = img_tensor\n    return forward, corrupted_img\n\n\ndef load_vgg_perceptual(\n    hparams: Namespace, target: torch.Tensor, device: torch.device\n) -> Tuple[Any, torch.Tensor]:\n    with open(PRETRAINED_MODEL_VGG_PATH, \"rb\") as f:\n        vgg16 = torch.jit.load(f).eval().to(device)\n\n    target_features = getVggFeatures(hparams, target, vgg16)\n    return vgg16, target_features\n\n\ndef logprint(message: str, verbose: bool) -> None:\n    if verbose:\n        print(message)\n\n\ndef add_hparams_to_tensorboard(\n    hparams: Namespace,\n    final_loss: torch.Tensor,\n    final_ssim: float,\n    final_psnr: float,\n    final_mse: float,\n    final_nmse: float,\n    inversed_ventricular: float,\n    inversed_brain: float,\n    writer: SummaryWriter,\n) -> None:\n    writer.add_hparams(\n        {\n            \"num_steps\": hparams.num_steps,\n            \"learning_rate\": hparams.learning_rate,\n            \"experiment_name\": hparams.experiment_name,\n            \"subject_id\": hparams.subject_id,\n            \"update_latent_variables\": hparams.update_latent_variables,\n            \"update_conditioning\": hparams.update_conditioning,\n            \"update_gender\": hparams.update_gender,\n            \"update_age\": hparams.update_age,\n            \"update_ventricular\": hparams.update_ventricular,\n            \"update_brain\": hparams.update_brain,\n            \"alpha\": hparams.lambda_alpha,\n            \"perc\": hparams.lambda_perc,\n            \"kernel_size\": hparams.kernel_size,\n        },\n        {\n            \"loss/final_loss\": final_loss,\n            \"measurement/final_ssim\": final_ssim,\n            \"measurement/final_psnr\": final_psnr,\n            \"measurement/final_mse\": final_mse,\n            \"measurement/final_nmse\": final_nmse,\n            \"conditional_variable/inversed_ventricular\": inversed_ventricular,\n            \"conditional_variable/inversed_brain\": inversed_brain,\n        },\n    )\n\n\ndef create_mask_for_backprop(hparams: Namespace, device: torch.device) -> torch.Tensor:\n    mask_cond = torch.ones((1, 4), device=device)\n    mask_cond[:, 0] = 0 if not hparams.update_gender else 1\n    mask_cond[:, 1] = 0 if not hparams.update_age else 1\n    mask_cond[:, 2] = 0 if not hparams.update_ventricular else 1\n    mask_cond[:, 3] = 0 if not hparams.update_brain else 1\n    return mask_cond\n\n\ndef project(\n    ddim: DDIMSampler,\n    decoder: torch.nn.Module,\n    forward: ForwardFillMask,\n    target: torch.Tensor,\n    device: torch.device,\n    writer: SummaryWriter,\n    hparams: Namespace,\n    verbose: bool = False,\n):\n    cond, latent_variable = setup_noise_inputs(device=device, hparams=hparams)\n\n    update_params = []\n    if hparams.update_latent_variables:\n        latent_variable.requires_grad = True\n        update_params.append(latent_variable)\n    if hparams.update_conditioning:\n        cond.requires_grad = True\n        update_params.append(cond)\n\n    optimizer_adam = torch.optim.Adam(\n        update_params,\n        betas=(0.9, 0.999),\n        lr=hparams.learning_rate,\n    )\n    latent_variable_out = torch.zeros(\n        [hparams.num_steps] + list(latent_variable.shape[1:]),\n        dtype=torch.float32,\n        device=device,\n    )\n    cond_out = torch.zeros(\n        [hparams.num_steps] + list(cond.shape[1:]),\n        dtype=torch.float32,\n        device=device,\n    )\n\n    mask_cond = create_mask_for_backprop(hparams, device)\n    target_img_corrupted = forward(target)\n    vgg16, target_features = load_vgg_perceptual(hparams, target_img_corrupted, device)\n    total_num_pixels = (\n        target_img_corrupted.numel()\n        if hparams.corruption != \"mask\"\n        else math.prod(forward.mask.shape) - forward.mask.sum()\n    )\n\n    for step in range(hparams.start_steps, hparams.num_steps):\n\n        def closure():\n            optimizer_adam.zero_grad()\n\n            synth_img = sampling_from_ddim(\n                ddim=ddim,\n                decoder=decoder,\n                latent_variable=latent_variable,\n                cond=cond,\n                hparams=hparams,\n            )\n            synth_img_corrupted = forward(synth_img)  # f(G(w))\n\n            loss = 0\n            prior_loss = 0\n            pixelwise_loss = (\n                synth_img_corrupted - target_img_corrupted\n            ).abs().sum() / total_num_pixels\n            loss += pixelwise_loss\n\n            synth_features = getVggFeatures(hparams, synth_img_corrupted, vgg16)\n            perceptual_loss = (target_features - synth_features).abs().mean()\n            loss += hparams.lambda_perc * perceptual_loss\n\n            loss.backward(create_graph=False)\n            cond.grad *= mask_cond\n\n            return (\n                loss,\n                pixelwise_loss,\n                perceptual_loss,\n                prior_loss,\n                synth_img,\n                synth_img_corrupted,\n            )\n\n        (\n            loss,\n            pixelwise_loss,\n            perceptual_loss,\n            prior_loss,\n            synth_img,\n            synth_img_corrupted,\n        ) = optimizer_adam.step(closure=closure)\n\n        synth_img_np = synth_img[0, 0].detach().cpu().numpy()\n        target_np = target[0, 0].detach().cpu().numpy()\n        ssim_ = ssim(\n            synth_img_np,\n            target_np,\n            win_size=11,\n            data_range=1.0,\n            gaussian_weights=True,\n            use_sample_covariance=False,\n        )\n        # Code for computing PSNR is adapted from\n        # https://github.com/agis85/multimodal_brain_synthesis/blob/master/error_metrics.py#L32\n        data_range = np.max([synth_img_np.max(), target_np.max()]) - np.min(\n            [synth_img_np.min(), target_np.min()]\n        )\n        psnr_ = psnr(target_np, synth_img_np, data_range=data_range)\n        mse_ = mse(target_np, synth_img_np)\n        nmse_ = nmse(target_np, synth_img_np)\n\n        writer.add_scalar(\"loss\", loss, global_step=step)\n        writer.add_scalar(\"pixelwise_loss\", pixelwise_loss, global_step=step)\n        writer.add_scalar(\"perceptual_loss\", perceptual_loss, global_step=step)\n        writer.add_scalar(\"prior_loss\", prior_loss, global_step=step)\n        writer.add_scalar(\"ssim\", ssim_, global_step=step)\n        writer.add_scalar(\"psnr\", psnr_, global_step=step)\n        writer.add_scalar(\"mse\", mse_, global_step=step)\n        writer.add_scalar(\"nmse\", nmse_, global_step=step)\n\n        if hparams.update_conditioning:\n            if hparams.update_gender:\n                writer.add_scalar(\"inversed_gender\", cond[0, 0], global_step=step)\n            if hparams.update_age:\n                writer.add_scalar(\"inversed_age\", cond[0, 1], global_step=step)\n            if hparams.update_ventricular:\n                writer.add_scalar(\"inversed_ventricular\", cond[0, 2], global_step=step)\n            if hparams.update_brain:\n                writer.add_scalar(\"inversed_brain\", cond[0, 3], global_step=step)\n        logprint(\n            f\"step {step + 1:>4d}/{hparams.num_steps}: tloss {float(loss):<5.8f} pix_loss {float(pixelwise_loss):<5.8f} perc_loss {float(perceptual_loss):<1.15f} pior_loss {float(prior_loss):<5.8f}\\n\"\n            f\"              : SSIM {float(ssim_):<5.8f} PSNR {float(psnr_):<5.8f} MSE {float(mse_):<5.8f} NMSE {float(nmse_):<5.8f}\",\n            verbose=verbose,\n        )\n\n        step_ = f\"{step}\".zfill(4)\n        draw_img(\n            synth_img_np,\n            title=\"synth\",\n            step=step_,\n            output_folder=OUTPUT_FOLDER,\n        )\n\n        if step % 25 == 0:\n            if hparams.corruption != \"None\":\n                imgs = draw_corrupted_images(\n                    synth_img_np,\n                    target_np,\n                    synth_img_corrupted[0, 0].detach().cpu().numpy(),\n                    target_img_corrupted[0, 0].detach().cpu().numpy(),\n                    ssim_=ssim_,\n                )\n            else:\n                imgs = draw_images(\n                    synth_img_np,\n                    target_np,\n                    ssim_=ssim_,\n                )\n            step_ = f\"{step}\".zfill(4)\n            writer.add_figure(f\"step: {step_}\", imgs, global_step=step)\n            plt.close(imgs)\n\n        latent_variable_out[step] = latent_variable.detach()[0]\n        cond_out[step] = cond.detach()[0]\n\n    add_hparams_to_tensorboard(\n        hparams,\n        final_loss=loss.item(),\n        final_ssim=ssim_,\n        final_psnr=psnr_,\n        final_mse=mse_,\n        final_nmse=nmse_,\n        inversed_ventricular=cond[0, 2].clone().detach().item(),\n        inversed_brain=cond[0, 3].clone().detach().item(),\n        writer=writer,\n    )\n\n    writer.flush()\n    writer.close()\n\n    torch.save(\n        {\n            \"epoch\": step,\n            \"latent_variable\": latent_variable,\n            \"cond\": cond,\n            \"optimizer\": optimizer_adam.state_dict(),\n        },\n        OUTPUT_FOLDER / \"checkpoint.pth\",\n    )\n\n    row = [\n        hparams.subject_id,\n        ssim_,\n        psnr_,\n        mse_,\n        nmse_,\n        cond[0, 0].clone().detach().item(),\n        cond[0, 1].clone().detach().item() * (82 - 44) + 44,\n        cond[0, 2].clone().detach().item(),\n        cond[0, 3].clone().detach().item(),\n    ]\n\n    # Save the statistics result to csv file\n    with open(\n        # Replace the path with your csv file path\n        \"./result_ddim_mask_9.csv\",\n        \"a\",\n    ) as file:\n        writer = csv.writer(file)\n        writer.writerow(row)\n\n    return latent_variable_out, cond_out\n\n\ndef main(hparams: Namespace) -> None:\n    # device = torch.device(\"cuda\" if COMPUTECANADA else \"cpu\")\n    # Don't have enough memory to run on GPU.\n    device = torch.device(\"cpu\")\n    img_tensor = load_target_image(hparams, device=device)\n    writer = SummaryWriter(log_dir=hparams.tensor_board_logger)\n\n    # Create forward corruption model that masks the image with the given mask\n    # Make the mask function work\n    forward = create_corruption_function(hparams=hparams, device=device)\n    diffusion, decoder = load_pre_trained_model(device=device)\n    ddim = DDIMSampler(diffusion)\n\n    # Call projector code\n    start_time = perf_counter()\n    latent_variable_out, cond_out = project(\n        ddim,\n        decoder,\n        writer=writer,\n        hparams=hparams,\n        forward=forward,\n        target=img_tensor,\n        device=device,\n        verbose=True,\n    )\n    print(f\"Elapsed: {(perf_counter() - start_time):.1f} s\")\n\n    torch.save(\n        {\"latent_variable\": latent_variable_out, \"cond\": cond_out},\n        OUTPUT_FOLDER / \"latent_cond.pth\",\n    )\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Trainer args\", add_help=False)\n    add_argument(parser)\n    hparams = parser.parse_args()\n    # seed_everything(42)\n    main(hparams)\n"}
{"type": "source_file", "path": "project/models/ddim.py", "content": "from inspect import isfunction\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n        shape[0], *((1,) * (len(shape) - 1))\n    )\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n\n\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt(\n        (1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev)\n    )\n\n    if verbose:\n        print(\n            f\"Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}\"\n        )\n        print(\n            f\"For the chosen value of eta, which is {eta}, \"\n            f\"this results in the following sigma_t schedule for ddim sampler {sigmas}\"\n        )\n    return sigmas, alphas, alphas_prev\n\n\ndef make_ddim_timesteps(\n    ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True\n):\n    if ddim_discr_method == \"uniform\":\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == \"quad\":\n        ddim_timesteps = (\n            (np.linspace(0, np.sqrt(num_ddpm_timesteps * 0.8), num_ddim_timesteps)) ** 2\n        ).astype(int)\n    else:\n        raise NotImplementedError(\n            f'There is no ddim discretization method called \"{ddim_discr_method}\"'\n        )\n\n    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n    steps_out = ddim_timesteps + 1\n    if verbose:\n        print(f\"Selected timesteps for ddim sampler: {steps_out}\")\n    return steps_out\n\n\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != torch.device(\"cpu\"):\n                attr = attr.to(torch.device(\"cpu\"))\n        setattr(self, name, attr)\n\n    def make_schedule(\n        self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0.0, verbose=True\n    ):\n        self.ddim_timesteps = make_ddim_timesteps(\n            ddim_discr_method=ddim_discretize,\n            num_ddim_timesteps=ddim_num_steps,\n            num_ddpm_timesteps=self.ddpm_num_timesteps,\n            verbose=verbose,\n        )\n        alphas_cumprod = self.model.alphas_cumprod\n        assert (\n            alphas_cumprod.shape[0] == self.ddpm_num_timesteps\n        ), \"alphas have to be defined for each timestep\"\n        to_torch = (\n            lambda x: x.clone().detach().to(torch.float32).to(torch.device(\"cpu\"))\n        )\n\n        self.register_buffer(\"betas\", to_torch(self.model.betas))\n        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n        self.register_buffer(\n            \"alphas_cumprod_prev\", to_torch(self.model.alphas_cumprod_prev)\n        )\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer(\n            \"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod.cpu()))\n        )\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            to_torch(np.sqrt(1.0 - alphas_cumprod.cpu())),\n        )\n        self.register_buffer(\n            \"log_one_minus_alphas_cumprod\", to_torch(np.log(1.0 - alphas_cumprod.cpu()))\n        )\n        self.register_buffer(\n            \"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod.cpu()))\n        )\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            to_torch(np.sqrt(1.0 / alphas_cumprod.cpu() - 1)),\n        )\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(\n            alphacums=alphas_cumprod.cpu(),\n            ddim_timesteps=self.ddim_timesteps,\n            eta=ddim_eta,\n            verbose=verbose,\n        )\n        self.register_buffer(\"ddim_sigmas\", ddim_sigmas)\n        self.register_buffer(\"ddim_alphas\", ddim_alphas)\n        self.register_buffer(\"ddim_alphas_prev\", ddim_alphas_prev)\n        self.register_buffer(\"ddim_sqrt_one_minus_alphas\", np.sqrt(1.0 - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev)\n            / (1 - self.alphas_cumprod)\n            * (1 - self.alphas_cumprod / self.alphas_cumprod_prev)\n        )\n        self.register_buffer(\n            \"ddim_sigmas_for_original_num_steps\", sigmas_for_original_sampling_steps\n        )\n\n    def sample(\n        self,\n        S,\n        batch_size,\n        shape,\n        first_img: torch.Tensor,\n        conditioning=None,\n        callback=None,\n        img_callback=None,\n        quantize_x0=False,\n        eta=0.0,\n        mask=None,\n        x0=None,\n        temperature=1.0,\n        noise_dropout=0.0,\n        score_corrector=None,\n        corrector_kwargs=None,\n        verbose=True,\n        x_T=None,\n        log_every_t=100,\n        **kwargs,\n    ):\n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n        # sampling\n        C, H, W, D = shape\n        size = (batch_size, C, H, W, D)\n        print(f\"Data shape for DDIM sampling is {size}, eta {eta}\")\n\n        samples, intermediates = self.ddim_sampling(\n            conditioning,\n            size,\n            first_img=first_img,\n            callback=callback,\n            img_callback=img_callback,\n            quantize_denoised=quantize_x0,\n            mask=mask,\n            x0=x0,\n            ddim_use_original_steps=False,\n            noise_dropout=noise_dropout,\n            temperature=temperature,\n            score_corrector=score_corrector,\n            corrector_kwargs=corrector_kwargs,\n            x_T=x_T,\n            log_every_t=log_every_t,\n        )\n        return samples, intermediates\n\n    def ddim_sampling(\n        self,\n        cond,\n        shape,\n        first_img: torch.Tensor,\n        x_T=None,\n        ddim_use_original_steps=False,\n        callback=None,\n        timesteps=None,\n        quantize_denoised=False,\n        mask=None,\n        x0=None,\n        img_callback=None,\n        log_every_t=100,\n        temperature=1.0,\n        noise_dropout=0.0,\n        score_corrector=None,\n        corrector_kwargs=None,\n    ):\n        device = self.model.betas.device\n        b = shape[0]\n        if x_T is None:\n            # first timepoint sample from here\n            img = first_img\n        else:\n            img = x_T\n\n        if timesteps is None:\n            timesteps = (\n                self.ddpm_num_timesteps\n                if ddim_use_original_steps\n                else self.ddim_timesteps\n            )\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = (\n                int(\n                    min(timesteps / self.ddim_timesteps.shape[0], 1)\n                    * self.ddim_timesteps.shape[0]\n                )\n                - 1\n            )\n            timesteps = self.ddim_timesteps[:subset_end]\n\n        intermediates = {\"x_inter\": [img], \"pred_x0\": [img]}\n        time_range = (\n            reversed(range(0, timesteps))\n            if ddim_use_original_steps\n            else np.flip(timesteps)\n        )\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc=\"DDIM Sampler\", total=total_steps)\n\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.model.q_sample(\n                    x0, ts\n                )\n                img = img_orig * mask + (1.0 - mask) * img\n\n            outs = self.p_sample_ddim(\n                img,\n                cond,\n                ts,\n                index=index,\n                use_original_steps=ddim_use_original_steps,\n                quantize_denoised=quantize_denoised,\n                temperature=temperature,\n                noise_dropout=noise_dropout,\n                score_corrector=score_corrector,\n                corrector_kwargs=corrector_kwargs,\n            )\n            img, pred_x0 = outs\n            if callback:\n                callback(i)\n            if img_callback:\n                img_callback(pred_x0, i)\n\n            if index % log_every_t == 0 or index == total_steps - 1:\n                intermediates[\"x_inter\"].append(img)\n                intermediates[\"pred_x0\"].append(pred_x0)\n\n        return img, intermediates\n\n    def p_sample_ddim(\n        self,\n        x,\n        c,\n        t,\n        index,\n        repeat_noise=False,\n        use_original_steps=False,\n        quantize_denoised=False,\n        temperature=1.0,\n        noise_dropout=0.0,\n        score_corrector=None,\n        corrector_kwargs=None,\n    ):\n        b, *_, device = *x.shape, x.device\n        e_t = self.model.apply_model(x, t, c)\n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\"\n            e_t = score_corrector.modify_score(\n                self.model, e_t, x, t, c, **corrector_kwargs\n            )\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = (\n            self.model.alphas_cumprod_prev\n            if use_original_steps\n            else self.ddim_alphas_prev\n        )\n        sqrt_one_minus_alphas = (\n            self.model.sqrt_one_minus_alphas_cumprod\n            if use_original_steps\n            else self.ddim_sqrt_one_minus_alphas\n        )\n        sigmas = (\n            self.model.ddim_sigmas_for_original_num_steps\n            if use_original_steps\n            else self.ddim_sigmas\n        )\n        # select parameters corresponding to the currently considered timestep\n        a_t = torch.full((b, 1, 1, 1, 1), alphas[index], device=device)\n        a_prev = torch.full((b, 1, 1, 1, 1), alphas_prev[index], device=device)\n        sigma_t = torch.full((b, 1, 1, 1, 1), sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full(\n            (b, 1, 1, 1, 1), sqrt_one_minus_alphas[index], device=device\n        )\n\n        # current prediction for x_0\n        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n        # direction pointing to x_t\n        dir_xt = (1.0 - a_prev - sigma_t**2).sqrt() * e_t\n        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.0:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        return x_prev, pred_x0\n"}
{"type": "source_file", "path": "project/utils/add_argument.py", "content": "from argparse import ArgumentParser\n\n\ndef add_argument(parser: ArgumentParser):\n    parser.add_argument(\n        \"--tensor_board_logger\",\n        default=r\"C:\\Users\\16446\\Documents\\GitHub\\Medical-Image-Reconstruction\\log\",\n        help=\"TensorBoardLogger dir\",\n    )\n    parser.add_argument(\n        \"--data_format\",\n        default=\"pth\",\n        type=str,\n        choices=[\"pth\", \"nii\", \"img\"],\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        dest=\"learning_rate\",\n        default=0.1,\n        type=float,\n    )\n    parser.add_argument(\n        \"--update_latent_variables\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--update_conditioning\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--subject_id\",\n        default=\"019\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--experiment_name\",\n        default=\"log_inversed_conditions\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--lambda_perc\",\n        default=0.001,\n        type=float,\n    )\n    parser.add_argument(\n        \"--perc_dim\",\n        default=\"axial\",\n        type=str,\n        choices=[\"axial\", \"coronal\", \"sagittal\"],\n    )\n    parser.add_argument(\n        \"--start_steps\",\n        default=0,\n        type=int,\n    )\n    parser.add_argument(\n        \"--num_steps\",\n        default=250,\n        type=int,\n    )\n    parser.add_argument(\n        \"--update_gender\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--update_age\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--update_ventricular\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--update_brain\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--corruption\",\n        default=\"None\",\n        type=str,\n        choices=[\"downsample\", \"mask\", \"None\"],\n    )\n    parser.add_argument(\n        \"--mask_id\",\n        default=\"0\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--prior_every\",\n        default=20,\n        type=int,\n    )\n    parser.add_argument(\n        \"--ddim_num_timesteps\",\n        default=250,\n        type=int,\n    )\n    parser.add_argument(\n        \"--downsample_factor\",\n        default=4,\n        type=int,\n        choices=[2, 4, 8, 16, 32, 64],\n    )\n    parser.add_argument(\n        \"--kernel_size\",\n        default=3,\n        type=int,\n    )\n    parser.add_argument(\n        \"--downsampling_loss\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--mean_latent_vector\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--alpha_downsampling_loss\",\n        default=0,\n        type=float,\n    )\n    parser.add_argument(\n        \"--downsampling_loss_factor\",\n        default=4,\n        type=int,\n    )\n    parser.add_argument(\n        \"--prior_after\",\n        default=50,\n        type=int,\n    )\n    parser.add_argument(\n        \"--n_latent_samples\",\n        default=10000,\n        type=int,\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        default=10,\n        type=int,\n    )\n    parser.add_argument(\n        \"--seed\",\n        default=42,\n        type=int,\n    )\n    parser.add_argument(\n        \"--n_samples\",\n        default=6,\n        type=int,\n    )\n    parser.add_argument(\n        \"--bandwidth\",\n        default=10,\n        type=float,\n    )\n    parser.add_argument(\n        \"--ddim_eta\",\n        default=0.0,\n        type=float,\n    )\n    parser.add_argument(\n        \"--k\",\n        default=1,\n        type=int,\n    )\n"}
{"type": "source_file", "path": "project/models/attention.py", "content": "from inspect import isfunction\n\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom torch import nn, einsum\n\n\ndef exists(val):\n    return val is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = (\n            nn.Sequential(nn.Linear(dim, inner_dim), nn.GELU())\n            if not glu\n            else GEGLU(dim, inner_dim)\n        )\n\n        self.net = nn.Sequential(\n            project_in, nn.Dropout(dropout), nn.Linear(inner_dim, dim_out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n    )\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim), nn.Dropout(dropout)\n        )\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n\n        if exists(mask):\n            mask = rearrange(mask, \"b ... -> b (...)\")\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n\n        # attention, what we cannot get enough of\n        attn = sim.softmax(dim=-1)\n\n        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n        return self.to_out(out)\n\n\nclass BasicTransformerBlock(nn.Module):\n    def __init__(\n        self, dim, n_heads, d_head, dropout=0.0, context_dim=None, gated_ff=True\n    ):\n        super().__init__()\n        self.attn1 = CrossAttention(\n            query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout\n        )  # is a self-attention\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n        self.attn2 = CrossAttention(\n            query_dim=dim,\n            context_dim=context_dim,\n            heads=n_heads,\n            dim_head=d_head,\n            dropout=dropout,\n        )  # is self-attn if context is none\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n\n    def forward(self, x, context=None):\n        x = self.attn1(self.norm1(x)) + x\n        x = self.attn2(self.norm2(x), context=context) + x\n        x = self.ff(self.norm3(x)) + x\n        return x\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    \"\"\"\n\n    def __init__(\n        self, in_channels, n_heads, d_head, depth=1, dropout=0.0, context_dim=None\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = Normalize(in_channels)\n\n        self.proj_in = nn.Conv3d(\n            in_channels, inner_dim, kernel_size=1, stride=1, padding=0\n        )\n\n        self.transformer_blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim\n                )\n                for d in range(depth)\n            ]\n        )\n\n        self.proj_out = zero_module(\n            nn.Conv3d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0)\n        )\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        b, c, h, w, d = x.shape\n        x_in = x\n        x = self.norm(x)\n        x = self.proj_in(x)\n        x = rearrange(x, \"b c h w d -> b (h w d) c\")\n        for block in self.transformer_blocks:\n            x = block(x, context=context)\n        x = rearrange(x, \"b (h w d) c -> b c h w d\", h=h, w=w, d=d)\n        x = self.proj_out(x)\n        return x + x_in\n"}
{"type": "source_file", "path": "project/models/aekl_no_attention.py", "content": "\"\"\"\nAUTOENCODER WITH ARCHTECTURE FROM VERSION 2\n\"\"\"\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n@torch.jit.script\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Conv3d(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Conv3d(\n            in_channels, in_channels, kernel_size=3, stride=2, padding=0\n        )\n\n    def forward(self, x):\n        pad = (0, 1, 0, 1, 0, 1)\n        x = nn.functional.pad(x, pad, mode=\"constant\", value=0)\n        x = self.conv(x)\n        return x\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=None):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = nn.Conv3d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.norm2 = Normalize(out_channels)\n        self.conv2 = nn.Conv3d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n\n        if self.in_channels != self.out_channels:\n            self.nin_shortcut = nn.Conv3d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n            )\n\n    def forward(self, x):\n        h = x\n        h = self.norm1(h)\n        h = F.silu(h)\n        h = self.conv1(h)\n\n        h = self.norm2(h)\n        h = F.silu(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            x = self.nin_shortcut(x)\n\n        return x + h\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        n_channels: int,\n        z_channels: int,\n        ch_mult: Tuple[int],\n        num_res_blocks: int,\n        resolution: Tuple[int],\n        attn_resolutions: Tuple[int],\n        **ignorekwargs,\n    ) -> None:\n        super().__init__()\n        self.in_channels = in_channels\n        self.n_channels = n_channels\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.attn_resolutions = attn_resolutions\n\n        curr_res = resolution\n        in_ch_mult = (1,) + tuple(ch_mult)\n\n        blocks = []\n        # initial convolution\n        blocks.append(\n            nn.Conv3d(in_channels, n_channels, kernel_size=3, stride=1, padding=1)\n        )\n\n        # residual and downsampling blocks, with attention on smaller res (16x16)\n        for i in range(self.num_resolutions):\n            block_in_ch = n_channels * in_ch_mult[i]\n            block_out_ch = n_channels * ch_mult[i]\n            for _ in range(self.num_res_blocks):\n                blocks.append(ResBlock(block_in_ch, block_out_ch))\n                block_in_ch = block_out_ch\n\n            if i != self.num_resolutions - 1:\n                blocks.append(Downsample(block_in_ch))\n                curr_res = tuple(ti // 2 for ti in curr_res)\n\n        # normalise and convert to latent size\n        blocks.append(Normalize(block_in_ch))\n        blocks.append(\n            nn.Conv3d(block_in_ch, z_channels, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        n_channels: int,\n        z_channels: int,\n        out_channels: int,\n        ch_mult: Tuple[int],\n        num_res_blocks: int,\n        resolution: Tuple[int],\n        attn_resolutions: Tuple[int],\n        **ignorekwargs,\n    ) -> None:\n        super().__init__()\n        self.n_channels = n_channels\n        self.z_channels = z_channels\n        self.out_channels = out_channels\n        self.ch_mult = ch_mult\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.attn_resolutions = attn_resolutions\n\n        block_in_ch = n_channels * self.ch_mult[-1]\n        curr_res = tuple(ti // 2 ** (self.num_resolutions - 1) for ti in resolution)\n\n        blocks = []\n        # initial conv\n        blocks.append(\n            nn.Conv3d(z_channels, block_in_ch, kernel_size=3, stride=1, padding=1)\n        )\n\n        for i in reversed(range(self.num_resolutions)):\n            block_out_ch = n_channels * self.ch_mult[i]\n\n            for _ in range(self.num_res_blocks):\n                blocks.append(ResBlock(block_in_ch, block_out_ch))\n                block_in_ch = block_out_ch\n\n            if i != 0:\n                blocks.append(Upsample(block_in_ch))\n                curr_res = tuple(ti * 2 for ti in curr_res)\n\n        blocks.append(Normalize(block_in_ch))\n        blocks.append(\n            nn.Conv3d(block_in_ch, out_channels, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n\nclass AutoencoderKL(nn.Module):\n    def __init__(self, embed_dim: int, hparams) -> None:\n        super().__init__()\n        self.encoder = Encoder(**hparams)\n        self.decoder = Decoder(**hparams)\n        self.quant_conv_mu = torch.nn.Conv3d(hparams[\"z_channels\"], embed_dim, 1)\n        self.quant_conv_log_sigma = torch.nn.Conv3d(hparams[\"z_channels\"], embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv3d(embed_dim, hparams[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n\n    def decode(self, z):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def reconstruct_ldm_outputs(self, z):\n        x_hat = self.decode(z)\n        return x_hat\n\n\nclass OnlyDecoder(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.decoder = None\n        self.post_quant_conv = None\n\n    def decode(self, z):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def reconstruct_ldm_outputs(self, z):\n        x_hat = self.decode(z)\n        return x_hat\n"}
{"type": "source_file", "path": "project/utils/convert_img_to_nii.py", "content": "import nibabel as nib\n\nif __name__ == \"__main__\":\n    file_path = r\"C:\\Users\\16446\\Documents\\GitHub\\Medical-Image-Reconstruction\\data\\OASIS\\oasis-1\\disc1\\OAS1_0002_MR1\\PROCESSED\\MPRAGE\\T88_111\\OAS1_0002_MR1_mpr_n4_anon_111_t88_gfc.img\"\n    img = nib.load(file_path)\n    nib.save(img, file_path.replace(\".img\", \".nii\"))\n"}
{"type": "source_file", "path": "project/utils/const.py", "content": "import os\nfrom pathlib import Path\n\n\n# Use environment variables to auto-detect whether we are running an a Compute Canada cluster:\n# Thanks to https://github.com/DM-Berger/unet-learn/blob/master/src/train/load.py for this trick.\nCOMPUTECANADA = False\nTMP = os.environ.get(\"SLURM_TMPDIR\")\n\nif TMP:\n    COMPUTECANADA = True\n\nif COMPUTECANADA:\n    INPUT_FOLDER = Path(str(TMP)).resolve() / \"work\" / \"inputs\"\n    MASK_FOLDER = Path(str(TMP)).resolve() / \"work\" / \"inputs\" / \"masks\"\n    PRETRAINED_MODEL_FOLDER = Path(str(TMP)).resolve() / \"work\" / \"trained_models\"\n    PRETRAINED_MODEL_DDPM_PATH = (\n        Path(str(TMP)).resolve() / \"work\" / \"trained_models\" / \"ddpm\"\n    )\n    PRETRAINED_MODEL_VAE_PATH = (\n        Path(str(TMP)).resolve() / \"work\" / \"trained_models\" / \"vae\"\n    )\n    PRETRAINED_MODEL_DECODER_PATH = (\n        Path(str(TMP)).resolve() / \"work\" / \"trained_models\" / \"decoder\"\n    )\n    PRETRAINED_MODEL_VGG_PATH = (\n        Path(str(TMP)).resolve() / \"work\" / \"trained_models\" / \"vgg16.pt\"\n    )\n    OUTPUT_FOLDER = Path(str(TMP)).resolve() / \"work\" / \"outputs\"\nelse:\n    INPUT_FOLDER = Path(__file__).resolve().parent.parent.parent / \"inputs\"\n    MASK_FOLDER = Path(__file__).resolve().parent.parent / \"masks\"\n    OASIS_FOLDER = Path(__file__).resolve().parent.parent.parent / \"data\" / \"OASIS\"\n    PRETRAINED_MODEL_FOLDER = (\n        Path(__file__).resolve().parent.parent.parent / \"data\" / \"trained_models\"\n    )\n    PRETRAINED_MODEL_DDPM_PATH = (\n        Path(__file__).resolve().parent.parent.parent\n        / \"data\"\n        / \"trained_models\"\n        / \"ddpm\"\n    )\n    PRETRAINED_MODEL_VAE_PATH = (\n        Path(__file__).resolve().parent.parent.parent\n        / \"data\"\n        / \"trained_models\"\n        / \"vae\"\n    )\n    PRETRAINED_MODEL_DECODER_PATH = (\n        Path(__file__).resolve().parent.parent.parent / \"decoder\"\n    )\n    PRETRAINED_MODEL_VGG_PATH = (\n        Path(__file__).resolve().parent.parent.parent\n        / \"data\"\n        / \"trained_models\"\n        / \"vgg16.pt\"\n    )\n    OUTPUT_FOLDER = (\n        Path(__file__).resolve().parent.parent.parent / \"data\" / \"outputs\" / \"ddpm\"\n    )\n    THESIS_IMG_FOLDER = (\n        Path(__file__).resolve().parent.parent.parent / \"data\" / \"thesis_imgs\"\n    )\n    FINAL_RESULTS_FOLDER = (\n        Path(__file__).resolve().parent.parent.parent\n        / \"data\"\n        / \"outputs\"\n        / \"final_results\"\n    )\n\nLATENT_SHAPE = [1, 3, 20, 28, 20]\nIMAGE_SHAPE = [1, 1, 160, 224, 160]\n\nIXI_IDs = [\n    \"002\",\n    \"013\",\n    \"015\",\n    \"019\",\n    \"022\",\n    \"025\",\n    \"027\",\n    \"030\",\n    \"031\",\n    \"033\",\n    \"034\",\n    \"036\",\n    \"039\",\n    \"040\",\n    \"041\",\n    \"042\",\n    \"043\",\n    \"045\",\n    \"046\",\n    \"048\",\n    \"049\",\n    \"050\",\n    \"051\",\n    \"058\",\n    \"059\",\n    \"067\",\n    \"068\",\n    \"069\",\n    \"072\",\n    \"073\",\n    \"074\",\n    \"076\",\n    \"077\",\n    \"079\",\n    \"080\",\n    \"083\",\n    \"087\",\n    \"089\",\n    \"090\",\n    \"091\",\n    \"092\",\n    \"093\",\n    \"095\",\n    \"096\",\n    \"097\",\n    \"098\",\n    \"099\",\n    \"102\",\n    \"104\",\n    \"105\",\n    \"108\",\n    \"109\",\n    \"110\",\n    \"111\",\n    \"114\",\n    \"115\",\n    \"118\",\n    \"119\",\n    \"120\",\n    \"121\",\n    \"122\",\n    \"123\",\n    \"126\",\n    \"127\",\n    \"128\",\n    \"129\",\n    \"130\",\n    \"131\",\n    \"132\",\n    \"135\",\n    \"140\",\n    \"141\",\n    \"146\",\n    \"154\",\n    \"156\",\n    \"157\",\n    \"158\",\n    \"159\",\n    \"160\",\n    \"161\",\n    \"162\",\n    \"163\",\n    \"164\",\n    \"167\",\n    \"168\",\n    \"173\",\n    \"174\",\n    \"175\",\n    \"176\",\n    \"178\",\n    \"179\",\n    \"180\",\n    \"181\",\n    \"184\",\n    \"186\",\n    \"191\",\n    \"192\",\n    \"195\",\n    \"197\",\n    \"200\",\n]\n"}
{"type": "source_file", "path": "project/models/BRGM/forward_models.py", "content": "# Code is adapted from: https://github.com/razvanmarinescu/brgm-pytorch/blob/master/forwardModels.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\nimport scipy.ndimage.morphology\n\n\nclass ForwardAbstract(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def __call__(self, x):\n        return x\n\n    def calcMaskFromImg(self, img):\n        pass\n\n    def initVars(self):\n        pass\n\n    def getVars(self):\n        return []\n\n\nclass ForwardNone(ForwardAbstract):\n    def __init__(self):\n        pass\n\n    def __call__(self, x):\n        return x\n\n\nclass ForwardDownsample(ForwardAbstract):\n    def __init__(self, factor):\n        self.factor = factor\n\n    # resolution of input x can be anything, but aspect ratio should be 1:1\n    def __call__(self, x):\n        x_down = F.interpolate(\n            x,\n            scale_factor=1 / self.factor,\n            mode=\"trilinear\",\n            recompute_scale_factor=True,\n            align_corners=False,\n        )  # BCHW\n        return x_down\n\n\nclass ForwardFillMask(ForwardAbstract):\n    def __init__(self, device, mask: Optional[np.ndarray] = None):\n        self.device = device\n        self.mask = torch.from_numpy(mask).to(device) if mask is not None else None\n\n    def calcMaskFromImg(self, img):\n        nrBins = 256\n        grayImg = np.squeeze(np.mean(img, axis=1))\n        gray1D = (\n            grayImg.ravel()\n        )  # eliminate the first bin with black pixels, as it doesn't work for brains (wrong mask is estimated)\n        hist, bins = np.histogram(gray1D, nrBins, [-1, 1])\n        print(hist, bins)\n        hist = hist[1:]\n        bins = bins[1:]\n        maxIndex = np.argmax(hist)\n\n        self.mask = np.abs(grayImg - bins[maxIndex]) < (3.0 / nrBins)\n        self.mask = torch.tensor(\n            scipy.ndimage.morphology.binary_opening(self.mask, iterations=3),\n            dtype=torch.bool,\n            device=self.device,\n        )\n        self.mask = torch.repeat(\n            torch.reshape(self.mask, (1, 1, *self.mask.shape)), img.shape[1], axis=1\n        )\n\n    def __call__(self, x):\n        if self.mask is None:\n            self.mask = torch.zeros(x.shape, dtype=torch.bool, device=self.device)\n\n        white_fill = torch.ones(x.shape, device=self.device, dtype=x.dtype)\n        # white_fill = torch.zeros(x.shape, device=self.device, dtype=x.dtype)\n        xFill = torch.where(self.mask, white_fill, x)  # if true, then whiteFill, else x\n\n        return xFill\n"}
{"type": "source_file", "path": "project/BRGM_decoder.py", "content": "# Code is adpated from: https://huggingface.co/spaces/Warvito/diffusion_brain/blob/main/app.py and\n# https://colab.research.google.com/drive/1xJAor6_Ky36gxIk6ICNP--NMBjSlYKil?usp=sharing#scrollTo=4XDeCy-Vj59b\n# A lot of thanks to the author of the code\n# Reference:\n# [1] Pinaya, W. H., et al. (2022). \"Brain Imaging Generation with Latent Diffusion Models.\" arXiv preprint arXiv:2209.07162.\n# [2] Marinescu, R., et al. (2020). Bayesian Image Reconstruction using Deep Generative Models.\n\nimport math\n\n# from joblib import dump, load\nfrom argparse import ArgumentParser, Namespace\nfrom time import perf_counter\nfrom typing import List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport csv\nfrom models.BRGM.forward_models import ForwardAbstract\nfrom skimage.metrics import mean_squared_error as mse\nfrom skimage.metrics import normalized_root_mse as nmse\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nfrom sklearn.neighbors import NearestNeighbors\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom utils.add_argument import add_argument\nfrom utils.const import (\n    INPUT_FOLDER,\n    LATENT_SHAPE,\n    OUTPUT_FOLDER,\n    PRETRAINED_MODEL_DDPM_PATH,\n    PRETRAINED_MODEL_DECODER_PATH,\n)\nfrom utils.plot import draw_corrupted_images, draw_images, draw_img\nfrom utils.utils import (\n    create_corruption_function,\n    generating_latent_vector,\n    getVggFeatures,\n    inference,\n    load_ddpm_latent_vectors,\n    load_ddpm_model,\n    load_pre_trained_decoder,\n    load_target_image,\n    load_vgg_perceptual,\n    setup_noise_inputs,\n)\n\n\ndef logprint(message: str, verbose: bool) -> None:\n    if verbose:\n        print(message)\n\n\ndef add_hparams_to_tensorboard(\n    hparams: Namespace,\n    final_loss: torch.Tensor,\n    final_ssim: float,\n    final_psnr: float,\n    final_mse: float,\n    final_nmse: float,\n    inversed_gender: float,\n    inversed_age: float,\n    inversed_ventricular: float,\n    inversed_brain: float,\n    writer: SummaryWriter,\n) -> None:\n    writer.add_hparams(\n        {\n            \"num_steps\": hparams.num_steps,\n            \"learning_rate\": hparams.learning_rate,\n            \"experiment_name\": hparams.experiment_name,\n            \"subject_id\": hparams.subject_id,\n            \"update_latent_variables\": hparams.update_latent_variables,\n            \"update_conditioning\": hparams.update_conditioning,\n            \"update_gender\": hparams.update_gender,\n            \"update_age\": hparams.update_age,\n            \"update_ventricular\": hparams.update_ventricular,\n            \"update_brain\": hparams.update_brain,\n            \"alpha\": hparams.lambda_alpha,\n            \"perc\": hparams.lambda_perc,\n            \"kernel_size\": hparams.kernel_size,\n        },\n        {\n            \"loss/final_loss\": final_loss,\n            \"measurement/final_ssim\": final_ssim,\n            \"measurement/final_psnr\": final_psnr,\n            \"measurement/final_mse\": final_mse,\n            \"measurement/final_nmse\": final_nmse,\n            \"conditional_variable/inversed_gender\": inversed_gender,\n            \"conditional_variable/inversed_age\": inversed_age,\n            \"conditional_variable/inversed_ventricular\": inversed_ventricular,\n            \"conditional_variable/inversed_brain\": inversed_brain,\n        },\n    )\n\n\ndef create_mask_for_backprop(hparams: Namespace, device: torch.device) -> torch.Tensor:\n    mask_cond = torch.ones((1, 4), device=device)\n    mask_cond[:, 0] = 0 if not hparams.update_gender else 1\n    mask_cond[:, 1] = 0 if not hparams.update_age else 1\n    mask_cond[:, 2] = 0 if not hparams.update_ventricular else 1\n    mask_cond[:, 3] = 0 if not hparams.update_brain else 1\n    return mask_cond\n\n\ndef compute_latent_vector_stats(\n    latent_vectors: torch.Tensor,\n    device: torch.device,\n    verbose: bool = False,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    logprint(\"Computing latent vector stats\", verbose)\n    latent_mean = torch.mean(latent_vectors, axis=0, keepdim=True)\n    latnet_std = torch.std(latent_vectors, dim=0, keepdim=True, unbiased=False)\n    return latent_mean, latnet_std\n\n\ndef compute_prior_loss(\n    cur_latent_vector: torch.Tensor,\n    latent_vectors: torch.Tensor,\n    latent_vector_std: torch.Tensor,\n    knn_model: NearestNeighbors,\n    hparams: Namespace,\n) -> Tuple[torch.Tensor, List[int]]:\n    cur_latent_vector_np = cur_latent_vector.detach().cpu().numpy().reshape((1, -1))\n    _, indices = knn_model.kneighbors(cur_latent_vector_np, n_neighbors=hparams.k)\n    nearest_latent_vectors = latent_vectors[indices[0]]\n    mean_nearest_latent_vector = torch.mean(\n        nearest_latent_vectors, axis=0, keepdim=True\n    )\n    prior_loss = (\n        (\n            (cur_latent_vector / latent_vector_std)\n            - (mean_nearest_latent_vector / latent_vector_std)\n        )\n        .abs()\n        .mean()\n    )\n    return prior_loss, indices[0]\n\n\ndef project(\n    vqvae: torch.nn.Module,\n    forward: ForwardAbstract,  # Corruption function\n    target: torch.Tensor,\n    device: torch.device,\n    writer: SummaryWriter,\n    hparams: Namespace,\n    verbose: bool = False,\n):\n    latent_vectors_tensor = load_ddpm_latent_vectors(device)\n    latent_vector_mean, latent_vector_std = compute_latent_vector_stats(\n        latent_vectors=latent_vectors_tensor, device=device, verbose=verbose\n    )\n\n    if not hparams.mean_latent_vector:\n        ddpm = load_ddpm_model(ddpm_path=PRETRAINED_MODEL_DDPM_PATH, device=device)\n        cond, latent_variable = setup_noise_inputs(device=device)\n        cond_crossatten = cond.unsqueeze(1)\n        cond_concat = cond.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        cond_concat = cond_concat.expand(list(cond.shape[0:2]) + list(LATENT_SHAPE[2:]))\n        conditioning = {\n            \"c_concat\": [cond_concat.float().to(device)],\n            \"c_crossattn\": [cond_crossatten.float().to(device)],\n        }\n        latent_vector = generating_latent_vector(\n            diffusion=ddpm,\n            latent_variable=latent_variable,\n            conditioning=conditioning,\n            batch_size=1,\n        )\n    else:\n        latent_vector = latent_vector_mean.clone().detach()\n    latent_vector.requires_grad = True\n\n    update_params = []\n    update_params.append(latent_vector)\n\n    optimizer_adam = torch.optim.Adam(\n        update_params,\n        betas=(0.9, 0.999),\n        lr=hparams.learning_rate,\n    )\n    latent_vector_out = torch.zeros(\n        [hparams.num_steps] + list(latent_vector.shape[1:]),\n        dtype=torch.float32,\n        device=device,\n    )\n\n    target_img_corrupted = forward(target)\n    vgg16, target_features = load_vgg_perceptual(hparams, target_img_corrupted, device)\n    total_num_pixels = (\n        target_img_corrupted.numel()\n        if hparams.corruption != \"mask\"\n        else math.prod(forward.mask.shape) - forward.mask.sum()\n    )\n\n    # Compute latent representation stats.\n    for step in range(hparams.start_steps, hparams.num_steps):\n\n        def closure():\n            optimizer_adam.zero_grad()\n\n            synth_img = inference(\n                vqvae=vqvae,\n                latent_vectors=latent_vector,\n            )\n            synth_img_corrupted = forward(synth_img)\n\n            loss = 0\n            downsampling_loss = 0\n            prior_loss = 0\n            indices = [0]\n            if hparams.corruption != \"None\":\n                pixelwise_loss = (\n                    synth_img_corrupted - target_img_corrupted\n                ).abs().sum() / total_num_pixels\n                loss += pixelwise_loss\n\n                synth_features = getVggFeatures(hparams, synth_img_corrupted, vgg16)\n                perc_loss = (target_features - synth_features).abs().mean()\n                loss += hparams.lambda_perc * perc_loss\n            else:\n                pixelwise_loss = (synth_img - target).abs().mean()\n                loss += (1 - hparams.alpha_downsampling_loss) * pixelwise_loss\n\n                synth_features = getVggFeatures(hparams, synth_img_corrupted, vgg16)\n                perc_loss = (target_features - synth_features).abs().mean()\n                loss += hparams.lambda_perc * perc_loss\n\n            loss.backward(create_graph=False)\n\n            return (\n                loss,\n                pixelwise_loss,\n                perc_loss,\n                downsampling_loss,\n                prior_loss,\n                synth_img,\n                synth_img_corrupted,\n                indices,\n            )\n\n        (\n            loss,\n            pixelwise_loss,\n            perc_loss,\n            downsampling_loss,\n            prior_loss,\n            synth_img,\n            synth_img_corrupted,\n            indices,\n        ) = optimizer_adam.step(closure=closure)\n\n        synth_img_np = synth_img[0, 0].detach().cpu().numpy()\n        target_np = target[0, 0].detach().cpu().numpy()\n        ssim_ = ssim(\n            synth_img_np,\n            target_np,\n            win_size=11,\n            data_range=1.0,\n            gaussian_weights=True,\n            use_sample_covariance=False,\n        )\n        # Code for computing PSNR is adapted from\n        # https://github.com/agis85/multimodal_brain_synthesis/blob/master/error_metrics.py#L32\n        data_range = np.max([synth_img_np.max(), target_np.max()]) - np.min(\n            [synth_img_np.min(), target_np.min()]\n        )\n        psnr_ = psnr(target_np, synth_img_np, data_range=data_range)\n        mse_ = mse(target_np, synth_img_np)\n        nmse_ = nmse(target_np, synth_img_np)\n\n        writer.add_scalar(\"loss\", loss, global_step=step)\n        writer.add_scalar(\"pixelwise_loss\", pixelwise_loss, global_step=step)\n        writer.add_scalar(\"perceptual_loss\", perc_loss, global_step=step)\n        writer.add_scalar(\"downsampling_loss\", downsampling_loss, global_step=step)\n        if prior_loss != 0:\n            writer.add_scalar(\"prior_loss\", prior_loss, global_step=step)\n            writer.add_scalar(\"indice\", indices[0], global_step=step)\n        writer.add_scalar(\"ssim\", ssim_, global_step=step)\n        writer.add_scalar(\"psnr\", psnr_, global_step=step)\n        writer.add_scalar(\"mse\", mse_, global_step=step)\n        writer.add_scalar(\"nmse\", nmse_, global_step=step)\n\n        logprint(\n            f\"step {step + 1:>4d}/{hparams.num_steps}: tloss {float(loss):<5.8f} pix_loss {float(pixelwise_loss):<5.8f} perc_loss {float(perc_loss):<1.15f} prior_loss {float(prior_loss):<5.8f}\\n\"\n            f\"              : SSIM {float(ssim_):<5.8f} PSNR {float(psnr_):<5.8f} MSE {float(mse_):<5.8f} NMSE {float(nmse_):<5.8f}\",\n            verbose=verbose,\n        )\n\n        step_ = f\"{step}\".zfill(4)\n        draw_img(\n            synth_img_np,\n            title=\"synth\",\n            step=step_,\n            output_folder=OUTPUT_FOLDER,\n        )\n\n        if step % 25 == 0:\n            if hparams.corruption != \"None\":\n                imgs = draw_corrupted_images(\n                    synth_img_np,\n                    target_np,\n                    synth_img_corrupted[0, 0].detach().cpu().numpy(),\n                    target_img_corrupted[0, 0].detach().cpu().numpy(),\n                    ssim_=ssim_,\n                )\n            else:\n                imgs = draw_images(\n                    synth_img_np,\n                    target_np,\n                    ssim_=ssim_,\n                )\n            step_ = f\"{step}\".zfill(4)\n            writer.add_figure(f\"step: {step_}\", imgs, global_step=step)\n            plt.close(imgs)\n\n        latent_vector_out[step] = latent_vector.detach()[0]\n\n    add_hparams_to_tensorboard(\n        hparams,\n        final_loss=loss.item(),\n        final_ssim=ssim_,\n        final_psnr=psnr_,\n        final_mse=mse_,\n        final_nmse=nmse_,\n        inversed_gender=cond[0, 0].clone().detach().item(),\n        inversed_age=cond[0, 1].clone().detach().item() * (82 - 44) + 44,\n        inversed_ventricular=cond[0, 2].clone().detach().item(),\n        inversed_brain=cond[0, 3].clone().detach().item(),\n        writer=writer,\n    )\n\n    draw_img(\n        target_np,\n        title=\"target\",\n        step=step_,\n        output_folder=OUTPUT_FOLDER,\n    )\n\n    draw_img(\n        synth_img_corrupted[0, 0].detach().cpu().numpy(),\n        title=\"corrupted\",\n        step=step_,\n        output_folder=OUTPUT_FOLDER,\n    )\n\n    writer.flush()\n    writer.close()\n\n    torch.save(\n        {\n            \"epoch\": step,\n            \"latent_vectors\": latent_vector,\n            \"optimizer\": optimizer_adam.state_dict(),\n        },\n        OUTPUT_FOLDER / \"checkpoint.pth\",\n    )\n\n    row = [\n        hparams.subject_id,\n        ssim_,\n        psnr_,\n        mse_,\n        nmse_,\n    ]\n\n    with open(\n        \"/scratch/j/jlevman/jueqi/thesis_experiments/decoder/result_decoder_downsample_2.csv\",\n        \"a\",\n    ) as file:\n        writer = csv.writer(file)\n        writer.writerow(row)\n\n    return latent_vector_out\n\n\ndef main(hparams: Namespace) -> None:\n    # device = torch.device(\"cuda\" if COMPUTECANADA else \"cpu\")\n    # Don't have enough memory to run on GPU. :(\n    device = torch.device(\"cpu\")\n    img_tensor = load_target_image(hparams, device)\n    writer = SummaryWriter(log_dir=hparams.tensor_board_logger)\n\n    forward = create_corruption_function(hparams=hparams, device=device)\n    decoder = load_pre_trained_decoder(\n        vae_path=PRETRAINED_MODEL_DECODER_PATH,\n        device=device,\n    )\n\n    start_time = perf_counter()\n    latent_vector_out = project(\n        decoder,\n        writer=writer,\n        hparams=hparams,\n        forward=forward,\n        target=img_tensor,\n        device=device,\n        verbose=True,\n    )\n    print(f\"Elapsed: {(perf_counter() - start_time):.1f} s\")\n\n    torch.save(\n        {\"latent_vector_out\": latent_vector_out},\n        OUTPUT_FOLDER / \"latent_vector_out.pth\",\n    )\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Trainer args\", add_help=False)\n    add_argument(parser)\n    hparams = parser.parse_args()\n    # seed_everything(hparams.seed)\n    main(hparams)\n"}
{"type": "source_file", "path": "project/models/unet_v2_conditioned.py", "content": "import math\nfrom abc import abstractmethod\n\nimport numpy as np\nimport torch\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import repeat\n\nfrom models.attention import SpatialTransformer\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period)\n            * torch.arange(start=0, end=half, dtype=torch.float32)\n            / half\n        ).to(device=timesteps.device)\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat(\n                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n            )\n    else:\n        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n    return embedding\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass TimestepBlock(nn.Module):\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb, context=None):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            elif isinstance(layer, SpatialTransformer):\n                x = layer(x, context)\n            else:\n                x = layer(x)\n        return x\n\n\ndef Normalize(in_channels):\n    return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial**2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = Normalize(channels)\n        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n        self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n\n    def forward(self, x):\n        return self._forward(\n            x,\n        )\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        if use_conv:\n            self.op = nn.Conv3d(\n                self.channels, self.out_channels, 3, stride=2, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = nn.AvgPool3d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        if use_conv:\n            self.conv = nn.Conv3d(self.channels, self.out_channels, 3, padding=padding)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            Normalize(channels),\n            nn.SiLU(),\n            nn.Conv3d(channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False)\n            self.x_upd = Upsample(channels, False)\n        elif down:\n            self.h_upd = Downsample(channels, False)\n            self.x_upd = Downsample(channels, False)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            Normalize(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Conv3d(self.out_channels, self.out_channels, 3, padding=1)),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = nn.Conv3d(channels, self.out_channels, 3, padding=1)\n        else:\n            self.skip_connection = nn.Conv3d(channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        return self._forward(x, emb)\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass UNetModel(nn.Module):\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        num_classes=None,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_spatial_transformer=False,  # custom transformer support\n        transformer_depth=1,  # custom transformer support\n        context_dim=None,  # custom transformer support\n        n_embed=None,  # custom support for prediction of discrete ids into codebook of first stage vq model\n    ):\n        super().__init__()\n\n        if use_spatial_transformer:\n            assert (\n                context_dim is not None\n            ), \"Fool!! You forgot to include the dimension of your cross-attention conditioning...\"\n\n        if context_dim is not None:\n            assert (\n                use_spatial_transformer\n            ), \"Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...\"\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            nn.Linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    nn.Conv3d(in_channels, model_channels, 3, padding=1)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    dim_head = ch // num_heads\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                        )\n                        if not use_spatial_transformer\n                        else SpatialTransformer(\n                            ch,\n                            num_heads,\n                            dim_head,\n                            depth=transformer_depth,\n                            context_dim=context_dim,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(ch, conv_resample, out_channels=out_ch)\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        dim_head = ch // num_heads\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n            )\n            if not use_spatial_transformer\n            else SpatialTransformer(\n                ch,\n                num_heads,\n                dim_head,\n                depth=transformer_depth,\n                context_dim=context_dim,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=model_channels * mult,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    dim_head = ch // num_heads\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                        )\n                        if not use_spatial_transformer\n                        else SpatialTransformer(\n                            ch,\n                            num_heads,\n                            dim_head,\n                            depth=transformer_depth,\n                            context_dim=context_dim,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            Normalize(ch),\n            nn.SiLU(),\n            zero_module(nn.Conv3d(model_channels, out_channels, 3, padding=1)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n                Normalize(ch),\n                nn.Conv3d(model_channels, n_embed, 1),\n            )\n\n    def forward(self, x, timesteps=None, context=None, y=None, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n        assert timesteps is not None, \"need to implement no-timestep usage\"\n        hs = []\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n        emb = self.time_embed(t_emb)\n\n        if self.num_classes is not None:\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n\n        h = x\n        for module in self.input_blocks:\n            h = module(h, emb, context)\n            hs.append(h)\n        h = self.middle_block(h, emb, context)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb, context)\n\n        if self.predict_codebook_ids:\n            # return self.out(h), self.id_predictor(h)\n            return self.id_predictor(h)\n        else:\n            return self.out(h)\n"}
{"type": "source_file", "path": "project/models/ddpm_v2_conditioned.py", "content": "from functools import partial\nfrom inspect import isfunction\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom models.unet_v2_conditioned import UNetModel\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n        shape[0], *((1,) * (len(shape) - 1))\n    )\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n\n\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef make_beta_schedule(\n    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n):\n    if schedule == \"linear\":\n        betas = (\n            torch.linspace(\n                linear_start**0.5, linear_end**0.5, n_timestep, dtype=torch.float64\n            )\n            ** 2\n        )\n\n    elif schedule == \"cosine\":\n        timesteps = (\n            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n        alphas = torch.cos(alphas).pow(2)\n        alphas = alphas / alphas[0]\n        betas = 1 - alphas[1:] / alphas[:-1]\n        betas = np.clip(betas, a_min=0, a_max=0.999)\n\n    elif schedule == \"sqrt_linear\":\n        betas = torch.linspace(\n            linear_start, linear_end, n_timestep, dtype=torch.float64\n        )\n    elif schedule == \"sqrt\":\n        betas = (\n            torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n            ** 0.5\n        )\n    else:\n        raise ValueError(f\"schedule '{schedule}' unknown.\")\n    return betas.numpy()\n\n\nclass DDPM(nn.Module):\n    def __init__(\n        self,\n        unet_config,\n        timesteps: int = 1000,\n        beta_schedule=\"linear\",\n        loss_type=\"l2\",\n        log_every_t=100,\n        clip_denoised=False,\n        linear_start=1e-4,\n        linear_end=2e-2,\n        cosine_s=8e-3,\n        original_elbo_weight=0.0,\n        v_posterior=0.0,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n        l_simple_weight=1.0,\n        parameterization=\"eps\",  # all assuming fixed variance schedules\n        learn_logvar=False,\n        logvar_init=0.0,\n        conditioning_key=None,\n    ):\n        super().__init__()\n        assert parameterization in [\n            \"eps\",\n            \"x0\",\n        ], 'currently only supporting \"eps\" and \"x0\"'\n        self.parameterization = parameterization\n\n        if conditioning_key == \"unconditioned\":\n            conditioning_key = None\n        self.conditioning_key = conditioning_key\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        self.loss_type = loss_type\n\n        self.register_schedule(\n            beta_schedule=beta_schedule,\n            timesteps=timesteps,\n            linear_start=linear_start,\n            linear_end=linear_end,\n            cosine_s=cosine_s,\n        )\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n    def register_schedule(\n        self,\n        beta_schedule=\"linear\",\n        timesteps=1000,\n        linear_start=1e-4,\n        linear_end=2e-2,\n        cosine_s=8e-3,\n    ):\n        betas = make_beta_schedule(\n            beta_schedule,\n            timesteps,\n            linear_start=linear_start,\n            linear_end=linear_end,\n            cosine_s=cosine_s,\n        )\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n\n        (timesteps,) = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer(\"betas\", to_torch(betas))\n        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer(\"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod))\n        )\n        self.register_buffer(\n            \"log_one_minus_alphas_cumprod\", to_torch(np.log(1.0 - alphas_cumprod))\n        )\n        self.register_buffer(\n            \"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod))\n        )\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod - 1))\n        )\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (\n            1.0 - alphas_cumprod_prev\n        ) / (1.0 - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer(\"posterior_variance\", to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer(\n            \"posterior_log_variance_clipped\",\n            to_torch(np.log(np.maximum(posterior_variance, 1e-20))),\n        )\n        self.register_buffer(\n            \"posterior_mean_coef1\",\n            to_torch(betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)),\n        )\n        self.register_buffer(\n            \"posterior_mean_coef2\",\n            to_torch(\n                (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n            ),\n        )\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas**2 / (\n                2\n                * self.posterior_variance\n                * to_torch(alphas)\n                * (1 - self.alphas_cumprod)\n            )\n        elif self.parameterization == \"x0\":\n            lvlb_weights = (\n                0.5\n                * np.sqrt(torch.Tensor(alphas_cumprod))\n                / (2.0 * 1 - torch.Tensor(alphas_cumprod))\n            )\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer(\"lvlb_weights\", lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean and variance of the diffusion posterior:\n        q(x_{t-1} | x_t, x_0)\n        \"\"\"\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(\n            self.posterior_log_variance_clipped, t, x_t.shape\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_x0=False):\n        \"\"\"\n        Apply the model to get p(x_{t-1} | x_t)\n        :param model: the model, which takes a signal and a batch of timesteps\n                      as input.\n        :param x: the [N x C x ...] tensor at time t.\n        :param t: a 1-D Tensor of timesteps.\n        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n\n        \"\"\"\n        t_in = t\n        model_out = self.apply_model(x, t_in, c)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n\n        if clip_denoised:\n            x_recon.clamp_(-1.0, 1.0)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n            x_start=x_recon, x_t=x, t=t\n        )\n        if return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance\n\n    def p_sample(\n        self,\n        x,\n        c,\n        t,\n        clip_denoised=True,\n        repeat_noise=False,\n        return_x0=False,\n        temperature=1.0,\n        noise_dropout=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model at the given timestep.\n        :param x: the current tensor at x_{t-1}.\n        :param t: the value of t, starting at 0 for the first diffusion step.\n        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n        \"\"\"\n\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(\n            x=x,\n            c=c,\n            t=t,\n            clip_denoised=clip_denoised,\n            return_x0=return_x0,\n        )\n        if return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.0:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        if return_x0:\n            return (\n                model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise,\n                x0,\n            )\n        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    def p_sample_loop(self, cond, shape, return_intermediates=False):\n        device = self.betas.device\n\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n\n        for i in tqdm(\n            reversed(range(0, self.num_timesteps)),\n            desc=\"sampling loop time step\",\n            total=self.num_timesteps,\n        ):\n            img = self.p_sample(\n                img,\n                cond,\n                torch.full((b,), i, device=device, dtype=torch.long),\n                clip_denoised=self.clip_denoised,\n            )\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop(\n            (batch_size, channels, image_size, image_size),\n            return_intermediates=return_intermediates,\n        )\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Diffuse the data for a given number of diffusion steps.\n        In other words, sample from q(x_t | x_0).\n        :param x_start: the initial data batch.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :param noise: if specified, the split-out normal noise.\n        :return: A noisy version of x_start.\n        \"\"\"\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def get_loss(self, pred, target, mean=True):\n        if self.loss_type == \"l1\":\n            loss = (target - pred).abs()\n            if mean:\n                loss = loss.mean()\n        elif self.loss_type == \"l2\":\n            if mean:\n                loss = torch.nn.functional.mse_loss(target, pred)\n            else:\n                loss = torch.nn.functional.mse_loss(target, pred, reduction=\"none\")\n        else:\n            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n\n        return loss\n\n    def p_losses(self, x_start, cond, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_output = self.apply_model(x_noisy, t, cond)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        else:\n            raise NotImplementedError(\n                f\"Paramterization {self.parameterization} not yet supported\"\n            )\n\n        loss_simple = self.get_loss(model_output, target, mean=False).mean(\n            dim=[1, 2, 3, 4]\n        )\n        loss_dict.update({f\"loss_simple\": loss_simple.mean()})\n\n        logvar_t = self.logvar[t].to(x_start.device)\n        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n        if self.learn_logvar:\n            loss_dict.update({f\"loss_gamma\": loss.mean()})\n            loss_dict.update({\"logvar\": self.logvar.data.mean()})\n\n        loss = self.l_simple_weight * loss.mean()\n\n        loss_vlb = self.get_loss(model_output, target, mean=False).mean(\n            dim=(1, 2, 3, 4)\n        )\n        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n        loss_dict.update({f\"loss_vlb\": loss_vlb})\n        loss += self.original_elbo_weight * loss_vlb\n        loss_dict.update({f\"loss\": loss})\n\n        return loss, loss_dict\n\n    def forward(self, x, c, *args, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n        return self.p_losses(x, c, t, *args, **kwargs)\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.learn_logvar:\n            print(\"Diffusion model optimizing logvar\")\n            params.append(self.logvar)\n        opt = torch.optim.AdamW(params, lr=lr)\n        return opt\n\n    def apply_model(self, x_noisy, t, cond, return_ids=False):\n        if isinstance(cond, dict):\n            # hybrid case, cond is exptected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = (\n                \"c_concat\" if self.model.conditioning_key == \"concat\" else \"c_crossattn\"\n            )\n            cond = {key: cond}\n\n        x_recon = self.model(x_noisy, t, **cond)\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n\nclass DiffusionWrapper(nn.Module):\n    def __init__(self, unet_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = UNetModel(**unet_config.get(\"params\", dict()))\n        self.conditioning_key = conditioning_key\n\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n        xc = torch.cat([x] + c_concat, dim=1)  # TODO:Can we update this latent space?\n        cc = torch.cat(c_crossattn, 1)\n        out = self.diffusion_model(xc, t, context=cc)\n        return out\n"}
{"type": "source_file", "path": "project/utils/exploring_latent_space.py", "content": "import numpy as np\nimport torch\n\nfrom utils.const import PRETRAINED_MODEL_FOLDER\n\n\ndef load_latent_vectors_from_ddim() -> np.ndarray:\n    latent_vectors = torch.load(\n        PRETRAINED_MODEL_FOLDER / \"latent_vector_all_samples.pt\"\n    )\n    return latent_vectors.detach().cpu().numpy()\n"}
{"type": "source_file", "path": "project/utils/transorms.py", "content": "import torch\nfrom monai.data import NibabelReader\nfrom monai.transforms import (\n    EnsureChannelFirstd,\n    Compose,\n    LoadImaged,\n    Orientationd,\n    ScaleIntensityd,\n    SpatialCropd,\n    SpatialPadd,\n    ToTensord,\n)\n\n\ndef get_preprocessing(device: torch.device) -> Compose:\n    return Compose(\n        [\n            LoadImaged(keys=[\"image\"], reader=NibabelReader()),\n            EnsureChannelFirstd(keys=[\"image\"]),\n            Orientationd(keys=[\"image\"], axcodes=\"LAS\"),\n            ScaleIntensityd(keys=[\"image\"], minv=0.0, maxv=1.0),\n            SpatialCropd(\n                keys=[\"image\"],\n                roi_start=[40, 12, 80],\n                roi_end=[200, 236, 240],\n            ),\n            SpatialPadd(\n                keys=[\"image\"],\n                spatial_size=[160, 224, 160],\n            ),\n            ToTensord(keys=[\"image\"], device=device),\n        ]\n    )\n"}
{"type": "source_file", "path": "project/utils/utils.py", "content": "import os\nimport random\nfrom argparse import Namespace\nfrom random import choice\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List, Any\n\nimport mlflow\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom monai.transforms import apply_transform\nfrom tqdm import tqdm\n\nfrom models.ddim import DDIMSampler\nfrom models.aekl_no_attention import OnlyDecoder\nfrom models.ddpm_v2_conditioned import DDPM\nfrom models.BRGM.forward_models import (\n    ForwardDownsample,\n    ForwardFillMask,\n    ForwardAbstract,\n)\nfrom utils.transorms import get_preprocessing\nfrom utils.const import (\n    INPUT_FOLDER,\n    MASK_FOLDER,\n    PRETRAINED_MODEL_VAE_PATH,\n    PRETRAINED_MODEL_DDPM_PATH,\n    PRETRAINED_MODEL_VGG_PATH,\n    PRETRAINED_MODEL_FOLDER,\n    OUTPUT_FOLDER,\n    LATENT_SHAPE,\n)\n\n\ndef transform_img(\n    img_path: Path,\n    device: torch.device,\n) -> Any:\n    data = {\"image\": img_path}\n    data = apply_transform(get_preprocessing(device), data)\n    return data[\"image\"]\n\n\ndef load_target_image(hparams: Namespace, device: torch.device) -> torch.Tensor:\n    if hparams.data_format == \"nii\":\n        img_path = list(INPUT_FOLDER.glob(f\"**/*ur_IXI{hparams.subject_id}*T1.nii.gz\"))[\n            0\n        ]\n        img_tensor = transform_img(img_path, device=device)\n    elif hparams.data_format == \"pth\":\n        img_path = list(INPUT_FOLDER.glob(f\"**/*IXI_T1_{hparams.subject_id}.pth\"))[0]\n        img_tensor = torch.load(img_path, map_location=device)\n    elif hparams.data_format == \"img\":\n        img_path = list(INPUT_FOLDER.glob(f\"**/*OAS1_0{hparams.subject_id}_MR1_*.img\"))[\n            0\n        ]\n        img_tensor = transform_img(img_path, device=device)\n    img_tensor = img_tensor.unsqueeze(0)\n    return img_tensor\n\n\ndef load_vgg_perceptual(\n    hparams: Namespace, target: torch.Tensor, device: torch.device\n) -> Tuple[Any, torch.Tensor]:\n    with open(PRETRAINED_MODEL_VGG_PATH, \"rb\") as f:\n        vgg16 = torch.jit.load(f).eval().to(device)\n\n    target_features = getVggFeatures(hparams, target, vgg16)\n    return vgg16, target_features\n\n\ndef create_corruption_function(\n    hparams: Namespace, device: torch.device\n) -> ForwardAbstract:\n    if hparams.corruption == \"downsample\":\n        forward = ForwardDownsample(factor=hparams.downsample_factor)\n    elif hparams.corruption == \"mask\":\n        mask = np.load(MASK_FOLDER / f\"{hparams.mask_id}.npy\")\n        forward = ForwardFillMask(mask=mask, device=device)\n    else:\n        # No corruption\n        forward = ForwardFillMask(device=device)\n    return forward\n\n\ndef load_latent_vector_stats(device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n    tmp = torch.load(\n        INPUT_FOLDER.parent / \"trained_models\" / \"latent_vector_mean_std.pt\",\n        # INPUT_FOLDER.parent / \"trained_models\" / \"latent_vector_zero_std.pt\",\n        map_location=device,\n    )\n    return tmp[\"latent_vector_mean\"], tmp[\"latent_vector_std\"]\n\n\ndef setup_noise_inputs(\n    device: torch.device, hparams: Namespace\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # gender = choice([0, 1]) if hparams.update_gender else 1  # F=0, M=1\n    # age = random.uniform(44, 82) if hparams.update_age else 55\n    # ventricular = random.random() if hparams.update_ventricular else 0.5\n    # brain_volume = random.random() if hparams.update_brain else 0.5\n    gender = 0.5 if hparams.update_gender else 1  # F=0, M=1\n    age = 63 if hparams.update_age else 55\n    ventricular = 0.5 if hparams.update_ventricular else 0.5\n    brain_volume = 0.5 if hparams.update_brain else 0.5\n\n    age_normalized = (age - 44) / (82 - 44)\n    cond = torch.tensor(\n        [[gender, age_normalized, ventricular, brain_volume]], device=device\n    )  # shape: [1, 4]\n\n    latent_variable = torch.randn(LATENT_SHAPE, device=device)\n    return cond, latent_variable\n\n\ndef load_pre_trained_model(\n    device: torch.device,\n) -> Tuple[torch.nn.Module, torch.nn.Module]:\n    decoder = mlflow.pytorch.load_model(\n        str(PRETRAINED_MODEL_VAE_PATH),\n        map_location=device,\n    )\n    ddpm = mlflow.pytorch.load_model(\n        str(PRETRAINED_MODEL_DDPM_PATH),\n        map_location=device,\n    )\n    decoder.eval()\n    ddpm.eval()\n    decoder = decoder.to(device)\n    ddpm = ddpm.to(device)\n    decoder.requires_grad_(False)\n    ddpm.requires_grad_(False)\n    return ddpm, decoder\n\n\ndef sample_fn(\n    diffusion: torch.nn.Module,\n    vqvae: Any,\n    gender: int,\n    age: float,\n    ventricular: float,\n    brain_volume: float,\n    device: torch.device,\n):\n    print(\"Sampling brain!\")\n    print(f\"Gender: {gender}\")\n    print(f\"Age: {age}\")\n    print(f\"Ventricular volume: {ventricular}\")\n    print(f\"Brain volume: {brain_volume}\")\n\n    age_normalized = (age - 44) / (82 - 44)\n    cond = torch.Tensor([[gender, age_normalized, ventricular, brain_volume]])\n    cond_crossatten = cond.unsqueeze(1)\n    cond_concat = cond.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n    cond_concat = cond_concat.expand(list(cond.shape[0:2]) + list(LATENT_SHAPE[2:]))\n    conditioning = {\n        \"c_concat\": [cond_concat.float().to(device)],\n        \"c_crossattn\": [cond_crossatten.float().to(device)],\n    }\n    latent_variable = torch.randn(LATENT_SHAPE, device=device)\n\n    ddim = DDIMSampler(diffusion)\n    num_timesteps = 50  # 50\n    latent_vectors, _ = ddim.sample(\n        num_timesteps,\n        first_img=latent_variable,\n        conditioning=conditioning,\n        batch_size=1,\n        shape=list(LATENT_SHAPE[1:]),\n        eta=1.0,\n    )\n\n    with torch.no_grad():\n        x_hat = vqvae.reconstruct_ldm_outputs(latent_vectors).cpu()\n\n    return latent_variable.numpy(), conditioning, latent_vectors, x_hat.numpy()\n\n\ndef sample_from_ddpm(\n    ddpm: DDPM,\n    cond: torch.Tensor,\n    img: torch.Tensor,\n    device: torch.device,\n    decoder: OnlyDecoder,\n    num_timesteps: int = 1000,\n) -> torch.Tensor:\n    cond_tmp = cond.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # shape: [1, 4, 1, 1, 1]\n    cond_crossatten = (\n        cond_tmp.squeeze(-1).squeeze(-1).squeeze(-1).unsqueeze(1)\n    )  # shape: [1, 1, 4]\n    cond_concat = torch.tile(cond_tmp, (20, 28, 20))\n    conditioning = {\n        \"c_concat\": [cond_concat],\n        \"c_crossattn\": [cond_crossatten],\n    }\n\n    for i in tqdm(\n        reversed(range(0, num_timesteps)),\n        desc=\"sampling loop time step\",\n        total=num_timesteps,\n    ):\n        img = ddpm.p_sample(\n            img,\n            conditioning,\n            torch.full((1,), i, device=device, dtype=torch.long),\n            clip_denoised=ddpm.clip_denoised,\n        )\n\n    brain_img = decoder.reconstruct_ldm_outputs(img)\n    return brain_img\n\n\ndef generating_latent_vector(\n    diffusion: torch.nn.Module,\n    latent_variable: torch.Tensor,\n    conditioning: Dict[str, List[torch.Tensor]],\n    batch_size: int,\n):\n    ddim = DDIMSampler(diffusion)\n    num_timesteps = 50\n    latent_vectors, _ = ddim.sample(\n        num_timesteps,\n        first_img=latent_variable,\n        conditioning=conditioning,\n        batch_size=batch_size,\n        shape=list(LATENT_SHAPE[1:]),\n        eta=1.0,\n        verbose=False,\n    )\n\n    return latent_vectors\n\n\ndef sampling_from_ddim(\n    ddim: DDIMSampler,\n    latent_variable: torch.Tensor,\n    decoder: OnlyDecoder,\n    cond: torch.Tensor,\n    hparams: Namespace,\n) -> torch.Tensor:\n    cond_tmp = cond.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # shape: [1, 4, 1, 1, 1]\n    cond_crossatten = (\n        cond_tmp.squeeze(-1).squeeze(-1).squeeze(-1).unsqueeze(1)\n    )  # shape: [1, 1, 4]\n    cond_concat = torch.tile(cond_tmp, (20, 28, 20))\n    conditioning = {\n        \"c_concat\": [cond_concat],\n        \"c_crossattn\": [cond_crossatten],\n    }\n\n    latent_vectors, _ = ddim.sample(\n        hparams.ddim_num_timesteps,\n        conditioning=conditioning,\n        batch_size=1,\n        shape=list(LATENT_SHAPE[1:]),\n        first_img=latent_variable,\n        eta=hparams.ddim_eta,\n        verbose=False,\n    )\n    brain_img = decoder.reconstruct_ldm_outputs(latent_vectors)\n    return brain_img\n\n\ndef load_ddpm_latent_vectors(device: torch.device) -> torch.Tensor:\n    ddpm_latent_vectors = torch.load(\n        INPUT_FOLDER.parent / \"trained_models\" / \"latent_vector_ddpm_samples_100000.pt\",\n        map_location=device,\n    )\n    return ddpm_latent_vectors\n\n\ndef inference(\n    vqvae: Any,\n    latent_vectors: torch.Tensor,\n):\n    x_hat = vqvae.reconstruct_ldm_outputs(latent_vectors)\n    return x_hat\n\n\ndef getVggFeatures(hparams: Namespace, img: torch.Tensor, vgg16: Any) -> torch.Tensor:\n    if hparams.corruption == \"downsample\":\n        # For super-resolution task, the downsampled image could be smaller than what is required for\n        # the VGG16 architecture, resize it back to the original size\n        tmp_img = F.interpolate(img, (160, 224, 160), mode=\"trilinear\")\n    else:\n        tmp_img = img\n\n    if hparams.perc_dim == \"coronal\":\n        # (1, 1, 160, 224, 160) -> (1, 160, 224, 160) -> (224, 160, 1, 160) -> (224, 1, 160, 160)\n        tmp_img = torch.transpose(torch.transpose(tmp_img.squeeze(0), 0, 2), 1, 2)\n    elif hparams.perc_dim == \"sagittal\":\n        # (1, 1, 160, 224, 160) -> (1, 160, 224, 160) -> (160, 1, 224, 160) -> (160, 1, 160, 224)\n        tmp_img = torch.rot90(torch.transpose(tmp_img.squeeze(0), 0, 1), 1, [2, 3])\n    elif hparams.perc_dim == \"axial\":\n        # (1, 1, 160, 224, 160) -> (1, 160, 224, 160) -> (160, 160, 224, 1) -> (160, 1, 224, 160) -> (160, 1, 224, 160)\n        tmp_img = torch.rot90(\n            torch.transpose(torch.transpose(tmp_img.squeeze(0), 3, 0), 1, 3),\n            2,\n            [2, 3],\n        )\n    tmp_img = tmp_img.repeat(1, 3, 1, 1)  # BCDWH\n\n    # Features for synth images.\n    features = vgg16(tmp_img, resize_images=False, return_lpips=True)\n    return features\n\n\n@torch.no_grad()\ndef compute_prior_stats(\n    diffusion_path: Path, n_samples: int, batch_size: int, device: torch.device\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    diffusion = mlflow.pytorch.load_model(\n        str(diffusion_path),\n        map_location=device,\n    )\n\n    latent_vector_list: List[torch.Tensor] = []\n    for idx in range(n_samples // batch_size):\n        cond_concat_list, cond_crossatten_list, latent_variable_list = [], [], []\n        for _ in range(batch_size):\n            cond, latent_variable = setup_noise_inputs(device)\n            cond_tmp = (\n                cond.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n            )  # shape: [1, 4, 1, 1, 1]\n            cond_crossatten = (\n                cond_tmp.squeeze(-1).squeeze(-1).squeeze(-1).unsqueeze(1)\n            )  # shape: [1, 1, 4]\n            cond_concat = torch.tile(cond_tmp, (20, 28, 20))\n            cond_concat_list.append(cond_concat)\n            cond_crossatten_list.append(cond_crossatten)\n            latent_variable_list.append(latent_variable)\n\n        cond_concat = torch.cat(cond_concat_list, dim=0)\n        cond_crossatten = torch.cat(cond_crossatten_list, dim=0)\n        latent_variable = torch.cat(latent_variable_list, dim=0)\n        del cond_concat_list, cond_crossatten_list, latent_variable_list\n        conditioning = {\n            \"c_concat\": [cond_concat],\n            \"c_crossattn\": [cond_crossatten],\n        }\n        latent_vector = generating_latent_vector(\n            diffusion=diffusion,\n            latent_variable=latent_variable,\n            conditioning=conditioning,\n            batch_size=batch_size,\n        )\n        latent_vector_list.append(latent_vector.to(torch.device(\"cpu\")))\n        print(f\"Finish {idx} / {n_samples // batch_size}\")\n\n    latent_vector_samples = torch.cat(\n        latent_vector_list, dim=0  # shape: [n_samples, 3, 20, 28, 20]\n    )\n    del latent_vector_list\n    latent_vector_mean = latent_vector_samples.mean(dim=0, keepdim=True)\n    latent_vector_std = latent_vector_samples.std(dim=0, keepdim=True)\n    # Save the latent vectors, mean and std\n    torch.save(latent_vector_samples, OUTPUT_FOLDER / \"latent_vector_samples.pt\")\n    torch.save(\n        {\n            \"latent_vector_mean\": latent_vector_mean,\n            \"latent_vector_std\": latent_vector_std,\n        },\n        OUTPUT_FOLDER / \"latent_vector_mean_std.pt\",\n    )\n    print(\"latent_vector_samples shape\", latent_vector_samples.shape)\n    print(\"latent_vector_mean shape\", latent_vector_mean.shape)\n    print(\"latent_vector_std shape\", latent_vector_std.shape)\n    del diffusion, latent_vector_samples\n    return latent_vector_mean, latent_vector_std\n\n\ndef load_ddpm_model(ddpm_path: Path, device: torch.device) -> torch.nn.Module:\n    diffusion = mlflow.pytorch.load_model(\n        str(ddpm_path),\n        map_location=device,\n    )\n    diffusion.eval()\n    diffusion = diffusion.to(device)\n    diffusion.requires_grad_(False)\n    return diffusion\n\n\ndef load_pre_trained_decoder(\n    vae_path: Path,\n    device: torch.device,\n) -> torch.nn.Module:\n    vqvae = mlflow.pytorch.load_model(\n        str(vae_path),\n        map_location=device,\n    )\n    vqvae.eval()\n    vqvae = vqvae.to(device)\n    vqvae.requires_grad_(False)\n    return vqvae\n\n\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef draw_img(img: np.ndarray, title: str, path: Path) -> None:\n    fig, ax = plt.subplots()\n    ax.imshow(np.rot90(img, 1), cmap=\"gray\")\n    ax.axis(\"off\")\n    fig.savefig(\n        path / f\"{title}.png\",\n        bbox_inches=\"tight\",\n        pad_inches=0,\n        format=\"png\",\n        dpi=300,\n    )\n    plt.close()\n"}
{"type": "source_file", "path": "project/utils/plot.py", "content": "from typing import List\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n\ndef merge_two_images(img1: np.ndarray, img2: np.ndarray, dim: int) -> plt.figure:\n    si, sj, sk = img1.shape\n    if dim == 1:\n        img1_slice = np.rot90(img1[si // 2, ...], -1)\n        img2_slice = np.rot90(img2[si // 2, ...], -1)\n    elif dim == 2:\n        img1_slice = np.rot90(img1[:, sj // 2, :], 1)\n        img2_slice = np.rot90(img2[:, sj // 2, :], 1)\n    elif dim == 3:\n        img1_slice = np.rot90(img1[:, :, sk // 2], 1)\n        img2_slice = np.rot90(img2[:, :, sk // 2], 1)\n    imgs_list = [img1_slice, img2_slice]\n    titles_list = [\"Reconstructed Image\", \"Original Corrupted\"]\n\n    fig = plt.figure(figsize=(8, 6))\n    gs = gridspec.GridSpec(1, 2)\n    for idx in range(2):\n        ax = plt.subplot(gs[idx])\n        ax.imshow(imgs_list[idx], cmap=\"gray\")\n        ax.grid(False)\n        ax.invert_xaxis()\n        ax.invert_yaxis()\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(titles_list[idx])\n\n    plt.tight_layout()\n    return fig\n\n\ndef draw_corrupted_images(\n    img1: np.ndarray, img2: np.ndarray, img3: np.ndarray, img4: np.ndarray, ssim_: float\n) -> np.ndarray:\n    si, sj, sk = img1.shape\n    si_, sj_, sk_ = img3.shape\n    img1_row1 = np.rot90(img1[:, :, sk // 2], -1)\n    img2_row1 = np.rot90(img2[:, :, sk // 2], -1)\n    img3_row1 = np.rot90(img3[:, :, sk_ // 2], -1)\n    img4_row1 = np.rot90(img4[:, :, sk_ // 2], -1)\n    img1_row2 = np.rot90(img1[:, sj // 2, :], -1)\n    img2_row2 = np.rot90(img2[:, sj // 2, :], -1)\n    img3_row2 = np.rot90(img3[:, sj_ // 2, :], -1)\n    img4_row2 = np.rot90(img4[:, sj_ // 2, :], -1)\n    img1_row3 = np.rot90(img1[si // 2, :, :], -1)\n    img2_row3 = np.rot90(img2[si // 2, :, :], -1)\n    img3_row3 = np.rot90(img3[si_ // 2, :, :], -1)\n    img4_row3 = np.rot90(img4[si_ // 2, :, :], -1)\n    imgs_list = [\n        img1_row1,\n        img2_row1,\n        img3_row1,\n        img4_row1,\n        img1_row2,\n        img2_row2,\n        img3_row2,\n        img4_row2,\n        img1_row3,\n        img2_row3,\n        img3_row3,\n        img4_row3,\n    ]\n    titles_list = [\n        \"Reconstructed Image\",\n        \"Original Corrupted\",\n        \"Reconstructed Image (downsampled)\",\n        \"Original Corrupted (downsampled)\",\n    ]\n\n    fig = plt.figure(figsize=(16, 18))\n    nrows, ncols = 3, 4\n    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n    for idx in range(nrows):\n        for jdx in range(ncols):\n            ax = plt.subplot(gs[idx * ncols + jdx])\n            ax.imshow(imgs_list[idx * ncols + jdx], cmap=\"gray\")\n            ax.grid(False)\n            ax.invert_xaxis()\n            ax.invert_yaxis()\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if idx == 0:\n                ax.set_title(titles_list[idx * ncols + jdx])\n\n    plt.tight_layout()\n    fig.suptitle(f\"SSIM: {ssim_:.4f}\", x=0.48, y=0.99, fontsize=12)\n    return fig\n\n\ndef draw_images_for_variational_inference(\n    corrupted: np.ndarray, target: np.ndarray, synth_imgs: np.ndarray, ssim_: float\n) -> np.ndarray:\n    _, _, si, sj, sk = target.shape\n    _, _, si_, sj_, sk_ = corrupted.shape\n    print(f\"synth_imgs.shape: {synth_imgs.shape}\")\n    img1_row1 = np.rot90(corrupted[0, 0, :, :, sk_ // 2], -1)\n    img2_row1 = np.rot90(target[0, 0, :, :, sk // 2], -1)\n    img3_row1 = np.rot90(synth_imgs[0, 0, :, :, sk // 2], -1)\n    img4_row1 = np.rot90(synth_imgs[1, 0, :, :, sk // 2], -1)\n    img5_row1 = np.rot90(synth_imgs[2, 0, :, :, sk // 2], -1)\n    img6_row1 = np.rot90(synth_imgs[3, 0, :, :, sk // 2], -1)\n    img1_row2 = np.rot90(corrupted[0, 0, :, sj_ // 2, :], -1)\n    img2_row2 = np.rot90(target[0, 0, :, sj // 2, :], -1)\n    img3_row2 = np.rot90(synth_imgs[0, 0, :, sj // 2, :], -1)\n    img4_row2 = np.rot90(synth_imgs[1, 0, :, sj // 2, :], -1)\n    img5_row2 = np.rot90(synth_imgs[2, 0, :, sj // 2, :], -1)\n    img6_row2 = np.rot90(synth_imgs[3, 0, :, sj // 2, :], -1)\n    img1_row3 = np.rot90(corrupted[0, 0, si_ // 2, :, :], -1)\n    img2_row3 = np.rot90(target[0, 0, si // 2, :, :], -1)\n    img3_row3 = np.rot90(synth_imgs[0, 0, si // 2, :, :], -1)\n    img4_row3 = np.rot90(synth_imgs[1, 0, si // 2, :, :], -1)\n    img5_row3 = np.rot90(synth_imgs[2, 0, si // 2, :, :], -1)\n    img6_row3 = np.rot90(synth_imgs[3, 0, si // 2, :, :], -1)\n    imgs_list = [\n        img1_row1,\n        img2_row1,\n        img3_row1,\n        img4_row1,\n        img5_row1,\n        img6_row1,\n        img1_row2,\n        img2_row2,\n        img3_row2,\n        img4_row2,\n        img5_row2,\n        img6_row2,\n        img1_row3,\n        img2_row3,\n        img3_row3,\n        img4_row3,\n        img5_row3,\n        img6_row3,\n    ]\n    titles_list = [\n        \"Corrupted Image\",\n        \"Target Image\",\n        \"Est. Mean\",\n        \"Sample 1\",\n        \"Sample 2\",\n        \"Sample 3\",\n    ]\n\n    fig = plt.figure(figsize=(24, 18))\n    nrows, ncols = 3, 6\n    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n    for idx in range(nrows):\n        for jdx in range(ncols):\n            ax = plt.subplot(gs[idx * ncols + jdx])\n            ax.imshow(imgs_list[idx * ncols + jdx], cmap=\"gray\")\n            ax.grid(False)\n            ax.invert_xaxis()\n            ax.invert_yaxis()\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if idx == 0:\n                ax.set_title(titles_list[idx * ncols + jdx])\n\n    plt.tight_layout()\n    fig.suptitle(f\"SSIM: {ssim_:.4f}\", x=0.48, y=0.99, fontsize=12)\n    return fig\n\n\ndef draw_images(\n    img1: np.ndarray,\n    img2: np.ndarray,\n    ssim_: float,\n    titles_list: List[str] = [\n        \"Reconstructed Image\",\n        \"Original Corrupted\",\n    ],\n) -> np.ndarray:\n    si, sj, sk = img1.shape\n    img1_row1 = np.rot90(img1[:, :, sk // 2], -1)\n    img2_row1 = np.rot90(img2[:, :, sk // 2], -1)\n    img1_row2 = np.rot90(img1[:, sj // 2, :], -1)\n    img2_row2 = np.rot90(img2[:, sj // 2, :], -1)\n    img1_row3 = np.rot90(img1[si // 2, :, :], -1)\n    img2_row3 = np.rot90(img2[si // 2, :, :], -1)\n    imgs_list = [\n        img1_row1,\n        img2_row1,\n        img1_row2,\n        img2_row2,\n        img1_row3,\n        img2_row3,\n    ]\n\n    fig = plt.figure(figsize=(8, 18))\n    nrows, ncols = 3, 2\n    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n    for idx in range(nrows):\n        for jdx in range(ncols):\n            ax = plt.subplot(gs[idx * ncols + jdx])\n            ax.imshow(imgs_list[idx * ncols + jdx], cmap=\"gray\")\n            ax.grid(False)\n            ax.invert_xaxis()\n            ax.invert_yaxis()\n            ax.set_xticks([])\n            ax.set_yticks([])\n            if idx == 0:\n                ax.set_title(titles_list[idx * ncols + jdx])\n\n    plt.tight_layout()\n    fig.suptitle(f\"SSIM: {ssim_:.4f}\", x=0.48, y=0.99, fontsize=12)\n    return fig\n\n\ndef draw_img(img: np.ndarray, title: str, step: str, output_folder: Path) -> None:\n    fig, ax = plt.subplots()\n    si, sj, sk = img.shape\n    img_slice = np.rot90(img[:, sj // 2, :], -1)\n    ax.imshow(img_slice, cmap=\"gray\")\n    ax.axis(\"off\")\n    fig.savefig(\n        output_folder / f\"{step}_{title}.png\",\n        bbox_inches=\"tight\",\n        pad_inches=0,\n        format=\"png\",\n        dpi=300,\n    )\n    # close\n    plt.close(fig)\n\n\ndef draw_line_plot(\n    img_path: Path,\n) -> None:\n    img = cv2.imread(str(img_path))\n    ix, iy, _ = img.shape\n    img_with_line = cv2.line(img, (iy // 2, 0), (iy // 2, ix), (0, 0, 255), 2)\n    img_with_line = cv2.line(img, (0, ix // 2), (iy, ix // 2), (0, 0, 255), 2)\n    cv2.imwrite(str(img_path), img_with_line)\n\n\ndef draw_img_in_three_dim(img: np.ndarray, title: str, output_folder: Path) -> None:\n    fig, ax = plt.subplots()\n    si, sj, sk = img.shape\n    dim_slices = [\"axial\", \"sagittal\", \"coronal\"]\n\n    img_slice = np.rot90(img[:, :, sk // 2], 1)\n    ax.imshow(img_slice, cmap=\"gray\")\n    ax.axis(\"off\")\n    fig.savefig(\n        output_folder / f\"{title}_{dim_slices[0]}.png\",\n        bbox_inches=\"tight\",\n        pad_inches=0,\n        format=\"png\",\n        dpi=300,\n    )\n    # draw_line_plot(output_folder / f\"{title}_{dim_slices[0]}.png\")\n\n    img_slice = np.rot90(img[:, sj // 2, :], 1)\n    ax.imshow(img_slice, cmap=\"gray\")\n    ax.axis(\"off\")\n    fig.savefig(\n        output_folder / f\"{title}_{dim_slices[1]}.png\",\n        bbox_inches=\"tight\",\n        pad_inches=0,\n        format=\"png\",\n        dpi=300,\n    )\n    # draw_line_plot(output_folder / f\"{title}_{dim_slices[1]}.png\")\n\n    img_slice = np.rot90(img[si // 2, :, :], 1)\n    ax.imshow(img_slice, cmap=\"gray\")\n    ax.axis(\"off\")\n    fig.savefig(\n        output_folder / f\"{title}_{dim_slices[2]}.png\",\n        bbox_inches=\"tight\",\n        pad_inches=0,\n        format=\"png\",\n        dpi=300,\n    )\n    # draw_line_plot(output_folder / f\"{title}_{dim_slices[2]}.png\")\n    plt.close(fig)\n\n\ndef draw_three_imgs_one_row(\n    img1: np.ndarray,\n    img2: np.ndarray,\n    img3: np.ndarray,\n    titles_list: List[str] = [\n        \"GT\",\n        \"Input_nearest\",\n        \"SynthSR\",\n    ],\n) -> np.ndarray:\n    si, sj, sk = img1.shape\n    img1_row1 = np.rot90(img1[:, sj // 2, :], -1)\n    img2_row2 = np.rot90(img2[:, sj // 2, :], -1)\n    img3_row3 = np.rot90(img3[:, sj // 2, :], -1)\n    imgs_list = [\n        img1_row1,\n        img2_row2,\n        img3_row3,\n    ]\n\n    fig = plt.figure(figsize=(18, 8))\n    nrows, ncols = 1, 3\n    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n    for idx in range(nrows):\n        for jdx in range(ncols):\n            ax = plt.subplot(gs[idx * ncols + jdx])\n            ax.imshow(imgs_list[idx * ncols + jdx], cmap=\"gray\")\n            ax.grid(False)\n            ax.invert_xaxis()\n            ax.invert_yaxis()\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(titles_list[idx * ncols + jdx])\n\n    plt.tight_layout()\n    return fig\n"}
