{"repo_info": {"repo_name": "youtube-transcribe", "repo_owner": "zleung9", "repo_url": "https://github.com/zleung9/youtube-transcribe"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test.py", "content": "import yt_dlp\nfrom datetime import datetime\n\ndef get_latest_videos_until(channel_handle, until_date=None, max_results=1):\n    \"\"\"\n    Fetches latest videos from a YouTube channel (by handle) until a specified date.\n\n    Parameters:\n        - channel_handle: YouTube channel handle (e.g., \"@ChannelHandle\")\n        - until_date: Cutoff date in \"YYYYMMDD\" format (e.g., \"20240101\" for Jan 1, 2024)\n        - max_results: Maximum number of videos to check (default: 50)\n\n    Returns:\n        - List of videos uploaded until the given date.\n    \"\"\"\n    if until_date is None:\n        until_date = datetime.now().strftime(\"%Y%m%d\")\n    else:\n        until_date = datetime.strptime(until_date, \"%Y%m%d\")\n        max_results = 50 \n    \n    url = f\"https://www.youtube.com/{channel_handle}\"\n    ydl_opts = {\n        'quiet': True,\n        'extract_flat': True,  # Extract metadata without downloading\n        'playlistend': max_results  # Fetch up to `max_results` videos\n    }\n\n    filtered_videos = []\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(url, download=False)\n        print(info.keys())\n        for video in info['entries']:\n            print(video['title'], video.keys())\n        # if upload_date and upload_date <= until_date:\n            filtered_videos.append({\n                'video_id': video['id'],\n                'title': video['title'],\n                'upload_date': video.get('upload_date'),\n            })\n\n        return filtered_videos\n\n# Example usage\nchannel_handle = \"@bohaixiaoli\"  # Replace with actual handle\nuntil_date = \"20250101\"  # Retrieve videos uploaded until January 1, 2024\nprint(datetime.strptime(until_date, \"%Y%m%d\"))\nlatest_videos = get_latest_videos_until(channel_handle, until_date=None)\n\nif not latest_videos:\n    print(\"No videos found before the specified date.\")\n\nfor idx, video in enumerate(latest_videos[:3], start=1):\n    print(f\"{video['video_id']}. {video['title']} ({video['upload_date']})\")"}
{"type": "test_file", "path": "tests/test_reporter.py", "content": "import unittest\nfrom unittest.mock import patch, MagicMock, AsyncMock\nfrom datetime import datetime\nimport os\nfrom yourtube import YoutubeVideo, Reporter, Video\nfrom uuid import uuid4\n\n\nclass TestReporter(unittest.TestCase):\n    def setUp(self):\n        self.config = {\n            'email': {\n                'smtp_server': 'smtp.example.com',\n                'smtp_port': 587,\n                'username': 'test@example.com',\n                'password': 'password',\n                'recipients': ['recipient@example.com']\n            },\n            'monitored_platforms': ['youtube'],\n            'youtube': {\n                'api_key': 'fake_api_key',\n                'channels': [\n                    {'channel_id': 'UC123', 'check_interval': 3600},\n                    {'channel_id': 'UC456', 'check_interval': 3600}\n                ]\n            }\n        }\n        self.reporter = Reporter(self.config)\n        \n        # Create a sample video for tests\n        current_time = datetime.now()\n        self.sample_video = Video(\n            id=uuid4(),\n            video_id='test123',\n            title='Test Video',\n            channel_id='UC123',\n            channel='Test Channel',\n            upload_date=current_time,\n            process_date=current_time,\n            language='en',\n            transcript=True,\n            fulltext=True,\n            summary=True\n        )\n        self.sample_video._metadata = {\n            'video_id': 'test123',\n            'title': 'Test Video',\n            'video_ext': 'mp4',\n            'channel': 'Test Channel',\n            'channel_id': 'UC123',\n            'language': 'en',\n            'upload_date': current_time.strftime('%Y%m%d'),\n            'has_video': 1,\n            'has_vtt': 1,\n            'has_srt': 1\n        }\n        self.sample_video._default_path = self.reporter.download_dir\n\n    @patch('yourtube.reporter.SqliteDB')\n    @patch('yourtube.reporter.Transcriber')\n    def test_process_video(self, mock_transcriber_class, mock_db_class):\n        # Create mock instances\n        mock_transcriber = MagicMock()\n        mock_transcriber_class.return_value = mock_transcriber\n        mock_db = MagicMock()\n        mock_db_class.return_value = mock_db\n        self.reporter.db = mock_db  # Replace the actual DB with mock\n        \n        # Create a test video using MagicMock\n        test_video = MagicMock(spec=YoutubeVideo)\n        current_time = datetime.now()\n        \n        # Match SQLAlchemy columns from Video class\n        test_video.video_id = 'test123'\n        test_video.title = 'Test Video'\n        test_video.channel_id = 'UC123'\n        test_video.channel = 'Test Channel'\n        test_video.upload_date = current_time\n        test_video.process_date = current_time\n        test_video.language = 'en'\n        test_video.transcript = True\n        test_video.fulltext = True\n        test_video.summary = True\n        \n        # Match Video class properties and instance variables\n        test_video.url = 'https://youtube.com/watch?v=test123'\n        test_video._default_path = '/tmp'\n        test_video._metadata = {\n            'video_id': 'test123',\n            'title': 'Test Video',\n            'video_ext': 'mp4',\n            'channel': 'Test Channel',\n            'channel_id': 'UC123',\n            'language': 'en',\n            'upload_date': current_time.strftime('%Y%m%d'),\n            'has_video': 1,\n            'has_vtt': 1,\n            'has_srt': 1\n        }\n        \n        # Mock the file existence checks\n        def mock_exists(path):\n            return True if path.endswith(('.mp4', '.vtt', '.srt')) else False\n        \n        # Set up mock returns\n        mock_transcriber.transcribe.return_value = \"Test transcription\"\n        mock_transcriber.extract_fulltext.return_value = \"Test processing\"\n        mock_transcriber.summarize.return_value = \"Test summary\"\n        \n        # Call the method\n        self.reporter._process_video(test_video)\n        \n        # Verify all expected methods were called\n        test_video.get.assert_called_once_with(test_video.video_id, download_video=True)\n        mock_transcriber.transcribe.assert_called_once_with(test_video)\n        mock_transcriber.extract_fulltext.assert_called_once_with(test_video)\n        mock_transcriber.summarize.assert_called_once_with(test_video)\n        mock_db.add_video.assert_called_once_with(test_video)\n\n    @patch('yourtube.Reporter._process_video')\n    @patch('yourtube.Reporter._send_update_report', new_callable=AsyncMock)\n    async def test_process_updates(self, mock_send_report, mock_process_video):\n        # Create mock monitor\n        mock_monitor = MagicMock()\n        \n        # Create mock videos\n        test_video_1 = MagicMock(spec=YoutubeVideo)\n        test_video_1.video_id = 'test123'\n        test_video_1.title = 'Test Video 1'\n        test_video_1.channel_id = 'UC123'\n        test_video_1.channel = 'Test Channel 1'\n        test_video_1.upload_date = datetime.now()\n\n        test_video_2 = MagicMock(spec=YoutubeVideo)\n        test_video_2.video_id = 'test456'\n        test_video_2.title = 'Test Video 2'\n        test_video_2.channel_id = 'UC456'\n        test_video_2.channel = 'Test Channel 2'\n        test_video_2.upload_date = datetime.now()\n\n        mock_monitor.process_new_videos.return_value = [test_video_1, test_video_2]\n        \n        # Set up mock monitors in the reporter\n        self.reporter.monitors = {'youtube': mock_monitor}\n        \n        # Call the method\n        self.reporter.process_updates()\n        \n        # Verify the monitor was called\n        mock_monitor.process_new_videos.assert_called_once()\n        \n        # Verify _process_video was called for each video\n        self.assertEqual(mock_process_video.call_count, 2)\n        \n        # Verify send_update_report was called with the list of videos\n        mock_send_report.assert_called_once()\n        \n        # Verify the videos passed to send_update_report\n        args, _ = mock_send_report.call_args\n        self.assertEqual(len(args[0]), 2)  # Should have 2 videos\n        self.assertIsInstance(args[0][0], MagicMock)\n        self.assertIsInstance(args[0][1], MagicMock)\n\n    def test_process_updates_no_new_videos(self):\n        # Create mock monitor that returns no videos\n        mock_monitor = MagicMock()\n        mock_monitor.process_new_videos.return_value = []\n        \n        # Set up mock monitors in the reporter\n        self.reporter.monitors = {'youtube': mock_monitor}\n        \n        # Call the method\n        self.reporter.process_updates()\n        \n        # Verify the monitor was called\n        mock_monitor.process_new_videos.assert_called_once()\n\n    def test_generate_video_summary(self):\n        \"\"\"Test the _generate_video_summary method\"\"\"\n        # Create a mock summary file\n        mock_content = \"This is the complete summary content\"\n        \n        with patch('builtins.open', unittest.mock.mock_open(read_data=mock_content)):\n            summary = self.reporter._generate_video_summary(self.sample_video)\n            \n            # Verify the summary contains all required elements\n            self.assertIn(self.sample_video.title, summary)\n            self.assertIn(f\"https://youtube.com/watch?v={self.sample_video.video_id}\", summary)\n            self.assertIn(self.sample_video.channel, summary)\n            self.assertIn(self.sample_video.upload_date.strftime('%Y-%m-%d %H:%M:%S'), summary)\n            self.assertIn(mock_content, summary)\n            \n            # Verify markdown formatting\n            self.assertIn('##', summary)  # Title heading\n            self.assertIn('**Channel:**', summary)  # Bold text\n            self.assertIn('[', summary)  # Link opening bracket\n            self.assertIn(']', summary)  # Link closing bracket\n            self.assertIn('(', summary)  # URL opening parenthesis\n            self.assertIn(')', summary)  # URL closing parenthesis\n    \n    def test_generate_video_summary_no_file(self):\n        \"\"\"Test generating video summary when file doesn't exist\"\"\"\n        with patch('builtins.open', side_effect=FileNotFoundError):\n            summary = self.reporter._generate_video_summary(self.sample_video)\n            self.assertIn('Summary not available', summary)\n            self.assertIn(self.sample_video.title, summary)\n            self.assertIn(self.sample_video.channel, summary)\n\n    def test_generate_report_content(self):\n        \"\"\"Test the _generate_report_content method\"\"\"\n        # Create multiple test videos\n        current_time = datetime.now()\n        test_video_2 = Video(\n            id=uuid4(),\n            video_id='test456',\n            title='Another Test Video',\n            channel_id='UC456',\n            channel='Another Channel',\n            upload_date=current_time,\n            process_date=current_time,\n            language='en',\n            transcript=True,\n            fulltext=True,\n            summary=True\n        )\n        test_video_2._metadata = {\n            'video_id': 'test456',\n            'title': 'Another Test Video',\n            'video_ext': 'mp4',\n            'channel': 'Another Channel',\n            'channel_id': 'UC456',\n            'language': 'en',\n            'upload_date': current_time.strftime('%Y%m%d'),\n            'has_video': 1,\n            'has_vtt': 1,\n            'has_srt': 1\n        }\n        test_video_2._default_path = self.reporter.download_dir\n        \n        videos = [self.sample_video, test_video_2]\n        \n        # Mock the file reading for both videos\n        mock_content_1 = \"First video summary content\"\n        mock_content_2 = \"Second video summary content\"\n        \n        def mock_open_file(file_path, *args, **kwargs):\n            if self.sample_video.video_id in file_path:\n                return unittest.mock.mock_open(read_data=mock_content_1)()\n            else:\n                return unittest.mock.mock_open(read_data=mock_content_2)()\n        \n        with patch('builtins.open', mock_open_file):\n            # Generate report content\n            report_content = self.reporter._generate_report_content(videos)\n            \n            # Verify report contains summaries for all videos\n            for video in videos:\n                self.assertIn(video.title, report_content)\n                self.assertIn(video.channel, report_content)\n                self.assertIn(f\"https://youtube.com/watch?v={video.video_id}\", report_content)\n            \n            # Verify summaries are included\n            self.assertIn(mock_content_1, report_content)\n            self.assertIn(mock_content_2, report_content)\n        \n        # Test empty video list\n        empty_report = self.reporter._generate_report_content([])\n        self.assertIn('No new videos available today', empty_report)\n        self.assertIn('Daily Video Update Report', empty_report)\n\n    @patch('aiosmtplib.SMTP')\n    async def test_send_update_report(self, mock_smtp):\n        \"\"\"Test the _send_update_report method\"\"\"\n        # Set up mock SMTP instance\n        mock_smtp_instance = AsyncMock()\n        mock_smtp.return_value = mock_smtp_instance\n        mock_smtp_instance.__aenter__.return_value = mock_smtp_instance\n\n        # Create test videos list\n        videos = [self.sample_video]\n        \n        # Mock the report content generation\n        with patch.object(self.reporter, '_generate_report_content') as mock_generate:\n            mock_generate.return_value = \"Test Report Content\"\n            \n            # Call the method\n            await self.reporter._send_update_report(videos)\n            \n            # Verify SMTP connection was established with correct credentials\n            mock_smtp_instance.connect.assert_called_once_with(\n                hostname=self.config['email']['smtp_server'],\n                port=self.config['email']['smtp_port']\n            )\n            mock_smtp_instance.login.assert_called_once_with(\n                self.config['email']['username'],\n                self.config['email']['password']\n            )\n            \n            # Verify email was sent with correct parameters\n            mock_smtp_instance.send_message.assert_called_once()\n            \n            # Get the email message that was sent\n            sent_message = mock_smtp_instance.send_message.call_args[0][0]\n            \n            # Verify email contents\n            self.assertEqual(sent_message['From'], self.config['email']['username'])\n            self.assertEqual(sent_message['To'], ', '.join(self.config['email']['recipients']))\n            self.assertIn('Daily Video Update Report', sent_message['Subject'])\n            self.assertEqual(sent_message.get_content(), \"Test Report Content\")\n\nif __name__ == '__main__':\n    unittest.main() "}
{"type": "test_file", "path": "tests/test_transcriber.py", "content": "import pytest\nfrom youtube.transcriber import srt_to_continuous_text\n\ndef test_srt_to_continuous_text(tmp_path):\n    # Create a temporary SRT file\n    srt_content = \"\"\"1\n    00:00:00,000 --> 00:00:08,199\n    我们不是没有需求,我们动过念头,我们在这需要一个什么东西。\n\n    2\n    00:00:08,199 --> 00:00:13,040\n    就只能找别人去问问,也有一个沟通成本。\n\n    3\n    00:00:13,040 --> 00:00:18,440\n    坦特最近大家聊得比较多的,有一个叫做AI福祝编程。\n    \"\"\"\n\n    srt_path = tmp_path / \"test.srt\"\n    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(srt_content)\n\n    # Expected continuous text\n    expected_text = (\n        \"我们不是没有需求,我们动过念头,我们在这需要一个什么东西。\\n\"\n        \"就只能找别人去问问,也有一个沟通成本。\\n\"\n        \"坦特最近大家聊得比较多的,有一个叫做AI福祝编程。\"\n    )\n\n    # Call the function\n    result = srt_to_continuous_text(srt_path)\n\n    # Assert the result matches the expected text\n    assert result == expected_text"}
{"type": "test_file", "path": "tests/test_youtube_monitor.py", "content": "import pytest\nfrom unittest.mock import patch, MagicMock\nfrom yourtube import Video, YoutubeMonitor\nfrom datetime import datetime\n\n@pytest.fixture\ndef youtube_monitor():\n    return YoutubeMonitor()\n\n@pytest.fixture\ndef mock_ydl():\n    \"\"\"Fixture to create a mock YoutubeDL instance\"\"\"\n    mock = MagicMock()\n    mock.__enter__.return_value = mock\n    mock.__exit__.return_value = None\n    mock.extract_info.return_value = {\n        'channel_url': 'https://www.youtube.com/channel/UC9bYDXoFxWC2DQatWI366UA',\n        'channel': 'Bohaixiaoli',\n        'uploader_id': 'UC9bYDXoFxWC2DQatWI366UA'\n    }\n    return mock\n\n\ndef test_get_latest_videos(youtube_monitor):\n    \"\"\"Test getting latest videos from a channel\"\"\"\n    handle = \"@bohaixiaoli\"\n    expected_video_ids = [\"xpXFECH5Z3M\", \"A6LKo6oKbS4\", \"WQV94q2iyFc\"]\n    \n    videos = youtube_monitor.get_latest_videos(handle, until_date=\"20250101\")\n    assert len(videos) == 3\n    assert all(video.video_id in expected_video_ids for video in videos)\n"}
{"type": "source_file", "path": "api/run.py", "content": "import sys\nimport os\nimport logging\nimport json\nimport glob\nfrom logging.handlers import RotatingFileHandler\nfrom flask import Flask, render_template, redirect, url_for, flash, request, jsonify, after_this_request\nfrom flask_cors import CORS \nfrom yourtube import Database, Video, Transcriber\nfrom yourtube.utils import get_download_dir, get_db_path, get_config_path, load_config, extract_youtube_id\nfrom yourtube.monitor import YoutubeMonitor\nfrom yourtube.main import process_video_pipeline\nfrom yourtube.async_worker import video_queue\n# Add the project root to the Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n# global variables setup: dadtabase, monitor, transcriber, config, video_queue\nDOWNLOAD_DIR = get_download_dir()\nDB_PATH = get_db_path()\nprint(f\"Download directory: {DOWNLOAD_DIR}\")\nprint(f\"Database path: {DB_PATH}\")\n\nconfig =load_config() #check if config.json exists, if not create it from template\ndb = Database(db_path=DB_PATH)\nmonitor = YoutubeMonitor(config=config)\ntranscriber = Transcriber(config=config)\nvideo_queue.start_worker(process_video_pipeline) # Start the video processing worker\n\n# Flask app setup\napp = Flask(__name__)\n# Configure logging\nif not app.debug:\n    # Create logs directory if it doesn't exist\n    if not os.path.exists('logs'):\n        os.makedirs('logs')\n    \n    # Set up file handler\n    file_handler = RotatingFileHandler('logs/api.log', maxBytes=10240, backupCount=10)\n    file_handler.setFormatter(logging.Formatter(\n        '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'\n    ))\n    file_handler.setLevel(logging.INFO)\n    app.logger.addHandler(file_handler)\n    \n    # Set up console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    app.logger.addHandler(console_handler)\n    \n    app.logger.setLevel(logging.INFO)\n    app.logger.info('API startup')\n\nCORS(app, resources={\n    r\"/*\": {\n        \"origins\": [\"*\"],  # Allow all origins for testing\n        \"methods\": [\"GET\", \"POST\", \"DELETE\", \"OPTIONS\"],\n        \"allow_headers\": [\"Content-Type\"]\n    }\n})\napp.secret_key = os.urandom(24)\n\n# Completely disable Werkzeug logging\nlog = logging.getLogger('werkzeug')\nlog.setLevel(logging.ERROR)  # Only show ERROR and above, suppressing INFO\n\ndef get_file_path(video_id, file_type, language=None):\n    \"\"\"Get file path based on video ID and type.\"\"\"\n    downloads_path = DOWNLOAD_DIR\n    \n    print(f\"Getting path for video: {video_id}, type: {file_type}, language: {language}\")  # Debug print\n    \n    if file_type == 'video':\n        path = os.path.join(downloads_path, f\"{video_id}.mp4\")\n    elif file_type == 'transcript':\n        path = os.path.join(downloads_path, f\"{video_id}.{language}.srt\")\n    elif file_type == 'summary':\n        path = os.path.join(downloads_path, f\"{video_id}.{language}.md\")\n    elif file_type == 'json':\n        path = os.path.join(downloads_path, f\"{video_id}.info.json\")\n    else:\n        return None\n        \n    print(f\"Constructed path: {path}\")  # Debug print\n    print(f\"File exists: {os.path.exists(path)}\")  # Debug print\n    return path\n\ndef scan_downloads_folder(downloads_path):\n    \n    global db\n    global monitor\n\n    \"\"\"Scan the downloads folder for video files and update the database.\"\"\"\n    stats = {\n        'new_videos': 0,\n        'updated_videos': 0,\n        'errors': []\n    }\n    \n    # Find all .info.json files in the downloads folder\n    json_files = glob.glob(os.path.join(downloads_path, \"*.info.json\"))\n    \n    for json_file in json_files:\n        try:\n            # Extract video_id from filename\n            video_id = os.path.basename(json_file).replace('.info.json', '')\n            \n            # Check if video exists in database\n            existing_video = db.get_video(video_id=video_id)\n            \n            if existing_video:\n                # Update existing video\n                video = monitor.download(video_id)\n                if video:\n                    db.update_video(video)\n                    stats['updated_videos'] += 1\n            else:\n                # Add new video\n                video = monitor.download(video_id)\n                if video:\n                    db.add_video(video)\n                    stats['new_videos'] += 1\n                    \n        except Exception as e:\n            error_msg = f\"Error processing {json_file}: {str(e)}\"\n            stats['errors'].append(error_msg)\n            print(error_msg)\n    \n    return stats\n\n@app.route('/')\ndef index():\n    sort_by = request.args.get('sort', 'process_date')  # Default to processed_date\n    \n    if sort_by == 'upload_date':\n        videos = db.session.query(Video).order_by(Video.upload_date.desc()).all()\n    else:  # processed_date\n        videos = db.session.query(Video).order_by(Video.process_date.desc()).all()\n    \n    if request.headers.get('HX-Request'):  # If it's an AJAX request\n        return render_template('video_list.html', videos=videos)\n    return render_template('index.html', videos=videos)\n\n\n#handle AJAX sorting requests\n@app.route('/videos')\ndef get_videos():\n    sort_by = request.args.get('sort', 'process_date')\n    \n    if sort_by == 'upload_date':\n        videos = db.session.query(Video).order_by(Video.upload_date.desc()).all()\n    else:\n        videos = db.session.query(Video).order_by(Video.process_date.desc()).all()\n        \n    return render_template('index.html', videos=videos)\n\n\n@app.route('/refresh-library')\ndef refresh_library():\n    downloads_path = DOWNLOAD_DIR\n    \n    try:\n        stats = scan_downloads_folder(downloads_path)\n        \n        # Create success message\n        success_msg = f\"Library refreshed! Found {stats['new_videos']} new videos, updated {stats['updated_videos']} existing videos.\"\n        \n        # Add errors if any\n        if stats.get('errors'):\n            success_msg += f\"\\nWarnings: {len(stats['errors'])} errors occurred.\"\n            for error in stats['errors']:\n                flash(error, \"warning\")\n                \n        flash(success_msg, \"success\")\n        \n    except Exception as e:\n        flash(f\"Error refreshing library: {str(e)}\", \"error\")\n    \n    return redirect(url_for('index'))\n\n\n@app.route('/video/<video_id>', methods=['GET'])\ndef video_detail(video_id):\n    \"\"\"Get details for a specific video\"\"\"\n    video = db.get_video(video_id=video_id)\n    if not video:\n        return jsonify({'error': 'Video not found'}), 404\n    \n    # Convert to dictionary for JSON response    \n    return jsonify(video.to_dict())\n\n\n@app.route('/transcript/<video_id>')\ndef view_transcript(video_id):\n    try:\n        video = db.get_video(video_id=video_id)\n        \n        if not video:\n            logging.error(f\"Video not found in database: {video_id}\")\n            return jsonify({\"error\": \"Video not found in database\"}), 404\n        \n        if not video.language:\n            logging.error(f\"Language not set for video: {video_id}\")\n            return jsonify({\"error\": \"Video language not set\"}), 400\n            \n        transcript_path = get_file_path(video_id, 'transcript', video.language)\n        logging.info(f\"Checking transcript at path: {transcript_path}\")\n        \n        if not transcript_path:\n            logging.error(f\"Invalid transcript path for video: {video_id}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n            \n        if not os.path.exists(transcript_path):\n            logging.error(f\"Transcript file not found: {transcript_path}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n        \n        try:\n            with open(transcript_path, 'r', encoding='utf-8') as f:\n                transcript_text = f.read()\n            if not transcript_text.strip():\n                logging.error(f\"Empty transcript file: {transcript_path}\")\n                return jsonify({\"content\": \"\"})  # Return empty content\n            return jsonify({\"content\": transcript_text})\n        except Exception as e:\n            logging.error(f\"Error reading transcript: {str(e)}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n    except Exception as e:\n        logging.error(f\"Unexpected error in view_transcript: {str(e)}\")\n        return jsonify({\"content\": \"\"})  # Return empty content instead of error\n\n\n@app.route('/summary/<video_id>')\ndef view_summary(video_id):\n    try:\n        video = db.get_video(video_id=video_id)\n        \n        if not video:\n            logging.error(f\"Video not found in database: {video_id}\")\n            return jsonify({\"error\": \"Video not found in database\"}), 404\n            \n        if not video.language:\n            logging.error(f\"Language not set for video: {video_id}\")\n            return jsonify({\"error\": \"Video language not set\"}), 400\n            \n        summary_path = get_file_path(video_id, 'summary', video.language)\n        logging.info(f\"Checking summary at path: {summary_path}\")\n        \n        if not summary_path:\n            logging.error(f\"Invalid summary path for video: {video_id}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n            \n        if not os.path.exists(summary_path):\n            logging.error(f\"Summary file not found: {summary_path}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n        \n        try:\n            with open(summary_path, 'r', encoding='utf-8') as f:\n                summary_text = f.read()\n            if not summary_text.strip():\n                logging.error(f\"Empty summary file: {summary_path}\")\n                return jsonify({\"content\": \"\"})  # Return empty content\n            return jsonify({\"content\": summary_text})\n        except Exception as e:\n            logging.error(f\"Error reading summary: {str(e)}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n    except Exception as e:\n        logging.error(f\"Unexpected error in view_summary: {str(e)}\")\n        return jsonify({\"content\": \"\"})  # Return empty content instead of error\n\n\n@app.route('/save-notes/<video_id>', methods=['POST'])\ndef save_notes(video_id):\n    try:\n        notes = request.json.get('notes')\n        # For now, just print the notes to verify the endpoint is working\n        print(f\"Saving notes for video {video_id}: {notes}\")\n        return jsonify({'status': 'success', 'message': 'Notes saved successfully'})\n    except Exception as e:\n        return jsonify({'status': 'error', 'message': str(e)}), 500\n\n\n@app.route('/test-paths/<video_id>')\ndef test_paths(video_id):\n    \"\"\"Debug endpoint to check file paths\"\"\"\n    try:\n        video = db.get_video(video_id=video_id)\n        if not video:\n            return jsonify({\"error\": \"Video not found\"}), 404\n            \n        summary_path = get_file_path(video_id, 'summary', video.language)\n        return jsonify({\n            \"video_id\": video_id,\n            \"language\": video.language,\n            \"summary_path\": summary_path,\n            \"file_exists\": os.path.exists(summary_path),\n            \"downloads_path\": DOWNLOAD_DIR\n        })\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n\n@app.route('/video-content/<video_id>')\ndef video_content(video_id):\n    try:\n        video = db.get_video(video_id=video_id)\n        if not video:\n            return jsonify({\n                'transcript': {'content': \"\"},\n                'summary': {'content': \"\"}\n            }), 404\n\n        # Get transcript\n        transcript_text = \"\"\n        transcript_path = get_file_path(video_id, 'transcript', video.language)\n        if os.path.exists(transcript_path):\n            try:\n                with open(transcript_path, 'r', encoding='utf-8') as f:\n                    transcript_text = f.read()\n            except Exception as e:\n                logging.error(f\"Error reading transcript: {str(e)}\")\n                # Keep transcript_text as empty string\n\n        # Get summary\n        summary_text = \"\"\n        summary_path = get_file_path(video_id, 'summary', video.language)\n        if os.path.exists(summary_path):\n            try:\n                with open(summary_path, 'r', encoding='utf-8') as f:\n                    summary_text = f.read()\n            except Exception as e:\n                logging.error(f\"Error reading summary: {str(e)}\")\n                # Keep summary_text as empty string\n\n        return jsonify({\n            'transcript': {\n                'content': transcript_text,\n                'error': None  # No error message even if empty\n            },\n            'summary': {\n                'content': summary_text,\n                'error': None  # No error message even if empty\n            }\n        })\n    except Exception as e:\n        logging.error(f\"Unexpected error in video_content: {str(e)}\")\n        return jsonify({\n            'transcript': {'content': \"\"},\n            'summary': {'content': \"\"}\n        })\n\n\n@app.route('/process-video', methods=['POST'])\ndef process_video(force=True, transcribe=True, process=True, summarize=True):\n    \n    global config\n    global transcriber\n    global monitor\n    global db\n    global video_queue\n        \n    url = request.json.get('url')\n    if not url:\n        return jsonify({'error': 'No URL provided'}), 400\n    \n    try:\n        # Extract video ID from URL\n        video_id = extract_youtube_id(url)\n        if not video_id:\n            return jsonify({'error': 'Invalid YouTube URL'}), 400\n        \n        # Check if video is already in queue or being processed\n        status = video_queue.get_status(video_id)\n        if status in ['queued', 'processing']:\n            return jsonify({'success': True, 'status': status, 'video_id': video_id})\n        \n        # Get basic video info before adding to queue\n        video_info = monitor.get_video_info(video_id)\n        \n        # Add video to processing queue\n        video_queue.add_task(\n            config=config,\n            url=url,\n            database=db,\n            monitor=monitor,\n            transcriber=transcriber,\n            force=force,\n            transcribe=transcribe,\n            process=process,\n            summarize=summarize,\n            video_id=video_id,\n            is_last=False\n        )\n\n        # Return immediately with success status and video info\n        return jsonify({\n            'success': True, \n            'status': 'queued', \n            'video_id': video_id,\n            'title': video_info.get('title', f'Processing: {video_id}'),\n            'channel': video_info.get('channel', 'Loading...'),\n            'upload_date': video_info.get('upload_date', '')\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/video-status/<video_id>', methods=['GET'])\ndef video_status(video_id):\n    # Disable logging for this specific endpoint completely\n    log = logging.getLogger('werkzeug')\n    log.disabled = True\n    \n    # Re-enable logging after the request\n    @after_this_request\n    def enable_logging(response):\n        log.disabled = False\n        return response\n    \n    status = video_queue.get_status(video_id)\n    return jsonify({'status': status or 'unknown'})\n\n\n@app.route('/delete-video/<video_id>', methods=['DELETE'])\ndef delete_video(video_id):\n    try:\n        # Use the database's delete_video method\n        success = db.delete_video(video_id=video_id)\n        \n        if success:\n            return jsonify({'success': True})\n        else:\n            return jsonify({'error': 'Video not found'}), 404\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/config', methods=['GET'])\ndef get_config():\n    try:\n        config_path = get_config_path()\n        \n        with open(config_path, 'r') as f:\n            config_content = f.read()\n        \n        return jsonify({'content': config_content})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/config', methods=['POST'])\ndef save_config():\n    try:\n        global config  # Reference the global config variable\n        config_path = get_config_path()\n        \n        config_content = request.json.get('content')\n        if not config_content:\n            return jsonify({'error': 'No content provided'}), 400\n        \n        # Validate JSON before saving\n        try:\n            new_config = json.loads(config_content)\n            # Update the global config variable\n            config = new_config\n        except json.JSONDecodeError as e:\n            return jsonify({'error': f'Invalid JSON: {str(e)}'}), 400\n        \n        with open(config_path, 'w') as f:\n            f.write(config_content)\n        \n        # Log that config was updated\n        app.logger.info(\"Configuration updated successfully\")\n        \n        return jsonify({'success': True})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/script/<video_id>')\ndef view_script(video_id):\n    try:\n        video = db.get_video(video_id=video_id)\n        \n        if not video:\n            logging.error(f\"Video not found in database: {video_id}\")\n            return jsonify({\"error\": \"Video not found in database\"}), 404\n        \n        if not video.language:\n            logging.error(f\"Language not set for video: {video_id}\")\n            return jsonify({\"error\": \"Video language not set\"}), 400\n            \n        # Construct the path to the processed.txt file\n        script_path = os.path.join(DOWNLOAD_DIR, f\"{video_id}.{video.language}.processed.txt\")\n        logging.info(f\"Checking script at path: {script_path}\")\n        \n        if not os.path.exists(script_path):\n            logging.error(f\"Script file not found: {script_path}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n        \n        try:\n            with open(script_path, 'r', encoding='utf-8') as f:\n                script_text = f.read()\n            if not script_text.strip():\n                logging.error(f\"Empty script file: {script_path}\")\n                return jsonify({\"content\": \"\"})  # Return empty content\n            return jsonify({\"content\": script_text})\n        except Exception as e:\n            logging.error(f\"Error reading script: {str(e)}\")\n            return jsonify({\"content\": \"\"})  # Return empty content instead of error\n    except Exception as e:\n        logging.error(f\"Unexpected error in view_script: {str(e)}\")\n        return jsonify({\"content\": \"\"})  # Return empty content instead of error\n\n\ndef main():\n    import webbrowser\n    from threading import Timer\n\n    def open_browser():\n        webbrowser.open('http://127.0.0.1:5001/')\n    \n    # Open browser after a short delay to ensure the server is running\n    Timer(1.5, open_browser).start()\n    \n    # Ensure the worker is stopped when the app exits\n    try:\n        app.run(host='0.0.0.0', port=5001, debug=True, use_reloader=False)\n    finally:\n        video_queue.stop_worker()\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "yourtube/__init__.py", "content": "from .database import SqliteDB as Database\nfrom .database import Video\nfrom .transcriber import Transcriber\nfrom .monitor import YoutubeMonitor, BilibiliMonitor\nfrom .reporter import Reporter\n\n__all__ = [\n    \"Video\",\n    \"Database\",\n    \"Transcriber\",\n    \"YoutubeMonitor\",\n    \"BilibiliMonitor\",\n    \"Reporter\"\n]"}
{"type": "source_file", "path": "yourtube/monitor.py", "content": "from typing import List, Dict, Optional\nfrom datetime import datetime\nfrom yourtube import Video\nfrom yourtube.utils import get_download_dir, convert_vtt_to_srt, download_youtube_video, load_config, get_language\nimport yt_dlp\nimport os\n\n\nclass Monitor:\n    \"\"\"Base class for platform-specific monitors\"\"\"\n    def __init__(self, config: Dict):\n        self._default_path = get_download_dir()\n        self._config = config\n    \n    def check_updates(self, handle: str, max_results: int = 10, until_date: str=\"\", end_date: str=\"\") -> List[Video]:\n        \"\"\"Get latest videos from a single channel. \n\n        Args:\n            channel_id (str): The channel ID or handle to fetch videos from\n            max_results (int, optional): Maximum number of videos to return. Defaults to 5.\n            until_date (datetime, optional): Only return videos published before this date. Defaults to None.\n\n        Returns:\n            List[video_id]: List of video ids representing the latest videos from the channel\n        \"\"\"\n        raise NotImplementedError\n    \n    def download(self, video_id: str):\n        \"\"\"Download the video from the platform\"\"\"\n        raise NotImplementedError\n\n\n\nclass YoutubeMonitor(Monitor):\n    def __init__(self, config: Dict):\n        super().__init__(config)\n        self.ydl_opts = {\n            'quiet': True,\n            'extract_flat': True,\n            'force_generic_extractor': False\n        }\n    \n    def get_video_info(self, video_id):\n        \"\"\"\n        Get basic information about a video without downloading it\n        \n        Args:\n            video_id (str): YouTube video ID\n            \n        Returns:\n            dict: Dictionary containing basic video information\n        \"\"\"\n        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n        ydl_opts = {\n            'quiet': True,\n            'skip_download': True,\n            'no_warnings': True,\n            'writeinfojson': False,\n            'noplaylist': True\n        }\n        \n        try:\n            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n                info = ydl.extract_info(video_url, download=False)\n                \n                # Extract only the needed information\n                return {\n                    'title': info.get('title', f'Video {video_id}'),\n                    'channel': info.get('uploader', 'Unknown channel'),\n                    'upload_date': info.get('upload_date', '')\n                }\n        except Exception as e:\n            print(f\"Error getting video info: {str(e)}\")\n            return {\n                'title': f'Processing: {video_id}',\n                'channel': 'Loading...',\n                'upload_date': ''\n            }\n\n    def check_updates(self, channel_handle, max_results=1):\n        \"\"\"\n        Fetches latest videos from a YouTube channel (by handle) from a given date span.\n\n        Parameters:\n            - channel_handle: YouTube channel handle (e.g., \"@ChannelHandle\")\n            - until_date: Cutoff date in \"YYYYMMDD\" format (e.g., \"20240101\" for Jan 1, 2024)\n            - max_results: Maximum number of videos to check (default: 50)\n\n        Returns:\n            - List of video ids uploaded until the given date.\n        \"\"\"\n        \n        url = f\"https://www.youtube.com/@{channel_handle}\"\n        ydl_opts = {\n            'quiet': True,\n            'extract_flat': True,  # Extract metadata without downloading\n            'playlistend': max_results  # Fetch up to `max_results` videos\n        }\n\n        video_ids = []\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            info = ydl.extract_info(url, download=False)\n        \n        video_entries = info['entries']\n        if not video_entries[0].get(\"url\", None): # some channels have two layers of entries\n                video_entries = video_entries[0]['entries']\n\n        for video in video_entries:\n            video_ids.append(video['id'])\n\n        return video_ids\n\n\n    def download(self, video_id, format='worst'):\n        '''\n        Download a YouTube video using yt-dlp library.\n        \n        Parameters:\n            - video_id: str, YouTube video ID\n            - format: str, format/quality specification for yt-dlp (default: 'worst')\n            \n        Returns:\n            - Video: Video object containing metadata and file paths\n            - None: If download fails\n        '''\n        \n        # load info either from local json or downlaod\n        info = download_youtube_video(path=self._default_path, video_id=video_id, video=False)\n        video_title = info.get('title', 'Untitled')\n        language = get_language(info, config=self._config)\n        \n        # get srt path\n        srt_path = os.path.join(self._default_path, f'{video_id}.{language}.srt')\n        if not os.path.exists(srt_path):\n            srt_path = None\n            vtt_path = os.path.join(self._default_path, f'{video_id}.{language}.vtt')\n            try:\n                srt_path = convert_vtt_to_srt(vtt_path)\n                print(f\"vtt converted to srt: {srt_path}\")\n            except FileNotFoundError:\n                srt_path = None\n                vtt_path = None\n                _ = download_youtube_video(path=self._default_path, video_id=video_id, format=format, video=True)\n                print(\"No subtitles for this video, transcribe it please\")\n\n\n        # rename paths    \n        video = Video.from_dict({\n            \"video_id\": video_id,\n            'title': video_title,\n            'channel': info.get('channel', \"\"),\n            'channel_id': info.get('channel_id', \"\"),  # Added channel_id extraction\n            'language': language,\n            'upload_date': info.get('upload_date'),\n            'transcript': True if srt_path else False\n        })\n\n        return video\n\n    \n\nclass BilibiliMonitor(Monitor):\n    def __init__(self, config: Dict):\n        super().__init__(config)\n        \n    def get_channel_info(self, channel_id: str) -> Dict:\n        pass\n    \n    def get_latest_videos(self, channel_id: str, max_results: int = 10, until_date: Optional[datetime] = None) -> List[Video]:\n        pass\n        \n\nif __name__ == \"__main__\":\n    monitor = YoutubeMonitor()\n    video_ids = monitor.check_updates(\"DavidOndrej\", max_results=2)\n    print(video_ids)\n    for id in video_ids[1:]:\n        video = monitor.download(id, download_video=True)\n        print(video)\n        break\n"}
{"type": "source_file", "path": "api/api.py", "content": "from fastapi import FastAPI, BackgroundTasks, HTTPException\nfrom fastapi_utils.tasks import repeat_every\nfrom typing import List, Dict\nimport uvicorn\nfrom pydantic import BaseModel\nfrom datetime import datetime, time\n\napp = FastAPI(title=\"YouTube Monitor API\")\n\nclass ChannelConfig(BaseModel):\n    channel_id: str\n    platform: str\n    scan_time: time  # Time of day to scan\n    enabled: bool = True\n\nclass MonitoringService:\n    def __init__(self):\n        self.config = load_config()\n        self.monitors = {}\n        self.report = Report(self.config)\n        self._initialize_monitors()\n    \n    def _initialize_monitors(self):\n        if 'youtube' in self.config:\n            self.monitors['youtube'] = YoutubeMonitor(self.config)\n    \n    async def process_channel(self, channel: ChannelConfig):\n        \"\"\"Process a single channel's updates\"\"\"\n        monitor = self.monitors.get(channel.platform)\n        if not monitor:\n            raise ValueError(f\"Unsupported platform: {channel.platform}\")\n            \n        videos = monitor.process_new_videos(channel.channel_id)\n        if videos:\n            for video in videos:\n                await self.process_video(video)\n            await self.report.send_update_report(videos)\n    \n    async def process_video(self, video_info: VideoInfo):\n        \"\"\"Process a single video asynchronously\"\"\"\n        video = YoutubeVideo()\n        video.get(video_info.video_id, download_video=True)\n        \n        transcriber = Transcriber()\n        transcriber.transcribe(video)\n        transcriber.process(video)\n        transcriber.summarize(video)\n        \n        self.report.db.add_video(video)\n\n# Initialize the monitoring service\nmonitor_service = MonitoringService()\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60)  # Check every minute\nasync def schedule_monitoring():\n    \"\"\"Check if it's time to scan any channels\"\"\"\n    current_time = datetime.now().time()\n    \n    for channel in monitor_service.config.get_channels():\n        if (channel.enabled and \n            channel.scan_time.hour == current_time.hour and \n            channel.scan_time.minute == current_time.minute):\n            await monitor_service.process_channel(channel)\n\n@app.get(\"/channels/\", response_model=List[ChannelConfig])\nasync def get_channels():\n    \"\"\"Get all monitored channels\"\"\"\n    return monitor_service.config.get_channels()\n\n@app.post(\"/channels/\")\nasync def add_channel(channel: ChannelConfig, background_tasks: BackgroundTasks):\n    \"\"\"Add a new channel to monitor\"\"\"\n    if channel.platform not in monitor_service.monitors:\n        raise HTTPException(400, f\"Unsupported platform: {channel.platform}\")\n    \n    monitor_service.config.add_channel(channel)\n    # Optionally run initial scan\n    background_tasks.add_task(monitor_service.process_channel, channel)\n    return {\"status\": \"success\", \"message\": \"Channel added\"}\n\n@app.delete(\"/channels/{channel_id}\")\nasync def remove_channel(channel_id: str):\n    \"\"\"Remove a channel from monitoring\"\"\"\n    monitor_service.config.remove_channel(channel_id)\n    return {\"status\": \"success\", \"message\": \"Channel removed\"}\n\n@app.post(\"/channels/{channel_id}/scan\")\nasync def trigger_scan(channel_id: str, background_tasks: BackgroundTasks):\n    \"\"\"Manually trigger a channel scan\"\"\"\n    channel = monitor_service.config.get_channel(channel_id)\n    if not channel:\n        raise HTTPException(404, \"Channel not found\")\n    \n    background_tasks.add_task(monitor_service.process_channel, channel)\n    return {\"status\": \"success\", \"message\": \"Scan initiated\"}\n\n@app.get(\"/videos/\")\nasync def get_videos(\n    channel_id: str = None,\n    start_date: datetime = None,\n    end_date: datetime = None\n):\n    \"\"\"Get processed videos with optional filters\"\"\"\n    return monitor_service.report.db.get_videos(\n        channel_id=channel_id,\n        start_date=start_date,\n        end_date=end_date\n    )\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"type": "source_file", "path": "yourtube/prompts.py", "content": "prompt_process_text = lambda content: f\"\"\"\n[Metadata]\nname: Process Transcription\ndescription: Generate well-structured articles from raw transcription text\n[/Metadata]\n\nYou are an expert editor specializing in transforming raw transcription text into a clear, coherent, and readable article. Your task is to convert the provided raw transcription text into a polished article without altering the original content. Follow these exact instructions:\n\n### 1. Input Analysis and Language Check\n- **Detect the Language:**  \n  Identify the language of the provided transcription text.  \n  *Note: If the detected language is Chinese, do not translate any part of the text to English.*\n\n### 2. Content Restructuring\n- **Remove Extraneous Data:**  \n  - Eliminate all index numbers, timestamps, and any metadata markers.\n  - Remove unnecessary line breaks, fragmented lines, and redundant spaces.\n  \n- **Reorganize the Text:**  \n  - Combine fragmented sentences and paragraphs to form coherent, well-structured paragraphs.\n  - Ensure that punctuation is correctly applied so the text flows naturally.\n  \n- **Maintain Original Content:**  \n  - **Do not add, remove, or modify any words** from the original transcription. Your role is solely to restructure and enhance readability.\n\n### 3. Output Requirements\n- **Final Output:**  \n  - Provide the transformed text as a continuous article.\n  - The output must be in the same language as the input text.\n\n### 4. Example for Clarity\n**Raw Transcription Example:**\n\nHere is the example of the raw transcription text:\n\n'''\n1\n00:00:00,000 --> 00:00:08,199\n我们不是没有需求,我们动过念头,我们在这需要一个什么东西。然后你往哪去求助啊。有人在上面找别人做好的软件,找不着。\n\n2\n00:00:08,199 --> 00:00:13,040\n就只能找别人去问问,也有一个沟通成本。有点在内下,还不好意思问。\n\n3\n00:00:13,040 --> 00:00:18,440\n坦特最近大家聊得比较多的,有一个叫做AI福祝编程。\n\n4\n00:00:18,440 --> 00:00:26,320\n这个东西实际上在Chat GPD之前,人们就开始在做这么个东西。\n\n5\n00:00:26,519 --> 00:00:36,000\n包括Gitlab Copilot,我们现在看到的大元模型越来越强之后,我也给你演示过,\n\n6\n00:00:36,000 --> 00:00:42,760\n对吧,咱们之前聊过Cursor,咱们之前还聊过Balt,BOLT,\n\n7\n00:00:42,760 --> 00:00:46,560\n然后可以一战势的开发网也应用。\n\n8\n00:00:46,560 --> 00:00:51,600\n本来大家骑了柔柔,对吧?我们就说现在AI可以帮助我们来编程了,\n\n9\n00:00:51,600 --> 00:00:55,160\n然后可以帮助我们解决很多的问题,我们很开心。\n\n10\n00:00:55,160 --> 00:00:59,880\n原本就是这么一个事,现在突然间就搞的,很多人就跳出来,\n\n11\n00:00:59,880 --> 00:01:06,640\n说AI变成这个东西,现在弄的越来越恶心,有的人已经开始用恶心这个词。\n'''\n\n**Processed Text Example:**\nHere is the processed text: \n'''\n我们不是没有需求,我们动过念头,我们在这需要一个什么东西。然后你往哪去求助啊。有人在上面找别人做好的软件,找不着。就只能找别人去问问,也有一个沟通成本。有点在内下,还不好意思问。坦特最近大家聊得比较多的,有一个叫做AI福祝编程。这个东西实际上在Chat GPD之前,人们就开始在做这么个东西。包括Gitlab Copilot,我们现在看到的大元模型越来越强之后,我也给你演示过,对吧,咱们之前聊过Cursor,咱们之前还聊过Balt,BOLT,然后可以一战势的开发网也应用。本来大家骑了柔柔,对吧?我们就说现在AI可以帮助我们来编程了,然后可以帮助我们解决很多的问题,我们很开心。原本就是这么一个事,现在突然间就搞的,很多人就跳出来,说AI变成这个东西,现在弄的越来越恶心,有的人已经开始用恶心这个词。\n'''\n\n### 5. Instructions Recap\n- **Detect the language** of the input and ensure your final output uses the same language.\n- **Eliminate indices, timestamps, and unnecessary breaks.**\n- **Restructure** the text into clear, natural-flowing paragraphs while preserving the original content.\n- **Do not modify or add content.**\n\nWhen you are ready, process the transcription text accordingly.\n<body>\n{content}\n</body>\n\"\"\"\n\n\nprompt_summarize = lambda content: f\"\"\"\nYou are a bilingual text summarization expert with deep fluency in both English and Chinese. Your task is to analyze a given piece of text, reorganize its information, and produce a comprehensive summary. Follow the steps below exactly:\n\n---\n\n### 1. Language Identification\n- **Determine the language:**  \n  Analyze the input text and denote the detected language as <LAN>.\n- **Uniform Output:**  \n  Ensure that the entire response is in <LAN>.  \n  *If <LAN> is Chinese, please rewrite all contents with Simplified Chinese.*\n  *If <LAN> is English, please rewrite all contents with English.*\n  *（如果<LAN>是中文，请用中文重写所有内容；如果<LAN>是英文，请用英文重写所有内容）*\n\n---\n\n### 2. Output Structure and Format\n- **Markdown Formatting:**  \n  Use Markdown with bold headings and bullet lists for clarity.\n- **Three Sections Required:**  \n  Your final output must contain exactly three sections:\n  1. **summary**\n  2. **insight**\n  3. **breakdown**\n\n---\n\n### 3. Processing Steps\n- Step 1. **Breakdown Section:**\n  - Conduct a comprehensive analysis of the text.\n  - Identify all major topics and subtopics.\n  - For each topic/subtopic, provide detailed descriptions including numbers, examples, or evidence.\n  - Organize your analysis in bullet lists and sub-bullets to form the **breakdown** section.\n  \n- Step 2. **Summary Section:**\n  - Based on your detailed analysis, create a concise summary capturing the main points.\n  - This concise overview will be your **summary** section.\n  \n- Step 3. **Insight Section:**\n  - Reflect on the information in both the **breakdown** and **summary**.\n  - Draw creative conclusions or additional insights.\n  - Present this reflective commentary in the **insight** section.\n\n---\n\n### 4. Final Verification\n- **Language Consistency:**  \n  Double-check that your final output is entirely in <LAN>.  \n  *If <LAN> is Chinese, ensure every section is in Chinese.*\n\n---\n\n**Remember:**  \nBelow is the content to be summarized:\n\n<body>\n{content}\n</body>\n\"\"\"\n\n\nprompt_process_fulltext_zh = lambda content, starting_text: f\"\"\"\n你很擅长在尊重原文的基础上重写内容。你会采用段落的形式，而不是列表的形式。你使用简体中文，而非繁体中文。你非常尊重原文，你不会对原文进行修改，仅纠正必要的错别字和汉语常用短语。\n\n以下是你需要重写的文本：\n\n\"{content}\"\n\n请重写以上文本。开头我已经为你写好，以下为开头，开头之前的内容请忽略\n\n\"{starting_text}\"\n\"\"\"\n\nprompt_process_fulltext_en = lambda content, starting_text: f\"\"\"\nYou excel at rewriting content while respecting the original text. You use paragraph format rather than list format. You deeply respect the original text and won't modify it, only correcting necessary typos and common phrases.\n\nHere is the text you need to rewrite:\n\n\"{content}\"\n\nPlease rewrite the text above. I have already written the beginning for you, please ignore any content before this beginning:\n\n\"{starting_text}\"\n\"\"\"\n\ndef prompt_process_fulltext(content, starting_text, language):\n    if language == \"zh\":\n        return prompt_process_fulltext_zh(content, starting_text)\n    else:\n        return prompt_process_fulltext_en(content, starting_text)\n\n\n"}
{"type": "source_file", "path": "yourtube/database.py", "content": "import os\nimport glob\nfrom uuid import uuid4\nfrom datetime import datetime\nfrom abc import ABC, abstractmethod\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import (\n    Column, \n    String, \n    DateTime, \n    Boolean, \n    UUID\n)\nfrom yourtube.utils import get_download_dir, get_db_path\n\nBase = declarative_base()\n\nclass Video(Base):\n    \"\"\"Abstract base class for Video objects\"\"\"\n    __tablename__ = \"videos\"\n\n    id              = Column(UUID(as_uuid=True), primary_key=True, unique=True, default=uuid4)\n    video_id        = Column(String(20), nullable=False)\n    title           = Column(String(200), nullable=False)\n    channel_id      = Column(String(20), nullable=True)\n    channel         = Column(String(100), nullable=True)\n    upload_date     = Column(DateTime)\n    process_date    = Column(DateTime)\n    language        = Column(String(10), nullable=True)  # Store primary language\n    transcript      = Column(Boolean, default=False)\n    fulltext        = Column(Boolean, default=False)\n    summary         = Column(Boolean, default=False)\n    _metadata       = {} # metadata from the \n    _default_path   = get_download_dir()\n\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n\n    def __repr__(self):\n        return f\"<Video(video_id='{self.video_id}', title='{self.title}', channel='{self.channel}')>\"\n    \n    @property\n    def short_id(self):\n        return self.id[:6]\n\n    @classmethod\n    def from_dict(cls, data):\n        '''Create video object from JSON data\n        Args:\n            data: the dicionary containing info of the video\n        Returns:\n            Video: video instance\n        '''\n        return cls(**data)\n\n\n    def to_dict(self):\n        '''Convert video object to dictionary for selection\n        Returns:\n            dict: dictionary of video object\n        '''\n        return {\n            'id': str(self.id),\n            \"video_id\": self.video_id,\n            'title': self.title,\n            'channel': self.channel,\n            'channel_id': self.channel_id,\n            'upload_date': self.upload_date.isoformat() if self.upload_date else None,\n            'process_date': self.process_date.isoformat() if self.process_date else None,\n            'language': self.language,\n            'transcript': self.transcript,\n            'fulltext': self.fulltext,\n            'summary': self.summary\n        }\n    \n    def update(self, **kwargs):\n        '''Update the video object with new attributes'''\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n        \n        if isinstance(self.upload_date, str):\n            try:\n                self.upload_date = datetime.strptime(self.upload_date, '%Y%m%d')\n            except ValueError:\n                self.upload_date = datetime.strptime(self.upload_date.split('T')[0], '%Y-%m-%d')\n        self.process_date = datetime.now()\n\n        #scan the folder to see if transcript, fulltext, and summary files exist\n        for file in glob.glob(f\"{get_download_dir()}/{self.video_id}.*\", recursive=True):\n            if file.endswith(f'{self.language}.srt'):\n                self.transcript = True\n            elif file.endswith(f'{self.language}.txt'):\n                self.fulltext = True\n            elif file.endswith(f'{self.language}.md'):\n                self.summary = True\n\nclass Database(ABC):\n    def __init__(self, db_path=None):\n        if db_path is None:\n            self.db_path = get_db_path()\n        else:\n            self.db_path = db_path\n\n    def add_video(self, video):\n        return self._add_video(video)\n\n    def delete_video(self, **kwargs):\n        return self._delete_video(**kwargs)\n    \n    def refresh_database(self):\n        raise NotImplementedError\n    \n    def get_video(self, **kwargs):\n        return  self._get_video(**kwargs)\n\n    \n    def update_video(self, video: Video):\n        \"\"\"Update an existing video in the database without deleting/re-adding.\"\"\"\n        try:\n            # Get existing video\n            existing_video = self.get_video(video_id=video.video_id)\n            \n            if existing_video:\n                # Instead of updating, delete the existing video and add the new one\n                # This ensures a clean state and avoids session tracking issues\n                \n                # Store the ID of the existing video\n                existing_id = existing_video.id\n                \n                # Remove the existing video from the session\n                self.session.delete(existing_video)\n                self.session.commit()\n                \n                # Set the ID on the new video to maintain the same primary key\n                video.id = existing_id\n                \n                # Add the new video to the session\n                self.session.add(video)\n                self.session.commit()\n                return True\n            else:\n                # If video doesn't exist, add it\n                return self.add_video(video)\n        except Exception as e:\n            self.session.rollback()\n            raise Exception(f\"Error updating video: {str(e)}\")\n    \n    @abstractmethod\n    def _add_video(self, video: Video):\n        \"\"\"Add a new video to the database.\"\"\"\n        raise NotImplementedError\n    \n    @abstractmethod\n    def _delete_video(self, **kwargs):\n        \"\"\"Delete a video from the database.\"\"\"\n        raise NotImplementedError\n    \n    @abstractmethod\n    def _get_video(self, **kwargs):\n        \"\"\"Get a video from the database.\"\"\"\n        raise NotImplementedError\n\n\nclass SqliteDB(Database):\n    def __init__(self, db_path='videos.db'):\n        super().__init__(db_path)\n        self.engine = create_engine(f'sqlite:///{self.db_path}', echo=False)\n        Base.metadata.create_all(self.engine)  # Create tables if they don't exist\n        Session = sessionmaker(bind=self.engine)\n        self.session = Session()\n\n    def _add_video(self, video: Video):\n        \"\"\"Add a new video to the database.\"\"\"\n        try:\n            self.session.add(video)\n            self.session.commit()\n        except Exception as e:\n            self.session.rollback()\n            raise Exception(f\"Error adding video: {str(e)}\")\n\n    def _delete_video(self, **kwargs):\n        \"\"\"Delete a video from the database.\"\"\"\n        try:\n            video = self.get_video(**kwargs)\n            if video:\n                self.session.delete(video) # remove from database\n                self.session.commit() \n                for file in glob.glob(f\"{get_download_dir()}/{video.video_id}.*\", recursive=True):\n                    print(file)\n                    try:\n                        os.remove(file)\n                    except OSError as e:\n                        print(f\"Error deleting file: {file}\")\n                print(f\"Successfully deleted videos filtered by: {kwargs}\")\n                return True\n            else:\n                print(f\"No videos found filtered by: {kwargs}\")\n                return False\n        except Exception as e:\n            self.session.rollback()\n            raise Exception(f\"Error deleting video: {str(e)}\")\n\n    def _get_video(self, **kwargs):\n        '''If video_id exists in database, return video, other wise None\n        '''\n        try:\n            video = self.session.query(Video).filter_by(**kwargs).first()\n            return video\n        except Exception as e:\n            self.session.rollback()\n            raise IndexError(f\"Something went wrong when trying to get video: {e}\")\n            return None\n\nif __name__ == \"__main__\":\n    db = SqliteDB()\n    deleted = db.delete_video(video_id=\"HeHnTfkCcok\")\n    print(f\"Deleted: {deleted}\")\n"}
{"type": "source_file", "path": "yourtube/main.py", "content": "import os\nimport argparse\nfrom yourtube import Database, Transcriber\nfrom yourtube.utils import extract_youtube_id, load_config, get_download_dir, get_db_path\nfrom yourtube.monitor import YoutubeMonitor, BilibiliMonitor\nfrom yourtube.reporter import Reporter\nfrom typing import Dict\nimport asyncio\nimport schedule\nimport time\n\n# global variables setup: dadtabase, monitor, transcriber, config, video_queue\nDOWNLOAD_DIR = get_download_dir()\nDB_PATH = get_db_path()\nprint(f\"Download directory: {DOWNLOAD_DIR}\")\nprint(f\"Database path: {DB_PATH}\")\n\nconfig =load_config() #check if config.json exists, if not create it from template\ndb = Database(db_path=DB_PATH)\nmonitor = YoutubeMonitor(config=config)\ntranscriber = Transcriber(config=config)\n\n\ndef initialize_monitors(config: Dict) -> Dict:\n    \"\"\"Initialize monitors for each platform\"\"\"\n    monitors = {}\n    platform_monitors = {\n        'youtube': YoutubeMonitor,\n        'bilibili': BilibiliMonitor\n    }\n    for platform in config.get('monitored_platforms', list(platform_monitors.keys())):\n        if platform in platform_monitors:\n            monitors[platform] = platform_monitors[platform](config)\n    return monitors\n\nasync def pull_updates(monitors: Dict):\n    \"\"\"Start monitors on the platforms and pull updates to database\"\"\"\n    for monitor in monitors.values():\n        await monitor.pull()\n\nasync def process_updates(monitors: Dict):\n        \"\"\"Check for the latest videos in database and create a report. start_date and end_date are in the format of YYYY-MM-DD\"\"\"\n        new_videos = []\n        \n        # Check each platform and process all channels for that platform\n        for monitor in monitors.values():\n            for video_id in monitor.check_updates():\n                video = monitor.download(video_id, download_video=True)\n                new_videos.append(video)\n        \n        if not new_videos:\n            print(\"No new videos found\")\n            return\n        \n        # Process all videos using a single Transcriber instance\n        transcriber = Transcriber()\n        for video in new_videos:\n            if not video.transcript:\n                transcriber.transcribe(video)\n            if not video.summary:\n                transcriber.summarize(video)\n        \n        # Generate and send report\n        \n        # Ensure model is turned off when finished\n        transcriber.release_model()\n\n\nasync def run_scheduler(\n        args: argparse.Namespace,\n        run_immediately=True, \n        daily_time=None,\n        reporter: Reporter=None,\n        monitors: Dict=None,\n        transcriber: Transcriber=None,\n        database: Database=None\n        ):\n        \"\"\"\n        Start the scheduling service\n        Args:\n            run_immediately (bool): Whether to run once immediately before starting scheduler\n            daily_time (str): Time to run daily in 24hr format (e.g., \"09:00\")\n        \"\"\"\n        if run_immediately:\n            print(\"Running immediate check...\")\n            for platform, monitor in monitors.items():\n                new_video_ids =await monitor.check_updates()\n                for video_id in new_video_ids:\n                    video = database.get_video(video_id=video_id)\n                    if video and not args.force:\n                        continue\n                    video = monitor.download(video_id, download_video=True, download_json=True)\n                    database.add_video(video)\n            print(\"Initial check completed.\")\n\n        # Schedule updates based on configuration\n        if daily_time:\n            # Schedule to run at specific time daily\n            print(f\"Scheduling daily check at {daily_time}\")\n            schedule.every().day.at(daily_time).do(\n                lambda: asyncio.run(process_updates())\n            )\n        \n        # Run the scheduler\n        print(\"Scheduler started...\")\n        while True:\n            schedule.run_pending()\n            time.sleep(60)  # Check every minute\n\ndef process_video_pipeline(\n        config, \n        url, \n        database, \n        monitor, \n        transcriber, transcribe=False, process=False, summarize=False, \n        force=False, \n        video_id=None,\n        is_last=False # whether this is the last video to process in a queue\n    ):\n    \"\"\"\n    Process a video from URL through the pipeline\n    \n    Args:\n        url (str): YouTube URL\n        database (Database): Database instance\n        monitor (YoutubeMonitor): Monitor instance\n        transcriber (Transcriber): Transcriber instance\n        transcribe (bool): Whether to transcribe the video\n        process (bool): Whether to process the transcript\n        summarize (bool): Whether to summarize the transcript\n        force (bool): Whether to force processing even if video exists\n        video_id (str, optional): Video ID if already extracted\n    \n    Returns:\n        int: 0 on success, non-zero on failure\n    \"\"\"\n    if not video_id:\n        video_id = extract_youtube_id(url)\n    \n    video = database.get_video(video_id=video_id)\n    if video and not force:\n        print(f\"Video {video_id} already processed\")\n        return 0\n\n    # Download and transcribe video flow\n    video = monitor.download(video_id)\n\n    if transcribe and not video.transcript:\n        model_size = config.get(\"transcribe\", {}).get(\"size\", \"base\")\n        print(f\"Transcribing video\")\n        if not transcriber.model:\n            transcriber.load_model(model_size=model_size)\n        _ = transcriber.transcribe(video)\n        if is_last:\n            transcriber.release_model()\n    \n    if process:\n        print(f\"Processing SRT file.\")\n        _ = transcriber.extract_fulltext(video)\n        _ = transcriber.process_fulltext(video)\n\n    if summarize:\n        print(f\"Summarizing transcription.\")\n        _ = transcriber.summarize(video)\n    \n    # Add to database\n    video.update(**transcriber.metadata) \n    database.update_video(video)\n    print(f\"Successfully downloaded video: {video.title}\")\n    \n    transcriber.release_model()\n\n    return 0\n\ndef main():\n    \n    global config, db, monitor, transcriber\n\n    parser = argparse.ArgumentParser(description=\"Transcribe video to SRT format or process an existing SRT file.\")\n    parser.add_argument(\"-y\", '--youtube_url', type=str, help=\"YouTube video url to download\")\n    parser.add_argument(\"-t\", \"--transcribe\", action=\"store_true\", default=False, help=\"Whether to transcribe a video file.\")\n    parser.add_argument(\"-p\", \"--process\", action=\"store_true\", default=False, help=\"Whether to process an existing SRT file.\")\n    parser.add_argument(\"-s\", \"--summarize\", action=\"store_true\", default=False, help=\"Whether to summarize the transcription.\")\n    parser.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"Force to update video even if it exists.\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", default=False, help=\"Display the summary in the terminal after processing.\")\n    # parser.add_argument(\"-r\", \"--report\", action=\"store_true\", default=False, help=\"Create a report of the latest videos.\")\n    \n    args=parser.parse_args()\n\n    config = load_config()\n    process_video_pipeline(\n        config=config,\n        database=db,\n        monitor=monitor,\n        transcriber=transcriber,\n        url=args.youtube_url,\n        transcribe=args.transcribe,\n        process=args.process,\n        summarize=args.summarize,\n        force=args.force\n    )\n \n\nif __name__ == \"__main__\":\n    main()   \n"}
{"type": "source_file", "path": "api/config.py", "content": "from pydantic import BaseSettings\nfrom typing import List, Dict\nfrom datetime import time\n\nclass EmailConfig(BaseSettings):\n    smtp_server: str\n    smtp_port: int\n    username: str\n    password: str\n    recipients: List[str]\n\nclass ChannelConfig(BaseSettings):\n    channel_id: str\n    platform: str\n    scan_time: time\n    enabled: bool = True\n\nclass AppConfig(BaseSettings):\n    youtube_api_key: str\n    email: EmailConfig\n    channels: List[ChannelConfig]\n    \n    class Config:\n        env_file = \".env\"\n\n    def get_channels(self) -> List[ChannelConfig]:\n        return self.channels\n    \n    def add_channel(self, channel: ChannelConfig):\n        self.channels.append(channel)\n        self._save()\n    \n    def remove_channel(self, channel_id: str):\n        self.channels = [c for c in self.channels if c.channel_id != channel_id]\n        self._save()\n    \n    def _save(self):\n        # Save configuration changes to file/database\n        pass\n"}
{"type": "source_file", "path": "yourtube/transcriber.py", "content": "# transcriber.py\nimport os\nimport whisper\nimport litellm\nimport ffmpeg\nimport logging\nfrom yourtube import Video\nfrom yourtube.utils import get_device, get_download_dir, get_llm_info\nfrom yourtube.prompts import prompt_summarize, prompt_process_fulltext\n\n# Configure litellm logging - fix the verbose setting\nlitellm.verbose = False  # Set the verbose attribute directly\nlogging.getLogger(\"litellm\").setLevel(logging.ERROR)  # Only show ERROR level logs\n\ndef format_timestamp(seconds):\n    \"\"\"\n    Convert seconds to SRT timestamp format (HH:MM:SS,mmm).\n\n    Args:\n        seconds (float): Time in seconds\n\n    Returns:\n        str: Formatted timestamp string in format \"HH:MM:SS,mmm\"\n    \"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    secs = int(seconds % 60)\n    millis = int((seconds - int(seconds)) * 1000)\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n\n\ndef create_srt(segments):\n    \"\"\"\n    Convert transcription segments to SRT format.\n\n    Args:\n        segments (list): List of dictionaries containing segment information\n                        Each segment should have 'start', 'end', and 'text' keys\n\n    Returns:\n        str: Complete SRT formatted string with all segments\n    \"\"\"\n    srt_content = []\n    for i, segment in enumerate(segments, start=1):\n        start_time = format_timestamp(segment['start'])\n        end_time = format_timestamp(segment['end'])\n        text = segment['text'].strip()\n\n        srt_entry = f\"{i}\\n{start_time} --> {end_time}\\n{text}\\n\"\n        srt_content.append(srt_entry)\n\n    return \"\\n\".join(srt_content)\n\n\ndef preprocess_audio(audio_path):\n    \"\"\"\n    Pre-process audio file for better transcription quality.\n    Converts audio to 16kHz mono WAV format.\n\n    Args:\n        audio_path (str): Path to the input audio/video file\n\n    Returns:\n        str: Path to the processed audio file, or None if processing fails\n    \"\"\"\n    try:\n        # Convert to 16kHz mono WAV\n        output_path = audio_path.replace('.mp4', '.wav')\n        ffmpeg.input(audio_path).output(\n            output_path, \n            ar='16000',    # Sample rate\n            ac=1,          # Mono audio\n            acodec='pcm_s16le'\n        ).run(overwrite_output=True, quiet=True)\n        print(f\"Audio preprocessed to: {output_path}\")\n        return output_path\n    except Exception as e:\n        print(f\"Error processing audio: {e}\")\n        return None\n        \nclass Transcriber:\n    def __init__(self, video: Video|None=None, model_size=None, config=None):\n        self._config = config   \n        self.working_dir = get_download_dir()\n        self.model_size = model_size\n        self.device = None\n        self.model = None\n        self._video_path = \"\"\n        self._srt_path = \"\"\n        self._txt_path = \"\"\n        self._md_path = \"\"\n        self._video_id = None\n        self._language = \"zh\" # default language is Chinese\n        if video:\n            self.load_video(video)\n        if model_size:\n            self.load_model(model_size)\n\n    @property\n    def metadata(self):\n        return {\n            \"summary\": True if self._md_path and os.path.exists(self._md_path) else False,\n            \"transcription\": True if self._srt_path and os.path.exists(self._srt_path) else False,\n            \"fulltext\": True if self._txt_path and os.path.exists(self._txt_path) else False,\n            \"language\": self._language\n        }\n\n    def load_model(self, model_size: str=\"base\"):\n        \"\"\"\n        Load the Whisper model with GPU acceleration if available.\n\n        Args:\n            model_size (str, optional): Name of the Whisper model to load. Defaults to \"base\".\n        \"\"\"\n        # Try to use MPS/GPU first, fallback to CPU if there are issues\n        try:\n            self.device = get_device()\n            self.model = whisper.load_model(model_size).to(self.device)\n        except (NotImplementedError, RuntimeError):\n            print(\"GPU acceleration failed, falling back to CPU...\")\n            self.device = \"cpu\"\n            self.model = whisper.load_model(model_size).to(self.device)\n    \n\n    def load_video(self, video: Video):\n        \"\"\"\n        Load video into the transcriber and set up file paths.\n\n        Args:\n            video (Video): Video object containing video information\n        \"\"\"\n        self._video = video\n        self._language = video.language\n\n        self._video_path = os.path.join(self.working_dir, f\"{video.video_id}.mp4\")\n        self._srt_path = self._video_path.replace(\".mp4\", f\".{self._language}.srt\")\n        self._txt_path = self._video_path.replace(\".mp4\", f\".{self._language}.txt\")\n        self._processed_txt_path = self._txt_path.replace(\".txt\", \".processed.txt\")\n        self._md_path = self._video_path.replace(\".mp4\", f\".{self._language}.md\")\n\n\n    def transcribe(self, video: Video):\n        \"\"\"\n        Transcribe video audio to text and save as SRT file.\n        Includes language detection and optimized transcription settings.\n\n        Args:\n            video (Video): Video object to transcribe\n\n        Returns:\n            int: 0 on success, 1 on failure\n        \"\"\"\n        assert self.model, \"Model not loaded.\"\n        \n        self.load_video(video)\n        print(\"Detecting language...\", end=\"\\r\", flush=True)\n        \n        # If audio processing fails, return an error\n        processed_audio_path = preprocess_audio(self._video_path)\n        if processed_audio_path is None:\n            print(\"Audio preprocessing failed, using video directly.\")\n            processed_audio_path = self._video_path\n        \n        # Use the model initialized in __init__\n        model = self.model\n\n        # First detect the language\n        audio = whisper.load_audio(processed_audio_path)\n        audio = whisper.pad_or_trim(audio)\n        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n        _, probs = model.detect_language(mel)\n        language = max(probs, key=probs.get)\n        print(f\"Detected language: {language}\")\n        print(\"Transcribing...\", end=\"\\r\", flush=True)\n        try:\n            result = model.transcribe(\n                processed_audio_path,\n                task=\"transcribe\",\n                language=language,\n                initial_prompt=\"以下是一段中文视频内容的转录。请使用简体中文准确转录，保持原有的语气和表达方式。\" if language == \"zh\" else None,\n                fp16=self.device != \"cpu\", # Use half-precision floating point for faster processing\n                beam_size=1, # Increase beam search width (default is 1)\n                best_of=1, # Generate multiple samples and select best (default is 1)\n                temperature=0.0, # Lower temperature for more deterministic output\n                condition_on_previous_text=True, # Use previous text as condition\n                compression_ratio_threshold=2, # Prevent empty segments\n                no_speech_threshold=0.6, # Prevent empty segments\n                word_timestamps=False # Generate word-level timestamps\n            )\n            srt_content = create_srt(result['segments'])\n            with open(self._srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n                srt_file.write(srt_content)\n            print(f\"Transcription saved to: {self._srt_path}\")\n            \n            # delete the video file after transcribing\n            for suffix in ['mp4', 'wav']:\n                try: \n                    os.remove(os.path.join(self.working_dir, f'{video.video_id}.{suffix}'))\n                except FileNotFoundError:\n                    continue\n            \n            return srt_content\n        \n        except Exception as e:\n            self._srt_path = \"\"\n            print(f\"Error transcribing video: {e}\")\n            return None\n\n    def release_model(self):\n        \"\"\"\n        Release the loaded Whisper model from memory.\n        \"\"\"\n        try:\n            del self.model\n            self.model = None\n            print(\"Model memory released\")\n        except Exception as e:\n            print(f\"Error releasing model: {e}\")\n\n    def extract_fulltext(self, video: Video):\n        \"\"\"\n        Process SRT file to extract clean text content.\n        Removes timestamps and formatting, combining all text into a single file.\n\n        Args:\n            video (Video): Video object containing file information\n\n        Returns:\n            int: 0 on success, 1 on failure\n        \"\"\"\n        self.load_video(video)\n        # Read the SRT file\n        try:\n            with open(self._srt_path, 'r', encoding='utf-8') as srt_file:\n                srt_text = srt_file.read()\n        except FileNotFoundError:\n            print(f\"SRT file not found: {self._srt_path}\")\n            self.transcribe(video)\n            with open(self._srt_path, 'r', encoding='utf-8') as srt_file:\n                srt_text = srt_file.read()\n\n        # Split into blocks by double newline\n        blocks = srt_text.split('\\n\\n')\n        \n        # Extract text lines (skip blocks with 4 lines, take last line from others)\n        text_lines = []\n        for block in blocks:\n            lines = block.strip().split('\\n')\n                \n            # Take the last line from valid blocks\n            if len(lines) >= 3:  # Valid blocks have at least 3 lines\n                text_lines.append(lines[-1].strip())\n        \n        # Join all text lines with a space\n        processed_text = ' '.join(line for line in text_lines if line)\n        \n        # Write processed text to new file\n        with open(self._txt_path, 'w', encoding='utf-8') as txt_file:\n            txt_file.write(processed_text)\n        \n        print(f\"Converted srt to txt: {self._txt_path}\")\n        return processed_text\n    \n    def process_fulltext(self, video: Video, chunk_size: int=2000, overlap: int=200):\n        \"\"\"\n        Process the fulltext of the video. The purpose is to reorganize the text into a more readable format. It does the following:\n        1. Read the fulltext from the txt file and divide it into chunks of 1000 tokens/words each with an overlap.\n        2. Iterate over chunks:\n            For each chunk, feed to `prompt_process_fulltext` function to create a prompt for the LLM, and get the output of from LLM.\n        4. Create a new txt file and write the output of the LLM to it and append the output of LLM to the file for each iteration.\n        \n        Args:\n            video (Video): Video object containing file information\n            chunk_size (int, optional): Size of each chunk in tokens/words. Defaults to 1000.\n            overlap (int, optional): Number of tokens/words to overlap between chunks. Defaults to 200.\n            \n        Returns:\n            str: Processed content\n        \"\"\"\n        self.load_video(video)\n        with open(self._txt_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        \n        # Get LLM information from config\n        llm_provider, llm_name, api_key, max_tokens, temperature = get_llm_info(\"process_fulltext\")\n        words = list(content) if self._language == \"zh\" else content.split() # For Chinese, we split by characters since each character is a token\n        \n        # Create chunks with overlap\n        chunks = []\n        for i in range(0, len(words), chunk_size - overlap):\n            # Make sure we don't go beyond the end of the list\n            end_idx = min(i + chunk_size, len(words))\n            chunk = words[i:end_idx]\n            chunks.append(''.join(chunk) if self._language == \"zh\" else ' '.join(chunk))\n            # If we've reached the end, break\n            if end_idx == len(words):\n                break\n        \n        # Process each chunk with LLM\n        processed_content = \"\"\n        starting_text = \"\"  # Initial starting text is empty\n        \n        for i, chunk in enumerate(chunks):\n            print(f\"Processing text: {i+1}/{len(chunks)}\", end=\"\\r\", flush=True)\n            # Create prompt for LLM\n            try:\n                response = litellm.completion(\n                    messages=[{\n                        \"role\": \"user\", \n                        \"content\": prompt_process_fulltext(chunk, starting_text, self._language)\n                    }],\n                    model=f\"{llm_provider}/{llm_name}\",\n                    api_key=api_key,\n                    max_tokens=max_tokens,\n                    temperature=temperature\n                )\n            except Exception as e:\n                print(f\"LLM error processing chunk {i+1}: {e}\")\n                raise\n            chunk_result = response.choices[0].message.content\n            \n            # Extract the last paragraph for use as starting text for next chunk\n            last_paragraph = \"\"\n            content_to_append = chunk_result\n            whitespace = '\\n\\n'\n            paragraphs = chunk_result.split(whitespace)\n            if len(paragraphs) > 1:\n                # If we have multiple paragraphs, use the last one as starting text\n                # and append all but the last paragraph to processed_content\n                last_paragraph = paragraphs[-1]\n                content_to_append = whitespace.join(paragraphs[:-1])\n            else:\n                # If no double-newline paragraph breaks, try splitting by single newlines\n                whitespace = '\\n'   \n                paragraphs = chunk_result.split(whitespace)\n                if len(paragraphs) > 1:\n                    last_paragraph = paragraphs[-1]\n                    content_to_append = whitespace.join(paragraphs[:-1])\n                else:\n                    # If no paragraph breaks at all, use the entire chunk as the last paragraph\n                    last_paragraph = chunk_result\n                    content_to_append = \"\"\n            \n            # Append the content (without the last paragraph) to the processed content\n            processed_content += whitespace + content_to_append\n            starting_text = last_paragraph\n\n        processed_content += whitespace + last_paragraph # Append the last paragraph to the processed content\n        # Create a new file for the processed content\n        processed_txt_path = self._txt_path.replace(\".txt\", \".processed.txt\")\n        with open(processed_txt_path, 'w', encoding='utf-8') as file:\n            # Strip leading newlines before writing to file\n            file.write(processed_content.lstrip('\\n'))\n        \n        print(f\"Processed fulltext saved to: {processed_txt_path}\")\n        \n        # Update the txt path to the processed file\n        self._txt_path = processed_txt_path\n        return processed_content\n    \n\n\n    def summarize(self, video: Video, verbose=False):\n        \"\"\"\n        Generate a summary of the transcribed content using LLM.\n\n        Args:\n            video (Video): Video object containing transcription.\n            verbose (bool, optional): Whether to print the summary. Defaults to False.\n\n        Returns:\n            int: 0 on success, 1 on failure\n\n        Notes:\n            - Reads from the processed text file (.txt)\n            - Uses LiteLLM to generate summary\n            - Saves summary in markdown format (.md)\n        \"\"\"\n        # Use the processed text file if it exists, otherwise use the original text file\n        txt_path = self._processed_txt_path if os.path.exists(self._processed_txt_path) else self._txt_path\n        \n        self.load_video(video)\n        llm_provider, llm_name, api_key, max_tokens, temperature = get_llm_info(\"summarize\")\n\n        # Read SRT content from the file\n        try:\n            with open(txt_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n        except FileNotFoundError:\n            self.extract_fulltext(video)\n            with open(txt_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n        try:\n            # Use LiteLLM to get response from the specified provider\n            response = litellm.completion(\n                messages=[{\"role\": \"user\", \"content\": prompt_summarize(content)}],\n                model=f\"{llm_provider}/{llm_name}\",\n                api_key=api_key,\n                max_tokens=max_tokens,\n                temperature=temperature\n            )\n            summary_text = response.choices[0].message.content\n        except Exception as e:\n            print(f\"\\nError processing transcription with {e}.\\n\")\n        \n        # Save formatted text to a file\n        summary_path = self._txt_path.replace(\".txt\", \".md\")\n        with open(summary_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(summary_text)\n        print(f\"Summary saved to file: {summary_path}\")\n\n        if verbose:\n            print(\"\\n Summary:\\n\")\n            print(summary_text)\n        \n        return summary_text\n\nif __name__ == \"__main__\":\n    from yourtube import Database\n    from yourtube.utils import get_db_path\n    db = Database(db_path=get_db_path())\n    transcriber = Transcriber()\n    video = db.get_video(video_id=\"yarle_bZDCs\")\n    transcriber.process_fulltext(video)"}
{"type": "source_file", "path": "yourtube/reporter.py", "content": "from datetime import datetime\nfrom typing import List, Dict\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport aiosmtplib\nimport markdown\nimport os\nfrom yourtube import Database, Video\nfrom yourtube.utils import get_download_dir\nimport asyncio\n\nREPORT_TEMPLATE_SINGLE = lambda title, url, channel, upload_date, summary: f\"\"\"\n## [{title}]({url})\n**Channel:** {channel}\n**Published:** {upload_date.strftime('%Y-%m-%d %H:%M:%S')}\n\n{summary}\n#########\n\n\n\"\"\"\n\nclass Reporter:\n    def __init__(self, config: Dict):\n        self.config = config\n        self.db = SqliteDB()\n        self.email_config = config['email']\n        self.download_dir = get_download_dir()\n    \n    \n    def _generate_report_single(self, video: Video) -> str:\n        \"\"\"Generate markdown summary for a single video by reading the summary file\"\"\"\n        # Read summary file\n        summary_path = os.path.join(self.download_dir, f\"{video.video_id}.md\")\n        try:\n            with open(summary_path, 'r', encoding='utf-8') as f:\n                summary_content = f.read().strip()\n        except FileNotFoundError:\n            print(f\"Summary file not found for video {video.video_id}\")\n            summary_content = 'Summary not available'\n\n        # Construct YouTube URL\n        video_url = f\"https://youtube.com/watch?v={video.video_id}\"\n\n        return REPORT_TEMPLATE_SINGLE(video.title, video_url, video.channel, video.upload_date, summary_content)\n\n\n    def _send_email(self, subject: str, content: str):\n        \"\"\"Send email asynchronously\"\"\"\n        msg = MIMEMultipart()\n        msg['Subject'] = subject\n        msg['From'] = self.email_config['username']\n        msg['To'] = ', '.join(self.email_config['recipients'])\n        \n        # Convert markdown to HTML\n        html_content = markdown.markdown(content)\n        msg.attach(MIMEText(html_content, 'html'))\n        \n        with aiosmtplib.SMTP(\n            self.email_config['smtp_server'],\n            self.email_config['smtp_port']\n        ) as server:\n            server.starttls()\n            server.login(\n                self.email_config['username'],\n                self.email_config['password']\n            )\n            server.send_message(msg)\n    \n\n    async def generate_report(self, videos: List[Video]) -> str:\n        \"\"\"Generate full report content from list of videos\"\"\"\n        if not videos:\n            return \"## Daily Video Update Report ## \\nNo new videos available today.\"\n        # Use asyncio.gather to properly handle multiple async tasks\n        summaries = await asyncio.gather(*[self._generate_report_single(video) for video in videos])\n        return \"\\n\\n\".join(summaries)\n\n\n    def send_report(self, videos: List[Video]):\n        \"\"\"Generate and send report for new videos\"\"\"\n        # Add await here since generate_report is async\n        report_content = self.generate_report(videos)\n        self._send_email(\n            subject=f\"Video Updates - {datetime.now().strftime('%Y-%m-%d')}\",\n            content=report_content\n        )\n"}
{"type": "source_file", "path": "yourtube/async_worker.py", "content": "import asyncio\nimport logging\nfrom typing import Dict, List, Optional\nfrom queue import Queue\nimport threading\nimport time\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass VideoProcessingQueue:\n    def __init__(self):\n        self.queue = Queue()\n        self.processing_videos = set()  # Set of video_ids currently being processed\n        self.worker_thread = None\n        self.running = False\n        self.status_dict = {}  # Dictionary to store status of each video: 'queued', 'processing', 'completed', 'error'\n        self.queued_tasks = []  # List to track tasks in the queue\n\n    def start_worker(self, process_func):\n        \"\"\"Start the worker thread if not already running\"\"\"\n        if self.worker_thread is None or not self.worker_thread.is_alive():\n            self.running = True\n            self.worker_thread = threading.Thread(target=self._worker_loop, args=(process_func,))\n            self.worker_thread.daemon = True  # Make thread a daemon so it exits when main program exits\n            self.worker_thread.start()\n            logger.info(\"Video processing worker started\")\n\n    def stop_worker(self):\n        \"\"\"Stop the worker thread\"\"\"\n        self.running = False\n        if self.worker_thread and self.worker_thread.is_alive():\n            self.worker_thread.join(timeout=5)\n            logger.info(\"Video processing worker stopped\")\n\n    def _worker_loop(self, process_func):\n        \"\"\"Worker loop that processes videos from the queue\"\"\"\n        while self.running:\n            try:\n                if not self.queue.empty():\n                    # Get task from queue\n                    task = self.queue.get()\n                    video_id = task.get('video_id')\n                    \n                    # Remove from queued_tasks list\n                    self.queued_tasks = [t for t in self.queued_tasks if t.get('video_id') != video_id]\n                    \n                    # Update status\n                    self.status_dict[video_id] = 'processing'\n                    self.processing_videos.add(video_id)\n                    \n                    logger.info(f\"Processing video {video_id}\")\n                    \n                    try:\n                        # Process the video\n                        process_func(**task)\n                        # Update status on success\n                        self.status_dict[video_id] = 'completed'\n                    except Exception as e:\n                        # Update status on error\n                        self.status_dict[video_id] = 'error'\n                        logger.error(f\"Error processing video {video_id}: {str(e)}\")\n                    \n                    # Remove from processing set\n                    self.processing_videos.remove(video_id)\n                    self.queue.task_done()\n                else:\n                    # Sleep a bit to avoid busy waiting\n                    time.sleep(0.5)\n            except Exception as e:\n                logger.error(f\"Error in worker loop: {str(e)}\")\n                time.sleep(1)  # Sleep to avoid rapid error loops\n\n    def add_task(self, **kwargs):\n        \"\"\"Add a video processing task to the queue\"\"\"\n        video_id = kwargs.get('video_id')\n        if not video_id:\n            raise ValueError(\"video_id is required\")\n        \n        # Check if video is already in queue or being processed\n        if video_id in self.processing_videos or video_id in self.status_dict and self.status_dict[video_id] in ['queued', 'processing']:\n            logger.info(f\"Video {video_id} is already in queue or being processed\")\n            return False\n        \n        # Update is_last flag for previous tasks\n        for task in self.queued_tasks:\n            if task.get('is_last', False):\n                task['is_last'] = False\n        \n        # Add to queue with is_last=True\n        kwargs['is_last'] = True\n        self.queued_tasks.append(kwargs)\n        self.status_dict[video_id] = 'queued'\n        self.queue.put(kwargs)\n        logger.info(f\"Added video {video_id} to processing queue\")\n        return True\n\n    def get_status(self, video_id):\n        \"\"\"Get the status of a video\"\"\"\n        return self.status_dict.get(video_id, None)\n\n    def get_queue_size(self):\n        \"\"\"Get the current queue size\"\"\"\n        return self.queue.qsize()\n\n    def get_processing_count(self):\n        \"\"\"Get the number of videos currently being processed\"\"\"\n        return len(self.processing_videos)\n\n# Create a global instance of the queue\nvideo_queue = VideoProcessingQueue() "}
{"type": "source_file", "path": "yourtube/scanner.py", "content": "import os\nimport json\nfrom datetime import datetime\nfrom youtube.db_module import Video, Session\nfrom app.utils import get_file_path\n\ndef get_video_info_from_json(json_path):\n    \"\"\"Extract video information from JSON file.\"\"\"\n    try:\n        _ = open(json_path.replace('.info.json', '.zh.srt'), 'r')\n        language = 'zh'\n    except FileNotFoundError:\n        try:\n            _ = open(json_path.replace('.info.json', '.en.srt'), 'r')\n            language = 'en'\n        except FileNotFoundError:\n            language = None\n\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        # Validate required fields\n        video_id = data.get('id')\n        if not video_id:\n            return None\n        \n        info = {\n            'video_id': data.get('id'),\n            'title': data.get('title'),\n            'channel': data.get('channel'),\n            'channel_id': data.get('channel_id'),  # Added channel_id extraction\n            'language': language,  # Default to 'en' if not specified\n            'upload_date': data.get('upload_date')\n        }\n        return info\n    \n    except Exception:\n        print(f\"Error parsing JSON file {json_path}: {str(e)}\")\n        return None\n\n\n\ndef scan_downloads_folder(downloads_path, video_ids=[]):\n    \"\"\"\n    Scan downloads folder and update database.\n    If video_ids is provided, only scan those videos.\n    \"\"\"\n    session = Session()\n\n    stats = {\n        'new_videos': 0,\n        'updated_videos': 0,\n        'total_scanned': 0,\n        'errors': []\n    }\n\n    try:\n        # Look for JSON files first\n        json_files = [f for f in os.listdir(downloads_path) if f.endswith('.info.json')]\n\n        for json_file in json_files:\n            # If video_ids is provided, skip if not in list\n            if video_ids and json_file.replace('.info.json', '') not in video_ids:\n                continue\n            stats['total_scanned'] += 1\n            video_id = json_file.replace('.info.json', '')\n            json_path = os.path.join(downloads_path, json_file)\n\n\n            try:\n                # Get info from JSON\n                info = get_video_info_from_json(json_path)\n                if not info:\n                    stats['errors'].append(f\"Error reading JSON file {json_file}\")\n                    continue\n\n                # Get all associated files\n                files = get_associated_files(video_id, downloads_path)\n\n                # Check if video exists in database\n                video = session.query(Video).filter_by(video_id=video_id).first()\n\n                if video is None:\n                    # Create new video entry\n                    video = Video.from_info(info)\n                    session.add(video)\n                    stats['new_videos'] += 1\n                    print(\"new video\", video)\n                else:\n                    # Update existing video information\n                    video.update_info(info)\n                    print(\"updated video\", video)\n\n                # Update transcript and summary flags based on existing files\n                video.transcript = len(files['transcripts']) > 0\n                video.summary = len(files['summaries']) > 0\n\n            except Exception as e:\n                stats['errors'].append(f\"Error processing video {video_id}: {str(e)}\")\n                continue\n        \n        session.commit()\n\n    except Exception as e:\n        stats['errors'].append(f\"General error: {str(e)}\")\n        session.rollback()\n    \n    finally:\n        session.close()\n    \n    return stats"}
{"type": "source_file", "path": "yourtube/utils.py", "content": "import json\nimport re\nimport os\nimport shutil\nfrom webvtt import WebVTT\nimport litellm\nimport torch\nimport yt_dlp\n\ndef extract_youtube_id(url):\n    \"\"\"Extract video ID from a YouTube URL. It can be a short URL, long URL, or live URL.\n    Examples:\n    - Regular: https://www.youtube.com/watch?v=ABC123\n    - Short: https://youtu.be/ABC123\n    - Live: https://www.youtube.com/live/ABC123\"\"\"\n    patterns = [\n        r'(?:youtube\\.com\\/watch\\?v=|youtu\\.be\\/)([\\w-]+)',\n        r'(?:youtube\\.com\\/embed\\/)([\\w-]+)',\n        r'(?:youtube\\.com\\/v\\/)([\\w-]+)',\n        r'(?:youtube\\.com\\/live\\/)([\\w-]+)(?:\\?|$)'  # Added pattern for live URLs\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, url)\n        if match:\n            return match.group(1)\n    return None\n\n    \ndef get_llm_info(agent):\n    \"\"\"Get model from config.json\"\"\"\n    config = load_config()\n    agent_options = config.get(agent, {})\n    title = agent_options.get(\"model_title\")\n    max_tokens = agent_options.get(\"max_tokens\")\n    temperature = agent_options.get(\"temperature\")\n    models = config.get(\"model\", [])\n\n    # Find the model with matching title\n    for model in models:\n        if model.get(\"title\") == title:\n            return (\n                model.get(\"provider\"), \n                model.get(\"name\"), \n                model.get(\"api_key\"), \n                max_tokens,\n                temperature\n            )\n            \n    raise ValueError(f\"Model {title} not found in config.json\")\n\ndef get_device():\n    \"\"\"Get the device for running Whisper\"\"\"\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device = \"mps\"\n    if device in [\"cuda\", \"mps\"]:\n        print(f\"Device detected: {device}\")\n    return device\n\n\ndef load_config():\n    \"\"\"Load configuration from config.json\"\"\"\n    config_path = get_config_path()\n    try:\n        with open(config_path, 'r') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(\"config.json not found. Please create it based on the template.\")\n        shutil.copy(config_path+'.template', config_path)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON in config.json\")\n\ndef get_config_path():\n    \"\"\"Get the path to the config.json file\"\"\"\n    return os.path.join(os.path.dirname(__file__), 'config.json')\n\n\ndef convert_vtt_to_srt(vtt_path):\n    \"\"\"Convert VTT file to SRT using webvtt-py and clean the resulting SRT file.\n    Cleaning removes duplicate subtitle entries that have multiple lines of text.\"\"\"\n    # Get the output path by replacing .vtt with .srt\n    srt_path = vtt_path.replace('vtt', 'srt')\n    \n    # Convert VTT to SRT\n    vtt = WebVTT().read(vtt_path)\n    vtt.save_as_srt(srt_path)\n    \n    # Clean the SRT file to remove duplicate entries\n    clean_srt_file(srt_path, srt_path)\n    \n    # Optionally remove the VTT file\n    # os.remove(vtt_path)\n    \n    return srt_path\n\ndef sanitize_filename(filename):\n    \"\"\"Remove or replace invalid characters in filename.\"\"\"\n    # Characters that are not allowed in filenames\n    invalid_chars = '<>:\"/\\\\|?*'\n    \n    # Replace invalid characters with a safe character (underscore)\n    for char in invalid_chars:\n        filename = filename.replace(char, '_')\n    \n    return filename\n\ndef rename_title(video_title, config):\n    \n    try:\n        # Use LiteLLM to get response from the specified provider\n        response = litellm.completion(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\n                    \"role\": \"user\", \n                    \"content\": f\"Make a concise title out of this: {video_title}, no ':', or '|'\"\n                }\n            ],\n            api_key=config.get(\"openai\", {}).get(\"api_key\"),\n            max_tokens=4096,\n            temperature=0.7\n        )\n        title = response.choices[0].message.content.strip('\"').replace(\" \", \"_\")\n        print(title)\n    except Exception:\n        # title = video_title.strip('\"').replace(\" \", \"_\")     \n        raise\n\n    \n    return title\n\n\ndef get_download_dir(path=\"downloads/\"):\n    package_root = os.path.dirname(\n        os.path.abspath(__file__)\n    )\n    download_dir = os.path.join(package_root, path)\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n    return download_dir\n\n\ndef get_db_path(path=\"videos.db\"):\n    db_path = os.path.join(get_download_dir(), path)\n    return db_path\n\n\ndef download_youtube_video(\n        path=get_download_dir(),  # Default download path\n        video_id=None, # Video ID\n        format=\"worst\", # The video quality to download\n        video=False, # weather download the actual video file\n        json=True,\n        subtitles=True,\n        auto_subtitles=True,\n        langs=[\"en\", \"zh\"]\n    ):\n    \n    ydl_opts = {\n            'quiet': False,\n            'extract_flat': True,\n            'outtmpl': os.path.join(path, f'{video_id}.%(ext)s'),\n            'format': format,\n            'writeinfojson': json,\n            'writesubtitles': subtitles,\n            'writeautomaticsub': auto_subtitles,  # Enable auto-generated subtitles if manual ones aren't available'\n            'subtitlesformat': 'srt/vtt',\n            'subtitleslangs': langs,\n            'skip_download': not video\n        }\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        try:\n            info = ydl.extract_info(\n                url=f\"https://www.youtube.com/watch?v={video_id}\", \n                download=True\n            )\n        except yt_dlp.utils.DownloadError as e:\n            return None\n    \n    return info\n\ndef clean_srt_file(input_file, output_file):\n    \"\"\"\n    Clean an SRT file by keeping only subtitle entries with a single line of text.\n    Removes entries with two or more lines of text (the duplicates).\n    \n    Args:\n        input_file (str): Path to the input SRT file\n        output_file (str): Path to the output SRT file\n    \"\"\"\n    with open(input_file, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Split the content by empty lines to get individual subtitle blocks\n    subtitle_blocks = content.split('\\n\\n')\n    \n    # Keep only subtitle blocks with 4 lines (subtitle number, timestamp, one line of text, and an empty line)\n    cleaned_blocks = []\n    subtitle_count = 1\n    \n    for block in subtitle_blocks:\n        if not block.strip():  # Skip empty blocks\n            continue\n            \n        lines = block.split('\\n')\n        \n        # A standard single-line subtitle entry has 3 lines:\n        # 1. Subtitle number\n        # 2. Timestamp\n        # 3. One line of text\n        if len(lines) == 3:\n            # Replace the subtitle number with the new sequence\n            lines[0] = str(subtitle_count)\n            cleaned_blocks.append('\\n'.join(lines))\n            subtitle_count += 1\n    \n    # Join the blocks with empty lines and write to output file\n    cleaned_content = '\\n\\n'.join(cleaned_blocks)\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(cleaned_content)\n\ndef get_language(info, config=None):\n    \"\"\"Get the language of the video from the config and info_json\"\"\"\n    # if defined in config, return the language\n    channel_id = info.get(\"channel_id\", None)\n    channel_handle = info.get(\"uploader_id\", None)\n    channels_in_config = config.get(\"youtube\", {}).get(\"channels\", [])\n    for channel in channels_in_config:\n        if channel.get(\"channel_id\") == channel_id or channel.get(\"channel_handle\") == channel_handle:\n            return channel.get(\"language\")\n    # if not defined in config, get the language from the video info\n    if 'subtitles' in info and info['subtitles']: # get the language from subtitles information\n        language_codes = [lang_code for lang_code in info['subtitles']]\n        for lang_code in language_codes:\n            if lang_code.startswith('zh'):\n                language = 'zh'\n                break\n        else:\n            language = 'en'\n    else:\n            language = info.get(\"language\") # if auto, get the language from explicit language information\n            if language is None:\n                language = config.get(\"default_lang\")\n    return language\n\n#  Usage example:\nif __name__ == \"__main__\":\n    # driver = webdriver.Chrome()\n    # load_youtube_cookies(driver, \"/Users/zhuliang/Downloads/youtube_cookies.json\")\n    config = load_config()\n    with open(\"yourtube/downloads/GU1WRD_v3h8.info.json\", \"r\") as f:\n        info = json.load(f)\n    language = get_language(info=info, config=config)\n    print(language)"}
