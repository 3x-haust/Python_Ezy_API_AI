{"repo_info": {"repo_name": "MHA2MLA", "repo_owner": "JT-Ushio", "repo_url": "https://github.com/JT-Ushio/MHA2MLA"}}
{"type": "test_file", "path": "utils/test_load_tensor.py", "content": "import pickle\nimport torch\n\n# with open(\"qk_tensor_135M.pkl\", \"rb\") as fin:\n#     qk_tensor = pickle.load(fin)  # torch.Size([30, 9, 32])\n#     qk_tensor = qk_tensor.view(30, 3, 3, 32).sum(dim=2)  # torch.Size([30, 3, 32])\n\n#     print(torch.any(qk_tensor < 0))\n#     topk_indices = torch.topk(input=qk_tensor[0], k=8, dim=1)[1]\n#     mask_matrix = torch.zeros_like(qk_tensor[0])\n#     mask_matrix.scatter_(1, topk_indices, 1)\n#     # print(qk_tensor[0][0])\n#     # print(mask_matrix[0][0])\n#     for x1, x2 in zip(qk_tensor[0][0], mask_matrix[0]):\n#         print(x1, x2)\n\n\nwith open(\"qk_tensor_135M.pth\", \"rb\") as fin:\n    qk_tensor = torch.load(fin)  # torch.Size([30, 9, 32])\n    qk_tensor = qk_tensor.view(30, 3, 3, 32).sum(dim=2)  # torch.Size([30, 3, 32])\n\n    print(torch.any(qk_tensor < 0))\n    topk_indices = torch.topk(input=qk_tensor[0], k=8, dim=1)[1]\n    mask_matrix = torch.zeros_like(qk_tensor[0])\n    mask_matrix.scatter_(1, topk_indices, 1)\n    # print(qk_tensor[0][0])\n    # print(mask_matrix[0][0])\n    for x1, x2 in zip(qk_tensor[0][0], mask_matrix[0]):\n        print(x1, x2)"}
{"type": "source_file", "path": "src/auto_encoder/patch_func_hf.py", "content": "from typing import Optional, Tuple\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport copy\nimport sys\n\n\nimport math\nfrom transformers.models.llama.modeling_llama import (\n    LlamaConfig,\n    logger,\n    LlamaRotaryEmbedding,\n    repeat_kv,\n)\nfrom transformers.models.llama import modeling_llama\nfrom transformers.cache_utils import Cache, StaticCache\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\nfrom transformers.utils import is_flash_attn_greater_or_equal_2_10\n\n\n\nfrom .utils import apply_activation\n\n\nclass AutoEncoderV1(nn.Module):\n    # Low-rank decomposition of k_nope and v without sharing cache.\n\n    def __init__(\n        self, config: LlamaConfig, layer_idx: Optional[int], nope_mask: torch.Tensor\n    ):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.W_down_k = nn.Linear(\n            nope_mask.sum().item(),\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n        )\n        self.W_down_v = nn.Linear(\n            config.num_key_value_heads * config.head_dim,\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n        )\n        self.W_up_k = nn.Linear(\n            self.W_down_k.out_features,\n            nope_mask.sum().item(),\n            bias=False,\n        )\n        self.W_up_v = nn.Linear(\n            self.W_down_v.out_features,\n            config.num_key_value_heads * config.head_dim,\n            bias=False,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, num_key_value_heads* head_dim]\n        cache_kwargs: dict = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, num_key_value_heads* head_dim]\n        bsz, q_len, _ = k_nope.size()\n        c_k = apply_activation(self.W_down_k(k_nope), self.config.AE[\"activation_fn\"])\n        c_v = apply_activation(\n            self.W_down_v(value_states), self.config.AE[\"activation_fn\"]\n        )\n        if past_key_value is not None:\n            # change shape for cache(Cache will cat the input on the -2 dim which is the q_len)\n            rope_dim = k_r.size(-1)\n            k_r = k_r.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                1, 2\n            )\n            c_kv = torch.cat(\n                [\n                    c_k.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                        1, 2\n                    ),\n                    c_v.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                        1, 2\n                    ),\n                ],\n                dim=-1,\n            )\n            k_r, c_kv = past_key_value.update(k_r, c_kv, self.layer_idx, cache_kwargs)\n            k_r = k_r.transpose(1, 2).reshape(bsz, -1, rope_dim)\n            c_k, c_v = c_kv.split(\n                [\n                    self.config.AE[\"low_rank\"],\n                    self.config.AE[\"low_rank\"],\n                ],\n                dim=-1,\n            )\n            c_k = c_k.transpose(1, 2).reshape(\n                bsz, -1, self.config.AE[\"low_rank\"] * self.config.num_key_value_heads\n            )\n            c_v = c_v.transpose(1, 2).reshape(\n                bsz, -1, self.config.AE[\"low_rank\"] * self.config.num_key_value_heads\n            )\n        k_c = self.W_up_k(c_k)\n        value_states = self.W_up_v(c_v)\n        key_states = torch.zeros(\n            bsz,\n            k_c.size(1),\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nclass AutoEncoderV2(nn.Module):\n    # Low-rank decomposition of k_nope and v with shared cache.\n\n    def __init__(\n        self, config: LlamaConfig, layer_idx: Optional[int], nope_mask: torch.Tensor\n    ):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.W_down_k = nn.Linear(\n            nope_mask.sum().item() + config.num_key_value_heads * config.head_dim,\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n        )\n        self.W_up_k = nn.Linear(\n            self.W_down_k.out_features,\n            nope_mask.sum().item() + config.num_key_value_heads * config.head_dim,\n            bias=False,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, num_key_value_heads* head_dim]\n        cache_kwargs: dict = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, num_key_value_heads* head_dim]\n        bsz, q_len, _ = k_nope.size()\n        kv = torch.cat([k_nope, value_states], dim=-1)\n        c_kv = apply_activation(self.W_down_k(kv), self.config.AE[\"activation_fn\"])\n        if past_key_value is not None:\n            # change shape for cache(Cache will cat the input on the -2 dim which is the q_len)\n            rope_dim = k_r.size(-1)\n            k_r = k_r.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                1, 2\n            )\n            c_kv = c_kv.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                1, 2\n            )\n            k_r, c_kv = past_key_value.update(k_r, c_kv, self.layer_idx, cache_kwargs)\n            k_r = k_r.transpose(1, 2).reshape(bsz, -1, rope_dim)\n            c_kv = c_kv.transpose(1, 2).reshape(\n                bsz, -1, self.config.AE[\"low_rank\"] * self.config.num_key_value_heads\n            )\n        kv = self.W_up_k(c_kv)\n        k_c, value_states = kv.split(\n            [\n                self.nope_mask.sum().item(),\n                self.config.num_key_value_heads * self.config.head_dim,\n            ],\n            dim=-1,\n        )\n        key_states = torch.zeros(\n            bsz,\n            k_c.size(1),\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nclass AutoEncoderV3(nn.Module):\n    # Low-rank decomposition of k_nope and v with shared cache. The difference from v2 is that W_down_k and W_up_k are specific to individual heads.\n\n    def __init__(\n        self, config: LlamaConfig, layer_idx: Optional[int], nope_mask: torch.Tensor\n    ):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.W_down_k = nn.Linear(\n            nope_mask.sum().item() // config.num_key_value_heads + config.head_dim,\n            config.AE[\"low_rank\"],\n            bias=False,\n        )\n        self.W_up_k = nn.Linear(\n            self.W_down_k.out_features,\n            self.W_down_k.in_features,\n            bias=False,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, num_key_value_heads* head_dim]\n        cache_kwargs: dict = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, num_key_value_heads* head_dim]\n        bsz, q_len, _ = k_nope.size()\n        # kv = torch.cat([k_nope,value_states],dim=-1)\n        kv = torch.cat(\n            [\n                k_nope.view(bsz, q_len, self.config.num_key_value_heads, -1),\n                value_states.view(\n                    bsz, q_len, self.config.num_key_value_heads, self.config.head_dim\n                ),\n            ],\n            dim=-1,\n        )\n        c_kv = apply_activation(self.W_down_k(kv), self.config.AE[\"activation_fn\"])\n        if past_key_value is not None:\n            # change shape for cache(Cache will cat the input on the -2 dim which is the q_len)\n            rope_dim = k_r.size(-1)\n            k_r = k_r.view(bsz, q_len, self.config.num_key_value_heads, -1).transpose(\n                1, 2\n            )\n            c_kv = c_kv.transpose(1, 2)\n            k_r, c_kv = past_key_value.update(k_r, c_kv, self.layer_idx, cache_kwargs)\n            k_r = k_r.transpose(1, 2).reshape(bsz, -1, rope_dim)\n            c_kv = c_kv.transpose(1, 2)\n        kv = self.W_up_k(c_kv)\n        k_c, value_states = kv.split(\n            [\n                self.nope_mask.sum().item()//self.config.num_key_value_heads,\n                self.config.head_dim,\n            ],\n            dim=-1,\n        )\n        k_c = k_c.reshape(bsz, -1, self.nope_mask.sum().item())\n        value_states = value_states.reshape(\n            bsz, -1, self.config.num_key_value_heads * self.config.head_dim\n        )\n        key_states = torch.zeros(\n            bsz,\n            k_c.size(1),\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nAUTO_ENCODER_VERSION_MAP = {\n    1: AutoEncoderV1,\n    2: AutoEncoderV2,\n    3: AutoEncoderV3,\n}\n\n\nclass CustomLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.nope_mask = IndexForNope.get_index_for_nope(\n            config.RoPE,\n            head_dim=self.head_dim,\n            head_num=self.num_key_value_heads,\n            layer_idx=layer_idx,\n        )\n        self.low_rank = config.AE[\"low_rank\"]\n\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.k_proj = nn.Linear(\n            self.hidden_size,\n            self.nope_mask.sum().item(),\n            bias=config.attention_bias,\n        )\n        self.v_proj = nn.Linear(\n            self.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.W_k_r = nn.Linear(\n            self.hidden_size,\n            (self.nope_mask == False).sum().item(),\n            bias=config.attention_bias,\n        )\n        self.o_proj = nn.Linear(\n            self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias\n        )\n        self.auto_encoder = AUTO_ENCODER_VERSION_MAP[config.AE[\"version\"]](\n            config, layer_idx, self.nope_mask\n        )\n\n        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def get_qkv_states(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ):\n        bsz, q_len, _ = hidden_states.size()\n        assert not self.config.pretraining_tp > 1, \"not support pretraining_tp>1\"\n        # prepare query_states and k_r\n        query_states = self.q_proj(hidden_states)\n        key_states = torch.zeros(\n            (bsz, q_len, self.num_key_value_heads * self.head_dim),\n            device=hidden_states.device,\n            dtype=hidden_states.dtype,\n        )\n        k_r = self.W_k_r(hidden_states)\n        key_states[..., ~self.nope_mask] = k_r\n\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        if position_embeddings is None:\n            logger.warning_once(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(key_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n        query_states, key_states = self.apply_custom_rotary_pos_emb(\n            query_states, key_states, cos, sin\n        )\n        key_states = key_states.transpose(1, 2).reshape(bsz, q_len, -1)\n        k_r = key_states[..., ~self.nope_mask]\n        k_nope = self.k_proj(hidden_states)\n        key_states, value_states = self.auto_encoder(\n            k_r=k_r,  # [bsz, q_len, rope_dim]\n            k_nope=k_nope,  # [bsz, q_len, nope_dim]\n            value_states=self.v_proj(\n                hidden_states\n            ),  # [bsz, q_len, num_key_value_heads* head_dim]\n            cache_kwargs={\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position},\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            position_embeddings=position_embeddings,  # will become mandatory in v4.46\n            **kwargs,\n        )  # [bsz, seq_len, num_key_value_heads* head_dim]\n\n        key_states = key_states.view(\n            bsz, -1, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, -1, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        return query_states, key_states, value_states\n\n    def apply_custom_rotary_pos_emb(self, query_states, key_states, cos, sin):\n        if self.config.RoPE[\"partial_rope_version\"] == 4:\n            query_states, key_states = modeling_llama.apply_rotary_pos_emb(\n                query_states, key_states, cos, sin, layer_idx=self.layer_idx\n            )\n        else:\n            query_states, key_states = modeling_llama.apply_rotary_pos_emb(\n                query_states, key_states, cos, sin\n            )\n        return query_states, key_states\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.attention_dropout, training=self.training\n        )\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, -1)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(\n                self.hidden_size // self.config.pretraining_tp, dim=2\n            )\n            o_proj_slices = self.o_proj.weight.split(\n                self.hidden_size // self.config.pretraining_tp, dim=1\n            )\n            attn_output = sum(\n                [\n                    F.linear(attn_output[i], o_proj_slices[i])\n                    for i in range(self.config.pretraining_tp)\n                ]\n            )\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass CustomLlamaFlashAttention2(CustomLlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n        )\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n        attn_output = _flash_attention_forward(\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            q_len,\n            position_ids=position_ids,\n            dropout=dropout_rate,\n            sliding_window=getattr(self, \"sliding_window\", None),\n            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n            is_causal=self.is_causal,\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass CustomLlamaSdpaAttention(CustomLlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, -1)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ndef state_dict_svd_init(model, state_dict):\n    for layer_idx in range(model.config.num_hidden_layers):\n        nope_mask = IndexForNope.get_index_for_nope(\n            rope_cfg=model.config.RoPE,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            head_num=model.config.num_key_value_heads,\n            layer_idx=layer_idx,\n        )\n        assert (nope_mask == model.model.layers[layer_idx].self_attn.nope_mask).all()\n        W_k = state_dict.pop(f\"model.layers.{layer_idx}.self_attn.k_proj.weight\").t()\n        W_v = state_dict.pop(f\"model.layers.{layer_idx}.self_attn.v_proj.weight\").t()\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_k_r.weight\"] = W_k[\n            ..., ~nope_mask\n        ].t()\n        W_down_k, W_up_k, W_down_v, W_up_v = SvdInit.init(\n            W_k[..., nope_mask],\n            W_v,\n            svd_method=model.config.SVD[\"method\"],\n            r=model.config.SVD[\"low_rank\"] * model.config.num_key_value_heads,\n        )\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_down_k.weight\"] = W_down_k\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_up_k.weight\"] = W_up_k\n        if not model.model.layers[layer_idx].self_attn.is_share_W_down:\n            state_dict[f\"model.layers.{layer_idx}.self_attn.W_down_v.weight\"] = W_down_v\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_up_v.weight\"] = W_up_v\n    return state_dict\n\n\n@classmethod\ndef custom_load_pretrained_model(\n    cls,\n    model,\n    state_dict,\n    loaded_keys,\n    resolved_archive_file,\n    pretrained_model_name_or_path,\n    ignore_mismatched_sizes=False,\n    sharded_metadata=None,\n    _fast_init=True,\n    low_cpu_mem_usage=False,\n    device_map=None,\n    offload_folder=None,\n    offload_state_dict=None,\n    dtype=None,\n    hf_quantizer=None,\n    keep_in_fp32_modules=None,\n    gguf_path=None,\n    weights_only=True,\n):\n    if all([\"W_k_r\" not in k for k in state_dict.keys()]) and isinstance(\n        model.model, modeling_llama.LlamaPreTrainedModel\n    ):\n        # replace the original llama weights with the mla weights\n        state_dict = state_dict_svd_init(model, state_dict)\n        loaded_keys = list(state_dict.keys())\n    old_k_r_weight = model.model.layers[0].self_attn.W_k_r.weight\n    outputs = cls.original_load_pretrained_model(\n        model,\n        state_dict,\n        loaded_keys,\n        resolved_archive_file,\n        pretrained_model_name_or_path,\n        ignore_mismatched_sizes,\n        sharded_metadata,\n        _fast_init,\n        low_cpu_mem_usage,\n        device_map,\n        offload_folder,\n        offload_state_dict,\n        dtype,\n        hf_quantizer,\n        keep_in_fp32_modules,\n        gguf_path,\n        weights_only,\n    )\n    new_k_r_weight = model.model.layers[0].self_attn.W_k_r.weight\n    assert not (old_k_r_weight == new_k_r_weight).all()\n    return outputs\n\n\ndef ae_patch_func_hf(rope_cfg=None):\n    modeling_llama.LLAMA_ATTENTION_CLASSES = {\n        \"eager\": CustomLlamaAttention,\n        \"flash_attention_2\": CustomLlamaFlashAttention2,\n        \"sdpa\": CustomLlamaSdpaAttention,\n    }\n\n    # if not hasattr(modeling_llama.LlamaPreTrainedModel, 'original_load_pretrained_model'):\n    #     modeling_llama.LlamaPreTrainedModel.original_load_pretrained_model = modeling_llama.LlamaPreTrainedModel._load_pretrained_model\n    #     modeling_llama.LlamaPreTrainedModel._load_pretrained_model = custom_load_pretrained_model\n\n    if rope_cfg is not None:\n        # replace apply_rotary_pos_emb function in llama model\n        from ..partial_rope.patch_func_hf import create_custom_apply_rotary_pos_emb_hf\n\n        modeling_llama.apply_rotary_pos_emb = create_custom_apply_rotary_pos_emb_hf(\n            rope_cfg\n        )\n"}
{"type": "source_file", "path": "src/auto_encoder/merge.py", "content": "from pathlib import Path\nfrom transformers import AutoModelForCausalLM,LlamaForCausalLM\nimport torch\n\nimport os\nimport dataclasses\nimport nanotron\nimport argparse\nfrom nanotron.config import LlamaConfig as NanotronLlamaConfig\n\nfrom ..conversation.convert_weights import load_nanotron_model\nfrom ..conversation.convert_nanotron_to_hf import get_hf_config,convert_nt_to_hf\nfrom ..conversation.convert_hf_to_nanotron import convert_hf_to_nt\nfrom .init import AttnForTraing\n\ndef merge(original_model_path:str,ae_model_path:str,save_path:str):\n    from .patch_func_hf import ae_patch_func_hf\n    from .patch_func_nt import ae_patch_func_nt,CustomLlamaConfig\n    import json\n    with open(os.path.join(ae_model_path,\"config.json\"),\"r\") as f:\n        config = json.load(f)\n    ae_patch_func_hf(config[\"RoPE\"])\n    ae_patch_func_nt(config[\"RoPE\"])\n    globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    # Load from nt ckpt\n    with open(os.path.join(original_model_path , \"model_config.json\"), \"r\") as f:\n        attrs = json.load(f)\n        model_config = NanotronLlamaConfig(**attrs)\n    setattr(model_config,\"RoPE\",config[\"RoPE\"])\n    setattr(model_config,\"AE\",config[\"AE\"])\n    original_model_nt = load_nanotron_model(\n        model_config=model_config,\n        checkpoint_path=Path(original_model_path),\n    )\n    # Init hf model\n    model_config_hf = get_hf_config(model_config)\n    original_model_hf = LlamaForCausalLM._from_config(model_config_hf)\n    convert_nt_to_hf(original_model_nt, original_model_hf, model_config)\n    # Load ae model\n    model = AttnForTraing.from_pretrained(ae_model_path)\n    with torch.no_grad():\n        for layer_idx, layer in enumerate(original_model_hf.model.layers):\n            attn = model.model[layer_idx]\n            layer.self_attn.auto_encoder.W_down_k.weight.data[:] = attn.auto_encoder.W_down_k.weight.detach()\n            layer.self_attn.auto_encoder.W_up_k.weight.data[:] = attn.auto_encoder.W_up_k.weight.detach()\n            assert hasattr(layer.self_attn.auto_encoder,\"W_down_v\") == hasattr(attn.auto_encoder,\"W_down_v\")\n            if hasattr(layer.self_attn.auto_encoder,\"W_down_v\"):\n                layer.self_attn.auto_encoder.W_down_v.weight.data[:] = attn.auto_encoder.W_down_v.weight.detach()\n                layer.self_attn.auto_encoder.W_up_v.weight.data[:] = attn.auto_encoder.W_up_v.weight.detach()\n    del model\n    # convert hf to nt\n    convert_hf_to_nt(original_model_hf, original_model_nt, model_config)\n    del original_model_hf\n    # save nt model\n    parallel_context = nanotron.parallel.ParallelContext(\n        data_parallel_size=1, pipeline_parallel_size=1, tensor_parallel_size=1\n    )\n    nanotron.serialize.save_weights(model=original_model_nt, parallel_context=parallel_context, root_folder=Path(save_path))\n    with open(Path(save_path) / \"model_config.json\", \"w+\") as f:\n        json.dump(dataclasses.asdict(model_config), f)\n    print(f\"Model saved to {save_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--original_model_path\",type=str,required=True)\n    parser.add_argument(\"--ae_model_path\",type=str,required=True)\n    parser.add_argument(\"--save_path\",type=str,required=True)\n    args = parser.parse_args()\n    merge(args.original_model_path,args.ae_model_path,args.save_path)\n\nif __name__ ==\"__main__\":\n    main()\n"}
{"type": "source_file", "path": "scripts/log_lighteval_to_wandb.py", "content": "# copied from https://github.com/huggingface/nanotron/blob/main/scripts/log_lighteval_to_wandb.py\n\"\"\"\nThis script use to log evaluation results to wandb.\n\npython3 log_eval_results_to_wandb.py --eval-path /path/to/eval/results --wandb-project project_name --wandb-name run_name\n\nThe folder that contains the evaluation results should have the following structure:\n- 5000:\n    results_x.json # where x is the ligheval's evaluation number\n- 10000:\n    ...\n...\n\"\"\"\nimport argparse\nimport json\nimport os\nimport re\nfrom pathlib import Path\n\nimport wandb\n\n\ndef run(current_path: Path):\n    def compute_avg_acc_of_a_benchmark(data, benchmark_prefix):\n        sum_acc, sum_acc_norm, sum_acc_stderr, sum_acc_norm_stderr, count = 0, 0, 0, 0, 0\n        for key, values in data.items():\n            if f\"{benchmark_prefix}:\" in key or f\"{benchmark_prefix}_\" in key or f\"{benchmark_prefix}\" in key:\n                # sum_acc += values[\"acc\"]\n                sum_acc_norm += values[\"acc_norm\"]\n                # sum_acc_stderr += values[\"acc_stderr\"]\n                sum_acc_norm_stderr += values[\"acc_norm_stderr\"]\n                count += 1\n\n        average_acc_norm = sum_acc_norm / count if count else 0\n        return average_acc_norm\n\n    def compute_avg_acc_of_all_tasks(data):\n        sum_acc_norm, count = 0, 0\n        for _, value in data.items():\n            sum_acc_norm += value\n            count += 1\n\n        average_acc_norm = sum_acc_norm / count if count else 0\n        return average_acc_norm\n\n    def precess_path(path:str):\n        if path.endswith(\"_hf\"):\n            path=path[:-3]\n        match = re.search(r\"(\\d+)$\", path)\n        if not match:\n            return 0\n        number = match.group(1)\n        return int(number)\n\n\n    list_checkpoints = os.listdir(current_path)\n    sorted_list_checkpoints = sorted(list_checkpoints, key=precess_path)\n\n    for item in sorted_list_checkpoints:\n        item_path = os.path.join(current_path, item)\n        if os.path.isdir(item_path):\n            json_files = [f for f in os.listdir(item_path) if f.endswith(\".json\")]\n            if len(json_files) == 1:\n                json_file_path = os.path.join(item_path, json_files[0])\n\n                with open(json_file_path, \"r\") as file:\n                    eval_data = json.load(file)\n                    iteration_step = precess_path(item)\n                    # consumed_train_samples = eval_data[\"config_general\"][\"config\"][\"general\"][\"consumed_train_samples\"]\n\n                    logging_results = {}\n                    eval_data[\"results\"] = {\n                        (k if \"|\" not in k else k.split(\"|\")[1]): v\n                        for k, v in eval_data[\"results\"].items()\n                    }\n                    for name, data in eval_data[\"results\"].items():\n                        if name.startswith(\"mmlu\") or name.startswith(\"arc\") or name.startswith(\"all\"):\n                            continue\n                        if \"acc_norm\" in data:\n                            logging_results[f\"{name}_acc_norm\"] = data[\"acc_norm\"]\n                        else:\n                            logging_results[f\"{name}_acc_norm\"] = data[\"qem\"]\n\n                    logging_results[\"mmlu:average_acc_norm\"] = compute_avg_acc_of_a_benchmark(eval_data[\"results\"], \"mmlu\")\n                    logging_results[\"arc:average_acc_norm\"] = compute_avg_acc_of_a_benchmark(eval_data[\"results\"], \"arc\")\n                    logging_results[\"all:average_acc_norm\"] = compute_avg_acc_of_all_tasks(logging_results)\n                    print(logging_results)\n\n                    wandb.log(\n                        {\n                            **logging_results,\n                            \"iteration_step\": iteration_step,\n                            # \"consumed_train_samples\": consumed_train_samples,\n                        },\n                        step=iteration_step,\n                    )\n\n            elif len(json_files) > 1:\n                print(f\"More than one JSON file found in {item_path}. Skipping.\")\n            else:\n                print(f\"No JSON file found in {item_path}.\")\n\n        print(f\"Checkpoint {item} is done. /n\")\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eval-path\", type=str, required=True, help=\"Path of the lighteval's evaluation results\")\n    parser.add_argument(\n        \"--wandb-project\", type=str, help=\"Path of the lighteval's evaluation results\", default=\"nanotron_evals\"\n    )\n    parser.add_argument(\n        \"--wandb-name\",\n        type=str,\n        required=True,\n        help=\"Path of the lighteval's evaluation results\",\n        default=\"sanity_evals\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    eval_path = args.eval_path\n    wandb_project = args.wandb_project\n    wandb_name = args.wandb_name\n\n    wandb.init(\n        project=wandb_project,\n        name=wandb_name,\n        config={\"eval_path\": eval_path},\n    )\n\n    run(eval_path)\n"}
{"type": "source_file", "path": "src/auto_encoder/utils.py", "content": "import torch\nimport torch.nn.functional as F\n\ndef apply_activation(x:torch.Tensor, activation_fn:str):\n    if activation_fn is not None:\n        activation_fn = activation_fn.lower()\n    if activation_fn is None:\n        return x\n    elif activation_fn == \"relu\":\n        return F.relu(x)\n    elif activation_fn == \"sigmoid\":\n        return torch.sigmoid(x)\n    elif activation_fn == \"tanh\":\n        return torch.tanh(x)\n    elif activation_fn == \"leaky_relu\":\n        return F.leaky_relu(x)\n    elif activation_fn == \"softmax\":\n        return F.softmax(x, dim=-1)\n    elif activation_fn == \"silu\" or activation_fn == \"swish\":\n        return F.silu(x)\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation_fn}\")"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/auto_encoder/eval.py", "content": "import argparse\nimport os\nfrom dataclasses import asdict\nfrom pprint import pformat\n\nfrom lighteval.parsers import parser_accelerate, parser_baseline, parser_nanotron, parser_utils_tasks\nfrom lighteval.tasks.registry import Registry, taskinfo_selector\nimport yaml\n\n\nCACHE_DIR = os.getenv(\"HF_HOME\")\n\n\n# copied from lighteval.main\ndef cli_evaluate():  # noqa: C901\n    parser = argparse.ArgumentParser(description=\"CLI tool for lighteval, a lightweight framework for LLM evaluation\")\n    parser.add_argument(\n        \"--cfg_RoPE\",\n        type=str,\n        required=True,\n        help=\"Path to the RoPE configuration file. This file should contain the RoPE configuration.\",\n    )\n\n    subparsers = parser.add_subparsers(help=\"help for subcommand\", dest=\"subcommand\")\n\n    # Subparser for the \"accelerate\" command\n    parser_a = subparsers.add_parser(\"accelerate\", help=\"use accelerate and transformers as backend for evaluation.\")\n    parser_accelerate(parser_a)\n\n    # Subparser for the \"nanotron\" command\n    parser_b = subparsers.add_parser(\"nanotron\", help=\"use nanotron as backend for evaluation.\")\n    parser_nanotron(parser_b)\n\n    parser_c = subparsers.add_parser(\"baseline\", help=\"compute baseline for a task\")\n    parser_baseline(parser_c)\n\n    # Subparser for task utils functions\n    parser_d = subparsers.add_parser(\"tasks\", help=\"display information about available tasks and samples.\")\n    parser_utils_tasks(parser_d)\n\n    args = parser.parse_args()\n\n    # Monkey patching for the partial rope\n    from .patch_func_hf import ae_patch_func_hf\n    with open(args.cfg_RoPE,\"r\") as f:\n        cfg_RoPE = yaml.load(f, Loader=yaml.FullLoader)\n        cfg_RoPE=cfg_RoPE[\"model\"][\"model_config\"][\"RoPE\"]\n    ae_patch_func_hf(cfg_RoPE)\n\n    if args.subcommand == \"accelerate\":\n        from lighteval.main_accelerate import main as main_accelerate\n\n        main_accelerate(args)\n\n    elif args.subcommand == \"nanotron\":\n        from lighteval.main_nanotron import main as main_nanotron\n\n        main_nanotron(args.checkpoint_config_path, args.lighteval_config_path, args.cache_dir)\n\n    elif args.subcommand == \"baseline\":\n        from lighteval.main_baseline import main as main_baseline\n\n        main_baseline(args)\n\n    elif args.subcommand == \"tasks\":\n        registry = Registry(cache_dir=args.cache_dir, custom_tasks=args.custom_tasks)\n        if args.list:\n            registry.print_all_tasks()\n\n        if args.inspect:\n            print(f\"Loading the tasks dataset to cache folder: {args.cache_dir}\")\n            print(\n                \"All examples will be displayed without few shot, as few shot sample construction requires loading a model and using its tokenizer. \"\n            )\n            # Loading task\n            task_names_list, _ = taskinfo_selector(args.inspect, task_registry=registry)\n            task_dict = registry.get_task_dict(task_names_list)\n            for name, task in task_dict.items():\n                print(\"-\" * 10, name, \"-\" * 10)\n                if args.show_config:\n                    print(\"-\" * 10, \"CONFIG\")\n                    task.cfg.print()\n                for ix, sample in enumerate(task.eval_docs()[: int(args.num_samples)]):\n                    if ix == 0:\n                        print(\"-\" * 10, \"SAMPLES\")\n                    print(f\"-- sample {ix} --\")\n                    print(pformat(asdict(sample), indent=1))\n    else:\n        print(\"You did not provide any argument. Exiting\")\n\n\nif __name__ == \"__main__\":\n    cli_evaluate()\n"}
{"type": "source_file", "path": "src/auto_encoder/patch_func_nt.py", "content": "from typing import Any, Dict, Optional, Tuple\nimport nanotron.config\nimport nanotron.config\nimport nanotron.config.config\nimport nanotron.config.models_config\nimport nanotron.trainer\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom packaging.version import Version\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nimport nanotron\nfrom nanotron.s3_checkpoints import check_path_is_local\nfrom safetensors.torch import safe_open\nfrom tqdm import tqdm\nfrom nanotron.config import (\n    ParallelismArgs,\n    LlamaConfig,\n    ExistingCheckpointInit,\n    RandomInit,\n    SpectralMupInit,\n)\nfrom nanotron.generation.generate_store import AttachableStore\nfrom torch.nn.parallel import DistributedDataParallel\nfrom nanotron.constants import CHECKPOINT_VERSION\nfrom nanotron.logging import log_rank\nfrom nanotron import logging\nfrom nanotron.distributed import get_global_rank\nfrom nanotron.parallel.tensor_parallel.nn import (\n    TensorParallelColumnLinear,\n    TensorParallelLinearMode,\n    TensorParallelRowLinear,\n)\nfrom nanotron.models import NanotronModel\nfrom nanotron.models import llama\nfrom nanotron.models.llama import (\n    CoreAttention,\n    logger,\n)\nfrom nanotron.parallel.tied_parameters import get_tied_id_to_param\nfrom nanotron.parallel.parameters import NanotronParameter\nfrom nanotron.serialize import weights as nt_weights\nfrom nanotron.serialize.weights import (\n    get_checkpoint_version,\n    read_checkpoint_version_from_shard_file,\n    CheckpointVersionFromShardFileException,\n    load_sharded_param_latest,\n)\nfrom nanotron.parallel import ParallelContext\nfrom nanotron.serialize.utils import (\n    ObjectType,\n    get_exp_tp_pp_rank_and_size_from,\n    get_path,\n)\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\n\nfrom ..mla.NopeIndex import IndexForNope\nfrom ..mla.svd_low_rank import SvdInit\nfrom .utils import apply_activation\n\n\n@dataclass\nclass CustomLlamaConfig(LlamaConfig):\n    RoPE: Dict = field(default_factory=dict)\n    AE: Dict = field(default_factory=dict)\n\n\n@dataclass\nclass CustomModelArgs(nanotron.config.config.ModelArgs):\n    model_config: CustomLlamaConfig\n\n\n@dataclass\nclass CustomConfig(nanotron.config.config.Config):\n    \"\"\"Main configuration class\"\"\"\n\n    model: CustomModelArgs\n\n\nclass AutoEncoderV1(nn.Module):\n    # Low-rank decomposition of k_nope and v without sharing cache.\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        parallel_config: Optional[ParallelismArgs],\n        tp_pg: dist.ProcessGroup,\n        layer_idx: Optional[int],\n        nope_mask: torch.Tensor,\n    ):\n        super().__init__()\n        self.config = config\n        self.parallel_config = parallel_config\n        self.tp_pg = tp_pg\n        self.tp_mode = (\n            self.parallel_config.tp_mode\n            if self.parallel_config is not None\n            else TensorParallelLinearMode.ALL_REDUCE\n        )\n        self.tp_linear_async_communication = (\n            self.parallel_config.tp_linear_async_communication\n            if self.parallel_config is not None\n            else False\n        )\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.d_qk = config.hidden_size // config.num_attention_heads\n        self.d_v = config.hidden_size // config.num_attention_heads\n        self.W_down_k = TensorParallelColumnLinear(\n            nope_mask.sum().item(),\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_down_v = TensorParallelColumnLinear(\n            config.num_key_value_heads * self.d_v,\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_up_k = TensorParallelColumnLinear(\n            self.W_down_k.out_features,\n            nope_mask.sum().item(),\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_up_v = TensorParallelColumnLinear(\n            self.W_down_v.out_features,\n            config.num_key_value_heads * self.d_v,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, n_local_kv_heads* d_v]\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, n_local_kv_heads* d_v]\n        bsz, q_len, _ = k_nope.size()\n        c_k = apply_activation(self.W_down_k(k_nope), self.config.AE[\"activation_fn\"])\n        c_v = apply_activation(\n            self.W_down_v(value_states), self.config.AE[\"activation_fn\"]\n        )\n        k_c = self.W_up_k(c_k)\n        value_states = self.W_up_v(c_v)\n        key_states = torch.zeros(\n            bsz,\n            q_len,\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nclass AutoEncoderV2(nn.Module):\n    # Low-rank decomposition of k_nope and v with shared cache.\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        parallel_config: Optional[ParallelismArgs],\n        tp_pg: dist.ProcessGroup,\n        layer_idx: Optional[int],\n        nope_mask: torch.Tensor,\n    ):\n        super().__init__()\n        self.config = config\n        self.parallel_config = parallel_config\n        self.tp_pg = tp_pg\n        self.tp_mode = (\n            self.parallel_config.tp_mode\n            if self.parallel_config is not None\n            else TensorParallelLinearMode.ALL_REDUCE\n        )\n        self.tp_linear_async_communication = (\n            self.parallel_config.tp_linear_async_communication\n            if self.parallel_config is not None\n            else False\n        )\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.d_qk = config.hidden_size // config.num_attention_heads\n        self.d_v = config.hidden_size // config.num_attention_heads\n        self.W_down_k = TensorParallelColumnLinear(\n            nope_mask.sum().item() + config.num_key_value_heads * self.d_v,\n            config.AE[\"low_rank\"] * config.num_key_value_heads,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_up_k = TensorParallelColumnLinear(\n            self.W_down_k.out_features,\n            nope_mask.sum().item() + config.num_key_value_heads * self.d_v,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, n_local_kv_heads* d_v]\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, n_local_kv_heads* d_v]\n        bsz, q_len, _ = k_nope.size()\n        kv = torch.cat([k_nope, value_states], dim=-1)\n        c_kv = apply_activation(self.W_down_k(kv), self.config.AE[\"activation_fn\"])\n        kv = self.W_up_k(c_kv)\n        k_c, value_states = kv.split(\n            [self.nope_mask.sum().item(), self.config.num_key_value_heads * self.d_v],\n            dim=-1,\n        )\n        key_states = torch.zeros(\n            bsz,\n            q_len,\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nclass AutoEncoderV3(nn.Module):\n    # Low-rank decomposition of k_nope and v with shared cache. The difference from v2 is that W_down_k and W_up_k are specific to individual heads.\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        parallel_config: Optional[ParallelismArgs],\n        tp_pg: dist.ProcessGroup,\n        layer_idx: Optional[int],\n        nope_mask: torch.Tensor,\n    ):\n        super().__init__()\n        self.config = config\n        self.parallel_config = parallel_config\n        self.tp_pg = tp_pg\n        self.tp_mode = (\n            self.parallel_config.tp_mode\n            if self.parallel_config is not None\n            else TensorParallelLinearMode.ALL_REDUCE\n        )\n        self.tp_linear_async_communication = (\n            self.parallel_config.tp_linear_async_communication\n            if self.parallel_config is not None\n            else False\n        )\n        self.layer_idx = layer_idx\n        self.nope_mask = nope_mask\n        self.d_qk = config.hidden_size // config.num_attention_heads\n        self.d_v = config.hidden_size // config.num_attention_heads\n        self.W_down_k = TensorParallelColumnLinear(\n            nope_mask.sum().item() // config.num_key_value_heads + self.d_v,\n            config.AE[\"low_rank\"],\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_up_k = TensorParallelColumnLinear(\n            self.W_down_k.out_features,\n            self.W_down_k.in_features,\n            bias=False,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def forward(\n        self,\n        k_r: torch.Tensor,  # [bsz, q_len, rope_dim]\n        k_nope: torch.Tensor,  # [bsz, q_len, nope_dim]\n        value_states: torch.Tensor,  # [bsz, q_len, n_local_kv_heads* d_v]\n    ) -> Tuple[\n        torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]\n    ]:  # [bsz, q_len, n_local_kv_heads* d_v]\n        bsz, q_len, _ = k_nope.size()\n        kv = torch.cat(\n            [\n                k_nope.view(bsz, q_len, self.config.num_key_value_heads, -1),\n                value_states.view(\n                    bsz, q_len, self.config.num_key_value_heads, self.d_v\n                ),\n            ],\n            dim=-1,\n        )\n        c_kv = apply_activation(self.W_down_k(kv), self.config.AE[\"activation_fn\"])\n        kv = self.W_up_k(c_kv)\n        k_c, value_states = kv.split(\n            [self.nope_mask.sum().item() // self.config.num_key_value_heads, self.d_v],\n            dim=-1,\n        )\n        k_c = k_c.reshape(bsz, -1, self.nope_mask.sum().item())\n        value_states = value_states.reshape(\n            bsz, -1, self.config.num_key_value_heads * self.d_v\n        )\n        key_states = torch.zeros(\n            bsz,\n            q_len,\n            self.nope_mask.shape[-1],\n            device=k_nope.device,\n            dtype=k_nope.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n        return key_states, value_states\n\n\nAUTO_ENCODER_VERSION_MAP = {\n    1: AutoEncoderV1,\n    2: AutoEncoderV2,\n    3: AutoEncoderV3,\n}\n\n\nclass CustomCausalSelfAttention(nn.Module, AttachableStore):\n    def __init__(\n        self,\n        config,\n        parallel_config: Optional[ParallelismArgs],\n        tp_pg: dist.ProcessGroup,\n        layer_idx: int,\n    ):\n        from flash_attn.layers.rotary import RotaryEmbedding as FlashRotaryEmbedding\n\n        super().__init__()\n        # Tensor parallel considerations: We split tensors along head dimension\n        assert (\n            config.num_attention_heads % tp_pg.size() == 0\n        ), f\"Number of attention heads ({config.num_attention_heads}) must be divisible by TP size ({tp_pg.size()}).\"\n        assert (\n            not config.rope_interleaved\n        ), \"MLA Causal attention does not support interleaved RoPE\"\n        try:\n            assert (\n                config.num_key_value_heads % tp_pg.size() == 0\n            ), f\"Number of key/value heads ({config.num_key_value_heads}) must be divisible by TP size ({tp_pg.size()}).\"\n        except AttributeError:\n            log_rank(\n                \"WARNING: num_key_value_heads not defined, assuming it is equal to num_attention_heads\",\n                logger=logger,\n                level=logging.WARNING,\n                rank=0,\n            )\n            # If num_key_value_heads is not defined, we assume that it is equal to num_attention_heads\n            config.num_key_value_heads = config.num_attention_heads\n        assert (\n            config.num_attention_heads % config.num_key_value_heads == 0\n        ), f\"Number of attention heads ({config.num_attention_heads}) must be divisible by number of key/value heads ({config.num_key_value_heads}).\"\n        self.config = config\n        self.parallel_config = parallel_config\n        self.tp_pg = tp_pg\n        self.n_local_q_heads = config.num_attention_heads // tp_pg.size()\n        self.n_local_kv_heads = config.num_key_value_heads // tp_pg.size()\n        self.n_repeats = config.num_attention_heads // config.num_key_value_heads\n        self.is_gqa = (\n            config.num_attention_heads != config.num_key_value_heads\n        )  # Whether we are using GQA or not\n        self.d_qk = config.hidden_size // config.num_attention_heads\n        self.d_v = config.hidden_size // config.num_attention_heads\n        self.d_model = config.hidden_size\n        self.is_using_mup = config.is_using_mup\n        self.layer_idx = layer_idx\n        self.low_rank = config.AE[\"low_rank\"]\n        self.tp_mode = (\n            self.parallel_config.tp_mode\n            if self.parallel_config is not None\n            else TensorParallelLinearMode.ALL_REDUCE\n        )\n        self.tp_linear_async_communication = (\n            self.parallel_config.tp_linear_async_communication\n            if self.parallel_config is not None\n            else False\n        )\n\n        self.nope_mask = IndexForNope.get_index_for_nope(\n            config.RoPE,\n            head_dim=self.d_qk,\n            head_num=self.n_local_kv_heads,\n            layer_idx=layer_idx,\n        )\n        # TODO @thomasw21: refactor so that we store that default in a single place.\n\n        # build the slice config for self.qkv for save/load\n        # shard are done within the contiguous chunk\n\n        # TODO(kunhao): We want to have only one version per device and not one version per layer.\n        self.rotary_embedding = llama.LlamaRotaryEmbedding(\n            dim=self.d_qk,\n            end=config.max_position_embeddings,\n            theta=config.rope_theta,\n        )\n        self.rope_interleaved = config.rope_interleaved\n        self.auto_encoder = AUTO_ENCODER_VERSION_MAP[config.AE[\"version\"]](\n            config=self.config,\n            parallel_config=self.parallel_config,\n            tp_pg=self.tp_pg,\n            layer_idx=self.layer_idx,\n            nope_mask=self.nope_mask,\n        )\n        self.init_w_q()\n        self.init_w_k()\n        self.init_w_v()\n        self.init_w_o()\n\n        # NOTE: Only supported for training (TODO(fmom): position_ids not supported yet)\n        self.flash_rotary_embedding = FlashRotaryEmbedding(\n            dim=self.d_qk, base=config.rope_theta, interleaved=config.rope_interleaved\n        )\n\n        self.attention = CoreAttention(\n            config,\n            parallel_config=parallel_config,\n            layer_idx=layer_idx,\n        )\n\n        self.prefill_kv_len = (\n            config.max_position_embeddings\n        )  # TODO @nouamane: compute based on free memory, because in rope we can surpass max_position_embeddings\n\n    def init_w_q(self):\n        self.q_proj = TensorParallelColumnLinear(\n            self.d_model,\n            self.config.num_attention_heads * self.d_qk,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            bias=False,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def init_w_k(self):\n        self.k_proj = TensorParallelColumnLinear(\n            self.d_model,\n            self.nope_mask.sum().item(),\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            bias=False,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n        self.W_k_r = TensorParallelColumnLinear(\n            self.d_model,\n            (self.nope_mask == False).sum().item(),\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            bias=False,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def init_w_v(self):\n        self.v_proj = TensorParallelColumnLinear(\n            self.d_model,\n            self.n_local_kv_heads * self.d_v,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            bias=False,\n            async_communication=self.tp_linear_async_communication,\n            tp_recompute_allgather=self.parallel_config.tp_recompute_allgather,\n        )\n\n    def init_w_o(self):\n        self.o_proj = TensorParallelRowLinear(\n            self.config.num_attention_heads * self.d_qk,\n            self.d_model,\n            pg=self.tp_pg,\n            mode=self.tp_mode,\n            bias=False,\n            async_communication=self.tp_linear_async_communication,\n        )\n\n    def get_query_states(\n        self,\n        hidden_states,  # [seq_length, batch_size, hidden_size]\n        sequence_mask,  # [batch_size, seq_length]\n    ):\n        query_states = self.q_proj(\n            hidden_states\n        )  # [seq_length, batch_size, n_local_q_heads * d_qk]\n        q_length, batch_size, _ = query_states.shape\n        query_states = (\n            query_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads, self.d_qk)\n        )  # [batch_size, seq_length, n_local_q_heads, d_qk]\n        return query_states\n\n    def get_key_value_states(\n        self,\n        hidden_states,  # [seq_length, batch_size, hidden_size]\n        sequence_mask,  # [batch_size, seq_length]\n    ):\n        k_r = self.W_k_r(hidden_states).transpose(\n            0, 1\n        )  # [seq_length, batch_size, rope_dim]\n        k_nope = self.k_proj(hidden_states).transpose(\n            0, 1\n        )  # [seq_length, batch_size, nope_dim]\n        value_states = self.v_proj(hidden_states).transpose(\n            0, 1\n        )  # [seq_length, batch_size, n_local_kv_heads * d_v]\n        key_states, value_states = self.auto_encoder(k_r, k_nope, value_states)\n        key_states = key_states.view(\n            key_states.size(0),\n            key_states.size(1),\n            self.n_local_kv_heads,\n            self.d_qk,\n        )\n        value_states = value_states.view(\n            value_states.size(0),\n            value_states.size(1),\n            self.n_local_kv_heads,\n            self.d_v,\n        )\n        return key_states, value_states\n\n    def forward(\n        self,\n        hidden_states,  # [seq_length, batch_size, hidden_size]\n        sequence_mask,  # [batch_size, seq_length]\n    ):\n        from flash_attn import bert_padding\n        from flash_attn.flash_attn_interface import flash_attn_varlen_func\n\n        batch_size, q_length = sequence_mask.shape\n\n        query_states = self.get_query_states(hidden_states, sequence_mask)\n        key_states, value_states = self.get_key_value_states(\n            hidden_states, sequence_mask\n        )\n\n        store = self.get_local_store()  # In fact, collections.defaultdict?\n        if store is not None:  # Inference case\n            assert False, \"Not implemented\"\n\n        else:  # Training case\n            position_ids = torch.cumsum(sequence_mask, dim=-1, dtype=torch.int32) - 1\n            cos, sin = self.rotary_embedding(value_states, position_ids)\n\n            if self.config.RoPE[\"partial_rope_version\"] == 4:\n                query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n                    query_states, key_states, cos, sin, layer_idx=self.layer_idx\n                )\n            else:\n                query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n                    query_states, key_states, cos, sin\n                )\n\n            q_sequence_mask = sequence_mask\n            kv_sequence_mask = sequence_mask\n\n            kv_length = key_states.shape[1]\n            # [batch_size, seq_length, num_heads, d_qk]\n            # Shaping for use in `flash-attn` version of flash-attn: `flash_attn_unpadded_func`\n            query_states = query_states.view(\n                batch_size * q_length, self.n_local_q_heads, self.d_qk\n            )  # [batch_size * q_length, self.n_heads, d_qk]\n\n            key_states = key_states.view(\n                batch_size * kv_length, self.n_local_kv_heads, self.d_qk\n            )  # [batch_size * kv_length, self.n_heads, d_qk]\n            value_states = value_states.view(\n                batch_size * kv_length, self.n_local_kv_heads, self.d_v\n            )  # [batch_size * kv_length, self.n_heads, d_v]\n\n            attention_output = self.attention(\n                query_states=query_states,\n                key_states=key_states,\n                value_states=value_states,\n                q_sequence_mask=q_sequence_mask,\n                kv_sequence_mask=kv_sequence_mask,\n            )\n\n        attention_output = (\n            attention_output.contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads * self.d_v)\n            .transpose(0, 1)\n        )\n        output = self.o_proj(attention_output)\n\n        return {\n            \"hidden_states\": output,\n            \"sequence_mask\": sequence_mask,\n        }\n\n\ndef custom_load_weights(\n    model: nn.Module,\n    parallel_context: ParallelContext,\n    root_folder: Path,\n    filtered_state_dict: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Load weights from a checkpoint\n\n    Args:\n        model: model to load weights into\n        parallel_context: distributed process groups\n        root_folder: root folder of the checkpoint\n        filtered_state_dict: state dict to load from (overrides model.state_dict()). if None, load from model.state_dict()\n    \"\"\"\n    param_root_folder = root_folder / \"model\"\n\n    module_id_to_prefix = {\n        id(module): f\"{module_name}.\" for module_name, module in model.named_modules()\n    }\n    # Fix the root_model\n    module_id_to_prefix[id(model)] = \"\"\n\n    checkpoint_version: Optional[Version] = None\n\n    filtered_state_dict = (\n        filtered_state_dict if filtered_state_dict is not None else model.state_dict()\n    )\n    param_shard_metadata = {}\n    missing_keys = []\n    for name, param_or_buffer in tqdm(\n        filtered_state_dict.items(),\n        disable=dist.get_rank(parallel_context.world_pg) != 0,\n        desc=\"Loading weights\",\n    ):\n        # NOTE: extract how does the current model parameter are sharded\n        # so that we can load optimizer checkpoints in this way\n        param_shard_metadata[name] = {}\n        # `state_dict` doesn't return a Param or a buffer, just a tensors which loses some metadata\n        try:\n            param = model.get_parameter(name)\n        except AttributeError:\n            param = None\n\n        if isinstance(param, NanotronParameter):\n            if param.is_tied:\n                tied_info = param.get_tied_info()\n                base_name = tied_info.get_full_name_from_module_id_to_prefix(\n                    module_id_to_prefix=module_id_to_prefix\n                )\n            else:\n                base_name = name\n\n            if param.is_sharded:\n                sharded_info = param.get_sharded_info()\n\n                if param.is_tied:\n                    # When params are tied only the first rank of tied param group stores weights (see save_weights)\n                    group = parallel_context.world_ranks_to_pg[tied_info.global_ranks]\n                    group_rank = 0\n                else:\n                    group = parallel_context.world_ranks_to_pg[\n                        sharded_info.global_ranks\n                    ]\n                    group_rank = dist.get_rank(group)\n\n                exp_tp_pp_rank_and_size = get_exp_tp_pp_rank_and_size_from(\n                    world_rank=get_global_rank(group=group, group_rank=group_rank),\n                    parallel_context=parallel_context,\n                )\n                # TODO @nouamane: do we consider exp_size=1 expert_sharded?\n                is_expert_sharded = sharded_info.is_expert_sharded(parallel_context)\n            else:\n                exp_tp_pp_rank_and_size = None\n                is_expert_sharded = False\n\n            path = get_path(\n                base_name,\n                type=ObjectType.MODEL,\n                exp_tp_pp_rank_and_size=exp_tp_pp_rank_and_size,\n                prefix=param_root_folder,\n                is_expert_sharded=is_expert_sharded,\n            )\n\n            if path.exists():\n                with safe_open(path, framework=\"pt\", device=str(param.device)) as fi:\n                    # TODO @thomasw21: Choose only a slice if we switch the TP topology\n                    param_or_buffer[:] = fi.get_tensor(\"data\")\n\n            elif not path.parent.exists():\n                missing_keys.append(name)\n                # raise ValueError(\n                #     f\"Checkpoint is empty or checkpoint structure is not matching the model architecture.\"\n                #     f\"Couldn't find folder {path.parent} in checkpoint at {root_folder}\"\n                # )\n            else:\n                # Let's assume that the topology changed and the param is sharded.\n                # We search for all the files from the shards, concatenate the \"unsharded\" tensor\n                # and load the specific shard we're interested in.\n                if not param.is_sharded:\n                    raise ValueError(\n                        f\"`{name}` is not a sharded parameter. It's possible you were expecting {path} to exist.\"\n                    )\n                # TODO @thomasw21: Make so that we don't need to code this logic somewhere else than in `get_path`\n                sharded_info = param.get_sharded_info()\n                suffix = base_name.rsplit(\".\", 1)[-1]\n                shards_path = list(\n                    path.parent.glob(f\"{ObjectType.MODEL.value}_{suffix}*.safetensors\")\n                )\n                if len(shards_path) <= 0:\n                    raise ValueError(\n                        f\"Could not find any shards {ObjectType.MODEL.value}_{suffix}*.safetensors in {path.parent}.\"\n                        f\"If you notice `.safetensors` in the middle of the name of some of the checkpoints files. You need to run `scripts/fix_checkpoint_bad_naming.py`.\"\n                    )\n\n                if checkpoint_version is None:\n                    checkpoint_version = get_checkpoint_version(\n                        parallel_context, root_folder, param_save_path=shards_path[0]\n                    )\n                else:\n                    current_checkpoint_version = None\n                    try:\n                        current_checkpoint_version = (\n                            read_checkpoint_version_from_shard_file(\n                                param_save_path=shards_path[0]\n                            )\n                        )\n                    except CheckpointVersionFromShardFileException:\n                        # The checkpoint version is read from the meta file\n                        current_checkpoint_version = checkpoint_version\n                    finally:\n                        assert (\n                            current_checkpoint_version == checkpoint_version\n                        ), f\"Checkpoint version mismatch at {shards_path[0]}.\"\n\n                if checkpoint_version <= CHECKPOINT_VERSION:\n                    load_sharded_param_latest(\n                        param_or_buffer=param_or_buffer,\n                        sharded_info=sharded_info,\n                        shards_path=shards_path,\n                        param_shard_metadata=param_shard_metadata[name],\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported checkpoint version {checkpoint_version}\"\n                    )\n\n        else:\n            raise NotImplementedError(\n                f\"Parameters {param} should be a NanotronParameter\"\n            )\n\n    low_rank = model.config.AE[\"low_rank\"] * model.config.num_key_value_heads\n    logger.info(f\"Low rank: {low_rank}\")\n    legal_keys = [\n        \"W_k_r.weight\",\n        \"k_proj.weight\",\n        \"W_down_k.weight\",\n        \"W_up_k.weight\",\n        \"W_down_v.weight\",\n        \"W_up_v.weight\",\n        \"v_proj.weight\",\n        \"q_proj.weight\",\n    ]\n    for missing_key in missing_keys:\n        layer_idx = int(missing_key.split(\".\")[2])\n        attn_module_prefix = f\"model.decoder.{layer_idx}.pp_block.attn\"\n        attn_module = model.model.decoder[layer_idx].pp_block.attn\n        base_name = f\"model.decoder.{layer_idx}.pp_block.attn.qkv_proj.weight\"\n        path = get_path(\n            base_name,\n            type=ObjectType.MODEL,\n            exp_tp_pp_rank_and_size=exp_tp_pp_rank_and_size,\n            prefix=param_root_folder,\n            is_expert_sharded=True,\n        )\n        with safe_open(path, framework=\"pt\", device=str(param.device)) as fi:\n            qkv_proj = fi.get_tensor(\"data\").t()\n        q_proj, k_proj, v_proj = qkv_proj.split(\n            [\n                attn_module.n_local_q_heads * attn_module.d_qk,\n                attn_module.n_local_kv_heads * attn_module.d_qk,\n                attn_module.n_local_kv_heads * attn_module.d_v,\n            ],\n            dim=-1,\n        )\n        if missing_key.endswith(\"W_down_k.weight\"):\n            dtype = filtered_state_dict[\n                f\"{attn_module_prefix}.auto_encoder.W_down_k.weight\"\n            ].dtype\n            in_features = attn_module.auto_encoder.W_down_k.in_features\n            out_features = attn_module.auto_encoder.W_down_k.out_features\n            filtered_state_dict[f\"{attn_module_prefix}.auto_encoder.W_down_k.weight\"][\n                :\n            ] = torch.nn.init.xavier_uniform_(\n                torch.empty(out_features, in_features, dtype=dtype)\n            )\n            filtered_state_dict[f\"{attn_module_prefix}.auto_encoder.W_up_k.weight\"][\n                :\n            ] = torch.nn.init.xavier_uniform_(\n                torch.empty(in_features, out_features, dtype=dtype)\n            )\n        elif missing_key.endswith(\"W_down_v.weight\"):\n            dtype = filtered_state_dict[\n                f\"{attn_module_prefix}.auto_encoder.W_down_v.weight\"\n            ].dtype\n            in_features = attn_module.auto_encoder.W_down_v.in_features\n            out_features = attn_module.auto_encoder.W_down_v.out_features\n            filtered_state_dict[f\"{attn_module_prefix}.auto_encoder.W_down_v.weight\"][\n                :\n            ] = torch.nn.init.xavier_uniform_(\n                torch.empty(out_features, in_features, dtype=dtype)\n            )\n            filtered_state_dict[f\"{attn_module_prefix}.auto_encoder.W_up_v.weight\"][\n                :\n            ] = torch.nn.init.xavier_uniform_(\n                torch.empty(in_features, out_features, dtype=dtype)\n            )\n        elif missing_key.endswith(\"q_proj.weight\"):\n            filtered_state_dict[f\"{attn_module_prefix}.q_proj.weight\"][:] = q_proj.t()\n        elif missing_key.endswith(\"v_proj.weight\"):\n            filtered_state_dict[f\"{attn_module_prefix}.v_proj.weight\"][:] = v_proj.t()\n        elif missing_key.endswith(\"W_k_r.weight\"):\n            filtered_state_dict[f\"{attn_module_prefix}.W_k_r.weight\"][:] = (\n                k_proj[:, ~attn_module.nope_mask]\n            ).t()\n        elif missing_key.endswith(\"k_proj.weight\"):\n            filtered_state_dict[f\"{attn_module_prefix}.k_proj.weight\"][:] = (\n                k_proj[:, attn_module.nope_mask]\n            ).t()\n        elif any([missing_key.endswith(suffix) for suffix in legal_keys]):\n            continue\n        else:\n            raise ValueError(\n                f\"Checkpoint is empty or checkpoint structure is not matching the model architecture.\"\n                f\"Couldn't find folder {path.parent} in checkpoint at {root_folder}.\"\n                f\"Missing key: {missing_key}\"\n            )\n\n    return param_shard_metadata\n\n\ndef ae_patch_func_nt(rope_cfg=None):\n    llama.CausalSelfAttention = CustomCausalSelfAttention\n    if not hasattr(nt_weights, \"original_load_weights\"):\n        nt_weights.original_load_weights = nt_weights.load_weights\n        nt_weights.load_weights = custom_load_weights\n    nanotron.config.models_config.LlamaConfig = CustomLlamaConfig\n    nanotron.trainer.CONFIG_TO_MODEL_CLASS.update(\n        {\"CustomLlamaConfig\": nanotron.trainer.CONFIG_TO_MODEL_CLASS[\"LlamaConfig\"]}\n    )\n\n    nanotron.serialize.load_weights = custom_load_weights\n    nanotron.trainer.load_weights = custom_load_weights\n    from ..partial_rope.patch_func import create_custom_apply_rotary_pos_emb\n\n    llama.LlamaRotaryEmbedding.apply_rotary_pos_emb = (\n        create_custom_apply_rotary_pos_emb(rope_cfg)\n    )\n    from ..mla.NopeIndex import IndexForNope\n\n    if rope_cfg[\"partial_rope_version\"] == 4:\n        IndexForNope._qk_tensor_cache = torch.load(rope_cfg[\"qk_tensor_path\"])\n        IndexForNope._qk_tensor_path = rope_cfg[\"qk_tensor_path\"]\n"}
{"type": "source_file", "path": "src/auto_encoder/train.py", "content": "\"\"\"\nNanotron training script.\n\nUsage:\n```\nexport CUDA_DEVICE_MAX_CONNECTIONS=1 # important for some distributed operations\ntorchrun --nproc_per_node=8 run_train.py --config-file examples/config_tiny_llama.yaml\n```\n\"\"\"\n\nimport argparse\nimport yaml\nfrom typing import Dict, cast\n\nimport numpy as np\nfrom nanotron import logging\nfrom nanotron.config import (\n    DataArgs,\n    DatasetStageArgs,\n    NanosetDatasetsArgs,\n    PretrainDatasetsArgs,\n)\nfrom nanotron.data.dataloader_builder import build_nanoset_dataloader\nfrom nanotron.dataloader import (\n    clm_process,\n    dummy_infinite_data_generator,\n    get_datasets,\n    get_train_dataloader,\n)\nfrom nanotron.helpers import (\n    compute_remain_train_steps_of_a_data_stage_from_ckp,\n    get_consumed_train_samples_of_a_data_stage_from_ckp,\n)\nfrom nanotron.logging import log_rank\nfrom nanotron.parallel.pipeline_parallel.utils import get_input_output_pp_ranks\nfrom nanotron.trainer import DistributedTrainer\nfrom nanotron.utils import main_rank_first\nfrom torch.utils.data import DataLoader\n\ntry:\n    from huggingface_hub import __version__ as hf_hub_version\n    from transformers import AutoTokenizer\n    from transformers import __version__ as tf_version\nexcept ImportError:\n    hf_hub_version = None\n    tf_version = None\n\nlogger = logging.get_logger(__name__)\n\n\ndef get_dataloader_from_data_stage(\n    trainer: DistributedTrainer,\n    data: DataArgs,\n    consumed_train_samples: int,\n    num_remaining_train_steps: int,\n):\n    \"\"\"\n    Returns a dataloader for a given data stage.\n\n    data: The data configuration for the current stage.\n    consumed_train_samples: The number of samples consumed by the model in the this stage (each stage starts from zero).\n    num_remaining_train_steps: The number of remaining training steps for this stage.\n    \"\"\"\n    assert (\n        consumed_train_samples >= 0\n    ), \"consumed_train_samples should be greater than 0\"\n    assert (\n        num_remaining_train_steps >= 0\n    ), \"num_remaining_train_steps should be greater than 0\"\n\n    # First, we need to know which ranks to feed the dataloader to\n    input_pp_rank, output_pp_rank = get_input_output_pp_ranks(model=trainer.model)\n\n    # Case 1: Dummy data generator\n    if data.dataset is None:\n        log_rank(\n            \"Using dummy data generator\", logger=logger, level=logging.INFO, rank=0\n        )\n        dataloader = dummy_infinite_data_generator(\n            micro_batch_size=trainer.micro_batch_size,\n            sequence_length=trainer.sequence_length,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            vocab_size=trainer.model_config.vocab_size,\n            seed=data.seed,\n            parallel_context=trainer.parallel_context,\n        )()\n\n    # Case 2: HuggingFace datasets\n    elif isinstance(data.dataset, PretrainDatasetsArgs):\n        log_rank(\"Using `datasets` library\", logger=logger, level=logging.INFO, rank=0)\n        tokenizer_path = trainer.config.tokenizer.tokenizer_name_or_path\n        log_rank(\n            f\"Loading tokenizer from {tokenizer_path} and transformers/hf_hub versions {tf_version, hf_hub_version}\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        # We need to the 1st device to process dataset and cache it, then other devices load from cache\n        with main_rank_first(trainer.parallel_context.world_pg):\n            # TODO @nouamanetazi: this may timeout before 1st device finishes processing dataset. Can we have a ctxmanager to modify timeout?\n            # TODO: generalise to include  for validation/test splits\n\n            # We load the raw dataset\n            raw_dataset = get_datasets(\n                hf_dataset_or_datasets=data.dataset.hf_dataset_or_datasets,\n                hf_dataset_config_name=data.dataset.hf_dataset_config_name,\n                splits=data.dataset.hf_dataset_splits,\n            )[\"train\"]\n\n            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.padding_side = \"left\"\n\n            # Check that tokenizer's vocab size is smaller than the model's vocab size\n            assert (\n                tokenizer.vocab_size <= trainer.model_config.vocab_size\n            ), f\"Tokenizer's vocab size ({tokenizer.vocab_size}) is larger than the model's vocab size ({trainer.model_config.vocab_size})\"\n\n            # We apply the Causal Language Modeling preprocessing\n            train_dataset = clm_process(\n                raw_dataset=raw_dataset,\n                tokenizer=tokenizer,\n                text_column_name=data.dataset.text_column_name,\n                dataset_processing_num_proc_per_process=data.dataset.dataset_processing_num_proc_per_process,\n                dataset_overwrite_cache=data.dataset.dataset_overwrite_cache,\n                sequence_length=trainer.sequence_length,\n            )\n\n            # We load the processed dataset on the ranks requiring it\n            dataloader = get_train_dataloader(\n                train_dataset=train_dataset,\n                sequence_length=trainer.sequence_length,\n                parallel_context=trainer.parallel_context,\n                input_pp_rank=input_pp_rank,\n                output_pp_rank=output_pp_rank,\n                micro_batch_size=trainer.micro_batch_size,\n                consumed_train_samples=consumed_train_samples,\n                dataloader_num_workers=data.num_loading_workers,\n                seed_worker=data.seed,\n                dataloader_drop_last=True,\n            )\n\n            # Check if we have enough samples for train_steps\n            total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n            num_tokens_needed_for_training = (\n                num_remaining_train_steps\n                * trainer.global_batch_size\n                * trainer.sequence_length\n            )\n            assert num_tokens_needed_for_training <= total_tokens_dataset, (\n                f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n                f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.iteration_step}\"\n            )\n\n    # Case 3: Nanosets\n    elif isinstance(data.dataset, NanosetDatasetsArgs):\n        # Get tokenizer cardinality\n        tokenizer = AutoTokenizer.from_pretrained(\n            trainer.config.tokenizer.tokenizer_name_or_path\n        )\n        token_size = 4 if len(tokenizer) > np.iinfo(np.uint16).max + 1 else 2\n        del tokenizer\n        # Create Nanoset\n        from nanotron.data.nanoset import Nanoset\n\n        with main_rank_first(trainer.parallel_context.world_pg):\n            train_dataset = Nanoset(\n                dataset_folders=data.dataset.dataset_folder,\n                dataset_weights=data.dataset.dataset_weights,\n                sequence_length=trainer.sequence_length,\n                token_size=token_size,\n                train_split_num_samples=trainer.config.tokens.train_steps\n                * trainer.global_batch_size,\n                random_seed=data.seed,\n            )\n\n        # Prepare dataloader\n        train_dataloader = build_nanoset_dataloader(\n            train_dataset,\n            trainer.sequence_length,\n            parallel_context=trainer.parallel_context,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            micro_batch_size=trainer.micro_batch_size,\n            consumed_train_samples=consumed_train_samples,\n            dataloader_num_workers=data.num_loading_workers,\n            dataloader_drop_last=True,\n        )\n\n        return train_dataloader\n    else:\n        raise ValueError(\n            f\"Unhandled case of `self.config.data.dataset`. Got: {data.dataset}\"\n        )\n\n    return dataloader\n\n\ndef get_dataloader(trainer: DistributedTrainer) -> Dict[str, DataLoader]:\n    dataloaders = {}\n\n    for stage_idx, stage in enumerate(trainer.config.data_stages):\n        # NOTE: we only create the dataloader for the first stage,\n        # then we lazy initialize the dataloader for the other stages\n        stage = cast(DatasetStageArgs, stage)\n        consumed_train_samples = get_consumed_train_samples_of_a_data_stage_from_ckp(\n            stage, trainer.metadata\n        )\n        assert (\n            consumed_train_samples is not None\n        ), f\"Cannot find consumed_train_samples for stage {stage.start_training_step} in the checkpoint\"\n\n        num_remaining_train_steps = compute_remain_train_steps_of_a_data_stage_from_ckp(\n            stage, trainer.config, trainer.metadata\n        )\n        log_rank(\n            f\"[Training Plan] Stage {stage.name} has {num_remaining_train_steps} remaining training steps and has consumed {consumed_train_samples} samples\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        dataloader = (\n            get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n            if stage_idx == 0\n            else lambda stage=stage: get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n        )\n        dataloaders[stage.name] = dataloader\n    return dataloaders\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config-file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML or python config file\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    config_file = args.config_file\n\n    # Monkey patch\n    from .patch_func_nt import ae_patch_func_nt,CustomConfig\n    import yaml\n    with open(config_file, \"r\") as fin:\n        config = yaml.safe_load(fin)\n    rope_cfg=config[\"model\"][\"model_config\"][\"RoPE\"]\n    ae_patch_func_nt(rope_cfg)\n\n    from nanotron import trainer as nt_trainer\n    # Load trainer and data\n    trainer = nt_trainer.DistributedTrainer(config_file,config_class=CustomConfig)\n    # print(trainer.unwrapped_model.config.rope_interleaved)\n    dataloader = get_dataloader(trainer)\n\n    # LlamaRotaryEmbedding.partial_rope_cfg = cfg\n    # Train\n    trainer.train(dataloader)\n"}
{"type": "source_file", "path": "src/auto_encoder/init.py", "content": "import torch\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom transformers import LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM,PreTrainedModel\nfrom transformers.models.llama import modeling_llama\nfrom nanotron.data.nanoset import Nanoset\nfrom nanotron.parallel.pipeline_parallel.block import PipelineBlock, TensorPointer\nimport os\nfrom typing import Dict, List, Tuple, Union\nimport numpy as np\nfrom nanotron.logging import warn_once\nimport logging\nimport importlib\nimport yaml\nimport torch\n\n\nfrom ..mha2mla.lr_scheduler import load_optimizer_scheduler\n\nTYPE_DICT = {\n    \"float32\": torch.float32,\n    \"float16\": torch.float16,\n    \"bfloat16\": torch.bfloat16,\n}\n\nclass AttnForTraing(PreTrainedModel):\n    config_class = LlamaConfig\n    def __init__(self,config):\n        super().__init__(config)\n        from .patch_func_hf import CustomLlamaSdpaAttention,CustomLlamaAttention\n        self.config = config\n        self.model = torch.nn.ModuleList(\n            [\n                CustomLlamaSdpaAttention(\n                    config=config,\n                    layer_idx=layer_idx,\n                ) \n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n\n    def post_init(self,original_model):\n        self.original_model = original_model\n        import os\n        self.loss_func = torch.nn.SmoothL1Loss(reduction=\"sum\")\n        for layer_idx, layer in enumerate(self.original_model.model.layers):\n            original_attn = layer.self_attn\n            target_attn = self.model[layer_idx]\n            # k_proj,v_proj\n            _, k_proj, v_proj = original_attn.q_proj.weight.detach(), original_attn.k_proj.weight.detach(), original_attn.v_proj.weight.detach()\n            target_attn.k_proj.weight.data[:] = (k_proj.T[:, target_attn.nope_mask].T)\n            target_attn.v_proj.weight.data[:] = v_proj\n            # W_down_k, W_up_k\n            U,S,V = torch.svd(torch.eye(target_attn.auto_encoder.W_down_k.in_features).to(dtype=torch.float32))\n            low_rank = self.config.AE[\"low_rank\"] * self.config.num_key_value_heads\n            dtype = target_attn.auto_encoder.W_down_k.weight.dtype\n            in_features = target_attn.auto_encoder.W_down_k.in_features\n            out_features = target_attn.auto_encoder.W_down_k.out_features\n            target_attn.auto_encoder.W_down_k.weight.data[:] = torch.nn.init.xavier_uniform_(torch.empty(out_features, in_features,dtype=dtype))\n            target_attn.auto_encoder.W_up_k.weight.data[:] = torch.nn.init.xavier_uniform_(torch.empty(in_features, out_features,dtype=dtype))\n            # W_down_v, W_up_v\n            if hasattr(target_attn.auto_encoder,\"W_down_v\"):\n                U,S,V = torch.svd(torch.eye(target_attn.auto_encoder.W_down_v.in_features).to(dtype=torch.float32))\n                low_rank = self.config.AE[\"low_rank\"] * self.config.num_key_value_heads\n                dtype = target_attn.auto_encoder.W_down_v.weight.dtype\n                in_features = target_attn.auto_encoder.W_down_v.in_features\n                out_features = target_attn.auto_encoder.W_down_v.out_features\n                target_attn.auto_encoder.W_down_v.weight.data[:] = torch.nn.init.xavier_uniform_(torch.empty(out_features, in_features,dtype=dtype))\n                target_attn.auto_encoder.W_up_v.weight.data[:] = torch.nn.init.xavier_uniform_(torch.empty(in_features, out_features,dtype=dtype))\n\n        for name,named_param in self.original_model.named_parameters():\n            named_param.requires_grad = False\n        for name,named_param in self.model.named_parameters():\n            if all([x not in name for x in [\"W_down_v\",\"W_up_v\",\"W_down_k\",\"W_up_k\"]]):\n                named_param.requires_grad = False\n            else:\n                named_param.requires_grad = True\n\n        self.inputs = {}\n        for layer_id, layer in enumerate(self.original_model.model.layers):\n            attn = layer.self_attn\n            original_forward = attn.forward\n            \n            def make_new_forward(layer_id, inp_dict):\n                def new_forward(self,*args, **kwargs):\n                    output = self.original_forward(*args, **kwargs)\n                    inp_dict[layer_id] = (args, kwargs)\n                    return output\n                return new_forward\n    \n            import types\n            attn.original_forward = original_forward\n            attn.forward = types.MethodType(make_new_forward(layer_id, self.inputs), attn)\n\n    def forward(\n        self,\n        input_ids: Union[torch.Tensor, TensorPointer],\n        attention_mask: Union[torch.Tensor, TensorPointer],\n    ) -> Dict[str, Union[torch.Tensor, TensorPointer]]:\n        self.inputs.clear()\n        importlib.reload(modeling_llama)\n        sharded_logits = self.original_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            use_cache=False,\n        )\n        loss = torch.zeros(1, device=input_ids.device)\n        warn_once(\n            msg = \"Using SmoothL1Loss for RoPE loss\",\n            logger = logging.getLogger(__name__),\n        )\n        for layer_idx,layer in enumerate(self.original_model.model.layers):\n            target_attn = self.model[layer_idx]\n            nope_mask = self.model[layer_idx].nope_mask\n            hidden_states = self.inputs[layer_idx][1][\"hidden_states\"]\n            k_r = target_attn.W_k_r(hidden_states)\n            original_k_nope = target_attn.k_proj(hidden_states)\n            original_value_states = target_attn.v_proj(hidden_states)\n            key_states,value_states = target_attn.auto_encoder(k_r,original_k_nope,original_value_states)\n            k_loss = self.loss_func(original_k_nope, key_states[...,nope_mask])\n            v_loss = self.loss_func(original_value_states, value_states)\n            layer_loss = k_loss + v_loss\n            loss += layer_loss / (hidden_states.shape[0] * hidden_states.shape[1])\n\n            # test\n            # test_key_states = self.model[layer_idx].W_down_k(self.model[layer_idx].k_proj(self.inputs[layer_idx][1][\"hidden_states\"]))\n            # from ..mla.utils import apply_activation\n            # test_key_states = apply_activation(test_key_states, self.config.SVD[\"activation_fn\"])\n            # test_key_states = self.model[layer_idx].W_up_k(test_key_states)\n            # assert torch.allclose(test_key_states, key_states[...,nope_mask])\n            # print(\"loss rate:\",(k_loss.item())/(k_loss + v_loss).item())\n            # k_norm = torch.mean(torch.norm(key_states[...,nope_mask], p=2, dim=-1))\n            # v_norm = torch.mean(torch.norm(value_states, p=2, dim=-1))\n            # print(\"norm rate:\",k_norm.item()/(k_norm + v_norm).item())\n        loss = loss / len(self.original_model.model.layers)\n        return {\"loss\": loss}\n\n\ndef load_config(config_path):\n    \"\"\"Load configuration from a YAML file.\"\"\"\n    with open(config_path, \"r\") as file:\n        return yaml.safe_load(file)\n\ndef load_tokenizer_and_model(model_arguments, model_config):\n    \"\"\"Load tokenizer and model from configuration.\"\"\"\n    # model\n    dtype = TYPE_DICT[model_arguments[\"dtype\"]]\n    model_name_or_path = model_arguments[\"model_name_or_path\"]\n    model_config = LlamaConfig(**model_config)\n    if model_name_or_path is not None:\n        model = LlamaForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype)\n    else:\n        model = LlamaForCausalLM(model_config)\n    # tokenizer\n    tokenizer_name_or_path = model_arguments[\"tokenizer_name_or_path\"]\n    if tokenizer_name_or_path is None:\n        tokenizer_name_or_path = model_name_or_path\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token  # Warning\n    original_model = model\n    model = AttnForTraing(model_config)\n    model.post_init(original_model)\n    return model, tokenizer\n\n\nclass CustomNanoset(Nanoset):\n    def __getitem__(self, idx: int) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Returns sequence_length + 1 tokens from the memmap dataset\n\n        Args:\n            idx (int): The index into the dataset\n\n        Returns:\n            Dict[str, torch.LongTensor]: The input ids wrapped in a dictionary\n        \"\"\"\n        item = super().__getitem__(idx)\n        return item\n\n\ndef load_dataset(config, tokenizer):\n    \"\"\"Load dataset from configuration.\"\"\"\n    data_arguments = config[\"DataArguments\"]\n    dataset_folders = data_arguments[\"dataset_folders\"]\n    dataset_weights = data_arguments[\"dataset_weights\"]\n    sequence_length = data_arguments[\"sequence_length\"]\n    traingingargs = TrainingArguments(**config[\"TrainingArguments\"])\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    token_size = 4 if len(tokenizer) > np.iinfo(np.uint16).max + 1 else 2\n    global_batch_size = (\n        traingingargs.per_device_train_batch_size\n        * world_size\n        * traingingargs.gradient_accumulation_steps\n    )\n    dataset = CustomNanoset(\n        dataset_folders=dataset_folders,\n        sequence_length=sequence_length,\n        dataset_weights=dataset_weights,\n        token_size=token_size,\n        train_split_num_samples=global_batch_size * traingingargs.max_steps,\n    )\n    return dataset\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config_file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML configuration file.\",\n    )\n    args = parser.parse_args()\n    config = load_config(args.config_file)\n    assert config[\"DataArguments\"][\"DP\"] == int(os.environ.get(\"WORLD_SIZE\", 1)), \"DP is not equal to WORLD_SIZE\"\n\n    # Trainer\n    model, tokenizer = load_tokenizer_and_model(\n        config[\"ModelArguments\"], config[\"ModelConfig\"]\n    )\n    train_dataset = load_dataset(config, tokenizer)\n    resume_from_checkpoint = config[\"TrainingArguments\"][\"resume_from_checkpoint\"]\n    training_args = TrainingArguments(**config[\"TrainingArguments\"])\n    optimizer, lr_scheduler = load_optimizer_scheduler(model, config)\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        optimizers=(optimizer, lr_scheduler),\n    )\n    # train\n    if resume_from_checkpoint is not None:\n        trainer.train(resume_from_checkpoint)\n    else:\n        trainer.train()\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "src/conversation/convert_hf_to_nanotron.py", "content": "# copied from https://github.com/huggingface/nanotron/blob/main/examples/llama/convert_hf_to_nanotron.py\n\"\"\"\nConverts a HF model to nanotron format\nCommand:\n    torchrun --nproc_per_node=1 convert_hf_to_nanotron.py --checkpoint_path=hf_weights --save_path=nanotron_weights\n\"\"\"\n\nimport dataclasses\nimport json\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nimport nanotron\nimport torch\nfrom .convert_weights import get_config_mapping, get_weight_mapping, load_nanotron_model\nfrom nanotron.config import LlamaConfig as NanotronLlamaConfig\nfrom nanotron.models.llama import LlamaForTraining\nfrom transformers import LlamaConfig as HFLlamaConfig\nfrom transformers import LlamaForCausalLM, AutoConfig\n\n\ndef _handle_attention_block(\n    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, n_q_heads: int, n_kv_heads: int, d_qk: int\n) -> torch.Tensor:\n    # Huggingface Llama separates the q, k, v weights (as opposed to nanotron).\n    # Furthermore, in the rotary embeddings in nanotron expects interleaved pairs of even\n    # and odd dimensions GPT-J style, while the huggingface implementation expects\n    # the whole 1st half and then the whole 2nd half GPT-NeoX style (for more information\n    # see flash_attn.layers.rotary.RotaryEmbedding).\n    # This function handles the concatenation of the q, k, v weights and proper permutation\n    # to ensure correct transformation.\n\n    def interleave(w: torch.Tensor):\n        w_new = []\n        for head_w in w.split(d_qk):\n            head_w = head_w.view(2, d_qk // 2, -1).transpose(0, 1).reshape(d_qk, -1)\n            w_new.append(head_w)\n        return torch.cat(w_new)\n\n    # q = interleave(q)\n    # k = interleave(k)\n    return torch.cat([q, k, v])\n\n\ndef convert_hf_to_nt(model_hf: LlamaForCausalLM, model_nt: LlamaForTraining, config: NanotronLlamaConfig):\n    \"\"\"Converts the weights from the model_hf to model_nt, making modifications\n    in-place.\"\"\"\n\n    hf_sd = model_hf.state_dict()\n    nt_to_hf = get_weight_mapping(config, nt_to_hf=True)\n\n    for module_name_nt, module_nt in model_nt.named_modules():\n        for param_name_nt, param_nt in module_nt.named_parameters(recurse=False):\n            # In the case of qkv_proj, the nt_to_hf has exactly three keys, ccorresponding\n            # to q, k, v.\n            if \"qkv_proj\" in module_name_nt:\n                key_k, key_q, key_v = sorted(nt_to_hf[f\"{module_name_nt}.{param_name_nt}\"])\n                q = hf_sd[key_q]\n                k = hf_sd[key_k]\n                v = hf_sd[key_v]\n                param = _handle_attention_block(\n                    q,\n                    k,\n                    v,\n                    config.num_attention_heads,\n                    config.num_key_value_heads,\n                    config.hidden_size // config.num_attention_heads,\n                )\n            # The case of gate_up_proj, nt_to_hf_map has two keys.\n            elif \"gate_up_proj\" in module_name_nt:\n                key_gate, key_up = sorted(nt_to_hf[f\"{module_name_nt}.{param_name_nt}\"])\n                gate = hf_sd[key_gate]\n                up = hf_sd[key_up]\n                param = torch.cat([gate, up])\n            # All other cases are simple 1-to-1 correspondence.\n            else:\n                hf_key = nt_to_hf[f\"{module_name_nt}.{param_name_nt}\"]\n                param = hf_sd[hf_key]\n\n            with torch.no_grad():\n                param_nt.copy_(param)\n\n\ndef get_nanotron_config(config: HFLlamaConfig) -> NanotronLlamaConfig:\n    \"\"\"Converts a huggingface configuration to nanotron configuration.\"\"\"\n    attrs = {key: getattr(config, value) for key, value in get_config_mapping(nt_to_hf=True).items() if hasattr(config, value)}\n    return NanotronLlamaConfig(**attrs)\n\n\ndef convert_checkpoint_and_save(checkpoint_path: Path, save_path: Path):\n    \"\"\"Loads the huggingface checkpoint in `checkpoint_path`, creates\n    a new nanotron instance, copies the weights from the huggingface checkpoint\n    and saves the transformed nanotron to `save_path`.\"\"\"\n\n    # Load huggingface.\n    hf_model = LlamaForCausalLM.from_pretrained(checkpoint_path)\n\n    # Init nanotron model.\n    model_config = get_nanotron_config(hf_model.config)\n    nanotron_model = load_nanotron_model(model_config=model_config)\n\n    # Copy weights and save model.\n    parallel_context = nanotron.parallel.ParallelContext(\n        data_parallel_size=1, pipeline_parallel_size=1, tensor_parallel_size=1\n    )\n    convert_hf_to_nt(hf_model, nanotron_model, model_config)\n    nanotron.serialize.save_weights(model=nanotron_model, parallel_context=parallel_context, root_folder=save_path)\n    with open(save_path / \"model_config.json\", \"w+\") as f:\n        json.dump(dataclasses.asdict(model_config), f)\n    print(f\"Model saved to {save_path}\")\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Convert HF weights to nanotron format\")\n    parser.add_argument(\"--checkpoint_path\", type=Path, default=\"llama-7b\", help=\"Path to the checkpoint\")\n    parser.add_argument(\"--save_path\", type=Path, default=\"llama-7b-hf\", help=\"Path to save the nanotron model\")\n    parser.add_argument(\"--is_mla\", action=\"store_true\", help=\"Whether the model is an MLA model\")\n    parser.add_argument(\"--auto_encoder\", action=\"store_true\", help=\"Whether the model is using auto-encoder\")\n    args = parser.parse_args()\n    config = AutoConfig.from_pretrained(args.checkpoint_path)\n    if hasattr(config,\"RoPE\"):\n        # partial RoPE\n        from ..mha2mla.monkey_patch import partial_rope_monkey_patch as partial_rope_monkey_patch_hf\n        from ..mha2mla_nt.monkey_patch import CustomLlamaConfig,partial_rope_monkey_patch as partial_rope_monkey_patch_nt\n        partial_rope_monkey_patch_hf(config.RoPE)\n        partial_rope_monkey_patch_nt(config.RoPE)\n        globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    if args.is_mla:\n        from ..mha2mla.monkey_patch import mla_monkey_patch as mla_monkey_patch_hf\n        from ..mha2mla_nt.monkey_patch import mla_monkey_patch as mla_monkey_patch_nt,CustomLlamaConfig\n        config = AutoConfig.from_pretrained(args.checkpoint_path)\n        mla_monkey_patch_hf(config.RoPE)\n        mla_monkey_patch_nt(config.RoPE)\n        globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    if args.auto_encoder:\n        import json,os\n        with open(os.path.join(args.checkpoint_path,\"config.json\")) as f:\n            config = json.load(f)\n        from ..auto_encoder.patch_func_hf import ae_patch_func_hf\n        from ..auto_encoder.patch_func_nt import ae_patch_func_nt,CustomLlamaConfig\n        ae_patch_func_hf(config[\"RoPE\"])\n        ae_patch_func_nt(config[\"RoPE\"])\n        globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    if not args.is_mla and not args.auto_encoder:\n        from .original_convert_weights import get_weight_mapping as original_get_weight_mapping\n        from .original_convert_weights import load_nanotron_model as original_load_nanotron_model\n        get_weight_mapping = original_get_weight_mapping\n        load_nanotron_model = original_load_nanotron_model\n\n    # Convert HF model to nanotron format.\n    convert_checkpoint_and_save(checkpoint_path=args.checkpoint_path, save_path=args.save_path)\n"}
{"type": "source_file", "path": "src/mha2mla/2_norm.py", "content": "from dataclasses import dataclass\nimport torch\nfrom transformers import TrainingArguments\nfrom transformers import HfArgumentParser, DataCollatorForLanguageModeling\nimport os\nfrom tqdm import tqdm\nimport datasets\n\nfrom run_train import (\n    ModelArguments,\n    DataArguments,\n    load_config,\n    load_dataset,\n    load_tokenizer_and_model,\n)\n\nhidden_states_dict = {}\n\ndef create_hook_fn(name):\n    def hook(module, args, kwargs, output):\n        hidden_states_dict[name] = kwargs[\"hidden_states\"]\n\n    return hook\n\n\ndef main():\n    import argparse\n\n    cmd_parser = argparse.ArgumentParser()\n    cmd_parser.add_argument(\n        \"--config_file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML configuration file.\",\n    )\n    cmd_parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=False,\n        help=\"Path to the output directory.\",\n    )\n    cmd_parser.add_argument(\n        \"--sample_size\",\n        type=int,\n        default=1024,\n    )\n    args = cmd_parser.parse_args()\n    config = load_config(args.config_file)\n    parser = HfArgumentParser((TrainingArguments, ModelArguments, DataArguments))\n    training_args, model_args, dataset_args = parser.parse_dict(config)\n    # assert config[\"DataArguments\"][\"DP\"] == int(os.environ.get(\"WORLD_SIZE\", 1)), \"DP is not equal to WORLD_SIZE\"\n\n    # Trainer\n    model, tokenizer = load_tokenizer_and_model(model_args)\n    train_dataset = load_dataset(dataset_args, training_args, tokenizer)\n    assert (\n        int(os.getenv(\"WORLD_SIZE\", 1)) == 1\n    ), \"Only support single process.\" \n\n    def preprocess_function(examples):\n        if \"input_ids\" in examples: \n            return {\"input_ids\": examples[\"input_ids\"]}\n        elif \"text\" in examples:\n            return tokenizer(\n                examples[\"text\"],\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n        else:\n            raise ValueError(\"Unsupported dataset format. Must be a dictionary containing 'input_ids' or 'text'.\")\n\n    if isinstance(train_dataset, datasets.Dataset):\n        train_dataset = train_dataset.map(preprocess_function, batched=True)\n        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        return_tensors=\"pt\",\n    )\n    data_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=training_args.per_device_train_batch_size,\n        drop_last=True,\n        collate_fn=data_collator,\n    )\n    num = args.sample_size\n    model.eval()\n    model.to(\"cuda\")\n    for name, module in model.named_modules():\n        from transformers.models.llama.modeling_llama import LlamaAttention\n        if not isinstance(module, LlamaAttention):\n            continue\n        hook_fn = create_hook_fn(name)\n        module.register_forward_hook(hook_fn,with_kwargs=True)\n    p_bar = tqdm(total=num)\n    model_config = model.config\n    head_dim = model_config.hidden_size // model_config.num_attention_heads\n    num_layers = model_config.num_hidden_layers\n    query_states = [[] for _ in range(num_layers)]\n    key_states = [[] for _ in range(num_layers)]\n    def cal_2_norm(states):\n        states = torch.norm(\n            states.reshape(states.shape[0],states.shape[1],states.shape[2],2,-1).transpose(-1,-2),\n            p=2,\n            dim=4,\n        )\n        return states\n    with torch.no_grad():\n        for _,batch in enumerate(data_loader):\n            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n            model(**batch)\n            num -= batch[\"input_ids\"].shape[0]\n            p_bar.update(batch[\"input_ids\"].shape[0])\n            for name,module in model.named_modules():\n                if not isinstance(module, LlamaAttention):\n                    continue\n                idx = int(name.split(\".\")[2])\n                bsz,q_len,_ = hidden_states_dict[name].shape\n                q = module.q_proj(hidden_states_dict[name]).reshape(bsz,q_len,model_config.num_attention_heads,head_dim) # [bsz,q_len,num_heads,head_dim]\n                k = module.k_proj(hidden_states_dict[name]).reshape(bsz,q_len,model_config.num_key_value_heads,head_dim)\n                query_states[idx].append(cal_2_norm(q).mean(dim=1,keepdim=False).cpu()) # [bsz,num_heads,head_dim//2]\n                key_states[idx].append(cal_2_norm(k).mean(dim=1,keepdim=False).cpu())\n            if num <= 0:\n                break\n    query_states = torch.stack([torch.cat(query_states[i],dim=0) for i in range(num_layers)],dim=0) # [num_layers,sample_size,num_heads,head_dim//2]\n    key_states = torch.stack([torch.cat(key_states[i],dim=0) for i in range(num_layers)],dim=0)\n    query_states = torch.mean(query_states,dim=1,keepdim=False) # [num_layers,num_heads,head_dim//2]\n    key_states = torch.mean(key_states,dim=1,keepdim=False)\n    group_size = model_config.num_attention_heads // model_config.num_key_value_heads\n    key_states = key_states.unsqueeze(2).expand(num_layers,model_config.num_key_value_heads,group_size,model_config.head_dim//2).reshape(num_layers,model_config.num_attention_heads,head_dim//2) # [num_layers,num_heads,head_dim//2]\n    qk_states = query_states + key_states\n    if group_size > 1:\n        qk_states = qk_states.reshape(num_layers,model_config.num_key_value_heads,group_size,head_dim//2).sum(dim=2,keepdim=False)\n    with open(args.output_dir,\"wb\") as f:\n        torch.save(qk_states,f)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/mha2mla/tasks.py", "content": "import re\nimport numpy as np\n\nfrom lighteval.tasks.lighteval_task import LightevalTaskConfig\nfrom lighteval.tasks.requests import Doc\nfrom lighteval.metrics.metrics import Metrics, SampleLevelMetric, MetricCategory, MetricUseCase, ExactMatches\nimport lighteval.tasks.default_prompts as prompt\nfrom .math_utils import parse_math_answer\n\n\ndef prompt_hellaswag(line, task_name: str = None):\n    def preprocess(text):\n        \"\"\"Comes from AiHarness\"\"\"\n        # text = text.strip()\n        # NOTE: Brackets are artifacts of the WikiHow dataset portion of HellaSwag.\n        text = text.replace(\" [title]\", \". \")\n        text = re.sub(\"\\\\[.*?\\\\]\", \"\", text)\n        text = text.replace(\"  \", \" \")\n        return text\n\n    ctx = f\"{line['ctx_a']} {line['ctx_b'].capitalize()} \"\n    return Doc(\n        task_name=task_name,\n        query=preprocess(line[\"activity_label\"] + \": \" + ctx),\n        choices=[\" \" + preprocess(ending) for ending in line[\"endings\"]],\n        gold_index=int(line[\"label\"]) if line[\"label\"] != \"\" else -1,  # -1 for test\n    )\n\ndef prompt_commonsense_qa(line, task_name: str = None):\n    return Doc(\n        task_name=task_name,\n        query=line[\"question\"],\n        choices=[f\" {c}\" for c in line[\"choices\"][\"text\"]],\n        gold_index=line[\"choices\"][\"label\"].index(line[\"answerKey\"].strip()),\n        instruction=\"\",\n    )\n\ndef mmlu_pro_mc_prompt(line, task_name: str = None):\n    options = line[\"options\"]\n    letters = [chr(ord(\"A\") + i) for i in range(len(options))]\n    topic = line[\"category\"].replace('_', ' ')\n    query = f\"The following are multiple choice questions (with answers) about {topic}.\\n\\n\"\n    query += line[\"question\"] + \"\\n\"\n    query += \"\".join([f\"{letter}. {choice}\\n\" for letter, choice in zip(letters, options)])\n    query += \"Answer:\"\n\n    return Doc(\n        task_name=task_name,\n        query=query,\n        choices=letters,\n        gold_index=line[\"answer_index\"],\n        instruction=f\"The following are multiple choice questions (with answers) about {topic}.\\n\\n\",\n    )\n\ndef mmlu_cloze_prompt(line, task_name: str = None):\n    \"\"\"MMLU prompt without choices\"\"\"\n    topic = line[\"subject\"]\n    prompt = f\"The following are questions about {topic.replace('_', ' ')}.\\nQuestion: \"\n    prompt += line[\"question\"] + \"\\nAnswer:\"\n\n    return Doc(\n        task_name=task_name,\n        query=prompt,\n        choices=[f\" {c}\" for c in line[\"choices\"]],\n        gold_index=int(line[\"answer\"]),\n        instruction=f\"The following are questions about {topic.replace('_', ' ')}.\\n\",\n    )\n\ndef bbh_prompt(line, task_name: str = None):\n    return Doc(\n        task_name=task_name,\n        query=\"Question: \" + line[\"input\"] + \"\\nAnswer: \",\n        choices=[line[\"target\"]],\n        gold_index=0,\n    )\n\ndef prompt_math(line, task_name: str = None):\n    return Doc(\n        task_name=task_name,\n        query=f\"{line['problem']}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\\n\\n\",\n        gold_index=0,\n        choices=[f\"{line['solution']}\\n\\n\"],\n    )\n\n\nTASKS_TABLE = [\n    LightevalTaskConfig(\n        name=\"arc:easy\",\n        prompt_function=prompt.arc,\n        suite=[\"custom\"],\n        hf_repo=\"ai2_arc\",\n        hf_revision=\"210d026faf9955653af8916fad021475a3f00453\",\n        hf_subset=\"ARC-Easy\",\n        evaluation_splits=[\"test\"],\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"arc:challenge\",\n        prompt_function=prompt.arc,\n        suite=[\"custom\"],\n        hf_repo=\"ai2_arc\",\n        hf_revision=\"210d026faf9955653af8916fad021475a3f00453\",\n        hf_subset=\"ARC-Challenge\",\n        evaluation_splits=[\"test\"],\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"openbook_qa\",\n        prompt_function=prompt.openbookqa,\n        suite=[\"custom\"],\n        hf_repo=\"allenai/openbookqa\",\n        hf_subset=\"main\",\n        hf_revision=\"388097ea7776314e93a529163e0fea805b8a6454\",\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"hellaswag\",\n        prompt_function=prompt_hellaswag,\n        suite=[\"custom\"],\n        hf_repo=\"Rowan/hellaswag\",\n        hf_subset=\"default\",\n        hf_revision=\"6002345709e0801764318f06bf06ce1e7d1a1fe3\",\n        trust_dataset=True,\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"commonsense_qa\",\n        prompt_function=prompt_commonsense_qa,\n        suite=[\"custom\"],\n        hf_repo=\"tau/commonsense_qa\",\n        hf_subset=\"default\",\n        hf_revision=\"94630fe30dad47192a8546eb75f094926d47e155\",\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"winogrande\",\n        prompt_function=prompt.winogrande,\n        suite=[\"custom\"],\n        hf_repo=\"allenai/winogrande\",\n        hf_subset=\"winogrande_xl\",\n        hf_revision=\"85ac5b5a3b7a930e22d590176e39460400d19e41\",\n        trust_dataset=True,\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"piqa\",\n        prompt_function=prompt.piqa_harness,\n        suite=[\"custom\"],\n        hf_repo=\"ybisk/piqa\",\n        hf_subset=\"plain_text\",\n        hf_revision=\"2e8ac2dffd59bac8c3c6714948f4c551a0848bb0\",\n        trust_dataset=True,\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n    ),\n    LightevalTaskConfig(\n        name=\"trivia_qa\",\n        prompt_function=prompt.triviaqa,\n        suite=[\"custom\"],\n        hf_repo=\"mandarjoshi/trivia_qa\",\n        hf_subset=\"rc.nocontext\",\n        hf_revision=\"0f7faf33a3908546c6fd5b73a660e0f8ff173c2f\",\n        hf_avail_splits=[\"train\", \"validation\"],\n        evaluation_splits=[\"validation\"],\n        metric=[Metrics.quasi_exact_match_triviaqa],\n        generation_size=20,\n        trust_dataset=True,\n        stop_sequence=[\"\\n\", \".\", \",\"],\n        few_shots_select=\"random_sampling_from_train\",\n    ),\n    LightevalTaskConfig(\n        name=\"mmlu_pro\",\n        prompt_function=mmlu_pro_mc_prompt,\n        suite=[\"custom\"],\n        hf_repo=\"TIGER-Lab/MMLU-Pro\",\n        hf_subset=\"default\",\n        hf_revision=\"3373e0b32277875b8db2aa555a333b78a08477ea\",\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n        evaluation_splits=[\"test\"],\n        few_shots_split=\"validation\",\n    ),\n    LightevalTaskConfig(\n        name=\"gsm8k\",\n        prompt_function=prompt.gsm8k,\n        suite=[\"custom\"],\n        hf_repo=\"openai/gsm8k\",\n        hf_subset=\"main\",\n        hf_revision=\"e53f048856ff4f594e959d75785d2c2d37b678ee\",\n        hf_avail_splits=[\"train\", \"test\"],\n        evaluation_splits=[\"test\"],\n        metric=[Metrics.quasi_exact_match_gsm8k],\n        generation_size=256,\n        stop_sequence=[\"Question:\", \"Question\"],\n        few_shots_select=\"random_sampling_from_train\",\n    ),\n    LightevalTaskConfig(\n        name=\"mmlu_stem\",\n        prompt_function=mmlu_cloze_prompt,\n        suite=[\"custom\"],\n        hf_repo=\"TIGER-Lab/MMLU-STEM\",\n        hf_subset=\"default\",\n        hf_revision=\"78a4b40757f31688d00426d1372dbbc6070d33a8\",\n        hf_avail_splits=[\"test\"],\n        evaluation_splits=[\"test\"],\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n        generation_size=-1,\n    ),\n    LightevalTaskConfig(\n        name=\"mmlu\",\n        prompt_function=mmlu_cloze_prompt,\n        suite=[\"custom\"],\n        hf_repo=\"cais/mmlu\",\n        hf_subset=\"all\",\n        hf_revision=\"c30699e8356da336a370243923dbaf21066bb9fe\",\n        hf_avail_splits=[\"test\"],\n        evaluation_splits=[\"test\"],\n        metric=[Metrics.loglikelihood_acc_norm_nospace],\n        generation_size=-1,\n    ),\n]\n\nBBH_TASKS = [\n    LightevalTaskConfig(\n        name=f\"bbh:{subset}\",\n        prompt_function=bbh_prompt,\n        suite=[\"custom\"],\n        hf_repo=\"lighteval/big_bench_hard\",\n        hf_subset=subset,\n        hf_revision=\"80610173426f05e6f1448f047e2db4840a7dd899\",\n        metric=[Metrics.exact_match],\n        hf_avail_splits=[\"train\"],\n        # this is the only split available, obviously not used in training\n        evaluation_splits=[\"train\"],\n        few_shots_split=\"train\",\n        trust_dataset=True,\n        stop_sequence=[\"Question:\", \"Question\"],\n    )\n    for subset in [\n        \"boolean_expressions\",\n        \"causal_judgement\",\n        \"date_understanding\",\n        \"disambiguation_qa\",\n        \"dyck_languages\",\n        \"formal_fallacies\",\n        \"geometric_shapes\",\n        \"hyperbaton\",\n        \"logical_deduction_five_objects\",\n        \"logical_deduction_seven_objects\",\n        \"logical_deduction_three_objects\",\n        \"movie_recommendation\",\n        \"multistep_arithmetic_two\",\n        \"navigate\",\n        \"object_counting\",\n        \"penguins_in_a_table\",\n        \"reasoning_about_colored_objects\",\n        \"ruin_names\",\n        \"salient_translation_error_detection\",\n        \"snarks\",\n        \"sports_understanding\",\n        \"temporal_sequences\",\n        \"tracking_shuffled_objects_five_objects\",\n        \"tracking_shuffled_objects_seven_objects\",\n        \"tracking_shuffled_objects_three_objects\",\n        \"web_of_lies\",\n        \"word_sorting\",\n    ]\n]\n\nTASKS_TABLE.extend(BBH_TASKS)\n\nquasi_exact_match_math = SampleLevelMetric(\n    metric_name=\"qem\",\n    sample_level_fn=ExactMatches(\n        strip_strings=True,\n        normalize_pred=lambda text: parse_math_answer(text, \"math\"),\n        normalize_gold=lambda text: parse_math_answer(text, \"math\")\n    ).compute,\n    category=MetricCategory.GENERATIVE,\n    use_case=MetricUseCase.MATH,\n    corpus_level_fn=np.mean,\n    higher_is_better=True,\n)\n\nMATH_TASKS = [\n    LightevalTaskConfig(\n        name=\"math\",\n        prompt_function=prompt_math,\n        suite=[\"custom\"],\n        hf_repo=\"HuggingFaceTB/math_tasks\",\n        hf_subset=\"math\",\n        hf_revision=\"3d34f1076f279000b9315583dcdacfd288898283\",\n        hf_avail_splits=[\"train\", \"test\", \"demo\"],\n        evaluation_splits=[\"test\"],\n        metric=[quasi_exact_match_math],\n        generation_size=1024,\n        stop_sequence=[\"\\n\\n\"],\n        few_shots_split=\"demo\",\n        few_shots_select=\"sequential\",\n        trust_dataset=True,\n    )\n]\n\nTASKS_TABLE.extend(MATH_TASKS)\n\n## MMLU ##\nclass CustomMMLUEvaluationTask(LightevalTaskConfig):\n    def __init__(\n        self,\n        name,\n        prompt_function=None,\n        hf_repo=\"lighteval/mmlu\",\n        hf_subset=None,\n        #  metric=[Metrics.loglikelihood_acc_single_token],\n        metric=[Metrics.loglikelihood_acc, Metrics.loglikelihood_acc_norm_nospace],\n        hf_avail_splits=None,\n        evaluation_splits=[\"test\"],\n        few_shots_split=\"dev\",\n        few_shots_select=None,\n        suite=[\"custom\"],\n        generation_size=-1,\n        stop_sequence=None,\n        output_regex=None,\n        frozen=False,\n    ):\n        super().__init__(\n            name=name,\n            prompt_function=prompt_function,\n            suite=suite,\n            hf_repo=hf_repo,\n            hf_subset=hf_subset,\n            metric=metric,\n            hf_avail_splits=hf_avail_splits,\n            evaluation_splits=evaluation_splits,\n            few_shots_split=few_shots_split,\n            few_shots_select=few_shots_select,\n            generation_size=generation_size,\n            stop_sequence=stop_sequence,\n            output_regex=output_regex,\n            frozen=frozen,\n        )\n\n\nif __name__ == \"__main__\":\n    print(t.name for t in TASKS_TABLE)\n    print(len(TASKS_TABLE))"}
{"type": "source_file", "path": "src/conversation/convert_weights.py", "content": "# copied from https://github.com/huggingface/nanotron/blob/main/examples/llama/convert_weights.py\n\nimport json\nfrom pathlib import Path\nfrom typing import Optional\n\nimport nanotron\nimport torch\nfrom nanotron.config import LlamaConfig as NanotronLlamaConfig\nfrom nanotron.models.llama import LlamaForTraining\nfrom nanotron.trainer import mark_tied_parameters\n\n\ndef get_weight_mapping(config: NanotronLlamaConfig, nt_to_hf: bool = True) -> dict[str, str]:\n    \"\"\"Returns the nanotron to huggingface parameter mapping if `nt_to_hf`, otherwise the\n    huggingface to nanotron mapping.\"\"\"\n\n    hf_to_nt_map = {}\n    hf_to_nt_map[\"lm_head.weight\"] = \"model.lm_head.pp_block.weight\"\n    hf_to_nt_map[\"model.embed_tokens.weight\"] = \"model.token_position_embeddings.pp_block.token_embedding.weight\"\n    hf_to_nt_map[\"model.norm.weight\"] = \"model.final_layer_norm.pp_block.weight\"\n\n    for i in range(config.num_hidden_layers):\n        hf_prefix = f\"model.layers.{i}\"\n        nt_prefix = f\"model.decoder.{i}.pp_block\"\n        # hf_to_nt_map[f\"{hf_prefix}.self_attn.q_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        # hf_to_nt_map[f\"{hf_prefix}.self_attn.k_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        # hf_to_nt_map[f\"{hf_prefix}.self_attn.v_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.q_proj.weight\"] = f\"{nt_prefix}.attn.q_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.k_proj.weight\"] = f\"{nt_prefix}.attn.k_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.v_proj.weight\"] = f\"{nt_prefix}.attn.v_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.o_proj.weight\"] = f\"{nt_prefix}.attn.o_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.gate_proj.weight\"] = f\"{nt_prefix}.mlp.gate_up_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.gate_proj.bias\"] = f\"{nt_prefix}.mlp.gate_up_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.up_proj.weight\"] = f\"{nt_prefix}.mlp.gate_up_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.up_proj.bias\"] = f\"{nt_prefix}.mlp.gate_up_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.down_proj.weight\"] = f\"{nt_prefix}.mlp.down_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.down_proj.bias\"] = f\"{nt_prefix}.mlp.down_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.input_layernorm.weight\"] = f\"{nt_prefix}.input_layernorm.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.post_attention_layernorm.weight\"] = f\"{nt_prefix}.post_attention_layernorm.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.W_k_r.weight\"] = f\"{nt_prefix}.attn.W_k_r.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.W_down_k.weight\"] = f\"{nt_prefix}.attn.W_down_k.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.W_down_v.weight\"] = f\"{nt_prefix}.attn.W_down_v.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.W_up_k.weight\"] = f\"{nt_prefix}.attn.W_up_k.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.W_up_v.weight\"] = f\"{nt_prefix}.attn.W_up_v.weight\"\n        # auto_encoder\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.auto_encoder.W_down_k.weight\"] = f\"{nt_prefix}.attn.auto_encoder.W_down_k.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.auto_encoder.W_down_v.weight\"] = f\"{nt_prefix}.attn.auto_encoder.W_down_v.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.auto_encoder.W_up_k.weight\"] = f\"{nt_prefix}.attn.auto_encoder.W_up_k.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.auto_encoder.W_up_v.weight\"] = f\"{nt_prefix}.attn.auto_encoder.W_up_v.weight\"\n\n\n    if nt_to_hf:\n        nt_to_hf_map = {}\n        for hf, nt in hf_to_nt_map.items():\n            # Because the qkv and gate_up projections are separated in the\n            # huggingface format, when we return nanotron to huggingface\n            # we will need to return a list of parameters instead (e.g.\n            # the `qkv_proj` will point to a list `[q_proj, k_proj, v_proj]`).\n            if nt in nt_to_hf_map and isinstance(nt_to_hf_map[nt], list):\n                nt_to_hf_map[nt].append(hf)\n            elif nt in nt_to_hf_map:\n                nt_to_hf_map[nt] = [nt_to_hf_map[nt], hf]\n            else:\n                nt_to_hf_map[nt] = hf\n        return nt_to_hf_map\n    return hf_to_nt_map\n\n\ndef get_config_mapping(nt_to_hf: bool = True) -> dict[str, str]:\n    \"\"\"Returns either the nanotron to huggingface (if `nt_to_hf`)\n    configuration mapping, or the huggingface to nanotron.\"\"\"\n\n    hf_to_nt_map = {\n        \"bos_token_id\": \"bos_token_id\",\n        \"eos_token_id\": \"eos_token_id\",\n        \"hidden_act\": \"hidden_act\",\n        \"hidden_size\": \"hidden_size\",\n        \"initializer_range\": \"initializer_range\",\n        \"intermediate_size\": \"intermediate_size\",\n        \"max_position_embeddings\": \"max_position_embeddings\",\n        \"num_attention_heads\": \"num_attention_heads\",\n        \"num_hidden_layers\": \"num_hidden_layers\",\n        \"num_key_value_heads\": \"num_key_value_heads\",\n        \"pad_token_id\": \"pad_token_id\",\n        \"pretraining_tp\": \"pretraining_tp\",\n        \"rms_norm_eps\": \"rms_norm_eps\",\n        \"rope_scaling\": \"rope_scaling\",\n        \"rope_theta\": \"rope_theta\",\n        \"tie_word_embeddings\": \"tie_word_embeddings\",\n        \"use_cache\": \"use_cache\",\n        \"vocab_size\": \"vocab_size\",\n        \"RoPE\": \"RoPE\",\n        \"SVD\": \"SVD\",\n        \"AE\": \"AE\",\n    }\n    if nt_to_hf:\n        return {nt: hf for hf, nt in hf_to_nt_map.items()}\n    return hf_to_nt_map\n\n\ndef make_parallel_config(\n    dp: int = 1,\n    pp: int = 1,\n    tp: int = 1,\n):\n    parallel_config = nanotron.config.ParallelismArgs(\n        dp=dp,\n        pp=pp,\n        tp=tp,\n        pp_engine=nanotron.config.AllForwardAllBackwardPipelineEngine(),\n        tp_mode=nanotron.config.TensorParallelLinearMode.ALL_REDUCE,\n        tp_linear_async_communication=False,\n    )\n    return parallel_config\n\n\ndef load_nanotron_model(\n    model_config: Optional[NanotronLlamaConfig] = None,\n    device: torch.device = torch.device(\"cuda\"),\n    dtype: torch.dtype = torch.bfloat16,\n    checkpoint_path: Optional[Path] = None,\n) -> LlamaForTraining:\n    \"\"\"\n    Creates and returns a nanotron model.\n    If `model_config` is None, then `checkpoint_path` must be set, in which case\n    the configuration will be loaded from such path.\n    If `checkpoint_path` is None, then `model_config` must be set, in which case\n    the model created will have random weights.\n    \"\"\"\n\n    if model_config is None:\n        assert checkpoint_path is not None\n        with open(checkpoint_path / \"model_config.json\") as f:\n            model_config = NanotronLlamaConfig(**json.load(f))\n    parallel_config = make_parallel_config()\n    parallel_context = nanotron.parallel.ParallelContext(\n        data_parallel_size=parallel_config.dp,\n        pipeline_parallel_size=parallel_config.pp,\n        tensor_parallel_size=parallel_config.tp,\n    )\n    nanotron_model = nanotron.models.build_model(\n        model_builder=lambda: LlamaForTraining(\n            config=model_config,\n            parallel_context=parallel_context,\n            parallel_config=parallel_config,\n            random_states=None,\n        ),\n        parallel_context=parallel_context,\n        dtype=dtype,\n        device=device,\n    )\n    mark_tied_parameters(model=nanotron_model, parallel_context=parallel_context)\n    # Load checkpoint directly in memory and then only keep the state dictionary\n    if checkpoint_path is not None:\n        nanotron.serialize.load_weights(\n            model=nanotron_model, parallel_context=parallel_context, root_folder=checkpoint_path\n        )\n    return nanotron_model\n"}
{"type": "source_file", "path": "src/conversation/original_convert_weights.py", "content": "import json\nfrom pathlib import Path\nfrom typing import Optional\n\nimport nanotron\nimport torch\nfrom nanotron.config import LlamaConfig as NanotronLlamaConfig\nfrom nanotron.models.llama import LlamaForTraining\nfrom nanotron.trainer import mark_tied_parameters\n\n\ndef get_weight_mapping(config: NanotronLlamaConfig, nt_to_hf: bool = True) -> dict[str, str]:\n    \"\"\"Returns the nanotron to huggingface parameter mapping if `nt_to_hf`, otherwise the\n    huggingface to nanotron mapping.\"\"\"\n\n    hf_to_nt_map = {}\n    hf_to_nt_map[\"lm_head.weight\"] = \"model.lm_head.pp_block.weight\"\n    hf_to_nt_map[\"model.embed_tokens.weight\"] = \"model.token_position_embeddings.pp_block.token_embedding.weight\"\n    hf_to_nt_map[\"model.norm.weight\"] = \"model.final_layer_norm.pp_block.weight\"\n    hf_to_nt_map[\"model.embed_tokens.weight\"] = \"model.token_position_embeddings.pp_block.token_embedding.weight\"\n\n    for i in range(config.num_hidden_layers):\n        hf_prefix = f\"model.layers.{i}\"\n        nt_prefix = f\"model.decoder.{i}.pp_block\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.q_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.k_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.v_proj.weight\"] = f\"{nt_prefix}.attn.qkv_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.self_attn.o_proj.weight\"] = f\"{nt_prefix}.attn.o_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.gate_proj.weight\"] = f\"{nt_prefix}.mlp.gate_up_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.gate_proj.bias\"] = f\"{nt_prefix}.mlp.gate_up_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.up_proj.weight\"] = f\"{nt_prefix}.mlp.gate_up_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.up_proj.bias\"] = f\"{nt_prefix}.mlp.gate_up_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.down_proj.weight\"] = f\"{nt_prefix}.mlp.down_proj.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.mlp.down_proj.bias\"] = f\"{nt_prefix}.mlp.down_proj.bias\"\n        hf_to_nt_map[f\"{hf_prefix}.input_layernorm.weight\"] = f\"{nt_prefix}.input_layernorm.weight\"\n        hf_to_nt_map[f\"{hf_prefix}.post_attention_layernorm.weight\"] = f\"{nt_prefix}.post_attention_layernorm.weight\"\n\n    if nt_to_hf:\n        nt_to_hf_map = {}\n        for hf, nt in hf_to_nt_map.items():\n            # Because the qkv and gate_up projections are separated in the\n            # huggingface format, when we return nanotron to huggingface\n            # we will need to return a list of parameters instead (e.g.\n            # the `qkv_proj` will point to a list `[q_proj, k_proj, v_proj]`).\n            if nt in nt_to_hf_map and isinstance(nt_to_hf_map[nt], list):\n                nt_to_hf_map[nt].append(hf)\n            elif nt in nt_to_hf_map:\n                nt_to_hf_map[nt] = [nt_to_hf_map[nt], hf]\n            else:\n                nt_to_hf_map[nt] = hf\n        return nt_to_hf_map\n    return hf_to_nt_map\n\n\ndef get_config_mapping(nt_to_hf: bool = True) -> dict[str, str]:\n    \"\"\"Returns either the nanotron to huggingface (if `nt_to_hf`)\n    configuration mapping, or the huggingface to nanotron.\"\"\"\n\n    hf_to_nt_map = {\n        \"bos_token_id\": \"bos_token_id\",\n        \"eos_token_id\": \"eos_token_id\",\n        \"hidden_act\": \"hidden_act\",\n        \"hidden_size\": \"hidden_size\",\n        \"initializer_range\": \"initializer_range\",\n        \"intermediate_size\": \"intermediate_size\",\n        \"max_position_embeddings\": \"max_position_embeddings\",\n        \"num_attention_heads\": \"num_attention_heads\",\n        \"num_hidden_layers\": \"num_hidden_layers\",\n        \"num_key_value_heads\": \"num_key_value_heads\",\n        \"pad_token_id\": \"pad_token_id\",\n        \"pretraining_tp\": \"pretraining_tp\",\n        \"rms_norm_eps\": \"rms_norm_eps\",\n        \"rope_scaling\": \"rope_scaling\",\n        \"rope_theta\": \"rope_theta\",\n        \"tie_word_embeddings\": \"tie_word_embeddings\",\n        \"use_cache\": \"use_cache\",\n        \"vocab_size\": \"vocab_size\",\n    }\n    if nt_to_hf:\n        return {nt: hf for hf, nt in hf_to_nt_map.items()}\n    return hf_to_nt_map\n\n\ndef make_parallel_config(\n    dp: int = 1,\n    pp: int = 1,\n    tp: int = 1,\n):\n    parallel_config = nanotron.config.ParallelismArgs(\n        dp=dp,\n        pp=pp,\n        tp=tp,\n        pp_engine=nanotron.config.AllForwardAllBackwardPipelineEngine(),\n        tp_mode=nanotron.config.TensorParallelLinearMode.ALL_REDUCE,\n        tp_linear_async_communication=False,\n    )\n    return parallel_config\n\n\ndef load_nanotron_model(\n    model_config: Optional[NanotronLlamaConfig] = None,\n    device: torch.device = torch.device(\"cuda\"),\n    dtype: torch.dtype = torch.bfloat16,\n    checkpoint_path: Optional[Path] = None,\n) -> LlamaForTraining:\n    \"\"\"\n    Creates and returns a nanotron model.\n    If `model_config` is None, then `checkpoint_path` must be set, in which case\n    the configuration will be loaded from such path.\n    If `checkpoint_path` is None, then `model_config` must be set, in which case\n    the model created will have random weights.\n    \"\"\"\n\n    if model_config is None:\n        assert checkpoint_path is not None\n        with open(checkpoint_path / \"model_config.json\") as f:\n            model_config = NanotronLlamaConfig(**json.load(f))\n    parallel_config = make_parallel_config()\n    parallel_context = nanotron.parallel.ParallelContext(\n        data_parallel_size=parallel_config.dp,\n        pipeline_parallel_size=parallel_config.pp,\n        tensor_parallel_size=parallel_config.tp,\n    )\n    nanotron_model = nanotron.models.build_model(\n        model_builder=lambda: LlamaForTraining(\n            config=model_config,\n            parallel_context=parallel_context,\n            parallel_config=parallel_config,\n            random_states=None,\n        ),\n        parallel_context=parallel_context,\n        dtype=dtype,\n        device=device,\n    )\n    mark_tied_parameters(model=nanotron_model, parallel_context=parallel_context)\n    # Load checkpoint directly in memory and then only keep the state dictionary\n    if checkpoint_path is not None:\n        nanotron.serialize.load_weights(\n            model=nanotron_model, parallel_context=parallel_context, root_folder=checkpoint_path\n        )\n    return nanotron_model"}
{"type": "source_file", "path": "src/mha2mla/longbench.py", "content": "import bisect\nfrom dataclasses import dataclass\nimport os\nimport re\nimport string\nfrom collections import Counter, defaultdict\nfrom itertools import accumulate\nfrom typing import TYPE_CHECKING, Optional\nfrom transformers import HfArgumentParser, AutoTokenizer, AutoModelForCausalLM\nimport logging\nimport jieba\nimport pandas as pd\nimport torch#\nimport transformers\nfrom accelerate.utils import gather_object\nfrom datasets import load_dataset\nfrom fuzzywuzzy import fuzz\nfrom rouge import Rouge\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import TrainerCallback\nfrom typing_extensions import override\nfrom transformers import (\n    PreTrainedTokenizerBase,\n    Trainer,\n    TrainerControl,\n    TrainerState,\n    TrainingArguments,\n)\n\n\n@dataclass\nclass LongBenchArguments:\n    longbench: bool = False\n    lb_max_tokens: int = 2048\n    lb_batch_size: int = 16\n\n\n@dataclass\nclass MLAArguments:\n    is_partial_rope: bool = False\n    is_mla: bool = False\n    model_path: str = \"\"\n    tokenizer_path: str = \"\"\n    dtype: str = \"float32\"\n\n\n@dataclass\nclass CacheArguments:\n    cache_implementation: str = None\n    backend: str = \"quanto\"\n    nbits: int = 8\n    residual_length: int = 128\n\n\n\nlogger = logging.getLogger(__name__)\n\nLEN2NAME = {1 << (10 + i): f\"{1<<i}k\" for i in range(9)}\n\n\nclass RunLongBenchCallback(TrainerCallback):\n    def __init__(\n        self,\n        args: \"LongBenchArguments\",\n        trainer: \"Trainer\",\n        cache_args: \"CacheArguments\",\n    ):\n        super().__init__()\n        self.args = args\n        self.cache_args = cache_args\n        self.trainer = trainer\n        self.model = trainer.model\n        self.tokenizer: \"PreTrainedTokenizerBase\" = trainer.tokenizer  # type: ignore\n        self.tokenizer.padding_side = \"left\"\n        self.is_world_process_zero = self.trainer.is_world_process_zero()\n\n        dataset = LongBenchV1Dataset(args, self.tokenizer)\n        dataloader = DataLoader(\n            dataset, batch_size=self.args.lb_batch_size, collate_fn=dataset.collate_fn\n        )\n        self.dataloaderV1 = self.trainer.accelerator.prepare(dataloader)\n\n    @override\n    def on_evaluate(\n        self,\n        args: \"TrainingArguments\",\n        state: \"TrainerState\",\n        control: \"TrainerControl\",\n        **kwargs,\n    ):\n        output_dir = None\n        if not args.do_train:\n            output_dir = args.output_dir  # save result when only do_eval\n        self.run(output_dir)\n\n    @torch.no_grad()\n    def run(self, output_dir: Optional[str] = None):\n        self.runV1(self.dataloaderV1, \"lbv1\", output_dir)\n\n\n    def runV1(\n        self,\n        dataloader,\n        task: str,\n        output_dir: Optional[str] = None,\n        soft_eval: bool = False,\n    ):\n        with torch.cuda.device(self.model.device):\n            torch.cuda.empty_cache()\n        if output_dir is not None and not os.path.exists(output_dir):\n            output_dir = None\n            logger.warning(f\"{task}: {output_dir=} does not exist, skipping output.\")\n        self.model.eval()\n        rst = []\n        for batch in tqdm(dataloader, disable=not self.is_world_process_zero):\n            _id = batch.pop(\"_id\")\n            dataset = batch.pop(\"dataset\")\n            answers = batch.pop(\"answers\")\n            all_classes = batch.pop(\"all_classes\")\n            max_gen = batch.pop(\"max_gen\")\n            should_save = batch.pop(\"save\")\n            pred = self.inferV1(batch, max_gen)\n            score = [\n                self.evalV1(d, a, c, p)\n                for d, a, c, p in zip(dataset, answers, all_classes, pred)\n            ]\n            assert (\n                len(_id)\n                == len(dataset)\n                == len(answers)\n                == len(all_classes)\n                == len(max_gen)\n                == len(should_save)\n                == len(pred)\n                == len(score)\n            ), f\"{len(_id)=}, {len(dataset)=}, {len(answers)=}, {len(all_classes)=}, {len(max_gen)=}, {len(should_save)=} {len(pred)=}, {len(score)=}\"\n            for i, d, a, c, save, p, s in zip(\n                _id, dataset, answers, all_classes, should_save, pred, score\n            ):\n                rst.append((i, s, d, p if save else \"\", a, c))\n        rst = gather_object(rst)\n        rst_bak = rst\n        rst = []\n        rst_keys = {}\n        for _id, *rest in rst_bak:\n            if _id not in rst_keys:\n                rst_keys[_id] = len(rst)\n                rst.append((_id, *rest))\n        logger.warning_once(f\"{task} all gather {len(rst)=}\")\n        assert len(rst) == len(dataloader.dataset)\n\n        if self.is_world_process_zero:\n            df = pd.DataFrame(\n                rst,\n                columns=[\"_id\", \"score\", \"dataset\", \"pred\", \"answers\", \"all_classes\"],\n            )\n            name = LEN2NAME[self.args.lb_max_tokens]\n            if output_dir is not None:\n                # run_name = (\n                #     f\"_{self.trainer.args.run_name}\"\n                #     if self.trainer.args.run_name\n                #     else \"\"\n                # )\n                run_name = \"\"\n                file_path = os.path.join(output_dir, f\"{task}{run_name}_{name}.jsonl\")\n\n                parent_dir = os.path.dirname(file_path)\n                os.makedirs(parent_dir, exist_ok=True)\n\n                if os.path.exists(file_path):\n                    logger.warning(f\"{file_path} already exists, overwriting.\")\n                else:\n                    logger.info(f\"Saving {task} to {file_path}\")\n                df.to_json(file_path, orient=\"records\", lines=True, force_ascii=True)\n            group_mean = df.groupby(\"dataset\")[\"score\"].mean()\n            overall_mean = group_mean.mean()\n            logger.info(f\"{task} dataset score: {group_mean}\")\n            logger.info(f\"{task} overall score: {overall_mean * 100:.1f}\")\n\n    def inferV1(self, batch, max_gen: list[int]):\n        gen_config = self.model.generation_config\n        gen_config.max_new_tokens = max(max_gen)\n        gen_config.num_beams = 1\n        gen_config.do_sample = False\n        gen_config.pad_token_id = self.tokenizer.eos_token_id\n        cache_config = {\n            \"backend\": self.cache_args.backend,\n            \"nbits\": self.cache_args.nbits,\n            \"residual_length\": self.cache_args.residual_length,\n            \"compute_dtype\": self.model.dtype,\n            \"device\": self.model.device,\n        }\n        output = self.model.generate(\n            **batch,\n            generation_config=gen_config,\n            cache_implementation=self.cache_args.cache_implementation,\n            **(\n                {\"cache_config\": cache_config}\n                if self.cache_args.cache_implementation == \"quantized\"\n                else {}\n            ),\n        )\n        pred = [\n            self.tokenizer.decode(output[i][-gen_config.max_new_tokens :][:gen_len])\n            for i, gen_len in enumerate(max_gen)\n        ]\n        return pred\n\n    def evalV1(\n        self, dataset: str, answers: list[str], all_classes: list[str], pred: str\n    ):\n        score = 0.0\n        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n            pred = pred.lstrip(\"\\n\").split(\"\\n\")[0]\n        for ground_truth in answers:\n            score = max(\n                score,\n                dataset2metric[dataset](pred, ground_truth, all_classes=all_classes),\n            )\n        return score\n\n    def _get_tb_writer(self):\n        for callback in self.trainer.callback_handler.callbacks:\n            if isinstance(callback, transformers.integrations.TensorBoardCallback):\n                return callback.tb_writer\n        return None\n\n    def tb_add_scalar(self, *args, **kwargs):\n        if self.tb_writer is None:\n            self.tb_writer = self._get_tb_writer()\n        if self.tb_writer is not None:\n            self.tb_writer.add_scalar(*args, **kwargs)\n\n\nclass LongBenchV2Dataset(Dataset):\n    prompt_template = \"\"\"\nPlease read the following text and answer the question below.\n<text>\n{context}\n</text>\n\nWhat is the correct answer to this question: {question}\nChoices:\n(A) {choice_A}\n(B) {choice_B}\n(C) {choice_C}\n(D) {choice_D}\n\nThe correct answer is\n\"\"\"\n\n    def __init__(\n        self, args: \"LongBenchArguments\", tokenizer: \"PreTrainedTokenizerBase\"\n    ):\n        self.args = args\n        self.tokenizer = tokenizer\n        self.dataset = load_dataset(\"data/LongBench-v2\", split=\"train\")\n\n    def __getitem__(self, idx):\n        data = self.dataset[idx]  # type: ignore\n        assert isinstance(data, dict)\n        context_max = 8 * self.args.lb_max_tokens  # text_len/token_len <4 for en\n        context_half = context_max // 2\n        prompt_truncated = False\n\n        if len(data[\"context\"]) > context_max:  # truncate to save CPU\n            data[\"context\"] = (\n                data[\"context\"][:context_half] + data[\"context\"][-context_half:]\n            )\n            prompt_truncated = True\n        prompt = self.prompt_template.strip().format(**data)  # strip to remove extra \\n\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n        prompt_max = self.args.lb_max_tokens - 1  # for answer token\n        half = prompt_max // 2\n        if input_ids.shape[0] > prompt_max:\n            input_ids = torch.cat([input_ids[: prompt_max - half], input_ids[-half:]])\n            assert input_ids.shape[0] == prompt_max\n        # else:\n        #     assert not prompt_truncated\n        return {\n            \"input_ids\": input_ids,\n            \"_id\": data[\"_id\"],\n            \"difficulty\": data[\"difficulty\"],\n            \"length\": data[\"length\"],\n            \"answer\": data[\"answer\"],\n        }\n\n    def __len__(self):\n        return len(self.dataset)  # type: ignore\n\n    def collate_fn(self, samples):\n        batch = {}\n        for k in samples[0].keys():\n            batch[k] = [f[k] for f in samples]\n        batch.update(\n            self.tokenizer.pad({\"input_ids\": batch[\"input_ids\"]}, return_tensors=\"pt\")\n        )\n        return batch\n\n\nclass LongBenchV1Dataset(Dataset):\n    dataset2prompt = {\n        \"narrativeqa\": \"You are given a story, which can be either a novel or a movie script, and a question. Answer the question asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nStory: {context}\\n\\nNow, answer the question based on the story asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:\",\n        \"qasper\": 'You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\\n\\nArticle: {context}\\n\\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:',\n        \"multifieldqa_en\": \"Read the following text and answer briefly.\\n\\n{context}\\n\\nNow, answer the following question based on the above text, only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n        \"multifieldqa_zh\": \"阅读以下文字并用中文简短回答：\\n\\n{context}\\n\\n现在请基于上面的文章回答下面的问题，只告诉我答案，不要输出任何其他字词。\\n\\n问题：{input}\\n回答：\",\n        \"hotpotqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n        \"2wikimqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n        \"musique\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n        \"dureader\": \"请基于给定的文章回答下述问题。\\n\\n文章：{context}\\n\\n请基于上述文章回答下面的问题。\\n\\n问题：{input}\\n回答：\",\n        \"gov_report\": \"You are given a report by a government agency. Write a one-page summary of the report.\\n\\nReport:\\n{context}\\n\\nNow, write a one-page summary of the report.\\n\\nSummary:\",\n        \"qmsum\": \"You are given a meeting transcript and a query containing a question or instruction. Answer the query in one or more sentences.\\n\\nTranscript:\\n{context}\\n\\nNow, answer the query based on the above meeting transcript in one or more sentences.\\n\\nQuery: {input}\\nAnswer:\",\n        \"multi_news\": \"You are given several news passages. Write a one-page summary of all news. \\n\\nNews:\\n{context}\\n\\nNow, write a one-page summary of all the news.\\n\\nSummary:\",\n        \"vcsum\": \"下面有一段会议记录，请你阅读后，写一段总结，总结会议的内容。\\n会议记录：\\n{context}\\n\\n会议总结：\",\n        \"trec\": \"Please determine the type of the question below. Here are some examples of questions.\\n\\n{context}\\n{input}\",\n        \"triviaqa\": \"Answer the question based on the given passage. Only give me the answer and do not output any other words. The following are some examples.\\n\\n{context}\\n\\n{input}\",\n        \"samsum\": \"Summarize the dialogue into a few short sentences. The following are some examples.\\n\\n{context}\\n\\n{input}\",\n        \"lsht\": \"请判断给定新闻的类别，下面是一些例子。\\n\\n{context}\\n{input}\",\n        \"passage_count\": \"There are some paragraphs below sourced from Wikipedia. Some of them may be duplicates. Please carefully read these paragraphs and determine how many unique paragraphs there are after removing duplicates. In other words, how many non-repeating paragraphs are there in total?\\n\\n{context}\\n\\nPlease enter the final count of unique paragraphs after removing duplicates. The output format should only contain the number, such as 1, 2, 3, and so on.\\n\\nThe final answer is: \",\n        \"passage_retrieval_en\": 'Here are 30 paragraphs from Wikipedia, along with an abstract. Please determine which paragraph the abstract is from.\\n\\n{context}\\n\\nThe following is an abstract.\\n\\n{input}\\n\\nPlease enter the number of the paragraph that the abstract is from. The answer format must be like \"Paragraph 1\", \"Paragraph 2\", etc.\\n\\nThe answer is: ',\n        \"passage_retrieval_zh\": '以下是若干段落文字，以及其中一个段落的摘要。请确定给定的摘要出自哪一段。\\n\\n{context}\\n\\n下面是一个摘要\\n\\n{input}\\n\\n请输入摘要所属段落的编号。答案格式必须是\"段落1\"，\"段落2\"等格式\\n\\n答案是：',\n        \"lcc\": \"Please complete the code given below. \\n{context}Next line of code:\\n\",\n        \"repobench-p\": \"Please complete the code given below. \\n{context}{input}Next line of code:\\n\",\n    }\n    dataset2maxlen = {\n        \"narrativeqa\": 128,\n        \"qasper\": 128,\n        \"multifieldqa_en\": 64,\n        \"multifieldqa_zh\": 64,\n        \"hotpotqa\": 32,\n        \"2wikimqa\": 32,\n        \"musique\": 32,\n        \"dureader\": 128,\n        \"gov_report\": 512,\n        \"qmsum\": 512,\n        \"multi_news\": 512,\n        \"vcsum\": 512,\n        \"trec\": 64,\n        \"triviaqa\": 32,\n        \"samsum\": 128,\n        \"lsht\": 64,\n        \"passage_count\": 32,\n        \"passage_retrieval_en\": 32,\n        \"passage_retrieval_zh\": 32,\n        \"lcc\": 64,\n        \"repobench-p\": 64,\n    }\n\n    def __init__(\n        self, args: \"LongBenchArguments\", tokenizer: \"PreTrainedTokenizerBase\"\n    ):\n        self.args = args\n        self.tokenizer = tokenizer\n        # fmt: off\n        subsets = [\"narrativeqa\", \"qasper\", \"multifieldqa_en\", \"multifieldqa_zh\", \"hotpotqa\", \"2wikimqa\", \"musique\", \"dureader\", \"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\", \"trec\", \"triviaqa\", \"samsum\", \"lsht\", \"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\", \"lcc\", \"repobench-p\"]\n        # fmt: on\n        self.datasets = [\n            load_dataset(\n                \"THUDM/LongBench\",\n                subset,\n                split=\"test\",\n                trust_remote_code=True,\n            )\n            for subset in subsets\n        ]\n        self.acc_lens = [0] + list(\n            accumulate([len(dataset) for dataset in self.datasets])\n        )\n        self.length = self.acc_lens[-1]\n\n    def __getitem__(self, idx):\n        # find data using acc_lens\n        dataset_idx = bisect.bisect_right(self.acc_lens, idx) - 1\n        data = self.datasets[dataset_idx][idx - self.acc_lens[dataset_idx]]\n        assert isinstance(data, dict)\n        name: str = data[\"dataset\"]\n        prompt_template = self.dataset2prompt[name]\n        context_max = 8 * self.args.lb_max_tokens  # text_len/token_len <4 for en\n        context_half = context_max // 2\n        prompt_truncated = False\n        if len(data[\"context\"]) > context_max:  # truncate to save CPU\n            data[\"context\"] = (\n                data[\"context\"][:context_half] + data[\"context\"][-context_half:]\n            )\n            prompt_truncated = True\n        prompt = prompt_template.format(**data)\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n        prompt_max = (\n            self.args.lb_max_tokens - self.dataset2maxlen[name]\n        )  # for answer token\n        half = prompt_max // 2\n        if input_ids.shape[0] > prompt_max:\n            input_ids = torch.cat([input_ids[: prompt_max - half], input_ids[-half:]])\n            assert input_ids.shape[0] == prompt_max\n        else:\n            assert not prompt_truncated\n        return {\n            \"input_ids\": input_ids,\n            \"_id\": data[\"_id\"],\n            \"dataset\": name,\n            \"answers\": data[\"answers\"],\n            \"all_classes\": data[\"all_classes\"],\n            \"max_gen\": self.dataset2maxlen[name],\n            \"save\": idx - self.acc_lens[dataset_idx] < 10,\n        }\n\n    def __len__(self):\n        return self.length\n\n    def collate_fn(self, samples):\n        batch = {}\n        for k in samples[0].keys():\n            batch[k] = [f[k] for f in samples]\n        batch.update(\n            self.tokenizer.pad({\"input_ids\": batch[\"input_ids\"]}, return_tensors=\"pt\")\n        )\n        return batch\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef normalize_zh_answer(s):\n    \"\"\"Lower text and remove punctuation, extra whitespace.\"\"\"\n\n    def white_space_fix(text):\n        return \"\".join(text.split())\n\n    def remove_punc(text):\n        cn_punctuation = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n        all_punctuation = set(string.punctuation + cn_punctuation)\n        return \"\".join(ch for ch in text if ch not in all_punctuation)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\n\ndef count_score(prediction, ground_truth, **kwargs):\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef retrieval_score(prediction, ground_truth, **kwargs):\n    pattern = r\"Paragraph (\\d+)\"\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef retrieval_zh_score(prediction, ground_truth, **kwargs):\n    pattern = r\"段落(\\d+)\"\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef code_sim_score(prediction, ground_truth, **kwargs):\n    all_lines = prediction.lstrip(\"\\n\").split(\"\\n\")\n    prediction = \"\"\n    for line in all_lines:\n        if (\"`\" not in line) and (\"#\" not in line) and (\"//\" not in line):\n            prediction = line\n            break\n    return fuzz.ratio(prediction, ground_truth) / 100\n\n\ndef classification_score(prediction, ground_truth, **kwargs):\n    em_match_list = []\n    all_classes = kwargs[\"all_classes\"]\n    for class_name in all_classes:\n        if class_name in prediction:\n            em_match_list.append(class_name)\n    for match_term in em_match_list:\n        if match_term in ground_truth and match_term != ground_truth:\n            em_match_list.remove(match_term)\n    if ground_truth in em_match_list:\n        score = 1.0 / len(em_match_list)\n    else:\n        score = 0.0\n    return score\n\n\ndef rouge_score(prediction, ground_truth, **kwargs):\n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores([prediction], [ground_truth], avg=True)\n    except:\n        return 0.0\n    return scores[\"rouge-l\"][\"f\"]\n\n\ndef rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False)))\n    score = rouge_score(prediction, ground_truth)\n    return score\n\n\ndef f1_score(prediction, ground_truth, **kwargs):\n    common = Counter(prediction) & Counter(ground_truth)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction)\n    recall = 1.0 * num_same / len(ground_truth)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef qa_f1_score(prediction, ground_truth, **kwargs):\n    normalized_prediction = normalize_answer(prediction)\n    normalized_ground_truth = normalize_answer(ground_truth)\n\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    return f1_score(prediction_tokens, ground_truth_tokens)\n\n\ndef qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens = [normalize_zh_answer(token) for token in prediction_tokens]\n    ground_truth_tokens = [normalize_zh_answer(token) for token in ground_truth_tokens]\n    prediction_tokens = [token for token in prediction_tokens if len(token) > 0]\n    ground_truth_tokens = [token for token in ground_truth_tokens if len(token) > 0]\n    return f1_score(prediction_tokens, ground_truth_tokens)\n\n\ndataset2metric = {\n    \"narrativeqa\": qa_f1_score,\n    \"qasper\": qa_f1_score,\n    \"multifieldqa_en\": qa_f1_score,\n    \"multifieldqa_zh\": qa_f1_zh_score,\n    \"hotpotqa\": qa_f1_score,\n    \"2wikimqa\": qa_f1_score,\n    \"musique\": qa_f1_score,\n    \"dureader\": rouge_zh_score,\n    \"gov_report\": rouge_score,\n    \"qmsum\": rouge_score,\n    \"multi_news\": rouge_score,\n    \"vcsum\": rouge_zh_score,\n    \"trec\": classification_score,\n    \"triviaqa\": qa_f1_score,\n    \"samsum\": rouge_score,\n    \"lsht\": classification_score,\n    \"passage_retrieval_en\": retrieval_score,\n    \"passage_count\": count_score,\n    \"passage_retrieval_zh\": retrieval_zh_score,\n    \"lcc\": code_sim_score,\n    \"repobench-p\": code_sim_score,\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef main():\n    parser = HfArgumentParser(\n        (TrainingArguments, LongBenchArguments, MLAArguments, CacheArguments)\n    )\n    training_args, lb_args, mla_args, cache_args = parser.parse_args_into_dataclasses()\n    model_path = mla_args.model_path\n    tokenizer_path = (\n        mla_args.tokenizer_path\n        if mla_args.tokenizer_path is not None\n        else mla_args.model_path\n    )\n    if mla_args.is_partial_rope:\n        raise NotImplementedError(\"Not implemented for partial-rope\")\n    if mla_args.is_mla:\n        import json, os\n\n        with open(os.path.join(model_path, \"config.json\")) as f:\n            config = json.load(f)\n        from monkey_patch import mla_monkey_patch\n        mla_monkey_patch(config[\"RoPE\"])\n\n    if mla_args.dtype == \"float32\":\n        dtype = torch.float32\n    elif mla_args.dtype == \"float16\":\n        dtype = torch.float16\n    elif mla_args.dtype == \"bfloat16\":\n        dtype = torch.bfloat16\n\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path).to(device, dtype=dtype)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        tokenizer=tokenizer,\n    )\n    longbench_callback = RunLongBenchCallback(\n        lb_args, trainer=trainer, cache_args=cache_args\n    )\n    # trainer.callback_handler.callbacks.append(longbench_callback)\n    trainer.add_callback(longbench_callback)\n\n    trainer.callback_handler.on_evaluate(\n        trainer.args, trainer.state, trainer.control, None\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/conversation/convert_nanotron_to_hf.py", "content": "# copied from https://github.com/huggingface/nanotron/blob/main/examples/llama/convert_nanotron_to_hf.py\n\"\"\"\nConverts a nanotron model to HF format\nCommand:\n    torchrun --nproc_per_node=1 convert_nanotron_to_hf.py --checkpoint_path=nanotron-path --save_path=hf-path\n\"\"\"\nimport json,os\nfrom argparse import ArgumentParser\nfrom pathlib import Path\nfrom typing import Literal, Optional\n\nimport torch\nfrom .convert_weights import get_config_mapping, get_weight_mapping, load_nanotron_model\nfrom nanotron.config import LlamaConfig as NanotronLlamaConfig\nfrom nanotron.models import init_on_device_and_dtype\nfrom nanotron.models.llama import LlamaForTraining\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom transformers import LlamaConfig as HFLlamaConfig\n\nTEST_PROMPT = \"What is the meaning of the word chutzpah?\\nThe word chutzpah means\"\n\n\ndef _handle_attention_block(\n    qkv: torch.Tensor, part: Literal[\"q\", \"k\", \"v\"], n_q_heads: int, n_kv_heads: int, d_qk: int\n) -> torch.Tensor:\n    # Huggingface Llama separates the q, k, v weights (as opposed to nanotron).\n    # Furthermore, in the rotary embeddings in nanotron expects interleaved pairs of even\n    # and odd dimensions GPT-J style, while the huggingface implementation expects\n    # the whole 1st half and then the whole 2nd half GPT-NeoX style (for more information\n    # see flash_attn.layers.rotary.RotaryEmbedding).\n    # This function selects the proper chunk of the bundled qkv tensor and permutation\n    # to ensure correct transformation to huggingface.\n\n    def interleave(w: torch.Tensor):\n        return w\n        # w_new = []\n        # for head_w in w.split(d_qk):\n        #     head_w = head_w.view(d_qk // 2, 2, -1).transpose(0, 1).reshape(d_qk, -1)\n        #     w_new.append(head_w)\n        # return torch.cat(w_new)\n\n    assert part in [\"q\", \"k\", \"v\"], \"part must be one of [q, k, v]\"\n\n    index_end_q = n_q_heads * d_qk\n    index_end_k = index_end_q + n_kv_heads * d_qk\n    if part == \"q\":\n        return interleave(qkv[:index_end_q])\n    if part == \"k\":\n        return interleave(qkv[index_end_q:index_end_k])\n    return qkv[index_end_k:]\n\n\ndef _handle_gate_up_proj(gate_up_proj: torch.Tensor, gate: bool) -> torch.Tensor:\n    # The gate and up projection are bundled in nanotron.\n    # This function selects the proper chunk in the bundled weights to return\n    # either the gate or the up projection only.\n    weight_size = gate_up_proj.shape[0] // 2\n    if gate:\n        return gate_up_proj[:weight_size]\n    else:\n        return gate_up_proj[weight_size:]\n\n\ndef convert_nt_to_hf(nanotron_model: LlamaForTraining, hf_model: LlamaForCausalLM, model_config: NanotronLlamaConfig):\n    \"\"\"Converts the weights from the nanotron_model to hf_model, making modifications\n    in-place.\"\"\"\n\n    nanotron_model_state_dict = nanotron_model.state_dict()\n\n    hf_to_nt = get_weight_mapping(model_config, nt_to_hf=False)\n    for module_name_hf, module_hf in hf_model.named_modules():\n        for param_name_hf, param_hf in module_hf.named_parameters(recurse=False):\n            # Get the Nanotron parameter\n            nanotron_key = hf_to_nt[f\"{module_name_hf}.{param_name_hf}\"]\n            param = nanotron_model_state_dict[nanotron_key]\n\n            if \"qkv_proj\" in nanotron_key:\n                proj_name = module_name_hf.split(\".\")[4][0]\n                param = _handle_attention_block(\n                    param,\n                    proj_name,\n                    model_config.num_attention_heads,\n                    model_config.num_key_value_heads,\n                    model_config.hidden_size // model_config.num_attention_heads,\n                )\n\n            elif \"gate_up_proj\" in nanotron_key:\n                gate = \"gate\" in module_name_hf\n                param = _handle_gate_up_proj(param, gate)\n\n            with torch.no_grad():\n                param_hf.copy_(param)\n\n\ndef get_hf_config(config: NanotronLlamaConfig) -> HFLlamaConfig:\n    \"\"\"Converts a nanotron configuration to huggingface configuration.\"\"\"\n    attrs = {key: getattr(config, value) for key, value in get_config_mapping(nt_to_hf=False).items() if hasattr(config, value)}\n    return HFLlamaConfig(**attrs)\n\n\ndef convert_checkpoint_and_save(checkpoint_path: Path, save_path: Path, tokenizer_name: Optional[str] = None):\n    \"\"\"Loads the nanotron checkpoint in `checkpoint_path`, creates\n    a new huggingface instance, copies the weights from the nanotron checkpoint\n    and saves the transformed huggingface to `save_path`.\"\"\"\n\n    # Init nanotron model.\n    with open(checkpoint_path / \"model_config.json\", \"r\") as f:\n        attrs = json.load(f)\n        model_config = NanotronLlamaConfig(**attrs)\n    nanotron_model = load_nanotron_model(\n        model_config=model_config,\n        checkpoint_path=checkpoint_path,\n    )\n    # Init huggingface model.\n    with init_on_device_and_dtype(torch.device(\"cuda\"), torch.bfloat16):\n        model_config_hf = get_hf_config(model_config)\n        hf_model = LlamaForCausalLM._from_config(model_config_hf)\n\n    # Copy weights, initialize tokenizer and save model.\n    if tokenizer_name is not None:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        tokenizer.save_pretrained(save_path)\n    convert_nt_to_hf(nanotron_model, hf_model, model_config)\n    hf_model.save_pretrained(save_path)\n    print(f\"Model saved to {save_path}\")\n\n\ndef check_converted_model_generation(save_path: Path):\n    \"\"\"Loads a huggingface model and tokenizer from `save_path` and\n    performs a dummy text generation.\"\"\"\n\n    tokenizer = AutoTokenizer.from_pretrained(save_path)\n    input_ids = tokenizer(TEST_PROMPT, return_tensors=\"pt\")[\"input_ids\"].cuda()\n    print(\"Inputs:\", tokenizer.batch_decode(input_ids))\n\n    model = LlamaForCausalLM.from_pretrained(save_path).cuda().bfloat16()\n    out = model.generate(input_ids, max_new_tokens=100)\n    print(\"Generation (converted): \", tokenizer.batch_decode(out))\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Convert Nanotron weights to HF format\")\n    parser.add_argument(\"--checkpoint_path\", type=Path, default=\"llama-7b\", help=\"Path to the checkpoint\")\n    parser.add_argument(\"--save_path\", type=Path, default=\"llama-7b-hf\", help=\"Path to save the HF model\")\n    parser.add_argument(\"--tokenizer_name\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")\n    parser.add_argument(\"--is_mla\", action=\"store_true\", help=\"Whether the model is an MLA model\")\n    parser.add_argument(\"--auto_encoder\", action=\"store_true\", help=\"Whether the model is using auto-encoder\")\n    args = parser.parse_args()\n    with open(os.path.join(args.checkpoint_path,\"model_config.json\")) as f:\n        config = json.load(f)\n    if \"RoPE\" in config:\n        # partial RoPE\n        from ..mha2mla.monkey_patch import partial_rope_monkey_patch as partial_rope_monkey_patch_hf\n        from ..mha2mla_nt.monkey_patch import CustomLlamaConfig,partial_rope_monkey_patch as partial_rope_monkey_patch_nt\n        partial_rope_monkey_patch_hf(config[\"RoPE\"])\n        partial_rope_monkey_patch_nt(config[\"RoPE\"])\n        globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    if args.is_mla:\n        from ..mha2mla.monkey_patch import mla_monkey_patch as mla_monkey_patch_hf\n        from ..mha2mla_nt.monkey_patch import mla_monkey_patch as mla_monkey_patch_nt\n        with open(os.path.join(args.checkpoint_path,\"model_config.json\")) as f:\n            config = json.load(f)\n        mla_monkey_patch_hf(config[\"RoPE\"])\n        mla_monkey_patch_nt(config[\"RoPE\"])\n    if args.auto_encoder:\n        with open(os.path.join(args.checkpoint_path,\"model_config.json\")) as f:\n            config = json.load(f)\n        from ..auto_encoder.patch_func_hf import ae_patch_func_hf\n        from ..auto_encoder.patch_func_nt import ae_patch_func_nt,CustomLlamaConfig\n        ae_patch_func_nt(config[\"RoPE\"])\n        ae_patch_func_hf(config[\"RoPE\"])\n        globals()[\"NanotronLlamaConfig\"] = CustomLlamaConfig\n    if not args.is_mla and not args.auto_encoder:\n        from .original_convert_weights import get_weight_mapping as original_get_weight_mapping\n        from .original_convert_weights import load_nanotron_model as original_load_nanotron_model\n        get_weight_mapping = original_get_weight_mapping\n        load_nanotron_model = original_load_nanotron_model\n    # Convert Nanotron model to HF format.\n    convert_checkpoint_and_save(\n        checkpoint_path=args.checkpoint_path, save_path=args.save_path, tokenizer_name=args.tokenizer_name\n    )\n\n    # Check if the conversion was successful by generating some text.\n    if args.tokenizer_name is not None:\n        check_converted_model_generation(save_path=args.save_path)\n"}
{"type": "source_file", "path": "src/mha2mla/math_utils.py", "content": "# Disclaimer: this code is adapted from the Qwen2.5-Math eval suite.\n# Source: https://github.com/QwenLM/Qwen2.5-Math/blob/cb098e13/evaluation/parser.py\n\nimport re\nfrom word2number import w2n\n\n\ndef _fix_fracs(string):\n    substrs = string.split(\"\\\\frac\")\n    new_str = substrs[0]\n    if len(substrs) > 1:\n        substrs = substrs[1:]\n        for substr in substrs:\n            new_str += \"\\\\frac\"\n            if len(substr) > 0 and substr[0] == \"{\":\n                new_str += substr\n            else:\n                try:\n                    assert len(substr) >= 2\n                except:\n                    return string\n                a = substr[0]\n                b = substr[1]\n                if b != \"{\":\n                    if len(substr) > 2:\n                        post_substr = substr[2:]\n                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n                    else:\n                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n                else:\n                    if len(substr) > 2:\n                        post_substr = substr[2:]\n                        new_str += \"{\" + a + \"}\" + b + post_substr\n                    else:\n                        new_str += \"{\" + a + \"}\" + b\n    string = new_str\n    return string\n\n\ndef _fix_a_slash_b(string):\n    if len(string.split(\"/\")) != 2:\n        return string\n    a = string.split(\"/\")[0]\n    b = string.split(\"/\")[1]\n    try:\n        if \"sqrt\" not in a:\n            a = int(a)\n        if \"sqrt\" not in b:\n            b = int(b)\n        assert string == \"{}/{}\".format(a, b)\n        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n        return new_string\n    except:\n        return string\n\n\ndef _fix_sqrt(string):\n    _string = re.sub(r\"\\\\sqrt(\\w+)\", r\"\\\\sqrt{\\1}\", string)\n    return _string\n\n\ndef convert_word_number(text: str) -> str:\n    try:\n        text = str(w2n.word_to_num(text))\n    except:\n        pass\n    return text\n\n\n# units mainly from MathQA\nunit_texts = [\n    \"east\",\n    \"degree\",\n    \"mph\",\n    \"kmph\",\n    \"ft\",\n    \"m sqaure\",\n    \" m east\",\n    \"sq m\",\n    \"deg\",\n    \"mile\",\n    \"q .\",\n    \"monkey\",\n    \"prime\",\n    \"ratio\",\n    \"profit of rs\",\n    \"rd\",\n    \"o\",\n    \"gm\",\n    \"p . m\",\n    \"lb\",\n    \"tile\",\n    \"per\",\n    \"dm\",\n    \"lt\",\n    \"gain\",\n    \"ab\",\n    \"way\",\n    \"west\",\n    \"a .\",\n    \"b .\",\n    \"c .\",\n    \"d .\",\n    \"e .\",\n    \"f .\",\n    \"g .\",\n    \"h .\",\n    \"t\",\n    \"a\",\n    \"h\",\n    \"no change\",\n    \"men\",\n    \"soldier\",\n    \"pie\",\n    \"bc\",\n    \"excess\",\n    \"st\",\n    \"inches\",\n    \"noon\",\n    \"percent\",\n    \"by\",\n    \"gal\",\n    \"kmh\",\n    \"c\",\n    \"acre\",\n    \"rise\",\n    \"a . m\",\n    \"th\",\n    \"π r 2\",\n    \"sq\",\n    \"mark\",\n    \"l\",\n    \"toy\",\n    \"coin\",\n    \"sq . m\",\n    \"gallon\",\n    \"° f\",\n    \"profit\",\n    \"minw\",\n    \"yr\",\n    \"women\",\n    \"feet\",\n    \"am\",\n    \"pm\",\n    \"hr\",\n    \"cu cm\",\n    \"square\",\n    \"v â € ™\",\n    \"are\",\n    \"rupee\",\n    \"rounds\",\n    \"cubic\",\n    \"cc\",\n    \"mtr\",\n    \"s\",\n    \"ohm\",\n    \"number\",\n    \"kmph\",\n    \"day\",\n    \"hour\",\n    \"minute\",\n    \"min\",\n    \"second\",\n    \"man\",\n    \"woman\",\n    \"sec\",\n    \"cube\",\n    \"mt\",\n    \"sq inch\",\n    \"mp\",\n    \"∏ cm ³\",\n    \"hectare\",\n    \"more\",\n    \"sec\",\n    \"unit\",\n    \"cu . m\",\n    \"cm 2\",\n    \"rs .\",\n    \"rs\",\n    \"kg\",\n    \"g\",\n    \"month\",\n    \"km\",\n    \"m\",\n    \"cm\",\n    \"mm\",\n    \"apple\",\n    \"liter\",\n    \"loss\",\n    \"yard\",\n    \"pure\",\n    \"year\",\n    \"increase\",\n    \"decrease\",\n    \"d\",\n    \"less\",\n    \"Surface\",\n    \"litre\",\n    \"pi sq m\",\n    \"s .\",\n    \"metre\",\n    \"meter\",\n    \"inch\",\n]\n\nunit_texts.extend([t + \"s\" for t in unit_texts])\n\n\ndef strip_string(string, skip_unit=False):\n    string = str(string).strip()\n    # linebreaks\n    string = string.replace(\"\\n\", \"\")\n\n    # right \".\"\n    string = string.rstrip(\".\")\n\n    # remove inverse spaces\n    # replace \\\\ with \\\n    string = string.replace(\"\\\\!\", \"\")\n    # string = string.replace(\"\\\\ \", \"\")\n    # string = string.replace(\"\\\\\\\\\", \"\\\\\")\n\n    # matrix\n    string = re.sub(r\"\\\\begin\\{array\\}\\{.*?\\}\", r\"\\\\begin{pmatrix}\", string)\n    string = re.sub(r\"\\\\end\\{array\\}\", r\"\\\\end{pmatrix}\", string)\n    string = string.replace(\"bmatrix\", \"pmatrix\")\n\n    # replace tfrac and dfrac with frac\n    string = string.replace(\"tfrac\", \"frac\")\n    string = string.replace(\"dfrac\", \"frac\")\n    string = (\n        string.replace(\"\\\\neq\", \"\\\\ne\")\n        .replace(\"\\\\leq\", \"\\\\le\")\n        .replace(\"\\\\geq\", \"\\\\ge\")\n    )\n\n    # remove \\left and \\right\n    string = string.replace(\"\\\\left\", \"\")\n    string = string.replace(\"\\\\right\", \"\")\n    string = string.replace(\"\\\\{\", \"{\")\n    string = string.replace(\"\\\\}\", \"}\")\n\n    # Remove unit: miles, dollars if after is not none\n    _string = re.sub(r\"\\\\text{.*?}$\", \"\", string).strip()\n    if _string != \"\" and _string != string:\n        # print(\"Warning: unit not removed: '{}' -> '{}'\".format(string, _string))\n        string = _string\n\n    if not skip_unit:\n        # Remove unit: texts\n        for _ in range(2):\n            for unit_text in unit_texts:\n                # use regex, the prefix should be either the start of the string or a non-alphanumeric character\n                # the suffix should be either the end of the string or a non-alphanumeric character\n                _string = re.sub(r\"(^|\\W)\" + unit_text + r\"($|\\W)\", r\"\\1\\2\", string)\n                if _string != \"\":\n                    string = _string\n\n    # Remove circ (degrees)\n    string = string.replace(\"^{\\\\circ}\", \"\")\n    string = string.replace(\"^\\\\circ\", \"\")\n\n    # remove dollar signs\n    string = string.replace(\"\\\\$\", \"\")\n    string = string.replace(\"$\", \"\")\n    string = string.replace(\"\\\\(\", \"\").replace(\"\\\\)\", \"\")\n\n    # convert word number to digit\n    string = convert_word_number(string)\n\n    # replace \"\\\\text{...}\" to \"...\"\n    string = re.sub(r\"\\\\text\\{(.*?)\\}\", r\"\\1\", string)\n    for key in [\"x=\", \"y=\", \"z=\", \"x\\\\in\", \"y\\\\in\", \"z\\\\in\", \"x\\\\to\", \"y\\\\to\", \"z\\\\to\"]:\n        string = string.replace(key, \"\")\n    string = string.replace(\"\\\\emptyset\", r\"{}\")\n    string = string.replace(\"(-\\\\infty,\\\\infty)\", \"\\\\mathbb{R}\")\n\n    # remove percentage\n    string = string.replace(\"\\\\%\", \"\")\n    string = string.replace(\"\\%\", \"\")\n    string = string.replace(\"%\", \"\")\n\n    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n    string = string.replace(\" .\", \" 0.\")\n    string = string.replace(\"{.\", \"{0.\")\n\n    # cdot\n    # string = string.replace(\"\\\\cdot\", \"\")\n    if (\n            string.startswith(\"{\")\n            and string.endswith(\"}\")\n            and string.isalnum()\n            or string.startswith(\"(\")\n            and string.endswith(\")\")\n            and string.isalnum()\n            or string.startswith(\"[\")\n            and string.endswith(\"]\")\n            and string.isalnum()\n    ):\n        string = string[1:-1]\n\n    # inf\n    string = string.replace(\"infinity\", \"\\\\infty\")\n    if \"\\\\infty\" not in string:\n        string = string.replace(\"inf\", \"\\\\infty\")\n    string = string.replace(\"+\\\\inity\", \"\\\\infty\")\n\n    # and\n    string = string.replace(\"and\", \"\")\n    string = string.replace(\"\\\\mathbf\", \"\")\n\n    # use regex to remove \\mbox{...}\n    string = re.sub(r\"\\\\mbox{.*?}\", \"\", string)\n\n    # quote\n    string.replace(\"'\", \"\")\n    string.replace('\"', \"\")\n\n    # i, j\n    if \"j\" in string and \"i\" not in string:\n        string = string.replace(\"j\", \"i\")\n\n    # replace a.000b where b is not number or b is end, with ab, use regex\n    string = re.sub(r\"(\\d+)\\.0*([^\\d])\", r\"\\1\\2\", string)\n    string = re.sub(r\"(\\d+)\\.0*$\", r\"\\1\", string)\n\n    # if empty, return empty string\n    if len(string) == 0:\n        return string\n    if string[0] == \".\":\n        string = \"0\" + string\n\n    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n    if len(string.split(\"=\")) == 2:\n        if len(string.split(\"=\")[0]) <= 2:\n            string = string.split(\"=\")[1]\n\n    string = _fix_sqrt(string)\n    string = string.replace(\" \", \"\")\n\n    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc. Even works with \\frac1{72} (but not \\frac{72}1). Also does a/b --> \\\\frac{a}{b}\n    string = _fix_fracs(string)\n\n    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model output is X/Y\n    string = _fix_a_slash_b(string)\n\n    return string\n\n\ndef extract_answer(pred_str, data_name, use_last_number=True):\n    pred_str = pred_str.replace(\"\\u043a\\u0438\", \"\")\n\n    if \"final answer is $\" in pred_str and \"$. I hope\" in pred_str:\n        # minerva_math\n        tmp = pred_str.split(\"final answer is $\", 1)[1]\n        pred = tmp.split(\"$. I hope\", 1)[0].strip()\n    elif \"boxed\" in pred_str:\n        ans = pred_str.split(\"boxed\")[-1]\n        if len(ans) == 0:\n            return \"\"\n        elif ans[0] == \"{\":\n            stack = 1\n            a = \"\"\n            for c in ans[1:]:\n                if c == \"{\":\n                    stack += 1\n                    a += c\n                elif c == \"}\":\n                    stack -= 1\n                    if stack == 0:\n                        break\n                    a += c\n                else:\n                    a += c\n        else:\n            a = ans.split(\"$\")[0].strip()\n        pred = a\n    elif \"he answer is\" in pred_str:\n        pred = pred_str.split(\"he answer is\")[-1].strip()\n    elif \"final answer is\" in pred_str:\n        pred = pred_str.split(\"final answer is\")[-1].strip()\n    elif \"答案是\" in pred_str:\n        # Handle Chinese few-shot multiple choice problem answer extraction\n        pred = pred_str.split(\"答案是\")[1].strip().split(\"\\n\\n\")[0].strip()\n    else:  # use the last number\n        if use_last_number:\n            pattern = \"-?\\d*\\.?\\d+\"\n            pred = re.findall(pattern, pred_str.replace(\",\", \"\"))\n            if len(pred) >= 1:\n                pred = pred[-1]\n            else:\n                pred = \"\"\n        else:\n            pred = \"\"\n\n    # choice answer\n    if (\n            data_name in [\"sat_math\", \"aqua\"]\n            or \"mmlu\" in data_name\n    ):\n        tmp = re.findall(r\"\\b(A|B|C|D|E)\\b\", pred.upper())\n        if tmp:\n            pred = tmp[-1]\n        else:\n            pred = pred.strip().strip(\".\")\n\n    # multiple line\n    # pred = pred.split(\"\\n\")[0]\n    pred = re.sub(r\"\\n\\s*\", \"\", pred)\n    if pred != \"\" and pred[0] == \":\":\n        pred = pred[1:]\n    if pred != \"\" and pred[-1] == \".\":\n        pred = pred[:-1]\n    if pred != \"\" and pred[-1] == \"/\":\n        pred = pred[:-1]\n    pred = strip_string(pred, skip_unit=data_name in [\"carp_en\", \"minerva_math\"])\n    return pred\n\n\ndef parse_math_answer(text: str, data_name):\n    # parse ground truth\n    if data_name in [\"math\", \"minerva_math\"]:\n        gt_cot = text\n        gt_ans = extract_answer(gt_cot, data_name)\n    elif data_name == \"gsm8k\":\n        gt_cot, gt_ans = text.split(\"####\")\n    else:\n        raise NotImplementedError(f\"`{data_name}`\")\n    # post process\n    gt_ans = (\n        gt_ans.replace(\"\\\\neq\", \"\\\\ne\")\n        .replace(\"\\\\leq\", \"\\\\le\")\n        .replace(\"\\\\geq\", \"\\\\ge\")\n    )\n    return gt_ans"}
{"type": "source_file", "path": "src/mha2mla/monkey_patch.py", "content": "from typing import List, Union, Dict, Optional, Tuple\n\nimport torch\nimport math\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom transformers.models.llama.modeling_llama import (\n    rotate_half,\n    repeat_kv,\n    logger,\n    StaticCache,\n)\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\nfrom transformers import Cache\nfrom transformers.models.llama.modeling_llama import (\n    LlamaConfig,\n    logger,\n    LlamaRotaryEmbedding,\n    repeat_kv,\n)\nfrom transformers.models.llama import modeling_llama\nfrom transformers.cache_utils import Cache, StaticCache\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\nfrom transformers.utils import is_flash_attn_greater_or_equal_2_10\n\n\ndef create_custom_apply_rotary_pos_emb_hf(cfg):\n\n    def apply_rotary_pos_emb_v0(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n        # Full-RoPE\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v1(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n        # retain the fastest-rotating (high-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        keep_dim = cfg[\"top_k_rope_dim\"]\n        if keep_dim <= 0:\n            return q, k\n        elif keep_dim >= q.size(-1):\n            return q_embed, k_embed\n        half = q.size(-1) // 2\n        q_embed = torch.cat(\n            (\n                q_embed[..., :keep_dim],\n                q[..., keep_dim:half],\n                q_embed[..., half : half + keep_dim],\n                q[..., half + keep_dim :],\n            ),\n            -1,\n        )\n        k_embed = torch.cat(\n            (\n                k_embed[..., :keep_dim],\n                k[..., keep_dim:half],\n                k_embed[..., half : half + keep_dim],\n                k[..., half + keep_dim :],\n            ),\n            -1,\n        )\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v2(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n        # select subspaces with equidistant intervals\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        indices = torch.arange(\n            cfg[\"uniform_start_point\"], q.size(-1), cfg[\"uniform_step\"], device=q.device\n        )\n        q[..., indices] = q_embed[..., indices]\n        k[..., indices] = k_embed[..., indices]\n        return q, k\n\n    def apply_rotary_pos_emb_v3(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n        # retain the fastest-rotating (high-frequency) subspaces and the slowest-rotating (low-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        top_k_dim, last_k_dim = cfg[\"top_k_rope_dim\"], cfg[\"last_k_rope_dim\"]\n        # assert top_k_dim + last_k_dim <= q.size(-1)\n        half = q.size(-1) // 2\n        qs = [\n            q_embed[..., :top_k_dim],\n            q[..., top_k_dim : half - last_k_dim],\n            q_embed[..., half - last_k_dim : half + top_k_dim],\n            q[..., half + top_k_dim : half + half - last_k_dim],\n            q_embed[..., half + half - last_k_dim :],\n        ]\n        ks = [\n            k_embed[..., :top_k_dim],\n            k[..., top_k_dim : half - last_k_dim],\n            k_embed[..., half - last_k_dim : half + top_k_dim],\n            k[..., half + top_k_dim : half + half - last_k_dim],\n            k_embed[..., half + half - last_k_dim :],\n        ]\n        q_embed = torch.cat([q for q in qs if q != []], -1)\n        k_embed = torch.cat([k for k in ks if k != []], -1)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v4(\n        q, k, cos, sin, position_ids=None, layer_idx=0, unsqueeze_dim=1\n    ):\n        # retain the subspaces with higher 2-norm score\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        top_k_dim = cfg[\"top_k_rope_dim\"]\n        topk_indices = torch.topk(qk_tensor[layer_idx], k=top_k_dim, dim=1)[1]\n        mask = torch.zeros_like(qk_tensor[layer_idx])\n        mask.scatter_(1, topk_indices, 1)\n        mask_for_k = (\n            torch.cat((mask, mask), dim=1).unsqueeze(0).unsqueeze(2).to(k.device)\n        )\n        mask_for_q = torch.repeat_interleave(\n            input=mask_for_k, repeats=cfg[\"n_gqa_group\"], dim=1\n        ).to(q.device)\n        q_embed = torch.where(mask_for_q == 1, q_embed, q)\n        k_embed = torch.where(mask_for_k == 1, k_embed, k)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v5(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n        # retain the slowest-rotating (low-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n        k_embed = (k * cos) + (rotate_half(k) * sin)\n        last_k_dim = cfg[\"last_k_rope_dim\"]\n        half = q.size(-1) // 2\n        qs = [\n            q[..., : half - last_k_dim],\n            q_embed[..., half - last_k_dim : half],\n            q[..., half : half + half - last_k_dim],\n            q_embed[..., half + half - last_k_dim :],\n        ]\n        ks = [\n            k[..., : half - last_k_dim],\n            k_embed[..., half - last_k_dim : half],\n            k[..., half : half + half - last_k_dim],\n            k_embed[..., half + half - last_k_dim :],\n        ]\n        q_embed = torch.cat([q for q in qs if q != []], -1)\n        k_embed = torch.cat([k for k in ks if k != []], -1)\n        return q_embed, k_embed\n\n    version = cfg[\"partial_rope_version\"]\n    version_str2int = {\n        \"full-rope\": 0,\n        \"high\": 1,\n        \"uniform\": 2,\n        \"2-norm\": 4,\n        \"low\": 5,\n    }\n    if isinstance(version, str):\n        version = version_str2int[version]\n    if version == 4:\n        with open(cfg[\"qk_tensor_path\"], \"rb\") as fin:\n            qk_tensor = torch.load(fin)\n    versions = {\n        0: apply_rotary_pos_emb_v0,\n        1: apply_rotary_pos_emb_v1,\n        2: apply_rotary_pos_emb_v2,\n        3: apply_rotary_pos_emb_v3,\n        4: apply_rotary_pos_emb_v4,\n        5: apply_rotary_pos_emb_v5,\n    }\n    return versions.get(version, apply_rotary_pos_emb_v0)\n\n\ndef custom_forward_LlamaAttention(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Cache] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    cache_position: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[\n        Tuple[torch.Tensor, torch.Tensor]\n    ] = None,  # will become mandatory in v4.46\n    **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\n    bsz, q_len, _ = hidden_states.size()\n\n    if self.config.pretraining_tp > 1:\n        key_value_slicing = (\n            self.num_key_value_heads * self.head_dim\n        ) // self.config.pretraining_tp\n        query_slices = self.q_proj.weight.split(\n            (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n        )\n        key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n        value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n        query_states = [\n            F.linear(hidden_states, query_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        query_states = torch.cat(query_states, dim=-1)\n\n        key_states = [\n            F.linear(hidden_states, key_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        key_states = torch.cat(key_states, dim=-1)\n\n        value_states = [\n            F.linear(hidden_states, value_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        value_states = torch.cat(value_states, dim=-1)\n\n    else:\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(\n        bsz, q_len, self.num_heads, self.head_dim\n    ).transpose(1, 2)\n    key_states = key_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n    value_states = value_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n\n    if position_embeddings is None:\n        logger.warning_once(\n            \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n            \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n            \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n            \"removed and `position_embeddings` will be mandatory.\"\n        )\n        cos, sin = self.rotary_emb(value_states, position_ids)\n    else:\n        cos, sin = position_embeddings\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, layer_idx=self.layer_idx\n    )\n\n    if past_key_value is not None:\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(\n            key_states, value_states, self.layer_idx, cache_kwargs\n        )\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n        self.head_dim\n    )\n\n    if attention_mask is not None:  # no matter the length, we just slice it\n        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n        attn_weights = attn_weights + causal_mask\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        query_states.dtype\n    )\n    attn_weights = nn.functional.dropout(\n        attn_weights, p=self.attention_dropout, training=self.training\n    )\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    attn_output = attn_output.reshape(bsz, q_len, -1)\n\n    if self.config.pretraining_tp > 1:\n        attn_output = attn_output.split(\n            self.hidden_size // self.config.pretraining_tp, dim=2\n        )\n        o_proj_slices = self.o_proj.weight.split(\n            self.hidden_size // self.config.pretraining_tp, dim=1\n        )\n        attn_output = sum(\n            [\n                F.linear(attn_output[i], o_proj_slices[i])\n                for i in range(self.config.pretraining_tp)\n            ]\n        )\n    else:\n        attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef custom_forward_LlamaFlashAttention2(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.LongTensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Cache] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    cache_position: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[\n        Tuple[torch.Tensor, torch.Tensor]\n    ] = None,  # will become mandatory in v4.46\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\n    if isinstance(past_key_value, StaticCache):\n        raise ValueError(\n            \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n            \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n        )\n\n    output_attentions = False\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    # Flash attention requires the input to have the shape\n    # batch_size x seq_length x head_dim x hidden_dim\n    # therefore we just need to keep the original shape\n    query_states = query_states.view(\n        bsz, q_len, self.num_heads, self.head_dim\n    ).transpose(1, 2)\n    key_states = key_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n    value_states = value_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n\n    if position_embeddings is None:\n        logger.warning_once(\n            \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n            \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n            \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n            \"removed and `position_embeddings` will be mandatory.\"\n        )\n        cos, sin = self.rotary_emb(value_states, position_ids)\n    else:\n        cos, sin = position_embeddings\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, layer_idx=self.layer_idx\n    )\n\n    if past_key_value is not None:\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(\n            key_states, value_states, self.layer_idx, cache_kwargs\n        )\n\n    # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n    # to be able to avoid many of these transpose/reshape/view.\n    query_states = query_states.transpose(1, 2)\n    key_states = key_states.transpose(1, 2)\n    value_states = value_states.transpose(1, 2)\n\n    dropout_rate = self.attention_dropout if self.training else 0.0\n\n    # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n    # therefore the input hidden states gets silently casted in float32. Hence, we need\n    # cast them back in the correct dtype just to be sure everything works as expected.\n    # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n    # in fp32. (LlamaRMSNorm handles it correctly)\n\n    input_dtype = query_states.dtype\n    if input_dtype == torch.float32:\n        if torch.is_autocast_enabled():\n            target_dtype = torch.get_autocast_gpu_dtype()\n        # Handle the case where the model is quantized\n        elif hasattr(self.config, \"_pre_quantization_dtype\"):\n            target_dtype = self.config._pre_quantization_dtype\n        else:\n            target_dtype = self.q_proj.weight.dtype\n\n        logger.warning_once(\n            f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n            f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n            f\" {target_dtype}.\"\n        )\n\n        query_states = query_states.to(target_dtype)\n        key_states = key_states.to(target_dtype)\n        value_states = value_states.to(target_dtype)\n\n    attn_output = _flash_attention_forward(\n        query_states,\n        key_states,\n        value_states,\n        attention_mask,\n        q_len,\n        position_ids=position_ids,\n        dropout=dropout_rate,\n        sliding_window=getattr(self, \"sliding_window\", None),\n        use_top_left_mask=self._flash_attn_uses_top_left_mask,\n        is_causal=self.is_causal,\n    )\n\n    attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef custom_forward_LlamaSdpaAttention(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Cache] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    cache_position: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[\n        Tuple[torch.Tensor, torch.Tensor]\n    ] = None,  # will become mandatory in v4.46\n    **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\n    if output_attentions:\n        # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n        logger.warning_once(\n            \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n            'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n        )\n        return super().forward(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            position_embeddings=position_embeddings,\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(\n        bsz, q_len, self.num_heads, self.head_dim\n    ).transpose(1, 2)\n    key_states = key_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n    value_states = value_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n\n    if position_embeddings is None:\n        logger.warning_once(\n            \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n            \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n            \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n            \"removed and `position_embeddings` will be mandatory.\"\n        )\n        cos, sin = self.rotary_emb(value_states, position_ids)\n    else:\n        cos, sin = position_embeddings\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, layer_idx=self.layer_idx\n    )\n\n    if past_key_value is not None:\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(\n            key_states, value_states, self.layer_idx, cache_kwargs\n        )\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    causal_mask = attention_mask\n    if attention_mask is not None:\n        causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n    if query_states.device.type == \"cuda\" and causal_mask is not None:\n        query_states = query_states.contiguous()\n        key_states = key_states.contiguous()\n        value_states = value_states.contiguous()\n\n    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n    is_causal = True if causal_mask is None and q_len > 1 else False\n\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\n        query_states,\n        key_states,\n        value_states,\n        attn_mask=causal_mask,\n        dropout_p=self.attention_dropout if self.training else 0.0,\n        is_causal=is_causal,\n    )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.view(bsz, q_len, -1)\n\n    attn_output = self.o_proj(attn_output)\n\n    return attn_output, None, past_key_value\n\n\nclass IndexForNope:\n    _qk_tensor_path = None\n    _qk_tensor_cache = None\n\n    @staticmethod\n    def get_index_for_nope_v0(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        nope_mask = torch.zeros((head_dim), dtype=torch.bool)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v1(rope_cfg, **kwargs):\n        keep_dim = rope_cfg[\"top_k_rope_dim\"]\n        head_dim = kwargs[\"head_dim\"]\n        if keep_dim <= 0:\n            nope_mask = torch.ones((head_dim), dtype=torch.bool)\n        elif keep_dim >= head_dim:\n            nope_mask = torch.zeros((head_dim), dtype=torch.bool)\n        else:\n            half = head_dim // 2\n            nope_mask = torch.ones((half), dtype=torch.bool)\n            nope_mask[:keep_dim] = False\n            nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v2(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        indices_to_remove = torch.arange(\n            rope_cfg[\"uniform_start_point\"], head_dim, rope_cfg[\"uniform_step\"]\n        )\n        nope_mask = torch.ones(head_dim, dtype=torch.bool)\n        nope_mask[indices_to_remove] = False\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v3(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        top_k_dim, last_k_dim = rope_cfg[\"top_k_rope_dim\"], rope_cfg[\"last_k_rope_dim\"]\n        half = head_dim // 2\n        assert top_k_dim + last_k_dim <= half\n        nope_mask = torch.zeros((half), dtype=torch.bool)\n        nope_mask[top_k_dim : half - last_k_dim] = True\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v4(rope_cfg, **kwargs):\n        if (\n            IndexForNope._qk_tensor_cache is None\n            or rope_cfg[\"qk_tensor_path\"] != IndexForNope._qk_tensor_path\n        ):\n            with open(rope_cfg[\"qk_tensor_path\"], \"rb\") as fin:\n                IndexForNope._qk_tensor_cache = torch.load(\n                    fin\n                )  # [layer_num, k_head_num, head_dim//2]\n                IndexForNope._qk_tensor_path = rope_cfg[\"qk_tensor_path\"]\n                assert len(IndexForNope._qk_tensor_cache.shape) == 3\n        qk_tensor = IndexForNope._qk_tensor_cache\n        layer_idx = kwargs[\"layer_idx\"]\n        top_k_dim = rope_cfg[\"top_k_rope_dim\"]\n        topk_indices = torch.topk(qk_tensor[layer_idx], k=top_k_dim, dim=1)[1]\n        nope_mask = torch.ones_like(qk_tensor[layer_idx], dtype=torch.bool)\n        nope_mask.scatter_(1, topk_indices, False)\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=-1)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v5(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        last_k_rope_dim = rope_cfg[\"last_k_rope_dim\"]\n        half = head_dim // 2\n        nope_mask = torch.ones((half), dtype=torch.bool)\n        nope_mask[half - last_k_rope_dim : half] = False\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope(rope_cfg, **kwargs):\n        logger.info(f\"rope_cfg: {rope_cfg}\")\n        version = rope_cfg[\"partial_rope_version\"]\n        versions = {\n            0: IndexForNope.get_index_for_nope_v0,\n            1: IndexForNope.get_index_for_nope_v1,\n            2: IndexForNope.get_index_for_nope_v2,\n            3: IndexForNope.get_index_for_nope_v3,\n            4: IndexForNope.get_index_for_nope_v4,\n            5: IndexForNope.get_index_for_nope_v5,\n        }\n        index_func = versions[version]\n        nope_mask = index_func(rope_cfg, **kwargs)\n        nope_mask = nope_mask.to(dtype=torch.bool)\n        if version == 4:\n            nope_mask = nope_mask.reshape(-1)\n        else:\n            nope_mask = nope_mask.repeat(repeats=(kwargs[\"head_num\"],))\n        return nope_mask\n\n\nclass SvdInit:\n    @staticmethod\n    def method_I(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down = (U_k[:, :r] + U_v[:, :r]) / 2\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_II(k, v, r=8):\n        # Separately decompose W_k_nope and W_v into truncated SVDs, allocating dimensions to each\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down_k = U_k\n        W_down_v = U_v\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down_k.t(), W_up_k.t(), W_down_v.t(), W_up_v.t()\n\n    @staticmethod\n    def method_III(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        Sigma_k_half = torch.diag(torch.sqrt(S_k))\n        Sigma_v_half = torch.diag(torch.sqrt(S_v))\n        W_down_k = U_k @ Sigma_k_half\n        W_down_v = U_v @ Sigma_v_half\n        W_up_k = Sigma_k_half @ V_k.t()\n        W_up_v = Sigma_v_half @ V_v.t()\n        return W_down_k.t(), W_up_k.t(), W_down_v.t(), W_up_v.t()\n\n    @staticmethod\n    def method_IV(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        Sigma_k_half = torch.diag(torch.sqrt(S_k))\n        Sigma_v_half = torch.diag(torch.sqrt(S_v))\n        W_down_k = U_k @ Sigma_k_half\n        W_down_v = U_v @ Sigma_v_half\n        W_down = (W_down_k + W_down_v) / 2\n        W_up_k = Sigma_k_half @ V_k.t()\n        W_up_v = Sigma_v_half @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_V(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        W_down = U_k\n        W_down_pseudo_inv = torch.linalg.pinv(W_down)\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.matmul(W_down_pseudo_inv, v)\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_VI(k, v, r=8):\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down = U_v\n        W_down_pseudo_inv = torch.linalg.pinv(W_down)\n        W_up_k = torch.matmul(W_down_pseudo_inv, k)\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_VII(k, v, r=8):\n        # jointly factorize the con-catenated matrix\n        U_kv, S_kv, V_kv = torch.svd(torch.cat([k, v], dim=1))\n        U_kv, S_kv, V_kv = U_kv[:, :r], S_kv[:r], V_kv[:, :r]\n        W_down = U_kv\n        split_sizes = [k.size(1), v.size(1)]\n        W_up_k, W_up_v = torch.split(V_kv, split_sizes, dim=0)\n        W_up_k = torch.diag(S_kv) @ W_up_k.t()\n        W_up_v = torch.diag(S_kv) @ W_up_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def init(k, v, svd_method=1, r=8):\n        assert k.dtype == v.dtype, \"k and v must have the same dtype\"\n        logger.info(f\"Using SVD method {svd_method} with rank {r}\")\n        original_dtype = k.dtype\n        k = k.to(torch.float32)\n        v = v.to(torch.float32)\n        versions = {\n            1: SvdInit.method_I,\n            2: SvdInit.method_II,\n            3: SvdInit.method_III,\n            4: SvdInit.method_IV,\n            5: SvdInit.method_V,\n            6: SvdInit.method_VI,\n            7: SvdInit.method_VII,\n        }\n        W_down_k, W_up_k, W_down_v, W_up_v = versions[svd_method](k, v, r)\n        W_down_k = W_down_k.to(original_dtype)\n        W_up_k = W_up_k.to(original_dtype)\n        if W_down_v is not None:\n            W_down_v = W_down_v.to(original_dtype)\n        W_up_v = W_up_v.to(original_dtype)\n        return W_down_k, W_up_k, W_down_v, W_up_v\n\n\nclass CustomLlamaAttention(nn.Module):\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.nope_mask = IndexForNope.get_index_for_nope(\n            config.RoPE,\n            head_dim=self.head_dim,\n            head_num=self.num_key_value_heads,\n            layer_idx=layer_idx,\n        )\n        self.is_share_W_down = bool(config.SVD[\"method\"] not in [2, 3])\n        self.low_rank = config.SVD[\"low_rank\"]\n\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias\n        )\n        # self.k_proj = nn.Linear(self.hidden_size,self.num_key_value_heads * self.head_dim,bias=config.attention_bias,)\n        # self.v_proj = nn.Linear(self.hidden_size,self.num_key_value_heads * self.head_dim,bias=config.attention_bias,)\n        self.W_k_r = nn.Linear(\n            self.hidden_size,\n            (self.nope_mask == False).sum().item(),\n            bias=config.attention_bias,\n        )\n        self.W_down_k = nn.Linear(\n            self.hidden_size,\n            self.low_rank * self.num_key_value_heads,\n            bias=config.attention_bias,\n        )\n        if not self.is_share_W_down:\n            self.W_down_v = nn.Linear(\n                self.hidden_size,\n                self.low_rank * self.num_key_value_heads,\n                bias=config.attention_bias,\n            )\n        self.W_up_k = nn.Linear(\n            self.low_rank * self.num_key_value_heads,\n            self.num_key_value_heads * self.head_dim\n            - (self.nope_mask == False).sum().item(),\n            bias=config.attention_bias,\n        )\n        self.W_up_v = nn.Linear(\n            self.low_rank * self.num_key_value_heads,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.o_proj = nn.Linear(\n            self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias\n        )\n\n        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def get_qkv_states(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ):\n        bsz, q_len, _ = hidden_states.size()\n        assert not self.config.pretraining_tp > 1, \"not support pretraining_tp>1\"\n        # prepare query_states and k_r\n        query_states = self.q_proj(hidden_states)\n        key_states = torch.zeros(\n            (bsz, q_len, self.num_key_value_heads * self.head_dim),\n            device=hidden_states.device,\n            dtype=hidden_states.dtype,\n        )\n        k_r = self.W_k_r(hidden_states)\n        key_states[..., ~self.nope_mask] = k_r\n\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        if position_embeddings is None:\n            logger.warning_once(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(key_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n        query_states, key_states = self.apply_custom_rotary_pos_emb(\n            query_states, key_states, cos, sin\n        )\n        key_states = key_states.transpose(1, 2).reshape(bsz, q_len, -1)\n        k_r = key_states[..., ~self.nope_mask]\n\n        # prepare c_kv\n        if self.is_share_W_down:\n            c_kv = self.W_down_k(hidden_states)\n        else:\n            c_kv = torch.cat(\n                [self.W_down_k(hidden_states), self.W_down_v(hidden_states)], dim=-1\n            )\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            r_dim, c_dim = k_r.size(-1), c_kv.size(-1)\n            k_r, c_kv = past_key_value.update(\n                k_r.view(bsz, q_len, self.num_key_value_heads, -1).transpose(1, 2),\n                c_kv.view(bsz, q_len, self.num_key_value_heads, -1).transpose(1, 2),\n                self.layer_idx,\n                cache_kwargs,\n            )\n            k_r = k_r.transpose(1, 2).reshape(bsz, -1, r_dim)\n            c_kv = c_kv.transpose(1, 2).reshape(bsz, -1, c_dim)\n        if self.is_share_W_down:\n            k_c = self.W_up_k(c_kv)\n            value_states = self.W_up_v(c_kv)\n        else:\n            c_k, c_v = c_kv.split(\n                [\n                    self.low_rank * self.num_key_value_heads,\n                    self.low_rank * self.num_key_value_heads,\n                ],\n                dim=-1,\n            )\n            k_c = self.W_up_k(c_k)\n            value_states = self.W_up_v(c_v)\n\n        # merge k_c and k_r into key_states\n        key_states = torch.zeros(\n            bsz,\n            c_kv.size(1),\n            self.nope_mask.shape[-1],\n            device=hidden_states.device,\n            dtype=hidden_states.dtype,\n        )\n        key_states[..., self.nope_mask] = k_c\n        key_states[..., ~self.nope_mask] = k_r\n\n        # prepare key_states and value_states\n        key_states = key_states.view(\n            bsz, -1, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, -1, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        return query_states, key_states, value_states\n\n    def apply_custom_rotary_pos_emb(self, query_states, key_states, cos, sin):\n        if self.config.RoPE[\"partial_rope_version\"] == 4:\n            query_states, key_states = modeling_llama.apply_rotary_pos_emb(\n                query_states, key_states, cos, sin, layer_idx=self.layer_idx\n            )\n        else:\n            query_states, key_states = modeling_llama.apply_rotary_pos_emb(\n                query_states, key_states, cos, sin\n            )\n        return query_states, key_states\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.attention_dropout, training=self.training\n        )\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, -1)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(\n                self.hidden_size // self.config.pretraining_tp, dim=2\n            )\n            o_proj_slices = self.o_proj.weight.split(\n                self.hidden_size // self.config.pretraining_tp, dim=1\n            )\n            attn_output = sum(\n                [\n                    F.linear(attn_output[i], o_proj_slices[i])\n                    for i in range(self.config.pretraining_tp)\n                ]\n            )\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass CustomLlamaFlashAttention2(CustomLlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n        )\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n        attn_output = _flash_attention_forward(\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            q_len,\n            position_ids=position_ids,\n            dropout=dropout_rate,\n            sliding_window=getattr(self, \"sliding_window\", None),\n            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n            is_causal=self.is_causal,\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass CustomLlamaSdpaAttention(CustomLlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states, key_states, value_states = self.get_qkv_states(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, -1)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nclass CustomLlamaMLAForInfer(CustomLlamaAttention):\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__(config, layer_idx)\n        self.is_inference = False\n\n    def matrix_absorb(self):\n        assert (\n            self.config.SVD[\"method\"] == 7\n        ), \"only support SVD method 7 which is SVD_{joint}\"\n        self.is_inference = True\n        # q_proj\n        group_size = self.num_heads // self.num_key_value_heads\n        nope_mask_for_q = (\n            self.nope_mask.reshape(self.num_key_value_heads, 1, self.head_dim)\n            .repeat_interleave(group_size, dim=-2)\n            .reshape(-1)\n        )\n        self.nope_mask_for_q = nope_mask_for_q\n        self.W_q_r_absorbed = nn.Linear(\n            in_features=self.hidden_size,\n            out_features=(~nope_mask_for_q).sum().item(),\n            dtype=self.q_proj.weight.dtype,\n            device=self.q_proj.weight.device,\n            bias=False,\n        )\n        self.W_q_r_absorbed.weight[:] = self.q_proj.weight.T[..., ~self.nope_mask_for_q].T\n        self.W_down_q_absorbed = nn.Linear(\n            in_features=self.hidden_size,\n            out_features=self.num_heads * self.num_key_value_heads * self.low_rank,\n            dtype=self.q_proj.weight.dtype,\n            device=self.q_proj.weight.device,\n            bias=False,\n        )\n        # GQA\n        W_q_nope = []\n        W_o_proj = []\n        for i in range(self.num_key_value_heads):\n            # q\n            head_nope_mask = self.nope_mask[\n                ..., i * self.head_dim : (i + 1) * self.head_dim\n            ]\n            head_k_nope_dim = self.W_up_k.out_features // self.num_key_value_heads\n            head_W_up_k = self.W_up_k.weight[\n                i * head_k_nope_dim : (i + 1) * head_k_nope_dim, :\n            ]\n            # o\n            head_W_up_v = self.W_up_v.weight[\n                i * self.head_dim : (i + 1) * self.head_dim, :\n            ]\n            for j in range(group_size):\n                # q\n                head_w_q_nope = self.q_proj.weight.T[\n                    ...,\n                    (i * group_size + j)\n                    * self.head_dim : (i * group_size + j + 1)\n                    * self.head_dim,\n                ]\n                head_w_q_nope = head_w_q_nope[..., head_nope_mask]\n                W_q_nope.append(head_w_q_nope @ (head_W_up_k))\n                # o\n                head_w_o_proj = self.o_proj.weight[\n                    ...,\n                    (i * group_size + j)\n                    * self.head_dim : (i * group_size + j + 1)\n                    * self.head_dim,\n                ]\n                W_o_proj.append((head_W_up_v.T) @ (head_w_o_proj.T))\n        W_q_nope = torch.cat(W_q_nope, dim=-1)\n        o_oroj_weight = torch.cat(W_o_proj, dim=0)\n        self.W_down_q_absorbed.weight[:] = W_q_nope.T\n        self.o_proj_absorbed = nn.Linear(\n            in_features=self.num_heads * self.num_key_value_heads * self.low_rank,\n            out_features=self.hidden_size,\n            dtype=self.q_proj.weight.dtype,\n            device=self.q_proj.weight.device,\n            bias=False,\n        )\n        self.o_proj_absorbed.weight[:] = o_oroj_weight.T\n\n    def get_qk_states_for_decode(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ):\n        bsz, q_len, _ = hidden_states.size()\n        assert not self.config.pretraining_tp > 1, \"not support pretraining_tp>1\"\n        # prepare q_r and k_r\n        query_states = torch.zeros(\n            (bsz, q_len, self.num_heads * self.head_dim),\n            device=hidden_states.device,\n            dtype=hidden_states.dtype,\n        )\n        key_states = torch.zeros(\n            (bsz, q_len, self.num_key_value_heads * self.head_dim),\n            device=hidden_states.device,\n            dtype=hidden_states.dtype,\n        )\n        q_r = self.W_q_r_absorbed(hidden_states)\n        k_r = self.W_k_r(hidden_states)\n        query_states[..., ~self.nope_mask_for_q] = q_r\n        key_states[..., ~self.nope_mask] = k_r\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        if position_embeddings is None:\n            logger.warning_once(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(key_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n        query_states, key_states = self.apply_custom_rotary_pos_emb(\n            query_states, key_states, cos, sin\n        )\n        query_states = query_states.transpose(1, 2).reshape(bsz, q_len, -1)\n        key_states = key_states.transpose(1, 2).reshape(bsz, q_len, -1)\n        q_r = query_states[..., ~self.nope_mask_for_q]\n        k_r = key_states[..., ~self.nope_mask]\n\n        # prepare c_kv\n        if self.is_share_W_down:\n            c_kv = self.W_down_k(hidden_states)\n        else:\n            c_kv = torch.cat(\n                [self.W_down_k(hidden_states), self.W_down_v(hidden_states)], dim=-1\n            )\n        q_r = q_r.view(bsz, q_len, self.num_heads, -1).transpose(1, 2)\n        k_r = k_r.view(bsz, q_len, self.num_key_value_heads, -1).transpose(\n            1, 2\n        )  # [bsz,num_key_value_heads,q_len,head_rope_dim]\n        c_kv = c_kv.view(\n            bsz, q_len, 1, self.low_rank * self.num_key_value_heads\n        ).transpose(\n            1, 2\n        )  # [bsz,1,q_len,self.low_rank * self.num_key_value_heads]\n\n        # prepare q_nope\n        q_nope = self.W_down_q_absorbed(hidden_states)\n        q_nope = q_nope.view(\n            bsz, q_len, self.num_heads, self.low_rank * self.num_key_value_heads\n        ).transpose(1, 2)\n\n        assert past_key_value is not None\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        if len(past_key_value.value_cache)>0 and past_key_value.value_cache[0].shape[1] != 1: # change c_kv from prefill format to decode format\n            value_cache = past_key_value.value_cache\n            value_cache = [\n                value_cache[i].transpose(1, 2).reshape(bsz, -1, 1, self.low_rank * self.num_key_value_heads).transpose(1, 2)\n                for i in range(len(value_cache))\n            ]\n            past_key_value.value_cache = value_cache\n\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        k_r, c_kv = past_key_value.update(\n            k_r,\n            c_kv,\n            self.layer_idx,\n            cache_kwargs,\n        )\n\n        query_states = torch.cat([q_nope, q_r], dim=-1)\n\n        return query_states, k_r, c_kv\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[\n            Tuple[torch.Tensor, torch.Tensor]\n        ] = None,  # will become mandatory in v4.46\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if not self.is_inference:\n            self.matrix_absorb()\n\n        bsz, q_len, _ = hidden_states.size()\n        if q_len!=1 or not use_cache:\n            # Prefill stage\n            return super().forward(\n                hidden_states,\n                attention_mask,\n                position_ids,\n                past_key_value,\n                output_attentions,\n                use_cache,\n                cache_position,\n                position_embeddings,\n                **kwargs,\n            )\n\n        # Decode stage\n        query_states, k_r, c_kv = self.get_qk_states_for_decode(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n        k_r = repeat_kv(k_r, self.num_key_value_groups)\n        c_kv = repeat_kv(c_kv, self.num_heads)\n        head_rope_dim = (self.nope_mask_for_q == False).sum().item() // self.num_heads\n        attn_weights_rope = torch.matmul(\n            query_states[..., -head_rope_dim:], k_r.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n        attn_weights_nope = torch.matmul(\n            query_states[..., :-head_rope_dim], c_kv.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n        attn_weights = attn_weights_rope + attn_weights_nope\n        assert attention_mask is not None, \"attention_mask is required\"\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : k_r.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.attention_dropout, training=self.training\n        )\n        attn_output = torch.matmul(attn_weights, c_kv)\n        attn_output = attn_output.transpose(1, 2).reshape(bsz, q_len, -1)\n        attn_output = self.o_proj_absorbed(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ndef state_dict_svd_init(model, state_dict):\n    for layer_idx in range(model.config.num_hidden_layers):\n        nope_mask = IndexForNope.get_index_for_nope(\n            rope_cfg=model.config.RoPE,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            head_num=model.config.num_key_value_heads,\n            layer_idx=layer_idx,\n        )\n        assert (nope_mask == model.model.layers[layer_idx].self_attn.nope_mask).all()\n        W_k = state_dict.pop(f\"model.layers.{layer_idx}.self_attn.k_proj.weight\").t()\n        W_v = state_dict.pop(f\"model.layers.{layer_idx}.self_attn.v_proj.weight\").t()\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_k_r.weight\"] = (\n            W_k[..., ~nope_mask].t().contiguous()\n        )\n        W_down_k, W_up_k, W_down_v, W_up_v = SvdInit.init(\n            W_k[..., nope_mask],\n            W_v,\n            svd_method=model.config.SVD[\"method\"],\n            r=model.config.SVD[\"low_rank\"] * model.config.num_key_value_heads,\n        )\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_down_k.weight\"] = (\n            W_down_k.contiguous()\n        )\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_up_k.weight\"] = (\n            W_up_k.contiguous()\n        )\n        if not model.model.layers[layer_idx].self_attn.is_share_W_down:\n            state_dict[f\"model.layers.{layer_idx}.self_attn.W_down_v.weight\"] = (\n                W_down_v.contiguous()\n            )\n        state_dict[f\"model.layers.{layer_idx}.self_attn.W_up_v.weight\"] = (\n            W_up_v.contiguous()\n        )\n    return state_dict\n\n@classmethod\ndef custom_load_pretrained_model(\n    cls,\n    model,\n    state_dict,\n    loaded_keys,\n    resolved_archive_file,\n    pretrained_model_name_or_path,\n    ignore_mismatched_sizes=False,\n    sharded_metadata=None,\n    _fast_init=True,\n    low_cpu_mem_usage=False,\n    device_map=None,\n    offload_folder=None,\n    offload_state_dict=None,\n    dtype=None,\n    hf_quantizer=None,\n    keep_in_fp32_modules=None,\n    gguf_path=None,\n    weights_only=True,\n):\n    is_svd_init = False\n    if state_dict is not None:\n        is_svd_init = bool(\n            all([\"W_k_r\" not in k for k in state_dict.keys()])\n            and any([\"W_k_r\" in k for k in model.state_dict().keys()])\n        )  # weather the the model is initialized by SVD\n    if is_svd_init:\n        # replace the original llama weights with the mla weights\n        state_dict = state_dict_svd_init(model, state_dict)\n        loaded_keys = list(state_dict.keys())\n    import copy\n    old_k_r_weight = copy.deepcopy(model.model.layers[0].self_attn.W_k_r.weight)\n    outputs = cls.original_load_pretrained_model(\n        model,\n        state_dict,\n        loaded_keys,\n        resolved_archive_file,\n        pretrained_model_name_or_path,\n        ignore_mismatched_sizes,\n        sharded_metadata,\n        _fast_init,\n        low_cpu_mem_usage,\n        device_map,\n        offload_folder,\n        offload_state_dict,\n        dtype,\n        hf_quantizer,\n        keep_in_fp32_modules,\n        gguf_path,\n        weights_only,\n    )\n    new_k_r_weight = model.model.layers[0].self_attn.W_k_r.weight\n    if is_svd_init:\n        assert not (old_k_r_weight == new_k_r_weight).all()\n    return outputs\n\ndef partial_rope_monkey_patch(rope_cfg):\n    \"\"\"\n    Monkey patching the LLAMA model to use the partial-RoPE.\n    \"\"\"\n\n    modeling_llama.apply_rotary_pos_emb = create_custom_apply_rotary_pos_emb_hf(\n        rope_cfg\n    )\n    if rope_cfg[\"partial_rope_version\"] == 4:\n        IndexForNope._qk_tensor_cache=torch.load(rope_cfg[\"qk_tensor_path\"])\n        IndexForNope._qk_tensor_path=rope_cfg[\"qk_tensor_path\"]\n        modeling_llama.LlamaAttention.forward = custom_forward_LlamaAttention\n        modeling_llama.LlamaFlashAttention2.forward = (\n            custom_forward_LlamaFlashAttention2\n        )\n        modeling_llama.LlamaSdpaAttention.forward = custom_forward_LlamaSdpaAttention\n\n\ndef mla_monkey_patch(rope_cfg=None):\n    \"\"\"\n    Monkey patching the LLAMA model to use the custom attention.\n    \"\"\"\n    modeling_llama.LLAMA_ATTENTION_CLASSES = {\n        \"eager\": CustomLlamaAttention,\n        \"flash_attention_2\": CustomLlamaFlashAttention2,\n        \"sdpa\": CustomLlamaSdpaAttention,\n    }\n\n    if not hasattr(\n        modeling_llama.LlamaPreTrainedModel, \"original_load_pretrained_model\"\n    ):\n        modeling_llama.LlamaPreTrainedModel.original_load_pretrained_model = (\n            modeling_llama.LlamaPreTrainedModel._load_pretrained_model\n        )\n        modeling_llama.LlamaPreTrainedModel._load_pretrained_model = (\n            custom_load_pretrained_model\n        )\n\n    if rope_cfg is not None:\n        partial_rope_monkey_patch(rope_cfg)\n\n\ndef infer_monkey_patch(rope_cfg=None):\n    @staticmethod\n    def custom_ignore_causal_mask_sdpa(\n        attention_mask: Optional[torch.Tensor],\n        inputs_embeds: torch.Tensor,\n        past_key_values_length: int,\n        sliding_window: Optional[int] = None,\n        is_training: bool = False,\n    ) -> bool:\n        return False\n\n    modeling_llama.LLAMA_ATTENTION_CLASSES = {\n        \"eager\": CustomLlamaMLAForInfer,\n        \"flash_attention_2\": CustomLlamaMLAForInfer,\n        \"sdpa\": CustomLlamaMLAForInfer,\n    }\n\n    import transformers\n\n    transformers.modeling_attn_mask_utils.AttentionMaskConverter._ignore_causal_mask_sdpa = (\n        custom_ignore_causal_mask_sdpa\n    )\n\n    if rope_cfg is not None:\n        partial_rope_monkey_patch(rope_cfg)\n"}
{"type": "source_file", "path": "src/mha2mla/lb_table.py", "content": "#!/usr/bin/env python3\nimport json\nimport re\nfrom collections import OrderedDict, defaultdict\nfrom dataclasses import dataclass, field\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport pandas as pd\n\n\n@dataclass\nclass Data:\n    name: str\n    path: Union[str, Path]\n    # dim: Optional[int] = None  # noqa: F821\n    task: str = \"lbv1\"\n    length: str = \"\"\n    data_path: Path = field(init=False)\n    score: dict[str, float] = field(init=False)\n\n    def __post_init__(self):\n        if self.path == \"\":\n            return\n        if isinstance(self.path, str):\n            self.path = Path(self.path)\n        assert self.path.is_dir(), f\"{self.path} is not a directory\"\n        if self.name == \"\":\n            self.name = self.path.name\n        if self.length == \"\":\n            files = sorted(self.path.glob(f\"{self.task}_*.jsonl\"))\n            assert files, f\"No {self.task} files found in {self.path}\"\n            self.length = str(files[-1].stem[len(f\"{self.task}_\") :])\n            if len(files) > 1:\n                print(f\"Warning: multiple {self.task} files found in {self.path}, using {self.length}\")\n        self.data_path = self.path / f\"{self.task}_{self.length}.jsonl\"\n        assert self.data_path.is_file(), f\"No {self.task} file found at {self.data_path}\"\n\n    def get_label(self):\n        return f\"{self.name}\"\n        # return f\"{self.name} (test {self.length})\"\n\n\n# data = [\n#     Data(\"\", \"saves/eval_pos/SmolLM-135M\", \"\"),\n#     Data(\"\", \"saves/eval_pos/SmolLM-360M\", \"\"),\n#     Data(\"\", \"saves/eval_pos/SmolLM-1.7B\", \"\"),\n#     Data(\"\", \"saves/eval_pos/SmolLM-1.7B-Instruct\", \"\"),\n#     Data(\"TinyLlama-1.1B\", \"saves/eval_pos/TinyLlama-1.1B-intermediate-step-1431k-3T\", \"\"),\n#     Data(\"\", \"saves/eval_pos/TinyLlama-NoPE-1.1B\", \"\"),\n# ]\ndata=[]\n\nname_map = {\n    \"Single-Document QA\": {\n        \"narrativeqa\": \"NarraQA\",\n        \"qasper\": \"Qspr.\",\n        \"multifieldqa_en\": \"MulF-en\",  # TODO\n        # \"multifieldqa_zh\": \"MulF-zh\"  # TODO\n    },\n    \"Multi-Document QA\": {\n        \"hotpotqa\": \"HpQA\",\n        \"2wikimqa\": \"2WiMQA\",\n        \"musique\": \"Musq.\",\n        # \"dureader\": \"DuRd-zh\"  # TODO\n    },\n    \"Summarization\": {\n        \"gov_report\": \"GRpt\",\n        \"qmsum\": \"QMSum\",\n        \"multi_news\": \"MultiN\",\n        # \"vcsum\": \"VCS-zh\"  # TODO\n    },\n    \"Few-shot Learning\": {\n        \"trec\": \"TREC\",\n        \"triviaqa\": \"TrivQA\",\n        \"samsum\": \"SAMSum\",\n        # \"lsht\": \"LSHT-zh\"  # TODO\n    },\n    \"Synthetic Tasks\": {\n        \"passage_count\": \"PsgC\",\n        \"passage_retrieval_en\": \"PsgR-en\",  # TODO\n        # \"passage_retrieval_zh\": \"PsgR-zh\"  # TODO\n    },\n    \"Code Completion\": {\n        \"lcc\": \"LCC\",\n        \"repobench-p\": \"RepoB-P\",\n    },\n}\n\n\ndef parse_result(d: Data):\n    score_list = defaultdict(list)\n    with open(d.data_path, \"r\") as f:\n        for line in f:\n            raw = json.loads(line)\n            score_list[raw[\"dataset\"]].append(raw[\"score\"])\n    score = {k: sum(v) * 100 / len(v) for k, v in score_list.items()}\n    d.score = score\n\n\ndef build_table():\n    columns = [column for v in name_map.values() for column in v.keys()]\n    df_columns = [\"Model\", \"Length\", \"Avg.\"] + [column for v in name_map.values() for column in v.values()]\n    # df_columns = [(\"\", \"Model\"), (\"\", \"Avg.\")] + [\n    #     (title, column) for title, v in name_map.items() for column in v.values()\n    # ]\n    # df_columns = pd.MultiIndex.from_tuples(df_columns)\n    df_data = []\n    for d in data:\n        row = [d.score[column] for column in columns]\n        row = [d.get_label(), d.length, sum(row) / len(row)] + row\n        df_data.append(row)\n    df = pd.DataFrame(df_data, columns=df_columns)\n    print(df)\n    return df\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--path\", type=str, required=True)\n    path = Path(parser.parse_args().path)\n    dirs = sorted([x for x in path.iterdir() if x.is_dir()])\n    for d in dirs:\n        data.append(Data(\"\", d))\n    for d in data:\n        parse_result(d)\n    df = build_table()\n    with open(\"longbench.md\", \"w\") as f:\n        f.write(df.round(1).to_markdown(index=False))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/mha2mla/eval.py", "content": "import argparse\nimport os\nfrom dataclasses import asdict\nfrom pprint import pformat\n\nfrom lighteval.parsers import parser_accelerate, parser_baseline, parser_nanotron, parser_utils_tasks\nfrom lighteval.tasks.registry import Registry, taskinfo_selector\nimport yaml,json\n\n\nCACHE_DIR = os.getenv(\"HF_HOME\")\n\n\n# copied from lighteval.main\ndef cli_evaluate():  # noqa: C901\n    parser = argparse.ArgumentParser(description=\"CLI tool for lighteval, a lightweight framework for LLM evaluation\")\n    parser.add_argument(\n        \"--is_partial_rope\",\n        action=\"store_true\",\n        help=\"Path to the RoPE configuration file. This file should contain the partial-RoPE configuration.\",\n    )\n    parser.add_argument(\n        \"--is_mla\",\n        action=\"store_true\",\n        help=\"Whether the model is a MLA model. If True, the model will be evaluated using the MLA architecture.\",\n    )\n\n    subparsers = parser.add_subparsers(help=\"help for subcommand\", dest=\"subcommand\")\n\n    # Subparser for the \"accelerate\" command\n    parser_a = subparsers.add_parser(\"accelerate\", help=\"use accelerate and transformers as backend for evaluation.\")\n    parser_accelerate(parser_a)\n\n    args = parser.parse_args()\n    if args.is_mla or args.is_partial_rope:\n        from monkey_patch import partial_rope_monkey_patch, mla_monkey_patch\n        model_args = args.model_args\n        model_args = {k.split(\"=\")[0]: k.split(\"=\")[1] if \"=\" in k else True for k in model_args.split(\",\")}\n        ckpt_path = model_args[\"pretrained\"]\n        with open(os.path.join(ckpt_path,\"config.json\")) as f:\n            model_config = json.load(f)\n        cfg_RoPE = model_config[\"RoPE\"]\n        assert not (args.is_mla and args.is_partial_rope), \"Cannot set both is_mla and is_partial_rope to True.\"\n        if args.is_mla:\n            mla_monkey_patch(cfg_RoPE)\n        else:\n            partial_rope_monkey_patch(cfg_RoPE)\n\n    if args.subcommand == \"accelerate\":\n        from lighteval.main_accelerate import main as main_accelerate\n\n        main_accelerate(args)\n    else:\n        print(\"You need to set the subcommand to 'accelerate'.\")\n\n\nif __name__ == \"__main__\":\n    cli_evaluate()\n"}
{"type": "source_file", "path": "src/mha2mla_nt/run_train.py", "content": "\nimport argparse\nimport yaml\nfrom typing import Dict, cast\n\nimport numpy as np\nfrom nanotron import logging\nfrom nanotron.config import (\n    DataArgs,\n    DatasetStageArgs,\n    NanosetDatasetsArgs,\n    PretrainDatasetsArgs,\n)\nfrom nanotron.data.dataloader_builder import build_nanoset_dataloader\nfrom nanotron.dataloader import (\n    clm_process,\n    dummy_infinite_data_generator,\n    get_datasets,\n    get_train_dataloader,\n)\nfrom nanotron.helpers import (\n    compute_remain_train_steps_of_a_data_stage_from_ckp,\n    get_consumed_train_samples_of_a_data_stage_from_ckp,\n)\nfrom nanotron.logging import log_rank\nfrom nanotron.parallel.pipeline_parallel.utils import get_input_output_pp_ranks\nfrom nanotron.trainer import DistributedTrainer\nfrom nanotron.utils import main_rank_first\nfrom torch.utils.data import DataLoader\n\ntry:\n    from huggingface_hub import __version__ as hf_hub_version\n    from transformers import AutoTokenizer\n    from transformers import __version__ as tf_version\nexcept ImportError:\n    hf_hub_version = None\n    tf_version = None\n\nlogger = logging.get_logger(__name__)\n\n\ndef get_dataloader_from_data_stage(\n    trainer: DistributedTrainer,\n    data: DataArgs,\n    consumed_train_samples: int,\n    num_remaining_train_steps: int,\n):\n    \"\"\"\n    Returns a dataloader for a given data stage.\n\n    data: The data configuration for the current stage.\n    consumed_train_samples: The number of samples consumed by the model in the this stage (each stage starts from zero).\n    num_remaining_train_steps: The number of remaining training steps for this stage.\n    \"\"\"\n    assert (\n        consumed_train_samples >= 0\n    ), \"consumed_train_samples should be greater than 0\"\n    assert (\n        num_remaining_train_steps >= 0\n    ), \"num_remaining_train_steps should be greater than 0\"\n\n    # First, we need to know which ranks to feed the dataloader to\n    input_pp_rank, output_pp_rank = get_input_output_pp_ranks(model=trainer.model)\n\n    # Case 1: Dummy data generator\n    if data.dataset is None:\n        log_rank(\n            \"Using dummy data generator\", logger=logger, level=logging.INFO, rank=0\n        )\n        dataloader = dummy_infinite_data_generator(\n            micro_batch_size=trainer.micro_batch_size,\n            sequence_length=trainer.sequence_length,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            vocab_size=trainer.model_config.vocab_size,\n            seed=data.seed,\n            parallel_context=trainer.parallel_context,\n        )()\n\n    # Case 2: HuggingFace datasets\n    elif isinstance(data.dataset, PretrainDatasetsArgs):\n        log_rank(\"Using `datasets` library\", logger=logger, level=logging.INFO, rank=0)\n        tokenizer_path = trainer.config.tokenizer.tokenizer_name_or_path\n        log_rank(\n            f\"Loading tokenizer from {tokenizer_path} and transformers/hf_hub versions {tf_version, hf_hub_version}\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        # We need to the 1st device to process dataset and cache it, then other devices load from cache\n        with main_rank_first(trainer.parallel_context.world_pg):\n            # TODO @nouamanetazi: this may timeout before 1st device finishes processing dataset. Can we have a ctxmanager to modify timeout?\n            # TODO: generalise to include  for validation/test splits\n\n            # We load the raw dataset\n            raw_dataset = get_datasets(\n                hf_dataset_or_datasets=data.dataset.hf_dataset_or_datasets,\n                hf_dataset_config_name=data.dataset.hf_dataset_config_name,\n                splits=data.dataset.hf_dataset_splits,\n            )[\"train\"]\n\n            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.padding_side = \"left\"\n\n            # Check that tokenizer's vocab size is smaller than the model's vocab size\n            assert (\n                tokenizer.vocab_size <= trainer.model_config.vocab_size\n            ), f\"Tokenizer's vocab size ({tokenizer.vocab_size}) is larger than the model's vocab size ({trainer.model_config.vocab_size})\"\n\n            # We apply the Causal Language Modeling preprocessing\n            train_dataset = clm_process(\n                raw_dataset=raw_dataset,\n                tokenizer=tokenizer,\n                text_column_name=data.dataset.text_column_name,\n                dataset_processing_num_proc_per_process=data.dataset.dataset_processing_num_proc_per_process,\n                dataset_overwrite_cache=data.dataset.dataset_overwrite_cache,\n                sequence_length=trainer.sequence_length,\n            )\n\n            # We load the processed dataset on the ranks requiring it\n            dataloader = get_train_dataloader(\n                train_dataset=train_dataset,\n                sequence_length=trainer.sequence_length,\n                parallel_context=trainer.parallel_context,\n                input_pp_rank=input_pp_rank,\n                output_pp_rank=output_pp_rank,\n                micro_batch_size=trainer.micro_batch_size,\n                consumed_train_samples=consumed_train_samples,\n                dataloader_num_workers=data.num_loading_workers,\n                seed_worker=data.seed,\n                dataloader_drop_last=True,\n            )\n\n            # Check if we have enough samples for train_steps\n            total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n            num_tokens_needed_for_training = (\n                num_remaining_train_steps\n                * trainer.global_batch_size\n                * trainer.sequence_length\n            )\n            assert num_tokens_needed_for_training <= total_tokens_dataset, (\n                f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n                f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.iteration_step}\"\n            )\n\n    # Case 3: Nanosets\n    elif isinstance(data.dataset, NanosetDatasetsArgs):\n        # Get tokenizer cardinality\n        tokenizer = AutoTokenizer.from_pretrained(\n            trainer.config.tokenizer.tokenizer_name_or_path\n        )\n        token_size = 4 if len(tokenizer) > np.iinfo(np.uint16).max + 1 else 2\n        del tokenizer\n        # Create Nanoset\n        from nanotron.data.nanoset import Nanoset\n\n        with main_rank_first(trainer.parallel_context.world_pg):\n            train_dataset = Nanoset(\n                dataset_folders=data.dataset.dataset_folder,\n                dataset_weights=data.dataset.dataset_weights,\n                sequence_length=trainer.sequence_length,\n                token_size=token_size,\n                train_split_num_samples=trainer.config.tokens.train_steps\n                * trainer.global_batch_size,\n                random_seed=data.seed,\n            )\n\n        # Prepare dataloader\n        train_dataloader = build_nanoset_dataloader(\n            train_dataset,\n            trainer.sequence_length,\n            parallel_context=trainer.parallel_context,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            micro_batch_size=trainer.micro_batch_size,\n            consumed_train_samples=consumed_train_samples,\n            dataloader_num_workers=data.num_loading_workers,\n            dataloader_drop_last=True,\n        )\n\n        return train_dataloader\n    else:\n        raise ValueError(\n            f\"Unhandled case of `self.config.data.dataset`. Got: {data.dataset}\"\n        )\n\n    return dataloader\n\n\ndef get_dataloader(trainer: DistributedTrainer) -> Dict[str, DataLoader]:\n    dataloaders = {}\n\n    for stage_idx, stage in enumerate(trainer.config.data_stages):\n        # NOTE: we only create the dataloader for the first stage,\n        # then we lazy initialize the dataloader for the other stages\n        stage = cast(DatasetStageArgs, stage)\n        consumed_train_samples = get_consumed_train_samples_of_a_data_stage_from_ckp(\n            stage, trainer.metadata\n        )\n        assert (\n            consumed_train_samples is not None\n        ), f\"Cannot find consumed_train_samples for stage {stage.start_training_step} in the checkpoint\"\n\n        num_remaining_train_steps = compute_remain_train_steps_of_a_data_stage_from_ckp(\n            stage, trainer.config, trainer.metadata\n        )\n        log_rank(\n            f\"[Training Plan] Stage {stage.name} has {num_remaining_train_steps} remaining training steps and has consumed {consumed_train_samples} samples\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        dataloader = (\n            get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n            if stage_idx == 0\n            else lambda stage=stage: get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n        )\n        dataloaders[stage.name] = dataloader\n    return dataloaders\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config-file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML or python config file\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    config_file = args.config_file\n\n    # Monkey patch\n    from monkey_patch import mla_monkey_patch,partial_rope_monkey_patch,CustomConfig\n    import yaml\n    with open(config_file, \"r\") as fin:\n        config = yaml.safe_load(fin)\n    if \"SVD\" in config[\"model\"][\"model_config\"] and config[\"model\"][\"model_config\"][\"SVD\"][\"method\"]!=0:\n        mla_monkey_patch(config[\"model\"][\"model_config\"][\"RoPE\"])\n    else:\n        partial_rope_monkey_patch(config[\"model\"][\"model_config\"][\"RoPE\"])\n    from nanotron import trainer as nt_trainer\n    # Load trainer and data\n    trainer = nt_trainer.DistributedTrainer(config_file,config_class=CustomConfig)\n    dataloader = get_dataloader(trainer)\n\n    # Train\n    trainer.train(dataloader)\n"}
{"type": "source_file", "path": "src/mha2mla_nt/2_norm.py", "content": "\"\"\"\nNanotron training script.\n\nUsage:\n```\nexport CUDA_DEVICE_MAX_CONNECTIONS=1 # important for some distributed operations\ntorchrun --nproc_per_node=8 run_train.py --config-file examples/config_tiny_llama.yaml\n```\n\"\"\"\n\nimport argparse\nimport os\nimport yaml\nfrom typing import Dict, cast\nfrom types import MethodType\nimport seaborn as sns\nimport torch\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nanotron import logging\nfrom nanotron.config import (\n    DataArgs,\n    DatasetStageArgs,\n    NanosetDatasetsArgs,\n    PretrainDatasetsArgs,\n)\nfrom nanotron.data.dataloader_builder import build_nanoset_dataloader\nfrom nanotron.dataloader import (\n    clm_process,\n    dummy_infinite_data_generator,\n    get_datasets,\n    get_train_dataloader,\n)\nfrom nanotron.helpers import (\n    compute_remain_train_steps_of_a_data_stage_from_ckp,\n    get_consumed_train_samples_of_a_data_stage_from_ckp,\n)\nfrom nanotron.logging import log_rank\nfrom nanotron.parallel.pipeline_parallel.utils import get_input_output_pp_ranks\nfrom nanotron.trainer import DistributedTrainer\nfrom nanotron.utils import main_rank_first\nfrom torch.utils.data import DataLoader\n\ntry:\n    from huggingface_hub import __version__ as hf_hub_version\n    from transformers import AutoTokenizer\n    from transformers import __version__ as tf_version\nexcept ImportError:\n    hf_hub_version = None\n    tf_version = None\n\nlogger = logging.get_logger(__name__)\n\n\ndef get_dataloader_from_data_stage(\n    trainer: DistributedTrainer,\n    data: DataArgs,\n    consumed_train_samples: int,\n    num_remaining_train_steps: int,\n):\n    \"\"\"\n    Returns a dataloader for a given data stage.\n\n    data: The data configuration for the current stage.\n    consumed_train_samples: The number of samples consumed by the model in the this stage (each stage starts from zero).\n    num_remaining_train_steps: The number of remaining training steps for this stage.\n    \"\"\"\n    assert (\n        consumed_train_samples >= 0\n    ), \"consumed_train_samples should be greater than 0\"\n    assert (\n        num_remaining_train_steps >= 0\n    ), \"num_remaining_train_steps should be greater than 0\"\n\n    # First, we need to know which ranks to feed the dataloader to\n    input_pp_rank, output_pp_rank = get_input_output_pp_ranks(model=trainer.model)\n\n    # Case 1: Dummy data generator\n    if data.dataset is None:\n        log_rank(\n            \"Using dummy data generator\", logger=logger, level=logging.INFO, rank=0\n        )\n        dataloader = dummy_infinite_data_generator(\n            micro_batch_size=trainer.micro_batch_size,\n            sequence_length=trainer.sequence_length,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            vocab_size=trainer.model_config.vocab_size,\n            seed=data.seed,\n            parallel_context=trainer.parallel_context,\n        )()\n\n    # Case 2: HuggingFace datasets\n    elif isinstance(data.dataset, PretrainDatasetsArgs):\n        log_rank(\"Using `datasets` library\", logger=logger, level=logging.INFO, rank=0)\n        tokenizer_path = trainer.config.tokenizer.tokenizer_name_or_path\n        log_rank(\n            f\"Loading tokenizer from {tokenizer_path} and transformers/hf_hub versions {tf_version, hf_hub_version}\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        # We need to the 1st device to process dataset and cache it, then other devices load from cache\n        with main_rank_first(trainer.parallel_context.world_pg):\n            # TODO @nouamanetazi: this may timeout before 1st device finishes processing dataset. Can we have a ctxmanager to modify timeout?\n            # TODO: generalise to include  for validation/test splits\n\n            # We load the raw dataset\n            raw_dataset = get_datasets(\n                hf_dataset_or_datasets=data.dataset.hf_dataset_or_datasets,\n                hf_dataset_config_name=data.dataset.hf_dataset_config_name,\n                splits=data.dataset.hf_dataset_splits,\n            )[\"train\"]\n\n            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.padding_side = \"left\"\n\n            # Check that tokenizer's vocab size is smaller than the model's vocab size\n            assert (\n                tokenizer.vocab_size <= trainer.model_config.vocab_size\n            ), f\"Tokenizer's vocab size ({tokenizer.vocab_size}) is larger than the model's vocab size ({trainer.model_config.vocab_size})\"\n\n            # We apply the Causal Language Modeling preprocessing\n            train_dataset = clm_process(\n                raw_dataset=raw_dataset,\n                tokenizer=tokenizer,\n                text_column_name=data.dataset.text_column_name,\n                dataset_processing_num_proc_per_process=data.dataset.dataset_processing_num_proc_per_process,\n                dataset_overwrite_cache=data.dataset.dataset_overwrite_cache,\n                sequence_length=trainer.sequence_length,\n            )\n\n            # We load the processed dataset on the ranks requiring it\n            dataloader = get_train_dataloader(\n                train_dataset=train_dataset,\n                sequence_length=trainer.sequence_length,\n                parallel_context=trainer.parallel_context,\n                input_pp_rank=input_pp_rank,\n                output_pp_rank=output_pp_rank,\n                micro_batch_size=trainer.micro_batch_size,\n                consumed_train_samples=consumed_train_samples,\n                dataloader_num_workers=data.num_loading_workers,\n                seed_worker=data.seed,\n                dataloader_drop_last=True,\n            )\n\n            # Check if we have enough samples for train_steps\n            total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n            num_tokens_needed_for_training = (\n                num_remaining_train_steps\n                * trainer.global_batch_size\n                * trainer.sequence_length\n            )\n            assert num_tokens_needed_for_training <= total_tokens_dataset, (\n                f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n                f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.iteration_step}\"\n            )\n\n    # Case 3: Nanosets\n    elif isinstance(data.dataset, NanosetDatasetsArgs):\n        # Get tokenizer cardinality\n        tokenizer = AutoTokenizer.from_pretrained(\n            trainer.config.tokenizer.tokenizer_name_or_path\n        )\n        token_size = 4 if len(tokenizer) > np.iinfo(np.uint16).max + 1 else 2\n        del tokenizer\n        # Create Nanoset\n        from nanotron.data.nanoset import Nanoset\n\n        with main_rank_first(trainer.parallel_context.world_pg):\n            train_dataset = Nanoset(\n                dataset_folders=data.dataset.dataset_folder,\n                dataset_weights=data.dataset.dataset_weights,\n                sequence_length=trainer.sequence_length,\n                token_size=token_size,\n                train_split_num_samples=trainer.config.tokens.train_steps\n                * trainer.global_batch_size,\n                random_seed=data.seed,\n            )\n\n        # Prepare dataloader\n        train_dataloader = build_nanoset_dataloader(\n            train_dataset,\n            trainer.sequence_length,\n            parallel_context=trainer.parallel_context,\n            input_pp_rank=input_pp_rank,\n            output_pp_rank=output_pp_rank,\n            micro_batch_size=trainer.micro_batch_size,\n            consumed_train_samples=consumed_train_samples,\n            dataloader_num_workers=data.num_loading_workers,\n            dataloader_drop_last=True,\n        )\n\n        return train_dataloader\n    else:\n        raise ValueError(\n            f\"Unhandled case of `self.config.data.dataset`. Got: {data.dataset}\"\n        )\n\n    return dataloader\n\n\ndef get_dataloader(trainer: DistributedTrainer) -> Dict[str, DataLoader]:\n    dataloaders = {}\n\n    for stage_idx, stage in enumerate(trainer.config.data_stages):\n        # NOTE: we only create the dataloader for the first stage,\n        # then we lazy initialize the dataloader for the other stages\n        stage = cast(DatasetStageArgs, stage)\n        consumed_train_samples = get_consumed_train_samples_of_a_data_stage_from_ckp(\n            stage, trainer.metadata\n        )\n        assert (\n            consumed_train_samples is not None\n        ), f\"Cannot find consumed_train_samples for stage {stage.start_training_step} in the checkpoint\"\n\n        num_remaining_train_steps = compute_remain_train_steps_of_a_data_stage_from_ckp(\n            stage, trainer.config, trainer.metadata\n        )\n        log_rank(\n            f\"[Training Plan] Stage {stage.name} has {num_remaining_train_steps} remaining training steps and has consumed {consumed_train_samples} samples\",\n            logger=logger,\n            level=logging.INFO,\n            rank=0,\n        )\n\n        dataloader = (\n            get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n            if stage_idx == 0\n            else lambda stage=stage: get_dataloader_from_data_stage(\n                trainer,\n                stage.data,\n                consumed_train_samples=consumed_train_samples,\n                num_remaining_train_steps=num_remaining_train_steps,\n            )\n        )\n        dataloaders[stage.name] = dataloader\n    return dataloaders\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config-file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML or python config file\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--sample-size\",\n        type=int,\n        default=1024,\n    )\n    return parser.parse_args()\n\n\nquery_states_dict = {}\nkey_states_dict = {}\n\nfrom typing import Optional, Tuple, Union\n\n\ndef get_rotary_forward(module_name):\n    def my_rotary_forward(\n        self,\n        qkv: torch.Tensor,\n        kv: Optional[torch.Tensor] = None,\n        seqlen_offset: Union[int, torch.Tensor] = 0,\n        max_seqlen: Optional[int] = None,\n        num_heads_q: Optional[int] = None,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        global query_states_dict\n        global key_states_dict\n        query_states_dict[module_name] = qkv\n        key_states_dict[module_name] = torch.split(kv, 1, dim=2)[0]\n        return self._forward(qkv, kv, seqlen_offset, max_seqlen, num_heads_q)\n\n    return my_rotary_forward\n\n\ndef flatten_avg_query_key_states(query_states: dict, key_states: dict):\n    max_num_heads = max(\n        list(query_states_dict.values())[0].shape[2],\n        list(key_states_dict.values())[0].shape[2],\n    )\n\n    def flatten_dict(states: dict):\n        f_states = []\n        for k, v in states.items():  # states: {module_name: hidden_states}\n            item = []\n            v = v.squeeze()  # bsz,seqlen,head,head_dim\n            if len(v.shape) == 3:\n                v = v.unsqueeze(0)\n            v = torch.norm(\n                v.reshape(v.shape[0], v.shape[1], v.shape[2], 2, -1).transpose(-1, -2),\n                p=2,\n                dim=4,\n            )\n            block_size = max_num_heads // v.shape[2]\n            for i in range(max_num_heads):\n                idx = i // block_size\n                head_i = v[:, :, idx, :]\n                item.append(\n                    torch.mean(\n                        head_i.reshape(v.shape[0] * v.shape[1], -1),\n                        dim=0,\n                        keepdim=False,\n                    )\n                    .to(dtype=torch.float32)\n                    .cpu()\n                )\n            f_states.append(torch.stack(item))\n        return torch.stack(f_states)\n\n    return flatten_dict(query_states), flatten_dict(key_states)\n\n\ndef get_fig_ax():\n    r, c = 4, 3\n    fig, axes = plt.subplots(r, c, figsize=(c * 4, r * 2))\n    axes = axes.flatten()\n    return fig, axes\n\n\ndef visualize(query_states, key_states):\n    dir = \"../images/2-norm\"\n    num_layers = query_states.shape[0]\n    # query\n    for idx in range(0, num_layers, 12):\n        st, ed = idx, min(idx + 12, num_layers)\n        fig, axes = get_fig_ax()\n        for i in range(0, 12):\n            if i + st >= ed:\n                break\n            data = query_states[i + st].numpy()\n            sns.heatmap(\n                data,\n                ax=axes[i],\n                cmap=\"Greens\",\n                xticklabels=False,\n                yticklabels=False,  \n            )\n            axes[i].set_title(f\"Layer {i + 1}\")\n\n            xticks = range(\n                0, data.shape[1], max(1, data.shape[1] // 6)\n            ) \n            axes[i].set_xticks(xticks)\n            axes[i].set_xticklabels([str(x) for x in xticks], rotation=45)\n\n            yticks = range(data.shape[0])\n            axes[i].set_yticks(yticks)\n            axes[i].set_yticklabels([str(y) for y in reversed(yticks)]) \n\n        fig.suptitle(\"Query x:n_dim y:head\")\n        fig.tight_layout()  \n        fig.savefig(f\"{dir}/query_layer{st}_{ed}.png\")\n\n    # key\n    for idx in range(0, num_layers, 12):\n        st, ed = idx, min(idx + 12, num_layers)\n        fig, axes = get_fig_ax()\n        for i in range(0, 12):\n            if i + st >= ed:\n                break\n            data = key_states[i + st].numpy()\n            sns.heatmap(\n                data,\n                ax=axes[i],\n                cmap=\"Greens\",\n                xticklabels=False, \n                yticklabels=False, \n            )\n            axes[i].set_title(f\"Layer {i + 1}\")\n\n            xticks = range(\n                0, data.shape[1], max(1, data.shape[1] // 6)\n            )  \n            axes[i].set_xticks(xticks)\n            axes[i].set_xticklabels([str(x) for x in xticks], rotation=45)\n\n            yticks = range(data.shape[0])\n            axes[i].set_yticks(yticks)\n            axes[i].set_yticklabels([str(y) for y in reversed(yticks)])  \n\n        fig.suptitle(\"Key x:n_dim y:head\")\n        fig.tight_layout() \n        fig.savefig(f\"{dir}/key_layer{st}_{ed}.png\")\n\n\ndef main():\n    args = get_args()\n    config_file = args.config_file\n    trainer = DistributedTrainer(config_file)\n    model = trainer.model\n    dataloader = get_dataloader(trainer)\n    dataloader = list(dataloader.values())[0]\n    model.eval()\n    model.to(\"cuda\")\n    from flash_attn.layers.rotary import RotaryEmbedding as FlashRotaryEmbedding\n\n    for name, module in model.named_modules():\n        module.to(\"cuda\")\n        if not isinstance(module, FlashRotaryEmbedding):\n            continue\n        module._forward = module.forward\n        module.forward = MethodType(get_rotary_forward(module_name=name), module)\n    num = args.sample_size\n    bsz = None\n    query_states = []\n    key_states = []\n    input_ids = []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {key: value.to(\"cuda\") for key, value in batch.items()}\n            input_ids.append(batch[\"input_ids\"])\n            if bsz is None:\n                bsz = batch[\"input_ids\"].shape[0]\n            model(**batch)\n            query, key = flatten_avg_query_key_states(\n                query_states_dict, key_states_dict\n            )\n            query_states.append(query)\n            key_states.append(key)\n            query_states_dict.clear()\n            key_states_dict.clear()\n            num -= bsz\n            if num == 0:\n                break\n    query_states = torch.stack(query_states)\n    key_states = torch.stack(key_states)\n    query_states = torch.mean(\n        query_states, dim=0, keepdim=False\n    )  # [n_layer][n_head][n_dim/2]\n    key_states = torch.mean(key_states, dim=0, keepdim=False)\n    # visualize(query_states, key_states)\n    qk_states = query_states + key_states\n    if qk_states.shape[1] != model.config.num_key_value_heads:\n        layer_num, _, dim = query_states.shape\n        qk_states = qk_states.view(\n            layer_num, model.config.num_key_value_heads, -1, dim\n        ).sum(dim=2)\n    with open(os.path.join(args.output_dir, \"qk_tensor.pth\"), \"wb\") as f:\n        torch.save(qk_states, f)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/mha2mla_nt/monkey_patch.py", "content": "from typing import List, Union, Dict\n\nimport torch\nimport pickle\nimport sys\n\nfrom nanotron.parallel.pipeline_parallel.block import TensorPointer\n\nfrom typing import Any, Dict, Optional\nimport nanotron.config\nimport nanotron.config\nimport nanotron.config.config\nimport nanotron.config.models_config\nimport nanotron.trainer\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom packaging.version import Version\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nimport nanotron\nfrom nanotron.s3_checkpoints import check_path_is_local\nfrom safetensors.torch import safe_open\nfrom tqdm import tqdm\nfrom nanotron.config import (\n    ParallelismArgs,\n    LlamaConfig,\n    ExistingCheckpointInit,\n    RandomInit,\n    SpectralMupInit,\n)\nfrom nanotron.generation.generate_store import AttachableStore\nfrom torch.nn.parallel import DistributedDataParallel\nfrom nanotron.constants import CHECKPOINT_VERSION\nfrom nanotron.logging import log_rank\nfrom nanotron import logging\nfrom nanotron.distributed import get_global_rank\nfrom nanotron.parallel.tensor_parallel.nn import (\n    TensorParallelColumnLinear,\n    TensorParallelLinearMode,\n    TensorParallelRowLinear,\n)\nfrom nanotron.models import NanotronModel\nfrom nanotron.models import llama\nfrom nanotron.models.llama import (\n    CoreAttention,\n    logger,\n)\nfrom nanotron.parallel.tied_parameters import get_tied_id_to_param\nfrom nanotron.parallel.parameters import NanotronParameter\nfrom nanotron.serialize import weights as nt_weights\nfrom nanotron.serialize.weights import (\n    get_checkpoint_version,\n    read_checkpoint_version_from_shard_file,\n    CheckpointVersionFromShardFileException,\n    load_sharded_param_latest,\n)\nfrom nanotron.parallel import ParallelContext\nfrom nanotron.serialize.utils import (\n    ObjectType,\n    get_exp_tp_pp_rank_and_size_from,\n    get_path,\n)\nfrom transformers.modeling_flash_attention_utils import _flash_attention_forward\n\n\ndef create_custom_apply_rotary_pos_emb(cfg):\n    def apply_rotary_pos_emb_v0(self, q, k, cos, sin, unsqueeze_dim=2):\n        # Full-RoPE\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v1(self, q, k, cos, sin, unsqueeze_dim=2):\n        # retain the fastest-rotating (high-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        keep_dim = cfg[\"top_k_rope_dim\"]\n        if keep_dim <= 0:\n            return q, k\n        elif keep_dim >= q.size(-1):\n            return q_embed, k_embed\n        half = q.size(-1) // 2\n        q_embed = torch.cat(\n            (\n                q_embed[..., :keep_dim],\n                q[..., keep_dim:half],\n                q_embed[..., half : half + keep_dim],\n                q[..., half + keep_dim :],\n            ),\n            -1,\n        )\n        k_embed = torch.cat(\n            (\n                k_embed[..., :keep_dim],\n                k[..., keep_dim:half],\n                k_embed[..., half : half + keep_dim],\n                k[..., half + keep_dim :],\n            ),\n            -1,\n        )\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v2(self, q, k, cos, sin, unsqueeze_dim=2):\n        # select subspaces with equidistant intervals\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        indices = torch.arange(\n            cfg[\"uniform_start_point\"], q.size(-1), cfg[\"uniform_step\"], device=q.device\n        )\n        q,k = q.clone(),k.clone()\n        q[..., indices] = q_embed[..., indices]\n        k[..., indices] = k_embed[..., indices]\n        return q, k\n\n    def apply_rotary_pos_emb_v3(self, q, k, cos, sin, unsqueeze_dim=2):\n        # retain the fastest-rotating (high-frequency) subspaces and the slowest-rotating (low-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        top_k_dim, last_k_dim = cfg[\"top_k_rope_dim\"], cfg[\"last_k_rope_dim\"]\n        # assert top_k_dim + last_k_dim <= q.size(-1)\n        half = q.size(-1) // 2\n        qs = [\n            q_embed[..., :top_k_dim],\n            q[..., top_k_dim : half - last_k_dim],\n            q_embed[..., half - last_k_dim : half + top_k_dim],\n            q[..., half + top_k_dim : half + half - last_k_dim],\n            q_embed[..., half + half - last_k_dim :],\n        ]\n        ks = [\n            k_embed[..., :top_k_dim],\n            k[..., top_k_dim : half - last_k_dim],\n            k_embed[..., half - last_k_dim : half + top_k_dim],\n            k[..., half + top_k_dim : half + half - last_k_dim],\n            k_embed[..., half + half - last_k_dim :],\n        ]\n        q_embed = torch.cat([q for q in qs if q != []], -1)\n        k_embed = torch.cat([k for k in ks if k != []], -1)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v4(self, q, k, cos, sin, layer_idx=0, unsqueeze_dim=2):\n        # retain the subspaces with higher 2-norm score\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        top_k_dim = cfg[\"top_k_rope_dim\"]\n        qk_tensor[layer_idx] = qk_tensor[layer_idx].to(q.device)\n        topk_indices = torch.topk(qk_tensor[layer_idx], k=top_k_dim, dim=1)[1]\n        mask = torch.zeros_like(qk_tensor[layer_idx])\n        mask.scatter_(1, topk_indices, 1)\n        mask_for_k = (\n            torch.cat((mask, mask), dim=1).unsqueeze(0).unsqueeze(1).to(q.device)\n        )\n        mask_for_q = torch.repeat_interleave(\n            input=mask_for_k, repeats=cfg[\"n_gqa_group\"], dim=2\n        ).to(q.device)\n        q_embed = torch.where(mask_for_q == 1, q_embed, q)\n        k_embed = torch.where(mask_for_k == 1, k_embed, k)\n        return q_embed, k_embed\n\n    def apply_rotary_pos_emb_v5(self, q, k, cos, sin, unsqueeze_dim=2):\n        # retain the slowest-rotating (low-frequency) subspaces\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        last_k_dim = cfg[\"last_k_rope_dim\"]\n        half = q.size(-1) // 2\n        qs = [\n            q[..., : half - last_k_dim],\n            q_embed[..., half - last_k_dim : half],\n            q[..., half : half + half - last_k_dim],\n            q_embed[..., half + half - last_k_dim :],\n        ]\n        ks = [\n            k[..., : half - last_k_dim],\n            k_embed[..., half - last_k_dim : half],\n            k[..., half : half + half - last_k_dim],\n            k_embed[..., half + half - last_k_dim :],\n        ]\n        q_embed = torch.cat([q for q in qs if q != []], -1)\n        k_embed = torch.cat([k for k in ks if k != []], -1)\n        return q_embed, k_embed\n    \n    def apply_rotary_pos_emb_v6(self, q, k, cos, sin, layer_idx=0, unsqueeze_dim=2):\n        # retain the subspaces with higher 2-norm score and different head could have different r\n        cos = cos.unsqueeze(unsqueeze_dim)\n        sin = sin.unsqueeze(unsqueeze_dim)\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        mask = mask_tensor[layer_idx].to(q.device)\n        mask_for_k = (\n            torch.cat((mask, mask), dim=1).unsqueeze(0).unsqueeze(1).to(q.device)\n        )\n        mask_for_q = torch.repeat_interleave(\n            input=mask_for_k, repeats=cfg[\"n_gqa_group\"], dim=2\n        ).to(q.device)\n        q_embed = torch.where(mask_for_q == 1, q_embed, q)\n        k_embed = torch.where(mask_for_k == 1, k_embed, k)\n        return q_embed, k_embed\n\n    version = cfg[\"partial_rope_version\"]\n    if version == 4 or version == 6:\n        with open(cfg[\"qk_tensor_path\"], \"rb\") as fin:\n            qk_tensor = torch.load(fin).cuda()\n    if version == 6:\n        flattened_qk = qk_tensor.view(qk_tensor.size(0), -1)\n        total_dim = qk_tensor.size(1) * cfg[\"top_k_rope_dim\"]\n        _, top_indices = torch.topk(flattened_qk, total_dim, dim=1)\n        mask_tensor = torch.zeros_like(flattened_qk, dtype=torch.bool)\n        mask_tensor.scatter_(1, top_indices, 1)\n        mask_tensor = mask_tensor.view_as(qk_tensor)\n\n    versions = {\n        0: apply_rotary_pos_emb_v0,\n        1: apply_rotary_pos_emb_v1,\n        2: apply_rotary_pos_emb_v2,\n        3: apply_rotary_pos_emb_v3,\n        4: apply_rotary_pos_emb_v4,\n        5: apply_rotary_pos_emb_v5,\n        6: apply_rotary_pos_emb_v6,\n    }\n    return versions.get(version, apply_rotary_pos_emb_v0)\n\n\ndef custom_forward_with_hidden_states_for_v4(\n    self,\n    input_ids: Union[torch.Tensor, TensorPointer],  # [batch_size, seq_length]\n    input_mask: Union[torch.Tensor, TensorPointer],  # [batch_size, seq_length]\n):\n    # all tensors are optional as most ranks don't need anything from the dataloader.\n\n    output = self.token_position_embeddings(input_ids=input_ids, input_mask=input_mask)\n\n    hidden_encoder_states = {\n        \"hidden_states\": output[\"input_embeds\"],\n        \"sequence_mask\": input_mask,\n    }\n    # solve module_input_keys not match error\n    if \"layer_idx\" not in self.decoder[0].module_input_keys:\n        for layer_idx, encoder_block in enumerate(self.decoder):\n            encoder_block.module_input_keys.add(\"layer_idx\")\n    for layer_idx, encoder_block in enumerate(self.decoder):\n        hidden_encoder_states = encoder_block(\n            **hidden_encoder_states, layer_idx=layer_idx\n        )\n\n    hidden_states = self.final_layer_norm(input=hidden_encoder_states[\"hidden_states\"])[\n        \"hidden_states\"\n    ]\n\n    sharded_logits = self.lm_head(x=hidden_states)[\"logits\"]\n\n    fp32_sharded_logits = self.cast_to_fp32(x=sharded_logits)[\"output\"]\n\n    return fp32_sharded_logits, hidden_states\n\n\ndef custom_decoder_forward_for_v4(\n    self,\n    hidden_states: Union[torch.Tensor, TensorPointer],\n    sequence_mask: Union[torch.Tensor, TensorPointer],\n    layer_idx: int,\n) -> Dict[str, Union[torch.Tensor, TensorPointer]]:\n    if self.recompute_layer and not isinstance(hidden_states, TensorPointer):\n        hidden_states, sequence_mask = self._checkpointed_forward(\n            hidden_states, sequence_mask\n        )\n    else:\n        hidden_states, sequence_mask = self._core_forward(\n            hidden_states, sequence_mask, layer_idx\n        )\n\n    return {\n        \"hidden_states\": hidden_states,\n        \"sequence_mask\": sequence_mask,\n    }\n\n\ndef custom_decoder_core_forward_for_v4(\n    self,\n    hidden_states: Union[torch.Tensor, TensorPointer],\n    sequence_mask: Union[torch.Tensor, TensorPointer],\n    layer_idx: int,\n) -> List[Union[torch.Tensor, TensorPointer]]:\n    residual = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n\n    output = self.attn(\n        hidden_states=hidden_states, sequence_mask=sequence_mask, layer_idx=layer_idx\n    )\n    hidden_states = output[\"hidden_states\"]\n    hidden_states = hidden_states + residual\n\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states=hidden_states)[\"hidden_states\"]\n    hidden_states = hidden_states + residual\n\n    return hidden_states, output[\"sequence_mask\"]\n\n\ndef custom_llama_forward_for_v4(\n    self,\n    hidden_states,  # [seq_length, batch_size, hidden_size]\n    sequence_mask,  # [batch_size, seq_length]\n    layer_idx,\n):\n    from flash_attn import bert_padding\n    from flash_attn.flash_attn_interface import (\n        flash_attn_varlen_func,\n        flash_attn_with_kvcache,\n    )\n\n    qkv_states = self.qkv_proj(\n        hidden_states\n    )  # [seq_length, batch_size, n_local_q_heads * d_qk + 2 * n_local_kv_heads * d_qk]\n    q_length, batch_size, _ = qkv_states.shape\n\n    if self.is_gqa:\n        query_states, key_states, value_states = torch.split(\n            qkv_states,\n            [\n                self.n_local_q_heads * self.d_qk,\n                self.n_local_kv_heads * self.d_qk,\n                self.n_local_kv_heads * self.d_qk,\n            ],\n            dim=-1,\n        )\n\n        query_states = (\n            query_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads, self.d_qk)\n        )\n        key_states = (\n            key_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_qk)\n        )\n        value_states = (\n            value_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_qk)\n        )\n    else:\n        query_states, key_states, value_states = (\n            qkv_states.view(q_length, batch_size, 3, self.n_local_q_heads, self.d_qk)\n            .permute(2, 1, 0, 3, 4)\n            .contiguous()\n        )  # [3, batch_size, seq_length, n_local_q_heads, d_qk]\n\n    store = self.get_local_store()\n    if store is not None:  # Inference case\n        # Double check that we use store only at inference time\n        assert key_states.requires_grad is False\n        assert value_states.requires_grad is False\n        if \"position_offsets\" in store:\n            old_position_offsets = store[\"position_offsets\"]\n            position_ids = old_position_offsets[:, None] + sequence_mask\n        else:\n            position_ids = torch.cumsum(sequence_mask, dim=-1, dtype=torch.int32) - 1\n        position_offsets = position_ids[:, -1]\n\n        # Compute rotary embeddings\n        # Note: keep track of old rotary embedding end to check if we need to enlarge k_cache and v_cache\n        old_rotary_embed_end = self.rotary_embedding.end\n        # interleaved version.\n        if self.rope_interleaved:\n            query_states = self.rotary_embedding(\n                query_states, position_ids=position_ids\n            )\n            key_states = self.rotary_embedding(key_states, position_ids=position_ids)\n        # non interleaved version.\n        else:\n            cos, sin = self.rotary_embedding(value_states, position_ids)\n            query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n                query_states, key_states, cos, sin\n            )\n\n        if \"key\" not in store:\n            # First inference iteration (Prefill)\n            # TODO @nouamane: support custom masking\n            # assert that [ False, False, False, False,  True,  True,  True,  True,  True,  True] is accepted\n            # but [ False, False, False, False,  True,  True,  False,  False,  True,  True] is not (can't mask in the middle of sequence)\n            assert ~(\n                sequence_mask[:, :-1]\n                & (~sequence_mask[:, 1:])  # True is never followed by False\n            ).any(), \"Can't mask in the middle of sequence, please make sure that pads are at the left of the sequence if existing\"\n\n            # preallocate k_cache, v_cache to self.prefill_kv_len\n            k_cache = torch.zeros(\n                (\n                    batch_size,\n                    self.prefill_kv_len,\n                    self.n_local_kv_heads,\n                    self.d_qk,\n                ),\n                dtype=query_states.dtype,\n                device=query_states.device,\n            )\n            v_cache = torch.zeros(\n                (batch_size, self.prefill_kv_len, self.n_local_kv_heads, self.d_v),\n                dtype=query_states.dtype,\n                device=query_states.device,\n            )\n            # Remove pad tokens from key_states and concatenate samples in key_unpad\n            # cu_seqlens_k is the cumulative sequence lengths of key_states\n            (query_unpad, indices_q, cu_seqlens_q, max_seqlen_q) = (\n                bert_padding.unpad_input(\n                    query_states,\n                    sequence_mask,\n                )\n            )\n            (key_unpad, indices_k, cu_seqlens_k, max_seqlen_k) = (\n                bert_padding.unpad_input(key_states, sequence_mask)\n            )\n            (value_unpad, _, _, _) = bert_padding.unpad_input(\n                value_states, sequence_mask\n            )\n\n            # NOTE: this scale is for µTransfer,\n            # in SP, we use sqrt(1/d_h)\n            softmax_scale = 1 / query_states.shape[-1] if self.is_using_mup else None\n            output_unpad = flash_attn_varlen_func(\n                q=query_unpad,  # (total_q, n_local_q_heads, d_qk)\n                k=key_unpad,  # (total_kv, n_local_kv_heads, d_qk)\n                v=value_unpad,  # (total_kv, n_local_kv_heads, d_v)\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_q,\n                max_seqlen_k=max_seqlen_k,\n                dropout_p=0.0,\n                softmax_scale=softmax_scale,\n                causal=True,  # True in prefill phase, False in subsequent phases\n                return_attn_probs=False,\n            )  # (total_unpadded, n_local_q_heads, d_v)\n\n            attention_output = bert_padding.pad_input(\n                output_unpad, indices_q, batch_size, q_length\n            )  # (batch_size, q_length, n_local_q_heads, d_v)\n\n            pad_to_right(key_states, sequence_mask, new_tensor=k_cache)\n            pad_to_right(value_states, sequence_mask, new_tensor=v_cache)\n\n        else:\n            # Pull pre-computed key/value states\n            # Subsequent inference iterations (q_length=1)\n            k_cache = store[\"key\"]\n            v_cache = store[\"value\"]\n\n            # NOTE(fmom): According to flash_attn_with_kvcache, \"If you pass in k / v, you must make sure that the cache is large enough to hold the new values\"\n            # Since rotary embedding has changed (to enable larger context), we need to enlarge k_cache and v_cache\n            if self.rotary_embedding.end > old_rotary_embed_end:\n                k_cache = torch.cat(\n                    [\n                        k_cache,\n                        torch.zeros(\n                            (\n                                batch_size,\n                                self.rotary_embedding.end - old_rotary_embed_end,\n                                self.n_local_kv_heads,\n                                self.d_qk,\n                            ),\n                            dtype=query_states.dtype,\n                            device=query_states.device,\n                        ),\n                    ],\n                    dim=1,\n                )\n\n                v_cache = torch.cat(\n                    [\n                        v_cache,\n                        torch.zeros(\n                            (\n                                batch_size,\n                                self.rotary_embedding.end - old_rotary_embed_end,\n                                self.n_local_kv_heads,\n                                self.d_v,\n                            ),\n                            dtype=query_states.dtype,\n                            device=query_states.device,\n                        ),\n                    ],\n                    dim=1,\n                )\n\n            assert (\n                k_cache.shape[1] == self.rotary_embedding.end\n            ), f\"Cache size {k_cache.shape[1]} is smaller than rotary embedding end {self.rotary_embedding.end}\"\n            assert (\n                v_cache.shape[1] == self.rotary_embedding.end\n            ), f\"Cache size {v_cache.shape[1]} is smaller than rotary embedding end {self.rotary_embedding.end}\"\n\n            # [batch_size, seq_length, num_heads, d_qk]\n            query_states = query_states.view(\n                batch_size, q_length, self.n_local_q_heads, self.d_qk\n            )  # [batch_size, q_length, self.n_heads, d_qk]\n            kv_length = key_states.shape[1]\n            key_states = key_states.view(\n                batch_size, kv_length, self.n_local_kv_heads, self.d_qk\n            )  # [batch_size, kv_length, self.n_heads, d_qk]\n            value_states = value_states.view(\n                batch_size, kv_length, self.n_local_kv_heads, self.d_v\n            )  # [batch_size, kv_length, self.n_heads, d_v]\n\n            # NOTE: this scale is for µTransfer,\n            # in SP, we use sqrt(1/d_h)\n            softmax_scale = 1 / query_states.shape[-1] if self.is_using_mup else None\n            attention_output = flash_attn_with_kvcache(\n                query_states,\n                k_cache,\n                v_cache,\n                key_states,\n                value_states,\n                rotary_cos=None,\n                rotary_sin=None,\n                # TODO @nouamane: seems like this doesn't help to indicate padding in (for first iteration it's just 0)\n                cache_seqlens=position_offsets.contiguous(),\n                softmax_scale=softmax_scale,\n                causal=True,\n                rotary_interleaved=False,  # the value is not used unless rotary_cos/sin is provided. https://github.com/Dao-AILab/flash-attention\n            )\n\n        store.update(\n            {\n                \"key\": k_cache,  # flash-attn has updated with new key_states using cache_seqlens\n                \"value\": v_cache,\n                \"position_offsets\": position_offsets,\n            }\n        )\n\n    else:  # Training case\n        # Apply rotary embeddings to query/key states\n        # NOTE: The layout is different from models/llama.py which is [batch_size, num_heads, seq_length, d_qk]\n        # # Here it is, [batch_size, seq_length, num_heads, d_qk]\n        # # [2, batch_size, seq_length, num_heads, d_qk]\n        # key_value_states = torch.cat([key_states.unsqueeze(0), value_states.unsqueeze(0)], dim=0)\n        # # [batch_size, seq_length, 2, num_heads, d_qk]\n        # key_value_states = key_value_states.permute(1, 2, 0, 3, 4).contiguous()\n        # query_states, key_value_states = self.flash_rotary_embedding(query_states, kv=key_value_states)\n        # # [batch_size, seq_length, num_heads, d_qk]\n        # key_states, value_states = torch.split(key_value_states, 1, dim=2)\n        position_ids = torch.cumsum(sequence_mask, dim=-1, dtype=torch.int32) - 1\n        cos, sin = self.rotary_embedding(value_states, position_ids)\n        query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n            query_states, key_states, cos, sin, layer_idx\n        )\n        q_sequence_mask = sequence_mask\n        kv_sequence_mask = sequence_mask\n\n        kv_length = key_states.shape[1]\n        # [batch_size, seq_length, num_heads, d_qk]\n        # Shaping for use in `flash-attn` version of flash-attn: `flash_attn_unpadded_func`\n        query_states = query_states.view(\n            batch_size * q_length, self.n_local_q_heads, self.d_qk\n        )  # [batch_size * q_length, self.n_heads, d_qk]\n\n        key_states = key_states.view(\n            batch_size * kv_length, self.n_local_kv_heads, self.d_qk\n        )  # [batch_size * kv_length, self.n_heads, d_qk]\n        value_states = value_states.view(\n            batch_size * kv_length, self.n_local_kv_heads, self.d_v\n        )  # [batch_size * kv_length, self.n_heads, d_v]\n\n        attention_output = self.attention(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            q_sequence_mask=q_sequence_mask,\n            kv_sequence_mask=kv_sequence_mask,\n        )\n\n    attention_output = (\n        attention_output.contiguous()\n        .view(batch_size, q_length, self.n_local_q_heads * self.d_v)\n        .transpose(0, 1)\n    )\n    output = self.o_proj(attention_output)\n\n    return {\"hidden_states\": output, \"sequence_mask\": sequence_mask}\n\n\ndef custom_llama_forward(\n    self,\n    hidden_states,  # [seq_length, batch_size, hidden_size]\n    sequence_mask,  # [batch_size, seq_length]\n):\n    from flash_attn import bert_padding\n    from flash_attn.flash_attn_interface import (\n        flash_attn_varlen_func,\n        flash_attn_with_kvcache,\n    )\n\n    qkv_states = self.qkv_proj(\n        hidden_states\n    )  # [seq_length, batch_size, n_local_q_heads * d_qk + 2 * n_local_kv_heads * d_qk]\n    q_length, batch_size, _ = qkv_states.shape\n\n    if self.is_gqa:\n        query_states, key_states, value_states = torch.split(\n            qkv_states,\n            [\n                self.n_local_q_heads * self.d_qk,\n                self.n_local_kv_heads * self.d_qk,\n                self.n_local_kv_heads * self.d_qk,\n            ],\n            dim=-1,\n        )\n\n        query_states = (\n            query_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads, self.d_qk)\n        )\n        key_states = (\n            key_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_qk)\n        )\n        value_states = (\n            value_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_qk)\n        )\n    else:\n        query_states, key_states, value_states = (\n            qkv_states.view(q_length, batch_size, 3, self.n_local_q_heads, self.d_qk)\n            .permute(2, 1, 0, 3, 4)\n            .contiguous()\n        )  # [3, batch_size, seq_length, n_local_q_heads, d_qk]\n\n    store = self.get_local_store()\n    if store is not None:  # Inference case\n        raise NotImplementedError(\"Only training is supported with nanotron framework.\")\n    else:  # Training case\n        # Apply rotary embeddings to query/key states\n        # NOTE: The layout is different from models/llama.py which is [batch_size, num_heads, seq_length, d_qk]\n        # # Here it is, [batch_size, seq_length, num_heads, d_qk]\n        # # [2, batch_size, seq_length, num_heads, d_qk]\n        # key_value_states = torch.cat([key_states.unsqueeze(0), value_states.unsqueeze(0)], dim=0)\n        # # [batch_size, seq_length, 2, num_heads, d_qk]\n        # key_value_states = key_value_states.permute(1, 2, 0, 3, 4).contiguous()\n        # query_states, key_value_states = self.flash_rotary_embedding(query_states, kv=key_value_states)\n        # # [batch_size, seq_length, num_heads, d_qk]\n        # key_states, value_states = torch.split(key_value_states, 1, dim=2)\n        position_ids = torch.cumsum(sequence_mask, dim=-1, dtype=torch.int32) - 1\n        cos, sin = self.rotary_embedding(value_states, position_ids)\n        query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n            query_states, key_states, cos, sin\n        )\n        q_sequence_mask = sequence_mask\n        kv_sequence_mask = sequence_mask\n\n        kv_length = key_states.shape[1]\n        # [batch_size, seq_length, num_heads, d_qk]\n        # Shaping for use in `flash-attn` version of flash-attn: `flash_attn_unpadded_func`\n        query_states = query_states.view(\n            batch_size * q_length, self.n_local_q_heads, self.d_qk\n        )  # [batch_size * q_length, self.n_heads, d_qk]\n\n        key_states = key_states.view(\n            batch_size * kv_length, self.n_local_kv_heads, self.d_qk\n        )  # [batch_size * kv_length, self.n_heads, d_qk]\n        value_states = value_states.view(\n            batch_size * kv_length, self.n_local_kv_heads, self.d_v\n        )  # [batch_size * kv_length, self.n_heads, d_v]\n\n        attention_output = self.attention(\n            query_states=query_states,\n            key_states=key_states,\n            value_states=value_states,\n            q_sequence_mask=q_sequence_mask,\n            kv_sequence_mask=kv_sequence_mask,\n        )\n\n    attention_output = (\n        attention_output.contiguous()\n        .view(batch_size, q_length, self.n_local_q_heads * self.d_v)\n        .transpose(0, 1)\n    )\n    output = self.o_proj(attention_output)\n\n    return {\"hidden_states\": output, \"sequence_mask\": sequence_mask}\n\n\ndef pad_to_right(tensor, mask, new_tensor=None):\n    \"\"\"Transform a left-padded tensor into a right-padded tensor. (Useful for prefilling key/value states)\n    Args:\n        tensor: (batch_size, seqlen, d1, d2)\n        mask: (batch_size, seqlen)\n        new_tensor: (batch_size, new_tensor_seqlen, d1, d2)\n    Returns:\n        new_tensor: (batch_size, new_tensor_seqlen, d1, d2)\n        right_padded_mask: (batch_size, seqlen)\n    \"\"\"\n    # First, we need to find the number of padding for each row\n    unpad_seqlens = mask.sum(1)\n    # Then, we need to find the maximum length of the tensor\n    max_seqlen = mask.shape[1]\n    # We can then create the indices to select the padded values\n    # The indices are the same for each row\n    indices = torch.arange(max_seqlen, device=mask.device)\n    # We can then create the mask for the padded values\n    right_padded_mask = indices < unpad_seqlens[:, None]\n    # We select the useful values\n    useful_values = tensor[mask]\n    # We create the new tensor (if not provided)\n    new_tensor = torch.zeros_like(tensor) if new_tensor is None else new_tensor\n    # We fill the new tensor with the useful values\n    new_tensor[:, : right_padded_mask.shape[1], :, :][right_padded_mask] = useful_values\n    return new_tensor, right_padded_mask\n\n\n@dataclass\nclass CustomLlamaConfig(LlamaConfig):\n    RoPE: Dict = field(default_factory=dict)\n    SVD: Dict = field(default_factory=dict)\n\n\n@dataclass\nclass CustomModelArgs(nanotron.config.config.ModelArgs):\n    model_config: CustomLlamaConfig\n\n\n@dataclass\nclass CustomConfig(nanotron.config.config.Config):\n    \"\"\"Main configuration class\"\"\"\n\n    model: CustomModelArgs\n\n\nclass CustomCausalSelfAttention(nn.Module, AttachableStore):\n    def __init__(\n        self,\n        config,\n        parallel_config: Optional[ParallelismArgs],\n        tp_pg: dist.ProcessGroup,\n        layer_idx: int,\n    ):\n        from flash_attn.layers.rotary import RotaryEmbedding as FlashRotaryEmbedding\n\n        super().__init__()\n        # Tensor parallel considerations: We split tensors along head dimension\n        assert (\n            config.num_attention_heads % tp_pg.size() == 0\n        ), f\"Number of attention heads ({config.num_attention_heads}) must be divisible by TP size ({tp_pg.size()}).\"\n        assert (\n            not config.rope_interleaved\n        ), \"MLA Causal attention does not support interleaved RoPE\"\n        try:\n            assert (\n                config.num_key_value_heads % tp_pg.size() == 0\n            ), f\"Number of key/value heads ({config.num_key_value_heads}) must be divisible by TP size ({tp_pg.size()}).\"\n        except AttributeError:\n            log_rank(\n                \"WARNING: num_key_value_heads not defined, assuming it is equal to num_attention_heads\",\n                logger=logger,\n                level=logging.WARNING,\n                rank=0,\n            )\n            # If num_key_value_heads is not defined, we assume that it is equal to num_attention_heads\n            config.num_key_value_heads = config.num_attention_heads\n        assert (\n            config.num_attention_heads % config.num_key_value_heads == 0\n        ), f\"Number of attention heads ({config.num_attention_heads}) must be divisible by number of key/value heads ({config.num_key_value_heads}).\"\n        self.config = config\n        self.parallel_config = parallel_config\n        self.tp_pg = tp_pg\n        self.n_local_q_heads = config.num_attention_heads // tp_pg.size()\n        self.n_local_kv_heads = config.num_key_value_heads // tp_pg.size()\n        self.n_repeats = config.num_attention_heads // config.num_key_value_heads\n        self.is_gqa = (\n            config.num_attention_heads != config.num_key_value_heads\n        )  # Whether we are using GQA or not\n        self.d_qk = config.hidden_size // config.num_attention_heads\n        self.d_v = config.hidden_size // config.num_attention_heads\n        self.d_model = config.hidden_size\n        self.is_using_mup = config.is_using_mup\n        self.layer_idx = layer_idx\n        self.nope_mask = IndexForNope.get_index_for_nope(\n            config.RoPE,\n            head_dim=self.d_qk,\n            head_num=self.n_local_kv_heads,\n            layer_idx=self.layer_idx,\n        )\n        self.is_share_W_down = bool(config.SVD[\"method\"] not in [2, 3])\n        self.low_rank = config.SVD[\"low_rank\"]\n\n        # TODO @thomasw21: refactor so that we store that default in a single place.\n        tp_mode = (\n            parallel_config.tp_mode\n            if parallel_config is not None\n            else TensorParallelLinearMode.ALL_REDUCE\n        )\n        tp_linear_async_communication = (\n            parallel_config.tp_linear_async_communication\n            if parallel_config is not None\n            else False\n        )\n\n        # build the slice config for self.qkv for save/load\n        # shard are done within the contiguous chunk\n        self.q_proj = TensorParallelColumnLinear(\n            self.d_model,\n            config.num_attention_heads * self.d_qk,\n            pg=tp_pg,\n            mode=tp_mode,\n            bias=False,\n            async_communication=tp_linear_async_communication,\n            tp_recompute_allgather=parallel_config.tp_recompute_allgather,\n        )\n        self.W_k_r = TensorParallelColumnLinear(\n            self.d_model,\n            (self.nope_mask == False).sum().item(),\n            bias=False,\n            pg=tp_pg,\n            mode=tp_mode,\n            async_communication=tp_linear_async_communication,\n        )\n        self.W_down_k = TensorParallelColumnLinear(\n            self.d_model,\n            self.low_rank * self.n_local_kv_heads,\n            bias=False,\n            pg=tp_pg,\n            mode=tp_mode,\n            async_communication=tp_linear_async_communication,\n        )\n        if not self.is_share_W_down:\n            self.W_down_v = TensorParallelColumnLinear(\n                self.d_model,\n                self.low_rank * self.n_local_kv_heads,\n                bias=False,\n                pg=tp_pg,\n                mode=tp_mode,\n                async_communication=tp_linear_async_communication,\n            )\n        self.W_up_k = TensorParallelColumnLinear(\n            self.low_rank * self.n_local_kv_heads,\n            self.n_local_kv_heads * self.d_qk - (self.nope_mask == False).sum().item(),\n            bias=False,\n            pg=tp_pg,\n            mode=tp_mode,\n            async_communication=tp_linear_async_communication,\n        )\n        self.W_up_v = TensorParallelColumnLinear(\n            self.low_rank * self.n_local_kv_heads,\n            self.n_local_kv_heads * self.d_v,\n            bias=False,\n            pg=tp_pg,\n            mode=tp_mode,\n            async_communication=tp_linear_async_communication,\n        )\n        # TODO(kunhao): We want to have only one version per device and not one version per layer.\n        self.rotary_embedding = llama.LlamaRotaryEmbedding(\n            dim=self.d_qk,\n            end=config.max_position_embeddings,\n            theta=config.rope_theta,\n        )\n        self.rope_interleaved = config.rope_interleaved\n\n        # NOTE: Only supported for training (TODO(fmom): position_ids not supported yet)\n        self.flash_rotary_embedding = FlashRotaryEmbedding(\n            dim=self.d_qk, base=config.rope_theta, interleaved=config.rope_interleaved\n        )\n\n        self.o_proj = TensorParallelRowLinear(\n            config.num_attention_heads * self.d_qk,\n            self.d_model,\n            pg=tp_pg,\n            mode=tp_mode,\n            bias=False,\n            async_communication=tp_linear_async_communication,\n        )\n\n        self.attention = CoreAttention(\n            config,\n            parallel_config=parallel_config,\n            layer_idx=layer_idx,\n        )\n\n        self.prefill_kv_len = (\n            config.max_position_embeddings\n        )  # TODO @nouamane: compute based on free memory, because in rope we can surpass max_position_embeddings\n\n    def forward(\n        self,\n        hidden_states,  # [seq_length, batch_size, hidden_size]\n        sequence_mask,  # [batch_size, seq_length]\n    ):\n        from flash_attn import bert_padding\n        from flash_attn.flash_attn_interface import flash_attn_varlen_func\n\n        query_states = self.q_proj(\n            hidden_states\n        )  # [seq_length, batch_size, n_local_q_heads * d_qk]\n        q_length, batch_size, _ = query_states.shape\n\n        query_states = (\n            query_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads, self.d_qk)\n        )  # [batch_size, seq_length, n_local_q_heads, d_qk]\n        k_r = self.W_k_r(hidden_states)  # [seq_len, bsz, -1]\n        if self.is_share_W_down:\n            c_kv = self.W_down_k(\n                hidden_states\n            )  # [seq_length, batch_size, low_rank * n_local_kv_heads]\n            k_c = self.W_up_k(c_kv)  # [seq_len, bsz, -1]\n            value_states = self.W_up_v(c_kv)  # [seq_len, bsz, -1]\n        else:\n            c_kv = torch.cat(\n                [self.W_down_k(hidden_states), self.W_down_v(hidden_states)], dim=-1\n            )  # [seq_length, batch_size, 2 * low_rank * n_local_kv_heads]\n            c_k, c_v = c_kv.split(\n                [\n                    self.low_rank * self.n_local_kv_heads,\n                    self.low_rank * self.n_local_kv_heads,\n                ],\n                dim=-1,\n            )\n            k_c = self.W_up_k(c_k)  # [seq_len, bsz, -1]\n            value_states = self.W_up_v(c_v)  # [seq_len, bsz, -1]\n\n        key_states = torch.zeros(\n            (q_length, batch_size, self.n_local_kv_heads * self.d_qk),\n            dtype=query_states.dtype,\n            device=query_states.device,\n        )\n        key_states[..., ~self.nope_mask] = k_r\n        key_states[..., self.nope_mask] = k_c\n        key_states = (\n            key_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_qk)\n        )  # [batch_size, seq_length, n_local_kv_heads, d_qk]\n        c_kv = (\n            c_kv.transpose(0, 1).contiguous().view(batch_size, q_length, -1)\n        )  # [batch_size, seq_length, -1]\n        value_states = (\n            value_states.transpose(0, 1)\n            .contiguous()\n            .view(batch_size, q_length, self.n_local_kv_heads, self.d_v)\n        )  # [batch_size, seq_length, n_local_kv_heads, d_v]\n\n        store = self.get_local_store()  # In fact, collections.defaultdict?\n        if store is not None:  # Inference case\n            raise NotImplementedError(\"Inference case not implemented yet because there is no need for inference using nanotron framework.\") \n        else:  # Training case\n            position_ids = torch.cumsum(sequence_mask, dim=-1, dtype=torch.int32) - 1\n            cos, sin = self.rotary_embedding(value_states, position_ids)\n            dbg_key_states = key_states\n\n            if self.config.RoPE[\"partial_rope_version\"] == 4:\n                query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n                    query_states,\n                    key_states,\n                    cos,\n                    sin,\n                    layer_idx=self.layer_idx,\n                )\n            else:\n                query_states, key_states = self.rotary_embedding.apply_rotary_pos_emb(\n                    query_states, key_states, cos, sin\n                )\n            # [batch_size, seq_length, n_local_kv_heads, d_qk]\n            assert torch.allclose(\n                dbg_key_states.view(\n                    batch_size, q_length, self.n_local_kv_heads * self.d_qk\n                )[..., self.nope_mask],\n                key_states.view(\n                    batch_size, q_length, self.n_local_kv_heads * self.d_qk\n                )[..., self.nope_mask],\n            )\n\n            q_sequence_mask = sequence_mask\n            kv_sequence_mask = sequence_mask\n\n            kv_length = key_states.shape[1]\n            # [batch_size, seq_length, num_heads, d_qk]\n            # Shaping for use in `flash-attn` version of flash-attn: `flash_attn_unpadded_func`\n            query_states = query_states.view(\n                batch_size * q_length, self.n_local_q_heads, self.d_qk\n            )  # [batch_size * q_length, self.n_heads, d_qk]\n\n            key_states = key_states.view(\n                batch_size * kv_length, self.n_local_kv_heads, self.d_qk\n            )  # [batch_size * kv_length, self.n_heads, d_qk]\n            value_states = value_states.view(\n                batch_size * kv_length, self.n_local_kv_heads, self.d_v\n            )  # [batch_size * kv_length, self.n_heads, d_v]\n\n            attention_output = self.attention(\n                query_states=query_states,\n                key_states=key_states,\n                value_states=value_states,\n                q_sequence_mask=q_sequence_mask,\n                kv_sequence_mask=kv_sequence_mask,\n            )\n\n        attention_output = (\n            attention_output.contiguous()\n            .view(batch_size, q_length, self.n_local_q_heads * self.d_v)\n            .transpose(0, 1)\n        )\n        output = self.o_proj(attention_output)\n\n        return {\n            \"hidden_states\": output,\n            \"sequence_mask\": sequence_mask,\n        }\n\n\nclass IndexForNope:\n    _qk_tensor_path = None\n    _qk_tensor_cache = None\n\n    @staticmethod\n    def get_index_for_nope_v0(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        nope_mask = torch.zeros((head_dim), dtype=torch.bool)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v1(rope_cfg, **kwargs):\n        keep_dim = rope_cfg[\"top_k_rope_dim\"]\n        head_dim = kwargs[\"head_dim\"]\n        if keep_dim <= 0:\n            nope_mask = torch.ones((head_dim), dtype=torch.bool)\n        elif keep_dim >= head_dim:\n            nope_mask = torch.zeros((head_dim), dtype=torch.bool)\n        else:\n            half = head_dim // 2\n            nope_mask = torch.ones((half), dtype=torch.bool)\n            nope_mask[:keep_dim] = False\n            nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v2(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        indices_to_remove = torch.arange(\n            rope_cfg[\"uniform_start_point\"], head_dim, rope_cfg[\"uniform_step\"]\n        )\n        nope_mask = torch.ones(head_dim, dtype=torch.bool)\n        nope_mask[indices_to_remove] = False\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v3(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        top_k_dim, last_k_dim = rope_cfg[\"top_k_rope_dim\"], rope_cfg[\"last_k_rope_dim\"]\n        half = head_dim // 2\n        assert top_k_dim + last_k_dim <= half\n        nope_mask = torch.zeros((half), dtype=torch.bool)\n        nope_mask[top_k_dim : half - last_k_dim] = True\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v4(rope_cfg, **kwargs):\n        if IndexForNope._qk_tensor_cache is None or rope_cfg[\"qk_tensor_path\"] != IndexForNope._qk_tensor_path:\n            with open(rope_cfg[\"qk_tensor_path\"], \"rb\") as fin:\n                IndexForNope._qk_tensor_cache = torch.load(fin)  # [layer_num, k_head_num, head_dim//2]\n                IndexForNope._qk_tensor_path = rope_cfg[\"qk_tensor_path\"]\n                assert len(IndexForNope._qk_tensor_cache.shape) == 3\n        qk_tensor = IndexForNope._qk_tensor_cache\n        layer_idx = kwargs[\"layer_idx\"]\n        top_k_dim = rope_cfg[\"top_k_rope_dim\"]\n        topk_indices = torch.topk(qk_tensor[layer_idx], k=top_k_dim, dim=1)[1]\n        nope_mask = torch.ones_like(qk_tensor[layer_idx], dtype=torch.bool)\n        nope_mask.scatter_(1, topk_indices, False)\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=-1)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope_v5(rope_cfg, **kwargs):\n        head_dim = kwargs[\"head_dim\"]\n        last_k_rope_dim = rope_cfg[\"last_k_rope_dim\"]\n        half = head_dim // 2\n        nope_mask = torch.ones((half), dtype=torch.bool)\n        nope_mask[half-last_k_rope_dim:half] = False\n        nope_mask = torch.cat([nope_mask, nope_mask], dim=0)\n        return nope_mask\n\n    @staticmethod\n    def get_index_for_nope(rope_cfg, **kwargs):\n        logger.info(f\"rope_cfg: {rope_cfg}\")\n        version = rope_cfg[\"partial_rope_version\"]\n        versions = {\n            0: IndexForNope.get_index_for_nope_v0,\n            1: IndexForNope.get_index_for_nope_v1,\n            2: IndexForNope.get_index_for_nope_v2,\n            3: IndexForNope.get_index_for_nope_v3,\n            4: IndexForNope.get_index_for_nope_v4,\n            5: IndexForNope.get_index_for_nope_v5,\n        }\n        index_func = versions[version]\n        nope_mask = index_func(rope_cfg, **kwargs)\n        nope_mask = nope_mask.to(dtype=torch.bool)\n        if version == 4:\n            nope_mask = nope_mask.reshape(-1)\n        else:\n            nope_mask = nope_mask.repeat(repeats=(kwargs[\"head_num\"],))\n        return nope_mask\n\n\nclass SvdInit:\n    @staticmethod\n    def method_I(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down = (U_k[:, :r] + U_v[:, :r]) / 2\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_II(k, v, r=8):\n        # Separately decompose W_k_nope and W_v into truncated SVDs, allocating dimensions to each\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down_k = U_k\n        W_down_v = U_v\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down_k.t(), W_up_k.t(), W_down_v.t(), W_up_v.t()\n\n    @staticmethod\n    def method_III(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        Sigma_k_half = torch.diag(torch.sqrt(S_k))\n        Sigma_v_half = torch.diag(torch.sqrt(S_v))\n        W_down_k = U_k @ Sigma_k_half\n        W_down_v = U_v @ Sigma_v_half\n        W_up_k = Sigma_k_half @ V_k.t()\n        W_up_v = Sigma_v_half @ V_v.t()\n        return W_down_k.t(), W_up_k.t(), W_down_v.t(), W_up_v.t()\n\n    @staticmethod\n    def method_IV(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        Sigma_k_half = torch.diag(torch.sqrt(S_k))\n        Sigma_v_half = torch.diag(torch.sqrt(S_v))\n        W_down_k = U_k @ Sigma_k_half\n        W_down_v = U_v @ Sigma_v_half\n        W_down = (W_down_k + W_down_v) / 2\n        W_up_k = Sigma_k_half @ V_k.t()\n        W_up_v = Sigma_v_half @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_V(k, v, r=8):\n        U_k, S_k, V_k = torch.svd(k)\n        U_k, S_k, V_k = U_k[:, :r], S_k[:r], V_k[:, :r]\n        W_down = U_k\n        W_down_pseudo_inv = torch.linalg.pinv(W_down)\n        W_up_k = torch.diag(S_k) @ V_k.t()\n        W_up_v = torch.matmul(W_down_pseudo_inv, v)\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_VI(k, v, r=8):\n        U_v, S_v, V_v = torch.svd(v)\n        U_v, S_v, V_v = U_v[:, :r], S_v[:r], V_v[:, :r]\n        W_down = U_v\n        W_down_pseudo_inv = torch.linalg.pinv(W_down)\n        W_up_k = torch.matmul(W_down_pseudo_inv, k)\n        W_up_v = torch.diag(S_v) @ V_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def method_VII(k, v, r=8):\n        # jointly factorize the con-catenated matrix\n        U_kv, S_kv, V_kv = torch.svd(torch.cat([k, v], dim=1))\n        U_kv, S_kv, V_kv = U_kv[:, :r], S_kv[:r], V_kv[:, :r]\n        W_down = U_kv\n        split_sizes = [k.size(1), v.size(1)]\n        W_up_k, W_up_v = torch.split(V_kv, split_sizes, dim=0)\n        W_up_k = torch.diag(S_kv) @ W_up_k.t()\n        W_up_v = torch.diag(S_kv) @ W_up_v.t()\n        return W_down.t(), W_up_k.t(), None, W_up_v.t()\n\n    @staticmethod\n    def init(k, v, svd_method=1, r=8):\n        assert k.dtype == v.dtype, \"k and v must have the same dtype\"\n        logger.info(f\"Using SVD method {svd_method} with rank {r}\")\n        original_dtype = k.dtype\n        k=k.to(torch.float32)\n        v=v.to(torch.float32)\n        versions = {\n            1: SvdInit.method_I,\n            2: SvdInit.method_II,\n            3: SvdInit.method_III,\n            4: SvdInit.method_IV,\n            5: SvdInit.method_V,\n            6: SvdInit.method_VI,\n            7: SvdInit.method_VII,\n        }\n        W_down_k, W_up_k, W_down_v, W_up_v = versions[svd_method](k, v, r)\n        W_down_k = W_down_k.to(original_dtype)\n        W_up_k = W_up_k.to(original_dtype)\n        if W_down_v is not None:\n            W_down_v = W_down_v.to(original_dtype)\n        W_up_v = W_up_v.to(original_dtype)\n        return W_down_k, W_up_k, W_down_v, W_up_v\n\n\ndef custom_load_weights(\n    model: nn.Module,\n    parallel_context: ParallelContext,\n    root_folder: Path,\n    filtered_state_dict: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Load weights from a checkpoint\n\n    Args:\n        model: model to load weights into\n        parallel_context: distributed process groups\n        root_folder: root folder of the checkpoint\n        filtered_state_dict: state dict to load from (overrides model.state_dict()). if None, load from model.state_dict()\n    \"\"\"\n    if all(\n        [\"W_down_k\" not in module_name for module_name in model.state_dict()]\n    ) or any(\n        [\n            \"W_down_k\" in str(file_name.absolute())\n            for file_name in (root_folder / \"model\").glob(\"**/*\")\n        ]\n    ):\n        return nt_weights.original_load_weights(\n            model, parallel_context, root_folder, filtered_state_dict\n        )\n    param_root_folder = root_folder / \"model\"\n\n    module_id_to_prefix = {\n        id(module): f\"{module_name}.\" for module_name, module in model.named_modules()\n    }\n    # Fix the root_model\n    module_id_to_prefix[id(model)] = \"\"\n\n    checkpoint_version: Optional[Version] = None\n\n    filtered_state_dict = (\n        filtered_state_dict if filtered_state_dict is not None else model.state_dict()\n    )\n    param_shard_metadata = {}\n    for name, param_or_buffer in tqdm(\n        filtered_state_dict.items(),\n        disable=dist.get_rank(parallel_context.world_pg) != 0,\n        desc=\"Loading weights\",\n    ):\n        if any(\n            [\n                mla_weight in name\n                for mla_weight in [\n                    \"W_down_k\",\n                    \"W_down_v\",\n                    \"W_up_k\",\n                    \"W_up_v\",\n                    \"W_k_r\",\n                    \"q_proj\",\n                ]\n            ]\n        ):\n            continue\n        # NOTE: extract how does the current model parameter are sharded\n        # so that we can load optimizer checkpoints in this way\n        param_shard_metadata[name] = {}\n        # `state_dict` doesn't return a Param or a buffer, just a tensors which loses some metadata\n        try:\n            param = model.get_parameter(name)\n        except AttributeError:\n            param = None\n\n        if isinstance(param, NanotronParameter):\n            if param.is_tied:\n                tied_info = param.get_tied_info()\n                base_name = tied_info.get_full_name_from_module_id_to_prefix(\n                    module_id_to_prefix=module_id_to_prefix\n                )\n            else:\n                base_name = name\n\n            if param.is_sharded:\n                sharded_info = param.get_sharded_info()\n\n                if param.is_tied:\n                    # When params are tied only the first rank of tied param group stores weights (see save_weights)\n                    group = parallel_context.world_ranks_to_pg[tied_info.global_ranks]\n                    group_rank = 0\n                else:\n                    group = parallel_context.world_ranks_to_pg[\n                        sharded_info.global_ranks\n                    ]\n                    group_rank = dist.get_rank(group)\n\n                exp_tp_pp_rank_and_size = get_exp_tp_pp_rank_and_size_from(\n                    world_rank=get_global_rank(group=group, group_rank=group_rank),\n                    parallel_context=parallel_context,\n                )\n                # TODO @nouamane: do we consider exp_size=1 expert_sharded?\n                is_expert_sharded = sharded_info.is_expert_sharded(parallel_context)\n            else:\n                exp_tp_pp_rank_and_size = None\n                is_expert_sharded = False\n\n            path = get_path(\n                base_name,\n                type=ObjectType.MODEL,\n                exp_tp_pp_rank_and_size=exp_tp_pp_rank_and_size,\n                prefix=param_root_folder,\n                is_expert_sharded=is_expert_sharded,\n            )\n\n            if path.exists():\n                with safe_open(path, framework=\"pt\", device=str(param.device)) as fi:\n                    # TODO @thomasw21: Choose only a slice if we switch the TP topology\n                    param_or_buffer[:] = fi.get_tensor(\"data\")\n\n            elif not path.parent.exists():\n                raise ValueError(\n                    f\"Checkpoint is empty or checkpoint structure is not matching the model architecture.\"\n                    f\"Couldn't find folder {path.parent} in checkpoint at {root_folder}\"\n                )\n            else:\n                # Let's assume that the topology changed and the param is sharded.\n                # We search for all the files from the shards, concatenate the \"unsharded\" tensor\n                # and load the specific shard we're interested in.\n                if not param.is_sharded:\n                    raise ValueError(\n                        f\"`{name}` is not a sharded parameter. It's possible you were expecting {path} to exist.\"\n                    )\n                # TODO @thomasw21: Make so that we don't need to code this logic somewhere else than in `get_path`\n                sharded_info = param.get_sharded_info()\n                suffix = base_name.rsplit(\".\", 1)[-1]\n                shards_path = list(\n                    path.parent.glob(f\"{ObjectType.MODEL.value}_{suffix}*.safetensors\")\n                )\n                if len(shards_path) <= 0:\n                    raise ValueError(\n                        f\"Could not find any shards {ObjectType.MODEL.value}_{suffix}*.safetensors in {path.parent}.\"\n                        f\"If you notice `.safetensors` in the middle of the name of some of the checkpoints files. You need to run `scripts/fix_checkpoint_bad_naming.py`.\"\n                    )\n\n                if checkpoint_version is None:\n                    checkpoint_version = get_checkpoint_version(\n                        parallel_context, root_folder, param_save_path=shards_path[0]\n                    )\n                else:\n                    current_checkpoint_version = None\n                    try:\n                        current_checkpoint_version = (\n                            read_checkpoint_version_from_shard_file(\n                                param_save_path=shards_path[0]\n                            )\n                        )\n                    except CheckpointVersionFromShardFileException:\n                        # The checkpoint version is read from the meta file\n                        current_checkpoint_version = checkpoint_version\n                    finally:\n                        assert (\n                            current_checkpoint_version == checkpoint_version\n                        ), f\"Checkpoint version mismatch at {shards_path[0]}.\"\n\n                if checkpoint_version <= CHECKPOINT_VERSION:\n                    load_sharded_param_latest(\n                        param_or_buffer=param_or_buffer,\n                        sharded_info=sharded_info,\n                        shards_path=shards_path,\n                        param_shard_metadata=param_shard_metadata[name],\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported checkpoint version {checkpoint_version}\"\n                    )\n\n        else:\n            raise NotImplementedError(\n                f\"Parameters {param} should be a NanotronParameter\"\n            )\n\n    for layer_idx in tqdm(range(len(model.model.decoder)), desc=\"Loading MLA weights\"):\n        base_name = f\"model.decoder.{layer_idx}.pp_block.attn.qkv_proj.weight\"\n        exp_tp_pp_rank_and_size = get_exp_tp_pp_rank_and_size_from(\n            world_rank=get_global_rank(group=group, group_rank=group_rank),\n            parallel_context=parallel_context,\n        )\n        path = get_path(\n            base_name,\n            type=ObjectType.MODEL,\n            exp_tp_pp_rank_and_size=exp_tp_pp_rank_and_size,\n            prefix=param_root_folder,\n            is_expert_sharded=True,\n        )\n        with safe_open(path, framework=\"pt\", device=str(param.device)) as fi:\n            qkv_proj = fi.get_tensor(\"data\").t()\n        attn_module_prefix = f\"model.decoder.{layer_idx}.pp_block.attn\"\n        attn_module = model.model.decoder[layer_idx].pp_block.attn\n        nope_mask = attn_module.nope_mask\n        q_proj, k_proj, v_proj = qkv_proj.split(\n            [\n                attn_module.n_local_q_heads * attn_module.d_qk,\n                attn_module.n_local_kv_heads * attn_module.d_qk,\n                attn_module.n_local_kv_heads * attn_module.d_v,\n            ],\n            dim=-1,\n        )\n        filtered_state_dict[f\"{attn_module_prefix}.q_proj.weight\"][:] = q_proj.t()\n        filtered_state_dict[f\"{attn_module_prefix}.W_k_r.weight\"][:] = k_proj[\n            ..., ~nope_mask\n        ].t()\n        W_down_k, W_up_k, W_down_v, W_up_v = SvdInit.init(\n            k_proj[..., nope_mask],\n            v_proj,\n            svd_method=model.config.SVD[\"method\"],\n            r=model.config.SVD[\"low_rank\"] * model.config.num_key_value_heads,\n        )\n        filtered_state_dict[f\"{attn_module_prefix}.W_down_k.weight\"][:] = W_down_k\n        filtered_state_dict[f\"{attn_module_prefix}.W_up_k.weight\"][:] = W_up_k\n        if not attn_module.is_share_W_down:\n            filtered_state_dict[f\"{attn_module_prefix}.W_down_v.weight\"][:] = W_down_v\n        filtered_state_dict[f\"{attn_module_prefix}.W_up_v.weight\"][:] = W_up_v\n\n    return param_shard_metadata\n\n\ndef partial_rope_monkey_patch(rope_cfg):\n    llama.LlamaRotaryEmbedding.apply_rotary_pos_emb = (\n        create_custom_apply_rotary_pos_emb(rope_cfg)\n    )\n    if \"CustomLlamaConfig\" in nanotron.trainer.CONFIG_TO_MODEL_CLASS:\n        return\n    nanotron.config.models_config.LlamaConfig = CustomLlamaConfig\n    nanotron.trainer.CONFIG_TO_MODEL_CLASS.update(\n        {\"CustomLlamaConfig\": nanotron.trainer.CONFIG_TO_MODEL_CLASS[\"LlamaConfig\"]}\n    )\n    if rope_cfg[\"partial_rope_version\"]==4:\n        from nanotron.models.llama import LlamaModel, LlamaDecoderLayer, CausalSelfAttention\n\n        LlamaModel.forward_with_hidden_states = custom_forward_with_hidden_states_for_v4\n        LlamaDecoderLayer.forward = custom_decoder_forward_for_v4\n        LlamaDecoderLayer._core_forward = custom_decoder_core_forward_for_v4\n        CausalSelfAttention.forward = custom_llama_forward_for_v4\n\n\ndef mla_monkey_patch(rope_cfg=None):\n    llama.CausalSelfAttention = CustomCausalSelfAttention\n    if not hasattr(nt_weights, \"original_load_weights\"):\n        nt_weights.original_load_weights = nt_weights.load_weights\n        nt_weights.load_weights = custom_load_weights\n    nanotron.config.models_config.LlamaConfig = CustomLlamaConfig\n    nanotron.trainer.CONFIG_TO_MODEL_CLASS.update(\n        {\"CustomLlamaConfig\": nanotron.trainer.CONFIG_TO_MODEL_CLASS[\"LlamaConfig\"]}\n    )\n    if rope_cfg[\"partial_rope_version\"]==4:\n        IndexForNope._qk_tensor_cache=torch.load(rope_cfg[\"qk_tensor_path\"])\n        IndexForNope._qk_tensor_path=rope_cfg[\"qk_tensor_path\"]\n\n    def custom_load_model_checkpoint(self, model: NanotronModel) -> NanotronModel:\n        unwrapped_model = (\n            model.module if isinstance(model, DistributedDataParallel) else model\n        )\n\n        # Load or initialize model weights\n        reloaded_from_checkpoint = False\n        if self.init_checkpoint_path is not None:\n            # Load from a pre existing checkpoint\n            if check_path_is_local(self.init_checkpoint_path):\n                # Reload from a training checkpoint\n                log_rank(\n                    f\"Loading weights from {self.init_checkpoint_path}\",\n                    logger=logger,\n                    level=logging.INFO,\n                    rank=0,\n                )\n                self.param_shard_metadata = nt_weights.load_weights(\n                    model=unwrapped_model,\n                    parallel_context=self.parallel_context,\n                    root_folder=self.init_checkpoint_path,\n                )\n            reloaded_from_checkpoint = True\n        if not reloaded_from_checkpoint:\n            log_rank(\n                \"No checkpoint path provided.\",\n                logger=logger,\n                level=logging.INFO,\n                rank=0,\n            )\n            if isinstance(self.config.model.init_method, ExistingCheckpointInit):\n                # Initialize model from an pretrained model checkpoint (without optimizer, lr_scheduler...)\n                self.param_shard_metadata = nt_weights.load_weights(\n                    model=unwrapped_model,\n                    parallel_context=self.parallel_context,\n                    root_folder=self.config.model.init_method.path,\n                )\n            elif isinstance(\n                self.config.model.init_method, (RandomInit, SpectralMupInit)\n            ):\n                unwrapped_model.init_model_randomly(config=self.config)\n\n                # Synchronize parameters so that the model is consistent\n                # sync all params across dp\n                for _, param in sorted(model.named_parameters(), key=lambda x: x[0]):\n                    dist.all_reduce(\n                        param, op=dist.ReduceOp.AVG, group=self.parallel_context.dp_pg\n                    )\n\n                # sync tied params across tied groups\n                for (_, group_ranks), param in sorted(\n                    get_tied_id_to_param(\n                        parameters=model.parameters(),\n                        root_module=unwrapped_model,\n                    ).items(),\n                    key=lambda x: x[0],\n                ):\n                    group = self.parallel_context.world_ranks_to_pg[group_ranks]\n                    dist.all_reduce(param, op=dist.ReduceOp.AVG, group=group)\n            else:\n                raise ValueError(f\"Unsupported {self.config.model.init_method}\")\n        return model\n\n    nanotron.trainer.DistributedTrainer._load_model_checkpoint = (\n        custom_load_model_checkpoint\n    )\n\n    if rope_cfg is not None:\n        partial_rope_monkey_patch(rope_cfg)\n"}
{"type": "source_file", "path": "utils/compare.py", "content": "import torch\nimport pickle\n\n# with open(\"qk_tensor_135M.pkl\", \"rb\") as f:\n#     a = pickle.load(f)\n# a=a.view(30,3,3,32).sum(dim=2)\n# with open(\"../utils/qk_tensor_135M.pth\", \"rb\") as fin:\n#     b = torch.load(fin)\n# print(torch.allclose(a,b))\n\nwith open(\"../utils/qk_tensor_360M.pth\", \"rb\") as fin:\n    b = torch.load(fin)\nprint(b.shape)\n"}
{"type": "source_file", "path": "src/mha2mla/lr_scheduler.py", "content": "import math\nimport logging\nfrom nanotron.logging import log_rank\nfrom functools import partial\nfrom torch.optim import Optimizer\nfrom types import SimpleNamespace\nfrom torch.optim.lr_scheduler import LambdaLR\n\nlogger = logging.getLogger(__name__)\n\n\ndef lr_scheduler_builder(\n    optimizer: Optimizer, lr_scheduler_args, total_training_steps: int\n):\n    if lr_scheduler_args.lr_decay_steps is None:\n        lr_decay_steps = total_training_steps\n        if lr_scheduler_args.lr_warmup_steps is not None:\n            lr_decay_steps -= lr_scheduler_args.lr_warmup_steps\n        if lr_scheduler_args.lr_decay_starting_step is not None:\n            lr_decay_steps -= lr_scheduler_args.lr_decay_starting_step\n    else:\n        lr_decay_steps = lr_scheduler_args.lr_decay_steps\n\n    if lr_scheduler_args.lr_decay_starting_step is None:\n        if lr_scheduler_args.lr_warmup_steps is not None:\n            lr_decay_starting_step = lr_scheduler_args.lr_warmup_steps\n        else:\n            lr_decay_starting_step = 0\n    else:\n        lr_decay_starting_step = lr_scheduler_args.lr_decay_starting_step\n\n    def lr_lambda(current_step: int, initial_lr: float):\n        \"\"\"\n        current_step: current training step\n        initial_lr: the learning rate of a parameter group\n\n        More info on initial_lr:\n        And in standard parameterization, lr_lambda only takes a single learning rate.\n        But in µTransfer, each parameter has a custom learning rate (custom_lr = lr_scheduler_args.learning_rate * scaling_factor),\n        so each parameter group has a custom lr_lambda function.\n\n        LR Scheduling function, it has from 2 up to 4 phases:\n        - warmup,\n        - optional: constant (if lr_decay_starting_step is set)\n        - decay\n        - optional: constant (if lr_decay_steps and/or lr_decay_starting_step are set)\n        Warmup starts at lr=0 and ends at `lr=lr`\n        Then it stays constant at lr if lr_decay_starting_step is set and larger than lr_warmup_steps\n        Then it decays until `min_decay_lr` for lr_decay_steps if set, else: (total_training_steps - lr_warmup_steps or lr_decay_starting_step)\n        Then it stays constant at min_decay_lr if lr_decay_starting_step is set and total_training_steps is larger)\n        \"\"\"\n        # No warmup or decay\n        if lr_scheduler_args.lr_warmup_steps == 0 and lr_decay_steps == 0:\n            return initial_lr\n\n        # Warmup phase\n        elif (\n            lr_scheduler_args.lr_warmup_style is not None\n            and current_step <= lr_scheduler_args.lr_warmup_steps\n        ):\n            if lr_scheduler_args.lr_warmup_style == \"linear\":\n                lmbda = (\n                    initial_lr\n                    * current_step\n                    / max(lr_scheduler_args.lr_warmup_steps, 1)\n                )\n            elif lr_scheduler_args.lr_warmup_style == \"constant\":\n                lmbda = lr_scheduler_args.learning_rate\n            else:\n                raise ValueError(\n                    f\"Unknown warmup style {lr_scheduler_args.lr_warmup_style}\"\n                )\n\n        # Optional constant phase at learning_rate\n        elif current_step < lr_decay_starting_step:\n            lmbda = initial_lr\n\n        # Decay phase\n        elif (\n            lr_scheduler_args.lr_decay_style is not None\n            and current_step < lr_decay_starting_step + lr_decay_steps\n        ):\n            if lr_scheduler_args.lr_decay_style == \"cosine\":\n                lmbda = (\n                    lr_scheduler_args.min_decay_lr\n                    + (initial_lr - lr_scheduler_args.min_decay_lr)\n                    * (\n                        1\n                        + math.cos(\n                            math.pi\n                            * (current_step - lr_decay_starting_step)\n                            / lr_decay_steps\n                        )\n                    )\n                    / 2\n                )\n            elif lr_scheduler_args.lr_decay_style == \"linear\":\n                lmbda = (\n                    lr_scheduler_args.min_decay_lr\n                    + (initial_lr - lr_scheduler_args.min_decay_lr)\n                    * (lr_decay_steps - (current_step - lr_decay_starting_step))\n                    / lr_decay_steps\n                )\n            elif lr_scheduler_args.lr_decay_style == \"1-sqrt\":\n                lmbda = lr_scheduler_args.min_decay_lr + (\n                    initial_lr - lr_scheduler_args.min_decay_lr\n                ) * (\n                    1\n                    - math.sqrt(\n                        (current_step - lr_decay_starting_step) / lr_decay_steps\n                    )\n                )\n            else:\n                raise ValueError(\n                    f\"Unknown decay style {lr_scheduler_args.lr_decay_style}\"\n                )\n\n        # Optional constant phase at min_decay_lr\n        else:\n            lmbda = lr_scheduler_args.min_decay_lr\n\n        lmbda /= initial_lr  # Normalization for pytorch\n        return lmbda\n\n    def get_lr_lambda_for_param_group(lr: float):\n        return partial(lr_lambda, initial_lr=lr)\n\n    # NOTE: get learning rate scheduler for each param group\n    lr_lambdas = []\n    for param_group in optimizer.param_groups:\n        lr_lambdas.append(get_lr_lambda_for_param_group(lr=param_group[\"lr\"]))\n\n    assert len(lr_lambdas) == len(\n        optimizer.param_groups\n    ), \"Custom learning rate functions dont match the number of param groups\"\n\n    log_rank(\n        f\"[Optimizer Building] There are total {len(lr_lambdas)} custom learning rate function for parameter groups\",\n        logger=logger,\n        level=logging.DEBUG,\n    )\n\n    lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambdas)\n    return lr_scheduler\n\n\ndef load_scheduler(optimizer, training_args):\n    \"\"\"Load learning rate scheduler from configuration.\"\"\"\n    # adamW\n    import json\n    # lr_scheduler\n    lr_scheduler_kwargs = training_args.lr_scheduler_kwargs\n    if isinstance(lr_scheduler_kwargs, str):\n        lr_scheduler_kwargs = json.loads(lr_scheduler_kwargs)\n    lr_scheduler = lr_scheduler_builder(\n        optimizer=optimizer,\n        lr_scheduler_args=SimpleNamespace(**lr_scheduler_kwargs),\n        total_training_steps=training_args.max_steps,\n    )\n    return lr_scheduler\n"}
{"type": "source_file", "path": "src/mha2mla/run_train.py", "content": "from dataclasses import dataclass\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom transformers import LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader\nimport datasets\nfrom transformers.utils import is_datasets_available\nfrom transformers.trainer_utils import seed_worker\nfrom transformers import HfArgumentParser,DataCollatorForLanguageModeling\nfrom nanotron.data.nanoset import Nanoset\nimport os\nfrom typing import Dict, List\nimport numpy as np\nimport yaml\n\nfrom lr_scheduler import load_scheduler as load_scheduler4constant_with_warmup_decay\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = None\n    tokenizer_name_or_path: str = None\n    save_initial_model: bool = False\n    use_constant_with_warmup_decay_scheduler: bool = False\n\n@dataclass\nclass DataArguments:\n    is_nanoset: bool = False\n    dataset_folders: List[str] = None\n    dataset_weights: List[float] = None\n    dataset_name_or_path: str = None\n    sequence_length: int = 2048\n\nclass CustomNanoset(Nanoset):\n    def __getitem__(self, idx: int) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Returns sequence_length + 1 tokens from the memmap dataset\n\n        Args:\n            idx (int): The index into the dataset\n\n        Returns:\n            Dict[str, torch.LongTensor]: The input ids wrapped in a dictionary\n        \"\"\"\n        item = super().__getitem__(idx)\n        return item\n\ndef load_dataset(dataset_args, training_args, tokenizer):\n    \"\"\"Load dataset from configuration.\"\"\"\n    tokenizer.model_max_length = dataset_args.sequence_length\n    if dataset_args.is_nanoset:\n        dataset_folders = dataset_args.dataset_folders\n        dataset_weights = dataset_args.dataset_weights\n        sequence_length = dataset_args.sequence_length\n        world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n        token_size = 4 if len(tokenizer) > np.iinfo(np.uint16).max + 1 else 2\n        global_batch_size = (\n            training_args.per_device_train_batch_size\n            * world_size\n            * training_args.gradient_accumulation_steps\n        )\n        dataset = CustomNanoset(\n            dataset_folders=dataset_folders,\n            sequence_length=sequence_length,\n            dataset_weights=dataset_weights,\n            token_size=token_size,\n            train_split_num_samples=global_batch_size * training_args.max_steps,\n        )\n    else:\n        import datasets\n        dataset = datasets.load_dataset(\n            dataset_args.dataset_name_or_path, split=\"train\"\n        )\n\n    return dataset\n\ndef load_config(config_path):\n    \"\"\"Load configuration from a YAML file.\"\"\"\n    with open(config_path, \"r\") as file:\n        return yaml.safe_load(file)\n\n\ndef load_tokenizer_and_model(model_args: ModelArguments,is_mla:bool=False,mla_kwargs:Dict=None):\n    \"\"\"Load tokenizer and model from configuration.\"\"\"\n    assert (\n        model_args.model_name_or_path is not None\n    ), \"Must provide the path to the model\"\n    if model_args.tokenizer_name_or_path is None:\n        model_args.tokenizer_name_or_path = model_args.model_name_or_path\n    config = LlamaConfig.from_pretrained(model_args.model_name_or_path)\n    if is_mla:\n        cfg_RoPE = mla_kwargs.get(\"RoPE\")\n        cfg_SVD = mla_kwargs.get(\"SVD\")\n        config.RoPE = cfg_RoPE\n        config.SVD = cfg_SVD\n    model = LlamaForCausalLM.from_pretrained(model_args.model_name_or_path,config=config)\n    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n\ndef load_optimizer_scheduler(model, training_args, model_args):\n    \"\"\"Load optimizer and scheduler from configuration.\"\"\"\n    optimizer_name = training_args.optim\n    if \"adam\" in optimizer_name:\n        optimizer = torch.optim.AdamW(\n            params=model.parameters(),\n            lr=training_args.learning_rate,\n            betas=(\n                training_args.adam_beta1,\n                training_args.adam_beta2,\n            ),\n            eps=training_args.adam_epsilon,\n            weight_decay=training_args.weight_decay,\n            fused=bool(training_args.optim==\"adamw_torch_fused\"),\n        )\n    else:\n        raise ValueError(\n            f\"Unknown optimizer factory {optimizer_name}\"\n        )\n    if model_args.use_constant_with_warmup_decay_scheduler:\n        lr_scheduler = load_scheduler4constant_with_warmup_decay(\n            optimizer, training_args\n        )\n    else:\n        from transformers import get_scheduler\n        lr_scheduler = get_scheduler(\n            training_args.lr_scheduler_type,\n            optimizer=optimizer,\n            num_warmup_steps=training_args.warmup_steps,\n            num_training_steps=training_args.max_steps,\n        )\n    return optimizer, lr_scheduler\n\ndef main():\n    import argparse\n\n    cmd_parser = argparse.ArgumentParser()\n    cmd_parser.add_argument(\n        \"--config_file\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML configuration file.\",\n    )\n    cmd_parser.add_argument(\n        \"--partial_rope_config\",\n        type=str,\n        required=True,\n        help=\"Path to the YAML configuration file for RoPE.\",\n    )\n    cmd_parser.add_argument(\n        \"--svd_config\",\n        type=str,\n        required=False,\n        default=None,\n        help=\"Path to the YAML configuration file for SVD init.\",\n    )\n    args = cmd_parser.parse_args()\n    is_mla = args.svd_config is not None\n    config = load_config(args.config_file)\n    cfg_RoPE = load_config(args.partial_rope_config)\n    parser = HfArgumentParser((TrainingArguments, ModelArguments,DataArguments))\n    training_args, model_args, dataset_args = parser.parse_dict(config)\n\n    # Monkey Pacth\n    if is_mla:\n        from monkey_patch import mla_monkey_patch\n        mla_monkey_patch(cfg_RoPE)\n    else:\n        from monkey_patch import partial_rope_monkey_patch\n        partial_rope_monkey_patch(cfg_RoPE)\n\n    # Trainer\n    if is_mla:\n        cfg_SVD = load_config(args.svd_config)\n    else:\n        cfg_SVD = None\n    model, tokenizer = load_tokenizer_and_model(model_args,is_mla=is_mla,mla_kwargs={\"RoPE\":cfg_RoPE,\"SVD\":cfg_SVD})\n    if training_args.bf16:\n        model = model.to(dtype=torch.bfloat16)\n    elif training_args.fp16:\n        model = model.to(dtype=torch.float16)\n\n    train_dataset = load_dataset(dataset_args, training_args , tokenizer)\n    resume_from_checkpoint = training_args.resume_from_checkpoint\n    optimizer, lr_scheduler = load_optimizer_scheduler(model, training_args, model_args)\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        return_tensors=\"pt\",\n    )\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        optimizers=(optimizer, lr_scheduler),\n        data_collator=data_collator,\n    )\n    # train\n    if resume_from_checkpoint is not None:\n        trainer.train(resume_from_checkpoint)\n    else:\n        if int(os.getenv(\"LOCAL_RANK\", 0)) == 0 and model_args.save_initial_model:\n            trainer._save_checkpoint()\n        trainer.train()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
