{"repo_info": {"repo_name": "FinancialAdvisorGPT", "repo_owner": "mburaksayici", "repo_url": "https://github.com/mburaksayici/FinancialAdvisorGPT"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_mock_authen.py", "content": "from fastapi.testclient import TestClient\nfrom httpx import AsyncClient\nimport pytest\n\nfrom app import app, token_listener\nfrom tests.conftest import mock_no_authentication\n\n\nclass TestMockAuthentication:\n    @classmethod\n    def setup_class(cls):\n        mock_no_authentication()\n\n    @pytest.mark.anyio\n    async def test_api_processed_jobs(self, client_test: AsyncClient):\n        response = await client_test.get(\"student\")\n\n        assert response.status_code == 200\n"}
{"type": "test_file", "path": "tests/test_mock_database.py", "content": "import asyncio\nfrom beanie import init_beanie\nfrom fastapi.testclient import TestClient\nfrom httpx import AsyncClient\nimport pytest\n\nfrom models.admin import Admin\nfrom models.student import Student\nfrom tests.conftest import mock_no_authentication\n\n\nclass TestMockAuthentication:\n    @classmethod\n    def setup_class(cls):\n        mock_no_authentication()\n\n    @pytest.mark.anyio\n    async def test_mock_databases(self, client_test: AsyncClient):\n        # generate data\n        await Admin(\n            fullname=\"admin\", email=\"admin@admin.com\", password=\"admin\"\n        ).create()\n\n        await Student(\n            fullname=\"student\",\n            email=\"student@student.com\",\n            course_of_study=\"computer science\",\n            year=2021,\n            gpa=4.0,\n        ).create()\n\n        response = await client_test.get(\"student\")\n\n        assert response.status_code == 200\n\n    @pytest.mark.anyio\n    async def test_mock_database(self, client_test: AsyncClient):\n        await Admin(\n            fullname=\"admin\", email=\"admin@admin.com\", password=\"admin\"\n        ).create()\n\n        await Student(\n            fullname=\"student\",\n            email=\"student@student.com\",\n            course_of_study=\"computer science\",\n            year=2021,\n            gpa=4.0,\n        ).create()\n\n        response = await client_test.get(\"student\")\n\n        assert response.status_code == 200\n"}
{"type": "test_file", "path": "tests/conftest.py", "content": "\"\"\"Tests fixtures.\"\"\"\nfrom beanie import init_beanie\nimport pytest\nfrom asgi_lifespan import LifespanManager\nfrom httpx import AsyncClient\n\nfrom app import app\nfrom mongomock_motor import AsyncMongoMockClient\nfrom config.config import initiate_database\n\nimport models as models\nfrom app import app, token_listener\n\n\nasync def mock_database():\n    client = AsyncMongoMockClient()\n    await init_beanie(\n        database=client[\"database_name\"],\n        recreate_views=True,\n        document_models=models.__all__,\n    )\n\n\ndef mock_no_authentication():\n    app.dependency_overrides[token_listener] = lambda: {}\n\n\n@pytest.fixture\nasync def client_test(mocker):\n    \"\"\"\n    Create an instance of the client.\n    :return: yield HTTP client.\n    \"\"\"\n\n    mocker.patch(\"config.config.initiate_database\", return_value=await mock_database())\n\n    async with LifespanManager(app):\n        async with AsyncClient(\n            app=app, base_url=\"http://test\", follow_redirects=True\n        ) as ac:\n            yield ac\n\n\n@pytest.fixture\ndef anyio_backend():\n    return \"asyncio\"\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "app.py", "content": "from fastapi import Depends, FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\n\nfrom apis.v1.routers.admin import router as AdminRouter\nfrom auth.jwt_bearer import JWTBearer\nfrom database.mongo.client import connect_mongodb\n\napp = FastAPI()\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Adjust this to the origin of your React application\n    allow_credentials=True,\n    allow_methods=[\n        \"GET\",\n        \"POST\",\n        \"PUT\",\n        \"DELETE\",\n    ],  # Adjust these based on your allowed methods\n    allow_headers=[\"*\"],  # Adjust this to the headers your React application sends\n)\n\ntoken_listener = JWTBearer()\n\n\n@app.on_event(\"startup\")\nasync def start_database():\n    connect_mongodb()\n\n\n@app.get(\"/\", tags=[\"Root\"])\nasync def read_root():\n    return {\"message\": \"Welcome to this fantastic app.\"}\n\n\n# TO DO : Separate that.\napp.include_router(AdminRouter, tags=[\"Administrator\"], prefix=\"/admin\")\n"}
{"type": "source_file", "path": "apis/v1/routers/admin.py", "content": "from typing import Optional, Annotated, Union\n\n\nfrom fastapi import APIRouter, Body, File, HTTPException, UploadFile, Header\nfrom fastapi.responses import StreamingResponse\nfrom passlib.context import CryptContext\n\nfrom auth.jwt_handler import sign_jwt\nfrom core.engine.data_pipelines.news.template import NewsDataChain\nfrom core.engine.data_pipelines.stocks.template import StockDataChain\nfrom core.engine.data_pipelines.db_retrieval.template import DBRetrievalChain\n\n# from core.engine.model_driver import ModelDriver\nfrom core.engine.online_model_driver import OnlineModelDriver\nfrom models.users.users import Users\nfrom schemas.admin import AdminData, AdminSignIn, ChatRequest\n\nmodel_driver = OnlineModelDriver()\nmodel_driver.set_pdf_loader(\"PyPDFLoader\")\nmodel_driver.load_model(\"mistral-api\")\nmodel_driver.set_data_chains(\n    [DBRetrievalChain, NewsDataChain, StockDataChain]\n)  # NewsDataChain StockDataChain\nmodel_driver.load_assets()\nrouter = APIRouter()\n\nhash_helper = CryptContext(schemes=[\"bcrypt\"])\n\n\"\"\"\n@router.post(\"/login\")\nasync def admin_login(admin_credentials: AdminSignIn = Body(...)):\n    admin_exists = await Admin.find_one(Admin.email == admin_credentials.username)\n    if admin_exists:\n        password = hash_helper.verify(admin_credentials.password, admin_exists.password)\n        if password:\n            return sign_jwt(admin_credentials.username)\n\n        raise HTTPException(status_code=403, detail=\"Incorrect email or password\")\n\n    raise HTTPException(status_code=403, detail=\"Incorrect email or password\")\n\"\"\"\n\n\n@router.post(\n    \"/add_user\",\n)\ndef add_user(user_name, user_password):\n    users_doc = Users(user_name=user_name, user_password=user_password)\n    users_doc.save()\n    return user_name\n\n\n\"\"\"\n@router.get(\"/chat/\")\nasync def chat(pdf_link: str, query: str):\n    print(\"CHATTING\")\n    model_driver.load_document(pdf_link)\n\n    async def generate():\n        async for chunk in model_driver.chat(query):\n            yield chunk\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n\n    # return StreamingResponse(model_driver.chat(query), media_type='text/event-stream')\n\n\"\"\"\n\n\n@router.get(\"/chat/\")\ndef answer(\n    query: str,\n    pdf_link: str = None,\n):\n    print(\"CHATTING\")\n    if pdf_link:\n        model_driver.load_document(pdf_link)\n\n    return {\"answer\": model_driver.answer(query)}\n\n\n@router.get(\"/initialise_conversation/\")\ndef initialise_conversation(\n    user_id: Annotated[Union[str, None], Header()] = \"admin\",\n):\n    return {\"conversation_id\": model_driver.initialise_conversation(user_id=user_id)}\n\n\n@router.get(\"/conversation/\")\ndef conversation(query: str, conversation_id: str, user_id: str):\n    print(\"conversation...\")  #  TO DO : use python logging later.\n    return model_driver.conversation(\n        query=query, user_id=user_id, conversation_id=conversation_id\n    )\n\n\n@router.post(\"/upload_pdf/\")\ndef upload_pdf(pdf_file: UploadFile = File(...)):\n    with open(f\"uploaded_files/{pdf_file.filename}\", \"wb\") as buffer:\n        buffer.write(pdf_file.file.read())\n    return {\"filename\": pdf_file.filename}\n"}
{"type": "source_file", "path": "config/__init__.py", "content": ""}
{"type": "source_file", "path": "auth/jwt_bearer.py", "content": "from fastapi import Request, HTTPException\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nfrom .jwt_handler import decode_jwt\n\n\ndef verify_jwt(jwtoken: str) -> bool:\n    isTokenValid: bool = False\n\n    payload = decode_jwt(jwtoken)\n    if payload:\n        isTokenValid = True\n    return isTokenValid\n\n\nclass JWTBearer(HTTPBearer):\n    def __init__(self, auto_error: bool = True):\n        super(JWTBearer, self).__init__(auto_error=auto_error)\n\n    async def __call__(self, request: Request):\n        credentials: HTTPAuthorizationCredentials = await super(\n            JWTBearer, self\n        ).__call__(request)\n        print(\"Credentials :\", credentials)\n        if credentials:\n            if not credentials.scheme == \"Bearer\":\n                raise HTTPException(\n                    status_code=403, detail=\"Invalid authentication token\"\n                )\n\n            if not verify_jwt(credentials.credentials):\n                raise HTTPException(\n                    status_code=403, detail=\"Invalid token or expired token\"\n                )\n\n            return credentials.credentials\n        else:\n            raise HTTPException(status_code=403, detail=\"Invalid authorization token\")\n"}
{"type": "source_file", "path": "auth/admin.py", "content": "from fastapi import HTTPException, Depends, status\nfrom fastapi.security import HTTPBasicCredentials, HTTPBasic\nfrom passlib.context import CryptContext\n\nfrom database.database import admin_collection\n\nsecurity = HTTPBasic()\nhash_helper = CryptContext(schemes=[\"bcrypt\"])\n\n\nasync def validate_login(credentials: HTTPBasicCredentials = Depends(security)):\n    admin = admin_collection.find_one({\"email\": credentials.username})\n    if admin:\n        password = hash_helper.verify(credentials.password, admin[\"password\"])\n        if not password:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Incorrect email or password\",\n            )\n        return True\n    raise HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Incorrect email or password\"\n    )\n"}
{"type": "source_file", "path": "auth/jwt_handler.py", "content": "import time\nfrom typing import Dict\n\nimport jwt\n\nfrom config.config import Settings\n\n\ndef token_response(token: str):\n    return {\"access_token\": token}\n\n\nsecret_key = Settings().secret_key\n\n\ndef sign_jwt(user_id: str) -> Dict[str, str]:\n    # Set the expiry time.\n    payload = {\"user_id\": user_id, \"expires\": time.time() + 2400}\n    return token_response(jwt.encode(payload, secret_key, algorithm=\"HS256\"))\n\n\ndef decode_jwt(token: str) -> dict:\n    decoded_token = jwt.decode(token.encode(), secret_key, algorithms=[\"HS256\"])\n    return decoded_token if decoded_token[\"expires\"] >= time.time() else {}\n"}
{"type": "source_file", "path": "core/engine/conversation/conversation_manager.py", "content": "\"\"\"\nConversation manager \n\"\"\"\nfrom typing import List\nimport uuid\n\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nfrom models import Conversations\n\n\nclass ConversationManager:\n    def __init__(\n        self,\n    ) -> None:\n        pass  # to be filled later\n\n    def initialise_conversation(self, user_id: str):\n        conversation_id = str(uuid.uuid4())\n        Conversations(\n            conversation_id=conversation_id, user_id=user_id, message_content=[]\n        ).save()\n        return conversation_id\n\n    def save_conversation(self, conversation_id: str, user_id: str, conversation: List):\n        message_content = list()\n        for message in conversation:\n            message_content.append({\"type\": message.type, \"content\": message.content})\n        Conversations(\n            conversation_id=conversation_id,\n            user_id=user_id,\n            message_content=message_content,\n        ).save()\n        return conversation_id\n\n    def append_to_conversation(self, conversation_id: str, user_id: str, last_message):\n        conversation_doc = Conversations.objects.get(\n            conversation_id=conversation_id, user_id=user_id\n        )\n        last_message_dict = {\"type\": last_message.type, \"content\": last_message.content}\n        conversation_doc.update(push__message_content=last_message_dict)\n\n    def load_conversation(self, conversation_id, user_id):\n        conversation_doc = Conversations.objects.get(\n            conversation_id=conversation_id, user_id=user_id\n        )\n        conversation = list()\n        for message in conversation_doc.message_content:\n            # TO DO : Reduce ifs by dynamicstring and getattr\n            if message[\"type\"] == \"human\":\n                conversation.append(HumanMessage(message[\"content\"]))\n            if message[\"type\"] == \"ai\":\n                conversation.append(AIMessage(message[\"content\"]))\n            if message[\"type\"] == \"system\":\n                conversation.append(SystemMessage(message[\"content\"]))\n        return conversation\n"}
{"type": "source_file", "path": "base/base_datachain.py", "content": "\"\"\"\nBase class for datachain models\n\"\"\"\n\n\nfrom abc import ABC, abstractmethod\n\n\nclass AbstractDataChain(ABC):\n    \"\"\"\n    Abstract base class for AbstractDataChain.\n    \"\"\"\n\n    def __init__(self, model, prompt_template):\n        self.model = model\n        self.prompt_template = prompt_template\n\n    @abstractmethod\n    def chat(self, context):\n        \"\"\"\n        Abstract method for generating a response from the language model given a context.\n\n        Args:\n        - context (str): The context for generating the response.\n\n        Returns:\n        - response (str): The response generated by the language model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_data(self, context):\n        \"\"\"\n        Abstract method for retrieving data based on the response from the language model.\n\n        Args:\n        - context (str): The context for generating the response and querying data.\n\n        Returns:\n        - data (list): A list of dictionaries containing the retrieved data for each parameter.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/stocks/stock_pipelines.py", "content": "\"\"\"\nStock pipelines from Alphavantage\n\"\"\"\n\n\n\"\"\"\nNews Pipelines that uses https://newsapi.org/docs/get-started\n\n\"\"\"\nimport os\n\nimport requests\nimport aiohttp\n\n# TO DO : Move to config.py\n\n\nclass StockPipeline:\n    def __init__(self):\n        self.api_key = os.getenv(\"STOCK_ALPHAVANTAGE_API_KEY\")\n\n    def query(self, function=\"OVERVIEW\", symbol=\"TESL\"):\n\n        request_url = self.query_builder(function=function, symbol=symbol)\n\n        response = requests.get(request_url)\n        return response.json()\n\n    async def aquery(self, function=\"OVERVIEW\", symbol=\"TESL\"):\n        request_url = self.query_builder(function=function, symbol=symbol)\n\n        async with aiohttp.ClientSession(trust_env=True) as session:\n            async with session.get(request_url) as response:\n                return await response.json()\n\n    def query_builder(self, function=\"OVERVIEW\", symbol=\"TESL\"):\n        \"\"\"\n        Query builder for Alphavantage API\n\n        Function : OVERVIEW, BALANCE_SHEET, INCOME_STATEMENT, EARNINGS, IPO_CALENDAR, WTI, REAL_GDP, TIME_SERIES_INTRADAY, TIME_SERIES_DAILY, TIME_SERIES_WEEKLY, TIME_SERIES_MONTHLY,\n        \"\"\"\n        url = f\"https://www.alphavantage.co/query?function={function}&symbol={symbol}&apikey={self.api_key}\"\n        return url\n"}
{"type": "source_file", "path": "config/config.py", "content": "from typing import Optional\nimport os\n\nfrom dotenv import load_dotenv\nfrom pydantic_settings import BaseSettings\nimport models as models\n\nload_dotenv()\n\n\nclass Settings(BaseSettings):\n    # database configurations\n    DATABASE_URL: Optional[str] = None\n\n    # JWT\n    secret_key: str = \"secret\"\n    algorithm: str = \"HS256\"\n\n    class Config:\n        env_file = \".env.dev\"\n        from_attributes = True\n\n\nclass MongoConfig:\n\n    MONGODB_HOST = os.getenv(\n        \"MONGODB_HOST\", \"localhost\"\n    )  # ?authSource=admin&ssl=true&replicaSet=globaldb\"\n    MONGODB_PORT = os.getenv(\"MONGODB_PORT\", \"27017\")\n    MONGO_URL = DATABASE_URL = f\"mongodb://{MONGODB_HOST}:{MONGODB_PORT}/finsean\"\n\n\nclass RedisConfig:\n\n    REDIS_HOST = os.getenv(\n        \"REDIS_HOST\", \"localhost\"\n    )  # ?authSource=admin&ssl=true&replicaSet=globaldb\"\n    REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")  # NOTE : Those configs can be merged.\n"}
{"type": "source_file", "path": "core/engine/driver.py", "content": "from logging_stack import logger\nfrom queue import Empty\nfrom threading import Thread\n\nfrom langchain import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain_core.callbacks.base import BaseCallbackHandler\n\nfrom core.engine.data_pipelines.pdf_pipelines import pdf_loader_factory\n\n# from core.engine.llm_models.mistral_7b_engine import Mistral7BInstructModel\nfrom core.engine.llm_models.mistral_api_engine import MistralAPIModel\nfrom core.engine.text_utils.text_utils import text_splitter_factory\n\n\nclass ModelDriver:\n    # Prompt\n    TEMPLATE = \"\"\"\n    I would like you to create a detailed report. Report should be based on the financial document I'm giving you between two *** . It is at the below.\n\n\n***\n\n {context}\n\n***\n\n    \n    Here's the additional question: {question}\n    Helpful Answer:\"\"\"\n    # TEMPLATE=\"Short answers only\"\n    QA_CHAIN_PROMPT = PromptTemplate(\n        input_variables=[\"context\", \"question\"],\n        template=TEMPLATE,\n    )\n\n    def __init__(self) -> None:\n        self.model = None\n        self.set_pdf_loader()  # to be fixed later\n        self.set_textsplitter()  # to be fixed later\n        self.set_embedding()  # to be fixed later\n\n    def set_textsplitter(self, text_splitter=\"RecursiveCharacterTextSplitter\"):\n        self.text_splitter = text_splitter_factory(text_splitter=text_splitter)\n        # all_splits = text_splitter.split_documents(data)\n\n    def set_pdf_loader(self, loader=\"OnlinePDFLoader\"):\n        self.loader = pdf_loader_factory(loader=loader)\n\n    def set_embedding(self, embedding=\"HuggingFaceEmbeddings\"):\n        self.embedding = HuggingFaceEmbeddings()\n\n    def load_document(self, document_link, is_bytes=False):\n        # document_link = \"https://www.apple.com/newsroom/pdfs/FY23_Q1_Consolidated_Financial_Statements.pdf\"\n        if not is_bytes:\n            loader = self.loader(\"uploaded_files/\" + document_link)\n        else:\n            loader = self.loader(document_link)\n        data = loader.load()\n        all_splits = self.text_splitter.split_documents(data)\n\n        self.vectorstore = Chroma.from_documents(\n            documents=all_splits, embedding=self.embedding\n        )\n\n    def load_model(self, model_name):  # TO DO : Move model DB to mongo.\n        # if model_name == \"mistral\":\n        #    self.model = Mistral7BInstructModel(\n        #        \"\"\n        #    ).load_model()  # TO DO : Model configs can be jsonable later on for distribution.\n        if model_name == \"mistral-api\":\n            self.model = MistralAPIModel(\n                \"\"\n            ).load_model()  # TO DO : Model configs can be jsonable later on for distribution.\n\n    async def chat(\n        self,\n        query,\n        streaming=True,\n    ):\n        qa_chain = RetrievalQA.from_chain_type(\n            self.model,\n            retriever=self.vectorstore.as_retriever(),\n            chain_type_kwargs={\"prompt\": self.QA_CHAIN_PROMPT},\n        )\n        # if streaming:\n        async for chunk in qa_chain.astream(\n            {\"query\": query}\n        ):  #         async for chunk in qa_chain.astream({\"query\": query}):\n            yield chunk[\"result\"]\n        # else:F\n        #    return qa_chain({\"query\": query})['result']\n\n    def get_chain(self):\n        qa_chain = RetrievalQA.from_chain_type(\n            self.model,\n            retriever=self.vectorstore.as_retriever(),\n            chain_type_kwargs={\"prompt\": self.QA_CHAIN_PROMPT},\n        )\n        return qa_chain\n\n    async def chat_chain_injected(self, query, qa_chain, streaming=True):\n\n        # if streaming:\n        async for chunk in qa_chain.astream(\n            {\"query\": query}\n        ):  #         async for chunk in qa_chain.astream({\"query\": query}):\n            yield chunk[\"result\"]\n        # else:F\n        #    return qa_chain({\"query\": query})['result']\n\n\nfrom langchain.chains import LLMChain\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\n\n# TO DO : Reduce model drivers to one. Or, may be we can have two drivers.\nclass ChainLLMModel:  # TO DO : Better name\n    def __init__(self, model=\"mistral-api\") -> None:\n        self.load_model(model)\n        self.set_embedding()  # to be fixed later\n        self.set_pdf_loader()  # to be fixed later\n        self.set_textsplitter()  # to be fixed later\n        self.set_embedding()  # to be fixed later\n\n    def set_textsplitter(self, text_splitter=\"RecursiveCharacterTextSplitter\"):\n        self.text_splitter = text_splitter_factory(text_splitter=text_splitter)\n        # all_splits = text_splitter.split_documents(data)\n\n    def set_pdf_loader(self, loader=\"OnlinePDFLoader\"):\n        self.loader = pdf_loader_factory(loader=loader)\n\n    def set_embedding(self, embedding=\"HuggingFaceEmbeddings\"):\n        self.embedding = HuggingFaceEmbeddings()\n\n    def load_model(self, model_name):  # TO DO : Move model DB to mongo.\n        if model_name == \"mistral-api\":\n            self.model = MistralAPIModel(\n                \"\"\n            ).load_model()  # TO DO : Model configs can be jsonable later on for distribution.\n\n    async def chat(\n        self,\n        query,\n        streaming=True,\n    ):\n        qa_chain = RetrievalQA.from_chain_type(\n            self.model,\n            chain_type_kwargs={\"prompt\": self.QA_CHAIN_PROMPT},\n        )\n        # if streaming:\n        async for chunk in qa_chain.astream({\"query\": query}):\n            yield chunk[\"result\"]\n\n    def nonasync_chat(self, query, prompt_template):\n\n        chain = LLMChain(llm=self.model, prompt=prompt_template)\n\n        return chain.run(query)  # qa_chain({\"query\": query})['result']\n\n    async def async_chat(self, query, prompt_template):\n\n        chain = LLMChain(llm=self.model, prompt=prompt_template)\n\n        return await chain.arun(query)  # qa_chain({\"query\": query})['result']\n"}
{"type": "source_file", "path": "schemas/admin.py", "content": "from typing import Optional\n\nfrom fastapi.security import HTTPBasicCredentials\nfrom pydantic import BaseModel, EmailStr\n\n\nclass AdminSignIn(HTTPBasicCredentials):\n    class Config:\n        json_schema_extra = {\n            \"example\": {\"username\": \"abdul@youngest.dev\", \"password\": \"3xt3m#\"}\n        }\n\n\nclass AdminData(BaseModel):\n    fullname: str\n    email: EmailStr\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"fullname\": \"Abdulazeez Abdulazeez Adeshina\",\n                \"email\": \"abdul@youngest.dev\",\n            }\n        }\n\n\nclass ChatRequest(BaseModel):\n    pdf_link: Optional[str]\n    query: str\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/pdf_pipelines.py", "content": "# TO DO : This is very ugly pipeline. It'll be fixed.\n\nfrom langchain.document_loaders import OnlinePDFLoader, PyPDFLoader\n\n\ndef pdf_loader_factory(loader=\"PyPDFLoader\"):\n    if loader == \"PyPDFLoader\":\n        return PyPDFLoader\n    elif loader == \"OnlinePDFLoader\":\n        return OnlinePDFLoader\n"}
{"type": "source_file", "path": "core/engine/embeddings/hf_embeddings.py", "content": "# TO DO : EMBEDDING BASECLASS OR TEMPLATE OR INTERFACE TO BE ADDED.\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nembedding = HuggingFaceEmbeddings()  # GPT4AllEmbeddings()\n\n\nclass HFEmbeddings:  # TO DO : BASECLASS OR TEMPLATE OR INTERFACE TO BE ADDED.\n    def __init__(self, **kwargs) -> None:\n        pass\n\n    def embed(self, text, **kwargs):\n        return embedding.embed(text, **kwargs)\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/news/news_pipelines.py", "content": "\"\"\"\nNews Pipelines that uses https://newsapi.org/docs/get-started\n\n\"\"\"\nimport os\nimport requests\nimport aiohttp\n\nnews_api_key = os.getenv(\n    \"NEWS_API_KEY\"\n)  ## NOTE : This is for persistend db storage. Clients with serving to be added.\n\n\nclass NewsPipeline:\n    def __init__(self):\n        self.api_key = news_api_key\n        self.base_url = \"https://newsapi.org/v2/everything\"\n\n    def query(self, query, sortBy=\"popularity\", country=\"us\", from_=None):\n        params = {\n            \"q\": query,\n            \"country\": country,\n            \"sortBy\": sortBy,\n            \"pageSize\": 5,\n            \"apiKey\": self.api_key,\n        }\n        # if from_:\n        #    params['from'] = from_\n\n        try:\n            response = requests.get(self.base_url, params=params)\n            response.raise_for_status()  # Raise an exception for any HTTP error\n            return response.json()\n        except requests.RequestException as e:\n            print(f\"Error occurred: {e}\")\n            return None\n\n    async def aquery(self, query, sortBy=\"popularity\", country=\"us\", from_=None):\n        params = {\n            \"q\": query,\n            \"country\": country,\n            \"sortBy\": sortBy,\n            \"pageSize\": 5,\n            \"apiKey\": self.api_key,\n        }\n        params_ = {k: v for k, v in params.items() if v is not None}\n\n        # if from_:\n        #    params['from'] = from_\n\n        try:\n            async with aiohttp.ClientSession(trust_env=True) as session:\n                async with session.get(self.base_url, params=params_) as response:\n                    response.raise_for_status()  # Raise an exception for any HTTP error\n                    return await response.json()\n        except aiohttp.ClientError as e:\n            print(f\"Error occurred: {e}\")\n            return {}\n"}
{"type": "source_file", "path": "core/engine/text_utils/text_utils.py", "content": "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# TO DO : Need baseclass for textsplitters.\n\n\ndef text_splitter_factory(text_splitter=\"RecursiveCharacterTextSplitter\"):\n    if text_splitter == \"RecursiveCharacterTextSplitter\":\n        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n"}
{"type": "source_file", "path": "core/engine/online_model_driver.py", "content": "from logging_stack import logger\nfrom queue import Empty\nfrom threading import Thread\nimport traceback\nimport os\n\nfrom langchain import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nfrom core.engine.message_prompts.message_prompts import chat_prompt\nfrom core.engine.data_chain_driver import DataChainDriver\nfrom core.engine.data_pipelines.pdf_pipelines import pdf_loader_factory\nfrom core.engine.conversation.conversation_manager import ConversationManager\nfrom database.vector_db import chromadb_client\nfrom models import *\n\n# from core.engine.llm_models.mistral_7b_engine import Mistral7BInstructModel\nfrom core.engine.llm_models.mistral_api_engine import MistralAPIModel\nfrom core.engine.text_utils.text_utils import text_splitter_factory\n\n\nclass OnlineModelDriver:\n    def __init__(self) -> None:\n        self.model = None\n        self.data_chains = None\n        self.chromadb_client = (\n            chromadb_client  # vectorstore, it can be injected through fastapi as well.\n        )\n        self.set_pdf_loader()  # to be fixed later\n        self.set_textsplitter()  # to be fixed later\n        self.set_embedding()  # to be fixed later\n        self.chat_prompt = chat_prompt\n        self.data_chain_driver = DataChainDriver()\n        self.conversation_manager = ConversationManager()\n\n    def set_textsplitter(self, text_splitter=\"RecursiveCharacterTextSplitter\"):\n        self.text_splitter = text_splitter_factory(text_splitter=text_splitter)\n        # all_splits = text_splitter.split_documents(data)\n\n    def set_pdf_loader(self, loader=\"OnlinePDFLoader\"):\n        self.loader = pdf_loader_factory(loader=loader)\n\n    def set_embedding(self, embedding=\"HuggingFaceEmbeddings\"):\n        self.embedding = HuggingFaceEmbeddings()\n\n    def set_data_chains(self, data_chains: list):\n        self.data_chain_driver.set_data_chains(data_chains)\n\n    def initialise_conversation(self, user_id):\n        return self.conversation_manager.initialise_conversation(user_id=user_id)\n\n    def load_document(self, document_link, is_bytes=False):\n        # document_link = \"https://www.apple.com/newsroom/pdfs/FY23_Q1_Consolidated_Financial_Statements.pdf\"\n        if not is_bytes:\n            loader = self.loader(\"uploaded_files/\" + document_link)\n        else:\n            loader = self.loader(document_link)\n        data = loader.load()\n        all_splits = self.text_splitter.split_documents(data)\n        self.chroma_collection = Chroma.from_documents(\n            documents=all_splits,\n            embedding=self.embedding,\n            client=self.chromadb_client,\n        )\n\n    def load_assets(\n        self,\n    ):\n        # TO DO : add path to config or env file.\n        uploaded_files_folder = \"uploaded_files/\"\n        logger.info(\"Assets directory is loading.\")\n        for pdf in os.listdir(uploaded_files_folder):\n            if \".pdf\" in pdf:\n                try:\n                    self.load_document(pdf)\n                except Exception as exc:\n                    logger.warning(f\"Couldnt load {pdf} file :: {exc}\", exc_info=True)\n\n    def load_model(self, model_name):  # TO DO : Move model DB to mongo.\n        # if model_name == \"mistral\":\n        #    self.model = Mistral7BInstructModel(\n        #        \"\"\n        #    ).load_model()  # TO DO : Model configs can be jsonable later on for distribution.\n        if model_name == \"mistral-api\":\n            print(\"using api model\")\n            self.model = MistralAPIModel(\n                \"\"\n            ).load_model()  # TO DO : Model configs can be jsonable later on for distribution.\n\n    def answer(\n        self,\n        query,\n        streaming=True,\n    ):\n        if self.data_chain_driver.data_chains:  # Fix later.\n            enriched_prompt = self.data_chain_driver.get_augmented_prompt(query)\n        formatted_chat_prompt = self.chat_prompt.format_messages(\n            question=query, context=enriched_prompt\n        )\n\n        # if streaming:\n        # async for chunk in qa_chain.astream(\n        #    {\"query\": query}\n        # ):  #         async for chunk in qa_chain.astream({\"query\": query}):\n        #    yield chunk[\"result\"]\n        # else:F\n        try:\n            result = self.model.invoke(formatted_chat_prompt)\n            return result\n        except Exception:\n            logger.error(traceback.format_exc())\n\n    async def aanswer(\n        self,\n        query,\n        streaming=True,\n    ):\n        if self.data_chain_driver.data_chains:  # Fix later.\n            enriched_prompt = await self.data_chain_driver.aget_augmented_prompt(query)\n        formatted_chat_prompt = self.chat_prompt.format_messages(\n            question=query, context=enriched_prompt\n        )\n\n        # if streaming:\n        # async for chunk in qa_chain.astream(\n        #    {\"query\": query}\n        # ):  #         async for chunk in qa_chain.astream({\"query\": query}):\n        #    yield chunk[\"result\"]\n        # else:F\n        try:\n            result = self.model.invoke(formatted_chat_prompt)\n            return result\n        except Exception:\n            logger.error(traceback.format_exc())\n\n    def chatt(\n        self,\n        query,\n        streaming=True,\n    ):\n        if self.data_chain_driver.data_chains:  # Fix later.\n            enriched_prompt = self.data_chain_driver.get_augmented_prompt(query)\n        template = self.get_template()\n        template += enriched_prompt\n        qa_chain_prompt = self.get_qa_chain_prompt(template)\n\n        qa_chain = RetrievalQA.from_chain_type(\n            self.model,\n            retriever=self.chroma_collection.as_retriever(),\n            chain_type_kwargs={\"prompt\": qa_chain_prompt},\n        )\n\n        # if streaming:\n        # async for chunk in qa_chain.astream(\n        #    {\"query\": query}\n        # ):  #         async for chunk in qa_chain.astream({\"query\": query}):\n        #    yield chunk[\"result\"]\n        # else:F\n        try:\n            result = qa_chain({\"query\": query})[\"result\"]\n            return result\n        except Exception:\n            logger.error(traceback.format_exc())\n\n    def conversation(\n        self,\n        query: str,\n        user_id: str,\n        conversation_id: str,\n    ):\n        conversation = self.conversation_manager.load_conversation(\n            conversation_id=conversation_id, user_id=user_id\n        )\n        first_message = len(conversation) == 0\n\n        if first_message:\n            enriched_prompt = query\n            if self.data_chain_driver.data_chains:  # Fix later.\n                enriched_prompt = self.data_chain_driver.get_augmented_prompt(query)\n            conversation = self.chat_prompt.format_messages(\n                question=query, context=enriched_prompt\n            )\n            print(enriched_prompt, \"*********\" * 4)\n\n        else:\n            conversation.append(HumanMessage(content=query))\n        try:\n            ai_message = self.model.invoke(conversation)\n            # save last conversations.\n            self.conversation_manager.append_to_conversation(\n                conversation_id=conversation_id,\n                user_id=user_id,\n                last_message=conversation[0],\n            )\n            self.conversation_manager.append_to_conversation(\n                conversation_id=conversation_id,\n                user_id=user_id,\n                last_message=ai_message,\n            )\n            return {\"role\": \"assistant\", \"content\": ai_message.content}\n\n        except Exception:\n            logger.error(traceback.format_exc())\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/db_retrieval/template.py", "content": "import ast\nimport asyncio\nfrom langchain import PromptTemplate\n\nfrom base.base_datachain import AbstractDataChain\nfrom core.engine.data_pipelines.db_retrieval.db_retrieval_pipelines import (\n    DBRetrievalPipeline,\n)\nfrom core.engine.driver import ChainLLMModel\nfrom core.engine.data_pipelines.summarizer.template import SummarizerChain\n\ntemplate = \"\"\"You are a financial analyst. I would like you to give me python list object, by  understanding a context. Python list object will include 20 questions where the context can be more understood if 5 questions are answered. \nYour answer must be python list, no other comments. Dont ever answer any comment. Just give me the python list of strings based on the rules! Dont ever give any comment! Just give me the python list based on the rules! If you can't find anything, just return an empty list, [] !\nYou are only allowed to respond either [] or the Python list that contains question strings.\nHere's an  example : \nContext : \"What is the financial status of the company?\"\nAnswer : [\"What is the net income of the company?\", \"What is the revenue of the company?\", \"What is the profit of the company?\", \"What is the loss of the company?\", \"What is the cash flow of the company?\", \"What is the debt of the company?\", \"What is the equity of the company?\", \"What is the assets of the company?\", \"What is the liabilities of the company?\", \"What is the operating income of the company?\", \"What is the gross profit of the company?\", \"What is the EBITDA of the company?\", \"What is the EBIT of the company?\", \"What is the EPS of the company?\", \"What is the PE ratio of the company?\", \"What is the PB ratio of the company?\", \"What is the PS ratio of the company?\", \"What is the ROE of the company?\", \"What is the ROA of the company?\", \"What is the ROIC of the company?\"]\n\nReturn me a 20 questions in python list of strings given below.\n\nContext : {context}\n\n\"\"\"\n# 5. If there's a date you want to search for, you can return the date, such as 2021-10-10. The date and time of the oldest article you want to get. If no date, don't place from to the dictionary.\n\n\nPROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"context\"],\n    template=template,\n)\n\n\nclass DBRetrievalChain(AbstractDataChain):\n    \"\"\"\n    Class that allows RAG to retrieve online data.\n\n    \"\"\"\n\n    def __init__(\n        self, model: ChainLLMModel, prompt_template: PromptTemplate = PROMPT_TEMPLATE\n    ) -> None:\n        self.model = model\n        self.prompt_template = prompt_template\n        self.db_retrieval_pipeline = DBRetrievalPipeline()\n        self.summarizer_chain = SummarizerChain(model)\n\n    def chat(self, context):\n        return self.model.nonasync_chat(context, prompt_template=self.prompt_template)\n\n    async def async_chat(self, context):\n        return await self.model.async_chat(\n            context, prompt_template=self.prompt_template\n        )\n\n    def get_data(self, context, return_augmented_prompt=True):\n        response = self.chat(context)\n        print(\"retrieving documents\")\n        db_retrieval_queries = ast.literal_eval(response)\n\n        data = relevant_docs_list = list()\n\n        for query in db_retrieval_queries:\n\n            response = self.db_retrieval_pipeline.query(query=query, k=5)\n\n            for resp in response:\n                resp[\"query\"] = query\n\n            relevant_docs_list.extend(response)\n\n        if return_augmented_prompt:\n\n            augmented_prompt = \"\"\"Here's the data sources I found for you to help you to answer the question. You may or may not prefer to consider all questions and answers. If you think questions are relevant, and context is relevant to the question, you can create an answer by referencing the facts from the documents.   Please cite the resources with links and dates if it's given. : \"\"\"\n            for i, db_source in enumerate(\n                relevant_docs_list[0:4]\n            ):  # TO DO : Uncover or fix limit\n                content = db_source[\"page_content\"]\n                if len(content) > 100:\n                    content = self.summarizer_chain.get_data(content, word_count=20)\n                augmented_prompt += f\"\"\"{i}. Question : {db_source[\"query\"]} . filename : {db_source[\"source\"]} . Context : {content} \\n\"\"\"\n\n            return augmented_prompt\n\n        return data\n\n    async def aget_data(self, context, return_augmented_prompt=True):\n        response = await self.async_chat(context)\n        print(\"retrieving documents\")\n        db_retrieval_queries = ast.literal_eval(response)\n\n        data = relevant_docs_list = list()\n\n        tasks = []\n        for query in db_retrieval_queries:\n            task = self.db_retrieval_pipeline.aquery(query=query, k=2)\n            tasks.append(task)\n\n        relevant_docs_lists = await asyncio.gather(*tasks)\n        relevant_docs_list = [doc for sublist in relevant_docs_lists for doc in sublist]\n\n        if return_augmented_prompt:\n            augmented_prompt = \"\"\"Here's the data sources I found for you to help you to answer the question. You may or may not prefer to consider all questions and answers. If you think questions are relevant, and context is relevant to the question, you can create an answer by referencing the facts from the documents. Please cite the resources with links and dates if it's given. : \"\"\"\n\n            # Concurrently gather content for each relevant document\n            acontent_tasks = []\n            content_results = []\n\n            for db_source in relevant_docs_list[0:1]:  # TO DO : Uncover the limit\n                content = db_source[\"page_content\"]\n                if len(content) > 100:\n                    task = self.summarizer_chain.aget_data(content, word_count=20)\n                    acontent_tasks.append(task)\n                else:\n                    # If content is less than 100 characters, append it directly\n                    content_results.append(content)\n\n            acontent_results = await asyncio.gather(\n                *acontent_tasks, return_exceptions=True\n            )\n            content_results.extend(acontent_results)\n\n            # Combine the content with other information and construct the augmented prompt\n            for i, (db_source, content) in enumerate(\n                zip(relevant_docs_list, content_results)\n            ):\n                if isinstance(content, Exception):\n                    # Handle exceptions if any occurred during gathering\n                    augmented_prompt += f\"Error fetching content for document {i}: {type(content).__name__}\\n\"\n                else:\n                    augmented_prompt += f\"{i}. Question : {db_source['query']} . filename : {db_source['source']} . Context : {content} \\n\"\n\n            return augmented_prompt\n\n        return data\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/db_retrieval/db_retrieval_pipelines.py", "content": "\"\"\"\nNews Pipelines that uses https://newsapi.org/docs/get-started\n\n\"\"\"\nimport os\nimport requests\nfrom logging_stack import logger\nimport asyncio\n\nfrom database.vector_db import chromadb_langchain_client\n\n\nclass DBRetrievalPipeline:\n    def __init__(self):\n        self.chromadb_langchain_client = chromadb_langchain_client\n\n    def query(self, query, k):\n        docs = self.chromadb_langchain_client.similarity_search(query, k=k)\n        logger.info(f\"Retrieved {len(docs)} documents from the database\")\n        return [\n            {\"page_content\": doc.page_content, \"source\": doc.metadata[\"source\"]}\n            for doc in docs\n        ]\n\n    async def aquery(self, query, k):\n        loop = asyncio.get_running_loop()\n        docs = await loop.run_in_executor(None, self._query_sync, query, k)\n        logger.info(f\"Retrieved {len(docs)} documents from the database\")\n        return [\n            {\n                \"page_content\": doc.page_content,\n                \"source\": doc.metadata[\"source\"],\n                \"query\": query,\n            }\n            for doc in docs\n        ]\n\n    def _query_sync(self, query, k):\n        return self.chromadb_langchain_client.similarity_search(query, k=k)\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/news/template.py", "content": "import ast\nimport asyncio\n\nfrom langchain import PromptTemplate\n\nfrom base.base_datachain import AbstractDataChain\nfrom core.engine.data_pipelines.news.news_pipelines import NewsPipeline\nfrom core.engine.driver import ChainLLMModel\nfrom core.engine.data_pipelines.summarizer.template import SummarizerChain\n\ntemplate = \"\"\"I would like you to give me python list object, by  understanding a context. You are a financial analyst, so you can classify the financial data request.\n\nYou are only allowed to respond either [] or the Python list of dictionaries based on the rules!\nDirect answers only. No AI narrator or voice, AI is silent. No AI introduction. No AI summary. No disclaimers or warnings or usage advisories.\n \n\nNews can have a significant impact on financial analysis and the financial markets in several ways:\n\nMarket Sentiment: News often influences investor sentiment, which can drive buying or selling activity in the markets. Positive news, such as strong earnings reports or favorable economic indicators, tends to boost investor confidence and lead to higher stock prices, while negative news, such as poor economic data or geopolitical tensions, can cause fear and uncertainty, resulting in market declines.\nPrice Movements: News events can directly affect the prices of financial assets, including stocks, bonds, currencies, and commodities. For example, a company announcing better-than-expected earnings may cause its stock price to rise, while news of a natural disaster affecting commodity production may lead to price spikes in the affected commodities.\nVolatility: News can increase market volatility as investors react to new information. Sudden and unexpected news events, such as political developments or corporate scandals, can lead to sharp price fluctuations as investors reassess the impact of the news on asset valuations and market conditions.\nFundamental Analysis: News provides valuable information for fundamental analysis, which involves evaluating the financial health and performance of companies and economies. Analysts use news to update their earnings forecasts, assess industry trends, and make investment recommendations based on changes in business fundamentals.\nRisk Management: News can impact risk management strategies by highlighting potential risks and opportunities in the markets. Traders and investors may adjust their portfolio allocations or implement hedging strategies in response to news events that could impact their investments.\nPolicy Decisions: News related to central bank decisions, government policies, and regulatory changes can have a significant impact on financial markets. For example, announcements of interest rate decisions or changes in monetary policy can influence borrowing costs, exchange rates, and asset prices.\nPsychological Factors: News can also affect investor psychology and behavior, leading to herd mentality, irrational exuberance, or panic selling. Behavioral biases, such as anchoring or confirmation bias, may influence how investors interpret and react to news events, potentially amplifying market movements.\n\nHere's one example of how news can impact financial analysis: Initial Vaccine Announcements (November 2020): When pharmaceutical companies like Pfizer and Moderna announced promising results from their COVID-19 vaccine trials, global financial markets responded positively. Stock markets rallied, with shares of companies in sectors most affected by the pandemic, such as travel, hospitality, and retail, experiencing significant gains. Investors became more optimistic about the prospect of a swift economic recovery as vaccination efforts ramped up.\n\nI'll give a context (or question) that there may be news published related to the context, which can help in financial analysis. Given a context, you'll give me a python list based on the rules. Here are some rules.\n1. If you think there's nothing to get from news, just return an empty list, []. For example, if the context is \"I want to see the balance sheet of TSLA\", you need to return []. I will search these kind of information by myself.\n2. If there is, you need to return a list of dictionaries, example: [{{\"query\":query, \"sortBy\":sortBy, \"country\":country, \"from\":from_}}].\n3. sortBy, The order to sort the articles in. Possible options: relevancy , popularity , publishedAt . relevancy = articles more closely related to q come first. popularity = articles from popular sources and publishers come first. publishedAt = newest articles come first.  For example, if you would like to get latest news, use publishedAt .\n4. If there's a country that you want to search for, you can return the country, such as us. The 2-letter ISO 3166-1 code of the country you want to get headlines for. If no country, don't place country to the dictionary.\n5. If there's a news search query that you would like to search, you need to set {{\"query\":query}}. It can be \"query\":\"Apple\" in python dict.\n6. Please also add why you think the news is important for the context in one small sentence. For example, if the context is \"I want to see if TESLA is able to produce batteries\", you may to return [{{\"query\":\"Lithium Stocks\", \"sortBy\":\"popularity\", \"country\":\"us\", \"description\":\"Tesla battery production is bounded to the lithium shortage.\"}}].\n7. Response Example: [{{\"query\":\"Apple\", \"sortBy\":\"publishedAt\", \"country\":\"us\", \"from\":\"2021-10-10\"}}]\n8. Response Example: [{{\"query\":\"Tesla\", \"sortBy\":\"popularity\", \"country\":\"tr\", \"from\":\"2020-10-10\"}}]\n\nDont ever answer any comment. Just give me the python list based on the rules! Dont ever give any comment! Just give me the python list based on the rules! If you can't find anything, just return an empty list, [] !\nYou are only allowed to respond either [] or the Python list of dictionaries based on the rules!\nDirect answers only. No AI narrator or voice, AI is silent. No AI introduction. No AI summary. No disclaimers or warnings or usage advisories.\n\n\nHere's the context : {context}\nPython list response answer : \n\"\"\"\n# 5. If there's a date you want to search for, you can return the date, such as 2021-10-10. The date and time of the oldest article you want to get. If no date, don't place from to the dictionary.\n\n\nPROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"context\"],\n    template=template,\n)\n\n\nclass NewsDataChain(AbstractDataChain):\n    \"\"\"\n    Class that allows RAG to retrieve online data.\n\n    \"\"\"\n\n    def __init__(\n        self, model: ChainLLMModel, prompt_template: PromptTemplate = PROMPT_TEMPLATE\n    ) -> None:\n        self.model = model\n        self.prompt_template = prompt_template\n        self.news_pipeline = NewsPipeline()\n        self.summarizer_chain = SummarizerChain(model)\n\n    def chat(self, context):\n        return self.model.nonasync_chat(context, prompt_template=self.prompt_template)\n\n    async def async_chat(self, context):\n        return await self.model.async_chat(\n            context, prompt_template=self.prompt_template\n        )\n\n    def get_data(self, context, return_augmented_prompt=True):\n        print(\"news obtaining\")\n        response = self.chat(context)\n        news_pipeline_parameters = ast.literal_eval(response)\n        print(\"searching this news: \", news_pipeline_parameters)\n\n        data = list()\n        for parameter in news_pipeline_parameters[0:1]:\n            query = parameter[\"query\"]\n            sortBy = parameter[\"sortBy\"]\n\n            country = parameter[\"country\"] if \"country\" in parameter else None\n            description = (\n                parameter[\"description\"] if \"description\" in parameter else None\n            )\n\n            response = self.news_pipeline.query(\n                query=query, sortBy=sortBy, country=country\n            )\n            data.append(\n                {\"query\": query, \"description\": description, \"response\": response}\n            )\n\n        if return_augmented_prompt:\n\n            if len(data) == 0:\n                return \"I couldn't find any news for you. Please try another context.\"\n            augmented_prompt = \"\"\"Here's the news I found for you. Please cite the resources with links and dates if it's given. : \"\"\"\n\n            for new in data[0:1]:\n                if new is not None:\n                    if (\n                        new[\"response\"][\"totalResults\"] != 0\n                    ):  #  TO DO : 2 ifs  Will be fixed later\n                        augmented_prompt += f\"\"\"I found the news below because {new[\"description\"]} . Those news are : \\n\"\"\"\n                        for article in new[\"response\"][\"articles\"][0:1]:\n                            content = str(article[\"content\"])\n                            if len(content) > 750:  # Skip if content is too long\n                                continue\n                            if (\n                                len(content) > 100\n                            ):  # If exceeds 50 words (roughly 250 characters), summarize\n                                content = self.summarizer_chain.get_data(\n                                    context=content, word_count=20\n                                )\n                            augmented_prompt += f\"\"\"Date : {article[\"publishedAt\"]} , url : {article[\"url\"]} , content : {content}\\n\"\"\"\n            return augmented_prompt\n\n        return data\n\n    async def aget_data(self, context, return_augmented_prompt=True):\n        print(\"news obtaining\")\n        response = await self.async_chat(context)\n        news_pipeline_parameters = ast.literal_eval(response.replace(\"'\", \"\"))\n        print(\"searching this news: \", news_pipeline_parameters)\n\n        data = []\n        tasks = []\n\n        for parameter in news_pipeline_parameters:\n            query = parameter[\"query\"]\n            sortBy = parameter[\"sortBy\"]\n            country = parameter.get(\"country\")\n            description = parameter.get(\"description\")\n\n            task = await self.news_pipeline.aquery(\n                query=query, sortBy=sortBy, country=country\n            )\n            tasks.append(task)\n\n        # Use asyncio.gather to concurrently execute the tasks\n\n        responses = await asyncio.gather(*tasks)\n        print(responses)\n        for parameter, response in zip(news_pipeline_parameters, responses):\n            query = parameter[\"query\"]\n            description = parameter.get(\"description\")\n            data.append(\n                {\"query\": query, \"description\": description, \"response\": response}\n            )\n\n        if return_augmented_prompt:\n\n            augmented_prompts = []\n\n            for new in data:\n                if new is not None and new[\"response\"][\"totalResults\"] != 0:\n                    augmented_prompt = f\"I found the news below because {new['description']} . Those news are : \\n\"\n                    article_tasks = []\n                    content_list = []\n                    for article in new[\"response\"][\"articles\"]:\n                        content = str(article[\"content\"])\n                        if len(content) > 750:  # Skip if content is too long\n                            content_list.append(content)\n                        if len(content) > 100:  # If exceeds 100 characters, summarize\n                            task = self.summarizer_chain.aget_data(\n                                context=content, word_count=20\n                            )\n                            article_tasks.append(task)\n                    print(article_tasks, \"*********\" * 10)\n                    # Use asyncio.gather to concurrently execute the summarizer tasks for articles\n                    content_summaries = await asyncio.gather(*article_tasks)\n                    content_list.extend(content_summaries)\n                    for article, summary in zip(\n                        new[\"response\"][\"articles\"], content_list\n                    ):\n                        augmented_prompt += f\"Date : {article['publishedAt']} , url : {article['url']} , content : {summary}\\n\"\n\n                    augmented_prompts.append(augmented_prompt)\n\n            return \"\\n\".join(augmented_prompts)\n\n        return data\n"}
{"type": "source_file", "path": "core/engine/message_prompts/message_prompts.py", "content": "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n\nchat_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"user\",\n            \"\"\"\n    You are a financial analyst that serves to VCs, Private and public equities, family offices and merge acquisitions. I would like you to create a 500 words report on a question. Here's the my  question for you : {question}\n    Please use the relevant sources and cite the sources I've shared with the links/infos/urls/authors  at the end of the document, Report should be based on the financial documents I'm giving you between two *** . It is at the below.\n***\n\n {context}\n\n***\n\n    There are other resources for you to use to answer the question : {question} .\n\n    \"\"\",\n        ),  # -->  SystemMessagePromptTemplate\n        # MessagesPlaceholder(variable_name=\"history\"),\n    ]\n)\n"}
{"type": "source_file", "path": "core/engine/llm_models/mistral_api_engine.py", "content": "import os\n\n# from mistralai.async_client import MistralAsyncClient\n# from mistralai.models.chat_completion import ChatMessage\nfrom langchain_mistralai.chat_models import ChatMistralAI\n\nmistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n\n\nclass MistralAPIModel:\n    def __init__(\n        self, model_path\n    ) -> None:  # TO DO : Model configs can be jsonable later on for distribution.\n        self.model_path = model_path\n        print(\"using api model\")\n\n    def load_model(self):\n        print(\"using api model\")\n        return ChatMistralAI(\n            mistral_api_key=mistral_api_key, model=\"mistral-large-latest\"\n        )\n"}
{"type": "source_file", "path": "core/engine/llm_models/mistral_7b_engine.py", "content": "## TO DO : BASE MODEL ENGINES TO BE ADDED.\n\nimport os\nimport sys\nfrom queue import Empty\nfrom threading import Thread\nfrom typing import *\nfrom uuid import UUID\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\nfrom langchain.llms import Ollama\nfrom langchain_core.callbacks.base import BaseCallbackHandler\n\n\nclass AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n    content: str = \"\"\n    final_answer: bool = False\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    async def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.content += token\n        # if we passed the final answer, we put tokens in queue\n        if self.final_answer:\n            if '\"action_input\": \"' in self.content:\n                if token not in ['\"', \"}\"]:\n                    self.queue.put_nowait(token)\n        elif \"Final Answer\" in self.content:\n            self.final_answer = True\n            self.content = \"\"\n\n\nclass QueueCallback(BaseCallbackHandler):\n    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n\n    def __init__(self, q):\n        self.q = q\n\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        self.q.put(token)\n\n    def on_llm_end(self, *args, **kwargs) -> None:\n        return self.q.empty()\n\n\nclass StreamingWebCallbackHandler(BaseCallbackHandler):\n    tokens: List[str] = []\n    is_responding: bool = False\n    response_id: str\n    response: str = None\n\n    def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        run_id: UUID,\n        parent_run_id: UUID | None = None,\n        **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n        sys.stdout.write(token)\n        sys.stdout.flush()\n        self.tokens.append(token)\n\n    def on_chain_start(\n        self,\n        serialized: Dict[str, Any],\n        inputs: Dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: UUID | None = None,\n        tags: List[str] | None = None,\n        metadata: Dict[str, Any] | None = None,\n        **kwargs: Any\n    ) -> Any:\n        self.is_responding = True\n        self.response_id = run_id\n        self.response = None\n\n    def on_chain_end(\n        self,\n        outputs: Dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: UUID | None = None,\n        **kwargs: Any\n    ) -> Any:\n        self.is_responding = False\n        self.response = outputs[\"response\"]\n        print(\"END: \" + self.response)\n\n    def get_response(self) -> str:\n        response_result = self.response\n        self.response = None\n\n        return response_result\n\n\ndef stream(cb, queue):\n    job_done = object()\n\n    def task():\n        queue.put(job_done)\n\n    t = Thread(target=task)\n    t.start()\n\n    while True:\n        try:\n            item = queue.get(True, timeout=1)\n            if item is job_done:\n                break\n            yield item\n        except Empty:\n            continue\n\n\n# TO DO : Fix the pipeline. It should have session for conversations, states should be kept in the session.\n# Sessions are simply conversations.\n# TO DO : Redis can be used for cacheing time-span calls.\n\n\nclass SuppressStdout:\n    def __enter__(self):\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n        sys.stderr = self._original_stderr\n\n\nclass Mistral7BInstructModel:\n    def __init__(\n        self, model_path\n    ) -> None:  # TO DO : Model configs can be jsonable later on for distribution.\n        self.model_path = model_path\n\n    def load_model(self):\n        return Ollama(\n            model=\"aisherpa/mistral-7b-instruct-v02:Q5_K_M\",\n            callback_manager=CallbackManager([AsyncIteratorCallbackHandler()]),\n        )\n"}
{"type": "source_file", "path": "schemas/student.py", "content": "from pydantic import BaseModel, EmailStr\nfrom typing import Optional, Any\n\nclass UpdateStudentModel(BaseModel):\n    fullname: Optional[str]\n    email: Optional[EmailStr]\n    course_of_study: Optional[str]\n    year: Optional[int]\n    gpa: Optional[float]\n\n    class Collection:\n        name = \"student\"\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"fullname\": \"Abdulazeez Abdulazeez\",\n                \"email\": \"abdul@school.com\",\n                \"course_of_study\": \"Water resources and environmental engineering\",\n                \"year\": 4,\n                \"gpa\": \"5.0\",\n            }\n        }\n\nclass Response(BaseModel):\n    status_code: int\n    response_type: str\n    description: str\n    data: Optional[Any]\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"status_code\": 200,\n                \"response_type\": \"success\",\n                \"description\": \"Operation successful\",\n                \"data\": \"Sample data\",\n            }\n        }\n"}
{"type": "source_file", "path": "core/engine/data_chain_driver.py", "content": "import asyncio\n\nfrom langchain import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain_core.callbacks.base import BaseCallbackHandler\n\nfrom core.engine.data_pipelines.pdf_pipelines import pdf_loader_factory\nfrom core.engine.driver import ChainLLMModel\nfrom core.engine.llm_models.mistral_api_engine import MistralAPIModel\nfrom core.engine.text_utils.text_utils import text_splitter_factory\n\n\nclass DataChainDriver:\n    \"\"\"\n    Class that allows RAG to retrieve online data.\n\n    \"\"\"\n\n    def __init__(self, model: str = \"mistral-api\") -> None:\n        self.model = ChainLLMModel(model=model)\n\n    def set_data_chains(self, data_chains: list):\n        data_chains_list = list()\n        for chain in data_chains:\n            data_chains_list.append(chain(model=self.model))\n        self.data_chains = data_chains_list\n\n    def set_textsplitter(self, text_splitter=\"RecursiveCharacterTextSplitter\"):\n        self.text_splitter = text_splitter_factory(text_splitter=text_splitter)\n        # all_splits = text_splitter.split_documents(data)\n\n    def set_pdf_loader(self, loader=\"OnlinePDFLoader\"):\n        self.loader = pdf_loader_factory(loader=loader)\n\n    def set_embedding(self, embedding=\"HuggingFaceEmbeddings\"):\n        self.embedding = HuggingFaceEmbeddings()\n\n    def get_augmented_prompt(\n        self,\n        query,\n        streaming=True,\n    ):\n        enriched_prompt = \"\"\"\\n\"\"\"\n        for chain in self.data_chains:\n            enriched_prompt += chain.get_data(query)\n            enriched_prompt += \"\\n\"\n        return enriched_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\n    async def aget_augmented_prompt(\n        self,\n        query,\n        streaming=True,\n    ):\n        tasks = [await chain.aget_data(query) for chain in self.data_chains]\n        results = await asyncio.gather(*tasks)\n        # enriched_prompt = \"\"\"\\n\"\"\"\n        # for chain in self.data_chains:\n        #    enriched_prompt += await chain.aget_data(query)\n        #    enriched_prompt += \"\\n\"\n\n        # results = enriched_prompt\n        enriched_prompt = \"\\n\".join(results)\n        return enriched_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/online_model_driver.py", "content": ""}
{"type": "source_file", "path": "core/engine/data_pipelines/stocks/template.py", "content": "import ast\nimport asyncio\n\nfrom langchain import PromptTemplate\n\nfrom base.base_datachain import AbstractDataChain\nfrom core.engine.data_pipelines.stocks.stock_pipelines import StockPipeline\nfrom core.engine.driver import ChainLLMModel\n\ntemplate = \"\"\"I would like you to give me python list object by understanding the context. You are a financial analyst, so you can classify the financial data request.\nI'll give a context (or question) that may include a stock data. Given a context, you'll give me a python list based on the rules. Don't answer anything except python list. Here are some rules.\n1. If there's no request on a stock data, just return an empty list, [].\n2. If there's a request on a stock data, you need to return a list of dictionaries, example: [{{\"function\":function_name, \"symbol\":company_symbol, \"description\":description}}].\n3. function_name are OVERVIEW BALANCE_SHEET INCOME_STATEMENT EARNINGS IPO_CALENDAR WTI BRENT REAL_GDP REAL_GDP_PER_CAPITA. \n4. function_name is the classification I want from you. Here are the options ( word before the colon is the function name, word after the colon is the description of the function):\n\nOVERVIEW : Company Overview, company information, financial ratios, and other key metrics for the equity specified. Data is generally refreshed on the same day a company reports its latest earnings and financials.\nBALANCE_SHEET :   annual and quarterly balance sheets for the company of interest, with normalized fields mapped to GAAP and IFRS taxonomies of the SEC. Data is generally refreshed on the same day a company reports its latest earnings and financials.\nINCOME_STATEMENT :  the annual and quarterly income statements for the company of interest, with normalized fields mapped to GAAP and IFRS taxonomies of the SEC. Data is generally refreshed on the same day a company reports its latest earnings and financials.\nEARNINGS :  the annual and quarterly earnings (EPS) for the company of interest. Quarterly data also includes analyst estimates and surprise metrics.\nIPO_CALENDAR :  a list of IPOs expected in the next 3 months.\nWTI : Crude Oil Prices: West Texas Intermediate (WTI) :  the West Texas Intermediate (WTI) crude oil prices in daily, weekly, and monthly horizons. Source is  U.S. Energy Information Administration, Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma, retrieved from FRED, Federal Reserve Bank of St. Louis. This data feed uses the FRED® API but is not endorsed or certified by the Federal Reserve Bank of St. Louis. \nBRENT :  the Brent (Europe) crude oil prices in daily, weekly, and monthly horizons. Source is U.S. Energy Information Administration, Crude Oil Prices: Brent - Europe, retrieved from FRED, Federal Reserve Bank of St. Louis.\nREAL_GDP : the annual and quarterly Real GDP of the United States. Source: U.S. Bureau of Economic Analysis, Real Gross Domestic Product, retrieved from FRED, Federal Reserve Bank of St. Louis. \nREAL_GDP_PER_CAPITA : the quarterly Real GDP per Capita data of the United States.\n\n\n5. If in the context, either there's a request to the data, or you think if the datas mentioned in 3 will be beneficial, you need to return a dictionary similar to [{{\"function\":function_name, \"symbol\":company_symbol}}]. If there's no request, you need to return an empty list, [].\nOne example, if the context is \"I want to see the balance sheet of TSLA\", you need to return [{{\"function\":\"BALANCE_SHEET\", \"symbol\":\"TSLA\", \"description\":\"This data is ..., and it can be helpful on ...\"}}]. However, if there are multiple requests, you need to return a list of dictionaries. For example, if the context is \"I want to see the balance sheet of TSLA and the income statement of AAPL\", you need to return [{{\"function\":\"BALANCE_SHEET\", \"symbol\":\"TSLA\", \"description\":\"This data is ..., and it can be helpful on ...\"}}, {{\"function\":\"INCOME_STATEMENT\", \"symbol\":\"AAPL\",\"description\":\"This data is ..., and it can be helpful on ...\"}}].\n\nDont ever give any comment! Just give me the python list based on the rules! If you can't find anything, just return an empty list, [] !\nYou are only allowed to respond either [] or the Python list of dictionaries based on the rules! DONT ADD ANY COMMENT! JUST GIVE ME THE PYTHON LIST BASED ON THE RULES!\nDirect answers only. No AI narrator or voice, AI is silent. No AI introduction. No AI summary. No disclaimers or warnings or usage advisories.\n\nHere's the context : {context}\n\"\"\"\n\n\nPROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"context\"],\n    template=template,\n)\n\n\nclass StockDataChain(AbstractDataChain):\n    \"\"\"\n    Class that allows RAG to retrieve online data.\n\n    \"\"\"\n\n    def __init__(\n        self, model: ChainLLMModel, prompt_template: PromptTemplate = PROMPT_TEMPLATE\n    ) -> None:\n        self.model = model\n        self.prompt_template = prompt_template\n        self.stock_pipeline = StockPipeline()\n\n    def chat(self, context):\n        return self.model.nonasync_chat(context, prompt_template=self.prompt_template)\n\n    async def async_chat(self, context):\n        return await self.model.async_chat(\n            context, prompt_template=self.prompt_template\n        )\n\n    def get_data(self, context, return_augmented_prompt=True):\n        response = self.chat(context)\n        stock_pipeline_parameters = ast.literal_eval(response)\n        print(\"searching this stock: \", stock_pipeline_parameters)\n        data = list()\n\n        for parameter in stock_pipeline_parameters[:1]:\n            function = parameter[\"function\"]\n            symbol = parameter[\"symbol\"]\n            description = (\n                parameter[\"description\"] if \"description\" in parameter else None\n            )\n\n            response = self.stock_pipeline.query(function=function, symbol=symbol)\n            data.append(\n                {\n                    \"function\": function,\n                    \"symbol\": symbol,\n                    \"description\": description,\n                    \"response\": response,\n                }\n            )\n\n        if return_augmented_prompt:\n\n            augmented_prompt = \"\"\"Here's the stock data I found for you. Please cite the resources with links and dates if it's given. Those data are provided from Alphavantage : \"\"\"\n\n            for stock in data:\n                augmented_prompt += f\"\"\"I found the stock data below because {stock[\"description\"]} . Those stock data are : \\n\"\"\"\n                augmented_prompt += f\"\"\"{str(stock[\"response\"])}\\n\"\"\"\n            return augmented_prompt\n\n        return data\n\n    async def aget_data(self, context, return_augmented_prompt=True):\n        response = await self.async_chat(context)\n        stock_pipeline_parameters = ast.literal_eval(response)\n        print(\"searching this stock: \", stock_pipeline_parameters)\n        tasks = []\n\n        for parameter in stock_pipeline_parameters:\n            task = self.process_parameter(parameter)  # Keep the same\n            tasks.append(task)  # Add await here\n\n        results = await asyncio.gather(*tasks)\n\n        if return_augmented_prompt:\n            augmented_prompt = \"\"\"Here's the stock data I found for you. Please cite the resources with links and dates if it's given. Those data are provided from Alphavantage : \"\"\"\n\n            for stock in results:\n                augmented_prompt += f\"\"\"I found the stock data below because {stock[\"description\"]} . Those stock data are : \\n\"\"\"\n                augmented_prompt += f\"\"\"{str(stock[\"response\"])}\\n\"\"\"\n            return augmented_prompt\n\n        return results\n\n    async def process_parameter(self, parameter):\n        function = parameter[\"function\"]\n        symbol = parameter[\"symbol\"]\n        description = parameter[\"description\"]\n\n        response = await self.stock_pipeline.aquery(function=function, symbol=symbol)\n\n        return {\n            \"function\": function,\n            \"symbol\": symbol,\n            \"description\": description,\n            \"response\": response,\n        }\n"}
{"type": "source_file", "path": "core/engine/data_pipelines/summarizer/template.py", "content": "import ast\n\nfrom langchain import PromptTemplate\n\nfrom base.base_datachain import AbstractDataChain\nfrom core.engine.driver import ChainLLMModel\n\ntemplate = \"\"\"Please summarize the given context to roughly {word_count} words. Please only give me the summary, no other comments. Dont ever answer any comment.\nContext : {context}\n\n\"\"\"\n# 5. If there's a date you want to search for, you can return the date, such as 2021-10-10. The date and time of the oldest article you want to get. If no date, don't place from to the dictionary.\n\n\nPROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"word_count\", \"context\"],\n    template=template,\n)\n\n\nclass SummarizerChain(AbstractDataChain):\n    \"\"\"\n    Class that allows RAG to retrieve online data.\n\n    \"\"\"\n\n    def __init__(\n        self, model: ChainLLMModel, prompt_template: PromptTemplate = PROMPT_TEMPLATE\n    ) -> None:\n        self.model = model\n        self.prompt_template = prompt_template\n\n    def chat(self, context):\n        return self.model.nonasync_chat(context, prompt_template=self.prompt_template)\n\n    async def async_chat(self, context):\n        return await self.model.async_chat(\n            context, prompt_template=self.prompt_template\n        )\n\n    def get_data(self, context, return_augmented_prompt=True, word_count: int = 50):\n        print(\"summarizing\")\n        return self.chat({\"word_count\": word_count, \"context\": context})\n\n    async def aget_data(\n        self, context, return_augmented_prompt=True, word_count: int = 50\n    ):\n        print(\"summarizing\")\n        return await self.async_chat({\"word_count\": word_count, \"context\": context})\n"}
{"type": "source_file", "path": "database/mongo/client.py", "content": "from mongoengine import connect\nfrom config.config import MongoConfig\nimport models\n\n\n# User DB\ndef connect_mongodb():\n    connect(host=MongoConfig.DATABASE_URL)\n"}
{"type": "source_file", "path": "database/vector_db.py", "content": "import os\n\nimport chromadb\nfrom langchain_community.vectorstores import Chroma\n\nfrom core.engine.embeddings.hf_embeddings import embedding\n\nchromadb_path = os.getenv(\n    \"LOCAL_CHROMADB_PATH\"\n)  ## NOTE : This is for persistend db storage. Clients with serving to be added\n\nchromadb_path = os.path.join(os.getcwd(), chromadb_path)\nchromadb_client = chromadb.PersistentClient(\n    path=chromadb_path,\n)\n\n\nchromadb_langchain_client = Chroma(\n    embedding_function=embedding,\n    client=chromadb_client,\n)\n"}
{"type": "source_file", "path": "models/conversations/conversations.py", "content": "from typing import Optional, Any, List\n\nfrom mongoengine import ListField, StringField, Document\n\n\nclass Conversations(Document):\n    conversation_id = StringField(max_length=200, required=True)\n    user_id = StringField(max_length=200, required=True)\n    message_content = ListField(required=False)\n\n    meta = {\"collection\": \"conversations\"}\n"}
{"type": "source_file", "path": "logging_stack/__init__.py", "content": "from .logging_stack import logger\n"}
{"type": "source_file", "path": "logging_stack/logging_stack.py", "content": "import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[logging.FileHandler(\"./assets/app.log\"), logging.StreamHandler()],\n)\n\n\nlogger = logging.getLogger()\n"}
{"type": "source_file", "path": "frontend/streamlit_app.py", "content": "import requests\nimport streamlit as st\nfrom PIL import Image\n\n\ndef send_pdf_to_fastapi(pdf_file):\n    url = \"http://localhost:8080/admin/upload_pdf/\"\n    files = {\"pdf_file\": pdf_file}\n    response = requests.post(url, files=files)\n    return response.json()\n\n\n# Function to send query to chatbot endpoint\ndef send_query_to_chatbot(pdf_link, query):\n    url = \"http://localhost:8080/admin/chat/\"\n    params = {\"pdf_link\": pdf_link, \"query\": query}\n    return requests.get(url, params=params, stream=True)\n\n\n# Define function to interact with backend API\ndef make_api_request():\n    # Code to interact with your backend API goes here\n    # Replace this with actual API call\n    return \"API request made successfully\"\n\n\n# Set up layout\nst.sidebar.title(\"Conversations\")\n# Display conversation list\nst.sidebar.text_area(\"Conversation List\", [], height=400, key=\"conversation_list\")\n\nst.title(\"Chatbot App\")\n\n# Upload PDF file\npdf_file = st.file_uploader(\"Upload PDF\", type=[\"pdf\"])\n\n\n# UI for sending PDF to FastAPI\npdf_button = st.button(\"Send PDF to FastAPI\", key=\"pdfsend\")\nif pdf_button and pdf_file:\n\n    response = send_pdf_to_fastapi(pdf_file)\n    uploaded_filename = response[\"filename\"]\n\n    st.session_state[\"uploaded_filename\"] = uploaded_filename\n\n    st.write(\"Response:\", response)\nelif pdf_button and not pdf_file:\n    st.warning(\"Please upload a PDF file before sending.\")\n\n\n# Chatbot section\nst.write(\"## Chatbot\")\ninput_text = st.text_input(\"Enter your message:\", key=\"chatbottext\")\nif st.button(\"Send\"):\n    if input_text:\n        response = send_query_to_chatbot(\n            st.session_state[\"uploaded_filename\"], input_text\n        )\n        # conversation_list += \"\\nYou: \" + input_text + \"\\nBot: \" + response + \"\\n\"\n        st.write(response.content.decode(\"utf-8\"))\n\n    else:\n        st.warning(\"Please enter a message.\")\n"}
{"type": "source_file", "path": "frontend/streamlit2_app.py", "content": "import os\nimport tempfile\nfrom pathlib import Path\n\nimport requests\nimport streamlit as st\nfrom PIL import Image\n\nTMP_DIR = Path(__file__).resolve().parent.joinpath(\"data\", \"tmp\")\nLOCAL_VECTOR_STORE_DIR = (\n    Path(__file__).resolve().parent.joinpath(\"data\", \"vector_store\")\n)\n\nst.set_page_config(page_title=\"RAG\")\nst.title(\"Retrieval Augmented Generation Engine\")\n\n\ndef send_pdf_to_fastapi(pdf_file):\n    url = \"http://localhost:8080/admin/upload_pdf/\"\n    files = {\"pdf_file\": pdf_file}\n    response = requests.post(url, files=files)\n    return response.json()\n\n\n# Function to send query to chatbot endpoint\ndef send_query_to_chatbot(pdf_link, query):\n    url = \"http://localhost:8080/admin/chat/\"\n    params = {\"pdf_link\": pdf_link, \"query\": query}\n    return requests.get(url, params=params, stream=True)\n\n\n# Define function to interact with backend API\ndef make_api_request():\n    # Code to interact with your backend API goes here\n    # Replace this with actual API call\n    return \"API request made successfully\"\n\n\ndef process_documents():\n\n    if not st.session_state.get(\"pdf_file\"):\n        st.warning(f\"Please upload the documents and provide the missing fields.\")\n    else:\n        try:\n            response = send_pdf_to_fastapi(st.session_state[\"pdf_file\"])\n\n            uploaded_filename = response[\"filename\"]\n            st.session_state[\"uploaded_filename\"] = uploaded_filename\n            st.write(\"Financial data loaded, you can talk with it!\")\n        except Exception as e:\n            st.error(f\"An error occurred: {e}\")\n\n\ndef boot():\n    #\n\n    if \"messages\" not in st.session_state:\n        st.session_state[\"messages\"] = list()\n    st.session_state[\"pdf_file\"] = st.file_uploader(\n        label=\"Upload Documents\", type=\"pdf\", accept_multiple_files=False\n    )\n    #\n    st.button(\"Submit Documents\", on_click=process_documents)\n    #\n    #\n    for message in st.session_state.messages:\n        print(message[0], message[1])\n        st.chat_message(\"human\").write(message[0])\n        st.chat_message(\"ai\").write(message[1])\n\n    if query := st.chat_input():\n        st.chat_message(\"human\").write(query)\n\n        response = send_query_to_chatbot(\n            st.session_state[\"uploaded_filename\"], query=query\n        ).content.decode(\"utf-8\")\n        # conversation_list += \"\\nYou: \" + input_text + \"\\nBot: \" + response + \"\\n\"\n\n        st.chat_message(\"ai\").write(response)\n        st.session_state.messages.append((query, response))\n\n\nif __name__ == \"__main__\":\n    #\n    boot()\n"}
{"type": "source_file", "path": "frontend/chainlit_app.py", "content": "import io\nimport os\nimport sys\nfrom typing import Optional\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n\nimport chainlit as cl\n\nfrom core.engine.driver import ModelDriver\n\nmodel_driver = ModelDriver()\nmodel_driver.set_pdf_loader(\"PyPDFLoader\")\nmodel_driver.load_model(\"mistral-api\")\n\n\n@cl.password_auth_callback\ndef auth_callback(username: str, password: str):\n    # Fetch the user matching username from your database\n    # and compare the hashed password with the value stored in the database\n    if (username, password) == (\"burak\", \"sean\"):  # temporary solution\n        return cl.User(\n            identifier=\"admin\", metadata={\"role\": \"admin\", \"provider\": \"credentials\"}\n        )\n    else:\n        return None\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n\n    # Sending an image with the local file path\n    await cl.Message(\n        content=\"Hello there, Welcome to AskAnyQuery related to Data!\"\n    ).send()\n\n    files = None\n\n    # Waits for user to upload csv data\n    while files is None:\n        files = await cl.AskFileMessage(\n            content=\"Please upload a pdf file to begin!\",\n            accept=[\"application/pdf\"],\n            max_size_mb=100,\n        ).send()\n\n    # load the csv data and store in user_session\n    file = files[0]\n    # creating user session to store data\n\n    cl.user_session.set(\"data\", file.path)\n    model_driver.load_document(file.path)\n    cl.user_session.set(\"chain\", model_driver.get_chain())\n\n    # Send response back to user\n    await cl.Message(\n        content=f\"`{file.name}` uploaded! Now you ask me anything related to your data\"\n    ).send()\n\n\n@cl.on_message\nasync def main(message: str):\n\n    \"\"\"cb = cl.AsyncLangchainCallbackHandler(\n    stream_final_answer=True, answer_prefix_tokens=[\"FINAL\",\"ANSWER\"]\n    )\n    \"\"\"\n    msg = cl.Message(content=\"\")\n\n    await msg.send()\n\n    # Send a response back to the user\n    # async for chunk in model_driver.chat_chain_injected(message,qa_chain=cl.user_session.get('chain')):\n    #    await msg.stream_token(chunk)\n\n    chain = cl.user_session.get(\"chain\")\n    async for chunk in chain.astream({\"query\": message.content}):\n        await msg.stream_token(str(chunk[\"result\"]))\n\n    await msg.update()\n"}
{"type": "source_file", "path": "models/admin.py", "content": "from beanie import Document\nfrom fastapi.security import HTTPBasicCredentials\nfrom pydantic import BaseModel, EmailStr\n\n\nclass Admin(Document):\n    fullname: str\n    email: EmailStr\n    password: str\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"fullname\": \"Abdulazeez Abdulazeez Adeshina\",\n                \"email\": \"abdul@youngest.dev\",\n                \"password\": \"3xt3m#\",\n            }\n        }\n\n    class Settings:\n        name = \"admin\"\n\n\nclass AdminSignIn(HTTPBasicCredentials):\n    class Config:\n        json_schema_extra = {\n            \"example\": {\"username\": \"abdul@youngest.dev\", \"password\": \"3xt3m#\"}\n        }\n\n\nclass AdminData(BaseModel):\n    fullname: str\n    email: EmailStr\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"fullname\": \"Abdulazeez Abdulazeez Adeshina\",\n                \"email\": \"abdul@youngest.dev\",\n            }\n        }\n"}
{"type": "source_file", "path": "main.py", "content": "import uvicorn\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8080, reload=False)\n"}
{"type": "source_file", "path": "models/users/users.py", "content": "from typing import Optional, Any\n\nfrom mongoengine import ListField, StringField, Document, ObjectIdField\nfrom bson.objectid import ObjectId\n\n\nclass Users(Document):\n    # user_id =  ObjectIdField(required=True, default=ObjectId,\n    #                unique=True, primary_key=True)\n\n    user_name = StringField(max_length=200, required=True)\n    user_password = StringField(\n        max_length=200, required=False\n    )  # those are all prototype right now.\n\n    meta = {\"collection\": \"users\"}\n"}
{"type": "source_file", "path": "models/student.py", "content": "from typing import Optional, Any\n\nfrom beanie import Document\nfrom pydantic import BaseModel, EmailStr\n\n\nclass Student(Document):\n    fullname: str\n    email: EmailStr\n    course_of_study: str\n    year: int\n    gpa: float\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"fullname\": \"Abdulazeez Abdulazeez Adeshina\",\n                \"email\": \"abdul@school.com\",\n                \"course_of_study\": \"Water resources engineering\",\n                \"year\": 4,\n                \"gpa\": \"3.76\",\n            }\n        }\n\n    class Settings:\n        name = \"student\"\n"}
{"type": "source_file", "path": "inmemcache/redis_client.py", "content": "import redis\nimport json\n\nfrom core.config import RedisConfig\n\n\nclass RedisClient:\n    def __init__(self, host=\"localhost\", port=6379, db=0, max_connections=10):\n        self.redis_pool = redis.ConnectionPool(\n            host=RedisConfig.REDIS_HOST,\n            port=RedisConfig.REDIS_PORT,\n            db=0,\n            max_connections=20,\n        )\n\n    def get_connection(self):\n        return redis.Redis(connection_pool=self.redis_pool)\n\n\nclass RedisManager:\n    def __init__(self, redis_client):\n        self.redis_client = RedisClient()\n\n    def save_data(self, key, data):\n        try:\n            conn = self.redis_client.get_connection()\n            conn.set(key, json.dumps(data))\n        except redis.RedisError as e:\n            # Handle Redis errors gracefully\n            print(f\"Error saving data to Redis: {e}\")\n\n    def load_data(self, key):\n        try:\n            conn = self.redis_client.get_connection()\n            data = conn.get(key)\n            if data:\n                return json.loads(data)\n            else:\n                return None\n        except redis.RedisError as e:\n            # Handle Redis errors gracefully\n            print(f\"Error loading data from Redis: {e}\")\n"}
{"type": "source_file", "path": "models/__init__.py", "content": "from models.conversations.conversations import Conversations\nfrom models.users.users import Users\n\n# word after models, defines db name. collection names are defined in inner class of document models, see the class inside User class\n\n# __all__ = [\"Users\", \"Conversations\"]\n"}
{"type": "source_file", "path": "database/mongo/db.py", "content": "\"\"\"Mongodb functions to crud data from\"\"\"\nfrom models import Users, Conversations\n\n\nclass DBUtils:\n    def write_chat(chat_history, conversation_id, user_id):\n        Conversations(\n            chat_history=chat_history, conversation_id=conversation_id, user_id=user_id\n        ).save()\n\n    def read_check(conversation_id):\n        return Conversations.objects(conversation_id=conversation_id)\n\n    def write_user(user_name, user_password):\n        Users(\n            user_name=user_name,\n            user_password=user_password,\n        ).save()\n\n    def read_user(user_name):\n        return Users.objects(user_name=user_name)\n"}
{"type": "source_file", "path": "core/engine/vectorstore/vectorstore_chroma.py", "content": "from langchain.vectorstores import Chroma\n"}
