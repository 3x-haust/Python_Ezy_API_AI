{"repo_info": {"repo_name": "InsightFlow", "repo_owner": "ilissrk", "repo_url": "https://github.com/ilissrk/InsightFlow"}}
{"type": "test_file", "path": "tests/test_analytics_engine.py", "content": "import pytest\nimport pandas as pd\nimport numpy as np\nfrom app.analytics.engine import AnalyticsEngine\n\n@pytest.fixture\ndef analytics_engine():\n    return AnalyticsEngine()\n\n@pytest.fixture\ndef sample_data():\n    return pd.DataFrame({\n        'value': [1, 2, 3, 4, 5, 100],  # Last value is an anomaly\n        'timestamp': pd.date_range(start='2024-01-01', periods=6, freq='H')\n    })\n\nasync def test_analyze(analytics_engine, sample_data):\n    metrics = ['count', 'average', 'sum']\n    results = await analytics_engine.analyze(sample_data['value'], metrics)\n    \n    assert results['count'] == 6\n    assert results['average'] == pytest.approx(19.17, rel=1e-2)\n    assert results['sum'] == 115\n\nasync def test_detect_anomalies(analytics_engine, sample_data):\n    anomalies = await analytics_engine.detect_anomalies(sample_data, 'value')\n    assert len(anomalies) == 1\n    assert anomalies.iloc[0]['value'] == 100\n\nasync def test_generate_insights(analytics_engine, sample_data):\n    insights = await analytics_engine.generate_insights(sample_data['value'])\n    assert len(insights) > 0\n    assert 'type' in insights[0]\n    assert 'description' in insights[0]"}
{"type": "test_file", "path": "tests/test_data_processor.py", "content": "import pytest\nfrom app.data.processors import DataProcessor\nfrom app.models.schema import DataSource, SourceType\n\n@pytest.fixture\ndef data_processor():\n    return DataProcessor()\n\n@pytest.fixture\ndef sample_source():\n    return DataSource(\n        name=\"test_source\",\n        type=SourceType.STREAM,\n        config={},\n        schema={\n            \"timestamp\": \"datetime\",\n            \"value\": \"float\"\n        }\n    )\n\nasync def test_process_stream_data(data_processor, sample_source):\n    data = {\n        \"timestamp\": \"2024-01-01T00:00:00\",\n        \"value\": 42.0\n    }\n    \n    processed_data = await data_processor.process_data(data, sample_source)\n    assert processed_data[\"value\"] == 42.0\n\nasync def test_invalid_data(data_processor, sample_source):\n    invalid_data = {\n        \"timestamp\": \"2024-01-01T00:00:00\"\n        # Missing required 'value' field\n    }\n    \n    with pytest.raises(ValueError):\n        await data_processor.process_data(invalid_data, sample_source)"}
{"type": "source_file", "path": "app/ai/nlp_processor.py", "content": "from typing import Dict, List, Optional\nimport re\nfrom datetime import datetime\n\nclass NLPProcessor:\n    def __init__(self):\n        self.query_patterns = {\n            \"time_range\": r\"(last|past|previous)\\s+(\\d+)\\s+(hour|day|week|month|year)s?\",\n            \"metrics\": r\"(count|average|sum|maximum|minimum|trend)\\s+of\\s+(\\w+)\",\n            \"filters\": r\"where\\s+(\\w+)\\s*(=|>|<|>=|<=)\\s*(\\w+)\",\n        }\n\n    async def parse_query(self, query: str) -> Dict:\n        \"\"\"Parse natural language query into structured format\"\"\"\n        try:\n            return {\n                \"time_range\": self._extract_time_range(query),\n                \"metrics\": self._extract_metrics(query),\n                \"filters\": self._extract_filters(query),\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n        except Exception as e:\n            raise Exception(f\"Error parsing query: {str(e)}\")\n\n    def _extract_time_range(self, query: str) -> Optional[Dict]:\n        \"\"\"Extract time range from query\"\"\"\n        match = re.search(self.query_patterns[\"time_range\"], query, re.IGNORECASE)\n        if match:\n            quantity = int(match.group(2))\n            unit = match.group(3)\n            return {\"unit\": unit, \"quantity\": quantity}\n        return None\n\n    def _extract_metrics(self, query: str) -> List[Dict]:\n        \"\"\"Extract metrics from query\"\"\"\n        metrics = []\n        for match in re.finditer(self.query_patterns[\"metrics\"], query, re.IGNORECASE):\n            metrics.append({\n                \"operation\": match.group(1),\n                \"field\": match.group(2)\n            })\n        return metrics\n\n    def _extract_filters(self, query: str) -> List[Dict]:\n        \"\"\"Extract filters from query\"\"\"\n        filters = []\n        for match in re.finditer(self.query_patterns[\"filters\"], query, re.IGNORECASE):\n            filters.append({\n                \"field\": match.group(1),\n                \"operator\": match.group(2),\n                \"value\": match.group(3)\n            })\n        return filters\n\n    async def format_response(self, data: Dict, query_context: Dict) -> str:\n        \"\"\"Format analytical results into natural language response\"\"\"\n        try:\n            # Add response formatting logic here\n            return \"Formatted response based on analysis results\"\n        except Exception as e:\n            raise Exception(f\"Error formatting response: {str(e)}\")"}
{"type": "source_file", "path": "app/analytics/engine.py", "content": "import logging\nfrom typing import Dict, Any, List\nimport pandas as pd\nimport numpy as np\nfrom ..data.storage import DataStorage\nfrom ..models.schema import AnalyticsConfig\n\nlogger = logging.getLogger(__name__)\n\nclass AnalyticsEngine:\n    def __init__(self):\n        self.storage = DataStorage()\n        self.config = AnalyticsConfig()\n        self._metrics = self._initialize_metrics()\n\n    def _initialize_metrics(self) -> Dict[str, callable]:\n        \"\"\"Initialize available metrics\"\"\"\n        return {\n            \"count\": len,\n            \"sum\": np.sum,\n            \"average\": np.mean,\n            \"min\": np.min,\n            \"max\": np.max,\n            \"std\": np.std,\n            \"variance\": np.var\n        }\n\n    async def analyze(self, data: pd.DataFrame, metrics: List[str]) -> Dict[str, Any]:\n        \"\"\"Perform analysis on data\"\"\"\n        try:\n            results = {}\n            for metric in metrics:\n                if metric not in self._metrics:\n                    raise ValueError(f\"Unknown metric: {metric}\")\n                results[metric] = self._metrics[metric](data)\n            return results\n        except Exception as e:\n            logger.error(f\"Error during analysis: {str(e)}\")\n            raise\n\n    async def detect_anomalies(self, data: pd.DataFrame, column: str) -> pd.DataFrame:\n        \"\"\"Detect anomalies in data\"\"\"\n        try:\n            mean = data[column].mean()\n            std = data[column].std()\n            threshold = 3  # Standard deviations\n            \n            anomalies = data[abs(data[column] - mean) > threshold * std]\n            return anomalies\n        except Exception as e:\n            logger.error(f\"Error detecting anomalies: {str(e)}\")\n            raise\n\n    async def generate_insights(self, data: pd.DataFrame) -> List[Dict[str, Any]]:\n        \"\"\"Generate insights from data\"\"\"\n        insights = []\n        try:\n            # Basic statistics\n            stats = data.describe()\n            \n            # Trend analysis\n            if len(data) > 1:\n                trend = np.polyfit(range(len(data)), data.values, 1)[0]\n                insights.append({\n                    \"type\": \"trend\",\n                    \"description\": \"increasing\" if trend > 0 else \"decreasing\",\n                    \"value\": float(trend)\n                })\n\n            # Add more insight generation logic here\n            \n            return insights\n        except Exception as e:\n            logger.error(f\"Error generating insights: {str(e)}\")\n            raise\n"}
{"type": "source_file", "path": "app/ai/claude_connector.py", "content": "from typing import Dict, Optional, List\nimport anthropic\nfrom pydantic import BaseModel\nimport json\nimport asyncio\n\nclass ClaudeConnector:\n    def __init__(self, api_key: str):\n        self.client = anthropic.Client(api_key)\n        self.system_prompt = \"\"\"You are an AI analytics assistant for InsightFlow.\n        Your role is to help analyze data, generate insights, and answer queries.\n        Use the available tools and data to provide accurate and helpful responses.\"\"\"\n\n    async def generate_insight(self, data: Dict, context: Optional[Dict] = None) -> Dict:\n        \"\"\"Generate insights from provided data using Claude\"\"\"\n        try:\n            prompt = self._build_insight_prompt(data, context)\n            response = await self._get_claude_response(prompt)\n            return self._parse_insight_response(response)\n        except Exception as e:\n            raise Exception(f\"Error generating insight: {str(e)}\")\n\n    async def process_query(self, query: str, context: Optional[Dict] = None) -> Dict:\n        \"\"\"Process natural language queries using Claude\"\"\"\n        try:\n            prompt = self._build_query_prompt(query, context)\n            response = await self._get_claude_response(prompt)\n            return self._parse_query_response(response)\n        except Exception as e:\n            raise Exception(f\"Error processing query: {str(e)}\")\n\n    async def _get_claude_response(self, prompt: str) -> str:\n        \"\"\"Get response from Claude API\"\"\"\n        try:\n            response = await self.client.messages.create(\n                model=\"claude-2\",\n                max_tokens=1000,\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n            return response.content[0].text\n        except Exception as e:\n            raise Exception(f\"Claude API error: {str(e)}\")\n\n    def _build_insight_prompt(self, data: Dict, context: Optional[Dict] = None) -> str:\n        \"\"\"Build prompt for insight generation\"\"\"\n        prompt = \"Analyze the following data and generate insights:\\n\\n\"\n        prompt += json.dumps(data, indent=2)\n        if context:\n            prompt += \"\\n\\nContext:\\n\" + json.dumps(context, indent=2)\n        return prompt\n\n    def _build_query_prompt(self, query: str, context: Optional[Dict] = None) -> str:\n        \"\"\"Build prompt for query processing\"\"\"\n        prompt = f\"Process the following analytics query:\\n{query}\\n\\n\"\n        if context:\n            prompt += \"Context:\\n\" + json.dumps(context, indent=2)\n        return prompt\n\n    def _parse_insight_response(self, response: str) -> Dict:\n        \"\"\"Parse and structure Claude's insight response\"\"\"\n        try:\n            # Add response parsing logic here\n            return {\n                \"insights\": response,\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n        except Exception as e:\n            raise Exception(f\"Error parsing insight response: {str(e)}\")\n\n    def _parse_query_response(self, response: str) -> Dict:\n        \"\"\"Parse and structure Claude's query response\"\"\"\n        try:\n            # Add response parsing logic here\n            return {\n                \"response\": response,\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n        except Exception as e:\n            raise Exception(f\"Error parsing query response: {str(e)}\")"}
{"type": "source_file", "path": "app/api/websocket.py", "content": "from fastapi import WebSocket, WebSocketDisconnect, Depends\nfrom typing import Dict, Optional\nimport json\nimport asyncio\nfrom uuid import uuid4\n\nfrom ..core.mcp_server import MCPServer\nfrom ..core.client_manager import ClientManager\n\nclass WebSocketAPI:\n    def __init__(self, mcp_server: MCPServer, client_manager: ClientManager):\n        self.mcp_server = mcp_server\n        self.client_manager = client_manager\n\n    async def handle_websocket(self, websocket: WebSocket):\n        client_id = str(uuid4())\n        try:\n            await self.mcp_server.register_client(websocket, client_id)\n            \n            while True:\n                message = await websocket.receive_json()\n                await self._process_websocket_message(client_id, message)\n                \n        except WebSocketDisconnect:\n            await self.client_manager.disconnect(client_id)\n        except Exception as e:\n            await self.client_manager.disconnect(client_id)\n            raise e\n\n    async def _process_websocket_message(self, client_id: str, message: dict):\n        \"\"\"Process incoming WebSocket messages\"\"\"\n        try:\n            if message.get(\"type\") == \"subscribe\":\n                topics = message.get(\"topics\", [])\n                for topic in topics:\n                    await self.client_manager.subscribe(client_id, topic)\n                    \n            elif message.get(\"type\") == \"unsubscribe\":\n                topics = message.get(\"topics\", [])\n                for topic in topics:\n                    await self.client_manager.unsubscribe(client_id, topic)\n                    \n            elif message.get(\"type\") == \"query\":\n                response = await self.mcp_server.handle_client_message(\n                    client_id=client_id,\n                    message=message\n                )\n                await self.client_manager.send_message(client_id, response)\n                \n            else:\n                await self.mcp_server.handle_client_message(\n                    client_id=client_id,\n                    message=message\n                )\n                \n        except Exception as e:\n            error_message = {\n                \"type\": \"error\",\n                \"error\": str(e)\n            }\n            await self.client_manager.send_message(client_id, error_message)"}
{"type": "source_file", "path": "app/core/message_handler.py", "content": "import logging\nfrom typing import Dict, Any, Optional\nfrom ..models.schema import DataSource\nfrom .client_manager import ClientManager\n\nlogger = logging.getLogger(__name__)\n\nclass MessageHandler:\n    def __init__(self):\n        self.client_manager = ClientManager()\n        self._message_processors = {}\n\n    async def process_message(self, message: Dict[str, Any], source: DataSource) -> Optional[Dict[str, Any]]:\n        \"\"\"Process incoming messages from various sources\"\"\"\n        try:\n            message_type = message.get(\"type\", \"default\")\n            processor = self._message_processors.get(message_type)\n            \n            if processor:\n                processed_message = await processor(message)\n                await self._broadcast_to_clients(processed_message)\n                return processed_message\n            \n            logger.warning(f\"No processor found for message type: {message_type}\")\n            return None\n            \n        except Exception as e:\n            logger.error(f\"Error processing message: {str(e)}\")\n            raise\n\n    async def _broadcast_to_clients(self, message: Dict[str, Any]):\n        \"\"\"Broadcast processed messages to connected clients\"\"\"\n        await self.client_manager.broadcast(message)\n\n    def register_processor(self, message_type: str, processor_func):\n        \"\"\"Register a new message processor\"\"\"\n        self._message_processors[message_type] = processor_func\n"}
{"type": "source_file", "path": "app/config.py", "content": "import os\nimport yaml\nfrom typing import Dict, Any\nfrom pathlib import Path\nfrom .models.schema import InsightFlowConfig, ServerConfig, DatabaseConfig, AnalyticsConfig, AIModelConfig, LogConfig\n\nclass Config:\n    def __init__(self):\n        self._config: Dict[str, Any] = {}\n        self.load_config()\n\n    def load_config(self) -> None:\n        \"\"\"Load configuration from environment variables and config files\"\"\"\n        # Default config path\n        config_path = os.getenv(\"INSIGHTFLOW_CONFIG\", \"config/config.yaml\")\n        \n        # Load from file if exists\n        if Path(config_path).exists():\n            with open(config_path, 'r') as f:\n                self._config = yaml.safe_load(f)\n        \n        # Override with environment variables\n        self._config = self._override_from_env(self._config)\n        \n        # Validate and create config objects\n        self.config = InsightFlowConfig(\n            server=ServerConfig(\n                host=self._config.get(\"server\", {}).get(\"host\", \"0.0.0.0\"),\n                port=int(self._config.get(\"server\", {}).get(\"port\", 8000)),\n                debug=bool(self._config.get(\"server\", {}).get(\"debug\", False)),\n                workers=int(self._config.get(\"server\", {}).get(\"workers\", 4)),\n                request_timeout=int(self._config.get(\"server\", {}).get(\"request_timeout\", 30))\n            ),\n            database=DatabaseConfig(\n                url=os.getenv(\"DATABASE_URL\", self._config.get(\"database\", {}).get(\"url\", \"sqlite:///data.db\")),\n                pool_size=int(self._config.get(\"database\", {}).get(\"pool_size\", 5)),\n                max_overflow=int(self._config.get(\"database\", {}).get(\"max_overflow\", 10)),\n                timeout=int(self._config.get(\"database\", {}).get(\"timeout\", 30))\n            ),\n            analytics=AnalyticsConfig(\n                metrics=self._config.get(\"analytics\", {}).get(\"metrics\", [\"count\", \"average\", \"sum\"]),\n                interval=int(self._config.get(\"analytics\", {}).get(\"interval\", 60)),\n                batch_size=int(self._config.get(\"analytics\", {}).get(\"batch_size\", 1000)),\n                cache_ttl=int(self._config.get(\"analytics\", {}).get(\"cache_ttl\", 300))\n            ),\n            ai=AIModelConfig(\n                model_name=os.getenv(\"AI_MODEL_NAME\", self._config.get(\"ai\", {}).get(\"model_name\", \"claude-2\")),\n                api_key=os.getenv(\"CLAUDE_API_KEY\", self._config.get(\"ai\", {}).get(\"api_key\", \"\")),\n                temperature=float(self._config.get(\"ai\", {}).get(\"temperature\", 0.7)),\n                max_tokens=int(self._config.get(\"ai\", {}).get(\"max_tokens\", 2000)),\n                context_window=int(self._config.get(\"ai\", {}).get(\"context_window\", 4000))\n            ),\n            logging=LogConfig(\n                level=os.getenv(\"LOG_LEVEL\", self._config.get(\"logging\", {}).get(\"level\", \"INFO\")),\n                format=self._config.get(\"logging\", {}).get(\"format\", \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"),\n                file_path=os.getenv(\"LOG_FILE\", self._config.get(\"logging\", {}).get(\"file_path\"))\n            ),\n            data_sources=self._config.get(\"data_sources\", {})\n        )\n\n    def _override_from_env(self, config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Override configuration with environment variables\"\"\"\n        env_prefix = \"INSIGHTFLOW_\"\n        for key in os.environ:\n            if key.startswith(env_prefix):\n                config_key = key[len(env_prefix):].lower()\n                config[config_key] = os.environ[key]\n        return config\n\n    def get_config(self) -> InsightFlowConfig:\n        \"\"\"Get the current configuration\"\"\"\n        return self.config\n\nconfig = Config()"}
{"type": "source_file", "path": "app/core/mcp_server.py", "content": "import asyncio\nimport logging\nfrom typing import Dict, Any, List, Optional\nfrom anthropic import Anthropic\nfrom fastapi import WebSocket\nimport json\nfrom ..config import config\nfrom ..models.schema import AIModelConfig\n\nlogger = logging.getLogger(__name__)\n\nclass MCPServer:\n    def __init__(self):\n        self.ai_config: AIModelConfig = config.get_config().ai\n        self.anthropic_client = Anthropic(api_key=self.ai_config.api_key)\n        self.active_sessions: Dict[str, Any] = {}\n        self.tools = self._initialize_tools()\n        self.websocket_connections: List[WebSocket] = []\n\n    def _initialize_tools(self):\n        \"\"\"Initialize available MCP tools\"\"\"\n        return {\n            \"data_analysis\": {\n                \"name\": \"analyze_data\",\n                \"description\": \"Analyze data and generate insights\",\n                \"parameters\": {\n                    \"data\": \"object\",\n                    \"metrics\": \"array\",\n                    \"timeframe\": \"string\"\n                }\n            },\n            \"query_data\": {\n                \"name\": \"query_data\",\n                \"description\": \"Query historical data\",\n                \"parameters\": {\n                    \"query\": \"string\",\n                    \"filters\": \"object\",\n                    \"limit\": \"integer\"\n                }\n            }\n        }\n\n    async def initialize(self):\n        \"\"\"Initialize the MCP server\"\"\"\n        try:\n            # Initialize core components\n            self.data_processor = DataProcessor()\n            self.analytics_engine = AnalyticsEngine()\n            \n            # Register default tools\n            await self._register_default_tools()\n            \n            return True\n        except Exception as e:\n            logger.error(f\"Failed to initialize MCP server: {e}\")\n            raise\n\n    async def _register_default_tools(self):\n        \"\"\"Register default MCP tools\"\"\"\n        self.tools.update({\n            \"data_analysis\": {\n                \"name\": \"analyze_data\",\n                \"description\": \"Analyze data and generate insights\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"data_source\": {\"type\": \"string\"},\n                        \"metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"timeframe\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"data_source\", \"metrics\"]\n                }\n            },\n            \"query_data\": {\n                \"name\": \"query_data\",\n                \"description\": \"Query historical data\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"filters\": {\"type\": \"object\"},\n                        \"limit\": {\"type\": \"integer\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            },\n            \"generate_insight\": {\n                \"name\": \"generate_insight\",\n                \"description\": \"Generate AI-powered insights from data\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"data_source\": {\"type\": \"string\"},\n                        \"context\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"data_source\"]\n                }\n            }\n        })\n\n    async def handle_client_message(self, message: Dict[str, Any], client_id: str) -> Dict[str, Any]:\n        \"\"\"Handle incoming MCP client messages\"\"\"\n        try:\n            message_type = message.get(\"type\")\n            if message_type == \"tool_call\":\n                return await self.handle_tool_call(message[\"tool\"], message[\"parameters\"], client_id)\n            elif message_type == \"list_tools\":\n                return {\"tools\": self.tools}\n            else:\n                raise ValueError(f\"Unsupported message type: {message_type}\")\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    async def shutdown(self):\n        \"\"\"Gracefully shutdown the MCP server\"\"\"\n        try:\n            # Close all active WebSocket connections\n            for ws in self.websocket_connections:\n                await ws.close()\n            \n            # Cleanup resources\n            self.active_sessions.clear()\n            self.websocket_connections.clear()\n            \n            logger.info(\"MCP server shutdown complete\")\n        except Exception as e:\n            logger.error(f\"Error during MCP server shutdown: {e}\")\n            raise\n\n    async def _cleanup_session(self, session_id: str):\n        \"\"\"Clean up a specific session\"\"\"\n        if session_id in self.active_sessions:\n            try:\n                # Get session data before cleanup\n                session_data = self.active_sessions[session_id]\n                \n                # Perform any necessary cleanup tasks\n                if 'resources' in session_data:\n                    await self._release_resources(session_data['resources'])\n                \n                # Remove session from active sessions\n                del self.active_sessions[session_id]\n                \n                logger.info(f\"Successfully cleaned up session: {session_id}\")\n            except Exception as e:\n                logger.error(f\"Error cleaning up session {session_id}: {str(e)}\")\n                raise\n"}
{"type": "source_file", "path": "app/api/rest.py", "content": "from fastapi import APIRouter, HTTPException, Depends, WebSocket\nfrom typing import Dict, Any, List\nfrom ..models.schema import DataSource, AnalyticsConfig\nfrom ..core.mcp_server import MCPServer\n\nclass RestAPI:\n    def __init__(self, mcp_server: MCPServer, data_processor: DataProcessor, ai_connector: ClaudeConnector):\n        self.router = APIRouter()\n        self.mcp_server = mcp_server\n        self.data_processor = data_processor\n        self.ai_connector = ai_connector\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.router.get(\"/tools\")\n        async def list_tools():\n            \"\"\"List all available MCP tools\"\"\"\n            return {\"tools\": self.mcp_server.tools}\n\n        @self.router.post(\"/tool/{tool_name}\")\n        async def call_tool(tool_name: str, parameters: Dict[str, Any]):\n            \"\"\"Execute an MCP tool\"\"\"\n            try:\n                result = await self.mcp_server.handle_tool_call(tool_name, parameters, \"rest-api\")\n                return result\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=str(e))\n\n        @self.router.websocket(\"/ws\")\n        async def websocket_endpoint(websocket: WebSocket):\n            \"\"\"WebSocket endpoint for real-time MCP communication\"\"\"\n            await websocket.accept()\n            self.mcp_server.websocket_connections.append(websocket)\n            try:\n                while True:\n                    data = await websocket.receive_json()\n                    response = await self.mcp_server.handle_client_message(data, str(websocket))\n                    await websocket.send_json(response)\n            except Exception as e:\n                logger.error(f\"WebSocket error: {e}\")\n            finally:\n                self.mcp_server.websocket_connections.remove(websocket)\n"}
{"type": "source_file", "path": "app/core/client_manager.py", "content": "import logging\nfrom typing import Dict, Set, Any\nfrom fastapi import WebSocket\n\nlogger = logging.getLogger(__name__)\n\nclass ClientManager:\n    def __init__(self):\n        self.active_clients: Set[WebSocket] = set()\n        self.client_subscriptions: Dict[str, Set[WebSocket]] = {}\n\n    async def connect(self, websocket: WebSocket):\n        \"\"\"Connect a new client\"\"\"\n        await websocket.accept()\n        self.active_clients.add(websocket)\n        logger.info(f\"Client connected. Total clients: {len(self.active_clients)}\")\n\n    async def disconnect(self, websocket: WebSocket):\n        \"\"\"Disconnect a client\"\"\"\n        self.active_clients.remove(websocket)\n        for subscriptions in self.client_subscriptions.values():\n            subscriptions.discard(websocket)\n        logger.info(f\"Client disconnected. Total clients: {len(self.active_clients)}\")\n\n    async def subscribe(self, websocket: WebSocket, topic: str):\n        \"\"\"Subscribe a client to a topic\"\"\"\n        if topic not in self.client_subscriptions:\n            self.client_subscriptions[topic] = set()\n        self.client_subscriptions[topic].add(websocket)\n\n    async def broadcast(self, message: Dict[str, Any], topic: str = None):\n        \"\"\"Broadcast message to all clients or topic subscribers\"\"\"\n        disconnected_clients = set()\n\n        target_clients = (self.client_subscriptions.get(topic, set()) \n                        if topic else self.active_clients)\n\n        for client in target_clients:\n            try:\n                await client.send_json(message)\n            except Exception as e:\n                logger.error(f\"Error sending message to client: {str(e)}\")\n                disconnected_clients.add(client)\n\n        for client in disconnected_clients:\n            await self.disconnect(client)\n"}
{"type": "source_file", "path": "app/analytics/insights.py", "content": "from typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport asyncio\nfrom collections import defaultdict\n\nclass InsightGenerator:\n    def __init__(self):\n        self.patterns: Dict[str, Dict] = {}\n        self.thresholds: Dict[str, float] = {}\n        self.insight_cache: Dict[str, List[Dict]] = {}\n        self.metrics_history: Dict[str, List[float]] = defaultdict(list)\n\n    async def generate_insights(self, data: Dict, context: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"Generate insights from data\"\"\"\n        try:\n            insights = []\n            \n            # Pattern-based insights\n            pattern_insights = await self._detect_patterns(data)\n            insights.extend(pattern_insights)\n            \n            # Anomaly detection\n            anomalies = await self._detect_anomalies(data)\n            insights.extend(anomalies)\n            \n            # Trend analysis\n            trends = await self._analyze_trends(data)\n            insights.extend(trends)\n            \n            # Context-based insights\n            if context:\n                context_insights = await self._generate_context_insights(data, context)\n                insights.extend(context_insights)\n            \n            return insights\n        except Exception as e:\n            raise Exception(f\"Insight generation error: {str(e)}\")\n\n    async def _detect_patterns(self, data: Dict) -> List[Dict]:\n        \"\"\"Detect patterns in data\"\"\"\n        try:\n            detected_patterns = []\n            for pattern_name, pattern in self.patterns.items():\n                if self._match_pattern(data, pattern):\n                    detected_patterns.append({\n                        \"type\": \"pattern\",\n                        \"name\": pattern_name,\n                        \"confidence\": self._calculate_confidence(data, pattern),\n                        \"timestamp\": datetime.utcnow().isoformat()\n                    })\n            return detected_patterns\n        except Exception as e:\n            raise Exception(f\"Pattern detection error: {str(e)}\")\n\n    async def _detect_anomalies(self, data: Dict) -> List[Dict]:\n        \"\"\"Detect anomalies in data\"\"\"\n        try:\n            anomalies = []\n            for metric, values in self.metrics_history.items():\n                if metric in data:\n                    current_value = data[metric]\n                    if self._is_anomaly(current_value, values):\n                        anomalies.append({\n                            \"type\": \"anomaly\",\n                            \"metric\": metric,\n                            \"value\": current_value,\n                            \"threshold\": self.thresholds.get(metric, 0),\n                            \"timestamp\": datetime.utcnow().isoformat()\n                        })\n                    # Update history\n                    values.append(current_value)\n                    if len(values) > 1000:  # Keep last 1000 values\n                        values.pop(0)\n            return anomalies\n        except Exception as e:\n            raise Exception(f\"Anomaly detection error: {str(e)}\")\n\n    async def _analyze_trends(self, data: Dict) -> List[Dict]:\n        \"\"\"Analyze trends in data\"\"\"\n        try:\n            trends = []\n            for metric, values in self.metrics_history.items():\n                if metric in data:\n                    trend = self._calculate_trend(values)\n                    if trend:\n                        trends.append({\n                            \"type\": \"trend\",\n                            \"metric\": metric,\n                            \"direction\": trend[\"direction\"],\n                            \"magnitude\": trend[\"magnitude\"],\n                            \"timestamp\": datetime.utcnow().isoformat()\n                        })\n            return trends\n        except Exception as e:\n            raise Exception(f\"Trend analysis error: {str(e)}\")\n\n    async def _generate_context_insights(self, data: Dict, context: Dict) -> List[Dict]:\n        \"\"\"Generate context-based insights\"\"\"\n        try:\n            # Implement context-based insight generation\n            return []\n        except Exception as e:\n            raise Exception(f\"Context insight error: {str(e)}\")\n\n    def _match_pattern(self, data: Dict, pattern: Dict) -> bool:\n        \"\"\"Match data against a pattern\"\"\"\n        # Implement pattern matching logic\n        return False\n\n    def _calculate_confidence(self, data: Dict, pattern: Dict) -> float:\n        \"\"\"Calculate confidence score for a pattern match\"\"\"\n        # Implement confidence calculation logic\n        return 0.0\n\n    def _is_anomaly(self, value: float, history: List[float]) -> bool:\n        \"\"\"Check if a value is anomalous\"\"\"\n        if not history:\n            return False\n        mean = sum(history) / len(history)\n        std_dev = (sum((x - mean) ** 2 for x in history) / len(history)) ** 0.5\n        return abs(value - mean) > (std_dev * 3)  # 3 sigma rule\n\n    def _calculate_trend(self, values: List[float]) -> Optional[Dict]:\n        \"\"\"Calculate trend direction and magnitude\"\"\"\n        if len(values) < 2:\n            return None\n            \n        slope = (values[-1] - values[0]) / len(values)\n        magnitude = abs(slope)\n        \n        if magnitude < 0.01:  # Threshold for significant trend\n            return None\n            \n        return {\n            \"direction\": \"up\" if slope > 0 else \"down\",\n            \"magnitude\": magnitude\n        }\n\n    def register_pattern(self, name: str, pattern: Dict) -> None:\n        \"\"\"Register a new pattern\"\"\"\n        self.patterns[name] = pattern\n\n    def set_threshold(self, metric: str, threshold: float) -> None:\n        \"\"\"Set threshold for a metric\"\"\"\n        self.thresholds[metric] = threshold"}
{"type": "source_file", "path": "app/data/ingestion.py", "content": "from typing import Dict, List, Any, Optional\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nimport asyncio\nfrom pydantic import BaseModel\nfrom collections import deque\n\nclass DataSource(BaseModel):\n    name: str\n    type: str\n    config: Dict[str, Any]\n    schema: Dict[str, Any]\n\nclass DataValidator:\n    def __init__(self, schema: Dict[str, Any]):\n        self.schema = schema\n\n    def validate(self, data: Dict) -> bool:\n        try:\n            # Implement schema validation logic\n            return True\n        except Exception as e:\n            raise ValueError(f\"Validation error: {str(e)}\")\n\nclass DataSourceAdapter(ABC):\n    @abstractmethod\n    async def connect(self) -> None:\n        pass\n\n    @abstractmethod\n    async def read(self) -> Dict:\n        pass\n\n    @abstractmethod\n    async def disconnect(self) -> None:\n        pass\n\nclass StreamAdapter(DataSourceAdapter):\n    def __init__(self, config: Dict):\n        self.config = config\n        self.buffer = deque(maxlen=1000)\n        self.connected = False\n\n    async def connect(self) -> None:\n        # Implement stream connection logic\n        self.connected = True\n\n    async def read(self) -> Dict:\n        if not self.connected:\n            raise ConnectionError(\"Stream not connected\")\n        return self.buffer.popleft() if self.buffer else None\n\n    async def disconnect(self) -> None:\n        self.connected = False\n\nclass BatchAdapter(DataSourceAdapter):\n    def __init__(self, config: Dict):\n        self.config = config\n\n    async def connect(self) -> None:\n        # Implement batch connection logic\n        pass\n\n    async def read(self) -> Dict:\n        # Implement batch read logic\n        pass\n\n    async def disconnect(self) -> None:\n        # Implement disconnect logic\n        pass\n\nclass DataIngestion:\n    def __init__(self):\n        self.sources: Dict[str, DataSource] = {}\n        self.adapters: Dict[str, DataSourceAdapter] = {}\n        self.validators: Dict[str, DataValidator] = {}\n        self.rate_limits: Dict[str, float] = {}\n\n    async def register_source(self, source: DataSource) -> None:\n        \"\"\"Register a new data source\"\"\"\n        try:\n            self.sources[source.name] = source\n            self.validators[source.name] = DataValidator(source.schema)\n            \n            if source.type == \"stream\":\n                self.adapters[source.name] = StreamAdapter(source.config)\n            elif source.type == \"batch\":\n                self.adapters[source.name] = BatchAdapter(source.config)\n            else:\n                raise ValueError(f\"Unsupported source type: {source.type}\")\n            \n            await self.adapters[source.name].connect()\n        except Exception as e:\n            raise Exception(f\"Error registering source: {str(e)}\")\n\n    async def ingest_data(self, source_name: str, data: Dict) -> Dict:\n        \"\"\"Ingest data from a source\"\"\"\n        try:\n            if source_name not in self.sources:\n                raise ValueError(f\"Unknown source: {source_name}\")\n\n            if not self.validators[source_name].validate(data):\n                raise ValueError(\"Data validation failed\")\n\n            # Apply rate limiting\n            if source_name in self.rate_limits:\n                await asyncio.sleep(1 / self.rate_limits[source_name])\n\n            # Add metadata\n            enriched_data = {\n                \"source\": source_name,\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"data\": data\n            }\n\n            return enriched_data\n        except Exception as e:\n            raise Exception(f\"Ingestion error: {str(e)}\")\n\n    async def start_streaming(self, source_name: str) -> None:\n        \"\"\"Start streaming from a source\"\"\"\n        try:\n            if source_name not in self.sources:\n                raise ValueError(f\"Unknown source: {source_name}\")\n\n            adapter = self.adapters[source_name]\n            while True:\n                data = await adapter.read()\n                if data:\n                    await self.ingest_data(source_name, data)\n                await asyncio.sleep(0.1)\n        except Exception as e:\n            raise Exception(f\"Streaming error: {str(e)}\")\n\n    def set_rate_limit(self, source_name: str, rate: float) -> None:\n        \"\"\"Set rate limit for a source in requests per second\"\"\"\n        self.rate_limits[source_name] = rate\n\n    async def cleanup(self) -> None:\n        \"\"\"Cleanup and disconnect all sources\"\"\"\n        for adapter in self.adapters.values():\n            await adapter.disconnect()"}
{"type": "source_file", "path": "app/data/storage.py", "content": "import logging\nfrom typing import Dict, Any, Optional\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom ..config import config\n\nlogger = logging.getLogger(__name__)\n\nclass DataStorage:\n    def __init__(self, connection_string: Optional[str] = None):\n        self.connection_string = connection_string or config.get_config().database.url\n        self.engine = create_engine(self.connection_string)\n        self._initialize_storage()\n\n    def _initialize_storage(self):\n        \"\"\"Initialize storage backend\"\"\"\n        try:\n            # Create necessary tables\n            with self.engine.connect() as conn:\n                # Add table creation logic here\n                pass\n        except Exception as e:\n            logger.error(f\"Error initializing storage: {str(e)}\")\n            raise\n\n    async def store(self, data: Dict[str, Any], source_id: str) -> bool:\n        \"\"\"Store processed data\"\"\"\n        try:\n            df = pd.DataFrame([data])\n            table_name = f\"data_{source_id}\"\n            df.to_sql(table_name, self.engine, if_exists='append', index=False)\n            return True\n        except Exception as e:\n            logger.error(f\"Error storing data: {str(e)}\")\n            raise\n\n    async def query(self, query: str, params: Dict[str, Any] = None) -> pd.DataFrame:\n        \"\"\"Query stored data\"\"\"\n        try:\n            return pd.read_sql(query, self.engine, params=params)\n        except Exception as e:\n            logger.error(f\"Error querying data: {str(e)}\")\n            raise\n\n    async def get_latest(self, source_id: str, limit: int = 100) -> pd.DataFrame:\n        \"\"\"Get latest records for a source\"\"\"\n        query = f\"\"\"\n        SELECT * FROM data_{source_id}\n        ORDER BY timestamp DESC\n        LIMIT {limit}\n        \"\"\"\n        return await self.query(query)\n"}
{"type": "source_file", "path": "app/models/schema.py", "content": "from pydantic import BaseModel, Field\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass SourceType(str, Enum):\n    STREAM = \"stream\"\n    BATCH = \"batch\"\n    API = \"api\"\n    DATABASE = \"database\"\n\nclass DataSource(BaseModel):\n    name: str\n    type: SourceType\n    config: Dict[str, Any]\n    schema: Dict[str, Any]\n    enabled: bool = True\n\nclass AnalyticsConfig(BaseModel):\n    metrics: List[str]\n    interval: int = Field(default=60, description=\"Processing interval in seconds\")\n    batch_size: int = 1000\n    cache_ttl: int = 300\n\nclass AIModelConfig(BaseModel):\n    model_name: str\n    api_key: str\n    temperature: float = 0.7\n    max_tokens: int = 2000\n    context_window: int = 4000\n\nclass ServerConfig(BaseModel):\n    host: str\n    port: int\n    debug: bool = False\n    workers: int = 4\n    request_timeout: int = 30\n\nclass DatabaseConfig(BaseModel):\n    url: str\n    pool_size: int = 5\n    max_overflow: int = 10\n    timeout: int = 30\n\nclass LogConfig(BaseModel):\n    level: str = \"INFO\"\n    format: str = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    file_path: Optional[str] = None\n\nclass InsightFlowConfig(BaseModel):\n    server: ServerConfig\n    database: DatabaseConfig\n    analytics: AnalyticsConfig\n    ai: AIModelConfig\n    logging: LogConfig\n    data_sources: Dict[str, DataSource]"}
{"type": "source_file", "path": "app/data/processors.py", "content": "import logging\nfrom typing import Dict, Any, List\nimport pandas as pd\nfrom .storage import DataStorage\nfrom ..models.schema import DataSource\n\nlogger = logging.getLogger(__name__)\n\nclass DataProcessor:\n    def __init__(self):\n        self.storage = DataStorage()\n        self._processors = {}\n        self._initialize_processors()\n\n    def _initialize_processors(self):\n        \"\"\"Initialize default data processors\"\"\"\n        self._processors = {\n            \"stream\": self._process_stream_data,\n            \"batch\": self._process_batch_data,\n            \"api\": self._process_api_data\n        }\n\n    async def process_data(self, data: Any, source: DataSource) -> Dict[str, Any]:\n        \"\"\"Process incoming data based on source type\"\"\"\n        try:\n            processor = self._processors.get(source.type)\n            if not processor:\n                raise ValueError(f\"No processor found for source type: {source.type}\")\n\n            processed_data = await processor(data, source)\n            await self.storage.store(processed_data, source)\n            return processed_data\n\n        except Exception as e:\n            logger.error(f\"Error processing data: {str(e)}\")\n            raise\n\n    async def _process_stream_data(self, data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:\n        \"\"\"Process streaming data\"\"\"\n        # Validate against schema\n        self._validate_data(data, source.schema)\n        \n        # Transform data\n        transformed_data = self._transform_data(data, source)\n        \n        return transformed_data\n\n    async def _process_batch_data(self, data: List[Dict[str, Any]], source: DataSource) -> List[Dict[str, Any]]:\n        \"\"\"Process batch data\"\"\"\n        processed_batch = []\n        for item in data:\n            self._validate_data(item, source.schema)\n            processed_item = self._transform_data(item, source)\n            processed_batch.append(processed_item)\n        \n        return processed_batch\n\n    def _validate_data(self, data: Dict[str, Any], schema: Dict[str, Any]):\n        \"\"\"Validate data against schema\"\"\"\n        for field, field_type in schema.items():\n            if field not in data:\n                raise ValueError(f\"Missing required field: {field}\")\n            # Add type validation logic here\n\n    def _transform_data(self, data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:\n        \"\"\"Transform data according to source configuration\"\"\"\n        # Add transformation logic here\n        return data\n"}
{"type": "source_file", "path": "app/main.py", "content": "import asyncio\nimport logging\nfrom fastapi import FastAPI\nfrom contextlib import asynccontextmanager\n\nfrom .config import config\nfrom .core.mcp_server import MCPServer\nfrom .core.message_handler import MessageHandler\nfrom .core.client_manager import ClientManager\nfrom .data.processors import DataProcessor\nfrom .data.ingestion import DataSourceAdapter\nfrom .data.storage import DataStorage\nfrom .analytics.engine import AnalyticsEngine\nfrom .analytics.insights import InsightGenerator\nfrom .ai.claude_connector import ClaudeConnector\nfrom .ai.nlp_processor import NLPProcessor\nfrom .api.rest import RestAPI\n\n# Configure logging\nlogging.basicConfig(\n    level=config.get_config().logging.level,\n    format=config.get_config().logging.format,\n    filename=config.get_config().logging.file_path\n)\nlogger = logging.getLogger(__name__)\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Initialize components\n    try:\n        # Core components\n        mcp_server = MCPServer()\n        message_handler = MessageHandler()\n        client_manager = ClientManager()\n\n        # Data components\n        data_processor = DataProcessor()\n        data_storage = DataStorage(None)  # Initialize with appropriate backend\n\n        # Analytics components\n        analytics_engine = AnalyticsEngine()\n        insight_generator = InsightGenerator()\n\n        # AI components\n        claude_connector = ClaudeConnector()\n        nlp_processor = NLPProcessor()\n\n        # Initialize MCP server\n        await mcp_server.initialize()\n\n        # Store components in app state\n        app.state.mcp_server = mcp_server\n        app.state.message_handler = message_handler\n        app.state.client_manager = client_manager\n        app.state.data_processor = data_processor\n        app.state.analytics_engine = analytics_engine\n        app.state.claude_connector = claude_connector\n\n        logger.info(\"InsightFlow initialized successfully\")\n        yield\n    except Exception as e:\n        logger.error(f\"Error during initialization: {str(e)}\")\n        raise\n    finally:\n        # Cleanup\n        await mcp_server.shutdown()\n        logger.info(\"InsightFlow shutdown complete\")\n\n# Initialize FastAPI application\napp = FastAPI(\n    title=\"InsightFlow\",\n    description=\"AI-powered data analytics and insights platform\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Initialize REST API\nrest_api = RestAPI(\n    mcp_server=app.state.mcp_server,\n    data_processor=app.state.data_processor,\n    ai_connector=app.state.claude_connector\n)\n\n# Include REST API routes\napp.include_router(rest_api.router, prefix=\"/api/v1\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    uvicorn.run(\n        \"main:app\",\n        host=config.get_config().server.host,\n        port=config.get_config().server.port,\n        reload=config.get_config().server.debug,\n        workers=config.get_config().server.workers\n    )"}
