{"repo_info": {"repo_name": "ASR-LLM-TTS", "repo_owner": "ABexit", "repo_url": "https://github.com/ABexit/ASR-LLM-TTS"}}
{"type": "source_file", "path": "0_Inference_QWen2.5.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom cosyvoice.cli.cosyvoice import CosyVoice\nfrom cosyvoice.utils.file_utils import load_wav\nimport torchaudio\n\nimport pygame\nimport time\n\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nmodel_name = r\".\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ncosyvoice = CosyVoice(r'../pretrained_models/CosyVoice-300M', load_jit=True, load_onnx=False, fp16=True)\n# sft usage\nprint(cosyvoice.list_avaliable_spks())\n\nprompt = \"你好，你叫什么名字?\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"Input:\", prompt)\nprint(\"Answer:\", response)\n\n# ['中文女', '中文男', '日语男', '粤语女', '英文女', '英文男', '韩语女']\nfor i, j in enumerate(cosyvoice.inference_sft(f'{prompt}', '中文男', stream=False)):\n    torchaudio.save('prompt_sft_{}.wav'.format(i), j['tts_speech'], 22050)\n    # play_audio('prompt_sft_{}.wav'.format(i))\n\n# change stream=True for chunk stream inference\nfor i, j in enumerate(cosyvoice.inference_sft(f'{response}', '中文女', stream=False)):\n    torchaudio.save('sft_{}.wav'.format(i), j['tts_speech'], 22050)\n    # play_audio('sft_{}.wav'.format(i))\n\nplay_audio('prompt_sft_0.wav')\nfor i, j in enumerate(f'{response}'):\n    play_audio('sft_{}.wav'.format(i))\n\n"}
{"type": "source_file", "path": "10_SenceVoice_QWen2.5_cosyVoice.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom cosyvoice.cli.cosyvoice import CosyVoice\nfrom cosyvoice.utils.file_utils import load_wav\nfrom funasr import AutoModel\nimport torchaudio\nimport pygame\nimport time\nimport sys\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\nimport numpy as np\n\ndef record_audio(filename=\"output.wav\", sample_rate=44100):\n    print(\"按下 Enter 开始录音...\")\n    input()  # 等待用户按下 Enter 键开始录音\n    print(\"录音中... 按下 Enter 键结束录音\")\n    \n    # 开始录音\n    recording = []\n    try:\n        def callback(indata, frames, time, status):\n            recording.append(indata.copy())\n        with sd.InputStream(samplerate=sample_rate, channels=1, callback=callback):\n            input()  # 等待用户再次按下 Enter 键结束录音\n    except Exception as e:\n        print(f\"录音出现错误: {e}\")\n        return\n    \n    # 将录音数据合并并保存为 WAV 文件\n    audio_data = np.concatenate(recording, axis=0)\n    write(filename, sample_rate, (audio_data * 32767).astype(np.int16))\n    print(f\"录音已保存为 {filename}\")\n\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nimport os\nimport shutil\n\ndef clear_folder(folder_path):\n    # 检查文件夹是否存在\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path, exist_ok=True)\n        print(f\"文件夹 '{folder_path}' 不存在，已创建\")\n        return\n    \n    # 获取文件夹中的所有文件和子文件夹\n    items = os.listdir(folder_path)\n    \n    # 如果文件夹为空，直接返回\n    if not items:\n        print(f\"文件夹 '{folder_path}' 已经为空\")\n        return\n    \n    # 遍历文件和文件夹并删除\n    for item in items:\n        item_path = os.path.join(folder_path, item)\n        \n        # 判断是否是文件夹或文件\n        if os.path.isfile(item_path):\n            os.remove(item_path)  # 删除文件\n            print(f\"删除文件: {item_path}\")\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)  # 删除文件夹及其内容\n            print(f\"删除文件夹: {item_path}\")\n    \n    print(f\"文件夹 '{folder_path}' 已清空\")\n\n# ------------------- 模型初始化 ---------------\n# --- SenceVoice-语音识别模型\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# --- QWen2.5大语言模型 ---\n# model_name = r\":\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r':\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# --- CosyVoice - 语音合成模型\ncosyvoice = CosyVoice(r'E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\CosyVoice-300M', load_jit=True, load_onnx=False, fp16=True)\n# --- CosyVoice - 支持的音色列表\nprint(cosyvoice.list_avaliable_spks())\n# ------------------ 模型初始化结束 ----------------\n\nwhile(1):\n    # 使用函数录音，作为输入\n    record_audio(\"my_recording.wav\")\n\n    # input_file = ( \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav\" )\n    input_file = (\"my_recording.wav\")\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n\n    # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(\"Input:\", prompt)\n    print(\"Answer:\", response)\n\n    # --- 答复输出文件夹 ---\n    folder_path = \"./out_answer/\"\n    clear_folder(folder_path)\n\n    # ['中文女', '中文男', '日语男', '粤语女', '英文女', '英文男', '韩语女']\n    # change stream=True for chunk stream inference\n    index_out = 0\n    for i, j in enumerate(cosyvoice.inference_sft(f'{response}', '中文女', stream=False)):\n        torchaudio.save('{}/sft_{}.wav'.format(folder_path,i), j['tts_speech'], 22050)\n        index_out += 1\n        # play_audio('sft_{}.wav'.format(i))\n\n    for idx in range(index_out):\n        play_audio('{}/sft_{}.wav'.format(folder_path,idx))\n\n"}
{"type": "source_file", "path": "7_Inference_QWen2-VL.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256*28*28\nmax_pixels = 1280*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n"}
{"type": "source_file", "path": "7.5_realTime_AV.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n    \n    if not segments_to_save:\n        return\n    \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # 保存视频\n    video_frames = []\n    while not video_queue.empty():\n        frame, timestamp = video_queue.get()\n        if start_time <= timestamp <= end_time:\n            video_frames.append(frame)\n    \n    if video_frames:\n        video_output_path = f\"{OUTPUT_DIR}/video_0.avi\"\n        out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n        for frame in video_frames:\n            out.write(frame)\n        out.release()\n        print(f\"视频保存至 {video_output_path}\")\n        Inference()\n    else:\n        pass\n        # print(\"无可保存的视频帧\")\n    \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n# -------------- Load QWen2-VL Model ------------\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# ------- 设置分辨率，降低现存占用 -------\nmin_pixels = 256*28*28\nmax_pixels = 512*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n# --------------------------------------\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\nfolder_path = \"./Test_QWen2_VL/\"\n\ndef Inference(TEMP_VIDEO_FILE=f\"{OUTPUT_DIR}/video_0.avi\", TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    file_path = os.path.join(folder_path, \"captured_image.jpg\")  # 设置保存路径\n    cap = cv2.VideoCapture(TEMP_VIDEO_FILE)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_index = int(total_frames // 2)\n    # 设置视频帧位置\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n    ret, frame = cap.read()\n    if not ret:\n        print(f\"无法读取帧索引 {frame_index}\")\n    else:\n        # 显示帧\n        cv2.imwrite(file_path, frame)\n        # cv2.imshow(f\"Frame {frame_index}\", frame)\n\n    # -------- SenceVoice 推理 ---------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    prompt = res[0]['text'].split(\">\")[-1]\n    # ---------SenceVoice --end----------\n\n    # -------- QWen2-VL 模型推理 ---------\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": f\"{file_path}\",\n                },\n                {\"type\": \"text\", \"text\": f\"{prompt}\"},\n            ],\n        }\n    ]\n\n    # Preparation for inference\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # Inference: Generation of the output\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    print(output_text)\n\n    # 输入文本\n    text = output_text[0]\n    # asyncio.run(amain(text, \"zh-CN-YunxiaNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    asyncio.run(amain(text, \"zh-CN-XiaoyiNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-YunjianNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-shaanxi-XiaoniNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "1_Inference_CosyVoice.py", "content": "from cosyvoice.cli.cosyvoice import CosyVoice\nfrom cosyvoice.utils.file_utils import load_wav\nimport torchaudio\n\ncosyvoice = CosyVoice(r'../pretrained_models/CosyVoice-300M', load_jit=True, load_onnx=False, fp16=True)\n# sft usage\nprint(cosyvoice.list_avaliable_spks())\n# change stream=True for chunk stream inference\n# out = cosyvoice.inference_sft('你好，我是通义生成式语音大模型，请问有什么可以帮您的吗？', '中文女', stream=False)\n# torchaudio.save('sft_0.wav', out['tts_speech'], 22050)\n\n# for i, j in enumerate(cosyvoice.inference_sft('你好，我是通义生成式语音大模型，请问有什么可以帮您的吗？', '中文女', stream=False)):\n#     torchaudio.save('sft_{}.wav'.format(i), j['tts_speech'], 22050)\n\nprompt_speech_16k = load_wav('vocal_3.mp3_10.wav_0006151680_0006360320.wav', 16000)\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜', '可以动动你的小手点个关注，感谢各位好哥哥，如果之后有新消息，我还会在更新呢。', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], 22050)\n\n# cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-25Hz') # or change to pretrained_models/CosyVoice-300M for 50Hz inference\n# # zero_shot usage, <|zh|><|en|><|jp|><|yue|><|ko|> for Chinese/English/Japanese/Cantonese/Korean\n# prompt_speech_16k = load_wav('zero_shot_prompt.wav', 16000)\n# for i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\n#     torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], 22050)\n# # cross_lingual usage\n# prompt_speech_16k = load_wav('cross_lingual_prompt.wav', 16000)\n# for i, j in enumerate(cosyvoice.inference_cross_lingual('<|en|>And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that\\'s coming into the family is a reason why sometimes we don\\'t buy the whole thing.', prompt_speech_16k, stream=False)):\n#     torchaudio.save('cross_lingual_{}.wav'.format(i), j['tts_speech'], 22050)\n# # vc usage\n# prompt_speech_16k = load_wav('zero_shot_prompt.wav', 16000)\n# source_speech_16k = load_wav('cross_lingual_prompt.wav', 16000)\n# for i, j in enumerate(cosyvoice.inference_vc(source_speech_16k, prompt_speech_16k, stream=False)):\n#     torchaudio.save('vc_{}.wav'.format(i), j['tts_speech'], 22050)\n\n# cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-Instruct')\n# # instruct usage, support <laughter></laughter><strong></strong>[laughter][breath]\n# for i, j in enumerate(cosyvoice.inference_instruct('在面对挑战时，他展现了非凡的<strong>勇气</strong>与<strong>智慧</strong>。', '中文男', 'Theo \\'Crimson\\', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.', stream=False)):\n#     torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], 22050)"}
{"type": "source_file", "path": "14_SenceVoice_QWen2VL_edgeTTS_realTime.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\n\nimport langid\nfrom langdetect import detect\n\n# --- 配置huggingFace国内镜像 ---\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\naudio_file_count = 0\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测，设置有效激活率rate=40%，低于此比例当作静音段\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    pygame.mixer.init()\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n\n    # 全局变量，用于保存音频文件名计数\n    global audio_file_count\n    audio_file_count += 1\n    audio_output_path = f\"{OUTPUT_DIR}/audio_{audio_file_count}.wav\"\n    video_output_path = f\"{OUTPUT_DIR}/video_{audio_file_count}.avi\"\n\n    if not segments_to_save:\n        return\n    \n    # 用于实时打断：接收到新保存文件需求，停止当前播放的音频\n    if pygame.mixer.music.get_busy():\n        pygame.mixer.music.stop()\n        print(\"检测到新的有效音，已停止当前音频播放\")\n\n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # 保存视频\n    video_frames = []\n    while not video_queue.empty():\n        frame, timestamp = video_queue.get()\n        if start_time <= timestamp <= end_time:\n            video_frames.append(frame)\n    \n    if video_frames:\n        out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n        for frame in video_frames:\n            out.write(frame)\n        out.release()\n        print(f\"视频保存至 {video_output_path}\")\n\n        # --- 直接推理会影响录制主线程，无法实现实时打断逻辑 ---\n        # Inference()\n\n        # --- 使用线程执行推理\n        inference_thread = threading.Thread(target=Inference, args=(video_output_path, audio_output_path))\n        inference_thread.start()\n    else:\n        pass\n        # print(\"无可保存的视频帧\")\n    \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n# -------------- Load QWen2-VL Model ------------\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# ------- 设置分辨率，降低现存占用 -------\nmin_pixels = 256*28*28\nmax_pixels = 512*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n# --------------------------------------\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\nfolder_path = \"./Test_QWen2_VL/\"\n\ndef Inference(TEMP_VIDEO_FILE, TEMP_AUDIO_FILE):\n    \n    cap = cv2.VideoCapture(TEMP_VIDEO_FILE)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # --- 设定视频截取帧时间比例\n    S_index = [0.2, 0.4, 0.6, 0.8]\n    frame_index = [int(total_frames * i) for i in S_index]\n    # 设置视频帧位置\n    for idx in frame_index:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"无法读取帧索引 {idx}\")\n        else:\n            # 保存帧\n            file_path = os.path.join(folder_path, f\"captured_image{idx}.jpg\")  # 设置保存路径\n            cv2.imwrite(file_path, frame)\n\n    # -------- SenceVoice 推理 --start-------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    # prompt = res[0]['text'].split(\">\")[-1]\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    print(\"ASR OUT:\", prompt)\n    # ---------SenceVoice 推理--end----------\n\n    MODE_FLAG = 0\n    # -------- QWen2-VL 模型推理 --------- 多图模式\n    # Messages containing a images list as a video and a text query\n    if not MODE_FLAG:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"video\",\n                        \"video\": [\n                            f'{os.path.join(folder_path, f\"captured_image{frame_index[0]}.jpg\")}',\n                            f'{os.path.join(folder_path, f\"captured_image{frame_index[1]}.jpg\")}',\n                            f'{os.path.join(folder_path, f\"captured_image{frame_index[2]}.jpg\")}',\n                            f'{os.path.join(folder_path, f\"captured_image{frame_index[3]}.jpg\")}',\n                        ],\n                        \"fps\": 1.0,\n                    },\n                    {\"type\": \"text\", \"text\": f\"{prompt}\"},\n                ],\n            }\n        ]\n\n    # -------- QWen2-VL 模型推理 --------- 视频模式\n    else:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"video\",\n                        \"video\": f\"{TEMP_VIDEO_FILE}\",\n                        \"max_pixels\": 360 * 420,\n                        \"fps\": 1.0,\n                    },\n                    {\"type\": \"text\", \"text\": f\"{prompt}\"},\n                ],\n            }\n        ]\n\n    # Preparation for inference\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # Inference\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    print(output_text)\n\n    # 输入文本\n    text = output_text[0]\n    # 语种识别 -- langid\n    language, confidence = langid.classify(text)\n    # 语种识别 -- langdetect -- 没做结果对应关键词映射\n    # language = detect(text)\n\n    language_speaker = {\n    \"ja\" : \"ja-JP-NanamiNeural\",            # ok\n    \"fr\" : \"fr-FR-DeniseNeural\",            # ok\n    \"es\" : \"ca-ES-JoanaNeural\",             # ok\n    \"de\" : \"de-DE-KatjaNeural\",             # ok\n    \"zh\" : \"zh-CN-XiaoyiNeural\",            # ok\n    \"en\" : \"en-US-AnaNeural\",               # ok\n    }\n\n    if language not in language_speaker.keys():\n        used_speaker = \"zh-CN-XiaoyiNeural\"\n    else:\n        used_speaker = language_speaker[language]\n        print(\"检测到语种：\", language, \"使用音色：\", language_speaker[language])\n\n    global audio_file_count\n    asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_{audio_file_count}.mp3\")))\n    play_audio(f'{folder_path}/sft_{audio_file_count}.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-YunjianNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-shaanxi-XiaoniNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "11_SenceVoice_QWen2.5_pytts3.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom cosyvoice.cli.cosyvoice import CosyVoice\nfrom cosyvoice.utils.file_utils import load_wav\nfrom funasr import AutoModel\nimport torchaudio\nimport pygame\nimport time\nimport sys\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\nimport numpy as np\n\ndef record_audio(filename=\"output.wav\", sample_rate=44100):\n    print(\"按下 Enter 开始录音...\")\n    input()  # 等待用户按下 Enter 键开始录音\n    print(\"录音中... 按下 Enter 键结束录音\")\n    \n    # 开始录音\n    recording = []\n    try:\n        def callback(indata, frames, time, status):\n            recording.append(indata.copy())\n        with sd.InputStream(samplerate=sample_rate, channels=1, callback=callback):\n            input()  # 等待用户再次按下 Enter 键结束录音\n    except Exception as e:\n        print(f\"录音出现错误: {e}\")\n        return\n    \n    # 将录音数据合并并保存为 WAV 文件\n    audio_data = np.concatenate(recording, axis=0)\n    write(filename, sample_rate, (audio_data * 32767).astype(np.int16))\n    print(f\"录音已保存为 {filename}\")\n\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nimport os\nimport shutil\n\ndef clear_folder(folder_path):\n    # 检查文件夹是否存在\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path, exist_ok=True)\n        print(f\"文件夹 '{folder_path}' 不存在，已创建\")\n        return\n    \n    # 获取文件夹中的所有文件和子文件夹\n    items = os.listdir(folder_path)\n    \n    # 如果文件夹为空，直接返回\n    if not items:\n        print(f\"文件夹 '{folder_path}' 已经为空\")\n        return\n    \n    # 遍历文件和文件夹并删除\n    for item in items:\n        item_path = os.path.join(folder_path, item)\n        \n        # 判断是否是文件夹或文件\n        if os.path.isfile(item_path):\n            os.remove(item_path)  # 删除文件\n            print(f\"删除文件: {item_path}\")\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)  # 删除文件夹及其内容\n            print(f\"删除文件夹: {item_path}\")\n    \n    print(f\"文件夹 '{folder_path}' 已清空\")\n\n# ------------------- 模型初始化 ---------------\n# --- SenceVoice-语音识别模型\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# --- QWen2.5大语言模型 ---\n# model_name = r\":\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r':\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nimport pyttsx3\n# 初始化 TTS 引擎\nengine = pyttsx3.init()\n# 设置语音属性\nengine.setProperty('rate', 200)    # 语速engine.setProperty('volume', 0.9)  # 音量（0.0 到 1.0）\n# 选择语音\nvoices = engine.getProperty('voices')\n# print(voices)\nengine.setProperty('voice', voices[0].id)  # 使用第一个语音\n\nwhile(1):\n    # 使用函数录音，作为输入\n    record_audio(\"my_recording.wav\")\n\n    # input_file = ( \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav\" )\n    input_file = (\"my_recording.wav\")\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n\n    # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(\"Input:\", prompt)\n    print(\"Answer:\", response)\n\n    # 答复输出文件夹\n    folder_path = \"./out_answer/\"\n    clear_folder(folder_path)\n\n    # 输入文本\n    text = response\n    # 朗读文本\n    # engine.say(text)\n    # # 等待朗读完成\n    # engine.runAndWait()\n    engine.save_to_file(text, os.path.join(folder_path,\"sft_0.wav\"))\n    engine.runAndWait()\n    play_audio(f'{folder_path}/sft_0.wav')\n"}
{"type": "source_file", "path": "13_1_SenceVoice_QWen2.5_kokoro_realTime.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\n# import edge_tts\nfrom kokoro import KPipeline\nimport asyncio\nfrom time import sleep\nimport langid\nfrom langdetect import detect\nfrom IPython.display import display, Audio\nimport soundfile as sf\n\n# kokoro 语音合成\n# 🇪🇸 'e' => Spanish es\n# 🇫🇷 'f' => French fr-fr\n# 🇮🇳 'h' => Hindi hi\n# 🇮🇹 'i' => Italian it\n# 🇧🇷 'p' => Brazilian Portuguese pt-br\n# 🇺🇸 'a' => American English, 🇬🇧 'b' => British English\n# 🇯🇵 'j' => Japanese: pip install misaki[ja]\n# 🇨🇳 'z' => Mandarin Chinese: pip install misaki[zh]\nroot_voice = r'E:\\2_PYTHON\\Project\\TTS\\Kokoro-82M\\voices'\n\ndef tts_kokoro(text, outpath, lid='z', voice_glo='zm_yunjian'):\n    global root_voice\n    pipeline = KPipeline(lang_code=lid)\n    voice_tensor = torch.load(os.path.join(root_voice, voice_glo+'.pt'), weights_only=True)\n    generator = pipeline(\n        text, voice=voice_tensor,\n        speed=1, split_pattern=r'\\n+'\n    )\n\n    for i, (gs, ps, audio) in enumerate(generator):\n        # display(Audio(data=audio, rate=24000, autoplay=i==0))\n        sf.write(f'{outpath}', audio, 24000) # save each audio file\n\n# --- 配置huggingFace国内镜像 ---\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\nfolder_path = \"./Test_QWen2_VL/\"\naudio_file_count = 0\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(folder_path, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    pygame.mixer.init()\n\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n\n    # 全局变量，用于保存音频文件名计数\n    global audio_file_count\n    audio_file_count += 1\n    audio_output_path = f\"{OUTPUT_DIR}/audio_{audio_file_count}.wav\"\n    # audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n\n    if not segments_to_save:\n        return\n    \n    # 停止当前播放的音频\n    if pygame.mixer.music.get_busy():\n        pygame.mixer.music.stop()\n        print(\"检测到新的有效音，已停止当前音频播放\")\n        \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    \n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # Inference()\n    # 使用线程执行推理\n    inference_thread = threading.Thread(target=Inference, args=(audio_output_path,))\n    inference_thread.start()\n        \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\n# async def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n#     \"\"\"Main function\"\"\"\n#     communicate = edge_tts.Communicate(TEXT, VOICE)\n#     await communicate.save(OUTPUT_FILE)\n\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# --- QWen2.5大语言模型 ---\n# model_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r'E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef Inference(TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    # -------- SenceVoice 推理 ---------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    # prompt = res[0]['text'].split(\">\")[-1]\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    print(\"ASR OUT:\", prompt)\n    # ---------SenceVoice --end----------\n    # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n    messages = [\n        {\"role\": \"system\", \"content\": \"你叫千问，是一个18岁的女大学生，性格活泼开朗，说话俏皮\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(\"answer\", output_text)\n\n    # 输入文本\n    text = output_text\n    # 语种识别 -- langid\n    language, confidence = langid.classify(text)\n    # 语种识别 -- langdetect \n    # language = detect(text).split(\"-\")[0]\n\n    # 🇪🇸 'e' => Spanish es\n    # 🇫🇷 'f' => French fr-fr\n    # 🇮🇳 'h' => Hindi hi\n    # 🇮🇹 'i' => Italian it\n    # 🇧🇷 'p' => Brazilian Portuguese pt-br\n    # 🇺🇸 'a' => American English, 🇬🇧 'b' => British English\n    # 🇯🇵 'j' => Japanese: pip install misaki[ja]\n    # 🇨🇳 'z' => Mandarin Chinese: pip install misaki[zh]\n\n    language_speaker = {\n        \"ja\" : \"j\",            # ok\n        \"fr\" : \"f\",            # ok\n        \"es\" : \"e\",            # ok\n        \"zh\" : \"z\",            # ok\n        \"en\" : \"a\",            # ok\n    }\n\n    language_spk = {\n        \"j\" : \"jf_nezumi\",            # ok\n        \"f\" : \"ff_siwis\",            # ok\n        \"e\" : \"em_santa\",            # ok\n        \"z\" : \"zm_yunyang\",            # ok\n        \"a\" : \"af_heart\",            # ok\n    }\n\n    if language not in language_speaker.keys():\n        used_speaker = \"z\"\n    else:\n        used_speaker = language_speaker[language]\n        print(\"检测到语种：\", language, \"使用音色：\", language_speaker[language])\n\n    global audio_file_count\n    outpath = os.path.join(folder_path,f\"sft_{audio_file_count}.wav\")\n    tts_kokoro(text, outpath, lid=used_speaker, voice_glo=language_spk[used_speaker])\n    play_audio(f'{folder_path}/sft_{audio_file_count}.wav')\n\n# 主函数\nif __name__ == \"__main__\":\n\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        # video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        # video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        # video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "7.7_realTime_AV_multiImage.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n    \n    if not segments_to_save:\n        return\n    \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # 保存视频\n    video_frames = []\n    while not video_queue.empty():\n        frame, timestamp = video_queue.get()\n        if start_time <= timestamp <= end_time:\n            video_frames.append(frame)\n    \n    if video_frames:\n        video_output_path = f\"{OUTPUT_DIR}/video_0.avi\"\n        out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n        for frame in video_frames:\n            out.write(frame)\n        out.release()\n        print(f\"视频保存至 {video_output_path}\")\n        Inference()\n    else:\n        pass\n        # print(\"无可保存的视频帧\")\n    \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n# -------------- Load QWen2-VL Model ------------\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# ------- 设置分辨率，降低现存占用 -------\nmin_pixels = 256*28*28\nmax_pixels = 512*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n# --------------------------------------\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\nfolder_path = \"./Test_QWen2_VL/\"\n\ndef Inference(TEMP_VIDEO_FILE=f\"{OUTPUT_DIR}/video_0.avi\", TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    \n    cap = cv2.VideoCapture(TEMP_VIDEO_FILE)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    S_index = [0.2, 0.4, 0.6, 0.8]\n    frame_index = [int(total_frames * i) for i in S_index]\n    # 设置视频帧位置\n    for idx in frame_index:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"无法读取帧索引 {idx}\")\n        else:\n            # 保存帧\n            file_path = os.path.join(folder_path, f\"captured_image{idx}.jpg\")  # 设置保存路径\n            cv2.imwrite(file_path, frame)\n\n    # -------- SenceVoice 推理 ---------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    prompt = res[0]['text'].split(\">\")[-1]\n    print(\"ASR OUT:\", prompt)\n    # ---------SenceVoice --end----------\n\n    # -------- QWen2-VL 模型推理 ---------\n    messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": f'{os.path.join(folder_path, f\"captured_image{frame_index[0]}.jpg\")}'},\n            {\"type\": \"image\", \"image\": f'{os.path.join(folder_path, f\"captured_image{frame_index[1]}.jpg\")}'},\n            # {\"type\": \"image\", \"image\": f'{os.path.join(folder_path, f\"captured_image{frame_index[2]}.jpg\")}'},\n            # {\"type\": \"image\", \"image\": f'{os.path.join(folder_path, f\"captured_image{frame_index[3]}.jpg\")}'},\n            {\"type\": \"text\", \"text\": f\"{prompt}\"},\n            ],\n        }\n    ]\n\n    # Preparation for inference\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # Inference\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    print(output_text)\n\n    # 输入文本\n    text = output_text[0]\n    # asyncio.run(amain(text, \"zh-CN-YunxiaNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    asyncio.run(amain(text, \"zh-CN-XiaoyiNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-YunjianNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-shaanxi-XiaoniNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "12_SenceVoice_QWen2.5_edgeTTS.py", "content": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom cosyvoice.cli.cosyvoice import CosyVoice\nfrom cosyvoice.utils.file_utils import load_wav\nfrom funasr import AutoModel\nimport torchaudio\nimport pygame\nimport time\nimport sys\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\nimport numpy as np\nimport asyncio\nimport edge_tts\nimport os\nimport shutil\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\ndef record_audio(filename=\"output.wav\", sample_rate=44100):\n    print(\"按下 Enter 开始录音...\")\n    input()  # 等待用户按下 Enter 键开始录音\n    print(\"录音中... 按下 Enter 键结束录音\")\n    \n    # 开始录音\n    recording = []\n    try:\n        def callback(indata, frames, time, status):\n            recording.append(indata.copy())\n        with sd.InputStream(samplerate=sample_rate, channels=1, callback=callback):\n            input()  # 等待用户再次按下 Enter 键结束录音\n    except Exception as e:\n        print(f\"录音出现错误: {e}\")\n        return\n    \n    # 将录音数据合并并保存为 WAV 文件\n    audio_data = np.concatenate(recording, axis=0)\n    write(filename, sample_rate, (audio_data * 32767).astype(np.int16))\n    print(f\"录音已保存为 {filename}\")\n\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\n\ndef clear_folder(folder_path):\n    # 检查文件夹是否存在\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path, exist_ok=True)\n        print(f\"文件夹 '{folder_path}' 不存在，已创建\")\n        return\n    \n    # 获取文件夹中的所有文件和子文件夹\n    items = os.listdir(folder_path)\n    \n    # 如果文件夹为空，直接返回\n    if not items:\n        print(f\"文件夹 '{folder_path}' 已经为空\")\n        return\n    \n    # 遍历文件和文件夹并删除\n    for item in items:\n        item_path = os.path.join(folder_path, item)\n        \n        # 判断是否是文件夹或文件\n        if os.path.isfile(item_path):\n            os.remove(item_path)  # 删除文件\n            print(f\"删除文件: {item_path}\")\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)  # 删除文件夹及其内容\n            print(f\"删除文件夹: {item_path}\")\n    \n    print(f\"文件夹 '{folder_path}' 已清空\")\n\n# ------------------- 模型初始化 ---------------\n# --- SenceVoice-语音识别模型\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# --- QWen2.5大语言模型 ---\nmodel_name = r\":\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\n# model_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r':\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# --- CosyVoice - 语音合成模型 ------\n# cosyvoice = CosyVoice(r'../pretrained_models/CosyVoice-300M', load_jit=True, load_onnx=False, fp16=True)\n# print(cosyvoice.list_avaliable_spks())\n#  --- CosyVoice - 支持的音色列表\n# ------------------ 模型初始化结束 ----------------\n\n\nwhile(1):\n    # 使用函数录音，作为输入\n    record_audio(\"my_recording.wav\")\n\n    input_file = (\"my_recording.wav\")\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n\n    # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(\"Input:\", prompt)\n    print(\"Answer:\", response)\n\n    # 答复输出文件夹\n    folder_path = \"./out_answer/\"\n    clear_folder(folder_path)\n\n    # 输入文本\n    text = response\n    asyncio.run(amain(text, \"zh-CN-XiaoyiNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    play_audio(f'{folder_path}/sft_0.mp3')"}
{"type": "source_file", "path": "7_0_FunASR.py", "content": "\nimport torchaudio\nfrom funasr import AutoModel\nfrom IPython.display import Audio\n\nspeaker1_wav = r'E:\\2_PYTHON\\Project\\GPT\\QWen\\CosyVoice\\output\\audio_0.wav'\nwaveform, sample_rate = torchaudio.load(speaker1_wav)\nAudio(waveform, rate=sample_rate, autoplay=True)\n\n# VAD检测\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"fsmn-vad\")\nres = model.generate(input=speaker1_wav)\nprint(res)\n\n\n# # 多说话人语音识别\n# funasr_model = AutoModel(model=\"iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\",\n#                         vad_model=\"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\",\n#                         punc_model=\"damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch\",\n#                         spk_model=\"damo/speech_campplus_sv_zh-cn_16k-common\",\n#                         )\n# res = funasr_model.generate(input=f\"multi_speaker.wav\", \n#             batch_size_s=300)\n# print(res[0]['text'])\n# res_srt = generate_srt(res[0]['sentence_info'])\n# print(res_srt)\n\n"}
{"type": "source_file", "path": "6_Inference_funasr.py", "content": "#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.\n#  MIT License  (https://opensource.org/licenses/MIT)\n\nimport sys\nfrom funasr import AutoModel\n\nmodel_dir = r\".\\QWen\\pretrained_models\\SenseVoiceSmall\"\ninput_file = (\n    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav\"\n)\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n)\n\nres = model.generate(\n    input=input_file,\n    cache={},\n    language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n)\n\nprint(res)\n# import pdb; pdb.set_trace()\nprint(res[0]['text'].split(\">\")[-1])\n"}
{"type": "source_file", "path": "13_SenceVoice_QWen2.5_edgeTTS_realTime.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\nimport langid\nfrom langdetect import detect\n\n# --- 配置huggingFace国内镜像 ---\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\nfolder_path = \"./Test_QWen2_VL/\"\naudio_file_count = 0\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(folder_path, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    pygame.mixer.init()\n\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n\n    # 全局变量，用于保存音频文件名计数\n    global audio_file_count\n    audio_file_count += 1\n    audio_output_path = f\"{OUTPUT_DIR}/audio_{audio_file_count}.wav\"\n    # audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n\n    if not segments_to_save:\n        return\n    \n    # 停止当前播放的音频\n    if pygame.mixer.music.get_busy():\n        pygame.mixer.music.stop()\n        print(\"检测到新的有效音，已停止当前音频播放\")\n        \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    \n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # Inference()\n    # 使用线程执行推理\n    inference_thread = threading.Thread(target=Inference, args=(audio_output_path,))\n    inference_thread.start()\n        \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# --- QWen2.5大语言模型 ---\n# model_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r'E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef Inference(TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n\n    # -------- SenceVoice 推理 ---------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    # prompt = res[0]['text'].split(\">\")[-1]\n    prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n    print(\"ASR OUT:\", prompt)\n    # ---------SenceVoice --end----------\n    # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n    messages = [\n        {\"role\": \"system\", \"content\": \"你叫千问，是一个18岁的女大学生，性格活泼开朗，说话俏皮\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(\"answer\", output_text)\n\n    # 输入文本\n    text = output_text\n    # 语种识别 -- langid\n    language, confidence = langid.classify(text)\n    # 语种识别 -- langdetect \n    # language = detect(text).split(\"-\")[0]\n\n    language_speaker = {\n    \"ja\" : \"ja-JP-NanamiNeural\",            # ok\n    \"fr\" : \"fr-FR-DeniseNeural\",            # ok\n    \"es\" : \"ca-ES-JoanaNeural\",             # ok\n    \"de\" : \"de-DE-KatjaNeural\",             # ok\n    \"zh\" : \"zh-CN-XiaoyiNeural\",            # ok\n    \"en\" : \"en-US-AnaNeural\",               # ok\n    }\n\n    if language not in language_speaker.keys():\n        used_speaker = \"zh-CN-XiaoyiNeural\"\n    else:\n        used_speaker = language_speaker[language]\n        print(\"检测到语种：\", language, \"使用音色：\", language_speaker[language])\n\n    global audio_file_count\n    asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_{audio_file_count}.mp3\")))\n    play_audio(f'{folder_path}/sft_{audio_file_count}.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        # video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        # video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        # video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "7.6_realTime_debug.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num = 0\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(0.8 * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n    \n    if not segments_to_save:\n        return\n    \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # 保存视频\n    video_frames = []\n    while not video_queue.empty():\n        frame, timestamp = video_queue.get()\n        if start_time <= timestamp <= end_time:\n            video_frames.append(frame)\n    \n    if video_frames:\n        video_output_path = f\"{OUTPUT_DIR}/video_0.avi\"\n        out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n        for frame in video_frames:\n            out.write(frame)\n        out.release()\n        print(f\"视频保存至 {video_output_path}\")\n        # Inference()\n    else:\n        pass\n        # print(\"无可保存的视频帧\")\n    \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n# # -------------- Load QWen2-VL Model ------------\n# # default: Load the model on the available device(s)\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n# )\n# # ------- 设置分辨率，降低现存占用 -------\n# min_pixels = 256*28*28\n# max_pixels = 512*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n# # --------------------------------------\n\n# # -------- SenceVoice 语音识别 --模型加载-----\n# model_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\n# model_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n# folder_path = \"./Test_QWen2_VL/\"\n\n# def Inference(TEMP_VIDEO_FILE=f\"{OUTPUT_DIR}/video_0.avi\", TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n#     file_path = os.path.join(folder_path, \"captured_image.jpg\")  # 设置保存路径\n#     cap = cv2.VideoCapture(TEMP_VIDEO_FILE)\n#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n#     frame_index = int(total_frames // 2)\n#     # 设置视频帧位置\n#     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n#     ret, frame = cap.read()\n#     if not ret:\n#         print(f\"无法读取帧索引 {frame_index}\")\n#     else:\n#         # 显示帧\n#         cv2.imwrite(file_path, frame)\n#         # cv2.imshow(f\"Frame {frame_index}\", frame)\n\n#     # -------- SenceVoice 推理 ---------\n#     input_file = (TEMP_AUDIO_FILE)\n#     res = model_senceVoice.generate(\n#         input=input_file,\n#         cache={},\n#         language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n#         use_itn=False,\n#     )\n#     prompt = res[0]['text'].split(\">\")[-1]\n#     # ---------SenceVoice --end----------\n\n#     # -------- QWen2-VL 模型推理 ---------\n#     messages = [\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\n#                     \"type\": \"image\",\n#                     \"image\": f\"{file_path}\",\n#                 },\n#                 {\"type\": \"text\", \"text\": f\"{prompt}\"},\n#             ],\n#         }\n#     ]\n\n#     # Preparation for inference\n#     text = processor.apply_chat_template(\n#         messages, tokenize=False, add_generation_prompt=True\n#     )\n#     image_inputs, video_inputs = process_vision_info(messages)\n#     inputs = processor(\n#         text=[text],\n#         images=image_inputs,\n#         videos=video_inputs,\n#         padding=True,\n#         return_tensors=\"pt\",\n#     )\n#     inputs = inputs.to(\"cuda\")\n\n#     # Inference: Generation of the output\n#     generated_ids = model.generate(**inputs, max_new_tokens=128)\n#     generated_ids_trimmed = [\n#         out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n#     ]\n#     output_text = processor.batch_decode(\n#         generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n#     )\n#     print(output_text)\n\n#     # 输入文本\n#     text = output_text[0]\n#     # asyncio.run(amain(text, \"zh-CN-YunxiaNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n#     # play_audio(f'{folder_path}/sft_0.mp3')\n\n#     asyncio.run(amain(text, \"zh-CN-XiaoyiNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n#     play_audio(f'{folder_path}/sft_0.mp3')\n\n#     # asyncio.run(amain(text, \"zh-CN-YunjianNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n#     # play_audio(f'{folder_path}/sft_0.mp3')\n\n#     # asyncio.run(amain(text, \"zh-CN-shaanxi-XiaoniNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n#     # play_audio(f'{folder_path}/sft_0.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "3_Inference_edgeTTS.py", "content": "#!/usr/bin/env python3\n\n\"\"\"\nBasic example of edge_tts usage.\n\"\"\"\n\nimport asyncio\n\nimport edge_tts\n\n# TEXT = \"今日特别报道，伊朗、舍沙特阿拉伯在北京中方见证下，决定恢复外交关系，重启大使馆并互相派驻外交大使\"\n# VOICE = \"zh-CN-YunxiNeural\"\n# OUTPUT_FILE = \"test_male.mp3\"\n\n# TEXT = \"はそれぞれ\"\n# VOICE = \"ja-JP-NanamiNeural\"\n# OUTPUT_FILE = \"test_male.mp3\"\n\n# TEXT = \"Estoy bien, gracias. ¿Y tú?\"\n# VOICE = \"ca-ES-JoanaNeural\"\n# OUTPUT_FILE = \"test_male.mp3\"\n\n# TEXT = \"Ça va bien, merci. Et toi ?\"\n# VOICE = \"fr-FR-DeniseNeural\"\n# OUTPUT_FILE = \"test_male.mp3\"\n\nTEXT = \"hello, whats your name\"\nVOICE = \"en-US-AnaNeural\"\nOUTPUT_FILE = \"test_male.mp3\"\n\nasync def amain() -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(amain())\n\n'''\nName: zh-CN-XiaoyiNeural\nGender: Female\n\nName: zh-CN-YunjianNeural\nGender: Male\n\nName: zh-CN-YunxiNeural\nGender: Male\n\nName: zh-CN-YunxiaNeural\nGender: Male\n\nName: zh-CN-YunyangNeural\nGender: Male\n\nName: zh-CN-liaoning-XiaobeiNeural\nGender: Female\n\nName: zh-CN-shaanxi-XiaoniNeural\nGender: Female\n\nName: zh-HK-HiuGaaiNeural\nGender: Female\n\nName: zh-HK-HiuMaanNeural\nGender: Female\n\nName: zh-HK-WanLungNeural\nGender: Male\n\nName: zh-TW-HsiaoChenNeural\nGender: Female\n'''"}
{"type": "source_file", "path": "15.1_SenceVoice_kws_CAM++.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\nimport langid\nfrom langdetect import detect\nimport re\nfrom pypinyin import pinyin, Style\nfrom modelscope.pipelines import pipeline\n\n# --- 配置huggingFace国内镜像 ---\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\nfolder_path = \"./Test_QWen2_VL/\"\naudio_file_count = 0\naudio_file_count_tmp = 0\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(folder_path, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n\n# --- 唤醒词、声纹变量配置 ---\n# set_KWS = \"ni hao xiao qian\"\n# set_KWS = \"shuo hua xiao qian\"\nset_KWS = \"zhan qi lai\"\nflag_KWS = 0\n\nflag_KWS_used = 1\nflag_sv_used = 1\n\nflag_sv_enroll = 0\nthred_sv = 0.35\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n\ndef extract_chinese_and_convert_to_pinyin(input_string):\n    \"\"\"\n    提取字符串中的汉字，并将其转换为拼音。\n    \n    :param input_string: 原始字符串\n    :return: 转换后的拼音字符串\n    \"\"\"\n    # 使用正则表达式提取所有汉字\n    chinese_characters = re.findall(r'[\\u4e00-\\u9fa5]', input_string)\n    # 将汉字列表合并为字符串\n    chinese_text = ''.join(chinese_characters)\n    \n    # 转换为拼音\n    pinyin_result = pinyin(chinese_text, style=Style.NORMAL)\n    # 将拼音列表拼接为字符串\n    pinyin_text = ' '.join([item[0] for item in pinyin_result])\n    \n    return pinyin_text\n\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.5\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    pygame.mixer.init()\n\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n\n    # 全局变量，用于保存音频文件名计数\n    global audio_file_count\n    global flag_sv_enroll\n    global set_SV_enroll\n\n    if flag_sv_enroll:\n        audio_output_path = f\"{set_SV_enroll}/enroll_0.wav\"\n    else:\n        audio_file_count += 1\n        audio_output_path = f\"{OUTPUT_DIR}/audio_{audio_file_count}.wav\"\n    # audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n\n    if not segments_to_save:\n        return\n    \n    # 停止当前播放的音频\n    if pygame.mixer.music.get_busy():\n        pygame.mixer.music.stop()\n        print(\"检测到新的有效音，已停止当前音频播放\")\n        \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    if flag_sv_enroll:\n        audio_length = 0.5 * len(segments_to_save)\n        if audio_length < 3:\n            print(\"声纹注册语音需大于3秒，请重新注册\")\n            return 1\n\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n\n    # Inference()\n\n    if flag_sv_enroll:\n        text = \"声纹注册完成！现在只有你可以命令我啦！\"\n        print(text)\n        flag_sv_enroll = 0\n        system_introduction(text)\n    else:\n    # 使用线程执行推理\n        inference_thread = threading.Thread(target=Inference, args=(audio_output_path,))\n        inference_thread.start()\n        \n        # 记录保存的区间\n        saved_intervals.append((start_time, end_time))\n        \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\nimport os\n\ndef is_folder_empty(folder_path):\n    \"\"\"\n    检测指定文件夹内是否有文件。\n    \n    :param folder_path: 文件夹路径\n    :return: 如果文件夹为空返回 True，否则返回 False\n    \"\"\"\n    # 获取文件夹中的所有条目（文件或子文件夹）\n    entries = os.listdir(folder_path)\n    # 检查是否存在文件\n    for entry in entries:\n        # 获取完整路径\n        full_path = os.path.join(folder_path, entry)\n        # 如果是文件，返回 False\n        if os.path.isfile(full_path):\n            return False\n    # 如果没有文件，返回 True\n    return True\n\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\n# -------- CAM++声纹识别 -- 模型加载 --------\nset_SV_enroll = r'.\\SpeakerVerification_DIR\\enroll_wav\\\\'\nsv_pipeline = pipeline(\n    task='speaker-verification',\n    model='damo/speech_campplus_sv_zh-cn_16k-common',\n    model_revision='v1.0.0'\n)\n\n# --------- QWen2.5大语言模型 ---------------\n# model_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r'E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# ---------- 模型加载结束 -----------------------\n\nclass ChatMemory:\n    def __init__(self, max_length=2048):\n        self.history = []\n        self.max_length = max_length  # 最大输入长度\n\n    def add_to_history(self, user_input, model_response):\n        \"\"\"\n        添加用户输入和模型响应到历史记录。\n        \"\"\"\n        self.history.append(f\"User: {user_input}\")\n        self.history.append(f\"system: {model_response}\")\n\n    def get_context(self):\n        \"\"\"\n        获取拼接后的对话上下文。\n        \"\"\"\n        context = \"\\n\".join(self.history)\n        # 截断上下文，使其不超过 max_length\n        if len(context) > self.max_length:\n            context = context[-self.max_length :]\n        return context\n    \n# -------- memory 初始化 --------\nmemory = ChatMemory(max_length=512)\n\ndef system_introduction(text):\n    global audio_file_count\n    global folder_path\n    text = text\n    print(\"LLM output:\", text)\n    used_speaker = \"zh-CN-XiaoyiNeural\"\n    asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_tmp_{audio_file_count}.mp3\")))\n    play_audio(f'{folder_path}/sft_tmp_{audio_file_count}.mp3')\n\ndef Inference(TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    '''\n    1. 使用senceVoice做asr，转换为拼音，检测唤醒词\n        - 首先检测声纹注册文件夹是否有注册文件，如果无，启动声纹注册\n    2. 使用CAM++做声纹识别\n        - 设置固定声纹注册语音目录，每次输入音频均进行声纹对比\n    3. 以上两者均通过，则进行大模型推理\n    '''\n    global audio_file_count\n\n    global set_SV_enroll\n    global flag_sv_enroll\n    global thred_sv\n    global flag_sv_used\n\n    global set_KWS\n    global flag_KWS\n    global flag_KWS_used\n    \n    os.makedirs(set_SV_enroll, exist_ok=True)\n    # --- 如果开启声纹识别，且声纹文件夹为空，则开始声纹注册。设定注册语音有效长度需大于3秒\n    if flag_sv_used and is_folder_empty(set_SV_enroll):\n        text = f\"无声纹注册文件！请先注册声纹，需大于三秒哦~\"\n        print(text)\n        system_introduction(text)\n        flag_sv_enroll = 1\n    \n    else:\n        # -------- SenceVoice 推理 ---------\n        input_file = (TEMP_AUDIO_FILE)\n        res = model_senceVoice.generate(\n            input=input_file,\n            cache={},\n            language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n            use_itn=False,\n        )\n        prompt = res[0]['text'].split(\">\")[-1]\n        prompt_pinyin = extract_chinese_and_convert_to_pinyin(prompt)\n        print(prompt, prompt_pinyin)\n\n        # --- 判断是否启动KWS\n        if not flag_KWS_used:\n            flag_KWS = 1\n        if not flag_KWS:\n            if set_KWS in prompt_pinyin:\n                flag_KWS = 1\n        \n        # --- KWS成功，或不设置KWS\n        if flag_KWS:\n            sv_score = sv_pipeline([os.path.join(set_SV_enroll, \"enroll_0.wav\"), TEMP_AUDIO_FILE], thr=thred_sv)\n            print(sv_score)\n            sv_result = sv_score['text']\n            if sv_result == \"yes\":\n\n                # --- 读取历史对话 ---\n                context = memory.get_context()\n                \n                # prompt_tmp = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n                prompt_tmp = res[0]['text'].split(\">\")[-1]\n                prompt = f\"{context}\\nUser:{prompt_tmp}\\n\"\n\n                print(\"History:\", context)\n                print(\"ASR OUT:\", prompt)\n                # ---------SenceVoice --end----------\n                # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n                messages = [\n                    {\"role\": \"system\", \"content\": \"你叫小千，是一个18岁的女大学生，性格活泼开朗，说话俏皮简洁，回答问题不会超过50字。\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ]\n                text = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                )\n                model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n                generated_ids = model.generate(\n                    **model_inputs,\n                    max_new_tokens=512,\n                )\n                generated_ids = [\n                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n                ]\n\n                output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n                print(\"answer\", output_text)\n\n                # -------- 更新记忆库 -----\n                memory.add_to_history(prompt_tmp, output_text)\n\n                # 输入文本\n                text = output_text\n                # 语种识别 -- langid\n                language, confidence = langid.classify(text)\n                # 语种识别 -- langdetect \n                # language = detect(text).split(\"-\")[0]\n\n                language_speaker = {\n                \"ja\" : \"ja-JP-NanamiNeural\",            # ok\n                \"fr\" : \"fr-FR-DeniseNeural\",            # ok\n                \"es\" : \"ca-ES-JoanaNeural\",             # ok\n                \"de\" : \"de-DE-KatjaNeural\",             # ok\n                \"zh\" : \"zh-CN-XiaoyiNeural\",            # ok\n                \"en\" : \"en-US-AnaNeural\",               # ok\n                }\n\n                if language not in language_speaker.keys():\n                    used_speaker = \"zh-CN-XiaoyiNeural\"\n                else:\n                    used_speaker = language_speaker[language]\n                    print(\"检测到语种：\", language, \"使用音色：\", language_speaker[language])\n\n                asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_{audio_file_count}.mp3\")))\n                play_audio(f'{folder_path}/sft_{audio_file_count}.mp3')\n            else:\n                text = \"很抱歉，声纹验证失败，我无法为您服务\"\n                print(text)\n                # system_introduction(text)\n        else:\n            text = \"很抱歉，唤醒词错误，请说出正确的唤醒词哦\"\n            system_introduction(text)\n\n# 主函数\nif __name__ == \"__main__\":\n\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        # video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        # video_thread.start()\n\n        flag_info = f'{flag_sv_used}-{flag_KWS_used}'\n        dict_flag_info = {\n            \"1-1\": \"您已开启声纹识别和关键词唤醒，\",\n            \"0-1\":\"您已开启关键词唤醒\",\n            \"1-0\":\"您已开启声纹识别\",\n            \"0-0\":\"\",\n        }\n        if flag_sv_used or flag_KWS_used:\n            text = dict_flag_info[flag_info]\n            system_introduction(text)\n\n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        # video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "4_Inference_QWen2Audio.py", "content": "from io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n\nmodel_name = r\".\\QWen\\Qwen2-Audio-7B-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_name)\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(model_name, device_map=\"cuda\")\n\n# conversation = [\n#     {\"role\": \"user\", \"content\": [\n#         {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n#     ]},\n#     {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n#     {\"role\": \"user\", \"content\": [\n#         {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n#     ]},\n# ]\n\n# 定义对话\nconversation = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_path\": r\".\\QWen\\Qwen2-Audio-7B-Instruct\\guess_age_gender.wav\"},     \n    ]},    \n    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"audio\", \"audio_path\": r\".\\QWen\\Qwen2-Audio-7B-Instruct\\translate_to_chinese.wav\"},\n    ]},\n]\n\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\n    if isinstance(message[\"content\"], list):\n        for ele in message[\"content\"]:\n            if ele[\"type\"] == \"audio\":\n                audios.append(librosa.load(\n                    BytesIO(urlopen(ele['audio_url']).read()), \n                    sr=processor.feature_extractor.sampling_rate)[0]\n                )\n\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\n\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\n\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(\"Answer:\", response)"}
{"type": "source_file", "path": "15.0_SenceVoice_kws_CAM++.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\nimport langid\nfrom langdetect import detect\nimport re\nfrom pypinyin import pinyin, Style\nfrom modelscope.pipelines import pipeline\n\n# --- 配置huggingFace国内镜像 ---\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\nfolder_path = \"./Test_QWen2_VL/\"\naudio_file_count = 0\naudio_file_count_tmp = 0\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(folder_path, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n\n# --- 唤醒词、声纹变量配置 ---\n# set_KWS = \"ni hao xiao qian\"\nset_KWS = \"shuo hua xiao qian\"\nflag_KWS = 0\nflag_KWS_used = 1\n\nflag_sv_used = 1\nflag_sv_enroll = 0\nthred_sv = 0.35\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n\ndef extract_chinese_and_convert_to_pinyin(input_string):\n    \"\"\"\n    提取字符串中的汉字，并将其转换为拼音。\n    \n    :param input_string: 原始字符串\n    :return: 转换后的拼音字符串\n    \"\"\"\n    # 使用正则表达式提取所有汉字\n    chinese_characters = re.findall(r'[\\u4e00-\\u9fa5]', input_string)\n    # 将汉字列表合并为字符串\n    chinese_text = ''.join(chinese_characters)\n    \n    # 转换为拼音\n    pinyin_result = pinyin(chinese_text, style=Style.NORMAL)\n    # 将拼音列表拼接为字符串\n    pinyin_text = ' '.join([item[0] for item in pinyin_result])\n    \n    return pinyin_text\n\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    pygame.mixer.init()\n\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n\n    # 全局变量，用于保存音频文件名计数\n    global audio_file_count\n    global flag_sv_enroll\n    global set_SV_enroll\n\n    if flag_sv_enroll:\n        audio_output_path = f\"{set_SV_enroll}/enroll_0.wav\"\n    else:\n        audio_file_count += 1\n        audio_output_path = f\"{OUTPUT_DIR}/audio_{audio_file_count}.wav\"\n    # audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n\n    if not segments_to_save:\n        return\n    \n    # 停止当前播放的音频\n    if pygame.mixer.music.get_busy():\n        pygame.mixer.music.stop()\n        print(\"检测到新的有效音，已停止当前音频播放\")\n        \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    if flag_sv_enroll:\n        audio_length = 0.5 * len(segments_to_save)\n        if audio_length < 3:\n            print(\"声纹注册语音需大于3秒，请重新注册\")\n            return 1\n\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n\n    # Inference()\n\n    if flag_sv_enroll:\n        text = \"声纹注册完成！现在只有你可以命令我啦！\"\n        print(text)\n        flag_sv_enroll = 0\n        system_introduction(text)\n    else:\n    # 使用线程执行推理\n        inference_thread = threading.Thread(target=Inference, args=(audio_output_path,))\n        inference_thread.start()\n        \n        # 记录保存的区间\n        saved_intervals.append((start_time, end_time))\n        \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\nimport os\n\ndef is_folder_empty(folder_path):\n    \"\"\"\n    检测指定文件夹内是否有文件。\n    \n    :param folder_path: 文件夹路径\n    :return: 如果文件夹为空返回 True，否则返回 False\n    \"\"\"\n    # 获取文件夹中的所有条目（文件或子文件夹）\n    entries = os.listdir(folder_path)\n    # 检查是否存在文件\n    for entry in entries:\n        # 获取完整路径\n        full_path = os.path.join(folder_path, entry)\n        # 如果是文件，返回 False\n        if os.path.isfile(full_path):\n            return False\n    # 如果没有文件，返回 True\n    return True\n\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\n\nset_SV_enroll = r'.\\SpeakerVerification_DIR\\enroll_wav\\\\'\n# -------- CAM++声纹识别 -- 模型加载 --------\nsv_pipeline = pipeline(\n    task='speaker-verification',\n    model='damo/speech_campplus_sv_zh-cn_16k-common',\n    model_revision='v1.0.0'\n)\n\n# --------- QWen2.5大语言模型 ---------------\n# model_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-0.5B-Instruct\"\nmodel_name = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-1.5B-Instruct\"\n# model_name = r'E:\\2_PYTHON\\Project\\GPT\\QWen\\Qwen2.5-7B-Instruct-GPTQ-Int4'\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef system_introduction(text):\n        global audio_file_count\n        global folder_path\n        text = text\n        print(\"LLM output:\", text)\n        used_speaker = \"zh-CN-XiaoyiNeural\"\n        asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_tmp_{audio_file_count}.mp3\")))\n        play_audio(f'{folder_path}/sft_tmp_{audio_file_count}.mp3')\n\ndef Inference(TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    '''\n    1. 使用senceVoice做asr，转换为拼音，检测唤醒词\n        - 首先检测声纹注册文件夹是否有注册文件，如果无，启动声纹注册\n    2. 使用CAM++做声纹识别\n        - 设置固定声纹注册语音目录，每次输入音频均进行声纹对比\n    3. 以上两者均通过，则进行大模型推理\n    '''\n    global audio_file_count\n\n    global set_SV_enroll\n    global flag_sv_enroll\n    global thred_sv\n    global flag_sv_used\n\n    global set_KWS\n    global flag_KWS\n    global flag_KWS_used\n    \n    \n    # flag_info = f'{flag_sv_used}-{flag_KWS_used}'\n    # dict_flag_info = {\n    #     \"1-1\": \"您已开启声纹识别和关键词唤醒，\",\n    #     \"0-1\":\"您已开启关键词唤醒\",\n    #     \"1-0\":\"您已开启声纹识别\",\n    #     \"0-0\":\"\",\n    # }\n    # if flag_sv_used or flag_KWS_used:\n    #     text = dict_flag_info[flag_info]\n    #     system_introduction(text)\n\n    os.makedirs(set_SV_enroll, exist_ok=True)\n    # --- 如果开启声纹识别，且声纹文件夹为空，则开始声纹注册。设定注册语音有效长度需大于3秒\n    if flag_sv_used and is_folder_empty(set_SV_enroll):\n        text = f\"无声纹注册文件！请先注册声纹，需大于三秒哦~\"\n        print(text)\n        system_introduction(text)\n        flag_sv_enroll = 1\n    \n    else:\n        # -------- SenceVoice 推理 ---------\n        input_file = (TEMP_AUDIO_FILE)\n        res = model_senceVoice.generate(\n            input=input_file,\n            cache={},\n            language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n            use_itn=False,\n        )\n        prompt = res[0]['text'].split(\">\")[-1]\n        prompt_pinyin = extract_chinese_and_convert_to_pinyin(prompt)\n        print(prompt, prompt_pinyin)\n\n        # --- 判断是否启动KWS\n        if not flag_KWS_used:\n            flag_KWS = 1\n        if not flag_KWS:\n            if set_KWS in prompt_pinyin:\n                flag_KWS = 1\n        \n        # --- KWS成功，或不设置KWS\n        if flag_KWS:\n            sv_score = sv_pipeline([os.path.join(set_SV_enroll, \"enroll_0.wav\"), TEMP_AUDIO_FILE], thr=thred_sv)\n            print(sv_score)\n            sv_result = sv_score['text']\n            if sv_result == \"yes\":\n                prompt = res[0]['text'].split(\">\")[-1] + \"，回答简短一些，保持50字以内！\"\n                print(\"ASR OUT:\", prompt)\n                # ---------SenceVoice --end----------\n                # -------- 模型推理阶段，将语音识别结果作为大模型Prompt ------\n                messages = [\n                    {\"role\": \"system\", \"content\": \"你叫小千，是一个18岁的女大学生，性格活泼开朗，说话俏皮\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ]\n                text = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                )\n                model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n                generated_ids = model.generate(\n                    **model_inputs,\n                    max_new_tokens=512,\n                )\n                generated_ids = [\n                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n                ]\n\n                output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n                print(\"answer\", output_text)\n\n                # 输入文本\n                text = output_text\n                # 语种识别 -- langid\n                language, confidence = langid.classify(text)\n                # 语种识别 -- langdetect \n                # language = detect(text).split(\"-\")[0]\n\n                language_speaker = {\n                \"ja\" : \"ja-JP-NanamiNeural\",            # ok\n                \"fr\" : \"fr-FR-DeniseNeural\",            # ok\n                \"es\" : \"ca-ES-JoanaNeural\",             # ok\n                \"de\" : \"de-DE-KatjaNeural\",             # ok\n                \"zh\" : \"zh-CN-XiaoyiNeural\",            # ok\n                \"en\" : \"en-US-AnaNeural\",               # ok\n                }\n\n                if language not in language_speaker.keys():\n                    used_speaker = \"zh-CN-XiaoyiNeural\"\n                else:\n                    used_speaker = language_speaker[language]\n                    print(\"检测到语种：\", language, \"使用音色：\", language_speaker[language])\n\n                asyncio.run(amain(text, used_speaker, os.path.join(folder_path,f\"sft_{audio_file_count}.mp3\")))\n                play_audio(f'{folder_path}/sft_{audio_file_count}.mp3')\n            else:\n                text = \"很抱歉，你不是我的主人哦，我无法为您服务\"\n                system_introduction(text)\n        else:\n            text = \"很抱歉，唤醒词错误，我无法为您服务。请说出正确的唤醒词哦\"\n            system_introduction(text)\n\n# 主函数\nif __name__ == \"__main__\":\n\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        # video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        # video_thread.start()\n\n        flag_info = f'{flag_sv_used}-{flag_KWS_used}'\n        dict_flag_info = {\n            \"1-1\": \"您已开启声纹识别和关键词唤醒，\",\n            \"0-1\":\"您已开启关键词唤醒\",\n            \"1-0\":\"您已开启声纹识别\",\n            \"0-0\":\"\",\n        }\n        if flag_sv_used or flag_KWS_used:\n            text = dict_flag_info[flag_info]\n            system_introduction(text)\n\n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        # video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "7.8_realTime_AV_video.py", "content": "import cv2\nimport pyaudio\nimport wave\nimport threading\nimport numpy as np\nimport time\nfrom queue import Queue\nimport webrtcvad\nimport os\nimport threading\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nfrom funasr import AutoModel\nimport pygame\nimport edge_tts\nimport asyncio\nfrom time import sleep\n\n# 参数设置\nAUDIO_RATE = 16000        # 音频采样率\nAUDIO_CHANNELS = 1        # 单声道\nCHUNK = 1024              # 音频块大小\nVAD_MODE = 3              # VAD 模式 (0-3, 数字越大越敏感)\nOUTPUT_DIR = \"./output\"   # 输出目录\nNO_SPEECH_THRESHOLD = 1   # 无效语音阈值，单位：秒\n\n# 确保输出目录存在\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 队列用于音频和视频同步缓存\naudio_queue = Queue()\nvideo_queue = Queue()\n\n# 全局变量\nlast_active_time = time.time()\nrecording_active = True\nsegments_to_save = []\nsaved_intervals = []\nlast_vad_end_time = 0  # 上次保存的 VAD 有效段结束时间\n\n# 初始化 WebRTC VAD\nvad = webrtcvad.Vad()\nvad.set_mode(VAD_MODE)\n\n# 音频录制线程\ndef audio_recorder():\n    global audio_queue, recording_active, last_active_time, segments_to_save, last_vad_end_time\n    \n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=AUDIO_CHANNELS,\n                    rate=AUDIO_RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n    \n    audio_buffer = []\n    print(\"音频录制已开始\")\n    \n    while recording_active:\n        data = stream.read(CHUNK)\n        audio_buffer.append(data)\n        \n        # 每 0.5 秒检测一次 VAD\n        if len(audio_buffer) * CHUNK / AUDIO_RATE >= 0.5:\n            # 拼接音频数据并检测 VAD\n            raw_audio = b''.join(audio_buffer)\n            vad_result = check_vad_activity(raw_audio)\n            \n            if vad_result:\n                print(\"检测到语音活动\")\n                last_active_time = time.time()\n                segments_to_save.append((raw_audio, time.time()))\n            else:\n                print(\"静音中...\")\n            \n            audio_buffer = []  # 清空缓冲区\n        \n        # 检查无效语音时间\n        if time.time() - last_active_time > NO_SPEECH_THRESHOLD:\n            # 检查是否需要保存\n            if segments_to_save and segments_to_save[-1][1] > last_vad_end_time:\n                save_audio_video()\n                last_active_time = time.time()\n            else:\n                pass\n                # print(\"无新增语音段，跳过保存\")\n    \n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n# 视频录制线程\ndef video_recorder():\n    global video_queue, recording_active\n    \n    cap = cv2.VideoCapture(0)  # 使用默认摄像头\n    print(\"视频录制已开始\")\n    \n    while recording_active:\n        ret, frame = cap.read()\n        if ret:\n            video_queue.put((frame, time.time()))\n            \n            # 实时显示摄像头画面\n            cv2.imshow(\"Real Camera\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):  # 按 Q 键退出\n                break\n        else:\n            print(\"无法获取摄像头画面\")\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# 检测 VAD 活动\ndef check_vad_activity(audio_data):\n    # 将音频数据分块检测\n    num, rate = 0, 0.4\n    step = int(AUDIO_RATE * 0.02)  # 20ms 块大小\n    flag_rate = round(rate * len(audio_data) // step)\n\n    for i in range(0, len(audio_data), step):\n        chunk = audio_data[i:i + step]\n        if len(chunk) == step:\n            if vad.is_speech(chunk, sample_rate=AUDIO_RATE):\n                num += 1\n\n    if num > flag_rate:\n        return True\n    return False\n\n# 保存音频和视频\ndef save_audio_video():\n    global segments_to_save, video_queue, last_vad_end_time, saved_intervals\n    \n    if not segments_to_save:\n        return\n    \n    # 获取有效段的时间范围\n    start_time = segments_to_save[0][1]\n    end_time = segments_to_save[-1][1]\n    \n    # 检查是否与之前的片段重叠\n    if saved_intervals and saved_intervals[-1][1] >= start_time:\n        print(\"当前片段与之前片段重叠，跳过保存\")\n        segments_to_save.clear()\n        return\n    \n    # 保存音频\n    audio_frames = [seg[0] for seg in segments_to_save]\n    audio_output_path = f\"{OUTPUT_DIR}/audio_0.wav\"\n    wf = wave.open(audio_output_path, 'wb')\n    wf.setnchannels(AUDIO_CHANNELS)\n    wf.setsampwidth(2)  # 16-bit PCM\n    wf.setframerate(AUDIO_RATE)\n    wf.writeframes(b''.join(audio_frames))\n    wf.close()\n    print(f\"音频保存至 {audio_output_path}\")\n    \n    # 保存视频\n    video_frames = []\n    while not video_queue.empty():\n        frame, timestamp = video_queue.get()\n        if start_time <= timestamp <= end_time:\n            video_frames.append(frame)\n    \n    if video_frames:\n        video_output_path = f\"{OUTPUT_DIR}/video_0.avi\"\n        out = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n        for frame in video_frames:\n            out.write(frame)\n        out.release()\n        print(f\"视频保存至 {video_output_path}\")\n        Inference()\n    else:\n        pass\n        # print(\"无可保存的视频帧\")\n    \n    # 记录保存的区间\n    saved_intervals.append((start_time, end_time))\n    \n    # 清空缓冲区\n    segments_to_save.clear()\n\n# --- 播放音频 -\ndef play_audio(file_path):\n    try:\n        pygame.mixer.init()\n        pygame.mixer.music.load(file_path)\n        pygame.mixer.music.play()\n        while pygame.mixer.music.get_busy():\n            time.sleep(1)  # 等待音频播放结束\n        print(\"播放完成！\")\n    except Exception as e:\n        print(f\"播放失败: {e}\")\n    finally:\n        pygame.mixer.quit()\n\nasync def amain(TEXT, VOICE, OUTPUT_FILE) -> None:\n    \"\"\"Main function\"\"\"\n    communicate = edge_tts.Communicate(TEXT, VOICE)\n    await communicate.save(OUTPUT_FILE)\n\n# -------------- Load QWen2-VL Model ------------\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# ------- 设置分辨率，降低现存占用 -------\nmin_pixels = 256*28*28\nmax_pixels = 512*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n# --------------------------------------\n\n# -------- SenceVoice 语音识别 --模型加载-----\nmodel_dir = r\"E:\\2_PYTHON\\Project\\GPT\\QWen\\pretrained_models\\SenseVoiceSmall\"\nmodel_senceVoice = AutoModel( model=model_dir, trust_remote_code=True, )\nfolder_path = \"./Test_QWen2_VL/\"\n\ndef Inference(TEMP_VIDEO_FILE=f\"{OUTPUT_DIR}/video_0.avi\", TEMP_AUDIO_FILE=f\"{OUTPUT_DIR}/audio_0.wav\"):\n    \n    cap = cv2.VideoCapture(TEMP_VIDEO_FILE)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    S_index = [0.2, 0.4, 0.6, 0.8]\n    frame_index = [int(total_frames * i) for i in S_index]\n    # 设置视频帧位置\n    for idx in frame_index:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"无法读取帧索引 {idx}\")\n        else:\n            # 保存帧\n            file_path = os.path.join(folder_path, f\"captured_image{idx}.jpg\")  # 设置保存路径\n            cv2.imwrite(file_path, frame)\n\n    # -------- SenceVoice 推理 ---------\n    input_file = (TEMP_AUDIO_FILE)\n    res = model_senceVoice.generate(\n        input=input_file,\n        cache={},\n        language=\"auto\", # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n        use_itn=False,\n    )\n    prompt = res[0]['text'].split(\">\")[-1]\n    print(\"ASR OUT:\", prompt)\n    # ---------SenceVoice --end----------\n\n    # -------- QWen2-VL 模型推理 ---------\n    # Messages containing a images list as a video and a text query\n    # messages = [\n    #     {\n    #         \"role\": \"user\",\n    #         \"content\": [\n    #             {\n    #                 \"type\": \"video\",\n    #                 \"video\": [\n    #                     f'{os.path.join(folder_path, f\"captured_image{frame_index[0]}.jpg\")}',\n    #                     f'{os.path.join(folder_path, f\"captured_image{frame_index[1]}.jpg\")}',\n    #                     f'{os.path.join(folder_path, f\"captured_image{frame_index[2]}.jpg\")}',\n    #                     f'{os.path.join(folder_path, f\"captured_image{frame_index[3]}.jpg\")}',\n    #                 ],\n    #                 \"fps\": 1.0,\n    #             },\n    #             {\"type\": \"text\", \"text\": f\"{prompt},同时描述这段视频\"},\n    #         ],\n    #     }\n    # ]\n\n    # Messages containing a video and a text query\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"video\",\n                    \"video\": f\"{OUTPUT_DIR}/video_0.avi\",\n                    \"max_pixels\": 360 * 420,\n                    \"fps\": 1.0,\n                },\n                {\"type\": \"text\", \"text\": f\"{prompt}\"},\n            ],\n        }\n    ]\n\n    # Preparation for inference\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # Inference\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    print(output_text)\n\n    # 输入文本\n    text = output_text[0]\n    # asyncio.run(amain(text, \"zh-CN-YunxiaNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    asyncio.run(amain(text, \"zh-CN-XiaoyiNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-YunjianNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n    # asyncio.run(amain(text, \"zh-CN-shaanxi-XiaoniNeural\", os.path.join(folder_path,\"sft_0.mp3\")))\n    # play_audio(f'{folder_path}/sft_0.mp3')\n\n# 主函数\nif __name__ == \"__main__\":\n    try:\n        # 启动音视频录制线程\n        audio_thread = threading.Thread(target=audio_recorder)\n        video_thread = threading.Thread(target=video_recorder)\n        audio_thread.start()\n        video_thread.start()\n        \n        print(\"按 Ctrl+C 停止录制\")\n        while True:\n            time.sleep(1)\n    \n    except KeyboardInterrupt:\n        print(\"录制停止中...\")\n        recording_active = False\n        audio_thread.join()\n        video_thread.join()\n        print(\"录制已停止\")\n"}
{"type": "source_file", "path": "cosyvoice/bin/export_jit.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nlogging.getLogger('matplotlib').setLevel(logging.WARNING)\nimport os\nimport sys\nimport torch\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append('{}/../..'.format(ROOT_DIR))\nsys.path.append('{}/../../third_party/Matcha-TTS'.format(ROOT_DIR))\nfrom cosyvoice.cli.cosyvoice import CosyVoice\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='export your model for deployment')\n    parser.add_argument('--model_dir',\n                        type=str,\n                        default='pretrained_models/CosyVoice-300M',\n                        help='local path')\n    args = parser.parse_args()\n    print(args)\n    return args\n\n\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(levelname)s %(message)s')\n\n    torch._C._jit_set_fusion_strategy([('STATIC', 1)])\n    torch._C._jit_set_profiling_mode(False)\n    torch._C._jit_set_profiling_executor(False)\n\n    cosyvoice = CosyVoice(args.model_dir, load_jit=False, load_onnx=False)\n\n    # 1. export llm text_encoder\n    llm_text_encoder = cosyvoice.model.llm.text_encoder.half()\n    script = torch.jit.script(llm_text_encoder)\n    script = torch.jit.freeze(script)\n    script = torch.jit.optimize_for_inference(script)\n    script.save('{}/llm.text_encoder.fp16.zip'.format(args.model_dir))\n\n    # 2. export llm llm\n    llm_llm = cosyvoice.model.llm.llm.half()\n    script = torch.jit.script(llm_llm)\n    script = torch.jit.freeze(script, preserved_attrs=['forward_chunk'])\n    script = torch.jit.optimize_for_inference(script)\n    script.save('{}/llm.llm.fp16.zip'.format(args.model_dir))\n\n    # 3. export flow encoder\n    flow_encoder = cosyvoice.model.flow.encoder\n    script = torch.jit.script(flow_encoder)\n    script = torch.jit.freeze(script)\n    script = torch.jit.optimize_for_inference(script)\n    script.save('{}/flow.encoder.fp32.zip'.format(args.model_dir))\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "cosyvoice/cli/model.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\nimport numpy as np\nimport threading\nimport time\nfrom torch.nn import functional as F\nfrom contextlib import nullcontext\nimport uuid\nfrom cosyvoice.utils.common import fade_in_out\n\n\nclass CosyVoiceModel:\n\n    def __init__(self,\n                 llm: torch.nn.Module,\n                 flow: torch.nn.Module,\n                 hift: torch.nn.Module,\n                 fp16: bool):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.llm = llm\n        self.flow = flow\n        self.hift = hift\n        self.fp16 = fp16\n        self.token_min_hop_len = 2 * self.flow.input_frame_rate\n        self.token_max_hop_len = 4 * self.flow.input_frame_rate\n        self.token_overlap_len = 20\n        # mel fade in out\n        self.mel_overlap_len = int(self.token_overlap_len / self.flow.input_frame_rate * 22050 / 256)\n        self.mel_window = np.hamming(2 * self.mel_overlap_len)\n        # hift cache\n        self.mel_cache_len = 20\n        self.source_cache_len = int(self.mel_cache_len * 256)\n        # speech fade in out\n        self.speech_window = np.hamming(2 * self.source_cache_len)\n        # rtf and decoding related\n        self.stream_scale_factor = 1\n        assert self.stream_scale_factor >= 1, 'stream_scale_factor should be greater than 1, change it according to your actual rtf'\n        self.llm_context = torch.cuda.stream(torch.cuda.Stream(self.device)) if torch.cuda.is_available() else nullcontext()\n        self.lock = threading.Lock()\n        # dict used to store session related variable\n        self.tts_speech_token_dict = {}\n        self.llm_end_dict = {}\n        self.mel_overlap_dict = {}\n        self.flow_cache_dict = {}\n        self.hift_cache_dict = {}\n\n    def load(self, llm_model, flow_model, hift_model):\n        self.llm.load_state_dict(torch.load(llm_model, map_location=self.device), strict=False)\n        self.llm.to(self.device).eval()\n        if self.fp16 is True:\n            self.llm.half()\n        self.flow.load_state_dict(torch.load(flow_model, map_location=self.device), strict=False)\n        self.flow.to(self.device).eval()\n        # in case hift_model is a hifigan model\n        hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_model, map_location=self.device).items()}\n        self.hift.load_state_dict(hift_state_dict, strict=False)\n        self.hift.to(self.device).eval()\n\n    def load_jit(self, llm_text_encoder_model, llm_llm_model, flow_encoder_model):\n        assert self.fp16 is True, \"we only provide fp16 jit model, set fp16=True if you want to use jit model\"\n        llm_text_encoder = torch.jit.load(llm_text_encoder_model, map_location=self.device)\n        self.llm.text_encoder = llm_text_encoder\n        llm_llm = torch.jit.load(llm_llm_model, map_location=self.device)\n        self.llm.llm = llm_llm\n        flow_encoder = torch.jit.load(flow_encoder_model, map_location=self.device)\n        self.flow.encoder = flow_encoder\n\n    def load_onnx(self, flow_decoder_estimator_model):\n        import onnxruntime\n        option = onnxruntime.SessionOptions()\n        option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n        option.intra_op_num_threads = 1\n        providers = ['CUDAExecutionProvider' if torch.cuda.is_available() else 'CPUExecutionProvider']\n        del self.flow.decoder.estimator\n        self.flow.decoder.estimator = onnxruntime.InferenceSession(flow_decoder_estimator_model, sess_options=option, providers=providers)\n\n    def llm_job(self, text, prompt_text, llm_prompt_speech_token, llm_embedding, uuid):\n        if self.fp16 is True:\n            llm_embedding = llm_embedding.half()\n        with self.llm_context:\n            for i in self.llm.inference(text=text.to(self.device),\n                                        text_len=torch.tensor([text.shape[1]], dtype=torch.int32).to(self.device),\n                                        prompt_text=prompt_text.to(self.device),\n                                        prompt_text_len=torch.tensor([prompt_text.shape[1]], dtype=torch.int32).to(self.device),\n                                        prompt_speech_token=llm_prompt_speech_token.to(self.device),\n                                        prompt_speech_token_len=torch.tensor([llm_prompt_speech_token.shape[1]], dtype=torch.int32).to(self.device),\n                                        embedding=llm_embedding.to(self.device)):\n                self.tts_speech_token_dict[uuid].append(i)\n        self.llm_end_dict[uuid] = True\n\n    def token2wav(self, token, prompt_token, prompt_feat, embedding, uuid, finalize=False, speed=1.0):\n        tts_mel, flow_cache = self.flow.inference(token=token.to(self.device),\n                                                  token_len=torch.tensor([token.shape[1]], dtype=torch.int32).to(self.device),\n                                                  prompt_token=prompt_token.to(self.device),\n                                                  prompt_token_len=torch.tensor([prompt_token.shape[1]], dtype=torch.int32).to(self.device),\n                                                  prompt_feat=prompt_feat.to(self.device),\n                                                  prompt_feat_len=torch.tensor([prompt_feat.shape[1]], dtype=torch.int32).to(self.device),\n                                                  embedding=embedding.to(self.device),\n                                                  flow_cache=self.flow_cache_dict[uuid])\n        self.flow_cache_dict[uuid] = flow_cache\n\n        # mel overlap fade in out\n        if self.mel_overlap_dict[uuid].shape[2] != 0:\n            tts_mel = fade_in_out(tts_mel, self.mel_overlap_dict[uuid], self.mel_window)\n        # append hift cache\n        if self.hift_cache_dict[uuid] is not None:\n            hift_cache_mel, hift_cache_source = self.hift_cache_dict[uuid]['mel'], self.hift_cache_dict[uuid]['source']\n            tts_mel = torch.concat([hift_cache_mel, tts_mel], dim=2)\n        else:\n            hift_cache_source = torch.zeros(1, 1, 0)\n        # keep overlap mel and hift cache\n        if finalize is False:\n            self.mel_overlap_dict[uuid] = tts_mel[:, :, -self.mel_overlap_len:]\n            tts_mel = tts_mel[:, :, :-self.mel_overlap_len]\n            tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)\n            if self.hift_cache_dict[uuid] is not None:\n                tts_speech = fade_in_out(tts_speech, self.hift_cache_dict[uuid]['speech'], self.speech_window)\n            self.hift_cache_dict[uuid] = {'mel': tts_mel[:, :, -self.mel_cache_len:],\n                                          'source': tts_source[:, :, -self.source_cache_len:],\n                                          'speech': tts_speech[:, -self.source_cache_len:]}\n            tts_speech = tts_speech[:, :-self.source_cache_len]\n        else:\n            if speed != 1.0:\n                assert self.hift_cache_dict[uuid] is None, 'speed change only support non-stream inference mode'\n                tts_mel = F.interpolate(tts_mel, size=int(tts_mel.shape[2] / speed), mode='linear')\n            tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)\n            if self.hift_cache_dict[uuid] is not None:\n                tts_speech = fade_in_out(tts_speech, self.hift_cache_dict[uuid]['speech'], self.speech_window)\n        return tts_speech\n\n    def tts(self, text, flow_embedding, llm_embedding=torch.zeros(0, 192),\n            prompt_text=torch.zeros(1, 0, dtype=torch.int32),\n            llm_prompt_speech_token=torch.zeros(1, 0, dtype=torch.int32),\n            flow_prompt_speech_token=torch.zeros(1, 0, dtype=torch.int32),\n            prompt_speech_feat=torch.zeros(1, 0, 80), stream=False, speed=1.0, **kwargs):\n        # this_uuid is used to track variables related to this inference thread\n        this_uuid = str(uuid.uuid1())\n        with self.lock:\n            self.tts_speech_token_dict[this_uuid], self.llm_end_dict[this_uuid] = [], False\n            self.hift_cache_dict[this_uuid] = None\n            self.mel_overlap_dict[this_uuid] = torch.zeros(1, 80, 0)\n            self.flow_cache_dict[this_uuid] = torch.zeros(1, 80, 0, 2)\n        p = threading.Thread(target=self.llm_job, args=(text, prompt_text, llm_prompt_speech_token, llm_embedding, this_uuid))\n        p.start()\n        if stream is True:\n            token_hop_len = self.token_min_hop_len\n            while True:\n                time.sleep(0.1)\n                if len(self.tts_speech_token_dict[this_uuid]) >= token_hop_len + self.token_overlap_len:\n                    this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid][:token_hop_len + self.token_overlap_len]) \\\n                        .unsqueeze(dim=0)\n                    this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                                     prompt_token=flow_prompt_speech_token,\n                                                     prompt_feat=prompt_speech_feat,\n                                                     embedding=flow_embedding,\n                                                     uuid=this_uuid,\n                                                     finalize=False)\n                    yield {'tts_speech': this_tts_speech.cpu()}\n                    with self.lock:\n                        self.tts_speech_token_dict[this_uuid] = self.tts_speech_token_dict[this_uuid][token_hop_len:]\n                    # increase token_hop_len for better speech quality\n                    token_hop_len = min(self.token_max_hop_len, int(token_hop_len * self.stream_scale_factor))\n                if self.llm_end_dict[this_uuid] is True and len(self.tts_speech_token_dict[this_uuid]) < token_hop_len + self.token_overlap_len:\n                    break\n            p.join()\n            # deal with remain tokens, make sure inference remain token len equals token_hop_len when cache_speech is not None\n            this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid]).unsqueeze(dim=0)\n            this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                             prompt_token=flow_prompt_speech_token,\n                                             prompt_feat=prompt_speech_feat,\n                                             embedding=flow_embedding,\n                                             uuid=this_uuid,\n                                             finalize=True)\n            yield {'tts_speech': this_tts_speech.cpu()}\n        else:\n            # deal with all tokens\n            p.join()\n            this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid]).unsqueeze(dim=0)\n            this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                             prompt_token=flow_prompt_speech_token,\n                                             prompt_feat=prompt_speech_feat,\n                                             embedding=flow_embedding,\n                                             uuid=this_uuid,\n                                             finalize=True,\n                                             speed=speed)\n            yield {'tts_speech': this_tts_speech.cpu()}\n        with self.lock:\n            self.tts_speech_token_dict.pop(this_uuid)\n            self.llm_end_dict.pop(this_uuid)\n            self.mel_overlap_dict.pop(this_uuid)\n            self.hift_cache_dict.pop(this_uuid)\n\n    def vc(self, source_speech_token, flow_prompt_speech_token, prompt_speech_feat, flow_embedding, stream=False, speed=1.0, **kwargs):\n        # this_uuid is used to track variables related to this inference thread\n        this_uuid = str(uuid.uuid1())\n        with self.lock:\n            self.tts_speech_token_dict[this_uuid], self.llm_end_dict[this_uuid] = source_speech_token.flatten().tolist(), True\n            self.hift_cache_dict[this_uuid] = None\n            self.mel_overlap_dict[this_uuid] = torch.zeros(1, 80, 0)\n            self.flow_cache_dict[this_uuid] = torch.zeros(1, 80, 0, 2)\n        if stream is True:\n            token_hop_len = self.token_min_hop_len\n            while True:\n                if len(self.tts_speech_token_dict[this_uuid]) >= token_hop_len + self.token_overlap_len:\n                    this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid][:token_hop_len + self.token_overlap_len]) \\\n                        .unsqueeze(dim=0)\n                    this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                                     prompt_token=flow_prompt_speech_token,\n                                                     prompt_feat=prompt_speech_feat,\n                                                     embedding=flow_embedding,\n                                                     uuid=this_uuid,\n                                                     finalize=False)\n                    yield {'tts_speech': this_tts_speech.cpu()}\n                    with self.lock:\n                        self.tts_speech_token_dict[this_uuid] = self.tts_speech_token_dict[this_uuid][token_hop_len:]\n                    # increase token_hop_len for better speech quality\n                    token_hop_len = min(self.token_max_hop_len, int(token_hop_len * self.stream_scale_factor))\n                if self.llm_end_dict[this_uuid] is True and len(self.tts_speech_token_dict[this_uuid]) < token_hop_len + self.token_overlap_len:\n                    break\n            # deal with remain tokens, make sure inference remain token len equals token_hop_len when cache_speech is not None\n            this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid]).unsqueeze(dim=0)\n            this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                             prompt_token=flow_prompt_speech_token,\n                                             prompt_feat=prompt_speech_feat,\n                                             embedding=flow_embedding,\n                                             uuid=this_uuid,\n                                             finalize=True)\n            yield {'tts_speech': this_tts_speech.cpu()}\n        else:\n            # deal with all tokens\n            this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid]).unsqueeze(dim=0)\n            this_tts_speech = self.token2wav(token=this_tts_speech_token,\n                                             prompt_token=flow_prompt_speech_token,\n                                             prompt_feat=prompt_speech_feat,\n                                             embedding=flow_embedding,\n                                             uuid=this_uuid,\n                                             finalize=True,\n                                             speed=speed)\n            yield {'tts_speech': this_tts_speech.cpu()}\n        with self.lock:\n            self.tts_speech_token_dict.pop(this_uuid)\n            self.llm_end_dict.pop(this_uuid)\n            self.mel_overlap_dict.pop(this_uuid)\n            self.hift_cache_dict.pop(this_uuid)\n"}
{"type": "source_file", "path": "cosyvoice/flow/length_regulator.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Tuple\nimport torch.nn as nn\nimport torch\nfrom torch.nn import functional as F\nfrom cosyvoice.utils.mask import make_pad_mask\n\n\nclass InterpolateRegulator(nn.Module):\n    def __init__(\n            self,\n            channels: int,\n            sampling_ratios: Tuple,\n            out_channels: int = None,\n            groups: int = 1,\n    ):\n        super().__init__()\n        self.sampling_ratios = sampling_ratios\n        out_channels = out_channels or channels\n        model = nn.ModuleList([])\n        if len(sampling_ratios) > 0:\n            for _ in sampling_ratios:\n                module = nn.Conv1d(channels, channels, 3, 1, 1)\n                norm = nn.GroupNorm(groups, channels)\n                act = nn.Mish()\n                model.extend([module, norm, act])\n        model.append(\n            nn.Conv1d(channels, out_channels, 1, 1)\n        )\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x, ylens=None):\n        # x in (B, T, D)\n        mask = (~make_pad_mask(ylens)).to(x).unsqueeze(-1)\n        x = F.interpolate(x.transpose(1, 2).contiguous(), size=ylens.max(), mode='linear')\n        out = self.model(x).transpose(1, 2).contiguous()\n        olens = ylens\n        return out * mask, olens\n\n    def inference(self, x1, x2, mel_len1, mel_len2, input_frame_rate=50):\n        # in inference mode, interploate prompt token and token(head/mid/tail) seprately, so we can get a clear separation point of mel\n        # x in (B, T, D)\n        if x2.shape[1] > 40:\n            x2_head = F.interpolate(x2[:, :20].transpose(1, 2).contiguous(), size=int(20 / input_frame_rate * 22050 / 256), mode='linear')\n            x2_mid = F.interpolate(x2[:, 20:-20].transpose(1, 2).contiguous(), size=mel_len2 - int(20 / input_frame_rate * 22050 / 256) * 2,\n                                   mode='linear')\n            x2_tail = F.interpolate(x2[:, -20:].transpose(1, 2).contiguous(), size=int(20 / input_frame_rate * 22050 / 256), mode='linear')\n            x2 = torch.concat([x2_head, x2_mid, x2_tail], dim=2)\n        else:\n            x2 = F.interpolate(x2.transpose(1, 2).contiguous(), size=mel_len2, mode='linear')\n        if x1.shape[1] != 0:\n            x1 = F.interpolate(x1.transpose(1, 2).contiguous(), size=mel_len1, mode='linear')\n            x = torch.concat([x1, x2], dim=2)\n        else:\n            x = x2\n        out = self.model(x).transpose(1, 2).contiguous()\n        return out, mel_len1 + mel_len2\n"}
{"type": "source_file", "path": "cosyvoice/llm/llm.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Dict, Optional, Callable, List, Generator\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence, unpad_sequence\nfrom cosyvoice.utils.common import IGNORE_ID\nfrom cosyvoice.transformer.label_smoothing_loss import LabelSmoothingLoss\nfrom cosyvoice.utils.common import th_accuracy\n\n\nclass TransformerLM(torch.nn.Module):\n    def __init__(\n            self,\n            text_encoder_input_size: int,\n            llm_input_size: int,\n            llm_output_size: int,\n            text_token_size: int,\n            speech_token_size: int,\n            text_encoder: torch.nn.Module,\n            llm: torch.nn.Module,\n            sampling: Callable,\n            length_normalized_loss: bool = True,\n            lsm_weight: float = 0.0,\n            spk_embed_dim: int = 192,\n    ):\n        super().__init__()\n        self.llm_input_size = llm_input_size\n        self.speech_token_size = speech_token_size\n        # 1. build text token inputs related modules\n        self.text_embedding = torch.nn.Embedding(text_token_size, text_encoder_input_size)\n        self.text_encoder = text_encoder\n        self.text_encoder_affine_layer = nn.Linear(\n            self.text_encoder.output_size(),\n            llm_input_size\n        )\n\n        # 2. build speech token language model related modules\n        self.sos_eos = 0\n        self.task_id = 1\n        self.llm_embedding = torch.nn.Embedding(2, llm_input_size)\n        self.llm = llm\n        self.llm_decoder = nn.Linear(llm_output_size, speech_token_size + 1)\n        self.criterion_ce = LabelSmoothingLoss(\n            size=speech_token_size + 1,\n            padding_idx=IGNORE_ID,\n            smoothing=lsm_weight,\n            normalize_length=length_normalized_loss,\n        )\n\n        # 3. [Optional] build speech token related modules\n        self.speech_embedding = torch.nn.Embedding(speech_token_size, llm_input_size)\n        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, llm_input_size)\n\n        # 4. sampling method\n        self.sampling = sampling\n\n    def encode(\n            self,\n            text: torch.Tensor,\n            text_lengths: torch.Tensor,\n    ):\n        encoder_out, encoder_mask = self.text_encoder(text, text_lengths, decoding_chunk_size=1, num_decoding_left_chunks=-1)\n        encoder_out_lens = encoder_mask.squeeze(1).sum(1)\n        encoder_out = self.text_encoder_affine_layer(encoder_out)\n        return encoder_out, encoder_out_lens\n\n    def pad_unpad_sequence(self, sos_eos_emb, embedding, text_token, text_token_len, task_id_emb, speech_token, speech_token_len):\n        text_token = unpad_sequence(text_token, text_token_len.cpu(), batch_first=True)\n        speech_token = unpad_sequence(speech_token, speech_token_len.cpu(), batch_first=True)\n        lm_input = [torch.concat([sos_eos_emb.squeeze(dim=0), embedding[i], text_token[i], task_id_emb.squeeze(dim=0), speech_token[i]], dim=0)\n                    for i in range(len(text_token))]\n        lm_input_len = torch.tensor([i.size(0) for i in lm_input], dtype=torch.int32)\n        lm_input = pad_sequence(lm_input, batch_first=True, padding_value=IGNORE_ID)\n        return lm_input, lm_input_len\n\n    def forward(\n            self,\n            batch: dict,\n            device: torch.device,\n    ) -> Dict[str, Optional[torch.Tensor]]:\n        \"\"\"\n        Args:\n            text: (B, L, D)\n            text_lengths: (B,)\n            audio: (B, T, N) or (B, T)\n            audio_lengths: (B,)\n        \"\"\"\n        text_token = batch['text_token'].to(device)\n        text_token_len = batch['text_token_len'].to(device)\n        speech_token = batch['speech_token'].to(device)\n        speech_token_len = batch['speech_token_len'].to(device)\n        embedding = batch['embedding'].to(device)\n\n        # 1. prepare llm_target\n        lm_target = [torch.tensor([IGNORE_ID] * (2 + text_token_len[i]) + speech_token[i, :speech_token_len[i]].tolist() +\n                                  [self.speech_token_size]) for i in range(text_token.size(0))]\n        lm_target = pad_sequence(lm_target, batch_first=True, padding_value=IGNORE_ID).to(device)\n\n        # 1. encode text_token\n        text_token = self.text_embedding(text_token)\n        text_token, text_token_len = self.encode(text_token, text_token_len)\n\n        # 2. embedding projection\n        embedding = F.normalize(embedding, dim=1)\n        embedding = self.spk_embed_affine_layer(embedding)\n        embedding = embedding.unsqueeze(1)\n\n        # 3. eos and task_id\n        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)\n        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)\n\n        # 4. encode speech_token\n        speech_token = self.speech_embedding(speech_token)\n\n        # 5. unpad and pad\n        lm_input, lm_input_len = self.pad_unpad_sequence(sos_eos_emb, embedding, text_token, text_token_len,\n                                                         task_id_emb, speech_token, speech_token_len)\n\n        # 6. run lm forward\n        lm_output, lm_output_mask = self.llm(lm_input, lm_input_len.to(device))\n        logits = self.llm_decoder(lm_output)\n        loss = self.criterion_ce(logits, lm_target)\n        acc = th_accuracy(logits.view(-1, self.speech_token_size + 1), lm_target, ignore_label=IGNORE_ID)\n        return {'loss': loss, 'acc': acc}\n\n    def sampling_ids(\n            self,\n            weighted_scores: torch.Tensor,\n            decoded_tokens: List,\n            sampling: int,\n            ignore_eos: bool = True,\n    ):\n        while True:\n            top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)\n            if (not ignore_eos) or (self.speech_token_size not in top_ids):\n                break\n        return top_ids\n\n    @torch.inference_mode()\n    def inference(\n            self,\n            text: torch.Tensor,\n            text_len: torch.Tensor,\n            prompt_text: torch.Tensor,\n            prompt_text_len: torch.Tensor,\n            prompt_speech_token: torch.Tensor,\n            prompt_speech_token_len: torch.Tensor,\n            embedding: torch.Tensor,\n            sampling: int = 25,\n            max_token_text_ratio: float = 20,\n            min_token_text_ratio: float = 2,\n    ) -> Generator[torch.Tensor, None, None]:\n        device = text.device\n        text = torch.concat([prompt_text, text], dim=1)\n        text_len += prompt_text_len\n        text = self.text_embedding(text)\n\n        # 1. encode text\n        text, text_len = self.encode(text, text_len)\n\n        # 2. encode embedding\n        if embedding.shape[0] != 0:\n            embedding = F.normalize(embedding, dim=1)\n            embedding = self.spk_embed_affine_layer(embedding)\n            embedding = embedding.unsqueeze(dim=1)\n        else:\n            embedding = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)\n\n        # 3. concat llm_input\n        sos_eos_emb = self.llm_embedding.weight[self.sos_eos].reshape(1, 1, -1)\n        task_id_emb = self.llm_embedding.weight[self.task_id].reshape(1, 1, -1)\n        if prompt_speech_token_len != 0:\n            prompt_speech_token_emb = self.speech_embedding(prompt_speech_token)\n        else:\n            prompt_speech_token_emb = torch.zeros(1, 0, self.llm_input_size, dtype=text.dtype).to(device)\n        lm_input = torch.concat([sos_eos_emb, embedding, text, task_id_emb, prompt_speech_token_emb], dim=1)\n\n        # 4. cal min/max_length\n        min_len = int((text_len - prompt_text_len) * min_token_text_ratio)\n        max_len = int((text_len - prompt_text_len) * max_token_text_ratio)\n\n        # 5. step by step decode\n        out_tokens = []\n        offset = 0\n        att_cache, cnn_cache = torch.zeros((0, 0, 0, 0), device=lm_input.device), torch.zeros((0, 0, 0, 0), device=lm_input.device)\n        for i in range(max_len):\n            y_pred, att_cache, cnn_cache = self.llm.forward_chunk(lm_input, offset=offset, required_cache_size=-1,\n                                                                  att_cache=att_cache, cnn_cache=cnn_cache,\n                                                                  att_mask=torch.tril(torch.ones((1, lm_input.shape[1], lm_input.shape[1]),\n                                                                                                 device=lm_input.device)).to(torch.bool))\n            logp = self.llm_decoder(y_pred[:, -1]).log_softmax(dim=-1)\n            top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()\n            if top_ids == self.speech_token_size:\n                break\n            # in stream mode, yield token one by one\n            yield top_ids\n            out_tokens.append(top_ids)\n            offset += lm_input.size(1)\n            lm_input = self.speech_embedding.weight[top_ids].reshape(1, 1, -1)\n"}
{"type": "source_file", "path": "cosyvoice/bin/export_onnx.py", "content": "# Copyright (c) 2024 Antgroup Inc (authors: Zhoubofan, hexisyztem@icloud.com)\n# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nlogging.getLogger('matplotlib').setLevel(logging.WARNING)\nimport os\nimport sys\nimport onnxruntime\nimport random\nimport torch\nfrom tqdm import tqdm\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append('{}/../..'.format(ROOT_DIR))\nsys.path.append('{}/../../third_party/Matcha-TTS'.format(ROOT_DIR))\nfrom cosyvoice.cli.cosyvoice import CosyVoice\n\n\ndef get_dummy_input(batch_size, seq_len, out_channels, device):\n    x = torch.rand((batch_size, out_channels, seq_len), dtype=torch.float32, device=device)\n    mask = torch.ones((batch_size, 1, seq_len), dtype=torch.float32, device=device)\n    mu = torch.rand((batch_size, out_channels, seq_len), dtype=torch.float32, device=device)\n    t = torch.rand((batch_size), dtype=torch.float32, device=device)\n    spks = torch.rand((batch_size, out_channels), dtype=torch.float32, device=device)\n    cond = torch.rand((batch_size, out_channels, seq_len), dtype=torch.float32, device=device)\n    return x, mask, mu, t, spks, cond\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='export your model for deployment')\n    parser.add_argument('--model_dir',\n                        type=str,\n                        default='pretrained_models/CosyVoice-300M',\n                        help='local path')\n    args = parser.parse_args()\n    print(args)\n    return args\n\n\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(levelname)s %(message)s')\n\n    cosyvoice = CosyVoice(args.model_dir, load_jit=False, load_onnx=False)\n\n    # 1. export flow decoder estimator\n    estimator = cosyvoice.model.flow.decoder.estimator\n\n    device = cosyvoice.model.device\n    batch_size, seq_len = 1, 256\n    out_channels = cosyvoice.model.flow.decoder.estimator.out_channels\n    x, mask, mu, t, spks, cond = get_dummy_input(batch_size, seq_len, out_channels, device)\n    torch.onnx.export(\n        estimator,\n        (x, mask, mu, t, spks, cond),\n        '{}/flow.decoder.estimator.fp32.onnx'.format(args.model_dir),\n        export_params=True,\n        opset_version=18,\n        do_constant_folding=True,\n        input_names=['x', 'mask', 'mu', 't', 'spks', 'cond'],\n        output_names=['estimator_out'],\n        dynamic_axes={\n            'x': {0: 'batch_size', 2: 'seq_len'},\n            'mask': {0: 'batch_size', 2: 'seq_len'},\n            'mu': {0: 'batch_size', 2: 'seq_len'},\n            'cond': {0: 'batch_size', 2: 'seq_len'},\n            't': {0: 'batch_size'},\n            'spks': {0: 'batch_size'},\n            'estimator_out': {0: 'batch_size', 2: 'seq_len'},\n        }\n    )\n\n    # 2. test computation consistency\n    option = onnxruntime.SessionOptions()\n    option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n    option.intra_op_num_threads = 1\n    providers = ['CUDAExecutionProvider' if torch.cuda.is_available() else 'CPUExecutionProvider']\n    estimator_onnx = onnxruntime.InferenceSession('{}/flow.decoder.estimator.fp32.onnx'.format(args.model_dir),\n                                                  sess_options=option, providers=providers)\n\n    for _ in tqdm(range(10)):\n        x, mask, mu, t, spks, cond = get_dummy_input(random.randint(1, 6), random.randint(16, 512), out_channels, device)\n        output_pytorch = estimator(x, mask, mu, t, spks, cond)\n        ort_inputs = {\n            'x': x.cpu().numpy(),\n            'mask': mask.cpu().numpy(),\n            'mu': mu.cpu().numpy(),\n            't': t.cpu().numpy(),\n            'spks': spks.cpu().numpy(),\n            'cond': cond.cpu().numpy()\n        }\n        output_onnx = estimator_onnx.run(None, ort_inputs)[0]\n        torch.testing.assert_allclose(output_pytorch, torch.from_numpy(output_onnx).to(device), rtol=1e-2, atol=1e-4)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "cosyvoice/hifigan/generator.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Kai Hu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"HIFI-GAN\"\"\"\n\nfrom typing import Dict, Optional, List\nimport numpy as np\nfrom scipy.signal import get_window\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv1d\nfrom torch.nn import ConvTranspose1d\nfrom torch.nn.utils import remove_weight_norm\nfrom torch.nn.utils import weight_norm\nfrom torch.distributions.uniform import Uniform\n\nfrom cosyvoice.transformer.activation import Snake\nfrom cosyvoice.utils.common import get_padding\nfrom cosyvoice.utils.common import init_weights\n\n\n\"\"\"hifigan based generator implementation.\n\nThis code is modified from https://github.com/jik876/hifi-gan\n ,https://github.com/kan-bayashi/ParallelWaveGAN and\n https://github.com/NVIDIA/BigVGAN\n\n\"\"\"\n\n\nclass ResBlock(torch.nn.Module):\n    \"\"\"Residual block module in HiFiGAN/BigVGAN.\"\"\"\n    def __init__(\n        self,\n        channels: int = 512,\n        kernel_size: int = 3,\n        dilations: List[int] = [1, 3, 5],\n    ):\n        super(ResBlock, self).__init__()\n        self.convs1 = nn.ModuleList()\n        self.convs2 = nn.ModuleList()\n\n        for dilation in dilations:\n            self.convs1.append(\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation,\n                        padding=get_padding(kernel_size, dilation)\n                    )\n                )\n            )\n            self.convs2.append(\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1)\n                    )\n                )\n            )\n        self.convs1.apply(init_weights)\n        self.convs2.apply(init_weights)\n        self.activations1 = nn.ModuleList([\n            Snake(channels, alpha_logscale=False)\n            for _ in range(len(self.convs1))\n        ])\n        self.activations2 = nn.ModuleList([\n            Snake(channels, alpha_logscale=False)\n            for _ in range(len(self.convs2))\n        ])\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for idx in range(len(self.convs1)):\n            xt = self.activations1[idx](x)\n            xt = self.convs1[idx](xt)\n            xt = self.activations2[idx](xt)\n            xt = self.convs2[idx](xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for idx in range(len(self.convs1)):\n            remove_weight_norm(self.convs1[idx])\n            remove_weight_norm(self.convs2[idx])\n\n\nclass SineGen(torch.nn.Module):\n    \"\"\" Definition of sine generator\n    SineGen(samp_rate, harmonic_num = 0,\n            sine_amp = 0.1, noise_std = 0.003,\n            voiced_threshold = 0,\n            flag_for_pulse=False)\n    samp_rate: sampling rate in Hz\n    harmonic_num: number of harmonic overtones (default 0)\n    sine_amp: amplitude of sine-wavefrom (default 0.1)\n    noise_std: std of Gaussian noise (default 0.003)\n    voiced_thoreshold: F0 threshold for U/V classification (default 0)\n    flag_for_pulse: this SinGen is used inside PulseGen (default False)\n    Note: when flag_for_pulse is True, the first time step of a voiced\n        segment is always sin(np.pi) or cos(0)\n    \"\"\"\n\n    def __init__(self, samp_rate, harmonic_num=0,\n                 sine_amp=0.1, noise_std=0.003,\n                 voiced_threshold=0):\n        super(SineGen, self).__init__()\n        self.sine_amp = sine_amp\n        self.noise_std = noise_std\n        self.harmonic_num = harmonic_num\n        self.sampling_rate = samp_rate\n        self.voiced_threshold = voiced_threshold\n\n    def _f02uv(self, f0):\n        # generate uv signal\n        uv = (f0 > self.voiced_threshold).type(torch.float32)\n        return uv\n\n    @torch.no_grad()\n    def forward(self, f0):\n        \"\"\"\n        :param f0: [B, 1, sample_len], Hz\n        :return: [B, 1, sample_len]\n        \"\"\"\n\n        F_mat = torch.zeros((f0.size(0), self.harmonic_num + 1, f0.size(-1))).to(f0.device)\n        for i in range(self.harmonic_num + 1):\n            F_mat[:, i: i + 1, :] = f0 * (i + 1) / self.sampling_rate\n\n        theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n        u_dist = Uniform(low=-np.pi, high=np.pi)\n        phase_vec = u_dist.sample(sample_shape=(f0.size(0), self.harmonic_num + 1, 1)).to(F_mat.device)\n        phase_vec[:, 0, :] = 0\n\n        # generate sine waveforms\n        sine_waves = self.sine_amp * torch.sin(theta_mat + phase_vec)\n\n        # generate uv signal\n        uv = self._f02uv(f0)\n\n        # noise: for unvoiced should be similar to sine_amp\n        #        std = self.sine_amp/3 -> max value ~ self.sine_amp\n        # .       for voiced regions is self.noise_std\n        noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3\n        noise = noise_amp * torch.randn_like(sine_waves)\n\n        # first: set the unvoiced part to 0 by uv\n        # then: additive noise\n        sine_waves = sine_waves * uv + noise\n        return sine_waves, uv, noise\n\n\nclass SourceModuleHnNSF(torch.nn.Module):\n    \"\"\" SourceModule for hn-nsf\n    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n                 add_noise_std=0.003, voiced_threshod=0)\n    sampling_rate: sampling_rate in Hz\n    harmonic_num: number of harmonic above F0 (default: 0)\n    sine_amp: amplitude of sine source signal (default: 0.1)\n    add_noise_std: std of additive Gaussian noise (default: 0.003)\n        note that amplitude of noise in unvoiced is decided\n        by sine_amp\n    voiced_threshold: threhold to set U/V given F0 (default: 0)\n    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n    F0_sampled (batchsize, length, 1)\n    Sine_source (batchsize, length, 1)\n    noise_source (batchsize, length 1)\n    uv (batchsize, length, 1)\n    \"\"\"\n\n    def __init__(self, sampling_rate, upsample_scale, harmonic_num=0, sine_amp=0.1,\n                 add_noise_std=0.003, voiced_threshod=0):\n        super(SourceModuleHnNSF, self).__init__()\n\n        self.sine_amp = sine_amp\n        self.noise_std = add_noise_std\n\n        # to produce sine waveforms\n        self.l_sin_gen = SineGen(sampling_rate, harmonic_num,\n                                 sine_amp, add_noise_std, voiced_threshod)\n\n        # to merge source harmonics into a single excitation\n        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)\n        self.l_tanh = torch.nn.Tanh()\n\n    def forward(self, x):\n        \"\"\"\n        Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n        F0_sampled (batchsize, length, 1)\n        Sine_source (batchsize, length, 1)\n        noise_source (batchsize, length 1)\n        \"\"\"\n        # source for harmonic branch\n        with torch.no_grad():\n            sine_wavs, uv, _ = self.l_sin_gen(x.transpose(1, 2))\n            sine_wavs = sine_wavs.transpose(1, 2)\n            uv = uv.transpose(1, 2)\n        sine_merge = self.l_tanh(self.l_linear(sine_wavs))\n\n        # source for noise branch, in the same shape as uv\n        noise = torch.randn_like(uv) * self.sine_amp / 3\n        return sine_merge, noise, uv\n\n\nclass HiFTGenerator(nn.Module):\n    \"\"\"\n    HiFTNet Generator: Neural Source Filter + ISTFTNet\n    https://arxiv.org/abs/2309.09493\n    \"\"\"\n    def __init__(\n            self,\n            in_channels: int = 80,\n            base_channels: int = 512,\n            nb_harmonics: int = 8,\n            sampling_rate: int = 22050,\n            nsf_alpha: float = 0.1,\n            nsf_sigma: float = 0.003,\n            nsf_voiced_threshold: float = 10,\n            upsample_rates: List[int] = [8, 8],\n            upsample_kernel_sizes: List[int] = [16, 16],\n            istft_params: Dict[str, int] = {\"n_fft\": 16, \"hop_len\": 4},\n            resblock_kernel_sizes: List[int] = [3, 7, 11],\n            resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n            source_resblock_kernel_sizes: List[int] = [7, 11],\n            source_resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5]],\n            lrelu_slope: float = 0.1,\n            audio_limit: float = 0.99,\n            f0_predictor: torch.nn.Module = None,\n    ):\n        super(HiFTGenerator, self).__init__()\n\n        self.out_channels = 1\n        self.nb_harmonics = nb_harmonics\n        self.sampling_rate = sampling_rate\n        self.istft_params = istft_params\n        self.lrelu_slope = lrelu_slope\n        self.audio_limit = audio_limit\n\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.m_source = SourceModuleHnNSF(\n            sampling_rate=sampling_rate,\n            upsample_scale=np.prod(upsample_rates) * istft_params[\"hop_len\"],\n            harmonic_num=nb_harmonics,\n            sine_amp=nsf_alpha,\n            add_noise_std=nsf_sigma,\n            voiced_threshod=nsf_voiced_threshold)\n        self.f0_upsamp = torch.nn.Upsample(scale_factor=np.prod(upsample_rates) * istft_params[\"hop_len\"])\n\n        self.conv_pre = weight_norm(\n            Conv1d(in_channels, base_channels, 7, 1, padding=3)\n        )\n\n        # Up\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        base_channels // (2**i),\n                        base_channels // (2**(i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        # Down\n        self.source_downs = nn.ModuleList()\n        self.source_resblocks = nn.ModuleList()\n        downsample_rates = [1] + upsample_rates[::-1][:-1]\n        downsample_cum_rates = np.cumprod(downsample_rates)\n        for i, (u, k, d) in enumerate(zip(downsample_cum_rates[::-1], source_resblock_kernel_sizes, source_resblock_dilation_sizes)):\n            if u == 1:\n                self.source_downs.append(\n                    Conv1d(istft_params[\"n_fft\"] + 2, base_channels // (2 ** (i + 1)), 1, 1)\n                )\n            else:\n                self.source_downs.append(\n                    Conv1d(istft_params[\"n_fft\"] + 2, base_channels // (2 ** (i + 1)), u * 2, u, padding=(u // 2))\n                )\n\n            self.source_resblocks.append(\n                ResBlock(base_channels // (2 ** (i + 1)), k, d)\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = base_channels // (2**(i + 1))\n            for _, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(ResBlock(ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, istft_params[\"n_fft\"] + 2, 7, 1, padding=3))\n        self.ups.apply(init_weights)\n        self.conv_post.apply(init_weights)\n        self.reflection_pad = nn.ReflectionPad1d((1, 0))\n        self.stft_window = torch.from_numpy(get_window(\"hann\", istft_params[\"n_fft\"], fftbins=True).astype(np.float32))\n        self.f0_predictor = f0_predictor\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        remove_weight_norm(self.conv_pre)\n        remove_weight_norm(self.conv_post)\n        self.m_source.remove_weight_norm()\n        for l in self.source_downs:\n            remove_weight_norm(l)\n        for l in self.source_resblocks:\n            l.remove_weight_norm()\n\n    def _stft(self, x):\n        spec = torch.stft(\n            x,\n            self.istft_params[\"n_fft\"], self.istft_params[\"hop_len\"], self.istft_params[\"n_fft\"], window=self.stft_window.to(x.device),\n            return_complex=True)\n        spec = torch.view_as_real(spec)  # [B, F, TT, 2]\n        return spec[..., 0], spec[..., 1]\n\n    def _istft(self, magnitude, phase):\n        magnitude = torch.clip(magnitude, max=1e2)\n        real = magnitude * torch.cos(phase)\n        img = magnitude * torch.sin(phase)\n        inverse_transform = torch.istft(torch.complex(real, img), self.istft_params[\"n_fft\"], self.istft_params[\"hop_len\"],\n                                        self.istft_params[\"n_fft\"], window=self.stft_window.to(magnitude.device))\n        return inverse_transform\n\n    def decode(self, x: torch.Tensor, s: torch.Tensor = torch.zeros(1, 1, 0)) -> torch.Tensor:\n        s_stft_real, s_stft_imag = self._stft(s.squeeze(1))\n        s_stft = torch.cat([s_stft_real, s_stft_imag], dim=1)\n\n        x = self.conv_pre(x)\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, self.lrelu_slope)\n            x = self.ups[i](x)\n\n            if i == self.num_upsamples - 1:\n                x = self.reflection_pad(x)\n\n            # fusion\n            si = self.source_downs[i](s_stft)\n            si = self.source_resblocks[i](si)\n            x = x + si\n\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        magnitude = torch.exp(x[:, :self.istft_params[\"n_fft\"] // 2 + 1, :])\n        phase = torch.sin(x[:, self.istft_params[\"n_fft\"] // 2 + 1:, :])  # actually, sin is redundancy\n\n        x = self._istft(magnitude, phase)\n        x = torch.clamp(x, -self.audio_limit, self.audio_limit)\n        return x\n\n    def forward(\n            self,\n            batch: dict,\n            device: torch.device,\n    ) -> Dict[str, Optional[torch.Tensor]]:\n        speech_feat = batch['speech_feat'].transpose(1, 2).to(device)\n        # mel->f0\n        f0 = self.f0_predictor(speech_feat)\n        # f0->source\n        s = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t\n        s, _, _ = self.m_source(s)\n        s = s.transpose(1, 2)\n        # mel+source->speech\n        generated_speech = self.decode(x=speech_feat, s=s)\n        return generated_speech, f0\n\n    @torch.inference_mode()\n    def inference(self, speech_feat: torch.Tensor, cache_source: torch.Tensor = torch.zeros(1, 1, 0)) -> torch.Tensor:\n        # mel->f0\n        f0 = self.f0_predictor(speech_feat)\n        # f0->source\n        s = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t\n        s, _, _ = self.m_source(s)\n        s = s.transpose(1, 2)\n        # use cache_source to avoid glitch\n        if cache_source.shape[2] != 0:\n            s[:, :, :cache_source.shape[2]] = cache_source\n        generated_speech = self.decode(x=speech_feat, s=s)\n        return generated_speech, s\n"}
{"type": "source_file", "path": "cosyvoice/flow/decoder.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\nimport torch.nn as nn\nfrom einops import pack, rearrange, repeat\nfrom matcha.models.components.decoder import SinusoidalPosEmb, Block1D, ResnetBlock1D, Downsample1D, TimestepEmbedding, Upsample1D\nfrom matcha.models.components.transformer import BasicTransformerBlock\n\n\nclass ConditionalDecoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        channels=(256, 256),\n        dropout=0.05,\n        attention_head_dim=64,\n        n_blocks=1,\n        num_mid_blocks=2,\n        num_heads=4,\n        act_fn=\"snake\",\n    ):\n        \"\"\"\n        This decoder requires an input with the same shape of the target. So, if your text content\n        is shorter or longer than the outputs, please re-sampling it before feeding to the decoder.\n        \"\"\"\n        super().__init__()\n        channels = tuple(channels)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.time_embeddings = SinusoidalPosEmb(in_channels)\n        time_embed_dim = channels[0] * 4\n        self.time_mlp = TimestepEmbedding(\n            in_channels=in_channels,\n            time_embed_dim=time_embed_dim,\n            act_fn=\"silu\",\n        )\n        self.down_blocks = nn.ModuleList([])\n        self.mid_blocks = nn.ModuleList([])\n        self.up_blocks = nn.ModuleList([])\n\n        output_channel = in_channels\n        for i in range(len(channels)):  # pylint: disable=consider-using-enumerate\n            input_channel = output_channel\n            output_channel = channels[i]\n            is_last = i == len(channels) - 1\n            resnet = ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)\n            transformer_blocks = nn.ModuleList(\n                [\n                    BasicTransformerBlock(\n                        dim=output_channel,\n                        num_attention_heads=num_heads,\n                        attention_head_dim=attention_head_dim,\n                        dropout=dropout,\n                        activation_fn=act_fn,\n                    )\n                    for _ in range(n_blocks)\n                ]\n            )\n            downsample = (\n                Downsample1D(output_channel) if not is_last else nn.Conv1d(output_channel, output_channel, 3, padding=1)\n            )\n            self.down_blocks.append(nn.ModuleList([resnet, transformer_blocks, downsample]))\n\n        for _ in range(num_mid_blocks):\n            input_channel = channels[-1]\n            out_channels = channels[-1]\n            resnet = ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)\n\n            transformer_blocks = nn.ModuleList(\n                [\n                    BasicTransformerBlock(\n                        dim=output_channel,\n                        num_attention_heads=num_heads,\n                        attention_head_dim=attention_head_dim,\n                        dropout=dropout,\n                        activation_fn=act_fn,\n                    )\n                    for _ in range(n_blocks)\n                ]\n            )\n\n            self.mid_blocks.append(nn.ModuleList([resnet, transformer_blocks]))\n\n        channels = channels[::-1] + (channels[0],)\n        for i in range(len(channels) - 1):\n            input_channel = channels[i] * 2\n            output_channel = channels[i + 1]\n            is_last = i == len(channels) - 2\n            resnet = ResnetBlock1D(\n                dim=input_channel,\n                dim_out=output_channel,\n                time_emb_dim=time_embed_dim,\n            )\n            transformer_blocks = nn.ModuleList(\n                [\n                    BasicTransformerBlock(\n                        dim=output_channel,\n                        num_attention_heads=num_heads,\n                        attention_head_dim=attention_head_dim,\n                        dropout=dropout,\n                        activation_fn=act_fn,\n                    )\n                    for _ in range(n_blocks)\n                ]\n            )\n            upsample = (\n                Upsample1D(output_channel, use_conv_transpose=True)\n                if not is_last\n                else nn.Conv1d(output_channel, output_channel, 3, padding=1)\n            )\n            self.up_blocks.append(nn.ModuleList([resnet, transformer_blocks, upsample]))\n        self.final_block = Block1D(channels[-1], channels[-1])\n        self.final_proj = nn.Conv1d(channels[-1], self.out_channels, 1)\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.GroupNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, mask, mu, t, spks=None, cond=None):\n        \"\"\"Forward pass of the UNet1DConditional model.\n\n        Args:\n            x (torch.Tensor): shape (batch_size, in_channels, time)\n            mask (_type_): shape (batch_size, 1, time)\n            t (_type_): shape (batch_size)\n            spks (_type_, optional): shape: (batch_size, condition_channels). Defaults to None.\n            cond (_type_, optional): placeholder for future use. Defaults to None.\n\n        Raises:\n            ValueError: _description_\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n\n        t = self.time_embeddings(t).to(t.dtype)\n        t = self.time_mlp(t)\n\n        x = pack([x, mu], \"b * t\")[0]\n\n        if spks is not None:\n            spks = repeat(spks, \"b c -> b c t\", t=x.shape[-1])\n            x = pack([x, spks], \"b * t\")[0]\n        if cond is not None:\n            x = pack([x, cond], \"b * t\")[0]\n\n        hiddens = []\n        masks = [mask]\n        for resnet, transformer_blocks, downsample in self.down_blocks:\n            mask_down = masks[-1]\n            x = resnet(x, mask_down, t)\n            x = rearrange(x, \"b c t -> b t c\").contiguous()\n            attn_mask = torch.matmul(mask_down.transpose(1, 2).contiguous(), mask_down)\n            for transformer_block in transformer_blocks:\n                x = transformer_block(\n                    hidden_states=x,\n                    attention_mask=attn_mask,\n                    timestep=t,\n                )\n            x = rearrange(x, \"b t c -> b c t\").contiguous()\n            hiddens.append(x)  # Save hidden states for skip connections\n            x = downsample(x * mask_down)\n            masks.append(mask_down[:, :, ::2])\n        masks = masks[:-1]\n        mask_mid = masks[-1]\n\n        for resnet, transformer_blocks in self.mid_blocks:\n            x = resnet(x, mask_mid, t)\n            x = rearrange(x, \"b c t -> b t c\").contiguous()\n            attn_mask = torch.matmul(mask_mid.transpose(1, 2).contiguous(), mask_mid)\n            for transformer_block in transformer_blocks:\n                x = transformer_block(\n                    hidden_states=x,\n                    attention_mask=attn_mask,\n                    timestep=t,\n                )\n            x = rearrange(x, \"b t c -> b c t\").contiguous()\n\n        for resnet, transformer_blocks, upsample in self.up_blocks:\n            mask_up = masks.pop()\n            skip = hiddens.pop()\n            x = pack([x[:, :, :skip.shape[-1]], skip], \"b * t\")[0]\n            x = resnet(x, mask_up, t)\n            x = rearrange(x, \"b c t -> b t c\").contiguous()\n            attn_mask = torch.matmul(mask_up.transpose(1, 2).contiguous(), mask_up)\n            for transformer_block in transformer_blocks:\n                x = transformer_block(\n                    hidden_states=x,\n                    attention_mask=attn_mask,\n                    timestep=t,\n                )\n            x = rearrange(x, \"b t c -> b c t\").contiguous()\n            x = upsample(x * mask_up)\n        x = self.final_block(x, mask_up)\n        output = self.final_proj(x * mask_up)\n        return output * mask\n"}
{"type": "source_file", "path": "cosyvoice/flow/flow.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport random\nfrom typing import Dict, Optional\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom omegaconf import DictConfig\nfrom cosyvoice.utils.mask import make_pad_mask\n\n\nclass MaskedDiffWithXvec(torch.nn.Module):\n    def __init__(self,\n                 input_size: int = 512,\n                 output_size: int = 80,\n                 spk_embed_dim: int = 192,\n                 output_type: str = \"mel\",\n                 vocab_size: int = 4096,\n                 input_frame_rate: int = 50,\n                 only_mask_loss: bool = True,\n                 encoder: torch.nn.Module = None,\n                 length_regulator: torch.nn.Module = None,\n                 decoder: torch.nn.Module = None,\n                 decoder_conf: Dict = {'in_channels': 240, 'out_channel': 80, 'spk_emb_dim': 80, 'n_spks': 1,\n                                       'cfm_params': DictConfig({'sigma_min': 1e-06, 'solver': 'euler', 't_scheduler': 'cosine',\n                                                                 'training_cfg_rate': 0.2, 'inference_cfg_rate': 0.7, 'reg_loss_type': 'l1'}),\n                                       'decoder_params': {'channels': [256, 256], 'dropout': 0.0, 'attention_head_dim': 64,\n                                                          'n_blocks': 4, 'num_mid_blocks': 12, 'num_heads': 8, 'act_fn': 'gelu'}},\n                 mel_feat_conf: Dict = {'n_fft': 1024, 'num_mels': 80, 'sampling_rate': 22050,\n                                        'hop_size': 256, 'win_size': 1024, 'fmin': 0, 'fmax': 8000}):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.decoder_conf = decoder_conf\n        self.mel_feat_conf = mel_feat_conf\n        self.vocab_size = vocab_size\n        self.output_type = output_type\n        self.input_frame_rate = input_frame_rate\n        logging.info(f\"input frame rate={self.input_frame_rate}\")\n        self.input_embedding = nn.Embedding(vocab_size, input_size)\n        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, output_size)\n        self.encoder = encoder\n        self.encoder_proj = torch.nn.Linear(self.encoder.output_size(), output_size)\n        self.decoder = decoder\n        self.length_regulator = length_regulator\n        self.only_mask_loss = only_mask_loss\n\n    def forward(\n            self,\n            batch: dict,\n            device: torch.device,\n    ) -> Dict[str, Optional[torch.Tensor]]:\n        token = batch['speech_token'].to(device)\n        token_len = batch['speech_token_len'].to(device)\n        feat = batch['speech_feat'].to(device)\n        feat_len = batch['speech_feat_len'].to(device)\n        embedding = batch['embedding'].to(device)\n\n        # xvec projection\n        embedding = F.normalize(embedding, dim=1)\n        embedding = self.spk_embed_affine_layer(embedding)\n\n        # concat text and prompt_text\n        mask = (~make_pad_mask(token_len)).float().unsqueeze(-1).to(device)\n        token = self.input_embedding(torch.clamp(token, min=0)) * mask\n\n        # text encode\n        h, h_lengths = self.encoder(token, token_len)\n        h = self.encoder_proj(h)\n        h, h_lengths = self.length_regulator(h, feat_len)\n\n        # get conditions\n        conds = torch.zeros(feat.shape, device=token.device)\n        for i, j in enumerate(feat_len):\n            if random.random() < 0.5:\n                continue\n            index = random.randint(0, int(0.3 * j))\n            conds[i, :index] = feat[i, :index]\n        conds = conds.transpose(1, 2)\n\n        mask = (~make_pad_mask(feat_len)).to(h)\n        feat = F.interpolate(feat.unsqueeze(dim=1), size=h.shape[1:], mode=\"nearest\").squeeze(dim=1)\n        loss, _ = self.decoder.compute_loss(\n            feat.transpose(1, 2).contiguous(),\n            mask.unsqueeze(1),\n            h.transpose(1, 2).contiguous(),\n            embedding,\n            cond=conds\n        )\n        return {'loss': loss}\n\n    @torch.inference_mode()\n    def inference(self,\n                  token,\n                  token_len,\n                  prompt_token,\n                  prompt_token_len,\n                  prompt_feat,\n                  prompt_feat_len,\n                  embedding,\n                  flow_cache):\n        assert token.shape[0] == 1\n        # xvec projection\n        embedding = F.normalize(embedding, dim=1)\n        embedding = self.spk_embed_affine_layer(embedding)\n\n        # concat text and prompt_text\n        token_len1, token_len2 = prompt_token.shape[1], token.shape[1]\n        token, token_len = torch.concat([prompt_token, token], dim=1), prompt_token_len + token_len\n        mask = (~make_pad_mask(token_len)).unsqueeze(-1).to(embedding)\n        token = self.input_embedding(torch.clamp(token, min=0)) * mask\n\n        # text encode\n        h, h_lengths = self.encoder(token, token_len)\n        h = self.encoder_proj(h)\n        mel_len1, mel_len2 = prompt_feat.shape[1], int(token_len2 / self.input_frame_rate * 22050 / 256)\n        h, h_lengths = self.length_regulator.inference(h[:, :token_len1], h[:, token_len1:], mel_len1, mel_len2, self.input_frame_rate)\n\n        # get conditions\n        conds = torch.zeros([1, mel_len1 + mel_len2, self.output_size], device=token.device)\n        conds[:, :mel_len1] = prompt_feat\n        conds = conds.transpose(1, 2)\n\n        mask = (~make_pad_mask(torch.tensor([mel_len1 + mel_len2]))).to(h)\n        feat, flow_cache = self.decoder(\n            mu=h.transpose(1, 2).contiguous(),\n            mask=mask.unsqueeze(1),\n            spks=embedding,\n            cond=conds,\n            n_timesteps=10,\n            prompt_len=mel_len1,\n            flow_cache=flow_cache\n        )\n        feat = feat[:, :, mel_len1:]\n        assert feat.shape[2] == mel_len2\n        return feat, flow_cache\n"}
{"type": "source_file", "path": "cosyvoice/cli/__init__.py", "content": ""}
{"type": "source_file", "path": "cosyvoice/dataset/__init__.py", "content": ""}
{"type": "source_file", "path": "8_Inference_QWen2-VL_offline_AV.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256*28*28\nmax_pixels = 1280*28*28\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n"}
{"type": "source_file", "path": "cosyvoice/bin/train.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport datetime\nimport logging\nlogging.getLogger('matplotlib').setLevel(logging.WARNING)\nfrom copy import deepcopy\nimport os\nimport torch\nimport torch.distributed as dist\nimport deepspeed\n\nfrom hyperpyyaml import load_hyperpyyaml\n\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\nfrom cosyvoice.utils.executor import Executor\nfrom cosyvoice.utils.train_utils import (\n    init_distributed,\n    init_dataset_and_dataloader,\n    init_optimizer_and_scheduler,\n    init_summarywriter, save_model,\n    wrap_cuda_model, check_modify_and_save_config)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='training your network')\n    parser.add_argument('--train_engine',\n                        default='torch_ddp',\n                        choices=['torch_ddp', 'deepspeed'],\n                        help='Engine for paralleled training')\n    parser.add_argument('--model', required=True, help='model which will be trained')\n    parser.add_argument('--config', required=True, help='config file')\n    parser.add_argument('--train_data', required=True, help='train data file')\n    parser.add_argument('--cv_data', required=True, help='cv data file')\n    parser.add_argument('--checkpoint', help='checkpoint model')\n    parser.add_argument('--model_dir', required=True, help='save model dir')\n    parser.add_argument('--tensorboard_dir',\n                        default='tensorboard',\n                        help='tensorboard log dir')\n    parser.add_argument('--ddp.dist_backend',\n                        dest='dist_backend',\n                        default='nccl',\n                        choices=['nccl', 'gloo'],\n                        help='distributed backend')\n    parser.add_argument('--num_workers',\n                        default=0,\n                        type=int,\n                        help='num of subprocess workers for reading')\n    parser.add_argument('--prefetch',\n                        default=100,\n                        type=int,\n                        help='prefetch number')\n    parser.add_argument('--pin_memory',\n                        action='store_true',\n                        default=False,\n                        help='Use pinned memory buffers used for reading')\n    parser.add_argument('--use_amp',\n                        action='store_true',\n                        default=False,\n                        help='Use automatic mixed precision training')\n    parser.add_argument('--deepspeed.save_states',\n                        dest='save_states',\n                        default='model_only',\n                        choices=['model_only', 'model+optimizer'],\n                        help='save model/optimizer states')\n    parser.add_argument('--timeout',\n                        default=60,\n                        type=int,\n                        help='timeout (in seconds) of cosyvoice_join.')\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args()\n    return args\n\n\n@record\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(levelname)s %(message)s')\n    # gan train has some special initialization logic\n    gan = True if args.model == 'hifigan' else False\n\n    override_dict = {k: None for k in ['llm', 'flow', 'hift', 'hifigan'] if k != args.model}\n    if gan is True:\n        override_dict.pop('hift')\n    with open(args.config, 'r') as f:\n        configs = load_hyperpyyaml(f, overrides=override_dict)\n    if gan is True:\n        configs['train_conf'] = configs['train_conf_gan']\n    configs['train_conf'].update(vars(args))\n\n    # Init env for ddp\n    init_distributed(args)\n\n    # Get dataset & dataloader\n    train_dataset, cv_dataset, train_data_loader, cv_data_loader = \\\n        init_dataset_and_dataloader(args, configs, gan)\n\n    # Do some sanity checks and save config to arsg.model_dir\n    configs = check_modify_and_save_config(args, configs)\n\n    # Tensorboard summary\n    writer = init_summarywriter(args)\n\n    # load checkpoint\n    model = configs[args.model]\n    if args.checkpoint is not None:\n        if os.path.exists(args.checkpoint):\n            model.load_state_dict(torch.load(args.checkpoint, map_location='cpu'), strict=False)\n        else:\n            logging.warning('checkpoint {} do not exsist!'.format(args.checkpoint))\n\n    # Dispatch model from cpu to gpu\n    model = wrap_cuda_model(args, model)\n\n    # Get optimizer & scheduler\n    model, optimizer, scheduler, optimizer_d, scheduler_d = init_optimizer_and_scheduler(args, configs, model, gan)\n\n    # Save init checkpoints\n    info_dict = deepcopy(configs['train_conf'])\n    save_model(model, 'init', info_dict)\n\n    # Get executor\n    executor = Executor(gan=gan)\n\n    # Init scaler, used for pytorch amp mixed precision training\n    scaler = torch.cuda.amp.GradScaler() if args.use_amp else None\n\n    # Start training loop\n    for epoch in range(info_dict['max_epoch']):\n        executor.epoch = epoch\n        train_dataset.set_epoch(epoch)\n        dist.barrier()\n        group_join = dist.new_group(backend=\"gloo\", timeout=datetime.timedelta(seconds=args.timeout))\n        if gan is True:\n            executor.train_one_epoc_gan(model, optimizer, scheduler, optimizer_d, scheduler_d, train_data_loader, cv_data_loader,\n                                        writer, info_dict, scaler, group_join)\n        else:\n            executor.train_one_epoc(model, optimizer, scheduler, train_data_loader, cv_data_loader, writer, info_dict, scaler, group_join)\n        dist.destroy_process_group(group_join)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "cosyvoice/dataset/dataset.py", "content": "# Copyright (c) 2021 Mobvoi Inc. (authors: Binbin Zhang)\n#               2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nimport json\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import IterableDataset\nfrom cosyvoice.utils.file_utils import read_lists, read_json_lists\n\n\nclass Processor(IterableDataset):\n\n    def __init__(self, source, f, *args, **kw):\n        assert callable(f)\n        self.source = source\n        self.f = f\n        self.args = args\n        self.kw = kw\n\n    def set_epoch(self, epoch):\n        self.source.set_epoch(epoch)\n\n    def __iter__(self):\n        \"\"\" Return an iterator over the source dataset processed by the\n            given processor.\n        \"\"\"\n        assert self.source is not None\n        assert callable(self.f)\n        return self.f(iter(self.source), *self.args, **self.kw)\n\n    def apply(self, f):\n        assert callable(f)\n        return Processor(self, f, *self.args, **self.kw)\n\n\nclass DistributedSampler:\n\n    def __init__(self, shuffle=True, partition=True):\n        self.epoch = -1\n        self.update()\n        self.shuffle = shuffle\n        self.partition = partition\n\n    def update(self):\n        assert dist.is_available()\n        if dist.is_initialized():\n            self.rank = dist.get_rank()\n            self.world_size = dist.get_world_size()\n        else:\n            self.rank = 0\n            self.world_size = 1\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            self.worker_id = 0\n            self.num_workers = 1\n        else:\n            self.worker_id = worker_info.id\n            self.num_workers = worker_info.num_workers\n        return dict(rank=self.rank,\n                    world_size=self.world_size,\n                    worker_id=self.worker_id,\n                    num_workers=self.num_workers)\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n    def sample(self, data):\n        \"\"\" Sample data according to rank/world_size/num_workers\n\n            Args:\n                data(List): input data list\n\n            Returns:\n                List: data list after sample\n        \"\"\"\n        data = list(range(len(data)))\n        # force datalist even\n        if self.partition:\n            if self.shuffle:\n                random.Random(self.epoch).shuffle(data)\n            if len(data) < self.world_size:\n                data = data * math.ceil(self.world_size / len(data))\n                data = data[:self.world_size]\n            data = data[self.rank::self.world_size]\n        if len(data) < self.num_workers:\n            data = data * math.ceil(self.num_workers / len(data))\n            data = data[:self.num_workers]\n        data = data[self.worker_id::self.num_workers]\n        return data\n\n\nclass DataList(IterableDataset):\n\n    def __init__(self, lists, shuffle=True, partition=True):\n        self.lists = lists\n        self.sampler = DistributedSampler(shuffle, partition)\n\n    def set_epoch(self, epoch):\n        self.sampler.set_epoch(epoch)\n\n    def __iter__(self):\n        sampler_info = self.sampler.update()\n        indexes = self.sampler.sample(self.lists)\n        for index in indexes:\n            data = dict(src=self.lists[index])\n            data.update(sampler_info)\n            yield data\n\n\ndef Dataset(data_list_file,\n            data_pipeline,\n            mode='train',\n            gan=False,\n            shuffle=True,\n            partition=True,\n            tts_file='',\n            prompt_utt2data=''):\n    \"\"\" Construct dataset from arguments\n\n        We have two shuffle stage in the Dataset. The first is global\n        shuffle at shards tar/raw file level. The second is global shuffle\n        at training samples level.\n\n        Args:\n            data_type(str): raw/shard\n            tokenizer (BaseTokenizer): tokenizer to tokenize\n            partition(bool): whether to do data partition in terms of rank\n    \"\"\"\n    assert mode in ['train', 'inference']\n    lists = read_lists(data_list_file)\n    if mode == 'inference':\n        with open(tts_file) as f:\n            tts_data = json.load(f)\n        utt2lists = read_json_lists(prompt_utt2data)\n        # filter unnecessary file in inference mode\n        lists = list({utt2lists[utt] for utt in tts_data.keys() if utt2lists[utt] in lists})\n    dataset = DataList(lists,\n                       shuffle=shuffle,\n                       partition=partition)\n    if mode == 'inference':\n        # map partial arg to parquet_opener func in inference mode\n        data_pipeline[0] = partial(data_pipeline[0], tts_data=tts_data)\n    if gan is True:\n        # map partial arg to padding func in gan mode\n        data_pipeline[-1] = partial(data_pipeline[-1], gan=gan)\n    for func in data_pipeline:\n        dataset = Processor(dataset, func, mode=mode)\n    return dataset\n"}
{"type": "source_file", "path": "cosyvoice/hifigan/f0_predictor.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Kai Hu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\n\n\nclass ConvRNNF0Predictor(nn.Module):\n    def __init__(self,\n                 num_class: int = 1,\n                 in_channels: int = 80,\n                 cond_channels: int = 512\n                 ):\n        super().__init__()\n\n        self.num_class = num_class\n        self.condnet = nn.Sequential(\n            weight_norm(\n                nn.Conv1d(in_channels, cond_channels, kernel_size=3, padding=1)\n            ),\n            nn.ELU(),\n            weight_norm(\n                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)\n            ),\n            nn.ELU(),\n            weight_norm(\n                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)\n            ),\n            nn.ELU(),\n            weight_norm(\n                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)\n            ),\n            nn.ELU(),\n            weight_norm(\n                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)\n            ),\n            nn.ELU(),\n        )\n        self.classifier = nn.Linear(in_features=cond_channels, out_features=self.num_class)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.condnet(x)\n        x = x.transpose(1, 2)\n        return torch.abs(self.classifier(x).squeeze(-1))\n"}
{"type": "source_file", "path": "cosyvoice/flow/flow_matching.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport torch\nimport torch.nn.functional as F\nfrom matcha.models.components.flow_matching import BASECFM\n\nimport os, sys\nsys.path.insert(0, os.path.abspath(r'E:\\2_PYTHON\\Project\\GPT\\QWen\\CosyVoice\\third_party\\Matcha-TTS'))\n\nclass ConditionalCFM(BASECFM):\n    def __init__(self, in_channels, cfm_params, n_spks=1, spk_emb_dim=64, estimator: torch.nn.Module = None):\n        super().__init__(\n            n_feats=in_channels,\n            cfm_params=cfm_params,\n            n_spks=n_spks,\n            spk_emb_dim=spk_emb_dim,\n        )\n        self.t_scheduler = cfm_params.t_scheduler\n        self.training_cfg_rate = cfm_params.training_cfg_rate\n        self.inference_cfg_rate = cfm_params.inference_cfg_rate\n        in_channels = in_channels + (spk_emb_dim if n_spks > 0 else 0)\n        # Just change the architecture of the estimator here\n        self.estimator = estimator\n\n    @torch.inference_mode()\n    def forward(self, mu, mask, n_timesteps, temperature=1.0, spks=None, cond=None, prompt_len=0, flow_cache=torch.zeros(1, 80, 0, 2)):\n        \"\"\"Forward diffusion\n\n        Args:\n            mu (torch.Tensor): output of encoder\n                shape: (batch_size, n_feats, mel_timesteps)\n            mask (torch.Tensor): output_mask\n                shape: (batch_size, 1, mel_timesteps)\n            n_timesteps (int): number of diffusion steps\n            temperature (float, optional): temperature for scaling noise. Defaults to 1.0.\n            spks (torch.Tensor, optional): speaker ids. Defaults to None.\n                shape: (batch_size, spk_emb_dim)\n            cond: Not used but kept for future purposes\n\n        Returns:\n            sample: generated mel-spectrogram\n                shape: (batch_size, n_feats, mel_timesteps)\n        \"\"\"\n\n        z = torch.randn_like(mu) * temperature\n        cache_size = flow_cache.shape[2]\n        # fix prompt and overlap part mu and z\n        if cache_size != 0:\n            z[:, :, :cache_size] = flow_cache[:, :, :, 0]\n            mu[:, :, :cache_size] = flow_cache[:, :, :, 1]\n        z_cache = torch.concat([z[:, :, :prompt_len], z[:, :, -34:]], dim=2)\n        mu_cache = torch.concat([mu[:, :, :prompt_len], mu[:, :, -34:]], dim=2)\n        flow_cache = torch.stack([z_cache, mu_cache], dim=-1)\n\n        t_span = torch.linspace(0, 1, n_timesteps + 1, device=mu.device, dtype=mu.dtype)\n        if self.t_scheduler == 'cosine':\n            t_span = 1 - torch.cos(t_span * 0.5 * torch.pi)\n        return self.solve_euler(z, t_span=t_span, mu=mu, mask=mask, spks=spks, cond=cond), flow_cache\n\n    def solve_euler(self, x, t_span, mu, mask, spks, cond):\n        \"\"\"\n        Fixed euler solver for ODEs.\n        Args:\n            x (torch.Tensor): random noise\n            t_span (torch.Tensor): n_timesteps interpolated\n                shape: (n_timesteps + 1,)\n            mu (torch.Tensor): output of encoder\n                shape: (batch_size, n_feats, mel_timesteps)\n            mask (torch.Tensor): output_mask\n                shape: (batch_size, 1, mel_timesteps)\n            spks (torch.Tensor, optional): speaker ids. Defaults to None.\n                shape: (batch_size, spk_emb_dim)\n            cond: Not used but kept for future purposes\n        \"\"\"\n        t, _, dt = t_span[0], t_span[-1], t_span[1] - t_span[0]\n        t = t.unsqueeze(dim=0)\n\n        # I am storing this because I can later plot it by putting a debugger here and saving it to a file\n        # Or in future might add like a return_all_steps flag\n        sol = []\n\n        for step in range(1, len(t_span)):\n            dphi_dt = self.forward_estimator(x, mask, mu, t, spks, cond)\n            # Classifier-Free Guidance inference introduced in VoiceBox\n            if self.inference_cfg_rate > 0:\n                cfg_dphi_dt = self.forward_estimator(\n                    x, mask,\n                    torch.zeros_like(mu), t,\n                    torch.zeros_like(spks) if spks is not None else None,\n                    torch.zeros_like(cond)\n                )\n                dphi_dt = ((1.0 + self.inference_cfg_rate) * dphi_dt -\n                           self.inference_cfg_rate * cfg_dphi_dt)\n            x = x + dt * dphi_dt\n            t = t + dt\n            sol.append(x)\n            if step < len(t_span) - 1:\n                dt = t_span[step + 1] - t\n\n        return sol[-1]\n\n    def forward_estimator(self, x, mask, mu, t, spks, cond):\n        if isinstance(self.estimator, torch.nn.Module):\n            return self.estimator.forward(x, mask, mu, t, spks, cond)\n        else:\n            ort_inputs = {\n                'x': x.cpu().numpy(),\n                'mask': mask.cpu().numpy(),\n                'mu': mu.cpu().numpy(),\n                't': t.cpu().numpy(),\n                'spks': spks.cpu().numpy(),\n                'cond': cond.cpu().numpy()\n            }\n            output = self.estimator.run(None, ort_inputs)[0]\n            return torch.tensor(output, dtype=x.dtype, device=x.device)\n\n    def compute_loss(self, x1, mask, mu, spks=None, cond=None):\n        \"\"\"Computes diffusion loss\n\n        Args:\n            x1 (torch.Tensor): Target\n                shape: (batch_size, n_feats, mel_timesteps)\n            mask (torch.Tensor): target mask\n                shape: (batch_size, 1, mel_timesteps)\n            mu (torch.Tensor): output of encoder\n                shape: (batch_size, n_feats, mel_timesteps)\n            spks (torch.Tensor, optional): speaker embedding. Defaults to None.\n                shape: (batch_size, spk_emb_dim)\n\n        Returns:\n            loss: conditional flow matching loss\n            y: conditional flow\n                shape: (batch_size, n_feats, mel_timesteps)\n        \"\"\"\n        b, _, t = mu.shape\n\n        # random timestep\n        t = torch.rand([b, 1, 1], device=mu.device, dtype=mu.dtype)\n        if self.t_scheduler == 'cosine':\n            t = 1 - torch.cos(t * 0.5 * torch.pi)\n        # sample noise p(x_0)\n        z = torch.randn_like(x1)\n\n        y = (1 - (1 - self.sigma_min) * t) * z + t * x1\n        u = x1 - (1 - self.sigma_min) * z\n\n        # during training, we randomly drop condition to trade off mode coverage and sample fidelity\n        if self.training_cfg_rate > 0:\n            cfg_mask = torch.rand(b, device=x1.device) > self.training_cfg_rate\n            mu = mu * cfg_mask.view(-1, 1, 1)\n            spks = spks * cfg_mask.view(-1, 1)\n            cond = cond * cfg_mask.view(-1, 1, 1)\n\n        pred = self.estimator(y, mask, mu, t.squeeze(), spks, cond)\n        loss = F.mse_loss(pred * mask, u * mask, reduction=\"sum\") / (torch.sum(mask) * u.shape[1])\n        return loss, y\n"}
{"type": "source_file", "path": "cosyvoice/hifigan/discriminator.py", "content": "import torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\nfrom typing import List, Optional, Tuple\nfrom einops import rearrange\nfrom torchaudio.transforms import Spectrogram\n\n\nclass MultipleDiscriminator(nn.Module):\n    def __init__(\n            self, mpd: nn.Module, mrd: nn.Module\n    ):\n        super().__init__()\n        self.mpd = mpd\n        self.mrd = mrd\n\n    def forward(self, y: torch.Tensor, y_hat: torch.Tensor):\n        y_d_rs, y_d_gs, fmap_rs, fmap_gs = [], [], [], []\n        this_y_d_rs, this_y_d_gs, this_fmap_rs, this_fmap_gs = self.mpd(y.unsqueeze(dim=1), y_hat.unsqueeze(dim=1))\n        y_d_rs += this_y_d_rs\n        y_d_gs += this_y_d_gs\n        fmap_rs += this_fmap_rs\n        fmap_gs += this_fmap_gs\n        this_y_d_rs, this_y_d_gs, this_fmap_rs, this_fmap_gs = self.mrd(y, y_hat)\n        y_d_rs += this_y_d_rs\n        y_d_gs += this_y_d_gs\n        fmap_rs += this_fmap_rs\n        fmap_gs += this_fmap_gs\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass MultiResolutionDiscriminator(nn.Module):\n    def __init__(\n        self,\n        fft_sizes: Tuple[int, ...] = (2048, 1024, 512),\n        num_embeddings: Optional[int] = None,\n    ):\n        \"\"\"\n        Multi-Resolution Discriminator module adapted from https://github.com/descriptinc/descript-audio-codec.\n        Additionally, it allows incorporating conditional information with a learned embeddings table.\n\n        Args:\n            fft_sizes (tuple[int]): Tuple of window lengths for FFT. Defaults to (2048, 1024, 512).\n            num_embeddings (int, optional): Number of embeddings. None means non-conditional discriminator.\n                Defaults to None.\n        \"\"\"\n\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [DiscriminatorR(window_length=w, num_embeddings=num_embeddings) for w in fft_sizes]\n        )\n\n    def forward(\n        self, y: torch.Tensor, y_hat: torch.Tensor, bandwidth_id: torch.Tensor = None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[List[torch.Tensor]], List[List[torch.Tensor]]]:\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n\n        for d in self.discriminators:\n            y_d_r, fmap_r = d(x=y, cond_embedding_id=bandwidth_id)\n            y_d_g, fmap_g = d(x=y_hat, cond_embedding_id=bandwidth_id)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n\n\nclass DiscriminatorR(nn.Module):\n    def __init__(\n        self,\n        window_length: int,\n        num_embeddings: Optional[int] = None,\n        channels: int = 32,\n        hop_factor: float = 0.25,\n        bands: Tuple[Tuple[float, float], ...] = ((0.0, 0.1), (0.1, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)),\n    ):\n        super().__init__()\n        self.window_length = window_length\n        self.hop_factor = hop_factor\n        self.spec_fn = Spectrogram(\n            n_fft=window_length, hop_length=int(window_length * hop_factor), win_length=window_length, power=None\n        )\n        n_fft = window_length // 2 + 1\n        bands = [(int(b[0] * n_fft), int(b[1] * n_fft)) for b in bands]\n        self.bands = bands\n        convs = lambda: nn.ModuleList(\n            [\n                weight_norm(nn.Conv2d(2, channels, (3, 9), (1, 1), padding=(1, 4))),\n                weight_norm(nn.Conv2d(channels, channels, (3, 9), (1, 2), padding=(1, 4))),\n                weight_norm(nn.Conv2d(channels, channels, (3, 9), (1, 2), padding=(1, 4))),\n                weight_norm(nn.Conv2d(channels, channels, (3, 9), (1, 2), padding=(1, 4))),\n                weight_norm(nn.Conv2d(channels, channels, (3, 3), (1, 1), padding=(1, 1))),\n            ]\n        )\n        self.band_convs = nn.ModuleList([convs() for _ in range(len(self.bands))])\n\n        if num_embeddings is not None:\n            self.emb = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=channels)\n            torch.nn.init.zeros_(self.emb.weight)\n\n        self.conv_post = weight_norm(nn.Conv2d(channels, 1, (3, 3), (1, 1), padding=(1, 1)))\n\n    def spectrogram(self, x):\n        # Remove DC offset\n        x = x - x.mean(dim=-1, keepdims=True)\n        # Peak normalize the volume of input audio\n        x = 0.8 * x / (x.abs().max(dim=-1, keepdim=True)[0] + 1e-9)\n        x = self.spec_fn(x)\n        x = torch.view_as_real(x)\n        x = rearrange(x, \"b f t c -> b c t f\")\n        # Split into bands\n        x_bands = [x[..., b[0]: b[1]] for b in self.bands]\n        return x_bands\n\n    def forward(self, x: torch.Tensor, cond_embedding_id: torch.Tensor = None):\n        x_bands = self.spectrogram(x)\n        fmap = []\n        x = []\n        for band, stack in zip(x_bands, self.band_convs):\n            for i, layer in enumerate(stack):\n                band = layer(band)\n                band = torch.nn.functional.leaky_relu(band, 0.1)\n                if i > 0:\n                    fmap.append(band)\n            x.append(band)\n        x = torch.cat(x, dim=-1)\n        if cond_embedding_id is not None:\n            emb = self.emb(cond_embedding_id)\n            h = (emb.view(1, -1, 1, 1) * x).sum(dim=1, keepdims=True)\n        else:\n            h = 0\n        x = self.conv_post(x)\n        fmap.append(x)\n        x += h\n\n        return x, fmap\n"}
{"type": "source_file", "path": "cosyvoice/__init__.py", "content": ""}
{"type": "source_file", "path": "cosyvoice/hifigan/hifigan.py", "content": "from typing import Dict, Optional\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matcha.hifigan.models import feature_loss, generator_loss, discriminator_loss\nfrom cosyvoice.utils.losses import tpr_loss, mel_loss\n\n\nclass HiFiGan(nn.Module):\n    def __init__(self, generator, discriminator, mel_spec_transform,\n                 multi_mel_spectral_recon_loss_weight=45, feat_match_loss_weight=2.0,\n                 tpr_loss_weight=1.0, tpr_loss_tau=0.04):\n        super(HiFiGan, self).__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n        self.mel_spec_transform = mel_spec_transform\n        self.multi_mel_spectral_recon_loss_weight = multi_mel_spectral_recon_loss_weight\n        self.feat_match_loss_weight = feat_match_loss_weight\n        self.tpr_loss_weight = tpr_loss_weight\n        self.tpr_loss_tau = tpr_loss_tau\n\n    def forward(\n            self,\n            batch: dict,\n            device: torch.device,\n    ) -> Dict[str, Optional[torch.Tensor]]:\n        if batch['turn'] == 'generator':\n            return self.forward_generator(batch, device)\n        else:\n            return self.forward_discriminator(batch, device)\n\n    def forward_generator(self, batch, device):\n        real_speech = batch['speech'].to(device)\n        pitch_feat = batch['pitch_feat'].to(device)\n        # 1. calculate generator outputs\n        generated_speech, generated_f0 = self.generator(batch, device)\n        # 2. calculate discriminator outputs\n        y_d_rs, y_d_gs, fmap_rs, fmap_gs = self.discriminator(real_speech, generated_speech)\n        # 3. calculate generator losses, feature loss, mel loss, tpr losses [Optional]\n        loss_gen, _ = generator_loss(y_d_gs)\n        loss_fm = feature_loss(fmap_rs, fmap_gs)\n        loss_mel = mel_loss(real_speech, generated_speech, self.mel_spec_transform)\n        if self.tpr_loss_weight != 0:\n            loss_tpr = tpr_loss(y_d_rs, y_d_gs, self.tpr_loss_tau)\n        else:\n            loss_tpr = torch.zeros(1).to(device)\n        loss_f0 = F.l1_loss(generated_f0, pitch_feat)\n        loss = loss_gen + self.feat_match_loss_weight * loss_fm + \\\n            self.multi_mel_spectral_recon_loss_weight * loss_mel + \\\n            self.tpr_loss_weight * loss_tpr + loss_f0\n        return {'loss': loss, 'loss_gen': loss_gen, 'loss_fm': loss_fm, 'loss_mel': loss_mel, 'loss_tpr': loss_tpr, 'loss_f0': loss_f0}\n\n    def forward_discriminator(self, batch, device):\n        real_speech = batch['speech'].to(device)\n        # 1. calculate generator outputs\n        with torch.no_grad():\n            generated_speech, generated_f0 = self.generator(batch, device)\n        # 2. calculate discriminator outputs\n        y_d_rs, y_d_gs, fmap_rs, fmap_gs = self.discriminator(real_speech, generated_speech)\n        # 3. calculate discriminator losses, tpr losses [Optional]\n        loss_disc, _, _ = discriminator_loss(y_d_rs, y_d_gs)\n        if self.tpr_loss_weight != 0:\n            loss_tpr = tpr_loss(y_d_rs, y_d_gs, self.tpr_loss_tau)\n        else:\n            loss_tpr = torch.zeros(1).to(device)\n        loss = loss_disc + self.tpr_loss_weight * loss_tpr\n        return {'loss': loss, 'loss_disc': loss_disc, 'loss_tpr': loss_tpr}\n"}
{"type": "source_file", "path": "cosyvoice/bin/average_model.py", "content": "# Copyright (c) 2020 Mobvoi Inc (Di Wu)\n# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\nimport glob\n\nimport yaml\nimport torch\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='average model')\n    parser.add_argument('--dst_model', required=True, help='averaged model')\n    parser.add_argument('--src_path',\n                        required=True,\n                        help='src model path for average')\n    parser.add_argument('--val_best',\n                        action=\"store_true\",\n                        help='averaged model')\n    parser.add_argument('--num',\n                        default=5,\n                        type=int,\n                        help='nums for averaged model')\n\n    args = parser.parse_args()\n    print(args)\n    return args\n\n\ndef main():\n    args = get_args()\n    val_scores = []\n    if args.val_best:\n        yamls = glob.glob('{}/*.yaml'.format(args.src_path))\n        yamls = [\n            f for f in yamls\n            if not (os.path.basename(f).startswith('train')\n                    or os.path.basename(f).startswith('init'))\n        ]\n        for y in yamls:\n            with open(y, 'r') as f:\n                dic_yaml = yaml.load(f, Loader=yaml.BaseLoader)\n                loss = float(dic_yaml['loss_dict']['loss'])\n                epoch = int(dic_yaml['epoch'])\n                step = int(dic_yaml['step'])\n                tag = dic_yaml['tag']\n                val_scores += [[epoch, step, loss, tag]]\n        sorted_val_scores = sorted(val_scores,\n                                   key=lambda x: x[2],\n                                   reverse=False)\n        print(\"best val (epoch, step, loss, tag) = \" +\n              str(sorted_val_scores[:args.num]))\n        path_list = [\n            args.src_path + '/epoch_{}_whole.pt'.format(score[0])\n            for score in sorted_val_scores[:args.num]\n        ]\n    print(path_list)\n    avg = {}\n    num = args.num\n    assert num == len(path_list)\n    for path in path_list:\n        print('Processing {}'.format(path))\n        states = torch.load(path, map_location=torch.device('cpu'))\n        for k in states.keys():\n            if k not in avg.keys():\n                avg[k] = states[k].clone()\n            else:\n                avg[k] += states[k]\n    # average\n    for k in avg.keys():\n        if avg[k] is not None:\n            # pytorch 1.6 use true_divide instead of /=\n            avg[k] = torch.true_divide(avg[k], num)\n    print('Saving to {}'.format(args.dst_model))\n    torch.save(avg, args.dst_model)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "cosyvoice/cli/frontend.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom functools import partial\nimport onnxruntime\nimport torch\nimport numpy as np\nimport whisper\nfrom typing import Callable\nimport torchaudio.compliance.kaldi as kaldi\nimport torchaudio\nimport os\nimport re\nimport inflect\ntry:\n    import ttsfrd\n    use_ttsfrd = True\nexcept ImportError:\n    print(\"failed to import ttsfrd, use WeTextProcessing instead\")\n    from tn.chinese.normalizer import Normalizer as ZhNormalizer\n    from tn.english.normalizer import Normalizer as EnNormalizer\n    use_ttsfrd = False\nfrom cosyvoice.utils.frontend_utils import contains_chinese, replace_blank, replace_corner_mark, remove_bracket, spell_out_number, split_paragraph\n\n\nclass CosyVoiceFrontEnd:\n\n    def __init__(self,\n                 get_tokenizer: Callable,\n                 feat_extractor: Callable,\n                 campplus_model: str,\n                 speech_tokenizer_model: str,\n                 spk2info: str = '',\n                 instruct: bool = False,\n                 allowed_special: str = 'all'):\n        self.tokenizer = get_tokenizer()\n        self.feat_extractor = feat_extractor\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        option = onnxruntime.SessionOptions()\n        option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n        option.intra_op_num_threads = 1\n        self.campplus_session = onnxruntime.InferenceSession(campplus_model, sess_options=option, providers=[\"CPUExecutionProvider\"])\n        self.speech_tokenizer_session = onnxruntime.InferenceSession(speech_tokenizer_model, sess_options=option,\n                                                                     providers=[\"CUDAExecutionProvider\" if torch.cuda.is_available() else\n                                                                                \"CPUExecutionProvider\"])\n        if os.path.exists(spk2info):\n            self.spk2info = torch.load(spk2info, map_location=self.device)\n        else:\n            self.spk2info = {}\n        self.instruct = instruct\n        self.allowed_special = allowed_special\n        self.inflect_parser = inflect.engine()\n        self.use_ttsfrd = use_ttsfrd\n        if self.use_ttsfrd:\n            self.frd = ttsfrd.TtsFrontendEngine()\n            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n            assert self.frd.initialize('{}/../../pretrained_models/CosyVoice-ttsfrd/resource'.format(ROOT_DIR)) is True, \\\n                'failed to initialize ttsfrd resource'\n            self.frd.set_lang_type('pinyin')\n            self.frd.enable_pinyin_mix(True)\n            self.frd.set_breakmodel_index(1)\n        else:\n            self.zh_tn_model = ZhNormalizer(remove_erhua=False, full_to_half=False)\n            self.en_tn_model = EnNormalizer()\n\n    def _extract_text_token(self, text):\n        text_token = self.tokenizer.encode(text, allowed_special=self.allowed_special)\n        text_token = torch.tensor([text_token], dtype=torch.int32).to(self.device)\n        text_token_len = torch.tensor([text_token.shape[1]], dtype=torch.int32).to(self.device)\n        return text_token, text_token_len\n\n    def _extract_speech_token(self, speech):\n        assert speech.shape[1] / 16000 <= 30, 'do not support extract speech token for audio longer than 30s'\n        feat = whisper.log_mel_spectrogram(speech, n_mels=128)\n        speech_token = self.speech_tokenizer_session.run(None,\n                                                         {self.speech_tokenizer_session.get_inputs()[0].name:\n                                                          feat.detach().cpu().numpy(),\n                                                          self.speech_tokenizer_session.get_inputs()[1].name:\n                                                          np.array([feat.shape[2]], dtype=np.int32)})[0].flatten().tolist()\n        speech_token = torch.tensor([speech_token], dtype=torch.int32).to(self.device)\n        speech_token_len = torch.tensor([speech_token.shape[1]], dtype=torch.int32).to(self.device)\n        return speech_token, speech_token_len\n\n    def _extract_spk_embedding(self, speech):\n        feat = kaldi.fbank(speech,\n                           num_mel_bins=80,\n                           dither=0,\n                           sample_frequency=16000)\n        feat = feat - feat.mean(dim=0, keepdim=True)\n        embedding = self.campplus_session.run(None,\n                                              {self.campplus_session.get_inputs()[0].name: feat.unsqueeze(dim=0).cpu().numpy()})[0].flatten().tolist()\n        embedding = torch.tensor([embedding]).to(self.device)\n        return embedding\n\n    def _extract_speech_feat(self, speech):\n        speech_feat = self.feat_extractor(speech).squeeze(dim=0).transpose(0, 1).to(self.device)\n        speech_feat = speech_feat.unsqueeze(dim=0)\n        speech_feat_len = torch.tensor([speech_feat.shape[1]], dtype=torch.int32).to(self.device)\n        return speech_feat, speech_feat_len\n\n    def text_normalize(self, text, split=True):\n        text = text.strip()\n        if contains_chinese(text):\n            if self.use_ttsfrd:\n                text = self.frd.get_frd_extra_info(text, 'input')\n            else:\n                text = self.zh_tn_model.normalize(text)\n            text = text.replace(\"\\n\", \"\")\n            text = replace_blank(text)\n            text = replace_corner_mark(text)\n            text = text.replace(\".\", \"。\")\n            text = text.replace(\" - \", \"，\")\n            text = remove_bracket(text)\n            text = re.sub(r'[，,、]+$', '。', text)\n            texts = list(split_paragraph(text, partial(self.tokenizer.encode, allowed_special=self.allowed_special), \"zh\", token_max_n=80,\n                                         token_min_n=60, merge_len=20, comma_split=False))\n        else:\n            if self.use_ttsfrd:\n                text = self.frd.get_frd_extra_info(text, 'input')\n            else:\n                text = self.en_tn_model.normalize(text)\n            text = spell_out_number(text, self.inflect_parser)\n            texts = list(split_paragraph(text, partial(self.tokenizer.encode, allowed_special=self.allowed_special), \"en\", token_max_n=80,\n                                         token_min_n=60, merge_len=20, comma_split=False))\n        if split is False:\n            return text\n        return texts\n\n    def frontend_sft(self, tts_text, spk_id):\n        tts_text_token, tts_text_token_len = self._extract_text_token(tts_text)\n        embedding = self.spk2info[spk_id]['embedding']\n        model_input = {'text': tts_text_token, 'text_len': tts_text_token_len, 'llm_embedding': embedding, 'flow_embedding': embedding}\n        return model_input\n\n    def frontend_zero_shot(self, tts_text, prompt_text, prompt_speech_16k):\n        tts_text_token, tts_text_token_len = self._extract_text_token(tts_text)\n        prompt_text_token, prompt_text_token_len = self._extract_text_token(prompt_text)\n        prompt_speech_22050 = torchaudio.transforms.Resample(orig_freq=16000, new_freq=22050)(prompt_speech_16k)\n        speech_feat, speech_feat_len = self._extract_speech_feat(prompt_speech_22050)\n        speech_token, speech_token_len = self._extract_speech_token(prompt_speech_16k)\n        embedding = self._extract_spk_embedding(prompt_speech_16k)\n        model_input = {'text': tts_text_token, 'text_len': tts_text_token_len,\n                       'prompt_text': prompt_text_token, 'prompt_text_len': prompt_text_token_len,\n                       'llm_prompt_speech_token': speech_token, 'llm_prompt_speech_token_len': speech_token_len,\n                       'flow_prompt_speech_token': speech_token, 'flow_prompt_speech_token_len': speech_token_len,\n                       'prompt_speech_feat': speech_feat, 'prompt_speech_feat_len': speech_feat_len,\n                       'llm_embedding': embedding, 'flow_embedding': embedding}\n        return model_input\n\n    def frontend_cross_lingual(self, tts_text, prompt_speech_16k):\n        model_input = self.frontend_zero_shot(tts_text, '', prompt_speech_16k)\n        # in cross lingual mode, we remove prompt in llm\n        del model_input['prompt_text']\n        del model_input['prompt_text_len']\n        del model_input['llm_prompt_speech_token']\n        del model_input['llm_prompt_speech_token_len']\n        return model_input\n\n    def frontend_instruct(self, tts_text, spk_id, instruct_text):\n        model_input = self.frontend_sft(tts_text, spk_id)\n        # in instruct mode, we remove spk_embedding in llm due to information leakage\n        del model_input['llm_embedding']\n        instruct_text_token, instruct_text_token_len = self._extract_text_token(instruct_text + '<endofprompt>')\n        model_input['prompt_text'] = instruct_text_token\n        model_input['prompt_text_len'] = instruct_text_token_len\n        return model_input\n\n    def frontend_vc(self, source_speech_16k, prompt_speech_16k):\n        prompt_speech_token, prompt_speech_token_len = self._extract_speech_token(prompt_speech_16k)\n        prompt_speech_22050 = torchaudio.transforms.Resample(orig_freq=16000, new_freq=22050)(prompt_speech_16k)\n        prompt_speech_feat, prompt_speech_feat_len = self._extract_speech_feat(prompt_speech_22050)\n        embedding = self._extract_spk_embedding(prompt_speech_16k)\n        source_speech_token, source_speech_token_len = self._extract_speech_token(source_speech_16k)\n        model_input = {'source_speech_token': source_speech_token, 'source_speech_token_len': source_speech_token_len,\n                       'flow_prompt_speech_token': prompt_speech_token, 'flow_prompt_speech_token_len': prompt_speech_token_len,\n                       'prompt_speech_feat': prompt_speech_feat, 'prompt_speech_feat_len': prompt_speech_feat_len,\n                       'flow_embedding': embedding}\n        return model_input\n"}
{"type": "source_file", "path": "cosyvoice/bin/inference.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nlogging.getLogger('matplotlib').setLevel(logging.WARNING)\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchaudio\nfrom hyperpyyaml import load_hyperpyyaml\nfrom tqdm import tqdm\nfrom cosyvoice.cli.model import CosyVoiceModel\nfrom cosyvoice.dataset.dataset import Dataset\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='inference with your model')\n    parser.add_argument('--config', required=True, help='config file')\n    parser.add_argument('--prompt_data', required=True, help='prompt data file')\n    parser.add_argument('--prompt_utt2data', required=True, help='prompt data file')\n    parser.add_argument('--tts_text', required=True, help='tts input file')\n    parser.add_argument('--llm_model', required=True, help='llm model file')\n    parser.add_argument('--flow_model', required=True, help='flow model file')\n    parser.add_argument('--hifigan_model', required=True, help='hifigan model file')\n    parser.add_argument('--gpu',\n                        type=int,\n                        default=-1,\n                        help='gpu id for this rank, -1 for cpu')\n    parser.add_argument('--mode',\n                        default='sft',\n                        choices=['sft', 'zero_shot'],\n                        help='inference mode')\n    parser.add_argument('--result_dir', required=True, help='asr result file')\n    args = parser.parse_args()\n    print(args)\n    return args\n\n\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(levelname)s %(message)s')\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\n    # Init cosyvoice models from configs\n    use_cuda = args.gpu >= 0 and torch.cuda.is_available()\n    device = torch.device('cuda' if use_cuda else 'cpu')\n    with open(args.config, 'r') as f:\n        configs = load_hyperpyyaml(f)\n\n    model = CosyVoiceModel(configs['llm'], configs['flow'], configs['hift'])\n    model.load(args.llm_model, args.flow_model, args.hifigan_model)\n\n    test_dataset = Dataset(args.prompt_data, data_pipeline=configs['data_pipeline'], mode='inference', shuffle=False, partition=False,\n                           tts_file=args.tts_text, prompt_utt2data=args.prompt_utt2data)\n    test_data_loader = DataLoader(test_dataset, batch_size=None, num_workers=0)\n\n    del configs\n    os.makedirs(args.result_dir, exist_ok=True)\n    fn = os.path.join(args.result_dir, 'wav.scp')\n    f = open(fn, 'w')\n    with torch.no_grad():\n        for _, batch in tqdm(enumerate(test_data_loader)):\n            utts = batch[\"utts\"]\n            assert len(utts) == 1, \"inference mode only support batchsize 1\"\n            text_token = batch[\"text_token\"].to(device)\n            text_token_len = batch[\"text_token_len\"].to(device)\n            tts_index = batch[\"tts_index\"]\n            tts_text_token = batch[\"tts_text_token\"].to(device)\n            tts_text_token_len = batch[\"tts_text_token_len\"].to(device)\n            speech_token = batch[\"speech_token\"].to(device)\n            speech_token_len = batch[\"speech_token_len\"].to(device)\n            speech_feat = batch[\"speech_feat\"].to(device)\n            speech_feat_len = batch[\"speech_feat_len\"].to(device)\n            utt_embedding = batch[\"utt_embedding\"].to(device)\n            spk_embedding = batch[\"spk_embedding\"].to(device)\n            if args.mode == 'sft':\n                model_input = {'text': tts_text_token, 'text_len': tts_text_token_len,\n                               'llm_embedding': spk_embedding, 'flow_embedding': spk_embedding}\n            else:\n                model_input = {'text': tts_text_token, 'text_len': tts_text_token_len,\n                               'prompt_text': text_token, 'prompt_text_len': text_token_len,\n                               'llm_prompt_speech_token': speech_token, 'llm_prompt_speech_token_len': speech_token_len,\n                               'flow_prompt_speech_token': speech_token, 'flow_prompt_speech_token_len': speech_token_len,\n                               'prompt_speech_feat': speech_feat, 'prompt_speech_feat_len': speech_feat_len,\n                               'llm_embedding': utt_embedding, 'flow_embedding': utt_embedding}\n            tts_speeches = []\n            for model_output in model.tts(**model_input):\n                tts_speeches.append(model_output['tts_speech'])\n            tts_speeches = torch.concat(tts_speeches, dim=1)\n            tts_key = '{}_{}'.format(utts[0], tts_index[0])\n            tts_fn = os.path.join(args.result_dir, '{}.wav'.format(tts_key))\n            torchaudio.save(tts_fn, tts_speeches, sample_rate=22050)\n            f.write('{} {}\\n'.format(tts_key, tts_fn))\n            f.flush()\n    f.close()\n    logging.info('Result wav.scp saved in {}'.format(fn))\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "cosyvoice/cli/cosyvoice.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport time\nfrom tqdm import tqdm\nfrom hyperpyyaml import load_hyperpyyaml\nfrom modelscope import snapshot_download\nfrom cosyvoice.cli.frontend import CosyVoiceFrontEnd\nfrom cosyvoice.cli.model import CosyVoiceModel\nfrom cosyvoice.utils.file_utils import logging\n\n\nclass CosyVoice:\n\n    def __init__(self, model_dir, load_jit=True, load_onnx=False, fp16=True):\n        instruct = True if '-Instruct' in model_dir else False\n        self.model_dir = model_dir\n        if not os.path.exists(model_dir):\n            model_dir = snapshot_download(model_dir)\n        with open('{}/cosyvoice.yaml'.format(model_dir), 'r') as f:\n            configs = load_hyperpyyaml(f)\n        self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\n                                          configs['feat_extractor'],\n                                          '{}/campplus.onnx'.format(model_dir),\n                                          '{}/speech_tokenizer_v1.onnx'.format(model_dir),\n                                          '{}/spk2info.pt'.format(model_dir),\n                                          instruct,\n                                          configs['allowed_special'])\n        self.model = CosyVoiceModel(configs['llm'], configs['flow'], configs['hift'], fp16)\n        self.model.load('{}/llm.pt'.format(model_dir),\n                        '{}/flow.pt'.format(model_dir),\n                        '{}/hift.pt'.format(model_dir))\n        if load_jit:\n            self.model.load_jit('{}/llm.text_encoder.fp16.zip'.format(model_dir),\n                                '{}/llm.llm.fp16.zip'.format(model_dir),\n                                '{}/flow.encoder.fp32.zip'.format(model_dir))\n        if load_onnx:\n            self.model.load_onnx('{}/flow.decoder.estimator.fp32.onnx'.format(model_dir))\n        del configs\n\n    def list_avaliable_spks(self):\n        spks = list(self.frontend.spk2info.keys())\n        return spks\n\n    def inference_sft(self, tts_text, spk_id, stream=False, speed=1.0):\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True)):\n            model_input = self.frontend.frontend_sft(i, spk_id)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / 22050\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_zero_shot(self, tts_text, prompt_text, prompt_speech_16k, stream=False, speed=1.0):\n        prompt_text = self.frontend.text_normalize(prompt_text, split=False)\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True)):\n            model_input = self.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / 22050\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_cross_lingual(self, tts_text, prompt_speech_16k, stream=False, speed=1.0):\n        if self.frontend.instruct is True:\n            raise ValueError('{} do not support cross_lingual inference'.format(self.model_dir))\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True)):\n            model_input = self.frontend.frontend_cross_lingual(i, prompt_speech_16k)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / 22050\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_instruct(self, tts_text, spk_id, instruct_text, stream=False, speed=1.0):\n        if self.frontend.instruct is False:\n            raise ValueError('{} do not support instruct inference'.format(self.model_dir))\n        instruct_text = self.frontend.text_normalize(instruct_text, split=False)\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True)):\n            model_input = self.frontend.frontend_instruct(i, spk_id, instruct_text)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / 22050\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_vc(self, source_speech_16k, prompt_speech_16k, stream=False, speed=1.0):\n        model_input = self.frontend.frontend_vc(source_speech_16k, prompt_speech_16k)\n        start_time = time.time()\n        for model_output in self.model.vc(**model_input, stream=stream, speed=speed):\n            speech_len = model_output['tts_speech'].shape[1] / 22050\n            logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n            yield model_output\n            start_time = time.time()\n"}
{"type": "source_file", "path": "cosyvoice/dataset/processor.py", "content": "# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport random\n\nimport pyarrow.parquet as pq\nfrom io import BytesIO\nimport torch\nimport torchaudio\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\n\ntorchaudio.set_audio_backend('soundfile')\n\nAUDIO_FORMAT_SETS = {'flac', 'mp3', 'm4a', 'ogg', 'opus', 'wav', 'wma'}\n\n\ndef parquet_opener(data, mode='train', tts_data={}):\n    \"\"\" Give url or local file, return file descriptor\n        Inplace operation.\n\n        Args:\n            data(Iterable[str]): url or local file list\n\n        Returns:\n            Iterable[{src, stream}]\n    \"\"\"\n    for sample in data:\n        assert 'src' in sample\n        url = sample['src']\n        try:\n            df = pq.read_table(url).to_pandas()\n            for i in range(len(df)):\n                if mode == 'inference' and df.loc[i, 'utt'] not in tts_data:\n                    continue\n                sample.update(dict(df.loc[i]))\n                if mode == 'train':\n                    # NOTE do not return sample directly, must initialize a new dict\n                    yield {**sample}\n                else:\n                    for index, text in enumerate(tts_data[df.loc[i, 'utt']]):\n                        yield {**sample, 'tts_index': index, 'tts_text': text}\n        except Exception as ex:\n            logging.warning('Failed to open {}, ex info {}'.format(url, ex))\n\n\ndef filter(data,\n           max_length=10240,\n           min_length=10,\n           token_max_length=200,\n           token_min_length=1,\n           min_output_input_ratio=0.0005,\n           max_output_input_ratio=1,\n           mode='train'):\n    \"\"\" Filter sample according to feature and label length\n        Inplace operation.\n\n        Args::\n            data: Iterable[{key, wav, label, sample_rate}]\n            max_length: drop utterance which is greater than max_length(10ms)\n            min_length: drop utterance which is less than min_length(10ms)\n            token_max_length: drop utterance which is greater than\n                token_max_length, especially when use char unit for\n                english modeling\n            token_min_length: drop utterance which is\n                less than token_max_length\n            min_output_input_ratio: minimal ration of\n                token_length / feats_length(10ms)\n            max_output_input_ratio: maximum ration of\n                token_length / feats_length(10ms)\n\n        Returns:\n            Iterable[{key, wav, label, sample_rate}]\n    \"\"\"\n    for sample in data:\n        sample['speech'], sample['sample_rate'] = torchaudio.load(BytesIO(sample['audio_data']))\n        sample['speech'] = sample['speech'].mean(dim=0, keepdim=True)\n        del sample['audio_data']\n        # sample['wav'] is torch.Tensor, we have 100 frames every second\n        num_frames = sample['speech'].size(1) / sample['sample_rate'] * 100\n        if num_frames < min_length:\n            continue\n        if num_frames > max_length:\n            continue\n        if len(sample['text_token']) < token_min_length:\n            continue\n        if len(sample['text_token']) > token_max_length:\n            continue\n        if len(sample['speech_token']) == 0:\n            continue\n        if num_frames != 0:\n            if len(sample['text_token']) / num_frames < min_output_input_ratio:\n                continue\n            if len(sample['text_token']) / num_frames > max_output_input_ratio:\n                continue\n        yield sample\n\n\ndef resample(data, resample_rate=22050, min_sample_rate=16000, mode='train'):\n    \"\"\" Resample data.\n        Inplace operation.\n\n        Args:\n            data: Iterable[{key, wav, label, sample_rate}]\n            resample_rate: target resample rate\n\n        Returns:\n            Iterable[{key, wav, label, sample_rate}]\n    \"\"\"\n    for sample in data:\n        assert 'sample_rate' in sample\n        assert 'speech' in sample\n        sample_rate = sample['sample_rate']\n        waveform = sample['speech']\n        if sample_rate != resample_rate:\n            if sample_rate < min_sample_rate:\n                continue\n            sample['sample_rate'] = resample_rate\n            sample['speech'] = torchaudio.transforms.Resample(\n                orig_freq=sample_rate, new_freq=resample_rate)(waveform)\n        max_val = sample['speech'].abs().max()\n        if max_val > 1:\n            sample['speech'] /= max_val\n        yield sample\n\n\ndef truncate(data, truncate_length=24576, mode='train'):\n    \"\"\" Truncate data.\n\n        Args:\n            data: Iterable[{key, wav, label, sample_rate}]\n            truncate_length: truncate length\n\n        Returns:\n            Iterable[{key, wav, label, sample_rate}]\n    \"\"\"\n    for sample in data:\n        waveform = sample['speech']\n        if waveform.shape[1] > truncate_length:\n            start = random.randint(0, waveform.shape[1] - truncate_length)\n            waveform = waveform[:, start: start + truncate_length]\n        else:\n            waveform = torch.concat([waveform, torch.zeros(1, truncate_length - waveform.shape[1])], dim=1)\n        sample['speech'] = waveform\n        yield sample\n\n\ndef compute_fbank(data,\n                  feat_extractor,\n                  mode='train'):\n    \"\"\" Extract fbank\n\n        Args:\n            data: Iterable[{key, wav, label, sample_rate}]\n\n        Returns:\n            Iterable[{key, feat, label}]\n    \"\"\"\n    for sample in data:\n        assert 'sample_rate' in sample\n        assert 'speech' in sample\n        assert 'utt' in sample\n        assert 'text_token' in sample\n        waveform = sample['speech']\n        mat = feat_extractor(waveform).squeeze(dim=0).transpose(0, 1)\n        sample['speech_feat'] = mat\n        yield sample\n\n\ndef compute_f0(data, pitch_extractor, mode='train'):\n    \"\"\" Extract f0\n\n        Args:\n            data: Iterable[{key, wav, label, sample_rate}]\n\n        Returns:\n            Iterable[{key, feat, label}]\n    \"\"\"\n    for sample in data:\n        assert 'sample_rate' in sample\n        assert 'speech' in sample\n        assert 'utt' in sample\n        assert 'text_token' in sample\n        waveform = sample['speech']\n        mat = pitch_extractor(waveform).transpose(1, 2)\n        mat = F.interpolate(mat, size=sample['speech_feat'].shape[0], mode='linear')\n        sample['pitch_feat'] = mat[0, 0]\n        yield sample\n\n\ndef parse_embedding(data, normalize, mode='train'):\n    \"\"\" Parse utt_embedding/spk_embedding\n\n        Args:\n            data: Iterable[{key, wav, label, sample_rate}]\n\n        Returns:\n            Iterable[{key, feat, label}]\n    \"\"\"\n    for sample in data:\n        sample['utt_embedding'] = torch.tensor(sample['utt_embedding'], dtype=torch.float32)\n        sample['spk_embedding'] = torch.tensor(sample['spk_embedding'], dtype=torch.float32)\n        if normalize:\n            sample['utt_embedding'] = F.normalize(sample['utt_embedding'], dim=0)\n            sample['spk_embedding'] = F.normalize(sample['spk_embedding'], dim=0)\n        yield sample\n\n\ndef tokenize(data, get_tokenizer, allowed_special, mode='train'):\n    \"\"\" Decode text to chars or BPE\n        Inplace operation\n\n        Args:\n            data: Iterable[{key, wav, txt, sample_rate}]\n\n        Returns:\n            Iterable[{key, wav, txt, tokens, label, sample_rate}]\n    \"\"\"\n    tokenizer = get_tokenizer()\n    for sample in data:\n        assert 'text' in sample\n        sample['text_token'] = tokenizer.encode(sample['text'], allowed_special=allowed_special)\n        if mode == 'inference':\n            sample['tts_text_token'] = tokenizer.encode(sample['tts_text'], allowed_special=allowed_special)\n        yield sample\n\n\ndef shuffle(data, shuffle_size=10000, mode='train'):\n    \"\"\" Local shuffle the data\n\n        Args:\n            data: Iterable[{key, feat, label}]\n            shuffle_size: buffer size for shuffle\n\n        Returns:\n            Iterable[{key, feat, label}]\n    \"\"\"\n    buf = []\n    for sample in data:\n        buf.append(sample)\n        if len(buf) >= shuffle_size:\n            random.shuffle(buf)\n            for x in buf:\n                yield x\n            buf = []\n    # The sample left over\n    random.shuffle(buf)\n    for x in buf:\n        yield x\n\n\ndef sort(data, sort_size=500, mode='train'):\n    \"\"\" Sort the data by feature length.\n        Sort is used after shuffle and before batch, so we can group\n        utts with similar lengths into a batch, and `sort_size` should\n        be less than `shuffle_size`\n\n        Args:\n            data: Iterable[{key, feat, label}]\n            sort_size: buffer size for sort\n\n        Returns:\n            Iterable[{key, feat, label}]\n    \"\"\"\n\n    buf = []\n    for sample in data:\n        buf.append(sample)\n        if len(buf) >= sort_size:\n            buf.sort(key=lambda x: x['speech_feat'].size(0))\n            for x in buf:\n                yield x\n            buf = []\n    # The sample left over\n    buf.sort(key=lambda x: x['speech_feat'].size(0))\n    for x in buf:\n        yield x\n\n\ndef static_batch(data, batch_size=16):\n    \"\"\" Static batch the data by `batch_size`\n\n        Args:\n            data: Iterable[{key, feat, label}]\n            batch_size: batch size\n\n        Returns:\n            Iterable[List[{key, feat, label}]]\n    \"\"\"\n    buf = []\n    for sample in data:\n        buf.append(sample)\n        if len(buf) >= batch_size:\n            yield buf\n            buf = []\n    if len(buf) > 0:\n        yield buf\n\n\ndef dynamic_batch(data, max_frames_in_batch=12000, mode='train'):\n    \"\"\" Dynamic batch the data until the total frames in batch\n        reach `max_frames_in_batch`\n\n        Args:\n            data: Iterable[{key, feat, label}]\n            max_frames_in_batch: max_frames in one batch\n\n        Returns:\n            Iterable[List[{key, feat, label}]]\n    \"\"\"\n    buf = []\n    longest_frames = 0\n    for sample in data:\n        assert 'speech_feat' in sample\n        assert isinstance(sample['speech_feat'], torch.Tensor)\n        new_sample_frames = sample['speech_feat'].size(0)\n        longest_frames = max(longest_frames, new_sample_frames)\n        frames_after_padding = longest_frames * (len(buf) + 1)\n        if frames_after_padding > max_frames_in_batch:\n            yield buf\n            buf = [sample]\n            longest_frames = new_sample_frames\n        else:\n            buf.append(sample)\n    if len(buf) > 0:\n        yield buf\n\n\ndef batch(data, batch_type='static', batch_size=16, max_frames_in_batch=12000, mode='train'):\n    \"\"\" Wrapper for static/dynamic batch\n    \"\"\"\n    if mode == 'inference':\n        return static_batch(data, 1)\n    else:\n        if batch_type == 'static':\n            return static_batch(data, batch_size)\n        elif batch_type == 'dynamic':\n            return dynamic_batch(data, max_frames_in_batch)\n        else:\n            logging.fatal('Unsupported batch type {}'.format(batch_type))\n\n\ndef padding(data, use_spk_embedding, mode='train', gan=False):\n    \"\"\" Padding the data into training data\n\n        Args:\n            data: Iterable[List[{key, feat, label}]]\n\n        Returns:\n            Iterable[Tuple(keys, feats, labels, feats lengths, label lengths)]\n    \"\"\"\n    for sample in data:\n        assert isinstance(sample, list)\n        speech_feat_len = torch.tensor([x['speech_feat'].size(1) for x in sample],\n                                       dtype=torch.int32)\n        order = torch.argsort(speech_feat_len, descending=True)\n\n        utts = [sample[i]['utt'] for i in order]\n        speech = [sample[i]['speech'].squeeze(dim=0) for i in order]\n        speech_len = torch.tensor([i.size(0) for i in speech], dtype=torch.int32)\n        speech = pad_sequence(speech, batch_first=True, padding_value=0)\n        speech_token = [torch.tensor(sample[i]['speech_token']) for i in order]\n        speech_token_len = torch.tensor([i.size(0) for i in speech_token], dtype=torch.int32)\n        speech_token = pad_sequence(speech_token,\n                                    batch_first=True,\n                                    padding_value=0)\n        speech_feat = [sample[i]['speech_feat'] for i in order]\n        speech_feat_len = torch.tensor([i.size(0) for i in speech_feat], dtype=torch.int32)\n        speech_feat = pad_sequence(speech_feat,\n                                   batch_first=True,\n                                   padding_value=0)\n        text = [sample[i]['text'] for i in order]\n        text_token = [torch.tensor(sample[i]['text_token']) for i in order]\n        text_token_len = torch.tensor([i.size(0) for i in text_token], dtype=torch.int32)\n        text_token = pad_sequence(text_token, batch_first=True, padding_value=0)\n        utt_embedding = torch.stack([sample[i]['utt_embedding'] for i in order], dim=0)\n        spk_embedding = torch.stack([sample[i]['spk_embedding'] for i in order], dim=0)\n        batch = {\n            \"utts\": utts,\n            \"speech\": speech,\n            \"speech_len\": speech_len,\n            \"speech_token\": speech_token,\n            \"speech_token_len\": speech_token_len,\n            \"speech_feat\": speech_feat,\n            \"speech_feat_len\": speech_feat_len,\n            \"text\": text,\n            \"text_token\": text_token,\n            \"text_token_len\": text_token_len,\n            \"utt_embedding\": utt_embedding,\n            \"spk_embedding\": spk_embedding,\n        }\n        if gan is True:\n            # in gan train, we need pitch_feat\n            pitch_feat = [sample[i]['pitch_feat'] for i in order]\n            pitch_feat_len = torch.tensor([i.size(0) for i in pitch_feat], dtype=torch.int32)\n            pitch_feat = pad_sequence(pitch_feat,\n                                      batch_first=True,\n                                      padding_value=0)\n            batch[\"pitch_feat\"] = pitch_feat\n            batch[\"pitch_feat_len\"] = pitch_feat_len\n        else:\n            # only gan train needs speech, delete it to save memory\n            del batch[\"speech\"]\n            del batch[\"speech_len\"]\n        if mode == 'inference':\n            tts_text = [sample[i]['tts_text'] for i in order]\n            tts_index = [sample[i]['tts_index'] for i in order]\n            tts_text_token = [torch.tensor(sample[i]['tts_text_token']) for i in order]\n            tts_text_token_len = torch.tensor([i.size(0) for i in tts_text_token], dtype=torch.int32)\n            tts_text_token = pad_sequence(tts_text_token, batch_first=True, padding_value=-1)\n            batch.update({'tts_text': tts_text,\n                          'tts_index': tts_index,\n                          'tts_text_token': tts_text_token,\n                          'tts_text_token_len': tts_text_token_len})\n        if use_spk_embedding is True:\n            batch[\"embedding\"] = batch[\"spk_embedding\"]\n        else:\n            batch[\"embedding\"] = batch[\"utt_embedding\"]\n        yield batch\n"}
{"type": "source_file", "path": "cosyvoice/transformer/activation.py", "content": "# Copyright (c) 2020 Johns Hopkins University (Shinji Watanabe)\n#               2020 Northwestern Polytechnical University (Pengcheng Guo)\n#               2020 Mobvoi Inc (Binbin Zhang)\n#               2024 Alibaba Inc (Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Swish() activation function for Conformer.\"\"\"\n\nimport torch\nfrom torch import nn, sin, pow\nfrom torch.nn import Parameter\n\n\nclass Swish(torch.nn.Module):\n    \"\"\"Construct an Swish object.\"\"\"\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Return Swish activation function.\"\"\"\n        return x * torch.sigmoid(x)\n\n\n# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n#   LICENSE is in incl_licenses directory.\nclass Snake(nn.Module):\n    '''\n    Implementation of a sine-based periodic activation function\n    Shape:\n        - Input: (B, C, T)\n        - Output: (B, C, T), same shape as the input\n    Parameters:\n        - alpha - trainable parameter\n    References:\n        - This activation function is from this paper by Liu Ziyin, Tilman Hartwig, Masahito Ueda:\n        https://arxiv.org/abs/2006.08195\n    Examples:\n        >>> a1 = snake(256)\n        >>> x = torch.randn(256)\n        >>> x = a1(x)\n    '''\n    def __init__(self, in_features, alpha=1.0, alpha_trainable=True, alpha_logscale=False):\n        '''\n        Initialization.\n        INPUT:\n            - in_features: shape of the input\n            - alpha: trainable parameter\n            alpha is initialized to 1 by default, higher values = higher-frequency.\n            alpha will be trained along with the rest of your model.\n        '''\n        super(Snake, self).__init__()\n        self.in_features = in_features\n\n        # initialize alpha\n        self.alpha_logscale = alpha_logscale\n        if self.alpha_logscale:  # log scale alphas initialized to zeros\n            self.alpha = Parameter(torch.zeros(in_features) * alpha)\n        else:  # linear scale alphas initialized to ones\n            self.alpha = Parameter(torch.ones(in_features) * alpha)\n\n        self.alpha.requires_grad = alpha_trainable\n\n        self.no_div_by_zero = 0.000000001\n\n    def forward(self, x):\n        '''\n        Forward pass of the function.\n        Applies the function to the input elementwise.\n        Snake ∶= x + 1/a * sin^2 (xa)\n        '''\n        alpha = self.alpha.unsqueeze(0).unsqueeze(-1)  # line up with x to [B, C, T]\n        if self.alpha_logscale:\n            alpha = torch.exp(alpha)\n        x = x + (1.0 / (alpha + self.no_div_by_zero)) * pow(sin(x * alpha), 2)\n\n        return x\n"}
{"type": "source_file", "path": "cosyvoice/transformer/__init__.py", "content": ""}
{"type": "source_file", "path": "cosyvoice/transformer/decoder_layer.py", "content": "# Copyright (c) 2019 Shigeki Karita\n#               2020 Mobvoi Inc (Binbin Zhang)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Decoder self-attention layer definition.\"\"\"\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"Single decoder layer module.\n\n    Args:\n        size (int): Input dimension.\n        self_attn (torch.nn.Module): Self-attention module instance.\n            `MultiHeadedAttention` instance can be used as the argument.\n        src_attn (torch.nn.Module): Inter-attention module instance.\n            `MultiHeadedAttention` instance can be used as the argument.\n            If `None` is passed, Inter-attention is not used, such as\n            CIF, GPT, and other decoder only model.\n        feed_forward (torch.nn.Module): Feed-forward module instance.\n            `PositionwiseFeedForward` instance can be used as the argument.\n        dropout_rate (float): Dropout rate.\n        normalize_before (bool):\n            True: use layer_norm before each sub-block.\n            False: to use layer_norm after each sub-block.\n    \"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        self_attn: nn.Module,\n        src_attn: Optional[nn.Module],\n        feed_forward: nn.Module,\n        dropout_rate: float,\n        normalize_before: bool = True,\n    ):\n        \"\"\"Construct an DecoderLayer object.\"\"\"\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.norm1 = nn.LayerNorm(size, eps=1e-5)\n        self.norm2 = nn.LayerNorm(size, eps=1e-5)\n        self.norm3 = nn.LayerNorm(size, eps=1e-5)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.normalize_before = normalize_before\n\n    def forward(\n        self,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor,\n        cache: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Compute decoded features.\n\n        Args:\n            tgt (torch.Tensor): Input tensor (#batch, maxlen_out, size).\n            tgt_mask (torch.Tensor): Mask for input tensor\n                (#batch, maxlen_out).\n            memory (torch.Tensor): Encoded memory\n                (#batch, maxlen_in, size).\n            memory_mask (torch.Tensor): Encoded memory mask\n                (#batch, maxlen_in).\n            cache (torch.Tensor): cached tensors.\n                (#batch, maxlen_out - 1, size).\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, maxlen_out, size).\n            torch.Tensor: Mask for output tensor (#batch, maxlen_out).\n            torch.Tensor: Encoded memory (#batch, maxlen_in, size).\n            torch.Tensor: Encoded memory mask (#batch, maxlen_in).\n\n        \"\"\"\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        if cache is None:\n            tgt_q = tgt\n            tgt_q_mask = tgt_mask\n        else:\n            # compute only the last frame query keeping dim: max_time_out -> 1\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), \"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"\n            tgt_q = tgt[:, -1:, :]\n            residual = residual[:, -1:, :]\n            tgt_q_mask = tgt_mask[:, -1:, :]\n\n        x = residual + self.dropout(\n            self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)[0])\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        if self.src_attn is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.norm2(x)\n            x = residual + self.dropout(\n                self.src_attn(x, memory, memory, memory_mask)[0])\n            if not self.normalize_before:\n                x = self.norm2(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm3(x)\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm3(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        return x, tgt_mask, memory, memory_mask\n"}
{"type": "source_file", "path": "cosyvoice/transformer/convolution.py", "content": "# Copyright (c) 2020 Mobvoi Inc. (authors: Binbin Zhang, Di Wu)\n#               2024 Alibaba Inc (Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Modified from ESPnet(https://github.com/espnet/espnet)\n\"\"\"ConvolutionModule definition.\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass ConvolutionModule(nn.Module):\n    \"\"\"ConvolutionModule in Conformer model.\"\"\"\n\n    def __init__(self,\n                 channels: int,\n                 kernel_size: int = 15,\n                 activation: nn.Module = nn.ReLU(),\n                 norm: str = \"batch_norm\",\n                 causal: bool = False,\n                 bias: bool = True):\n        \"\"\"Construct an ConvolutionModule object.\n        Args:\n            channels (int): The number of channels of conv layers.\n            kernel_size (int): Kernel size of conv layers.\n            causal (int): Whether use causal convolution or not\n        \"\"\"\n        super().__init__()\n\n        self.pointwise_conv1 = nn.Conv1d(\n            channels,\n            2 * channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        # self.lorder is used to distinguish if it's a causal convolution,\n        # if self.lorder > 0: it's a causal convolution, the input will be\n        #    padded with self.lorder frames on the left in forward.\n        # else: it's a symmetrical convolution\n        if causal:\n            padding = 0\n            self.lorder = kernel_size - 1\n        else:\n            # kernel_size should be an odd number for none causal convolution\n            assert (kernel_size - 1) % 2 == 0\n            padding = (kernel_size - 1) // 2\n            self.lorder = 0\n        self.depthwise_conv = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size,\n            stride=1,\n            padding=padding,\n            groups=channels,\n            bias=bias,\n        )\n\n        assert norm in ['batch_norm', 'layer_norm']\n        if norm == \"batch_norm\":\n            self.use_layer_norm = False\n            self.norm = nn.BatchNorm1d(channels)\n        else:\n            self.use_layer_norm = True\n            self.norm = nn.LayerNorm(channels)\n\n        self.pointwise_conv2 = nn.Conv1d(\n            channels,\n            channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n        )\n        self.activation = activation\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask_pad: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),\n        cache: torch.Tensor = torch.zeros((0, 0, 0)),\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute convolution module.\n        Args:\n            x (torch.Tensor): Input tensor (#batch, time, channels).\n            mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),\n                (0, 0, 0) means fake mask.\n            cache (torch.Tensor): left context cache, it is only\n                used in causal convolution (#batch, channels, cache_t),\n                (0, 0, 0) meas fake cache.\n        Returns:\n            torch.Tensor: Output tensor (#batch, time, channels).\n        \"\"\"\n        # exchange the temporal dimension and the feature dimension\n        x = x.transpose(1, 2)  # (#batch, channels, time)\n\n        # mask batch padding\n        if mask_pad.size(2) > 0:  # time > 0\n            x.masked_fill_(~mask_pad, 0.0)\n\n        if self.lorder > 0:\n            if cache.size(2) == 0:  # cache_t == 0\n                x = nn.functional.pad(x, (self.lorder, 0), 'constant', 0.0)\n            else:\n                assert cache.size(0) == x.size(0)  # equal batch\n                assert cache.size(1) == x.size(1)  # equal channel\n                x = torch.cat((cache, x), dim=2)\n            assert (x.size(2) > self.lorder)\n            new_cache = x[:, :, -self.lorder:]\n        else:\n            # It's better we just return None if no cache is required,\n            # However, for JIT export, here we just fake one tensor instead of\n            # None.\n            new_cache = torch.zeros((0, 0, 0), dtype=x.dtype, device=x.device)\n\n        # GLU mechanism\n        x = self.pointwise_conv1(x)  # (batch, 2*channel, dim)\n        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)\n\n        # 1D Depthwise Conv\n        x = self.depthwise_conv(x)\n        if self.use_layer_norm:\n            x = x.transpose(1, 2)\n        x = self.activation(self.norm(x))\n        if self.use_layer_norm:\n            x = x.transpose(1, 2)\n        x = self.pointwise_conv2(x)\n        # mask batch padding\n        if mask_pad.size(2) > 0:  # time > 0\n            x.masked_fill_(~mask_pad, 0.0)\n\n        return x.transpose(1, 2), new_cache\n"}
{"type": "source_file", "path": "cosyvoice/transformer/decoder.py", "content": "# Copyright (c) 2021 Mobvoi Inc. (authors: Binbin Zhang, Di Wu)\n#               2024 Alibaba Inc (Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Modified from ESPnet(https://github.com/espnet/espnet)\n\"\"\"Decoder definition.\"\"\"\nfrom typing import Tuple, List, Optional\n\nimport torch\nimport torch.utils.checkpoint as ckpt\nimport logging\n\nfrom cosyvoice.transformer.decoder_layer import DecoderLayer\nfrom cosyvoice.transformer.positionwise_feed_forward import PositionwiseFeedForward\nfrom cosyvoice.utils.class_utils import (\n    COSYVOICE_EMB_CLASSES,\n    COSYVOICE_ATTENTION_CLASSES,\n    COSYVOICE_ACTIVATION_CLASSES,\n)\nfrom cosyvoice.utils.mask import (subsequent_mask, make_pad_mask)\n\n\nclass TransformerDecoder(torch.nn.Module):\n    \"\"\"Base class of Transfomer decoder module.\n    Args:\n        vocab_size: output dim\n        encoder_output_size: dimension of attention\n        attention_heads: the number of heads of multi head attention\n        linear_units: the hidden units number of position-wise feedforward\n        num_blocks: the number of decoder blocks\n        dropout_rate: dropout rate\n        self_attention_dropout_rate: dropout rate for attention\n        input_layer: input layer type\n        use_output_layer: whether to use output layer\n        pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n        normalize_before:\n            True: use layer_norm before each sub-block of a layer.\n            False: use layer_norm after each sub-block of a layer.\n        src_attention: if false, encoder-decoder cross attention is not\n                       applied, such as CIF model\n        key_bias: whether use bias in attention.linear_k, False for whisper models.\n        gradient_checkpointing: rerunning a forward-pass segment for each\n            checkpointed segment during backward.\n        tie_word_embedding: Tie or clone module weights depending of whether we are\n            using TorchScript or not\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        encoder_output_size: int,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        self_attention_dropout_rate: float = 0.0,\n        src_attention_dropout_rate: float = 0.0,\n        input_layer: str = \"embed\",\n        use_output_layer: bool = True,\n        normalize_before: bool = True,\n        src_attention: bool = True,\n        key_bias: bool = True,\n        activation_type: str = \"relu\",\n        gradient_checkpointing: bool = False,\n        tie_word_embedding: bool = False,\n    ):\n        super().__init__()\n        attention_dim = encoder_output_size\n        activation = COSYVOICE_ACTIVATION_CLASSES[activation_type]()\n\n        self.embed = torch.nn.Sequential(\n            torch.nn.Identity() if input_layer == \"no_pos\" else\n            torch.nn.Embedding(vocab_size, attention_dim),\n            COSYVOICE_EMB_CLASSES[input_layer](attention_dim,\n                                               positional_dropout_rate),\n        )\n\n        self.normalize_before = normalize_before\n        self.after_norm = torch.nn.LayerNorm(attention_dim, eps=1e-5)\n        self.use_output_layer = use_output_layer\n        if use_output_layer:\n            self.output_layer = torch.nn.Linear(attention_dim, vocab_size)\n        else:\n            self.output_layer = torch.nn.Identity()\n        self.num_blocks = num_blocks\n        self.decoders = torch.nn.ModuleList([\n            DecoderLayer(\n                attention_dim,\n                COSYVOICE_ATTENTION_CLASSES[\"selfattn\"](\n                    attention_heads, attention_dim,\n                    self_attention_dropout_rate, key_bias),\n                COSYVOICE_ATTENTION_CLASSES[\"selfattn\"](\n                    attention_heads, attention_dim, src_attention_dropout_rate,\n                    key_bias) if src_attention else None,\n                PositionwiseFeedForward(attention_dim, linear_units,\n                                        dropout_rate, activation),\n                dropout_rate,\n                normalize_before,\n            ) for _ in range(self.num_blocks)\n        ])\n\n        self.gradient_checkpointing = gradient_checkpointing\n        self.tie_word_embedding = tie_word_embedding\n\n    def forward(\n        self,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor,\n        ys_in_pad: torch.Tensor,\n        ys_in_lens: torch.Tensor,\n        r_ys_in_pad: torch.Tensor = torch.empty(0),\n        reverse_weight: float = 0.0,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward decoder.\n        Args:\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            memory_mask: encoder memory mask, (batch, 1, maxlen_in)\n            ys_in_pad: padded input token ids, int64 (batch, maxlen_out)\n            ys_in_lens: input lengths of this batch (batch)\n            r_ys_in_pad: not used in transformer decoder, in order to unify api\n                with bidirectional decoder\n            reverse_weight: not used in transformer decoder, in order to unify\n                api with bidirectional decode\n        Returns:\n            (tuple): tuple containing:\n                x: decoded token score before softmax (batch, maxlen_out,\n                    vocab_size) if use_output_layer is True,\n                torch.tensor(0.0), in order to unify api with bidirectional decoder\n                olens: (batch, )\n        NOTE(xcsong):\n            We pass the `__call__` method of the modules instead of `forward` to the\n            checkpointing API because `__call__` attaches all the hooks of the module.\n            https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2\n        \"\"\"\n        tgt = ys_in_pad\n        maxlen = tgt.size(1)\n        # tgt_mask: (B, 1, L)\n        tgt_mask = ~make_pad_mask(ys_in_lens, maxlen).unsqueeze(1)\n        tgt_mask = tgt_mask.to(tgt.device)\n        # m: (1, L, L)\n        m = subsequent_mask(tgt_mask.size(-1),\n                            device=tgt_mask.device).unsqueeze(0)\n        # tgt_mask: (B, L, L)\n        tgt_mask = tgt_mask & m\n        x, _ = self.embed(tgt)\n        if self.gradient_checkpointing and self.training:\n            x = self.forward_layers_checkpointed(x, tgt_mask, memory,\n                                                 memory_mask)\n        else:\n            x = self.forward_layers(x, tgt_mask, memory, memory_mask)\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.use_output_layer:\n            x = self.output_layer(x)\n        olens = tgt_mask.sum(1)\n        return x, torch.tensor(0.0), olens\n\n    def forward_layers(self, x: torch.Tensor, tgt_mask: torch.Tensor,\n                       memory: torch.Tensor,\n                       memory_mask: torch.Tensor) -> torch.Tensor:\n        for layer in self.decoders:\n            x, tgt_mask, memory, memory_mask = layer(x, tgt_mask, memory,\n                                                     memory_mask)\n        return x\n\n    @torch.jit.unused\n    def forward_layers_checkpointed(self, x: torch.Tensor,\n                                    tgt_mask: torch.Tensor,\n                                    memory: torch.Tensor,\n                                    memory_mask: torch.Tensor) -> torch.Tensor:\n        for layer in self.decoders:\n            x, tgt_mask, memory, memory_mask = ckpt.checkpoint(\n                layer.__call__, x, tgt_mask, memory, memory_mask)\n        return x\n\n    def forward_one_step(\n        self,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        cache: Optional[List[torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"Forward one step.\n            This is only used for decoding.\n        Args:\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            memory_mask: encoded memory mask, (batch, 1, maxlen_in)\n            tgt: input token ids, int64 (batch, maxlen_out)\n            tgt_mask: input token mask,  (batch, maxlen_out)\n                      dtype=torch.uint8 in PyTorch 1.2-\n                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            cache: cached output list of (batch, max_time_out-1, size)\n        Returns:\n            y, cache: NN output value and cache per `self.decoders`.\n            y.shape` is (batch, maxlen_out, token)\n        \"\"\"\n        x, _ = self.embed(tgt)\n        new_cache = []\n        for i, decoder in enumerate(self.decoders):\n            if cache is None:\n                c = None\n            else:\n                c = cache[i]\n            x, tgt_mask, memory, memory_mask = decoder(x,\n                                                       tgt_mask,\n                                                       memory,\n                                                       memory_mask,\n                                                       cache=c)\n            new_cache.append(x)\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.use_output_layer:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n        return y, new_cache\n\n    def tie_or_clone_weights(self, jit_mode: bool = True):\n        \"\"\"Tie or clone module weights (between word_emb and output_layer)\n            depending of whether we are using TorchScript or not\"\"\"\n        if not self.use_output_layer:\n            return\n        if jit_mode:\n            logging.info(\"clone emb.weight to output.weight\")\n            self.output_layer.weight = torch.nn.Parameter(\n                self.embed[0].weight.clone())\n        else:\n            logging.info(\"tie emb.weight with output.weight\")\n            self.output_layer.weight = self.embed[0].weight\n\n        if getattr(self.output_layer, \"bias\", None) is not None:\n            self.output_layer.bias.data = torch.nn.functional.pad(\n                self.output_layer.bias.data,\n                (\n                    0,\n                    self.output_layer.weight.shape[0] -\n                    self.output_layer.bias.shape[0],\n                ),\n                \"constant\",\n                0,\n            )\n\n\nclass BiTransformerDecoder(torch.nn.Module):\n    \"\"\"Base class of Transfomer decoder module.\n    Args:\n        vocab_size: output dim\n        encoder_output_size: dimension of attention\n        attention_heads: the number of heads of multi head attention\n        linear_units: the hidden units number of position-wise feedforward\n        num_blocks: the number of decoder blocks\n        r_num_blocks: the number of right to left decoder blocks\n        dropout_rate: dropout rate\n        self_attention_dropout_rate: dropout rate for attention\n        input_layer: input layer type\n        use_output_layer: whether to use output layer\n        pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n        normalize_before:\n            True: use layer_norm before each sub-block of a layer.\n            False: use layer_norm after each sub-block of a layer.\n        key_bias: whether use bias in attention.linear_k, False for whisper models.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        encoder_output_size: int,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        r_num_blocks: int = 0,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        self_attention_dropout_rate: float = 0.0,\n        src_attention_dropout_rate: float = 0.0,\n        input_layer: str = \"embed\",\n        use_output_layer: bool = True,\n        normalize_before: bool = True,\n        key_bias: bool = True,\n        gradient_checkpointing: bool = False,\n        tie_word_embedding: bool = False,\n    ):\n\n        super().__init__()\n        self.tie_word_embedding = tie_word_embedding\n        self.left_decoder = TransformerDecoder(\n            vocab_size,\n            encoder_output_size,\n            attention_heads,\n            linear_units,\n            num_blocks,\n            dropout_rate,\n            positional_dropout_rate,\n            self_attention_dropout_rate,\n            src_attention_dropout_rate,\n            input_layer,\n            use_output_layer,\n            normalize_before,\n            key_bias=key_bias,\n            gradient_checkpointing=gradient_checkpointing,\n            tie_word_embedding=tie_word_embedding)\n\n        self.right_decoder = TransformerDecoder(\n            vocab_size,\n            encoder_output_size,\n            attention_heads,\n            linear_units,\n            r_num_blocks,\n            dropout_rate,\n            positional_dropout_rate,\n            self_attention_dropout_rate,\n            src_attention_dropout_rate,\n            input_layer,\n            use_output_layer,\n            normalize_before,\n            key_bias=key_bias,\n            gradient_checkpointing=gradient_checkpointing,\n            tie_word_embedding=tie_word_embedding)\n\n    def forward(\n        self,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor,\n        ys_in_pad: torch.Tensor,\n        ys_in_lens: torch.Tensor,\n        r_ys_in_pad: torch.Tensor,\n        reverse_weight: float = 0.0,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward decoder.\n        Args:\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            memory_mask: encoder memory mask, (batch, 1, maxlen_in)\n            ys_in_pad: padded input token ids, int64 (batch, maxlen_out)\n            ys_in_lens: input lengths of this batch (batch)\n            r_ys_in_pad: padded input token ids, int64 (batch, maxlen_out),\n                used for right to left decoder\n            reverse_weight: used for right to left decoder\n        Returns:\n            (tuple): tuple containing:\n                x: decoded token score before softmax (batch, maxlen_out,\n                    vocab_size) if use_output_layer is True,\n                r_x: x: decoded token score (right to left decoder)\n                    before softmax (batch, maxlen_out, vocab_size)\n                    if use_output_layer is True,\n                olens: (batch, )\n        \"\"\"\n        l_x, _, olens = self.left_decoder(memory, memory_mask, ys_in_pad,\n                                          ys_in_lens)\n        r_x = torch.tensor(0.0)\n        if reverse_weight > 0.0:\n            r_x, _, olens = self.right_decoder(memory, memory_mask,\n                                               r_ys_in_pad, ys_in_lens)\n        return l_x, r_x, olens\n\n    def forward_one_step(\n        self,\n        memory: torch.Tensor,\n        memory_mask: torch.Tensor,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        cache: Optional[List[torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"Forward one step.\n            This is only used for decoding.\n        Args:\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            memory_mask: encoded memory mask, (batch, 1, maxlen_in)\n            tgt: input token ids, int64 (batch, maxlen_out)\n            tgt_mask: input token mask,  (batch, maxlen_out)\n                      dtype=torch.uint8 in PyTorch 1.2-\n                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            cache: cached output list of (batch, max_time_out-1, size)\n        Returns:\n            y, cache: NN output value and cache per `self.decoders`.\n            y.shape` is (batch, maxlen_out, token)\n        \"\"\"\n        return self.left_decoder.forward_one_step(memory, memory_mask, tgt,\n                                                  tgt_mask, cache)\n\n    def tie_or_clone_weights(self, jit_mode: bool = True):\n        \"\"\"Tie or clone module weights (between word_emb and output_layer)\n            depending of whether we are using TorchScript or not\"\"\"\n        self.left_decoder.tie_or_clone_weights(jit_mode)\n        self.right_decoder.tie_or_clone_weights(jit_mode)\n"}
{"type": "source_file", "path": "cosyvoice/tokenizer/tokenizer.py", "content": "import base64\nimport os\nfrom functools import lru_cache\nfrom typing import Optional\nfrom whisper.tokenizer import Tokenizer\n\nimport tiktoken\n\nLANGUAGES = {\n    \"en\": \"english\",\n    \"zh\": \"chinese\",\n    \"de\": \"german\",\n    \"es\": \"spanish\",\n    \"ru\": \"russian\",\n    \"ko\": \"korean\",\n    \"fr\": \"french\",\n    \"ja\": \"japanese\",\n    \"pt\": \"portuguese\",\n    \"tr\": \"turkish\",\n    \"pl\": \"polish\",\n    \"ca\": \"catalan\",\n    \"nl\": \"dutch\",\n    \"ar\": \"arabic\",\n    \"sv\": \"swedish\",\n    \"it\": \"italian\",\n    \"id\": \"indonesian\",\n    \"hi\": \"hindi\",\n    \"fi\": \"finnish\",\n    \"vi\": \"vietnamese\",\n    \"he\": \"hebrew\",\n    \"uk\": \"ukrainian\",\n    \"el\": \"greek\",\n    \"ms\": \"malay\",\n    \"cs\": \"czech\",\n    \"ro\": \"romanian\",\n    \"da\": \"danish\",\n    \"hu\": \"hungarian\",\n    \"ta\": \"tamil\",\n    \"no\": \"norwegian\",\n    \"th\": \"thai\",\n    \"ur\": \"urdu\",\n    \"hr\": \"croatian\",\n    \"bg\": \"bulgarian\",\n    \"lt\": \"lithuanian\",\n    \"la\": \"latin\",\n    \"mi\": \"maori\",\n    \"ml\": \"malayalam\",\n    \"cy\": \"welsh\",\n    \"sk\": \"slovak\",\n    \"te\": \"telugu\",\n    \"fa\": \"persian\",\n    \"lv\": \"latvian\",\n    \"bn\": \"bengali\",\n    \"sr\": \"serbian\",\n    \"az\": \"azerbaijani\",\n    \"sl\": \"slovenian\",\n    \"kn\": \"kannada\",\n    \"et\": \"estonian\",\n    \"mk\": \"macedonian\",\n    \"br\": \"breton\",\n    \"eu\": \"basque\",\n    \"is\": \"icelandic\",\n    \"hy\": \"armenian\",\n    \"ne\": \"nepali\",\n    \"mn\": \"mongolian\",\n    \"bs\": \"bosnian\",\n    \"kk\": \"kazakh\",\n    \"sq\": \"albanian\",\n    \"sw\": \"swahili\",\n    \"gl\": \"galician\",\n    \"mr\": \"marathi\",\n    \"pa\": \"punjabi\",\n    \"si\": \"sinhala\",\n    \"km\": \"khmer\",\n    \"sn\": \"shona\",\n    \"yo\": \"yoruba\",\n    \"so\": \"somali\",\n    \"af\": \"afrikaans\",\n    \"oc\": \"occitan\",\n    \"ka\": \"georgian\",\n    \"be\": \"belarusian\",\n    \"tg\": \"tajik\",\n    \"sd\": \"sindhi\",\n    \"gu\": \"gujarati\",\n    \"am\": \"amharic\",\n    \"yi\": \"yiddish\",\n    \"lo\": \"lao\",\n    \"uz\": \"uzbek\",\n    \"fo\": \"faroese\",\n    \"ht\": \"haitian creole\",\n    \"ps\": \"pashto\",\n    \"tk\": \"turkmen\",\n    \"nn\": \"nynorsk\",\n    \"mt\": \"maltese\",\n    \"sa\": \"sanskrit\",\n    \"lb\": \"luxembourgish\",\n    \"my\": \"myanmar\",\n    \"bo\": \"tibetan\",\n    \"tl\": \"tagalog\",\n    \"mg\": \"malagasy\",\n    \"as\": \"assamese\",\n    \"tt\": \"tatar\",\n    \"haw\": \"hawaiian\",\n    \"ln\": \"lingala\",\n    \"ha\": \"hausa\",\n    \"ba\": \"bashkir\",\n    \"jw\": \"javanese\",\n    \"su\": \"sundanese\",\n    \"yue\": \"cantonese\",\n    \"minnan\": \"minnan\",\n    \"wuyu\": \"wuyu\",\n    \"dialect\": \"dialect\",\n    \"zh/en\": \"zh/en\",\n    \"en/zh\": \"en/zh\",\n}\n\n# language code lookup by name, with a few language aliases\nTO_LANGUAGE_CODE = {\n    **{language: code for code, language in LANGUAGES.items()},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n    \"mandarin\": \"zh\",\n}\n\nAUDIO_EVENT = {\n    \"ASR\": \"ASR\",\n    \"AED\": \"AED\",\n    \"SER\": \"SER\",\n    \"Speech\": \"Speech\",\n    \"/Speech\": \"/Speech\",\n    \"BGM\": \"BGM\",\n    \"/BGM\": \"/BGM\",\n    \"Laughter\": \"Laughter\",\n    \"/Laughter\": \"/Laughter\",\n    \"Applause\": \"Applause\",\n    \"/Applause\": \"/Applause\",\n}\n\nEMOTION = {\n    \"HAPPY\": \"HAPPY\",\n    \"SAD\": \"SAD\",\n    \"ANGRY\": \"ANGRY\",\n    \"NEUTRAL\": \"NEUTRAL\",\n}\n\nTTS_Vocal_Token = {\n    \"TTS/B\": \"TTS/B\",\n    \"TTS/O\": \"TTS/O\",\n    \"TTS/Q\": \"TTS/Q\",\n    \"TTS/A\": \"TTS/A\",\n    \"TTS/CO\": \"TTS/CO\",\n    \"TTS/CL\": \"TTS/CL\",\n    \"TTS/H\": \"TTS/H\",\n    **{f\"TTS/SP{i:02d}\": f\"TTS/SP{i:02d}\" for i in range(1, 14)}\n}\n\n\n@lru_cache(maxsize=None)\ndef get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n    vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n    ranks = {\n        base64.b64decode(token): int(rank)\n        for token, rank in (line.split() for line in open(vocab_path) if line)\n    }\n    n_vocab = len(ranks)\n    special_tokens = {}\n\n    specials = [\n        \"<|endoftext|>\",\n        \"<|startoftranscript|>\",\n        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n        *[f\"<|{audio_event}|>\" for audio_event in list(AUDIO_EVENT.keys())],\n        *[f\"<|{emotion}|>\" for emotion in list(EMOTION.keys())],\n        \"<|translate|>\",\n        \"<|transcribe|>\",\n        \"<|startoflm|>\",\n        \"<|startofprev|>\",\n        \"<|nospeech|>\",\n        \"<|notimestamps|>\",\n        *[f\"<|SPECIAL_TOKEN_{i}|>\" for i in range(1, 31)],        # register special tokens for ASR\n        *[f\"<|{tts}|>\" for tts in list(TTS_Vocal_Token.keys())],  # register special tokens for TTS\n        *[f\"<|{i * 0.02:.2f}|>\" for i in range(1501)],\n    ]\n\n    for token in specials:\n        special_tokens[token] = n_vocab\n        n_vocab += 1\n\n    return tiktoken.Encoding(\n        name=os.path.basename(vocab_path),\n        explicit_n_vocab=n_vocab,\n        pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        mergeable_ranks=ranks,\n        special_tokens=special_tokens,\n    )\n\n\n@lru_cache(maxsize=None)\ndef get_tokenizer(\n    multilingual: bool,\n    *,\n    num_languages: int = 99,\n    language: Optional[str] = None,\n    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n) -> Tokenizer:\n    if language is not None:\n        language = language.lower()\n        if language not in LANGUAGES:\n            if language in TO_LANGUAGE_CODE:\n                language = TO_LANGUAGE_CODE[language]\n            else:\n                raise ValueError(f\"Unsupported language: {language}\")\n\n    if multilingual:\n        encoding_name = \"multilingual_zh_ja_yue_char_del\"\n        language = language or \"en\"\n        task = task or \"transcribe\"\n    else:\n        encoding_name = \"gpt2\"\n        language = None\n        task = None\n\n    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n\n    return Tokenizer(\n        encoding=encoding, num_languages=num_languages, language=language, task=task\n    )\n"}
{"type": "source_file", "path": "cosyvoice/transformer/embedding.py", "content": "# Copyright (c) 2020 Mobvoi Inc. (authors: Binbin Zhang, Di Wu)\n#               2024 Alibaba Inc (Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Modified from ESPnet(https://github.com/espnet/espnet)\n\"\"\"Positonal Encoding Module.\"\"\"\n\nimport math\nfrom typing import Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass PositionalEncoding(torch.nn.Module):\n    \"\"\"Positional encoding.\n\n    :param int d_model: embedding dim\n    :param float dropout_rate: dropout rate\n    :param int max_len: maximum input length\n\n    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))\n    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))\n    \"\"\"\n\n    def __init__(self,\n                 d_model: int,\n                 dropout_rate: float,\n                 max_len: int = 5000,\n                 reverse: bool = False):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.max_len = max_len\n\n        self.pe = torch.zeros(self.max_len, self.d_model)\n        position = torch.arange(0, self.max_len,\n                                dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32) *\n            -(math.log(10000.0) / self.d_model))\n        self.pe[:, 0::2] = torch.sin(position * div_term)\n        self.pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = self.pe.unsqueeze(0)\n\n    def forward(self,\n                x: torch.Tensor,\n                offset: Union[int, torch.Tensor] = 0) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n            offset (int, torch.tensor): position offset\n\n        Returns:\n            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)\n            torch.Tensor: for compatibility to RelPositionalEncoding\n        \"\"\"\n\n        self.pe = self.pe.to(x.device)\n        pos_emb = self.position_encoding(offset, x.size(1), False)\n        x = x * self.xscale + pos_emb\n        return self.dropout(x), self.dropout(pos_emb)\n\n    def position_encoding(self,\n                          offset: Union[int, torch.Tensor],\n                          size: int,\n                          apply_dropout: bool = True) -> torch.Tensor:\n        \"\"\" For getting encoding in a streaming fashion\n\n        Attention!!!!!\n        we apply dropout only once at the whole utterance level in a none\n        streaming way, but will call this function several times with\n        increasing input size in a streaming scenario, so the dropout will\n        be applied several times.\n\n        Args:\n            offset (int or torch.tensor): start offset\n            size (int): required size of position encoding\n\n        Returns:\n            torch.Tensor: Corresponding encoding\n        \"\"\"\n        # How to subscript a Union type:\n        #   https://github.com/pytorch/pytorch/issues/69434\n        if isinstance(offset, int):\n            assert offset + size <= self.max_len\n            pos_emb = self.pe[:, offset:offset + size]\n        elif isinstance(offset, torch.Tensor) and offset.dim() == 0:  # scalar\n            assert offset + size <= self.max_len\n            pos_emb = self.pe[:, offset:offset + size]\n        else:  # for batched streaming decoding on GPU\n            assert torch.max(offset) + size <= self.max_len\n            index = offset.unsqueeze(1) + \\\n                torch.arange(0, size).to(offset.device)  # B X T\n            flag = index > 0\n            # remove negative offset\n            index = index * flag\n            pos_emb = F.embedding(index, self.pe[0])  # B X T X d_model\n\n        if apply_dropout:\n            pos_emb = self.dropout(pos_emb)\n        return pos_emb\n\n\nclass RelPositionalEncoding(PositionalEncoding):\n    \"\"\"Relative positional encoding module.\n    See : Appendix B in https://arxiv.org/abs/1901.02860\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 5000):\n        \"\"\"Initialize class.\"\"\"\n        super().__init__(d_model, dropout_rate, max_len, reverse=True)\n\n    def forward(self,\n                x: torch.Tensor,\n                offset: Union[int, torch.Tensor] = 0) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n            torch.Tensor: Positional embedding tensor (1, time, `*`).\n        \"\"\"\n        self.pe = self.pe.to(x.device)\n        x = x * self.xscale\n        pos_emb = self.position_encoding(offset, x.size(1), False)\n        return self.dropout(x), self.dropout(pos_emb)\n\n\nclass WhisperPositionalEncoding(PositionalEncoding):\n    \"\"\" Sinusoids position encoding used in openai-whisper.encoder\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 1500):\n        super().__init__(d_model, dropout_rate, max_len)\n        self.xscale = 1.0\n        log_timescale_increment = np.log(10000) / (d_model // 2 - 1)\n        inv_timescales = torch.exp(-log_timescale_increment *\n                                   torch.arange(d_model // 2))\n        scaled_time = torch.arange(max_len)[:, np.newaxis] * \\\n            inv_timescales[np.newaxis, :]\n        pe = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n        delattr(self, \"pe\")\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n\nclass LearnablePositionalEncoding(PositionalEncoding):\n    \"\"\" Learnable position encoding used in openai-whisper.decoder\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 448):\n        super().__init__(d_model, dropout_rate, max_len)\n        # NOTE(xcsong): overwrite self.pe & self.xscale\n        self.pe = torch.nn.Parameter(torch.empty(1, max_len, d_model))\n        self.xscale = 1.0\n\n\nclass NoPositionalEncoding(torch.nn.Module):\n    \"\"\" No position encoding\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float):\n        super().__init__()\n        self.d_model = d_model\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n\n    def forward(self,\n                x: torch.Tensor,\n                offset: Union[int, torch.Tensor] = 0) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\" Just return zero vector for interface compatibility\n        \"\"\"\n        pos_emb = torch.zeros(1, x.size(1), self.d_model).to(x.device)\n        return self.dropout(x), pos_emb\n\n    def position_encoding(self, offset: Union[int, torch.Tensor],\n                          size: int) -> torch.Tensor:\n        return torch.zeros(1, size, self.d_model)\n\n\nclass EspnetRelPositionalEncoding(torch.nn.Module):\n    \"\"\"Relative positional encoding module (new implementation).\n\n    Details can be found in https://github.com/espnet/espnet/pull/2816.\n\n    See : Appendix B in https://arxiv.org/abs/1901.02860\n\n    Args:\n        d_model (int): Embedding dimension.\n        dropout_rate (float): Dropout rate.\n        max_len (int): Maximum input length.\n\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 5000):\n        \"\"\"Construct an PositionalEncoding object.\"\"\"\n        super(EspnetRelPositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n\n    def extend_pe(self, x: torch.Tensor):\n        \"\"\"Reset the positional encodings.\"\"\"\n        if self.pe is not None:\n            # self.pe contains both positive and negative parts\n            # the length of self.pe is 2 * input_len - 1\n            if self.pe.size(1) >= x.size(1) * 2 - 1:\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        # Suppose `i` means to the position of query vecotr and `j` means the\n        # position of key vector. We use position relative positions when keys\n        # are to the left (i>j) and negative relative positions otherwise (i<j).\n        pe_positive = torch.zeros(x.size(1), self.d_model)\n        pe_negative = torch.zeros(x.size(1), self.d_model)\n        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe_positive[:, 0::2] = torch.sin(position * div_term)\n        pe_positive[:, 1::2] = torch.cos(position * div_term)\n        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n\n        # Reserve the order of positive indices and concat both positive and\n        # negative indices. This is used to support the shifting trick\n        # as in https://arxiv.org/abs/1901.02860\n        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n        pe_negative = pe_negative[1:].unsqueeze(0)\n        pe = torch.cat([pe_positive, pe_negative], dim=1)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor, offset: Union[int, torch.Tensor] = 0) \\\n            -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n\n        \"\"\"\n        self.extend_pe(x)\n        x = x * self.xscale\n        pos_emb = self.position_encoding(size=x.size(1), offset=offset)\n        return self.dropout(x), self.dropout(pos_emb)\n\n    def position_encoding(self,\n                          offset: Union[int, torch.Tensor],\n                          size: int) -> torch.Tensor:\n        \"\"\" For getting encoding in a streaming fashion\n\n        Attention!!!!!\n        we apply dropout only once at the whole utterance level in a none\n        streaming way, but will call this function several times with\n        increasing input size in a streaming scenario, so the dropout will\n        be applied several times.\n\n        Args:\n            offset (int or torch.tensor): start offset\n            size (int): required size of position encoding\n\n        Returns:\n            torch.Tensor: Corresponding encoding\n        \"\"\"\n        pos_emb = self.pe[\n            :,\n            self.pe.size(1) // 2 - size + 1: self.pe.size(1) // 2 + size,\n        ]\n        return pos_emb\n"}
{"type": "source_file", "path": "cosyvoice/transformer/attention.py", "content": "# Copyright (c) 2019 Shigeki Karita\n#               2020 Mobvoi Inc (Binbin Zhang)\n#               2022 Xingchen Song (sxc19@mails.tsinghua.edu.cn)\n#               2024 Alibaba Inc (Xiang Lyu)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Multi-Head Attention layer definition.\"\"\"\n\nimport math\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\n\nclass MultiHeadedAttention(nn.Module):\n    \"\"\"Multi-Head Attention layer.\n\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n\n    \"\"\"\n\n    def __init__(self,\n                 n_head: int,\n                 n_feat: int,\n                 dropout_rate: float,\n                 key_bias: bool = True):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super().__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        self.linear_q = nn.Linear(n_feat, n_feat)\n        self.linear_k = nn.Linear(n_feat, n_feat, bias=key_bias)\n        self.linear_v = nn.Linear(n_feat, n_feat)\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward_qkv(\n        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Transform query, key and value.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n\n        Returns:\n            torch.Tensor: Transformed query tensor, size\n                (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor, size\n                (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor, size\n                (#batch, n_head, time2, d_k).\n\n        \"\"\"\n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        return q, k, v\n\n    def forward_attention(\n        self,\n        value: torch.Tensor,\n        scores: torch.Tensor,\n        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool)\n    ) -> torch.Tensor:\n        \"\"\"Compute attention context vector.\n\n        Args:\n            value (torch.Tensor): Transformed value, size\n                (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score, size\n                (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask, size (#batch, 1, time2) or\n                (#batch, time1, time2), (0, 0, 0) means fake mask.\n\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n\n        \"\"\"\n        n_batch = value.size(0)\n        # NOTE(xcsong): When will `if mask.size(2) > 0` be True?\n        #   1. onnx(16/4) [WHY? Because we feed real cache & real mask for the\n        #           1st chunk to ease the onnx export.]\n        #   2. pytorch training\n        if mask.size(2) > 0:  # time2 > 0\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n            # For last chunk, time2 might be larger than scores.size(-1)\n            mask = mask[:, :, :, :scores.size(-1)]  # (batch, 1, *, time2)\n            scores = scores.masked_fill(mask, -float('inf'))\n            attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0)  # (batch, head, time1, time2)\n        # NOTE(xcsong): When will `if mask.size(2) > 0` be False?\n        #   1. onnx(16/-1, -1/-1, 16/0)\n        #   2. jit (16/-1, -1/-1, 16/0, 16/4)\n        else:\n            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(attn)\n        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n        x = (x.transpose(1, 2).contiguous().view(n_batch, -1,\n                                                 self.h * self.d_k)\n             )  # (batch, time1, d_model)\n\n        return self.linear_out(x)  # (batch, time1, d_model)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),\n        pos_emb: torch.Tensor = torch.empty(0),\n        cache: torch.Tensor = torch.zeros((0, 0, 0, 0))\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute scaled dot product attention.\n\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n                1.When applying cross attention between decoder and encoder,\n                the batch padding mask for input is in (#batch, 1, T) shape.\n                2.When applying self attention of encoder,\n                the mask is in (#batch, T, T)  shape.\n                3.When applying self attention of decoder,\n                the mask is in (#batch, L, L)  shape.\n                4.If the different position in decoder see different block\n                of the encoder, such as Mocha, the passed in mask could be\n                in (#batch, L, T) shape. But there is no such case in current\n                CosyVoice.\n            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),\n                where `cache_t == chunk_size * num_decoding_left_chunks`\n                and `head * d_k == size`\n\n\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)\n                where `cache_t == chunk_size * num_decoding_left_chunks`\n                and `head * d_k == size`\n\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n\n        # NOTE(xcsong):\n        #   when export onnx model, for 1st chunk, we feed\n        #       cache(1, head, 0, d_k * 2) (16/-1, -1/-1, 16/0 mode)\n        #       or cache(1, head, real_cache_t, d_k * 2) (16/4 mode).\n        #       In all modes, `if cache.size(0) > 0` will alwayse be `True`\n        #       and we will always do splitting and\n        #       concatnation(this will simplify onnx export). Note that\n        #       it's OK to concat & split zero-shaped tensors(see code below).\n        #   when export jit  model, for 1st chunk, we always feed\n        #       cache(0, 0, 0, 0) since jit supports dynamic if-branch.\n        # >>> a = torch.ones((1, 2, 0, 4))\n        # >>> b = torch.ones((1, 2, 3, 4))\n        # >>> c = torch.cat((a, b), dim=2)\n        # >>> torch.equal(b, c)        # True\n        # >>> d = torch.split(a, 2, dim=-1)\n        # >>> torch.equal(d[0], d[1])  # True\n        if cache.size(0) > 0:\n            key_cache, value_cache = torch.split(cache,\n                                                 cache.size(-1) // 2,\n                                                 dim=-1)\n            k = torch.cat([key_cache, k], dim=2)\n            v = torch.cat([value_cache, v], dim=2)\n        # NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it's\n        #   non-trivial to calculate `next_cache_start` here.\n        new_cache = torch.cat((k, v), dim=-1)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        return self.forward_attention(v, scores, mask), new_cache\n\n\nclass RelPositionMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"Multi-Head Attention layer with relative position encoding.\n    Paper: https://arxiv.org/abs/1901.02860\n    Args:\n        n_head (int): The number of heads.\n        n_feat (int): The number of features.\n        dropout_rate (float): Dropout rate.\n    \"\"\"\n\n    def __init__(self,\n                 n_head: int,\n                 n_feat: int,\n                 dropout_rate: float,\n                 key_bias: bool = True):\n        \"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"\n        super().__init__(n_head, n_feat, dropout_rate, key_bias)\n        # linear transformation for positional encoding\n        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)\n        # these two learnable bias are used in matrix c and matrix d\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))\n        torch.nn.init.xavier_uniform_(self.pos_bias_u)\n        torch.nn.init.xavier_uniform_(self.pos_bias_v)\n\n    def rel_shift(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute relative positional encoding.\n\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).\n            time1 means the length of query vector.\n\n        Returns:\n            torch.Tensor: Output tensor.\n\n        \"\"\"\n        zero_pad = torch.zeros((x.size()[0], x.size()[1], x.size()[2], 1),\n                               device=x.device,\n                               dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=-1)\n\n        x_padded = x_padded.view(x.size()[0],\n                                 x.size()[1],\n                                 x.size(3) + 1, x.size(2))\n        x = x_padded[:, :, 1:].view_as(x)[\n            :, :, :, : x.size(-1) // 2 + 1\n        ]  # only keep the positions from 0 to time2\n        return x\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),\n        pos_emb: torch.Tensor = torch.empty(0),\n        cache: torch.Tensor = torch.zeros((0, 0, 0, 0))\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2), (0, 0, 0) means fake mask.\n            pos_emb (torch.Tensor): Positional embedding tensor\n                (#batch, time2, size).\n            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),\n                where `cache_t == chunk_size * num_decoding_left_chunks`\n                and `head * d_k == size`\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)\n                where `cache_t == chunk_size * num_decoding_left_chunks`\n                and `head * d_k == size`\n        \"\"\"\n        q, k, v = self.forward_qkv(query, key, value)\n        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n\n        # NOTE(xcsong):\n        #   when export onnx model, for 1st chunk, we feed\n        #       cache(1, head, 0, d_k * 2) (16/-1, -1/-1, 16/0 mode)\n        #       or cache(1, head, real_cache_t, d_k * 2) (16/4 mode).\n        #       In all modes, `if cache.size(0) > 0` will alwayse be `True`\n        #       and we will always do splitting and\n        #       concatnation(this will simplify onnx export). Note that\n        #       it's OK to concat & split zero-shaped tensors(see code below).\n        #   when export jit  model, for 1st chunk, we always feed\n        #       cache(0, 0, 0, 0) since jit supports dynamic if-branch.\n        # >>> a = torch.ones((1, 2, 0, 4))\n        # >>> b = torch.ones((1, 2, 3, 4))\n        # >>> c = torch.cat((a, b), dim=2)\n        # >>> torch.equal(b, c)        # True\n        # >>> d = torch.split(a, 2, dim=-1)\n        # >>> torch.equal(d[0], d[1])  # True\n        if cache.size(0) > 0:\n            key_cache, value_cache = torch.split(cache,\n                                                 cache.size(-1) // 2,\n                                                 dim=-1)\n            k = torch.cat([key_cache, k], dim=2)\n            v = torch.cat([value_cache, v], dim=2)\n        # NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it's\n        #   non-trivial to calculate `next_cache_start` here.\n        new_cache = torch.cat((k, v), dim=-1)\n\n        n_batch_pos = pos_emb.size(0)\n        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n        p = p.transpose(1, 2)  # (batch, head, time1, d_k)\n\n        # (batch, head, time1, d_k)\n        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n        # (batch, head, time1, d_k)\n        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n\n        # compute attention score\n        # first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # (batch, head, time1, time2)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n\n        # compute matrix b and matrix d\n        # (batch, head, time1, time2)\n        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n        # NOTE(Xiang Lyu): Keep rel_shift since espnet rel_pos_emb is used\n        if matrix_ac.shape != matrix_bd.shape:\n            matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) / math.sqrt(\n            self.d_k)  # (batch, head, time1, time2)\n\n        return self.forward_attention(v, scores, mask), new_cache\n"}
