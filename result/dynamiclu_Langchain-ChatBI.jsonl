{"repo_info": {"repo_name": "Langchain-ChatBI", "repo_owner": "dynamiclu", "repo_url": "https://github.com/dynamiclu/Langchain-ChatBI"}}
{"type": "source_file", "path": "chains/chatbi_chain.py", "content": "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain.output_parsers.json import parse_json_markdown\nfrom langchain.chains import RetrievalQA, LLMChain\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom common.structured import StructuredOutputParser, ResponseSchema\nfrom common.log import logger\nfrom common.llm_output import out_json_data\nfrom configs.config import *\nfrom knowledge.source_service import SourceService\nfrom models.llm_chatglm import ChatGLM\nfrom models.llm_baichuan import LLMBaiChuan\nfrom models.llm_tongyi import LLMTongyi\nfrom query_data.query_execute import exe_query\nimport datetime\n\nline_template = '\\t\"{name}\": {type} '\n\ntime_today = datetime.date.today()\nclass ChatBiChain:\n    llm: object = None\n    service: object = None\n    memory: object = None\n    top_k: int = LLM_TOP_K\n    llm_model: str\n    his_query: str\n\n    def init_cfg(self,\n                 llm_model: str = LLM_MODEL_CHAT_GLM,\n                 embedding_model: str = EMBEDDING_MODEL_DEFAULT,\n                 llm_history_len=LLM_HISTORY_LEN,\n                 top_k=LLM_TOP_K\n                 ):\n        self.init_mode(llm_model, llm_history_len)\n        self.service = SourceService(embedding_model, LOCAL_EMBEDDING_DEVICE)\n        self.his_query = \"\"\n        self.top_k = top_k\n        logger.info(\"--\" * 30 + \"ChatBiChain init \" + \"--\" * 30)\n\n    def init_mode(self, llm_model: str = LLM_MODEL_CHAT_GLM, llm_history_len: str = LLM_HISTORY_LEN):\n        self.llm_model = llm_model\n        self.memory = ConversationBufferWindowMemory(k=llm_history_len)\n        if llm_model == LLM_MODEL_CHAT_GLM:\n            self.llm = ChatGLM()\n            self.llm.load_model(model_name_or_path=llm_model_dict[LLM_MODEL_CHAT_GLM],\n                                llm_device=LOCAL_LLM_DEVICE)\n            self.llm.history_len = llm_history_len\n        elif llm_model == LLM_MODEL_BAICHUAN:\n            self.llm = LLMBaiChuan()\n        elif llm_model == LLM_MODEL_QIANWEN:\n            self.llm = LLMTongyi()\n\n    def run_answer(self, query, vs_path, chat_history, top_k=VECTOR_SEARCH_TOP_K):\n        result_dict = {\"data\": \"sorry，the query is fail\"}\n        out_dict = self.get_intent_identify(query)\n        out_str = out_dict[\"info\"]\n        if out_dict[\"code\"] == 200 and \"回答:\" in out_str:\n            if \"意图:完整\" in out_str or \"意图: 完整\" in out_str:\n                query = out_str.split(\"回答:\")[1]\n                # chat_history = chat_history + [[None, query]]\n            else:\n                result_dict[\"data\"] = out_str.split(\"回答:\")[1]\n                return result_dict, chat_history\n        else:\n            result_dict[\"data\"] = out_str\n            return result_dict, chat_history\n        try:\n            resp = self.get_answer(query, vs_path, top_k)\n            res_dict = parse_json_markdown(resp[\"result\"])\n            out_json = out_json_data(res_dict)\n            result_dict[\"data\"] = str(exe_query(out_json))\n        except Exception as e:\n            logger.error(e)\n        return result_dict, chat_history\n\n    def get_intent_identify(self, query: str):\n        template = \"\"\" 你是智能数据分析助手，根据上下文和Human提问，识别对方数据分析意图('完整'、'缺失'、'闲聊')\n                        ## 背景知识\n                        完整：对方上下文信息中必须同时包含指标和时间范围，否则是缺失，例如：微博过去一个月的访问量，为完整\n                        缺失：对方上下文信息不完整，只有时间段或只有指标，例如：微博的访问量量或过去一个月的访问量，都为缺失\n                        闲聊：跟数据查询无关，如：你是谁\n\n                        ## 回答约束\n                        若数据分析意图为完整，要根据上下文信息总结成一句完整的语句，否则礼貌询问对方需要查询什么\n\n                        ## 输出格式\n                        意图:#，回答:#\n\n                        {history}\n                        Human: {human_input}\n                    \"\"\"\n        out_dict = {\"code\": 500}\n        prompt = PromptTemplate(\n            input_variables=[\"history\", \"human_input\"],\n            template=template\n        )\n        _chain = LLMChain(llm=self.llm, prompt=prompt, verbose=True, memory=self.memory)\n        try:\n            out_dict[\"info\"] = _chain.predict(human_input=query)\n            out_dict[\"code\"] = 200\n        except Exception as e:\n            print(e)\n            out_dict[\"info\"] = \"sorry，LLM model (%s) is fail，wait a minute...\" % self.llm_model\n        return out_dict\n\n    def get_answer(self, query: object, vs_path: str = VECTOR_STORE_PATH, top_k=VECTOR_SEARCH_TOP_K):\n        response_schemas = [\n            ResponseSchema(name=\"data_indicators\", description=\"数据指标: 如 PV、UV\"),\n            ResponseSchema(name=\"operator_type\", description=\"计算类型: 明细，求和，最大值，最小值，平均值\"),\n            ResponseSchema(name=\"time_type\", description=\"时间类型: 天、周、月、小时\"),\n            ResponseSchema(name=\"dimensions\", description=\"维度\"),\n            ResponseSchema(name=\"filters\", description=\"过滤条件\"),\n            ResponseSchema(name=\"filter_type\", description=\"过滤条件类型：大于，等于，小于，范围\"),\n            ResponseSchema(name=\"date_range\", description=\"日期范围,需按当前日期计算，假如当前日期为：2023-12-01，问 过去三个月或近几个月，则输出2023-09-01，2023-11-30；问过去一个月或上个月，则输出2023-11-01，2023-11-30；问八月或8月，则输出2023-08-01，2023-08-31；\"),\n            ResponseSchema(name=\"compare_type\", description=\"比较类型：无，同比，环比\")\n        ]\n        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n        format_instructions = output_parser.get_format_instructions(only_json=False)\n        prompt = ChatPromptTemplate(\n            messages=[\n                HumanMessagePromptTemplate.from_template(\n                    \"从问题中抽取准确的信息，若不匹配，返回空，\\n{format_instructions}，输出时，去掉备注 \\n 当前日期:%s  \\n 已知内容:{context}  \\n 问题：{question}  \" % time_today\n                )\n            ],\n            input_variables=[\"context\", \"question\"],\n            partial_variables={\"format_instructions\": format_instructions}\n        )\n        vector_store = self.service.load_vector_store(vs_path)\n        knowledge_chain = RetrievalQA.from_llm(\n            llm=self.llm,\n            retriever=vector_store.as_retriever(search_kwargs={\"k\": top_k}),\n            prompt=prompt\n        )\n        knowledge_chain.combine_documents_chain.document_prompt = PromptTemplate(\n            input_variables=[\"page_content\"], template=\"{page_content}\"\n        )\n        knowledge_chain.return_source_documents = True\n        result_dict = {}\n        try:\n            result_dict = knowledge_chain({\"query\": query})\n        except Exception as e:\n            logger.error(e)\n            result_dict[\"result\"] = \"sorry，LLM model (%s) is fail，wait a minute...\" % self.llm_model\n        return result_dict\n"}
{"type": "source_file", "path": "common/dict.py", "content": "import sys\nfrom common.log import *\nfrom configs.config import *\n\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nconfig_dict = {\n    \"operator_type\": APP_BOOT_PATH+\"/knowledge/data/operator_type.csv\",\n    \"dict_type\": APP_BOOT_PATH+\"/knowledge/data/dict_type.csv\"\n}\n\nFILE_OPERATOR_TYPE = \"operator_type\"\nFILE_DICT_TYPE = \"dict_type\"\n\ndict_operator_type = {}\ndict_type = {}\n\ndef init_dict(file_path: str, dict_name: str, key: int, val: int):\n    try:\n        with open(file_path, \"r\") as f:\n            for line in f:\n                line = line.strip().split(\",\")\n                if dict_name == FILE_OPERATOR_TYPE:\n                    dict_operator_type[str(line[key])] = str(line[val])\n                elif dict_name == FILE_DICT_TYPE:\n                    dict_type[str(line[key])] = str(line[val])\n    except FileNotFoundError:\n        logger.error(\" %s File not found !\" % file_path)\n    except Exception as e:\n        logger.error(\"Error:\", e)\n\n\nclass Dict:\n    def __init__(self) -> object:\n        logger.info(\"--\" * 10 + \"Dict init start \" + \"--\" * 10)\n        self.__init_dict__()\n        logger.info(\"--\" * 10 + \"Dict init end \" + \"--\" * 10)\n\n    @staticmethod\n    def __init_dict__():\n        init_dict(config_dict[FILE_OPERATOR_TYPE], FILE_OPERATOR_TYPE, 1, 0)\n        init_dict(config_dict[FILE_DICT_TYPE], FILE_DICT_TYPE, 1, 0)\n\n    @staticmethod\n    def __value__(dict_name: str, val: str):\n        if dict_name == FILE_OPERATOR_TYPE:\n            if val in dict_operator_type:\n                return dict_operator_type[val]\n        elif dict_name == FILE_DICT_TYPE:\n            if val in dict_type:\n                return dict_type[val]\n        return \"\"\n\n\nif __name__ == \"__main__\":\n    dict_obj = Dict()\n"}
{"type": "source_file", "path": "common/llm_output.py", "content": "from common.log import logger\nfrom common.dict import *\n\n\"\"\"\n  大模型输出结构化处理\n input: \n {\n        \"data_indicators\": \"pv\", \n        \"operator_type\": \"求和\", \n        \"time_type\": \"半年\", \n        \"dimension\": \"一汽红旗\", \n        \"filter\": \"一汽红旗\", \n        \"filter_type\": \"范围\", \n        \"date_range\": \"2023-07-01,2023-12-31\", \n        \"compare_type\": \"无\"\n }\n output:\n {\"data_indicators\": \"pv\", \"operator_type\": \"101\", \"time_type\": \"day\",\n     \"dimensions\": [{\"enName\": \"name\"}, {\"enName\": \"id\"}], \"filters\": [{\"enName\": \"name\", \"val\": \"一汽红旗\"}],\n     \"filter_type\": \"eq\", \"date_range\": \"2023-02-01,2023-02-25\", \"compare_type\": \"无\"}\n                        \n\"\"\"\nobj_dict = Dict()\n\ndef out_json_data(info):\n    out_json = {}\n    if \"data_indicators\" in info:\n        out_json[\"data_indicators\"] = obj_dict.__value__(FILE_DICT_TYPE, str(info[\"data_indicators\"]))\n    if \"operator_type\" in info:\n        out_json[\"operator_type\"] = obj_dict.__value__(FILE_OPERATOR_TYPE, str(info[\"operator_type\"]))\n    if \"time_type\" in info:\n        out_json[\"time_type\"] = obj_dict.__value__(FILE_DICT_TYPE, str(info[\"time_type\"]))\n    if \"dimension\" in info:\n        out_json[\"dimensions\"] = [{\"enName\": \"name\"}]\n    if \"filter\" in info:\n        out_json[\"filters\"] = [{\"enName\": \"name\", \"val\": info[\"filter\"]}]\n    if \"filter_type\" in info:\n        out_json[\"filter_type\"] = obj_dict.__value__(FILE_DICT_TYPE, str(info[\"filter_type\"]))\n    if \"date_range\" in info:\n        out_json[\"date_range\"] = info[\"date_range\"]\n    if \"compare_type\" in info:\n        out_json[\"compare_type\"] = info[\"compare_type\"]\n    return out_json\n\ndef dict_to_md(dictionary):\n    md = \"\"\n    formatted_data = json.dumps(dictionary, indent=4, ensure_ascii=False)\n    md += f\"```json\\n\"+formatted_data+\"\\n```\\n\"\n    return md\n\n"}
{"type": "source_file", "path": "configs/config.py", "content": "import torch.cuda\nimport torch.backends\n\nAPP_BOOT_PATH = \"/Users/PycharmProjects/Langchain-ChatBI\"\nMODEL_BOOT_PATH = \"/Users/PycharmProjects/Langchain-ChatBI/llm/models\"\n\n# 本地chatGLM模型配置\nVECTOR_SEARCH_TOP_K = 10\nLOCAL_EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nLOCAL_LLM_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nVECTOR_STORE_PATH = APP_BOOT_PATH + \"/vector_store\"\n# 多模型选择，向量模型选择\nembedding_model_dict = {\n    \"bge-large-zh\": MODEL_BOOT_PATH + \"/bge-large-zh-v1.5\",\n    \"text2vec\": MODEL_BOOT_PATH + \"/text2vec-large-chinese\",\n}\nLLM_TOP_K = 6\nLLM_HISTORY_LEN = 8\n\nllm_model_dict = {\n    \"chatglm2-6b-int4\": MODEL_BOOT_PATH + \"/chatglm2-6b-int4\",\n    \"Baichuan2-53B\": \"\",\n    \"qwen-turbo\": \"\",\n}\nEMBEDDING_MODEL_DEFAULT = \"bge-large-zh\"\n\nLLM_MODEL_CHAT_GLM = \"chatglm2-6b-int4\"\nLLM_MODEL_BAICHUAN = \"Baichuan2-53B\"\nLLM_MODEL_QIANWEN = \"qwen-turbo\"\n\n\"\"\"\n  百川公司大模型\n\"\"\"\nchat_model_baichuan_dict = {\n    \"BAICHUAN_API_KEY\": \"####\",\n    \"BAICHUAN_SECRET_KEY\": \"######\",\n    \"DEFAULT_API_BASE\": \"https://api.baichuan-ai.com/v1/chat/completions\"\n}\n\n\"\"\"\n 阿里通义千问大模型key\n\"\"\"\nDASHSCOPE_API_KEY = \"#########\"\n\n\nWEB_SERVER_NAME = \"127.0.0.1\"\nWEB_SERVER_PORT = 8080\n"}
{"type": "source_file", "path": "knowledge/source_service.py", "content": "import os\nfrom langchain.document_loaders import UnstructuredFileLoader\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders.csv_loader import CSVLoader\nfrom common.log import logger\nfrom configs.config import *\nimport sentence_transformers\nfrom typing import List\nimport datetime\n\n\"\"\"\n  知识库向量化服务\n\"\"\"\nclass SourceService:\n    def __init__(self,\n                 embedding_model: str = EMBEDDING_MODEL_DEFAULT,\n                 embedding_device=LOCAL_EMBEDDING_DEVICE):\n        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_dict[embedding_model], )\n        self.embeddings.client = sentence_transformers.SentenceTransformer(self.embeddings.model_name,\n                                                                           device=embedding_device)\n        self.vector_store = None\n        self.vector_store_path = VECTOR_STORE_PATH\n\n    def init_source_vector(self, docs_path):\n        \"\"\"\n        初始化本地知识库向量\n        :return:\n        \"\"\"\n        docs = []\n        for doc in os.listdir(docs_path):\n            if doc.endswith('.txt'):\n                logger.info(doc)\n                loader = UnstructuredFileLoader(f'{docs_path}/{doc}', mode=\"elements\")\n                doc = loader.load()\n                docs.extend(doc)\n        self.vector_store = FAISS.from_documents(docs, self.embeddings)\n        self.vector_store.save_local(self.vector_store_path)\n\n    def init_knowledge_vector_store(self,\n                                    filepath: str or List[str]):\n        if isinstance(filepath, str):\n            if not os.path.exists(filepath):\n                logger.error(\"路径不存在\")\n                return None\n            elif os.path.isfile(filepath):\n                file = os.path.split(filepath)[-1]\n                try:\n                    loader = UnstructuredFileLoader(filepath, mode=\"elements\")\n                    docs = loader.load()\n                    logger.info(f\"{file} 已成功加载\")\n                except Exception as e:\n                    logger.error(f\"{file} 未能成功加载\", e)\n                    return None\n            elif os.path.isdir(filepath):\n                docs = []\n                for file in os.listdir(filepath):\n                    fullfilepath = os.path.join(filepath, file)\n                    try:\n                        loader = UnstructuredFileLoader(fullfilepath, mode=\"elements\")\n                        docs += loader.load()\n                        logger.info(f\"{file} 已成功加载\")\n                    except Exception as e:\n                        logger.error(f\"{file} 未能成功加载\", e)\n        else:\n            docs = []\n            for file in filepath:\n                try:\n                    loader = UnstructuredFileLoader(file, mode=\"elements\")\n                    docs += loader.load()\n                    logger.info(f\"{file} 已成功加载\")\n                except Exception as e:\n                    logger.error(f\"{file} 未能成功加载\", e)\n\n        vector_store = FAISS.from_documents(docs, self.embeddings)\n        vs_path = f\"\"\"{VECTOR_STORE_PATH}/{os.path.splitext(file)[0]}_FAISS_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\"\"\"\n        vector_store.save_local(vs_path)\n        return vs_path if len(docs) > 0 else None\n\n    def add_document(self, document_path):\n        loader = UnstructuredFileLoader(document_path, mode=\"elements\")\n        doc = loader.load()\n        self.vector_store.add_documents(doc)\n        self.vector_store.save_local(self.vector_store_path)\n\n    def load_vector_store(self, path):\n        if path is None:\n            self.vector_store = FAISS.load_local(self.vector_store_path, self.embeddings)\n        else:\n            self.vector_store = FAISS.load_local(path, self.embeddings)\n        return self.vector_store\n\n    def add_csv(self, document_path):\n        loader = CSVLoader(file_path=document_path)\n        doc = loader.load()\n        logger.info(\"doc:\", doc)\n\n"}
{"type": "source_file", "path": "common/log.py", "content": "import os\nimport cv2\nimport json\nimport datetime\nimport base64\nimport logging\nimport numpy as np\nfrom logging.handlers import RotatingFileHandler\nfrom logging.handlers import TimedRotatingFileHandler\nfrom threading import Lock\n\nLOG_BASE_PATH = '../log/'\nnow = datetime.datetime.now()\nnowtime = now.strftime(\"%Y-%m-%d\")\nLOG_PATH = LOG_BASE_PATH + nowtime + \"/\"\ntry:\n    os.makedirs(LOG_PATH, exist_ok=True)\nexcept Exception as e:\n    pass\n\n\nclass LoggerProject(object):\n\n    def __init__(self, name):\n        self.mutex = Lock()\n        self.name = name\n        self.formatter = '%(asctime)s -<>- %(filename)s -<>- [line]:%(lineno)d -<>- %(levelname)s -<>- %(message)s'\n\n    def _create_logger(self):\n        _logger = logging.getLogger(self.name + __name__)\n        _logger.setLevel(level=logging.INFO)\n        return _logger\n\n    def _file_logger(self):\n        time_rotate_file = TimedRotatingFileHandler(filename=LOG_BASE_PATH + self.name, when='D', interval=1,\n                                                    backupCount=30)\n        time_rotate_file.setFormatter(logging.Formatter(self.formatter))\n        time_rotate_file.setLevel(logging.INFO)\n        return time_rotate_file\n\n    def _console_logger(self):\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(level=logging.INFO)\n        console_handler.setFormatter(logging.Formatter(self.formatter))\n        return console_handler\n\n    def pub_logger(self):\n        logger = self._create_logger()\n        self.mutex.acquire()\n        logger.addHandler(self._file_logger())\n        logger.addHandler(self._console_logger())\n        self.mutex.release()\n        return logger\n\n\nlog_api = LoggerProject('Langchain-ChatBI')\nlogger = log_api.pub_logger()\n"}
{"type": "source_file", "path": "common/structured.py", "content": "from __future__ import annotations\n\nfrom typing import Any, List\n\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom langchain_core.pydantic_v1 import BaseModel\n\nfrom langchain.output_parsers.format_instructions import (\n    STRUCTURED_FORMAT_INSTRUCTIONS,\n    STRUCTURED_FORMAT_SIMPLE_INSTRUCTIONS,\n)\nfrom langchain.output_parsers.json import parse_and_check_json_markdown\n\nline_template = '\\t\"{name}\": {type} '\n\n\nclass ResponseSchema(BaseModel):\n    \"\"\"A schema for a response from a structured output parser.\"\"\"\n\n    name: str\n    \"\"\"The name of the schema.\"\"\"\n    description: str\n    \"\"\"The description of the schema.\"\"\"\n    type: str = \"string\"\n    \"\"\"The type of the response.\"\"\"\n\n\ndef _get_sub_string(schema: ResponseSchema) -> str:\n    return line_template.format(\n        name=schema.name, description=schema.description, type=schema.type\n    )\n\n\nclass StructuredOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a structured output.\"\"\"\n\n    response_schemas: List[ResponseSchema]\n    \"\"\"The schemas for the response.\"\"\"\n\n    @classmethod\n    def from_response_schemas(\n        cls, response_schemas: List[ResponseSchema]\n    ) -> StructuredOutputParser:\n        return cls(response_schemas=response_schemas)\n\n    def get_format_instructions(self, only_json: bool = False) -> str:\n        \"\"\"Get format instructions for the output parser.\n\n        example:\n        ```python\n        from langchain.output_parsers.structured import (\n            StructuredOutputParser, ResponseSchema\n        )\n\n        response_schemas = [\n            ResponseSchema(\n                name=\"foo\",\n                description=\"a list of strings\",\n                type=\"List[string]\"\n                ),\n            ResponseSchema(\n                name=\"bar\",\n                description=\"a string\",\n                type=\"string\"\n                ),\n        ]\n\n        parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\n        print(parser.get_format_instructions())\n\n        output:\n        # The output should be a Markdown code snippet formatted in the following\n        # schema, including the leading and trailing \"```json\" and \"```\":\n        #\n        # ```json\n        # {\n        #     \"foo\": List[string]  // a list of strings\n        #     \"bar\": string  // a string\n        # }\n        # ```\n\n        Args:\n            only_json (bool): If True, only the json in the Markdown code snippet\n                will be returned, without the introducing text. Defaults to False.\n        \"\"\"\n        schema_str = \"\\n\".join(\n            [_get_sub_string(schema) for schema in self.response_schemas]\n        )\n        if only_json:\n            return STRUCTURED_FORMAT_SIMPLE_INSTRUCTIONS.format(format=schema_str)\n        else:\n            return STRUCTURED_FORMAT_INSTRUCTIONS.format(format=schema_str)\n\n    def parse(self, text: str) -> Any:\n        expected_keys = [rs.name for rs in self.response_schemas]\n        return parse_and_check_json_markdown(text, expected_keys)\n\n    @property\n    def _type(self) -> str:\n        return \"structured\"\n"}
{"type": "source_file", "path": "models/llm_tongyi.py", "content": "import os\nfrom langchain.chat_models.tongyi import ChatTongyi\nfrom configs.config import DASHSCOPE_API_KEY\n\n\nos.environ[\"DASHSCOPE_API_KEY\"] = DASHSCOPE_API_KEY\n\nclass LLMTongyi(ChatTongyi):\n    streaming = True\n\n    def __init__(self):\n        super().__init__()\n"}
{"type": "source_file", "path": "query_data/db.py", "content": "import sys\nimport os\nimport pymysql\n\ninitSQL = \"\"\"\n    use demo;\n    CREATE TABLE `query_route` (\n        `id` bigint NOT NULL AUTO_INCREMENT COMMENT '主键',\n        `indicators_code` varchar(100) NOT NULL COMMENT '指标编码',\n        `indicators_name` varchar(200) NOT NULL COMMENT '指标名称',\n        `dim_code_list` varchar(200) NOT NULL COMMENT '维度编码列表',\n        `dim_query_type` tinyint DEFAULT '0' COMMENT '维度匹配类型，0:任意组合、1:等匹配',\n        `indicators_operator_type` varchar(200) DEFAULT '101' COMMENT '指标支持操作类型，101:明细、102:求和、103:平均值、104:最大值、105：最小值',\n        `query_info` text NOT NULL COMMENT '查询信息',\n        `datasource_info` varchar(2048) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL COMMENT '数据源信息',\n        `datasource_type` tinyint DEFAULT '0' COMMENT '数据源类型，0:数据表、1:接口、2: 现成SQL',\n        PRIMARY KEY (`id`)\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC COMMENT='查询路由表';\n    \n    CREATE TABLE `website_data` (\n        `dt` varchar(100) NOT NULL COMMENT '日期',\n        `id` varchar(100) NOT NULL COMMENT '网站ID',\n        `name` varchar(100) NOT NULL COMMENT '网站名',\n        `pv` bigint NOT NULL COMMENT 'PV',\n        `uv` bigint NOT NULL COMMENT 'UV'\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC COMMENT='网站测试数据';\n    \n    INSERT INTO query_route(id, indicators_code, indicators_name, dim_code_list, dim_query_type, indicators_operator_type, query_info, datasource_info, datasource_type) VALUES(1, 'pv', '曝光量', 'name,id,dt', 0, '101', 'website_data', '{\"driverName\":\"com.mysql.cj.jdbc.Driver\",\"jdbcUrl\":\"jdbc:mysql://127.0.0.1:3306/demo?allowMultiQueries=true&useUnicode=true&characterEncoding=utf8&autoReconnect=true&zeroDateTimeBehavior=convertToNull&useSSL=false&serverTimezone=Asia/Shanghai\",\"username\":\"root\",\"password\":\"\"}', 0);\n    INSERT INTO query_route(id, indicators_code, indicators_name, dim_code_list, dim_query_type, indicators_operator_type, query_info, datasource_info, datasource_type) VALUES(2, 'uv', '用户数', 'name,id', 1, '101', 'website_data', '{\"driverName\":\"com.mysql.cj.jdbc.Driver\",\"jdbcUrl\":\"jdbc:mysql://127.0.0.1:3306/demo?allowMultiQueries=true&useUnicode=true&characterEncoding=utf8&autoReconnect=true&zeroDateTimeBehavior=convertToNull&useSSL=false&serverTimezone=Asia/Shanghai\",\"username\":\"root\",\"password\":\"\"}', 0);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-22', '11', '微博', 333333, 33333);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-22', '12', '京东', 105933, 45533);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-23', '12', '京东', 444444, 34444);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-22', '13', '淘宝', 333555, 55555);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-23', '13', '淘宝', 145555, 32355);\n    INSERT INTO website_data(dt, id, name, pv, uv) VALUES('2023-02-23', '11', '微博', 445333, 32333);\n\"\"\"\n\ndef selectMysql(sql):\n    conn = pymysql.connect(\n        host='127.0.0.1',\n        port=3306,\n        database='demo',\n        user='root',\n        password='####'\n    )\n    try:\n        cursor = conn.cursor()\n        cursor.execute(sql)\n\n        return cursor.fetchall()\n    finally:\n        cursor.close()\n        conn.close()\n\n"}
{"type": "source_file", "path": "models/llm_baichuan.py", "content": "import os\nfrom langchain.chat_models.baichuan import ChatBaichuan\n\nfrom configs.config import chat_model_baichuan_dict\n\nos.environ[\"DEFAULT_API_BASE\"] = chat_model_baichuan_dict[\"DEFAULT_API_BASE\"]\nos.environ[\"BAICHUAN_API_KEY\"] = chat_model_baichuan_dict[\"BAICHUAN_API_KEY\"]\nos.environ[\"BAICHUAN_SECRET_KEY\"] = chat_model_baichuan_dict[\"BAICHUAN_SECRET_KEY\"]\n\nclass LLMBaiChuan(ChatBaichuan):\n    model = \"Baichuan2-53B\"\n    \"\"\"model name of Baichuan, default is `Baichuan2-53B`.\"\"\"\n    temperature: float = 0.3\n    \"\"\"What sampling temperature to use.\"\"\"\n    top_k: int = 5\n    \"\"\"What search sampling control to use.\"\"\"\n    top_p: float = 0.85\n\n    def __init__(self):\n        super().__init__()\n\n"}
{"type": "source_file", "path": "models/llm_chatglm.py", "content": "from langchain.llms.base import LLM\nfrom typing import Optional, List\nfrom langchain.llms.utils import enforce_stop_tokens\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom configs.config import LOCAL_LLM_DEVICE\n\nDEVICE = LOCAL_LLM_DEVICE\nDEVICE_ID = \"0\" if torch.cuda.is_available() else None\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\n\ndef torch_gc():\n    if torch.cuda.is_available():\n        with torch.cuda.device(CUDA_DEVICE):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\nclass ChatGLM(LLM):\n    max_token: int = 10000\n    temperature: float = 0.01\n    top_p = 0.9\n    history = []\n    tokenizer: object = None\n    model: object = None\n    history_len: int = 10\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def _llm_type(self) -> str:\n        return \"ChatGLM\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        response, _ = self.model.chat(\n            self.tokenizer,\n            prompt,\n            history=self.history[-self.history_len:] if self.history_len > 0 else [],\n            max_length=self.max_token,\n            temperature=self.temperature,\n        )\n        torch_gc()\n        if stop is not None:\n            response = enforce_stop_tokens(response, stop)\n        self.history = self.history + [[None, response]]\n        return response\n\n    def load_model(self,\n                   model_name_or_path: str = \"THUDM/chatglm-6b\",\n                   llm_device=LOCAL_LLM_DEVICE):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_name_or_path,\n            trust_remote_code=True\n        )\n        if torch.cuda.is_available() and llm_device.lower().startswith(\"cuda\"):\n            self.model = (\n                AutoModel.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=True)\n                .half()\n                .cuda()\n            )\n        else:\n            self.model = (\n                AutoModel.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=True)\n                .float()\n                .to(llm_device)\n            )\n        self.model = self.model.eval()\n"}
{"type": "source_file", "path": "main_webui.py", "content": "from configs.config import *\nfrom chains.chatbi_chain import ChatBiChain\nfrom common.log import logger\nfrom common.llm_output import dict_to_md\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nimport gradio as gr\nimport argparse\nimport uvicorn\nimport os\nimport re\nimport shutil\n\nchain = ChatBiChain()\nembedding_model_dict_list = list(embedding_model_dict.keys())\n\nllm_model_dict_list = list(llm_model_dict.keys())\n\ndef get_file_list():\n    if not os.path.exists(\"knowledge/content\"):\n        return []\n    return [f for f in os.listdir(\"knowledge/content\")]\n\n\nfile_list = get_file_list()\n\n\ndef upload_file(file):\n    if not os.path.exists(\"knowledge/content\"):\n        os.mkdir(\"knowledge/content\")\n    filename = os.path.basename(file.name)\n    shutil.move(file.name, \"knowledge/content/\" + filename)\n    file_list.insert(0, filename)\n    return gr.Dropdown(choices=file_list, value=filename)\n\n\ndef reinit_model(llm_model, embedding_model, llm_history_len, top_k, history):\n    try:\n        chain.init_cfg(llm_model=llm_model,\n                       embedding_model=embedding_model,\n                       llm_history_len=llm_history_len,\n                       top_k=top_k)\n        model_msg = \"\"\"The LLM model has been successfully reloaded. Please select the file and click the \"Load File\" button to send the message again\"\"\"\n    except Exception as e:\n        logger.error(e)\n        model_msg = \"\"\"sorry，If the model does not reload successfully, click \"Load model\" button\"\"\"\n    return history + [[None, model_msg]]\n\n\ndef get_answer(query, vs_path, history, top_k):\n    if vs_path:\n        history = history + [[query, None]]\n        result_data, history = chain.run_answer(query=query, vs_path=vs_path, chat_history=history, top_k=top_k)\n        history = history + [[None, result_data[\"data\"]]]\n        return history, \"\"\n    else:\n        history = history + [[None, \"Please load the file before you ask questions.\"]]\n        return history, \"\"\n\n\ndef get_vector_store(filepath, history):\n    if chain.llm and chain.service:\n        vs_path = chain.service.init_knowledge_vector_store([\"knowledge/content/\" + filepath])\n        if vs_path:\n            file_status = \"The file has been successfully loaded. Please start asking questions\"\n        else:\n            file_status = \"The file did not load successfully, please upload the file again\"\n    else:\n        file_status = \"The model did not finished loading, please load the model before loading the file\"\n        vs_path = None\n    return vs_path, history + [[None, file_status]]\n\n\ndef init_model():\n    try:\n        chain.init_cfg()\n        return \"\"\"The model has been loaded successfully, please select the file and click the \"Load file\" button\"\"\"\n    except:\n        return \"\"\"The model did not load successfully, please click \"Load model\" button\"\"\"\n\n\nblock_css = \"\"\".importantButton {\n    background: linear-gradient(45deg, #7e05ff,#5d1c99, #6e00ff) !important;\n    border: none !important;\n}\n\n.importantButton:hover {\n    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;\n    border: none !important;\n}\n\n#chat_bi {\n    height: 100%;\n    min-height: 455px;\n}\n\"\"\"\n\nwebui_title = \"\"\"\n# Langchain-ChatBI Project\n\"\"\"\ninit_message = \"\"\"Welcome to the ChatBI, click 'Reload the model', if you choose the Embedding model, select or upload the corpus, and then click 'Load the File' \"\"\"\n\nmodel_status = init_model()\n\nwith gr.Blocks(css=block_css) as demo:\n    vs_path, file_status, model_status = gr.State(\"\"), gr.State(\"\"), gr.State(model_status)\n    gr.Markdown(webui_title)\n    with gr.Row():\n        with gr.Column(scale=1):\n            llm_model = gr.Radio(llm_model_dict_list,\n                                 label=\"LLM Model\",\n                                 value=LLM_MODEL_CHAT_GLM,\n                                 interactive=True)\n            llm_history_len = gr.Slider(0,\n                                        10,\n                                        value=5,\n                                        step=1,\n                                        label=\"LLM history len\",\n                                        interactive=True)\n            embedding_model = gr.Radio(embedding_model_dict_list,\n                                       label=\"Embedding Model\",\n                                       value=EMBEDDING_MODEL_DEFAULT,\n                                       interactive=True)\n            top_k = gr.Slider(1,\n                              20,\n                              value=6,\n                              step=1,\n                              label=\"top k\",\n                              interactive=True)\n            load_model_button = gr.Button(\"Reload Model\")\n\n            with gr.Tab(\"select\"):\n                selectFile = gr.Dropdown(file_list,\n                                         label=\"content file\",\n                                         interactive=True,\n                                         value=file_list[0] if len(file_list) > 0 else None)\n            with gr.Tab(\"upload\"):\n                file = gr.File(label=\"content file\",\n                               file_types=['.txt', '.md', '.docx', '.pdf']\n                               )  # .style(height=100)\n            load_file_button = gr.Button(\"Load File\")\n        with gr.Column(scale=2):\n            chatbot = gr.Chatbot(label=init_message, elem_id=\"chat_bi\", show_label=True)\n            query = gr.Textbox(show_label=True,\n                               placeholder=\"Please enter the questions and submit them according to the return\",\n                               label=\"Input Field\")\n            send = gr.Button(\" Submit\")\n    load_model_button.click(reinit_model,\n                            show_progress=True,\n                            inputs=[llm_model, embedding_model, llm_history_len, top_k, chatbot],\n                            outputs=chatbot\n                            )\n    # 将上传的文件保存到content文件夹下,并更新下拉框\n    file.upload(upload_file,\n                inputs=file,\n                outputs=selectFile)\n    load_file_button.click(get_vector_store,\n                           show_progress=True,\n                           inputs=[selectFile, chatbot],\n                           outputs=[vs_path, chatbot],\n                           )\n    query.submit(get_answer,\n                 show_progress=True,\n                 inputs=[query, vs_path, chatbot, top_k],\n                 outputs=[chatbot, query],\n                 )\n    # 发送按钮 提交\n    send.click(get_answer,\n               show_progress=True,\n               inputs=[query, vs_path, chatbot, top_k],\n               outputs=[chatbot, query],\n               )\n\napp = FastAPI()\napp = gr.mount_gradio_app(app, demo, path=\"/\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=WEB_SERVER_NAME)\n    parser.add_argument(\"--port\", type=int, default=WEB_SERVER_PORT)\n    parser.add_argument(\"--async\", type=int, default=0)\n    args = parser.parse_args()\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    uvicorn.run(app, host=args.host, port=args.port)\n"}
{"type": "source_file", "path": "query_data/query_route.py", "content": "from common.log import logger\nfrom query_data.db import selectMysql\n\nclass QueryRoute:\n\n    def __init__(self):\n        logger.info(\"--\" * 10 + \"queryRoute init \" + \"--\" * 10)\n\n    def verify_query(self, out_dict: dict):\n        if out_dict is None:\n            out_dict = {\"data_indicators\": \"pv\", \"operator_type\": \"detail\", \"time_type\": \"day\",\n                        \"dimensions\": [{\"enName\": \"name\"}, {\"enName\": \"id\"}], \"filters\": [{\"enName\": \"name\", \"val\": \"一汽\"}],\n                        \"filter_type\": \"eq\", \"date_range\": \"2024-01-01,2024-02-01\", \"compare_type\": \"无\"}\n        indicators_code = out_dict[\"data_indicators\"]\n        dim_code_list = out_dict[\"dimensions\"]\n        dim_code = \"\"\n        index = 1\n        if dim_code_list:\n            for line in dim_code_list:\n                if index == 1:\n                    dim_code = line[\"enName\"]\n                else:\n                    dim_code += \",\" + line[\"enName\"]\n                index = index + 1\n\n        SQL_like = \"\"\"\n            SELECT query_info,datasource_info,datasource_type\n            FROM query_route\n            WHERE indicators_code = '%s'\n            AND dim_code_list like '%s'\n            AND dim_query_type = 0\n        \"\"\" % (indicators_code, \"%\"+dim_code+\"%\")\n\n        SQL_eq = \"\"\"\n            SELECT query_info,datasource_info,datasource_type\n            FROM query_route\n            WHERE indicators_code = '%s'\n            AND dim_code_list =  '%s' \n            AND dim_query_type = 1\n        \"\"\" % (indicators_code, dim_code)\n        # print(\"SQL_like:\", SQL_like)\n        # print(\"SQL_eq:\", SQL_eq)\n        result_like = selectMysql(SQL_like)\n        result_eq = selectMysql(SQL_eq)\n        if (result_like and len(result_like) > 0) or (result_eq and len(result_eq) > 0):\n            if result_like:\n                return result_like[0][0], result_like[0][1], result_like[0][2]\n            elif result_eq:\n                return result_eq[0][0], result_like[0][1], result_like[0][2]\n        else:\n            return None\n\n\nif __name__ == \"__main__\":\n\n    qr = QueryRoute()\n    sql = qr.verify_query(None)\n    print(\"sql:\", sql)\n\n\n\n\n\n"}
{"type": "source_file", "path": "query_data/query_execute.py", "content": "from query_data.query_route import QueryRoute\nfrom query_data.db import selectMysql\nfrom common.log import logger\nimport json\nimport requests\n\nquery_route = QueryRoute()\n\ndef exe_query(out_dict):\n    result_data = []\n    if out_dict:\n        out_dict_result, datasource_info, datasource_type = query_route.verify_query(out_dict)\n        if out_dict_result:\n            if datasource_type == 0:\n                out_dict[\"table_name\"] = out_dict_result\n                sql_query = sql_assemble(out_dict)\n                list_data = selectMysql(sql_query)\n                for row in list_data:\n                    result = {\n                        \"name\": row[0],\n                        \"value\": int(row[1])\n                    }\n                    result_data.append(result)\n            elif datasource_type == 1:\n                out_dict[\"url\"] = out_dict_result\n                result_data = url_get_data(out_dict, datasource_info)\n\n    return result_data\n\ndef url_get_data(out_dict, datasource_info):\n    # req_params_map = {\n    #\n    # }\n    req_params_map = datasource_info\n    try:\n        json_data = json.dumps(req_params_map)\n        res = requests.post(\n            url=out_dict[\"url\"],\n            headers={\n                \"Content-Type\": \"application/json\",\n            },\n            data=json_data,\n            timeout=60\n        )\n        res_json = json.loads(res.text)\n        if res.status_code == 200:\n            return res_json[\"data\"], res.status_code\n        else:\n            return res_json[\"msg\"], res.status_code\n    except Exception as e:\n        logger.error(e)\n        return \"query  fail, wait a second! \", 500\n\ndef sql_assemble(out_dict: dict):\n    if out_dict is None:\n        out_dict = {'data_indicators': 'pv', 'operator_type': 'sum', 'time_type': 'quarter', 'dimensions': [{'enName': 'name'}], 'filters': [{'enName': 'name', 'val': '一汽大众'}], 'filter_type': '=', 'date_range': '2023-04-01,2023-06-30', 'compare_type': '无', 'table_name': 'brand_data'}\n    data_indicators = out_dict[\"data_indicators\"]\n    operator_type = out_dict[\"operator_type\"]\n    time_type = out_dict[\"time_type\"]\n    dimensions = out_dict[\"dimensions\"]\n    filters = out_dict[\"filters\"]\n    filter_type = out_dict[\"filter_type\"]\n    date_range = out_dict[\"date_range\"]\n    table_name = out_dict[\"table_name\"]\n    # compare_type = out_dict[\"compare_type\"]\n    group_by_sql, dim_sql, dim_group = \"\", \"\", \"\"\n    condition = \"1=1 \"\n    if dimensions:\n        for line in dimensions:\n            dim_sql = line[\"enName\"] + \" as name\"\n            dim_group = line[\"enName\"]\n    if filters:\n        for fi in filters:\n            key = fi[\"enName\"]\n            val = fi[\"val\"]\n            condition += \" and \" + filters_join(key, val, filter_type)\n    if date_range:\n        if \",\" in date_range:\n            begin_date = date_range.split(\",\")[0]\n            end_date = date_range.split(\",\")[1]\n            condition += time_type_format(begin_date, end_date, time_type)\n        else:\n            condition += time_type_format_eq(date_range, time_type)\n\n    operator_type_sql = \"\"\n    if operator_type:\n        if operator_type == \"sum\":\n            operator_type_sql = \"sum(%s) as value\" % data_indicators\n            group_by_sql = \"group by %s\" % dim_group\n        elif operator_type == \"avg\":\n            operator_type_sql = \"avg(%s) as value\" % data_indicators\n            group_by_sql = \"group by %s\" % dim_group\n        elif operator_type == \"max\":\n            operator_type_sql = \"max(%s) as value\" % data_indicators\n            group_by_sql = \"group by %s\" % dim_group\n        elif operator_type == \"min\":\n            operator_type_sql = \"min(%s) as value\" % data_indicators\n            group_by_sql = \"group by %s\" % dim_group\n        else:\n            operator_type_sql = data_indicators\n\n    SQL = \"\"\"\n            SELECT %s,%s\n            FROM %s\n            WHERE %s\n            %s\n            \"\"\" % (dim_sql, operator_type_sql, table_name, condition, group_by_sql)\n    return SQL\n\n\ndef filters_join(key: str, val: str, filter_type: str):\n    filter_sql = \"\"\n    if filter_type:\n        if filter_type == \"=\":\n            filter_sql = \" \" + key + \" like '%\"+val+\"%'\"\n            # filter_sql = \" %s = '%s' \" % (key, val)\n        if filter_type == \">\":\n            filter_sql = \" %s > '%s' \" % (key, val)\n        if filter_type == \">=\":\n            filter_sql = \" %s >= '%s' \" % (key, val)\n        if filter_type == \"in\":\n            filter_sql = \" %s in('%s')\" % (key, val)\n        if filter_type == \"like\":\n            filter_sql = \" %s like '%s'\" % (key, val)\n        if filter_type == \"<\":\n            filter_sql = \" %s < '%s'\" % (key, val)\n        if filter_type == \"<=\":\n            filter_sql = \" %s <= '%s'\" % (key, val)\n    return filter_sql\n\n\ndef time_type_format(begin_date: str, end_date: str, time_type: str):\n    condition = \"\"\n    if time_type:\n        if time_type == \"day\" or time_type == \"quarter\" or time_type == \"week\":\n            condition = \" and dt >= '%s' and  dt <= '%s' \" % (begin_date, end_date)\n        elif time_type == \"month\":\n            condition = \" and DATE_FORMAT(dt, '%Y-%m') >= DATE_FORMAT('\" + begin_date + \"', '%Y-%m')  and DATE_FORMAT(dt, '%Y-%m') <= DATE_FORMAT('\" + end_date + \"', '%Y-%m')  \"\n    return condition\n\n\ndef time_type_format_eq(date_range: str, time_type: str):\n    condition = \"\"\n    if time_type:\n        if time_type == \"day\":\n            condition = \"dt = '%s'  \"\"\" % date_range\n        elif time_type == \"month\":\n            condition = \" DATE_FORMAT(dt, '%Y-%m') =  DATE_FORMAT('\" + date_range + \"', '%Y-%m') \"\n    return condition\n\n\nif __name__ == \"__main__\":\n    sql = sql_assemble(None)\n    print(\"sql=\", sql)\n"}
