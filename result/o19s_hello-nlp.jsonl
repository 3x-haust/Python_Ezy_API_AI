{"repo_info": {"repo_name": "hello-nlp", "repo_owner": "o19s", "repo_url": "https://github.com/o19s/hello-nlp"}}
{"type": "test_file", "path": "tests/units/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/units/test_auth.py", "content": "import pytest\nfrom fastapi import HTTPException\nfrom fastapi.security.http import HTTPBasicCredentials\n\n\ndef test_basic_auth(environment):\n    # Import is inside because PROXY_USERNAME and PROXY_PASSWORD\n    # environment variables are checked on the import time.\n    # These variables are set by the `environment` fixture.\n    from hello_nlp.auth import basic_auth\n\n    result = basic_auth(HTTPBasicCredentials(username=\"cat\", password=\"tac\"))\n    assert result == \"cat\"\n\n\ndef test_basic_auth_fails_on_invalid_credentials(environment):\n    from hello_nlp.auth import basic_auth\n\n    with pytest.raises(HTTPException):\n        result = basic_auth(HTTPBasicCredentials(username=\"dog\", password=\"tac\"))\n        assert result.status_code == 401\n"}
{"type": "test_file", "path": "tests/units/test_main.py", "content": "import pytest\nfrom asynctest import patch\n\n\n@pytest.mark.parametrize(\"engine_name\", [\"elastic\",\"elasticsearch\",\"es\"])\nasync def test_explain_missing_documents(monkeypatch, engine_name):\n    monkeypatch.setenv(\"ENGINE_NAME\", engine_name)\n\n    from hello_nlp import main\n\n    with patch.object(\n        main.elastic_executor, \"passthrough\", return_value={\"test\": \"passed\"}\n    ) as search_mock:\n        result = await main.explain_missing_documents(\n            index_name=\"index-123\", _source=\"_id,title\", q=\"title:Berlin\", size=2\n        )\n    assert result == {\"test\": \"passed\"}\n    search_mock.assert_awaited_once_with(\n        \"index-123\", 0, 2, False, \"_id,title\", None, \"title:Berlin\"\n    )\n\n\nasync def test_explain():\n\n    from hello_nlp import main\n\n    with patch.object(\n        main.elastic_executor, \"explain\", return_value={\"test\": \"passed again!\"}\n    ) as explain_mock:\n        result = await main.explain(\n            index_name=\"index-123\", doc_id=\"123_321\", query={\"match\": \"all\"}\n        )\n    assert result == {\"test\": \"passed again!\"}\n    explain_mock.assert_awaited_once_with(\"index-123\", \"123_321\", {\"match\": \"all\"})\n"}
{"type": "test_file", "path": "tests/units/conftest.py", "content": "import pytest\n\n\n@pytest.fixture\ndef environment(monkeypatch):\n    monkeypatch.setenv(\"PROXY_USERNAME\", \"cat\")\n    monkeypatch.setenv(\"PROXY_PASSWORD\", \"tac\")\n    monkeypatch.setenv(\"NLP_HOST\", \"localhost\")\n    monkeypatch.setenv(\"NLP_PORT\", \"9200\")\n    monkeypatch.setenv(\"NLP_USE_SSL\", \"false\")\n"}
{"type": "source_file", "path": "hello_nlp/exceptions.py", "content": "class MissingEnvironmentVariable(Exception):\n    pass\n"}
{"type": "source_file", "path": "hello_nlp/__init__.py", "content": ""}
{"type": "source_file", "path": "hello_nlp/analyzers/html_strip.py", "content": "from .interfaces import Text_to_Text_PipelineInterface\n\nimport re\n\n# --------------------------------------\nfrom io import StringIO\nfrom html import unescape\nfrom html.parser import HTMLParser\n\n#Strips HTML tags and entities\nclass stdlib_strip(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.text = StringIO()\n    def handle_data(self, d):\n        self.text.write(d)\n    def get_data(self):\n        return self.text.getvalue()\n\ndef strip_html(html,strip):\n    html = unescape(html)\n    strip.feed(html)\n    text = strip.get_data()\n    text = text.strip()\n    text = re.sub(r'\\s+',' ',text)\n    return text\n\n# --------------------------------------\nfrom bs4 import BeautifulSoup\ndef strip_html_bs4(html):\n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text(separator=' ')\n    text = text.strip()\n    text = re.sub(r'\\s+',' ',text)\n    return text\n\n# --------------------------------------\ndef strip_html_lxml(html):\n    soup = BeautifulSoup(html, 'lxml')\n    text = soup.get_text(separator=' ')\n    text = text.strip()\n    text = re.sub(r'\\s+',' ',text)\n    return text\n\nclass HTML_Strip(Text_to_Text_PipelineInterface):\n    def analyze(self,text:str,context:dict=None) -> str:\n        if isinstance(text,list):\n            text = ' '.join(text)\n        if self.parser == \"lxml\":\n            text = strip_html_lxml(text)\n        elif self.parser == \"bs4\":\n            text = strip_html_bs4(text)\n        elif self.parser == \"html\":\n            text = strip_html(text,self.strip)\n        return text\n\n    def debug(self,text:str,context:dict=None) -> str:\n        return \"<em>[PARSER=\"+self.parser+\"]</em><br>\" + text\n\n    def __init__(self,metadata):\n        self.name=\"html_strip\"\n        self.pipeline = metadata\n        self.pipeline[self.name] = True\n        if \"html_parser\" not in self.pipeline.keys():\n            self.pipeline[\"html_parser\"]=\"lxml\"\n        self.parser = self.pipeline[\"html_parser\"]\n        if self.parser == \"html\":\n            self.strip = stdlib_strip()"}
{"type": "source_file", "path": "hello_nlp/analyzers/interfaces.py", "content": "# -*- coding: utf-8 -*-\n\n\"\"\"\nInterfaces.\n\nThese Interfaces are used to reliably join pipeline stages together\n  Each pipeline analyze method accepts one input param and returns one output param\n  Defining these classes allows us to declare what can come in and what can go out\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom spacy.tokens import Doc, Span\n\nclass Text_to_Text_PipelineInterface(ABC):\n    @abstractmethod\n    def analyze(self,text:str,context:dict=None)->str:\n        pass\n    @abstractmethod\n    def debug(self,text:str,context:dict=None)->str:\n        pass\n\n\nclass Text_to_Doc_PipelineInterface(ABC):\n    @abstractmethod\n    def analyze(self,text:str,context:dict=None)->Doc:\n        pass\n    @abstractmethod\n    def debug(self,doc:Doc,context:dict=None)->str:\n        pass\n\nclass Doc_to_Doc_PipelineInterface(ABC):\n    @abstractmethod\n    def analyze(self,doc:Doc,context:dict=None)->Doc:\n        pass\n    @abstractmethod\n    def debug(self,doc:Doc,context:dict=None)->str:\n        pass\n\n\nclass Doc_to_Text_PipelineInterface(ABC):\n    @abstractmethod\n    def analyze(self,doc:Doc,context:dict=None)->str:\n        pass\n    @abstractmethod\n    def debug(self,text:str,context:dict=None)->str:\n        pass\n"}
{"type": "source_file", "path": "hello_nlp/analyzers/tokenize.py", "content": "from .interfaces import Text_to_Doc_PipelineInterface\nfrom spacy.tokens import Doc\nfrom spacy import displacy\n\nclass Tokenizer(Text_to_Doc_PipelineInterface):\n\n\tdef tokenize(self,text:str)->Doc:\n\t\tdocs = None\n\t\tfor doc in self.pipeline[\"nlp\"].pipe([text]):\n\t\t\tdocs = doc\n\t\treturn docs\n\n\tdef analyze(self,text:str,context:dict=None)->Doc:\n\t\treturn self.tokenize(text)\n\n\tdef debug(self,doc:Doc,context:dict=None)->str:\n\t\tsvgs = []\n\t\tfor sent in doc.sents:\n\t\t\tsvgs.append(displacy.render(sent, style=\"dep\", jupyter=False))\n\t\treturn svgs\n\n\tdef __init__(self,metadata):\n\t\tself.name=\"tokenize\"\n\t\tself.pipeline = metadata\n\t\tself.pipeline[self.name] = True\n\n\t\tif \"nlp\" not in self.pipeline.keys():\n\t\t\traise ValueError('Yo! Tokenizer needs Spacy to be added to the pipeline config first!')\n"}
{"type": "source_file", "path": "hello_nlp/elastic/connection.py", "content": "# Copyright, 2020 AMBOSS GmbH\n# Adopted from https://github.com/amboss-mededu/quepid-es-proxy/\n# MIT License\n\nimport os\nfrom distutils.util import strtobool\n\nfrom elasticsearch import AsyncElasticsearch\n\nfrom hello_nlp.exceptions import MissingEnvironmentVariable\n\n_client = None\n\ntry:\n    es_host = os.environ[\"ENGINE_HOST\"]\n    es_port = os.environ[\"ENGINE_PORT\"]\n    es_use_ssl = os.environ[\"ENGINE_USE_SSL\"]\nexcept KeyError as err:\n    raise MissingEnvironmentVariable(f\"Environment variable {err} is not set.\")\n\n\nasync def get_connection() -> AsyncElasticsearch:\n    \"\"\"\n    Returns a connection to the Elasticsearch.\n    The connection is cached and reused. It is not\n    creating a new connection every time.\n    Required environment variables to create a connection:\n        - ENGINE_HOST\n        - ENGINE_PORT\n        - ENGINE_USE_SSL\n    \"\"\"\n    global _client\n    if _client:\n        return _client\n    client = AsyncElasticsearch(\n        hosts=[es_host],\n        port=int(es_port),\n        use_ssl=bool(strtobool(es_use_ssl)),\n        verify_certs=True,\n        http_compress=True,\n        max_retries=5,\n        retry_on_timeout=True,\n    )\n    _client = client\n    return client\n"}
{"type": "source_file", "path": "hello_nlp/analyzers/payload.py", "content": "# -*- coding: utf-8 -*-\n\nfrom .interfaces import Doc_to_Text_PipelineInterface\nfrom spacy.tokens import Doc\n\n\n\"\"\"\nPayloads are assigned to Open Class Words ONLY!\n\nhttps://universaldependencies.org/docs/u/pos/\n\nOpen class words \tClosed class words \tOther\n----------------\t------------------\t-----\nADJ \t\t\t\tADP \t\t\t\tPUNCT\nADV \t\t\t\tAUX \t\t\t\tSYM\nINTJ \t\t\t\tCONJ \t\t\t\tX\nNOUN \t\t\t\tDET \t \nPROPN \t\t\t\tNUM \t \nVERB \t\t\t\tPART \t \n  \t\t\t\t\tPRON \t \n  \t\t\t\t\tSCONJ \t \n\"\"\"\n\n_POS_SCORES_ = {\n\t'ADJ'  :  1.5, \t#adjective\n\t'ADV'  :  1.5, \t#adverb\n\t'INTJ' :  1.0, \t#interjection\n\t'NOUN' :  2.0, \t#noun\n\t'PROPN':  2.0, \t#proper noun\n\t'VERB' :  2.0   #verb\n}\n\n\"\"\"\nPart of Speech payloads are cool, but the real juice is from dependencies!\nSpecifically: subjects, objects, and roots.\n\nIntuition: the \"aboutness\" of sentences is more important than just the concepts mentioned.\n\"\"\"\n\n_DEP_SCORES_ = {\n\t'nsubjpass': 2.0,\n\t'nsubj'    : 2.0,\n\t'dobj'     : 2.0,\n\t'pobj'     : 1.5,\n\t'root'     : 2.0\n}\n\n\nclass Payloader(Doc_to_Text_PipelineInterface):\n\n\tdef analyze(self, doc:Doc, context:dict=None) -> str:\n\t\t\n\t\tsentences = []\n\n\t\tfor stream in doc.sents:\n\n\t\t\tpayloads = []\n\n\t\t\tfor tok in stream:\n\t\t\t\tscore = None\n\n\t\t\t\tif (len(tok.text)>0) and (self.delimiter not in tok.text):\n\n\t\t\t\t\tif (tok.is_alpha) and (len(tok.lemma_)>0) and (tok.pos_ in self.pos_scores):\n\t\t\t\t\t\tscore = self.pos_scores[tok.pos_]\n\n\t\t\t\t\t\tif tok.dep_ in self.dep_scores:\n\t\t\t\t\t\t\tscore += self.dep_scores[tok.dep_]\n\n\t\t\t\t\t\tvalue = str(score) # + 'f'\n\n\t\t\t\t\t\tpayloads.append(tok.lemma_ + self.delimiter + value + ' ')\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tpayloads.append(tok.text_with_ws)\n\n\t\t\t\telse:\n\t\t\t\t\t#We need to remove any other delimiter chars in the text\n\t\t\t\t\t#Otherwise it will be picked up by the PayloadDelimiterFilter\n\t\t\t\t\tpayloads.append(tok.text_with_ws.replace(self.delimiter,''))\n\n\n\t\t\ttext = ''.join([t for t in payloads if len(t)>0])\n\n\t\t\tsentences.append(text)\n\n\t\treturn sentences\n\n\tdef debug(self,text:str,context:dict=None) -> str:\n\t\treturn text\n\n\tdef __init__(self,metadata,delimiter='|',pos_scores=_POS_SCORES_,dep_scores=_DEP_SCORES_):\n\t\tself.name=\"payload\"\n\t\tself.pipeline = metadata\n\t\tself.pipeline[self.name] = True\n\n\t\tself.delimiter = delimiter\n\t\tself.dep_scores = dep_scores\n\t\tself.pos_scores = pos_scores\n\n"}
{"type": "source_file", "path": "hello_nlp/elastic/executor.py", "content": "# Copyright, 2020 AMBOSS GmbH, OpenSource Connections\n# Adopted from https://github.com/amboss-mededu/quepid-es-proxy/\n# MIT License\n\nfrom typing import List, Optional\n\nfrom .connection import get_connection\n\nasync def passthrough(index_name:str, body:dict):\n    conn = await get_connection()\n\n    query = {\"query\": body[\"query\"]} if \"query\" in body else None\n\n    payload = {\n        \"from_\": body[\"from\"],\n        \"size\": body[\"size\"],\n        \"explain\": body[\"explain\"],\n        \"_source\": body[\"_source\"]\n    }\n\n    body.pop(\"from\")\n    body.pop(\"size\")\n    body.pop(\"explain\")\n    body.pop(\"_source\")\n\n    payload[\"body\"] = body\n\n    r = conn.search(\n        index=index_name,\n        **payload,\n    )\n\n    return await r\n\nasync def explain(\n    index_name: str,\n    document_id: str,\n    query: str,\n):\n    conn = await get_connection()\n    return await conn.explain(index_name, document_id, query)\n\ndef index(index_name,document):\n    pass\n"}
{"type": "source_file", "path": "hello_nlp/analyzers/lemmatize.py", "content": "from .interfaces import Doc_to_Doc_PipelineInterface\nfrom spacy.tokens import Doc\n\n_POS_LEMMA_ = {\n\t'ADJ'  , #adjective\n\t'ADV'  , #adverb\n\t'INTJ' , #interjection\n\t'NOUN' , #noun\n\t'PROPN', #proper noun\n\t'VERB' , #verb\n}\n\nclass Lemmatizer(Doc_to_Doc_PipelineInterface):\n\tdef analyze(self,doc:Doc,context:dict=None):\n\n\t\tif not self.is_last_stage:\n\t\t\treturn doc\n\n\t\tsentences = []\n\t\tfor stream in doc.sents:\n\t\t\tlemmas = []\n\t\t\tfor tok in stream:\n\t\t\t\tif (len(tok.text)>0) and (tok.is_alpha) and (len(tok.lemma_)>0) and (tok.pos_ in self.pos_lemma):\n\t\t\t\t\tws = tok.text_with_ws.replace(tok.text,'')\n\t\t\t\t\tlemmas.append(tok.lemma_ + ws)\n\n\t\t\t\telse:\n\t\t\t\t\tlemmas.append(tok.text_with_ws)\n\t\t\ttext = ''.join([t for t in lemmas if len(t)>0])\n\n\t\t\tsentences.append(text)\n\n\t\treturn sentences\n\n\tdef debug(self,text:str,context:dict=None) -> str:\n\t\tif not self.is_last_stage:\n\t\t\treturn \"lemmatize = true\"\n\t\treturn text\n\n\tdef __init__(self,metadata,pos_lemma=_POS_LEMMA_):\n\t\tself.name=\"lemmatize\"\n\t\tself.pipeline = metadata\n\t\tself.pipeline[self.name] = True\n\n\t\tself.pos_lemma = pos_lemma\n\t\tself.is_last_stage = False\n\t\tstages = self.pipeline[\"stages\"]\n\t\tif \"lemmatize\" == stages[len(stages)-1]:\n\t\t\tself.is_last_stage = True\n"}
{"type": "source_file", "path": "hello_nlp/auth.py", "content": "import os\nimport secrets\n\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\n\nfrom .exceptions import MissingEnvironmentVariable\n\nsecurity = HTTPBasic()\n\ntry:\n    USER = os.environ[\"PROXY_USERNAME\"]\n    PSWD = os.environ[\"PROXY_PASSWORD\"]\nexcept KeyError as err:\n    raise MissingEnvironmentVariable(f\"{err} must be define in the environment.\")\n\n\ndef basic_auth(credentials: HTTPBasicCredentials = Depends(security)):\n    correct_username = secrets.compare_digest(credentials.username, USER)\n    correct_password = secrets.compare_digest(credentials.password, PSWD)\n    if not (correct_username and correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect email or password\",\n            headers={\"WWW-Authenticate\": \"Basic\"},\n        )\n    return credentials.username\n"}
{"type": "source_file", "path": "hello_nlp/solr/executor.py", "content": "import httpx\n\nasync def passthrough(uri):\n    async with httpx.AsyncClient() as client:\n        r = await client.get(uri)\n    return r"}
{"type": "source_file", "path": "hello_nlp/main.py", "content": "import os\nimport json\nimport urllib \nfrom typing import List, Optional\n\nfrom fastapi import Depends, FastAPI, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import RedirectResponse, JSONResponse\n\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\napp.mount(\"/ui/\", StaticFiles(directory=\"ui\"), name=\"ui\")\n\nif os.environ[\"CUDA\"] in ['true','cuda','True']:\n    os.environ[\"CUDA\"]=\"true\"\n    print(\"I will attempt to enable CUDA.\")\nelse:\n    os.environ[\"CUDA\"]=\"false\"\n    print(\"CUDA is disabled. To enable it set CUDA=true in your *.conf file.\")\n\nfrom .auth import basic_auth\nfrom .elastic import executor as elastic_executor\nfrom .solr import executor as solr_executor\n\nfrom .skipchunkconnect import Connect\nfrom .pipeline import Pipelines\n\nfrom .storage import saveDocument,indexableDocuments\n\npipeline_filename = os.environ[\"PIPELINE\"]\nwith open(pipeline_filename,'r') as fd:\n    config_json = json.load(fd)\n\npipelines = Pipelines(config_json)\n\nskipchunk = Connect(\n    os.environ[\"ENGINE_USE_SSL\"],\n    os.environ[\"ENGINE_HOST\"],\n    os.environ[\"ENGINE_PORT\"],\n    os.environ[\"APP_NAME\"],\n    os.environ[\"ENGINE_NAME\"],\n    os.environ[\"DOCUMENTS_PATH\"],\n    config_json[\"model\"],\n    config_json[\"skipchunk\"])\n\nif os.environ[\"ENGINE_NAME\"] in [\"solr\"]:\n    executor = solr_executor\nelif os.environ[\"ENGINE_NAME\"] in [\"elastic\",\"elasticsearch\",\"es\"]:\n    executor = elastic_executor\n\n# Replace \"*\" to the list of your origins, e.g.\n# origins = [\"quepid.yourcompany.com\", \"localhost:8080\"]\norigins = \"*\"\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get('/')\nasync def index():\n    \"\"\"Redirects to the ui index.html page\"\"\"\n    return RedirectResponse(\"/ui/index.html\")\n\n@app.get('/environment')\nasync def show_environment() -> dict:\n    \"\"\"Shows the env-file configuration loaded for the Hello-NLP instance\"\"\"\n    return {\n        \"ENGINE_USE_SSL\": os.environ[\"ENGINE_USE_SSL\"],\n        \"ENGINE_HOST\": os.environ[\"ENGINE_HOST\"],\n        \"ENGINE_PORT\": os.environ[\"ENGINE_PORT\"],\n        \"APP_NAME\": os.environ[\"APP_NAME\"],\n        \"ENGINE_NAME\": os.environ[\"ENGINE_NAME\"],\n        \"DOCUMENTS_PATH\": os.environ[\"DOCUMENTS_PATH\"],\n        \"WEB_CONCURRENCY\":os.environ[\"WEB_CONCURRENCY\"],\n        \"PROXY_USERNAME\":os.environ[\"PROXY_USERNAME\"]\n        #,\"PROXY_PASSWORD\":os.environ[\"PROXY_PASSWORD\"] <-- UNCOMMENT AT YOUR OWN RISK!\n    }\n\n@app.get('/pipeline')\nasync def show_pipeline() -> dict:\n    \"\"\"Shows the config.json pipeline loaded for the Hello-NLP instance\"\"\"\n    return config_json\n\n@app.get('/suggest/{index_name}')\nasync def suggest(\n    index_name: str, query: str\n) -> dict:\n    \"\"\"Provides autocomplete suggestions, given a query prefix and graph index name\"\"\"\n    gq = skipchunk.graph_connect(index_name)\n    suggestions = gq.suggestConcepts(query)\n    return {'suggestions':suggestions}\n\n@app.get('/indexes')\nasync def indexes() -> dict:\n    \"\"\"Gets the list of search graph indexes\"\"\"\n    indexes = skipchunk.gq.indexes()\n    return {'indexes':indexes}\n\n@app.get('/indexes/{index_name}')\nasync def index_summarize(index_name: str) -> dict:\n    \"\"\"Returns the top concepts and predicates for the given graph index\"\"\"\n    gq = skipchunk.graph_connect(index_name)\n    concepts,predicates = gq.summarize()\n    return {'concepts':concepts,'predicates':predicates}\n\n@app.get('/graph/{index_name}')\nasync def graph(index_name: str, subject: str) -> list:\n    \"\"\"Returnsthe graph walk for a subject term\"\"\"\n    gq = skipchunk.graph_connect(index_name)\n    objects = 5\n    branches = 10\n    tree = gq.graph(subject,objects=objects,branches=branches)\n    return tree\n\n@app.get('/analyzers')\nasync def analyzers() -> dict:\n    \"\"\"Returns a list of analyzers as provided in config.json\"\"\"\n    try:\n        data = list(pipelines.analyzers.keys())\n        res = {'data':data}\n    except ValueError as e:\n        print(e)\n        res = {'error':e}\n    return res\n\n@app.get('/analyze/{analyzer}')\nasync def analyze_text(analyzer: str, text: str, debug:bool=False) -> dict:\n    \"\"\"Analyzes the provided querystring text with the given analyzer\"\"\"\n    try:\n        data, data_debug = pipelines.analyze(analyzer,text,debug=debug)\n        res = {\"data\":str(data),\"debug\":data_debug}\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\nclass AnalyzeRequest(BaseModel):\n    text:str\n\n@app.post('/analyze/{analyzer}')\nasync def analyze_body(analyzer: str, body: AnalyzeRequest) -> dict:\n    \"\"\"Analyzes the provided body text with the given analyzer\"\"\"\n    try:\n        data, data_debug = pipelines.analyze(analyzer,body.text,debug=debug)\n        res = {\"data\":str(data),\"debug\":data_debug}\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\n\nclass IndexableDocument(BaseModel):\n    doc:Optional[dict]\n\nclass IndexableDocuments(BaseModel):\n    docs:Optional[list]\n\n\n@app.post('/enrich/{index_name}')\nasync def enrich_document(index_name:str, document: IndexableDocument) -> dict:\n    \"\"\"Enriches a document based on the field analysis as provided in config.json, saves the enriched document to disk, and returns the enriched document.\"\"\"\n    try:\n        doc = document.doc\n        enriched = pipelines.enrich(doc)\n        idfield = config_json[\"id\"]\n        docid = doc[idfield]\n        saveDocument(docid,enriched,os.environ[\"DOCUMENTS_PATH\"])\n        res = enriched\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\n@app.post('/index/{index_name}')\nasync def index_document(index_name:str, document: IndexableDocument) -> dict:\n    \"\"\"Enriches a document based on the field analysis as provided in config.json, saves the enriched document to disk, and indexes the document in the search engine.\"\"\"\n    try:\n        doc = document.doc\n        enriched = pipelines.enrich(doc)\n        idfield = config_json[\"id\"]\n        docid = doc[idfield]\n        iq = skipchunk.index_connect(index_name)\n        iq.indexDocument(enriched)\n        saveDocument(docid,enriched,iq.engine.document_data)\n        skipchunk.extract_one(index_name,doc)\n        res = enriched\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\n@app.post('/bulk/{index_name}')\nasync def bulk_index_documents(index_name:str, document: IndexableDocuments) -> dict:\n    \"\"\"Bulk enriches the provided documents based on the field analysis as provided in config.json, saves the enriched documents to disk, and indexes the documents in the search engine.\n       WARNING! Using the bulk operation will monopolize the service and block other calls.  It is recommended that bulk processing be done on a dedicated container.\n    \"\"\"\n    try:\n        iq = skipchunk.index_connect(index_name)\n        idfield = config_json[\"id\"]\n        docs = document.docs\n        enriched = []\n        for doc in docs:\n            docid = doc[idfield]\n            analyzed = pipelines.enrich(doc)\n            enriched.append(analyzed)\n            saveDocument(docid,analyzed,iq.engine.document_data)\n        iq.indexGenerator(enriched)\n        skipchunk.extract_batch(index_name,docs)\n        res = {\"success\":True,\"total\":len(enriched)}\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\n@app.post('/reindex/{index_name}')\nasync def reindex_all_documents(index_name:str) -> dict:\n    \"\"\"Sends all enriched documents on disk for indexing into the search engine.\n       NOTE: this does NOT re-enrich the documents!\n    \"\"\"\n    try:\n        iq = skipchunk.index_connect(index_name)\n        iq.indexGenerator(indexableDocuments(iq.engine.document_data))\n        res = {\"success\":True}\n    except ValueError as e:\n        print(e)\n        res = {\"error\":e}\n    return res\n\n## ====================\n## Search Proxy\n## ====================\n\n# Search the Solr core\n@app.get('/solr/{index_name}')\nasync def solr_query(index_name: str, request: Request):\n    \"\"\"Enriches a Solr query, and execute it against the search engine\"\"\"\n    params = pipelines.solr_query(request.query_params)\n    qs = '&'.join(params)    \n    uri = skipchunk.uri + index_name + '/select?' + qs\n    res = await executor.passthrough(uri)\n    return Response(content=res.text, media_type=\"application/json\")\n\n# Enrich a Solr query\n@app.get('/solr_enrich/{index_name}')\nasync def enrich_solr_query(index_name: str, request: Request) -> str:\n    \"\"\"Enriches a Solr query and returns it to the client without executing\"\"\"\n    params = pipelines.solr_query(request.query_params)\n    qs = '&'.join(params)    \n    uri = skipchunk.uri + index_name + '/select?' + qs\n    return uri\n\n\n# Search the Elastic core\n@app.post('/elastic/{index_name}')\nasync def elastic_query(index_name: str, request: Request) -> dict:\n    \"\"\"Enriches an Elastic QueryDSL request, and query the search engine\"\"\"\n    body = json.loads(await request.body())\n    enriched = pipelines.elastic_query(body)\n    return await executor.passthrough(index_name,enriched)\n\n\n# Enrich an Elastic query\n@app.post('/elastic_enrich/{index_name}')\nasync def enrich_elastic_query(index_name: str, request: Request) -> dict:\n    \"\"\"Enriches an Elastic QueryDSL request and returns it to the client without executing\"\"\"\n    body = json.loads(await request.body())\n    return pipelines.elastic_query(body)\n\n\n## =====================\n## Quepid/Splainer Proxy\n## =====================\n\n\n@app.get(\"/healthcheck\")\nasync def health_check():\n    \"\"\"Quepid health check call, just say 'yes' :)\"\"\"\n    return {\"status\": \"OK\"}\n\n@app.get(\"/explain/{index_name}\")\nasync def explain_missing_documents(\n    index_name: str,\n    _source: str,\n    q: str,\n    size: int,\n    username: str = Depends(basic_auth),\n) -> dict:\n    \"\"\"Quepid explain missing documents query for Solr\"\"\"\n    result = await executor.passthrough(\n        index_name,\n        0,\n        size,\n        False,\n        _source,\n        None,\n        q,\n    )\n    return result\n\n@app.post(\"/explain/{index_name}/_doc/{doc_id}/_explain\")\nasync def explain(\n    index_name: str,\n    doc_id: str,\n    query: dict,\n    username: str = Depends(basic_auth),\n) -> dict:\n    \"\"\"Quepid explain missing documents query for Elasticsearch\"\"\"\n    return await executor.explain(index_name, doc_id, query)\n"}
{"type": "source_file", "path": "hello_nlp/plugins.py", "content": "import imp\nimport os\n\nMainModule = \"__init__\"\n\ndef get_plugins(root):\n    plugins = []\n    candidates = os.listdir(root)\n    filename = MainModule + \".py\"\n    for i in candidates:\n        location = os.path.join(root, i)\n        if not os.path.isdir(location) or not filename in os.listdir(location):\n            continue\n        info = imp.find_module(MainModule, [location])\n        plugins.append({\"name\": i, \"info\": info})\n    return plugins\n\ndef load_plugin(plugin):\n    return imp.load_module(MainModule, *plugin[\"info\"])\n"}
{"type": "source_file", "path": "hello_nlp/query.py", "content": "import re\n\n#------------------------------------------\n\ndef queryparser(q_param,val):\n    #A simple and fast query parser\n    #format MUST be:\n    # {!hello_nlp f=[FIELDNAME] v=[TEXT|$q] func=[ANALYZER]}\n\n    template = \"\"\n    text = \"\"\n    analyzer = \"\"\n\n    m = re.search(r'(\\{\\!hello_nlp)([^\\}]+)(\\})', val)\n    if m and m.groups:\n        subquery = str(m.groups(0)[0])\n        props = str(m.groups(0)[1]).strip().split(' ')\n        endtag = str(m.groups(0)[2])\n\n        for prop in props:\n            param = prop.split('=')\n            k = param[0]\n            v = param[1]                        \n\n            if k=='f':\n                template = v\n            elif k =='v':\n                if v == '$q':\n                    v = q_param\n                text = v\n            elif k in ['analyzer','func']:\n                analyzer = v\n        \n    return template,analyzer,text"}
{"type": "source_file", "path": "hello_nlp/skipchunkconnect.py", "content": "from skipchunk.skipchunk import Skipchunk, textFromFields\nfrom skipchunk.graphquery import GraphQuery\nfrom skipchunk.indexquery import IndexQuery\nfrom skipchunk.enrichquery import EnrichQuery\n\ndef tuplize(data,fields=[],strip_html=True):\n    tuples = []\n    for post in data:\n        text = textFromFields(post,fields,strip_html=strip_html)\n        tuples.append((text,post))\n    return tuples\n\nclass Connect:\n\n    def __init__(self,use_ssl,host,port,index,engine_name,path,model,settings):\n\n        uri = \"\"\n\n        if use_ssl in [\"True\",\"true\",True,1,\"1\"]:\n            uri += 'https://'\n        else:\n            uri += 'http://'\n\n        uri += host + \":\" + str(port) + \"/\"\n\n        if engine_name in [\"solr\"]:\n            uri += \"solr/\"\n\n        self.config = {\n            \"host\":uri,\n            \"name\":index,\n            \"engine_name\":engine_name,\n            \"path\":path,\n            \"model\":model\n        }\n\n        self.uri = uri\n\n        self.fields = settings.pop(\"fields\")\n\n        self.sc = Skipchunk(self.config,spacy_model=model,**settings)\n        print(\"But no need to worry, Hello-NLP is saving your stuff.\")\n\n        self.eq = EnrichQuery(model=model)\n        self.iq = IndexQuery(self.config,enrich_query=self.eq)\n        self.gq = GraphQuery(self.config)\n\n        self.graph_connections = {}\n        self.index_connections = {}\n\n    def graph_connect(self,name):\n        if name not in self.graph_connections.keys():\n            graph_config = self.config.copy()\n            graph_config[\"name\"] = name\n            self.graph_connections[name] = GraphQuery(graph_config)\n        return self.graph_connections[name]\n\n    def index_connect(self,name):\n        if name not in self.index_connections.keys():\n            index_config = self.config.copy()\n            index_config[\"name\"] = name\n            self.index_connections[name] = IndexQuery(index_config,enrich_query=self.eq)\n        return self.index_connections[name]\n\n    def extract_one(self,name,doc):\n        tuples = tuplize([doc],fields=self.fields)\n        self.sc.enrich(tuples)\n        gq = self.graph_connect(name)\n        gq.index(self.sc)\n\n    def extract_batch(self,name,docs):\n        tuples = tuplize(docs,fields=self.fields)\n        self.sc.enrich(tuples)\n        gq = self.graph_connect(name)\n        gq.index(self.sc)\n"}
{"type": "source_file", "path": "hello_nlp/pipeline.py", "content": "import datetime\nimport spacy\nimport re\nimport urllib\n\nimport json\n\nfrom html import escape\n\nfrom .analyzers import html_strip, tokenize, lemmatize, payload\nfrom .plugins import load_plugin, get_plugins\n\nfrom .query import queryparser\n\n#from .elastic import parser as elastic_parser\n#from .solr import parser as solr_parser\n\n#------------------------------------------\n\ndef time():\n    return datetime.datetime.now().timestamp() * 1000\n\n#------------------------------------------\n\npipelines = {\n    \"html_strip\":html_strip.HTML_Strip,\n    \"tokenize\":tokenize.Tokenizer,\n    \"lemmatize\":lemmatize.Lemmatizer,\n    \"payload\":payload.Payloader\n}\n\ndef add_to_pipelines(plugin:dict):\n    pipelines[plugin[\"name\"]] = load_plugin(plugin).Plugin\n\n#------------------------------------------\n\nclass Analyzer:\n    def analyze(self, text: str, context:dict=None, debug:bool=False) -> str:\n        data = text\n        data_debug = []\n        total_time = 0\n        if debug:\n            data_debug.append({\"name\":\"(start)\",\"time\":0,\"debug\":escape(text)})\n        for stage in self.stages:\n            if debug:\n                startwatch = time()\n            data = stage.analyze(data, context=context)\n            if debug:\n                stopwatch = time() - startwatch\n                total_time += stopwatch\n                data_debug.append({\"name\":stage.name,\"time\":stopwatch,\"debug\":stage.debug(data)})\n        if debug:\n            data_debug.append({\"name\":\"(end)\",\"time\":total_time,\"debug\":data})\n        return data,data_debug\n\n    def __init__(self,analyzers,nlp):\n        self.stages = []\n        self.metadata = {\"nlp\":nlp,\"stages\":[]}\n        for stage in analyzers:\n            if stage in pipelines.keys():\n                self.metadata[\"stages\"].append(stage)\n                self.stages.append(pipelines[stage](self.metadata))\n        print(self.metadata)\n\n#------------------------------------------\n\nclass Field:\n    def analyze(self, text: str, context:dict=None, debug:bool=False) -> str:\n        data,data_debug = self.analyzer.analyze(text,context=context,debug=debug)\n        return data,data_debug \n\n    def __init__(self,field:dict,analyzer:Analyzer):\n        self.source = field[\"source\"]\n        self.target = field[\"target\"]\n        self.analyzer = analyzer\n\n#------------------------------------------\n\nclass Query:\n    def analyze(self,text: str, context:dict=None, debug:bool=False) -> str:\n        data,data_debug = self.analyzer.analyze(text,context=context,debug=debug)\n        return data,data_debug \n\n    def __init__(self,query:dict,analyzer:Analyzer):\n        self.source = query[\"source\"]\n        self.target = query[\"target\"]\n        self.analyzer = analyzer\n\n#------------------------------------------\n\nclass Pipelines:    \n\n    def analyze(self, analyzer:str, text:str, context:dict=None, debug:bool=False) -> str:\n        data,data_debug = self.analyzers[analyzer].analyze(text,context=context,debug=debug)\n        return data,data_debug\n\n    def enrich(self, document:dict, debug:bool=False) -> dict:\n        for f in self.fields.keys():\n            fields = self.fields[f]\n            for field in fields:\n                data,data_debug = field.analyze(document[f],context=document,debug=debug)\n                document[field.target] = data\n        return document\n\n    def query(self, key:str, data:str, debug:bool=False) -> dict:\n        if key in self.queries.keys():\n            enrichers = self.queries[key]\n            for enricher in enrichers:\n                data,data_debug = enricher.analyze(data,debug=debug)\n        if isinstance(data,list):\n            data = ' '.join(data)\n        return data\n\n    def solr_query(self,querystring):\n        params = []\n        q_param = \"\"\n        for t in querystring.items():\n            key = t[0]\n            val = t[1]\n            if key in self.queries.keys():\n                val = self.query(key,val)\n                if key == \"q\":\n                    q_param = val\n                else:\n                    params.append(key+'='+urllib.parse.quote(str(val)))\n\n            else:\n                resolved = val\n\n                #Detects and parses a {!hello_nlp ...} subquery\n                template,analyzer,text = queryparser(q_param,val)\n                print(template,analyzer,text)\n\n                #Expands the subquery to its resolved text\n                if (analyzer in self.analyzers.keys()) and len(text):\n                    analyzer = self.analyzers[analyzer]\n                    resolved,resolved_debug = analyzer.analyze(text,debug=False)\n                    \n                    if isinstance(resolved,dict):\n                        v = None\n                        if 'v' in resolved.keys() and (len(resolved[\"v\"])) :\n                            v = ' '.join(resolved[\"v\"])\n                            val = template.replace('$v',str(v))\n                            params.append(key+'='+urllib.parse.quote(str(val)))\n                        if 'q_param' in resolved.keys() and (len(resolved[\"q_param\"])) :\n                            q_param = resolved[\"q_param\"]\n\n                    else:\n                        if isinstance(resolved,list):\n                            resolved = ' '.join(resolved)\n                        resolved = str(resolved)\n\n                        if len(resolved):\n                            val = template.replace('$v',str(resolved))\n                            params.append(key+'='+urllib.parse.quote(str(val)))\n                            print(key,val)\n                else:\n                    params.append(key+'='+urllib.parse.quote(str(val)))\n\n        params.append('q='+urllib.parse.quote(q_param))\n\n        return params\n\n    def elastic_query(self,obj,root=None,enrich=None):\n\n        keywords = {\"query\",\"match\",\"match_phrase\",\"match_all\",\"should\",\"must\",\"should_not\",\"must_not\",\"filter\",\"bool\",\"term\",\"terms\",\"script_score\",\"params\",\"script\",\"rescore\",\"rescore_query\",\"function_score\"}\n\n        if not root:\n            root = obj\n\n        if isinstance(obj,dict):\n\n            for key in obj.keys():\n                #print(key)\n\n                if key == \"!hello_nlp\":\n                    name = obj[key][\"name\"]\n                    value = obj[key][\"value\"]\n                    analyzer = obj[key][\"analyzer\"]\n                    analyzed = self.analyzers[analyzer].analyze(value,context={\"root\":root,\"node\":obj},debug=False)\n                    return {name:analyzed[0]}\n\n                elif (key in self.queryfields):\n                    #print('fields')\n                    if isinstance(obj[key],str):\n                        #print('ANALYZE A')\n                        data,_ = self.query_analyzer.analyze(obj[key],debug=False)\n                        obj[key] = str(data)\n                    else:\n                        #print('RECURSE A')\n                        obj[key] = self.elastic_query(obj[key],root=root,enrich=key)\n\n                elif key in keywords:\n                    #print('KEYWORD')\n                    if isinstance(obj[key],str):\n                        #print(obj[key])\n                        if isinstance(enrich,str):\n                            #print('ANALYZE B')\n                            data,_ = self.query_analyzer.analyze(obj[key],context={\"root\":root,\"node\":obj},debug=False)\n                            obj[key] = str(data)\n                    else:\n                        #print('RECURSE B')\n                        obj[key] = self.elastic_query(obj[key],root=root,enrich=enrich)\n\n        elif isinstance(obj,list):\n            for i in range(len(obj)-1):\n                if isinstance(obj[i],str):\n                    #print(i,obj[i])\n                    if (enrich):\n                        #print('ANALZE C')\n                        data,_ = self.query_analyzer.analyze(obj[i],debug=False)\n                        obj[i] = str(data)\n                else:\n                    #print('RECURSE C')\n                    obj[i] = self.elastic_query(obj[i],root=root,enrich=enrich)\n\n        #print('DONE')\n        #print(json.dumps(obj,indent=2))\n        return obj\n\n    def add_analyzer(self, analyzer:dict):\n        self.analyzers[analyzer[\"name\"]] = Analyzer(analyzer[\"pipeline\"],self.nlp)\n\n    def add_field(self, field:dict):\n        analyzer = self.analyzers[field[\"analyzer\"]]\n        if field[\"source\"] not in self.fields.keys():\n            self.fields[field[\"source\"]] = []   \n        newfield = Field(field,analyzer)\n        self.fields[field[\"source\"]].append(newfield)\n        self.queryfields.add(newfield.source)\n        self.queryfields.add(newfield.target)\n\n    def add_query(self, query:dict):\n        analyzer = self.analyzers[query[\"analyzer\"]]\n        if query[\"source\"] not in self.queries.keys():\n            self.queries[query[\"source\"]] = []  \n        self.queries[query[\"source\"]].append(Query(query,analyzer))\n\n    def __init__(self,config):\n\n        if \"model\" not in config.keys():\n            raise ValueError(\"Yo! I need a model for spacy to load! Specify it as a model property in your config!\")\n\n        self.config = config\n\n        self.nlp = spacy.load(self.config[\"model\"])\n\n        self.plugins = {}\n        if \"plugin_path\" in config.keys():\n            for plugin in get_plugins(config[\"plugin_path\"]):\n                add_to_pipelines(plugin)\n\n        self.analyzers = {}\n        for analyzer in config[\"analyzers\"]:\n            self.add_analyzer(analyzer)\n\n        self.fields = {}\n        self.queryfields = set()\n        for field in config[\"fields\"]:\n            self.add_field(field)\n\n        self.queries = {}\n        for query in config[\"query\"]:\n            self.add_query(query)\n\n\n        if len(config[\"query\"])>0:\n            self.query_analyzer = self.analyzers[config[\"query\"][0][\"analyzer\"]]\n        else:\n            self.query_analyzer = self.analyzers[\"lemmatizer\"]\n\n"}
{"type": "source_file", "path": "plugins/entitize/__init__.py", "content": "from spacy.tokens import Doc\nfrom spacy import displacy,util\n\n## --------------------------------------\n## Adopted from https://spacy.io/usage/examples#entity-relations\n\ndef extract_people(doc):\n    # Merge entities and noun chunks into one token\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = util.filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n\n    #Get the folks\n    people = [str(person) for person in filter(lambda w: w.ent_type_ == \"PERSON\", doc)]\n    return people\n\n## --------------------------------------\n\n## The following are required:\n##   - This file must be named __init__.py\n##   - The folder name must match self.name\n##   - The class name must be 'Plugin'\n##   - The Plugin class must implement the 'analyze' method\n##   - The Plugin class must implement the 'debug' method\n\n##   - Please Enjoy and happy searching!!\n\nclass Plugin():\n\n    def analyze(self,doc:Doc,context:dict=None)->list:\n        sentences = []\n        people = extract_people(doc)\n        return people\n\n    def debug(self,doc:Doc,context:dict=None)->list:\n        #return list(doc)\n        return []\n        \n    def __init__(self,metadata):\n        self.name=\"entitize\"\n        self.pipeline = metadata\n        self.pipeline[self.name] = True"}
{"type": "source_file", "path": "plugins/vectorize/__init__.py", "content": "import os\nimport numpy as np\nfrom spacy.tokens import Doc\nfrom sentence_transformers import SentenceTransformer\n\n## --------------------------------------\n\n## The following are required:\n##   - This file must be named __init__.py\n##   - The folder name must match self.name\n##   - The class name must be 'Plugin'\n##   - The Plugin class must implement the 'analyze' method\n##   - The Plugin class must implement the 'debug' method\n\n##   - Please Enjoy and happy searching!!\n\nclass Plugin():\n\n    def analyze(self,doc:Doc,context:dict=None)->list:\n        #This example only analyzes the first sentence of the document\n        #Dense Vector fields in Elastic/Solr are not multivalued\n        #When you use this in real life, distill all sentences down to one vector\n        sentence = [span.text for span in doc.sents][0]\n        embeddings = self.model.encode([sentence])\n        vector = [embedding.tolist() for embedding in embeddings][0]\n        return vector\n\n    def debug(self,vector:list,context:dict=None)->list:\n        return vector\n        \n    def __init__(self,metadata):\n        self.name=\"vectorize\"\n        self.pipeline = metadata\n        self.pipeline[self.name] = True\n        self.index = None\n\n        self.cuda=os.environ[\"CUDA\"]\n        self.device='cpu'\n        if self.cuda=='true':\n            self.device='cuda'\n\n        #To use CUDA on your system/container, set CUDA=true in /hello-nlp/config.json\n        self.model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens',device=self.device)"}
{"type": "source_file", "path": "hello_nlp/storage.py", "content": "import os\nimport json\n\ndef saveDocument(docid,doc,path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n    filename = path + '/' + docid + '.json'\n    with open(filename,'w',encoding='utf-8') as fd:\n        json.dump(doc, fd)\n\ndef indexableDocuments(path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n    for f in os.listdir(path):\n        filename = os.path.join(path, f) \n        if os.path.isfile(filename) and '.json' in filename:\n            with open(filename,'r') as doc:\n                yield json.load(doc)"}
{"type": "source_file", "path": "plugins/quantize/__init__.py", "content": "import base64\nimport numpy as np\nfrom spacy.tokens import Doc\n\ndef quantize(data,scale):\n    data = data.astype(np.float32) / scale # normalize the data\n    norm = 255 * data # Now scale by 255\n    norm = norm.astype(np.int8) # Cast to signed integer\n    b64e = base64.b64encode(norm) # Encode it\n    return b64e.decode()\n\ndef extract_vectors(doc:Doc) -> list:\n    vecs = []\n    scale = -999999.00\n    for span in doc.sents:\n        if span.has_vector:\n            v = span.vector\n            m = max(v)\n            i = abs(min(v))\n            if m>scale:\n                scale = m\n            if i>scale:\n                scale = i\n            vecs.append(v)\n    quantized = [quantize(v,scale) for v in vecs]\n    return quantized\n\n## --------------------------------------\n\n## The following are required:\n##   - This file must be named __init__.py\n##   - The folder name must match self.name\n##   - The class name must be 'Plugin'\n##   - The Plugin class must implement the 'analyze' method\n##   - The Plugin class must implement the 'debug' method\n\n##   - Please Enjoy and happy searching!!\n\nclass Plugin():\n\n    def analyze(self,doc:Doc,context:dict=None)->list:\n        vectors = extract_vectors(doc)\n        return vectors\n\n    def debug(self,vectors:list,context:dict=None)->list:\n        return vectors\n        \n    def __init__(self,metadata):\n        self.name=\"quantize\"\n        self.pipeline = metadata\n        self.pipeline[self.name] = True\n        self.index = None\n"}
{"type": "source_file", "path": "plugins/prepositionize/__init__.py", "content": "from spacy.tokens import Doc\nfrom spacy import displacy,util\nimport json\n\n## --------------------------------------\n\ndef rewrite(text,vals):\n    for v in vals:\n        text = text.replace(v,'')\n    return text.strip()\n\ndef prepose(doc):\n    preps = [tok.text for tok in doc if tok.dep_=='prep']\n    pobjs = [tok.text for tok in doc if tok.dep_=='pobj']\n    return preps,pobjs\n\n## The following are required:\n##   - This file must be named __init__.py\n##   - The folder name must match self.name\n##   - The class name must be 'Plugin'\n##   - The Plugin class must implement the 'analyze' method\n##   - The Plugin class must implement the 'debug' method\n\n##   - Please Enjoy and happy searching!!\n\nclass Plugin():\n\n    def analyze(self,doc:Doc,context:dict=None)->dict:\n        preps,pobjs = prepose(doc)\n        text = doc.text\n        data = {\"q\":text}\n        if 'without' in preps:\n            data[\"q_param\"] = rewrite(text,preps+pobjs)\n            data[\"v\"] = pobjs\n        return data\n\n    def debug(self,data:dict,context:dict=None)->str:\n        return json.dumps(data)\n        \n    def __init__(self,metadata):\n        self.name=\"prepositionize\"\n        self.pipeline = metadata\n        self.pipeline[self.name] = True\n        self.rewriter = True"}
