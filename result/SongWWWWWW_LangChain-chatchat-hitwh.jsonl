{"repo_info": {"repo_name": "LangChain-chatchat-hitwh", "repo_owner": "SongWWWWWW", "repo_url": "https://github.com/SongWWWWWW/LangChain-chatchat-hitwh"}}
{"type": "test_file", "path": "test_all.py", "content": "import pytest\nfrom server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nclass TestFaissKBService:\n    kbService = FaissKBService(\"test\")\n    test_kb_name = \"test\"\n    test_file_name = \"README.md\"\n    testKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\n    search_content = \"如何启动api服务\"\n\n    def test_init(self):\n        create_tables()\n\n    def test_create_db(self):\n        assert self.kbService.create_kb()\n\n    def test_add_doc(self):\n        assert self.kbService.add_doc(self.testKnowledgeFile)\n\n    def test_search_db(self):\n        result = self.kbService.search_docs(self.search_content)\n        assert len(result) > 0\n\n    def test_delete_doc(self):\n        assert self.kbService.delete_doc(self.testKnowledgeFile)\n\n    def test_delete_db(self):\n        assert self.kbService.drop_kb()\n\nif __name__ == \"__main__\":\n    pytest.main()\n\n"}
{"type": "test_file", "path": "test_test_test.py", "content": "# import pytesseract\n# from pdf2image import convert_from_path\n\n# # 将PDF的每一页转换为图片\n# images = convert_from_path(\"transformer.pdf\")\n\n# # 在Linux上，通常不需要指定tesseract_cmd\n# # pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"  \n\n# # 遍历所有图片并使用Tesseract识别文本\n# for i, image in enumerate(images):\n#     text = pytesseract.image_to_string(image, lang='eng')  # 使用英文语言模型\n#     print(f\"Page {i+1}:\")\n#     print(text)\n\n\n\n\n\n# import pytesseract\n# from pdf2image import convert_from_path\n\n# def pdf_to_text_list(pdf_path):\n#     # 将PDF的每一页转换为图片\n#     images = convert_from_path(pdf_path, 500)  # 500是DPI设置，可以根据需要调整\n\n#     # 初始化Tesseract引擎\n#     #pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"  # \n#     # 遍历所有图片并使用Tesseract识别文本\n#     text_list = []\n#     for i, image in enumerate(images):\n#         text = pytesseract.image_to_string(image, lang='eng')  # 使用英文语言模型\n#         text_list.append(f\"Page {i+1}:\\n{text}\\n\")  # 添加页码和换行符\n\n#     return text_list\n\n# # 使用示例\n# pdf_path = \"transformer.pdf\"  # 替换为您的PDF文件路径\n# text_list = pdf_to_text_list(pdf_path)\n\n# # 输出结果\n# i = 1\n# for text in text_list:\n#     # print('-'*200)\n#     # print(str(i)*200)\n#     # print('\\n'+'\\n')\n#     print(text)\n#     #i+=1\n\n\n\n\n\nfrom langchain.text_splitter import CharacterTextSplitter, LatexTextSplitter\nimport re\nfrom typing import List\n# import PyPDF2\nimport pytesseract\nfrom pdf2image import convert_from_path\n\ndef extract_text_from_pdf(pdf_path):\n    texts = \"\"\n    # with open(pdf_path, 'rb') as file:\n    #     reader = PyPDF2.PdfReader(file)\n    images = convert_from_path(pdf_path)\n\n    # 遍历所有图片并使用Tesseract识别文本\n    for i, image in enumerate(images):\n        text = pytesseract.image_to_string(image, lang='eng')  # 使用英文语言模型\n        texts +=  text\n        texts += '\\n'        \n    return texts\nclass AliTextSplitter(CharacterTextSplitter):\n    def __init__(self, pdf: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.pdf = pdf\n\n        def split_text(self, text: str) -> List[str]:\n            if self.pdf:\n                # text = re.sub(r\"\\n{3,}\", r\"\\n\", text)\n                # text = re.sub('\\s', \" \", text)\n                # text = re.sub(\"\\n\\n\", \"\", text)\n                try:\n                    from modelscope.pipelines import pipeline\n                except ImportError:\n                        raise ImportError(\n                                \"Could not import modelscope python package. \"\n                \"Please install modelscope with `pip install modelscope`. \"\n                            )\n\n                p = pipeline(\n                    task=\"document-segmentation\",\n                    model='damo/nlp_bert_document-segmentation_chinese-base',\n                    device=\"gpu\")\n                result = p(documents=text)\n                sent_list = [i for i in result[\"text\"].split(\"\\n\\t\") if i]\n                return sent_list\npdf_path=\"transformer.pdf\"\npdf = extract_text_from_pdf(pdf_path)\n# print(pdf)\n# print('\\n'*10)\ntext_splitter = AliTextSplitter(pdf=True)\nresult = text_splitter.split_text(pdf)\n# latex_splitter = LatexTextSplitter(chunk_size=100,chunk_overlap=0)\n# docs = latex_splitter.create_documents([pdf])\nfor i in result:\n    print(\"---===---\"*50)\n    print(i)\n    "}
{"type": "test_file", "path": "tests/agent/test_agent_function.py", "content": "import sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import get_ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.agents import LLMSingleActionAgent, AgentExecutor\nfrom server.agent.tools import tools, tool_names\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=5)\nmodel = get_ChatOpenAI(\n        model_name=LLM_MODELS[0],\n        temperature=TEMPERATURE,\n    )\nfrom server.agent.custom_template import CustomOutputParser, prompt\n\noutput_parser = CustomOutputParser()\nllm_chain = LLMChain(llm=model, prompt=prompt)\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain,\n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"],\n    allowed_tools=tool_names\n)\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=True)\n\nimport pytest\n@pytest.mark.parametrize(\"text_prompt\",\n                         [\"北京市朝阳区未来24小时天气如何？\",  # 天气功能函数\n                          \"计算 (2 + 2312312)/4 是多少？\", # 计算功能函数\n                          \"翻译这句话成中文：Life is the art of drawing sufficient conclusions form insufficient premises.\"] # 翻译功能函数\n)\ndef test_different_agent_function(text_prompt):\n    try:\n        text_answer = agent_executor.run(text_prompt)\n        assert text_answer is not None\n    except Exception as e:\n        pytest.fail(f\"agent_function failed with {text_prompt}, error: {str(e)}\")\n"}
{"type": "test_file", "path": "tests/api/test_kb_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"wiki/Home.MD\": get_file_path(\"samples\", \"wiki/Home.md\"),\n    \"wiki/开发环境部署.MD\": get_file_path(\"samples\", \"wiki/开发环境部署.md\"),\n    \"test_files/test.txt\": get_file_path(\"samples\", \"test_files/test.txt\"),\n}\n\nprint(\"\\n\\n直接url访问\\n\")\n\n\ndef test_delete_kb_before(api=\"/knowledge_base/delete_knowledge_base\"):\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    url = api_base_url + api\n    print(\"\\n测试知识库存在，需要删除\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb(api=\"/knowledge_base/create_knowledge_base\"):\n    url = api_base_url + api\n\n    print(f\"\\n尝试用空名称创建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": \" \"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs(api=\"/knowledge_base/list_knowledge_bases\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb in data[\"data\"]\n\n\ndef test_upload_docs(api=\"/knowledge_base/upload_docs\"):\n    url = api_base_url + api\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": json.dumps(docs)}\n    files = [(\"files\", (name, open(path, \"rb\"))) for name, path in test_files.items()]\n    r = requests.post(url, data=data, files=files)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files(api=\"/knowledge_base/list_files\"):\n    url = api_base_url + api\n    print(\"\\n获取知识库中文件列表：\")\n    r = requests.get(url, params={\"knowledge_base_name\": kb})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list)\n    for name in test_files:\n        assert name in data[\"data\"]\n\n\ndef test_search_docs(api=\"/knowledge_base/search_docs\"):\n    url = api_base_url + api\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_info(api=\"/knowledge_base/update_info\"):\n    url = api_base_url + api\n    print(\"\\n更新知识库介绍\")\n    r = requests.post(url, json={\"knowledge_base_name\": \"samples\", \"kb_info\": \"你好\"})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n\ndef test_update_docs(api=\"/knowledge_base/update_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n更新知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs(api=\"/knowledge_base/delete_docs\"):\n    url = api_base_url + api\n\n    print(f\"\\n删除知识文件\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"file_names\": list(test_files)})\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs(api=\"/knowledge_base/recreate_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n重建知识库：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb}, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    url = api_base_url + \"/knowledge_base/search_docs\"\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    r = requests.post(url, json={\"knowledge_base_name\": kb, \"query\": query})\n    data = r.json()\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after(api=\"/knowledge_base/delete_knowledge_base\"):\n    url = api_base_url + api\n    print(\"\\n删除知识库\")\n    r = requests.post(url, json=kb)\n    data = r.json()\n    pprint(data)\n\n    # check kb not exists anymore\n    url = api_base_url + \"/knowledge_base/list_knowledge_bases\"\n    print(\"\\n获取知识库列表：\")\n    r = requests.get(url)\n    data = r.json()\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n"}
{"type": "test_file", "path": "tests/api/test_kb_api_request.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\nfrom configs import VECTOR_SEARCH_TOP_K\nfrom server.knowledge_base.utils import get_kb_path, get_file_path\nfrom webui_pages.utils import ApiRequest\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\napi: ApiRequest = ApiRequest(api_base_url)\n\n\nkb = \"kb_for_api_test\"\ntest_files = {\n    \"FAQ.MD\": str(root_path / \"docs\" / \"FAQ.MD\"),\n    \"README.MD\": str(root_path / \"README.MD\"),\n    \"test.txt\": get_file_path(\"samples\", \"test.txt\"),\n}\n\nprint(\"\\n\\nApiRquest调用\\n\")\n\n\ndef test_delete_kb_before():\n    if not Path(get_kb_path(kb)).exists():\n        return\n\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert isinstance(data[\"data\"], list) and len(data[\"data\"]) > 0\n    assert kb not in data[\"data\"]\n\n\ndef test_create_kb():\n    print(f\"\\n尝试用空名称创建知识库：\")\n    data = api.create_knowledge_base(\" \")\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == \"知识库名称不能为空，请重新填写知识库名称\"\n\n    print(f\"\\n创建新知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert data[\"msg\"] == f\"已新增知识库 {kb}\"\n\n    print(f\"\\n尝试创建同名知识库： {kb}\")\n    data = api.create_knowledge_base(kb)\n    pprint(data)\n    assert data[\"code\"] == 404\n    assert data[\"msg\"] == f\"已存在同名知识库 {kb}\"\n\n\ndef test_list_kbs():\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb in data\n\n\ndef test_upload_docs():\n    files = list(test_files.values())\n\n    print(f\"\\n上传知识文件\")\n    data = {\"knowledge_base_name\": kb, \"override\": True}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    print(f\"\\n尝试重新上传知识文件， 不覆盖\")\n    data = {\"knowledge_base_name\": kb, \"override\": False}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == len(test_files)\n\n    print(f\"\\n尝试重新上传知识文件， 覆盖，自定义docs\")\n    docs = {\"FAQ.MD\": [{\"page_content\": \"custom docs\", \"metadata\": {}}]}\n    data = {\"knowledge_base_name\": kb, \"override\": True, \"docs\": docs}\n    data = api.upload_kb_docs(files, **data)\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_list_files():\n    print(\"\\n获取知识库中文件列表：\")\n    data = api.list_kb_docs(knowledge_base_name=kb)\n    pprint(data)\n    assert isinstance(data, list)\n    for name in test_files:\n        assert name in data\n\n\ndef test_search_docs():\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_update_docs():\n    print(f\"\\n更新知识文件\")\n    data = api.update_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n\ndef test_delete_docs():\n    print(f\"\\n删除知识文件\")\n    data = api.delete_kb_docs(knowledge_base_name=kb, file_names=list(test_files))\n    pprint(data)\n    assert data[\"code\"] == 200\n    assert len(data[\"data\"][\"failed_files\"]) == 0\n\n    query = \"介绍一下langchain-chatchat项目\"\n    print(\"\\n尝试检索删除后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == 0\n\n\ndef test_recreate_vs():\n    print(\"\\n重建知识库：\")\n    r = api.recreate_vector_store(kb)\n    for data in r:\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n    query = \"本项目支持哪些文件格式?\"\n    print(\"\\n尝试检索重建后的检索知识库：\")\n    print(query)\n    data = api.search_kb_docs(query, kb)\n    pprint(data)\n    assert isinstance(data, list) and len(data) == VECTOR_SEARCH_TOP_K\n\n\ndef test_delete_kb_after():\n    print(\"\\n删除知识库\")\n    data = api.delete_knowledge_base(kb)\n    pprint(data)\n\n    # check kb not exists anymore\n    print(\"\\n获取知识库列表：\")\n    data = api.list_knowledge_bases()\n    pprint(data)\n    assert isinstance(data, list) and len(data) > 0\n    assert kb not in data\n"}
{"type": "test_file", "path": "tests/api/test_kb_summary_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom server.utils import api_address\n\napi_base_url = api_address()\n\nkb = \"samples\"\nfile_name = \"/media/gpt4-pdf-chatbot-langchain/langchain-ChatGLM/knowledge_base/samples/content/llm/大模型技术栈-实战与应用.md\"\ndoc_ids = [\n    \"357d580f-fdf7-495c-b58b-595a398284e8\",\n    \"c7338773-2e83-4671-b237-1ad20335b0f0\",\n    \"6da613d1-327d-466f-8c1a-b32e6f461f47\"\n]\n\n\ndef test_summary_file_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_file_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"file_name\": file_name\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data[\"msg\"])\n\n\ndef test_summary_doc_ids_to_vector_store(api=\"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\"):\n    url = api_base_url + api\n    print(\"\\n文件摘要：\")\n    r = requests.post(url, json={\"knowledge_base_name\": kb,\n                                 \"doc_ids\": doc_ids\n                                 }, stream=True)\n    for chunk in r.iter_content(None):\n        data = json.loads(chunk)\n        assert isinstance(data, dict)\n        assert data[\"code\"] == 200\n        print(data)\n"}
{"type": "test_file", "path": "tests/api/test_llm_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom configs.server_config import FSCHAT_MODEL_WORKERS\nfrom server.utils import api_address, get_model_worker_config\n\nfrom pprint import pprint\nimport random\nfrom typing import List\n\n\ndef get_configured_models() -> List[str]:\n    model_workers = list(FSCHAT_MODEL_WORKERS)\n    if \"default\" in model_workers:\n        model_workers.remove(\"default\")\n    return model_workers\n\n\napi_base_url = api_address()\n\n\ndef get_running_models(api=\"/llm_model/list_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    if r.status_code == 200:\n        return r.json()[\"data\"]\n    return []\n\n\ndef test_running_models(api=\"/llm_model/list_running_models\"):\n    url = api_base_url + api\n    r = requests.post(url)\n    assert r.status_code == 200\n    print(\"\\n获取当前正在运行的模型列表：\")\n    pprint(r.json())\n    assert isinstance(r.json()[\"data\"], list)\n    assert len(r.json()[\"data\"]) > 0\n\n\n# 不建议使用stop_model功能。按现在的实现，停止了就只能手动再启动\n# def test_stop_model(api=\"/llm_model/stop\"):\n#     url = api_base_url + api\n#     r = requests.post(url, json={\"\"})\n\n\ndef test_change_model(api=\"/llm_model/change_model\"):\n    url = api_base_url + api\n\n    running_models = get_running_models()\n    assert len(running_models) > 0\n\n    model_workers = get_configured_models()\n\n    availabel_new_models = list(set(model_workers) - set(running_models))\n    assert len(availabel_new_models) > 0\n    print(availabel_new_models)\n\n    local_models = [x for x in running_models if not get_model_worker_config(x).get(\"online_api\")]\n    model_name = random.choice(local_models)\n    new_model_name = random.choice(availabel_new_models)\n    print(f\"\\n尝试将模型从 {model_name} 切换到 {new_model_name}\")\n    r = requests.post(url, json={\"model_name\": model_name, \"new_model_name\": new_model_name})\n    assert r.status_code == 200\n\n    running_models = get_running_models()\n    assert new_model_name in running_models\n"}
{"type": "test_file", "path": "tests/api/test_server_state_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\n\nfrom webui_pages.utils import ApiRequest\n\nimport pytest\nfrom pprint import pprint\nfrom typing import List\n\n\napi = ApiRequest()\n\n\ndef test_get_default_llm():\n    llm = api.get_default_llm_model()\n    \n    print(llm)\n    assert isinstance(llm, tuple)\n    assert isinstance(llm[0], str) and isinstance(llm[1], bool)\n\n\ndef test_server_configs():\n    configs = api.get_server_configs()\n    pprint(configs, depth=2)\n\n    assert isinstance(configs, dict)\n    assert len(configs) > 0\n\n\ndef test_list_search_engines():\n    engines = api.list_search_engines()\n    pprint(engines)\n\n    assert isinstance(engines, list)\n    assert len(engines) > 0\n\n\n@pytest.mark.parametrize(\"type\", [\"llm_chat\", \"agent_chat\"])\ndef test_get_prompt_template(type):\n    print(f\"prompt template for: {type}\")\n    template = api.get_prompt_template(type=type)\n\n    print(template)\n    assert isinstance(template, str)\n    assert len(template) > 0\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"query\": \"请用100字左右的文字介绍自己\",\n    \"history\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你好\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"你好，我是人工智能大模型\"\n        }\n    ],\n    \"stream\": True,\n    \"temperature\": 0.7,\n}\n\n\ndef test_chat_fastchat(api=\"/chat/fastchat\"):\n    url = f\"{api_base_url}{api}\"\n    data2 = {\n        \"stream\": True,\n        \"messages\": data[\"history\"] + [{\"role\": \"user\", \"content\": \"推荐一部科幻电影\"}]\n    }\n    dump_input(data2, api)\n    response = requests.post(url, headers=headers, json=data2, stream=True)\n    dump_output(response, api)\n    assert response.status_code == 200\n\n\ndef test_chat_chat(api=\"/chat/chat\"):\n    url = f\"{api_base_url}{api}\"\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    dump_output(response, api)\n    assert response.status_code == 200\n\n\ndef test_knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    dump_input(data, api)\n    response = requests.post(url, headers=headers, json=data, stream=True)\n    print(\"\\n\")\n    print(\"=\" * 30 + api + \"  output\" + \"=\"*30)\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line)\n        if \"answer\" in data:\n            print(data[\"answer\"], end=\"\", flush=True)\n    pprint(data)\n    assert \"docs\" in data and len(data[\"docs\"]) > 0\n    assert response.status_code == 200\n\n\ndef test_search_engine_chat(api=\"/chat/search_engine_chat\"):\n    global data\n\n    data[\"query\"] = \"室温超导最新进展是什么样？\"\n\n    url = f\"{api_base_url}{api}\"\n    for se in [\"bing\", \"duckduckgo\"]:\n        data[\"search_engine_name\"] = se\n        dump_input(data, api + f\" by {se}\")\n        response = requests.post(url, json=data, stream=True)\n        if se == \"bing\" and not BING_SUBSCRIPTION_KEY:\n            data = response.json()\n            assert data[\"code\"] == 404\n            assert data[\"msg\"] == f\"要使用Bing搜索引擎，需要设置 `BING_SUBSCRIPTION_KEY`\"\n\n        print(\"\\n\")\n        print(\"=\" * 30 + api + f\" by {se}  output\" + \"=\"*30)\n        for line in response.iter_content(None, decode_unicode=True):\n            data = json.loads(line)\n            if \"answer\" in data:\n                print(data[\"answer\"], end=\"\", flush=True)\n        assert \"docs\" in data and len(data[\"docs\"]) > 0\n        pprint(data[\"docs\"])\n        assert response.status_code == 200\n\n"}
{"type": "test_file", "path": "tests/api/test_stream_chat_api_thread.py", "content": "import requests\nimport json\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom configs import BING_SUBSCRIPTION_KEY\nfrom server.utils import api_address\n\nfrom pprint import pprint\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n\n\napi_base_url = api_address()\n\n\ndef dump_input(d, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  input \" + \"=\"*30)\n    pprint(d)\n\n\ndef dump_output(r, title):\n    print(\"\\n\")\n    print(\"=\" * 30 + title + \"  output\" + \"=\"*30)\n    for line in r.iter_content(None, decode_unicode=True):\n        print(line, end=\"\", flush=True)\n\n\nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n}\n\n\ndef knowledge_chat(api=\"/chat/knowledge_base_chat\"):\n    url = f\"{api_base_url}{api}\"\n    data = {\n        \"query\": \"如何提问以获得高质量答案\",\n        \"knowledge_base_name\": \"samples\",\n        \"history\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"你好\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"你好，我是 ChatGLM\"\n            }\n        ],\n        \"stream\": True\n    }\n    result = []\n    response = requests.post(url, headers=headers, json=data, stream=True)\n\n    for line in response.iter_content(None, decode_unicode=True):\n        data = json.loads(line)\n        result.append(data)\n    \n    return result\n\n\ndef test_thread():\n    threads = []\n    times = []\n    pool = ThreadPoolExecutor()\n    start = time.time()\n    for i in range(10):\n        t = pool.submit(knowledge_chat)\n        threads.append(t)\n    \n    for r in as_completed(threads):\n        end = time.time()\n        times.append(end - start)\n        print(\"\\nResult:\\n\")\n        pprint(r.result())\n\n    print(\"\\nTime used:\\n\")\n    for x in times:\n        print(f\"{x}\")\n"}
{"type": "test_file", "path": "tests/custom_splitter/test_different_splitter.py", "content": "import os\n\nfrom transformers import AutoTokenizer\nimport sys\n\nsys.path.append(\"../..\")\nfrom configs import (\n    CHUNK_SIZE,\n    OVERLAP_SIZE\n)\n\nfrom server.knowledge_base.utils import make_text_splitter\n\ndef text(splitter_name):\n    from langchain import document_loaders\n\n    # 使用DocumentLoader读取文件\n    filepath = \"../../knowledge_base/samples/content/test.txt\"\n    loader = document_loaders.UnstructuredFileLoader(filepath, autodetect_encoding=True)\n    docs = loader.load()\n    text_splitter = make_text_splitter(splitter_name, CHUNK_SIZE, OVERLAP_SIZE)\n    if splitter_name == \"MarkdownHeaderTextSplitter\":\n        docs = text_splitter.split_text(docs[0].page_content)\n        for doc in docs:\n            if doc.metadata:\n                doc.metadata[\"source\"] = os.path.basename(filepath)\n    else:\n        docs = text_splitter.split_documents(docs)\n    for doc in docs:\n        print(doc)\n    return docs\n\n\n\n\nimport pytest\nfrom langchain.docstore.document import Document\n\n@pytest.mark.parametrize(\"splitter_name\",\n                         [\n                             \"ChineseRecursiveTextSplitter\",\n                             \"SpacyTextSplitter\",\n                             \"RecursiveCharacterTextSplitter\",\n                             \"MarkdownHeaderTextSplitter\"\n                         ])\ndef test_different_splitter(splitter_name):\n    try:\n        docs = text(splitter_name)\n        assert isinstance(docs, list)\n        if len(docs)>0:\n            assert isinstance(docs[0], Document)\n    except Exception as e:\n        pytest.fail(f\"test_different_splitter failed with {splitter_name}, error: {str(e)}\")\n"}
{"type": "test_file", "path": "tests/document_loader/test_imgloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.jpg\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.jpg\"),\n}\n\ndef test_rapidocrloader():\n    img_path = test_files[\"ocr_test.jpg\"]\n    from document_loaders import RapidOCRLoader\n\n    loader = RapidOCRLoader(img_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "test_file", "path": "tests/kb_vector_db/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/kb_vector_db/test_faiss_kb.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\n\nkbService = FaissKBService(\"test\")\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\n\n\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n\ndef test_delete_db():\n    assert kbService.drop_kb()\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_pg_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = PGKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/kb_vector_db/test_milvus_db.py", "content": "from server.knowledge_base.kb_service.faiss_kb_service import FaissKBService\nfrom server.knowledge_base.kb_service.milvus_kb_service import MilvusKBService\nfrom server.knowledge_base.kb_service.pg_kb_service import PGKBService\nfrom server.knowledge_base.migrate import create_tables\nfrom server.knowledge_base.utils import KnowledgeFile\n\nkbService = MilvusKBService(\"test\")\n\ntest_kb_name = \"test\"\ntest_file_name = \"README.md\"\ntestKnowledgeFile = KnowledgeFile(test_file_name, test_kb_name)\nsearch_content = \"如何启动api服务\"\n\ndef test_init():\n    create_tables()\n\n\ndef test_create_db():\n    assert kbService.create_kb()\n\n\ndef test_add_doc():\n    assert kbService.add_doc(testKnowledgeFile)\n\n\ndef test_search_db():\n    result = kbService.search_docs(search_content)\n    assert len(result) > 0\ndef test_delete_doc():\n    assert kbService.delete_doc(testKnowledgeFile)\n\n"}
{"type": "test_file", "path": "tests/test_migrate.py", "content": "from pathlib import Path\nfrom pprint import pprint\nimport os\nimport shutil\nimport sys\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom server.knowledge_base.kb_service.base import KBServiceFactory\nfrom server.knowledge_base.utils import get_kb_path, get_doc_path, KnowledgeFile\nfrom server.knowledge_base.migrate import folder2db, prune_db_docs, prune_folder_files\n\n\n# setup test knowledge base\nkb_name = \"test_kb_for_migrate\"\ntest_files = {\n    \"faq.md\": str(root_path / \"docs\" / \"faq.md\"),\n    \"install.md\": str(root_path / \"docs\" / \"install.md\"),\n}\n\n\nkb_path = get_kb_path(kb_name)\ndoc_path = get_doc_path(kb_name)\n\nif not os.path.isdir(doc_path):\n    os.makedirs(doc_path)\n\nfor k, v in test_files.items():\n    shutil.copy(v, os.path.join(doc_path, k))\n\n\ndef test_recreate_vs():\n    folder2db([kb_name], \"recreate_vs\")\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb and kb.exists()\n\n    files = kb.list_files()\n    print(files)\n    for name in test_files:\n        assert name in files\n        path = os.path.join(doc_path, name)\n\n        # list docs based on file name\n        docs = kb.list_docs(file_name=name)\n        assert len(docs) > 0\n        pprint(docs[0])\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n        # list docs base on metadata\n        docs = kb.list_docs(metadata={\"source\": name})\n        assert len(docs) > 0\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == name\n\n\ndef test_increament():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.clear_vs()\n    assert kb.list_files() == []\n    assert kb.list_docs() == []\n\n    folder2db([kb_name], \"increament\")\n\n    files = kb.list_files()\n    print(files)\n    for f in test_files:\n        assert f in files\n\n        docs = kb.list_docs(file_name=f)\n        assert len(docs) > 0\n        pprint(docs[0])\n\n        for doc in docs:\n            assert doc.metadata[\"source\"] == f\n\n\ndef test_prune_db():\n    del_file, keep_file = list(test_files)[:2]\n    os.remove(os.path.join(doc_path, del_file))\n\n    prune_db_docs([kb_name])\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n    pprint(docs[0])\n\n    shutil.copy(test_files[del_file], os.path.join(doc_path, del_file))\n\n\ndef test_prune_folder():\n    del_file, keep_file = list(test_files)[:2]\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n\n    # delete docs for file\n    kb.delete_doc(KnowledgeFile(del_file, kb_name))\n    files = kb.list_files()\n    print(files)\n    assert del_file not in files\n    assert keep_file in files\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    docs = kb.list_docs(file_name=keep_file)\n    assert len(docs) > 0\n\n    docs = kb.list_docs(file_name=del_file)\n    assert len(docs) == 0\n\n    assert os.path.isfile(os.path.join(doc_path, del_file))\n\n    # prune folder\n    prune_folder_files([kb_name])\n\n    # check result\n    assert not os.path.isfile(os.path.join(doc_path, del_file))\n    assert os.path.isfile(os.path.join(doc_path, keep_file))\n\n\ndef test_drop_kb():\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    kb.drop_kb()\n    assert not kb.exists()\n    assert not os.path.isdir(kb_path)\n\n    kb = KBServiceFactory.get_service_by_name(kb_name)\n    assert kb is None\n"}
{"type": "test_file", "path": "tests/test_online_api.py", "content": "import sys\nfrom pathlib import Path\nroot_path = Path(__file__).parent.parent\nsys.path.append(str(root_path))\n\nfrom configs import ONLINE_LLM_MODEL\nfrom server.model_workers.base import *\nfrom server.utils import get_model_worker_config, list_config_llm_models\nfrom pprint import pprint\nimport pytest\n\n\nworkers = []\nfor x in list_config_llm_models()[\"online\"]:\n    if x in ONLINE_LLM_MODEL and x not in workers:\n        workers.append(x)\nprint(f\"all workers to test: {workers}\")\n\n# workers = [\"fangzhou-api\"]\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_chat(worker):\n    params = ApiChatParams(\n        messages = [\n            {\"role\": \"user\", \"content\": \"你是谁\"},\n        ],\n    )\n    print(f\"\\nchat with {worker} \\n\")\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        for x in worker_class().do_chat(params):\n            pprint(x)\n            assert isinstance(x, dict)\n            assert x[\"error_code\"] == 0\n\n\n@pytest.mark.parametrize(\"worker\", workers)\ndef test_embeddings(worker):\n    params = ApiEmbeddingsParams(\n        texts = [\n            \"LangChain-Chatchat (原 Langchain-ChatGLM): 基于 Langchain 与 ChatGLM 等大语言模型的本地知识库问答应用实现。\",\n            \"一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。\",\n        ]\n    )\n\n    if worker_class := get_model_worker_config(worker).get(\"worker_class\"):\n        if worker_class.can_embedding():\n            print(f\"\\embeddings with {worker} \\n\")\n            resp = worker_class().do_embeddings(params)\n\n            pprint(resp, depth=2)\n            assert resp[\"code\"] == 200\n            assert \"data\" in resp\n            embeddings = resp[\"data\"]\n            assert isinstance(embeddings, list) and len(embeddings) > 0\n            assert isinstance(embeddings[0], list) and len(embeddings[0]) > 0\n            assert isinstance(embeddings[0][0], float)\n            print(\"向量长度：\", len(embeddings[0]))\n\n\n# @pytest.mark.parametrize(\"worker\", workers)\n# def test_completion(worker):\n#     params = ApiCompletionParams(prompt=\"五十六个民族\")\n    \n#     print(f\"\\completion with {worker} \\n\")\n\n#     worker_class = get_model_worker_config(worker)[\"worker_class\"]\n#     resp = worker_class().do_completion(params)\n#     pprint(resp)\n"}
{"type": "test_file", "path": "tests/document_loader/test_pdfloader.py", "content": "import sys\nfrom pathlib import Path\n\nroot_path = Path(__file__).parent.parent.parent\nsys.path.append(str(root_path))\nfrom pprint import pprint\n\ntest_files = {\n    \"ocr_test.pdf\": str(root_path / \"tests\" / \"samples\" / \"ocr_test.pdf\"),\n}\n\ndef test_rapidocrpdfloader():\n    pdf_path = test_files[\"ocr_test.pdf\"]\n    from document_loaders import RapidOCRPDFLoader\n\n    loader = RapidOCRPDFLoader(pdf_path)\n    docs = loader.load()\n    pprint(docs)\n    assert isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0].page_content, str)\n\n\n"}
{"type": "source_file", "path": "MinerU_model_download.py", "content": "import json\nimport os\n\nimport requests\nfrom modelscope import snapshot_download\n\n\ndef download_json(url):\n    # 下载JSON文件\n    response = requests.get(url)\n    response.raise_for_status()  # 检查请求是否成功\n    return response.json()\n\n\ndef download_and_modify_json(url, local_filename, modifications):\n    if os.path.exists(local_filename):\n        data = json.load(open(local_filename))\n        config_version = data.get('config_version', '0.0.0')\n        if config_version < '1.0.0':\n            data = download_json(url)\n    else:\n        data = download_json(url)\n\n\n    # 修改内容\n    for key, value in modifications.items():\n        data[key] = value\n\n    # 保存修改后的内容\n    with open(local_filename, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n\n\nif __name__ == '__main__':\n\n    mineru_patterns = [\n        \"models/Layout/LayoutLMv3/*\",\n        \"models/Layout/YOLO/*\",\n        \"models/MFD/YOLO/*\",\n        \"models/MFR/unimernet_small/*\",\n        \"models/TabRec/TableMaster/*\",\n        \"models/TabRec/StructEqTable/*\",\n    ]\n    model_dir = snapshot_download('opendatalab/PDF-Extract-Kit-1.0', allow_patterns=mineru_patterns)\n    layoutreader_model_dir = snapshot_download('ppaanngggg/layoutreader')\n    model_dir = model_dir + '/models'\n    print(f'model_dir is: {model_dir}')\n    print(f'layoutreader_model_dir is: {layoutreader_model_dir}')\n\n    json_url = 'https://gitee.com/myhloli/MinerU/raw/dev/magic-pdf.template.json'\n    config_file_name = 'magic-pdf.json'\n    # home_dir = os.path.expanduser('~')\n    home_dir = os.getcwd()\n\n    config_file = os.path.join(home_dir, config_file_name)\n\n    json_mods = {\n        'models-dir': model_dir,\n        'layoutreader-model-dir': layoutreader_model_dir,\n    }\n\n    download_and_modify_json(json_url, config_file, json_mods)\n\n    print(f'The configuration file has been configured successfully, the path is: {config_file}')\n"}
{"type": "source_file", "path": "coding/tmp_code_a93d4118a587a520cf4783054b081968.py", "content": "\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport io\n\n# Use the user uploaded file directly\nuploaded_content = b'h,T\\n1,20\\n2,30\\n3,25\\n'  # example content, replace with actual file content\n\n# Read the uploaded content\ndata = pd.read_csv(io.BytesIO(uploaded_content))\n\n# Create a bar chart\nplt.bar(data['h'], data['T'])\nplt.xlabel('h')\nplt.ylabel('T')\nplt.title('Bar Chart of h vs T')\nplt.show()\n"}
{"type": "source_file", "path": "chains/llmchain_with_history.py", "content": "from server.utils import get_ChatOpenAI\nfrom configs.model_config import LLM_MODELS, TEMPERATURE\nfrom langchain.chains import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nmodel = get_ChatOpenAI(model_name=LLM_MODELS[0], temperature=TEMPERATURE)\n\n\nhuman_prompt = \"{input}\"\nhuman_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n\nchat_prompt = ChatPromptTemplate.from_messages(\n    [(\"human\", \"我们来玩成语接龙，我先来，生龙活虎\"),\n     (\"ai\", \"虎头虎脑\"),\n     (\"human\", \"{input}\")])\n\n\nchain = LLMChain(prompt=chat_prompt, llm=model, verbose=True)\nprint(chain({\"input\": \"恼羞成怒\"}))"}
{"type": "source_file", "path": "configs/__init__.py", "content": "from .basic_config import *\nfrom .model_config import *\nfrom .kb_config import *\nfrom .server_config import *\nfrom .prompt_config import *\n\n\nVERSION = \"v0.2.8\"\n"}
{"type": "source_file", "path": "common/__init__.py", "content": ""}
{"type": "source_file", "path": "configs/basic_config.py", "content": "import logging\nimport os\nimport langchain\nimport tempfile\nimport shutil\n\n\n# 是否显示详细日志\nlog_verbose = False\nlangchain.verbose = False\n\n# 通常情况下不需要更改以下内容\n\n# 日志格式\nLOG_FORMAT = \"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\"\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nlogging.basicConfig(format=LOG_FORMAT)\n\n\n# 日志存储路径\nLOG_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"logs\")\nif not os.path.exists(LOG_PATH):\n    os.mkdir(LOG_PATH)\n\n# 临时文件目录，主要用于文件对话\nBASE_TEMP_DIR = os.path.join(tempfile.gettempdir(), \"chatchat\")\nif os.path.isdir(BASE_TEMP_DIR):\n    shutil.rmtree(BASE_TEMP_DIR)\nos.makedirs(BASE_TEMP_DIR, exist_ok=True)\n"}
{"type": "source_file", "path": "configs/kb_config.py", "content": "import os\n\n# 默认使用的知识库\nDEFAULT_KNOWLEDGE_BASE = \"samples\"\n\n# 默认向量库/全文检索引擎类型。可选：faiss, milvus(离线) & zilliz(在线), pgvector,全文检索引擎es\nDEFAULT_VS_TYPE = \"faiss\"\n\n# 缓存向量库数量（针对FAISS）\nCACHED_VS_NUM = 1\n\n# 缓存临时向量库数量（针对FAISS），用于文件对话\nCACHED_MEMO_VS_NUM = 10\n\n# 知识库中单段文本长度(不适用MarkdownHeaderTextSplitter)\nCHUNK_SIZE = 250\n\n# 知识库中相邻文本重合长度(不适用MarkdownHeaderTextSplitter)\nOVERLAP_SIZE = 50\n\n# 知识库匹配向量数量\nVECTOR_SEARCH_TOP_K = 3\n\n# 知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右\nSCORE_THRESHOLD = 1\n\n# 默认搜索引擎。可选：bing, duckduckgo, metaphor\nDEFAULT_SEARCH_ENGINE = \"duckduckgo\"\n\n# 搜索引擎匹配结题数量\nSEARCH_ENGINE_TOP_K = 3\n\n\n# Bing 搜索必备变量\n# 使用 Bing 搜索需要使用 Bing Subscription Key,需要在azure port中申请试用bing search\n# 具体申请方式请见\n# https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource\n# 使用python创建bing api 搜索实例详见:\n# https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/quickstarts/rest/python\nBING_SEARCH_URL = \"https://api.bing.microsoft.com/v7.0/search\"\n# 注意不是bing Webmaster Tools的api key，\n\n# 此外，如果是在服务器上，报Failed to establish a new connection: [Errno 110] Connection timed out\n# 是因为服务器加了防火墙，需要联系管理员加白名单，如果公司的服务器的话，就别想了GG\nBING_SUBSCRIPTION_KEY = \"\"\n\n# metaphor搜索需要KEY\nMETAPHOR_API_KEY = \"\"\n\n\n# 是否开启中文标题加强，以及标题增强的相关配置\n# 通过增加标题判断，判断哪些文本为标题，并在metadata中进行标记；\n# 然后将文本与往上一级的标题进行拼合，实现文本信息的增强。\nZH_TITLE_ENHANCE = False\n\n\n# 每个知识库的初始化介绍，用于在初始化知识库时显示和Agent调用，没写则没有介绍，不会被Agent调用。\nKB_INFO = {\n    \"知识库名称\": \"知识库介绍\",\n    \"samples\": \"关于本项目issue的解答\",\n}\n\n\n# 通常情况下不需要更改以下内容\n\n# 知识库默认存储路径\nKB_ROOT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"knowledge_base\")\nif not os.path.exists(KB_ROOT_PATH):\n    os.mkdir(KB_ROOT_PATH)\n# 数据库默认存储路径。\n# 如果使用sqlite，可以直接修改DB_ROOT_PATH；如果使用其它数据库，请直接修改SQLALCHEMY_DATABASE_URI。\nDB_ROOT_PATH = os.path.join(KB_ROOT_PATH, \"info.db\")\nSQLALCHEMY_DATABASE_URI = f\"sqlite:///{DB_ROOT_PATH}\"\n\n# 可选向量库类型及对应配置\nkbs_config = {\n    \"faiss\": {\n    },\n    \"milvus\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"19530\",\n        \"user\": \"\",\n        \"password\": \"\",\n        \"secure\": False,\n    },\n    \"zilliz\": {\n        \"host\": \"in01-a7ce524e41e3935.ali-cn-hangzhou.vectordb.zilliz.com.cn\",\n        \"port\": \"19530\",\n        \"user\": \"\",\n        \"password\": \"\",\n        \"secure\": True,\n        },\n    \"pg\": {\n        \"connection_uri\": \"postgresql://postgres:postgres@127.0.0.1:5432/langchain_chatchat\",\n    },\n\n    \"es\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": \"9200\",\n        \"index_name\": \"test_index\",\n        \"user\": \"\",\n        \"password\": \"\"\n    }\n}\n\n# TextSplitter配置项，如果你不明白其中的含义，就不要修改。\ntext_splitter_dict = {\n    \"ChineseRecursiveTextSplitter\": {\n        \"source\": \"huggingface\",   # 选择tiktoken则使用openai的方法\n        \"tokenizer_name_or_path\": \"\",\n    },\n    \"SpacyTextSplitter\": {\n        \"source\": \"huggingface\",\n        \"tokenizer_name_or_path\": \"gpt2\",\n    },\n    \"RecursiveCharacterTextSplitter\": {\n        \"source\": \"tiktoken\",\n        \"tokenizer_name_or_path\": \"cl100k_base\",\n    },\n    \"MarkdownHeaderTextSplitter\": {\n        \"headers_to_split_on\":\n            [\n                (\"#\", \"head1\"),\n                (\"##\", \"head2\"),\n                (\"###\", \"head3\"),\n                (\"####\", \"head4\"),\n            ]\n    },\n}\n\n# TEXT_SPLITTER 名称\nTEXT_SPLITTER_NAME = \"ChineseRecursiveTextSplitter\"\n\n# Embedding模型定制词语的词表文件\nEMBEDDING_KEYWORD_FILE = \"embedding_keywords.txt\"\n"}
{"type": "source_file", "path": "configs/prompt_config.py", "content": "# prompt模板使用Jinja2语法，简单点就是用双大括号代替f-string的单大括号\n# 本配置文件支持热加载，修改prompt模板后无需重启服务。\n\n# LLM对话支持的变量：\n#   - input: 用户输入内容\n\n# 知识库和搜索引擎对话支持的变量：\n#   - context: 从检索结果拼接的知识文本\n#   - question: 用户提出的问题\n\n# Agent对话支持的变量：\n\n#   - tools: 可用的工具列表\n#   - tool_names: 可用的工具名称列表\n#   - history: 用户和Agent的对话历史\n#   - input: 用户输入内容\n#   - agent_scratchpad: Agent的思维记录\nExcel_path = None\nsave_path = None\n\nPROMPT_TEMPLATES = {\n    \"llm_chat\": {\n        \"default\":\n            '{{ input }}',\n\n        \"with_history\":\n            'The following is a friendly conversation between a human and an AI. '\n            'The AI is talkative and provides lots of specific details from its context. '\n            'If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\n'\n            'Current conversation:\\n'\n            '{history}\\n'\n            'Human: {input}\\n'\n            'AI:',\n\n        \"py\":\n            '你是一个聪明的代码助手，请你给我写出简单的py代码。 \\n'\n            '{{ input }}',\n\n        \"translator\":\n            '你是翻译论文方面的专家，以下给出相应的论文内容，请进行翻译。注意，'\n            '1. 每句话都要翻译.'\n            '2. 如果遇到你不确定的词汇，请在你翻译的这个词之后直接用小括号标注原文的这个词'\n            '3. 翻译要做到语言连贯，上下文内容衔接。'\n            '4. 翻译目标语言为中文'\n            '如果没有给出相应的论文内容，请提醒用户上传文件'\n            '以下为论文内容，请开始进行翻译:\\n'\n            '用户输入：{{ input }}',\n\n        \"summary\":\n            '1. Mark the title of the paper (with Chinese translation)\\n'\n            '2. list all the authors\\' names (use English) \\n'\n            '3. mark the first author\\'s affiliation (output Chinese translation only) \\n'\n            '4. mark the keywords of this article (use English)\\n'\n            '5. link to the paper, Github code link (if available, fill in Github:None if not)\\n'\n            '6. summarize according to the following four points.Be sure to use Chinese answers (proper nouns need to be marked in English)\\n'\n            '   - (1):What is the research background of this article?(use Chinese)\\n'\n            '   - (2):What are the past methods? What are the problems with them? Is the approach well motivated?(use Chinese)\\n'\n            '   - (3):What is the research methodology proposed in this paper?(use Chinese)\\n'\n            '   - (4):On what task and what performance is achieved by the methods in this paper? Can the performance support their goals?(use Chinese)\\n'\n            'Follow the format of the output that follows:\\n'\n            '   1. Title: xxx\\n\\n'\n            '   2. Authors: xxx\\n\\n'\n            '   3. Affiliation: xxx\\n\\n'\n            '   4. Keywords: xxx\\n\\n'\n            '   5. Urls: xxx or xxx , xxx \\n\\n'\n            '   6. Summary: \\n\\n'\n            '       - (1):xxx;\\n'\n            '       - (2):xxx;\\n'\n            '       - (3):xxx;\\n'\n            '       - (4):xxx.\\n\\n'\n            'Be sure to use Chinese answers (proper nouns need to be marked in English), statements as concise and academic as possible,\\n'\n            'do not have too much repetitive information, numerical values using the original numbers.\\n'\n            'Artical content is: \\n'\n            '{{ input }}',\n        \"summary_paper\":\n            '你是一个请按照以下步骤进行，并为每个部分提供详细的总结：'\n\n            '1.标题和摘要:(60字)'\n\n            '简要描述论文的主题和研究范围。'\n            '摘要中提出了哪些关键点？'\n            '2.研究问题和目的:(60字)'\n\n            '明确指出作者旨在解答的问题或研究的主要目的。'\n            '3.研究背景:(100字)'\n\n            '概括研究背景，包括研究领域的现状和研究的必要性。'\n\n            '4.研究方法:(100字)'\n\n            '描述作者使用的研究设计、样本选择、数据收集和分析方法。'\n            '5.主要发现:(50字)'\n\n            '突出显示论文中的主要研究结果和发现。'\n            '6.讨论:(50字)'\n\n            '概述作者如何解释研究结果及其对现有理论和实践的意义。'\n\n            '7.结论:(50字)'\n\n            '总结作者的结论，并评估其对研究领域的贡献。'\n            '引用和参考文献:'\n\n\n            '8.研究的原创性和创新点:(60字)'\n\n            '识别研究的新颖之处和对现有知识体系的贡献。'\n\n            '9.未来研究方向:(50字)'\n\n            '基于对论文的分析，提出未来研究的可能方向。'\n\n            '请在阅读完论文后，提供一份包含以上各点的详细总结，请按照实际标定字数进行总结，不能太少。这将帮助我们更好地理解论文内容，并在必要时进行深入讨论。(注意，请用中文回答)'\n            '论文内容如下：\\n',\n        \"SWOT分析\":\n            '你是SWOT分析方面的专家，你能根据给定的信息进行详细而又准确的分析。'\n            '请提供SWOT分析的全面内容，确保包括以下方面：\\n'\n            '- Strengths (优势)：组织或项目的内部优势因素，帮助实现目标和成功的方面。\\n'\n            '- Weaknesses (劣势)：组织或项目的内部不利因素，可能阻碍实现目标和成功的方面。\\n'\n            '- Opportunities (机会)：外部环境中组织或项目可以利用的有利条件或情况。\\n'\n            '- Threats (威胁)：外部环境中可能对组织或项目造成负面影响的不利条件或情况。\\n'\n            '确保对每个方面进行详细描述，并提供具体的例子或数据支持。'\n            '{{ input }}',\n        \"文献综述\":\n            '你是综合文献综述撰写助手'\n\n            '根据以下提供的多篇文献的总结，我需要你帮我撰写一篇综合文献综述。请按照以下步骤进行：'\n\n            '1.引言:(200字)'\n\n            '简要介绍研究领域和文献综述的目的。'\n            '阐述文献综述的范围和所涵盖的文献。'\n\n            '2.研究背景和现状:(200字)'\n\n            '综合各篇文献的背景信息，概述研究领域的现状。'\n\n            '3.研究问题和目的的比较:(200字)'\n\n            '对比不同文献中提出的研究问题和目的，找出共性和差异。'\n\n            '4.文献回顾的综合:(200字)'\n\n            '汇总并分析不同文献对现有研究的回顾，识别研究领域中的主要理论和概念。'\n            \n            '5.研究方法的对比分析:(200字)'\n\n            '对比不同文献中使用的研究方法，包括研究设计、样本选择和数据分析。'\n            '6.主要发现的汇总:(200字)'\n\n            '综合各篇文献的主要发现，概括研究领域的关键结果。'\n            '7.讨论:(200字)'\n\n            '分析不同研究结果之间的一致性和差异性，并讨论其对理论和实践的影响。'\n            '8.研究限制的总结:(200字)'\n\n            '总结各篇文献中提到的研究限制，并讨论这些限制对整个研究领域的共同影响。'\n            '9.结论的提炼:(200字)'\n\n            '从各篇文献的结论中提炼出核心观点，并综合形成整体研究领域的结论。'\n            '10.研究原创性和创新点的归纳:(200字)'\n\n            '归纳各篇文献的原创性和创新之处，展现研究领域的新进展。'\n            '11.实际应用的探讨:(200字)'\n\n            '探讨研究结果对实践领域的潜在应用和影响。'\n            '12.未来研究方向的展望:(200字)'\n\n            '基于综合分析，提出未来研究的可能方向和建议。'\n            '13.总结:(400字)'\n\n            '对文献综述进行总结，强调研究领域的重要性和未来研究的潜力。'\n            '请确保文献综述中引用的文献信息准确无误并且确保按照相应的字数进行编写，并在综述中体现批判性思维。撰写完成后，进行校对，确保文献综述的流畅性和逻辑性（注意，请用中文回答）'\n            '下面是相关文献的综述:\\n',\n                },\n\n    \"knowledge_base_chat\": {\n        \"default\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，'\n            '不允许在答案中添加编造成分，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"text\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"empty\":  # 搜不到知识库的时候使用\n            '请你回答我的问题:\\n'\n            '{{ question }}\\n\\n',\n        \"文献综述\":\n            '你是写文献综述方面的专家，请根据提供的信息，按要求用文献综述的格式给出一段文献综述: \\n'\n            '{{ question }}\\n\\n',\n        \"文献总结\":\n            '你是文献总结方面的专家，请根据提供的信息进行详细整体的概括，给出一个详细的总结\\n'\n            '给出人类可能感兴趣的几个问题供选择'\n            '{{ question }}\\n\\n',\n         \"SWOT分析\":\n            '你是SWOT分析方面的专家，你能根据给定的信息进行详细而又准确的分析'\n            '{{ input }}',  \n         \"论文润色\":\n            '你是论文润色方面的专家，你能根据给定的论文内容提供高质量的论文润色服务'\n            '服务内容：\\n-仔细校对和编辑给定论文内容，确保语法、拼写和标点的准确性。\\n- 优化论文结构和段落组织，以确保逻辑清晰、表达流畅。\\n-检查参考文献和引用格式，确保符合学术规范。\\n'\n            '在润色过程中确保用原文章的语言进行润色，回答直接从文章的润色开始，不需要加入任何的提示信息，不要省略任何内容。'\n            '给出具体需要润色的地方，并给出润色结果'\n            '{{ input }}',\n        \"论文翻译\":\n            '你是论文翻译方面的专家，你能根据给定的信息进行详细而又准确的翻译，你的翻译结果基本和原文的文字数量能进行一一对应，而不是进行总结翻译。'\n            '翻译遵守：\\n- 专业的翻译论文从原文翻译为中文，确保内容的完整和准确性。\\n- 对翻译文稿进行校对和润色，以确保语言表达流畅、通顺。\\n- 可根据需要提供专业术语的翻译和解释，确保翻译结果符合学术标准。'\n            '用户输入：{{ input }}',\n        \"文章总结\":\n            '你是文章总结方面的专家，你能根据给定的信息进行详细而又准确的总结，确保不遗漏任何一个重点，你能够提供出色的文章总结服务'\n            '服务内容：\\n- 仔细阅读和理解文章，把握文章的核心思想和主题。\\n- 撰写简洁清晰的文章总结，突出文章的重点和亮点。\\n- 确保文章总结符合读者的阅读习惯和理解水平，简洁明了。'\n            '{{ input }}',\n        \"论文英文纠错\":\n            '你是论文英文方面的专家，你能根据给定的论文信息进行细致而又谨慎的纠错，并指出纠错原因和出错位置。你能够提供出色论文英文纠错的服务'\n            '服务内容：\\n- 对您的论文进行全面的语法、拼写和标点符号的校对和修正。\\n- 优化论文的句子结构和语言表达，确保论文通顺、流畅。\\n- 检查参考文献和引用格式，确保符合学术规范和期刊要求。'\n            '{{ input }}',\n    },\n\n\n    \"search_engine_chat\": {\n        \"default\":\n            '<指令>这是我搜索到的互联网信息，请你根据这些信息进行提取并有调理，简洁的回答问题。'\n            '如果无法从中得到答案，请说 “无法搜索到能回答问题的内容”。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n\n        \"search\":\n            '<指令>根据已知信息，简洁和专业的来回答问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题”，答案请使用中文。 </指令>\\n'\n            '<已知信息>{{ context }}</已知信息>\\n'\n            '<问题>{{ question }}</问题>\\n',\n    },\n\n    \n    \"agent_chat\": {\n        \"default\":\n            'Answer the following questions as best you can. If it is in order, you can use some tools appropriately. '\n            'You have access to the following tools:\\n\\n'\n            '{tools}\\n\\n'\n            'Use the following format:\\n'\n            'Question: the input question you must answer1\\n'\n            'Thought: you should always think about what to do and what tools to use.\\n'\n            'Action: the action to take, should be one of [{tool_names}]\\n'\n            'Action Input: the input to the action\\n'\n            'Observation: the result of the action\\n'\n            '... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\\n'\n            'Thought: I now know the final answer\\n'\n            'Final Answer: the final answer to the original input question\\n'\n            'Begin!\\n\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}\\n',\n\n        \"ChatGLM3\":\n            'You can answer using the tools, or answer directly using your knowledge without using the tools. '\n            'Respond to the human as helpfully and accurately as possible.\\n'\n            'You have access to the following tools:\\n'\n            '{tools}\\n'\n            'Use a json blob to specify a tool by providing an action key (tool name) '\n            'and an action_input key (tool input).\\n'\n            'Valid \"action\" values: \"Final Answer\" or  [{tool_names}]'\n            'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n            '```\\n'\n            '{{{{\\n'\n            '  \"action\": $TOOL_NAME,\\n'\n            '  \"action_input\": $INPUT\\n'\n            '}}}}\\n'\n            '```\\n\\n'\n            'Follow this format:\\n\\n'\n            'Question: input question to answer\\n'\n            'Thought: consider previous and subsequent steps\\n'\n            'Action:\\n'\n            '```\\n'\n            '$JSON_BLOB\\n'\n            '```\\n'\n            'Observation: action result\\n'\n            '... (repeat Thought/Action/Observation N times)\\n'\n            'Thought: I know what to respond\\n'\n            'Action:\\n'\n            '```\\n'\n            '{{{{\\n'\n            '  \"action\": \"Final Answer\",\\n'\n            '  \"action_input\": \"Final response to human\"\\n'\n            '}}}}\\n'\n            'Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. '\n            'Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}',\n        \"绘制柱形图\":\n            # '用户需要你绘制柱形图，请利用python工具进行绘制'\n            # '绘制过程如果需要代码，请将代码一次性输出'\n            # '使用下面的格式:\\n'\n            # 'Question: 你必须回答用户的问题\\n'\n            # 'Thought: 你应该时刻思考做什么.\\n'\n            # 'Action: 你应执行的操作应为 [{python: \"Use python interpreter to run python code\"}, {show_image: \"display image by passing an address\"}]\\n'\n            # 'Action Input: 操作的输入\\n'\n            # 'Observation: 操作的结果\\n'\n            # '... ( Thought/Action/Action Input/Observation 能够被重复多次)\\n'\n            # 'Thought: 我现在知道了最终答案\\n'\n            # 'Final Answer: 原始输入的最终答案\\n'\n            # 'Begin!\\n\\n'\n            # 'history: {history}\\n\\n'\n            # 'Question: {input}\\n\\n'\n            # 'Thought: {agent_scratchpad}\\n',\n            'Answer the following questions as best you can. If it is in order, you can use some tools appropriately. '\n            'You have access to the following tools:\\n\\n'\n            '{tools}\\n\\n'\n            '!!!Note that when you discover errors, please wash and correct the code and provide it in its entirety!!!'\n            'Use the following format:\\n'\n            'Question: the input question you must answer1\\n'\n            'Thought: you should always think about what to do and what tools to use.\\n'\n            'Action: the action to take, should be one of [{tool_names}]\\n'\n            'Action Input: the input to the action\\n'\n            'Observation: the result of the action\\n'\n            '... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\\n'\n            'Thought: I now know the final answer\\n'\n            'Final Answer: the final answer to the original input question\\n'\n            'Begin!\\n\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}\\n',\n        \"绘制折线图\":\n            '用户需要你绘制折线图，你可以使用python代码 '\n            '绘制过程如果需要代码，请将代码一次性输出'\n            '使用下面的格式:\\n'\n            'Question: 你必须回答的的问题\\n'\n            'Thought: 你应该时刻思考做什么，用什么工具来做.\\n'\n            'Action: 你应执行的操作应为 [{python: \"Use python interpreter to run python code\"}]\\n'\n            'Action Input: 操作的输入\\n'\n            'Observation: 操作的结果\\n'\n            '... ( Thought/Action/Action Input/Observation 能够被重复多次)\\n'\n            'Thought: 我现在知道了最终答案\\n'\n            'Final Answer: 原始输入的最终答案\\n'\n            'Begin!\\n\\n'\n            'history: {history}\\n\\n'\n            'Question: {input}\\n\\n'\n            'Thought: {agent_scratchpad}\\n',\n        \n    }\n}"}
{"type": "source_file", "path": "configs/server_config.py", "content": "import sys\nfrom configs.model_config import LLM_DEVICE\n\n# httpx 请求默认超时时间（秒）。如果加载模型或对话较慢，出现超时错误，可以适当加大该值。\nHTTPX_DEFAULT_TIMEOUT = 300.0\n\n# API 是否开启跨域，默认为False，如果需要开启，请设置为True\n# is open cross domain\nOPEN_CROSS_DOMAIN = False\n\n# 各服务器默认绑定host。如改为\"0.0.0.0\"需要修改下方所有XX_SERVER的host\n# DEFAULT_BIND_HOST = \"0.0.0.0\" if sys.platform != \"win32\" else \"127.0.0.1\"\nDEFAULT_BIND_HOST = \"172.26.94.25\"\n\n# webui.py server\nWEBUI_SERVER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 8501,\n}\n\n# api.py server\nAPI_SERVER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 8000,\n}\n\n# fastchat openai_api server\nFSCHAT_OPENAI_API = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 20000,\n}\n\n# fastchat model_worker server\n# 这些模型必须是在model_config.MODEL_PATH或ONLINE_MODEL中正确配置的。\n# 在启动startup.py时，可用通过`--model-name xxxx yyyy`指定模型，不指定则为LLM_MODELS\nFSCHAT_MODEL_WORKERS = {\n    # 所有模型共用的默认配置，可在模型专项配置中进行覆盖。\n    \"default\": {\n        \"host\": DEFAULT_BIND_HOST,\n        \"port\": 20002,\n        \"device\": LLM_DEVICE,\n        # False,'vllm',使用的推理加速框架,使用vllm如果出现HuggingFace通信问题，参见doc/FAQ\n        # vllm对一些模型支持还不成熟，暂时默认关闭\n        # fschat=0.2.33的代码有bug, 如需使用，源码修改fastchat.server.vllm_worker，\n        # 将103行中sampling_params = SamplingParams的参数stop=list(stop)修改为stop= [i for i in stop if i!=\"\"]\n        \"infer_turbo\": False,\n\n        # model_worker多卡加载需要配置的参数\n        # \"gpus\": None, # 使用的GPU，以str的格式指定，如\"0,1\"，如失效请使用CUDA_VISIBLE_DEVICES=\"0,1\"等形式指定\n        # \"num_gpus\": 1, # 使用GPU的数量\n        # \"max_gpu_memory\": \"20GiB\", # 每个GPU占用的最大显存\n\n        # 以下为model_worker非常用参数，可根据需要配置\n        # \"load_8bit\": False, # 开启8bit量化\n        # \"cpu_offloading\": None,\n        # \"gptq_ckpt\": None,\n        # \"gptq_wbits\": 16,\n        # \"gptq_groupsize\": -1,\n        # \"gptq_act_order\": False,\n        # \"awq_ckpt\": None,\n        # \"awq_wbits\": 16,\n        # \"awq_groupsize\": -1,\n        # \"model_names\": LLM_MODELS,\n        # \"conv_template\": None,\n        # \"limit_worker_concurrency\": 5,\n        # \"stream_interval\": 2,\n        # \"no_register\": False,\n        # \"embed_in_truncate\": False,\n\n        # 以下为vllm_worker配置参数,注意使用vllm必须有gpu，仅在Linux测试通过\n\n        # tokenizer = model_path # 如果tokenizer与model_path不一致在此处添加\n        # 'tokenizer_mode':'auto',\n        # 'trust_remote_code':True,\n        # 'download_dir':None,\n        # 'load_format':'auto',\n        # 'dtype':'auto',\n        # 'seed':0,\n        # 'worker_use_ray':False,\n        # 'pipeline_parallel_size':1,\n        # 'tensor_parallel_size':1,\n        # 'block_size':16,\n        # 'swap_space':4 , # GiB\n        # 'gpu_memory_utilization':0.90,\n        # 'max_num_batched_tokens':2560,\n        # 'max_num_seqs':256,\n        # 'disable_log_stats':False,\n        # 'conv_template':None,\n        # 'limit_worker_concurrency':5,\n        # 'no_register':False,\n        # 'num_gpus': 1\n        # 'engine_use_ray': False,\n        # 'disable_log_requests': False\n\n    },\n    # 可以如下示例方式更改默认配置\n    # \"Qwen-1_8B-Chat\": { # 使用default中的IP和端口\n    #    \"device\": \"cpu\",\n    # },\n    \"chatglm3-6b\": {  # 使用default中的IP和端口\n        \"device\": \"cuda\",\n    },\n\n    # 以下配置可以不用修改，在model_config中设置启动的模型\n    \"zhipu-api\": {\n        \"port\": 21001,\n    },\n    \"minimax-api\": {\n        \"port\": 21002,\n    },\n    \"xinghuo-api\": {\n        \"port\": 21003,\n    },\n    \"qianfan-api\": {\n        \"port\": 21004,\n    },\n    \"fangzhou-api\": {\n        \"port\": 21005,\n    },\n    \"qwen-api\": {\n        \"port\": 21006,\n    },\n    \"baichuan-api\": {\n        \"port\": 21007,\n    },\n    \"azure-api\": {\n        \"port\": 21008,\n    },\n    \"tiangong-api\": {\n        \"port\": 21009,\n    },\n}\n\n# fastchat multi model worker server\nFSCHAT_MULTI_MODEL_WORKERS = {\n    # TODO:\n}\n\n# fastchat controller server\nFSCHAT_CONTROLLER = {\n    \"host\": DEFAULT_BIND_HOST,\n    \"port\": 20001,\n    \"dispatch_method\": \"shortest_queue\",\n}\n"}
{"type": "source_file", "path": "copy_config_example.py", "content": "# 用于批量将configs下的.example文件复制并命名为.py文件\nimport os\nimport shutil\n\nif __name__ == \"__main__\":\n    files = os.listdir(\"configs\")\n\n    src_files = [os.path.join(\"configs\", file) for file in files if \".example\" in file]\n\n    for src_file in src_files:\n        tar_file = src_file.replace(\".example\", \"\")\n        shutil.copy(src_file, tar_file)\n"}
{"type": "source_file", "path": "document_loaders/FilteredCSVloader.py", "content": "## 指定制定列的csv文件加载器\n\nfrom langchain.document_loaders import CSVLoader\nimport csv\nfrom io import TextIOWrapper\nfrom typing import Dict, List, Optional\nfrom langchain.docstore.document import Document\nfrom langchain.document_loaders.helpers import detect_file_encodings\n\n\nclass FilteredCSVLoader(CSVLoader):\n    def __init__(\n            self,\n            file_path: str,\n            columns_to_read: List[str],\n            source_column: Optional[str] = None,\n            metadata_columns: List[str] = [],\n            csv_args: Optional[Dict] = None,\n            encoding: Optional[str] = None,\n            autodetect_encoding: bool = False,\n    ):\n        super().__init__(\n            file_path=file_path,\n            source_column=source_column,\n            metadata_columns=metadata_columns,\n            csv_args=csv_args,\n            encoding=encoding,\n            autodetect_encoding=autodetect_encoding,\n        )\n        self.columns_to_read = columns_to_read\n\n    def load(self) -> List[Document]:\n        \"\"\"Load data into document objects.\"\"\"\n\n        docs = []\n        try:\n            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n                docs = self.__read_file(csvfile)\n        except UnicodeDecodeError as e:\n            if self.autodetect_encoding:\n                detected_encodings = detect_file_encodings(self.file_path)\n                for encoding in detected_encodings:\n                    try:\n                        with open(\n                            self.file_path, newline=\"\", encoding=encoding.encoding\n                        ) as csvfile:\n                            docs = self.__read_file(csvfile)\n                            break\n                    except UnicodeDecodeError:\n                        continue\n            else:\n                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n\n        return docs\n\n    def __read_file(self, csvfile: TextIOWrapper) -> List[Document]:\n        docs = []\n        csv_reader = csv.DictReader(csvfile, **self.csv_args)  # type: ignore\n        for i, row in enumerate(csv_reader):\n            if self.columns_to_read[0] in row:\n                content = row[self.columns_to_read[0]]\n                # Extract the source if available\n                source = (\n                    row.get(self.source_column, None)\n                    if self.source_column is not None\n                    else self.file_path\n                )\n                metadata = {\"source\": source, \"row\": i}\n\n                for col in self.metadata_columns:\n                    if col in row:\n                        metadata[col] = row[col]\n\n                doc = Document(page_content=content, metadata=metadata)\n                docs.append(doc)\n            else:\n                raise ValueError(f\"Column '{self.columns_to_read[0]}' not found in CSV file.\")\n\n        return docs\n"}
{"type": "source_file", "path": "document_loaders/__init__.py", "content": "from .mypdfloader import RapidOCRPDFLoader\nfrom .myimgloader import RapidOCRLoader"}
{"type": "source_file", "path": "document_loaders/mypdfloader.py", "content": "from typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\nimport tqdm\nfrom magic_pdf import DiskReaderWriter, UNIPipe\nimport os\nclass RapidOCRPDFLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def pdf2text(filepath):\n            import fitz # pyMuPDF里面的fitz包，不要与pip install fitz混淆\n            from rapidocr_onnxruntime import RapidOCR\n            import numpy as np\n            ocr = RapidOCR()\n            doc = fitz.open(filepath)\n            resp = \"\"\n\n            b_unit = tqdm.tqdm(total=doc.page_count, desc=\"RapidOCRPDFLoader context page index: 0\")\n            for i, page in enumerate(doc):\n\n                # 更新描述\n                b_unit.set_description(\"RapidOCRPDFLoader context page index: {}\".format(i))\n                # 立即显示进度条更新结果\n                b_unit.refresh()\n                # TODO: 依据文本与图片顺序调整处理方式\n                text = page.get_text(\"\")\n                resp += text + \"\\n\"\n\n                img_list = page.get_images()\n                for img in img_list:\n                    pix = fitz.Pixmap(doc, img[0])\n                    img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, -1)\n                    result, _ = ocr(img_array)\n                    if result:\n                        ocr_result = [line[1] for line in result]\n                        resp += \"\\n\".join(ocr_result)\n\n                # 更新进度\n                b_unit.update(1)\n            return resp\n\n        text = pdf2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nclass MinerUPDFLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def pdf2text(local_image_dir):\n            image_writer = DiskReaderWriter(local_image_dir)\n            image_dir = str(os.path.basename(local_image_dir))\n            jso_useful_key = {\"_pdf_type\": \"\", \"model_list\": []}\n            with open(local_image_dir, 'rb') as f:\n                pdf_bytes = f.read()\n\n            pipe = UNIPipe(pdf_bytes, jso_useful_key, image_writer)\n            pipe.pipe_classify()\n            pipe.pipe_analyze()\n            pipe.pipe_parse()\n            md_content = pipe.pipe_mk_markdown(image_dir, drop_mode=\"none\")\n            return md_content\n\n        text = pdf2text(self.file_path)\n        return text\n\n\nif __name__ == \"__main__\":\n    #loader = RapidOCRPDFLoader(file_path=\"../tests/samples/ocr_test.pdf\")\n    loader = MinerUPDFLoader(file_path=\"../transformer.pdf\")\n\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "document_loaders/myimgloader.py", "content": "from typing import List\nfrom langchain.document_loaders.unstructured import UnstructuredFileLoader\n\n\nclass RapidOCRLoader(UnstructuredFileLoader):\n    def _get_elements(self) -> List:\n        def img2text(filepath):\n            from rapidocr_onnxruntime import RapidOCR\n            resp = \"\"\n            ocr = RapidOCR()\n            result, _ = ocr(filepath)\n            if result:\n                ocr_result = [line[1] for line in result]\n                resp += \"\\n\".join(ocr_result)\n            return resp\n\n        text = img2text(self.file_path)\n        from unstructured.partition.text import partition_text\n        return partition_text(text=text, **self.unstructured_kwargs)\n\n\nif __name__ == \"__main__\":\n    loader = RapidOCRLoader(file_path=\"../tests/samples/ocr_test.jpg\")\n    docs = loader.load()\n    print(docs)\n"}
{"type": "source_file", "path": "embeddings/__init__.py", "content": ""}
{"type": "source_file", "path": "embeddings/add_embedding_keywords.py", "content": "'''\r\n该功能是为了将关键词加入到embedding模型中，以便于在embedding模型中进行关键词的embedding\r\n该功能的实现是通过修改embedding模型的tokenizer来实现的\r\n该功能仅仅对EMBEDDING_MODEL参数对应的的模型有效，输出后的模型保存在原本模型\r\n感谢@CharlesJu1和@charlesyju的贡献提出了想法和最基础的PR\r\n\r\n保存的模型的位置位于原本嵌入模型的目录下，模型的名称为原模型名称+Merge_Keywords_时间戳\r\n'''\r\nimport sys\r\nsys.path.append(\"..\")\r\nfrom datetime import datetime\r\nfrom configs import (\r\n    MODEL_PATH,\r\n    EMBEDDING_MODEL,\r\n    EMBEDDING_KEYWORD_FILE,\r\n)\r\nimport os\r\nimport torch\r\nfrom safetensors.torch import save_model\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n\r\ndef get_keyword_embedding(bert_model, tokenizer, key_words):\r\n    tokenizer_output = tokenizer(key_words, return_tensors=\"pt\", padding=True, truncation=True)\r\n\r\n    # No need to manually convert to tensor as we've set return_tensors=\"pt\"\r\n    input_ids = tokenizer_output['input_ids']\r\n\r\n    # Remove the first and last token for each sequence in the batch\r\n    input_ids = input_ids[:, 1:-1]\r\n\r\n    keyword_embedding = bert_model.embeddings.word_embeddings(input_ids)\r\n    keyword_embedding = torch.mean(keyword_embedding, 1)\r\n\r\n    return keyword_embedding\r\n\r\n\r\ndef add_keyword_to_model(model_name=EMBEDDING_MODEL, keyword_file: str = \"\", output_model_path: str = None):\r\n    key_words = []\r\n    with open(keyword_file, \"r\") as f:\r\n        for line in f:\r\n            key_words.append(line.strip())\r\n\r\n    st_model = SentenceTransformer(model_name)\r\n    key_words_len = len(key_words)\r\n    word_embedding_model = st_model._first_module()\r\n    bert_model = word_embedding_model.auto_model\r\n    tokenizer = word_embedding_model.tokenizer\r\n    key_words_embedding = get_keyword_embedding(bert_model, tokenizer, key_words)\r\n    # key_words_embedding = st_model.encode(key_words)\r\n\r\n    embedding_weight = bert_model.embeddings.word_embeddings.weight\r\n    embedding_weight_len = len(embedding_weight)\r\n    tokenizer.add_tokens(key_words)\r\n    bert_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=32)\r\n\r\n    # key_words_embedding_tensor = torch.from_numpy(key_words_embedding)\r\n    embedding_weight = bert_model.embeddings.word_embeddings.weight\r\n    with torch.no_grad():\r\n        embedding_weight[embedding_weight_len:embedding_weight_len + key_words_len, :] = key_words_embedding\r\n\r\n    if output_model_path:\r\n        os.makedirs(output_model_path, exist_ok=True)\r\n        word_embedding_model.save(output_model_path)\r\n        safetensors_file = os.path.join(output_model_path, \"model.safetensors\")\r\n        metadata = {'format': 'pt'}\r\n        save_model(bert_model, safetensors_file, metadata)\r\n        print(\"save model to {}\".format(output_model_path))\r\n\r\n\r\ndef add_keyword_to_embedding_model(path: str = EMBEDDING_KEYWORD_FILE):\r\n    keyword_file = os.path.join(path)\r\n    model_name = MODEL_PATH[\"embed_model\"][EMBEDDING_MODEL]\r\n    model_parent_directory = os.path.dirname(model_name)\r\n    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\r\n    output_model_name = \"{}_Merge_Keywords_{}\".format(EMBEDDING_MODEL, current_time)\r\n    output_model_path = os.path.join(model_parent_directory, output_model_name)\r\n    add_keyword_to_model(model_name, keyword_file, output_model_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    add_keyword_to_embedding_model(EMBEDDING_KEYWORD_FILE)\r\n\r\n    # input_model_name = \"\"\r\n    # output_model_path = \"\"\r\n    # # 以下为加入关键字前后tokenizer的测试用例对比\r\n    # def print_token_ids(output, tokenizer, sentences):\r\n    #     for idx, ids in enumerate(output['input_ids']):\r\n    #         print(f'sentence={sentences[idx]}')\r\n    #         print(f'ids={ids}')\r\n    #         for id in ids:\r\n    #             decoded_id = tokenizer.decode(id)\r\n    #             print(f'    {decoded_id}->{id}')\r\n    #\r\n    # sentences = [\r\n    #     '数据科学与大数据技术',\r\n    #     'Langchain-Chatchat'\r\n    # ]\r\n    #\r\n    # st_no_keywords = SentenceTransformer(input_model_name)\r\n    # tokenizer_without_keywords = st_no_keywords.tokenizer\r\n    # print(\"===== tokenizer with no keywords added =====\")\r\n    # output = tokenizer_without_keywords(sentences)\r\n    # print_token_ids(output, tokenizer_without_keywords, sentences)\r\n    # print(f'-------- embedding with no keywords added -----')\r\n    # embeddings = st_no_keywords.encode(sentences)\r\n    # print(embeddings)\r\n    #\r\n    # print(\"--------------------------------------------\")\r\n    # print(\"--------------------------------------------\")\r\n    # print(\"--------------------------------------------\")\r\n    #\r\n    # st_with_keywords = SentenceTransformer(output_model_path)\r\n    # tokenizer_with_keywords = st_with_keywords.tokenizer\r\n    # print(\"===== tokenizer with keyword added =====\")\r\n    # output = tokenizer_with_keywords(sentences)\r\n    # print_token_ids(output, tokenizer_with_keywords, sentences)\r\n    #\r\n    # print(f'-------- embedding with keywords added -----')\r\n    # embeddings = st_with_keywords.encode(sentences)\r\n    # print(embeddings)"}
{"type": "source_file", "path": "extra_function/PdfCleaner.py", "content": "import os,sys\nsys.path.append(os.path.dirname(__file__))\nimport fitz # pyMuPDF里面的fitz包，不要与pip install fitz混淆\nfrom rapidocr_onnxruntime import RapidOCR\nimport numpy as np\nfrom typing import List,Tuple,Optional\nimport tqdm\nimport re\nfrom logger import Logger\nRECOGNIZE_SENTENCE_LEN = 50\nNUM_PERCENT = 0.6\nclass TitleTree:\n    # 用单引号来包裹自身类作为参数传递，来解决无法将自身作为参数传递的问题\n    def __init__(self, value:List[int]=[], parent: 'TitleTree' =None) -> None:\n        self.value = value\n        self.parent = parent\n        self.sons : List[TitleTree] = []\nclass PaperCleaner:\n    def __init__(self,path: str,debug=None) -> None:\n        self.logger = Logger().get_logger\n        self.RECOGNIZE_SENTENCE_LEN = 40 #识别句子的长度,找表格描述时小于这个长度被认为是疑似表格内容\n        self.paper_path = path # pdf的path\n        self.NUM_PERCENT = 0.4 # 句子被认为是表格内容的数字比例\n        self.text :List[str] = None # paper原文本内容\n        self.cleaned_text: List[str]=None # 删除table内容后的文本内容\n        self.paper_table_name_type = 0 # 用来解决table的name的匹配问题\n        self.paper_table_position_type = 0 # 用来解决table的内容的位置问题，0：在下方，1：在上方\n        self.debug = debug\n        self.MINI_LINES = 10 # 超短句子，被认为是表格的内容\n        self.title_pattern = None # 用于确定title的模式\n        self.title_tree = TitleTree(value=[])\n        self.chunk_paper = []\n        self.read()\n        self.search_title()\n        if self.title_pattern == 1:\n            self.title_pattern_1()\n        elif self.title_pattern == 2:\n            self.title_pattern_2()\n        else:\n            self.logger.error(\"未找到title_pattern\")\n        self.find_matches()\n        self.clean_table_context()\n        self.chunk_by_title()\n    def read(self) -> None:\n        \"\"\"\n        读取paper的内容，返回List[str]\n        \"\"\"\n        pattern1 = r'Table (\\d+): .+?(?=\\.|\\s*\\n)'\n        pattern2 = r'Table (\\d+)\\. .+?(?=\\.|\\s*\\n)'\n        doc = fitz.open(self.paper_path)\n        self.text = [\"\"]*doc.page_count\n        self.cleaned_text = [\"\"]*doc.page_count\n        print(doc.page_count)\n        b_unit = tqdm.tqdm(total=doc.page_count, desc=\"PaperPDFLoader context page index: 0\")\n        for i, page in enumerate(doc):\n            b_unit.set_description(\"PaperPDFLoader context page index: {}\".format(i))\n            b_unit.refresh()\n            text = page.get_text(\"\")\n            cleaned_text = re.sub(r'<latexit[^>]*>(.*?)</latexit>', '', text, flags=re.DOTALL | re.IGNORECASE)\n            # 第一页页脚不在最后\n            if i != 0:\n                self.text[i] = self.clean_page_num(cleaned_text)\n            else:\n                self.text[i] = cleaned_text\n            # 判断一下paper的table标题的匹配模式 和 table的位置\n            if not self.paper_table_name_type:\n                matches = list(re.finditer(pattern1,self.text[i]))\n                if matches:\n                    self.paper_table_name_type = 1\n                    self.seek_table_content_position(self.text[i],matches[0].start())\n                else: \n                    matches = list(re.finditer(pattern2,self.text[i]))\n                    if matches and not self.paper_table_name_type:\n                        self.paper_table_name_type = 2\n                        self.seek_table_content_position(self.text[i],matches[0].start())\n\n            b_unit.update(1)\n        print(f\"Paper path: {self.paper_path} has already been read\")\n    def chunk_by_title(self):\n        texts = \"\"\n        for i in self.cleaned_text:\n            texts += i + \"\\n\"\n        chunks = texts.split(\"###\")\n        for chunk in chunks:\n            self.chunk_paper.append([(chunk.split(\"\\n\"))[0],chunk])\n    def seek_table_content_position(self,text:str,head:int):\n        \"\"\"\n        通过pos位置的上下文，来确定paper的表格内容的位置，先从上面找，如果找不到\n        就再从下面找\n        \"\"\"\n        if not text:\n            return \n        # print(head,type(head))\n        head_lines = text[:head].splitlines(keepends=True) # 保留换行符\n        head_lines.reverse()\n        reverse_text = \"\"\n        for line in head_lines: # 反转查询\n            if len(line) > self.RECOGNIZE_SENTENCE_LEN:\n                break\n            else:\n                reverse_text += line\n        if self.design_table_content(reverse_text):\n            # print(reverse_text)\n            self.paper_table_position_type = 1\n            if self.debug:\n                # print(\"\\033[31m表格在上面\\033[0m\")\n                self.logger.info(\"表格在上面\")\n            return \n        table_start,table_end =self.find_line_is_table_content(text[head:])\n        if self.design_table_content(text[head+table_start:head+table_end]):\n            self.paper_table_position_type = 0\n            if self.debug:\n                print(\"\\033[31m表格在下面\\033[0m\")\n                self.logger.info(\"表格在下面\")\n            return \n        # print(\"\\033[31m没有找到table的内容的位置\\033[0m\")\n        self.logger.warning(\"没有找到表格的内容\")\n\n\n    def clean_page_num(self,text:str):\n        \"\"\"\n        清除页脚\n        \"\"\"\n        log = 0\n        pos = 0\n        # print(repr(text[len(text)-70:]))\n        # for index, chara in enumerate(text):\n        #     if text[len(text)-1-index] >= '0' and text[len(text)-1-index] <= '9' :\n        #         log = 1\n        #         # print(text[len(text)-1-index])\n        #     if log and text[len(text)-1-index] == '\\n':\n        #         pos = len(text) - 1 - index\n        #         # print(repr(text[pos-8:pos+4]))\n        #         break\n        if self.is_num(text[len(text)-1]):\n            log += 1\n        if self.is_num(text[len(text)-2]):\n            log += 1\n        if self.is_num(text[len(text)-3]):\n            log += 1\n        return text[:len(text)-log]\n\n\n    def find_matches(self) -> None:\n        \"\"\"\n        调试函数，用来查看匹配的table\n        \"\"\"\n        #  两种匹配模式\n        # 第一种：Table 3: Experimental results on the DOTA dataset compared with state-of-the-art methods.\n        # 第二种：Table 3. Experimental results on the DOTA dataset compared with state-of-the-art methods.\n        pattern1 = r'Table (\\d+): .+?(?=\\.|\\s*\\n)'\n        pattern2 = r'Table (\\d+)\\. .+?(?=\\.|\\s*\\n)'\n        print(\"=\"*100)\n        for text in self.text:\n            if self.paper_table_name_type == 1:\n                matches = list(re.finditer(pattern1,text))\n            else:\n                matches = list(re.finditer(pattern2,text))\n            for match in matches:\n                print(match.group())  # 打印匹配的文本\n                print(match)\n        print(\"=\"*100)\n    def is_num(self,c:str):\n        if len(c) == 1:\n            if c >= '0' and c <= '9':\n                return True\n        return False\n    def design_table_content(self,text:str) -> bool:\n        \"\"\"\n        1.用text的char的table类型占比得分来判断\n        2.用数字比例大的行的占比来判断\n        \"\"\"\n        num = 0\n        if not text:\n            return False\n        for index,i in enumerate(text):\n            if self.is_num(i):\n                num += 1\n            if i == \".\" :\n                if index + 1 <= len(text) - 1:\n                    if self.is_num(text[index-1]) and self.is_num(text[index+1]):\n                        num += 2\n                    else:\n                        num += 1\n            if i == '±' or i == \"≤\" or i == '≥': # 给权重\n                num += 2\n        num_line = 0\n        chunk = text.split(\"\\n\")\n        for k in chunk:\n            if self.is_table_line(k):\n                num_line += 1\n        # print(num/len(text))\n        # print(num_line/len(chunk))\n        if num/len(text) >=  0.4 or num_line/len(chunk) >= 0.4:\n            return True\n        return False\n    def is_table_line(self,text:str):\n        \"\"\"\n        用text的table元素占比来判断是否是table的行\n        \"\"\"\n        num = 0\n        if not text:\n            return False\n        for i in text:\n            if self.is_num(i) or i == '–'or i == '-' or i == '±' or i == '≤' or i == '≥' or i == '.' or i == '✓' or i == '✗':\n                num += 1\n        if num / len(text) >= 0.6:\n            return True\n        return False\n    def find_row_low_50_next(self,text:str) -> int:\n        \"\"\"\n        text: 传入text \n        从0位置开始，找到小于长度50且数字多的行开始。\n        或者长度小于10的行\n\n        \"\"\"\n        matches = re.finditer(r'\\A.{0,49}\\Z',text)\n        for match in matches:\n            if self.design_table_content(text[match.start(),match.end()]):\n                if self.debug:\n                    # print(f\"\\n\\033[31m表格内容：数字多开始,{match.group}\\033[0m\\n\")\n                    self.logger.info(\"表格内容：数字多开始\")\n                return match.start()\n            elif len(match.group) < self.MINI_LINES:\n                if self.debug:\n                    # print(f\"\\033[31m长度短{match.group}\\033[0m\")\n                    self.logger.warning(f\"长度短，{match.group}\")\n                return match.start()\n        return len(text)-1\n    def find_row_over_50_next(self,text:str) -> int:\n\n        \"\"\"\n        text: 从第一个小于50且数字多的行开始，找到第一个长度大于50的非数字行\n        int：返回大于50的非数字行的首字母的位置\n        \n        \"\"\"\n        # pos = self.find_row_low_50_next(text)\n        # for match in re.finditer(r'^(.{51,})\\n', text[pos:], re.DOTALL):\n        #     if not self.design_excel_content(match):\n        #         if match.start() > 0:\n        #             print(f\"\\033[31m匹配到了{match.group}\\033[0m\")\n\n        #             return match.start() + pos\n        # return len(text)-1\n        next_line_start = 0\n        lines = text[next_line_start:].split(\"\\n\")\n        table_position_start = 0\n        table_position_end = 0\n        # if self.debug:\n        #     print(\">\"*100)\n        for index,line in enumerate(lines):\n            if self.debug:\n                print(line)\n            if len(line) >= self.RECOGNIZE_SENTENCE_LEN and not self.design_table_content(line):\n                if self.debug:\n                    # print(\"ok了，准备撤退\")\n                    self.logger.info(\"ok了，准备撤退\")\n                break\n            else:\n                if self.debug:\n                    # print(\"不是\")\n                    self.logger.info(\"不是\")\n                table_position_end += len(line) + 1\n        # if self.debug:\n            # print(\"<\"*100)\n        ###################################测试代码#################################\n        if self.debug:\n        #     print(\"\\033[31m\\033>>>>find_line_is_table_content测试信息----------------开始-------\\033[0m\\033[0m\")\n        #     print(\"\\033[31m\\033[1m找到的text table 内容 \\033[0m\\033[0m\")\n        #     print(text[table_position_start:table_position_end])\n        #     print(\"\\033[31m\\033<<<<find_line_is_table_content测试信息----------------结束-------\\033[0m\\033[0m\")\n            self.logger.info(f\"找到的text table 内容,{text[table_position_start:table_position_end]}\")\n        ############################################################################\n\n        return table_position_end\n    def design_without_other_word(self,text:str) -> bool:\n        \"\"\"\n        判断text之后在\\n之前有无字符\n        \"\"\"\n        for i in text:\n            if i == ' ' or i == '.':\n                continue\n            elif i != '\\n':\n                return False\n            elif i == '\\n':\n                return True\n        return True\n    def find_line_is_table_content(self,text:str)-> Tuple[int,int]:\n        \"\"\"\n        找到table内容的开头和结尾+1\n        开头是从第一个'\\n'以后，找到'.'结尾的且长度小于50的或者之后小于50的句子\n        结尾是超过50的行，且数字比例少\n        \"\"\"\n        next_line_start = 0\n        for index,character in enumerate(text):\n            if character == \"\\n\":\n                next_line_start = index + 1\n                break\n        lines = text[next_line_start:].split(\"\\n\")\n        table_position_start = next_line_start\n        table_position_end = next_line_start\n        transfer_travel = 0\n        if self.debug:\n            print(\">\"*100)\n        for index,line in enumerate(lines):\n            if self.debug:\n                print(line)\n            if not transfer_travel:\n                # 找start，从第一个小于设定长度的句子开始\n                if len(line) < self.RECOGNIZE_SENTENCE_LEN:\n                    if line:\n                        if line[-1] == '.':\n                            table_position_start += len(line) + 1\n                            table_position_end += len(line) + 1\n                            transfer_travel = 1\n                            if self.debug:\n                                # print(\"找到.了，OK了\") # 测试函数\n                                self.logger.info(\"找到了，ok\")\n                            continue\n                        elif index > 0 and lines[index-1][-1] == '.':\n                            transfer_travel = 1\n                            table_position_end += len(line) + 1\n                            if self.debug:\n                                # print(\"上一行最后是.\")\n                                self.logger.info(\"上一行最后是.\")\n                            continue\n                        elif len(line) < self.MINI_LINES:\n                            transfer_travel = 1\n                            table_position_end += len(line) + 1\n                            if self.debug:\n                                # print(\"这一行很短，鉴定为表格内容\")\n                                self.logger.info(\"上一行最后是.\")\n                            continue\n                    else:\n                        table_position_end += 1\n                        table_position_start += 1\n\n                table_position_start += len(line) + 1\n                table_position_end += len(line) + 1\n            else:\n                # 找end，从第一个\n                if line == \"###\":\n                    self.logger.warning(f\"发现###，切除部分终止，后面是{lines[index+1]}\")\n                    break\n                if len(line) >= self.RECOGNIZE_SENTENCE_LEN and not self.design_table_content(line):\n                    if self.debug:\n                        # print(\"ok了，准备撤退\")\n                        self.logger.info(\"ok了，准备撤退\")\n                    break\n                else:\n                    if self.debug:\n                        # print(\"不是\")\n                        self.logger.info(\"不是\")\n                    table_position_end += len(line) + 1\n        # if self.debug:\n        #     print(\"<\"*100)\n        ###################################测试代码#################################\n        if self.debug:\n            # print(\"\\033[31m\\033>>>>find_line_is_table_content测试信息----------------开始-------\\033[0m\\033[0m\")\n            # print(\"\\033[31m\\033[1m找到的text table 内容 \\033[0m\\033[0m\")\n            # print(text[table_position_start:table_position_end])\n            # print(\"\\033[31m\\033<<<<find_line_is_table_content测试信息----------------结束-------\\033[0m\\033[0m\")\n            self.logger.info(f\"找到的text table内容,{text[table_position_start:table_position_end]}\")\n        ############################################################################\n\n        return table_position_start,table_position_end        \n    def cut_table_str(self,text:str,pos:int) -> str:\n        \"\"\"\n        text: 需要切除table内容部分的text\n        pos:text的table标题之后的第一个字符的位置\n        \n        \"\"\"\n        new_text = text\n        table_start = 0\n        table_end = 0\n        # if not self.design_without_other_word(text[pos:]): # 判断table这一行之后还有没有内容\n        #     table_start,table_end = self.find_line_is_table_content(text[pos:]) # 将标题的内容隔过去，找到tabel的前和后\n        # else:\n        #     # table_start = pos\n        #     # table 这一行之后没有其他word，从下一行开始，找到长度小于设定长度且数字多的行\n        #     # 或者找到行长度极其小的行\n        #     table_end = self.find_row_over_50_next(text[pos:])\n        table_start,table_end = self.find_line_is_table_content(text[pos:]) # 将标题的内容隔过去，找到tabel的前和后\n        \n            \n        if self.design_table_content(text[pos+table_start:pos+table_end]):\n            return text[:pos+table_start] + text[pos+table_end:]\n        else:\n            # print(\"\\033[31m\\033[1m error, 切除部分是非表格内容\\033[0m\\033[0m\")\n            # print(\"-\"*100)\n            # print(f\"<<<<<<<<<<<<<<位置{pos+table_start}-{pos+table_end}>>>>>>>>>>>>>>>>>>>\")\n            # print(\"--------------切除内容开始--------------\")\n            # print(f\"\\033[34m{text[pos+table_start:pos+table_end]}\\033[0m\")\n            # print(\"-\"*100)\n            self.logger.error(f\"切除部分是非表格内容: {text[pos+table_start:pos+table_end]}\")\n        return new_text \n    def recognize_table(self,text:str) ->str:\n\n        \"\"\"\n        识别table的内容，并且切除table内部的内容\n        \"\"\"\n        pattern1 = r'Table (\\d+): .+?(?=\\.|\\s*\\n)'\n        pattern2 = r'Table (\\d+)\\. .+?(?=\\.|\\s*\\n)'\n        # 在没有确定类型的时候，根据第一个匹配的table的类型类确定是哪种pattern\n        if not self.paper_table_name_type:\n            matches = list(re.finditer(pattern1, text))\n            if matches:\n                self.paper_table_name_type = 1\n            else:\n                matches = list(re.finditer(pattern2, text))\n                if not self.paper_table_name_type and matches:\n                        self.paper_table_name_type = 2\n        else:\n            if self.paper_table_name_type == 1:\n                matches = list(re.finditer(pattern1, text))\n            else:\n                matches = list(re.finditer(pattern2, text))\n        if not matches:  \n            return text\n        else:\n            index_first = [] # 匹配的标题的首个字母的text中的位置\n            index_end = [] # 匹配的标题的最后的字母的之后第一个text中的位置\n            new_text = \"\"\n            if self.paper_table_position_type == 0:\n                for match in matches:\n                    index_first.append(match.start())\n                    index_end.append(match.end())\n                index_first.append(len(text)-1) # 这里为了顺应后面的cut操作，通过添加一个元素，能够用index_first将text的分成若干个区间\n                new_text += text[:index_first[0]] # 这里默认是table之后的内容是表格内容，之后会加入选择的情况\n                for i in range(len(index_end)):\n                    new_text += self.cut_table_str(text[index_first[i]:index_first[i+1]],index_end[i]-index_first[i])\n                    # 上面将index_first分开的区间作为text传给cut函数，并将传入的text的table的长度传进去\n                return new_text\n            elif self.paper_table_position_type == 1:\n                index_end.append(0)\n                for match in matches:\n                    index_first.append(match.start())\n                    index_end.append(match.end())\n                for i in range(len(index_first)):\n                    new_text += self.cut_table_str_before(text[index_end[i]:index_first[i]]) if  self.cut_table_str_before(text[index_end[i]:index_first[i]]) is not None else \"\"\n                    new_text += text[index_first[i]:index_end[i+1]]\n                new_text += text[index_end[-1]:]\n                return new_text\n    def cut_table_str_before(self,text:str) -> str:\n        text = text.splitlines(keepends=True)\n        num_table_line = 0\n        table_text = \"\"\n        for index in range(len(text)):\n            if len(text[len(text)-1-index]) > self.RECOGNIZE_SENTENCE_LEN:\n                if self.is_table_line(text[len(text)-1-index]):\n                    continue\n                else:\n                    num_table_line = len(text) -index\n                    break\n        if num_table_line > len(text) - 1:\n            # print(f\"\\033[34m 表格在前,没有表格内容 \\033[0m\")\n            # print(\">\"*100)\n            # print(text)\n            # print(\"<\"*100)\n            self.logger.warning(f\"表格在前，没有表格内容 {text}\")\n            no_table_text = \"\"\n            for line in text:\n                no_table_text += line\n            return no_table_text\n        for line in text[num_table_line:]:\n            # print(\"line\")\n            # print(line)\n            table_text += line\n        \n        if self.design_table_content(table_text):\n            no_table_text = \"\"\n            for line in text[:num_table_line]:\n                no_table_text += line\n            return no_table_text\n        else:\n            # print(\"\\033[31m\\033[1m error, 切除部分是非表格内容\\033[0m\\033[0m\")\n            # print(\"-\"*100)\n            # print(f\"<<<<<<<<<<<<<<<<<>>><<<<>>>>>>>>>>>>>>>>>>>\")\n            # print(\"--------------切除内容开始--------------\")\n            # print(f\"\\033[34m{table_text}\\033[0m\")\n            # print(\"-\"*100)\n            self.logger.warning(f\"切除部分是非表格部分，停止切除{table_text}\")\n    def clean_table_context(self) -> None:\n        for index,text in enumerate(self.text):\n            self.cleaned_text[index] = self.recognize_table(text)\n            if self.debug:\n                print(self.text[index])\n    def list_to_str(self,L:List[str]) -> str :\n        output = \"\"\n        for i in L:\n            output += str(i) + '.'\n        return output[:-1]\n    def design_en_or_space_1(self,text:str) -> bool:\n        # 只有英文和空格,-,()，True\n        log = 0\n        for i in text:\n            if log == 0 or log == 1:\n                if i == ' ':\n                    log = 2\n                    continue\n                if self.is_num(i) and log == 0:\n                    log = 1\n                elif i=='.' and log == 1:\n                    log = 0\n                else:\n                    # self.logger.error(f\"{text} 在 {i} 处没通过\")\n                    return False\n            else:\n                if i>='a' and i<='z' or i>='A' and i <='Z' or i==' ' or i == '-' or i == '(' or i == ')' or self.is_num(i):\n                    log += 1\n                    pass\n                else:\n                    # self.logger.error(f\"{text} 在 {i} 处没通过\")\n                    return False\n        if log >= 5:\n            return True\n        return False\n    def design_en_or_space_2(self,text:str) -> bool:\n        # 只有英文和空格,-,()，True\n        log = 0\n        for i in text:\n            if i>='a' and i<='z' or i>='A' and i <='Z' or i==' ' or i == '-' or i == '(' or i == ')' or self.is_num(i) or \\\n                i == \":\" or i==\"−\" or i==',':\n                log += 1\n            # else:\n                # self.logger.error(f\"{text} 在 {i} 处没通过\")\n                # return False\n        if log >= 5 or log/len(text) > 0.8 :\n            return True\n        return False\n    def search_title(self):\n        # 先查找Abstract，然后找到Introduction的样式，根据Introduction的样式查找\n        # Introduction样式\n        # 1. Introduction\n        # 3. Method\n        # 3.1. Rotate the convolution kernels\n        # 3.2. Routing function\n        # 3.3. Adaptive rotated convolution module\n\n        # 3. Background\n        # 3.1 Denoising Diffusion Probabilistic Model\n        \n        # 1\n        # Introduction\n\n        # 判断逻辑，每行第一个是数字，根据标题树，判断属于兄弟节点还是父兄弟节点递归往上判断，或者给一个队列，把现在和下一个大标题的数字放进来\n        # 俩都进行判断， 然后匹配以后之后插入树节点，在第一个空格之后逐个元素判断，只能是英文字母或者空格，匹配的话，记录下来，然后打上标记（放到一行），在进行删除表格操作的时候判断标题\n        # design\n        for index_text,text in enumerate(self.text):\n            texts = text.split(\"\\n\")\n            if self.title_pattern is not None:\n                break\n            for index_i,i in enumerate(texts):\n                if \"Introduction\" in i:\n                    if self.is_num(i[0]):\n                        self.title_pattern = 1 # 1. Introduction\n                        # self.title_tree = TitleTree(value=[])\n                        # self.title_tree.sons = [TitleTree(value=[\"1\"],parent=self.title_tree)]\n                        # self.title_tree = self.title_tree.sons[0]\n                        self.logger.info(f\"paper标题模式确认，{i}\")\n                        break\n                    \n                    elif index_i != 0 and self.is_num(texts[index_i-1][0]):\n                        self.title_pattern = 2\n                        # 1\n                        # Introduction\n                        # self.title_tree.sons = [TitleTree(value=[\"1\"],parent=self.title_tree)]\n                        # self.title_tree = self.title_tree.sons[0]\n                        self.logger.info(f\"paper标题模型确认，\\n{texts[index_i-1]}\\n{i}\")\n                        break\n        if self.title_pattern is None:\n            self.logger.error(\"标题未找到\")\n    def title_queue(self,title_node:TitleTree) -> List[int]:\n        # 生成当前步的查询列表, 将可变对象赋值个一个新的变量，这个变量被认为是可变对象的别名，\n        # 会和可变对象同步改变，用copy方法能够避免这种情况\n        if title_node.value == []:\n            name = []\n        else:\n             name = title_node.value.copy()\n        output = []\n        name.append(1)\n        output.append(name) # 子树\n\n        if title_node.parent:\n            name = title_node.value.copy()\n            name[-1] += 1\n            output.append(name) # 兄弟树\n            if title_node.parent.parent:\n                name = title_node.parent.value.copy()\n                name[-1] += 1\n                output.append(name) # 父兄树\n                # print(output)\n                if title_node.parent.parent.parent:\n                    name = title_node.parent.parent.value.copy()\n                    name[-1] += 1\n                    output.append(name) # 父父兄树\n        # print(output)\n        # self.logger.info(f\"队列创建成功，{output}\")\n        return output\n\n    def title_pattern_1(self):\n        # 匹配模式为 1. Introduction\n        # 从self.text中进行标记\n        queue = []\n        log_title = 0\n        log_abstract = 0\n        # 遍历每页\n        for index,texts in enumerate(self.text):\n            texts = texts.split(\"\\n\")\n            self.text[index] = \"\"\n            # 遍历每行\n            for text in texts:\n                if log_abstract == 0 and text == \"Abstract\":\n                    self.text[index] += \"#\"*3 + \"\\n\" + text + '\\n'\n                    self.logger.success(f\"成功找到标题：{text}\")\n                    log_abstract = 1\n                    log_title = 1\n                    continue\n                if text == \"\\n\":\n                    continue\n                if queue == []:\n                    queue = self.title_queue(self.title_tree)\n                # 遍历每个可能的标题号\n                for i,lis in enumerate(queue):\n                    lis_to_str = self.list_to_str(lis)\n                    if lis_to_str in text:\n                        if self.design_en_or_space_1(text.strip(\".\")):\n                            self.text[index] += \"#\"*3 + \"\\n\" + text + \"\\n\"\n                            log_title = 1\n                            queue = []\n                            self.logger.success(f\"成功找到标题：{text}\")\n                            if i == 0:\n                                # self.logger.info(\"i==0\")\n                                if self.title_tree.value == []:\n                                    self.title_tree.sons = [TitleTree(value=[1],parent=self.title_tree)]\n                                else:\n                                    _list = self.title_tree.value.copy()\n                                    _list.append(1)\n                                    self.title_tree.sons = [TitleTree(value=_list,parent=self.title_tree)]\n                                self.title_tree = self.title_tree.sons[0]\n                                # self.logger.info(f\"end {self.title_tree.value}\")\n\n                            elif i == 1:\n                                # self.logger.info(\"i=1\")\n                                _list = self.title_tree.value.copy()\n                                _list[-1] += 1\n                                self.title_tree.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent))\n                                self.title_tree = self.title_tree.parent.sons[-1]\n                            elif i == 2:\n                                # self.logger.info(\"i=2\")\n                                _list = self.title_tree.parent.value.copy()\n                                _list[-1] += 1\n                                self.title_tree.parent.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent.parent))\n                                self.title_tree = self.title_tree.parent.parent.sons[-1]\n                            elif i == 3:\n                                # self.logger.info(\"i=3\")\n                                _list = self.title_tree.parent.parent.value.copy()\n                                _list[-1] += 1\n                                self.title_tree.parent.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent.parent.parent))\n                                self.title_tree = self.title_tree.parent.parent.parent.sons[-1]\n                            break\n                if log_title==0:\n                    self.text[index] += text + \"\\n\"\n                else:\n                    log_title = 0\n\n    def title_pattern_2(self):\n            # 匹配模式为 1. Introduction\n            # 从self.text中进行标记\n            queue = []\n            log_title = 0\n            log_abstract = 0\n            count = 0\n            # 遍历每页\n            for index,texts in enumerate(self.text):\n                texts = texts.split(\"\\n\")\n                self.text[index] = \"\"\n                # 遍历每行\n                for index_1,text in enumerate(texts):\n                    if log_abstract == 0 and text == \"Abstract\":\n                        self.text[index] += \"#\"*3 + \"\\n\" + text + \"\\n\"\n                        self.logger.success(f\"成功找到标题：{text}\")\n                        log_abstract = 1\n                        log_title = 1\n                        continue\n                    if text == \"\\n\":\n                        continue\n                    if queue == []:\n                        queue = self.title_queue(self.title_tree)\n                    # 遍历每个可能的标题号\n                    for i,lis in enumerate(queue):\n                        lis_to_str = self.list_to_str(lis)\n\n                        if lis_to_str ==  text:\n                            if index_1 != len(texts)-1: \n                                if self.design_en_or_space_2(texts[index_1+1].strip(\".\")):\n                                    self.text[index] += \"#\"*3 + \"\\n\" + text + \" \"\n                                    log_title = 1\n                                    queue = []\n                                    self.logger.success(f\"成功找到标题：{text} {texts[index_1+1]}\")\n                                    if i == 0:\n                                        # self.logger.info(\"i==0\")\n                                        if self.title_tree.value == []:\n                                            self.title_tree.sons = [TitleTree(value=[1],parent=self.title_tree)]\n                                        else:\n                                            _list = self.title_tree.value.copy()\n                                            _list.append(1)\n                                            self.title_tree.sons = [TitleTree(value=_list,parent=self.title_tree)]\n                                        self.title_tree = self.title_tree.sons[0]\n                                        # self.logger.info(f\"end {self.title_tree.value}\")\n\n                                    elif i == 1:\n                                        # self.logger.info(\"i=1\")\n                                        _list = self.title_tree.value.copy()\n                                        _list[-1] += 1\n                                        self.title_tree.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent))\n                                        self.title_tree = self.title_tree.parent.sons[-1]\n                                    elif i == 2:\n                                        # self.logger.info(\"i=2\")\n                                        _list = self.title_tree.parent.value.copy()\n                                        _list[-1] += 1\n                                        self.title_tree.parent.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent.parent))\n                                        self.title_tree = self.title_tree.parent.parent.sons[-1]\n                                    elif i == 3:\n                                        # self.logger.info(\"i=3\")\n                                        _list = self.title_tree.parent.parent.value.copy()\n                                        _list[-1] += 1\n                                        self.title_tree.parent.parent.sons.append(TitleTree(value=_list,parent=self.title_tree.parent.parent.parent))\n                                        self.title_tree = self.title_tree.parent.parent.parent.sons[-1]\n                                    break\n                            else:\n                                self.logger.warning(f\"{text} ，达到了{index+1}页的末尾\")\n                    if log_title==0:\n                        self.text[index] += text + \"\\n\"\n                    else:\n                        log_title = 0\n                    \n\n            \n\n\n            \n        \n\n    # 下面是新的函数用来将表格内容在上方的情况解决\n\n\n\ndef design_excel_content(text:str) -> bool:\n    num = 0\n    for i in text:\n        if i>='0' or i<='9':\n            num += 1\n    if num/len(text) > NUM_PERCENT:\n        return True\n    return False\ndef find_row_low_50_next(text:str) -> int:\n    \"\"\"\n    text: 从0位置开始，找到小于50且数字多的行开始。\n\n    \"\"\"\n    matches = re.finditer(r'\\A.{0,49}\\Z',text)\n    for match in matches:\n        if design_excel_content(text[match.start(),match.end()]):\n            return match.start()\n    return len(text)-1\ndef find_row_over_50_next(text:str) -> int:\n\n    \"\"\"\n    text: 从第一个小于50且数字多的行开始，找到第一个长度大于50的非数字行\n    int：返回大于50的非数字行的首字母的位置\n    \n    \"\"\"\n    # print(\"`\"*70)\n    # print(text)\n    # print(\"`\"*70)\n    # for match in re.finditer(r'(?!\\n)(.{50,})(?!\\n)', text, re.DOTALL):\n    pos = find_row_low_50_next(text)\n    for match in re.finditer(r'^(.{51,})\\n', text[pos:], re.DOTALL):\n        # 检查匹配的开始位置是否大于指定的开始位置\n        # print(match.group(0))\n        # print(match.start())\n        # print(match.end())\n        # print(\"`\"*70)\n        if not design_excel_content(match):\n            if match.start() > 0:\n                # print(\"match.start()\",match.start())\n                # 打印匹配的子字符串\n                # print(\"找到长度大于50的子字符串:\",text[match.start():match.end()])\n                # 由于我们只需要找到第一个匹配，所以在这里我们可以停止搜索\n                \n                return match.start()\n    return len(text)-1\ndef cut_middle_str(text:str,pos:int) -> str:\n    \"\"\"\n    text: 需要切除table内容部分的text\n    pos:text的table标题之后的第一个字符的位置\n    \n    \"\"\"\n    new_text = text\n    middle = find_row_over_50_next(text[pos:])\n    if design_excel_content(text[pos:pos+middle]):\n        new_text = text[:pos] +\"\\n\"+ text[pos + middle:]\n    else:\n        pass\n    # print(\"*\"*100)\n    # print(\"cut_middle_str\"+\"-\"*30)\n    # print(text[pos:pos+middle])\n    # print(\"*\"*100)\n\n    return new_text \ndef recognize_table(text:str) ->str:\n\n    \"\"\"\n    识别table的内容，并且切除table内部的内容\n    \"\"\"\n    pattern1 = r'Table (\\d+): .+?(?=\\.|\\s*\\n)'\n    pattern2 = r'Table (\\d+)\\. .+?(?=\\.|\\s*\\n)'\n\n    matches = list(re.finditer(pattern1, text))\n    if not matches:\n        matches = list(re.finditer(pattern2, text))\n    if not matches:  \n        return text\n    else:\n        index_first = []\n        index_end = []\n        new_text = \"\"\n        for match in matches:\n             index_first.append(match.start())\n             index_end.append(match.end())\n        index_first.append(len(text)-1)\n        new_text += text[:index_first[0]]\n        print(\"-=-=-=-=-=-==-=--=========-=-=-=-=--=-=--===--=\")\n        \n        print(\"len(index_end):  \",len(index_end))\n        print(\"-=-=-=-=-=-==-=--=========-=-=-=-=--=-=--===--=\")\n        for i in range(len(index_end)):\n             new_text += cut_middle_str(text[index_first[i]:index_first[i+1]],index_end[i]-index_first[i])\n        return new_text \ndef pdf_page_2text(filepath) ->List[str]:\n    \"\"\"\n    读取每页pdf，返回List[page.content]，用的是fitz（pyMuPDF）\n    \"\"\"\n    # ocr = RapidOCR()\n    doc = fitz.open(filepath)\n    resp = [\"\"]*doc.page_count\n    print(doc.page_count)\n    b_unit = tqdm.tqdm(total=doc.page_count, desc=\"RapidOCRPDFLoader context page index: 0\")\n    for i, page in enumerate(doc):\n        b_unit.set_description(\"RapidOCRPDFLoader context page index: {}\".format(i))\n        b_unit.refresh()\n        text = page.get_text(\"\")\n        # 清除latex的公式内容\n        cleaned_text = re.sub(r'<latexit[^>]*>(.*?)</latexit>', '', text, flags=re.DOTALL | re.IGNORECASE)\n        resp[i] = cleaned_text + \"\\n\"\n\n        # 识别表格内容并进行清除\n        resp[i] = recognize_table(resp[i])\n        b_unit.update(1)\n    return resp\ndef find_matches(text:str) -> int:\n    \"\"\"\n    调试函数，用来查看匹配的table\n    \"\"\"\n    log_pattern = 0\n    #  两种匹配模式\n    # 第一种：Table 3: Experimental results on the DOTA dataset compared with state-of-the-art methods.\n    # 第二种：Table 3. Experimental results on the DOTA dataset compared with state-of-the-art methods.\n    pattern1 = r'Table (\\d+): .+?(?=\\.|\\s*\\n)'\n    pattern2 = r'Table (\\d+)\\. .+?(?=\\.|\\s*\\n)'\n\n    matches = list(re.finditer(pattern1, text))\n    print(matches)\n    if not matches:\n        matches = re.finditer(pattern2, text)\n        print(\"kong\")\n        \n    print(\"=\"*100)\n    for match1 in matches:\n        print(match1.group())  # 打印匹配的文本\n        print(match1)\n    print(\"=\"*100)\n    return log_pattern \n\n        #  print(text[match.start():match.end()])\n    print(\"=\"*100)\n# if __name__ == \"__main__\": \n\n    # files = PaperCleaner(\"./5.pdf\")\n    # # files.search_title()\n    # # tree = TitleTree(value=[1])\n    # # tree1 = TitleTree(value=[1,1],parent=tree)\n    # # tree2 = TitleTree(value=[1,1,1],parent=tree1)\n    # # files.title_queue(tree2)\n    # # files.title_pattern_1()\n    # # with open(\"log.txt\",'w') as f:\n    # #     for text in files.text:\n    # #         # print(text)\n    # #         f.write(text)\n    # for i in files.chunk_paper:\n    #     print(\"-\"*10)\n    #     print(i[1])\n\n\n# 对表格的内容清除还不到位\n"}
{"type": "source_file", "path": "extra_function/__init__.py", "content": "from .PdfCleaner import PaperCleaner\nfrom .translator import OpenAIModel, Logger, PDFTranslator"}
{"type": "source_file", "path": "extra_function/logger.py", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nloguru 封装类，导入即可直接使用\n# 当前文件名 logger.py\n\"\"\"\nimport sys\nfrom functools import wraps\nimport os\nimport datetime\nimport loguru\n\n\n# 单例类的装饰器\ndef singleton_class_decorator(cls):\n    \"\"\"\n    装饰器，单例类的装饰器\n    \"\"\"\n    # 在装饰器里定义一个字典，用来存放类的实例。\n    _instance = {}\n\n    # 装饰器，被装饰的类\n    @wraps(cls)\n    def wrapper_class(*args, **kwargs):\n        # 判断，类实例不在类实例的字典里，就重新创建类实例\n        if cls not in _instance:\n            # 将新创建的类实例，存入到实例字典中\n            _instance[cls] = cls(*args, **kwargs)\n        # 如果实例字典中，存在类实例，直接取出返回类实例\n        return _instance[cls]\n\n    # 返回，装饰器中，被装饰的类函数\n    return wrapper_class\n\n\n@singleton_class_decorator # 这是一个单例类，单例类是指一个类只能实例化一个对象\nclass Logger:\n    def __init__(self, log_dir=None):\n        self.log_dir = log_dir or self.get_default_log_dir()\n        self.logger_add()\n\n    def get_default_log_dir(self):\n        # 获取当前脚本的目录\n        current_script_dir = os.path.dirname(os.path.abspath(__file__))\n        # 默认日志目录设置为当前脚本目录下的 'log' 文件夹\n        return os.path.join(current_script_dir, 'log')\n\n    def get_log_format(self):\n        # 定义日志格式，将MAC地址放在日期时间之前\n        return f\"{{time:YYYY-MM-DD HH:mm:ss.SSS}} | {{level: <8}} | {{name}}:{{function}}:{{line}} - {{message}}\"\n\n\n    def get_log_path(self):\n        # 日志文件名\n        project_log_filename = 'runtime_{}.log'.format(datetime.date.today())\n        # 日志文件路径\n        project_log_path = os.path.join(self.log_dir, project_log_filename)\n        # 返回日志路径\n        return project_log_path\n\n    def logger_add(self):\n        loguru.logger.add(\n            sink=self.get_log_path(),\n            rotation='00:00',\n            retention='1 year',\n            compression='zip',\n            encoding=\"utf-8\",\n            enqueue=True,\n            format=self.get_log_format()\n        )\n\n        # # 控制台日志，同样更新filter函数\n        # loguru.logger.add(\n        #     sink=sys.stdout,\n        #     format=self.get_log_format()\n        # )\n\n\n    @property\n    def get_logger(self):\n        return loguru.logger\n\nlogger = Logger().get_logger\n\nif __name__ == '__main__':\n    '''\n    # 实例化日志类\n    '''\n    logger = Logger().get_logger\n    logger.debug('调试代码')\n    logger.info('输出信息')\n    logger.success('输出成功')\n    logger.warning('错误警告')\n    logger.error('代码错误')\n    logger.critical('崩溃输出')\n\n    \"\"\"\n    pip install loguru \n\n    \"\"\"\n    logger.info('----原始测试----')\n"}
{"type": "source_file", "path": "extra_function/translator.py", "content": "from openai import OpenAI\nimport requests\nimport simplejson\nimport time\nimport os\nimport sys\nfrom loguru import logger\nfrom typing import List\nfrom .PdfCleaner import PaperCleaner\nfrom configs.model_config import ONLINE_LLM_MODEL\nfrom typing import Generator\nLOG_FILE = \"translation.log\"\nROTATION_TIME = \"02:00\"\nclass Logger:\n    def __init__(self, name=\"translation\", log_dir=\"logs\", debug=False):\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_file_path = os.path.join(log_dir, LOG_FILE)\n\n        # Remove default loguru handler\n        logger.remove()\n\n        # Add console handler with a specific log level\n        level = \"DEBUG\" if debug else \"INFO\"\n        logger.add(sys.stdout, level=level)\n        # Add file handler with a specific log level and timed rotation\n        logger.add(log_file_path, rotation=ROTATION_TIME, level=\"DEBUG\")\n        self.logger = logger\n\nLOG = Logger(debug=True).logger\nclient = OpenAI(api_key = ONLINE_LLM_MODEL[\"openai-api\"][\"api_key\"])\nclass OpenAIModel():\n    def __init__(self, model: str):\n        self.model = model\n    def make_request(self, prompt):\n        attempts = 0\n        while attempts < 3:\n            try:\n                if self.model == \"gpt-3.5-turbo\":\n                    response = client.chat.completions.create(model=self.model,\n                                                            #   api_key=self.api_key,\n                    messages=[\n                        {\"role\": \"user\", \"content\": prompt}\n                    ])\n                    translation = response.choices[0].message.content.strip()\n                else:\n                    response = client.completions.create(model=self.model,\n                    prompt=prompt,\n                    max_tokens=150,\n                    temperature=0)\n                    translation = response.choices[0].text.strip()\n                print(translation)\n                return translation, True\n            except Exception as e:\n                attempts += 1\n                if attempts < 3:\n                    LOG.warning(\"Rate limit reached. Waiting for 60 seconds before retrying.\")\n                    time.sleep(30)\n                else:\n                    raise Exception(\"Rate limit reached. Maximum attempts exceeded.\")\n                print(e)\n        return \"\", False\nclass PDFTranslator:\n    def __init__(self, model):\n        self.model = model\n        self.prompt = \"\"\"你是翻译论文方面的专家，以下给出相应的论文内容，请进行翻译。注意，\n        1. 每句话都要翻译.\n        2. 如果遇到你不确定的词汇，请在你翻译的这个词之后直接用小括号标注原文的这个词\n        3. 翻译要做到语言连贯，上下文内容衔接。\n        以下给出相应的论文内容，请开始进行翻译:\\n\"\"\"\n\n    def translate_pdf(self,texts: List[str]) -> Generator[str,None,None]:\n        for index,text in enumerate(texts):\n            yield f\"\\033[30m\\033[1m第 {index + 1}页的翻译\\033[0m\\033[0m\\n\" + self.model.make_request(self.prompt + text)[0] \n\n            \n"}
{"type": "source_file", "path": "release.py", "content": "import os\nimport subprocess\nimport re\n\ndef get_latest_tag():\n    output = subprocess.check_output(['git', 'tag'])\n    tags = output.decode('utf-8').split('\\n')[:-1]\n    latest_tag = sorted(tags, key=lambda t: tuple(map(int, re.match(r'v(\\d+)\\.(\\d+)\\.(\\d+)', t).groups())))[-1]\n    return latest_tag\n\ndef update_version_number(latest_tag, increment):\n    major, minor, patch = map(int, re.match(r'v(\\d+)\\.(\\d+)\\.(\\d+)', latest_tag).groups())\n    if increment == 'X':\n        major += 1\n        minor, patch = 0, 0\n    elif increment == 'Y':\n        minor += 1\n        patch = 0\n    elif increment == 'Z':\n        patch += 1\n    new_version = f\"v{major}.{minor}.{patch}\"\n    return new_version\n\ndef main():\n    print(\"当前最近的Git标签：\")\n    latest_tag = get_latest_tag()\n    print(latest_tag)\n\n    print(\"请选择要递增的版本号部分（X, Y, Z）：\")\n    increment = input().upper()\n\n    while increment not in ['X', 'Y', 'Z']:\n        print(\"输入错误，请输入X, Y或Z：\")\n        increment = input().upper()\n\n    new_version = update_version_number(latest_tag, increment)\n    print(f\"新的版本号为：{new_version}\")\n\n    print(\"确认更新版本号并推送到远程仓库？（y/n）\")\n    confirmation = input().lower()\n\n    if confirmation == 'y':\n        subprocess.run(['git', 'tag', new_version])\n        subprocess.run(['git', 'push', 'origin', new_version])\n        print(\"新版本号已创建并推送到远程仓库。\")\n    else:\n        print(\"操作已取消。\")\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "server/agent/model_contain.py", "content": "\n## 由于工具类无法传参，所以使用全局变量来传递模型和对应的知识库介绍\nclass ModelContainer:\n    def __init__(self):\n        self.MODEL = None\n        self.DATABASE = None\n\nmodel_container = ModelContainer()\n"}
{"type": "source_file", "path": "server/agent/tools/search_internet.py", "content": "import json\nfrom server.chat.search_engine_chat import search_engine_chat\nfrom configs import VECTOR_SEARCH_TOP_K, MAX_TOKENS\nimport asyncio\nfrom server.agent import model_container\nfrom pydantic import BaseModel, Field\n\nasync def search_engine_iter(query: str):\n    response = await search_engine_chat(query=query,\n                                         search_engine_name=\"bing\", # 这里切换搜索引擎\n                                         model_name=model_container.MODEL.model_name,\n                                         temperature=0.01, # Agent 搜索互联网的时候，温度设置为0.01\n                                         history=[],\n                                         top_k = VECTOR_SEARCH_TOP_K,\n                                         max_tokens= MAX_TOKENS,\n                                         prompt_name = \"default\",\n                                         stream=False)\n\n    contents = \"\"\n\n    async for data in response.body_iterator: # 这里的data是一个json字符串\n        data = json.loads(data)\n        contents = data[\"answer\"]\n        docs = data[\"docs\"]\n\n    return contents\n\ndef search_internet(query: str):\n    return asyncio.run(search_engine_iter(query))\n\nclass SearchInternetInput(BaseModel):\n    location: str = Field(description=\"Query for Internet search\")\n\n\nif __name__ == \"__main__\":\n    result = search_internet(\"今天星期几\")\n    print(\"答案:\",result)\n"}
{"type": "source_file", "path": "server/agent/tools/weather_check.py", "content": "from __future__ import annotations\n\n## 单独运行的时候需要添加\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nimport re\nimport warnings\nfrom typing import Dict\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.pydantic_v1 import Extra, root_validator\nfrom langchain.schema import BasePromptTemplate\nfrom langchain.schema.language_model import BaseLanguageModel\nimport requests\nfrom typing import List, Any, Optional\nfrom datetime import datetime\nfrom langchain.prompts import PromptTemplate\nfrom server.agent import model_container\nfrom pydantic import BaseModel, Field\n\n## 使用和风天气API查询天气\nKEY = \"ac880e5a877042809ac7ffdd19d95b0d\"\n# key长这样，这里提供了示例的key，这个key没法使用，你需要自己去注册和风天气的账号，然后在这里填入你的key\n\n\n_PROMPT_TEMPLATE = \"\"\"\n用户会提出一个关于天气的问题，你的目标是拆分出用户问题中的区，市 并按照我提供的工具回答。\n例如 用户提出的问题是: 上海浦东未来1小时天气情况？\n则 提取的市和区是: 上海 浦东\n如果用户提出的问题是: 上海未来1小时天气情况？\n则 提取的市和区是: 上海 None\n请注意以下内容:\n1. 如果你没有找到区的内容,则一定要使用 None 替代，否则程序无法运行\n2. 如果用户没有指定市 则直接返回缺少信息\n\n问题: ${{用户的问题}}\n\n你的回答格式应该按照下面的内容，请注意，格式内的```text 等标记都必须输出，这是我用来提取答案的标记。\n```text\n\n${{拆分的市和区，中间用空格隔开}}\n```\n... weathercheck(市 区)...\n```output\n\n${{提取后的答案}}\n```\n答案: ${{答案}}\n\n\n\n这是一个例子：\n问题: 上海浦东未来1小时天气情况？\n\n\n```text\n上海 浦东\n```\n...weathercheck(上海 浦东)...\n\n```output\n预报时间: 1小时后\n具体时间: 今天 18:00\n温度: 24°C\n天气: 多云\n风向: 西南风\n风速: 7级\n湿度: 88%\n降水概率: 16%\n\nAnswer: 上海浦东一小时后的天气是多云。\n\n现在，这是我的问题：\n\n问题: {question}\n\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n)\n\n\ndef get_city_info(location, adm, key):\n    base_url = 'https://geoapi.qweather.com/v2/city/lookup?'\n    params = {'location': location, 'adm': adm, 'key': key}\n    response = requests.get(base_url, params=params)\n    data = response.json()\n    return data\n\n\ndef format_weather_data(data, place):\n    hourly_forecast = data['hourly']\n    formatted_data = f\"\\n 这是查询到的关于{place}未来24小时的天气信息: \\n\"\n    for forecast in hourly_forecast:\n        # 将预报时间转换为datetime对象\n        forecast_time = datetime.strptime(forecast['fxTime'], '%Y-%m-%dT%H:%M%z')\n        # 获取预报时间的时区\n        forecast_tz = forecast_time.tzinfo\n        # 获取当前时间（使用预报时间的时区）\n        now = datetime.now(forecast_tz)\n        # 计算预报日期与当前日期的差值\n        days_diff = (forecast_time.date() - now.date()).days\n        if days_diff == 0:\n            forecast_date_str = '今天'\n        elif days_diff == 1:\n            forecast_date_str = '明天'\n        elif days_diff == 2:\n            forecast_date_str = '后天'\n        else:\n            forecast_date_str = str(days_diff) + '天后'\n        forecast_time_str = forecast_date_str + ' ' + forecast_time.strftime('%H:%M')\n        # 计算预报时间与当前时间的差值\n        time_diff = forecast_time - now\n        # 将差值转换为小时\n        hours_diff = time_diff.total_seconds() // 3600\n        if hours_diff < 1:\n            hours_diff_str = '1小时后'\n        elif hours_diff >= 24:\n            # 如果超过24小时，转换为天数\n            days_diff = hours_diff // 24\n            hours_diff_str = str(int(days_diff)) + '天'\n        else:\n            hours_diff_str = str(int(hours_diff)) + '小时'\n        # 将预报时间和当前时间的差值添加到输出中\n        formatted_data += '预报时间: ' + forecast_time_str + '  距离现在有: ' + hours_diff_str + '\\n'\n        formatted_data += '温度: ' + forecast['temp'] + '°C\\n'\n        formatted_data += '天气: ' + forecast['text'] + '\\n'\n        formatted_data += '风向: ' + forecast['windDir'] + '\\n'\n        formatted_data += '风速: ' + forecast['windSpeed'] + '级\\n'\n        formatted_data += '湿度: ' + forecast['humidity'] + '%\\n'\n        formatted_data += '降水概率: ' + forecast['pop'] + '%\\n'\n        # formatted_data += '降水量: ' + forecast['precip'] + 'mm\\n'\n        formatted_data += '\\n'\n    return formatted_data\n\n\ndef get_weather(key, location_id, place):\n    url = \"https://devapi.qweather.com/v7/weather/24h?\"\n    params = {\n        'location': location_id,\n        'key': key,\n    }\n    response = requests.get(url, params=params)\n    data = response.json()\n    return format_weather_data(data, place)\n\n\ndef split_query(query):\n    parts = query.split()\n    adm = parts[0]\n    if len(parts) == 1:\n        return adm, adm\n    location = parts[1] if parts[1] != 'None' else adm\n    return location, adm\n\n\ndef weather(query):\n    location, adm = split_query(query)\n    key = KEY\n    if key == \"\":\n        return \"请先在代码中填入和风天气API Key\"\n    try:\n        city_info = get_city_info(location=location, adm=adm, key=key)\n        location_id = city_info['location'][0]['id']\n        place = adm + \"市\" + location + \"区\"\n\n        weather_data = get_weather(key=key, location_id=location_id, place=place)\n        return weather_data + \"以上是查询到的天气信息，请你查收\\n\"\n    except KeyError:\n        try:\n            city_info = get_city_info(location=adm, adm=adm, key=key)\n            location_id = city_info['location'][0]['id']\n            place = adm + \"市\"\n            weather_data = get_weather(key=key, location_id=location_id, place=place)\n            return weather_data + \"重要提醒：用户提供的市和区中，区的信息不存在，或者出现错别字，因此该信息是关于市的天气，请你查收\\n\"\n        except KeyError:\n            return \"输入的地区不存在，无法提供天气预报\"\n\n\nclass LLMWeatherChain(Chain):\n    llm_chain: LLMChain\n    llm: Optional[BaseLanguageModel] = None\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\n    prompt: BasePromptTemplate = PROMPT\n    \"\"\"[Deprecated] Prompt to use to translate to python if necessary.\"\"\"\n    input_key: str = \"question\"  #: :meta private:\n    output_key: str = \"answer\"  #: :meta private:\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @root_validator(pre=True)\n    def raise_deprecation(cls, values: Dict) -> Dict:\n        if \"llm\" in values:\n            warnings.warn(\n                \"Directly instantiating an LLMWeatherChain with an llm is deprecated. \"\n                \"Please instantiate with llm_chain argument or using the from_llm \"\n                \"class method.\"\n            )\n            if \"llm_chain\" not in values and values[\"llm\"] is not None:\n                prompt = values.get(\"prompt\", PROMPT)\n                values[\"llm_chain\"] = LLMChain(llm=values[\"llm\"], prompt=prompt)\n        return values\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Expect input key.\n\n        :meta private:\n        \"\"\"\n        return [self.input_key]\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Expect output key.\n\n        :meta private:\n        \"\"\"\n        return [self.output_key]\n\n    def _evaluate_expression(self, expression: str) -> str:\n        try:\n            output = weather(expression)\n        except Exception as e:\n            output = \"输入的信息有误，请再次尝试\"\n        return output\n\n    def _process_llm_result(\n            self, llm_output: str, run_manager: CallbackManagerForChainRun\n    ) -> Dict[str, str]:\n\n        run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n\n        llm_output = llm_output.strip()\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\n        if text_match:\n            expression = text_match.group(1)\n            output = self._evaluate_expression(expression)\n            run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\n        else:\n            return {self.output_key: f\"输入的格式不对: {llm_output},应该输入 (市 区)的组合\"}\n        return {self.output_key: answer}\n\n    async def _aprocess_llm_result(\n            self,\n            llm_output: str,\n            run_manager: AsyncCallbackManagerForChainRun,\n    ) -> Dict[str, str]:\n        await run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n        llm_output = llm_output.strip()\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\n\n        if text_match:\n            expression = text_match.group(1)\n            output = self._evaluate_expression(expression)\n            await run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            await run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\n        else:\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\n        return {self.output_key: answer}\n\n    def _call(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        _run_manager.on_text(inputs[self.input_key])\n        llm_output = self.llm_chain.predict(\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return self._process_llm_result(llm_output, _run_manager)\n\n    async def _acall(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\n        await _run_manager.on_text(inputs[self.input_key])\n        llm_output = await self.llm_chain.apredict(\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return await self._aprocess_llm_result(llm_output, _run_manager)\n\n    @property\n    def _chain_type(self) -> str:\n        return \"llm_weather_chain\"\n\n    @classmethod\n    def from_llm(\n            cls,\n            llm: BaseLanguageModel,\n            prompt: BasePromptTemplate = PROMPT,\n            **kwargs: Any,\n    ) -> LLMWeatherChain:\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\n        return cls(llm_chain=llm_chain, **kwargs)\n\n\ndef weathercheck(query: str):\n    model = model_container.MODEL\n    llm_weather = LLMWeatherChain.from_llm(model, verbose=True, prompt=PROMPT)\n    ans = llm_weather.run(query)\n    return ans\n\n\nclass WhetherSchema(BaseModel):\n    location: str = Field(description=\"应该是一个地区的名称，用空格隔开，例如：上海 浦东，如果没有区的信息，可以只输入上海\")\n\nif __name__ == '__main__':\n    result = weathercheck(\"苏州姑苏区今晚热不热？\")\n"}
{"type": "source_file", "path": "server/agent/tools/python.py", "content": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\nfrom pydantic import BaseModel, Field\nimport streamlit as st\nimport re\nfrom PIL import Image\ndef python(string: str):\n    print(\"python函数:\\n\",string)\n    config_list = config_list_from_json(env_or_file=\"/home/root1/wcc/Langchain-Chatchat/server/agent/tools/OAI_CONFIG_LIST\")\n    user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"/home/root1/wcc/Langchain-Chatchat/coding\"})\n    code_message = {\"content\": string, \"role\": \"system\",\"language\":  \"python\"}\n    result = user_proxy.execute_code_blocks([(code_message[\"language\"], code_message[\"content\"])])\n#     image_name = re.search(r\"savefig\\('(.+?)'\\)\",string)\n#     if not image_name:\n#          image_name = re.search(r\"savefig\\(\\\"(.+?)\\\"\\)\",string)\n#     print(\"image_name: \",image_name,\"group(1)\",image_name.group(1))\n#     if image_name:\n#          image_path = \"/home/root1/wcc/Langchain-Chatchat/coding/\"+image_name.group(1)\n#     else:\n#          image_path = None\n#     try:\n#          image = Image.open(image_path)\n#          st.image(image, caption=\" LLM's Image\", use_column_width=True)\n#          string = \"streamlit run /home/root1/wcc/Langchain-Chatchat/startup.py\"\n#          code_message = {\"content\": string, \"role\": \"system\",\"language\":  \"shell\"}\n#          results = user_proxy.execute_code_blocks([(code_message[\"language\"], code_message[\"content\"])])\n    \n#     except Exception  as e:\n#          print(\"Error displaying image:\",e)\n    return result\nclass PythonInput(BaseModel):\n     string: str = Field(description=\"一个能在python解释器运行的python代码\")"}
{"type": "source_file", "path": "server/agent/tools/shell.py", "content": "# LangChain 的 Shell 工具\nfrom pydantic import BaseModel, Field\nfrom langchain.tools import ShellTool\ndef shell(query: str):\n    tool = ShellTool()\n    return tool.run(tool_input=query)\n\nclass ShellInput(BaseModel):\n    query: str = Field(description=\"一个能在Linux命令行运行的Shell命令\")"}
{"type": "source_file", "path": "server/agent/tools/wolfram.py", "content": "# Langchain 自带的 Wolfram Alpha API 封装\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom pydantic import BaseModel, Field\nwolfram_alpha_appid = \"your key\"\ndef wolfram(query: str):\n    wolfram = WolframAlphaAPIWrapper(wolfram_alpha_appid=wolfram_alpha_appid)\n    ans = wolfram.run(query)\n    return ans\n\nclass WolframInput(BaseModel):\n    location: str = Field(description=\"需要运算的具体问题\")"}
{"type": "source_file", "path": "server/agent/tools/show_image.py", "content": "from pydantic import BaseModel, Field\nimport streamlit as st\nimport os\nimport imghdr\ndef show_image(string: str): \n    def is_image_file(path):\n        if os.path.exists(path) and os.path.isfile(path):\n            try:\n                img_format = imghdr.what(path)\n                if img_format:\n                    return True\n                else:\n                    return False\n            except Exception as e:\n                print(f\"Error checking image format: {e}\")\n                return False\n        else:\n            return False\n\n    if is_image_file(string):\n        return st.image(string, caption=\"LLM's Image\", use_column_width=True)\n    else:\n        return None\nclass ShowimageInput(BaseModel):\n    string: str = Field(description=\"要展示图片文件的路径\")"}
{"type": "source_file", "path": "server/agent/tools/search_knowledgebase_simple.py", "content": "from server.chat.knowledge_base_chat import knowledge_base_chat\nfrom configs import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, MAX_TOKENS\nimport json\nimport asyncio\nfrom server.agent import model_container\n\nasync def search_knowledge_base_iter(database: str, query: str) -> str:\n    response = await knowledge_base_chat(query=query,\n                                         knowledge_base_name=database,\n                                         model_name=model_container.MODEL.model_name,\n                                         temperature=0.01,\n                                         history=[],\n                                         top_k=VECTOR_SEARCH_TOP_K,\n                                         max_tokens=MAX_TOKENS,\n                                         prompt_name=\"knowledge_base_chat\",\n                                         score_threshold=SCORE_THRESHOLD,\n                                         stream=False)\n\n    contents = \"\"\n    async for data in response.body_iterator: # 这里的data是一个json字符串\n        data = json.loads(data)\n        contents = data[\"answer\"]\n        docs = data[\"docs\"]\n    return contents\n\ndef search_knowledgebase_simple(query: str):\n    return asyncio.run(search_knowledge_base_iter(query))\n\n\nif __name__ == \"__main__\":\n    result = search_knowledgebase_simple(\"大数据男女比例\")\n    print(\"答案:\",result)"}
{"type": "source_file", "path": "server/agent/callbacks.py", "content": "from __future__ import annotations\nfrom uuid import UUID\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nimport json\nimport asyncio\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.schema import AgentFinish, AgentAction\nfrom langchain.schema.output import LLMResult\n\n\ndef dumps(obj: Dict) -> str:\n    return json.dumps(obj, ensure_ascii=False)\n\n\nclass Status:\n    start: int = 1\n    running: int = 2\n    complete: int = 3\n    agent_action: int = 4\n    agent_finish: int = 5\n    error: int = 6\n    tool_finish: int = 7\n\n\nclass CustomAsyncIteratorCallbackHandler(AsyncIteratorCallbackHandler):\n    def __init__(self):\n        super().__init__()\n        self.queue = asyncio.Queue()\n        self.done = asyncio.Event()\n        self.cur_tool = {}\n        self.out = True\n\n    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, *, run_id: UUID,\n                            parent_run_id: UUID | None = None, tags: List[str] | None = None,\n                            metadata: Dict[str, Any] | None = None, **kwargs: Any) -> None:\n\n        # 对于截断不能自理的大模型，我来帮他截断\n        # stop_words = [\"Observation:\", \"Thought\",\"\\\"\",\"（\", \"\\n\",\"\\t\"]\n        # for stop_word in stop_words:\n        #     index = input_str.find(stop_word)\n        #     if index != -1:\n        #         input_str = input_str[:index]\n        #         break\n\n        self.cur_tool = {\n            \"tool_name\": serialized[\"name\"],\n            \"input_str\": input_str,\n            \"output_str\": \"\",\n            \"status\": Status.agent_action,\n            \"run_id\": run_id.hex,\n            \"llm_token\": \"\",\n            \"final_answer\": \"\",\n            \"error\": \"\",\n        }\n        # print(\"\\nInput Str:\",self.cur_tool[\"input_str\"])\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_tool_end(self, output: str, *, run_id: UUID, parent_run_id: UUID | None = None,\n                          tags: List[str] | None = None, **kwargs: Any) -> None:\n        self.out = True ## 重置输出\n        self.cur_tool.update(\n            status=Status.tool_finish,\n            output_str=output.replace(\"Answer:\", \"\"),\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_tool_error(self, error: Exception | KeyboardInterrupt, *, run_id: UUID,\n                            parent_run_id: UUID | None = None, tags: List[str] | None = None, **kwargs: Any) -> None:\n        self.cur_tool.update(\n            status=Status.error,\n            error=str(error),\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    # async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n    #     if \"Action\" in token: ## 减少重复输出\n    #         before_action = token.split(\"Action\")[0]\n    #         self.cur_tool.update(\n    #             status=Status.running,\n    #             llm_token=before_action + \"\\n\",\n    #         )\n    #         self.queue.put_nowait(dumps(self.cur_tool))\n    #\n    #         self.out = False\n    #\n    #     if token and self.out:\n    #         self.cur_tool.update(\n    #                 status=Status.running,\n    #                 llm_token=token,\n    #         )\n    #         self.queue.put_nowait(dumps(self.cur_tool))\n    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        special_tokens = [\"Action\", \"<|observation|>\"]\n        for stoken in special_tokens:\n            if stoken in token:\n                before_action = token.split(stoken)[0]\n                self.cur_tool.update(\n                    status=Status.running,\n                    llm_token=before_action + \"\\n\",\n                )\n                self.queue.put_nowait(dumps(self.cur_tool))\n                self.out = False\n                break\n\n        if token and self.out:\n            self.cur_tool.update(\n                status=Status.running,\n                llm_token=token,\n            )\n            self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n        self.cur_tool.update(\n            status=Status.start,\n            llm_token=\"\",\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n    async def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        self.cur_tool.update(\n            status=Status.start,\n            llm_token=\"\",\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        self.cur_tool.update(\n            status=Status.complete,\n            llm_token=\"\\n\",\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_llm_error(self, error: Exception | KeyboardInterrupt, **kwargs: Any) -> None:\n        self.cur_tool.update(\n            status=Status.error,\n            error=str(error),\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n\n    async def on_agent_finish(\n            self, finish: AgentFinish, *, run_id: UUID, parent_run_id: Optional[UUID] = None,\n            tags: Optional[List[str]] = None,\n            **kwargs: Any,\n    ) -> None:\n        # 返回最终答案\n        self.cur_tool.update(\n            status=Status.agent_finish,\n            final_answer=finish.return_values[\"output\"],\n        )\n        self.queue.put_nowait(dumps(self.cur_tool))\n        self.cur_tool = {}\n"}
{"type": "source_file", "path": "server/agent/tools/__init__.py", "content": "## 导入所有的工具类\nfrom .search_knowledgebase_simple import search_knowledgebase_simple\nfrom .search_knowledgebase_once import search_knowledgebase_once, KnowledgeSearchInput\nfrom .search_knowledgebase_complex import search_knowledgebase_complex, KnowledgeSearchInput\nfrom .calculate import calculate, CalculatorInput\nfrom .weather_check import weathercheck, WhetherSchema\nfrom .shell import shell, ShellInput\nfrom .search_internet import search_internet, SearchInternetInput\nfrom .wolfram import wolfram, WolframInput\nfrom .search_youtube import search_youtube, YoutubeInput\nfrom .arxiv import arxiv, ArxivInput\nfrom .python import python, PythonInput"}
{"type": "source_file", "path": "server/agent/tools/search_youtube.py", "content": "# Langchain 自带的 YouTube 搜索工具封装\nfrom langchain.tools import YouTubeSearchTool\nfrom pydantic import BaseModel, Field\ndef search_youtube(query: str):\n    tool = YouTubeSearchTool()\n    return tool.run(tool_input=query)\n\nclass YoutubeInput(BaseModel):\n    location: str = Field(description=\"Query for Videos search\")"}
{"type": "source_file", "path": "server/agent/tools/calculate.py", "content": "from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMMathChain\nfrom server.agent import model_container\nfrom pydantic import BaseModel, Field\n\n_PROMPT_TEMPLATE = \"\"\"\n将数学问题翻译成可以使用Python的numexpr库执行的表达式。使用运行此代码的输出来回答问题。\n问题: ${{包含数学问题的问题。}}\n```text\n${{解决问题的单行数学表达式}}\n```\n...numexpr.evaluate(query)...\n```output\n${{运行代码的输出}}\n```\n答案: ${{答案}}\n\n这是两个例子：\n\n问题: 37593 * 67是多少？\n```text\n37593 * 67\n```\n...numexpr.evaluate(\"37593 * 67\")...\n```output\n2518731\n\n答案: 2518731\n\n问题: 37593的五次方根是多少？\n```text\n37593**(1/5)\n```\n...numexpr.evaluate(\"37593**(1/5)\")...\n```output\n8.222831614237718\n\n答案: 8.222831614237718\n\n\n问题: 2的平方是多少？\n```text\n2 ** 2\n```\n...numexpr.evaluate(\"2 ** 2\")...\n```output\n4\n\n答案: 4\n\n\n现在，这是我的问题：\n问题: {question}\n\"\"\"\n\nPROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n)\n\n\nclass CalculatorInput(BaseModel):\n    query: str = Field()\n\ndef calculate(query: str):\n    model = model_container.MODEL\n    llm_math = LLMMathChain.from_llm(model, verbose=True, prompt=PROMPT)\n    ans = llm_math.run(query)\n    return ans\n\nif __name__ == \"__main__\":\n    result = calculate(\"2的三次方\")\n    print(\"答案:\",result)\n\n\n\n"}
{"type": "source_file", "path": "init_database.py", "content": "import sys\nsys.path.append(\".\")\nfrom server.knowledge_base.migrate import (create_tables, reset_tables, import_from_db,\n                                           folder2db, prune_db_docs, prune_folder_files)\nfrom configs.model_config import NLTK_DATA_PATH, EMBEDDING_MODEL\nimport nltk\nnltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\nfrom datetime import datetime\nimport sys\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"please specify only one operate method once time.\")\n\n    parser.add_argument(\n        \"-r\",\n        \"--recreate-vs\",\n        action=\"store_true\",\n        help=('''\n            recreate vector store.\n            use this option if you have copied document files to the content folder, but vector store has not been populated or DEFAUL_VS_TYPE/EMBEDDING_MODEL changed.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--create-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables if not existed\")\n    )\n    parser.add_argument(\n        \"--clear-tables\",\n        action=\"store_true\",\n        help=(\"create empty tables, or drop the database tables before recreate vector stores\")\n    )\n    parser.add_argument(\n        \"--import-db\",\n        help=\"import tables from specified sqlite database\"\n    )\n    parser.add_argument(\n        \"-u\",\n        \"--update-in-db\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in database.\n            use this option if you want to recreate vectors for files exist in db and skip files exist in local folder only.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--increament\",\n        action=\"store_true\",\n        help=('''\n            update vector store for files exist in local folder and not exist in database.\n            use this option if you want to create vectors increamentally.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--prune-db\",\n        action=\"store_true\",\n        help=('''\n            delete docs in database that not existed in local folder.\n            it is used to delete database docs after user deleted some doc files in file browser\n            '''\n        )\n    )\n    parser.add_argument(\n        \"--prune-folder\",\n        action=\"store_true\",\n        help=('''\n            delete doc files in local folder that not existed in database.\n            is is used to free local disk space by delete unused doc files.\n            '''\n        )\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--kb-name\",\n        type=str,\n        nargs=\"+\",\n        default=[],\n        help=(\"specify knowledge base names to operate on. default is all folders exist in KB_ROOT_PATH.\")\n    )\n    parser.add_argument(\n        \"-e\",\n        \"--embed-model\",\n        type=str,\n        default=EMBEDDING_MODEL,\n        help=(\"specify embeddings model.\")\n    )\n\n    args = parser.parse_args()\n    start_time = datetime.now()\n\n    if args.create_tables:\n        create_tables() # confirm tables exist\n\n    if args.clear_tables:\n        reset_tables()\n        print(\"database talbes reseted\")\n\n    if args.recreate_vs:\n        create_tables()\n        print(\"recreating all vector stores\")\n        folder2db(kb_names=args.kb_name, mode=\"recreate_vs\", embed_model=args.embed_model)\n    elif args.import_db:\n        import_from_db(args.import_db)\n    elif args.update_in_db:\n        folder2db(kb_names=args.kb_name, mode=\"update_in_db\", embed_model=args.embed_model)\n    elif args.increament:\n        folder2db(kb_names=args.kb_name, mode=\"increament\", embed_model=args.embed_model)\n    elif args.prune_db:\n        prune_db_docs(args.kb_name)\n    elif args.prune_folder:\n        prune_folder_files(args.kb_name)\n\n    end_time = datetime.now()\n    print(f\"总计用时： {end_time-start_time}\")\n"}
{"type": "source_file", "path": "server/agent/custom_agent/ChatGLM3Agent.py", "content": "\"\"\"\nThis file is a modified version for ChatGLM3-6B the original ChatGLM3Agent.py file from the langchain repo.\n\"\"\"\nfrom __future__ import annotations\n\nimport yaml\nfrom langchain.agents.structured_chat.output_parser import StructuredChatOutputParser\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom typing import Any, List, Sequence, Tuple, Optional, Union\nimport os\nfrom langchain.agents.agent import Agent\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate, MessagesPlaceholder,\n)\nimport json\nimport logging\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.output_parsers import OutputFixingParser\nfrom langchain.pydantic_v1 import Field\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException, BasePromptTemplate\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom langchain.tools.base import BaseTool\n\nHUMAN_MESSAGE_TEMPLATE = \"{input}\\n\\n{agent_scratchpad}\"\nlogger = logging.getLogger(__name__)\n\n\nclass StructuredChatOutputParserWithRetries(AgentOutputParser):\n    \"\"\"Output parser with retries for the structured chat agent.\"\"\"\n\n    base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)\n    \"\"\"The base parser to use.\"\"\"\n    output_fixing_parser: Optional[OutputFixingParser] = None\n    \"\"\"The output fixing parser to use.\"\"\"\n\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        special_tokens = [\"Action:\", \"<|observation|>\"]\n        first_index = min([text.find(token) if token in text else len(text) for token in special_tokens])\n        text = text[:first_index]\n        if \"tool_call\" in text:\n            tool_name_end = text.find(\"```\")\n            tool_name = text[:tool_name_end].strip()\n            input_para = text.split(\"='\")[-1].split(\"'\")[0]\n            action_json = {\n                \"action\": tool_name,\n                \"action_input\": input_para\n            }\n        else:\n            action_json = {\n                \"action\": \"Final Answer\",\n                \"action_input\": text\n            }\n        action_str = f\"\"\"\nAction:\n```\n{json.dumps(action_json, ensure_ascii=False)}\n```\"\"\"\n        try:\n            if self.output_fixing_parser is not None:\n                parsed_obj: Union[\n                    AgentAction, AgentFinish\n                ] = self.output_fixing_parser.parse(action_str)\n            else:\n                parsed_obj = self.base_parser.parse(action_str)\n            return parsed_obj\n        except Exception as e:\n            raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\n\n    @property\n    def _type(self) -> str:\n        return \"structured_chat_ChatGLM3_6b_with_retries\"\n\n\nclass StructuredGLM3ChatAgent(Agent):\n    \"\"\"Structured Chat Agent.\"\"\"\n\n    output_parser: AgentOutputParser = Field(\n        default_factory=StructuredChatOutputParserWithRetries\n    )\n    \"\"\"Output parser for the agent.\"\"\"\n\n    @property\n    def observation_prefix(self) -> str:\n        \"\"\"Prefix to append the ChatGLM3-6B observation with.\"\"\"\n        return \"Observation:\"\n\n    @property\n    def llm_prefix(self) -> str:\n        \"\"\"Prefix to append the llm call with.\"\"\"\n        return \"Thought:\"\n\n    def _construct_scratchpad(\n            self, intermediate_steps: List[Tuple[AgentAction, str]]\n    ) -> str:\n        agent_scratchpad = super()._construct_scratchpad(intermediate_steps)\n        if not isinstance(agent_scratchpad, str):\n            raise ValueError(\"agent_scratchpad should be of type string.\")\n        if agent_scratchpad:\n            return (\n                f\"This was your previous work \"\n                f\"(but I haven't seen any of it! I only see what \"\n                f\"you return as final answer):\\n{agent_scratchpad}\"\n            )\n        else:\n            return agent_scratchpad\n\n    @classmethod\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\n        pass\n\n    @classmethod\n    def _get_default_output_parser(\n            cls, llm: Optional[BaseLanguageModel] = None, **kwargs: Any\n    ) -> AgentOutputParser:\n        return StructuredChatOutputParserWithRetries(llm=llm)\n\n    @property\n    def _stop(self) -> List[str]:\n        return [\"```<observation>\"]\n\n    @classmethod\n    def create_prompt(\n            cls,\n            tools: Sequence[BaseTool],\n            prompt: str = None,\n            input_variables: Optional[List[str]] = None,\n            memory_prompts: Optional[List[BasePromptTemplate]] = None,\n    ) -> BasePromptTemplate:\n        def tool_config_from_file(tool_name, directory=\"server/agent/tools/\"):\n            \"\"\"search tool yaml and return simplified json format\"\"\"\n            file_path = os.path.join(directory, f\"{tool_name.lower()}.yaml\")\n            try:\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    tool_config = yaml.safe_load(file)\n                    # Simplify the structure if needed\n                    simplified_config = {\n                        \"name\": tool_config.get(\"name\", \"\"),\n                        \"description\": tool_config.get(\"description\", \"\"),\n                        \"parameters\": tool_config.get(\"parameters\", {})\n                    }\n                    return simplified_config\n            except FileNotFoundError:\n                logger.error(f\"File not found: {file_path}\")\n                return None\n            except Exception as e:\n                logger.error(f\"An error occurred while reading {file_path}: {e}\")\n                return None\n\n        tools_json = []\n        tool_names = []\n        for tool in tools:\n            tool_config = tool_config_from_file(tool.name)\n            if tool_config:\n                tools_json.append(tool_config)\n                tool_names.append(tool.name)\n\n        # Format the tools for output\n        formatted_tools = \"\\n\".join([\n            f\"{tool['name']}: {tool['description']}, args: {tool['parameters']}\"\n            for tool in tools_json\n        ])\n        formatted_tools = formatted_tools.replace(\"'\", \"\\\\'\").replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\n        template = prompt.format(tool_names=tool_names,\n                                 tools=formatted_tools,\n                                 history=\"{history}\",\n                                 input=\"{input}\",\n                                 agent_scratchpad=\"{agent_scratchpad}\")\n\n        if input_variables is None:\n            input_variables = [\"input\", \"agent_scratchpad\"]\n        _memory_prompts = memory_prompts or []\n        messages = [\n            SystemMessagePromptTemplate.from_template(template),\n            *_memory_prompts,\n        ]\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)\n\n    @classmethod\n    def from_llm_and_tools(\n            cls,\n            llm: BaseLanguageModel,\n            tools: Sequence[BaseTool],\n            prompt: str = None,\n            callback_manager: Optional[BaseCallbackManager] = None,\n            output_parser: Optional[AgentOutputParser] = None,\n            human_message_template: str = HUMAN_MESSAGE_TEMPLATE,\n            input_variables: Optional[List[str]] = None,\n            memory_prompts: Optional[List[BasePromptTemplate]] = None,\n            **kwargs: Any,\n    ) -> Agent:\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\n        cls._validate_tools(tools)\n        prompt = cls.create_prompt(\n            tools,\n            prompt=prompt,\n            input_variables=input_variables,\n            memory_prompts=memory_prompts,\n        )\n        llm_chain = LLMChain(\n            llm=llm,\n            prompt=prompt,\n            callback_manager=callback_manager,\n        )\n        tool_names = [tool.name for tool in tools]\n        _output_parser = output_parser or cls._get_default_output_parser(llm=llm)\n        return cls(\n            llm_chain=llm_chain,\n            allowed_tools=tool_names,\n            output_parser=_output_parser,\n            **kwargs,\n        )\n\n    @property\n    def _agent_type(self) -> str:\n        raise ValueError\n\n\ndef initialize_glm3_agent(\n        tools: Sequence[BaseTool],\n        llm: BaseLanguageModel,\n        prompt: str = None,\n        callback_manager: Optional[BaseCallbackManager] = None,\n        memory: Optional[ConversationBufferWindowMemory] = None,\n        agent_kwargs: Optional[dict] = None,\n        *,\n        tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n) -> AgentExecutor:\n    tags_ = list(tags) if tags else []\n    agent_kwargs = agent_kwargs or {}\n    agent_obj = StructuredGLM3ChatAgent.from_llm_and_tools(\n        llm=llm,\n        tools=tools,\n        prompt=prompt,\n        callback_manager=callback_manager, **agent_kwargs\n    )\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent_obj,\n        tools=tools,\n        callback_manager=callback_manager,\n        memory=memory,\n        tags=tags_,\n        **kwargs,\n    )\n\n"}
{"type": "source_file", "path": "server/agent/__init__.py", "content": "from .model_contain import *\nfrom .callbacks import *\nfrom .custom_template import *\nfrom .tools import *"}
{"type": "source_file", "path": "server/agent/tools/arxiv.py", "content": "# LangChain 的 ArxivQueryRun 工具\nfrom pydantic import BaseModel, Field\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\ndef arxiv(query: str):\n    tool = ArxivQueryRun()\n    return tool.run(tool_input=query)\n\nclass ArxivInput(BaseModel):\n    query: str = Field(description=\"The search query title\")"}
{"type": "source_file", "path": "server/agent/tools_select.py", "content": "from langchain.tools import Tool\nfrom server.agent.tools import *\n\n## 请注意，如果你是为了使用AgentLM，在这里，你应该使用英文版本。\n#print(server.agent.tools.show_image)\ntools = [\n    # Tool.from_function(\n    #     func=calculate,\n    #     name=\"calculate\",\n    #     description=\"Useful for when you need to answer questions about simple calculations\",\n    #     args_schema=CalculatorInput,\n    # ),\n    # Tool.from_function(\n    #     func=arxiv,\n    #     name=\"arxiv\",\n    #     description=\"A wrapper around Arxiv.org for searching and retrieving scientific articles in various fields.\",\n    #     args_schema=ArxivInput,\n    # ),\n    # Tool.from_function(\n    #     func=weathercheck,\n    #     name=\"weather_check\",\n    #     description=\"\",\n    #     args_schema=WhetherSchema,\n    # ),\n    # Tool.from_function(\n    #     func=shell,\n    #     name=\"shell\",\n    #     description=\"Use Shell to execute Linux commands\",\n    #     args_schema=ShellInput,\n    #  ),\n    # Tool.from_function(\n    #     func=search_knowledgebase_complex,\n    #     name=\"search_knowledgebase_complex\",\n    #     description=\"Use Use this tool to search local knowledgebase and get information\",\n    #     args_schema=KnowledgeSearchInput,\n    # ),\n    # Tool.from_function(\n    #     func=search_internet,\n    #     name=\"search_internet\",\n    #     description=\"Use this tool to use bing search engine to search the internet\",\n    #     args_schema=SearchInternetInput,\n    # ),\n    # Tool.from_function(\n    #     func=wolfram,\n    #     name=\"Wolfram\",\n    #     description=\"Useful for when you need to calculate difficult formulas\",\n    #     args_schema=WolframInput,\n    # ),\n    # Tool.from_function(\n    #     func=search_youtube,\n    #     name=\"search_youtube\",\n    #     description=\"use this tools to search youtube videos\",\n    #     args_schema=YoutubeInput,\n    # ),\n    Tool.from_function(\n        func=python,\n        name=\"python\",\n        description=\"Use python interpreter to run python code\",\n        args_schema=PythonInput,\n    ),\n    # Tool.from_function(\n    #     func=show_image,\n    #     name=\"show_image\",\n    #     description=\"display image by passing an address\",\n    #     args_schema=ShowimageInput,\n    # ),\n]\n\ntool_names = [tool.name for tool in tools]\n"}
{"type": "source_file", "path": "server/chat/agent_chat.py", "content": "from langchain.memory import ConversationBufferWindowMemory\n\nfrom server.agent.custom_agent.ChatGLM3Agent import initialize_glm3_agent\nfrom server.agent.tools_select import tools, tool_names\nfrom server.agent.callbacks import CustomAsyncIteratorCallbackHandler, Status\nfrom langchain.agents import LLMSingleActionAgent, AgentExecutor\nfrom server.agent.custom_template import CustomOutputParser, CustomPromptTemplate\nfrom fastapi import Body\nfrom fastapi.responses import StreamingResponse\nfrom configs import LLM_MODELS, TEMPERATURE, HISTORY_LEN, Agent_MODEL\nfrom server.utils import wrap_done, get_ChatOpenAI, get_prompt_template\nfrom langchain.chains import LLMChain\nfrom typing import AsyncIterable, Optional\nimport asyncio\nfrom typing import List\nfrom server.chat.utils import History\nimport json\nfrom server.agent import model_container\nfrom server.knowledge_base.kb_service.base import get_kb_details\n\n\nasync def agent_chat(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n                     history: List[History] = Body([],\n                                                   description=\"历史对话\",\n                                                   examples=[[\n                                                       {\"role\": \"user\", \"content\": \"请使用知识库工具查询今天北京天气\"},\n                                                       {\"role\": \"assistant\",\n                                                        \"content\": \"使用天气查询工具查询到今天北京多云，10-14摄氏度，东北风2级，易感冒\"}]]\n                                                   ),\n                     stream: bool = Body(False, description=\"流式输出\"),\n                     model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                     temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                     max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                     prompt_name: str = Body(\"default\",\n                                             description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                     # top_p: float = Body(TOP_P, description=\"LLM 核采样。勿与temperature同时设置\", gt=0.0, lt=1.0),\n                     ):\n    history = [History.from_data(h) for h in history]\n\n    async def agent_chat_iterator(\n            query: str,\n            history: Optional[List[History]],\n            model_name: str = LLM_MODELS[0],\n            prompt_name: str = prompt_name,\n    ) -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = CustomAsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n        )\n\n        ## 传入全局变量来实现agent调用\n        kb_list = {x[\"kb_name\"]: x for x in get_kb_details()}\n        model_container.DATABASE = {name: details['kb_info'] for name, details in kb_list.items()}\n\n        if Agent_MODEL:\n            ## 如果有指定使用Agent模型来完成任务\n            model_agent = get_ChatOpenAI(\n                model_name=Agent_MODEL,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                callbacks=[callback],\n            )\n            model_container.MODEL = model_agent\n        else:\n            model_container.MODEL = model\n\n        prompt_template = get_prompt_template(\"agent_chat\", prompt_name)\n        prompt_template_agent = CustomPromptTemplate(\n            template=prompt_template,\n            tools=tools,\n            input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n        )\n        output_parser = CustomOutputParser()\n        llm_chain = LLMChain(llm=model, prompt=prompt_template_agent)\n        # 把history转成agent的memory\n        memory = ConversationBufferWindowMemory(k=HISTORY_LEN * 2)\n        for message in history:\n            # 检查消息的角色\n            if message.role == 'user':\n                # 添加用户消息\n                memory.chat_memory.add_user_message(message.content)\n            else:\n                # 添加AI消息\n                memory.chat_memory.add_ai_message(message.content)\n\n        if \"chatglm3\" in model_container.MODEL.model_name:\n            agent_executor = initialize_glm3_agent(\n                llm=model,\n                tools=tools,\n                callback_manager=None,\n                # Langchain Prompt is not constructed directly here, it is constructed inside the GLM3 agent.\n                prompt=prompt_template,\n                input_variables=[\"input\", \"intermediate_steps\", \"history\"],\n                memory=memory,\n                verbose=True,\n            )\n        else:\n            agent = LLMSingleActionAgent(\n                llm_chain=llm_chain,\n                output_parser=output_parser,\n                stop=[\"\\nObservation:\", \"Observation\"],\n                allowed_tools=tool_names,\n            )\n            agent_executor = AgentExecutor.from_agent_and_tools(agent=agent,\n                                                                tools=tools,\n                                                                verbose=True,\n                                                                memory=memory,\n                                                                )\n        while True:\n            try:\n                task = asyncio.create_task(wrap_done(\n                    agent_executor.acall(query, callbacks=[callback], include_run_info=True),\n                    callback.done))\n                break\n            except:\n                pass\n\n        if stream:\n            async for chunk in callback.aiter():\n                tools_use = []\n                # Use server-sent-events to stream the response\n                data = json.loads(chunk)\n                # print(\"\\n\\n\\n\\n\\n\\n\\n\\nchunk:\",chunk)\n                # print(\"\\n\\n\\n\\n\\n\\n\\n\\ndata:\",data)\n                if data[\"status\"] == Status.start or data[\"status\"] == Status.complete:\n                    continue\n                elif data[\"status\"] == Status.error:\n                    tools_use.append(\"\\n```\\n\")\n                    tools_use.append(\"工具名称: \" + data[\"tool_name\"])\n                    tools_use.append(\"工具状态: \" + \"调用失败\")\n                    tools_use.append(\"错误信息: \" + data[\"error\"])\n                    tools_use.append(\"重新开始尝试\")\n                    tools_use.append(\"\\n```\\n\")\n                    yield json.dumps({\"tools\": tools_use}, ensure_ascii=False)\n                elif data[\"status\"] == Status.tool_finish:\n                    tools_use.append(\"\\n```\\n\")\n                    tools_use.append(\"工具名称: \" + data[\"tool_name\"])\n                    tools_use.append(\"工具状态: \" + \"调用成功\")\n                    tools_use.append(\"工具输入: \" + data[\"input_str\"])\n                    tools_use.append(\"工具输出: \" + data[\"output_str\"])\n                    tools_use.append(\"\\n```\\n\")\n                    yield json.dumps({\"tools\": tools_use}, ensure_ascii=False)\n                elif data[\"status\"] == Status.agent_finish:\n                    yield json.dumps({\"final_answer\": data[\"final_answer\"]}, ensure_ascii=False)\n                else:\n                    yield json.dumps({\"answer\": data[\"llm_token\"]}, ensure_ascii=False)\n\n\n        else:\n            answer = \"\"\n            final_answer = \"\"\n            async for chunk in callback.aiter():\n                # Use server-sent-events to stream the response\n                data = json.loads(chunk)\n                if data[\"status\"] == Status.start or data[\"status\"] == Status.complete:\n                    continue\n                if data[\"status\"] == Status.error:\n                    answer += \"\\n```\\n\"\n                    answer += \"工具名称: \" + data[\"tool_name\"] + \"\\n\"\n                    answer += \"工具状态: \" + \"调用失败\" + \"\\n\"\n                    answer += \"错误信息: \" + data[\"error\"] + \"\\n\"\n                    answer += \"\\n```\\n\"\n                if data[\"status\"] == Status.tool_finish:\n                    answer += \"\\n```\\n\"\n                    answer += \"工具名称: \" + data[\"tool_name\"] + \"\\n\"\n                    answer += \"工具状态: \" + \"调用成功\" + \"\\n\"\n                    answer += \"工具输入: \" + data[\"input_str\"] + \"\\n\"\n                    answer += \"工具输出: \" + data[\"output_str\"] + \"\\n\"\n                    answer += \"\\n```\\n\"\n                if data[\"status\"] == Status.agent_finish:\n                    final_answer = data[\"final_answer\"]\n                else:\n                    answer += data[\"llm_token\"]\n\n            yield json.dumps({\"answer\": answer, \"final_answer\": final_answer}, ensure_ascii=False)\n        await task\n\n    return StreamingResponse(agent_chat_iterator(query=query,\n                                                 history=history,\n                                                 model_name=model_name,\n                                                 prompt_name=prompt_name),\n                             media_type=\"text/event-stream\")\n"}
{"type": "source_file", "path": "server/callback_handler/conversation_callback_handler.py", "content": "from typing import Any, Dict, List, Union, Optional\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import LLMResult\nfrom server.db.repository import update_message\n\n\nclass ConversationCallbackHandler(BaseCallbackHandler):\n    raise_error: bool = True\n\n    def __init__(self, conversation_id: str, message_id: str, chat_type: str, query: str):\n        self.conversation_id = conversation_id\n        self.message_id = message_id\n        self.chat_type = chat_type\n        self.query = query\n        self.start_at = None\n\n    @property\n    def always_verbose(self) -> bool:\n        \"\"\"Whether to call verbose callbacks even if verbose is False.\"\"\"\n        return True\n\n    def on_llm_start(\n            self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        # 如果想存更多信息，则prompts 也需要持久化\n        pass\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        answer = response.generations[0][0].text\n        update_message(self.message_id, answer)\n"}
{"type": "source_file", "path": "server/chat/completion.py", "content": "from fastapi import Body\nfrom fastapi.responses import StreamingResponse\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import wrap_done, get_OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, Optional\nimport asyncio\nfrom langchain.prompts import PromptTemplate\n\nfrom server.utils import get_prompt_template\n\n\nasync def completion(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n                     stream: bool = Body(False, description=\"流式输出\"),\n                     echo: bool = Body(False, description=\"除了输出之外，还回显输入\"),\n                     model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                     temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                     max_tokens: Optional[int] = Body(1024, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                     # top_p: float = Body(TOP_P, description=\"LLM 核采样。勿与temperature同时设置\", gt=0.0, lt=1.0),\n                     prompt_name: str = Body(\"default\",\n                                             description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                     ):\n\n    #todo 因ApiModelWorker 默认是按chat处理的，会对params[\"prompt\"] 解析为messages，因此ApiModelWorker 使用时需要有相应处理\n    async def completion_iterator(query: str,\n                                  model_name: str = LLM_MODELS[0],\n                                  prompt_name: str = prompt_name,\n                                  echo: bool = echo,\n                                  ) -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_OpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n            echo=echo\n        )\n\n        prompt_template = get_prompt_template(\"completion\", prompt_name)\n        prompt = PromptTemplate.from_template(prompt_template)\n        chain = LLMChain(prompt=prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"input\": query}),\n            callback.done),\n        )\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield token\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield answer\n\n        await task\n\n    return StreamingResponse(completion_iterator(query=query,\n                                                 model_name=model_name,\n                                                 prompt_name=prompt_name),\n                             media_type=\"text/event-stream\")\n"}
{"type": "source_file", "path": "server/api.py", "content": "import nltk\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom configs import VERSION\nfrom configs.model_config import NLTK_DATA_PATH\nfrom configs.server_config import OPEN_CROSS_DOMAIN\nimport argparse\nimport uvicorn\nfrom fastapi import Body\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.responses import RedirectResponse\nfrom server.chat.chat import chat\nfrom server.chat.openai_chat import openai_chat\nfrom server.chat.search_engine_chat import search_engine_chat\nfrom server.chat.completion import completion\nfrom server.chat.feedback import chat_feedback\nfrom server.embeddings_api import embed_texts_endpoint\nfrom server.llm_api import (list_running_models, list_config_models,\n                            change_llm_model, stop_llm_model,\n                            get_model_config, list_search_engines)\nfrom server.utils import (BaseResponse, ListResponse, FastAPI, MakeFastAPIOffline,\n                          get_server_configs, get_prompt_template)\nfrom typing import List, Literal\nimport shutil\nfrom fastapi.responses import JSONResponse\nfrom fastapi import File,UploadFile,Form\n\nnltk.data.path = [NLTK_DATA_PATH] + nltk.data.path\n\n\nasync def document():\n    return RedirectResponse(url=\"/docs\")\n\n\ndef create_app(run_mode: str = None):\n    app = FastAPI(\n        title=\"Langchain-Chatchat API Server\",\n        version=VERSION\n    )\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    MakeFastAPIOffline(app)\n    # Add CORS middleware to allow all origins\n    # 在config.py中设置OPEN_DOMAIN=True，允许跨域\n    # set OPEN_DOMAIN=True in config.py to allow cross-domain\n    if OPEN_CROSS_DOMAIN:\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n    mount_app_routes(app, run_mode=run_mode)\n    return app\n\n\ndef mount_app_routes(app: FastAPI, run_mode: str = None):\n    app.get(\"/\",\n            response_model=BaseResponse,\n            summary=\"swagger 文档\")(document)\n\n    # Tag: Chat\n    app.post(\"/chat/fastchat\",\n             tags=[\"Chat\"],\n             summary=\"与llm模型对话(直接与fastchat api对话)\",\n             )(openai_chat)\n\n    app.post(\"/chat/chat\",\n             tags=[\"Chat\"],\n             summary=\"与llm模型对话(通过LLMChain)\",\n             )(chat)\n\n    app.post(\"/chat/search_engine_chat\",\n             tags=[\"Chat\"],\n             summary=\"与搜索引擎对话\",\n             )(search_engine_chat)\n\n    app.post(\"/chat/feedback\",\n             tags=[\"Chat\"],\n             summary=\"返回llm模型对话评分\",\n             )(chat_feedback)\n\n    # 知识库相关接口\n    mount_knowledge_routes(app)\n    # 摘要相关接口\n    mount_filename_summary_routes(app)\n\n    # LLM模型相关接口\n    app.post(\"/llm_model/list_running_models\",\n             tags=[\"LLM Model Management\"],\n             summary=\"列出当前已加载的模型\",\n             )(list_running_models)\n\n    app.post(\"/llm_model/list_config_models\",\n             tags=[\"LLM Model Management\"],\n             summary=\"列出configs已配置的模型\",\n             )(list_config_models)\n\n    app.post(\"/llm_model/get_model_config\",\n             tags=[\"LLM Model Management\"],\n             summary=\"获取模型配置（合并后）\",\n             )(get_model_config)\n\n    app.post(\"/llm_model/stop\",\n             tags=[\"LLM Model Management\"],\n             summary=\"停止指定的LLM模型（Model Worker)\",\n             )(stop_llm_model)\n\n    app.post(\"/llm_model/change\",\n             tags=[\"LLM Model Management\"],\n             summary=\"切换指定的LLM模型（Model Worker)\",\n             )(change_llm_model)\n    # 文件相关接口\n    # @app.post(\"/uploadfile\",\n    #           tags=[\"File\"],\n    #           summary=\"上传文件，以供对话\")\n    # async def upload_file(files: List[UploadFile] = File(...)):\n    #     try:\n    #         upload_file = []\n    #         for file in files:\n    #             with open(\"~/wcc/Langchain-Chatchat/save_pdf/\"+file.filename,\"wb\") as buffer:\n    #                 shutil.copyfileobj(file.file,buffer)\n    #             upload_file.append({\"filename\":file.filename})\n    #         return JSONResponse(status_code=200, content={\"message\": \"Files upload successfully\",\"uploaded_files\":upload_file})\n    #     except Exception as e:\n    #         return JSONResponse(status_code=500,content={\"message\": f\"Failed to upload files: {str(e)}\"})\n    # 服务器相关接口\n    app.post(\"/server/configs\",\n             tags=[\"Server State\"],\n             summary=\"获取服务器原始配置信息\",\n             )(get_server_configs)\n\n    app.post(\"/server/list_search_engines\",\n             tags=[\"Server State\"],\n             summary=\"获取服务器支持的搜索引擎\",\n             )(list_search_engines)\n\n    @app.post(\"/server/get_prompt_template\",\n             tags=[\"Server State\"],\n             summary=\"获取服务区配置的 prompt 模板\")\n    def get_server_prompt_template(\n        type: Literal[\"llm_chat\", \"knowledge_base_chat\", \"search_engine_chat\", \"agent_chat\"]=Body(\"llm_chat\", description=\"模板类型，可选值：llm_chat，knowledge_base_chat，search_engine_chat，agent_chat\"),\n        name: str = Body(\"default\", description=\"模板名称\"),\n    ) -> str:\n        return get_prompt_template(type=type, name=name)\n\n    # 其它接口\n    app.post(\"/other/completion\",\n             tags=[\"Other\"],\n             summary=\"要求llm模型补全(通过LLMChain)\",\n             )(completion)\n\n    app.post(\"/other/embed_texts\",\n            tags=[\"Other\"],\n            summary=\"将文本向量化，支持本地模型和在线模型\",\n            )(embed_texts_endpoint)\n\n\ndef mount_knowledge_routes(app: FastAPI):\n    from server.chat.knowledge_base_chat import knowledge_base_chat\n    from server.chat.file_chat import upload_temp_docs, file_chat\n    from server.chat.agent_chat import agent_chat\n    from server.knowledge_base.kb_api import list_kbs, create_kb, delete_kb, list_kbs_all\n    from server.knowledge_base.kb_doc_api import (list_files, upload_docs, delete_docs,\n                                                update_docs, download_doc, recreate_vector_store,\n                                                search_docs, DocumentWithScore, update_info,list_files_all)\n    from server.file.file_upload import upload_file\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    app.post(\"/chat/knowledge_base_chat\",\n             tags=[\"Chat\"],\n             summary=\"与知识库对话\")(knowledge_base_chat)\n\n    app.post(\"/chat/file_chat\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"文件对话\"\n             )(file_chat)\n\n    app.post(\"/chat/agent_chat\",\n             tags=[\"Chat\"],\n             summary=\"与agent对话\")(agent_chat)\n    # Tag: writen by wcc\n    app.post(\"/uploadfiles/\",\n             tags=[\"File\"],\n             summary=\"上传文件用于llm_chat\")(upload_file)\n    # Tag: Knowledge Base Management\n    app.get(\"/knowledge_base/list_knowledge_bases\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=BaseResponse,\n            summary=\"获取知识库列表\")(list_kbs)\n    app.get(\"/knowledge_base/list_knowledge_bases/all\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=BaseResponse,\n            summary=\"获取知识库列表和其中详细信息\")(list_kbs_all)\n    app.post(\"/knowledge_base/create_knowledge_base\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"创建知识库\"\n             )(create_kb)\n\n    app.post(\"/knowledge_base/delete_knowledge_base\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"删除知识库\"\n             )(delete_kb)\n\n    app.get(\"/knowledge_base/list_files\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=BaseResponse,\n            summary=\"获取知识库内的文件列表\"\n            )(list_files)\n    app.get(\"/knowledge_base/list_files/all\",\n            tags=[\"Knowledge Base Management\"],\n            response_model=BaseResponse,\n            summary=\"获取知识库内的文件列表及其详细信息\"\n            )(list_files_all)\n    app.post(\"/knowledge_base/search_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=List[DocumentWithScore],\n             summary=\"搜索知识库\"\n             )(search_docs)\n\n    app.post(\"/knowledge_base/upload_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"上传文件到知识库，并/或进行向量化\"\n             )(upload_docs)\n\n    app.post(\"/knowledge_base/delete_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"删除知识库内指定文件\"\n             )(delete_docs)\n\n    app.post(\"/knowledge_base/update_info\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"更新知识库介绍\"\n             )(update_info)\n    app.post(\"/knowledge_base/update_docs\",\n             tags=[\"Knowledge Base Management\"],\n             response_model=BaseResponse,\n             summary=\"更新现有文件到知识库\"\n             )(update_docs)\n\n    app.get(\"/knowledge_base/download_doc\",\n            tags=[\"Knowledge Base Management\"],\n            summary=\"下载对应的知识文件\")(download_doc)\n\n    app.post(\"/knowledge_base/recreate_vector_store\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"根据content中文档重建向量库，流式输出处理进度。\"\n             )(recreate_vector_store)\n\n    app.post(\"/knowledge_base/upload_temp_docs\",\n             tags=[\"Knowledge Base Management\"],\n             summary=\"上传文件到临时目录，用于文件对话。\"\n             )(upload_temp_docs)\n\n\ndef mount_filename_summary_routes(app: FastAPI):\n    from server.knowledge_base.kb_summary_api import (summary_file_to_vector_store, recreate_summary_vector_store,\n                                                      summary_doc_ids_to_vector_store)\n\n    app.post(\"/knowledge_base/kb_summary_api/summary_file_to_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"单个知识库根据文件名称摘要\"\n             )(summary_file_to_vector_store)\n    app.post(\"/knowledge_base/kb_summary_api/summary_doc_ids_to_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"单个知识库根据doc_ids摘要\",\n             response_model=BaseResponse,\n             )(summary_doc_ids_to_vector_store)\n    app.post(\"/knowledge_base/kb_summary_api/recreate_summary_vector_store\",\n             tags=[\"Knowledge kb_summary_api Management\"],\n             summary=\"重建单个知识库文件摘要\"\n             )(recreate_summary_vector_store)\n\n\n\ndef run_api(host, port, **kwargs):\n    if kwargs.get(\"ssl_keyfile\") and kwargs.get(\"ssl_certfile\"):\n        uvicorn.run(app,\n                    host=host,\n                    port=port,\n                    ssl_keyfile=kwargs.get(\"ssl_keyfile\"),\n                    ssl_certfile=kwargs.get(\"ssl_certfile\"),\n                    )\n    else:\n        uvicorn.run(app, host=host, port=port)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(prog='langchain-ChatGLM',\n                                     description='About langchain-ChatGLM, local knowledge based ChatGLM with langchain'\n                                                 ' ｜ 基于本地知识库的 ChatGLM 问答')\n    parser.add_argument(\"--host\", type=str, default=\"172.26.94.25\")\n    parser.add_argument(\"--port\", type=int, default=7861)\n    parser.add_argument(\"--ssl_keyfile\", type=str)\n    parser.add_argument(\"--ssl_certfile\", type=str)\n    # 初始化消息\n    args = parser.parse_args()\n    args_dict = vars(args)\n\n    app = create_app()\n\n    run_api(host=args.host,\n            port=args.port,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            )"}
{"type": "source_file", "path": "server/agent/tools/search_knowledgebase_complex.py", "content": "from __future__ import annotations\nimport json\nimport re\nimport warnings\nfrom typing import Dict\nfrom langchain.callbacks.manager import AsyncCallbackManagerForChainRun, CallbackManagerForChainRun\nfrom langchain.chains.llm import LLMChain\nfrom langchain.pydantic_v1 import Extra, root_validator\nfrom langchain.schema import BasePromptTemplate\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom typing import List, Any, Optional\nfrom langchain.prompts import PromptTemplate\nfrom server.chat.knowledge_base_chat import knowledge_base_chat\nfrom configs import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, MAX_TOKENS\nimport asyncio\nfrom server.agent import model_container\nfrom pydantic import BaseModel, Field\n\nasync def search_knowledge_base_iter(database: str, query: str) -> str:\n    response = await knowledge_base_chat(query=query,\n                                         knowledge_base_name=database,\n                                         model_name=model_container.MODEL.model_name,\n                                         temperature=0.01,\n                                         history=[],\n                                         top_k=VECTOR_SEARCH_TOP_K,\n                                         max_tokens=MAX_TOKENS,\n                                         prompt_name=\"default\",\n                                         score_threshold=SCORE_THRESHOLD,\n                                         stream=False)\n\n    contents = \"\"\n    async for data in response.body_iterator:  # 这里的data是一个json字符串\n        data = json.loads(data)\n        contents += data[\"answer\"]\n        docs = data[\"docs\"]\n    return contents\n\n\nasync def search_knowledge_multiple(queries) -> List[str]:\n    # queries 应该是一个包含多个 (database, query) 元组的列表\n    tasks = [search_knowledge_base_iter(database, query) for database, query in queries]\n    results = await asyncio.gather(*tasks)\n    # 结合每个查询结果，并在每个查询结果前添加一个自定义的消息\n    combined_results = []\n    for (database, _), result in zip(queries, results):\n        message = f\"\\n查询到 {database} 知识库的相关信息:\\n{result}\"\n        combined_results.append(message)\n\n    return combined_results\n\n\ndef search_knowledge(queries) -> str:\n    responses = asyncio.run(search_knowledge_multiple(queries))\n    # 输出每个整合的查询结果\n    contents = \"\"\n    for response in responses:\n        contents += response + \"\\n\\n\"\n    return contents\n\n\n_PROMPT_TEMPLATE = \"\"\"\n用户会提出一个需要你查询知识库的问题，你应该对问题进行理解和拆解，并在知识库中查询相关的内容。\n\n对于每个知识库，你输出的内容应该是一个一行的字符串，这行字符串包含知识库名称和查询内容，中间用逗号隔开，不要有多余的文字和符号。你可以同时查询多个知识库，下面这个例子就是同时查询两个知识库的内容。\n\n例子:\n\nrobotic,机器人男女比例是多少\nbigdata,大数据的就业情况如何 \n\n\n这些数据库是你能访问的，冒号之前是他们的名字，冒号之后是他们的功能，你应该参考他们的功能来帮助你思考\n\n\n{database_names}\n\n你的回答格式应该按照下面的内容，请注意```text 等标记都必须输出，这是我用来提取答案的标记。\n不要输出中文的逗号，不要输出引号。\n\nQuestion: ${{用户的问题}}\n\n```text\n${{知识库名称,查询问题,不要带有任何除了,之外的符号,比如不要输出中文的逗号，不要输出引号}}\n\n```output\n数据库查询的结果\n\n现在，我们开始作答\n问题: {question}\n\"\"\"\n\nPROMPT = PromptTemplate(\n    input_variables=[\"question\", \"database_names\"],\n    template=_PROMPT_TEMPLATE,\n)\n\n\nclass LLMKnowledgeChain(LLMChain):\n    llm_chain: LLMChain\n    llm: Optional[BaseLanguageModel] = None\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\n    prompt: BasePromptTemplate = PROMPT\n    \"\"\"[Deprecated] Prompt to use to translate to python if necessary.\"\"\"\n    database_names: Dict[str, str] = None\n    input_key: str = \"question\"  #: :meta private:\n    output_key: str = \"answer\"  #: :meta private:\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @root_validator(pre=True)\n    def raise_deprecation(cls, values: Dict) -> Dict:\n        if \"llm\" in values:\n            warnings.warn(\n                \"Directly instantiating an LLMKnowledgeChain with an llm is deprecated. \"\n                \"Please instantiate with llm_chain argument or using the from_llm \"\n                \"class method.\"\n            )\n            if \"llm_chain\" not in values and values[\"llm\"] is not None:\n                prompt = values.get(\"prompt\", PROMPT)\n                values[\"llm_chain\"] = LLMChain(llm=values[\"llm\"], prompt=prompt)\n        return values\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Expect input key.\n\n        :meta private:\n        \"\"\"\n        return [self.input_key]\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Expect output key.\n\n        :meta private:\n        \"\"\"\n        return [self.output_key]\n\n    def _evaluate_expression(self, queries) -> str:\n        try:\n            output = search_knowledge(queries)\n        except Exception as e:\n            output = \"输入的信息有误或不存在知识库,错误信息如下:\\n\"\n            return output + str(e)\n        return output\n\n    def _process_llm_result(\n            self,\n            llm_output: str,\n            run_manager: CallbackManagerForChainRun\n    ) -> Dict[str, str]:\n\n        run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n\n        llm_output = llm_output.strip()\n        # text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\n        text_match = re.search(r\"```text(.*)\", llm_output, re.DOTALL)\n        if text_match:\n            expression = text_match.group(1).strip()\n            cleaned_input_str = (expression.replace(\"\\\"\", \"\").replace(\"“\", \"\").\n                                 replace(\"”\", \"\").replace(\"```\", \"\").strip())\n            lines = cleaned_input_str.split(\"\\n\")\n            # 使用逗号分割每一行，然后形成一个（数据库，查询）元组的列表\n\n            try:\n                queries = [(line.split(\",\")[0].strip(), line.split(\",\")[1].strip()) for line in lines]\n            except:\n                queries = [(line.split(\"，\")[0].strip(), line.split(\"，\")[1].strip()) for line in lines]\n            run_manager.on_text(\"知识库查询询内容:\\n\\n\" + str(queries) + \" \\n\\n\", color=\"blue\", verbose=self.verbose)\n            output = self._evaluate_expression(queries)\n            run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = llm_output.split(\"Answer:\")[-1]\n        else:\n            return {self.output_key: f\"输入的格式不对:\\n {llm_output}\"}\n        return {self.output_key: answer}\n\n    async def _aprocess_llm_result(\n            self,\n            llm_output: str,\n            run_manager: AsyncCallbackManagerForChainRun,\n    ) -> Dict[str, str]:\n        await run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n        llm_output = llm_output.strip()\n        text_match = re.search(r\"```text(.*)\", llm_output, re.DOTALL)\n        if text_match:\n\n            expression = text_match.group(1).strip()\n            cleaned_input_str = (\n                expression.replace(\"\\\"\", \"\").replace(\"“\", \"\").replace(\"”\", \"\").replace(\"```\", \"\").strip())\n            lines = cleaned_input_str.split(\"\\n\")\n            try:\n                queries = [(line.split(\",\")[0].strip(), line.split(\",\")[1].strip()) for line in lines]\n            except:\n                queries = [(line.split(\"，\")[0].strip(), line.split(\"，\")[1].strip()) for line in lines]\n            await run_manager.on_text(\"知识库查询询内容:\\n\\n\" + str(queries) + \" \\n\\n\", color=\"blue\",\n                                      verbose=self.verbose)\n\n            output = self._evaluate_expression(queries)\n            await run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            await run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\n        else:\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\n        return {self.output_key: answer}\n\n    def _call(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        _run_manager.on_text(inputs[self.input_key])\n        self.database_names = model_container.DATABASE\n        data_formatted_str = ',\\n'.join([f' \"{k}\":\"{v}\"' for k, v in self.database_names.items()])\n        llm_output = self.llm_chain.predict(\n            database_names=data_formatted_str,\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return self._process_llm_result(llm_output, _run_manager)\n\n    async def _acall(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\n        await _run_manager.on_text(inputs[self.input_key])\n        self.database_names = model_container.DATABASE\n        data_formatted_str = ',\\n'.join([f' \"{k}\":\"{v}\"' for k, v in self.database_names.items()])\n        llm_output = await self.llm_chain.apredict(\n            database_names=data_formatted_str,\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return await self._aprocess_llm_result(llm_output, inputs[self.input_key], _run_manager)\n\n    @property\n    def _chain_type(self) -> str:\n        return \"llm_knowledge_chain\"\n\n    @classmethod\n    def from_llm(\n            cls,\n            llm: BaseLanguageModel,\n            prompt: BasePromptTemplate = PROMPT,\n            **kwargs: Any,\n    ) -> LLMKnowledgeChain:\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\n        return cls(llm_chain=llm_chain, **kwargs)\n\n\ndef search_knowledgebase_complex(query: str):\n    model = model_container.MODEL\n    llm_knowledge = LLMKnowledgeChain.from_llm(model, verbose=True, prompt=PROMPT)\n    ans = llm_knowledge.run(query)\n    return ans\n\nclass KnowledgeSearchInput(BaseModel):\n    location: str = Field(description=\"The query to be searched\")\n\nif __name__ == \"__main__\":\n    result = search_knowledgebase_complex(\"机器人和大数据在代码教学上有什么区别\")\n    print(result)\n\n# 这是一个正常的切割\n#     queries = [\n#         (\"bigdata\", \"大数据专业的男女比例\"),\n#         (\"robotic\", \"机器人专业的优势\")\n#     ]\n#     result = search_knowledge(queries)\n#     print(result)\n"}
{"type": "source_file", "path": "server/agent/custom_template.py", "content": "from __future__ import annotations\nfrom langchain.agents import Tool, AgentOutputParser\nfrom langchain.prompts import StringPromptTemplate\nfrom typing import List\nfrom langchain.schema import AgentAction, AgentFinish\n\nfrom configs import SUPPORT_AGENT_MODEL\nfrom server.agent import model_container\nimport re\ndef get_action_input(input_string):\n    # 使用正则表达式匹配\"Action Input:\"之后的所有内容\n    # match = re.search(r'Action Input:\\s*(.*)', input_string, re.DOTALL)\n    match = re.search(r\"'''python(.*?)'''\", input_string, re.DOTALL)\n    if not match:\n        match = re.search(r\"'''(.*?)'''\", input_string, re.DOTALL)\n    if match:\n        # 如果找到匹配，返回匹配的组（即\"Action Input:\"之后的内容）\n        print(match.group(1))\n        return match.group(1)\n    else:\n        # 如果没有找到匹配，返回None或者一个空字符串\n        return None\n\nclass CustomPromptTemplate(StringPromptTemplate):\n    template: str\n    tools: List[Tool]\n\n    def format(self, **kwargs) -> str:\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n        kwargs[\"agent_scratchpad\"] = thoughts\n        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n        return self.template.format(**kwargs)\n\nclass CustomOutputParser(AgentOutputParser):\n    begin: bool = False\n    def __init__(self):\n        super().__init__()\n        self.begin = True\n\n    def parse(self, llm_output: str) -> AgentFinish | tuple[dict[str, str], str] | AgentAction:\n        if not any(agent in model_container.MODEL for agent in SUPPORT_AGENT_MODEL) and self.begin:\n            self.begin = False\n            stop_words = [\"Observation:\"]\n            min_index = len(llm_output)\n            for stop_word in stop_words:\n                index = llm_output.find(stop_word)\n                if index != -1 and index < min_index:\n                    min_index = index\n                llm_output = llm_output[:min_index]\n\n        if \"Final Answer:\" in llm_output:\n            self.begin = True\n            return AgentFinish(\n                return_values={\"output\": llm_output.split(\"Final Answer:\", 1)[-1].strip()},\n                log=llm_output,\n            )\n        parts = llm_output.split(\"Action:\")\n        print(\"parts\",parts)\n        if len(parts) < 2:\n            return AgentFinish(\n                #return_values={\"output\": f\"调用agent工具失败，该回答为大模型自身能力的回答:\\n\\n `{llm_output}`\"},\n                return_values={\"`{llm_output}`\"},\n                log=llm_output,\n            )\n        print('-'*50)\n        print(parts)\n        print('-'*50)\n        print(llm_output)\n        print('-'*50)\n        action = parts[1].split(\"Action Input:\")[0].strip()\n        print(action)\n        if action == \"```python\":\n            action = \"python\"\n        # if action ==\"python\":\n        #     print(\"! ! ! python ! ! !\")\n        #     action_input = get_action_input(parts[1])\n        # else:\n        action_input = parts[1].split(\"Action Input:\")[1].strip()\n        try:\n            ans = AgentAction(\n                tool=action.strip(\"'''\"),\n                tool_input=action_input.strip(\" \").strip('\"').strip(\"```python\").strip(\"```\"),\n                log=llm_output\n            )\n            print(\"\\nans:\",ans)\n            return ans\n        except:\n            return AgentFinish(\n                return_values={\"output\": f\"调用agent失败: `{llm_output}`\"},\n                log=llm_output,\n            )\n"}
{"type": "source_file", "path": "server/chat/chat.py", "content": "from fastapi import Body\nfrom fastapi.responses import StreamingResponse\nfrom configs import LLM_MODELS, TEMPERATURE\nfrom server.utils import wrap_done, get_ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable\nimport asyncio\nimport json\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom typing import List, Optional, Union\nfrom server.chat.utils import History\nfrom langchain.prompts import PromptTemplate\nfrom server.utils import get_prompt_template\nfrom server.memory.conversation_db_buffer_memory import ConversationBufferDBMemory\nfrom server.db.repository import add_message_to_db\nfrom server.callback_handler.conversation_callback_handler import ConversationCallbackHandler\n\n\nasync def chat(query: str = Body(..., description=\"用户输入\", examples=[\"恼羞成怒\"]),\n               conversation_id: str = Body(\"\", description=\"对话框ID\"),\n               history_len: int = Body(-1, description=\"从数据库中取历史消息的数量\"),\n               history: Union[int, List[History]] = Body([],\n                                                         description=\"历史对话，设为一个整数可以从数据库中读取历史消息\",\n                                                         examples=[[\n                                                             {\"role\": \"user\",\n                                                              \"content\": \"我们来玩成语接龙，我先来，生龙活虎\"},\n                                                             {\"role\": \"assistant\", \"content\": \"虎头虎脑\"}]]\n                                                         ),\n               stream: bool = Body(False, description=\"流式输出\"),\n               model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n               temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n               max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n               # top_p: float = Body(TOP_P, description=\"LLM 核采样。勿与temperature同时设置\", gt=0.0, lt=1.0),\n               prompt_name: str = Body(\"default\", description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n               ):\n    async def chat_iterator() -> AsyncIterable[str]:\n        nonlocal history, max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        callbacks = [callback]\n        memory = None\n        print(\"\\033[31mquery\\033[0m: \",query)\n        if conversation_id:\n            message_id = add_message_to_db(chat_type=\"llm_chat\", query=query, conversation_id=conversation_id)\n            # 负责保存llm response到message db\n            conversation_callback = ConversationCallbackHandler(conversation_id=conversation_id, message_id=message_id,\n                                                                chat_type=\"llm_chat\",\n                                                                query=query)\n            callbacks.append(conversation_callback)\n        print(\"\\033[31m 1 \\033[0m\")\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=callbacks,\n        )\n        print(\"\\033[31m 2 \\033[0m\")\n\n        if history: # 优先使用前端传入的历史消息\n            history = [History.from_data(h) for h in history]\n            prompt_template = get_prompt_template(\"llm_chat\", prompt_name)\n            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n            chat_prompt = ChatPromptTemplate.from_messages(\n                [i.to_msg_template() for i in history] + [input_msg])\n        elif conversation_id and history_len > 0: # 前端要求从数据库取历史消息\n            # 使用memory 时必须 prompt 必须含有memory.memory_key 对应的变量\n            prompt = get_prompt_template(\"llm_chat\", \"with_history\")\n            chat_prompt = PromptTemplate.from_template(prompt)\n            # 根据conversation_id 获取message 列表进而拼凑 memory\n            memory = ConversationBufferDBMemory(conversation_id=conversation_id,\n                                                llm=model,\n                                                message_limit=history_len)\n        else:\n            prompt_template = get_prompt_template(\"llm_chat\", prompt_name)\n            input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n            chat_prompt = ChatPromptTemplate.from_messages([input_msg])\n            if prompt_name == \"summary_paper\" or prompt_name == \"文献综述\":\n                chat_prompt = ChatPromptTemplate.from_messages([prompt_template+query.replace(\"{\",\"{{\").replace(\"}\",\"}}\")])\n        print(\"\\033[31mchat_prompt:\\033[0m\\n\",chat_prompt)\n        print(\"\\033[memory:\\033[0m\\n\",memory)\n\n        chain = LLMChain(prompt=chat_prompt, llm=model, memory=memory)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"input\": query}),\n            callback.done),\n        )\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps(\n                    {\"text\": token, \"message_id\": message_id},\n                    ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps(\n                {\"text\": answer, \"message_id\": message_id},\n                ensure_ascii=False)\n\n        await task\n\n    return StreamingResponse(chat_iterator(), media_type=\"text/event-stream\")\n"}
{"type": "source_file", "path": "server/chat/__init__.py", "content": ""}
{"type": "source_file", "path": "server/api_allinone_stale.py", "content": "\"\"\"Usage\n调用默认模型：\npython server/api_allinone.py\n\n加载多个非默认模型：\npython server/api_allinone.py --model-path-address model1@host1@port1 model2@host2@port2 \n\n多卡启动：\npython server/api_allinone.py --model-path-address model@host@port --num-gpus 2 --gpus 0,1 --max-gpu-memory 10GiB\n\n\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(__file__))\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom llm_api_stale import launch_all, parser, controller_args, worker_args, server_args\nfrom api import create_app\nimport uvicorn\n\nparser.add_argument(\"--api-host\", type=str, default=\"0.0.0.0\")\nparser.add_argument(\"--api-port\", type=int, default=7861)\nparser.add_argument(\"--ssl_keyfile\", type=str)\nparser.add_argument(\"--ssl_certfile\", type=str)\n\napi_args = [\"api-host\", \"api-port\", \"ssl_keyfile\", \"ssl_certfile\"]\n\n\ndef run_api(host, port, **kwargs):\n    app = create_app()\n    if kwargs.get(\"ssl_keyfile\") and kwargs.get(\"ssl_certfile\"):\n        uvicorn.run(app,\n                    host=host,\n                    port=port,\n                    ssl_keyfile=kwargs.get(\"ssl_keyfile\"),\n                    ssl_certfile=kwargs.get(\"ssl_certfile\"),\n                    )\n    else:\n        uvicorn.run(app, host=host, port=port)\n\n\nif __name__ == \"__main__\":\n    print(\"Luanching api_allinone，it would take a while, please be patient...\")\n    print(\"正在启动api_allinone，LLM服务启动约3-10分钟，请耐心等待...\")\n    # 初始化消息\n    args = parser.parse_args()\n    args_dict = vars(args)\n    launch_all(args=args, controller_args=controller_args, worker_args=worker_args, server_args=server_args)\n    run_api(\n        host=args.api_host,\n        port=args.api_port,\n        ssl_keyfile=args.ssl_keyfile,\n        ssl_certfile=args.ssl_certfile,\n    )\n    print(\"Luanching api_allinone done.\")\n    print(\"api_allinone启动完毕.\")\n"}
{"type": "source_file", "path": "server/chat/feedback.py", "content": "from fastapi import Body\nfrom configs import logger, log_verbose\nfrom server.utils import BaseResponse\nfrom server.db.repository import feedback_message_to_db\n\ndef chat_feedback(message_id: str = Body(\"\", max_length=32, description=\"聊天记录id\"),\n            score: int = Body(0, max=100, description=\"用户评分，满分100，越大表示评价越高\"),\n            reason: str = Body(\"\", description=\"用户评分理由，比如不符合事实等\")\n            ):\n    try:\n        feedback_message_to_db(message_id, score, reason)\n    except Exception as e:\n        msg = f\"反馈聊天记录出错： {e}\"\n        logger.error(f'{e.__class__.__name__}: {msg}',\n                     exc_info=e if log_verbose else None)\n        return BaseResponse(code=500, msg=msg)\n\n    return BaseResponse(code=200, msg=f\"已反馈聊天记录 {message_id}\")\n"}
{"type": "source_file", "path": "server/agent/tools/search_knowledgebase_once.py", "content": "from __future__ import annotations\nimport re\nimport warnings\nfrom typing import Dict\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.pydantic_v1 import Extra, root_validator\nfrom langchain.schema import BasePromptTemplate\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom typing import List, Any, Optional\nfrom langchain.prompts import PromptTemplate\nimport sys\nimport os\nimport json\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom server.chat.knowledge_base_chat import knowledge_base_chat\nfrom configs import VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, MAX_TOKENS\n\nimport asyncio\nfrom server.agent import model_container\nfrom pydantic import BaseModel, Field\n\nasync def search_knowledge_base_iter(database: str, query: str):\n    response = await knowledge_base_chat(query=query,\n                                         knowledge_base_name=database,\n                                         model_name=model_container.MODEL.model_name,\n                                         temperature=0.01,\n                                         history=[],\n                                         top_k=VECTOR_SEARCH_TOP_K,\n                                         max_tokens=MAX_TOKENS,\n                                         prompt_name=\"knowledge_base_chat\",\n                                         score_threshold=SCORE_THRESHOLD,\n                                         stream=False)\n\n    contents = \"\"\n    async for data in response.body_iterator:  # 这里的data是一个json字符串\n        data = json.loads(data)\n        contents += data[\"answer\"]\n        docs = data[\"docs\"]\n    return contents\n\n\n_PROMPT_TEMPLATE = \"\"\"\n用户会提出一个需要你查询知识库的问题，你应该按照我提供的思想进行思考\nQuestion: ${{用户的问题}}\n这些数据库是你能访问的，冒号之前是他们的名字，冒号之后是他们的功能：\n\n{database_names}\n\n你的回答格式应该按照下面的内容，请注意，格式内的```text 等标记都必须输出，这是我用来提取答案的标记。\n```text\n${{知识库的名称}}\n```\n```output\n数据库查询的结果\n```\n答案: ${{答案}}\n\n现在，这是我的问题：\n问题: {question}\n\n\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"question\", \"database_names\"],\n    template=_PROMPT_TEMPLATE,\n)\n\n\nclass LLMKnowledgeChain(LLMChain):\n    llm_chain: LLMChain\n    llm: Optional[BaseLanguageModel] = None\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\n    prompt: BasePromptTemplate = PROMPT\n    \"\"\"[Deprecated] Prompt to use to translate to python if necessary.\"\"\"\n    database_names: Dict[str, str] = model_container.DATABASE\n    input_key: str = \"question\"  #: :meta private:\n    output_key: str = \"answer\"  #: :meta private:\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @root_validator(pre=True)\n    def raise_deprecation(cls, values: Dict) -> Dict:\n        if \"llm\" in values:\n            warnings.warn(\n                \"Directly instantiating an LLMKnowledgeChain with an llm is deprecated. \"\n                \"Please instantiate with llm_chain argument or using the from_llm \"\n                \"class method.\"\n            )\n            if \"llm_chain\" not in values and values[\"llm\"] is not None:\n                prompt = values.get(\"prompt\", PROMPT)\n                values[\"llm_chain\"] = LLMChain(llm=values[\"llm\"], prompt=prompt)\n        return values\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Expect input key.\n\n        :meta private:\n        \"\"\"\n        return [self.input_key]\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Expect output key.\n\n        :meta private:\n        \"\"\"\n        return [self.output_key]\n\n    def _evaluate_expression(self, dataset, query) -> str:\n        try:\n            output = asyncio.run(search_knowledge_base_iter(dataset, query))\n        except Exception as e:\n            output = \"输入的信息有误或不存在知识库\"\n            return output\n        return output\n\n    def _process_llm_result(\n            self,\n            llm_output: str,\n            llm_input: str,\n            run_manager: CallbackManagerForChainRun\n    ) -> Dict[str, str]:\n\n        run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n\n        llm_output = llm_output.strip()\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\n        if text_match:\n            database = text_match.group(1).strip()\n            output = self._evaluate_expression(database, llm_input)\n            run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\n        else:\n            return {self.output_key: f\"输入的格式不对: {llm_output}\"}\n        return {self.output_key: answer}\n\n    async def _aprocess_llm_result(\n            self,\n            llm_output: str,\n            run_manager: AsyncCallbackManagerForChainRun,\n    ) -> Dict[str, str]:\n        await run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\n        llm_output = llm_output.strip()\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\n        if text_match:\n            expression = text_match.group(1)\n            output = self._evaluate_expression(expression)\n            await run_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n            await run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n            answer = \"Answer: \" + output\n        elif llm_output.startswith(\"Answer:\"):\n            answer = llm_output\n        elif \"Answer:\" in llm_output:\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\n        else:\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\n        return {self.output_key: answer}\n\n    def _call(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        _run_manager.on_text(inputs[self.input_key])\n        data_formatted_str = ',\\n'.join([f' \"{k}\":\"{v}\"' for k, v in self.database_names.items()])\n        llm_output = self.llm_chain.predict(\n            database_names=data_formatted_str,\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return self._process_llm_result(llm_output, inputs[self.input_key], _run_manager)\n\n    async def _acall(\n            self,\n            inputs: Dict[str, str],\n            run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\n        await _run_manager.on_text(inputs[self.input_key])\n        data_formatted_str = ',\\n'.join([f' \"{k}\":\"{v}\"' for k, v in self.database_names.items()])\n        llm_output = await self.llm_chain.apredict(\n            database_names=data_formatted_str,\n            question=inputs[self.input_key],\n            stop=[\"```output\"],\n            callbacks=_run_manager.get_child(),\n        )\n        return await self._aprocess_llm_result(llm_output, inputs[self.input_key], _run_manager)\n\n    @property\n    def _chain_type(self) -> str:\n        return \"llm_knowledge_chain\"\n\n    @classmethod\n    def from_llm(\n            cls,\n            llm: BaseLanguageModel,\n            prompt: BasePromptTemplate = PROMPT,\n            **kwargs: Any,\n    ) -> LLMKnowledgeChain:\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\n        return cls(llm_chain=llm_chain, **kwargs)\n\n\ndef search_knowledgebase_once(query: str):\n    model = model_container.MODEL\n    llm_knowledge = LLMKnowledgeChain.from_llm(model, verbose=True, prompt=PROMPT)\n    ans = llm_knowledge.run(query)\n    return ans\n\n\nclass KnowledgeSearchInput(BaseModel):\n    location: str = Field(description=\"The query to be searched\")\n\n\nif __name__ == \"__main__\":\n    result = search_knowledgebase_once(\"大数据的男女比例\")\n    print(result)\n"}
{"type": "source_file", "path": "server/chat/file_chat.py", "content": "from fastapi import Body, File, Form, UploadFile\nfrom fastapi.responses import StreamingResponse\nfrom configs import (LLM_MODELS, VECTOR_SEARCH_TOP_K, SCORE_THRESHOLD, TEMPERATURE,\n                     CHUNK_SIZE, OVERLAP_SIZE, ZH_TITLE_ENHANCE)\nfrom server.utils import (wrap_done, get_ChatOpenAI,\n                        BaseResponse, get_prompt_template, get_temp_dir, run_in_thread_pool)\nfrom server.knowledge_base.kb_cache.faiss_cache import memo_faiss_pool\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks import AsyncIteratorCallbackHandler\nfrom typing import AsyncIterable, List, Optional\nimport asyncio\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom server.chat.utils import History\nfrom server.knowledge_base.kb_service.base import EmbeddingsFunAdapter\nfrom server.knowledge_base.utils import KnowledgeFile\nimport json\nimport os\nfrom pathlib import Path\n\n\ndef _parse_files_in_thread(\n    files: List[UploadFile],\n    dir: str,\n    zh_title_enhance: bool,\n    chunk_size: int,\n    chunk_overlap: int,\n):\n    \"\"\"\n    通过多线程将上传的文件保存到对应目录内。\n    生成器返回保存结果：[success or error, filename, msg, docs]\n    \"\"\"\n    def parse_file(file: UploadFile) -> dict:\n        '''\n        保存单个文件。\n        '''\n        try:\n            filename = file.filename\n            file_path = os.path.join(dir, filename)\n            file_content = file.file.read()  # 读取上传文件的内容\n            print(\"file_content\",\"-\"*100)\n            #print(file_content)\n            if not os.path.isdir(os.path.dirname(file_path)):\n                os.makedirs(os.path.dirname(file_path))\n            with open(file_path, \"wb\") as f:\n                f.write(file_content)\n            kb_file = KnowledgeFile(filename=filename, knowledge_base_name=\"temp\")\n            kb_file.filepath = file_path\n            docs = kb_file.file2text(zh_title_enhance=zh_title_enhance,\n                                     chunk_size=chunk_size,\n                                     chunk_overlap=chunk_overlap)\n            return True, filename, f\"成功上传文件 {filename}\", docs\n        except Exception as e:\n            msg = f\"{filename} 文件上传失败，报错信息为: {e}\"\n            return False, filename, msg, []\n\n    params = [{\"file\": file} for file in files]\n    for result in run_in_thread_pool(parse_file, params=params):\n        yield result\n\n\ndef upload_temp_docs(\n    files: List[UploadFile] = File(..., description=\"上传文件，支持多文件\"),\n    prev_id: str = Form(None, description=\"前知识库ID\"),\n    chunk_size: int = Form(CHUNK_SIZE, description=\"知识库中单段文本最大长度\"),\n    chunk_overlap: int = Form(OVERLAP_SIZE, description=\"知识库中相邻文本重合长度\"),\n    zh_title_enhance: bool = Form(ZH_TITLE_ENHANCE, description=\"是否开启中文标题加强\"),\n) -> BaseResponse:\n    '''\n    将文件保存到临时目录，并进行向量化。\n    返回临时目录名称作为ID，同时也是临时向量库的ID。\n    '''\n    if prev_id is not None:\n        memo_faiss_pool.pop(prev_id)\n\n    failed_files = []\n    documents = []\n    path, id = get_temp_dir(prev_id)\n    for success, file, msg, docs in _parse_files_in_thread(files=files,\n                                                        dir=path,\n                                                        zh_title_enhance=zh_title_enhance,\n                                                        chunk_size=chunk_size,\n                                                        chunk_overlap=chunk_overlap):\n        if success:\n            documents += docs\n        else:\n            failed_files.append({file: msg})\n\n    with memo_faiss_pool.load_vector_store(id).acquire() as vs:\n        vs.add_documents(documents)\n    return BaseResponse(data={\"id\": id, \"failed_files\": failed_files})\n\n\nasync def file_chat(query: str = Body(..., description=\"用户输入\", examples=[\"你好\"]),\n                    knowledge_id: str = Body(..., description=\"临时知识库ID\"),\n                    top_k: int = Body(VECTOR_SEARCH_TOP_K, description=\"匹配向量数\"),\n                    score_threshold: float = Body(SCORE_THRESHOLD, description=\"知识库匹配相关度阈值，取值范围在0-1之间，SCORE越小，相关度越高，取到1相当于不筛选，建议设置在0.5左右\", ge=0, le=2),\n                    history: List[History] = Body([],\n                                                description=\"历史对话\",\n                                                examples=[[\n                                                    {\"role\": \"user\",\n                                                    \"content\": \"我们来玩成语接龙，我先来，生龙活虎\"},\n                                                    {\"role\": \"assistant\",\n                                                    \"content\": \"虎头虎脑\"}]]\n                                                ),\n                    stream: bool = Body(False, description=\"流式输出\"),\n                    model_name: str = Body(LLM_MODELS[0], description=\"LLM 模型名称。\"),\n                    temperature: float = Body(TEMPERATURE, description=\"LLM 采样温度\", ge=0.0, le=1.0),\n                    max_tokens: Optional[int] = Body(None, description=\"限制LLM生成Token数量，默认None代表模型最大值\"),\n                    prompt_name: str = Body(\"default\", description=\"使用的prompt模板名称(在configs/prompt_config.py中配置)\"),\n                ):\n    if knowledge_id not in memo_faiss_pool.keys():\n        return BaseResponse(code=404, msg=f\"未找到临时知识库 {knowledge_id}，请先上传文件\")\n\n    history = [History.from_data(h) for h in history]\n\n    async def knowledge_base_chat_iterator() -> AsyncIterable[str]:\n        nonlocal max_tokens\n        callback = AsyncIteratorCallbackHandler()\n        if isinstance(max_tokens, int) and max_tokens <= 0:\n            max_tokens = None\n\n        model = get_ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            callbacks=[callback],\n        )\n        embed_func = EmbeddingsFunAdapter()\n        embeddings = embed_func.embed_query(query)\n        with memo_faiss_pool.acquire(knowledge_id) as vs:\n            docs = vs.similarity_search_with_score_by_vector(embeddings, k=top_k, score_threshold=score_threshold)\n            docs = [x[0] for x in docs]\n\n        context = \"\\n\".join([doc.page_content for doc in docs])\n        if len(docs) == 0: ## 如果没有找到相关文档，使用Empty模板\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", \"empty\")\n        else:\n            prompt_template = get_prompt_template(\"knowledge_base_chat\", prompt_name)\n        input_msg = History(role=\"user\", content=prompt_template).to_msg_template(False)\n        chat_prompt = ChatPromptTemplate.from_messages(\n            [i.to_msg_template() for i in history] + [input_msg])\n\n        chain = LLMChain(prompt=chat_prompt, llm=model)\n\n        # Begin a task that runs in the background.\n        task = asyncio.create_task(wrap_done(\n            chain.acall({\"context\": context, \"question\": query}),\n            callback.done),\n        )\n\n        source_documents = []\n        for inum, doc in enumerate(docs):\n            filename = doc.metadata.get(\"source\")\n            text = f\"\"\"出处 [{inum + 1}] [{filename}] \\n\\n{doc.page_content}\\n\\n\"\"\"\n            source_documents.append(text)\n\n        if len(source_documents) == 0: # 没有找到相关文档\n            source_documents.append(f\"\"\"<span style='color:red'>未找到相关文档,该回答为大模型自身能力解答！</span>\"\"\")\n\n        if stream:\n            async for token in callback.aiter():\n                # Use server-sent-events to stream the response\n                yield json.dumps({\"answer\": token}, ensure_ascii=False)\n            yield json.dumps({\"docs\": source_documents}, ensure_ascii=False)\n        else:\n            answer = \"\"\n            async for token in callback.aiter():\n                answer += token\n            yield json.dumps({\"answer\": answer,\n                              \"docs\": source_documents},\n                             ensure_ascii=False)\n        await task\n\n    return StreamingResponse(knowledge_base_chat_iterator(), media_type=\"text/event-stream\")\n"}
