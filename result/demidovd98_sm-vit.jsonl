{"repo_info": {"repo_name": "sm-vit", "repo_owner": "demidovd98", "repo_url": "https://github.com/demidovd98/sm-vit"}}
{"type": "source_file", "path": "U2Net/data_loader.py", "content": "# data loader\nfrom __future__ import print_function, division\nimport glob\nimport torch\nfrom skimage import io, transform, color\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\n\n\n#==========================dataset load==========================\nclass RescaleT(object):\n\n\tdef __init__(self,output_size):\n\t\tassert isinstance(output_size,(int,tuple))\n\t\tself.output_size = output_size\n\n\tdef __call__(self,sample):\n\t\timidx, image, label = sample['imidx'], sample['image'],sample['label']\n\n\t\th, w = image.shape[:2]\n\n\t\tif isinstance(self.output_size,int):\n\t\t\tif h > w:\n\t\t\t\tnew_h, new_w = self.output_size*h/w,self.output_size\n\t\t\telse:\n\t\t\t\tnew_h, new_w = self.output_size,self.output_size*w/h\n\t\telse:\n\t\t\tnew_h, new_w = self.output_size\n\n\t\tnew_h, new_w = int(new_h), int(new_w)\n\n\t\t# # #resize the image to new_h x new_w and convert image from range [0,255] to [0,1]\n\t\t# # img = transform.resize(image,(new_h,new_w),mode='constant')\n\t\t# # lbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True)\n\n\t\t# img = transform.resize(image,(self.output_size,self.output_size),mode='constant')\n\t\t# lbl = transform.resize(label,(self.output_size,self.output_size),mode='constant', order=0, preserve_range=True)\n\t\timg = transform.resize(image,(self.output_size,self.output_size),mode='constant', anti_aliasing=True, anti_aliasing_sigma=None)\n\t\tlbl = transform.resize(label,(self.output_size,self.output_size),mode='constant', order=0, preserve_range=True, anti_aliasing=True, anti_aliasing_sigma=None)\n\n\t\treturn {'imidx':imidx, 'image':img,'label':lbl}\n\n\nclass Rescale(object):\n\n\tdef __init__(self,output_size):\n\t\tassert isinstance(output_size,(int,tuple))\n\t\tself.output_size = output_size\n\n\tdef __call__(self,sample):\n\t\timidx, image, label = sample['imidx'], sample['image'],sample['label']\n\n\t\tif random.random() >= 0.5:\n\t\t\timage = image[::-1]\n\t\t\tlabel = label[::-1]\n\n\t\th, w = image.shape[:2]\n\n\t\tif isinstance(self.output_size,int):\n\t\t\tif h > w:\n\t\t\t\tnew_h, new_w = self.output_size*h/w,self.output_size\n\t\t\telse:\n\t\t\t\tnew_h, new_w = self.output_size,self.output_size*w/h\n\t\telse:\n\t\t\tnew_h, new_w = self.output_size\n\n\t\tnew_h, new_w = int(new_h), int(new_w)\n\n\t\t# #resize the image to new_h x new_w and convert image from range [0,255] to [0,1]\n\t\t# img = transform.resize(image,(new_h,new_w),mode='constant')\n\t\t# lbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True)\n\t\timg = transform.resize(image,(new_h,new_w),mode='constant', anti_aliasing=True, anti_aliasing_sigma=None)\n\t\tlbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True, anti_aliasing=True, anti_aliasing_sigma=None)\n\n\t\treturn {'imidx':imidx, 'image':img,'label':lbl}\n\n\nclass RandomCrop(object):\n\n\tdef __init__(self,output_size):\n\t\tassert isinstance(output_size, (int, tuple))\n\t\tif isinstance(output_size, int):\n\t\t\tself.output_size = (output_size, output_size)\n\t\telse:\n\t\t\tassert len(output_size) == 2\n\t\t\tself.output_size = output_size\n\n\tdef __call__(self,sample):\n\t\timidx, image, label = sample['imidx'], sample['image'], sample['label']\n\n\t\tif random.random() >= 0.5:\n\t\t\timage = image[::-1]\n\t\t\tlabel = label[::-1]\n\n\t\th, w = image.shape[:2]\n\t\tnew_h, new_w = self.output_size\n\n\t\ttop = np.random.randint(0, h - new_h)\n\t\tleft = np.random.randint(0, w - new_w)\n\n\t\timage = image[top: top + new_h, left: left + new_w]\n\t\tlabel = label[top: top + new_h, left: left + new_w]\n\n\t\treturn {'imidx':imidx,'image':image, 'label':label}\n\n\nclass ToTensor(object):\n\t\"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n\tdef __call__(self, sample):\n\n\t\timidx, image, label = sample['imidx'], sample['image'], sample['label']\n\n\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n\t\ttmpLbl = np.zeros(label.shape)\n\n\t\timage = image/np.max(image)\n\t\tif(np.max(label)<1e-6):\n\t\t\tlabel = label\n\t\telse:\n\t\t\tlabel = label/np.max(label)\n\n\t\tif image.shape[2]==1:\n\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,1] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229\n\t\telse:\n\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n\t\t\ttmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n\n\t\ttmpLbl[:,:,0] = label[:,:,0]\n\n\t\ttmpImg = tmpImg.transpose((2, 0, 1))\n\t\ttmpLbl = label.transpose((2, 0, 1))\n\n\t\treturn {'imidx':torch.from_numpy(imidx), 'image': torch.from_numpy(tmpImg), 'label': torch.from_numpy(tmpLbl)}\n\n\nclass ToTensorLab(object):\n\t\"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\tdef __init__(self,flag=0):\n\t\tself.flag = flag\n\n\tdef __call__(self, sample):\n\n\t\timidx, image, label =sample['imidx'], sample['image'], sample['label']\n\n\t\ttmpLbl = np.zeros(label.shape)\n\n\t\tif(np.max(label)<1e-6):\n\t\t\tlabel = label\n\t\telse:\n\t\t\tlabel = label/np.max(label)\n\n\t\t# change the color space\n\t\tif self.flag == 2: # with rgb and Lab colors\n\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],6))\n\t\t\ttmpImgt = np.zeros((image.shape[0],image.shape[1],3))\n\t\t\tif image.shape[2]==1:\n\t\t\t\ttmpImgt[:,:,0] = image[:,:,0]\n\t\t\t\ttmpImgt[:,:,1] = image[:,:,0]\n\t\t\t\ttmpImgt[:,:,2] = image[:,:,0]\n\t\t\telse:\n\t\t\t\ttmpImgt = image\n\t\t\ttmpImgtl = color.rgb2lab(tmpImgt)\n\n\t\t\t# nomalize image to range [0,1]\n\t\t\ttmpImg[:,:,0] = (tmpImgt[:,:,0]-np.min(tmpImgt[:,:,0]))/(np.max(tmpImgt[:,:,0])-np.min(tmpImgt[:,:,0]))\n\t\t\ttmpImg[:,:,1] = (tmpImgt[:,:,1]-np.min(tmpImgt[:,:,1]))/(np.max(tmpImgt[:,:,1])-np.min(tmpImgt[:,:,1]))\n\t\t\ttmpImg[:,:,2] = (tmpImgt[:,:,2]-np.min(tmpImgt[:,:,2]))/(np.max(tmpImgt[:,:,2])-np.min(tmpImgt[:,:,2]))\n\t\t\ttmpImg[:,:,3] = (tmpImgtl[:,:,0]-np.min(tmpImgtl[:,:,0]))/(np.max(tmpImgtl[:,:,0])-np.min(tmpImgtl[:,:,0]))\n\t\t\ttmpImg[:,:,4] = (tmpImgtl[:,:,1]-np.min(tmpImgtl[:,:,1]))/(np.max(tmpImgtl[:,:,1])-np.min(tmpImgtl[:,:,1]))\n\t\t\ttmpImg[:,:,5] = (tmpImgtl[:,:,2]-np.min(tmpImgtl[:,:,2]))/(np.max(tmpImgtl[:,:,2])-np.min(tmpImgtl[:,:,2]))\n\n\t\t\t# tmpImg = tmpImg/(np.max(tmpImg)-np.min(tmpImg))\n\n\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.mean(tmpImg[:,:,0]))/np.std(tmpImg[:,:,0])\n\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.mean(tmpImg[:,:,1]))/np.std(tmpImg[:,:,1])\n\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.mean(tmpImg[:,:,2]))/np.std(tmpImg[:,:,2])\n\t\t\ttmpImg[:,:,3] = (tmpImg[:,:,3]-np.mean(tmpImg[:,:,3]))/np.std(tmpImg[:,:,3])\n\t\t\ttmpImg[:,:,4] = (tmpImg[:,:,4]-np.mean(tmpImg[:,:,4]))/np.std(tmpImg[:,:,4])\n\t\t\ttmpImg[:,:,5] = (tmpImg[:,:,5]-np.mean(tmpImg[:,:,5]))/np.std(tmpImg[:,:,5])\n\n\t\telif self.flag == 1: #with Lab color\n\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n\n\t\t\tif image.shape[2]==1:\n\t\t\t\ttmpImg[:,:,0] = image[:,:,0]\n\t\t\t\ttmpImg[:,:,1] = image[:,:,0]\n\t\t\t\ttmpImg[:,:,2] = image[:,:,0]\n\t\t\telse:\n\t\t\t\ttmpImg = image\n\n\t\t\ttmpImg = color.rgb2lab(tmpImg)\n\n\t\t\t# tmpImg = tmpImg/(np.max(tmpImg)-np.min(tmpImg))\n\n\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.min(tmpImg[:,:,0]))/(np.max(tmpImg[:,:,0])-np.min(tmpImg[:,:,0]))\n\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.min(tmpImg[:,:,1]))/(np.max(tmpImg[:,:,1])-np.min(tmpImg[:,:,1]))\n\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.min(tmpImg[:,:,2]))/(np.max(tmpImg[:,:,2])-np.min(tmpImg[:,:,2]))\n\n\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.mean(tmpImg[:,:,0]))/np.std(tmpImg[:,:,0])\n\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.mean(tmpImg[:,:,1]))/np.std(tmpImg[:,:,1])\n\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.mean(tmpImg[:,:,2]))/np.std(tmpImg[:,:,2])\n\n\t\telse: # with rgb color\n\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n\t\t\timage = image/np.max(image)\n\t\t\tif image.shape[2]==1:\n\t\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\t\ttmpImg[:,:,1] = (image[:,:,0]-0.485)/0.229\n\t\t\t\ttmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229\n\t\t\telse:\n\t\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\t\ttmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n\t\t\t\ttmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n\n\t\ttmpLbl[:,:,0] = label[:,:,0]\n\n\t\ttmpImg = tmpImg.transpose((2, 0, 1))\n\t\ttmpLbl = label.transpose((2, 0, 1))\n\n\t\treturn {'imidx':torch.from_numpy(imidx), 'image': torch.from_numpy(tmpImg), 'label': torch.from_numpy(tmpLbl)}\n\n\nclass SalObjDataset(Dataset):\n\n\tdef __init__(self,img_name_list,lbl_name_list,transform=None):\n\t\t# self.root_dir = root_dir\n\t\t# self.image_name_list = glob.glob(image_dir+'*.png')\n\t\t# self.label_name_list = glob.glob(label_dir+'*.png')\n\t\tself.image_name_list = img_name_list\n\t\tself.label_name_list = lbl_name_list\n\t\tself.transform = transform\n\n\tdef __len__(self):\n\t\treturn len(self.image_name_list)\n\n\tdef __getitem__(self,idx):\n\n\t\t# image = Image.open(self.image_name_list[idx])#io.imread(self.image_name_list[idx])\n\t\t# label = Image.open(self.label_name_list[idx])#io.imread(self.label_name_list[idx])\n\n\t\timage = io.imread(self.image_name_list[idx])\n\t\timname = self.image_name_list[idx]\n\t\timidx = np.array([idx])\n\n\t\tif(0==len(self.label_name_list)):\n\t\t\tlabel_3 = np.zeros(image.shape)\n\t\telse:\n\t\t\tlabel_3 = io.imread(self.label_name_list[idx])\n\n\t\tlabel = np.zeros(label_3.shape[0:2])\n\t\tif(3==len(label_3.shape)):\n\t\t\tlabel = label_3[:,:,0]\n\t\telif(2==len(label_3.shape)):\n\t\t\tlabel = label_3\n\n\t\tif(3==len(image.shape) and 2==len(label.shape)):\n\t\t\tlabel = label[:,:,np.newaxis]\n\t\telif(2==len(image.shape) and 2==len(label.shape)):\n\t\t\timage = image[:,:,np.newaxis]\n\t\t\tlabel = label[:,:,np.newaxis]\n\n\t\tsample = {'imidx':imidx, 'image':image, 'label':label}\n\n\t\tif self.transform:\n\t\t\tsample = self.transform(sample)\n\n\t\treturn sample"}
{"type": "source_file", "path": "U2Net/model/__init__.py", "content": "from .u2net import U2NET\nfrom .u2net import U2NETP\n"}
{"type": "source_file", "path": "train.py", "content": "# coding=utf-8\nfrom __future__ import absolute_import, division, print_function\n\nwnb = False\nif wnb:\n    import wandb\n    wandb.init(project=\"sm-vit\", entity=\"xxx\")\n\nimport logging\nimport argparse\nimport os\nimport random\nimport numpy as np\n\nfrom datetime import timedelta\nimport time\n\nimport torch\nimport torch.distributed as dist\n\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nfrom apex import amp\nfrom apex.parallel import DistributedDataParallel as DDP\n\nfrom models.modeling import VisionTransformer, CONFIGS\nfrom utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\nfrom utils.data_utils import get_loader\nfrom utils.dist_util import get_world_size\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\n\ndef save_model(args, model):\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_checkpoint = os.path.join(args.output_dir, \"%s_checkpoint.bin\" % args.name)\n    torch.save(model_to_save.state_dict(), model_checkpoint)\n    logger.info(\"Saved model checkpoint to [DIR: %s]\", args.output_dir)\n\ndef reduce_mean(tensor, nprocs):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= nprocs\n\n    return rt\n\ndef setup(args):\n    # Prepare model\n    config = CONFIGS[args.model_type]\n    \n    if args.dataset == \"dogs\":\n        num_classes = 120\n    elif args.dataset == \"CUB\":\n        num_classes=200\n    elif args.dataset == \"nabirds\":\n        num_classes = 555\n    else:\n        raise Exception(f'Unknown dataset \"{args.dataset}\"')\n\n    model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes, vis=True, smoothing_value=args.smoothing_value, dataset=args.dataset, \\\n        coeff_max=args.coeff_max, contr_loss=args.contr_loss, focal_loss=args.focal_loss)\n    model.load_from(np.load(args.pretrained_dir))\n    model.to(args.device)\n    num_params = count_parameters(model)\n\n    logger.info(\"{}\".format(config))\n    logger.info(\"Training parameters %s\", args)\n    logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)\n\n    return args, model\n\n\ndef count_parameters(model):\n    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    return params/1000000\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef valid(args, model, writer, test_loader, global_step):\n    # Validation!\n    eval_losses = AverageMeter()\n\n    logger.info(\"***** Running Validation *****\")\n    logger.info(\"  Num steps = %d\", len(test_loader))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n\n    model.eval()\n    all_preds, all_label = [], []\n    epoch_iterator = tqdm(test_loader,\n                          desc=\"Validating... (loss=X.X)\",\n                          bar_format=\"{l_bar}{r_bar}\",\n                          dynamic_ncols=True,\n                          disable=args.local_rank not in [-1, 0])\n    loss_fct = torch.nn.CrossEntropyLoss()\n    for step, batch in enumerate(epoch_iterator):\n\n        if wnb: wandb.log({\"step\": step})\n\n        batch = tuple(t.to(args.device) for t in batch)\n        \n        if args.sm_vit:\n            x, y, mask = batch\n        else:\n            x, y = batch\n\n        with torch.no_grad():\n            if args.sm_vit:\n                logits = model(x, None, mask)[0]\n            else:\n                logits = model(x)[0]\n\n            eval_loss = loss_fct(logits, y)\n\n            if args.contr_loss:\n                eval_loss = eval_loss.mean()\n\n            eval_losses.update(eval_loss.item())\n\n            preds = torch.argmax(logits, dim=-1)\n\n        if len(all_preds) == 0:\n            all_preds.append(preds.detach().cpu().numpy())\n            all_label.append(y.detach().cpu().numpy())\n        else:\n            all_preds[0] = np.append(\n                all_preds[0], preds.detach().cpu().numpy(), axis=0 )\n            all_label[0] = np.append(\n                all_label[0], y.detach().cpu().numpy(), axis=0 )\n\n        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n\n    all_preds, all_label = all_preds[0], all_label[0]\n    accuracy = simple_accuracy(all_preds, all_label)\n\n    logger.info(\"\\n\")\n    logger.info(\"Validation Results\")\n    logger.info(\"Global Steps: %d\" % global_step)\n    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n\n    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n\n    if wnb: wandb.log({\"acc_test\": accuracy})\n\n    return accuracy\n\n\ndef train(args, model):\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir, exist_ok=True)\n        writer = SummaryWriter(log_dir=os.path.join(\"logs\", args.name))\n        \n    best_step=0\n\n    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n\n    # Prepare dataset\n    train_loader, test_loader = get_loader(args)\n\n    # Prepare optimizer and scheduler\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.learning_rate,\n                                momentum=0.9,\n                                weight_decay=args.weight_decay)\n    \n    t_total = args.num_steps\n    if args.decay_type == \"cosine\":\n        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n    else:\n        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n\n    if args.fp16:\n        model, optimizer = amp.initialize(models=model,\n                                          optimizers=optimizer,\n                                          opt_level=args.fp16_opt_level)\n        amp._amp_state.loss_scalers[0]._loss_scale = 2**20\n\n    # Distributed training\n    if args.local_rank != -1:\n        model = DDP(model, message_size=250000000, gradient_predivide_factor=get_world_size())\n\n    # Train!\n    start_time = time.time()\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Total optimization steps = %d\", args.num_steps)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n                args.train_batch_size * args.gradient_accumulation_steps * (\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n\n    model.zero_grad()\n    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n    losses = AverageMeter()\n    global_step, best_acc = 0, 0\n\n    while True:\n        model.train()\n        epoch_iterator = tqdm(train_loader,\n                              desc=\"Training (X / X Steps) (loss=X.X)\",\n                              bar_format=\"{l_bar}{r_bar}\",\n                              dynamic_ncols=True,\n                              disable=args.local_rank not in [-1, 0])\n\n        all_preds, all_label = [], []\n\n        for step, batch in enumerate(epoch_iterator):       \n            batch = tuple(t.to(args.device) for t in batch)\n\n            if args.sm_vit:\n                x, y, mask = batch\n                loss, logits = model(x, y, mask)\n            else:\n                x, y = batch\n                loss, logits = model(x, y)\n\n            if args.contr_loss:\n                loss = loss.mean()\n\n            preds = torch.argmax(logits, dim=-1)\n\n            if len(all_preds) == 0:\n                all_preds.append(preds.detach().cpu().numpy())\n                all_label.append(y.detach().cpu().numpy())\n            else:\n                all_preds[0] = np.append(\n                    all_preds[0], preds.detach().cpu().numpy(), axis=0 )\n                all_label[0] = np.append(\n                    all_label[0], y.detach().cpu().numpy(), axis=0 )\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                losses.update(loss.item()*args.gradient_accumulation_steps)\n\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                global_step += 1\n\n                epoch_iterator.set_description(\n                    \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val) )\n                if args.local_rank in [-1, 0]:\n                    writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n                    writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n                if global_step % args.eval_every == 0 and args.local_rank in [-1, 0]:\n                    accuracy = valid(args, model, writer, test_loader, global_step)\n                    if best_acc < accuracy:\n                        save_model(args, model)\n                        best_acc = accuracy\n                        best_step = global_step\n                    logger.info(\"best accuracy so far: %f\" % best_acc)\n                    logger.info(\"best accuracy in step: %f\" % best_step)\n                    model.train()\n                if global_step % t_total == 0:\n                    break\n\n        all_preds, all_label = all_preds[0], all_label[0]\n        accuracy = simple_accuracy(all_preds, all_label)\n        accuracy = torch.tensor(accuracy).to(args.device)\n        dist.barrier()\n        train_accuracy = reduce_mean(accuracy, args.nprocs)\n        train_accuracy = train_accuracy.detach().cpu().numpy()\n\n        writer.add_scalar(\"train/accuracy\", scalar_value=train_accuracy, global_step=global_step)\n\n        if wnb: wandb.log({\"acc_train\": train_accuracy})\n\n        logger.info(\"train accuracy so far: %f\" % train_accuracy)\n        logger.info(\"best valid accuracy in step: %f\" % best_step)\n        losses.reset()\n        if global_step % t_total == 0:\n            break\n\n    if args.local_rank in [-1, 0]:\n        writer.close()\n    end_time = time.time()\n    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n    logger.info(\"Total Training Time: \\t%f\" % ((end_time - start_time) / 3600))\n    logger.info(\"End Training!\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    # Required parameters\n    parser.add_argument(\"--name\", required=True,\n                        default=\"output\",\n                        help=\"Name of this run. Used for monitoring.\")\n    parser.add_argument(\"--dataset\", choices=[\"CUB\", \"dogs\", \"nabirds\"], default=\"CUB\",\n                        help=\"Which downstream task.\")\n    parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n                                                 \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n                        default=\"ViT-B_16\",\n                        help=\"Which ViT variant to use.\")\n    parser.add_argument(\"--pretrained_dir\", type=str, default=\"models/pre_trained/ViT-B_16.npz\",\n                        help=\"Where to search for pretrained ViT models.\")\n    parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n                        help=\"The output directory where checkpoints will be saved.\")\n    parser.add_argument(\"--img_size\", default=400, type=int,\n                        help=\"After-crop image resolution\")\n    parser.add_argument(\"--resize_size\", default=448, type=int,\n                        help=\"Pre-crop image resolution\")\n    parser.add_argument(\"--train_batch_size\", default=16, type=int,\n                        help=\"Total batch size for training.\")\n    parser.add_argument(\"--eval_batch_size\", default=16, type=int,\n                        help=\"Total batch size for eval.\")\n    parser.add_argument(\"--eval_every\", default=200, type=int,\n                        help=\"Run prediction on validation set every so many steps.\"\n                             \"Will always run one evaluation at the end of training.\")\n    parser.add_argument(\"--num_workers\", default=4, type=int,\n                        help=\"Number of workers for dataset preparation.\")\n    parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n                        help=\"The initial learning rate for SGD.\")\n    parser.add_argument(\"--weight_decay\", default=0, type=float,\n                        help=\"Weight deay if we apply some.\")\n    parser.add_argument(\"--num_steps\", default=10000, type=int,\n                        help=\"Total number of training steps to perform.\")\n    parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n                        help=\"How to decay the learning rate.\")\n    parser.add_argument(\"--warmup_steps\", default=500, type=int,\n                        help=\"Step of training to perform learning rate warmup for.\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n                        help=\"Max gradient norm.\")\n    parser.add_argument(\"--local_rank\", type=int, default=-1,\n                        help=\"local_rank for distributed training on gpus\")\n    parser.add_argument('--seed', type=int, default=42,\n                        help=\"random seed for initialization\")\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n    parser.add_argument('--fp16', action='store_true',\n                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n    parser.add_argument('--fp16_opt_level', type=str, default='O2',\n                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n                             \"See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--loss_scale', type=float, default=0,\n                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n                             \"0 (default value): dynamic loss scaling.\\n\"\n                             \"Positive power of 2: static loss scaling value.\\n\")\n    parser.add_argument('--smoothing_value', type=float, default=0.0,\n                        help=\"Label smoothing value\\n\")\n    parser.add_argument('--sm_vit', action='store_true',\n                        help=\"Whether to use SM-ViT\")\n    parser.add_argument('--coeff_max', type=float, default=0.25,\n                        help=\"Coefficient for attention guiding (see Eq. 3 in the SM-ViT paper). Best for CUB adn NABirds: '0.25', best for St Dogs: '0.3'.\\n\")\n    parser.add_argument('--low_memory', action='store_true',\n                        help=\"Allows to use less memory (RAM) during input image feeding. False: Slower - Do image pre-processing for the whole dataset at the beginning and store the results in memory. True: Faster - Do pre-processing on-the-go.\")\n    parser.add_argument('--contr_loss', action='store_true',\n                        help=\"Whether to use contrastive loss\")\n    parser.add_argument('--focal_loss', action='store_true',\n                        help=\"Whether to use focal loss\")\n\n    parser.add_argument('--data_root', type=str, default='./data', # Originall\n                        help=\"Path to the dataset\\n\")\n                        # '/l/users/20020067/Datasets/CUB_200_2011/CUB_200_2011/CUB_200_2011') # CUB\n                        # '/l/users/20020067/Datasets/Stanford Dogs/Stanford_Dogs') # dogs\n                        # '/l/users/20020067/Datasets/NABirds/NABirds') # NABirds\n\n    args = parser.parse_args()\n    \n    #args.data_root = '{}/{}'.format(args.data_root, args.dataset) # for future development\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend='nccl',\n                                             timeout=timedelta(minutes=60))\n        args.n_gpu = 1\n    args.device = device\n    args.nprocs = torch.cuda.device_count()\n\n    # Setup logging\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                        datefmt='%m/%d/%Y %H:%M:%S',\n                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\" %\n                   (args.local_rank, args.device, args.n_gpu, bool(args.local_rank != -1), args.fp16))\n\n    # Set seed\n    set_seed(args)\n\n    # Model & Tokenizer Setup\n    args, model = setup(args)\n\n    if wnb: wandb.watch(model)\n\n    # Training\n    train(args, model)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "U2Net/model/setup_model_weights.py", "content": "import os\nimport gdown\n\nos.makedirs('./saved_models/u2net', exist_ok=True)\nos.makedirs('./saved_models/u2net_portrait', exist_ok=True)\n\ngdown.download('https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ',\n    './saved_models/u2net/u2net.pth',\n    quiet=False)\n\ngdown.download('https://drive.google.com/uc?id=1IG3HdpcRiDoWNookbncQjeaPN28t90yW',\n    './saved_models/u2net_portrait/u2net_portrait.pth',\n    quiet=False)\n"}
{"type": "source_file", "path": "U2Net/model/u2net_refactor.py", "content": "import torch\nimport torch.nn as nn\n\nimport math\n\n__all__ = ['U2NET_full', 'U2NET_lite']\n\n\ndef _upsample_like(x, size):\n    return nn.Upsample(size=size, mode='bilinear', align_corners=False)(x)\n\n\ndef _size_map(x, height):\n    # {height: size} for Upsample\n    size = list(x.shape[-2:])\n    sizes = {}\n    for h in range(1, height):\n        sizes[h] = size\n        size = [math.ceil(w / 2) for w in size]\n    return sizes\n\n\nclass REBNCONV(nn.Module):\n    def __init__(self, in_ch=3, out_ch=3, dilate=1):\n        super(REBNCONV, self).__init__()\n\n        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1 * dilate, dilation=1 * dilate)\n        self.bn_s1 = nn.BatchNorm2d(out_ch)\n        self.relu_s1 = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu_s1(self.bn_s1(self.conv_s1(x)))\n\n\nclass RSU(nn.Module):\n    def __init__(self, name, height, in_ch, mid_ch, out_ch, dilated=False):\n        super(RSU, self).__init__()\n        self.name = name\n        self.height = height\n        self.dilated = dilated\n        self._make_layers(height, in_ch, mid_ch, out_ch, dilated)\n\n    def forward(self, x):\n        sizes = _size_map(x, self.height)\n        x = self.rebnconvin(x)\n\n        # U-Net like symmetric encoder-decoder structure\n        def unet(x, height=1):\n            if height < self.height:\n                x1 = getattr(self, f'rebnconv{height}')(x)\n                if not self.dilated and height < self.height - 1:\n                    x2 = unet(getattr(self, 'downsample')(x1), height + 1)\n                else:\n                    x2 = unet(x1, height + 1)\n\n                x = getattr(self, f'rebnconv{height}d')(torch.cat((x2, x1), 1))\n                return _upsample_like(x, sizes[height - 1]) if not self.dilated and height > 1 else x\n            else:\n                return getattr(self, f'rebnconv{height}')(x)\n\n        return x + unet(x)\n\n    def _make_layers(self, height, in_ch, mid_ch, out_ch, dilated=False):\n        self.add_module('rebnconvin', REBNCONV(in_ch, out_ch))\n        self.add_module('downsample', nn.MaxPool2d(2, stride=2, ceil_mode=True))\n\n        self.add_module(f'rebnconv1', REBNCONV(out_ch, mid_ch))\n        self.add_module(f'rebnconv1d', REBNCONV(mid_ch * 2, out_ch))\n\n        for i in range(2, height):\n            dilate = 1 if not dilated else 2 ** (i - 1)\n            self.add_module(f'rebnconv{i}', REBNCONV(mid_ch, mid_ch, dilate=dilate))\n            self.add_module(f'rebnconv{i}d', REBNCONV(mid_ch * 2, mid_ch, dilate=dilate))\n\n        dilate = 2 if not dilated else 2 ** (height - 1)\n        self.add_module(f'rebnconv{height}', REBNCONV(mid_ch, mid_ch, dilate=dilate))\n\n\nclass U2NET(nn.Module):\n    def __init__(self, cfgs, out_ch):\n        super(U2NET, self).__init__()\n        self.out_ch = out_ch\n        self._make_layers(cfgs)\n\n    def forward(self, x):\n        sizes = _size_map(x, self.height)\n        maps = []  # storage for maps\n\n        # side saliency map\n        def unet(x, height=1):\n            if height < 6:\n                x1 = getattr(self, f'stage{height}')(x)\n                x2 = unet(getattr(self, 'downsample')(x1), height + 1)\n                x = getattr(self, f'stage{height}d')(torch.cat((x2, x1), 1))\n                side(x, height)\n                return _upsample_like(x, sizes[height - 1]) if height > 1 else x\n            else:\n                x = getattr(self, f'stage{height}')(x)\n                side(x, height)\n                return _upsample_like(x, sizes[height - 1])\n\n        def side(x, h):\n            # side output saliency map (before sigmoid)\n            x = getattr(self, f'side{h}')(x)\n            x = _upsample_like(x, sizes[1])\n            maps.append(x)\n\n        def fuse():\n            # fuse saliency probability maps\n            maps.reverse()\n            x = torch.cat(maps, 1)\n            x = getattr(self, 'outconv')(x)\n            maps.insert(0, x)\n            return [torch.sigmoid(x) for x in maps]\n\n        unet(x)\n        maps = fuse()\n        return maps\n\n    def _make_layers(self, cfgs):\n        self.height = int((len(cfgs) + 1) / 2)\n        self.add_module('downsample', nn.MaxPool2d(2, stride=2, ceil_mode=True))\n        for k, v in cfgs.items():\n            # build rsu block\n            self.add_module(k, RSU(v[0], *v[1]))\n            if v[2] > 0:\n                # build side layer\n                self.add_module(f'side{v[0][-1]}', nn.Conv2d(v[2], self.out_ch, 3, padding=1))\n        # build fuse layer\n        self.add_module('outconv', nn.Conv2d(int(self.height * self.out_ch), self.out_ch, 1))\n\n\ndef U2NET_full():\n    full = {\n        # cfgs for building RSUs and sides\n        # {stage : [name, (height(L), in_ch, mid_ch, out_ch, dilated), side]}\n        'stage1': ['En_1', (7, 3, 32, 64), -1],\n        'stage2': ['En_2', (6, 64, 32, 128), -1],\n        'stage3': ['En_3', (5, 128, 64, 256), -1],\n        'stage4': ['En_4', (4, 256, 128, 512), -1],\n        'stage5': ['En_5', (4, 512, 256, 512, True), -1],\n        'stage6': ['En_6', (4, 512, 256, 512, True), 512],\n        'stage5d': ['De_5', (4, 1024, 256, 512, True), 512],\n        'stage4d': ['De_4', (4, 1024, 128, 256), 256],\n        'stage3d': ['De_3', (5, 512, 64, 128), 128],\n        'stage2d': ['De_2', (6, 256, 32, 64), 64],\n        'stage1d': ['De_1', (7, 128, 16, 64), 64],\n    }\n    return U2NET(cfgs=full, out_ch=1)\n\n\ndef U2NET_lite():\n    lite = {\n        # cfgs for building RSUs and sides\n        # {stage : [name, (height(L), in_ch, mid_ch, out_ch, dilated), side]}\n        'stage1': ['En_1', (7, 3, 16, 64), -1],\n        'stage2': ['En_2', (6, 64, 16, 64), -1],\n        'stage3': ['En_3', (5, 64, 16, 64), -1],\n        'stage4': ['En_4', (4, 64, 16, 64), -1],\n        'stage5': ['En_5', (4, 64, 16, 64, True), -1],\n        'stage6': ['En_6', (4, 64, 16, 64, True), 64],\n        'stage5d': ['De_5', (4, 128, 16, 64, True), 64],\n        'stage4d': ['De_4', (4, 128, 16, 64), 64],\n        'stage3d': ['De_3', (5, 128, 16, 64), 64],\n        'stage2d': ['De_2', (6, 128, 16, 64), 64],\n        'stage1d': ['De_1', (7, 128, 16, 64), 64],\n    }\n    return U2NET(cfgs=lite, out_ch=1)\n"}
{"type": "source_file", "path": "models/modeling.py", "content": "# coding=utf-8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport math\n\nfrom os.path import join as pjoin\nfrom re import X\nfrom matplotlib.cbook import flatten\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\nfrom torch.nn.modules.utils import _pair\nfrom scipy import ndimage\nimport torch.nn.functional as F\n\nimport models.configs as configs\n\ndebug_mode = False # For debug\nif debug_mode: import random\n\n\n\nATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\nATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\nlogger = logging.getLogger(__name__)\n\nATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\nATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\nFC_0 = \"MlpBlock_3/Dense_0\"\nFC_1 = \"MlpBlock_3/Dense_1\"\nATTENTION_NORM = \"LayerNorm_0\"\nMLP_NORM = \"LayerNorm_2\"\n\n\ndef np2th(weights, conv=False):\n    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n    if conv:\n        weights = weights.transpose([3, 2, 0, 1])\n    return torch.from_numpy(weights)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nclass LabelSmoothing(nn.Module):\n    \"\"\"\n    NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n\n\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        BCE_loss = torch.nn.CrossEntropyLoss()(inputs, targets)\n\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\n\nACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n\n\nclass Attention(nn.Module):\n    \n    def __init__(self, config, vis, coeff_max=0.25):\n        super(Attention, self).__init__()\n\n        self.coeff_max = coeff_max\n\n        self.vis = vis\n        self.num_attention_heads = config.transformer[\"num_heads\"]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n\n        self.softmax = Softmax(dim=-1)\n        self.softmax2 = Softmax(dim=-2)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, mask=None):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n    \n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\n\n        if mask is not None:\n\n            if debug_mode:\n                print_info = True if (random.random() < 0.000001) else False\n                x = random.random()\n                if (x > 0.00005) and (x < 0.00007):\n                    print_info = True\n                else:\n                    print_info = False\n            else:\n                print_info = False\n\n            max_as = torch.max(attention_scores[:, :, 0, :], dim=2, keepdim=False)[0]\n            max_as = max_as.to(device='cuda')\n\n            if print_info:\n                print(\"mask before:\", mask)\n                print(\"attn scores before:\", attention_scores[:, :, 0, :])\n\n                print(\"attn scores max_min before:\")\n                print(max_as, torch.min(attention_scores[:, :, 0, :], dim=2, keepdim=False)[0])\n\n                print(torch.topk(attention_scores[:, :, 0, :], 5, largest=True), torch.topk(attention_scores[:, :, 0, :], 5, largest=False))\n\n            mask_626 = torch.zeros(mask.size(0), (mask.size(1) + 1)) #, dtype=torch.float64) # dtype=torch.double)\n            mask_626 = mask_626.to(device='cuda')\n            mask_626[:, 1:] = mask[:, :]\n            mask_626[:, 0] = 0\n\n            if print_info: print(\"mask626:\", mask_626)\n            \n            # positive only, obj + (max * coeff):\n            attention_scores[:, :, 0, :] = \\\n                torch.where( mask_626[:, None, :] < 0.5, \\\n                        torch.add( attention_scores[:, :, 0, :], \\\n                            torch.mul( max_as[:, :, None] , torch.tensor(self.coeff_max).cuda()) ), \\\n                        attention_scores[:, :, 0, :] #.float()\n                            )\n\n            if print_info:\n                print(\"attn scores after:\", attention_scores[:, :, 0, :])\n\n                print(\"attn scores max_min after:\")\n                print(torch.max(attention_scores[:, :, 0, :]), torch.min(attention_scores[:, :, 0, :]))\n          \n                print(torch.topk(attention_scores[:, :, 0, :], 5, largest=True), torch.topk(attention_scores[:, :, 0, :], 5, largest=False))\n\n\n        attention_probs = self.softmax(attention_scores)\n        \n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n\n        return attention_output, weights, self.softmax2(attention_scores)[:,:,:,0]\n\n\nclass Mlp(nn.Module):\n    def __init__(self, config):\n        super(Mlp, self).__init__()\n        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n        self.act_fn = ACT2FN[\"gelu\"]\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Embeddings(nn.Module):\n    \"\"\"Construct the embeddings from patch, position embeddings.\n    \"\"\"\n    def __init__(self, config, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        img_size = _pair(img_size)\n\n        # EXPERIMENTAL. Overlapping patches:\n        overlap = False\n        if overlap: slide = 12 # 14\n\n        if config.patches.get(\"grid\") is not None:\n            grid_size = config.patches[\"grid\"]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[\"size\"])\n\n            if overlap:\n                n_patches = ((img_size[0] - patch_size[0]) // slide + 1) * ((img_size[1] - patch_size[1]) // slide + 1)\n            else:\n                n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n\n            self.hybrid = False\n\n        if overlap:\n            self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                        out_channels=config.hidden_size,\n                                        kernel_size=patch_size,\n                                        stride=(slide, slide) )                 \n        else:\n            self.patch_embeddings = Conv2d(in_channels=in_channels,\n                                        out_channels=config.hidden_size,\n                                        kernel_size=patch_size,\n                                        stride=patch_size )\n\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n\n        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n\n    def forward(self, x):\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n\n        x = self.patch_embeddings(x)\n        x = x.flatten(2)\n        x = x.transpose(-1, -2)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass Block(nn.Module):\n    def __init__(self, config, vis, coeff_max):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis, coeff_max)\n\n    def forward(self, x, mask=None):\n        h = x\n        x = self.attention_norm(x)\n        x, weights, contribution = self.attn(x, mask)\n        x = x + h\n\n        h = x\n        x = self.ffn_norm(x)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights, contribution\n\n    def load_from(self, weights, n_block):\n        ROOT = f\"Transformer/encoderblock_{n_block}\"\n        with torch.no_grad():\n            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n\n            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n\n            self.attn.query.weight.copy_(query_weight)\n            self.attn.key.weight.copy_(key_weight)\n            self.attn.value.weight.copy_(value_weight)\n            self.attn.out.weight.copy_(out_weight)\n            self.attn.query.bias.copy_(query_bias)\n            self.attn.key.bias.copy_(key_bias)\n            self.attn.value.bias.copy_(value_bias)\n            self.attn.out.bias.copy_(out_bias)\n\n            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n\n            self.ffn.fc1.weight.copy_(mlp_weight_0)\n            self.ffn.fc2.weight.copy_(mlp_weight_1)\n            self.ffn.fc1.bias.copy_(mlp_bias_0)\n            self.ffn.fc2.bias.copy_(mlp_bias_1)\n\n            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n     \n\nclass Encoder(nn.Module):\n    def __init__(self, config, vis, coeff_max):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        num_layers = config.transformer[\"num_layers\"]\n\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(num_layers):\n            layer = Block(config, vis, coeff_max)\n            self.layer.append(copy.deepcopy(layer))\n\n    def forward(self, hidden_states, mask=None):\n        attn_weights = []\n        contributions = []\n        tokens = [[] for i in range(hidden_states.shape[0])]\n\n        for layer_block in self.layer:\n            hidden_states, weights, contribution = layer_block(hidden_states, mask)\n\n            if self.vis:\n                attn_weights.append(weights)\n                contributions.append(contribution)\n\n        encoded = self.encoder_norm(hidden_states)\n\n        return encoded, attn_weights\n        \n\n\nclass Transformer(nn.Module):\n    def __init__(self, config, img_size, vis, coeff_max):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size)\n        self.encoder = Encoder(config, vis, coeff_max)\n\n    def forward(self, input_ids, mask=None):\n        embedding_output = self.embeddings(input_ids)\n        encoded, attn_weights = self.encoder(embedding_output, mask)\n\n        return encoded, attn_weights\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=400, num_classes=200, smoothing_value=0, zero_head=False, vis=False, dataset='CUB', coeff_max=0.25, contr_loss=False, focal_loss=False):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.smoothing_value = smoothing_value\n        self.classifier = config.classifier\n        self.dataset=dataset\n\n        self.contr_loss = contr_loss\n        self.focal_loss = focal_loss\n\n        self.transformer = Transformer(config, img_size, vis, coeff_max)\n        self.head = Linear(config.hidden_size, num_classes)\n\n    def forward(self, x, labels=None, mask=None):\n        x, attn_weights = self.transformer(x, mask)\n        logits = self.head(x[:, 0])\n\n        if labels is not None:\n            if self.smoothing_value == 0:\n                loss_fct = CrossEntropyLoss()\n            else:\n                loss_fct = LabelSmoothing(self.smoothing_value)\n\n            if self.focal_loss: # enforce another type of loss\n                loss_fct = FocalLoss()\n\n            ce_loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n\n            if self.contr_loss:\n                contrast_loss = con_loss(x[:, 0], labels.view(-1))\n                loss = ce_loss + contrast_loss\n            else:\n                loss = ce_loss # FFVT\n\n            return loss, logits\n        else:\n            return logits, attn_weights\n\n    def load_from(self, weights):\n        with torch.no_grad():\n            if self.zero_head:\n                nn.init.zeros_(self.head.weight)\n                nn.init.zeros_(self.head.bias)\n            else:\n                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n\n            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n\n            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n\n            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n            posemb_new = self.transformer.embeddings.position_embeddings\n            if posemb.size() == posemb_new.size():\n                self.transformer.embeddings.position_embeddings.copy_(posemb)\n            else:\n                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n                ntok_new = posemb_new.size(1)\n\n                if self.classifier == \"token\":\n                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n                    ntok_new -= 1\n                else:\n                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n\n                gs_old = int(np.sqrt(len(posemb_grid)))\n                gs_new = int(np.sqrt(ntok_new))\n                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n\n                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n\n            for bname, block in self.transformer.encoder.named_children():\n                if bname.startswith('ff') == False:\n                    for uname, unit in block.named_children():\n                        unit.load_from(weights, n_block=uname)\n\n            if self.transformer.embeddings.hybrid:\n                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n\n                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n                    for uname, unit in block.named_children():\n                        unit.load_from(weights, n_block=bname, n_unit=uname)\n\n\ndef con_loss(features, labels):\n    B, _ = features.shape\n    features = F.normalize(features)\n    cos_matrix = features.mm(features.t())\n    pos_label_matrix = torch.stack([labels == labels[i] for i in range(B)]).float()\n    neg_label_matrix = 1 - pos_label_matrix\n    pos_cos_matrix = 1 - cos_matrix\n    neg_cos_matrix = cos_matrix - 0.4\n    neg_cos_matrix[neg_cos_matrix < 0] = 0\n    loss = (pos_cos_matrix * pos_label_matrix).sum() + (neg_cos_matrix * neg_label_matrix).sum()\n    loss /= (B * B)\n    return loss                        \n                        \n\nCONFIGS = {\n    'ViT-B_16': configs.get_b16_config(),\n    'ViT-B_32': configs.get_b32_config(),\n    'ViT-L_16': configs.get_l16_config(),\n    'ViT-L_32': configs.get_l32_config(),\n    'ViT-H_14': configs.get_h14_config(),\n    'R50-ViT-B_16': configs.get_r50_b16_config(),\n    'testing': configs.get_testing(),\n}"}
{"type": "source_file", "path": "utils/dist_util.py", "content": "import torch.distributed as dist\n\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\ndef is_main_process():\n    return get_rank() == 0\n\ndef format_step(step):\n    if isinstance(step, str):\n        return step\n    s = \"\"\n    if len(step) > 0:\n        s += \"Training Epoch: {} \".format(step[0])\n    if len(step) > 1:\n        s += \"Training Iteration: {} \".format(step[1])\n    if len(step) > 2:\n        s += \"Validation Iteration: {} \".format(step[2])\n    return s\n"}
{"type": "source_file", "path": "utils/data_utils.py", "content": "import logging\n\nimport torch\n\nfrom torchvision import transforms, datasets\nfrom .dataset import *\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\nfrom PIL import Image\nfrom .autoaugment import AutoAugImageNetPolicy\nimport os\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_loader(args):\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n\n    transform_train = transforms.Compose([\n        transforms.RandomResizedCrop((args.img_size, args.img_size), scale=(0.05, 1.0)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    ])\n    transform_test = transforms.Compose([\n        transforms.Resize((args.img_size, args.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    ])\n\n\n    if args.dataset == 'dogs':\n\n        if args.sm_vit:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size), Image.BILINEAR),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                # transforms.RandomHorizontalFlip(), !!! FLIPPING in dataset.py !!!\n                \n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n                                        \n            test_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size), Image.BILINEAR),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n        else:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                transforms.RandomCrop((args.img_size, args.img_size)),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                transforms.RandomHorizontalFlip(),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n                                        \n            test_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                transforms.CenterCrop((args.img_size, args.img_size)),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n\n        trainset = dogs(args.dataset, \n                        root=args.data_root,\n                        is_train=True,\n                        cropped=False,\n                        transform=train_transform,\n                        download=False,\n                        sm_vit=args.sm_vit,\n                        low_memory=args.low_memory,\n                        img_size=args.img_size\n                        )\n        testset = dogs(args.dataset, \n                        root=args.data_root,\n                        is_train=False,\n                        cropped=False,\n                        transform=test_transform,\n                        download=False,\n                        sm_vit=args.sm_vit,\n                        low_memory=args.low_memory,\n                        img_size=args.img_size\n                        )\n\n\n    elif args.dataset== \"CUB\":\n\n        if args.sm_vit:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size),Image.BILINEAR),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                #transforms.RandomHorizontalFlip(), # !!! FLIPPING in dataset.py !!!\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                ])\n\n            test_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size),Image.BILINEAR),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n        else:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size),Image.BILINEAR),\n                transforms.RandomCrop((args.img_size, args.img_size)),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                transforms.RandomHorizontalFlip(),\n                \n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                ])\n                                            \n            test_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                transforms.CenterCrop((args.img_size, args.img_size)),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ])\n        \n\n        trainset = eval(args.dataset)(args.dataset, root=args.data_root, is_train=True, \\\n            transform=train_transform, sm_vit=args.sm_vit, low_memory=args.low_memory, img_size=args.img_size)\n        testset = eval(args.dataset)(args.dataset, root=args.data_root, is_train=False, \\\n            transform = test_transform, sm_vit=args.sm_vit, low_memory=args.low_memory, img_size=args.img_size)\n\n\n    elif args.dataset == 'nabirds':\n\n        if args.sm_vit:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size), Image.BILINEAR),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # my add (from FFVT) mb try?\n                #transforms.RandomHorizontalFlip(), # !!! FLIPPING in dataset.py !!!\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                        \n                ])\n\n            test_transform=transforms.Compose([\n                transforms.Resize((args.img_size, args.img_size), Image.BILINEAR),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                          \n                ])\n        else:\n            train_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                transforms.RandomCrop((args.img_size, args.img_size)),\n\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                transforms.RandomHorizontalFlip(),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                        \n                ])\n\n            test_transform=transforms.Compose([\n                transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                transforms.CenterCrop((args.img_size, args.img_size)),\n\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                          \n                ])            \n\n\n        trainset = NABirds(args.dataset, root=args.data_root, is_train=True, \\\n            transform=train_transform, sm_vit=args.sm_vit, low_memory=args.low_memory, img_size=args.img_size)\n        testset = NABirds(args.dataset, root=args.data_root, is_train=False, \\\n            transform=test_transform, sm_vit=args.sm_vit, low_memory=args.low_memory, img_size=args.img_size)\n\n\n\n\n    ### Not optimised datasets:\n\n    if args.dataset == 'INat2017':\n        train_transform=transforms.Compose([transforms.Resize((400, 400), Image.BILINEAR),\n                                    transforms.RandomCrop((304, 304)),\n                                    transforms.RandomHorizontalFlip(),\n                                    AutoAugImageNetPolicy(),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        test_transform=transforms.Compose([transforms.Resize((400, 400), Image.BILINEAR),\n                                    transforms.CenterCrop((304, 304)),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        trainset = INat2017(args.data_root, 'train', train_transform)\n        testset = INat2017(args.data_root, 'val', test_transform)        \n    \n\n    elif args.dataset == 'car':\n        trainset = CarsDataset(os.path.join(args.data_root,'devkit/cars_train_annos.mat'),\n                            os.path.join(args.data_root,'cars_train'),\n                            os.path.join(args.data_root,'devkit/cars_meta.mat'),\n                            # cleaned=os.path.join(data_dir,'cleaned.dat'),\n                            transform=transforms.Compose([\n                                    transforms.Resize((600, 600), Image.BILINEAR),\n                                    transforms.RandomCrop((448, 448)),\n                                    transforms.RandomHorizontalFlip(),\n                                    AutoAugImageNetPolicy(),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n                            )\n        testset = CarsDataset(os.path.join(args.data_root,'cars_test_annos_withlabels.mat'),\n                            os.path.join(args.data_root,'cars_test'),\n                            os.path.join(args.data_root,'devkit/cars_meta.mat'),\n                            # cleaned=os.path.join(data_dir,'cleaned_test.dat'),\n                            transform=transforms.Compose([\n                                    transforms.Resize((600, 600), Image.BILINEAR),\n                                    transforms.CenterCrop((448, 448)),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n                            )\n\n\n    elif args.dataset== \"air\":\n        train_transform=transforms.Compose([transforms.Resize((args.resize_size, args.resize_size),Image.BILINEAR),\n                                    transforms.RandomCrop((args.img_size, args.img_size)),\n                                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # my add\n                                    transforms.RandomHorizontalFlip(),\n                                    #transforms.RandomVerticalFlip(),\n                                    transforms.ToTensor(),\n                                    #transforms.Normalize([0.8416, 0.867, 0.8233], [0.2852, 0.246, 0.3262])])\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        test_transform=transforms.Compose([\n            transforms.Resize((args.resize_size, args.resize_size), Image.BILINEAR),\n                                    transforms.CenterCrop((args.img_size, args.img_size)),\n                                    #transforms.Resize((args.img_size, args.img_size)),\n                                    transforms.ToTensor(),\n                                    #transforms.Normalize([0.8416, 0.867, 0.8233], [0.2852, 0.246, 0.3262])])\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        trainset = FGVC_aircraft(root=args.data_root, is_train=True, transform=train_transform)\n        testset = FGVC_aircraft(root=args.data_root, is_train=False, transform = test_transform)\n    \n\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n\n    train_sampler = RandomSampler(trainset) if args.local_rank == -1 else DistributedSampler(trainset)\n    test_sampler = SequentialSampler(testset)\n    train_loader = DataLoader(trainset,\n                              sampler=train_sampler,\n                              batch_size=args.train_batch_size,\n                              num_workers=args.num_workers,\n                              pin_memory=True)\n    test_loader = DataLoader(testset,\n                             sampler=test_sampler,\n                             batch_size=args.eval_batch_size,\n                             num_workers=args.num_workers,\n                             pin_memory=True) if testset is not None else None\n\n    return train_loader, test_loader\n"}
{"type": "source_file", "path": "U2Net/model/u2net.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass REBNCONV(nn.Module):\n    def __init__(self,in_ch=3,out_ch=3,dirate=1):\n        super(REBNCONV,self).__init__()\n\n        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n        self.bn_s1 = nn.BatchNorm2d(out_ch)\n        self.relu_s1 = nn.ReLU(inplace=True)\n\n    def forward(self,x):\n\n        hx = x\n        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))\n\n        return xout\n\n## upsample tensor 'src' to have the same spatial size with tensor 'tar'\ndef _upsample_like(src,tar):\n\n    src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n\n    return src\n\n\n### RSU-7 ###\nclass RSU7(nn.Module):#UNet07DRES(nn.Module):\n\n    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n        super(RSU7,self).__init__()\n\n        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n\n        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n\n        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n\n        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n\n    def forward(self,x):\n\n        hx = x\n        hxin = self.rebnconvin(hx)\n\n        hx1 = self.rebnconv1(hxin)\n        hx = self.pool1(hx1)\n\n        hx2 = self.rebnconv2(hx)\n        hx = self.pool2(hx2)\n\n        hx3 = self.rebnconv3(hx)\n        hx = self.pool3(hx3)\n\n        hx4 = self.rebnconv4(hx)\n        hx = self.pool4(hx4)\n\n        hx5 = self.rebnconv5(hx)\n        hx = self.pool5(hx5)\n\n        hx6 = self.rebnconv6(hx)\n\n        hx7 = self.rebnconv7(hx6)\n\n        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),1))\n        hx6dup = _upsample_like(hx6d,hx5)\n\n        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),1))\n        hx5dup = _upsample_like(hx5d,hx4)\n\n        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n\n        return hx1d + hxin\n\n### RSU-6 ###\nclass RSU6(nn.Module):#UNet06DRES(nn.Module):\n\n    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n        super(RSU6,self).__init__()\n\n        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n\n        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n\n        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n\n        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n\n    def forward(self,x):\n\n        hx = x\n\n        hxin = self.rebnconvin(hx)\n\n        hx1 = self.rebnconv1(hxin)\n        hx = self.pool1(hx1)\n\n        hx2 = self.rebnconv2(hx)\n        hx = self.pool2(hx2)\n\n        hx3 = self.rebnconv3(hx)\n        hx = self.pool3(hx3)\n\n        hx4 = self.rebnconv4(hx)\n        hx = self.pool4(hx4)\n\n        hx5 = self.rebnconv5(hx)\n\n        hx6 = self.rebnconv6(hx5)\n\n\n        hx5d =  self.rebnconv5d(torch.cat((hx6,hx5),1))\n        hx5dup = _upsample_like(hx5d,hx4)\n\n        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n\n        return hx1d + hxin\n\n### RSU-5 ###\nclass RSU5(nn.Module):#UNet05DRES(nn.Module):\n\n    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n        super(RSU5,self).__init__()\n\n        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n\n        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n\n        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n\n        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n\n    def forward(self,x):\n\n        hx = x\n\n        hxin = self.rebnconvin(hx)\n\n        hx1 = self.rebnconv1(hxin)\n        hx = self.pool1(hx1)\n\n        hx2 = self.rebnconv2(hx)\n        hx = self.pool2(hx2)\n\n        hx3 = self.rebnconv3(hx)\n        hx = self.pool3(hx3)\n\n        hx4 = self.rebnconv4(hx)\n\n        hx5 = self.rebnconv5(hx4)\n\n        hx4d = self.rebnconv4d(torch.cat((hx5,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n\n        return hx1d + hxin\n\n### RSU-4 ###\nclass RSU4(nn.Module):#UNet04DRES(nn.Module):\n\n    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n        super(RSU4,self).__init__()\n\n        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n\n        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n\n        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n\n        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n\n    def forward(self,x):\n\n        hx = x\n\n        hxin = self.rebnconvin(hx)\n\n        hx1 = self.rebnconv1(hxin)\n        hx = self.pool1(hx1)\n\n        hx2 = self.rebnconv2(hx)\n        hx = self.pool2(hx2)\n\n        hx3 = self.rebnconv3(hx)\n\n        hx4 = self.rebnconv4(hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n\n        return hx1d + hxin\n\n### RSU-4F ###\nclass RSU4F(nn.Module):#UNet04FRES(nn.Module):\n\n    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n        super(RSU4F,self).__init__()\n\n        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n\n        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n\n        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n\n        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n\n    def forward(self,x):\n\n        hx = x\n\n        hxin = self.rebnconvin(hx)\n\n        hx1 = self.rebnconv1(hxin)\n        hx2 = self.rebnconv2(hx1)\n        hx3 = self.rebnconv3(hx2)\n\n        hx4 = self.rebnconv4(hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n        hx2d = self.rebnconv2d(torch.cat((hx3d,hx2),1))\n        hx1d = self.rebnconv1d(torch.cat((hx2d,hx1),1))\n\n        return hx1d + hxin\n\n\n##### U^2-Net ####\nclass U2NET(nn.Module):\n\n    def __init__(self,in_ch=3,out_ch=1):\n        super(U2NET,self).__init__()\n\n        self.stage1 = RSU7(in_ch,32,64)\n        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage2 = RSU6(64,32,128)\n        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage3 = RSU5(128,64,256)\n        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage4 = RSU4(256,128,512)\n        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage5 = RSU4F(512,256,512)\n        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage6 = RSU4F(512,256,512)\n\n        # decoder\n        self.stage5d = RSU4F(1024,256,512)\n        self.stage4d = RSU4(1024,128,256)\n        self.stage3d = RSU5(512,64,128)\n        self.stage2d = RSU6(256,32,64)\n        self.stage1d = RSU7(128,16,64)\n\n        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n\n        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n\n    def forward(self,x):\n\n        hx = x\n\n        #stage 1\n        hx1 = self.stage1(hx)\n        hx = self.pool12(hx1)\n\n        #stage 2\n        hx2 = self.stage2(hx)\n        hx = self.pool23(hx2)\n\n        #stage 3\n        hx3 = self.stage3(hx)\n        hx = self.pool34(hx3)\n\n        #stage 4\n        hx4 = self.stage4(hx)\n        hx = self.pool45(hx4)\n\n        #stage 5\n        hx5 = self.stage5(hx)\n        hx = self.pool56(hx5)\n\n        #stage 6\n        hx6 = self.stage6(hx)\n        hx6up = _upsample_like(hx6,hx5)\n\n        #-------------------- decoder --------------------\n        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n        hx5dup = _upsample_like(hx5d,hx4)\n\n        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n\n\n        #side output\n        d1 = self.side1(hx1d)\n\n        d2 = self.side2(hx2d)\n        d2 = _upsample_like(d2,d1)\n\n        d3 = self.side3(hx3d)\n        d3 = _upsample_like(d3,d1)\n\n        d4 = self.side4(hx4d)\n        d4 = _upsample_like(d4,d1)\n\n        d5 = self.side5(hx5d)\n        d5 = _upsample_like(d5,d1)\n\n        d6 = self.side6(hx6)\n        d6 = _upsample_like(d6,d1)\n\n        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n\n        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n\n### U^2-Net small ###\nclass U2NETP(nn.Module):\n\n    def __init__(self,in_ch=3,out_ch=1):\n        super(U2NETP,self).__init__()\n\n        self.stage1 = RSU7(in_ch,16,64)\n        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage2 = RSU6(64,16,64)\n        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage3 = RSU5(64,16,64)\n        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage4 = RSU4(64,16,64)\n        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage5 = RSU4F(64,16,64)\n        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage6 = RSU4F(64,16,64)\n\n        # decoder\n        self.stage5d = RSU4F(128,16,64)\n        self.stage4d = RSU4(128,16,64)\n        self.stage3d = RSU5(128,16,64)\n        self.stage2d = RSU6(128,16,64)\n        self.stage1d = RSU7(128,16,64)\n\n        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side3 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side4 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side5 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side6 = nn.Conv2d(64,out_ch,3,padding=1)\n\n        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n\n    def forward(self,x):\n\n        hx = x\n\n        #stage 1\n        hx1 = self.stage1(hx)\n        hx = self.pool12(hx1)\n\n        #stage 2\n        hx2 = self.stage2(hx)\n        hx = self.pool23(hx2)\n\n        #stage 3\n        hx3 = self.stage3(hx)\n        hx = self.pool34(hx3)\n\n        #stage 4\n        hx4 = self.stage4(hx)\n        hx = self.pool45(hx4)\n\n        #stage 5\n        hx5 = self.stage5(hx)\n        hx = self.pool56(hx5)\n\n        #stage 6\n        hx6 = self.stage6(hx)\n        hx6up = _upsample_like(hx6,hx5)\n\n        #decoder\n        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n        hx5dup = _upsample_like(hx5d,hx4)\n\n        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n\n\n        #side output\n        d1 = self.side1(hx1d)\n\n        d2 = self.side2(hx2d)\n        d2 = _upsample_like(d2,d1)\n\n        d3 = self.side3(hx3d)\n        d3 = _upsample_like(d3,d1)\n\n        d4 = self.side4(hx4d)\n        d4 = _upsample_like(d4,d1)\n\n        d5 = self.side5(hx5d)\n        d5 = _upsample_like(d5,d1)\n\n        d6 = self.side6(hx6)\n        d6 = _upsample_like(d6,d1)\n\n        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n\n        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n"}
{"type": "source_file", "path": "utils/autoaugment.py", "content": "\"\"\"\nCopy from https://github.com/DeepVoltaire/AutoAugment/blob/master/autoaugment.py\n\"\"\"\n\nfrom PIL import Image, ImageEnhance, ImageOps\nimport numpy as np\nimport random\n\n__all__ = ['AutoAugImageNetPolicy', 'AutoAugCIFAR10Policy', 'AutoAugSVHNPolicy']\n\n\nclass AutoAugImageNetPolicy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment ImageNet Policy\"\n\n\nclass AutoAugCIFAR10Policy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR10 Policy\"\n\n\nclass AutoAugSVHNPolicy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.9, \"shearX\", 4, 0.2, \"invert\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.7, \"invert\", 5, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.6, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 3, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"equalize\", 1, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.8, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.4, \"invert\", 5, fillcolor),\n            SubPolicy(0.9, \"shearY\", 5, 0.2, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 6, 0.8, \"autocontrast\", 1, fillcolor),\n            SubPolicy(0.6, \"equalize\", 3, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.3, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 8, 0.7, \"invert\", 4, fillcolor),\n            SubPolicy(0.9, \"equalize\", 5, 0.6, \"translateY\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 4, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.3, \"contrast\", 3, 0.8, \"rotate\", 4, fillcolor),\n\n            SubPolicy(0.8, \"invert\", 5, 0.0, \"translateY\", 2, fillcolor),\n            SubPolicy(0.7, \"shearY\", 6, 0.4, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 0.8, \"rotate\", 4, fillcolor),\n            SubPolicy(0.3, \"shearY\", 7, 0.9, \"translateX\", 3, fillcolor),\n            SubPolicy(0.1, \"shearX\", 6, 0.6, \"invert\", 5, fillcolor),\n\n            SubPolicy(0.7, \"solarize\", 2, 0.6, \"translateY\", 7, fillcolor),\n            SubPolicy(0.8, \"shearY\", 4, 0.8, \"invert\", 8, fillcolor),\n            SubPolicy(0.7, \"shearX\", 9, 0.8, \"translateY\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 5, 0.7, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.7, \"shearX\", 2, 0.1, \"invert\", 5, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment SVHN Policy\"\n\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            \"shearX\": np.linspace(0, 0.3, 10),\n            \"shearY\": np.linspace(0, 0.3, 10),\n            \"translateX\": np.linspace(0, 150 / 331, 10),\n            \"translateY\": np.linspace(0, 150 / 331, 10),\n            \"rotate\": np.linspace(0, 30, 10),\n            \"color\": np.linspace(0.0, 0.9, 10),\n            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n            \"solarize\": np.linspace(256, 0, 10),\n            \"contrast\": np.linspace(0.0, 0.9, 10),\n            \"sharpness\": np.linspace(0.0, 0.9, 10),\n            \"brightness\": np.linspace(0.0, 0.9, 10),\n            \"autocontrast\": [0] * 10,\n            \"equalize\": [0] * 10,\n            \"invert\": [0] * 10\n        }\n\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(\"RGBA\").rotate(magnitude)\n            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            \"shearX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"shearY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"translateX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            \"translateY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n        #     operation1, ranges[operation1][magnitude_idx1],\n        #     operation2, ranges[operation2][magnitude_idx2])\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n    def __call__(self, img):\n        if random.random() < self.p1:\n            img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2:\n            img = self.operation2(img, self.magnitude2)\n        return img\n"}
{"type": "source_file", "path": "utils/dataset.py", "content": "import os\nimport json\nfrom os.path import join\n\nimport numpy as np\nimport scipy\nfrom scipy import io\nimport scipy.misc\nfrom PIL import Image\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets import VisionDataset\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.datasets.utils import download_url, list_dir, check_integrity, extract_archive, verify_str_arg\n\n\n# My:\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nimport random\nfrom torchvision.transforms import functional as F\n\nimport U2Net\nfrom U2Net.u2net_test import mask_hw\n\nfrom skimage import transform as transform_sk\nimport gc\n#\n\n\n\nclass Generic_smvit_DS():\n\n\n    def generic_preprocess(self, file_list, file_list_full, shape_hw_list, data_len, train_test_list=None):\n\n        img = []\n        mask = []\n\n        ## For other experiments:\n        if self.ds_name != \"CUB\":\n            self.gt_bbox = False\n            self.gt_parts = False\n\n        if self.gt_bbox:\n            bounding_boxes_file = open(os.path.join(self.root, 'bounding_boxes.txt'))\n\n            bb_list = []\n            for line in bounding_boxes_file:\n                bb_list_x = line[:-1].split(' ')[-4]\n                bb_list_y = line[:-1].split(' ')[-3]\n                bb_list_w = line[:-1].split(' ')[-2]\n                bb_list_h = line[:-1].split(' ')[-1]\n\n                bb_list.append( [ int(bb_list_x.split('.')[0]),\n                                    int(bb_list_y.split('.')[0]),\n                                    int(bb_list_w.split('.')[0]),\n                                    int(bb_list_h.split('.')[0]) ]\n                                    )\n\n            bb_list = [x for i, x in zip(train_test_list, bb_list) if i]\n\n        if self.gt_parts:\n            parts_file = open(os.path.join(self.root, 'parts/part_locs.txt'))\n\n            PARTS_NUM = 15\n            parts_list = []\n            part_t = []\n            part_count = 0\n\n            for line in parts_file:\n                part_t_raw_x = line[:-1].split(' ')[-3]\n                part_t_raw_y = line[:-1].split(' ')[-2]\n                part_t_pres = line[:-1].split(' ')[-1]\n\n                part_t.append ( [ int(part_t_pres),\n                                    int(part_t_raw_x.split('.')[0]),\n                                    int(part_t_raw_y.split('.')[0]) ]\n                                    )\n                part_count = part_count + 1\n\n                if (part_count >= PARTS_NUM):\n                    parts_list.append( part_t )\n                    part_t = []\n                    part_count = 0\n\n            parts_list = [x for i, x in zip(train_test_list, parts_list) if i]\n        ##\n\n\n        print(f'[INFO] Pre-processing {self.mode} files...')\n\n        if self.sm_vit:\n            if self.full_ds:\n                mask_u2n_list, x_u2n_list, y_u2n_list, h_u2n_list, w_u2n_list = \\\n                    mask_hw(full_ds=self.full_ds, img_path=file_list_full, shape_hw=shape_hw_list)\n            else: # for debug\n                img_path = os.path.join(self.root, self.base_folder, file_list)\n                img_temp = scipy.misc.imread(img_path)\n                h_max_temp = img_temp.shape[0] # y\n                w_max_temp = img_temp.shape[1] # x\n                mask_u2n, x_u2n, y_u2n, h_u2n, w_u2n = \\\n                    mask_hw(full_ds=self.full_ds, img_path=img_path, shape_hw=(h_max_temp, w_max_temp))\n                mask_temp, x, y, h, w = mask_u2n, x_u2n, y_u2n, h_u2n, w_u2n\n\n        for ind, file in enumerate(file_list[:data_len]):\n\n            if self.debug: print(f\"{self.mode} file:\", file)\n\n            img_temp = scipy.misc.imread(os.path.join(self.root, self.base_folder, file))\n\n\n            ## Downscale large images for memory efficiency\n            if self.ds_name != \"CUB\":\n\n                img_temp = (img_temp).astype(np.uint8)\n\n                if (img_temp.shape[0] > self.max_res) or (img_temp.shape[1] > self.max_res):\n\n                    if self.debug and ind < 10:\n                        print(\"Before:\", img_temp.shape[0], img_temp.shape[1])\n                        img_name = (\"test/img_before_tr\" + str(ind) + \".png\")\n                        Image.fromarray(img_temp, mode='RGB').save(img_name)\n\n                    if img_temp.shape[0] > img_temp.shape[1]:\n                        downscale_coef = img_temp.shape[0] / self.max_res \n                    else:\n                        downscale_coef = img_temp.shape[1] / self.max_res \n                    \n                    img_temp = transform_sk.resize( img_temp, ( int((img_temp.shape[0] // downscale_coef)), int((img_temp.shape[1] // downscale_coef)) ), \\\n                                                                mode='constant', anti_aliasing=True, anti_aliasing_sigma=None, preserve_range=True )\n                \n                    if self.debug and ind < 10:\n                        print(\"After:\", img_temp.shape[0], img_temp.shape[1])\n                        img_temp = (img_temp).astype(np.uint8)\n                        img_name = (\"test/img_after_tr\" + str(ind) + \".png\")\n                        Image.fromarray(img_temp, mode='RGB').save(img_name)\n                else:\n                    if self.debug and ind < 10:\n                        print(\"Normal:\", img_temp.shape[0], img_temp.shape[1])\n                        img_name = (\"test/img_normal_tr\" + str(ind) + \".png\")\n                        Image.fromarray(img_temp, mode='RGB').save(img_name)\n\n            h_max = img_temp.shape[0] # y\n            w_max = img_temp.shape[1] # x\n            #ch_max = img_temp.shape[2] # ch\n\n            if self.gt_bbox:\n                x, y, w, h = bb_list[ind] # x - distance from top up left (width), y - distance from top up left (height)\n            \n            if self.gt_parts:\n                parts = parts_list[ind] # list of 15 parts with [x, y] center corrdinates\n\n                #mask_temp = np.zeros((int(h_max), int(w_max))) # Black mask\n                mask_temp = np.ones((int(h_max), int(w_max)))\n\n                p_part = 16*3 # padding around center point\n\n                for part_n in range(len(parts)):\n                    part = parts[part_n]\n\n                    if part[0] != 0:\n                        x_min_p = part[1] - p_part\n                        if x_min_p < 0:\n                            x_min_p = 0\n                        x_max_p = part[1] + p_part\n                        if x_max_p > w_max:\n                            x_max_p = w_max\n\n                        y_min_p = part[2] - p_part\n                        if y_min_p < 0:\n                            y_min_p = 0\n                        y_max_p = part[2] + p_part\n                        if y_max_p > h_max:\n                            y_max_p = h_max\n\n                        #mask_temp[int(y_min_p):int(y_max_p), int(x_min_p):int(x_max_p)] = 1 # Black mask\n                        mask_temp[int(y_min_p):int(y_max_p), int(x_min_p):int(x_max_p)] = 0\n\n            if self.sm_vit and self.full_ds:\n                mask_temp = mask_u2n_list[ind]\n                x = x_u2n_list[ind]\n                y = y_u2n_list[ind]\n                h = h_u2n_list[ind]\n                w = w_u2n_list[ind]\n\n\n            ## Image and Mask Padding:\n            if self.sm_vit or self.gt_bbox:\n                if self.padding:\n                    p = 15 # extra space around bbox\n                else:\n                    p = 0\n\n                x_min = x - p \n                if x_min < 0:\n                    x_min = 0\n                x_max = x + w + p\n                if x_max > w_max:\n                    x_max = w_max\n\n                y_min = y - p\n                if y_min < 0:\n                    y_min = 0\n                y_max = y + h + p\n                if y_max > h_max:\n                    y_max = h_max\n\n                if h_max <=1:\n                    print(\"[WARNING] bad_h\", h_max)\n                if w_max <=1:\n                    print(\"[WARNING] bad_w\", w_max)\n                if y_min >= y_max:\n                    print(\"[WARNING] bad_y\", \"min:\", y_min, \"max:\", y_max)\n                    print(\"[WARNING] y:\", y, \"h:\", h)\n                if x_min >= x_max:\n                    print(\"[WARNING] bad_x\", \"min:\", x_min, \"max:\", x_max)\n                    print(\"[WARNING] x:\", x, \"w:\", w)                                  \n            ##\n\n\n            ## Crop with bbox:\n            if self.rand_crop:\n                #prob_rcrop = 0.25 # 0.07 # 0.3 # 0.5\n                #rand_crop_mask_temp = bool(random.random() < prob_rcrop)\n                #if rand_crop_mask_temp:\n\n                h_max_img = img_temp.shape[0]\n                w_max_img = img_temp.shape[1]\n\n                #h_crop_mid = 368 # 384 (92%), 368 (84%), 352 (77%), 336 (70%), 320 (64%), 304 (57%)\n                h_crop_mid_img = int(h_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n                #w_crop_mid = 368 # 384 (92%), 368 (84%), 352 (77%), 336 (70%), 320 (64%), 304 (57%)\n                w_crop_mid_img = int(w_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n                h_crop_min_img = random.randint(0, (h_max_img - h_crop_mid_img)) # 40) #, 400-360) #, h - th)\n                w_crop_min_img = random.randint(0, (w_max_img - w_crop_mid_img)) # 40)  #, 400-360) #, w - tw)\n\n                h_crop_max_img = h_crop_mid_img + h_crop_min_img\n                w_crop_max_img = w_crop_mid_img + w_crop_min_img\n\n                # Crop image with bbox:\n                if len(img_temp.shape) == 3:\n                    img_temp = img_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img), :] # h, w, ch\n                else:\n                    img_temp = img_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)] # h, w\n\n                # Crop mask with bbox:\n                mask_temp = mask_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)]\n\n            else:\n                # Crop image with bbox:\n                if len(img_temp.shape) == 3:\n                    if self.gt_parts:\n                        for j in range(3):\n                            img_temp[:, :, j] = img_temp[:, :, j] * mask_temp # Black mask\n                    else:\n                        #test_img_temp = test_img_temp[int(y):int(y + h), int(x):int(x + w), :] # h, w, ch\n                        img_temp = img_temp[int(y_min):int(y_max), int(x_min):int(x_max), :] # h, w, ch\n                else:\n                    if self.gt_parts:                        \n                        img_temp[:, :] = img_temp[:, :] * mask_temp # Black mask:\n                    else:\n                        img_temp = img_temp[int(y_min):int(y_max), int(x_min):int(x_max)] # h, w\n\n                # Crop mask with bbox:\n                if self.sm_vit or self.gt_bbox:\n                    mask_temp = mask_temp[int(y_min):int(y_max), int(x_min):int(x_max)]\n            ##\n\n\n            if ( (img_temp.shape[0] != mask_temp.shape[0]) or (img_temp.shape[1] != mask_temp.shape[1]) ):\n                print(\"[WARNING] Image shape does not match mask shape for sample:\", ind, \". \\t\" , \"Found shapes:\", img_temp.shape, mask_temp.shape)\n\n            img.append(img_temp)\n            mask.append(mask_temp)\n\n        return img, mask\n\n\n    def generic_preprocess_lowMem(self, file_list, file_list_full, shape_hw_list):\n\n        print(f'[INFO] Pre-processing {self.mode} files in the low memory mode...')\n\n        if self.sm_vit:\n            if self.full_ds:\n                mask_u2n_list, x_u2n_list, y_u2n_list, h_u2n_list, w_u2n_list = \\\n                    mask_hw(full_ds=self.full_ds, img_path=file_list_full, shape_hw=shape_hw_list)\n            else: # for debug\n                img_path = os.path.join(self.root, self.base_folder, file_list)\n                img_temp = scipy.misc.imread(img_path)\n                h_max_temp = img_temp.shape[0] # y\n                w_max_temp = img_temp.shape[1] # x\n                mask_u2n, x_u2n, y_u2n, h_u2n, w_u2n = \\\n                    mask_hw(full_ds=self.full_ds, img_path=img_path, shape_hw=(h_max_temp, w_max_temp))\n                mask_temp, x, y, h, w = mask_u2n, x_u2n, y_u2n, h_u2n, w_u2n\n                # mask_u2n_list, x_u2n_list, y_u2n_list, h_u2n_list, w_u2n_list = mask_temp, x, y, h, w\n\n        return mask_u2n_list, x_u2n_list, y_u2n_list, h_u2n_list, w_u2n_list\n\n\n    def generic_getitem(self, index,  img, mask):\n\n        if self.is_train:\n            if self.rand_crop_im_mask:\n                h_max_img = img.shape[0]\n                w_max_img = img.shape[1]\n\n                h_crop_mid_img = int(h_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n                w_crop_mid_img = int(w_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n                h_crop_min_img = random.randint(0, (h_max_img - h_crop_mid_img)) # 40) #, 400-360) #, h - th)\n                w_crop_min_img = random.randint(0, (w_max_img - w_crop_mid_img)) # 40)  #, 400-360) #, w - tw)\n\n                h_crop_max_img = h_crop_mid_img + h_crop_min_img\n                w_crop_max_img = w_crop_mid_img + w_crop_min_img\n\n                # Crop image:\n                if len(img.shape) == 3:\n                    img = img[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img), :] # h, w, ch\n                else:\n                    img = img[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)] # h, w\n\n                # Crop mask:\n                mask = mask[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)]\n\n        if len(img.shape) == 2:\n            img = np.stack([img] * 3, 2)\n\n        if self.ds_name != \"CUB\":\n            img = (img).astype(np.uint8)\n\n        img = Image.fromarray(img, mode='RGB')\n\n        if self.debug and index < 10:\n            img_tem = transforms.ToTensor()(img)\n            img_name = (\"test/img_bef\" + str(index) + \".png\")\n            save_image( img_tem, img_name)\n        \n\n        ## Image:\n        if self.transform is not None:\n            if self.is_train:\n                if not self.flip_mask_as_image: # normal\n                    img = self.transform(img) \n                else:\n                    if random.random() < 0.5:\n                        flipped = False\n                        img = self.transform(img)\n                    else: \n                        flipped = True\n                        transform_img_flip = transforms.Compose([\n                            #transforms.Resize((args.resize_size, args.resize_size),Image.BILINEAR),\n                            #transforms.RandomCrop((args.img_size, args.img_size)),\n                        \n                            transforms.Resize((self.img_size, self.img_size),Image.BILINEAR), # my for bbox\n\n                            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # my add (FFVT)\n                            transforms.RandomHorizontalFlip(p=1.0), # !!! FLIPPING in dataset.py !!!\n\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                            ])\n                        img = transform_img_flip(img)\n            else:\n                img = self.transform(img)\n\n        if self.debug and index < 10:\n            img_name = (\"test/img_aft\" + str(index) + \".png\")\n            save_image( img, img_name)            \n\n\n        ## Mask:\n        if self.crop_mask:\n            h_max_im = mask.shape[0]\n            w_max_im = mask.shape[1]\n\n            h_crop_mid = int(h_max_im * 0.84) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n            w_crop_mid = int(w_max_im * 0.84) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n            cropped = np.ones_like(mask)\n\n            if self.mid_val:\n                cropped = cropped * 0.125 # (for 0.2)\n\n            h_crop_min = random.randint(0, (h_max_im - h_crop_mid)) # 40) #, 400-360) #, h - th)\n            w_crop_min = random.randint(0, (w_max_im - w_crop_mid)) # 40)  #, 400-360) #, w - tw)\n\n            h_crop_max = h_crop_mid + h_crop_min\n            w_crop_max = w_crop_mid + w_crop_min\n\n            cropped[int(h_crop_min):int(h_crop_max), int(w_crop_min):int(w_crop_max)] = 0\n            \n            mask = mask + cropped\n\n            if self.mid_val:\n                mask[mask > 1.1] = 1\n            else:\n                mask[mask > 1] = 1\n\n        mask = (mask * 255).astype(np.uint8)\n        mask = Image.fromarray(mask, mode='L')            \n\n        if self.debug and index < 10:\n            mask_tem = transforms.ToTensor()(mask)\n            img_name = (\"test/mask_bef\" + str(index) + \".png\")\n            save_image( mask_tem, img_name)\n\n\n        mask_size = int(self.img_size // 16)\n\n        if self.is_train:\n            if not self.flip_mask_as_image: # normal\n                transform_mask = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.ToPILImage(), #(mode='1'),\n\n                    # non-overlapped:\n                    transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                    transforms.ToTensor()\n                    ])\n            else:\n                if flipped:\n                    transform_mask = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.ToPILImage(), #(mode='1'),\n                        transforms.RandomHorizontalFlip(p=1.0),\n                                                                        \n                        # non-overlapped:\n                        transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                        transforms.ToTensor()\n                        ])\n                else:\n                    transform_mask = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.ToPILImage(), #(mode='1'),\n                                        \n                        # non-overlapped:\n                        transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                        transforms.ToTensor()\n                        ])\n        else:\n            transform_mask = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.ToPILImage(), #(mode='1'),\n                                 \n                # non-overlapped:\n                transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                transforms.ToTensor()])\n\n        mask = transform_mask(mask)\n\n        if self.debug and index < 10:\n            img_name = (\"test/mask_aft\" + str(index) + \".png\")\n            save_image(mask, img_name)\n\n        mask = torch.flatten(mask)\n\n        return img, mask\n\n\n    def generic_getitem_lowMem(self, index):\n\n        file_temp = self.file_list[index]\n        img_temp = scipy.misc.imread(os.path.join(self.root, self.base_folder, file_temp))\n\n        ## Downscale large images for memory efficiency\n        if self.ds_name != \"CUB\":\n\n            self.gt_bbox = False\n            self.gt_parts = False\n\n            img_temp = (img_temp).astype(np.uint8)\n\n            if (img_temp.shape[0] > self.max_res) or (img_temp.shape[1] > self.max_res):\n\n                if self.debug and index < 10:\n                    print(\"Before:\", img_temp.shape[0], img_temp.shape[1])\n                    img_name = (\"test/img_before_tr\" + str(index) + \".png\")\n                    Image.fromarray(img_temp, mode='RGB').save(img_name)\n\n                if img_temp.shape[0] > img_temp.shape[1]:\n                    downscale_coef = img_temp.shape[0] / self.max_res \n                else:\n                    downscale_coef = img_temp.shape[1] / self.max_res \n                \n                img_temp = transform_sk.resize( img_temp, ( int((img_temp.shape[0] // downscale_coef)), int((img_temp.shape[1] // downscale_coef)) ), \\\n                                                            mode='constant', anti_aliasing=True, anti_aliasing_sigma=None, preserve_range=True )\n            \n                if self.debug and index < 10:\n                    print(\"After:\", img_temp.shape[0], img_temp.shape[1])\n                    img_temp = (img_temp).astype(np.uint8)\n                    img_name = (\"test/img_after_tr\" + str(index) + \".png\")\n                    Image.fromarray(img_temp, mode='RGB').save(img_name)\n            else:\n                if self.debug and index < 10:\n                    print(\"Normal:\", img_temp.shape[0], img_temp.shape[1])\n                    img_name = (\"test/img_normal_tr\" + str(index) + \".png\")\n                    Image.fromarray(img_temp, mode='RGB').save(img_name)        \n        ##\n\n\n        h_max = img_temp.shape[0] # y\n        w_max = img_temp.shape[1] # x\n        #ch_max = img_temp.shape[2] # ch\n\n        mask_temp = self.mask_u2n_list[index]\n        x, y, h, w = self.x_u2n_list[index], self.y_u2n_list[index], self.h_u2n_list[index], self.w_u2n_list[index]\n\n\n        ## Image and Mask Padding:\n        if self.sm_vit or self.gt_bbox:\n            if self.padding:\n                p = 15 # extra space around bbox\n            else:\n                p = 0\n\n            x_min = x - p \n            if x_min < 0:\n                x_min = 0\n            x_max = x + w + p\n            if x_max > w_max:\n                x_max = w_max\n\n            y_min = y - p\n            if y_min < 0:\n                y_min = 0\n            y_max = y + h + p\n            if y_max > h_max:\n                y_max = h_max\n\n            if h_max <=1:\n                print(\"[WARNING] bad_h\", h_max)\n            if w_max <=1:\n                print(\"[WARNING] bad_w\", w_max)\n            if y_min >= y_max:\n                print(\"[WARNING] bad_y\", \"min:\", y_min, \"max:\", y_max)\n                print(\"[WARNING] y:\", y, \"h:\", h)\n            if x_min >= x_max:\n                print(\"[WARNING] bad_x\", \"min:\", x_min, \"max:\", x_max)\n                print(\"[WARNING] x:\", x, \"w:\", w)                                  \n        ##\n\n\n        ## Crop with bbox:\n        if self.rand_crop:\n            #prob_rcrop = 0.25 # 0.07 # 0.3 # 0.5\n            #rand_crop_mask_temp = bool(random.random() < prob_rcrop)\n            #if rand_crop_mask_temp:\n\n            h_max_img = img_temp.shape[0]\n            w_max_img = img_temp.shape[1]\n\n            #h_crop_mid = 368 # 384 (92%), 368 (84%), 352 (77%), 336 (70%), 320 (64%), 304 (57%)\n            h_crop_mid_img = int(h_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n            #w_crop_mid = 368 # 384 (92%), 368 (84%), 352 (77%), 336 (70%), 320 (64%), 304 (57%)\n            w_crop_mid_img = int(w_max_img * 0.88) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n            h_crop_min_img = random.randint(0, (h_max_img - h_crop_mid_img)) # 40) #, 400-360) #, h - th)\n            w_crop_min_img = random.randint(0, (w_max_img - w_crop_mid_img)) # 40)  #, 400-360) #, w - tw)\n\n            h_crop_max_img = h_crop_mid_img + h_crop_min_img\n            w_crop_max_img = w_crop_mid_img + w_crop_min_img\n\n            # Crop image with bbox:\n            if len(img_temp.shape) == 3:\n                img_temp = img_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img), :] # h, w, ch\n            else:\n                img_temp = img_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)] # h, w\n\n            # Crop mask with bbox:\n            mask_temp = mask_temp[int(h_crop_min_img):int(h_crop_max_img), int(w_crop_min_img):int(w_crop_max_img)]\n\n        else:\n            # Crop image with bbox:\n            if len(img_temp.shape) == 3:\n                if self.gt_parts:\n                    for j in range(3):\n                        img_temp[:, :, j] = img_temp[:, :, j] * mask_temp # Black mask\n                else:\n                    #test_img_temp = test_img_temp[int(y):int(y + h), int(x):int(x + w), :] # h, w, ch\n                    img_temp = img_temp[int(y_min):int(y_max), int(x_min):int(x_max), :] # h, w, ch\n            else:\n                if self.gt_parts:                        \n                    img_temp[:, :] = img_temp[:, :] * mask_temp # Black mask:\n                else:\n                    img_temp = img_temp[int(y_min):int(y_max), int(x_min):int(x_max)] # h, w\n\n            # Crop mask with bbox:\n            if self.sm_vit or self.gt_bbox:\n                mask_temp = mask_temp[int(y_min):int(y_max), int(x_min):int(x_max)]\n        ##\n\n\n        if ( (img_temp.shape[0] != mask_temp.shape[0]) or (img_temp.shape[1] != mask_temp.shape[1]) ):\n            print(\"[WARNING] Image shape does not match mask shape for sample:\", index, \". \\t\" , \\\n                        \"Found shapes:\", img_temp.shape, mask_temp.shape)\n\n\n        img = img_temp\n\n        if len(img.shape) == 2:\n            img = np.stack([img] * 3, 2)\n\n        if self.ds_name != \"CUB\":\n            img = (img).astype(np.uint8)\n\n        img = Image.fromarray(img, mode='RGB')\n\n        if self.debug and index < 10:\n            img_tem = transforms.ToTensor()(img)\n            img_name = (\"test/img_bef\" + str(index) + \".png\")\n            save_image( img_tem, img_name)\n        \n\n        ## Image:\n        if self.transform is not None:\n            if self.is_train:\n                if not self.flip_mask_as_image: # normal\n                    img = self.transform(img) \n                else:\n                    if random.random() < 0.5:\n                        flipped = False\n                        img = self.transform(img)\n                    else: \n                        flipped = True\n                        transform_img_flip = transforms.Compose([\n                            #transforms.Resize((args.resize_size, args.resize_size),Image.BILINEAR),\n                            #transforms.RandomCrop((args.img_size, args.img_size)),\n                        \n                            transforms.Resize((self.img_size, self.img_size),Image.BILINEAR), # my for bbox\n\n                            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # my add (FFVT)\n                            transforms.RandomHorizontalFlip(p=1.0), # !!! FLIPPING in dataset.py !!!\n\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                            ])\n                        img = transform_img_flip(img)\n            else:\n                img = self.transform(img)\n\n        if self.debug and index < 10:\n            img_name = (\"test/img_aft\" + str(index) + \".png\")\n            save_image( img, img_name)            \n\n\n        ## Mask:\n        mask = mask_temp\n\n        if self.crop_mask:\n            h_max_im = mask.shape[0]\n            w_max_im = mask.shape[1]\n\n            h_crop_mid = int(h_max_im * 0.84) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n            w_crop_mid = int(w_max_im * 0.84) # 384 (92% - 0.96), 368 (84% - 0.92), 352 (77% - 0.88), 336 (70% - 0.84), 320 (64% - 0.80), 304 (57% - 0.75)\n\n            cropped = np.ones_like(mask)\n\n            if self.mid_val:\n                cropped = cropped * 0.125 # (for 0.2)\n\n            h_crop_min = random.randint(0, (h_max_im - h_crop_mid)) # 40) #, 400-360) #, h - th)\n            w_crop_min = random.randint(0, (w_max_im - w_crop_mid)) # 40)  #, 400-360) #, w - tw)\n\n            h_crop_max = h_crop_mid + h_crop_min\n            w_crop_max = w_crop_mid + w_crop_min\n\n            cropped[int(h_crop_min):int(h_crop_max), int(w_crop_min):int(w_crop_max)] = 0\n            \n            mask = mask + cropped\n\n            if self.mid_val:\n                mask[mask > 1.1] = 1\n            else:\n                mask[mask > 1] = 1\n\n        mask = (mask * 255).astype(np.uint8)\n        mask = Image.fromarray(mask, mode='L')            \n\n        if self.debug and index < 10:\n            mask_tem = transforms.ToTensor()(mask)\n            img_name = (\"test/mask_bef\" + str(index) + \".png\")\n            save_image( mask_tem, img_name)\n\n\n        mask_size = int(self.img_size // 16)\n\n        if self.is_train:\n            if not self.flip_mask_as_image: # normal\n                transform_mask = transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.ToPILImage(), #(mode='1'),\n\n                    # non-overlapped:\n                    transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                    transforms.ToTensor()\n                    ])\n            else:\n                if flipped:\n                    transform_mask = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.ToPILImage(), #(mode='1'),\n                        transforms.RandomHorizontalFlip(p=1.0),\n                                                                        \n                        # non-overlapped:\n                        transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                        transforms.ToTensor()\n                        ])\n                else:\n                    transform_mask = transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.ToPILImage(), #(mode='1'),\n                                        \n                        # non-overlapped:\n                        transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                        transforms.ToTensor()\n                        ])\n        else:\n            transform_mask = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.ToPILImage(), #(mode='1'),\n                                 \n                # non-overlapped:\n                transforms.Resize((mask_size, mask_size), interpolation=Image.NEAREST), #Image.BILINEAR), # interpolation=T.InterpolationMode.NEAREST\n                transforms.ToTensor()])\n\n        mask = transform_mask(mask)\n\n        if self.debug and index < 10:\n            img_name = (\"test/mask_aft\" + str(index) + \".png\")\n            save_image(mask, img_name)\n\n        mask = torch.flatten(mask)\n\n        return img, mask\n\n\n\n\nclass CUB(Generic_smvit_DS):\n\n    def __init__(self, ds_name, root, is_train=True, data_len=None, transform=None, sm_vit=True, low_memory=True, img_size=400):\n\n        self.ds_name = ds_name\n        self.img_size = img_size\n        self.max_res = int(self.img_size * 1.5)\n\n\n        self.full_ds = True # pre-processing full dataset\n        self.padding = True # image and mask padding\n        self.rand_crop = False # if no other cropping\n\n        self.flip_mask_as_image = True # if False - turn on RandomHorizontalFlip in data_utils !!!\n        self.rand_crop_im_mask = False # randomly crop both image and mask\n\n        self.crop_mask = False # randomly crop mask only\n        self.mid_val = False # 3-state mask\n\n        self.debug = False # for debug info\n        if self.debug:\n            os.makedirs(\"./test\")\n\n        self.gt_bbox = False # for other experiments\n        self.gt_parts = False # for other experiments\n\n\n        self.sm_vit = sm_vit\n        self.low_memory = low_memory\n\n        if (self.sm_vit + self.gt_bbox + self.gt_parts) > 1 :\n            raise Exception(\"Only one cropping mode (SM-ViT, bbox, parts) can be chosen\")\n\n\n        self.root = root\n        self.base_folder = \"images\"\n        self.transform = transform\n\n        self.is_train = is_train\n\n        if self.is_train:\n            self.mode = \"Train\"\n        else:\n            self.mode = \"Test\"\n\n\n        img_txt_file = open(os.path.join(self.root, 'images.txt'))\n        label_txt_file = open(os.path.join(self.root, 'image_class_labels.txt'))\n        train_val_file = open(os.path.join(self.root, 'train_test_split.txt'))\n\n        img_name_list = []\n        for line in img_txt_file:\n            img_name_list.append(line[:-1].split(' ')[-1])\n        label_list = []\n        for line in label_txt_file:\n            label_list.append(int(line[:-1].split(' ')[-1]) - 1)\n        train_test_list = []\n        for line in train_val_file:\n            train_test_list.append(int(line[:-1].split(' ')[-1]))\n\n        if self.is_train:\n            self.file_list = [x for i, x in zip(train_test_list, img_name_list) if i]\n            file_list_full = [ os.path.join(self.root, self.base_folder, x)  for i, x in zip(train_test_list, img_name_list) if i]\n        else:\n            self.file_list = [x for i, x in zip(train_test_list, img_name_list) if not i]\n            file_list_full = [ os.path.join(self.root, self.base_folder, x)  for i, x in zip(train_test_list, img_name_list) if not i]\n\n\n        if self.sm_vit:\n            print(f\"[INFO] Preparing {self.mode} shape_hw list...\")\n\n            shape_hw_list = []\n\n            for img_name in self.file_list:\n                img_temp = scipy.misc.imread(os.path.join(self.root, self.base_folder, img_name))\n                shape_hw_temp = [img_temp.shape[0], img_temp.shape[1]] # h_max (y), w_max (x)\n                shape_hw_list.append(shape_hw_temp)\n\n            if self.low_memory:\n                self.mask_u2n_list, self.x_u2n_list, self.y_u2n_list, self.h_u2n_list, self.w_u2n_list = \\\n                        super(CUB, self).generic_preprocess_lowMem(self.file_list,\n                                                                    file_list_full, \n                                                                    shape_hw_list\n                                                                    )\n                del shape_hw_list\n                del file_list_full\n                gc.collect()                                                          \n            else:\n                self.img, self.mask = \\\n                    super(CUB, self).generic_preprocess(self.file_list, \n                                                        file_list_full, \n                                                        shape_hw_list,\n                                                        data_len,\n                                                        train_test_list\n                                                        )\n        else:\n            self.img = \\\n                [scipy.misc.imread(os.path.join(self.root, self.base_folder, file)) \\\n                    for file in self.file_list[:data_len]]\n\n        if self.is_train:\n            self.label = [x for i, x in zip(train_test_list, label_list) if i][:data_len]\n        else:\n            self.label = [x for i, x in zip(train_test_list, label_list) if not i][:data_len]\n\n        self.imgname = [x for x in self.file_list[:data_len]]\n\n\n\n    def __getitem__(self, index):\n\n        if self.sm_vit:\n            if self.low_memory:\n                target, imgname = self.label[index], self.imgname[index]\n                img, mask = super(CUB, self).generic_getitem_lowMem(index)\n            else:\n                img, target, imgname, mask = self.img[index], self.label[index], self.imgname[index], self.mask[index]\n                img, mask = super(CUB, self).generic_getitem(index, img, mask)\n            \n            return img, target, mask\n\n        else:\n            img, target, imgname = self.img[index], self.label[index], self.imgname[index]\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n\n            img = Image.fromarray(img, mode='RGB')\n            if self.transform is not None:\n                img = self.transform(img)\n\n            return img, target\n\n\n    def __len__(self):\n        return len(self.label)\n\n\n\n\nclass dogs(Generic_smvit_DS): #(Dataset):\n    \"\"\"`Stanford Dogs <http://vision.stanford.edu/aditya86/ImageNetDogs/>`_ Dataset.\n    Args:\n        root (string): Root directory of dataset where directory\n            ``omniglot-py`` exists.\n        cropped (bool, optional): If true, the images will be cropped into the bounding box specified\n            in the annotations\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        download (bool, optional): If true, downloads the dataset tar files from the internet and\n            puts it in root directory. If the tar files are already downloaded, they are not\n            downloaded again.\n    \"\"\"\n    folder = 'dog'\n    download_url_prefix = 'http://vision.stanford.edu/aditya86/ImageNetDogs'\n\n    def __init__(self,\n                 ds_name,\n                 root,\n                 is_train=True,\n                 cropped=False,\n                 transform=None,\n                 target_transform=None,\n                 download=False,\n                 sm_vit=True,\n                 low_memory=True,\n                 img_size=400):\n\n\n        self.ds_name = ds_name\n        self.img_size = img_size\n        self.max_res = int(self.img_size * 1.5)\n\n\n        self.full_ds = True # pre-processing full dataset\n        self.padding = True # image and mask padding\n        self.rand_crop = False # if no other cropping\n    \n        self.flip_mask_as_image = True # if False - turn on RandomHorizontalFlip in data_utils !!!\n        self.rand_crop_im_mask = False # randomly crop both image and mask\n\n        self.crop_mask = False # randomly crop mask only\n        self.mid_val = False # 3-state mask\n\n        self.debug = False # for debug info\n        if self.debug:\n            os.makedirs(\"./test\")\n\n\n        self.sm_vit = sm_vit\n        self.low_memory = low_memory\n\n        # self.root = join(os.path.expanduser(root), self.folder)\n        self.root = root\n        self.base_folder = \"Images\"\n\n        self.is_train = is_train\n\n        if self.is_train:\n            self.mode = \"Train\"\n        else:\n            self.mode = \"Test\"\n\n        self.cropped = cropped\n        self.transform = transform\n        self.target_transform = target_transform\n\n        if download:\n            self.download()\n\n        split = self.load_split()\n\n        self.images_folder = join(self.root, 'Images')\n        self.annotations_folder = join(self.root, 'Annotation')\n        self._breeds = list_dir(self.images_folder)\n\n        if self.cropped:\n            self._breed_annotations = [[(annotation, box, idx)\n                                        for box in self.get_boxes(join(self.annotations_folder, annotation))]\n                                        for annotation, idx in split]\n            self._flat_breed_annotations = sum(self._breed_annotations, [])\n\n            self._flat_breed_images = [(annotation+'.jpg', idx) for annotation, box, idx in self._flat_breed_annotations]\n        else:\n            self._breed_images = [(annotation+'.jpg', idx) for annotation, idx in split]\n\n            self._flat_breed_images = self._breed_images\n\n        self.classes = [\"Chihuaha\",\n                        \"Japanese Spaniel\",\n                        \"Maltese Dog\",\n                        \"Pekinese\",\n                        \"Shih-Tzu\",\n                        \"Blenheim Spaniel\",\n                        \"Papillon\",\n                        \"Toy Terrier\",\n                        \"Rhodesian Ridgeback\",\n                        \"Afghan Hound\",\n                        \"Basset Hound\",\n                        \"Beagle\",\n                        \"Bloodhound\",\n                        \"Bluetick\",\n                        \"Black-and-tan Coonhound\",\n                        \"Walker Hound\",\n                        \"English Foxhound\",\n                        \"Redbone\",\n                        \"Borzoi\",\n                        \"Irish Wolfhound\",\n                        \"Italian Greyhound\",\n                        \"Whippet\",\n                        \"Ibizian Hound\",\n                        \"Norwegian Elkhound\",\n                        \"Otterhound\",\n                        \"Saluki\",\n                        \"Scottish Deerhound\",\n                        \"Weimaraner\",\n                        \"Staffordshire Bullterrier\",\n                        \"American Staffordshire Terrier\",\n                        \"Bedlington Terrier\",\n                        \"Border Terrier\",\n                        \"Kerry Blue Terrier\",\n                        \"Irish Terrier\",\n                        \"Norfolk Terrier\",\n                        \"Norwich Terrier\",\n                        \"Yorkshire Terrier\",\n                        \"Wirehaired Fox Terrier\",\n                        \"Lakeland Terrier\",\n                        \"Sealyham Terrier\",\n                        \"Airedale\",\n                        \"Cairn\",\n                        \"Australian Terrier\",\n                        \"Dandi Dinmont\",\n                        \"Boston Bull\",\n                        \"Miniature Schnauzer\",\n                        \"Giant Schnauzer\",\n                        \"Standard Schnauzer\",\n                        \"Scotch Terrier\",\n                        \"Tibetan Terrier\",\n                        \"Silky Terrier\",\n                        \"Soft-coated Wheaten Terrier\",\n                        \"West Highland White Terrier\",\n                        \"Lhasa\",\n                        \"Flat-coated Retriever\",\n                        \"Curly-coater Retriever\",\n                        \"Golden Retriever\",\n                        \"Labrador Retriever\",\n                        \"Chesapeake Bay Retriever\",\n                        \"German Short-haired Pointer\",\n                        \"Vizsla\",\n                        \"English Setter\",\n                        \"Irish Setter\",\n                        \"Gordon Setter\",\n                        \"Brittany\",\n                        \"Clumber\",\n                        \"English Springer Spaniel\",\n                        \"Welsh Springer Spaniel\",\n                        \"Cocker Spaniel\",\n                        \"Sussex Spaniel\",\n                        \"Irish Water Spaniel\",\n                        \"Kuvasz\",\n                        \"Schipperke\",\n                        \"Groenendael\",\n                        \"Malinois\",\n                        \"Briard\",\n                        \"Kelpie\",\n                        \"Komondor\",\n                        \"Old English Sheepdog\",\n                        \"Shetland Sheepdog\",\n                        \"Collie\",\n                        \"Border Collie\",\n                        \"Bouvier des Flandres\",\n                        \"Rottweiler\",\n                        \"German Shepard\",\n                        \"Doberman\",\n                        \"Miniature Pinscher\",\n                        \"Greater Swiss Mountain Dog\",\n                        \"Bernese Mountain Dog\",\n                        \"Appenzeller\",\n                        \"EntleBucher\",\n                        \"Boxer\",\n                        \"Bull Mastiff\",\n                        \"Tibetan Mastiff\",\n                        \"French Bulldog\",\n                        \"Great Dane\",\n                        \"Saint Bernard\",\n                        \"Eskimo Dog\",\n                        \"Malamute\",\n                        \"Siberian Husky\",\n                        \"Affenpinscher\",\n                        \"Basenji\",\n                        \"Pug\",\n                        \"Leonberg\",\n                        \"Newfoundland\",\n                        \"Great Pyrenees\",\n                        \"Samoyed\",\n                        \"Pomeranian\",\n                        \"Chow\",\n                        \"Keeshond\",\n                        \"Brabancon Griffon\",\n                        \"Pembroke\",\n                        \"Cardigan\",\n                        \"Toy Poodle\",\n                        \"Miniature Poodle\",\n                        \"Standard Poodle\",\n                        \"Mexican Hairless\",\n                        \"Dingo\",\n                        \"Dhole\",\n                        \"African Hunting Dog\"]\n\n\n        data_len = None\n\n        if self.sm_vit:\n            print(f\"[INFO] Preparing {self.mode} shape_hw list...\")\n\n            shape_hw_list = []\n            self.file_list = []\n            file_list_full = []\n\n            for image_name, target_class in self._flat_breed_images:\n                img_name = join(self.images_folder, image_name)\n                img_temp = scipy.misc.imread(os.path.join(img_name))\n                shape_hw_temp = [img_temp.shape[0], img_temp.shape[1]] # h_max (y), w_max (x)\n\n                if (shape_hw_temp[0] > self.max_res) or (shape_hw_temp[1] > self.max_res):\n                    if shape_hw_temp[0] > shape_hw_temp[1]:\n                        downscale_coef = shape_hw_temp[0] / self.max_res\n                    else:\n                        downscale_coef = shape_hw_temp[1] / self.max_res\n                        \n                    shape_hw_temp[0] = int(shape_hw_temp[0] // downscale_coef)\n                    shape_hw_temp[1] = int(shape_hw_temp[1] // downscale_coef)\n\n                shape_hw_list.append(shape_hw_temp)\n                self.file_list.append(image_name)\n                file_list_full.append(img_name)\n\n            if self.low_memory:\n                self.mask_u2n_list, self.x_u2n_list, self.y_u2n_list, self.h_u2n_list, self.w_u2n_list = \\\n                        super(dogs, self).generic_preprocess_lowMem(self.file_list,\n                                                                    file_list_full, \n                                                                    shape_hw_list\n                                                                    )\n                del shape_hw_list\n                del file_list_full\n                gc.collect()                                                          \n            else:\n                self.img, self.mask = \\\n                    super(dogs, self).generic_preprocess(self.file_list, \n                                                        file_list_full, \n                                                        shape_hw_list,\n                                                        data_len\n                                                        )            \n\n        if self.is_train:\n            self.label = [x for i, x in self._flat_breed_images][:data_len]\n        else:\n            self.label = [x for i, x in self._flat_breed_images][:data_len]\n\n        self.imgname = [x for x in self.file_list[:data_len]]                  \n\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is index of the target character class.\n        \"\"\"\n\n        if self.sm_vit:\n            if self.low_memory:\n                target, imgname = self.label[index], self.imgname[index]\n                img, mask = super(dogs, self).generic_getitem_lowMem(index)\n            else:\n                img, target, imgname, mask = self.img[index], self.label[index], self.imgname[index], self.mask[index]\n                img, mask = super(dogs, self).generic_getitem(index, img, mask)\n            \n            return img, target, mask\n\n        else:\n            image_name, target = self._flat_breed_images[index]\n            image_path = join(self.images_folder, image_name)\n            img = Image.open(image_path).convert('RGB')\n\n            if self.cropped:\n                img = img.crop(self._flat_breed_annotations[index][1])\n\n            if self.transform:\n                img = self.transform(img)\n\n            if self.target_transform:\n                target = self.target_transform(target)\n\n            return img, target\n\n\n    def __len__(self):\n        return len(self._flat_breed_images)\n\n\n    def download(self):\n        import tarfile\n\n        if os.path.exists(join(self.root, 'Images')) and os.path.exists(join(self.root, 'Annotation')):\n            if len(os.listdir(join(self.root, 'Images'))) == len(os.listdir(join(self.root, 'Annotation'))) == 120:\n                print('Files already downloaded and verified')\n                return\n\n        for filename in ['images', 'annotation', 'lists']:\n            tar_filename = filename + '.tar'\n            url = self.download_url_prefix + '/' + tar_filename\n            download_url(url, self.root, tar_filename, None)\n            print('Extracting downloaded file: ' + join(self.root, tar_filename))\n            with tarfile.open(join(self.root, tar_filename), 'r') as tar_file:\n                tar_file.extractall(self.root)\n            os.remove(join(self.root, tar_filename))\n\n\n    @staticmethod\n    def get_boxes(path):\n        import xml.etree.ElementTree\n        e = xml.etree.ElementTree.parse(path).getroot()\n        boxes = []\n        for objs in e.iter('object'):\n            boxes.append([int(objs.find('bndbox').find('xmin').text),\n                          int(objs.find('bndbox').find('ymin').text),\n                          int(objs.find('bndbox').find('xmax').text),\n                          int(objs.find('bndbox').find('ymax').text)])\n        return boxes\n\n\n    def load_split(self):\n        if self.is_train:\n            # split = scipy.io.loadmat(join(self.root, 'train_list.mat'))['annotation_list']\n            # labels = scipy.io.loadmat(join(self.root, 'train_list.mat'))['labels']\n            split = scipy.io.loadmat(join(self.root, 'splits/train_list.mat'))['annotation_list']\n            labels = scipy.io.loadmat(join(self.root, 'splits/train_list.mat'))['labels']\n        else:\n            # split = scipy.io.loadmat(join(self.root, 'test_list.mat'))['annotation_list']\n            # labels = scipy.io.loadmat(join(self.root, 'test_list.mat'))['labels']\n            split = scipy.io.loadmat(join(self.root, 'splits/test_list.mat'))['annotation_list']\n            labels = scipy.io.loadmat(join(self.root, 'splits/test_list.mat'))['labels']\n\n        split = [item[0][0] for item in split]\n        labels = [item[0]-1 for item in labels]\n        return list(zip(split, labels))\n\n\n    def stats(self):\n        counts = {}\n        for index in range(len(self._flat_breed_images)):\n            image_name, target_class = self._flat_breed_images[index]\n            if target_class not in counts.keys():\n                counts[target_class] = 1\n            else:\n                counts[target_class] += 1\n\n        print(\"%d samples spanning %d classes (avg %f per class)\"%(len(self._flat_breed_images), len(counts.keys()), float(len(self._flat_breed_images))/float(len(counts.keys()))))\n\n        return counts\n\n\n\n\nclass NABirds(Generic_smvit_DS): #(Dataset):\n    \"\"\"`NABirds <https://dl.allaboutbirds.org/nabirds>`_ Dataset.\n\n        Args:\n            root (string): Root directory of the dataset.\n            train (bool, optional): If True, creates dataset from training set, otherwise\n               creates from test set.\n            transform (callable, optional): A function/transform that  takes in an PIL image\n               and returns a transformed version. E.g, ``transforms.RandomCrop``\n            target_transform (callable, optional): A function/transform that takes in the\n               target and transforms it.\n            download (bool, optional): If true, downloads the dataset from the internet and\n               puts it in root directory. If dataset is already downloaded, it is not\n               downloaded again.\n    \"\"\"\n    #base_folder = 'nabirds/images'\n\n    def __init__(self, ds_name, root, is_train=True, data_len=None, transform=None, sm_vit=True, low_memory=True, img_size=448):\n\n        self.ds_name = ds_name\n        self.img_size = img_size\n        self.max_res = int(self.img_size * 1.25) # 1.5\n\n\n        self.full_ds = True # pre-processing full dataset\n        self.padding = True # image and mask padding\n        self.rand_crop = False # if no other cropping\n\n        self.flip_mask_as_image = True # if False - turn on RandomHorizontalFlip in data_utils !!!\n        self.rand_crop_im_mask = False # randomly crop both image and mask\n\n        self.crop_mask = False # randomly crop mask only\n        self.mid_val = False # 3-state mask\n\n        self.debug = False # for debug info\n        if self.debug:\n            os.makedirs(\"./test\")\n\n\n        self.sm_vit = sm_vit\n        self.low_memory = low_memory\n\n        dataset_path = os.path.join(root)\n        self.root = root\n        self.base_folder = \"images\"\n\n        self.loader = default_loader\n        self.transform = transform\n\n        self.is_train = is_train\n\n        if self.is_train:\n            self.mode = \"Train\"\n        else:\n            self.mode = \"Test\"\n\n\n        image_paths = pd.read_csv(os.path.join(dataset_path, 'images.txt'),\n                                  sep=' ', names=['img_id', 'filepath'])\n        image_class_labels = pd.read_csv(os.path.join(dataset_path, 'image_class_labels.txt'),\n                                         sep=' ', names=['img_id', 'target'])\n        # Since the raw labels are non-continuous, map them to new ones\n        self.label_map = get_continuous_class_map(image_class_labels['target'])\n        train_test_split = pd.read_csv(os.path.join(dataset_path, 'train_test_split.txt'),\n                                       sep=' ', names=['img_id', 'is_training_img'])\n        data = image_paths.merge(image_class_labels, on='img_id')\n        self.data = data.merge(train_test_split, on='img_id')\n        # Load in the train / test split\n        if self.is_train:\n            self.data = self.data[self.data.is_training_img == 1]\n        else:\n            self.data = self.data[self.data.is_training_img == 0]\n\n        # Load in the class data\n        self.class_names = load_class_names(dataset_path)\n        self.class_hierarchy = load_hierarchy(dataset_path)\n\n\n        self.data_len = None\n\n        if self.sm_vit:\n            print(f\"[INFO] Preparing {self.mode} shape_hw list...\")\n            \n            shape_hw_list = []\n            self.file_list = []\n            file_list_full = []\n\n            for sample in self.data.iloc:\n                image_name = sample.filepath\n                img_name_full = join(self.root, self.base_folder, image_name)\n                img_temp = scipy.misc.imread(os.path.join(img_name_full))\n                shape_hw_temp = [img_temp.shape[0], img_temp.shape[1]] # h_max (y), w_max (x)\n\n                if (shape_hw_temp[0] > self.max_res) or (shape_hw_temp[1] > self.max_res):\n                    if shape_hw_temp[0] > shape_hw_temp[1]:\n                        downscale_coef = shape_hw_temp[0] / self.max_res\n                    else:\n                        downscale_coef = shape_hw_temp[1] / self.max_res\n                        \n                    shape_hw_temp[0] = int(shape_hw_temp[0] // downscale_coef)\n                    shape_hw_temp[1] = int(shape_hw_temp[1] // downscale_coef)\n\n                shape_hw_list.append(shape_hw_temp)\n                self.file_list.append(image_name)\n                file_list_full.append(img_name_full)\n\n            if self.low_memory:\n                self.mask_u2n_list, self.x_u2n_list, self.y_u2n_list, self.h_u2n_list, self.w_u2n_list = \\\n                        super(NABirds, self).generic_preprocess_lowMem(self.file_list,\n                                                                    file_list_full, \n                                                                    shape_hw_list\n                                                                    )\n                del shape_hw_list\n                del file_list_full\n                gc.collect()                                                          \n            else:\n                self.img, self.mask = \\\n                    super(NABirds, self).generic_preprocess(self.file_list, \n                                                        file_list_full, \n                                                        shape_hw_list,\n                                                        self.data_len\n                                                        )\n\n        if self.is_train:\n            self.label = [ (self.label_map[x.target]) for x in self.data.iloc ][:self.data_len]\n        else:\n            self.label = [ (self.label_map[x.target]) for x in self.data.iloc ][:self.data_len]\n\n        self.imgname = [x for x in self.file_list[:self.data_len]]\n\n\n\n    def __getitem__(self, index):\n\n        if self.sm_vit:\n            if self.low_memory:\n                target, imgname = self.label[index], self.imgname[index]\n                img, mask = super(NABirds, self).generic_getitem_lowMem(index)\n            else:\n                img, target, imgname, mask = self.img[index], self.label[index], self.imgname[index], self.mask[index]\n                img, mask = super(NABirds, self).generic_getitem(index, img, mask)\n            \n            return img, target, mask\n\n        else:\n            sample = self.data.iloc[index]\n            path = os.path.join(self.root, self.base_folder, sample.filepath)\n            target = self.label_map[sample.target]\n\n            img = self.loader(path)\n            if self.transform is not None:\n                img = self.transform(img)\n\n            return img, target\n\n\n    def __len__(self):\n        return len(self.data)\n\n\ndef get_continuous_class_map(class_labels):\n    label_set = set(class_labels)\n    return {k: i for i, k in enumerate(label_set)}\n\n\ndef load_class_names(dataset_path=''):\n    names = {}\n\n    with open(os.path.join(dataset_path, 'classes.txt')) as f:\n        for line in f:\n            pieces = line.strip().split()\n            class_id = pieces[0]\n            names[class_id] = ' '.join(pieces[1:])\n    return names\n\n\ndef load_hierarchy(dataset_path=''):\n    parents = {}\n\n    with open(os.path.join(dataset_path, 'hierarchy.txt')) as f:\n        for line in f:\n            pieces = line.strip().split()\n            child_id, parent_id = pieces\n            parents[child_id] = parent_id\n\n    return parents\n\n\n\n\n\n### Not optimised datasets:\n\nclass INat2017(VisionDataset):\n    \"\"\"`iNaturalist 2017 <https://github.com/visipedia/inat_comp/blob/master/2017/README.md>`_ Dataset.\n        Args:\n            root (string): Root directory of the dataset.\n            split (string, optional): The dataset split, supports ``train``, or ``val``.\n            transform (callable, optional): A function/transform that  takes in an PIL image\n               and returns a transformed version. E.g, ``transforms.RandomCrop``\n            target_transform (callable, optional): A function/transform that takes in the\n               target and transforms it.\n            download (bool, optional): If true, downloads the dataset from the internet and\n               puts it in root directory. If dataset is already downloaded, it is not\n               downloaded again.\n    \"\"\"\n    base_folder = 'train_val_images/'\n    file_list = {\n        'imgs': ('https://storage.googleapis.com/asia_inat_data/train_val/train_val_images.tar.gz',\n                 'train_val_images.tar.gz',\n                 '7c784ea5e424efaec655bd392f87301f'),\n        'annos': ('https://storage.googleapis.com/asia_inat_data/train_val/train_val2017.zip',\n                  'train_val2017.zip',\n                  '444c835f6459867ad69fcb36478786e7')\n    }\n\n    def __init__(self, root, split='train', transform=None, target_transform=None, download=False):\n        super(INat2017, self).__init__(root, transform=transform, target_transform=target_transform)\n        self.loader = default_loader\n        self.split = verify_str_arg(split, \"split\", (\"train\", \"val\",))\n\n        if self._check_exists():\n            print('Files already downloaded and verified.')\n        elif download:\n            if not (os.path.exists(os.path.join(self.root, self.file_list['imgs'][1]))\n                    and os.path.exists(os.path.join(self.root, self.file_list['annos'][1]))):\n                print('Downloading...')\n                self._download()\n            print('Extracting...')\n            extract_archive(os.path.join(self.root, self.file_list['imgs'][1]))\n            extract_archive(os.path.join(self.root, self.file_list['annos'][1]))\n        else:\n            raise RuntimeError(\n                'Dataset not found. You can use download=True to download it.')\n        anno_filename = split + '2017.json'\n        with open(os.path.join(self.root, anno_filename), 'r') as fp:\n            all_annos = json.load(fp)\n\n        self.annos = all_annos['annotations']\n        self.images = all_annos['images']\n\n    def __getitem__(self, index):\n        path = os.path.join(self.root, self.images[index]['file_name'])\n        target = self.annos[index]['category_id']\n\n        image = self.loader(path)\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.images)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.base_folder))\n\n    def _download(self):\n        for url, filename, md5 in self.file_list.values():\n            download_url(url, root=self.root, filename=filename)\n            if not check_integrity(os.path.join(self.root, filename), md5):\n                raise RuntimeError(\"File not found or corrupted.\")\n\n\n\nclass CarsDataset(Dataset):\n\n    def __init__(self, mat_anno, data_dir, car_names, cleaned=None, transform=None):\n        \"\"\"\n        Args:\n            mat_anno (string): Path to the MATLAB annotation file.\n            data_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.full_data_set = io.loadmat(mat_anno)\n        self.car_annotations = self.full_data_set['annotations']\n        self.car_annotations = self.car_annotations[0]\n\n        if cleaned is not None:\n            cleaned_annos = []\n            print(\"Cleaning up data set (only take pics with rgb chans)...\")\n            clean_files = np.loadtxt(cleaned, dtype=str)\n            for c in self.car_annotations:\n                if c[-1][0] in clean_files:\n                    cleaned_annos.append(c)\n            self.car_annotations = cleaned_annos\n\n        self.car_names = scipy.io.loadmat(car_names)['class_names']\n        self.car_names = np.array(self.car_names[0])\n\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.car_annotations)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.data_dir, self.car_annotations[idx][-1][0])\n        image = Image.open(img_name).convert('RGB')\n        car_class = self.car_annotations[idx][-2][0][0]\n        car_class = torch.from_numpy(np.array(car_class.astype(np.float32))).long() - 1\n        assert car_class < 196\n        \n        if self.transform:\n            image = self.transform(image)\n\n        # return image, car_class, img_name\n        return image, car_class\n\n    def map_class(self, id):\n        id = np.ravel(id)\n        ret = self.car_names[id - 1][0][0]\n        return ret\n\n    def show_batch(self, img_batch, class_batch):\n\n        for i in range(img_batch.shape[0]):\n            ax = plt.subplot(1, img_batch.shape[0], i + 1)\n            title_str = self.map_class(int(class_batch[i]))\n            img = np.transpose(img_batch[i, ...], (1, 2, 0))\n            ax.imshow(img)\n            ax.set_title(title_str.__str__(), {'fontsize': 5})\n            plt.tight_layout()\n\ndef make_dataset(dir, image_ids, targets):\n    assert(len(image_ids) == len(targets))\n    images = []\n    dir = os.path.expanduser(dir)\n    for i in range(len(image_ids)):\n        item = (os.path.join(dir, 'data', 'images',\n                             '%s.jpg' % image_ids[i]), targets[i])\n        images.append(item)\n    return images\n    \ndef find_classes(classes_file):\n    # read classes file, separating out image IDs and class names\n    image_ids = []\n    targets = []\n    f = open(classes_file, 'r')\n    for line in f:\n        split_line = line.split(' ')\n        image_ids.append(split_line[0])\n        targets.append(' '.join(split_line[1:]))\n    f.close()\n\n    # index class names\n    classes = np.unique(targets)\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    targets = [class_to_idx[c] for c in targets]\n\n    return (image_ids, targets, classes, class_to_idx)\n\n\n\nclass FGVC_aircraft():\n    def __init__(self, root, is_train=True, data_len=None, transform=None):\n        self.root = root\n        self.is_train = is_train\n        self.transform = transform\n        train_img_path = os.path.join(self.root, 'data', 'images')\n        test_img_path = os.path.join(self.root, 'data', 'images')\n        train_label_file = open(os.path.join(self.root, 'data', 'train.txt'))\n        test_label_file = open(os.path.join(self.root, 'data', 'test.txt'))\n        train_img_label = []\n        test_img_label = []\n        for line in train_label_file:\n            train_img_label.append([os.path.join(train_img_path,line[:-1].split(' ')[0]), int(line[:-1].split(' ')[1])-1])\n        for line in test_label_file:\n            test_img_label.append([os.path.join(test_img_path,line[:-1].split(' ')[0]), int(line[:-1].split(' ')[1])-1])\n        self.train_img_label = train_img_label[:data_len]\n        self.test_img_label = test_img_label[:data_len]\n\n    def __getitem__(self, index):\n        if self.is_train:\n            img, target = scipy.misc.imread(self.train_img_label[index][0]), self.train_img_label[index][1]\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            img = Image.fromarray(img, mode='RGB')\n            if self.transform is not None:\n                img = self.transform(img)\n\n        else:\n            img, target = scipy.misc.imread(self.test_img_label[index][0]), self.test_img_label[index][1]\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            img = Image.fromarray(img, mode='RGB')\n            if self.transform is not None:\n                img = self.transform(img)\n\n        return img, target\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train_img_label)\n        else:\n            return len(self.test_img_label)\n"}
{"type": "source_file", "path": "models/configs.py", "content": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport ml_collections\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    del config.patches.size\n    config.patches.grid = (28, 28)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n    return config\n\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config\n"}
{"type": "source_file", "path": "utils/scheduler.py", "content": "import logging\nimport math\n\nfrom torch.optim.lr_scheduler import LambdaLR\n\nlogger = logging.getLogger(__name__)\n\nclass ConstantLRSchedule(LambdaLR):\n    \"\"\" Constant learning rate schedule.\n    \"\"\"\n    def __init__(self, optimizer, last_epoch=-1):\n        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n\n\nclass WarmupConstantSchedule(LambdaLR):\n    \"\"\" Linear warmup and then constant.\n        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n        Keeps learning rate schedule equal to 1. after warmup_steps.\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1.0, self.warmup_steps))\n        return 1.\n\n\nclass WarmupLinearSchedule(LambdaLR):\n    \"\"\" Linear warmup and then linear decay.\n        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1, self.warmup_steps))\n        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n\n\nclass WarmupCosineSchedule(LambdaLR):\n    \"\"\" Linear warmup and then cosine decay.\n        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        self.cycles = cycles\n        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1.0, self.warmup_steps))\n        # progress after warmup\n        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n"}
