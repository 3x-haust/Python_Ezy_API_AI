{"repo_info": {"repo_name": "dtd", "repo_owner": "madhubabuv", "repo_url": "https://github.com/madhubabuv/dtd"}}
{"type": "source_file", "path": "datasets/base/__init__.py", "content": ""}
{"type": "source_file", "path": "datasets/ms2/save_sgm_to_npy.py", "content": "import numpy as np\nimport os\nimport cv2\nimport tqdm\nif __name__ == \"__main__\":\n\n    data = '/hdd1/madhu/data/ms2/test_nighttime_list.txt'\n    data = np.loadtxt(data, dtype=str, delimiter=',')\n    left_file_names = data[:,0]\n    right_file_names = data[:,1]\n\n    sgm_filenames = []\n\n    sgm_path = '/mnt/nas/madhu/data/predictions/baslines/MS2_sgm/'\n\n    for name in left_file_names:\n\n        split_name = name.split('/')\n        image_idx = split_name[-1].split('.')[0]\n        seq_id = split_name[-4]\n        file_path = os.path.join(sgm_path, seq_id+'_'+image_idx+'.png')\n        assert os.path.exists(file_path), file_path\n        sgm_filenames.append(file_path)\n\n    all_disps = []\n    for file_name in tqdm.tqdm(sgm_filenames, total = len(sgm_filenames)):\n\n        img = cv2.imread(file_name, cv2.IMREAD_UNCHANGED)\n        #center crop\n        height, width = img.shape[0], img.shape[1]\n        new_height = 384\n        new_width = 640\n\n        left = (width - new_width)//2\n        top = (height - new_height)//2\n        right = (width + new_width)//2\n        bottom = (height + new_height)//2\n\n        img = img[top:bottom, left:right]\n\n        all_disps.append(img)\n\n    all_disps = np.stack(all_disps, axis=0)\n    np.save('/mnt/nas/madhu/data/predictions/baslines/MS2_sgm.npy', all_disps)\n\n\n\n"}
{"type": "source_file", "path": "datasets/robotcar/sdk/__init__.py", "content": ""}
{"type": "source_file", "path": "datasets/base/image.py", "content": "import numpy as np\nimport cv2 \n\n\nclass Image:\n\n    def __init__(self):\n\n        pass\n    \n    @staticmethod\n    def read( path):\n        return cv2.imread(path)\n\n    @staticmethod\n    def write( path, image):\n        cv2.imwrite(path, image)\n\n    @staticmethod\n    def resize( image, width, height):\n        return cv2.resize(image, (width, height))\n\n    @staticmethod\n    def undistort(img, K, dist_coef):\n        \n        h, w = img.shape[:2]\n        newcameramtx, roi = cv2.getOptimalNewCameraMatrix(K, dist_coef, (w, h), 0, (w, h))\n        undistorted_img = cv2.undistort(img, K, dist_coef, None, newcameramtx)\n        x, y, w, h = roi\n        undistorted_img = undistorted_img[y:y + h, x:x + w]\n\n        return undistorted_img,newcameramtx\n\n    @staticmethod\n    def to_grayscale( image):\n        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    @staticmethod\n    def to_rgb( image):\n        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    @staticmethod\n    def normalize( image):\n        return image / 255.0\n\n    @staticmethod\n    def denormalize( image):\n\n        image = image * 255.0\n        image = np.clip(image, 0, 255)\n        image = image.astype(np.uint8)\n\n        return image\n\n\n    @staticmethod\n    def to_float( image):\n        return image.astype(np.float32)\n\n\n    "}
{"type": "source_file", "path": "datasets/base/camera.py", "content": "import numpy as np\nimport os\nfrom abc import ABCMeta, abstractmethod\nfrom .image import Image\nimport copy\n\nclass Capture(metaclass=ABCMeta):\n\n    @abstractmethod\n    def capture_image(self, camera, timestamp):\n        pass\n    \n    def convert_raw_images(self, raw_images):\n        pass\n\n\n\nclass Camera(Capture):\n\n    def __init__(self, name, image_resolution,\n                    focal_x, focal_y, principal_x, principal_y,distortion_coefficients = None):\n\n        self.name = name\n        self.image_resolution = image_resolution\n        self.camera_matrix = self.to_camera_matrix(focal_x, focal_y, principal_x, principal_y)\n        self.full_res_camera_matrix = copy.deepcopy(self.camera_matrix)#.copy()\n        self.distortion_coefficients = distortion_coefficients\n        self.working_resolution = None\n\n\n    def to_camera_matrix(self, focal_x, focal_y, principal_x, principal_y):\n        return np.array([[focal_x, 0, principal_x],\n                         [0, focal_y, principal_y],\n                         [0, 0, 1]])\n\n\n    def set_working_resolution(self, resolution):\n\n        orig_width, orig_height = self.image_resolution\n        new_width, new_height = resolution\n\n        if orig_width != new_width or orig_height != new_height:\n            self.working_resolution = resolution\n            self.camera_matrix[0, 0] = self.camera_matrix[0, 0] * new_width / orig_width\n            self.camera_matrix[1, 1] = self.camera_matrix[1, 1] * new_height / orig_height\n            self.camera_matrix[0, 2] = self.camera_matrix[0, 2] * new_width / orig_width\n            self.camera_matrix[1, 2] = self.camera_matrix[1, 2] * new_height / orig_height\n\n    def set_data_path(self, data_path):\n\n        self.data_path = data_path\n\n    def capture_image(self, timestamp,ext = '.png', undistort = False, full_resolution = False):\n\n        image_path = os.path.join(self.data_path, self.name, 'data', str(timestamp) + ext)\n        assert os.path.exists(image_path), \"Image does not exist: {}\".format(image_path)\n        image = Image.read(image_path)\n        full_res_image = image.copy()\n        if undistort:\n            image =  Image.undistort(image, self.camera_matrix, self.distortion_coefficients)\n\n        if not self.working_resolution is None:\n            image = Image.resize(image, self.working_resolution[0], self.working_resolution[1])\n            \n        # cv2 reads images in BGR format, so we convert it to RGB\n        image = Image.to_rgb(image)\n        image = Image.normalize(image) \n        \n        if full_resolution:\n            full_res_image = Image.to_rgb(full_res_image)\n            full_res_image = Image.normalize(full_res_image)\n            return (image, full_res_image)\n        return image\n\n\n\n\n        \n\n\nclass StereoCamera:\n\n    def __init__(self, camera_rig_info):\n\n        camera_0 = camera_rig_info['camera_0']\n        camera_1 = camera_rig_info['camera_1']\n\n        self.camera_0 = Camera(camera_0['name'], camera_0['resolution'],\n                                camera_0['focal_x'], camera_0['focal_y'],\n                                camera_0['principal_x'], camera_0['principal_y'],\n                                camera_0['distortion_coefficients'])\n\n        self.camera_1 = Camera(camera_1['name'], camera_1['resolution'],\n                                camera_1['focal_x'], camera_1['focal_y'],\n                                camera_1['principal_x'], camera_1['principal_y'],\n                                camera_1['distortion_coefficients'])\n\n        self.baseline_distance = camera_rig_info['baseline_distance']\n            \n    def set_data_path(self, data_path):\n\n        self.data_path = data_path\n        self.camera_0.set_data_path(data_path)\n        self.camera_1.set_data_path(data_path)\n\n    def capture_image(self, timestamp,ext = '.png', undistort = False, full_resolution = False):\n        \n        image_0 = self.camera_0.capture_image(timestamp,ext, undistort, full_resolution = full_resolution)\n        image_1 = self.camera_1.capture_image(timestamp,ext, undistort, full_resolution = full_resolution)\n\n        return image_0, image_1\n\n    def set_working_resolution(self, resolution):\n\n        self.camera_0.set_working_resolution(resolution)\n        self.camera_1.set_working_resolution(resolution)\n\n\n\nclass MonoCamera:\n\n    def __init__(self, camera_info):\n\n        self.camera = Camera(camera_info['name'], camera_info['resolution'],\n                                camera_info['focal_x'], camera_info['focal_y'],\n                                camera_info['principal_x'], camera_info['principal_y'],\n                                camera_info['distortion_coefficients'])\n\n    def set_data_path(self, data_path):\n\n        self.camera.set_data_path(data_path)\n\n    def set_working_resolution(self, resolution):\n        \n        self.camera.set_working_resolution(resolution)\n\n    def capture_image(self, timestamp,ext = '.png', undistort = False):\n\n        return self.camera.capture_image(timestamp,ext, undistort)\n\n    def undistort_image(self, image):\n\n        return self.camera.undistort_image(image)"}
{"type": "source_file", "path": "datasets/ms2/__init__.py", "content": ""}
{"type": "source_file", "path": "datasets/base/lidar.py", "content": "import numpy as np\nimport os\n\nfrom abc import ABCMeta, abstractmethod\n\nclass Capture(metaclass=ABCMeta):\n\n    @abstractmethod\n    def capture_scan(self, camera, timestamp):\n        pass\n    \n    def to_camera(self, scan, Rt):\n        pass\n\n\nclass Lidar(Capture):\n\n    def __init__(self, name,):\n\n        self.name = name\n\n\n\n    def set_data_path(self, data_path):\n\n        self.data_path = data_path\n\n    def capture_scan(self, timestamp, ext = '.bin'):\n\n\n        scan_path = os.path.join(self.data_path, self.name, 'data', str(timestamp) + ext)\n        points = np.fromfile(scan_path, dtype=np.float32).reshape(-1, 4)\n        points[:, 3] = 1.0  # homogeneous\n\n        return points\n\n\n    def to_camera(self, scan, Rt):\n\n        assert scan.shape[1] == 4, \"Scan is not in homogeneous form!\"\n\n        transformed_pts = np.dot(Rt, scan.T).T\n\n        return transformed_pts[:, :3]\n\n\n    \n\n    \n\n\n    \n\n\n\n\n\n"}
{"type": "source_file", "path": "datasets/ms2/read_depth.py", "content": "import numpy as np\nimport cv2\nimport argparse\nimport os\nimport tqdm\nimport torch\nfrom datasets.ms2.depth_test_dataloader import MS2Dataset as RobotcarTest\nfrom matplotlib import pyplot as plt\n\ndef center_crop(image):\n\n    height, width = image.shape[0], image.shape[1]\n    new_height = 384\n    new_width = 640\n    #center crop\n    left = (width - new_width)//2\n    top = (height - new_height)//2\n    right = (width + new_width)//2\n    bottom = (height + new_height)//2\n\n    image = image[top:bottom, left:right]\n\n    return image\n\ndef test():\n\n\n    all_depths = []\n\n\n    for idx, data in tqdm.tqdm(enumerate(dataloader), total = len(dataloader)):\n\n        key = 'frame0'\n        #image  = data[key]['image']\n        path_split = data[key]['timestamp'][0].split(' ')\n        folder = path_split[0]\n        img_id = path_split[-1]\n        depth_path = os.path.join(args.data_path, 'proj_depth', folder, 'rgb/depth_filtered', img_id+'.png')\n        assert os.path.exists(depth_path), depth_path\n        depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)\n        depth = depth.astype(np.float32)/256.0\n        depth = center_crop(depth)\n\n        all_depths.append(depth)\n\n    \n\n\n        # fig, ax = plt.subplots(1,2, figsize=(10,5))\n        # ax[0].imshow(image[0].permute(1,2,0).numpy())\n        # ax[1].imshow(depth, cmap='plasma')\n        # ax[0].axis('off')\n        # ax[1].axis('off')\n        # plt.tight_layout()\n        # plt.savefig('depth.png')\n\n        # breakpoint()\n\n    breakpoint()\n    save_path = os.path.join(args.data_path, 'gt_test_depths_filtered.npy')\n    np.save(save_path, all_depths)\n    \n\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Feature-based SLAM')\n    args = parser.parse_args()\n    args.image_height = 384\n    args.image_width = 640\n    args.batch_size =1\n    args.working_resolution = (args.image_width, args.image_height)\n    args.use_stereo = True\n    args.test_file_path = '/hdd1/madhu/data/ms2/test_nighttime_list.txt'\n    args.data_path = \"/hdd1/madhu/data/ms2\"\n    dataset = RobotcarTest(args)\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=args.batch_size, shuffle=False\n    )\n\n    depth_path = os.path.join(args.data_path, 'proj_depth')\n\n\n    test()\n"}
{"type": "source_file", "path": "datasets/robotcar/dataloader.py", "content": "import os\nimport numpy as np\nimport torch\nimport copy\nfrom PIL import Image\nfrom datasets.base.camera import StereoCamera, MonoCamera\nfrom torchvision.transforms.functional import rgb_to_grayscale\n\noriginal_resolution = [1280, 768]\n\ndef get_camera_matrix(scale_x=1.0, scale_y=1.0):\n\n    # scale is set to 2 because oxfor images are captured\n    # in raw format, so interms of space and speed, I never converted them to the\n    # original resolution, also, removed the car hood as it does not carry any useful info\n    # that is why the principal point is not at the center of the image\n\n    fx, fy, cx, cy = 983.044006, 983.044006, 643.646973, 493.378998\n    cy *= 4 / 5\n    fx = fx * scale_x\n    fy = fy * scale_y\n    cx = cx * scale_x\n    cy = cy * scale_y\n\n    camera_info = {}\n    camera_info[\"focal_x\"] = fx\n    camera_info[\"focal_y\"] = fy\n    camera_info[\"principal_x\"] = cx\n    camera_info[\"principal_y\"] = cy\n    camera_info[\"resolution\"] = original_resolution  # [1280, 768]\n    camera_info[\"distortion_coefficients\"] = None\n\n    camera_info[\n        \"name\"\n    ] = \"left_rgb\"  # this is the last folder name where the images are saved\n\n    return camera_info\n\n\nclass RobotCarDataset(torch.utils.data.Dataset):\n    def __init__(self, args):\n\n        self.args = args\n        camera0_info = get_camera_matrix(scale_x=1.0, scale_y=1.0)\n        self.baseline_distance = 0.239\n        # load timestamps\n        self.timestamps = self.load_timestamps()\n\n        # camera realted initailization\n        camera1_info = copy.deepcopy(camera0_info)\n        camera1_info[\"name\"] = \"right_rgb\"\n        camera_info = {\n            \"camera_0\": camera0_info,\n            \"camera_1\": camera1_info,\n            \"baseline_distance\": self.baseline_distance,\n        }\n        self.camera_rig = StereoCamera(camera_info)\n       \n\n        self.camera_rig.set_data_path(args.data_path)\n        self.camera_rig.set_working_resolution(args.working_resolution)\n\n\n    def load_timestamps(self):\n\n        timestamps_path = (\"datasets/robotcar/files/2014-12-16-18-44-24_train.txt\")\n        stamps = np.loadtxt(timestamps_path, dtype=int)\n        stamps = stamps[:,1] # middle stamps        \n        return stamps\n\n    def __len__(self):\n        return len(self.timestamps)\n\n\n    def load_data_at(self, timestamp):\n\n        output_dict = {}\n\n        full_resolution = self.args.use_full_res\n\n        #if self.args.use_stereo:\n        image, stereo_pair = self.camera_rig.capture_image(\n            timestamp,\n            undistort=False,\n            full_resolution=full_resolution,\n        )\n\n        if full_resolution:\n\n            image, full_res_image = image\n            stereo_pair, full_res_stereo_pair = stereo_pair\n\n            output_dict[\"full_res_image\"] = (\n                torch.from_numpy(full_res_image).permute(2, 0, 1).float()\n            )\n            output_dict[\"full_res_stereo_pair\"] = (\n                torch.from_numpy(full_res_stereo_pair).permute(2, 0, 1).float()\n            )\n            output_dict[\"full_res_camera_matrix\"] = torch.from_numpy(\n                self.camera_rig.camera_0.full_res_camera_matrix\n            ).float()\n\n        image = torch.from_numpy(image).permute(2, 0, 1).float()\n\n        stereo_pair = torch.from_numpy(stereo_pair).permute(2, 0, 1).float()\n        camera_matrix0 = self.camera_rig.camera_0.camera_matrix\n        camera_matrix1 = self.camera_rig.camera_1.camera_matrix\n        camera_matrix0 = torch.from_numpy(camera_matrix0).float()\n        camera_matrix1 = torch.from_numpy(camera_matrix1).float()\n\n        output_dict[\"image\"] = image\n        output_dict[\"stereo_pair\"] = stereo_pair\n        output_dict[\"camera_matrix\"] = camera_matrix0\n        output_dict[\"camera_matrix1\"] = camera_matrix1\n        return output_dict\n\n    def __getitem__(self, index):\n\n        stamp = self.timestamps[index]\n        output_dict = {}\n        frame_data = self.load_data_at(\"{:010d}\".format(stamp))\n        output_dict[\"frame0\"] = frame_data\n        output_dict[\"dataset_index\"] = index\n        output_dict['timestamps'] = stamp\n        return output_dict\n\n\nif __name__ == \"__main__\":\n\n    import argparse\n    import copy\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n\n    args.working_resolution = (640, 384)\n    args.use_stereo = True\n    args.undistort = False\n    args.use_seq = False\n    args.seq_length = 3\n    args.stride = 1\n    args.use_multi_scale_images = False\n    args.num_scales = 4\n    args.use_full_res = False\n    args.use_gray_scale = False\n    args.use_gt_poses = False\n    args.data_path = \"/hdd1/madhu/data/robotcar/2014-12-16-18-44-24/stereo/\"\n    dataset = RobotCarDataset(args)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n\n    all_xyz = []\n\n    import tqdm\n    for idx, data in tqdm.tqdm(enumerate(dataloader),total = len(dataloader)):\n\n        pass\n\n        \n"}
{"type": "source_file", "path": "datasets/robotcar/sdk/transform.py", "content": "################################################################################\n#\n# Copyright (c) 2017 University of Oxford\n# Authors:\n#  Geoff Pascoe (gmp@robots.ox.ac.uk)\n#\n# This work is licensed under the Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n#\n################################################################################\n\nimport numpy as np\nimport numpy.matlib as matlib\nfrom math import sin, cos, atan2, sqrt\n\nMATRIX_MATCH_TOLERANCE = 1e-4\n\n\ndef build_se3_transform(xyzrpy):\n    \"\"\"Creates an SE3 transform from translation and Euler angles.\n\n    Args:\n        xyzrpy (list[float]): translation and Euler angles for transform. Must have six components.\n\n    Returns:\n        numpy.matrixlib.defmatrix.matrix: SE3 homogeneous transformation matrix\n\n    Raises:\n        ValueError: if `len(xyzrpy) != 6`\n\n    \"\"\"\n    if len(xyzrpy) != 6:\n        raise ValueError(\"Must supply 6 values to build transform\")\n\n    se3 = matlib.identity(4)\n    se3[0:3, 0:3] = euler_to_so3(xyzrpy[3:6])\n    se3[0:3, 3] = np.matrix(xyzrpy[0:3]).transpose()\n    return se3\n\n\ndef euler_to_so3(rpy):\n    \"\"\"Converts Euler angles to an SO3 rotation matrix.\n\n    Args:\n        rpy (list[float]): Euler angles (in radians). Must have three components.\n\n    Returns:\n        numpy.matrixlib.defmatrix.matrix: 3x3 SO3 rotation matrix\n\n    Raises:\n        ValueError: if `len(rpy) != 3`.\n\n    \"\"\"\n    if len(rpy) != 3:\n        raise ValueError(\"Euler angles must have three components\")\n\n    R_x = np.matrix([[1, 0, 0],\n                     [0, cos(rpy[0]), -sin(rpy[0])],\n                     [0, sin(rpy[0]), cos(rpy[0])]])\n    R_y = np.matrix([[cos(rpy[1]), 0, sin(rpy[1])],\n                     [0, 1, 0],\n                     [-sin(rpy[1]), 0, cos(rpy[1])]])\n    R_z = np.matrix([[cos(rpy[2]), -sin(rpy[2]), 0],\n                     [sin(rpy[2]), cos(rpy[2]), 0],\n                     [0, 0, 1]])\n    R_zyx = R_z * R_y * R_x\n    return R_zyx\n\n\ndef so3_to_euler(so3):\n    \"\"\"Converts an SO3 rotation matrix to Euler angles\n\n    Args:\n        so3: 3x3 rotation matrix\n\n    Returns:\n        numpy.matrixlib.defmatrix.matrix: list of Euler angles (size 3)\n\n    Raises:\n        ValueError: if so3 is not 3x3\n        ValueError: if a valid Euler parametrisation cannot be found\n\n    \"\"\"\n    if so3.shape != (3, 3):\n        raise ValueError(\"SO3 matrix must be 3x3\")\n    roll = atan2(so3[2, 1], so3[2, 2])\n    yaw = atan2(so3[1, 0], so3[0, 0])\n    denom = sqrt(so3[0, 0] ** 2 + so3[1, 0] ** 2)\n    pitch_poss = [atan2(-so3[2, 0], denom), atan2(-so3[2, 0], -denom)]\n\n    R = euler_to_so3((roll, pitch_poss[0], yaw))\n\n    if (so3 - R).sum() < MATRIX_MATCH_TOLERANCE:\n        return np.matrix([roll, pitch_poss[0], yaw])\n    else:\n        R = euler_to_so3((roll, pitch_poss[1], yaw))\n        if (so3 - R).sum() > MATRIX_MATCH_TOLERANCE:\n            raise ValueError(\"Could not find valid pitch angle\")\n        return np.matrix([roll, pitch_poss[1], yaw])\n\n\ndef so3_to_quaternion(so3):\n    \"\"\"Converts an SO3 rotation matrix to a quaternion\n\n    Args:\n        so3: 3x3 rotation matrix\n\n    Returns:\n        numpy.ndarray: quaternion [w, x, y, z]\n\n    Raises:\n        ValueError: if so3 is not 3x3\n    \"\"\"\n    if so3.shape != (3, 3):\n        raise ValueError(\"SO3 matrix must be 3x3\")\n\n    R_xx = so3[0, 0]\n    R_xy = so3[0, 1]\n    R_xz = so3[0, 2]\n    R_yx = so3[1, 0]\n    R_yy = so3[1, 1]\n    R_yz = so3[1, 2]\n    R_zx = so3[2, 0]\n    R_zy = so3[2, 1]\n    R_zz = so3[2, 2]\n\n    try:\n        w = sqrt(so3.trace() + 1) / 2\n    except(ValueError):\n        # w is non-real\n        w = 0\n\n    x = sqrt(1 + R_xx - R_yy - R_zz) / 2\n    y = sqrt(1 + R_yy - R_xx - R_zz) / 2\n    z = sqrt(1 + R_zz - R_yy - R_xx) / 2\n\n    max_index = max(range(4), key=[w, x, y, z].__getitem__)\n\n    if max_index == 0:\n        x = (R_zy - R_yz) / (4 * w)\n        y = (R_xz - R_zx) / (4 * w)\n        z = (R_yx - R_xy) / (4 * w)\n    elif max_index == 1:\n        w = (R_zy - R_yz) / (4 * x)\n        y = (R_xy + R_yx) / (4 * x)\n        z = (R_zx + R_xz) / (4 * x)\n    elif max_index == 2:\n        w = (R_xz - R_zx) / (4 * y)\n        x = (R_xy + R_yx) / (4 * y)\n        z = (R_yz + R_zy) / (4 * y)\n    elif max_index == 3:\n        w = (R_yx - R_xy) / (4 * z)\n        x = (R_zx + R_xz) / (4 * z)\n        y = (R_yz + R_zy) / (4 * z)\n\n    return np.array([w, x, y, z])\n\n\ndef se3_to_components(se3):\n    \"\"\"Converts an SE3 rotation matrix to linear translation and Euler angles\n\n    Args:\n        se3: 4x4 transformation matrix\n\n    Returns:\n        numpy.matrixlib.defmatrix.matrix: list of [x, y, z, roll, pitch, yaw]\n\n    Raises:\n        ValueError: if se3 is not 4x4\n        ValueError: if a valid Euler parametrisation cannot be found\n\n    \"\"\"\n    if se3.shape != (4, 4):\n        raise ValueError(\"SE3 transform must be a 4x4 matrix\")\n    xyzrpy = np.empty(6)\n    xyzrpy[0:3] = se3[0:3, 3].transpose()\n    xyzrpy[3:6] = so3_to_euler(se3[0:3, 0:3])\n    return xyzrpy\n"}
{"type": "source_file", "path": "datasets/robotcar/sdk/convert_images.py", "content": "# The original code is taken from the robotcar-dataset-sdk and modified to work in parallael \nimport sys\nsys.path.append('robotcar-dataset-sdk/python')\nimport re\nimport tqdm\nimport os\nimport cv2\nimport numpy as np\nimport argparse\nfrom camera_model import CameraModel\nfrom PIL import Image\nfrom colour_demosaicing import demosaicing_CFA_Bayer_bilinear as demosaic\nfrom multiprocessing.pool import ThreadPool\nfrom pathlib import Path\nBAYER_STEREO = 'gbrg'\nBAYER_MONO = 'rggb'\n\ndef load_image(image_path, model=None):\n    \"\"\"Loads and rectifies an image from file.\n\n    Args:\n        image_path (str): path to an image from the dataset.\n        model (camera_model.CameraModel): if supplied, model will be used to undistort image.\n\n    Returns:\n        numpy.ndarray: demosaiced and optionally undistorted image\n\n    \"\"\"\n    if model:\n        camera = model.camera\n    else:\n        camera = re.search('(stereo|mono_(left|right|rear))', image_path).group(0)\n    if camera == 'stereo':\n        pattern = BAYER_STEREO\n    else:\n        pattern = BAYER_MONO\n\n    img = Image.open(image_path)\n    img = demosaic(img, pattern)\n    if model:\n        img = model.undistort(img)\n\n    return img\n\n\ndef read_and_save(timestamp):\n    save_image_path = os.path.join(save_image_dir, str(timestamp) + '.png')\n    if os.path.exists(save_image_path):\n         pass\n    else:\n        filename = os.path.join(args.data_path, str(timestamp) + '.png')\n        img = load_image(filename, camera_model)\n        if args.keep_car_hood:\n            cropped_img = img[:768, :,:] # crop the car hood 1/5th of the image height 4*960 / 5 = 768\n        else:\n            cropped_img = img\n        re_img = cv2.resize(cropped_img, (args.image_width, args.image_height))\n        re_img = re_img.astype(np.uint8)\n        re_img = cv2.cvtColor(re_img, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(save_image_path, re_img)\n\nif __name__ == \"__main__\": \n\n    parser = argparse.ArgumentParser(description='Convert images to RGB')\n    parser.add_argument('--data_path', type=str, help='Path to the timestamps file',required=True)\n    parser.add_argument('--models_dir', type=str, help='Path to the camera models directory',default=\n                            'robotcar-dataset-sdk/models/')\n    parser.add_argument('--timestamp_path', type=str, help='Path to the timestamps file', default=None)\n    parser.add_argument('--save_image_dir', type=str, help='Path to save the RGB images', default = None)   \n    parser.add_argument('--keep_car_hood',action='store_false', help='Crop the car hood from the images')\n    parser.add_argument('--image_height', type=int, help='Height of the images', default=384)\n    parser.add_argument('--image_width', type=int, help='Width of the images', default=640)\n    args = parser.parse_args()\n\n    # Arugments\n    print('-------------------')\n    print(\"Arguments:\")\n    print('-------------------')\n    for arg in vars(args):\n        print(arg,\":\",getattr(args, arg))\n    print('-------------------')\n\n    assert 'right' in args.data_path or 'left' in args.data_path, 'Please provide the path until stereo/right' \n    if args.timestamp_path is None:\n        print('Using the files in the directory to get the timestamps')\n        #files\n        all_files = os.listdir(args.data_path)\n        assert len(all_files)>0, 'No files found in the directory'\n        data = [Path(f).stem for f in all_files if f.endswith('.png')]\n        #base_dir = os.path.join(args.data_path,'../../')\n        #timestamps_path = os.path.join(base_dir, 'stereo.timestamps')\n    else:\n        data = np.loadtxt(args.timestamps_path, dtype = int)\n        if len(data.shape)>1:\n            print('There are multiple columns, by default using the first one')\n            data = data[:,0]\n\n    if args.save_image_dir is None:\n        stereo_base_dir = os.path.join(args.data_path,'../')\n        if 'right' in args.data_path:\n            save_image_dir = os.path.join(stereo_base_dir, 'right_rgb/data')\n        else:\n            save_image_dir = os.path.join(stereo_base_dir, 'left_rgb/data')\n\n    if not os.path.exists(save_image_dir):\n        print('Creating the models directory')\n        os.makedirs(save_image_dir)\n\n    camera_model = CameraModel(args.models_dir, args.data_path)\n\n    if not os.path.exists(save_image_dir):\n        os.makedirs(save_image_dir)\n\n    try:\n        with ThreadPool() as pool:\n            for _ in tqdm.tqdm(pool.imap_unordered(read_and_save, data), total=len(data)):\n                pass\n\n        print('All images in train set are converted to RGB and saved in {}'.format(save_image_dir))\n    except Exception as e:\n        print(e)\n        breakpoint()\n    \n\n"}
{"type": "source_file", "path": "evaluation/junk/eval_SGM_weighted.py", "content": "from __future__ import absolute_import, division, print_function\n\nimport os\nimport tqdm\nimport cv2\nimport numpy as np\nimport pdb\n\ncv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n\nROBOTCAR_SCALE_FACTOR = (0.239983 * 983.044006)\nMS2_SCALE_FACTOR = (0.2991842 * 764.51385)\n#ROBOTCAR_SCALE_FACTOR = 0.23998*100.\n#MS2_SCALE_FACTOR = (0.2991842 * 100)\n\n\ndef compute_errors(gt, pred):\n    \"\"\"Computation of error metrics between predicted and ground truth depths\n    \"\"\"\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n\n\ndef batch_post_process_disparity(l_disp, r_disp):\n    \"\"\"Apply the disparity post-processing method as introduced in Monodepthv1\n    \"\"\"\n    _, h, w = l_disp.shape\n    m_disp = 0.5 * (l_disp + r_disp)\n    l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n    l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n    r_mask = l_mask[:, :, ::-1]\n    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n\n\ndef evaluate(pred_disps, gt_depths):\n    \"\"\"Evaluates a pretrained model using a specified test set\n    \"\"\"\n    MIN_DEPTH = 1e-3\n    MAX_DEPTH = 50\n    histogram_bins = np.arange(0 , MAX_DEPTH+5)[::5]\n    \n    errors = []\n    all_bin_wise_metrics = []\n    for i in tqdm.tqdm(range(gt_depths.shape[0]),total = gt_depths.shape[0]):\n        gt_depth = gt_depths[i]\n        gt_height, gt_width = gt_depth.shape[:2]\n        pred_disp = pred_disps[i]\n        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n        pred_depth = ROBOTCAR_SCALE_FACTOR / (pred_disp + 1e-6)\n        #pred_depth = MS2_SCALE_FACTOR / (pred_disp + 1e-6)\n        mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n\n        # mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n        # crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n        #                     0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n        # crop_mask = np.zeros(mask.shape)\n        # crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n        # mask = np.logical_and(mask, crop_mask)\n        if mask.sum() == 0:\n            print(\"mask sum is 0\")\n            continue\n\n        pred_depth = pred_depth[mask]\n        gt_depth = gt_depth[mask]\n        bin_inds = np.digitize(gt_depth,histogram_bins)\n        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n        bin_wise_metrics = []\n        #if len(np.unique(bin_inds)) < 10: continue\n        for bin_ind in range(len(histogram_bins)):\n            bin_mask = bin_inds == bin_ind\n            bin_gt_depth = gt_depth[bin_mask]\n            bin_pred_depth = pred_depth[bin_mask]\n            if sum(bin_mask) == 0:\n                bin_errors = np.array([999,999,999,999,999,999,999])\n            else:\n                bin_errors = compute_errors(bin_gt_depth,bin_pred_depth)\n            bin_wise_metrics.append(bin_errors)\n\n        all_bin_wise_metrics.append(np.array(bin_wise_metrics))\n        errors.append(compute_errors(gt_depth, pred_depth))\n\n\n    mean_errors = np.array(errors).mean(0)\n    all_bin_wise_metrics = np.stack(all_bin_wise_metrics)\n    np.save(\"bin_wise_metrics_sgm.npy\",all_bin_wise_metrics)\n    return mean_errors\n\nif __name__ == \"__main__\":\n\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--pred_disp_path\", type=str, required=True, help=\"path to the disparities to evaluate\")\n    parser.add_argument(\"--gt_depths_path\", type=str, required=True, help=\"path to the ground truth depths\")\n    #parser.add_argument(\"--eval_split\", type=str,  choices=[\"eigen\", \"eigen_benchmark\", \"garg\"])\n    #parser.add_argument(\"--disable_median_scaling\", action=\"store_true\", help=\"if set, don't scale the disparities by the median ratio\")\n\n    args = parser.parse_args()\n\n    args.timestamps_path = '/mnt/nas/madhu/robotcar/night/2014-12-16-18-44-24_test.txt'\n    stamps = np.loadtxt(args.timestamps_path, dtype=str, delimiter=' ')\n    stamps = stamps[:,0]\n    print(\"-> Loading predictions from \",args.pred_disp_path)\n    pred_disps = []\n    for stamp in stamps:\n        pred_disps.append(cv2.imread(os.path.join(args.pred_disp_path, stamp+'.png'),cv2.IMREAD_UNCHANGED))\n\n    #pred_disps = np.load(args.pred_disp_path)\n    print(\"-> Loading ground truth from \",args.gt_depths_path)\n    gt_depths = np.load(args.gt_depths_path, allow_pickle=True)\n    results  = evaluate(pred_disps, gt_depths)\n\n    print(results)"}
{"type": "source_file", "path": "datasets/robotcar/sdk/build_pointcloud.py", "content": "################################################################################\n#\n# Copyright (c) 2017 University of Oxford\n# Authors:\n#  Geoff Pascoe (gmp@robots.ox.ac.uk)\n#\n# This work is licensed under the Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n#\n################################################################################\n\nimport os\nimport re\nimport numpy as np\n\nfrom ..junk.transform import build_se3_transform\nfrom ..junk.interpolate_poses import interpolate_vo_poses, interpolate_ins_poses\nfrom .velodyne import load_velodyne_raw, load_velodyne_binary, velodyne_raw_to_pointcloud\n\n\ndef build_pointcloud(lidar_dir, poses_file, extrinsics_dir, start_time, end_time, origin_time=-1):\n    \"\"\"Builds a pointcloud by combining multiple LIDAR scans with odometry information.\n\n    Args:\n        lidar_dir (str): Directory containing LIDAR scans.\n        poses_file (str): Path to a file containing pose information. Can be VO or INS data.\n        extrinsics_dir (str): Directory containing extrinsic calibrations.\n        start_time (int): UNIX timestamp of the start of the window over which to build the pointcloud.\n        end_time (int): UNIX timestamp of the end of the window over which to build the pointcloud.\n        origin_time (int): UNIX timestamp of origin frame. Pointcloud coordinates are relative to this frame.\n\n    Returns:\n        numpy.ndarray: 3xn array of (x, y, z) coordinates of pointcloud\n        numpy.array: array of n reflectance values or None if no reflectance values are recorded (LDMRS)\n\n    Raises:\n        ValueError: if specified window doesn't contain any laser scans.\n        IOError: if scan files are not found.\n\n    \"\"\"\n    if origin_time < 0:\n        origin_time = start_time\n\n    lidar = re.search('(lms_front|lms_rear|ldmrs|velodyne_left|velodyne_right)', lidar_dir).group(0)\n    timestamps_path = os.path.join(lidar_dir, os.pardir, lidar + '.timestamps')\n\n\n    timestamps = []\n    with open(timestamps_path) as timestamps_file:\n        for line in timestamps_file:\n            timestamp = int(line.split(' ')[0])\n            if start_time <= timestamp <= end_time:\n                timestamps.append(timestamp)\n\n    if len(timestamps) == 0:\n        raise ValueError(\"No LIDAR data in the given time bracket.\")\n\n    with open(os.path.join(extrinsics_dir, lidar + '.txt')) as extrinsics_file:\n        extrinsics = next(extrinsics_file)\n    G_posesource_laser = build_se3_transform([float(x) for x in extrinsics.split(' ')])\n\n    poses_type = re.search('(vo|ins|rtk)\\.csv', poses_file).group(1)\n\n\n    if poses_type in ['ins', 'rtk']:\n        with open(os.path.join(extrinsics_dir, 'ins.txt')) as extrinsics_file:\n            extrinsics = next(extrinsics_file)\n            G_posesource_laser = np.linalg.solve(build_se3_transform([float(x) for x in extrinsics.split(' ')]),\n                                                 G_posesource_laser)\n\n        poses = interpolate_ins_poses(poses_file, timestamps, origin_time, use_rtk=(poses_type == 'rtk'))\n    else:\n        # sensor is VO, which is located at the main vehicle frame\n        poses = interpolate_vo_poses(poses_file, timestamps, origin_time)\n\n    pointcloud = np.array([[0], [0], [0], [0]])\n    if lidar == 'ldmrs':\n        reflectance = None\n    else:\n        reflectance = np.empty((0))\n\n    for i in range(0, len(poses)):\n\n        scan_path = os.path.join(lidar_dir, str(timestamps[i]) + '.bin')\n        if \"velodyne\" not in lidar:\n            if not os.path.isfile(scan_path):\n                continue\n\n            scan_file = open(scan_path)\n            scan = np.fromfile(scan_file, np.double)\n            scan_file.close()\n\n            scan = scan.reshape((len(scan) // 3, 3)).transpose()\n\n            if lidar != 'ldmrs':\n                # LMS scans are tuples of (x, y, reflectance)\n                reflectance = np.concatenate((reflectance, np.ravel(scan[2, :])))\n                scan[2, :] = np.zeros((1, scan.shape[1]))\n        else:\n            if os.path.isfile(scan_path):\n                ptcld = load_velodyne_binary(scan_path)\n            else:\n                scan_path = os.path.join(lidar_dir, str(timestamps[i]) + '.png')\n                if not os.path.isfile(scan_path):\n                    continue\n                ranges, intensities, angles, approximate_timestamps = load_velodyne_raw(scan_path)\n                ptcld = velodyne_raw_to_pointcloud(ranges, intensities, angles)\n\n            reflectance = np.concatenate((reflectance, ptcld[3]))\n            scan = ptcld[:3]\n\n        scan = np.dot(np.dot(poses[i], G_posesource_laser), np.vstack([scan, np.ones((1, scan.shape[1]))]))\n        pointcloud = np.hstack([pointcloud, scan])\n\n    pointcloud = pointcloud[:, 1:]\n    if pointcloud.shape[1] == 0:\n        raise IOError(\"Could not find scan files for given time range in directory \" + lidar_dir)\n\n    return pointcloud, reflectance\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import open3d\n\n    parser = argparse.ArgumentParser(description='Build and display a pointcloud')\n    parser.add_argument('--poses_file', type=str, default=None, help='File containing relative or absolute poses')\n    parser.add_argument('--extrinsics_dir', type=str, default=None,\n                        help='Directory containing extrinsic calibrations')\n    parser.add_argument('--laser_dir', type=str, default=None, help='Directory containing LIDAR data')\n\n    args = parser.parse_args()\n\n    lidar = re.search('(lms_front|lms_rear|ldmrs|velodyne_left|velodyne_right)', args.laser_dir).group(0)\n    timestamps_path = os.path.join(args.laser_dir, os.pardir, lidar + '.timestamps')\n    with open(timestamps_path) as timestamps_file:\n        start_time = int(next(timestamps_file).split(' ')[0])\n\n    end_time = start_time + 2e7\n\n    pointcloud, reflectance = build_pointcloud(args.laser_dir, args.poses_file,\n                                               args.extrinsics_dir, start_time, end_time)\n\n    if reflectance is not None:\n        colours = (reflectance - reflectance.min()) / (reflectance.max() - reflectance.min())\n        colours = 1 / (1 + np.exp(-10 * (colours - colours.mean())))\n    else:\n        colours = 'gray'\n\n    # Pointcloud Visualisation using Open3D\n    vis = open3d.Visualizer()\n    vis.create_window(window_name=os.path.basename(__file__))\n    render_option = vis.get_render_option()\n    render_option.background_color = np.array([0.1529, 0.1569, 0.1333], np.float32)\n    render_option.point_color_option = open3d.PointColorOption.ZCoordinate\n    coordinate_frame = open3d.geometry.create_mesh_coordinate_frame()\n    vis.add_geometry(coordinate_frame)\n    pcd = open3d.geometry.PointCloud()\n    pcd.points = open3d.utility.Vector3dVector(\n        -np.ascontiguousarray(pointcloud[[1, 0, 2]].transpose().astype(np.float64)))\n    pcd.colors = open3d.utility.Vector3dVector(np.tile(colours[:, np.newaxis], (1, 3)).astype(np.float64))\n    # Rotate pointcloud to align displayed coordinate frame colouring\n    pcd.transform(build_se3_transform([0, 0, 0, np.pi, 0, -np.pi / 2]))\n    vis.add_geometry(pcd)\n    view_control = vis.get_view_control()\n    params = view_control.convert_to_pinhole_camera_parameters()\n    params.extrinsic = build_se3_transform([0, 3, 10, 0, -np.pi * 0.42, -np.pi / 2])\n    view_control.convert_from_pinhole_camera_parameters(params)\n    vis.run()\n    vis.capture_screen_image('test.png')\n"}
{"type": "source_file", "path": "models/depth_model.py", "content": "import torch\nfrom .masked_stereo import DTD\nfrom torchvision import transforms as T\nfrom .dino_v1 import ViTExtractor as DinoV1ExtractFeatures\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\ndef get_dtd_config(args):\n    args.num_scales = 2\n    args.feature_channels = 128\n    args.upsample_factor = 4\n    args.num_head = 1\n    args.ffn_dim_expansion = 4\n    args.num_transformer_layers = 6\n    args.attn_type = \"self_swin2d_cross_1d\"\n    args.attn_splits_list = [1, 8]\n    args.corr_radius_list = [-1, 4]\n    args.prop_radius_list = [-1, 1]\n    args.num_reg_refine = 3\n    args.mask_thr = 0.2\n\n    return args\n\ndef get_dino_config(args):\n\n    args.dino_model_name= 'dino_vits8'\n    args.dino_patch_size = 8\n    args.dino_stride = 4\n    args.dino_input_dim = 384\n    args.dino_layers = [5,11]\n\n    return args\n\nclass StereoDepthNet(torch.nn.Module):\n    def __init__(self, args, reg_refine=False):\n        super(StereoDepthNet, self).__init__()\n        args = get_dtd_config(args)\n        args = get_dino_config(args)\n        if reg_refine:\n            args.reg_refine = reg_refine\n            print(\"=> reg refine is set to: \", args.reg_refine)\n\n        dtd_model = DTD(\n            feature_channels=args.feature_channels,\n            num_scales=args.num_scales,\n            upsample_factor=args.upsample_factor,\n            num_head=args.num_head,\n            ffn_dim_expansion=args.ffn_dim_expansion,\n            num_transformer_layers=args.num_transformer_layers,\n            input_dim = args.dino_input_dim,\n            mask_thr=args.mask_thr\n        )\n\n        model = torch.nn.DataParallel(dtd_model)\n        self.model = model.module\n        self.args = args\n        self.image_net_normalizer = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n        self.dino = DinoV1ExtractFeatures(args.dino_model_name, args.dino_stride, device=\"cuda\")\n        self.dino.model.eval()\n        self.model.extract_feature = self.extract_features_dino_v1\n        self.feat = []\n\n\n    def extract_features_dino_v1(self, image0, image1):\n\n        image_shape = image0.shape[2:]\n        fine_height = 1 + (image_shape[0] - self.args.dino_patch_size) // self.args.dino_stride\n        fine_width = 1 + (image_shape[1] - self.args.dino_patch_size) // self.args.dino_stride\n\n        coarse_height = image_shape[0] // self.args.dino_patch_size\n        coarse_width = image_shape[1] // self.args.dino_patch_size\n\n\n        with torch.no_grad():\n            desc0 = self.dino._extract_features(image0, \n                                                layers=self.args.dino_layers, \n                                                facet=\"token\")\n            desc1 = self.dino._extract_features(image1, \n                                                layers=self.args.dino_layers, \n                                                facet=\"token\")\n\n            coarse_feat0 = desc0[-1][:, 1:, :].permute(0, 2, 1)\n            coarse_feat1 = desc1[-1][:, 1:, :].permute(0, 2, 1)\n\n            fine_feat0 = desc0[0][:, 1:, :].permute(0, 2, 1)\n            fine_feat1 = desc1[0][:, 1:, :].permute(0, 2, 1)\n\n        batch_size = desc0[0].shape[0]\n        coarse_feat0 = coarse_feat0.view(batch_size, -1, coarse_height, coarse_width)\n        coarse_feat1 = coarse_feat1.contiguous().view(batch_size, -1, coarse_height, coarse_width)\n\n        fine_feat0 = fine_feat0.contiguous().view(batch_size, -1, fine_height, fine_width)\n        fine_feat1 = fine_feat1.contiguous().view(batch_size, -1, fine_height, fine_width)\n\n        fine_feat0 = torch.nn.functional.pad(fine_feat0, (0, 1, 0, 1))\n        fine_feat1 = torch.nn.functional.pad(fine_feat1, (0, 1, 0, 1))\n\n        self.feat = ([coarse_feat0, fine_feat0],[coarse_feat1, fine_feat1]) \n\n        return [coarse_feat0, fine_feat0], [coarse_feat1, fine_feat1]\n\n\n    def forward(self, left, right, return_distance=False, masks=False, norm=False):\n\n        if norm:\n            left = self.image_net_normalizer(left)\n            right = self.image_net_normalizer(right)\n\n        outputs = self.model(\n            left,\n            right,\n            attn_type=self.args.attn_type,\n            attn_splits_list=self.args.attn_splits_list,\n            corr_radius_list=self.args.corr_radius_list,\n            prop_radius_list=self.args.prop_radius_list,\n        )\n\n        pred_disp = outputs[\"pred_disparities\"]\n        if return_distance:\n            distance = outputs[\"nn_distance\"]\n            if masks:\n                masks = outputs[\"bad_pixel_mask\"]\n                return pred_disp, distance, masks\n            return pred_disp, distance, None\n\n        return pred_disp, None, None\n"}
{"type": "source_file", "path": "losses/regularization_loss.py", "content": "import numpy as np\nimport torch\n\ndef get_disparity_smooth_loss(disp, img):\n    \"\"\"Computes the smoothness loss for a disparity image\n    The color image is used for edge-aware smoothness\n    \"\"\"\n    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n\n    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n\n    grad_disp_x *= torch.exp(-grad_img_x)\n    grad_disp_y *= torch.exp(-grad_img_y)\n\n    return grad_disp_x.mean() + grad_disp_y.mean()\n\ndef multi_scale_disparity_smooth_loss(multi_scale_disparities,\n                                         multi_scale_images, weight = 1e-3):\n\n    loss = 0\n    for scale in range(len(multi_scale_disparities)):\n        loss += weight * get_disparity_smooth_loss(\n            multi_scale_disparities[scale], multi_scale_images[scale]\n        )\n\n    return loss"}
{"type": "source_file", "path": "evaluation/eval_depth_weighted.py", "content": "from __future__ import absolute_import, division, print_function\nimport os\nimport tqdm\nimport cv2\nimport numpy as np\nimport pdb\ncv2.setNumThreads(0) \n#ROBOTCAR_SCALE_FACTOR = (0.239983 * 983.044006)\n#MS2_SCALE_FACTOR = (0.2991842 * 764.51385)\nROBOTCAR_SCALE_FACTOR = 0.23998*100.\n#MS2_SCALE_FACTOR = (0.2991842 * 100)\n\ndef weighted_mean(data):\n    weights = data != 999\n    weights = weights.astype(np.float32)\n    data = weights * data\n    data = np.sum(data,axis=0)\n    weights = np.sum(weights,axis=0)\n    data = data / weights\n\n    return data\ndef compute_errors(gt, pred):\n    \"\"\"Computation of error metrics between predicted and ground truth depths\n    \"\"\"\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n\n\ndef evaluate(pred_disps, gt_depths):\n    \"\"\"Evaluates a pretrained model using a specified test set\n    \"\"\"\n    MIN_DEPTH = 1e-3\n    MAX_DEPTH = 50\n    histogram_bins = np.arange(0 , MAX_DEPTH+5)[::5]\n\n    errors = []\n    all_bin_wise_metrics = []\n    for i in tqdm.tqdm(range(pred_disps.shape[0]),total = pred_disps.shape[0]):\n        gt_depth = gt_depths[i]\n        gt_height, gt_width = gt_depth.shape[:2]\n        pred_disp = pred_disps[i]\n        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n        pred_depth = ROBOTCAR_SCALE_FACTOR / (pred_disp + 1e-6)\n        mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n        if mask.sum() == 0:\n            print(\"mask sum is 0\")\n            continue\n        pred_depth = pred_depth[mask]\n        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n        gt_depth = gt_depth[mask]\n\n        bin_inds = np.digitize(gt_depth,histogram_bins)\n        bin_wise_metrics = []\n        sum_inds = 0\n        for bin_ind in range(1,len(histogram_bins)):\n            bin_mask = bin_inds == bin_ind\n            sum_inds += sum(bin_mask)\n            bin_gt_depth = gt_depth[bin_mask]\n            bin_pred_depth = pred_depth[bin_mask]\n            if sum(bin_mask) == 0:\n                bin_errors = np.array([999,999,999,999,999,999,999])\n            else:\n                bin_errors = compute_errors(bin_gt_depth,bin_pred_depth)\n            bin_wise_metrics.append(bin_errors)\n\n        all_bin_wise_metrics.append(np.array(bin_wise_metrics))\n        errors.append(compute_errors(gt_depth, pred_depth))\n        assert sum_inds == mask.sum()\n\n    mean_errors = np.array(errors).mean(0)\n    all_bin_wise_metrics = np.stack(all_bin_wise_metrics)\n    bin_means = weighted_mean(all_bin_wise_metrics).mean(0)\n    return mean_errors, bin_means\n\nif __name__ == \"__main__\":\n\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--pred_disp_path\", type=str, required=True, help=\"path to the disparities to evaluate\")\n    parser.add_argument(\"--gt_depths_path\", type=str, required=True, help=\"path to the ground truth depths\")\n    args = parser.parse_args()\n    print(\"-> Loading predictions from \",args.pred_disp_path)\n    pred_disps = np.load(args.pred_disp_path)\n    print(\"-> Loading ground truth from \",args.gt_depths_path)\n    gt_depths = np.load(args.gt_depths_path, allow_pickle=True)\n    unweighted_res, weighted_res  = evaluate(pred_disps, gt_depths)\n\n    print(\"-> Unweighted Metrics\")\n    print(unweighted_res)\n    print(\"-> Weighted Metrics\")\n    print(weighted_res)"}
{"type": "source_file", "path": "datasets/robotcar/sdk/interpolate_poses.py", "content": "################################################################################\n#\n# Copyright (c) 2017 University of Oxford\n# Authors:\n#  Geoff Pascoe (gmp@robots.ox.ac.uk)\n#\n# This work is licensed under the Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n#\n################################################################################\n\nimport bisect\nimport csv\nimport numpy as np\nimport numpy.matlib as ml\nfrom .transform import *\n\n\ndef interpolate_vo_poses(vo_path, pose_timestamps, origin_timestamp):\n    \"\"\"Interpolate poses from visual odometry.\n\n    Args:\n        vo_path (str): path to file containing relative poses from visual odometry.\n        pose_timestamps (list[int]): UNIX timestamps at which interpolated poses are required.\n        origin_timestamp (int): UNIX timestamp of origin frame. Poses will be reported relative to this frame.\n\n    Returns:\n        list[numpy.matrixlib.defmatrix.matrix]: SE3 matrix representing interpolated pose for each requested timestamp.\n\n    \"\"\"\n    with open(vo_path) as vo_file:\n        vo_reader = csv.reader(vo_file)\n        headers = next(vo_file)\n        vo_timestamps = [0]\n        abs_poses = [ml.identity(4)]\n\n        lower_timestamp = min(min(pose_timestamps), origin_timestamp)\n        upper_timestamp = max(max(pose_timestamps), origin_timestamp)\n\n        for row in vo_reader:\n            timestamp = int(row[0])\n            if timestamp < lower_timestamp:\n                vo_timestamps[0] = timestamp\n                continue\n\n            vo_timestamps.append(timestamp)\n\n            xyzrpy = [float(v) for v in row[2:8]]\n\n            #to_kitti#\n            xyzrpy = [xyzrpy[1], xyzrpy[2], xyzrpy[0], xyzrpy[4], xyzrpy[5], xyzrpy[3]]\n\n            rel_pose = build_se3_transform(xyzrpy)\n            abs_pose = abs_poses[-1] * rel_pose\n            abs_poses.append(abs_pose)\n\n            if timestamp >= upper_timestamp:\n                break\n\n    return interpolate_poses(vo_timestamps, abs_poses, pose_timestamps, origin_timestamp)\n\n\ndef interpolate_ins_poses(ins_path, pose_timestamps, origin_timestamp, use_rtk=False):\n    \"\"\"Interpolate poses from INS.\n\n    Args:\n        ins_path (str): path to file containing poses from INS.\n        pose_timestamps (list[int]): UNIX timestamps at which interpolated poses are required.\n        origin_timestamp (int): UNIX timestamp of origin frame. Poses will be reported relative to this frame.\n\n    Returns:\n        list[numpy.matrixlib.defmatrix.matrix]: SE3 matrix representing interpolated pose for each requested timestamp.\n\n    \"\"\"\n    with open(ins_path) as ins_file:\n        ins_reader = csv.reader(ins_file)\n        headers = next(ins_file)\n\n        ins_timestamps = [0]\n        abs_poses = [ml.identity(4)]\n\n        upper_timestamp = max(max(pose_timestamps), origin_timestamp)\n\n        for row in ins_reader:\n            timestamp = int(row[0])\n            ins_timestamps.append(timestamp)\n\n            utm = row[5:8] if not use_rtk else row[4:7]\n            rpy = row[-3:] if not use_rtk else row[11:14]\n            xyzrpy = [float(v) for v in utm] + [float(v) for v in rpy]\n            if use_rtk: xyzrpy[-1] =  xyzrpy[-1] + (3. * np.pi / 2) # this is a fix for the rtk data\n            xyzrpy = [xyzrpy[1], xyzrpy[2], xyzrpy[0], xyzrpy[4], xyzrpy[5], xyzrpy[3]]\n            abs_pose = build_se3_transform(xyzrpy)\n            abs_poses.append(abs_pose)\n\n            if timestamp >= upper_timestamp:\n                break\n\n    ins_timestamps = ins_timestamps[1:]\n    abs_poses = abs_poses[1:]\n\n    return interpolate_poses(ins_timestamps, abs_poses, pose_timestamps, origin_timestamp)\n\n\ndef interpolate_poses(pose_timestamps, abs_poses, requested_timestamps, origin_timestamp):\n    \"\"\"Interpolate between absolute poses.\n\n    Args:\n        pose_timestamps (list[int]): Timestamps of supplied poses. Must be in ascending order.\n        abs_poses (list[numpy.matrixlib.defmatrix.matrix]): SE3 matrices representing poses at the timestamps specified.\n        requested_timestamps (list[int]): Timestamps for which interpolated timestamps are required.\n        origin_timestamp (int): UNIX timestamp of origin frame. Poses will be reported relative to this frame.\n\n    Returns:\n        list[numpy.matrixlib.defmatrix.matrix]: SE3 matrix representing interpolated pose for each requested timestamp.\n\n    Raises:\n        ValueError: if pose_timestamps and abs_poses are not the same length\n        ValueError: if pose_timestamps is not in ascending order\n\n    \"\"\"\n    #requested_timestamps.insert(0, origin_timestamp)\n    requested_timestamps = np.insert(requested_timestamps, 0, origin_timestamp)\n    requested_timestamps = np.array(requested_timestamps)\n    pose_timestamps = np.array(pose_timestamps)\n\n    if len(pose_timestamps) != len(abs_poses):\n        raise ValueError('Must supply same number of timestamps as poses')\n\n    abs_quaternions = np.zeros((4, len(abs_poses)))\n    abs_positions = np.zeros((3, len(abs_poses)))\n    for i, pose in enumerate(abs_poses):\n        if i > 0 and pose_timestamps[i-1] >= pose_timestamps[i]:\n            raise ValueError('Pose timestamps must be in ascending order')\n\n        abs_quaternions[:, i] = so3_to_quaternion(pose[0:3, 0:3])\n        abs_positions[:, i] = np.ravel(pose[0:3, 3])\n\n    upper_indices = [bisect.bisect(pose_timestamps, pt) for pt in requested_timestamps]\n    lower_indices = [u - 1 for u in upper_indices]\n\n    if max(upper_indices) >= len(pose_timestamps):\n        upper_indices = [min(i, len(pose_timestamps) - 1) for i in upper_indices]\n\n    fractions = (requested_timestamps - pose_timestamps[lower_indices]) // \\\n                (pose_timestamps[upper_indices] - pose_timestamps[lower_indices])\n\n    quaternions_lower = abs_quaternions[:, lower_indices]\n    quaternions_upper = abs_quaternions[:, upper_indices]\n\n    d_array = (quaternions_lower * quaternions_upper).sum(0)\n\n    linear_interp_indices = np.nonzero(d_array >= 1)\n    sin_interp_indices = np.nonzero(d_array < 1)\n\n    scale0_array = np.zeros(d_array.shape)\n    scale1_array = np.zeros(d_array.shape)\n\n    scale0_array[linear_interp_indices] = 1 - fractions[linear_interp_indices]\n    scale1_array[linear_interp_indices] = fractions[linear_interp_indices]\n\n    theta_array = np.arccos(np.abs(d_array[sin_interp_indices]))\n\n    scale0_array[sin_interp_indices] = \\\n        np.sin((1 - fractions[sin_interp_indices]) * theta_array) / np.sin(theta_array)\n    scale1_array[sin_interp_indices] = \\\n        np.sin(fractions[sin_interp_indices] * theta_array) / np.sin(theta_array)\n\n    negative_d_indices = np.nonzero(d_array < 0)\n    scale1_array[negative_d_indices] = -scale1_array[negative_d_indices]\n\n    quaternions_interp = np.tile(scale0_array, (4, 1)) * quaternions_lower \\\n                         + np.tile(scale1_array, (4, 1)) * quaternions_upper\n\n    positions_lower = abs_positions[:, lower_indices]\n    positions_upper = abs_positions[:, upper_indices]\n\n    positions_interp = np.multiply(np.tile((1 - fractions), (3, 1)), positions_lower) \\\n                       + np.multiply(np.tile(fractions, (3, 1)), positions_upper)\n\n    poses_mat = ml.zeros((4, 4 * len(requested_timestamps)))\n\n    poses_mat[0, 0::4] = 1 - 2 * np.square(quaternions_interp[2, :]) - \\\n                         2 * np.square(quaternions_interp[3, :])\n    poses_mat[0, 1::4] = 2 * np.multiply(quaternions_interp[1, :], quaternions_interp[2, :]) - \\\n                         2 * np.multiply(quaternions_interp[3, :], quaternions_interp[0, :])\n    poses_mat[0, 2::4] = 2 * np.multiply(quaternions_interp[1, :], quaternions_interp[3, :]) + \\\n                         2 * np.multiply(quaternions_interp[2, :], quaternions_interp[0, :])\n\n    poses_mat[1, 0::4] = 2 * np.multiply(quaternions_interp[1, :], quaternions_interp[2, :]) \\\n                         + 2 * np.multiply(quaternions_interp[3, :], quaternions_interp[0, :])\n    poses_mat[1, 1::4] = 1 - 2 * np.square(quaternions_interp[1, :]) \\\n                         - 2 * np.square(quaternions_interp[3, :])\n    poses_mat[1, 2::4] = 2 * np.multiply(quaternions_interp[2, :], quaternions_interp[3, :]) - \\\n                         2 * np.multiply(quaternions_interp[1, :], quaternions_interp[0, :])\n\n    poses_mat[2, 0::4] = 2 * np.multiply(quaternions_interp[1, :], quaternions_interp[3, :]) - \\\n                         2 * np.multiply(quaternions_interp[2, :], quaternions_interp[0, :])\n    poses_mat[2, 1::4] = 2 * np.multiply(quaternions_interp[2, :], quaternions_interp[3, :]) + \\\n                         2 * np.multiply(quaternions_interp[1, :], quaternions_interp[0, :])\n    poses_mat[2, 2::4] = 1 - 2 * np.square(quaternions_interp[1, :]) - \\\n                         2 * np.square(quaternions_interp[2, :])\n\n    poses_mat[0:3, 3::4] = positions_interp\n    poses_mat[3, 3::4] = 1\n\n    poses_mat = np.linalg.solve(poses_mat[0:4, 0:4], poses_mat)\n\n    poses_out = [0] * (len(requested_timestamps) - 1)\n    for i in range(1, len(requested_timestamps)):\n        poses_out[i - 1] = poses_mat[0:4, i * 4:(i + 1) * 4]\n\n    return poses_out\n\nif __name__ == '__main__':\n\n    import os\n    import re\n\n    pose_file_path = '/mnt/nas/madhu/robotcar/night/2014-12-16-18-44-24/gps/rtk.csv'\n    image_timestamp_path = '/mnt/nas/madhu/robotcar/night/2014-12-16-18-44-24/files/train.txt'\n    extrinsics_dir = '/home/madhu/code/feature-slam/playground/robotcar-dataset-sdk/extrinsics'\n    camera = 'stereo'\n    extrinsics_path = os.path.join(extrinsics_dir, camera + '.txt')\n    with open(extrinsics_path) as extrinsics_file:\n        extrinsics = [float(x) for x in next(extrinsics_file).split(' ')]\n\n    G_camera_vehicle = build_se3_transform(extrinsics)\n    G_camera_posesource = None\n\n    poses_type = re.search('(vo|ins|rtk)\\.csv', pose_file_path).group(1)\n    if poses_type in ['ins', 'rtk']:\n        with open(os.path.join(extrinsics_dir, 'ins.txt')) as extrinsics_file:\n            extrinsics = next(extrinsics_file)\n            G_camera_posesource = G_camera_vehicle * build_se3_transform([float(x) for x in extrinsics.split(' ')])\n    \n    timestamps = np.loadtxt(image_timestamp_path, dtype=np.int64)\n    unique_timestamps = np.unique(timestamps)\n    unique_timestamps.sort()\n    poses = interpolate_ins_poses(pose_file_path, unique_timestamps, unique_timestamps[0],use_rtk = True)\n    poses = np.array(poses)\n\n\n\n    pose_dict = {}\n    for stamp, pose in zip(unique_timestamps, poses):\n        pose_dict[stamp] = pose\n        \n    rand_ind = np.random.randint(0, len(timestamps), 1)[0]\n    print(rand_ind)\n    triplet = timestamps[rand_ind]\n\n    pose1 = pose_dict[triplet[0]]\n    pose2 = pose_dict[triplet[1]]\n    np.set_printoptions(suppress=True)\n\n\n    print(np.linalg.inv(pose2) @ pose1)\n\n    "}
{"type": "source_file", "path": "losses/photometric_loss.py", "content": "import torch\nimport torch.nn as nn\n\nclass SSIM(nn.Module):\n    \"\"\"Layer to compute the SSIM loss between a pair of images\n    \"\"\"\n    def __init__(self):\n        super(SSIM, self).__init__()\n        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n\n        self.refl = nn.ReflectionPad2d(1)\n\n        self.C1 = 0.01 ** 2\n        self.C2 = 0.03 ** 2\n\n    def forward(self, x, y):\n        x = self.refl(x)\n        y = self.refl(y)\n\n        mu_x = self.mu_x_pool(x)\n        mu_y = self.mu_y_pool(y)\n\n        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n\n        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n\n        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n\nclass PhotometricLoss:\n\n    def __init__(self, weights = [0.15,0.85]):\n\n        self.weights = weights\n        self.ssim = SSIM()\n\n    def simple_photometric_loss(self,original_image, reconstructed_image, weights = [0.15,0.85]):\n\n        l1_loss = torch.abs(original_image - reconstructed_image).mean(1,True)\n        ssim_loss = self.ssim(original_image, reconstructed_image).mean(1,True)\n\n        losses = [l1_loss, ssim_loss]\n\n        weighted_loss = 0\n        for i in range(len(weights)):\n            weighted_loss += weights[i] * losses[i]\n\n        return weighted_loss\n\n    def identiy_photometric_loss(self, source_image, targate_image, weights = [0.15,0.85]):\n\n        return self.simple_photometric_loss(source_image, targate_image, weights)\n\n    def minimum_photometric_loss(self,original_image, reconstructed_images):\n        \n\n        losses = []\n        for idx, recon_image in enumerate(reconstructed_images):\n            photometric_loss = self.simple_photometric_loss(original_image, recon_image, self.weights)\n            losses.append(photometric_loss)\n        losses = torch.stack(losses, dim=1)\n\n        return torch.min(losses, dim=1)[0]\n\n\nclass MultiScalePhotometricLoss(nn.Module):\n\n    def __init__(self, full_scale = False):\n\n        super(MultiScalePhotometricLoss, self).__init__()\n\n        self.ssim = SSIM()\n\n        self.full_scale = full_scale\n\n    def simple_loss(self,original_image, reconstructed_image, weights = [0.15,0.85]):\n\n        assert original_image.shape == reconstructed_image.shape\n\n        l1_loss = torch.abs(original_image - reconstructed_image).mean(1,True)\n        ssim_loss = self.ssim(original_image, reconstructed_image).mean(1,True)\n\n        losses = [l1_loss, ssim_loss]\n\n        weighted_loss = 0\n        for i in range(len(weights)):\n            weighted_loss += weights[i] * losses[i]\n\n        return weighted_loss\n\n    def forward(self, reconstructed_images,original_images, reduce_mean = True):\n\n        assert len(reconstructed_images) == len(original_images)\n\n        total_loss = 0.0\n        for original, recon in zip(original_images, reconstructed_images):\n            \n            if self.full_scale:\n                loss = self.simple_loss(original_images[0], recon)\n            else:\n                loss = self.simple_loss(original, recon)\n\n            total_loss += loss.mean()\n\n        total_loss = total_loss / len(original_images)\n        \n        return total_loss\n\n\nif __name__ == \"__main__\":\n    import cv2\n\n    image = cv2.imread(\"/mnt/nas/kaichen/kitti/2011_09_26/2011_09_26_drive_0001_sync/image_02/data/0000000000.png\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = torch.from_numpy(image).permute(2, 0, 1).float().unsqueeze(0)\n    image = image / 255.0\n\n    \n\n\n\n   \n"}
{"type": "source_file", "path": "datasets/robotcar/sdk/velodyne.py", "content": "################################################################################\n#\n# Copyright (c) 2017 University of Oxford\n# Authors:\n#  Dan Barnes (dbarnes@robots.ox.ac.uk)\n#\n# This work is licensed under the Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n#\n###############################################################################\n\nfrom typing import AnyStr\nimport numpy as np\nimport os\nimport cv2\n\n# Hard coded configuration to simplify parsing code\nhdl32e_range_resolution = 0.002  # m / pixel\nhdl32e_minimum_range = 1.0\nhdl32e_elevations = np.array([-0.1862, -0.1628, -0.1396, -0.1164, -0.0930,\n                              -0.0698, -0.0466, -0.0232, 0., 0.0232, 0.0466, 0.0698,\n                              0.0930, 0.1164, 0.1396, 0.1628, 0.1862, 0.2094, 0.2327,\n                              0.2560, 0.2793, 0.3025, 0.3259, 0.3491, 0.3723, 0.3957,\n                              0.4189, 0.4421, 0.4655, 0.4887, 0.5119, 0.5353])[:, np.newaxis]\nhdl32e_base_to_fire_height = 0.090805\nhdl32e_cos_elevations = np.cos(hdl32e_elevations)\nhdl32e_sin_elevations = np.sin(hdl32e_elevations)\n\n\ndef load_velodyne_binary(velodyne_bin_path: AnyStr):\n    \"\"\"Decode a binary Velodyne example (of the form '<timestamp>.bin')\n    Args:\n        example_path (AnyStr): Oxford Radar RobotCar Dataset binary Velodyne pointcloud example path\n    Returns:\n        ptcld (np.ndarray): XYZI pointcloud from the binary Velodyne data Nx4\n    Notes:\n        - The pre computed points are *NOT* motion compensated.\n        - Converting a raw velodyne scan to pointcloud can be done using the\n            `velodyne_ranges_intensities_angles_to_pointcloud` function.\n    \"\"\"\n    ext = os.path.splitext(velodyne_bin_path)[1]\n    if ext != \".bin\":\n        raise RuntimeError(\"Velodyne binary pointcloud file should have `.bin` extension but had: {}\".format(ext))\n    if not os.path.isfile(velodyne_bin_path):\n        raise FileNotFoundError(\"Could not find velodyne bin example: {}\".format(velodyne_bin_path))\n    data = np.fromfile(velodyne_bin_path, dtype=np.float32)\n    ptcld = data.reshape((4, -1))\n    return ptcld\n\n\ndef load_velodyne_raw(velodyne_raw_path: AnyStr):\n    \"\"\"Decode a raw Velodyne example. (of the form '<timestamp>.png')\n    Args:\n        example_path (AnyStr): Oxford Radar RobotCar Dataset raw Velodyne example path\n    Returns:\n        ranges (np.ndarray): Range of each measurement in meters where 0 == invalid, (32 x N)\n        intensities (np.ndarray): Intensity of each measurement where 0 == invalid, (32 x N)\n        angles (np.ndarray): Angle of each measurement in radians (1 x N)\n        approximate_timestamps (np.ndarray): Approximate linearly interpolated timestamps of each mesaurement (1 x N).\n            Approximate as we only receive timestamps for each packet. The timestamp of the next frame will was used to\n            interpolate the last packet timestamps. If there was no next frame, the last packet timestamps was\n            extrapolated. The original packet timestamps can be recovered with:\n                approximate_timestamps(:, 1:12:end) (12 is the number of azimuth returns in each packet)\n     Notes:\n       Reference: https://velodynelidar.com/lidar/products/manual/63-9113%20HDL-32E%20manual_Rev%20E_NOV2012.pdf\n    \"\"\"\n    ext = os.path.splitext(velodyne_raw_path)[1]\n    if ext != \".png\":\n        raise RuntimeError(\"Velodyne raw file should have `.png` extension but had: {}\".format(ext))\n    if not os.path.isfile(velodyne_raw_path):\n        raise FileNotFoundError(\"Could not find velodyne raw example: {}\".format(velodyne_raw_path))\n    example = cv2.imread(velodyne_raw_path, cv2.IMREAD_GRAYSCALE)\n    intensities, ranges_raw, angles_raw, timestamps_raw = np.array_split(example, [32, 96, 98], 0)\n    ranges = np.ascontiguousarray(ranges_raw.transpose()).view(np.uint16).transpose()\n    ranges = ranges * hdl32e_range_resolution\n    angles = np.ascontiguousarray(angles_raw.transpose()).view(np.uint16).transpose()\n    angles = angles * (2. * np.pi) / 36000\n    approximate_timestamps = np.ascontiguousarray(timestamps_raw.transpose()).view(np.int64).transpose()\n    return ranges, intensities, angles, approximate_timestamps\n\n\ndef velodyne_raw_to_pointcloud(ranges: np.ndarray, intensities: np.ndarray, angles: np.ndarray):\n    \"\"\" Convert raw Velodyne data (from load_velodyne_raw) into a pointcloud\n    Args:\n        ranges (np.ndarray): Raw Velodyne range readings\n        intensities (np.ndarray): Raw Velodyne intensity readings\n        angles (np.ndarray): Raw Velodyne angles\n    Returns:\n        pointcloud (np.ndarray): XYZI pointcloud generated from the raw Velodyne data Nx4\n\n    Notes:\n        - This implementation does *NOT* perform motion compensation on the generated pointcloud.\n        - Accessing the pointclouds in binary form via `load_velodyne_pointcloud` is approximately 2x faster at the cost\n            of 8x the storage space\n    \"\"\"\n    valid = ranges > hdl32e_minimum_range\n    z = hdl32e_sin_elevations * ranges - hdl32e_base_to_fire_height\n    xy = hdl32e_cos_elevations * ranges\n    x = np.sin(angles) * xy\n    y = -np.cos(angles) * xy\n\n    xf = x[valid].reshape(-1)\n    yf = y[valid].reshape(-1)\n    zf = z[valid].reshape(-1)\n    intensityf = intensities[valid].reshape(-1).astype(np.float32)\n    ptcld = np.stack((xf, yf, zf, intensityf), 0)\n    return ptcld\n"}
{"type": "source_file", "path": "evaluation/junk/eval_depth_unweighted.py", "content": "from __future__ import absolute_import, division, print_function\n\nimport os\nimport tqdm\nimport cv2\nimport numpy as np\nimport pdb\n\ncv2.setNumThreads(0)  \n\n\nROBOTCAR_SCALE_FACTOR = 0.239983*100.\n#MS2_SCALE_FACTOR = 0.2991842 * 100.\n\n\ndef compute_errors(gt, pred):\n    \"\"\"Computation of error metrics between predicted and ground truth depths\n    \"\"\"\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n\ndef evaluate(pred_disps, gt_depths):\n    \"\"\"Evaluates a pretrained model using a specified test set\n    \"\"\"\n    MIN_DEPTH = 1e-3\n    MAX_DEPTH = 50\n    print(\"-> Evaluating\")\n\n    errors = []\n    for i in tqdm.tqdm(range(pred_disps.shape[0]),total = pred_disps.shape[0]):\n        gt_depth = gt_depths[i]\n        gt_height, gt_width = gt_depth.shape[:2]\n        pred_disp = pred_disps[i]\n        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n        pred_depth = ROBOTCAR_SCALE_FACTOR / (pred_disp + 1e-6)\n        mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n        if mask.sum() == 0:\n            print(\"mask sum is 0\")\n            continue\n        pred_depth = pred_depth[mask]\n        gt_depth = gt_depth[mask]\n\n        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n        errors.append(compute_errors(gt_depth, pred_depth))\n\n    mean_errors = np.array(errors).mean(0)\n    return mean_errors\n\nif __name__ == \"__main__\":\n\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--pred_disp_path\", type=str, required=True, help=\"path to the disparities to evaluate\")\n    parser.add_argument(\"--gt_depths_path\", type=str, required=True, help=\"path to the ground truth depths\")\n\n    args = parser.parse_args()\n    print(\"-> Loading predictions from \",args.pred_disp_path)\n    pred_disps = np.load(args.pred_disp_path)\n    print(\"-> Loading ground truth from \",args.gt_depths_path)\n    gt_depths = np.load(args.gt_depths_path, allow_pickle=True)\n    results  = evaluate(pred_disps, gt_depths)\n    print(results)"}
{"type": "source_file", "path": "models/__init__.py", "content": ""}
{"type": "source_file", "path": "models/image_warping.py", "content": "import torch\nfrom .utils import BackprojectDepth\nfrom .utils import Project3D\nimport torch.nn.functional as F\nimport pdb\n\n\n\n\n\nclass ImageWarping(torch.nn.Module):\n    def __init__(self, batch_size, image_height, image_width):\n        super(ImageWarping, self).__init__()\n        self.to_pts = BackprojectDepth(batch_size, image_height, image_width).cuda()\n        self.to_pix = Project3D(batch_size, image_height, image_width).cuda()\n\n    def forward(self, image, depth, pose, intrinsics):\n\n        assert (\n            len(image.shape) == 4\n            and len(depth.shape) == 4\n            and len(pose.shape) == 3\n            and len(intrinsics.shape) == 3\n        )\n\n        pts = self.to_pts(depth, intrinsics)\n        pixels = self.to_pix(pts, intrinsics, pose)\n\n        reconstrcted_image = F.grid_sample(image, pixels, padding_mode=\"border\")\n\n        return reconstrcted_image\n\n\n\n\n\nclass SingleScaleImageWarping(torch.nn.Module):\n    def __init__(self, batch_size, image_width, image_height, num_scales):\n        super(SingleScaleImageWarping, self).__init__()\n        self.num_scales = num_scales\n\n        self.image_warper = ImageWarping(batch_size, image_width, image_height).cuda()\n        print(\"Single Scale ImageWarping initialized\")\n\n    def forward(self, images, depths, poses, intrinsics):\n\n        assert len(images) == len(depths) == len(intrinsics) == self.num_scales\n\n\n        reconstrcted_images = []\n        for i in range(self.num_scales):\n            reconstrcted_images.append(\n                self.image_warper(images[0], depths[i], poses, intrinsics[0])\n            )\n\n        return reconstrcted_images\n\n\nclass MultiScaleImageWarping(torch.nn.Module):\n    def __init__(self, batch_size, image_width, image_height, num_scales):\n        super(MultiScaleImageWarping, self).__init__()\n        self.num_scales = num_scales\n        self.image_warpers = []\n        for i in range(num_scales):\n            scaled_image_width = image_width // (2**i)\n            scaled_image_height = image_height // (2**i)\n\n            self.image_warpers.append(\n                ImageWarping(batch_size, scaled_image_width, scaled_image_height).cuda()\n            )\n        print(\"MultiScaleImageWarping initialized\")\n\n    def forward(self, images, depths, poses, intrinsics):\n\n        assert len(images) == len(depths) == len(intrinsics) == self.num_scales\n\n\n        reconstrcted_images = []\n        for i in range(self.num_scales):\n            reconstrcted_images.append(\n                self.image_warpers[i](images[i], depths[i], poses, intrinsics[i])\n            )\n\n        return reconstrcted_images\n\n\n\nclass MaskedImageWarping(torch.nn.Module):\n\n    def __init__(self, batch_size, image_width, image_height):\n        super(MaskedImageWarping, self).__init__()\n        self.to_pts = BackprojectDepth(batch_size, image_height, image_width).cuda()\n        self.to_pix = Project3D(batch_size, image_height, image_width).cuda()\n\n        self.image_height = image_height\n        self.image_width = image_width\n        self.batch_size = batch_size\n\n    def generate_mask(self, scene_coords, pix_coords):\n\n        # in this function we are going to generate a mask \n\n        # scene coords \n\n        pdb.set_trace() \n        delta = torch.norm(pix_coords[:, 2, :, :], self.to_pts.get_original_pix_coords()[:, 2, :, :], dim=1,keepdim=True)\n        scene_coords_mask = torch.logical_and(scene_coords[:, 2, :, :] > 0, scene_coords[:, 2, :, :] < 100)\n        pix_coords_mask = delta > 32 # 32 is the threshold for the flow\n        mask = torch.logical_and(scene_coords_mask, pix_coords_mask)\n\n        \n\n\n\n\n    def forward(self, image, depth, pose, intrinsics):\n\n        assert (\n            len(image.shape) == 4\n            and len(depth.shape) == 4\n            and len(pose.shape) == 3\n            and len(intrinsics.shape) == 3\n        )\n\n        pts = self.to_pts(depth, intrinsics)\n\n        scene_coords = torch.matmul(pose, pts)\n        pixels = self.to_pix(pts, intrinsics, pose)\n        pix_coords = self.to_pix.get_original_pix_coords()\n\n        mask = self.generate_mask(scene_coords, pix_coords)\n\n        reconstrcted_image = F.grid_sample(image, pixels, padding_mode=\"border\")\n\n        return reconstrcted_image\n"}
{"type": "source_file", "path": "evaluation/junk/evaluation_stats.py", "content": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef comparison_bar_plot(mono,stereo):\n\n    print(mono[2:].mean(0))\n    print(stereo[2:].mean(0))\n\n    print(mono.shape)\n    print(stereo.shape)\n\n    #plt.figure(figsize=(6,4))\n\n    bar_width = 1.5\n    x = np.arange(0,55)[::5]\n\n    plt.bar(x, mono[:,1], bar_width)\n    plt.bar(x + bar_width, stereo[:,1], bar_width)\n    plt.xlabel('Depth buckets (m)',fontsize=18)\n    plt.grid()\n    plt.tight_layout()\n    plt.ylabel('Sq. Rel Error',fontsize=18)\n    plt.xticks(fontsize = 18)\n    plt.yticks(fontsize = 18)\n    plt.legend(['Ours','IGEV-Stereo'],fontsize=18)\n    \n\n    # ax[0].bar(x , mono[1:,2], bar_width, label='DINO')\n    # #ax[0].bar(x, stereo[1:,2][::space], bar_width, label='UniMatch')\n    # #ax[0].bar(x + bar_width, mono_stereo[1:,2][::space], bar_width, label='mono_stereo')\n    # ax[0].set_ylabel('RMSE')\n    # ax[0].legend()\n    # ax[0].grid()\n\n    # ax[1].bar(x - bar_width, mono[1:,-3][::space], bar_width, label='DINO')\n    # ax[1].bar(x, stereo[1:,-3][::space], bar_width, label='UniMatch')\n    # #ax[1].bar(x + bar_width, mono_stereo[1:,-3][::space], bar_width, label='mono_stereo')\n    # ax[1].set_xlabel('Depth buckets')\n    # ax[1].set_ylabel('delta1')\n    # ax[1].legend()\n    # ax[1].grid()\n    plt.tight_layout()\n    plt.savefig('ours_vs_igev.png')\n\n\n\ndef weighted_mean(data_path):\n    data = np.load(data_path)\n    weights = data != 999\n    weights = weights.astype(np.float32)\n    data = weights * data\n    data = np.sum(data,axis=0)\n    weights = np.sum(weights,axis=0)\n    data = data / weights\n\n    return data\n\n# I am going to plot two main metrics: RMSE and delta1\n\ndata2 = weighted_mean('bin_wise_metrics_MS2_sgm.npy')\ndata1 = weighted_mean('bin_wise_metrics_MS2_ours.npy')\n\n# data3 = weighted_mean('bin_wise_metrics_mono_stereo_monodepth2.npy')\n\ncomparison_bar_plot(data1,data2)\n\n\n\n'''\ndata = weighted_mean('bin_wise_metrics.npy')\nplt.grid()\nfig, ax = plt.subplots(2,1)\n\nax[0].bar(np.arange(1,80,2)[1:],data[1:,2], label='RMSE',align='edge',width=1.0)\nax[0].set_ylabel('RMSE')\nax[0].grid()\n\nax[1].bar(np.arange(1,80,2)[1:],data[1:,-3], label='delta1',align='edge',width=1.0)\nax[1].set_ylabel('delta1')\nax[1].set_xlabel('Depth buckets')\nax[1].grid()\nplt.tight_layout() \nplt.savefig('metrics.png')\n'''"}
{"type": "source_file", "path": "train.py", "content": "import torch\nimport tqdm\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom models.depth_model import StereoDepthNet as DepthNetwork\nfrom models.image_warping import ImageWarping\nfrom losses.photometric_loss import PhotometricLoss\nfrom losses.regularization_loss import get_disparity_smooth_loss\nimport argparse\n\ndef viz(left_image, disp):\n    disp = disp[0].detach().cpu().numpy()\n    left_image = left_image.squeeze().detach().permute(1, 2, 0).cpu().numpy()\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    ax[0].imshow(left_image)\n    ax[1].imshow(disp.squeeze(), cmap=\"plasma\")\n    ax[0].axis(\"off\")\n    ax[1].axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(viz_dir+\"/disparity.png\")\n    plt.clf()\n    plt.close()\n\n\ndef viz_mask(left_image, disp):\n    disp = disp[0].detach().cpu().numpy()\n    left_image = left_image.squeeze().detach().permute(1, 2, 0).cpu().numpy()\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    ax[0].imshow(left_image)\n    ax[1].imshow(disp.squeeze(), cmap=\"plasma\")\n    ax[0].axis(\"off\")\n    ax[1].axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(viz_dir+\"/mask.png\")\n    plt.clf()\n    plt.close()\n\n\ndef viz_error(image, error):\n    image = image.squeeze().detach().permute(1, 2, 0).cpu().numpy()\n    error = error.squeeze().detach().cpu().numpy()\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    ax[0].imshow(image)\n    ax[1].imshow(error.squeeze())\n    ax[0].axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(viz_dir+\"/photometric_error.png\")\n    plt.clf()\n    plt.close()\n\n\ndef get_stereo_pose(batch_size):\n    # assuming the dataset is KITTI\n    LEFT_TO_RIGHT_STEREO_POSE = np.eye(4, dtype=np.float32)\n    LEFT_TO_RIGHT_STEREO_POSE[0, 3] = -1  # 0.239983 # baseline\n    LEFT_TO_RIGHT_STEREO_POSE = torch.from_numpy(LEFT_TO_RIGHT_STEREO_POSE).cuda()\n    LEFT_TO_RIGHT_STEREO_POSE = LEFT_TO_RIGHT_STEREO_POSE.unsqueeze(0).repeat(\n        batch_size, 1, 1\n    )\n    return LEFT_TO_RIGHT_STEREO_POSE\n\n\ndef train(epoch):\n\n    depth_net.model.train()\n    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n    for iter, data in progress_bar:\n        reference_idx = 0\n        reference_key = \"frame{}\".format(reference_idx)\n        left_image = data[reference_key][\"image\"].cuda()\n        right_image = data[reference_key][\"stereo_pair\"].cuda()\n        K = data[reference_key][\"camera_matrix\"].cuda()\n        predicted_disparities, nn_distances, mask = depth_net(\n            left_image, right_image, return_distance=True, masks=True, norm=True\n        )\n        pose = get_stereo_pose(left_image.shape[0])\n        total_loss = 0\n\n        ############ koleo reg_loss ############\n        distance = nn_distances[-1]\n        distance_loss = -1.0 * ((1 - distance) ** 2) * torch.log(distance + 1e-6)\n        distance_loss = distance_loss.mean()\n        total_loss += distance_loss\n        \n\n        for idx, disp in enumerate(predicted_disparities):\n            disp = disp.unsqueeze(1)\n            depth = 100.0 / (disp + 1e-6)\n            warped_right_image = warper(right_image, depth, pose, K)\n            photo_loss = loss_fn.simple_photometric_loss(left_image, warped_right_image)\n            loss = photo_loss.mean(2, True).mean(3, True).mean()\n            total_loss += loss\n\n            ############ Gradient Smoothing ###############\n            if idx < 2:\n                weight = 1 / 8\n            else:\n                weight = 1 / 4\n            smoothloss = get_disparity_smooth_loss(disp, left_image)\n            total_loss += smoothloss * 0.1 * weight\n\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        progress_bar.set_description(\n            \"epoch: {}/{} training loss: {:.4f}\".format(\n                epoch, args.num_epochs, loss.item()\n            )\n        )\n        if iter % 100 == 0:\n            viz(warped_right_image[0:1], disp[:, 0:1])\n            viz_error(left_image[0:1], photo_loss[0:1])\n            viz_mask(warped_right_image[0:1], mask[0][0:1])\n\n        torch.cuda.empty_cache()\n\n\nif __name__ == \"__main__\":\n\n    #args = argparse.ArgumentParser()\n\n    parser = argparse.ArgumentParser(description=\"Depth Estimation\")\n    parser.add_argument(\"--image_height\", type=int, default=192,help=\"image height\")\n    parser.add_argument(\"--image_width\", type=int, default=320,help=\"image width\")\n    parser.add_argument(\"--batch_size\", type=int, default=1,help=\"batch size\")\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4,help=\"learning rate\")\n    parser.add_argument(\"--dataset\", type=str, default=\"robotcar\",help=\"dataset\")\n    parser.add_argument('--data_path', type=str, \n                            default=\"/hdd1/madhu/data/robotcar/2014-12-16-18-44-24/stereo/\",\n                            help=\"path to the dataset\")\n    parser.add_argument('--ms2_train_file', type=str,\n                            default=\"/hdd1/madhu/data/ms2/train_nighttime_list.txt\",\n                            help=\"path to the ms2 trian file, only used when dataset is ms2\")\n    parser.add_argument('--use_full_res',action='store_true',help=\"use full resolution images\")\n    parser.add_argument('--checkpoint_dir', type=str, \n                        default=\"checkpoints/test_model\",\n                            help=\"path to the checkpoint directory\")\n    parser.add_argument('--resume',action='store_true',help=\"resume training\")\n    parser.add_argument('--pretrained_ckpt', type=str, \n                        default=\"/mnt/nas/madhu/data/checkpoints/chapter_3/dino_unimatch_v2_smooth_0.1/depth_net_20.pth\",\n                            help=\"path to the pretrained checkpoint\")\n    parser.add_argument('--num_epochs', type=int, default=20,help=\"number of epochs\")\n\n    \n    args = parser.parse_args()\n    args.working_resolution = (args.image_width, args.image_height)\n    if args.dataset == \"robotcar\":\n        from datasets.robotcar.dataloader import RobotCarDataset\n        dataset = RobotCarDataset(args)\n\n    elif args.dataset == \"ms2\":\n        from datasets.ms2.depth_test_dataloader import MS2Dataset\n        args.test_file_path = args.ms2_train_file\n        #args.data_path = \"/hdd1/madhu/data/ms2\"\n        dataset = MS2Dataset(args)\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=8,\n        pin_memory=True,\n        sampler=None,\n    )\n\n    depth_net = DepthNetwork(args)\n    depth_net.cuda()\n    depth_net.train()\n\n    warper = ImageWarping(args.batch_size, \n                          args.image_height, \n                          args.image_width)\n    warper.cuda()\n\n    loss_fn = PhotometricLoss()\n\n    optimizer = torch.optim.Adam(depth_net.model.parameters(), lr=args.learning_rate)\n    #checkpoint_dir = \"/mnt/nas/madhu/data/checkpoints/chapter_3/dino_unimatch_v2_full\"\n    #checkpoint_path = \"/mnt/nas/madhu/data/checkpoints/chapter_3/dino_unimatch_v2_smooth_0.1/depth_net_20.pth\"\n    depth_net.load_state_dict(torch.load(args.pretrained_ckpt), strict=False)\n\n    if not os.path.exists(args.checkpoint_dir):\n        os.makedirs(args.checkpoint_dir)\n\n    #for visualization\n    viz_dir = os.path.join(args.checkpoint_dir, \"viz\")\n    if not os.path.exists(viz_dir):\n        os.makedirs(viz_dir)\n        \n    # training loop\n    for epoch in range(args.num_epochs):\n        try:\n            train(epoch)\n            if epoch % 5 == 0:\n\n                torch.save(\n                    depth_net.state_dict(),\n                    os.path.join(args.checkpoint_dir, \"depth_net_{}.pth\".format(epoch)),\n                )\n\n        except Exception as e:\n            print(e)\n            breakpoint()\n"}
{"type": "source_file", "path": "models/dino_v1.py", "content": "# Dino extractor test\n\"\"\"\n    Code directly from: https://github.com/ShirAmir/dino-vit-features/blob/main/extractor.py\n    \n    Modifications:\n    - `preprocess` function takes numpy or torch images as well and\n        implicitly converts to PIL (instead of just path).\n\"\"\"\n\n# %%\nimport argparse\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nimport torch.nn.modules.utils as nn_utils\nimport math\nimport timm\nimport types\nfrom pathlib import Path\nfrom typing import Union, List, Tuple\nfrom PIL import Image\nimport numpy as np\nimport pathlib\nimport sys\n\n\n# Convert to numpy\ndef to_np(x, ret_type=float) -> np.ndarray:\n    \"\"\"\n        Converts 'x' to numpy object of `dtype` as 'ret_type'\n        \n        Parameters:\n        - x:    An object\n        \n        Returns:\n        - x_np:     A numpy array of dtype `ret_type`\n    \"\"\"\n    x_np: np.ndarray = None\n    if type(x) == torch.Tensor:\n        x_np = x.detach().cpu().numpy()\n    else:\n        x_np = np.array(x)\n    x_np = x_np.astype(ret_type)\n    return x_np\n\n\n# %%\nclass ViTExtractor:\n    \"\"\" This class facilitates extraction of features, descriptors, and saliency maps from a ViT.\n\n    We use the following notation in the documentation of the module's methods:\n    B - batch size\n    h - number of heads. usually takes place of the channel dimension in pytorch's convention BxCxHxW\n    p - patch size of the ViT. either 8 or 16.\n    t - number of tokens. equals the number of patches + 1, e.g. HW / p**2 + 1. Where H and W are the height and width\n    of the input image.\n    d - the embedding dimension in the ViT.\n    \"\"\"\n\n    def __init__(self, model_type: str = 'dino_vits8', stride: int = 8, model: nn.Module = None, device: str = 'cuda'):\n        \"\"\"\n        :param model_type: A string specifying the type of model to extract from.\n                          [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 |\n                          vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]\n        :param stride: stride of first convolution layer. small stride -> higher resolution.\n        :param model: Optional parameter. The nn.Module to extract from instead of creating a new one in ViTExtractor.\n                      should be compatible with model_type.\n        \"\"\"\n        self.model_type = model_type\n        self.device = device\n        if model is not None:\n            self.model = model\n        else:\n            self.model = ViTExtractor.create_model(model_type)\n\n        self.model = ViTExtractor.patch_vit_resolution(self.model, stride=stride)\n        self.model.eval()\n        self.model.to(self.device)\n        self.p = self.model.patch_embed.patch_size\n        self.stride = self.model.patch_embed.proj.stride\n\n        self.mean = (0.485, 0.456, 0.406) if \"dino\" in self.model_type else (0.5, 0.5, 0.5)\n        self.std = (0.229, 0.224, 0.225) if \"dino\" in self.model_type else (0.5, 0.5, 0.5)\n\n        self._feats = []\n        self.hook_handlers = []\n        self.load_size = None\n        self.num_patches = None\n\n    @staticmethod\n    def create_model(model_type: str) -> nn.Module:\n        \"\"\"\n        :param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |\n                           dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |\n                           vit_base_patch16_224]\n        :return: the model\n        \"\"\"\n        if 'dino' in model_type:\n            # model = torch.load(\"/ocean/projects/cis220039p/jkarhade/dino_out_pretrained/checkpoint0020.pth\")#\n            model = torch.hub.load('facebookresearch/dino:main', model_type)\n            # pretrained_dict=torch.load(\"/ocean/projects/cis220039p/jkarhade/dino_out_s16/checkpoint0080.pth\")['student']\n            # pretrained_dict = {key.replace(\"module.backbone.\", \"\"): value for key, value in pretrained_dict.items()}\n            # model.load_state_dict(pretrained_dict,strict=False)\n        else:  # model from timm -- load weights from timm to dino model (enables working on arbitrary size images).\n            temp_model = timm.create_model(model_type, pretrained=True)\n            model_type_dict = {\n                'vit_small_patch16_224': 'dino_vits16',\n                'vit_small_patch8_224': 'dino_vits8',\n                'vit_base_patch16_224': 'dino_vitb16',\n                'vit_base_patch8_224': 'dino_vitb8'\n            }\n            # model = torch.load(\"/ocean/projects/cis220039p/jkarhade/dino_out_pretrained/checkpoint0020.pth\")\n            model = torch.hub.load('facebookresearch/dino:main', model_type_dict[model_type])\n            temp_state_dict = temp_model.state_dict()\n            del temp_state_dict['head.weight']\n            del temp_state_dict['head.bias']\n            model.load_state_dict(temp_state_dict)\n            \n        # print(model)\n        return model\n\n    @staticmethod\n    def _fix_pos_enc(patch_size: int, stride_hw: Tuple[int, int]):\n        \"\"\"\n        Creates a method for position encoding interpolation.\n        :param patch_size: patch size of the model.\n        :param stride_hw: A tuple containing the new height and width stride respectively.\n        :return: the interpolation method\n        \"\"\"\n        def interpolate_pos_encoding(self, x: torch.Tensor, w: int, h: int) -> torch.Tensor:\n            npatch = x.shape[1] - 1\n            N = self.pos_embed.shape[1] - 1\n            if npatch == N and w == h:\n                return self.pos_embed\n            class_pos_embed = self.pos_embed[:, 0]\n            patch_pos_embed = self.pos_embed[:, 1:]\n            dim = x.shape[-1]\n            # compute number of tokens taking stride into account\n            w0 = 1 + (w - patch_size) // stride_hw[1]\n            h0 = 1 + (h - patch_size) // stride_hw[0]\n            assert (w0 * h0 == npatch), f\"\"\"got wrong grid size for {h}x{w} with patch_size {patch_size} and \n                                            stride {stride_hw} got {h0}x{w0}={h0 * w0} expecting {npatch}\"\"\"\n            # we add a small number to avoid floating point error in the interpolation\n            # see discussion at https://github.com/facebookresearch/dino/issues/8\n            w0, h0 = w0 + 0.1, h0 + 0.1\n            patch_pos_embed = nn.functional.interpolate(\n                patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n                scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n                mode='bicubic',\n                align_corners=False, recompute_scale_factor=False\n            )\n            assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n            patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n            return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\n        return interpolate_pos_encoding\n\n    @staticmethod\n    def patch_vit_resolution(model: nn.Module, stride: int) -> nn.Module:\n        \"\"\"\n        change resolution of model output by changing the stride of the patch extraction.\n        :param model: the model to change resolution for.\n        :param stride: the new stride parameter.\n        :return: the adjusted model\n        \"\"\"\n        patch_size = model.patch_embed.patch_size\n        if stride == patch_size:  # nothing to do\n            return model\n\n        stride = nn_utils._pair(stride)\n        assert all([(patch_size // s_) * s_ == patch_size for s_ in\n                    stride]), f'stride {stride} should divide patch_size {patch_size}'\n\n        # fix the stride\n        model.patch_embed.proj.stride = stride\n        # fix the positional encoding code\n        model.interpolate_pos_encoding = types.MethodType(ViTExtractor._fix_pos_enc(patch_size, stride), model)\n        return model\n\n    def preprocess(self, image: Union[str, Path, np.ndarray, torch.Tensor],\n                   load_size: Union[int, Tuple[int, int]] = None) -> Tuple[torch.Tensor, Image.Image]:\n        \"\"\"\n        Preprocesses an image before extraction.\n        :param image: Image (as path, ndarray or Tensor). \n                    If it's ndarray or Tensor, then range (and type) should be uint8.\n        :param load_size: optional. Size to resize image before the rest of preprocessing.\n        :return: a tuple containing:\n                    (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.\n                    (2) the pil image in relevant dimensions\n        \"\"\"\n        # Convert input to PIL image\n        pil_image = None\n        if type(image) == str or \\\n                issubclass(type(image), pathlib.Path):\n            pil_image = Image.open(image).convert('RGB')\n        else:\n            pil_image = Image.fromarray(to_np(image, np.uint8))\\\n                    .convert(\"RGB\")\n        if load_size is not None:\n            pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.LANCZOS)(pil_image)\n        prep = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=self.mean, std=self.std)\n        ])\n        prep_img = prep(pil_image)[None, ...]\n        return prep_img, pil_image\n\n    def _get_hook(self, facet: str, idx = None):\n        \"\"\"\n        generate a hook method for a specific block and facet.\n        \"\"\"\n        if facet in ['attn', 'token']:\n            def _hook(model, input, output):\n                self._feats.append(output)\n                if not idx is None:\n                    coarse_height = 1 + (self.image_height - self.p) // self.stride[0]\n                    coarse_width = 1 + (self.image_width - self.p) // self.stride[1]\n                    cls_token = output[:,:1,:]\n                    batch_size = output.shape[0]\n                    features = output[:,1:,:].permute(0,2,1).view(batch_size, -1, coarse_height, coarse_width)\n                    coarse_features = features[:,:, ::2,::2]\n                    coarse_features = coarse_features.reshape(batch_size, 384, -1).permute(0,2,1)\n                    re_output = torch.cat((cls_token, coarse_features), dim=1)\n                    return re_output \n                    \n            return _hook\n\n        if facet == 'query':\n            facet_idx = 0\n        elif facet == 'key':\n            facet_idx = 1\n        elif facet == 'value':\n            facet_idx = 2\n        else:\n            raise TypeError(f\"{facet} is not a supported facet.\")\n\n        def _inner_hook(module, input, output):\n            input = input[0]\n            B, N, C = input.shape\n            qkv = module.qkv(input).reshape(B, N, 3, module.num_heads, C // module.num_heads).permute(2, 0, 3, 1, 4)\n            self._feats.append(qkv[facet_idx]) #Bxhxtxd\n        return _inner_hook\n\n    def _register_hooks(self, layers: List[int], facet: str) -> None:\n        \"\"\"\n        register hook to extract features.\n        :param layers: layers from which to extract features.\n        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token' | 'attn']\n        \"\"\"\n\n        for block_idx, block in enumerate(self.model.blocks):\n            if block_idx in layers:\n\n                idx = None\n                if block_idx ==layers[0]:\n                    idx = block_idx\n                if facet == 'token':\n                    self.hook_handlers.append(block.register_forward_hook(self._get_hook(facet, idx)))\n                elif facet == 'attn':\n                    self.hook_handlers.append(block.attn.attn_drop.register_forward_hook(self._get_hook(facet)))\n                elif facet in ['key', 'query', 'value']:\n                    self.hook_handlers.append(block.attn.register_forward_hook(self._get_hook(facet)))\n                else:\n                    raise TypeError(f\"{facet} is not a supported facet.\")\n\n    def _unregister_hooks(self) -> None:\n        \"\"\"\n        unregisters the hooks. should be called after feature extraction.\n        \"\"\"\n        for handle in self.hook_handlers:\n            handle.remove()\n        self.hook_handlers = []\n\n    def _extract_features(self, batch: torch.Tensor, layers: List[int] = 11, facet: str = 'key') -> List[torch.Tensor]:\n        \"\"\"\n        extract features from the model\n        :param batch: batch to extract features for. Has shape BxCxHxW.\n        :param layers: layer to extract. A number between 0 to 11.\n        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token' | 'attn']\n        :return : tensor of features.\n                  if facet is 'key' | 'query' | 'value' has shape Bxhxtxd\n                  if facet is 'attn' has shape Bxhxtxt\n                  if facet is 'token' has shape Bxtxd\n        \"\"\"\n        B, C, H, W = batch.shape\n        self.image_height = H\n        self.image_width = W\n        self._feats = []\n        self._register_hooks(layers, facet)\n        _ = self.model(batch)\n        self._unregister_hooks()\n        self.load_size = (H, W)\n        self.num_patches = (1 + (H - self.p) // self.stride[0], 1 + (W - self.p) // self.stride[1])\n        return self._feats\n\n    def _log_bin(self, x: torch.Tensor, hierarchy: int = 2) -> torch.Tensor:\n        \"\"\"\n        create a log-binned descriptor.\n        :param x: tensor of features. Has shape Bxhxtxd.\n        :param hierarchy: how many bin hierarchies to use.\n        \"\"\"\n        B = x.shape[0]\n        num_bins = 1 + 8 * hierarchy\n\n        bin_x = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1)  # Bx(t-1)x(dxh)\n        bin_x = bin_x.permute(0, 2, 1)\n        bin_x = bin_x.reshape(B, bin_x.shape[1], self.num_patches[0], self.num_patches[1])\n        # Bx(dxh)xnum_patches[0]xnum_patches[1]\n        sub_desc_dim = bin_x.shape[1]\n\n        avg_pools = []\n        # compute bins of all sizes for all spatial locations.\n        for k in range(0, hierarchy):\n            # avg pooling with kernel 3**kx3**k\n            win_size = 3 ** k\n            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)\n            avg_pools.append(avg_pool(bin_x))\n\n        bin_x = torch.zeros((B, sub_desc_dim * num_bins, self.num_patches[0], self.num_patches[1])).to(self.device)\n        for y in range(self.num_patches[0]):\n            for x in range(self.num_patches[1]):\n                part_idx = 0\n                # fill all bins for a spatial location (y, x)\n                for k in range(0, hierarchy):\n                    kernel_size = 3 ** k\n                    for i in range(y - kernel_size, y + kernel_size + 1, kernel_size):\n                        for j in range(x - kernel_size, x + kernel_size + 1, kernel_size):\n                            if i == y and j == x and k != 0:\n                                continue\n                            if 0 <= i < self.num_patches[0] and 0 <= j < self.num_patches[1]:\n                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][\n                                                                                                           :, :, i, j]\n                            else:  # handle padding in a more delicate way than zero padding\n                                temp_i = max(0, min(i, self.num_patches[0] - 1))\n                                temp_j = max(0, min(j, self.num_patches[1] - 1))\n                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][\n                                                                                                           :, :, temp_i,\n                                                                                                           temp_j]\n                            part_idx += 1\n        bin_x = bin_x.flatten(start_dim=-2, end_dim=-1).permute(0, 2, 1).unsqueeze(dim=1)\n        # Bx1x(t-1)x(dxh)\n        return bin_x\n\n    def extract_descriptors(self, batch: torch.Tensor, layer: int = 11, facet: str = 'key',\n                            bin: bool = False, include_cls: bool = False) -> torch.Tensor:\n        \"\"\"\n        extract descriptors from the model\n        :param batch: batch to extract descriptors for. Has shape BxCxHxW.\n        :param layers: layer to extract. A number between 0 to 11.\n        :param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token']\n        :param bin: apply log binning to the descriptor. default is False.\n        :return: tensor of descriptors. Bx1xtxd' where d' is the dimension of the descriptors.\n        \"\"\"\n        assert facet in ['key', 'query', 'value', 'token'], f\"\"\"{facet} is not a supported facet for descriptors. \n                                                             choose from ['key' | 'query' | 'value' | 'token'] \"\"\"\n        self._extract_features(batch, [layer], facet)\n        x = self._feats[0]\n        if facet == 'token':\n            x.unsqueeze_(dim=1) #Bx1xtxd\n        if not include_cls:\n            x = x[:, :, 1:, :]  # remove cls token\n        else:\n            assert not bin, \"bin = True and include_cls = True are not supported together, set one of them False.\"\n        if not bin:\n            desc = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1).unsqueeze(dim=1)  # Bx1xtx(dxh)\n        else:\n            desc = self._log_bin(x)\n        return desc\n\n    def extract_saliency_maps(self, batch: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer\n        in of the CLS token. All values are then normalized to range between 0 and 1.\n        :param batch: batch to extract saliency maps for. Has shape BxCxHxW.\n        :return: a tensor of saliency maps. has shape Bxt-1\n        \"\"\"\n        assert self.model_type == \"dino_vits8\", f\"saliency maps are supported only for dino_vits model_type.\"\n        self._extract_features(batch, [11], 'attn')\n        head_idxs = [0, 2, 4, 5]\n        curr_feats = self._feats[0] #Bxhxtxt\n        cls_attn_map = curr_feats[:, head_idxs, 0, 1:].mean(dim=1) #Bx(t-1)\n        temp_mins, temp_maxs = cls_attn_map.min(dim=1)[0], cls_attn_map.max(dim=1)[0]\n        cls_attn_maps = (cls_attn_map - temp_mins) / (temp_maxs - temp_mins)  # normalize to range [0,1]\n        return cls_attn_maps\n\n\"\"\" taken from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\"\"\"\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n# %%\nif __name__ == \"__main__\" and (not \"ipykernel\" in sys.argv[0]):\n    class BatterFormatter(argparse.ArgumentDefaultsHelpFormatter, \n                argparse.MetavarTypeHelpFormatter):\n        pass\n    parser = argparse.ArgumentParser(description='Facilitate ViT Descriptor extraction.', formatter_class=BatterFormatter)\n    parser.add_argument('--image_path', type=str, required=True, \n            help='path of the extracted image.')\n    parser.add_argument('--output_path', type=str, required=True, \n            help='path to file containing extracted descriptors.')\n    parser.add_argument('--load_size', default=224, type=int, \n            help='load size of the input image.')\n    parser.add_argument('--stride', default=4, type=int, \n            help=\"stride of first convolution layer. small stride \"\\\n                \"-> higher resolution.\")\n    parser.add_argument('--model_type', default='dino_vits8', type=str,\n                        help=\"\"\"type of model to extract. \n                        Choose from [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 | \n                        vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]\"\"\")\n    parser.add_argument('--facet', default='key', type=str, help=\"\"\"facet to create descriptors from. \n                                                                    options: ['key' | 'query' | 'value' | 'token']\"\"\")\n    parser.add_argument('--layer', default=11, type=int, help=\"layer to create descriptors from.\")\n    parser.add_argument('--bin', default='False', type=str2bool, help=\"create a binned descriptor if True.\")\n\n    args, unknown_args = parser.parse_known_args()\n    print(f\"Arguments: {args}\")\n    print(f\"Unknown arguments: {unknown_args}\")\n\n    with torch.no_grad():\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        extractor = ViTExtractor(args.model_type, args.stride, device=device)\n        image_batch, image_pil = extractor.preprocess(args.image_path, args.load_size)\n        print(f\"Image {args.image_path} is preprocessed to tensor of size {image_batch.shape}.\")\n        descriptors = extractor.extract_descriptors(image_batch.to(device), args.layer, args.facet, args.bin)\n        print(f\"Descriptors are of size: {descriptors.shape}\")\n        torch.save(descriptors, args.output_path)\n        print(f\"Descriptors saved to: {args.output_path}\")\n\n\n# %%\n# Experiments\n\n\n\n# %%\n# Jay's experiment\ndef _jay_internal_test():\n    \"\"\"\n        So that this script can load as a module, Jay's test code has\n        been moved into this function. Just copy it out and remove a\n        single indent to revert.\n    \"\"\"\n    # %%\n    _context = torch.no_grad()\n    _context.__enter__()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    extractor = ViTExtractor(\"dino_vits8\", 4, device=device)\n    img = Image.open(\"/home/jay/Documents/vl-vpr/images/dog.jpeg\")\n\n    # %%\n    img_np = np.array(img)\n\n    # %%\n    img_pt, img_pil = extractor.preprocess(img_np, 224)\n\n\n    # %%\n    _context.__exit__(None, None, None)\n\n\n# %%"}
{"type": "source_file", "path": "evaluation/junk/gt_depth_stats.py", "content": "import numpy as np\nimport pdb\nimport tqdm\nimport matplotlib.pyplot as plt\n\n#gt_depth_path = '/mnt/nas/madhu/data/KITTI/gt_depths/gt_depths.npz'\n#depth_data = np.load(gt_depth_path, fix_imports=True, encoding='latin1',allow_pickle=True)[\"data\"]\n\ngt_depth_data = '/hdd1/madhu/data/robotcar/2014-12-16-18-44-24/depth_evaluation/gt_depths.npy'\n#gt_depth_data = '/hdd1/madhu/data/ms2/gt_test_depths_filtered.npy'\n\ndepth_data = np.load(gt_depth_data,allow_pickle=True)\n\nhist_bins = np.arange(1, 100, 2)\nbins_data = []\nfor depth_map in tqdm.tqdm(depth_data,total = len(depth_data)):\n    hist_data = np.histogram(depth_map, bins=hist_bins)[0]\n    bins_data.append(hist_data)\n\nbins_data = np.array(bins_data)\nbins_data = bins_data.sum(axis=0)\n\nfig, ax = plt.subplots()\n\nax.bar(hist_bins[:-1], bins_data, width = 1.0,align='edge')\nax.grid()\n#plt.bar(hist_bins[:-1], bins_data, width = 0.5)\nplt.xlabel('Depth buckets (m)',fontsize = 18)\nplt.ylabel('Number of points',fontsize = 18)\nplt.xticks(fontsize = 18)\nplt.yticks(fontsize = 18)\nplt.tight_layout()\nplt.savefig('MS2_num_of_points_gtdepth_hist.png')\n\n'''\n\n\n\n\n"}
{"type": "source_file", "path": "models/utils.py", "content": "import torch\nimport numpy as np\nfrom torch import nn\n\nclass BackprojectDepth(nn.Module):\n    \"\"\"Layer to transform a depth image into a point cloud\"\"\"\n\n    def __init__(self, batch_size, height, width):\n        super(BackprojectDepth, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n\n        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing=\"xy\")\n        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n        self.id_coords = nn.Parameter(\n            torch.from_numpy(self.id_coords), requires_grad=False\n        )\n\n        self.ones = nn.Parameter(\n            torch.ones(self.batch_size, 1, self.height * self.width),\n            requires_grad=False,\n        )\n\n        self.pix_coords = torch.unsqueeze(\n            torch.stack([self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0\n        )\n        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n        self.pix_coords = nn.Parameter(\n            torch.cat([self.pix_coords, self.ones], 1), requires_grad=False\n        )\n\n    def forward(self, depth, camera_matrix):\n\n        inv_K = torch.inverse(camera_matrix)\n        cam_points = torch.matmul(inv_K, self.pix_coords)\n        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n        cam_points = torch.cat([cam_points, self.ones], 1)\n\n        return cam_points\n\n\nclass Project3D(nn.Module):\n    \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\"\"\"\n\n    def __init__(self, batch_size, height, width, eps=1e-7):\n        super(Project3D, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.eps = eps\n\n    def set_original_pix_coords(self, original_pix_coords):\n        \n        self.original_pix_coords = original_pix_coords\n        \n    def get_original_pix_coords(self):\n        \n        return self.original_pix_coords\n\n    def forward(self, points, K, T):\n\n        cam_points = torch.matmul(T, points)\n        cam_points = torch.matmul(K, cam_points[:, :3, :])\n        pix_coords = cam_points[:, :2, :] / (\n            cam_points[:, 2, :].unsqueeze(1) + self.eps\n        )\n        self.set_original_pix_coords(pix_coords)\n        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n        pix_coords = pix_coords.permute(0, 2, 3, 1)\n        pix_coords[..., 0] /= self.width - 1\n        pix_coords[..., 1] /= self.height - 1\n        pix_coords = (pix_coords - 0.5) * 2\n\n        return pix_coords"}
{"type": "source_file", "path": "models/masked_stereo.py", "content": "import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom .unimatch.unimatch.transformer import FeatureTransformer\r\nfrom .unimatch.unimatch.matching import global_correlation_softmax_stereo, local_correlation_softmax_stereo\r\nfrom .unimatch.unimatch.attention import SelfAttnPropagation\r\nfrom .unimatch.unimatch.geometry import flow_warp as disp_warp\r\nfrom .unimatch.unimatch.utils import feature_add_position, upsample_flow_with_mask\r\n\r\nclass DTD(nn.Module):\r\n    def __init__(self,\r\n                 num_scales=1,\r\n                 feature_channels=128,\r\n                 upsample_factor=4,\r\n                 num_head=1,\r\n                 ffn_dim_expansion=4,\r\n                 num_transformer_layers=6,\r\n                 input_dim = 384,\r\n                 output_dim = 128,\r\n                 mask_thr = 0.2\r\n                 ):\r\n        super(DTD, self).__init__()\r\n        self.feature_channels = feature_channels\r\n        self.num_scales = num_scales\r\n        self.upsample_factor = upsample_factor\r\n        self.mask_thr = mask_thr\r\n        # CNN\r\n        self.backbone = None\r\n        # Transformer\r\n        self.transformer = FeatureTransformer(num_layers=num_transformer_layers,\r\n                                              d_model=feature_channels,\r\n                                              nhead=num_head,\r\n                                              ffn_dim_expansion=ffn_dim_expansion,\r\n                                              )\r\n\r\n        # propagation with self-attn\r\n        self.feature_flow_attn = SelfAttnPropagation(in_channels=feature_channels)\r\n\r\n        self.projector = nn.Sequential(nn.Conv2d(input_dim, 256, 1),\r\n                                        nn.ReLU(inplace=True),\r\n                                        nn.Conv2d(256, output_dim, 1))\r\n\r\n        # convex upsampling simiar to RAFT\r\n        self.upsampler = nn.Sequential(nn.Conv2d(2 + feature_channels, 256, 3, 1, 1),\r\n                                        nn.ReLU(inplace=True),\r\n                                        nn.Conv2d(256, upsample_factor ** 2 * 9, 1, 1, 0))\r\n\r\n    \r\n    def extract_feature(self, img0, img1):\r\n        # this should be from DINO\r\n        return None, None\r\n\r\n    def upsample_disp(self, disp, feature, bilinear=False, upsample_factor=8,\r\n                      is_depth=False):\r\n        if bilinear:\r\n            multiplier = 1 if is_depth else upsample_factor\r\n            up_disp = F.interpolate(disp, scale_factor=upsample_factor,\r\n                                    mode='bilinear', align_corners=True) * multiplier\r\n        else:\r\n            concat = torch.cat((disp, feature), dim=1)\r\n            mask = self.upsampler(concat)\r\n            up_disp = upsample_flow_with_mask(disp, mask, upsample_factor=self.upsample_factor,\r\n                                              is_depth=is_depth)\r\n\r\n        return up_disp\r\n\r\n    def get_mask(self, features):\r\n\r\n        '''\r\n        This function is compute the mask the features that are not unique enought to have \r\n        the distance grather than a threshold\r\n\r\n        Args:\r\n            feature: the feature map of the image\r\n            thr: the threshold of the distance\r\n\r\n        Returns:\r\n            mask: the mask of the feature map\r\n    \r\n        '''\r\n        normlize_features = torch.nn.functional.normalize(features, dim=1)\r\n        re_feat = normlize_features.permute(0, 2, 3, 1).contiguous()\r\n        re_feat = re_feat.view(re_feat.shape[0], -1, re_feat.shape[3])\r\n        similarity = torch.bmm(re_feat, re_feat.permute(0, 2, 1).contiguous())\r\n        nearest_nn_inds = torch.topk(similarity, k=2, dim=-1, largest=True)[1][:, :, 1]\r\n        nearest_nn = torch.gather(re_feat, dim=1, index=nearest_nn_inds.unsqueeze(-1).expand(-1, -1, re_feat.shape[-1]))\r\n        distance = torch.norm(re_feat - nearest_nn, dim=-1)\r\n        mask = torch.where(distance > self.mask_thr, torch.ones_like(distance), torch.zeros_like(distance))\r\n        mask = mask.view(features.shape[0],1, features.shape[2], features.shape[3])\r\n\r\n        return mask, distance\r\n\r\n\r\n    def forward(self, img0, img1,\r\n                attn_type=None,\r\n                attn_splits_list=None,\r\n                corr_radius_list=None,\r\n                prop_radius_list=None,\r\n                ):\r\n\r\n\r\n        results_dict = {}\r\n        pred_disparities = []\r\n        distances = []\r\n        masks = []\r\n        disp = None\r\n        with torch.no_grad():\r\n            # list of features, resolution low to high\r\n            feature0_list, feature1_list = self.extract_feature(img0, img1)  # list of features\r\n\r\n        assert len(attn_splits_list) == len(corr_radius_list) == len(prop_radius_list) == self.num_scales\r\n        for scale_idx in range(self.num_scales):\r\n            feature0, feature1 = feature0_list[scale_idx].detach(), feature1_list[scale_idx].detach()\r\n            feature0 = self.projector(feature0) # to downsample the feature\r\n            feature1 = self.projector(feature1) # to downsample the feature\r\n            bad_pixel_mask, nearest_neighbour_distances = self.get_mask(feature0)\r\n            distances.append(nearest_neighbour_distances)\r\n            masks.append(bad_pixel_mask)\r\n\r\n            upsample_factor = self.upsample_factor * (2 ** (self.num_scales - 1 - scale_idx))\r\n            if scale_idx > 0:\r\n                disp = F.interpolate(disp, scale_factor=2, mode='bilinear', align_corners=True) * 2\r\n            if disp is not None:\r\n                disp = disp.detach()\r\n                zeros = torch.zeros_like(disp)  # [B, 1, H, W]\r\n                # NOTE: reverse disp, disparity is positive\r\n                displace = torch.cat((-disp, zeros), dim=1)  # [B, 2, H, W]\r\n                feature1 = disp_warp(feature1, displace)  # [B, C, H, W]\r\n\r\n\r\n            attn_splits = attn_splits_list[scale_idx]\r\n            corr_radius = corr_radius_list[scale_idx]\r\n            prop_radius = prop_radius_list[scale_idx]\r\n\r\n            # add position to features\r\n            feature0, feature1 = feature_add_position(feature0, feature1, attn_splits, self.feature_channels)\r\n\r\n            # here they do global attention, can we do local attention?            \r\n            # Transformer\r\n            feature0, feature1 = self.transformer(feature0, feature1,\r\n                                                  attn_type=attn_type,\r\n                                                  attn_num_splits=attn_splits,\r\n                                                  )\r\n\r\n                      \r\n            if corr_radius == -1:  # global matching\r\n                disp_pred = global_correlation_softmax_stereo(feature0, feature1)[0]\r\n            else: # local matching\r\n                disp_pred = local_correlation_softmax_stereo(feature0, feature1, corr_radius)[0]\r\n\r\n                \r\n            disp_pred = disp_pred * bad_pixel_mask\r\n            # flow or residual flow\r\n            disp = disp + disp_pred if disp is not None else disp_pred\r\n            disp = disp.clamp(min=0)  # positive disparity\r\n            # upsample to the original resolution for supervison at training time only\r\n            if self.training:\r\n                disp_bilinear = self.upsample_disp(disp, None, \r\n                                                   bilinear=True,\r\n                                                    upsample_factor=upsample_factor,\r\n                                                   is_depth=False)# this last flag is redundant but added to use unimatch code\r\n                \r\n                pred_disparities.append(disp_bilinear)\r\n\r\n            disp = self.feature_flow_attn(feature0, disp.detach(),\r\n                                          local_window_attn=prop_radius > 0,\r\n                                          local_window_radius=prop_radius,\r\n                                          )\r\n            # bilinear exclude the last one and for the last one use RAFT- style upsampling\r\n            if self.training and scale_idx < self.num_scales - 1:\r\n                disp_up = self.upsample_disp(disp, feature0, bilinear=True,\r\n                                             upsample_factor=upsample_factor,\r\n                                             is_depth=False)\r\n                pred_disparities.append(disp_up)\r\n\r\n            if scale_idx == self.num_scales - 1:\r\n                disp_pad = torch.cat((-disp, torch.zeros_like(disp)), dim=1)  # [B, 2, H, W]\r\n                disp_up_pad = self.upsample_disp(disp_pad, feature0)\r\n                disp_up = -disp_up_pad[:, :1]  # [B, 1, H, W]\r\n                pred_disparities.append(disp_up)\r\n                \r\n        for i in range(len(pred_disparities)):\r\n            pred_disparities[i] = pred_disparities[i].squeeze(1)  # [B, H, W]\r\n        results_dict.update({'pred_disparities': pred_disparities})\r\n        results_dict.update({'nn_distance': distances})\r\n        results_dict.update({'bad_pixel_mask': masks})\r\n\r\n        return results_dict\r\n"}
