{"repo_info": {"repo_name": "SolidGPT", "repo_owner": "AI-Citizen", "repo_url": "https://github.com/AI-Citizen/SolidGPT"}}
{"type": "source_file", "path": "run_api.py", "content": "import uvicorn\nimport threading\nimport solidgpt.src.api.api\nimport celery.fixups\nimport kombu.transport.sqlalchemy\nimport celery.fixups.django\nimport celery.app.amqp\nimport celery.backends\nimport celery.backends.redis\n\nfrom celery import Celery\n\n# PyInstaller friendly imports --start--\nimport celery.app.amqp\nimport celery.app.log\nimport celery.worker.autoscale\nimport celery.worker.components\nimport celery.bin\nimport celery.utils\nimport celery.utils.dispatch\nimport celery.contrib.testing\nimport celery.utils.static\nimport celery.concurrency.prefork\nimport celery.app.events\nimport celery.events.state\nimport celery.app.control\nimport celery.backends.redis\nimport celery.backends\nimport celery.backends.database\nimport celery.worker\nimport celery.worker.consumer\nimport celery.app\nimport celery.loaders\nimport celery.security\nimport celery.fixups\nimport celery.concurrency\nimport celery.concurrency.thread\nimport celery.events\nimport celery.contrib\nimport celery.apps\nimport celery\nimport celery.fixups\nimport celery.fixups.django\nimport celery.apps.worker\nimport celery.worker.strategy\nimport kombu.transport.redis\nimport sqlalchemy.sql.default_comparator               \nimport sqlalchemy.ext.baked\nimport subprocess\nimport platform\n\n\nif __name__ == \"__main__\":\n    # Define the Celery command as a list of strings\n    celery_command = []\n    if platform.system() == 'Windows':\n        celery_command = [\n            'celery',\n            '-A',\n            'solidgpt.src.api.celery_tasks',\n            'worker',\n            '--loglevel=info',\n            '-P',\n            'eventlet'\n        ]\n    else:\n        celery_command = [\n            'celery',\n            '-A',\n            'solidgpt.src.api.celery_tasks',\n            'worker',\n            '--loglevel=info'\n        ]\n\n    # Start the Celery worker process in the background\n    #celery_process = subprocess.Popen(celery_command)\n    # Run your UVicorn server\n    uvicorn.run(\"solidgpt.src.api.api:app\", port=8000)"}
{"type": "source_file", "path": "solidgpt/src/manager/autogenmanager.py", "content": "import logging\n\nimport autogen\nfrom autogen import oai\nimport openai\nfrom solidgpt.src.configuration.configreader import ConfigReader\nfrom typing import Callable, Dict, Optional, Union\n\nfrom solidgpt.src.manager.promptresource import DEFAULT_SYSTEM_MESSAGE, ASSISTANT_SYSTEM_MESSAGE\n\n\ndef colored(x, *args, **kwargs):\n    return x\n\n\nclass SolidUserProxyAgent(autogen.UserProxyAgent):\n\n    manager = None\n    callback_map = {\n\n    }\n\n    def __init__(\n            self,\n            name: str,\n            is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n            max_consecutive_auto_reply: Optional[int] = None,\n            human_input_mode: Optional[str] = \"ALWAYS\",\n            function_map: Optional[Dict[str, Callable]] = None,\n            code_execution_config: Optional[Union[Dict, bool]] = None,\n            default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n            llm_config: Optional[Union[Dict, bool]] = False,\n            system_message: Optional[str] = \"\",\n    ):\n        super().__init__(\n            name=name,\n            system_message=system_message,\n            is_termination_msg=is_termination_msg,\n            max_consecutive_auto_reply=max_consecutive_auto_reply,\n            human_input_mode=human_input_mode,\n            function_map=function_map,\n            code_execution_config=code_execution_config,\n            llm_config=llm_config,\n            default_auto_reply=default_auto_reply,\n        )\n\n    def _print_received_message(self, message: Union[Dict, str], sender):\n        # print the message received\n        self.manager.add_message(sender.name, \"(to\", f\"{self.name}):\\n\")\n        if message.get(\"role\") == \"function\":\n            func_print = f\"***** Response from calling function \\\"{message['name']}\\\" *****\"\n            self.manager.add_message(func_print)\n            self.manager.add_message(message[\"content\"])\n            self.manager.add_message(\"*\" * len(func_print))\n        else:\n            content = message.get(\"content\")\n            if content is not None:\n                if \"context\" in message:\n                    content = oai.ChatCompletion.instantiate(\n                        content,\n                        message[\"context\"],\n                        self.llm_config and self.llm_config.get(\"allow_format_str_template\", False),\n                    )\n                self.manager.add_message(content)\n            if \"function_call\" in message:\n                func_print = f\"***** Suggested function Call: {message['function_call'].get('name', '(No function name found)')} *****\"\n                self.manager.add_message(func_print)\n                self.manager.add_message(\"Arguments: \")\n                self.manager.add_message(message[\"function_call\"].get(\"arguments\", \"(No arguments found)\"))\n                self.manager.add_message(\"*\" * len(func_print))\n        self.manager.add_message(\"\")\n        self.manager.add_message(\"-\" * 80)\n\n    def get_human_input(self, prompt: str) -> str:\n        reply = \"\"\n        # get reply from frontend\n        msg = self.manager.retrieve_message()\n        if self.callback_map.get(\"autogen_update_result_callback\"):\n            self.callback_map.get(\"autogen_update_result_callback\")(msg)\n        # display the chat\n        logging.info(msg)\n\n        if self.callback_map.get(\"autogen_message_input_callback\"):\n            reply = self.callback_map.get(\"autogen_message_input_callback\")()\n        else:\n            reply = input()\n\n        return reply\n\n\nclass SolidAssistantAgent(autogen.AssistantAgent):\n\n    manager = None\n\n    def __init__(\n            self,\n            name: str,\n            system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,\n            llm_config: Optional[Union[Dict, bool]] = None,\n            is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n            max_consecutive_auto_reply: Optional[int] = None,\n            human_input_mode: Optional[str] = \"NEVER\",\n            code_execution_config: Optional[Union[Dict, bool]] = False,\n            **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            system_message=system_message,\n            is_termination_msg=is_termination_msg,\n            max_consecutive_auto_reply=max_consecutive_auto_reply,\n            human_input_mode=human_input_mode,\n            code_execution_config=code_execution_config,\n            llm_config=llm_config,\n            **kwargs,\n        )\n\n    def _print_received_message(self, message: Union[Dict, str], sender):\n        # print the message received\n        self.manager.add_message(sender.name, \"(to\", f\"{self.name}):\\n\")\n        if message.get(\"role\") == \"function\":\n            func_print = f\"***** Response from calling function \\\"{message['name']}\\\" *****\"\n            self.manager.add_message(func_print)\n            self.manager.add_message(message[\"content\"])\n            self.manager.add_message(\"*\" * len(func_print))\n        else:\n            content = message.get(\"content\")\n            if content is not None:\n                if \"context\" in message:\n                    content = oai.ChatCompletion.instantiate(\n                        content,\n                        message[\"context\"],\n                        self.llm_config and self.llm_config.get(\"allow_format_str_template\", False),\n                    )\n                self.manager.add_message(content)\n            if \"function_call\" in message:\n                func_print = f\"***** Suggested function Call: {message['function_call'].get('name', '(No function name found)')} *****\"\n                self.manager.add_message(func_print)\n                self.manager.add_message(\"Arguments: \")\n                self.manager.add_message(message[\"function_call\"].get(\"arguments\", \"(No arguments found)\"))\n                self.manager.add_message(\"*\" * len(func_print))\n        self.manager.add_message(\"\")\n        self.manager.add_message(\"-\" * 80)\n\n    def get_human_input(self, prompt: str) -> str:\n        print(prompt)\n        reply = \"\"\n\n        return reply\n\n\nclass AutoGenManager:\n    cumulative_message = \"\"\n\n    def __init__(self, if_show_reply=False):\n        # read api key from config file\n        global_openai_key = ConfigReader().get_property(\"openai_api_key\")\n        if global_openai_key is not None and global_openai_key != \"\":\n            openai.api_key = global_openai_key\n        self.__default_model = ConfigReader().get_property(\"openai_model\")\n        self.config_list = [{\"model\": self.__default_model, \"api_key\": openai.api_key}]\n        self.if_show_reply = if_show_reply\n        self.planner = None\n        self.planner_user = None\n        self.assistant = None\n        self.user_proxy = None\n\n    def run(self, requirement, relatived_code):\n        self.construct_agents(relatived_code)\n        self.user_proxy.initiate_chat(\n            self.assistant,\n            message=requirement,\n        )\n\n    @staticmethod\n    def get_customized_assistant_agent(name: str,\n                                       system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,\n                                       llm_config: Optional[Union[Dict, bool]] = None,\n                                       is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n                                       max_consecutive_auto_reply: Optional[int] = None,\n                                       human_input_mode: Optional[str] = \"NEVER\",\n                                       code_execution_config: Optional[Union[Dict, bool]] = False,\n                                       call_back=None,\n                                       **kwargs):\n        return SolidAssistantAgent(\n            name=name,\n            system_message=system_message,\n            is_termination_msg=is_termination_msg,\n            max_consecutive_auto_reply=max_consecutive_auto_reply,\n            human_input_mode=human_input_mode,\n            code_execution_config=code_execution_config,\n            call_back=call_back,\n            llm_config=llm_config,\n            **kwargs)\n\n    @staticmethod\n    def get_customized_user_proxy_agent(name: str,\n                                        is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n                                        max_consecutive_auto_reply: Optional[int] = None,\n                                        human_input_mode: Optional[str] = \"ALWAYS\",\n                                        function_map: Optional[Dict[str, Callable]] = None,\n                                        code_execution_config: Optional[Union[Dict, bool]] = None,\n                                        default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n                                        llm_config: Optional[Union[Dict, bool]] = False,\n                                        system_message: Optional[str] = \"\",\n                                        ):\n        return SolidUserProxyAgent(\n            name,\n            system_message=system_message,\n            is_termination_msg=is_termination_msg,\n            max_consecutive_auto_reply=max_consecutive_auto_reply,\n            human_input_mode=human_input_mode,\n            function_map=function_map,\n            code_execution_config=code_execution_config,\n            llm_config=llm_config,\n            default_auto_reply=default_auto_reply,\n            )\n\n    def construct_agents(self, relatived_code):\n        self.planner = self.generate_default_planner()\n        self.planner_user = self.generate_default_planner_user()\n        self.assistant = self.generate_default_assistant(relatived_code)\n        self.user_proxy = self.generate_default_user_proxy()\n        self.planner.manager = self\n        self.planner_user.manager = self\n        self.assistant.manager = self\n        self.user_proxy.manager = self\n        return\n\n    def add_message(self, *args):\n        # Joining all arguments with a space after converting each to a string\n        messages = ' '.join(map(str, args))\n        self.cumulative_message += messages + \"\\n\"\n\n    def retrieve_message(self):\n        msg = self.cumulative_message\n        self.cumulative_message = \"\"\n        return msg\n\n    def generate_default_planner(self):\n        # todo: update callback function\n        planner = SolidAssistantAgent(\n            name=\"Planner\",\n            llm_config={\"config_list\": self.config_list},\n            # the default system message of the AssistantAgent is overwritten here\n            system_message=DEFAULT_SYSTEM_MESSAGE)\n        return planner\n\n    def generate_default_planner_user(self):\n        # todo: update callback function\n        planner_user = SolidUserProxyAgent(\n            name=\"Your_Proxy\",\n            max_consecutive_auto_reply=0,  # terminate without auto-reply\n            human_input_mode=\"NEVER\",\n        )\n        return planner_user\n\n    def ask_planner(self, message):\n        self.planner_user.initiate_chat(self.planner, message=message)\n        self.planner_msg = self.planner_user.last_message()[\"content\"]\n        # return the last message received from the planner\n        return self.planner_user.last_message()[\"content\"]\n\n    def generate_default_assistant(self, relatived_code: str):\n        # todo: update callback function\n        assistant = SolidAssistantAgent(\n            name=\"SolidGPT\",\n            system_message=ASSISTANT_SYSTEM_MESSAGE + f\"\"\"Relatived code as follow: {relatived_code}\"\"\",\n            llm_config={\n                \"temperature\": 0,\n                \"request_timeout\": 600,\n                \"seed\": 42,\n                \"model\": self.__default_model,\n                \"config_list\": self.config_list,\n                \"functions\": [\n                    {\n                        \"name\": \"ask_planner\",\n                        \"description\": \"ask planner to: 1. get a plan for finishing a task, 2. verify the execution result of the plan and potentially suggest new plan.\",\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"message\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"question to ask planner. Make sure the question include enough context, such as the code and the execution result. The planner does not know the conversation between you and the user, unless you share the conversation with the planner.\",\n                                },\n                            },\n                            \"required\": [\"message\"],\n                        },\n                    },\n                ],\n            }\n        )\n        return assistant\n\n    def generate_default_user_proxy(self):\n        # todo: update callback function\n        user_proxy = SolidUserProxyAgent(\n            name=\"Your_Proxy\",\n            human_input_mode=\"ALWAYS\",\n            max_consecutive_auto_reply=10,\n            is_termination_msg=lambda x: \"content\" in x and x[\"content\"] is not None and x[\"content\"].rstrip().endswith(\n                \"TERMINATE\"),\n            code_execution_config={\"work_dir\": \"planning\"},\n            function_map={\"ask_planner\": self.ask_planner},\n        )\n        return user_proxy\n\n"}
{"type": "source_file", "path": "solidgpt/src/configuration/configreader.py", "content": "import yaml\nimport os\n\nfrom solidgpt.definitions import ROOT_DIR\n\nclass ConfigReader:\n    # gpt-4-1106-preview, gpt-3.5-turbo-16k\n    config_map = { \"openai_model\": \"gpt-3.5-turbo-16k\" }\n    def __init__(self):\n        # self.file_path = os.path.join(ROOT_DIR, \"src\", \"configuration\", \"Configuration.yaml\")\n        # config_path = \"configuration.yaml\"  # This should match the expected path in your code.\n        pass\n\n    def read(self):\n        # with open(self.file_path, 'r') as file:\n        #     data = yaml.safe_load(file)\n        # return data\n        pass\n\n    def get_property(self, key):\n        return self.config_map.get(key, \"\")\n    \n    def set_default_openai_model(self, model:str):\n        self.__set_property(\"openai_model\", model)\n    \n    def __set_property(self, key, value):\n        self.config_map[key] = value\n"}
{"type": "source_file", "path": "solidgpt/src/diy/custom/customskillgenerator.py", "content": "\n\nimport re\nimport logging\nimport os\nfrom solidgpt.definitions import ROOT_DIR\nfrom solidgpt.src.diy.custom.customizedskilldefinition import CustomizedSkillDefinition\nfrom solidgpt.src.manager.gptmanager import GPTManager\nfrom solidgpt.src.manager.promptresource import CUSTOM_GENERATE_LIST_SKILLS_OUTPUT_FORMAT, CUSTOM_GENERATE_PRINCIPLES, \\\n    build_gpt_prompt, get_custom_skills_assumption_role_prompt\nfrom solidgpt.src.util.util import save_to_json\n\nclass CustomSkillGenerator:\n    def __init__(self):\n        self.gpt_manager = GPTManager._instance\n        self.cache = {}\n\n    def generate_custom_skill(self, business:str):\n        logging.info(\"Generating custom skill...\")\n        skills = self.__list_essential_skill_list(business)\n        for skill in skills:\n            self.cache[\"skill_short_description\"] = skill\n            self.__get_custom_skill_detail()\n            self.__generate_principles()\n            custom_skill_definition = self.__format_custom_skill()\n            logging.info(custom_skill_definition)\n        return\n        \n    def __list_essential_skill_list(self, business:str):\n        role_prompt = get_custom_skills_assumption_role_prompt(business)\n        prompt = build_gpt_prompt(role_assumption=role_prompt, output_format=CUSTOM_GENERATE_LIST_SKILLS_OUTPUT_FORMAT)\n        skill_list : str = self.gpt_manager.create_and_chat_with_model(\n            prompt=prompt,\n            gpt_model_label=\"list_essential_skill_list\",\n            input_message=\"Always use && to separate each skill \",\n            temperature=0\n        )\n        skill_list_tmp = skill_list.split(\"&&\")\n        if len(skill_list_tmp) < 2:\n            lines = skill_list.split('\\n')\n            skill_list_tmp = [line for line in lines if re.match(r'^\\d+\\.', line)]\n        skill_list_final = [item for item in skill_list_tmp if not item.isdigit() and item != '']\n        return skill_list_final\n    \n    def __get_custom_skill_detail(self):\n        skill_short_description = self.cache[\"skill_short_description\"]\n        logging.info(f\"\"\"Explore skill {skill_short_description}\"\"\")\n        detail = self.gpt_manager.create_and_chat_with_model(\n            prompt=f\"\"\"I want to create the {skill_short_description} AI agent, \n            Can you list more detail about the {skill_short_description}?\n            can you give me an input and output format of agent?\n            And also give an instruction of how to do/implement {skill_short_description} step by step\"\"\",\n            gpt_model_label=\"get_custom_skills_detail\",\n            input_message=skill_short_description\n        )\n        qa_example = self.gpt_manager.create_and_chat_with_model(\n            prompt= f\"\"\"Your idea for skill {skill_short_description} is: {detail}. Directly response no extra words \"\"\",\n            gpt_model_label=\"get_custom_skills_example\",\n            input_message=\"\"\"Can you give a example Input and output base on your idea.\"\"\"\n        )\n        self.cache[\"detail\"] = detail\n        self.cache[\"qa_example\"] = qa_example\n\n    def __generate_principles(self):\n        if self.cache.get(\"detail\") is None or self.cache.get(\"skill_short_description\") is None:\n            logging.error(\"Don't have enough information to generate principles\")\n        prompt = build_gpt_prompt(f'''Assuem you are the expert with {self.cache[\"skill_short_description\"]}''',\n                                   CUSTOM_GENERATE_PRINCIPLES)\n        self.cache['principles'] = self.gpt_manager.create_and_chat_with_model(\n            prompt=prompt,\n            gpt_model_label=\"generate principles\",\n            input_message=f'''Task description: {self.cache[\"skill_short_description\"]}\\n\\n \n            Task instruction{self.cache[\"detail\"]}'''\n        ) \n\n    def __format_custom_skill(self):\n        if self.cache.get(\"qa_example\") is None or self.cache.get(\"detail\") is None:\n            logging.error(\"Please run list_essential_skill_list or get_custom_skill_detail first\")\n        \n        skill_name = self.gpt_manager.create_and_chat_with_model(\n            prompt=f\"\"\"Base on the skill short description, give me short clear Camel Case no space name . \n            For example - Quantitative Analyst, Programming, Write PRD etc.\"\"\",\n            gpt_model_label=\"format_custom_skill\",\n            input_message=f'''Describtion: {self.cache[\"detail\"]}'''\n        )\n        definition = CustomizedSkillDefinition(\n            skill_name = skill_name,\n            basic_description = self.cache[\"skill_short_description\"],\n            instruction= self.cache[\"detail\"],\n            qa_example = self.cache[\"qa_example\"],\n            principles = self.cache['principles'],\n            embedding_background_data_list= \"\", \n            input_method= \"SkillIOParamCategory.PlainText\", \n            output_method= \"SkillIOParamCategory.PlainText\"\n        )\n        save_to_json(definition.toDict(), os.path.join(ROOT_DIR, \"localstorage\", \"customizedskilldefinition\", f\"{skill_name}.json\"))\n        # Clean cache\n        self.cache = {}\n        return \n\n# Sample code\n# GPTManager()\n# c = CustomSkillGenerator()\n# c.generate_custom_skill(\"Product Manager\")"}
{"type": "source_file", "path": "solidgpt/src/diy/custom/customizedskilldefinition.py", "content": "from dataclasses import dataclass\n\nfrom solidgpt.src.workskill.skillio import SkillIOParamCategory\n\n\n@dataclass\nclass CustomizedSkillDefinition:\n    def __init__(self, skill_name, basic_description, instruction, \n                 qa_example, principles, embedding_background_data_list, \n                 input_method : SkillIOParamCategory, output_method : SkillIOParamCategory, model_name = None):\n        self.skill_name : str = skill_name\n        self.basic_description : str = basic_description\n        self.instruction : str = instruction\n        self.qa_example : str = qa_example\n        self.principles : str = principles\n        self.embedding_background_data_list : str = embedding_background_data_list\n        self.model_name : str = model_name\n        self.input_method : SkillIOParamCategory = input_method\n        self.output_method : SkillIOParamCategory = output_method\n\n    def toDict(self):\n        return {\n            \"skill_name\": self.skill_name,\n            \"basic_description\": self.basic_description,\n            \"instruction\": self.instruction,\n            \"qa_example\": self.qa_example,\n            \"principles\": self.principles,\n            \"embedding_background_data_list\": self.embedding_background_data_list,\n            \"model_name\": self.model_name,\n            \"input_method\": self.input_method,\n            \"output_method\": self.output_method\n        }"}
{"type": "source_file", "path": "solidgpt/src/manager/embedding/embeddingmodel.py", "content": "import logging\nimport os\nfrom typing import List\nfrom numpy.linalg import norm\nimport numpy as np\nimport pandas as pd\nimport openai\nfrom solidgpt.definitions import LOCAL_STORAGE_DIR\nfrom solidgpt.src.configuration.configreader import ConfigReader\n\nLOCAL_EMBEDDING_STORAGE_ORIGINAL_RESOURCE_DIR = os.path.join(LOCAL_STORAGE_DIR, \"embedding\", \"originalresources\")\nLOCAL_EMBEDDING_STORAGE_DIVIDED_RESOURCE_DIR = os.path.join(LOCAL_STORAGE_DIR, \"embedding\", \"dividedresources\")\nLOCAL_EMBEDDING_STORAGE_EMBEDDED_RESOURCE_DIR = os.path.join(LOCAL_STORAGE_DIR, \"embedding\", \"embeddedresources\")\n\nclass EmbeddingModelParameter:\n    def __init__(self, resource_name: str, original_resources_folder_path: str, divided_resources_folder_path: str, embedded_resources_folder_path: str, has_embedded: bool = False):\n        self.resource_name = resource_name\n        self.original_resources_folder_path = original_resources_folder_path\n        self.divided_resources_folder_path = divided_resources_folder_path\n        self.embedded_resources_folder_path = embedded_resources_folder_path\n        self.has_embedded = has_embedded\n\nclass EmbeddingModel:\n    def __init__(self, param : EmbeddingModelParameter):\n        # read api key from config file\n        openai.api_key = ConfigReader().get_property(\"openai_api_key\")\n        self.EMBEDDINGS_MODEL = f\"text-embedding-ada-002\"\n        self.resource_name = param.resource_name\n        self.original_resources_folder_path = param.original_resources_folder_path\n        self.divided_resources_folder_path = param.divided_resources_folder_path\n        self.embedded_resources_folder_path = param.embedded_resources_folder_path\n        self.has_embedded = param.has_embedded\n    \n    def embed_resources(self, split_word_num = 300):\n        self.__split_resources_into_sections(split_word_num)\n        self.__compute_save_resource_embedding()\n        self.has_embedded = True\n\n    def query_most_match_result_from_resource(self, message, candidate_num, resources_list = [], is_debug = False):\n        if not self.has_embedded:\n            logging.warning(\"Please embed resources first.\")\n            return\n        query_embedding = self.__get_text_embedding(message)\n        if len(resources_list) == 0:\n            resources_list = os.listdir(self.embedded_resources_folder_path)\n        resources_list = [item for item in resources_list if not self.__skip_placeholder_file(item)]\n        ret = []\n        try:\n            candidate_num_for_each_resource = int(candidate_num / len(resources_list))\n        except:\n            logging.error(\"\"\"Please embed the data first. \n            This issue often arises when the original resources aren't provided in the correct location.\"\"\")\n        for resource_name in resources_list:\n            embedded_background = pd.read_csv(\n                os.path.join(self.embedded_resources_folder_path, resource_name, f\"{self.resource_name}{resource_name}Embedding.csv\"),\n            )\n            best_section = [(0, \"\")]\n            for col_name in embedded_background.columns.tolist():\n                embedded_background_section = embedded_background[col_name].values.tolist()\n                cur_score = self.__vector_similarity(query_embedding, embedded_background_section)\n                \n                if cur_score > best_section[-1][0] and len(best_section) < candidate_num_for_each_resource:\n                    best_section.append((cur_score, col_name))\n                    best_section = sorted(best_section, reverse=True)\n                elif cur_score > best_section[-1][0] and len(best_section) == candidate_num_for_each_resource:\n                    best_section.pop()\n                    best_section.append((cur_score, col_name))\n                    best_section = sorted(best_section, reverse=True)\n            for score, idx in best_section:\n                if idx == \"\":\n                    continue\n                with open(os.path.join(self.divided_resources_folder_path, resource_name, f\"{self.resource_name}Section{idx}.txt\"), encoding=\"utf-8\") as f:\n                    lines = f.read()\n                    ret.append((lines, score)) if is_debug else ret.append(lines)\n        return ret\n    \n    def __get_embedding(self, text: str) -> List[float]:\n        result = openai.Embedding.create(\n        model=self.EMBEDDINGS_MODEL,\n        input=text)\n        return result[\"data\"][0][\"embedding\"]\n\n    def __get_text_embedding(self, text: str) -> List[float]:\n        return self.__get_embedding(text)\n\n    def __vector_similarity(self, x: List[float], y: List[float]) -> float:\n        \"\"\"\n        We could use cosine similarity or dot product to calculate the similarity between vectors.\n        In practice, we have found it makes little difference.\n        \"\"\"\n        return np.dot(np.array(x), np.array(y)) / (norm(np.array(x)) * norm(np.array(y)))\n\n    def __split_resources_into_sections(self, split_word_num):\n        for file in os.listdir(self.original_resources_folder_path):\n            if self.__skip_placeholder_file(file):\n                continue\n            file_name = file.split(\".\")[0]\n            section_id = 0\n            if not os.path.exists(os.path.join(self.divided_resources_folder_path, file_name)):\n                os.mkdir(os.path.join(self.divided_resources_folder_path, file_name))\n            with open(os.path.join(self.original_resources_folder_path, file), encoding=\"utf-8\") as f:\n                # TODO: Will support more language. Currently we only support the english text.\n                lines = f.read().replace(\"\\n\", \"\").split(\".\")\n                lines = list(filter(lambda x: True if x else False, lines))\n                word_count = 0\n                idx = 0\n                last_idx = 0\n                logging.info(file_name)\n                while idx < len(lines):\n                    while word_count < split_word_num:\n                        if idx >= len(lines):\n                            break\n                        word_count += len(lines[idx])\n                        idx += 1\n                    with open(os.path.join(self.divided_resources_folder_path, file_name, f\"{self.resource_name}Section{str(section_id).zfill(4)}.txt\"), \"w\",\n                            encoding='utf-8') as f2:\n                        f2.write(\"ã€‚\".join(lines[last_idx:idx]))\n                    section_id += 1\n                    last_idx = idx\n                    word_count = 0\n\n\n    def __compute_save_resource_embedding(self):\n        ret_df = pd.DataFrame()\n        for file_name in os.listdir(self.divided_resources_folder_path):\n            if self.__skip_placeholder_file(file_name):\n                continue\n            ret_df = pd.DataFrame()\n            for file in os.listdir(os.path.join(self.divided_resources_folder_path, file_name)):\n                with open(os.path.join(self.divided_resources_folder_path, file_name, file), encoding=\"utf-8\") as f:\n                    lines = f.read()\n                    section_id = file.split(\".\")[0][-4:]\n                    print(file_name, section_id)\n                    try:\n                        ret_df[str(section_id).zfill(4)] = self.__get_text_embedding(lines)\n                    except BaseException as ex:\n                        print(f\"{str(ex)}\")\n            if not os.path.exists(os.path.join(self.embedded_resources_folder_path, file_name)):\n                os.mkdir(os.path.join(self.embedded_resources_folder_path, file_name))\n            ret_df.to_csv(os.path.join(self.embedded_resources_folder_path, file_name, f\"{self.resource_name}{file_name}Embedding.csv\"), index=False)\n\n    def __skip_placeholder_file(self, file:str) -> bool:\n        return file == \"Placeholder.md\"\n\n\n# Sample code\n# param = EmbeddingModelParameter(\"test\", LOCAL_EMBEDDING_STORAGE_ORIGINAL_RESOURCE_DIR, \n#                    LOCAL_EMBEDDING_STORAGE_DIVIDED_RESOURCE_DIR, \n#                    LOCAL_EMBEDDING_STORAGE_EMBEDDED_RESOURCE_DIR, True)\n# embed = EmbeddingModel(param)\n# embed.embed_resources()\n# embed.query_most_match_result_from_resource(\"I want to know how to make a cake\", 12)\n"}
{"type": "source_file", "path": "solidgpt/src/diy/chatgpt_diy_finetune/dataset_checker.py", "content": "# We start by importing the required packages\n# The scripts adopt from the https://platform.openai.com/docs/guides/fine-tuning\n# The script will check if the data a good to use chatgpt to train. User need\n# to update the `data_path` to the training files. The output is the format\n# error or format approved. Also provide an estimation of the token usa byto\n# the training files. The output is the format error or format approved. Also\n# provide an estimation of the token usage.\nimport json\nimport os\nimport tiktoken\nimport numpy as np\nfrom collections import defaultdict\n\n# Next, we specify the data path and open the JSONL file\n\ndata_path = \"\"\n\n# Load dataset\nwith open(data_path) as f:\n    dataset = [json.loads(line) for line in f]\n\n# We can inspect the data quickly by checking the number of examples and the first item\n\n# Initial dataset stats\nprint(\"Num examples:\", len(dataset))\nprint(\"First example:\")\nfor message in dataset[0][\"messages\"]:\n    print(message)\n\n# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n\n# Format error checks\nformat_errors = defaultdict(int)\n\nfor ex in dataset:\n    if not isinstance(ex, dict):\n        format_errors[\"data_type\"] += 1\n        continue\n\n    messages = ex.get(\"messages\", None)\n    if not messages:\n        format_errors[\"missing_messages_list\"] += 1\n        continue\n\n    for message in messages:\n        if \"role\" not in message or \"content\" not in message:\n            format_errors[\"message_missing_key\"] += 1\n\n        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n            format_errors[\"message_unrecognized_key\"] += 1\n\n        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n            format_errors[\"unrecognized_role\"] += 1\n\n        content = message.get(\"content\", None)\n        if not content or not isinstance(content, str):\n            format_errors[\"missing_content\"] += 1\n\n    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n        format_errors[\"example_missing_assistant_message\"] += 1\n\nif format_errors:\n    print(\"Found errors:\")\n    for k, v in format_errors.items():\n        print(f\"{k}: {v}\")\nelse:\n    print(\"No errors found\")\n\n# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n\n# Token counting functions\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n\n# not exact!\n# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\ndef num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3\n    return num_tokens\n\ndef num_assistant_tokens_from_messages(messages):\n    num_tokens = 0\n    for message in messages:\n        if message[\"role\"] == \"assistant\":\n            num_tokens += len(encoding.encode(message[\"content\"]))\n    return num_tokens\n\ndef print_distribution(values, name):\n    print(f\"\\n#### Distribution of {name}:\")\n    print(f\"min / max: {min(values)}, {max(values)}\")\n    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n\n# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n\n# Warnings and tokens counts\nn_missing_system = 0\nn_missing_user = 0\nn_messages = []\nconvo_lens = []\nassistant_message_lens = []\n\nfor ex in dataset:\n    messages = ex[\"messages\"]\n    if not any(message[\"role\"] == \"system\" for message in messages):\n        n_missing_system += 1\n    if not any(message[\"role\"] == \"user\" for message in messages):\n        n_missing_user += 1\n    n_messages.append(len(messages))\n    convo_lens.append(num_tokens_from_messages(messages))\n    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n\nprint(\"Num examples missing system message:\", n_missing_system)\nprint(\"Num examples missing user message:\", n_missing_user)\nprint_distribution(n_messages, \"num_messages_per_example\")\nprint_distribution(convo_lens, \"num_total_tokens_per_example\")\nprint_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\nn_too_long = sum(l > 4096 for l in convo_lens)\nprint(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n\n# Pricing and default n_epochs estimate\nMAX_TOKENS_PER_EXAMPLE = 4096\n\nMIN_TARGET_EXAMPLES = 100\nMAX_TARGET_EXAMPLES = 25000\nTARGET_EPOCHS = 3\nMIN_EPOCHS = 1\nMAX_EPOCHS = 25\n\nn_epochs = TARGET_EPOCHS\nn_train_examples = len(dataset)\nif n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\nelif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n\nn_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\nprint(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\nprint(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\nprint(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\nprint(\"See pricing page to estimate total costs\")\n"}
{"type": "source_file", "path": "solidgpt/src/manager/llamanager.py", "content": "import os\nfrom solidgpt.src.configuration.configreader import ConfigReader\nfrom text_generation import Client\nfrom solidgpt.src.manager.promptresource import llama_v2_prompt\n\nclass LLAManager:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(LLAManager, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, if_show_reply=False):\n        self.llama2_base_url = ConfigReader().get_property(\"HF_API_LLAMA2_BASE\")\n        self.llama2_api_key = ConfigReader().get_property(\"HF_API_KEY\")\n        self.llama_models_container = {}\n        self.if_show_reply = if_show_reply\n\n    def create_model(self, prompt, llama_api, llama_model_label, temperature=1, model=None):\n        if model is None:\n            model = self.llama2_base_url  # Use LLAMA2 base URL as the model\n        llama_model = LLamaModel(prompt, self.llama2_api_key,  self.llama2_base_url, self.if_show_reply, temperature)\n        self.llama_models_container[llama_model_label] = llama_model\n        return llama_model\n\n    def create_and_chat_with_model(self, prompt, llama_model_label, input_message, temperature=0.1, model=None):\n        llama_model = self.create_model(prompt, llama_model_label, temperature, model)\n        return llama_model.chat_with_model(input_message)\n\n    def get_llama_model(self, llama_model_label):\n        return self.llama_models_container.get(llama_model_label)\n\n    def remove_llama_model(self, llama_model_label):\n        self.llama_models_container.pop(llama_model_label, None)\n\nclass LLamaModel:\n    def __init__(self, prompt, api, model, if_show_reply=True, temperature=0.1):\n        self.prompt = prompt\n        self.api = api\n        self.model = model\n        self.messages = [{\"role\": \"system\", \"content\": self.prompt}]\n        self.last_reply = None\n        self.if_show_reply = if_show_reply\n        self.temperature = temperature\n\n    def chat_with_model(self, input_message):\n        self.messages.append({\"role\": \"user\", \"content\": input_message})\n        self._run_model()\n        return self.last_reply\n\n    def _run_model(self):\n        client = Client(self.model, headers={\"Authorization\": f\"Bearer {self.api}\"}, timeout=120)\n        chat = client.generate(\n            llama_v2_prompt(self.messages),  # Convert messages to LLAMA2 prompt\n            temperature=self.temperature,\n            max_new_tokens=1000\n        )\n        reply = chat.generated_text\n        if self.if_show_reply:\n            print(f\"LLAMA2: {reply}\")\n        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n        self.last_reply = reply\n\n    def add_background(self, background_message):\n        self.messages.append({\"role\": \"assistant\", \"content\": background_message})\n\n"}
{"type": "source_file", "path": "solidgpt/src/tools/lowdefy/runner/buildwebapprunner.py", "content": "import os\nimport subprocess\n\nclass WebAppRunner:\n\n    def __init__(self, app_name : str, project_folder : str):\n        self.app_name = app_name\n        self.project_folder = project_folder\n\n    def build_run_webapp(self):\n        # Command to run\n        command = [\"pnpx\", \"lowdefy@rc\", \"dev\"]\n        if os.name == 'nt':\n            command = [\"cmd.exe\", \"/c\", \"pnpx\", \"lowdefy@rc\", \"dev\"]\n        # Change the current working directory to the absolute path\n        subprocess.run(command, cwd=self.project_folder)\n"}
{"type": "source_file", "path": "solidgpt/src/diy/llama2_diy_finetune/llama2modelsetting.py", "content": "import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\nclass LlamaBasicModelFactory:\n    def __init__(self, model_name, lora_r, lora_alpha, lora_dropout, use_4bit, bnb_4bit_compute_dtype,\n                 bnb_4bit_quant_type, use_nested_quant, device_map):\n        self.model_name = model_name\n        self.lora_r = lora_r\n        self.lora_alpha = lora_alpha\n        self.lora_dropout = lora_dropout\n        self.use_4bit = use_4bit\n        self.bnb_4bit_compute_dtype = bnb_4bit_compute_dtype\n        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n        self.use_nested_quant = use_nested_quant\n        self.device_map = device_map\n\n    def create_model(self):\n        compute_dtype = getattr(torch, self.bnb_4bit_compute_dtype)\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=self.use_4bit,\n            bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=self.use_nested_quant,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=bnb_config,\n            device_map=self.device_map\n        )\n        model.config.use_cache = False\n        model.config.pretraining_tp = 1\n        tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"left\"\n        return model, tokenizer\n"}
{"type": "source_file", "path": "solidgpt/src/api/api_response.py", "content": "def response_upload(message=\"\", status=\"\", progress=\"\", error=\"\"):\r\n    if progress == \"\":\r\n        progress = {}\r\n    return {\r\n        \"message\": message,\r\n        \"status\": status,\r\n        \"progress\": progress,\r\n        \"error\": error,\r\n    }\r\n\r\n\r\ndef response_serverless(message=\"\", status=\"\", error=\"\"):\r\n    return {\r\n        \"message\": message,\r\n        \"status\": status,\r\n        \"error\": error,\r\n    }\r\n\r\n\r\ndef response_graph(graph=\"\", message=\"\", status=\"\", progress=\"\", error=\"\", result=\"\", extra_payload=None):\r\n    if progress == \"\":\r\n        progress = {}\r\n    return {\r\n        \"graph\": graph,\r\n        \"message\": message,\r\n        \"status\": status,\r\n        \"progress\": progress,\r\n        \"error\": error,\r\n        \"result\": result,\r\n        \"payload\": extra_payload,\r\n    }"}
{"type": "source_file", "path": "solidgpt/src/workgraph/workgraph.py", "content": "\"\"\"everything in workgraph can be considered as global\"\"\"\nimport os\nimport time\nfrom solidgpt.src.saveload.saveload import *\n# from solidgpt.src.tools.notion.notionactions import NotionActions\nfrom solidgpt.src.workgraph.displayresult import DisplayResult\n\n\nclass WorkGraph:\n\n    nodes: list[WorkNode] = []\n    node_map: dict[str, WorkNode] = {}\n    output_map: dict[int, SkillOutput] = {}\n    output_id_to_node_map: dict[int, WorkNode] = {}\n    display_result: DisplayResult\n    notion = None\n    cache = {}\n    callback_map: dict = {}\n    custom_data: dict = {}\n\n    def __init__(self, output_directory_path_override: str = \"\", output_id = None):\n        # need to manually initialize here\n        self.nodes = []\n        self.node_map = {}\n        self.output_map = {}\n        self.output_id_to_node_map = {}\n        self.callback_map = {}\n        self.display_result = DisplayResult()\n        self.output_directory_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, output_id or time.strftime(\"%Y%m%d%H%M%S\"))\n        if output_directory_path_override:\n            self.output_directory_path = os.path.abspath(output_directory_path_override)\n        return\n\n    def add_node(self, node: WorkNode):\n        node.graph_cache = self.cache\n        self.nodes.append(node)\n\n    def init_node_dependencies(self):\n\n        # clear node map and output map\n        self.node_map.clear()\n        self.output_map.clear()\n        self.output_id_to_node_map.clear()\n\n        for node in self.nodes:\n            # add node to node map\n            self.node_map[node.node_id] = node\n\n            # initialize display_result for children\n            node.display_result = self.display_result\n            node.skill.display_result = self.display_result\n\n            # intialize callback func for skills\n            if node.skill.name in self.callback_map:\n                node.skill.callback_func = self.callback_map.get(node.skill.name, None)\n\n            # keep a graph reference in skill\n            node.skill.graph = self\n\n            # create directory for node\n            node_directory_path = os.path.join(self.output_directory_path,\n                                               (node.skill.name + \"_\" + str(node.node_id)).replace(\" \", \"_\"))\n            if not os.path.exists(node_directory_path):\n                # Create the output folder\n                os.makedirs(node_directory_path)\n                print(f\"Directory '{node_directory_path}' created.\")\n\n            # add output to output map\n            for o in node.skill.outputs:\n                # initialize output paths\n                o.param_path = os.path.join(node_directory_path, (o.param_name + \" \" + str(o.id)).replace(\" \", \"_\"))\n                # output can be consumed by inputs of other nodes\n                if o.id >= 0:\n                    self.output_map[o.id] = o\n                    self.output_id_to_node_map[o.id] = node\n\n        # second iteration after output map has been initialized\n        for node in self.nodes:\n            # add output dependencies for node\n            for i in node.skill.inputs:\n                if i.loading_method == SkillInputLoadingMethod.LOAD_FROM_OUTPUT_ID:\n                    if i.load_from_output_id == -1:\n                        print(\"Error, \" + i.param_name + \": cannot load from output id: -1.\")\n                        continue\n                    node.output_id_dependencies.add(i.load_from_output_id)\n\n                    # add current node to next_node_ids of the output node\n                    if i.load_from_output_id not in self.output_map:\n                        print(\"Error, input %s: loading from an invalid output id %d.\"\n                              % (i.param_name, i.load_from_output_id))\n                        continue\n                    output_node = self.output_id_to_node_map[i.load_from_output_id]\n                    if output_node.node_id == node.node_id:\n                        print(\"Error, \" + i.param_name + \": cannot load from its own output.\")\n                        continue\n                    output_node.next_node_ids.add(node.node_id)\n                    i.skill_output = self.output_map[i.load_from_output_id]\n\n        if self.__is_circular():\n            print_error_message(\"Circular graph detected. Terminating program...\")\n            exit(1)\n        return\n\n    def execute(self):\n        logging.info(\"Executing SolidGPT...\")\n        first_nodes = []\n        for node in self.nodes:\n            if len(node.output_id_dependencies) == 0:\n                first_nodes.append(node)\n\n        if len(first_nodes) == 0:\n            print_error_message(\"Cannot execute graph, no node can be executed.\")\n\n        for node in first_nodes:\n            self.__execute_node(node)\n        logging.info(f\"SolidGPT execution completed. All results are saved in {self.output_directory_path}\")\n        \n\n    def __execute_node(self, node: WorkNode):\n        if node.can_execute():\n            # execute skill\n            node.skill.execute()\n            # wait for potential node pause\n            self.__node_pause(node)\n            # notify other nodes\n            for next_node_id in node.next_node_ids:\n                next_node = self.node_map[next_node_id]\n                for o in node.skill.outputs:\n                    next_node.output_id_dependencies.remove(o.id)\n                self.__execute_node(next_node)\n\n    def __node_pause(self, node: WorkNode):\n        if node.manual_review_result:\n            time.sleep(0.25)\n            print(\"\\nPlease review result generated by %s skill in node %s\"\n                  % (node.skill.name, str(node.node_id)))\n            notion_file_opened = False\n            while True:\n                user_input = input(\"Execution is halted. Please specify an action, then press Enter. \"\n                                   \"To view all available actions, enter 'help':\")\n\n                if same_string(user_input, \"help\"):\n                    help_msg: str = \"{:<18}{}\\n\".format(\"help\", \"Show this help message.\") + \\\n                                    \"{:<18}{}\\n\".format(\"continue\", \"Continue execution.\") + \\\n                                    \"{:<18}{}\\n\".format(\"stop\", \"Stop program.\") + \\\n                                    \"{:<18}{}\\n\".format(\"path\", \"Show the path of this result.\") + \\\n                                    \"{:<18}{}\\n\".format(\"notion-open\", \"Open the markdown result in notion.\") + \\\n                                    \"{:<18}{}\\n\".format(\"notion-sync\", \"Sync the notion result, save it as new output.\")\n                    print(help_msg)\n                    continue\n                elif same_string(user_input, \"continue\"):\n                    print(\"Continuing execution...\")\n                    break\n                elif same_string(user_input, \"stop\"):\n                    print(\"Exiting the program...\")\n                    exit(0)\n                elif same_string(user_input, \"path\"):\n                    print(os.path.abspath(os.path.dirname(node.skill.outputs[0].param_path)))\n                    continue\n                elif same_string(user_input, \"notion-open\"):\n                    if self.notion is None:\n                        # self.notion = NotionActions()\n                        pass\n                    if not notion_file_opened:\n                        skill_outputs = node.skill.outputs\n                        if len(skill_outputs) > 0:\n                            first_output = skill_outputs[0]\n                            category = first_output.param_category\n                            output_path = first_output.param_path\n                            print(f\"!!!!!!!{type(category)} {category}\")\n                            if category == SkillIOParamCategory.ProductRequirementsDocument or \\\n                                    category == SkillIOParamCategory.BusinessRequirementsDocument or \\\n                                    category == SkillIOParamCategory.HighLevelDesignDocument or \\\n                                    category == SkillIOParamCategory.PlainText or \\\n                                    category == SkillIOParamCategory.KanbanBoard:\n\n                                output_md_filepath = add_extension_if_not_exist(output_path, \".md\")\n                                self.notion.process_markdown_and_upload(output_md_filepath)\n                            else:\n                                print(f\"Notion does not support {SkillIOParamCategory.PlainText} output.\")\n                                continue\n                        else:\n                            print(\"Notion does not support skill with no outputs.\")\n                            continue\n                        notion_file_opened = True\n                    else:\n                        print(\"File already opened in Notion.\")\n                    continue\n                elif same_string(user_input, \"notion-sync\"):\n                    if notion_file_opened:\n                        skill_outputs = node.skill.outputs\n                        first_output = skill_outputs[0]\n                        category = first_output.param_category\n                        output_path = first_output.param_path\n                        if category == SkillIOParamCategory.ProductRequirementsDocument or \\\n                                category == SkillIOParamCategory.BusinessRequirementsDocument or \\\n                                category == SkillIOParamCategory.HighLevelDesignDocument or \\\n                                category == SkillIOParamCategory.PlainText or \\\n                                category == SkillIOParamCategory.KanbanBoard:\n                            output_md_file_dir = os.path.dirname(output_path)\n                            output_md_file_name = os.path.basename(output_path)\n                            self.notion.sync_from_notion(output_md_file_dir, output_md_file_name)\n                        print(\"Notion file synced.\")\n                    else:\n                        print(\"notion-open command needs to be used first.\")\n                    continue\n                else:\n                    print(\"Invalid input entered.\")\n                    continue\n\n    def save_data(self, filename: str = \"data.json\", generate_debug_info: bool = False):\n        save_data = generate_save_data_from_nodes(self.nodes, generate_debug_info)\n        save_to_json(save_data, filename)\n        return\n\n    def load_data(self, filename: str = \"data.json\"):\n        loaded_data = load_from_json(filename)\n        self.nodes.clear()\n        self.nodes = load_save_data_to_nodes(loaded_data)\n        self.init_node_dependencies()\n        return\n\n    def __is_circular(self):\n        visited = {node_id: False for node_id in self.node_map}\n        stack = {node_id: False for node_id in self.node_map}\n\n        # check every node because the graph might be disjoint\n        for node_id in self.node_map:\n            if not visited[node_id]:\n                if self.__has_cycle(node_id, visited, stack):\n                    return True\n\n        return False\n\n    def __has_cycle(self, current_node_id, visited, stack):\n        # mark the current node as visited\n        visited[current_node_id] = True\n        # add the current node to the stack representing the current path\n        stack[current_node_id] = True\n\n        # visit all the neighbors of the current node\n        for neighbor_id in self.node_map[current_node_id].next_node_ids:\n            # if the neighbor is not visited, visit it\n            if not visited[neighbor_id]:\n                if self.__has_cycle(neighbor_id, visited, stack):\n                    return True\n            # if the neighbor is already in the current path, we have found a cycle\n            elif stack[neighbor_id]:\n                return True\n\n        # remove the current node from the current path stack\n        stack[current_node_id] = False\n        return False\n"}
{"type": "source_file", "path": "solidgpt/src/request/basic_request.py", "content": "import requests\nimport logging\nfrom enum import Enum\n\nclass RequestMethod(Enum):\n    POST = 1\n    GET = 2\n    PATCH = 3\n    DELETE = 3\n\nclass BasicRequest:\n    def __init__(self, url : str, method : RequestMethod, headers : str, params : str, data : dict):\n        self.url = url\n        self.method = method\n        self.headers = headers\n        self.params = params\n        self.data = data\n        self.response = None\n\n    def __str__(self):\n        return f\"basic_request(url={self.url}, method={self.method}, headers={self.headers}, params={self.params}, data={self.data})\"\n\n    def __repr__(self):\n        return self.__str__()\n    \n    def call(self):\n        if self.method == RequestMethod.GET:\n            self.response = requests.get(self.url, headers=self.headers, params=self.params)\n        elif self.method == RequestMethod.POST:\n            self.response = requests.post(self.url, headers=self.headers, json=self.data)\n        else:\n            raise Exception(\"Invalid method\")\n        logging.info(f\"Response: {self.response}\")\n        return self.response"}
{"type": "source_file", "path": "solidgpt/src/manager/promptresource.py", "content": "PRODUCT_MANAGER_PRD_OUTPUT_TEMPLATE = f'''Base on the information help me generate Markdown PRD follow this format. \nHere is the output template and explain each sections mean. Always output with this template in Markdown format.\n\nOverview\n[what is the problem you are trying to solve?]\n\nTarget Customers\n[Who is the primary person you are solving a problem for? If there are multiple groups of people, \nwhich ones ar you prioritizing first? \nProducts optimized for power users will be different thant if they were designed for new users]\n\nBusiness Ojective Opportunity\n[What is the business reason for why we should solve this problem? What is the business strategy \nhere? Are we falling behind our competitors? Are we entireing a new market? What's the opportunity size?]\n[How will you measure the success of this product? How will you know you are ready to ramp/launch this product]\n[What about downside metrics?(e.g. customer complain)]\n\nScope/Out-of-Scope\n[What will your product NOT do? How will you know when this product is \"done\"]\n\nUser Stories\n[Based on each of the provided sub-features and key features, \nBreak down the features into user stories as much as possible. Try to write a user story for each sub-feature.\nAdd more user stories based on similar products or your professional insight.]\n| Story                                                                                       | Value(L,M,H) | Cost/Complexity(L,M,H) | Priority |\n|---------------------------------------------------------------------------------------------|--------------|------------------------|----------|\n| As a [user type], I want to [action], so that [benefit/outcome].                             | M            | H                      | 1        |\n| As a [user type], I need [some feature], in order to [achieve some goal].                   | L            | M                      | 2        |\n| As a [user type], I wish [some desire], ensuring [some reason].                             | H            | L                      | 3        |\n\n\n\t\t\nAdditional Requirements and Feature Details\n[What key features are absolutely required? What should a user NOT be able to do? \nWhat coner case we should whatch out for?\nWhich features might we want to build in the future but which are not planned for initial release]\n[What's the feature details? What are the sub-features? What are the acceptance criteria?]\nFeatures Release Table\n| Features                           | Description   | Accept Criteria | Priority |\n|------------------------------------|---------------|-----------------|----------|\n| Feature Name (e.g. Login System)   | Short description of the feature. | Criteria for acceptance (e.g. User can log in with valid credentials). | High/Medium/Low |\n\t\t\nLaunch Milestones & Phases\n[Break down the product into manageable chunks in terms of features]\n[What are key milestones in research/discovery, design, development, pilot, and launch?]\n[What are the criteria for pilot and launch? Acceptance criteria(typically eng/pm-defined)and acceptance testing(business user)?]\nWe will often build a vision for phases of launch, along with stories/features a addressed in each stage.\n| Timing             | Features/User Stories         | Key Objectives                            |\n|--------------------|-------------------------------|-------------------------------------------|\n| Quarter/Month/Date | [Feature x, y,z]              | example: Successful pilot with 20 customer|\n'''\n\nPRODUCT_MANAGER_PRD_ROLE_ASSUMPTION = f'''Assume you are product manager, \nyou will help me write a formal PRD and output in Markdown and please use the markdown table if necessary to make things clear.\nAdd more user stories based on similar products or your professional insight. These are the basic information'''\n\nPRODUCT_MANAGER_BRAINSTORM_OUTPUT_TEMPLATE = ''' \nGiven the provided information, let's expand on the key features for product to create a more comprehensive product offering.\nBase on the information help me generate brainstorm follow this format.\nProduct Name: {Product Name}\nOne Sentence Description: {One Sentence Description}\nTarget User: {Target User}\nBusiness Goal: {Business Goal}\nKey Features:\n{Feature 1}\n{Feature 2}\n{Feature 3}\n{Feature 4}\n[Continue this structure for as many features as needed]\n{Feature n}\nTimeline:\n{Start Date}: {Description of what happens on this date or what gets released/begins.}\n{Another Important Date}: {Description of what happens on this date or what gets released/updated.}\n{Yet Another Important Date}: {Description of what happens on this date or what gets released/completed.}\nWith these added details, the product should offer a more comprehensive solution for the target user and help you better achieve your business goals.\n'''\n\nPRODUCT_MANAGER_BRAINSTORM_ROLE_ASSUMPTION = f'''Assuming you are a product manager, \nbased on the information I provide, do following things:\n1. Point out each key features and details.\n2. Get more key features from similar products or your professional insight.'''\n\nPRODUCT_MANAGER_ANALYSIS_ROLE_ASSUMPTION = f'''Assume you are Principal Product Manager, \nI will provide you our product instruction, product background additional information and requirements. \nHelp me use 5W2H to analysis and expend the requirments base on the project instrucution and product background additional information.'''\n\nPRODUCT_MANAGER_5H2W_OUTPUT_TEMPLATE = '''## 1. What:\n- **Description**: \n  - What is the main problem or goal?\n  - What are the features or functionalities involved?\n## 2. Why:\n- **Rationale & Benefits**:\n  - Why is this necessary?\n  - Why are we doing this now?\n  - What benefits will it bring?\n## 3. Where:\n- **Location & Placement**:\n  - Where will this be implemented or take place?\n  - Where will the end results be visible?\n## 4. Who:\n- **Stakeholders & Responsibilities**:\n  - Who is responsible for this?\n  - Who are the end-users or beneficiaries?\n  - Who will be affected by this?\n## 5. When:\n- **Timeline & Deadlines**:\n  - When will this start?\n  - When is the expected completion or launch date?\n  - Are there any key milestones?\n## 6. How:\n- **Process & Method**:\n  - How will this be accomplished or implemented?\n  - What tools, techniques, or methods will be used?\n## 7. How Much:\n- **Cost & Resources**:\n  - How much will this cost?\n  - How much time will it require?\n  - What resources are needed?'''\n\nPE_FRONTEND_ROLE_ASSUPTION = f'''Assuming you are a Principal Frontend Software Engineer, \nhelp me write a Frontend Dev Design Document using the PRD Doc provided. \nEnsure you adhere to the Frontend Design Principles listed below and use it as a guide.\n'''\n\nPE_FRONTEND_DESIGN_OUTPUT_TEMPLATE = '''Frontend Design Principles:\n1. User Experience (UX):\nResponsiveness: Ensure mobile-responsive design.\nNavigation: Intuitive navigation.\nFeedback: Feedback after key interactions.\nLoading Time: Fast loading times.\nAccessibility: Adherence to accessibility standards.\n2. User Interface (UI) Always elaberate more details as much as you can for each page in this section. Please include the layout, component, color theme, part of important text, font size, api request, page transition logic, business logic\n3. Design Considerations:\nConsistency: Uniform design language.\nModularity: Reusable components.\nInteractivity: Subtle animations/transitions.\n4. Performance:\nOptimize Images: Compressed and sized images.\nLazy Loading: Load assets on demand.\nCaching: Cache frequently accessed data/assets.\n5. Security:\nInput Validation: Validate user inputs.\nData Transmission: Secure data transmission methods.\nCookies and Local Storage: Encrypted or minimal data storage.\n6. Testing:\nCross-Browser Testing: Uniformity across browsers.\nMobile Testing: Testing on different mobile devices.\nUser Testing: Real user feedback.\n7. Rollout Strategy:\nBeta Testing: Limited audience release.\nVersioning: Easily roll back to stable versions.\nFeedback Loop: System for user feedback.\n'''\n\nPE_ROE_ASSUMPTION = f'''Assuming you are a Software Development Manager, \nhelp me create a Kanban board baseon the Dev Desgin.\n'''\n\nPE_KANBAN_OUTPUT_TEMPLATE = f'''Based on the provided high level design, \ncould you help me create a development Kanban board using Markdown format? \nThe columns I'd like are: \n|Task Name|Task Description|User story|Acceptance Criteria|Priority (H/M/L)|Status (Input/Spec/Imple/PR/Done/Pending)|Due Date|Engineer Points|.'''\n\nSDE_LOWDEFY_ASSUMPTION = f'''Assume you are a Software developer specializing in lowdefy,\nYou will create lowdefy yaml files given kanban tasks.\nYou must follow theses principles.\n1. Do not generate actions.\n2. Do not import plugins that are not already built.\n3. Do not generate reference pages (fields with '_ref').\n4. Do not create functions or events.\n5. Do not generate duplicate map keys in yaml file.\n6. Be concise. Keep the yaml file less than 200 lines.\n7. Always use lowdefy version 4.0.0-rc.10\"\n8. Do not use Nested mappings, only one colon is allowed in each line.\n'''\n\nSDE_LOWDEFY_YAML_OUTPUT_TEMPLATE = '''\nTask: Create a sample main page with the name lowdefy.yaml\nAnswer:\n```yaml\nlowdefy: 4.0.0-rc.10\nname: Lowdefy starter\nmenus:\n  - id: default\n    links:\n      - id: new-ticket\n        type: MenuLink\n        properties:\n          icon: AiOutlineAlert\n          title: New ticket\n        pageId: new-ticket\n      - id: welcome\n        type: MenuLink\n        properties:\n          icon: AiOutlineHome\n          title: Home\n        pageId: welcome\npages:\n  - _ref: new-ticket.yaml\n  - id: welcome\n    type: PageHeaderMenu\n    properties:\n      title: Welcome\n    areas:\n      content:\n        justify: center\n        blocks:\n          - id: content_card\n            type: Card\n            style:\n              maxWidth: 800\n            blocks:\n              - id: content\n                type: Result\n                properties:\n                  title: Welcome to your Lowdefy app\n                  subTitle: We are excited to see what you are going to build\n                  icon:\n                    name: AiOutlineHeart\n                    color: '#f00'\n                areas:\n                  extra:\n                    blocks:\n                      - id: docs_button\n                        type: Button\n                        properties:\n                          size: large\n                          title: Let's build something\n                        events:\n                          onClick:\n                            - id: link_to_docs\n                              type: Link\n                              params:\n                                url: https://docs.lowdefy.com\n                                newWindow: true\n```\nTask: Create a page for a web form where users can log a new ticket with the name new-ticket.yaml\nAnswer:\n```yaml\nid: new-ticket\ntype: PageHeaderMenu\nproperties:\n  title: New ticket # The title in the browser tab.\nlayout:\n  contentJustify: center # Center the contents of the page.\nblocks:\n  - id: content_card\n    type: Card\n    layout:\n      size: 800 # Set the size of the card so it does not fill the full screen.\n      contentGutter: 16 # Make a 16px gap between all blocks in this card.\n    blocks:\n      - id: page_heading\n        type: Title\n        properties:\n          content: Log a ticket # Change the title on the page.\n          level: 3 # Make the title a little smaller (an html `<h3>`).\n```\nTask: Create a BI report/dashboard pages in Lowdefy with the name lowdefy.yaml\nAnswer:\n```yaml\nname: Lowdefy Reporting Example\nlowdefy: 4.0.0-rc.10\nlicence: MIT\n\n# description\n# This example show patterns that can be used to implement a BI report/dashboard.\n# It assumes that it is connected to a MongoDB database with the Atlas sample dataset loaded.\n\n# Define all the data connections, in this case the brands and products MongoDB collections\nconnections:\n  - id: movies_mongodb # The connectionId that will be used when defining requests and mutations on our pages\n    type: MongoDBCollection\n    properties:\n      databaseName: sample_mflix # The database name\n      collection: movies # The collection name\n      databaseUri:\n        _secret: EXAMPLES_MDB # The database connection uri that is stored as a secret and accessed using the _secret operator\n\n# Menus used in the app can be listed here\n# By default, the menu with id default, or the first menu defined is used.\n# If no menu is defined, a default menu is created using all the defined pages.\nmenus:\n  - id: default\n    links:\n      - id: report # Define the menu link that directs to the report page\n        type: MenuLink\n        pageId: report # Id of the report page\n        properties:\n          title: Report # Title to show on the menu\n          icon: AiOutlineLineChart\n\n# All the pages in the app are listed here\n# Instead of defining the page in the lowdefy.yaml file, it is defined in its own yaml file and referenced here\npages:\n  - _ref: report.yaml\n```\nTask: Create the main page of blog web app with the name lowdefy.yaml\nAnswer: \n```yaml\nlowdefy: 3.23.3\nname: Lowdefy starter\nlicence: MIT\n\nconfig:\n  # Always direct users to home.\n  homePageId: home\n  auth:\n    openId:\n      # The url the user should be redirected to after logout.\n      logoutRedirectUri: '{{ openid_domain }}/v2/logout?returnTo={{ host }}/home&client_id={{ client_id }}'\n    pages:\n      # All pages in the app can be seen only by logged in users\n      protected: true\n      # except for the following pages:\n      public:\n        - login\n        - '404'\n        - home\n\ntypes:\n  AmChartsPie:\n    url: https://blocks-cdn.lowdefy.com/v3.10.1/blocks-amcharts/meta/AmChartsPie.json\n\nconnections:\n  - id: blog_posts\n    type: MongoDBCollection\n    properties:\n      databaseUri:\n        _secret: MONGODB_URI\n      databaseName: lowdefy_blog\n      collection: blog_posts\n      write: true\n\nmenus:\n  - id: default\n    links:\n      - id: home\n        type: MenuLink\n        properties:\n          icon: HomeOutlined\n          title: Home\n        pageId: home\n      - id: analytics\n        type: MenuLink\n        properties:\n          icon: LineChartOutlined\n          title: Analytics\n        pageId: analytics\n      - id: new-blog-post\n        type: MenuLink\n        properties:\n          icon: FormOutlined\n          title: New Blog Post\n        pageId: new-blog-post\n\npages:\n  - _ref: home.yaml\n  - _ref: analytics.yaml\n  - _ref: new-blog-post.yaml\n  - _ref: edit-blog-post.yaml\n  - _ref: login.yaml\n```\n'''\n\nSDE_LOWDEFY_PAGE_ASSUMPTION = f'''Assume you are a Software developer specializing in lowdefy,\nYou will create yaml files given kanban board tasks.\nYou must follow theses principles.\n1. Do not generate actions.\n2. Do not import plugins that are not already built.\n3. Do not generate reference pages (fields with '_ref').\n4. Do not create functions or events.\n5. Do not generate duplicate map keys in yaml file.\n6. Be concise. Keep the yaml file less than 200 lines.\n7. Do not use Nested mappings, only one colon is allowed in each line.\n'''\n\nSDE_PAGE_YAML_OUTPUT_TEMPLATE = f'''\nTask: Create a page referred in the main page that can generate business report.\nAnswer:\n```yaml\nid: report\ntype: PageHeaderMenu\nproperties:\n  title: Report\nlayout:\n  contentGutter: 16 # Set a gutter of 16px between all the cards on the page\nrequests:\n  # Request for the bar and pie charts\n  - id: scores_by_genre\n    type: MongoDBAggregation # MongoDB Aggregation to get the data\n    connectionId: movies_mongodb\n    properties:\n      pipeline:\n        - $unwind:\n            path: $genres # Genres is an array, so unwind to create 1 document for every array entry\n        - $match: # Only look at top 6 genres\n            genres:\n              $in:\n                - Drama\n                - Comedy\n                - Romance\n                - Crime\n                - Thriller\n                - Action\n        - $group:\n            # Calculate the average Rotten Tomatoes viewer and critic ratings for each genre.\n            _id: $genres # Group data by the genre field\n            viewerRating:\n              $avg: $tomatoes.viewer.rating\n            criticRating:\n              $avg: $tomatoes.critic.rating\n            count:\n              $sum: 1 # Count the number of documents by summing 1 for every document\n        - $addFields:\n            # Multiply viewerRating by 2 as it is out of 5 not 10.\n            viewerRating:\n              $multiply:\n                - $viewerRating\n                - 2\n        - $sort:\n            count: -1 # Sort by descending count\n\n  # Request for the table\n  - id: top_100_score_difference_movies\n    type: MongoDBAggregation # MongoDB Aggregation to get the data\n    connectionId: movies_mongodb\n    properties:\n      pipeline:\n        - $match:\n            tomatoes.critic.numReviews: # Match where there are 20 or more critic reviews\n              $gte: 20\n            tomatoes.viewer.numReviews: # and 100 or more viewer reviews\n              $gte: 100\n            genres: # Only look at top 6 genres\n              $in:\n                - Drama\n                - Comedy\n                - Romance\n                - Crime\n                - Thriller\n                - Action\n        - $project: # Include fields we want to show in the table\n            title: 1\n            year: 1\n            rated: 1\n            viewerRating: # Multiply viewerRating by 2 as it is out of 5 not 10.\n              $multiply:\n                - $tomatoes.viewer.rating\n                - 2\n            criticRating: $tomatoes.critic.rating\n            viewerReviews: $tomatoes.viewer.numReviews\n            criticReviews: $tomatoes.critic.numReviews\n            difference: # Calculate the difference between the critic and viewer scores\n              $abs: # Take the absolute (positive) value\n                $subtract:\n                  - $multiply:\n                      - $tomatoes.viewer.rating\n                      - 2\n                  - $tomatoes.critic.rating\n        - $sort:\n            difference: -1 # Sort by biggest difference\n        - $limit: 100 # Only return the first 100 results\n\nevents:\n  # A list of actions that gets completed when this page is first loaded.\n  onInitAsync:\n    - id: fetch_data # Fetch the request data before the page renders in order to populate the charts\n      type: Request\n      params:\n        - scores_by_genre\n        - top_100_score_difference_movies\n\nareas:\n  content:\n    blocks:\n      - id: title # Title on page\n        type: Title\n        properties:\n          content: Movie Critic and Viewer Ratings\n          level: 4\n      - id: genre_counts_bar_chart_card\n        type: Card\n        properties:\n          title: Comparison of Critic and Viewer Ratings by Genre\n        layout:\n          span: 16 # Make the card span 2 thirds of the screen\n        blocks:\n          - id: genre_counts_bar_chart\n            type: EChart\n            properties:\n              height: 400\n              option:\n                dataset:\n                  source:\n                    _request: scores_by_genre # Use scores_by_genre request for chart data\n                legend:\n                  show: true\n                  bottom: 0 # Display legend below chart\n                grid:\n                  bottom: 100\n                tooltip:\n                  show: true\n                  trigger: item\n                xAxis:\n                  type: category # Add a category for the x axis\n                  data:\n                    _array.map: # Map over the data and get the list of _ids which will serve as our categories\n                      - _request: scores_by_genre\n                      - _function:\n                          __args: 0._id\n                  axisLabel:\n                    rotate: 60 # Rotate the labels\n                yAxis:\n                  - type: value # Add a value for the y axis\n                    name: Rating # Give the y axis a title\n                    nameRotate: 90 ÃŸ\n                    nameLocation: middle\n                    nameGap: 40\n                    min: # Set minimum y value\n                      _function:\n                        __math.floor:\n                          __if_none:\n                            - __args: 0.min\n                            - 0\n                series:\n                  - type: bar # Create a column series to show columns\n                    name: Critics Rating\n                    itemStyle:\n                      color: '#5D7092' # Set column fill color\n                      borderColor: '#5D7092' # Set column border color\n                    encode:\n                      x: _id # Set the category value to the field _id in the data\n                      y: criticRating # Set the value value to the field in the data\n                  - type: bar\n                    name: Viewer Rating\n                    itemStyle:\n                      color: '#5AD8A6'\n                      borderColor: '#5AD8A6'\n                    encode:\n                      x: _id\n                      y: viewerRating\n\n      - id: pie_chart_card\n        type: Card\n        layout:\n          span: 8\n        properties:\n          title: Genre Counts\n        blocks:\n          - id: pie_chart\n            type: EChart\n            properties:\n              height: 400\n              option:\n                series:\n                  - name: genre_counts\n                    type: pie\n                    radius: [30%, 50%] # Make the chart a donut chart\n                    label:\n                      fontSize: 12\n                    data:\n                      _mql.aggregate: # Format data to have fields name and value\n                        on:\n                          _request: scores_by_genre # Share the same request as the bar chart\n                        pipeline:\n                          - $project:\n                              name: $_id\n                              value: $count\n                          - $sort:\n                              value: -1\n                    color: # Add custom colors\n                      - '#122C6A'\n                      - '#0044A4'\n                      - '#005BBF'\n                      - '#3874DB'\n                      - '#5A8DF8'\n                      - '#7EABFF'\n\n      - id: table_card\n        type: Card\n        properties:\n          title: 100 Movies with Largest Difference between Critic and Viewer Ratings\n        blocks:\n          - id: table\n            type: AgGridAlpine\n            properties:\n              theme: basic\n              rowData:\n                _request: top_100_score_difference_movies\n              defaultColDef: # Define default column definitions that apply to all the defined columns\n                sortable: true # Enables sorting on the columns when the header is clicked\n                resizable: true # Enables resizing of column widths\n                filter: true # Enables filtering of the columns using agGrid's default filter\n              columnDefs: # Define all the columns\n                - headerName: Title # Display name\n                  field: title # The field name in the data\n                  minWidth: 350\n                  flex: 1 0 auto\n                - headerName: Year\n                  field: year\n                  width: 100\n                - headerName: Difference\n                  field: difference\n                  width: 160\n                  type: numericColumn # Setting this aligns the number on the right\n                  valueFormatter:\n                    _function: # Provide a fprmatter function to pretty render the data value.\n                      __intl.numberFormat:\n                        on:\n                          __args: 0.value\n                        params:\n                          options:\n                            minimumFractionDigits: 1 # Format the number with 1 decimal place\n                - headerName: Viewer Rating\n                  field: viewerRating\n                  width: 160\n                  type: numericColumn\n                  valueFormatter:\n                    _function:\n                      __intl.numberFormat:\n                        on:\n                          __args: 0.value\n                        params:\n                          options:\n                            maximumFractionDigits: 1\n                - headerName: Critic Rating\n                  field: criticRating\n                  width: 160\n                  type: numericColumn\n                  valueFormatter:\n                    _function:\n                      __intl.numberFormat:\n                        on:\n                          __args: 0.value\n                        params:\n                          options:\n                            maximumFractionDigits: 1\n                - headerName: Viewer Reviews\n                  field: viewerReviews\n                  width: 160\n                  type: numericColumn\n                  valueFormatter:\n                    _function:\n                      __intl.numberFormat:\n                        on:\n                          __args: 0.value\n                        params:\n                          options:\n                            maximumFractionDigits: 0\n                - headerName: Critic Reviews\n                  field: criticReviews\n                  width: 160\n                  type: numericColumn\n                  valueFormatter:\n                    _function:\n                      __intl.numberFormat:\n                        on:\n                          __args: 0.value\n                        params:\n                          options:\n                            maximumFractionDigits: 0\n  header:\n    blocks:\n      - id: affix\n        type: Affix\n        blocks:\n          - id: source_button\n            type: Button\n            properties:\n              icon: AiOutlineGithub\n              title: View App Source Code\n              type: default\n              shape: round\n            events:\n              onClick:\n                - id: link_repo\n                  type: Link\n                  params:\n                    url: https://github.com/lowdefy/lowdefy-example-reporting\n                    newTab: true\n```\nTask: Create a page referred in the main page that can let users fill in a survey.\nAnswer:\n```yaml\n# Define the survey page\nid: survey\ntype: Box\nstyle:\n  background: '#ababab'\nlayout:\n  contentAlign: center\nrequests:\n  - id: get_employees\n    type: GoogleSheetGetMany\n    connectionId: employee_sheet\n    properties:\n      # Filter all employees which has the role of \"Sales person\" in the data\n      filter:\n        role: Sales person\nevents:\n  onInitAsync:\n    # When the page initializes, the get_employee request is executed to fetch all \"Sales person\" employees.\n    - id: go_get_employees\n      type: Request\n      params: get_employees # Here we refer to the get_employees id.\nblocks:\n  # Add some very basic HTML to create a nice customer friendly company banner.\n  - id: logo\n    type: Html\n    style:\n      borderBottom: '0.3em solid #000'\n    properties:\n      html: '<div style=\"background: #fff; text-align: center; padding: 10px\"><img src=\"https://lowdefy-public.s3-eu-west-1.amazonaws.com/dunder_logo.jpg\" height=\"80px\"/></div>'\n  # Wrap our survey questioner to center it nicely for all screen sizes.\n  - id: content_box\n    type: Box\n    style:\n      maxWidth: 660\n      padding: 30px 30px 60px 30px\n      minHeight: 90vh\n      background: '#fff'\n    layout:\n      contentGutter: 20\n    blocks:\n      - id: title\n        type: Title\n        style:\n          textAlign: center\n          paddingTop: 20\n        properties:\n          content: How was your paper experience?\n          level: 1\n      - id: intro\n        type: Title\n        style:\n          textAlign: center\n          paddingTop: 20\n        properties:\n          content: Your input is highly valued here at Dunder Mifflin. Your feedback will mostly be used to improve our service to you.\n          level: 4\n      # The first input field will manage the name field in the page context state variable.\n      - id: name\n        type: TextInput\n        required: true # We indicate that some fields are required, later we will validate our input before submitting the data to the server.\n        properties:\n          title: Name & Surname\n          size: large\n      - id: company\n        type: TextInput\n        required: true\n        properties:\n          title: Company Name\n          size: large\n      - id: type\n        type: ButtonSelector\n        required: true\n        properties:\n          title: Type\n          size: large\n          options:\n            - Feedback\n            - Query\n            - Complaint\n      # The visible field is used to determine when a block should exist. As a block goes invisible, its field is also remove from the context state.\n      - id: sales_person\n        type: Selector\n        required: true\n        visible:\n          # These operators evaluate to `true` when the type field is selected and is not equal to the \"Feedback\" option.\n          _and:\n            - _not:\n                _eq:\n                  - _state: type\n                  - Feedback\n            - _state: type\n        properties:\n          title: Sales Person\n          size: large\n          options:\n            # The list of selector options are populated from the result of our get_employees request. Here we make use of the mql aggregate operator to modify our request response and sort according to label. The mql.aggregate operator is a client side implementation to run aggregations on client side data using an implementation of MongoDB's Aggregation language.\n            _mql.aggregate:\n              on:\n                _if_none: # Since we are fetching the `get_employees` with onInitAsync, `_request: get_employees` will be `null` until a request response is received. Checking for _if_none here handles the non-array type `on` input error until the data is returned be the request.\n                  - _request: get_employees\n                  - []\n              pipeline:\n                - $project:\n                    value: $name\n                    label: $name\n                - $sort:\n                    label: 1\n      - id: response\n        type: TextArea\n        required: true\n        visible: # The `response` field will only be visible in our webform if the `type` field has a value.\n          _if_none:\n            - _state: type\n            - false\n        properties:\n          title: Please tell us more\n          size: large\n      - id: satisfaction_title\n        type: Title\n        visible:\n          _if_none:\n            - _state: response\n            - false\n        style:\n          textAlign: center\n          paddingTop: 20\n        properties:\n          content: One last thing, based on your experience with us, how likely are you to recommend Dunder Mufflin Paper Company?\n          level: 4\n      - id: satisfaction\n        type: RatingSlider\n        visible:\n          _if_none:\n            - _state: response\n            - false\n        properties:\n          label:\n            disabled: true\n      - id: detractor_response_title\n        type: Title\n        visible:\n          # Show a different question based on the customer satisfaction rating.\n          _and:\n            - _lte:\n                - _state: satisfaction\n                - 7\n            - _if_none:\n                - _state: satisfaction\n                - false\n        style:\n          textAlign: center\n          paddingTop: 20\n        properties:\n          content:\n            # Some unfair logic to bias Dwight's scores, and also because Jim is probably up to no good.\n            _if:\n              test:\n                _eq:\n                  - _state: sales_person\n                  - Dwight Schrute\n              then: Wait! Be careful what you write... (Did Jim put you up to this?)\n              else: Oh no! We can do better!\n          level: 4\n      - id: detractor_response\n        type: TextArea\n        required: true\n        visible:\n          _and:\n            - _lte:\n                - _state: satisfaction\n                - 7\n            - _if_none:\n                - _state: satisfaction\n                - false\n        properties:\n          title: Please could your provide us with some further detail on your hesitancy to recommend us so we know what to work on.\n          placeholder: What could we have done better?\n          size: large\n          label:\n            colon: false\n      - id: promoter_response_title\n        type: Title\n        visible:\n          _gt:\n            - _state: satisfaction\n            - 7\n        style:\n          textAlign: center\n          paddingTop: 20\n        properties:\n          content: Your smile makes us smile!\n          level: 4\n      - id: promoter_response\n        type: TextArea\n        required: true\n        visible:\n          _gt:\n            - _state: satisfaction\n            - 7\n        properties:\n          title: Good news should be shared! Please can you let us know what made you smile so we can keep up the good work.\n          placeholder: What did you like?\n          size: large\n          label:\n            colon: false\n      - id: save\n        type: Button\n        visible:\n          _if_none:\n            - _state: satisfaction\n            - false\n        requests:\n          - id: save_survey\n            type: GoogleSheetAppendOne\n            connectionId: survey_sheet\n            payload:\n              row:\n                _state: true\n            properties:\n              row:\n                _payload: row\n        events:\n          # When the save button is clicked:\n          onClick:\n            - id: set_state # Add a timestamp variable to the context state.\n              type: SetState\n              params:\n                timestamp:\n                  _date: now\n            - id: validate # Then validate our webform input.\n              type: Validate\n            - id: call_save # Then call the `save_survey` request which will insert the new response record on the survey Google sheet.\n              type: Request\n              params: save_survey\n            - id: to_thank_you_page # Lastly redirect the customer to the `think-you` page.\n              type: Link\n              params:\n                pageId: thank-you\n        properties:\n          title: Submit\n          block: true\n          color: '#000'\n          icon: AiOutlineCheck\n          size: large\n      - id: header_bar\n        type: Html\n        visible:\n          _if_none:\n            - _state: satisfaction\n            - false\n        style:\n          fontSize: 10\n          textAlign: center\n          color: red\n        properties:\n          html: For this example, the submitted data will be public.\n  # Add a static footer to the bottom of the page to link to this repository.\n  - id: affix\n    type: Affix\n    properties:\n      offsetBottom: 0\n    blocks:\n      - id: bar_footer\n        type: Box\n        layout:\n          contentJustify: center\n        style:\n          background: '#fff'\n          padding: 5\n          borderTop: '0.3em solid #000'\n        blocks:\n          - id: link_repo\n            type: Anchor\n            layout:\n              shrink: 1\n            properties:\n              url: https://github.com/lowdefy/lowdefy-example-survey\n              title: âš¡ï¸ View the Lowdefy config for this app âš¡ï¸\n              newTab: true\n```\nTask: Create a page referred in the main page that can post new blog.\nAnswer:\n```yaml\nid: new-blog-post\ntype: PageHeaderMenu\nproperties:\n  title: New Blog Post # The title in the browser tab.\nlayout:\n  contentJustify: center # Center the contents of the page.\n\nrequests:\n  - id: insert_new_blog_post\n    type: MongoDBInsertOne\n    connectionId: blog_posts\n    properties:\n      doc:\n        blog_post_title:\n          _state: blog_post_title\n        blog_post_flair:\n          _state: blog_post_flair\n        blog_post_description:\n          _state: blog_post_description\n        blog_post_likes: 0\n        created_at:\n          _date: now\n        updated_at:\n          _date: now\n\nareas:\n  content:\n    blocks:\n      - id: content_card\n        type: Card\n        layout:\n          size: 800 # Set the size of the card so it does not fill the full screen.\n          contentGutter: 16 # Make a 16px gap between all blocks in this card.\n        blocks:\n          - id: page_heading\n            type: Title\n            properties:\n              content: New blog post # Change the title on the page.\n              level: 3 # Make the title a little smaller (an html `<h3>`).\n          - id: blog_post_title\n            type: TextInput\n            required: true\n            properties:\n              title: Title\n          - id: blog_post_flair\n            type: ButtonSelector\n            required: true\n            properties:\n              title: Flair\n              options: # Set the allowed options\n                - Informative\n                - Update\n                - Fact\n                - Funny\n                - Patch\n                - Feedback\n          - id: blog_post_description\n            type: TextArea\n            required: true\n            properties:\n              title: Description\n          - id: clear_button\n            type: Button\n            layout:\n              span: 12 # Set the size of the button (span 12 of 24 columns)\n            properties:\n              title: Clear\n              block: true # Make the button fill all the space available to it\n              type: default # Make the button a plain button\n              icon: ClearOutlined\n            events:\n              onClick:\n                - id: reset\n                  type: Reset\n          - id: submit_button\n            type: Button\n            layout:\n              span: 12\n            properties:\n              title: Submit\n              block: true\n              type: primary # Make the button a primary button\n              icon: SaveOutlined\n            events:\n              onClick:\n                - id: validate\n                  type: Validate\n                - id: insert_new_blog_post # Make a request to the database\n                  type: Request\n                  params: insert_new_blog_post\n                - id: reset # Reset the form once data has been submitted\n                  type: Reset\n                - id: link_to_blog_posts # Link back to the blog_posts page.\n                  type: Link\n                  params:\n                    pageId: home\n\n  footer:\n    blocks:\n      - id: footer\n        type: Paragraph\n        properties:\n          type: secondary\n          content: |\n            Made using Lowdefy\n        style:\n          text-align: center\n      - id: block_id\n        type: Icon\n        properties:\n          name: RobotOutlined\n        style:\n          text-align: center\n```\n'''\n\nSDE_KANBAN_ITEM_TO_LOWDEFY_DESCRIPTION_ASSUMPTION=\"\"\"Imagine you're a proficient Lowdefy expert and experienced frontend engineer. \nSummarize the four most significant pages derived from the Kanban board items. Subsequently, \nfurnish a detailed description of your approach to implementing these pages using Lowdefy, \nkeeping in mind that a maximum of four pages can be generated. User Interface (UI) Always elaberate more details as much as you can for each page in this section. Please include the layout, component, color theme, part of important text, font size, api request, page transition logic, business logic\nPages & Routing:\nHomepage: Primary landing page.\nAbout Page: Company or app info.\nProducts Page: Lists products/services.\nContact Page: Contact info & form.\nHomepage Layout:Header: App's title/logo and navigation.\nBanner: Featured image or slider.\nFeature Cards Row: Three cards with icon, title, and description.\nFooter: Links and secondary navigation.\nAbout Page Layout:Header: Consistent with Homepage.\nCompany Overview: Text about company.\nTeam Section: Team members' photos and bios.\nFooter: Consistent with Homepage.\nProducts Page Layout:Header: Consistent with Homepage.\nProduct List: Cards with image, title, description, and price.\nFilter & Sorting Options: Filter by categories or sort.\nFooter: Consistent with Homepage.\nContact Page Layout:Header: Consistent with Homepage.\nContact Form: Fields like Name, Email, Subject, and Message.\nLocation Map: Embedded map.\nFooter: Consistent with Homepage.\nCommon Components:Navigation Menu: In Header.\nCall to Action Buttons: Styled buttons for key actions.\"\"\"\n\nSDE_AI_TASKS_OUTPUT_TEMPLATE = \"\"\"\nBase on the kanban information help me generate task description follow this format.\n***New Page***\n{PAGE NAME}:\n   - Layout: {Page layout details}\n   - Components: {Page component details}\n   - Color Theme: {Color Theme details}\n   - Important Text: {Specific Text appears on the page}\n   - Font Size: {Font size details}\n   - API Request: {API Request details}.\n   - Page Transition Logic: {Page Transition Logic details}.\n   - Business Logic: {Business Logic Details}.\n\"\"\"\n\nSDE_FRONTEND_HOMEPAGE_ASSUMPTION = f\"\"\"\nFind and output a single page description of main page/homepage. \nOutput the exact page description word by word without any omission.\nDo not output the pages that are not related.\n\"\"\"\n\nSDE_SUMMARIZE_TASK_ASSUMPTION = f\"\"\"\nFind one word in the page name that best describes the page. Only output a single word.\nexample: homepage, about, product, contact\n\"\"\"\n\nSDE_SUMMARIZE_CODE_ASSUMPTION = f\"\"\"\nSummarize the functionality of the code in three words. Only output the summarized result.\n\"\"\"\n\n\n### SDE Tech Solution\nSDE_TECH_SOLUTION_ASSUMPTION = f\"\"\"Assume you are a principal engineer, and you are going to provide a tech solution for the requirement of project. I will\ngive you the project information, current code schema, related code, and customer requirements. Always base on the information I provide, \ndesign a tech solution for the project. You can use Mermaid diagram to show the workflow. And give the step by step instructions, actual code for the solution.\n\"\"\"\n\n### CUSTOM SKILL GENERATOR\nCUSTOM_GENERATE_PRINCIPLES = f'''Base on the inforamtion I provide, help me generate principles follow this format.\nPrinciple : [Principle  description]'''\n\nCUSTOM_GENERATE_LIST_SKILLS_OUTPUT_FORMAT = '''\nskill1:description &&skill2:description &&skill3:description &&skill4:description &&skill5:description&&\n'''\n\nCUSTOM_GENERATE_SKILL_JSON_ROLE_ASSUMPTION = f'''Base on the inforamtion I provide, help me generate individual json file for the skill. Only output json file'''\n\nCUSTOM_GENERATE_SKILL_JSON_OUTPUT_FORMAT = '''{\n    \"skill_name\": \"skill name\",\n    \"basic_description\": \"give a short description for this skill\",\n    \"instruction\": \"How to do what's the tenant, and what's the step by step instructions\",\n    \"qa_example\": \"give one or two, input output example\",\n    \"background_data_path\": \"\", (Don't need to change this)\n    \"model_name\": \"gpt-4\", (Don't need to change this)\n    \"input_method\": \"SkillIOParamCategory.PlainText\", (Don't need to change this)\n    \"output_method\": \"SkillIOParamCategory.PlainText\" (Don't need to change this)\n}'''\n\nSUMMARY_CODE_SUMMARY_README = '''Summary the readme file of the repo into 100 words. The readme file is as below:'''\nSUMMARY_PROJECT = '''Assume you are Senior Software engineer, help me summarize the project given the summary of all code files based on the following criteria:\nAlways use Class, Function, Variable naming, code logic, code path, hierarchy and your professional knowledge to summarize the project.\nWord Limit: Ensure the summary is less than 100 words in total. Be consise.\nProfessional Analysis Summary:\nProject's Purpose: Describe the primary objective or role of the project.\nProblem Addressed: Explain the specific problem or issue that the code solves.\nOutput format\n1. Project's Purpose:[Your description here]\n\n2. Problem Addressed:[Your description here]\n'''\nSUMMARY_CODE_SUMMARY_PYTHON = '''Summary the functions of python file into 100 words. Be concise. The python file is as below:'''\nSUMMARY_CODE_SUMMARY_SCHEMA = '''Format the schema of the repo. The schema is as below:'''\n\nDEFAULT_SYSTEM_MESSAGE = \"\"\"You are a helpful AI assistant.\nSolve tasks using your coding and language skills.\nIn the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\nSolve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\nWhen using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\nIf you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\nIf the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\nWhen you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\nReply \"TERMINATE\" in the end when everything is done.\n\"\"\"\n\nASSISTANT_SYSTEM_MESSAGE = \"\"\"Assume you are principle SDE, you will be an code expert to \n            give code plan, \n            code advise, \n            explain the code.\n            Please base on the Project Instruction, Code Schema,\n            Relatived code files, and Background I provide below and your professional relatived knowledge\n            to response to the Requirements. The requirements as follow:\"\"\"\nPYTHON_EXPERT_ASSUMPTION = \"Assume you are an Senior Software Engineer, I will give you template the code and user requirement instructions. You need to generate the code that satisfy the instructions based on template codes. You should only output the code block. \"\nPYTHON_DEPENDENCY_EXPERT_ASSUMPTION = \"Assume you are an Senior Software Engineer, I will give you the code, list all the object you don't have the source code or you don't have any knowledge about it. ONLY OUTPUT OBJECT NAME SEPERATE BY ,\"\nPYTHON_IMPORT_TEMPLATE = \"\"\"\nobject1,object2,objectx,objecty\n\"\"\"\nPYTHON_CORRECTION_EXPERT_ASSUMPTION = \"Assume you are an Senior Software Engineer, I will give you a target the code, and the dependency code in target code. Could you help me check if there are any mistakes in target python code based on the dependency codes. For exmaple, wrong functions, wrong parameters and grammar mistakes.\"\nPYTHON_CODE_BLOCK_TEMPLATE = \"Based on the provided code and its dependencies, please correct all the mistakes ans misuse of functions and paramters. You should only output the FINAL CODE BLOCK. IF NO MISTAKE PLEASE OUT PUT FINAL CODE AS WELL\"\n\n\n### V2 Prompt\nSDE_CHAT_ADVISOR_ASSUMPTION = f\"\"\"Assume you are software engineer, \nplease answer the question base on the code plan I provide. And always input the Markdown clean format \"\"\"\n\n### code chat Prompt\nSDE_CODE_CHAT_ASSUMPTION = f\"\"\"Assume you are a senior software engineer, \nTry to answer the user input base on Related Code Files.\nIf there is no related content then response base on your professional knowledge. \nAnd indicate there is no related content in the workplace.\"\"\"\n\nSDE_NOTION_CHAT_ASSUMPTION = f\"\"\"Assume you are a senior software engineer, \nanswer User In[ut] always base on Related Notion Files. If there is no related content\nthen response base on your professional knowledge. And indicate there is no related content in the workplace.\"\"\"\n\nCODE_SUMMARY_V2 = f\"\"\"\nAssume you are Senior Software engineer, help me summary code file based on the following criteria:\nAlways from Class, Function, Variable naming, code logic, and your professional knowledge to summary a code file.\nWord Limit: Ensure the summary is less than 100 words in total.\nProfessional Analysis Summary:\nCode's Purpose: Describe the primary objective or role of the code.\nProblem Addressed: Explain the specific problem or issue that the code solves.\nOutput format\n1. Code's Purpose:[Your description here]\n\n2. Problem Addressed:[Your description here]\n\n\"\"\"\n\n\nSCHEMA_DESIGN_PRINCIPLE = \"\"\"\n                NOTICE\n                Role: You are a professional database engineer; the main goal is to follow user requirement to design a NoSQL collection schema.\n                ATTENTION: Output format carefully referenced \"Format example\".\n                ATTENTION: Only design ONE collection. Be concise, only includes the necessary property fields in this collection. \n                ATTENTION: DO NOT MODIFY id part. Only change name in the curly brackets. The output must not contain curly brackets.\n                ATTENTION: Only select the property types from 'timestamp', 'True', 'False' or data['{Property_name}']. Do not improvise field types.\n                ***Format example***\n                -----\n                ```\n                ##Collection##: {name}\n                'id': str(uuid.uuid1()),\n                '{Property_name1}': data['{Property_name1}'],\n                '{Property_name2}': data['{Property_name2}'],\n                '{Property_name3}': False,\n                'createdAt': timestamp,\n                'updatedAt': timestamp,\n                ```\n                -----\n\"\"\"\n\nRELATED_CODE_FILTER_ASSUMPTION = f\"\"\"\nAssume you're a Senior Software Engineer tasked with creating a concise implementation plan for specific requirements, \nbased on existing code files and their summaries. Please identify which code files are related or as dependencies,\nwhich ones require modification, and which files are unrelated and won't be used. List all the files you'll need for dependencies and modifications, as well as those you won't use.\nATTENTION: When the input is start with the format [file name, file name...], the task is to search for the most similar file names within the 'Code Files' I provided below and then HAVE TO output ALL OF these files as the required files.\nATTENTION: ALWAYS OUTPUT AS FOLLOW FORMAT DON'T OUTPUT ANYOTHERS:\nRequired:[file name, file name...]\nUnuse:[file name, file name...]\"\"\"\n\ndef get_custom_skills_assumption_role_prompt(question_subject):\n    return f\"\"\"Assume you are the expert of {question_subject}. \nI want to know the list of top 5 essential actual hard skills (no softskill) for the {question_subject}. Can you please list them for me and use && sign to seperate them?\"\"\"\n\ndef build_gpt_prompt(role_assumption: str, output_format: str):\n    return f\"{role_assumption}\\n\\nAlways follow the Output format which is: {output_format}\"\n\ndef build_gpt_standard_prompt(role_assumption: str, description: str, output_format: str):\n    return f\"{role_assumption}\\n\\nThis task description: {description}\\n\\n Output format: {output_format}\"\n\ndef build_custom_skill_gpt_prompt(role_assumption: str, instruction: str, principles: str, few_shots: str):\n    return f'''{role_assumption}\\n\\n \n    Here are instruction, always response follow the instruction: {instruction}\\n\\n\n    Here are principles you need to always follow when give the response: {principles}\\n\\n \n    Here are the prompt and completion examples: {few_shots}\n    If no suitable content then response base on your professional knowledge. '''\n\ndef llama_v2_prompt(messages):\n    \"\"\"\n    Convert the messages in list of dictionary format to Llama2 compliant format.\n    \"\"\"\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n    BOS, EOS = \"<s>\", \"</s>\"\n    DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n\n    if messages[0][\"role\"] != \"system\":\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": DEFAULT_SYSTEM_PROMPT,\n            }\n        ] + messages\n    messages = [\n        {\n            \"role\": messages[1][\"role\"],\n            \"content\": B_SYS + messages[0][\"content\"] + E_SYS + messages[1][\"content\"],\n        }\n    ] + messages[2:]\n\n    messages_list = [\n        f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\n        for prompt, answer in zip(messages[::2], messages[1::2])\n    ]\n    messages_list.append(\n        f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\")\n\n    return \"\".join(messages_list)"}
{"type": "source_file", "path": "solidgpt/src/manager/blobmanager.py", "content": "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobProperties\nfrom solidgpt.src.configuration.configreader import ConfigReader\n\n\nclass AzureBlobManager:\n    def __init__(self, connection_string):\n        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n    def upload_blob(self, container_name, blob_name, data, overwrite=True):\n        blob_client = self.blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n        blob_client.upload_blob(data, overwrite=overwrite)\n\n    def download_blob(self, container_name, blob_name):\n        blob_client = self.blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n        return blob_client.download_blob().readall()\n\n    def list_blobs(self, container_name : str):\n        container_client = self.blob_service_client.get_container_client(container=container_name)\n        return [blob.name for blob in container_client.list_blobs()]\n\n    def delete_blob(self, container_name, blob_name):\n        blob_client = self.blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n        blob_client.delete_blob()\n\n    def clear_container(self, container_name):\n        container_client: ContainerClient = self.blob_service_client.get_container_client(container_name)\n        container_client.delete_blobs(*container_client.list_blobs())\n\n\n# Usage\nif __name__ == \"__main__\":\n    CONNECTION_STRING = ConfigReader().get_property(\"azure_blob_connection_string\")\n    manager = AzureBlobManager(CONNECTION_STRING)\n\n    # Sample Usage\n    manager.upload_blob(\"repos\", \"sample.txt\", \"This is a sample text.\", overwrite=True)\n    manager.upload_blob(\"repos\", \"sample1.txt\", \"This is another sample text.\", overwrite=True)\n    print(manager.list_blobs(\"repos\"))\n    sample_text = manager.download_blob(\"repos\", \"sample.txt\").decode(\"utf-8\")\n    # manager.clear_container(\"repos\")\n    print(sample_text)\n"}
{"type": "source_file", "path": "solidgpt/src/workgraph/displayresult.py", "content": "class DisplayResult:\r\n    result: str = \"\"\r\n    custom_result = None\r\n\r\n    def __init__(self):\r\n        self.result = \"No result\"\r\n\r\n    def set_result(self, s):\r\n        self.result = s\r\n\r\n    def get_result(self):\r\n        return self.result\r\n\r\n    def set_custom_result(self, data):\r\n        self.custom_result = data\r\n\r\n    def get_custom_result(self):\r\n        return self.custom_result\r\n"}
{"type": "source_file", "path": "solidgpt/definitions.py", "content": "\nimport os\nimport logging\n\nROOT_DIR = os.path.dirname(os.path.abspath(__file__)) # This is your Project Root\nSRC_DIR = os.path.join(ROOT_DIR, \"src\")\nTEST_DIR = os.path.join(ROOT_DIR, \"test\")\nLOCAL_STORAGE_DIR = os.path.join(ROOT_DIR, \"../localstorage\")\nREPO_STORAGE_DIR = os.path.join(LOCAL_STORAGE_DIR, \"repo\")\nLOCAL_STORAGE_WORKSPACE_DIR = os.path.join(LOCAL_STORAGE_DIR, \"workspace\")\nLOCAL_STORAGE_OUTPUT_DIR = os.path.join(LOCAL_STORAGE_WORKSPACE_DIR, \"out0325\") # You can change this to your own output dir\nTEST_SKILL_WORKSPACE = os.path.join(TEST_DIR, \"workskill\", \"skills\", \"workspace\")\nEMBEDDING_BLOB_CONTAINER = \"embedding1002\"\nlogging.basicConfig(level=logging.INFO)\nSUPPORT_EXTENSION = [\".py\", \".java\", \".js\", \".ts\", \"tsx\", \"css\", \"jsx\", \"go\", \".cs\", \".cpp\", \".rb\", \".rs\", \".swift\", \".m\", \".h\", \".kt\", \".php\", \".scala\", \".dart\", \".R\", \".clj\", \".ex\", \".hs\", \".jl\", \".lua\", \".ml\", \".pl\", \".sql\", \".vb\"]"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-flask-dynamodb-api/app.py", "content": "import os\n\nimport boto3\nfrom flask import Flask, jsonify, make_response, request\n\napp = Flask(__name__)\n\n\ndynamodb_client = boto3.client('dynamodb')\n\nif os.environ.get('IS_OFFLINE'):\n    dynamodb_client = boto3.client(\n        'dynamodb', region_name='localhost', endpoint_url='http://localhost:8000'\n    )\n\n\nUSERS_TABLE = os.environ['USERS_TABLE']\n\n\n@app.route('/users/<string:user_id>')\ndef get_user(user_id):\n    result = dynamodb_client.get_item(\n        TableName=USERS_TABLE, Key={'userId': {'S': user_id}}\n    )\n    item = result.get('Item')\n    if not item:\n        return jsonify({'error': 'Could not find user with provided \"userId\"'}), 404\n\n    return jsonify(\n        {'userId': item.get('userId').get('S'), 'name': item.get('name').get('S')}\n    )\n\n\n@app.route('/users', methods=['POST'])\ndef create_user():\n    user_id = request.json.get('userId')\n    name = request.json.get('name')\n    if not user_id or not name:\n        return jsonify({'error': 'Please provide both \"userId\" and \"name\"'}), 400\n\n    dynamodb_client.put_item(\n        TableName=USERS_TABLE, Item={'userId': {'S': user_id}, 'name': {'S': name}}\n    )\n\n    return jsonify({'userId': user_id, 'name': name})\n\n\n@app.errorhandler(404)\ndef resource_not_found(e):\n    return make_response(jsonify(error='Not found!'), 404)\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/list.py", "content": "import json\nimport os\n\nfrom todos import decimalencoder\nimport boto3\ndynamodb = boto3.resource('dynamodb')\n\n\ndef list(event, context):\n    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n\n    # fetch all todos from the database\n    result = table.scan()\n\n    # create a response\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(result['Items'], cls=decimalencoder.DecimalEncoder)\n    }\n\n    return response\n"}
{"type": "source_file", "path": "solidgpt/src/manager/gptmanager.py", "content": "import openai\nfrom solidgpt.src.configuration.configreader import ConfigReader\n\nclass GPTManager:\n\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(GPTManager, cls).__new__(cls)\n            # You can initialize the instance attributes here\n        return cls._instance\n    \n    def __init__(self, if_show_reply = False):\n        # read api key from config file\n        global_openai_key = ConfigReader().get_property(\"openai_api_key\")\n        if global_openai_key is not None and global_openai_key != \"\":\n            openai.api_key = global_openai_key\n        self.__default_model = ConfigReader().get_property(\"openai_model\")\n        self.gpt_models_container = {}\n        self.if_show_reply = if_show_reply\n\n    def create_model(self, prompt, gpt_model_label, temperature=1.0, model = None):\n        if model is None:\n            model = self.__default_model\n        gpt_model = GPTModel(prompt, self.__default_model, self.if_show_reply, temperature)\n        self.gpt_models_container[gpt_model_label] = gpt_model\n        return gpt_model\n    \n    def create_and_chat_with_model(self, prompt, gpt_model_label, input_message, temperature=0.1, model=None, is_stream = False):\n        gpt_model = self.create_model(prompt, gpt_model_label, temperature, model)\n        return gpt_model.chat_with_model(input_message, is_stream=is_stream)\n\n    def get_gpt_model(self, gpt_model_label):\n        return self.completions_container[gpt_model_label]\n    \n    def remove_gpt_model(self, gpt_model_label):\n        self.completions_container.pop(gpt_model_label)\n    \nclass GPTModel:\n    def __init__(self, prompt, model, if_show_reply = True, temperature = 0.1):\n        self.prompt = prompt\n        self.model = model\n        self.messages = [{\"role\": \"system\", \"content\": self.prompt}]\n        self.last_reply = None\n        self.if_show_reply = if_show_reply\n        self.temperature = temperature\n\n    def chat_with_model(self, input_message, is_stream=False):\n        self.messages.append({\"role\": \"user\", \"content\": input_message})\n        if not is_stream:\n            self._run_model()\n        else:\n            return self._run_model_stream()\n        return self.last_reply\n\n    def _run_model(self):\n        chat = openai.ChatCompletion.create(\n            model=self.model,\n            messages=self.messages,\n            temperature=self.temperature,\n        )\n        reply = chat.choices[0].message.content\n        if self.if_show_reply:\n            print(f\"ChatGPT: {reply}\")\n        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n        self.last_reply = reply\n\n    def _run_model_stream(self):\n        stream = openai.ChatCompletion.create(\n            model=self.model,\n            messages=self.messages,\n            temperature=self.temperature,\n            stream=True,\n        )\n        return stream\n\n\n    def add_background(self, background_message):\n        self.messages.append({\"role\": \"assistant\", \"content\": background_message})"}
{"type": "source_file", "path": "solidgpt/src/workgraph/graph_helper.py", "content": "\nfrom enum import Enum\nimport uuid\n\n\nclass GraphType(Enum):\n    OnboardingGraph = 1\n    WritePRDGraph = 2\n    ProvideTechSolutionGraph = 3\n    RepoChat = 4\n\nclass GraphStatus(Enum):\n    NotStarted = 0\n    Running = 1\n    Completed = 2\n    Failed = 3"}
{"type": "source_file", "path": "solidgpt/src/diy/custom/customizeskillmanager.py", "content": "import logging\nimport os\nfrom solidgpt.definitions import *\nfrom solidgpt.src.diy.custom.customizedskilldefinition import CustomizedSkillDefinition\nfrom solidgpt.src.util.util import load_from_json\nfrom solidgpt.src.workskill.skills.custom_skill import CustomSkill\n\nclass CustomizeSkillManager:\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            # You can initialize the instance attributes here\n        return cls._instance\n    \n    def __init__(self, custom_skill_definition_folder_path=os.path.join(LOCAL_STORAGE_DIR, 'customizedskilldefinition')):\n        self.customized_skills_map: dict[str, CustomSkill] = {}\n        self.custom_skill_definition_folder_path = custom_skill_definition_folder_path\n        self.__load_customized_skills()\n\n    def get_customzied_skill(self, skill_name: str)-> CustomSkill:\n        skill = self.customized_skills_map.get(skill_name)\n        if skill is None:\n            logging.error(f\"Error, Customized skill {skill_name} is not found.\")\n        return skill\n    \n    def __load_customized_skills(self):\n        # load all of the customized skills josn files\n        skill_definitions = self.__load_customzied_skill_from_folder()\n        for skill_definition in skill_definitions:\n            skill = self.__load_customized_skill(skill_definition)\n            self.customized_skills_map[skill_definition.skill_name] = skill\n        return\n    \n    def __load_customized_skill(self, skill_definition: CustomizedSkillDefinition)-> CustomSkill:\n        # load all of the customized skills josn files\n        return CustomSkill(skill_definition)\n    \n    def __load_customzied_skill_from_folder(self):\n        # Get a list of all files in the folder\n        file_list = os.listdir(self.custom_skill_definition_folder_path)\n\n        # Filter JSON files from the list\n        json_files = [file for file in file_list if file.endswith('.json')]\n        logging.info(f\"Found {json_files} json files in {self.custom_skill_definition_folder_path}\")\n        \n        # Load JSON data from each JSON file\n        customized_skills_definition: list(CustomizedSkillDefinition)= []\n        for json_file in json_files:\n            customized_skills_definition.append(CustomizedSkillDefinition(**load_from_json(os.path.join(self.custom_skill_definition_folder_path, json_file))))\n        return customized_skills_definition"}
{"type": "source_file", "path": "solidgpt/src/api/celery_tasks.py", "content": "import base64\nimport threading\nimport uuid\n\nfrom celery import Celery\nfrom celery.apps.worker import Worker\n\nfrom pathlib import Path\nfrom solidgpt.src.configuration.configreader import ConfigReader\n\nfrom solidgpt.src.constants import *\nfrom solidgpt.src.util.util import *\nfrom solidgpt.src.workgraph.graph import *\nfrom solidgpt.src.manager.initializer import *\nimport redis\nimport os\nimport subprocess\n\nuse_redis = False\n\nif use_redis:\n    app = Celery('celery_tasks',\n                 BROKER_URL='redis://localhost:6379/0',  # Using Redis as the broker\n                 CELERY_RESULT_BACKEND='redis://localhost:6379/1'\n                 )\nelse:\n    app = Celery('celery_tasks',\n                 broker='sqla+sqlite:///celerydb.sqlite',\n                 CELERY_RESULT_BACKEND='db+sqlite:///results.db'\n                 )\n\napp.autodiscover_tasks(['solidgpt.src.api'])\nif use_redis:\n    redis_instance = redis.Redis()\nInitializer()\n\n\ndef start_celery():\n    app.worker_main(['worker', '--loglevel=info', '--concurrency=1', '--pool=threads'])\n\n\ncelery_thread = threading.Thread(target=start_celery)\ncelery_thread.daemon = True  # Set the thread as a daemon thread\ncelery_thread.start()\n\n\n@app.task\ndef hello_world():\n    logging.info(\"hello repo!\")\n    print(\"hello repo!\")\n    return \"hello world\"\n\n\n@app.task(bind=True)\ndef celery_task_repo_chat_graph_v2(requirement, session_id):\n    logging.info(\"celery task: repo chat graph\")\n    openai.api_key = ConfigReader.get_property('openai_api_key')\n    g = build_repo_chat_graph_v2(requirement, session_id)\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n@app.task(bind=True)\ndef celery_task_onboard_repo_graph_v4(self, openai_key, session_id, base_path):\n    logging.info(\"celery task: tech solution graph\")\n    openai.api_key = openai_key\n    # logging.info(f\"base path:{base_path}\")\n    g = build_onboarding_graph_v4(session_id=session_id, base_path=base_path)\n\n    def update_progress(current, total):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current': current, 'total': total}\n        )\n\n    g.callback_map[SKILL_NAME_VSCODE_EMBED2] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    extra_payload = g.display_result.get_custom_result()\n    return [text_result, extra_payload]\n\n\n@app.task(bind=True)\ndef celery_task_code_plan_graph_v4(self, openai_key, requirement, graph_id, summary):\n    logging.info(\"celery task: tech solution graph\")\n    openai.api_key = openai_key\n    g = build_code_plan_graph_v4(requirement, graph_id, summary)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_CREATE_CODE_PLAN] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n@app.task(bind=True)\ndef celery_task_code_solution_graph_v3(self, openai_key, requirement, code_plan, graph_id):\n    logging.info(\"celery task: tech solution graph\")\n    openai.api_key = openai_key\n    g = build_code_solution_graph_v3(requirement, graph_id, code_plan)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_CREATE_CODE_SOLUTION] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n# Severless actions\n@app.task(bind=True)\ndef celery_task_serverless_deploy(self, yml_path: str, aws_key_id: str, aws_access_key: str):\n    logging.info(\"celery task: serverless deploy\")\n\n    # Extract the directory from the file path\n    directory = Path(yml_path).parent\n\n    # Set AWS credentials\n    env = os.environ.copy()\n    env['AWS_ACCESS_KEY_ID'] = aws_key_id\n    env['AWS_SECRET_ACCESS_KEY'] = aws_access_key\n\n    # Define the command to run\n    command = [\"serverless\", \"deploy\", \"--config\", yml_path, \"--verbose\"]\n    self.update_state(\n        state='PROGRESS',\n        meta={}\n    )\n\n    # Execute the command\n    process = subprocess.run(command, shell=True, capture_output=True, text=True, cwd=directory, env=env)\n\n    # Check if the command was successful\n    if process.returncode == 0:\n        print(\"Deployment successful.\")\n        print(\"Output:\\n\", process.stdout)\n        return {'status': 'Succeeded', 'message': 'Deployment successful.', 'output': process.stdout}\n    else:\n        print(\"Deployment failed.\")\n        print(\"Error:\\n\", process.stdout)\n        return {'status': 'Failed', 'message': 'Deployment failed.', 'output': process.stdout}\n\n\n@app.task(bind=True)\ndef celery_task_serverless_remove(self, yml_path: str, aws_key_id: str, aws_access_key: str):\n    logging.info(\"celery task: serverless remove\")\n\n    # Extract the directory from the file path\n    directory = Path(yml_path).parent\n\n    # Set AWS credentials\n    env = os.environ.copy()\n    env['AWS_ACCESS_KEY_ID'] = aws_key_id\n    env['AWS_SECRET_ACCESS_KEY'] = aws_access_key\n\n    # Define the command to run\n    command = [\"serverless\", \"remove\", \"--config\", yml_path, \"--verbose\"]\n    self.update_state(\n        state='PROGRESS',\n        meta={}\n    )\n\n    # Execute the command\n    process = subprocess.run(command, shell=True, capture_output=True, text=True, cwd=directory, env=env)\n\n    # Check if the command was successful\n    if process.returncode == 0:\n        print(\"Removal successful.\")\n        print(\"Output:\\n\", process.stdout)\n        return {'status': 'Succeeded', 'message': 'Removal successful.', 'output': process.stdout}\n    else:\n        print(\"Removal failed.\")\n        print(\"Error:\\n\", process.stdout)\n        return {'status': 'Failed', 'message': 'Removal failed.', 'output': process.stdout}\n\n\n@app.task(bind=True)\ndef celery_task_select_template(self, openai_key, requirement, graph_id):\n    logging.info(\"celery task: select_template\")\n    openai.api_key = openai_key\n    g = build_select_template_graph(requirement, graph_id)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_SELECT_TEMPLATE] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n@app.task(bind=True)\ndef celery_task_http_solution(self, openai_key, requirement, graph_id):\n    logging.info(\"celery task: http solution\")\n    openai.api_key = openai_key\n    g = build_http_solution_graph(requirement, graph_id)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_HTTP_SOLUTION] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n@app.task(bind=True)\ndef celery_task_notion_embed(self, openai_key, onboarding_id, workspace_token, top_level_page_id):\n    logging.info(\"celery task: notion embed\")\n    openai.api_key = openai_key\n    g = build_notion_graph(onboarding_id=onboarding_id, workspace_token=workspace_token, page_id=top_level_page_id )\n    def update_progress(current_status):\n        self.update_state(\n            state='PROGRESS',\n            meta={'status': current_status}\n        )\n    g.callback_map[SKILL_NAME_NOTION_EMBED] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    extra_payload = g.display_result.get_custom_result()\n    return [text_result, extra_payload]\n\n@app.task(bind=True)\ndef celery_task_code_chat(self, openai_key, requirement, graph_id, scope):\n    logging.info(\"celery task: code chat\")\n    openai.api_key = openai_key\n    g = build_code_chat_graph(requirement, graph_id, scope)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_CODE_CHAT] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n\n\n@app.task(bind=True)\ndef celery_task_notion_chat(self, openai_key, requirement, graph_id, scope):\n    logging.info(\"celery task: notion chat\")\n    openai.api_key = openai_key\n    g = build_notion_chat_graph(requirement, graph_id, scope)\n\n    def update_progress(current_content):\n        self.update_state(\n            state='PROGRESS',\n            meta={'current_content': current_content}\n        )\n\n    g.callback_map[SKILL_NAME_NOTION_CHAT] = update_progress\n    g.init_node_dependencies()\n    g.execute()\n    text_result = g.display_result.get_result()\n    return text_result\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/get.py", "content": "import os\nimport json\n\nfrom todos import decimalencoder\nimport boto3\ndynamodb = boto3.resource('dynamodb')\n\n\ndef get(event, context):\n    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n\n    # fetch todo from the database\n    result = table.get_item(\n        Key={\n            'id': event['pathParameters']['id']\n        }\n    )\n\n    # create a response\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(result['Item'],\n                           cls=decimalencoder.DecimalEncoder)\n    }\n\n    return response\n"}
{"type": "source_file", "path": "solidgpt/src/imports.py", "content": "from solidgpt.src.workskill.skills.create_kanban import CreateKanBan\n"}
{"type": "source_file", "path": "solidgpt/src/tools/lowdefy/validator/yaml_validator.py", "content": "import os\nimport openai\nimport pandas as pd\nimport numpy as np\nfrom numpy.linalg import norm\nfrom solidgpt.src.configuration.configreader import ConfigReader\nfrom solidgpt.definitions import ROOT_DIR\n\n\nclass YAMLValidator:\n    def __init__(self, yaml_str: str, filename: str, subpages: list):\n        self.yaml = yaml_str\n        self.filename = filename\n        self.subpages = subpages\n        self.yaml_list = self.yaml.split(\"\\n\")\n        self.homepage_id = None\n        self.container_df = pd.read_csv(\n            os.path.join(ROOT_DIR, \"src\", \"tools\", \"lowdefy\", \"embedding\", \"container_block_embedding.csv\"))\n        self.input_df = pd.read_csv(\n            os.path.join(ROOT_DIR, \"src\", \"tools\", \"lowdefy\", \"embedding\", \"input_block_embedding.csv\"))\n        self.display_df = pd.read_csv(\n            os.path.join(ROOT_DIR, \"src\", \"tools\", \"lowdefy\", \"embedding\", \"display_block_embedding.csv\"))\n        self.all_embedding_df = pd.concat([self.container_df, self.input_df, self.display_df], axis=1)\n        openai.api_key = ConfigReader().get_property(\"openai_api_key\")\n\n    def validate(self) -> str:\n        \"\"\"\n        Check and convert the GPT created yaml file string to valid lowdefy yaml file string\n        :return: Converted valid lowdefy yaml file string\n        \"\"\"\n        self.verify_block_type()\n        self.remove_keys(\"events\")\n        self.remove_keys(\"requests\")\n        self.verify_duplicate_keys()\n        if len(self.subpages) > 0 and self.filename == \"lowdefy\":\n            self.verify_reference(self.subpages)\n            self.verify_menu(self.subpages)\n            self.verify_duplicate_pages()\n        return \"\\n\".join(self.yaml_list)\n\n    def verify_block_type(self):\n        \"\"\"\n        Using embedding to convert random block types to valid lowdefy block\n        :return: Converted yaml file string\n        \"\"\"\n        idx = 0\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            if key == \"id\":\n                self.yaml_list[idx] = f\"{key}: {self.filename}\"\n            elif key.strip() == \"type\":\n                query_type = tokens[1].strip().split(\" \")[0]\n                all_types = self.all_embedding_df.columns.values.tolist()\n                cache = {}\n                if query_type not in all_types:\n                    if query_type not in cache:\n                        query_type_embedding = openai.Embedding.create(\n                            model=\"text-embedding-ada-002\",\n                            input=query_type)[\"data\"][0][\"embedding\"]\n                        score, valid_type = YAMLValidator.find_best_type(query_type_embedding, self.all_embedding_df)\n                        cache[query_type] = valid_type\n                    else:\n                        valid_type = cache[query_type]\n                    self.yaml_list[idx] = f\"{key}: {valid_type}\"\n            idx += 1\n        return\n\n    def remove_blocks(self, key, idx):\n        indentation = key.rfind(\" \") if \"-\" not in key else key.rfind(\"-\") - 1\n        next_indentation = float(\"inf\")\n        while idx < len(self.yaml_list) and next_indentation > indentation:\n            self.yaml_list.pop(idx)\n            if idx >= len(self.yaml_list):\n                break\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            next_indentation = key.rfind(\" \") if \"-\" not in key else key.rfind(\"-\") - 1\n\n    def remove_keys(self, query_key):\n        idx = 0\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            if key.strip() == query_key:\n                self.remove_blocks(key, idx)\n            idx += 1\n        return\n\n    def verify_duplicate_pages(self):\n        idx = 0\n        page_flag = False\n        page_indentation = float(\"inf\")\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            indentation = key.rfind(\" \") if \"-\" not in key else key.rfind(\"-\") - 1\n            if page_flag and indentation <= page_indentation:\n                page_flag = False\n            if key.strip() == \"pages\":\n                page_flag = True\n                page_indentation = indentation\n            if len(tokens) > 0:\n                # print(key == \"  - id\")\n                if page_flag and key == \"  - id\" and tokens[1].strip().split(\" \")[0] in self.subpages:\n                    self.remove_blocks(key, idx)\n                    idx -= 1\n            idx += 1\n        return\n\n    def verify_duplicate_keys(self):\n        seen_keys = set()\n        cur_path_list = []\n        idx = 0\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            if line == \"\":\n                self.yaml_list.pop(idx)\n                continue\n            tokens = line.split(\":\")\n            key = tokens[0]\n            indentation = key.rfind(\" \") if \"-\" not in key else key.rfind(\"-\") - 1\n            indent_level = indentation // 2 if indentation >= 0 else 0\n            if len(cur_path_list) <= indent_level:\n                cur_path_list.append(line.strip())\n            else:\n                while len(cur_path_list) > indent_level:\n                    cur_path_list.pop()\n                cur_path_list.append(line.strip())\n            cur_path = tuple(cur_path_list)\n            if cur_path in seen_keys:\n                self.remove_blocks(key, idx)\n            seen_keys.add(cur_path)\n            idx += 1\n        return\n\n    def verify_menu(self, page_list: list[str]):\n        idx = 0\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            if key.strip() == \"menus\":\n                self.remove_blocks(key, idx)\n            idx += 1\n\n        menu_list = [\"menus:\", \"  - id: default\", \"    links:\"]\n        page_list.insert(0, self.homepage_id)\n        for page in page_list:\n            cur_list = [f\"        - id: {page}\", \"          type: MenuLink\", \"          properties:\",\n                        f\"            title: {page.capitalize()}\", f\"          pageId: {page}\"]\n            menu_list.extend(cur_list)\n        self.yaml_list.extend(menu_list)\n        return\n\n    def verify_reference(self, page_list: list[str]):\n        ref_list = [f\"  - _ref: {page_name}.yaml\" for page_name in page_list]\n        idx = 0\n        flag = False\n        while idx < len(self.yaml_list):\n            line = self.yaml_list[idx]\n            tokens = line.split(\":\")\n            key = tokens[0]\n            if key == \"pages\":\n                self.yaml_list[idx + 1:1] = ref_list\n                flag = True\n            if flag and key == \"  - id\":\n                self.homepage_id = tokens[1].strip().split(\" \")[0]\n                break\n            idx += 1\n        return\n\n    @staticmethod\n    def vector_similarity(x: list[float], y: list[float]) -> float:\n        \"\"\"\n        Compute cosine similarity between two vectors.\n        :param x: Vector one\n        :param y: Vector two\n        :return: Cosine similarity between input vectors\n        \"\"\"\n        return np.dot(np.array(x), np.array(y)) / (norm(np.array(x)) * norm(np.array(y)))\n\n    @staticmethod\n    def find_best_type(query_key, embedding_df) -> tuple[float, str]:\n        \"\"\"\n        Find the value whose embedding vector has the highest cosine similarity with query\n        :param query_key: The embedding vector of query\n        :param embedding_df: Dataframe that stores all the embedding vectors\n        :return:\n        \"\"\"\n        best_type = (0, \"\")\n        for col_name in embedding_df.columns.tolist():\n            section_embedding = embedding_df[col_name].values.tolist()\n            cur_score = YAMLValidator.vector_similarity(query_key, section_embedding)\n\n            if cur_score > best_type[0]:\n                best_type = cur_score, col_name\n        return best_type\n\n    @staticmethod\n    def parse(org_str: str) -> str:\n        \"\"\"\n        Given the original output string generated from LLM, parse the yaml file provided in the output.\n        :param org_str: original string from LLM\n        :return: the string of yaml file\n        \"\"\"\n        reading_state = 0\n        yaml_lines = []\n        for line in org_str.split(\"\\n\"):\n            if line[:3] == \"```\" and reading_state == 0:\n                reading_state = 1\n                continue\n            elif line[:3] == \"```\":\n                break\n            if reading_state == 1:\n                yaml_lines.append(line)\n        ret = \"\\n\".join(yaml_lines)\n        return ret\n"}
{"type": "source_file", "path": "solidgpt/src/diy/llama2_diy_finetune/dataloader.py", "content": "from datasets import load_dataset\n\nclass ModelDataLoader:\n    def __init__(self, train_dataset_path, valid_dataset_path, system_message=None):\n        self.train_dataset_path = train_dataset_path\n        self.valid_dataset_path = valid_dataset_path\n        self.system_message = system_message\n\n    def load_and_preprocess_datasets(self):\n        train_dataset = load_dataset('json', data_files=self.train_dataset_path, split=\"train\")\n        valid_dataset = load_dataset('json', data_files=self.valid_dataset_path, split=\"train\")\n\n# Define a default system message if not provided\n        if self.system_message is None:\n            self.system_message = \"The system message is: `Given a function description, you will generate the Lowdefy YAML file to configure a website. The generated YAML file should follow the general Lowdefy indentation format.`. Feel free to re-run this cell if you want a better result.\"\n\n        train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{self.system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['completion'])]}, batched=True)\n        valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{self.system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['completion'])]}, batched=True)\n\n        return train_dataset_mapped, valid_dataset_mapped\n\n"}
{"type": "source_file", "path": "solidgpt/src/diy/llama2_diy_finetune/data_generator.py", "content": "import os\nimport json\nimport tiktoken\nfrom solidgpt.src.manager.gptmanager import GPTModel  # Import GPTModel from the correct location\nfrom solidgpt.src.configuration.configreader import ConfigReader\n\n# Set your OpenAI API key here\nopenai.api_key = ConfigReader().get_property(\"openai_api_key\")\n\n\nMAX_TOKENS_LIMIT = 16000\n\nclass GenerateDataset:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        prompt = \"Explain the content of a Lowdefy YAML file.\"  # Define the prompt\n        self.gpt_model = GPTModel(prompt, model_name)\n\n    def _num_tokens_from_string(self, string: str, encoding_name: str) -> int:\n        encoding = tiktoken.encoding_for_model(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def get_files_with_filter(self, root_path: str, regex_filter: str) -> list:\n        filtered_files = []\n        for dirpath, dirnames, filenames in os.walk(root_path):\n            for filename in filenames:\n                if filename.lower().endswith(regex_filter.lower()):\n                    filtered_files.append(os.path.join(dirpath, filename))\n        return filtered_files\n\n    def generate_dataset(self, files_list):\n        dataset = []\n        for file_path in files_list:\n            with open(file_path, \"r\") as f:\n                yaml_content = f.read()\n\n            yaml_prompt = f\"Explain the following lowdefy YAML file's content:\\n{yaml_content}\"\n            try:\n                yaml_prompt_tokens = self._num_tokens_from_string(yaml_prompt, self.model_name)\n                if yaml_prompt_tokens <= MAX_TOKENS_LIMIT:\n                    yaml_explanation = self.gpt_model.chat_with_model(yaml_prompt)\n                    dataset.append({\"prompt\": yaml_explanation, \"completion\": yaml_content})\n                else:\n                    print(f\"Skipping {file_path} as it exceeds the token limit.\")\n            except openai.error.OpenAIError as e:\n                print(f\"Error generating YAML explanation: {e}\")\n                continue\n        return dataset\n\ndef main():\n    model_name = \"gpt-3.5-turbo-16k\"  # Replace with your desired model name\n    root_folder = input(\"Enter the root folder path containing sub-folders with YAML files: \")\n\n    dataset_generator = GenerateDataset(model_name)\n\n    yaml_files = dataset_generator.get_files_with_filter(root_folder, \".yaml\")\n    dataset = dataset_generator.generate_dataset(yaml_files)\n\n    output_jsonl_file = os.path.join(root_folder, 'localstorage', 'train.jsonl')\n    with open(output_jsonl_file, \"w\") as f:\n        for item in dataset:\n            json.dump(item, f)\n            f.write(\"\\n\")\n\n    print(f\"Dataset saved to {output_jsonl_file}\")\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"type": "source_file", "path": "solidgpt/src/diy/chatgpt_diy_finetune/chatgpt_train.py", "content": "import openai\nimport logging\nimport asyncio\n\nclass GPTFinetune:\n    def __init__(self, model_name, suffix_name, training_file_path):\n        self.training_file_id = None\n        self.model_name = model_name\n        self.suffix_name = suffix_name\n        self.job_id = None\n        self.training_file_path = training_file_path\n        openai.api_key = ConfigReader().get_property(\"openai_api_key\")\n\n    async def start_fine_tuning(self):\n        await self.__upload_training_file()\n        model_details = openai.FineTuningJob.create(\n            training_file=self.training_file_id,\n            model=self.model_name,\n            suffix=self.suffix_name\n        )\n        self.job_id = model_details[\"id\"]\n        logging.info(\"Fine-tuning job started: %s\", self.job_id)\n\n    async def __upload_training_file(self):\n        training_response = await openai.File.create(\n            file=open(self.training_file_path, \"rb\"), purpose=\"fine-tune\"\n        )\n        self.training_file_id = training_response[\"id\"]\n        logging.info(\"Training file ID is ready: %s\", self.training_file_id)\n\n    async def get_fine_tuning_status(self):\n        response = await openai.FineTuningJob.retrieve(self.job_id)\n        return response[\"status\"]\n\n# Sample\nif __name__ == \"__main__\":\n    training_file_path = input(\"Enter the path to train.jsonl: \")\n    model_name = \"gpt-3.5-turbo\"\n    suffix_name = \"Quantchat\"\n    finetune_instance = GPTFinetune(model_name, suffix_name, training_file_path)\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(finetune_instance.start_fine_tuning())\n\n    async def wait_for_finetuning_complete():\n        while True:\n            status = await finetune_instance.get_fine_tuning_status()\n            logging.info(\"Fine-tuning status: %s\", status)\n            if status == \"succeeded\" or status == \"failed\":\n                break\n            await asyncio.sleep(60)\n\n    loop.run_until_complete(wait_for_finetuning_complete())\n"}
{"type": "source_file", "path": "solidgpt/src/util/util.py", "content": "import json\nimport os\nimport sys\nimport openai\nimport uuid\nfrom qdrant_client import QdrantClient\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom qdrant_client.http.models import PointStruct, Distance, VectorParams\nfrom solidgpt.definitions import *\n\n\ndef save_to_json(data, filename=\"data.json\"):\n    create_directories_if_not_exist(filename)\n    # Save data to a JSON file\n    with open(filename, \"w\") as json_file:\n        print(json.dump(data, json_file, indent=4))\n\n\ndef load_from_json(filename=\"data.json\"):\n    # Load data from a JSON file\n    with open(filename, \"r\") as json_file:\n        loaded_data = json.load(json_file)\n    return loaded_data\n\n\ndef save_to_md(filename, content: str, path = \"\") -> str:\n    create_directories_if_not_exist(filename)\n    path = os.path.join(ROOT_DIR, path)\n    full_path = os.path.join(path, filename)\n    with open(full_path, \"w\") as md_file:\n        md_file.write(content)\n    logging.info(f\"Information saved to {full_path}\")\n    return full_path\n\n\ndef save_to_md2(filename, content: str) -> str:\n    create_directories_if_not_exist(filename)\n    full_path = filename\n    full_path = add_extension_if_not_exist(full_path, \".md\")\n    with open(full_path, \"w\") as md_file:\n        md_file.write(content)\n        md_file.flush()\n    logging.info(f\"Information saved to {full_path}\")\n    return full_path\n\n\ndef save_to_yaml(filename, content: str) -> str:\n    create_directories_if_not_exist(filename)\n    full_path = filename\n    full_path = add_extension_if_not_exist(full_path, \".yaml\")\n    with open(full_path, \"w\", encoding='utf-8') as md_file:\n        md_file.write(content)\n        md_file.flush()\n    logging.info(f\"Information saved to {full_path}\")\n    return full_path\n\n\ndef save_to_text(filename, content):\n    create_directories_if_not_exist(filename)\n    full_path = filename\n    full_path = add_extension_if_not_exist(full_path, \".txt\")\n    with open(full_path, \"w\", encoding='utf-8') as txt_file:\n        txt_file.write(content)\n    logging.info(f\"Information saved to {full_path}\")\n    return full_path\n\n\ndef create_directories_if_not_exist(filepath: str):\n    dir_name = os.path.dirname(filepath)\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n    return\n\n\ndef load_from_text(filename, path = \"\", extension = \".md\") -> str:\n    full_path = os.path.join(path, filename)\n    full_path = add_extension_if_not_exist(full_path, extension)\n    with open(full_path, \"r\", encoding='utf-8') as md_file:\n        content = md_file.read()\n    logging.info(f\"Information loaded from {full_path}\")\n    return content\n\ndef add_extension_if_not_exist(input_string, extension):\n    if not input_string.endswith(extension):\n        return input_string + extension\n    else:\n        return input_string\n\n\ndef same_string(s1: str, s2: str, case_sensitive: bool = False):\n    if case_sensitive:\n        return s1 == s2\n    return s1.lower() == s2.lower()\n\n\ndef print_error_message(message):\n    print(f\"Error: {message}\", file=sys.stderr)\n\n\ndef delete_directory_contents(directory):\n    for root, dirs, files in os.walk(directory, topdown=False):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                os.remove(file_path)\n                print(f\"Deleted file: {file_path}\")\n            except Exception as e:\n                print(f\"Error deleting file {file_path}: {str(e)}\")\n        for dir_name in dirs:\n            dir_path = os.path.join(root, dir_name)\n            try:\n                os.rmdir(dir_path)\n                print(f\"Deleted directory: {dir_path}\")\n            except Exception as e:\n                print(f\"Error deleting directory {dir_path}: {str(e)}\")\n\n\ndef embed_templates():\n    qdrant_path = os.path.join(SRC_DIR, \"tools\", \"qdrant\", \"embedding\")\n    client = QdrantClient(path=qdrant_path)\n    template_lists = list(filter(lambda x: os.path.isdir(os.path.join(SRC_DIR, \"tools\", \"templates\", x)),\n                                 os.listdir(os.path.join(SRC_DIR, \"tools\", \"templates\"))))\n\n    def get_uuid():\n        return str(uuid.uuid4().hex)\n\n    def __embed_summary(summary, path):\n        payload_dict = {\"path\": path, \"summary\": summary}\n        embeddings_model = OpenAIEmbeddings(openai_api_key=openai.api_key)\n        embedded_query = embeddings_model.embed_query(summary)\n        try:\n            client.upsert(\n                collection_name=\"templates\",\n                points=[\n                    PointStruct(id=get_uuid(), vector=embedded_query, payload=payload_dict)\n                ]\n            )\n        except ValueError:\n            client.recreate_collection(\n                collection_name=\"templates\",\n                vectors_config=VectorParams(size=len(embedded_query), distance=Distance.COSINE),\n            )\n            client.upsert(\n                collection_name=\"templates\",\n                points=[\n                    PointStruct(id=get_uuid(), vector=embedded_query, payload=payload_dict)\n                ]\n            )\n        return\n\n    for cur_dir in template_lists:\n        cur_path = os.path.join(SRC_DIR, \"tools\", \"templates\", cur_dir)\n        readme = os.path.join(cur_path, \"README.md\")\n        with open(readme) as f:\n            content = f.read()\n            __embed_summary(content, cur_path)\n    return\n\n\nif __name__ == \"__main__\":\n    # embed_templates()\n    pass\n"}
{"type": "source_file", "path": "solidgpt/src/constants.py", "content": "\"\"\"Skill Names\"\"\"\nSKILL_NAME_WRITE_PRODUCT_REQUIREMENTS_DOCUMENTATION = \"Write Product Requirement Documentation\"\nSKILL_NAME_WRITE_HLD = \"Write High Level Design\"\nSKILL_NAME_CREATE_KANBAN_BOARD = \"Create Kanban Board\"\nSKILL_NAME_CUSTOM_SKILL = \"Custom Skill\"\nSKILL_NAME_WRITE_YAML = \"Write lowdefy YAML\"\nSKILL_NAME_RUN_APP = \"Host and run web app\"\nSKILL_NAME_LOAD_REPO = \"Load repo\"\nSKILL_NAME_SUMMARY_PROJECT = \"Summary project\"\nSKILL_NAME_QUERY_CODE = \"Query code\"\nSKILL_NAME_SUMMARY_FILE = \"Summary file\"\nSKILL_NAME_ANALYSIS_PRODUCT = \"Analysis Product\"\nSKILL_NAME_PROVIDE_TECH_SOLUTION = \"Provide Tech Solution\"\nSKILL_NAME_REPO_CHAT = \"Repo Chat\"\nSKILL_NAME_AUTOGEN_ANALYSIS = \"AutoGen analysis\"\nSKILL_NAME_VSCODE_EMBED = \"VSCODE embed\"\nSKILL_NAME_VSCODE_EMBED2 = \"VSCODE embed 2\"\nSKILL_NAME_CREATE_CODE_PLAN = \"Create Code Plan\"\nSKILL_NAME_CREATE_CODE_SOLUTION = \"Create Code Solution\"\nSKILL_NAME_REPO_CHAT_V2 = \"Repo Chat 2\"\nSKILL_NAME_SELECT_TEMPLATE = \"Select Template\"\nSKILL_NAME_HTTP_CODE_PLAN = \"HTTP Code Plan\"\nSKILL_NAME_HTTP_SOLUTION = \"HTTP Code Solution\"\nSKILL_NAME_HTTP_SOLUTION_V2 = \"HTTP Code Solution V2\"\nSKILL_NAME_NOTION_EMBED = \"Notion Embed\"\nSKILL_NAME_CODE_CHAT = \"Code chat\"\nSKILL_NAME_NOTION_CHAT = \"Notion chat\"\n"}
{"type": "source_file", "path": "solidgpt/src/worknode/worknode.py", "content": "from solidgpt.src.workgraph.displayresult import DisplayResult\nfrom solidgpt.src.workskill.workskill import *\nfrom solidgpt.src.util.util import *\n\n\nclass WorkNode:\n    node_id: str = 0\n    skill: WorkSkill = None\n    next_node_ids: set[str] = []\n    output_id_dependencies: set[int] = []\n    manual_review_result: bool = False\n    graph_cache: dict = {}\n    display_result: DisplayResult = None\n\n    def __init__(self, node_id: str, work_skill: WorkSkill, manual_review_result: bool = False, graph_cache : dict = {}):\n        # Initialization\n        self.node_id = node_id\n        self.graph_cache = graph_cache\n        work_skill.graph_cache = self.graph_cache\n        self.skill = work_skill\n        self.next_node_ids = set([])\n        self.output_id_dependencies = set([])\n        self.manual_review_result = manual_review_result\n        self.display_result = DisplayResult()\n        return\n\n    def can_execute(self):\n        if len(self.output_id_dependencies) == 0:\n            return True\n        print(\"still need to wait other nodes to finish first for node \" + str(self.node_id))\n        return False\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/update.py", "content": "import json\nimport time\nimport logging\nimport os\n\nfrom todos import decimalencoder\nimport boto3\ndynamodb = boto3.resource('dynamodb')\n\n\ndef update(event, context):\n    data = json.loads(event['body'])\n    if 'text' not in data or 'checked' not in data:\n        logging.error(\"Validation Failed\")\n        raise Exception(\"Couldn't update the todo item.\")\n        return\n\n    timestamp = int(time.time() * 1000)\n\n    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n\n    # update the todo in the database\n    result = table.update_item(\n        Key={\n            'id': event['pathParameters']['id']\n        },\n        ExpressionAttributeNames={\n          '#todo_text': 'text',\n        },\n        ExpressionAttributeValues={\n          ':text': data['text'],\n          ':checked': data['checked'],\n          ':updatedAt': timestamp,\n        },\n        UpdateExpression='SET #todo_text = :text, '\n                         'checked = :checked, '\n                         'updatedAt = :updatedAt',\n        ReturnValues='ALL_NEW',\n    )\n\n    # create a response\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(result['Attributes'],\n                           cls=decimalencoder.DecimalEncoder)\n    }\n\n    return response\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/create.py", "content": "import json\nimport logging\nimport os\nimport time\nimport uuid\n\nimport boto3\ndynamodb = boto3.resource('dynamodb')\n\n\ndef create(event, context):\n    data = json.loads(event['body'])\n    if 'text' not in data:\n        logging.error(\"Validation Failed\")\n        raise Exception(\"Couldn't create the todo item.\")\n    \n    timestamp = str(time.time())\n\n    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n\n    item = {\n        'id': str(uuid.uuid1()),\n        'text': data['text'],\n        'checked': False,\n        'createdAt': timestamp,\n        'updatedAt': timestamp,\n    }\n\n    # write the todo to the database\n    table.put_item(Item=item)\n\n    # create a response\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(item)\n    }\n\n    return response\n"}
{"type": "source_file", "path": "solidgpt/src/manager/initializer.py", "content": "from solidgpt.src.diy.custom.customizeskillmanager import CustomizeSkillManager\nfrom solidgpt.src.manager.gptmanager import GPTManager\n\n\nclass Initializer:\n\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(Initializer, cls).__new__(cls)\n            # You can initialize the instance attributes here\n        return cls._instance\n\n    def __init__(self):\n        self._initialize()\n\n    def _initialize(self):\n        self.gpt_manager = GPTManager()\n        # self.customize_skill_manager = CustomizeSkillManager()\n"}
{"type": "source_file", "path": "solidgpt/src/diy/llama2_diy_finetune/train.py", "content": "from transformers import TrainingArguments, pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom llamamanager import LlamaManager\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel\nfrom dataloader import ModelDataLoader\nfrom llama2modelsetting import LlamaBasicModelFactory\n\nclass Llama2TrainerParam:\n    def __init__(self, model, tokenizer, train_dataset, valid_dataset, peft_config, max_seq_length, training_arguments, packing, model_name):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.train_dataset = train_dataset\n        self.valid_dataset = valid_dataset\n        self.peft_config = peft_config\n        self.max_seq_length = max_seq_length\n        self.training_arguments = training_arguments\n        self.packing = packing\n        self.model_name = model_name\n\nclass LLamaModel:\n    def __init__(self, model_type=\"togethercomputer/LLaMA-2-7B-32K-Instruct\",\n                 model_name=\"llama-2-7b-lowdefy_Instruct\",\n                 result_dir=\"./result\",\n                 model_path='./llama2-7b-lowdefy_generator_saved',\n                 train_dataset_path=\"./dataset/train.jsonl\",\n                 valid_dataset_path=\"./dataset/test.jsonl\"):\n        self.model_type = model_type\n        self.model_name_ = model_name\n        self.result_dir = result_dir\n        self.model_path = model_path\n        self.train_dataset_path = train_dataset_path\n        self.valid_dataset_path = valid_dataset_path\n        self.train_param = None\n\n    def set_train_config(self, train_param: Llama2TrainerParam):\n        self.train_param = train_param\n\n    def __set_default_config(self):\n        train, valid = self.__load_train_dataset()\n        model_settings = LlamaBasicModelFactory(\n            model_name=self.model_type,\n            lora_r=512,\n            lora_alpha=64,\n            lora_dropout=0.1,\n            use_4bit=True,\n            bnb_4bit_compute_dtype=\"float16\",\n            bnb_4bit_quant_type=\"nf4\",\n            use_nested_quant=False,\n            device_map={\"\": 0}\n        )\n        model, tokenizer = model_settings.create_model()\n        peft_config = LoraConfig(\n            lora_alpha=64,\n            lora_dropout=0.1,\n            r=512,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        training_arguments = TrainingArguments(\n            output_dir=self.result_dir,\n            num_train_epochs=1,\n            per_device_train_batch_size=8,\n            gradient_accumulation_steps=1,\n            optim=\"paged_adamw_32bit\",\n            save_steps=25,\n            logging_steps=5,\n            learning_rate=1e-3,\n            weight_decay=0.001,\n            fp16=False,\n            bf16=False,\n            max_grad_norm=0.3,\n            max_steps=-1,\n            warmup_ratio=0.03,\n            group_by_length=True,\n            lr_scheduler_type=\"constant\",\n            report_to=\"all\",\n            evaluation_strategy=\"steps\",\n            eval_steps=5\n        )\n        return Llama2TrainerParam(\n            model=model,\n            tokenizer=tokenizer,\n            train_dataset=train,\n            valid_dataset=valid,\n            peft_config=peft_config,\n            max_seq_length=None,  # Set your desired max sequence length\n            training_arguments=training_arguments,\n            packing=False,\n            model_name=self.model_name_\n        )\n\n    def __load_train_dataset(self):\n        data_loader = ModelDataLoader(self.train_dataset_path, self.valid_dataset_path)\n        train_dataset_mapped, valid_dataset_mapped = data_loader.load_and_preprocess_datasets()\n        return train_dataset_mapped, valid_dataset_mapped\n\n    def train_model(self):\n        if self.train_param is None:\n            self.train_param = self.__set_default_config()\n\n        trainer = SFTTrainer(\n            model=self.train_param.model,\n            train_dataset=self.train_param.train_dataset,\n            eval_dataset=self.train_param.valid_dataset,\n            peft_config=self.train_param.peft_config,\n            dataset_text_field=\"text\",\n            max_seq_length=self.train_param.max_seq_length,\n            tokenizer=self.train_param.tokenizer,\n            args=self.train_param.training_arguments,\n            packing=self.train_param.packing,\n        )\n\n        trainer.train()\n        trainer.model.save_pretrained(self.model_name_)\n        self.__save_model()\n\n    def __save_model(self):\n        base_model = AutoModelForCausalLM.from_pretrained(self.model_type, low_cpu_mem_usage=True, return_dict=True, torch_dtype=torch.float16, device_map={\"\":0})\n        model = PeftModel.from_pretrained(base_model, self.model_name_)\n        model = model.merge_and_unload()\n        tokenizer = AutoTokenizer.from_pretrained(self.model_type, trust_remote_code=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"left\"\n        model.save_pretrained(self.model_path)\n        tokenizer.save_pretrained(self.model_path)\n\n    def predict(self, prompt, template_path=None):\n        if self.train_param is None:\n            self.train_param = self.__set_default_config()\n\n        user_prompt = self.template + \"\\n\" + prompt\n        llama_manager = LlamaManager(model_path=self.model_path, template_path=template_path)\n        generated_yaml = llama_manager.generate_yaml(user_prompt)\n        return generated_yaml\n\n# Usage example\nif __name__ == \"__main__\":\n    llama_model = LLamaModel()\n    llama_model.train_model()\n\n    prompt = \"help me create lowdefy file for personal website\"\n    user_template_path = \"./dataset/template.yaml\"\n    generated_yaml = llama_model.predict(prompt, template_path=user_template_path)\n    print(\"Generated YAML:\\n\", generated_yaml)\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-telegram-bot/handler.py", "content": "import json\nimport telegram\nimport os\nimport logging\n\n\n# Logging is cool!\nlogger = logging.getLogger()\nif logger.handlers:\n    for handler in logger.handlers:\n        logger.removeHandler(handler)\nlogging.basicConfig(level=logging.INFO)\n\nOK_RESPONSE = {\n    'statusCode': 200,\n    'headers': {'Content-Type': 'application/json'},\n    'body': json.dumps('ok')\n}\nERROR_RESPONSE = {\n    'statusCode': 400,\n    'body': json.dumps('Oops, something went wrong!')\n}\n\n\ndef configure_telegram():\n    \"\"\"\n    Configures the bot with a Telegram Token.\n\n    Returns a bot instance.\n    \"\"\"\n\n    TELEGRAM_TOKEN = os.environ.get('TELEGRAM_TOKEN')\n    if not TELEGRAM_TOKEN:\n        logger.error('The TELEGRAM_TOKEN must be set')\n        raise NotImplementedError\n\n    return telegram.Bot(TELEGRAM_TOKEN)\n\n\ndef webhook(event, context):\n    \"\"\"\n    Runs the Telegram webhook.\n    \"\"\"\n\n    bot = configure_telegram()\n    logger.info('Event: {}'.format(event))\n\n    if event.get('requestContext', {}).get('http', {}).get('method') == 'POST' and event.get('body'): \n        logger.info('Message received')\n        update = telegram.Update.de_json(json.loads(event.get('body')), bot)\n        chat_id = update.message.chat.id\n        text = update.message.text\n\n        if text == '/start':\n            text = \"\"\"Hello, human! I am an echo bot, built with Python and the Serverless Framework.\n            You can take a look at my source code here: https://github.com/jonatasbaldin/serverless-telegram-bot.\n            If you have any issues, please drop a tweet to my creator: https://twitter.com/jonatsbaldin. Happy botting!\"\"\"\n\n        bot.sendMessage(chat_id=chat_id, text=text)\n        logger.info('Message sent')\n\n        return OK_RESPONSE\n\n    return ERROR_RESPONSE\n\n\ndef set_webhook(event, context):\n    \"\"\"\n    Sets the Telegram bot webhook.\n    \"\"\"\n\n    logger.info('Event: {}'.format(event))\n    bot = configure_telegram()\n    url = 'https://{}/{}/'.format(\n        event.get('headers').get('host'),\n        event.get('requestContext').get('stage'),\n    )\n    webhook = bot.set_webhook(url)\n\n    if webhook:\n        return OK_RESPONSE\n\n    return ERROR_RESPONSE\n"}
{"type": "source_file", "path": "solidgpt/src/saveload/saveload.py", "content": "from typing import Type\nfrom solidgpt.src.diy.custom.customizeskillmanager import CustomizeSkillManager\nfrom solidgpt.src.worknode.worknode import *\nfrom solidgpt.src.imports import *\nfrom solidgpt.src.constants import *\n\n\nSKILL_NAME_TO_CONSTRUCTOR: dict[str, Type[WorkSkill]] = {\n    SKILL_NAME_CREATE_KANBAN_BOARD: CreateKanBan,\n}\n\n\ndef generate_save_data_from_nodes(nodes: list[WorkNode], generate_debug_info: bool = False):\n    save_data = []\n    for node in nodes:\n        node_data = {\n            \"node_id\": node.node_id,\n            \"manual_review_result\": node.manual_review_result\n        }\n\n        if generate_debug_info:\n            node_data[\"next_node_ids\"] = list(node.next_node_ids)\n            node_data[\"output_id_dependencies\"] = list(node.output_id_dependencies)\n\n        skill = node.skill\n        node_data[\"skill\"] = skill.name\n        node_data[\"inputs\"] = []\n        node_data[\"outputs\"] = []\n\n        for i in skill.inputs:\n            temp_input = {\n                # \"param_type\": str(i.param_type),\n                \"param_path\": i.param_path,\n                \"loading_method\": str(i.loading_method),\n                \"load_from_output_id\": i.load_from_output_id,\n            }\n            if generate_debug_info:\n                temp_input[\"param_name\"] = i.param_name\n                temp_input[\"param_category\"] = str(i.param_category)\n                temp_input[\"optional\"] = str(i.optional)\n\n            node_data[\"inputs\"].append(temp_input)\n\n        for o in skill.outputs:\n            temp_output = {\n                \"id\": o.id,\n            }\n\n            if generate_debug_info:\n                temp_output[\"param_category\"] = str(o.param_category)\n                # temp_output[\"param_type\"] = str(o.param_type)\n                temp_output[\"id\"] = o.id\n\n            node_data[\"outputs\"].append(temp_output)\n\n        save_data.append(node_data)\n    return save_data\n\n\ndef load_save_data_to_nodes(loaded_data):\n    nodes: list[WorkNode] = []\n    for node_data in loaded_data:\n        skill_name = node_data[\"skill\"]\n        inputs_data = node_data[\"inputs\"]\n        outputs_data = node_data[\"outputs\"]\n        skill_constructor = SKILL_NAME_TO_CONSTRUCTOR.get(skill_name, None)\n        if skill_constructor is not None:\n            skill: WorkSkill = skill_constructor()\n        else:\n            skill = CustomizeSkillManager._instance.get_customzied_skill(skill_name)\n        if skill is not None:\n            skill.init_config(inputs_data, outputs_data)\n            node = WorkNode(node_data[\"node_id\"], skill, node_data[\"manual_review_result\"])\n            nodes.append(node)\n    return nodes\n\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/__init__.py", "content": ""}
{"type": "source_file", "path": "solidgpt/src/manager/embedding/embeddingmanager.py", "content": "\nimport logging\nfrom solidgpt.src.manager.embedding.embeddingmodel import LOCAL_EMBEDDING_STORAGE_DIVIDED_RESOURCE_DIR, LOCAL_EMBEDDING_STORAGE_EMBEDDED_RESOURCE_DIR, LOCAL_EMBEDDING_STORAGE_ORIGINAL_RESOURCE_DIR, EmbeddingModel, EmbeddingModelParameter\n\n\nDEFAULT_EMBEDDING_MODEL_LABEL = 'DefaultEmbeddingModel'\nDEFAULT_EMBEDDING_RESOURCE_NAME = 'Default'\n\nclass EmbeddingManager:\n\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(EmbeddingManager, cls).__new__(cls)\n            # You can initialize the instance attributes here\n        return cls._instance\n    \n    def __init__(self):\n        self.embed_models_container : dict(str, EmbeddingModel) = {}\n\n    def add_default_embed_model(self):\n        default_param = EmbeddingModelParameter(resource_name=DEFAULT_EMBEDDING_RESOURCE_NAME, \n                                                    original_resources_folder_path=LOCAL_EMBEDDING_STORAGE_ORIGINAL_RESOURCE_DIR,\n                                                    divided_resources_folder_path=LOCAL_EMBEDDING_STORAGE_DIVIDED_RESOURCE_DIR,\n                                                    embedded_resources_folder_path=LOCAL_EMBEDDING_STORAGE_EMBEDDED_RESOURCE_DIR,\n                                                    has_embedded=False)\n        self.add_embed_model(DEFAULT_EMBEDDING_MODEL_LABEL, default_param)\n\n    def add_embed_model(self, label : str, param : EmbeddingModelParameter, do_embedding = True):\n        self.embed_models_container[label] = EmbeddingModel(param)\n        if do_embedding:\n            self.embed_models_container[label].embed_resources()\n\n    def query_from_embed_model(self, query_message : str, model_label = DEFAULT_EMBEDDING_MODEL_LABEL, response_num = 12):\n        if self.embed_models_container.get(model_label) is None:\n            logging.error(f\"Embedding model {model_label} not found.\")\n            return\n        return self.embed_models_container[model_label].query_most_match_result_from_resource(query_message, response_num)"}
{"type": "source_file", "path": "solidgpt/src/api/api.py", "content": "## api.py\nimport asyncio\nimport logging\nimport uuid\nfrom pydantic import BaseModel\nfrom solidgpt.src.api.api_response import *\nfrom solidgpt.src.manager.initializer import Initializer\nfrom fastapi import FastAPI, HTTPException, Body\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware  # Import the CORSMiddleware\nfrom celery.result import allow_join_result\nfrom solidgpt.src.util.util import *\nfrom solidgpt.src.workgraph.graph import *\nimport os\nfrom solidgpt.src.api.celery_tasks import *  # Import task function\n\n\nclass GraphResult:\n    __result = None\n    __graph_name = \"\"\n\n    def __init__(self, result, graph_name):\n        self.set_result_obj(result)\n        self.__graph_name = graph_name\n\n    def set_result_obj(self, result_obj):\n        self.__result = result_obj\n\n    def get_result_obj(self):\n        return self.__result\n\n    def get_name(self):\n        return self.__graph_name\n\n    def set_name(self, new_name):\n        self.__graph_name = new_name\n\n\napp = FastAPI()\n\n# Add the CORS middleware with the allowed origins\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Later change this to \"origins\"\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # You can restrict this to specific HTTP methods if needed\n    allow_headers=[\"*\"],  # You can restrict this to specific headers if needed\n)\n\nInitializer()\n# orchestration = Orchestration()\nuploaded_repo_map: dict = {}\nautogen_task_map: dict = {}\ngraph_result_map: dict[str, GraphResult] = {}\ngraph_stage_map: dict = {}\nserverless_task_map: dict = {}\n\n\nclass FileData(BaseModel):\n    file_contents: list[str]\n    filenames: list[str]\n\n\n@app.get(\"/test/getmsg\")\nasync def test_get():\n    return JSONResponse(content={\"message\": f\"[Test Get] Message 'Hello World!'.\"}, status_code=200)\n\n\n@app.post(\"/test/postmsg\")\nasync def test_post(body: dict = Body(...)):\n    return JSONResponse(content={\"message\": f\"[Test Post] Message '{body['msg']}'.\"}, status_code=200)\n\n\n@app.post(\"/remove/files\")\nasync def remove_all_files():\n    try:\n        # Remove existing files\n        delete_directory_contents(REPO_STORAGE_DIR)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Server error: {e}\")\n\n    return JSONResponse(content={\"message\": f\"All files removed\"}, status_code=200)\n\n@app.post(\"/status/graph\")\nasync def get_graph_status(body: dict = Body(...)):\n    result = await get_graph_status_impl(body)\n    return result\n\nasync def get_graph_status_impl(body: dict = Body(...)):\n    graph_id = body['graph_id']\n    graph_result = graph_result_map.get(graph_id, None)\n    result = None\n    if graph_result is not None:\n        result = graph_result.get_result_obj()\n\n    if graph_result is None:\n        return JSONResponse(content=response_graph(\n            message=f\"Graph does not exist.\",\n            status=0,\n        ), status_code=200)\n    elif not result.ready():\n        return JSONResponse(content=response_graph(\n            graph=graph_result.get_name(),\n            message=f\"Graph is still executing.\",\n            status=1,\n            progress=result.info,\n        ), status_code=200)\n    elif result.ready():\n        if result.status == \"SUCCESS\":\n            result_txt = \"\"\n            extra_payload = None\n            with allow_join_result():\n                result_obj = result.get()\n                if isinstance(result_obj, str):\n                    result_txt = result_obj\n                elif isinstance(result_obj, list):\n                    if len(result_obj) >= 1:\n                        result_txt = result_obj[0]\n                    if len(result_obj) >= 2:\n                        extra_payload = result_obj[1]\n            return JSONResponse(content=response_graph(\n                graph=graph_result.get_name(),\n                message=f\"Graph finishes running.\",\n                status=2,\n                result=result_txt,\n                extra_payload=extra_payload,\n            ), status_code=200)\n        elif result.status == \"FAILURE\":\n            return JSONResponse(content=response_graph(\n                graph=graph_result.get_name(),\n                message=f\"Graph has an error.\",\n                status=4,\n                error=result.traceback,\n            ), status_code=200)\n    return JSONResponse(content=response_graph(\n        graph=graph_result.get_name(),\n        message=f\"Graph in unknown state.\",\n        status=3,\n    ), status_code=200)\n\n@app.post(\"/repochat/v2\")\nasync def repo_chat_v2(body: dict = Body(...)):\n    # Enqueue the background task: repo chat\n    try:\n        logging.info(\"celery task: repo chat graph\")\n        session_id = body['session_id']\n        openai_key = body['openai_key']\n        requirement = body['requirement']\n        logging.info(\"celery task: repo chat graph\")\n        openai.api_key = openai_key\n        g = build_repo_chat_graph_v2(requirement, session_id)\n        g.init_node_dependencies()\n        g.execute()\n        result = g.display_result.get_result()\n\n        return JSONResponse(content={\n            \"message\": f\"Running repo chat graph...\",\n            \"session_id\": session_id,\n            \"result\": result,\n            \"status\": \"Succeeded\"\n        }, status_code=200)\n    except Exception as e:\n        session_id = body.get('session_id', \"\")\n        return JSONResponse(content={\n            \"message\": f\"Running repo chat graph...\",\n            \"session_id\": session_id,\n            \"result\": str(e),\n            \"status\": \"Failed\"\n        }, status_code=200)\n\n@app.post(\"/onboardrepo/v4\")\nasync def onboard_repo_v4(body: dict = Body(...)):\n    # Enqueue the background task: onboard repo\n    logging.info(\"celery task: onboard repo graph\")\n    # TODO: Remove graph_id in the future\n    graph_id = f'placeholder'\n    openai_key = body['openai_key']\n    base_path = body['base_path']\n    # logging.info(f\"onboard_repo_v4: {base_path}\")\n    result = celery_task_onboard_repo_graph_v4.apply_async(args=[openai_key, graph_id, base_path])\n    graph_result = GraphResult(result, \"Onboard Repo Graph\")\n    graph_result_map[graph_id] = graph_result\n    return JSONResponse(content={\n        \"message\": f\"Indexing codebase...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"current_work_name\": \"onboard repo\"\n    }, status_code=200)\n\n@app.post(\"/codeplan/v4\")\nasync def code_plan_v4(body: dict = Body(...)):\n    # Enqueue the background task: code plan\n    logging.info(\"celery task: code plan graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    logging.info(\"celery task: code plan graph\")\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    result = celery_task_code_plan_graph_v4.apply_async(args=[openai_key, requirement, graph_id])\n    graph_result = GraphResult(result, \"Code Plan Graph\")\n    graph_result_map[graph_id] = graph_result\n\n    return JSONResponse(content={\n        \"message\": f\"Running code plan graph...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"current_work_name\": \"code plan\"\n    }, status_code=200)\n\n@app.post(\"/codesolution/v3\")\nasync def code_solution_v3(body: dict = Body(...)):\n    # Enqueue the background task: code plan\n    logging.info(\"celery task: code solution graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    code_plan = body['code_plan']\n    # If user input the openai model, set it as default for most of the skills\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n\n    logging.info(\"celery task: code solution graph\")\n    openai.api_key = openai_key\n    result = celery_task_code_solution_graph_v3.apply_async(args=[openai_key, requirement, code_plan, graph_id])\n    graph_result = GraphResult(result, \"Code Solution Graph\")\n    graph_result_map[graph_id] = graph_result\n\n    return JSONResponse(content={\n        \"message\": f\"Running tech solution graph...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"current_work_name\": \"tech solution\"\n    }, status_code=200)\n\n@app.post(\"/serverless/deploy\")\nasync def deploy_serverless(body: dict = Body(...)):\n    task_id = f'deploy-{str(uuid.uuid4())}'\n    session_id = body.get(\"session_id\", \"\")\n    yml_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id, \"architecture\", \"serverless.yml\")\n    aws_key_id = body.get(\"aws_key_id\", \"\")\n    aws_access_key = body.get(\"aws_access_key\", \"\")\n    result = celery_task_serverless_deploy.apply_async(args=[yml_path, aws_key_id, aws_access_key])\n    serverless_task_map[task_id] = result\n\n    return JSONResponse(content={\n        \"message\": f\"Deploying to AWS...\",\n        \"task_id\": task_id\n    }, status_code=200)\n\n\n@app.post(\"/serverless/remove\")\nasync def remove_serverless(body: dict = Body(...)):\n    task_id = f'remove-{str(uuid.uuid4())}'\n    session_id = body.get(\"session_id\", \"\")\n    yml_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id, \"architecture\", \"serverless.yml\")\n    aws_key_id = body.get(\"aws_key_id\", \"\")\n    aws_access_key = body.get(\"aws_access_key\", \"\")\n    result = celery_task_serverless_remove.apply_async(args=[yml_path, aws_key_id, aws_access_key])\n    serverless_task_map[task_id] = result\n\n    return JSONResponse(content={\n        \"message\": f\"Removing from AWS...\",\n        \"task_id\": task_id\n    }, status_code=200)\n\n\n@app.post(\"/status/serverless\")\nasync def get_serverless_task_status(body: dict = Body(...)):\n    task_id: str = body['task_id']\n    celery_task_result = serverless_task_map.get(task_id, None)\n\n    if celery_task_result is None:\n        return JSONResponse(content=response_serverless(\n                                message=\"status: not exist or not started\",\n                                status=\"Failed\"\n                            ), status_code=200)\n    if celery_task_result.ready():\n        if celery_task_result.status == \"SUCCESS\":\n            res_dict = celery_task_result.info\n            if res_dict.get(\"status\", \"Failed\") == \"Succeeded\":\n                return JSONResponse(content=response_serverless(\n                    message=\"status: succeeded\",\n                    status=\"Succeeded\"\n                ), status_code=200)\n            else:\n                return JSONResponse(content=response_serverless(\n                    message=\"status: \" + str(res_dict.get(\"status\", \"Failed\")),\n                    status=\"Failed\",\n                    error=res_dict.get(\"output\", \"no error output\")\n                ), status_code=200)\n        elif celery_task_result.status == \"FAILURE\":\n            return JSONResponse(content=response_serverless(\n                message=\"status: failed due to an unexpected error\",\n                status=\"Failed\",\n                error=celery_task_result.traceback\n            ), status_code=200)\n        else:\n            return JSONResponse(content=response_serverless(\n                message=\"status: unknown\",\n                status=\"Failed\"\n            ), status_code=200)\n    return JSONResponse(content=response_upload(\n        message=\"status: executing serverless...\",\n        status=\"Running\",\n        progress=celery_task_result.info\n    ), status_code=200)\n\n@app.post(\"/selecttemplate\")\nasync def select_template(body: dict = Body(...)):\n    # Enqueue the background task: code plan\n    logging.info(\"celery task: select template graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    logging.info(\"celery task: select template\")\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    result = celery_task_select_template.apply_async(args=[openai_key, requirement, graph_id])\n    graph_result = GraphResult(result, \"Select Template Graph\")\n    graph_result_map[graph_id] = graph_result\n\n    return JSONResponse(content={\n        \"message\": f\"Running Select Template graph...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"current_work_name\": \"Select Template\"\n    }, status_code=200)\n\n@app.post(\"/httpsolution/v1\")\nasync def http_solution_v1(body: dict = Body(...)):\n    logging.info(\"celery task: http solution graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    logging.info(\"celery task: http solution v1\")\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    result = celery_task_http_solution.apply_async(args=[openai_key, requirement, graph_id])\n    graph_result = GraphResult(result, \"HTTP Solution Graph\")\n    graph_result_map[graph_id] = graph_result\n\n    return JSONResponse(content={\n        \"message\": f\"Running HTTP Solution graph...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"path\": result,\n        \"current_work_name\": \"HTTP Solution\"\n    }, status_code=200)\n\n\n@app.post(\"/notionembed\")\nasync def notion_embed(body: dict = Body(...)):\n    logging.info(\"celery task: notion embed\")\n    # TODO: Remove graph_id in the future\n    graph_id = f'placeholder'\n    onboarding_id = body['onboarding_id']\n    openai_key = body['openai_key']\n    workspace_token = body['workspace_token']\n    page_id = body['top_level_page_id']\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    result = celery_task_notion_embed.apply_async(args=[openai_key, onboarding_id, workspace_token, page_id])\n    graph_result = GraphResult(result, \"Notion Embed Graph\")\n    graph_result_map[graph_id] = graph_result\n\n    return JSONResponse(content={\n        \"message\": f\"Indexing Notion workspace...\",\n        \"graph_id\": graph_id,\n        \"is_final\": True,\n        \"current_work_name\": \"Notion Embed\"\n    }, status_code=200)\n\n@app.post(\"/codechatapi\")\nasync def codechatapi(body: dict = Body(...)):\n    logging.info(\"celery task: code chat api graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    scope = body['scope']\n    logging.info(\"celery task: code chat api\")\n    if len(scope) > 5:\n        logging.info(\"can only refer to 5 files, using top five files listed\")\n        scope = scope[:5]\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    try:\n        code_result = None\n        code_scope = []\n        for path in scope:\n            if path[:9] == \"codebase/\":\n                code_scope.append(path)\n        code_result = celery_task_code_chat.apply_async(args=[openai_key, requirement, graph_id, code_scope])\n        code_graph_graph_result = GraphResult(code_result, \"code chat api Graph\")\n        graph_result_map[graph_id] = code_graph_graph_result\n\n        return JSONResponse(content={\n            \"message\": f\"I am thinking...\",\n            \"graph_id\": graph_id,\n            \"is_final\": True,\n            \"status\": \"Succeeded\",\n            \"current_work_name\": \"chat api\"\n        }, status_code=200)\n    except Exception as e:\n        session_id = body.get('session_id', \"\")\n        return JSONResponse(content={\n            \"message\": f\"Running celery task: chat api...\",\n            \"session_id\": session_id,\n            \"result\": str(e.with_traceback()),\n            \"status\": \"Failed\"\n        }, status_code=200)\n\n@app.post(\"/notionchatapi\")\nasync def notionchatapi(body: dict = Body(...)):\n    logging.info(\"celery task: notion chat api graph\")\n    graph_id = body['graph_id']\n    openai_key = body['openai_key']\n    requirement = body['requirement']\n    scope = body['scope']\n    logging.info(\"celery task: notion chat api\")\n    if len(scope) > 5:\n        logging.info(\"can only refer to 5 files, using top five files listed\")\n        scope = scope[:5]\n    openai.api_key = openai_key\n    if 'openai_model' in body:\n        ConfigReader().set_default_openai_model(body['openai_model'])\n    try:\n        notion_result = None\n        notion_scope = []\n        for path in scope:\n            if path[:7] == \"notion/\":\n                notion_scope.append(path)\n        notion_result = celery_task_notion_chat.apply_async(args=[openai_key, requirement, graph_id, notion_scope])\n        notion_graph_graph_result = GraphResult(notion_result, \"notion chat api Graph\")\n        graph_result_map[graph_id] = notion_graph_graph_result\n\n        return JSONResponse(content={\n            \"message\": f\"I am thinking...\",\n            \"graph_id\": graph_id,\n            \"is_final\": True,\n            \"status\": \"Succeeded\",\n            \"current_work_name\": \"chat api\"\n        }, status_code=200)\n    except Exception as e:\n        session_id = body.get('session_id', \"\")\n        return JSONResponse(content={\n            \"message\": f\"Running celery task: chat api...\",\n            \"session_id\": session_id,\n            \"result\": str(e.with_traceback()),\n            \"status\": \"Failed\"\n        }, status_code=200)\n    \n@app.post(\"/clean/chat\")\nasync def clean_chat_history():\n    # Implement the logic to clean the chat history here\n    file_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, \"placeholder\", \"placeholder_chat.json\")\n    try:\n        os.remove(file_path)\n        logging.info(\"File deleted successfully.\")\n    except OSError as e:\n        logging.warn(f\"Error deleting the file: {e}\")\n        return JSONResponse(content={\"message\": f\"Error deleting the file: {e}\"}, status_code=200)\n    return JSONResponse(content={\"message\": \"Chat history cleaned successfully.\"}, status_code=200)\n\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/delete.py", "content": "import os\n\nimport boto3\ndynamodb = boto3.resource('dynamodb')\n\n\ndef delete(event, context):\n    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])\n\n    # delete the todo from the database\n    table.delete_item(\n        Key={\n            'id': event['pathParameters']['id']\n        }\n    )\n\n    # create a response\n    response = {\n        \"statusCode\": 200\n    }\n\n    return response\n"}
{"type": "source_file", "path": "solidgpt/src/tools/notion/notionactions.py", "content": "# import time\n#\n# from solidgpt.definitions import ROOT_DIR\n# from solidgpt.src.configuration.configreader import ConfigReader\n# import os\n# import notional\n# from notional import blocks\n# from notion2md.exporter.block import MarkdownExporter\n#\n# NOTION_API_KEY = ConfigReader().get_property(\"notion_api_key\")\n# PAGE_ID = ConfigReader().get_property(\"notion_page_id\")\n#\n# class NotionActions:\n#\n#     def __init__(self):\n#         self.auth_token = NOTION_API_KEY\n#         os.environ[\"NOTION_TOKEN\"] = NOTION_API_KEY\n#         self.notion = notional.connect(auth=self.auth_token)\n#         self.page = self.notion.pages.retrieve(PAGE_ID)\n#\n#     def process_markdown_and_upload(self, md_file_path: str):\n#\n#         # clear current notion page\n#         for child_block in self.notion.blocks.children.list(self.page):\n#             self.notion.blocks.delete(child_block.id)\n#\n#         # Initialize time variables\n#         start_time = time.time()\n#         timeout = 20  # 20 seconds\n#\n#         # Check if all blocks are deleted, with a timeout\n#         while True:\n#             elapsed_time = time.time() - start_time\n#             if elapsed_time > timeout:\n#                 print(\"Timeout reached. Exiting loop. Not all blocks have been cleared.\")\n#                 break\n#\n#             remaining_blocks = self.notion.blocks.children.list(self.page)\n#             for block in remaining_blocks:\n#                 time.sleep(1)\n#                 continue\n#\n#             print(\"All blocks deleted. Exiting loop.\")\n#             break\n#\n#         with open(md_file_path, \"r\", encoding=\"utf-8\") as mdFile:\n#             lastLine = \"\"\n#             table = None\n#             content = None\n#             for line in mdFile:\n#                 line = line.strip()\n#                 if line.startswith(\"|\"):\n#                     if not lastLine.startswith(\"|\"):\n#                         headers = line.split(\"|\")\n#                         headers = list(filter(None, headers))\n#                         table = blocks.Table[blocks.TableRow[headers]]\n#                     else:\n#                         row = line.split(\"|\")\n#                         row = list(\n#                             map(lambda item: item.replace('-', ''), row))\n#                         row = list(filter(None, row))\n#                         if len(row) > 0:\n#                             table.append(blocks.TableRow[row])\n#                 elif line.startswith(\"###\"):\n#                     content = blocks.Heading3[line.replace(\"###\", \"\", 1)]\n#                 elif line.startswith(\"##\"):\n#                     content = blocks.Heading2[line.replace(\"##\", \"\", 1)]\n#                 elif line.startswith(\"#\"):\n#                     content = blocks.Heading1[line.replace(\"#\", \"\", 1)]\n#                 elif line.startswith(\"-\"):\n#                     content = blocks.BulletedListItem[line.replace(\"-\", \"\", 1)]\n#                 else:\n#                     content = blocks.Paragraph[line]\n#\n#                 if table is not None and not line.startswith(\"|\"):\n#                     self.notion.blocks.children.append(self.page, table)\n#                     table = None\n#                 if content is not None:\n#                     self.notion.blocks.children.append(self.page, content)\n#                     content = None\n#\n#                 lastLine = line\n#\n#         if table is not None:\n#             self.notion.blocks.children.append(self.page, table)\n#         if content is not None:\n#             self.notion.blocks.children.append(self.page, content)\n#\n#     def sync_from_notion(self, path: str = os.path.join(ROOT_DIR), doc_name: str = \"PRDDocument\"):\n#         # MarkdownExporter will make markdown file on your output path\n#         MarkdownExporter(block_id=PAGE_ID, output_path=path, download=True, unzipped=True, output_filename= doc_name).export()\n#\n#\n# # # Test will remove later\n# # actions = NotionActions()\n# # #actions.process_markdown_and_upload(os.path.join(ROOT_DIR, \"PRDDocument.md\"))\n# # actions.sync_from_notion()\n"}
{"type": "source_file", "path": "solidgpt/src/tools/templates/aws-python-http-api-with-dynamodb/todos/decimalencoder.py", "content": "import decimal\nimport json\n\n\n# This is a workaround for: http://bugs.python.org/issue16535\nclass DecimalEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, decimal.Decimal):\n            return int(obj)\n        return super(DecimalEncoder, self).default(obj)\n"}
{"type": "source_file", "path": "solidgpt/src/workskill/skillio.py", "content": "from enum import Enum\n\n\nclass SkillIOParamCategory(int, Enum):\n    BusinessRequirementsDocument = 1\n    ProductRequirementsDocument = 2\n    HighLevelDesignDocument = 3\n    SourceCode = 4\n    PlainText = 5\n    KanbanBoard = 6\n    YAML = 7\n    CODE_PLAIN_TEXT = 8\n    CODE_PLAN = 9\n\nclass SkillInputLoadingMethod(Enum):\n    LOAD_FROM_OUTPUT_ID = 1\n    LOAD_FROM_STRING = 2\n    LOAD_FROM_CACHE_STRING = 3\n\n\nSTRING_TO_SKILL_INPUT_LOADING_METHOD_DICT: dict[str, SkillInputLoadingMethod] = {\n    str(SkillInputLoadingMethod.LOAD_FROM_OUTPUT_ID): SkillInputLoadingMethod.LOAD_FROM_OUTPUT_ID,\n    str(SkillInputLoadingMethod.LOAD_FROM_STRING): SkillInputLoadingMethod.LOAD_FROM_STRING,\n    str(SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING): SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING,\n}\n\n\ndef string_to_skill_input_loading_method(s: str):\n    return STRING_TO_SKILL_INPUT_LOADING_METHOD_DICT.get(s, SkillInputLoadingMethod.LOAD_FROM_STRING)\n\n\nclass SkillOutput:\n    config = None\n    param_name: str = \"\"\n    param_category: SkillIOParamCategory = SkillIOParamCategory.BusinessRequirementsDocument\n    param_path: str = \"\"\n    id: int = -1\n    to_display: bool = False\n\n    def __init__(self,\n                 param_name: str,\n                 param_category: SkillIOParamCategory,\n                 ):\n        # Initialization\n        self.param_name = param_name\n        self.param_category = param_category\n        self.param_path = \"\"\n        self.id = -1\n        self.to_display = False\n\n    def apply_config(self, config):\n        if config is None:\n            return\n        self.config = config\n        self.id = config[\"id\"]\n        self.to_display = config.get(\"to_display\", False)\n\n\nclass SkillInput:\n    config: dict = None\n    param_name: str = \"\"\n    param_category: SkillIOParamCategory = SkillIOParamCategory.BusinessRequirementsDocument\n    param_path: str = \"\"\n    optional: bool = False\n    loading_method: SkillInputLoadingMethod = SkillInputLoadingMethod.LOAD_FROM_STRING\n    load_from_output_id: int = -1\n    skill_output: SkillOutput = None\n    content: str = \"\"\n\n    def __init__(self,\n                 param_name: str,\n                 param_category: SkillIOParamCategory,\n                 optional: bool = False,\n                 ):\n        # Initialization\n        self.param_name = param_name\n        self.param_category = param_category\n        self.optional = optional\n        self.loading_method = SkillInputLoadingMethod.LOAD_FROM_STRING\n        self.load_from_output_id = -1\n        return\n\n    def apply_config(self, config):\n        if config is None:\n            return\n        self.config = config\n        self.param_path = config[\"param_path\"]\n        self.loading_method = string_to_skill_input_loading_method(config[\"loading_method\"])\n        if \"load_from_output_id\" in config:\n            self.load_from_output_id = config[\"load_from_output_id\"]\n        else:\n            self.load_from_output_id = -1\n        if self.loading_method == SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING:\n            self.content = config[\"content\"]\n\n    def get_input_path(self):\n        if self.loading_method == SkillInputLoadingMethod.LOAD_FROM_STRING:\n            return self.param_path\n        elif self.loading_method == SkillInputLoadingMethod.LOAD_FROM_OUTPUT_ID:\n            return self.skill_output.param_path\n        return \"\"\n    \nclass SkillInputConfig:\n    def __init__(self, param_path: str, loading_method: SkillInputLoadingMethod, load_from_output_id: int, content: str = None):\n        self.param_path = param_path\n        self.loading_method = loading_method\n        self.load_from_output_id = load_from_output_id\n        self.content = content\n    \n    def to_dict(self):\n        return {\n            \"param_path\": self.param_path,\n            \"loading_method\": str(self.loading_method),\n            \"load_from_output_id\": self.load_from_output_id,\n            \"content\": self.content\n        }\n"}
{"type": "source_file", "path": "solidgpt/src/orchestration/orchestration.py", "content": "import uuid\nfrom solidgpt.src.workgraph.graph import build_onboarding_graph\nfrom solidgpt.src.workgraph.graph_helper import GraphStatus, GraphType\nfrom solidgpt.src.manager.initializer import Initializer\nfrom solidgpt.src.workgraph.workgraph import *\n\n\nclass GraphInfo:\n    __graph: WorkGraph = None\n    __graph_name: str = \"\"\n    graph_id: str\n    graph_type: GraphType\n    graph_status: GraphStatus\n\n    def __init__(self, graph_name):\n        self.__graph = WorkGraph()\n        self.__graph_name = graph_name\n\n    def __init__(self, init_graph: WorkGraph, init_graph_id: str, graph_type: GraphType = None, graph_status: GraphStatus = None):\n        self.__graph = init_graph\n        self.graph_type = graph_type\n        self.graph_status = graph_status\n        self.graph_id = init_graph_id\n\n    def get_graph(self):\n        return self.__graph\n\n    def get_name(self):\n        return self.__graph_name\n\n\nclass Orchestration:\n    _instance = None\n    __graphs: list[GraphInfo] = []\n\n    # Using a dict to monitor the status of each graph, always update the status of the graph in the dict\n    # When the graph is completed and expired (e.g. 1 hour), remove the graph from the dict\n    graph_monitor: dict[str, GraphInfo] = {}\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(Orchestration, cls).__new__(cls)\n            # You can initialize the instance attributes here\n        return cls._instance\n\n    def __init__(self):\n        Initializer()\n        self.__graphs = []\n\n    def add_graph(self, json_file_path: str, graph_name: str):\n        if not os.path.isfile(json_file_path):\n            print(\"Cannot add graph, the specified json file path does not contain a file.\", file=sys.stderr)\n            return\n        if not json_file_path.endswith(\".json\"):\n            print(\"Cannot add graph, the specified file is not a json file.\", file=sys.stderr)\n            return\n\n        temp_graph = GraphInfo(graph_name)\n        temp_graph.get_graph().load_data(json_file_path)\n        # try:\n        #     temp_graph.get_graph().load_data(json_file_path)\n        # except:\n        #     print(\"Cannot add graph, loading json file into graph failed.\", file=sys.stderr)\n        #     return\n\n        self.__graphs.append(temp_graph)\n        return\n    \n    def add_graph(self, init_graph: WorkGraph, init_graph_id: str, graph_type: GraphType):\n        graph_info = GraphInfo(init_graph, init_graph_id, graph_type=graph_type, graph_status=GraphStatus.NotStarted)\n        self.graph_monitor[init_graph_id] = graph_info\n        return init_graph_id\n\n    def get_graph_status(self, graph_id: str) -> GraphStatus:\n        if graph_id not in self.graph_monitor:\n            return GraphStatus.NotStarted\n        return self.graph_monitor.get(graph_id).graph_status\n    \n    def run_graph_with_id(self, graph_id: str):\n        if graph_id not in self.graph_monitor:\n            logging.error(\"Cannot run graph, invalid graph id.\")\n            return\n        self.graph_monitor[graph_id].graph_status = GraphStatus.Running\n        graph = self.graph_monitor[graph_id].get_graph()\n        graph.init_node_dependencies()\n        graph.execute()\n        self.graph_monitor[graph_id].graph_status = GraphStatus.Completed\n        return\n\n    def run_graph_with_index(self, graph_index: int):\n        if 0 < graph_index < len(self.__graphs):\n            print(\"Cannot run graph, index out of range.\", file=sys.stderr)\n            return\n        self.__graphs[graph_index].get_graph().execute()\n\n    def run_graph_with_name(self, graph_name: str):\n        idx = self.get_graph_index(graph_name)\n        if idx < 0:\n            print_error_message(\"Cannot run graph, invalid graph name.\")\n            return\n        return self.run_graph_with_index(idx)\n\n    def get_graph_index(self, graph_name: str):\n        for graph in self.__graphs:\n            if graph.get_name() == graph_name:\n                return self.__graphs.index(graph)\n        return -1\n\n    def remove_graph(self, graph_index: int):\n        if 0 <= graph_index < len(self.__graphs):\n            print(\"Cannot remove graph, index out of range.\", file=sys.stderr)\n            return\n        self.__graphs.pop(graph_index)\n\n    def remove_all_graphs(self):\n        self.__graphs.clear()\n\n    def show_graphs(self):\n        graph_str = \"Graphs: \\n\"\n        idx = 0\n        for graph in self.__graphs:\n            graph_str += str(idx) + \": \" + graph.get_name() + \"\\n\"\n            idx += 1\n        print(graph_str)\n\n\n# Running sample\nif __name__ == \"__main__\":\n    orchestration = Orchestration()\n    # onborading API call will trigger the following code\n    graph = build_onboarding_graph(os.path.join(TEST_SKILL_WORKSPACE, \"in\", \"repo\"), True)\n    graph_id = orchestration.add_graph(graph, GraphType.OnboardingGraph)\n    orchestration.run_graph_with_id(graph_id)\n"}
{"type": "source_file", "path": "solidgpt/src/tools/repo/gpt_repository_loader.py", "content": "import os\nimport sys\nimport fnmatch\n\nfrom solidgpt.definitions import ROOT_DIR\n\nclass GitToTextConverter:\n    def __init__(self, repo_path, ignore_file_path=None, preamble_file=None, output_file_path='output.txt'):\n        self.repo_path = repo_path\n        self.ignore_file_path = ignore_file_path\n        self.preamble_file = preamble_file\n        self.output_file_path = output_file_path\n\n    def get_ignore_list(self):\n        ignore_list = []\n        if not self.ignore_file_path:\n            return ignore_list\n        with open(self.ignore_file_path, 'r') as ignore_file:\n            for line in ignore_file:\n                if sys.platform == \"win32\":\n                    line = line.replace(\"/\", \"\\\\\")\n                ignore_list.append(line.strip())\n        # Add default ignore patterns\n        ignore_list.append('.DS_Store')\n        return ignore_list\n\n    def should_ignore(self, file_path, ignore_list):\n        for pattern in ignore_list:\n            if fnmatch.fnmatch(file_path, pattern):\n                return True\n        return False\n\n    def process_repository(self):\n        with open(self.output_file_path, 'w') as output_file:\n            if self.preamble_file:\n                with open(self.preamble_file, 'r') as pf:\n                    preamble_text = pf.read()\n                    output_file.write(f\"{preamble_text}\\n\")\n            else:\n                output_file.write(\"The following text is a Git repository with code. The structure of the text are sections that begin with **-****-****-****-**, followed by a single line containing the file path and file name, followed by a variable amount of lines containing the file contents. The text representing the Git repository ends when the symbols --END-- are encounted. Any further text beyond --END-- are meant to be interpreted as instructions using the aforementioned Git repository as context.\\n\")\n            \n            ignore_list = self.get_ignore_list()\n            for root, _, files in os.walk(self.repo_path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    relative_file_path = os.path.relpath(file_path, self.repo_path)\n\n                    if not self.should_ignore(relative_file_path, ignore_list):\n                        with open(file_path, 'r', errors='ignore') as file:\n                            contents = file.read()\n                        output_file.write(\"**-**\" * 4 + \"\\n\")\n                        output_file.write(f\"{relative_file_path}\\n\")\n                        output_file.write(f\"{contents}\\n\")\n\n    def convert(self):\n        self.process_repository()\n        with open(self.output_file_path, 'a') as output_file:\n            output_file.write(\"--END--\")\n        print(f\"Repository contents written to {self.output_file_path}.\")\n\nif __name__ == \"__main__\":\n    converter = GitToTextConverter(os.path.join(ROOT_DIR, '..', '..', 'textbase', 'textbase'),  output_file_path='output.txt')\n    converter.convert()\n"}
{"type": "source_file", "path": "solidgpt/src/workskill/skills/code_chat.py", "content": "import logging\nimport os\n\nfrom solidgpt.src.configuration.configreader import ConfigReader\nfrom solidgpt.src.manager.gptmanager import GPTManager\nfrom solidgpt.src.manager.promptresource import SDE_CHAT_ADVISOR_ASSUMPTION, SDE_CODE_CHAT_ASSUMPTION\nfrom solidgpt.src.util.util import *\nfrom solidgpt.src.workskill.workskill import *\nimport tiktoken\n\nHistoryContextSeperator = '*------*'\nHistoryUserInuptLabel = 'UserInput:'\nHistorySystemOutputLabel = 'SystemOutput:'\n\n\nclass HistoryContext:\n    system_output: str\n    user_input: str\n\n    def __init__(self, system_output, user_input):\n        self.system_output = system_output\n        self.user_input = user_input\n\n    def __str__(self):\n        return f\"User Input: {self.user_input}, System Output: {self.system_output}\"\n\n\nclass CodeChat(WorkSkill):\n    Memory_Length = 3\n\n    def __init__(self, scope):\n        super().__init__()\n        self.gpt_manager = GPTManager._instance\n        self.name = SKILL_NAME_CODE_CHAT\n        self.onboarding_id_input = SkillInput(\n            \"Onboarding ID\",\n            SkillIOParamCategory.PlainText,\n        )\n        self.message = SkillInput(\n            \"User Message\",\n            SkillIOParamCategory.PlainText,\n        )\n        self.history_context = SkillInput(\n            \"History Context\",\n            SkillIOParamCategory.PlainText,\n        )\n        self.scope = scope\n        self.add_input(self.onboarding_id_input)\n        self.add_input(self.message)\n        self.add_input(self.history_context)\n        self.output_response = SkillOutput(\n            \"GPT response\",\n            SkillIOParamCategory.PlainText,\n        )\n        self.add_output(self.output_response)\n        self.code_plan_content = None\n        self.history_contexts_content = []\n        self.related_files_content = None\n        self.message_content = None\n        self.related_code_file_content = None\n        self.VECTOR_SEARCH_RESULT_LIMIT = 1\n        self.model_version = ConfigReader().config_map['openai_model']\n        self.encoding = tiktoken.encoding_for_model(self.model_version)\n\n    def _read_input(self):\n        # Get from cache or read from file\n        self.message_content = self.message.content\n        self.history_contexts_content = self.__get_history_context()\n        self.onboarding_id = self.onboarding_id_input.content\n        self.client = QdrantClient(path=os.path.join(LOCAL_STORAGE_WORKSPACE_DIR, \"qdrant\", self.onboarding_id))\n\n    def __get_history_context(self):\n        path = self.get_input_path(self.history_context)\n        if not os.path.exists(path):\n            return []\n        json_data = load_from_text(self.get_input_path(self.history_context), extension=\".json\")\n        print(json_data)\n        # Load JSON data\n        data = json.loads(json_data)\n\n        # Extract HistoryContent list\n        history_content = data[\"HistoryContent\"]\n        history_contexts_content = []\n\n        # Create a list of HistoryContext objects\n        for item in history_content:\n            system_output = item[\"SystemOutput\"]\n            user_input = item[\"UserInput\"]\n            history_context = HistoryContext(system_output, user_input)\n            history_contexts_content.append(history_context)\n        return history_contexts_content\n\n    def execution_impl(self):\n        try:\n            if len(self.scope) == 0 or (len(self.scope) == 1 and self.scope[0] == \"codebase/\"):\n                self.__build_unspecified_code_prompt()\n            else:\n                self.__build_specified_code_prompt()\n            system_output = self.__run_chat_with_repo_model()\n            # Save system_output into the history context\n            current_context = HistoryContext(system_output, self.message_content)\n            self.history_contexts_content.append(current_context)\n            # Convert the list of HistoryContext objects to a list of dictionaries\n            history_list = [{\"UserInput\": hc.user_input, \"SystemOutput\": hc.system_output} for hc in\n                            self.history_contexts_content]\n\n            # Create a dictionary with the HistoryContent key\n            data = {\"HistoryContent\": history_list}\n\n            save_to_json(data, self.history_context.param_path)\n            # Show the result\n            self._save_to_result_cache(self.output_response, str(self.__get_last_response()))\n        except Exception as e:\n            logging.error(f\"Code chat failed with error: {str(e)}\")\n            self.client.close()\n            raise e\n        # self._save_to_result_cache(self.output_response, system_output)\n        self.client.close()\n        return system_output\n\n    def __run_chat_with_repo_model(self):\n        logging.info(\"Running code chat model...\")\n        solution_stream = self.gpt_manager.create_and_chat_with_model(\n            prompt=f\"\"\"{SDE_CODE_CHAT_ASSUMPTION}\"\"\",\n            gpt_model_label=\"code chat\",\n            temperature=0.01,\n            model=self.model_version,\n            input_message=self.__get_model_input(),\n            is_stream=True\n        )\n        solution = \"\"\n        for chunk in solution_stream:\n            try:\n                if chunk.choices[0].delta.content is not None:\n                    solution += chunk.choices[0].delta.content\n                    self.callback_func(solution)\n            except Exception as e:\n                # print(chunk, solution)\n                logging.warn(f\"Failed to get the delta content with error {e}\")\n                continue\n        return solution\n\n    # def __get_model_input(self):\n    #     return f'''User Requirement: {self.message_content} \\n\n    #     Related Code Files: {self.related_code_file_content}\\n\n    #     Chat History Context: {self.history_contexts_content[-self.Memory_Length:] if len(self.history_contexts_content) > self.Memory_Length else self.history_contexts_content}\\n'''\n    \n    def __get_model_input(self):\n        return f'''User Input: {self.message_content} \\n\n        Related Code Files: {self.related_code_file_content}\\n'''\n\n    def __get_last_response(self):\n        return self.history_contexts_content[::-1][0].system_output\n\n    def __find_top_code(self):\n        embeddings_model = OpenAIEmbeddings(openai_api_key=openai.api_key)\n        embedding_query = embeddings_model.embed_query(self.message_content)\n        search = self.client.search(\n            collection_name=self.onboarding_id,\n            query_vector=embedding_query,\n            limit=self.VECTOR_SEARCH_RESULT_LIMIT\n        )\n        return search\n\n    def __build_specified_code_prompt(self):\n        content_list = []\n        for front_path in self.scope:\n            real_path = front_path[9:]\n            try:\n                with open(real_path, \"r\") as file:\n                    content = file.read()\n                    content_list.append(f\"File Path: {real_path}\\nCode content:\\n{content}\")\n            except Exception as e:\n                logging.error(f\"read file content in code chat failed. Error: {str(e)}\")\n\n        self.related_code_file_content = \"\\n\".join(content_list)\n        return\n\n    def __build_unspecified_code_prompt(self):\n        candidates = self.__find_top_code()\n        content_list = []\n        for candidate in candidates:\n            name = candidate.dict()[\"payload\"][\"filename\"]\n            code = candidate.dict()[\"payload\"][\"code\"]\n            content_list.append(f\"File Path: {name}\\nCode content:\\n{code}\")\n        self.related_code_file_content = \"\\n\".join(content_list)\n        return\n"}
{"type": "source_file", "path": "solidgpt/src/workgraph/graph.py", "content": "import logging\nimport os\nimport glob\nimport openai\nfrom solidgpt.definitions import LOCAL_STORAGE_OUTPUT_DIR\nfrom solidgpt.src.configuration.configreader import ConfigReader\nfrom solidgpt.src.manager.gptmanager import GPTManager\nfrom solidgpt.src.util.util import save_to_json\nfrom solidgpt.src.workgraph.workgraph import WorkGraph\nfrom solidgpt.src.worknode.worknode import WorkNode\nfrom solidgpt.src.workskill.skillio import SkillInputConfig, SkillInputLoadingMethod\nfrom solidgpt.src.workskill.skills.code_chat import CodeChat\nfrom solidgpt.src.workskill.skills.create_codeplan_v4 import CreateCodePlanV4\nfrom solidgpt.src.workskill.skills.create_codesolution_v3 import CreateCodeSolutionV3\nfrom solidgpt.src.workskill.skills.http_codesolution import HTTPCodeSolution\nfrom solidgpt.src.workskill.skills.notion_chat import NotionChat\nfrom solidgpt.src.workskill.skills.select_template import SelectTemplate\nfrom solidgpt.src.workskill.skills.vscode_embed_v2 import VscodeEmbedV2\nfrom solidgpt.src.workskill.skills.notion_embed import NotionEmbed\nfrom solidgpt.src.workskill.skills.repo_chat_v2 import RepoChatV2\nfrom solidgpt.src.workskill.workskill import WorkSkill\n\ndef generate_node(node_id: str, skill: WorkSkill, input_configs: list[SkillInputConfig], output_ids: list[int], manual_review_result: bool = False, graph_cache: dict = {}):\n    skill.init_config(\n        [\n            config.to_dict() for config in input_configs\n        ],\n        [\n            {\"id\": output_id} for output_id in output_ids\n        ])\n    node: WorkNode = WorkNode(node_id, skill, manual_review_result, graph_cache)\n    return node\n\n\ndef generate_node_with_output_configs(node_id: str, skill: WorkSkill, input_configs: list[SkillInputConfig], output_configs: list, manual_review_result: bool = False,  graph_cache: dict = {}):\n    skill.init_config(\n        [\n            config.to_dict() for config in input_configs\n        ],\n        [\n            config_dict for config_dict in output_configs\n        ])\n    node: WorkNode = WorkNode(node_id, skill, manual_review_result, graph_cache)\n    return node\n\ndef build_repo_chat_graph_v2(message: str, session_id: str):\n    graph = WorkGraph(output_id=session_id)\n    session_folder_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id)\n    code_plan_path = glob.glob(os.path.join(session_folder_path,  \"Code_Plan_*\"))[0]\n    relatived_code_files_path = glob.glob(os.path.join(session_folder_path,  \"Relatived_Code_File_*\"))[0]\n    # Create history context json file if not exist\n    # Define the path to the JSON file\n    history_context_path = os.path.join(session_folder_path, f'{session_id}_repochat.json')\n\n    # Check if the file already exists\n    if not os.path.exists(history_context_path):\n        # Create a default JSON data structure if the file doesn't exist\n        default_data = {\"HistoryContent\": []}\n        save_to_json(default_data, history_context_path)\n\n    tech_solution = generate_node_with_output_configs(\"1\", RepoChatV2(),\n                                                      [\n                                                          SkillInputConfig(code_plan_path, SkillInputLoadingMethod.LOAD_FROM_STRING, -1),\n                                                          SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, message),\n                                                          SkillInputConfig(history_context_path, SkillInputLoadingMethod.LOAD_FROM_STRING, -1),\n                                                          SkillInputConfig(relatived_code_files_path, SkillInputLoadingMethod.LOAD_FROM_STRING, -1),\n                                                      ],\n                                                      [\n                                                          {\"id\": 1, \"to_display\": True}\n                                                      ])\n\n    graph.add_node(tech_solution)\n    return graph\n\ndef build_code_plan_graph_v4(requirement : str, session_id : str):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    summary_path = glob.glob(os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id, \"RelatedCodeFiles*\"))[0]\n    create_code_plan = generate_node_with_output_configs(\"0\", CreateCodePlanV4(),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_STRING, -1, summary_path),\n                                ], output_configs=[{\"id\": 0, \"to_display\": True}], graph_cache=graph_cache)\n    graph.add_node(create_code_plan)\n    return graph\n\ndef build_code_solution_graph_v3(requirement : str, session_id : str, code_plan : str):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    related_code_files_path = glob.glob(os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id,  \"ProjectSummary\"))[0]\n    create_code_solution = generate_node_with_output_configs(\"1\", CreateCodeSolutionV3(),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, code_plan),\n                                    SkillInputConfig(related_code_files_path, SkillInputLoadingMethod.LOAD_FROM_STRING, -1)\n                                ], output_configs=[{\"id\": 1, \"to_display\": True}], graph_cache=graph_cache)\n    graph.add_node(create_code_solution)\n    return graph\n\n\ndef build_onboarding_graph_v4(session_id : str, base_path : str):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    logging.info(f\"base path: {base_path}\")\n    vscode_embed = generate_node_with_output_configs(\"0\", VscodeEmbedV2(),\n                                 [\n                                     SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                     SkillInputConfig(base_path,\n                                                      SkillInputLoadingMethod.LOAD_FROM_STRING,\n                                                      -1),\n                                 ],\n                                output_configs=[{\"id\": 0, \"to_display\": True}, {\"id\": 1, \"to_display\": True}])\n    graph.add_node(vscode_embed)\n    return graph\n\n\ndef build_select_template_graph(requirement: str,session_id : str):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    select_template = generate_node_with_output_configs(\"0\", SelectTemplate(),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                ], output_configs=[{\"id\": 0, \"to_display\": True}], graph_cache=graph_cache)\n    graph.add_node(select_template)\n    return graph\n\n\ndef build_http_solution_graph(requirement: str,session_id : str):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    http_solution = generate_node_with_output_configs(\"0\", HTTPCodeSolution(),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                ], output_configs=[{\"id\": 1, \"to_display\": True}])\n    graph.add_node(http_solution)\n    return graph\n\n\ndef build_notion_graph(onboarding_id : str, workspace_token: str, page_id : str):\n    graph = WorkGraph(output_id=onboarding_id)\n    notion_embed = generate_node_with_output_configs(\"0\", NotionEmbed(),\n                                 [\n                                     SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, onboarding_id),\n                                     SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, page_id),\n                                     SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, workspace_token),\n                                 ],\n                                output_configs=[{\"id\": 0, \"to_display\": True}])\n    graph.add_node(notion_embed)\n    return graph\n\n  \ndef build_code_chat_graph(requirement: str, session_id: str, scope: list[str]):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    session_folder_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id)\n    history_context_path = os.path.join(session_folder_path, f'{session_id}_chat.json')\n\n    # Check if the file already exists\n    if not os.path.exists(history_context_path):\n        # Create a default JSON data structure if the file doesn't exist\n        default_data = {\"HistoryContent\": []}\n        save_to_json(default_data, history_context_path)\n\n    chat_response = generate_node_with_output_configs(\"0\", CodeChat(scope),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                    SkillInputConfig(history_context_path, SkillInputLoadingMethod.LOAD_FROM_STRING,\n                                                     -1),\n                                ], output_configs=[{\"id\": 1, \"to_display\": True}])\n    graph.add_node(chat_response)\n    return graph\n\n\ndef build_notion_chat_graph(requirement: str, session_id: str, scope: list[str]):\n    graph = WorkGraph(output_id=session_id)\n    graph_cache = {}\n    graph_cache[\"session_id\"] = session_id\n    session_folder_path = os.path.join(LOCAL_STORAGE_OUTPUT_DIR, session_id)\n    history_context_path = os.path.join(session_folder_path, f'{session_id}_chat.json')\n\n    # Check if the file already exists\n    if not os.path.exists(history_context_path):\n        # Create a default JSON data structure if the file doesn't exist\n        default_data = {\"HistoryContent\": []}\n        save_to_json(default_data, history_context_path)\n\n    chat_response = generate_node_with_output_configs(\"0\", NotionChat(scope),\n                                [\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, session_id),\n                                    SkillInputConfig(\"\", SkillInputLoadingMethod.LOAD_FROM_CACHE_STRING, -1, requirement),\n                                    SkillInputConfig(history_context_path, SkillInputLoadingMethod.LOAD_FROM_STRING,\n                                                     -1),\n                                ], output_configs=[{\"id\": 1, \"to_display\": True}])\n    graph.add_node(chat_response)\n    return graph\n\n\n\nif __name__ == \"__main__\":\n    pass\n    # GPTManager()\n    # # openai.api_key =\n    # session_id = \"standard-test\"\n    # # base_path = \"/Users/daviddai/Documents/GitHub/SolidGPT-Private/solidgpt/src/workskill\"\n    # # graph = build_onboarding_graph_v4(session_id=session_id, base_path=base_path)\n    # # graph.init_node_dependencies()\n    # # graph.execute()\n    #\n    # requirement = \"Can you create a new skill that can generate a html page given user requirement based on this file.\"\n    # scope = [\"/Users/daviddai/Documents/GitHub/SolidGPT-Private/solidgpt/src/workskill/skills/http_codesolution.py\"]\n    # graph = build_code_chat_graph(requirement=requirement, session_id=session_id, scope=scope)\n    # # need a call back func in defined in celery task, cannot test here\n    # graph.init_node_dependencies()\n    # graph.execute()\n\n"}
{"type": "source_file", "path": "solidgpt/src/api/celery_config.py", "content": "# celery -A solidgpt.src.api.celery_tasks worker --loglevel=info -P eventlet\n# celery_config.py\n\nBROKER_URL = 'redis://localhost:6379/0'  # Using Redis as the broker\nCELERY_RESULT_BACKEND = 'redis://localhost:6379/1'\n"}
