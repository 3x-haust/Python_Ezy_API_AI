{"repo_info": {"repo_name": "EuropeanFootballLeaguePredictor", "repo_owner": "nickpadd", "repo_url": "https://github.com/nickpadd/EuropeanFootballLeaguePredictor"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_predictor_network.py", "content": "from europeanfootballleaguepredictor.models.probability_estimator import ProbabilityEstimatorNetwork\nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nfrom loguru import logger \nimport pandas as pd\nimport argparse\nimport pytest\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\n\n# Set a seed for reproducibility\nseed_value = 42  # You can choose any integer as the seed\nnp.random.seed(seed_value)\n\n@pytest.fixture \ndef create_dummy_dataframe() ->pd.DataFrame:\n    \"\"\"Creates a dummy dataframe in the format of the gathered preprocessed datasets that are input to the probability_estimator_network\n\n    Returns:\n        pd.DataFrame: A dummy dataframe in the format of the gathered preprocessed datasets\n    \"\"\"\n    dummy_data = pd.DataFrame({\n        'Date': pd.date_range(start='2023-01-01', periods=12, freq='D'),\n        'HomeTeam': np.random.choice(['TeamA', 'TeamB', 'TeamC'], size=12),\n        'AwayTeam': np.random.choice(['TeamX', 'TeamY', 'TeamZ'], size=12),\n        'Result': [f\"{np.random.randint(0, 5)}-{np.random.randint(0, 5)}\" for _ in range(12)],\n        'HomeWinOdds': np.round(np.random.uniform(1, 5, size=12), 2),\n        'DrawOdds': np.round(np.random.uniform(1, 5, size=12), 2),\n        'AwayWinOdds': np.round(np.random.uniform(1, 6, size=12), 2),\n        'OverOdds': np.round(np.random.uniform(1, 3, size=12), 2),\n        'UnderOdds': np.round(np.random.uniform(1, 3, size=12), 2),\n        'HM': np.random.randint(0, 15, size=12),\n        'HW/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'HD/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'HL/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'HG/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'HGA/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'HPTS/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'HxG/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HNPxG/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HxGA/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HNPxGA/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HNPxGD/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HPPDA': np.round(np.random.uniform(5, 20, size=12), 2),\n        'HOPPDA': np.round(np.random.uniform(5, 20, size=12), 2),\n        'HDC/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HODC/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'HxPTS/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'AM': np.random.randint(0, 15, size=12),\n        'AW/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'AD/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'AL/M': np.round(np.random.uniform(0.1, 0.9, size=12), 2),\n        'AG/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'AGA/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'APTS/M': np.round(np.random.uniform(0.1, 2.5, size=12), 2),\n        'AxG/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'ANPxG/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'AxGA/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'ANPxGA/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'ANPxGD/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'APPDA': np.round(np.random.uniform(5, 20, size=12), 2),\n        'AOPPDA': np.round(np.random.uniform(5, 20, size=12), 2),\n        'ADC/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'AODC/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'AxPTS/M': np.round(np.random.uniform(0.5, 2.5, size=12), 2),\n        'Match_id': [f\"match_{i}\" for i in range(1, 13)]\n    })\n    logger.debug(dummy_data)\n    return dummy_data\n\n\nclass TestProbabilityEstimatorNetwork:\n    \"\"\"A class of tests of the probability_estimator_network\n    \"\"\"\n    voting_dict = { 'long_term': 0.6, 'short_term': 0.4}\n    matchdays_to_drop = 4\n    tolerance = 0.03\n    def test_build_network(self) ->None:\n        \"\"\"Tests weather the network is build correctly with the regression models having been initialized as expected class instances\n        \"\"\"\n        lin = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        lin.build_network(LinearRegression)\n        svr = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        svr.build_network(SVR)\n        assert isinstance(lin.short_term_model.home_side, LinearRegression) and isinstance(lin.short_term_model.away_side, LinearRegression) and isinstance(lin.long_term_model.home_side, LinearRegression) and isinstance(lin.long_term_model.away_side, LinearRegression)\n        assert isinstance(svr.short_term_model.home_side, SVR) and isinstance(svr.short_term_model.away_side, SVR) and isinstance(svr.long_term_model.home_side, SVR) and isinstance(svr.long_term_model.away_side, SVR)\n\n    def test_drop_matchdays(self, create_dummy_dataframe :pd.DataFrame) -> None:\n        \"\"\"Tests the drop_matchdays method. Makes sure the length of the long/short form data is equal and that there are no matchdays that were supposed to be dropped in the resulting dataset\n\n        Args:\n            create_dummy_dataframe (pd.DataFrame): _description_\n        \"\"\"\n        # Create sample data for testing\n        long_term_data = create_dummy_dataframe\n        short_term_data = create_dummy_dataframe\n        \n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        filtered_long_term_data, filtered_short_term_data = network.drop_matchdays(long_term_data=long_term_data, short_term_data=short_term_data)\n        assert all(filtered_long_term_data['AM'] > TestProbabilityEstimatorNetwork.matchdays_to_drop) and all(filtered_long_term_data['HM'] > TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        assert len(filtered_long_term_data) == len(filtered_short_term_data)\n    \n    def test_normalize_array(self) ->None:\n        \"\"\"Tests the normalize_array method. Makes sure the values of the array are all in [0, 1] as expected\n        \"\"\"\n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        # Test case 1: Positive values\n        input_array = np.random.uniform(0, 10, size=(4, 4))\n        normalized_array = network.normalize_array(input_array)\n        assert ((normalized_array >= 0).all and (normalized_array <= 1).all)\n        \n    def test_get_scoreline_probabilities(self) ->None:\n        \"\"\"Tests get_scoreline_probabilities method. Makes sure that the sum of the distinct scoreline probabilities is close to 1 with a tolerance of atol\n        \"\"\"\n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        home_goal_rate = np.random.uniform(0, 5, size=10)\n        away_goal_rate = np.random.uniform(0, 4, size=10)\n        poisson_array_list = network.get_scoreline_probabilities(home_goal_rate_array= home_goal_rate, away_goal_rate_array= away_goal_rate)\n        for array in poisson_array_list:\n            assert np.isclose(np.round(np.sum(array), 2), 1, atol=self.tolerance)\n\n    def test_get_betting_probabilities(self) ->None:\n        \"\"\"Tests get_betting_probabilities method. Makes sure that the sum of the distinct betting categories is close to 1 with a tolerance of atol\n        \"\"\"\n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        home_goal_rate = np.random.uniform(0, 5, size=10)\n        away_goal_rate = np.random.uniform(0, 4, size=10)\n        poisson_array_list = network.get_scoreline_probabilities(home_goal_rate_array= home_goal_rate, away_goal_rate_array= away_goal_rate)\n        betting_probabilities_list = network.get_betting_probabilities(scoreline_prob_list=poisson_array_list)\n        for prob_list in betting_probabilities_list:\n            assert np.isclose(np.round(prob_list['home'] + prob_list['draw'] + prob_list['away'], 2), 1, atol=0.2)\n            assert np.isclose(np.round(prob_list['over2.5'] + prob_list['under2.5'], 2), 1, atol=0.2)\n            assert np.isclose(np.round(prob_list['over3.5'] + prob_list['under3.5'], 2), 1, atol=0.2)\n            assert np.isclose(np.round(prob_list['gg'] + prob_list['ng'], 2), 1, atol=self.tolerance)\n    \n    def test_prepare_for_prediction(self, create_dummy_dataframe) ->None:\n        \"\"\"Tests the prepare_for_prediction method. Makessure the length of the sort/long form as well as home/away goals and match info is equal. Tests weather there are remaining null values in the resulting data\n\n        Args:\n            create_dummy_dataframe (func): The function that creates a dummy dataframe in the format of the collected preprocessed datasets\n        \"\"\"\n        short, long, for_pred_short, for_pred_long = [create_dummy_dataframe for i in range(4)]\n        \n        for_pred_short['Yes']=np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_long['Yes']=np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_short['No'] =np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_long['No'] =np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_short['Line'] = np.random.choice(['2.5', '3.5'], size=12)\n        for_pred_long['Line'] = np.random.choice(['2.5', '3.5'], size=12)\n        for_pred_short = for_pred_short.rename(columns={'OverOdds': 'OverLineOdds', 'UnderOdds': 'UnderLineOdds'})\n        for_pred_long = for_pred_long.rename(columns={'OverOdds': 'OverLineOdds', 'UnderOdds': 'UnderLineOdds'})\n\n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        long_term_array, short_term_array, home_goals_array, away_goals_array, match_info, for_prediction_info, for_prediction_short_array, for_prediction_long_array = network.prepare_for_prediction(short_term_data=short, long_term_data=long, for_prediction_long=for_pred_long, for_prediction_short=for_pred_short)\n        assert len(long_term_array) == len(short_term_array) == len(home_goals_array) == len(away_goals_array) == len(match_info)\n        assert len(for_prediction_info) == len(for_prediction_short_array) == len(for_prediction_long_array)\n        assert not np.any(np.isnan(long_term_array))\n        assert not np.any(np.isnan(short_term_array))\n        assert not np.any(np.isnan(home_goals_array))\n        assert not np.any(np.isnan(away_goals_array))\n        assert not np.any(np.isnan(for_prediction_short_array))\n        assert not np.any(np.isnan(for_prediction_long_array))\n        assert not for_prediction_info.isna().any().any()\n        assert not match_info.isna().any().any()       \n    \n    def test_deduct_goalrate(self) ->None:\n        \"\"\"Tests the deduct goalrate method. Makes sure that there are no null goal rate values\n        \"\"\"\n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        network.build_network(regressor = LinearRegression)\n        \n        train_short = np.round(np.random.uniform(0, 1, size=(12, 34)), 2)\n        train_long = np.round(np.random.uniform(0, 1, size=(12, 34)), 2)\n        for_pred_short = np.round(np.random.uniform(0, 1, size=(12, 34)), 2)\n        for_pred_long = np.round(np.random.uniform(0, 1, size=(12, 34)), 2)\n        home_goals = np.random.randint(0, 5, size=12)\n        away_goals = np.random.randint(0, 5, size=12)\n        \n        \n        network.train_network(short_term_data=train_short, long_term_data=train_long, home_goals=home_goals, away_goals=away_goals)\n        goal_rate = network.deduct_goal_rate(for_prediction_long_form=for_pred_short, for_prediction_short_form=for_pred_long)\n        logger.debug(goal_rate)\n        assert not any(np.any(np.isnan(value)) for value in goal_rate.values())\n\n    def test_produce_probabilities(self, create_dummy_dataframe) ->None:\n        \"\"\"Tests producce_probabilities method. Makes sure the sum of the the distinc betting category probabilities is close to 1 with a tolerance of atol\n\n        Args:\n            create_dummy_dataframe (func): The function that creates a dummy dataframe in the format of the collected preprocessed datasets\n        \"\"\"\n        short, long, for_pred_short, for_pred_long = [create_dummy_dataframe for i in range(4)]\n        \n        for_pred_short['Yes']=np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_long['Yes']=np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_short['No'] =np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_long['No'] =np.round(np.random.uniform(1, 3, size=12), 2)\n        for_pred_short['Line'] = np.random.choice(['2.5', '3.5'], size=12)\n        for_pred_long['Line'] = np.random.choice(['2.5', '3.5'], size=12)\n        for_pred_short = for_pred_short.rename(columns={'OverOdds': 'OverLineOdds', 'UnderOdds': 'UnderLineOdds'})\n        for_pred_long = for_pred_long.rename(columns={'OverOdds': 'OverLineOdds', 'UnderOdds': 'UnderLineOdds'})\n        \n        network = ProbabilityEstimatorNetwork(voting_dict=TestProbabilityEstimatorNetwork.voting_dict, matchdays_to_drop=TestProbabilityEstimatorNetwork.matchdays_to_drop)\n        network.build_network(regressor = LinearRegression)\n        prediction_frame = network.produce_probabilities(long_term_data=long, short_term_data=short, for_prediction_long=for_pred_long, for_prediction_short=for_pred_short)\n        \n        logger.debug(prediction_frame)\n        for index, row in prediction_frame.iterrows():\n            win_sum = row['HomeWinProbability'] + row['DrawProbability'] + row['AwayWinProbability']\n            line2_sum = row['Under2.5Probability'] + row['Over2.5Probability']\n            line3_sum = row['Under3.5Probability'] + row['Over3.5Probability']\n            gg_sum = row['GGProbability'] + row['NGProbability']\n            assert np.isclose(np.round(win_sum, 2), 1, atol=self.tolerance)\n            assert np.isclose(np.round(line2_sum, 2), 1, atol=self.tolerance)\n            assert np.isclose(np.round(line3_sum, 2), 1, atol=self.tolerance)\n            assert np.isclose(np.round(gg_sum, 2), 1, atol=self.tolerance)\n"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/data/bookmaker_scraper.py", "content": "from bs4 import BeautifulSoup\nimport time\nfrom datetime import datetime\nimport pandas as pd\nfrom loguru import logger\nimport undetected_chromedriver as uc\nimport uuid\nimport os \nimport certifi\n\n\ndef combine_dictionaries(dictionary_list: list):\n    combined_list = []\n    for odds_type_dict in dictionary_list:\n        for dic in odds_type_dict:\n            identifier = dic[\"id\"]\n            if not has_dict_with_id(combined_list, identifier):\n                combined_list.append(dic)\n            else:\n                for match in combined_list:\n                    if match['id']==identifier:\n                        match.update(dic)\n                        pass\n                    \n    return combined_list\n    \ndef has_dict_with_id(lst, target_id):\n    if not lst:  # Check if the list is empty\n        return False\n    for d in lst:\n        if 'id' in d and d['id'] == target_id:\n            return True\n    return False\n    \nclass BookmakerScraper():\n    \"\"\"A class responsible for scraping the bookmaker website\"\"\"\n    def __init__(self, url: str, dictionary: dict):\n        \"\"\"Initializing the scraper by specifying the url and the dictionary of the team names used by the bookmaker.\n\n        Args:\n            url (str): The url corresponding to the certain webpage with the betting odds of the league specified in the configuration.\n            dictionary (dict): A dictionary of the team names used by the bookmaker.\n        \"\"\"\n        self.result_url, self.over_under_url, self.btts_url = BookmakerScraper.produce_urls(url)\n        self.dictionary = dictionary\n        os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n        os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n        self.driver = uc.Chrome(version_main = 131)\n\n    @staticmethod    \n    def produce_urls(base_url: str):\n        \"\"\"\n        Transforms the base URL of match result to over/under and btts urls.\n        \"\"\"\n        result_url = base_url + '?bt=matchresult'\n        over_under_url = base_url + '?bt=overunder'\n        btts_url = base_url + '?bt=bothteamstoscore'\n        \n        return result_url, over_under_url, btts_url\n    \n    def scroll_down(self):\n        \"\"\"A method for scrolling the page.\"\"\"\n\n        # Get scroll height.\n        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n        while True:\n\n            # Scroll down to the bottom.\n            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n            # Wait to load the page.\n            time.sleep(2)\n\n            # Calculate new scroll height and compare with last scroll height.\n            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n\n            if new_height == last_height:\n\n                break\n\n            last_height = new_height\n    \n    def get_page_soup(self, url):\n        self.driver.get(url)\n        self.driver.implicitly_wait(4)\n        self.scroll_down()\n        self.driver.implicitly_wait(4)\n        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n        \n        return soup\n    \n    def extract_odds(self, soup):\n        dictionary_list = []\n        all_matches = soup.find('div', {\"class\":\"league-page\"}).find('div', {\"class\":\"vue-recycle-scroller__item-wrapper\"}).find_all('div', {\"class\":[\"vue-recycle-scroller__item-view\", \"vue-recycle-scroller__item-view hover\"]})\n        year = datetime.now().year\n        for match in all_matches:\n            #date = match.find('span', {\"class\":\"tw-mr-0\"}).text + '/' + str(year)\n            teams = match.find_all('div', class_='tw-truncate')\n            home_team = teams[0].text.strip()\n            away_team = teams[1].text.strip()\n            selections = match.find_all('div', \"selections__selection\")\n            odds_names = []\n            odds_values = []\n            for selection in selections:\n                odds_names.extend([odd.text for odd in selection.find('span', class_='selection-horizontal-button__title')])\n                odds_values.extend([odd.text for odd in selection.find('span', class_='tw-text-s')])\n            \n            line_values = ['0.5', '1.5', '2.5', '3.5', '4.5', '5.5', '6.5']\n            match_dictionary = {'HomeTeam': home_team, 'AwayTeam': away_team}\n\n            for title in odds_names:\n                if title in line_values:\n                    match_dictionary['Line'] = title\n                    odds_names.remove(title)\n                    break \n\n            for title, value in zip(odds_names, odds_values):\n                if 'Over' in title:\n                    title = 'OverLine'\n                if 'Under' in title:\n                    title = 'UnderLine'\n                \n                match_dictionary[title] = value\n            dictionary_list.append(match_dictionary)\n        \n        identified_list = self.generate_uuids(dictionary_list, ['HomeTeam', 'AwayTeam'])   \n\n        return identified_list\n    \n    @staticmethod\n    def generate_uuids(list_of_dicts, keys):\n        for i, data in enumerate(list_of_dicts):\n            key_values = '-'.join([str(data[key]) for key in keys])\n            id = uuid.uuid5(uuid.NAMESPACE_OID, key_values)\n            data['id'] = str(id)\n            list_of_dicts[i] = data\n            \n        return list_of_dicts\n    \n    def return_odds(self):\n        odds_json_list = []\n        for url in [self.result_url, self.over_under_url, self.btts_url]:\n            soup = self.get_page_soup(url)\n            extracted_odds_dict = self.extract_odds(soup)\n            odds_json_list.append(extracted_odds_dict)\n            \n        combined_odds = combine_dictionaries(odds_json_list)\n        odds_dataframe = pd.DataFrame(combined_odds).drop(columns=['id'])\n        translated_names_odds = self.replace_team_names(odds_dataframe, self.dictionary)\n        self.driver.quit()\n        return translated_names_odds\n    \n    def replace_team_names(self, input_dataframe: pd.DataFrame, replacing_dict: dict) -> pd.DataFrame:\n        \"\"\"\n        Replaces the team names used by bookmaker with the format of understat.\n\n        Args:\n            input_dataframe (pd.DataFrame): The scraped odds dataframe.\n            replacing_dict (dict): A team name mapping to communicate different team names between bookmaker and understat.\n        \"\"\"\n        input_dataframe['HomeTeam'] = input_dataframe['HomeTeam'].replace(replacing_dict, regex=False)\n        input_dataframe['AwayTeam'] = input_dataframe['AwayTeam'].replace(replacing_dict, regex=False)\n        \n        return input_dataframe\n            \n    \n"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/__init__.py", "content": ""}
{"type": "source_file", "path": "europeanfootballleaguepredictor/models/bettor.py", "content": "import numpy as np \nfrom loguru import logger \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os\nfrom tqdm import tqdm \nfrom sklearn.svm import SVC\nimport pandas as pd\n\nclass Bettor:\n    \"\"\"A Bettor class to find and place value bets as well as give report on the results in a given set of matches\n    \"\"\"\n    def __init__(self, bank: float, kelly_cap: float) -> None:\n        \"\"\"Initializes the bettor object\n\n        Args:\n            bank (float): The initial investment the bettor has in its disposal at the start of the betting for each different betting category \n            kelly_cap (float): The max percentage of the current bankroll to bet\n        \"\"\"\n        self.starting_bank = bank\n        self.current_bankroll = {\n            'home_win_bank': bank,\n            'draw_bank': bank,\n            'away_win_bank': bank,\n            'over2.5_bank': bank,\n            'under2.5_bank': bank\n        }\n        self.ROI = {\n            'home_win_roi': 0,\n            'draw_roi': 0,\n            'away_win_roi': 0,\n            'over2.5_roi': 0,\n            'under2.5_roi': 0\n        }\n        self.NetGain = {\n            'home_win_gain': 0,\n            'draw_gain': 0,\n            'away_win_gain': 0,\n            'over2.5_gain': 0,\n            'under2.5_gain': 0\n        }\n        self.kelly_cap = kelly_cap\n        self.result_dict = {}\n    \n    def reset_bank(self) -> None:\n        \"\"\"Resests the starting bank by re-initializing the bettor\n        \"\"\"\n        self.__init__(bank= self.starting_bank, kelly_cap=self.kelly_cap)\n        \n    def preprocess(self, prediction_dataframe: pd.DataFrame, results: pd.DataFrame) -> None:    \n        \"\"\"Prepares the bettor for prediction by parsing the prediction dataframe\n\n        Args:\n            prediction_dataframe (pd.DataFrame): A dataframe containing match information, predicted model probabilities and bookmaker odds that the bettor uses to find and place value bets\n            results (pd.DataFrame): A dataframe containing the resulting scoreline of each match\n        \"\"\"\n        info_columns = ['Match_id', 'Date', 'HomeTeam', 'AwayTeam']\n        bookmaker_columns =['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverOdds', 'UnderOdds']\n        model_columns = ['HomeWinProbability', 'DrawProbability', 'AwayWinProbability', 'Over2.5Probability', 'Under2.5Probability']\n        self.info = prediction_dataframe[info_columns]\n        self.bookmaker_probabilities = prediction_dataframe[bookmaker_columns+['Match_id']]\n        self.model_probabilities = prediction_dataframe[model_columns+['Match_id']]\n        self.results = results.reset_index(drop=True)\n        \n    def quarter_kelly_criterion(self, bookmaker_odds: float, estimated_true_probability: float, bet_name: str, kelly_cap: float) -> tuple:\n        \"\"\"Returns the portion of the current bankroll to bet as well as the bet, using the quarter kelly criterion and taking into account the kelly cap as the maximum portion to bet, specified by the user in the configuration file\n\n        Args:\n            bookmaker_odds (float): A dataframe containing the bookmaker odds of the for-prediction matches\n            estimated_true_probability (float): A dataframe containing the models estimated probabilities of popular bets of the for-prediction matches\n            bet_name (str): The name of the bet the kelly criterion should consider, one of ['home_win', 'draw', 'away_win', 'over2.5', 'under2.5']\n            kelly_cap (float): A float indicating the maximum portion of the current bankroll the criterion should output, should be (0, 1]. For example for kelly_cap=0.1 the maximum output bet would be 10% of the current bankroll.\n\n        Returns:\n            tuple:\n                float: The portion of the current bankroll to bet\n                float: The value of the bet\n        \"\"\" \n        #QUARTER KELLY\n        portion_of_bet_gained_with_win = bookmaker_odds - 1\n        bankroll_portion = min(estimated_true_probability - (1 - estimated_true_probability)/portion_of_bet_gained_with_win, kelly_cap) #Capping the portion at 10%\n        bet = bankroll_portion*self.current_bankroll[f'{bet_name}_bank']/4\n        return max(bankroll_portion, 0), max(bet, 0)\n    \n    def get_betting_result(self, match_id: str) -> dict:\n        \"\"\"Searches and returns for the result dictionary of the specified match\n\n        Args:\n            match_id (str): The id of the match to return the result dictionary\n\n        Returns:\n            dict: The result dictionary with keys ['scoreline', 'home_win', 'draw', 'away_win', 'over2.5', 'under2.5'] with a string in the format of '{home_goals}-{away_goals}' for scoreline and with True or False for the betting outcomes\n        \"\"\"\n        scoreline = self.results.loc[self.results['Match_id']==match_id, 'Result'].values[0]\n        home_goals = int(scoreline.split('-')[0])\n        away_goals = int(scoreline.split('-')[1])\n    \n        result_dict = {\n            'scoreline': scoreline,\n            'home_win': True if home_goals>away_goals else False,\n            'draw': True if home_goals==away_goals else False,\n            'away_win': True if home_goals<away_goals else False,\n            'over2.5': True if home_goals+away_goals>=3 else False,\n            'under2.5': True if home_goals+away_goals<3 else False\n        }\n        self.result_dict[match_id] = result_dict\n        return result_dict\n    \n    def pay_bet(self, bet: float, bet_name: str) -> None:\n        \"\"\"Pays the specified bet amount in the bank of the bet category\n\n        Args:\n            bet (float): The value of the bet that is to be paid\n            bet_name (str): An identification of the bet category\n        \"\"\"\n        self.current_bankroll[f'{bet_name}_bank'] -= bet\n    \n    def get_payed_if_won(self, bet: float, bet_name: str, result: bool, bookmaker_odds: float) -> None:\n        \"\"\"Gets payed in the specific bet category bank and updates the ROI and NetGain of the bettor\n\n        Args:\n            bet (float): The bet value\n            bet_name (str): An identification of the bet category\n            result (bool): The result of the bet [True, False]\n            bookmaker_odds (float): The odds provided by the bookmaker\n        \"\"\"\n        if result == True:\n            self.current_bankroll[f'{bet_name}_bank'] += bet*bookmaker_odds\n        elif result == False:\n            pass\n        \n        self.NetGain[f'{bet_name}_gain'] = self.current_bankroll[f'{bet_name}_bank'] - self.starting_bank\n        self.ROI[f'{bet_name}_roi'] = 100*self.NetGain[f'{bet_name}_gain']/self.starting_bank\n            \n    def place_value_bets(self) -> dict:\n        \"\"\"Conducts the search and placement as well as the payment of the bets in each betting category and gets the results\n\n        Returns:\n            dict: A dictionary with the results of the betting process. Each of the key values contains another dictionary with the specified information divided in the betting categories. \n        \"\"\"\n        logger.info('Betting on the season...')\n        bet_columns = ['home_win', 'draw', 'away_win', 'over2.5', 'under2.5']\n        value_bets = self.info.copy()\n        for index, id in tqdm(enumerate(value_bets['Match_id']), total=len(value_bets['Match_id'])):\n            result_dict = self.get_betting_result(match_id = id)\n            for bookmaker_odds_column, model_probability_column, bet_name in zip(self.bookmaker_probabilities.columns, self.model_probabilities.columns, bet_columns):\n                bookmaker_odds = self.bookmaker_probabilities.loc[self.bookmaker_probabilities['Match_id']==id, bookmaker_odds_column].values[0]\n                model_probability = self.model_probabilities.loc[self.model_probabilities['Match_id']==id, model_probability_column].values[0]\n                portion, bet = self.quarter_kelly_criterion(bookmaker_odds=bookmaker_odds, estimated_true_probability=model_probability, bet_name=bet_name, kelly_cap=self.kelly_cap)\n                value_bets.loc[value_bets['Match_id']==id, f'scoreline'] = result_dict['scoreline']\n                value_bets.loc[value_bets['Match_id']==id, f'{bet_name}_bet'] = bet\n                value_bets.loc[value_bets['Match_id']==id, f'{bet_name}_portion'] = portion\n                self.pay_bet(bet=bet, bet_name=bet_name)\n                value_bets.loc[value_bets['Match_id']==id, f'{bet_name}_result'] = str(result_dict[bet_name])\n                self.get_payed_if_won(bet=bet, bet_name=bet_name, result=result_dict[bet_name], bookmaker_odds=bookmaker_odds)\n                value_bets.loc[value_bets['Match_id']==id,f'{bet_name}_bank'] = self.current_bankroll[f'{bet_name}_bank']\n                value_bets.loc[value_bets['Match_id']==id,f'{bet_name}_ROI'] = self.ROI[f'{bet_name}_roi']\n                value_bets.loc[value_bets['Match_id']==id,f'{bet_name}_NetGain'] = self.NetGain[f'{bet_name}_gain']\n        \n        self.value_bets = value_bets\n        return {'Investment': self.starting_bank, 'NetGain': self.NetGain, 'ROI': self.ROI}\n        \n    def produce_report_figures(self, validation_season: str, evaluation_output: str) -> dict:\n        \"\"\"Produces and saves the reporting figures in the specified path\n\n        Args:\n            validation_season (str): The season that is evaluated, one of ['2017', '2018', '2019', '2020', '2021', '2022', '2023']\n            evaluation_output (str): The specified path to save the evaluation figures\n\n        Returns:\n            dict: A dictionary containing with each key value containing the figure that corresponds to the betting of the certain betting category\n        \"\"\"\n        logger.info('Producing report figures...')\n        sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n        self.value_bets['CombinedLabel'] = self.value_bets['HomeTeam'] + '-' + self.value_bets['AwayTeam'] + ' | ' + self.value_bets['Date'].astype(str)\n        figure_dict = {}\n        for bet, bookmaker_column in tqdm(zip(['home_win', 'draw', 'away_win', 'over2.5', 'under2.5'], ['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverOdds', 'UnderOdds']), total=5):\n            value_bets_only_played = self.value_bets[self.value_bets[f'{bet}_bet'] != 0].copy()\n            fig, ax1 = plt.subplots(figsize=(40, 10))\n            fig.suptitle(f'{bet} {validation_season} \\n Investment: {self.starting_bank}€ | ROI: {round(self.ROI[f\"{bet}_roi\"], 2)}% Net Gain: {round(self.NetGain[f\"{bet}_gain\"], 2)}€')\n    \n            # Plot ROI on the first y-axis (ax1)\n            ax1.set_xlabel('Teams | Date')\n            ax1.set_ylabel('ROI', color='tab:blue')\n            sns.lineplot(data=value_bets_only_played, x='CombinedLabel', y=f'{bet}_ROI', ax=ax1, color='tab:blue', marker='o', errorbar=None)\n            ax1.tick_params(axis='y', labelcolor='tab:blue')\n    \n            # Create a second y-axis sharing the same x-axis\n            ax2 = ax1.twinx()\n            ax2.set_ylabel('Net Gain', color='tab:orange')\n            sns.lineplot(data=value_bets_only_played, x='CombinedLabel', y=f'{bet}_NetGain', ax=ax2, color='tab:orange', marker='o', errorbar=None)\n            ax2.tick_params(axis='y', labelcolor='tab:orange')\n    \n            # Find the minimum and maximum values of both y-axes\n            y_min, y_max = min(ax1.get_ylim()[0], ax2.get_ylim()[0]), max(ax1.get_ylim()[1], ax2.get_ylim()[1], 0)\n    \n            ax1.set_ylim(y_min, y_max)\n            ax2.set_ylim(y_min, y_max)\n    \n            # Set the x-axis labels and rotate them\n            x_labels = value_bets_only_played['CombinedLabel'].tolist()\n            x_labels_step = 1  # Adjust the step to control label spacing\n            ax1.set_xticks(range(0, len(x_labels), x_labels_step))\n            ax1.set_xticklabels(x_labels[::x_labels_step], rotation=90)\n    \n            # Add text labels at specific x-axis ticks using ax1.annotate()\n            for x_tick, id in zip(x_labels[::x_labels_step], value_bets_only_played['Match_id'].tolist()):\n                k = x_labels.index(x_tick)  # Get the index of the x_tick label in the DataFrame\n                odds_value = self.bookmaker_probabilities.loc[self.bookmaker_probabilities['Match_id']==id, bookmaker_column].values[0]\n                result_value = self.results.loc[self.results['Match_id']==id, 'Result'].values[0]\n    \n                label = f\"{odds_value} | {result_value}\"\n                # Calculate the y-coordinate for the label\n                # You can adjust the value (e.g., +0.02) to control the vertical placement\n                y_coord = max(y_min, y_max) + 0.02\n\n                # Add the label at the specified x and y coordinates\n                ax1.text(x_tick, y_coord, label, rotation=90, ha='center', va='center')\n\n            # Show the legend\n            ax1.legend(['ROI'], loc='lower left')\n            ax2.legend(['Net Gain'], loc='lower right')\n\n            # Customize the line appearance\n            ax1.axhline(y=-self.starting_bank, color='red', linestyle='--', label='Empty Bankroll', linewidth=2, alpha=0.5)\n            # Save the figure instead of showing it\n            directory_path = os.path.join(evaluation_output, validation_season)\n            os.makedirs(directory_path, exist_ok=True)\n            plt.tight_layout()\n            fig.savefig(os.path.join(directory_path, f'{bet}_figure.png'))\n            figure_dict[f'{bet}_figure'] = fig\n            plt.close(fig)  # Close the figure to free up resources\n        return figure_dict\n\n            \n"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/data/upcoming_matches.py", "content": "import pandas as pd \nfrom loguru import logger  \nimport requests\nimport os\nfrom datetime import datetime, timedelta\nfrom europeanfootballleaguepredictor.data.understat_gatherer import Understat_Parser\nimport uuid\nimport tempfile\nimport asyncio\nfrom europeanfootballleaguepredictor.data.preprocessor import Preprocessor\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler\nimport sys \n\nclass UpcomingMatchScheduler():\n    \"\"\"The class responsible for making the important database updates in order to predict the upcoming matches\"\"\"\n    def __init__(self, current_season: str, months_of_form_list: list, league: str, data_co_uk_ulr: str, data_co_uk_dict: dict, fixtures_url: str, fixtures_dict: dict, database = str, odds : pd.DataFrame =None) ->None:\n        \"\"\"\n        Initializing the class\n\n        Args:\n            odds (pd.DataFrame, None): A dataframe containing the bookmaker odds for the upcoming matches. Default is None in case the user does not have access to the bookmaker url.\n            current_season (str): The current season. '2023' corresponds to 2023/2024 season\n            months_of_form_list (list): A list containing the long term form and short term form months. None corresponds to season-long form\n            league (str): A string identifier of the league to gather. One of the available ['EPL', 'La_Liga', 'Bundesliga', 'Ligue_1', 'Serie_A']\n            data_co_uk_ulr (str): The url that contains the current season dataset download for the corresponding league from the data.co.uk website\n            data_co_uk_dict (dict): The dictionary that maps team names of data_co_uk format to understat format\n            fixtures_url (str): The url to update the season fixtures \n            fixtures_dict (dict): The dictionary that maps team names of fixtures dataset format to understat format\n            database (str): The database name corresponding to the league\n        \"\"\"\n        self.odds = odds \n        self.league = league\n        self.current_season = current_season \n        self.months_of_form_list = months_of_form_list\n        self.data_co_uk_url = data_co_uk_ulr\n        self.data_co_uk_dict = data_co_uk_dict\n        self.fixtures_url = fixtures_url\n        self.fixtures_dict = fixtures_dict\n        self.database_handler = DatabaseHandler(league=league, database=database)\n        self.upcoming_table = \"UpcomingFixtures\"\n        self.data_co_uk_current_season_table = f\"DataCoUk_Season{self.current_season}_{str(int(self.current_season) + 1)}\"\n        self.fixtures_table = \"SeasonFixtures\"\n    \n    def update_dataset(self, category: str) -> None:\n        \"\"\"\n        Updates the datasets of the specified category\n\n        Args:\n            category (str): A string identifier of what datasets to update. One of available 'odds', 'fixtures'\n        \"\"\"\n        if category == 'odds':\n            table_name, replacing_dict, url = self.data_co_uk_current_season_table, self.data_co_uk_dict, self.data_co_uk_url\n        elif category == 'fixtures':\n            table_name, replacing_dict, url = self.fixtures_table, self.fixtures_dict, self.fixtures_url\n            \n        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n            try:\n                response = requests.get(url)\n                if response.status_code == 200:\n                    temp_path = temp_file.name\n                    # Write the content to the temporary file\n                    temp_file.write(response.content)\n\n                    # Perform other operations (replace_team_names) on the temporary file\n                    self.replace_team_names(data_path=temp_path, replacing_dict=replacing_dict)\n                \n                    data = pd.read_csv(temp_path)\n                    self.database_handler.save_dataframes(dataframes=data, table_names=table_name)\n            \n                else:\n                    # Log failure\n                    logger.error(f\"Failed to download the file. Status code: {response.status_code}\")\n                    sys.exit(1)\n\n            finally:\n                temp_file.close()\n                os.remove(temp_path)\n                # Log success\n                logger.success(f\"File downloaded and saved to database table {table_name}\")\n\n            \n    def replace_team_names(self, data_path :str, replacing_dict : dict) -> None:\n        \"\"\"\n        Replaces the team names using a dictionary mapping in the specified file of the data_path\n\n        Args:\n            data_path (str): The path to the file to replace the team names of\n            replacing_dict (dict): The dictionary that maps team names of the dataset format to understat format\n        \"\"\"\n        logger.info('Replacing team names.')\n        data = pd.read_csv(data_path)\n        team_dict = replacing_dict\n        try:\n            data['HomeTeam'] = data['HomeTeam'].replace(team_dict)\n            data['AwayTeam'] = data['AwayTeam'].replace(team_dict)\n        except KeyError:\n            data['Home Team'] = data['Home Team'].replace(team_dict)\n            data['Away Team'] = data['Away Team'].replace(team_dict)\n            \n        data.to_csv(data_path, index=False)\n        \n    def setup_upcoming_fixtures(self) ->None:\n        \"\"\"A pipeline to read update and write the updated datasets in order for the model to predict upcoming matches\"\"\"\n        fixtures = self.database_handler.get_data(table_names=self.fixtures_table)[0]\n        fixtures.rename(columns={'Home Team': 'HomeTeam', 'Away Team': 'AwayTeam'}, inplace=True)\n        fixtures['Date'] = pd.to_datetime(fixtures['Date'], format=\"%d/%m/%Y %H:%M\")\n        fixtures['Date'] = fixtures['Date'].dt.strftime(\"%d/%m/%Y\")\n        \n        try:\n            upcoming_matches = pd.merge(fixtures[['Date', 'HomeTeam', 'AwayTeam']], self.odds, on=['HomeTeam', 'AwayTeam'], how='inner')\n            upcoming_matches.rename(columns={'1': 'HomeWinOdds', 'X': 'DrawOdds', '2': 'AwayWinOdds', 'OverLine': 'OverLineOdds', 'UnderLine': 'UnderLineOdds'}, inplace=True)\n            upcoming_matches.drop(columns=['Yes', 'No'])\n        except (KeyError, TypeError) as e:\n            logger.warning(e)\n            today = datetime.now()\n            fifteen_days_from_now = today + timedelta(days=15)\n            fixtures['Date'] = pd.to_datetime(fixtures['Date'], format='%d/%m/%Y')\n            upcoming_matches = fixtures[(fixtures['Date'] >= today) & (fixtures['Date'] <= fifteen_days_from_now)][['Date', 'HomeTeam', 'AwayTeam']]\n            upcoming_matches['Date'] = upcoming_matches['Date'].dt.strftime('%d/%m/%Y')\n             \n        self.database_handler.save_dataframes(dataframes=upcoming_matches, table_names=self.upcoming_table)\n        logger.success(f'Upcoming fixtures saved succesfully at {self.upcoming_table}')\n        \n        database = f'europeanfootballleaguepredictor/data/database/{self.league}_database.db'\n        understat_parser = Understat_Parser(league=self.league, dictionary=self.data_co_uk_dict, database=database)\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(understat_parser.get_upcoming_match_stats(current_season= self.current_season, months_of_form_list= self.months_of_form_list))\n            \n        preprocessor = Preprocessor(league=self.league, database=database)\n        upcoming_raw_long, upcoming_raw_short = preprocessor.database_handler.get_data(table_names=[\"Raw_UpcomingLongTerm\", \"Raw_UpcomingShortTerm\"])\n        upcoming_preprocessed_list = preprocessor.preprocessing_pipeline(data = [upcoming_raw_long, upcoming_raw_short])\n        preprocessor.database_handler.save_dataframes(dataframes=upcoming_preprocessed_list, table_names=[\"Preprocessed_UpcomingLongTerm\", \"Preprocessed_UpcomingShortTerm\"])\n        \n        \n"}
{"type": "source_file", "path": "run_data_preprocessing.py", "content": "from europeanfootballleaguepredictor.common.config_parser import Config_Parser\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom europeanfootballleaguepredictor.data.preprocessor import Preprocessor\nimport os \nfrom loguru import logger\nimport argparse\nimport sys\nimport pandas as pd \n\ndef main():\n    \"\"\"Main entry point for the data preprocessing script.\"\"\"\n    \n    #Parsing the configuration file\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    config_file_path = parser.parse_args().config\n    \n    #Loading the configuration settings\n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n    \n    logger.info(config)\n    \n    preprocessor = Preprocessor(league=config.league, database=config.database)\n\n    #Gathering all the seasons into two concatenated dataframes one for long term and one for short term form\n    long_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_LongTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    short_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_ShortTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    long_term_form_dataframe = pd.concat([dataframe for dataframe in long_term_form_season_list])\n    short_term_form_dataframe = pd.concat([dataframe for dataframe in short_term_form_season_list])\n    \n    preprocessed_dataframes = preprocessor.preprocessing_pipeline(data=[long_term_form_dataframe, short_term_form_dataframe])\n    preprocessor.database_handler.save_dataframes(dataframes=preprocessed_dataframes, table_names=['Preprocessed_LongTermForm', 'Preprocessed_ShortTermForm'])\n    \n    \n    \n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/data/preprocessor.py", "content": "import pandas as pd \nfrom loguru import logger \nimport uuid\nimport numpy as np\nimport sys\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler\n\nclass Preprocessor():\n    \"\"\"A class responsible for preprocessing the raw collected datasets\n    \"\"\"\n    def __init__(self, league :str, database :str):\n        \"\"\"\n        Initializes the Preprocessor.\n\n        Args:\n            league (str): The name of the league associated with the preprocessing.\n            database (str): The path to the SQLite database file.\n        \"\"\"\n        self.database_handler = DatabaseHandler(league = league, database= database)\n    \n    def produce_match_id(self, dataframe_list : list) -> list:\n        \"\"\"\n        Produces a unique identification in 'Match_id' for each match in the list of dataframes.\n\n        Args:\n            dataframe_list (list): List of dataframes for which 'Match_id' will be created.\n\n        Returns:\n            list: The input list of dataframes with the unique identifier 'Match_id' column.\n        \"\"\"\n\n        dataframes_with_id = []\n\n        for dataframe in dataframe_list:\n            dataframe['Match_id'] = dataframe.apply(lambda row: uuid.uuid5(uuid.NAMESPACE_DNS, f\"{row['HomeTeam']}_{row['AwayTeam']}_{row['Date']}\"), axis=1)\n            dataframe = dataframe.drop(columns=['HTeam', 'ATeam'])\n            dataframe['Match_id'] = dataframe['Match_id'].astype(str)\n            dataframes_with_id.append(dataframe)\n        logger.success(f'Succesfully produced Match_id')\n        return dataframes_with_id\n\n    def stats_per_match(self, dataframe_list : list) -> list:\n        \"\"\"\n        Normalizes the advanced statistics to convey statistics per match.\n\n        Args:\n            dataframe_list (list): List of dataframes.\n\n        Returns:\n            list or pd.DataFrame: The input list of dataframes with the statistics normalized per match and the columns renamed.\n        \"\"\"\n        data_per_match = []\n        for dataframe in dataframe_list:\n            for col in ['HW', 'HD', 'HL', 'HG', 'HGA', 'HPTS', 'HxG', 'HNPxG', 'HxGA', 'HNPxGA', 'HNPxGD', 'HDC', 'HODC', 'HxPTS']:\n                dataframe[col] = dataframe[col].astype(float).div(dataframe['HM'].astype(int)).fillna(0)\n                dataframe.rename(columns = {col: f'{col}/M'}, inplace=True)\n\n            for col in ['AW', 'AD', 'AL', 'AG', 'AGA', 'APTS', 'AxG', 'ANPxG', 'AxGA', 'ANPxGA', 'ANPxGD', 'ADC', 'AODC', 'AxPTS']:\n                dataframe[col] = dataframe[col].astype(float).div(dataframe['AM'].astype(int)).fillna(0)\n                dataframe.rename(columns = {col: f'{col}/M'}, inplace=True)\n\n            data_per_match.append(dataframe)\n                \n        logger.success('Succesfully normalized stats per match.')   \n        return data_per_match\n                \n    def test_for_nulls(self, data: list, data_status: str) -> list:\n        \"\"\"\n        Tests the datasets and deals with null values.\n\n        Args:\n            data (list): A list of dataframes or a dataframe to be checked.\n            data_status (str): Identifier for the status of the data. One of ['Raw', 'Preprocessed'].\n\n        Returns:\n            list: A list of dataframes with checked and dealt with null values according to the data status.\n        \"\"\"\n        filtered_data_list=[]\n        odds_columns = ['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverOdds', 'UnderOdds']\n        if data_status == 'Raw':\n            stats_columns = ['HW', 'HD', 'HL', 'HG', 'HGA', 'HPTS', 'HxG', 'HNPxG', 'HxGA', 'HNPxGA', 'HNPxGD', 'HDC', 'HODC', 'HxPTS', 'HPPDA', 'HOPPDA',\n                             'AW', 'AD', 'AL', 'AG', 'AGA', 'APTS', 'AxG', 'ANPxG', 'AxGA', 'ANPxGA', 'ANPxGD', 'ADC', 'AODC', 'AxPTS', 'APPDA', 'AOPPDA']\n        if data_status == 'Preprocessed':\n            stats_columns = ['HW/M', 'HD/M', 'HL/M', 'HG/M', 'HGA/M', 'HPTS/M', 'HxG/M', 'HNPxG/M', 'HxGA/M', 'HNPxGA/M', 'HNPxGD/M', 'HDC/M', 'HODC/M', 'HxPTS/M', 'HPPDA', 'HOPPDA',\n                             'AW/M', 'AD/M', 'AL/M', 'AG/M', 'AGA/M', 'APTS/M', 'AxG/M', 'ANPxG/M', 'AxGA/M', 'ANPxGA/M', 'ANPxGD/M', 'ADC/M', 'AODC/M', 'AxPTS/M', 'APPDA', 'AOPPDA']\n        \n        info_columns = ['HomeTeam', 'AwayTeam', 'Result']\n        \n        for dataframe in data:\n            contains_nulls = dataframe.isnull().any().any()\n            if contains_nulls:\n                info_has_nulls = dataframe[info_columns].isnull().any()\n                stats_have_nulls = dataframe[stats_columns].isnull().any()\n                odds_have_nulls = dataframe[odds_columns].isnull().any()\n                if info_has_nulls.any():\n                    null_rows = dataframe[info_columns].isnull().any(axis=1)\n                    logger.error(f'{data_status} data contain {len(dataframe.loc[null_rows, info_columns])} NaN values in the team names:\\n {dataframe.loc[null_rows, info_columns]} \\n Usually, this error occurs when league dictionaries are not updated correctly! \\n Ending the run...')\n                    sys.exit(1)\n                if stats_have_nulls.any():\n                    null_rows = dataframe[stats_columns].isnull().any(axis=1)\n                    logger.warning(f'{data_status} data contain {len(dataframe.loc[null_rows, stats_columns])} NaN values in the statistics:\\n {dataframe.loc[null_rows, stats_columns]} \\n Usually, this warning occurs due to data_co_uk datasets containing {None} values. DELETING THE ABOVE ENTRIES!')\n                    logger.warning(f\"This might be the case because of the replacing dictionary missing a team. Team names: {np.unique(dataframe.loc[null_rows, info_columns], return_counts=True)}\")\n                    dataframe.dropna(subset=stats_columns, inplace=True)\n                if odds_have_nulls.any():\n                    null_rows = dataframe[odds_columns].isnull().any(axis=1)\n                    logger.warning(f'{data_status} data contain NaN values in the odds:\\n {dataframe.loc[null_rows, odds_columns]} \\n Usually, this warning occurs due to data_co_uk datasets containing {None} values. DELETING THE ABOVE ENTRIES!')\n                    dataframe.dropna(subset=odds_columns, inplace=True)\n                \n                filtered_data_list.append(dataframe)\n            if contains_nulls and data_status=='Preprocessed':\n                logger.error('Unexpected nulls in the preprocessed datasets! \\n Ending the run...')\n                sys.exit(1)\n            if not contains_nulls:\n                logger.success(f'No NaN values were detected!')\n                filtered_data_list.append(dataframe)\n                \n        return filtered_data_list\n                \n            \n    def preprocessing_pipeline(self, data: list) -> list:\n        \"\"\"\n        A pipeline that produces preprocessed dataframes out of the raw dataframes list.\n\n        Args:\n            data (list): A list containing dataframes in the raw format of collected dataframes.\n\n        Returns:\n            list: A list of preprocessed dataframes.\n        \"\"\"\n        data = self.test_for_nulls(data,  data_status='Raw')\n        data = self.produce_match_id(data)\n        data = self.stats_per_match(data)\n        self.test_for_nulls(data, data_status='Preprocessed')\n        \n        return data\n    \n    "}
{"type": "source_file", "path": "run_updates.py", "content": "from europeanfootballleaguepredictor.data.bookmaker_scraper import BookmakerScraper\nfrom europeanfootballleaguepredictor.data.understat_gatherer import Understat_Parser\nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nfrom europeanfootballleaguepredictor.data.upcoming_matches import UpcomingMatchScheduler\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom loguru import logger\nimport asyncio\nimport os\nfrom europeanfootballleaguepredictor.data.preprocessor import Preprocessor\nimport argparse\nimport pandas as pd\n        \n\"\"\"\nEuropean Football League Predictor\n\nThis script performs data scraping and processing for predicting outcomes in the configuration specified European Football League.\n\"\"\"\n\ndef main():\n    \"\"\"Main entry point for the script.\n\n    This function orchestrates the entire data scraping and processing pipeline.\n    \"\"\"\n    \n    #Parsing the configuration file\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    config_file_path = parser.parse_args().config\n    \n    # Loading and extracting configuration data\n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n    \n    logger.info(config)\n    \n\n    understat_parser = Understat_Parser(league = config.league, dictionary = config.data_co_uk_dictionary, database=config.database)\n    try:\n        bookmaker_scraper = BookmakerScraper(url = config.bookmaker_url, dictionary = config.bookmaker_dictionary)\n        odds_dataframe = bookmaker_scraper.return_odds()\n        logger.success('Successfully retrieved odds!')\n        logger.info(f'\\n {odds_dataframe}')\n    except Exception as e:\n        logger.warning(f'Error while fetching bookmaker odds: {e}')\n        logger.warning('The bookmaker url may not be accessible from your IP address. Try using a Greek IP vpn! Will proceed without scraped bookmaker odds!')\n        odds_dataframe = None\n    \n\n    upcoming_match_scheduler = UpcomingMatchScheduler(\n        league = config.league,\n        odds = odds_dataframe, \n        current_season = config.current_season,\n        months_of_form_list= config.months_of_form_list,\n        data_co_uk_ulr= config.data_co_uk_url, \n        data_co_uk_dict= config.data_co_uk_dictionary, \n        fixtures_url = config.fixture_download_url,\n        fixtures_dict = config.fixture_download_dictionary,\n        database = config.database\n        )\n    upcoming_match_scheduler.update_dataset('odds')\n    upcoming_match_scheduler.update_dataset('fixtures')\n    upcoming_match_scheduler.setup_upcoming_fixtures()\n    \n    #Updating the UpcomingShortTerm and UpcomingLongTerm tables\n    for name, months_of_form in zip(['Raw_LongTermForm', 'Raw_ShortTermForm'], config.months_of_form_list):\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(understat_parser.get_understat_season(season = config.current_season, months_of_form = months_of_form, output_table_name=name))\n    \n    #Preprocessing season raw statistics   \n    preprocessor = Preprocessor(league=config.league, database=config.database)\n    long_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_LongTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    short_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_ShortTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    long_term_form_dataframe = pd.concat([dataframe for dataframe in long_term_form_season_list])\n    short_term_form_dataframe = pd.concat([dataframe for dataframe in short_term_form_season_list])\n    \n    preprocessed_dataframes = preprocessor.preprocessing_pipeline(data=[long_term_form_dataframe, short_term_form_dataframe])\n    preprocessor.database_handler.save_dataframes(dataframes=preprocessed_dataframes, table_names=['Preprocessed_LongTermForm', 'Preprocessed_ShortTermForm'])\n    \nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/models/probability_estimator.py", "content": "from europeanfootballleaguepredictor.models.base_model import BaseModel\nfrom europeanfootballleaguepredictor.models.bettor import Bettor\nimport numpy as np\nfrom scipy.stats import poisson\nimport pandas as pd \nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom loguru import logger\nimport json\n\nclass FootballPredictor(BaseModel):\n    \"\"\"A model that combines regressor models for home and away side goal rate prediction.\n\n    Args:\n        BaseModel : A base model upon which FootballPredictor is built.\n    \"\"\"\n    def __init__(self) -> None:\n        \"\"\"Initializing the home_side and away_side predictors as None.\n        \"\"\"\n        self.home_side = None\n        self.away_side = None\n    \n    def build_model(self, regressor) ->None:\n        \"\"\"Making home_side and away_side instances of the regressor. In case of PoissonRegressor sets max_iter = 1000 to ensure convergance.\n\n        Args:\n            regressor (class): The regressor class.\n        \"\"\"\n        self.home_side = regressor()\n        self.away_side = regressor()\n        \n        if isinstance(self.home_side, PoissonRegressor):\n            self.home_side = regressor(max_iter = 1000)\n            self.away_side = regressor(max_iter = 1000)\n        \n\n    \n    def train_model(self, train_data, home_goals, away_goals) -> None:\n        \"\"\"Trains the regressors \n\n        Args:\n            train_data (np.array): An array of the normalized train dataset X.\n            home_goals (np.array): An array of the target values Y of the home_side predictor.\n            away_goals (np.array): An array of the target values Y of the away_side predictor.\n        \"\"\"\n        self.home_side.fit(train_data, home_goals.ravel())\n        self.away_side.fit(train_data, away_goals.ravel())\n    \n    def evaluate(self):\n        \"\"\"Not utilized\n        \"\"\"\n        pass \n    \n    def predict(self, data_for_prediction) -> dict:\n        \"\"\"Predicts the input data_for_prediction.\n\n        Args:\n            data_for_prediction (np.array): A normalized array in the format of train_data that is given for prediction to the regressor models.\n\n        Returns:\n            dict: A dictionary with the prediction of the model for 'home', 'away' goal rate. Minimum of 0 for both.\n        \"\"\"\n        return {'home': np.maximum(self.home_side.predict(data_for_prediction), 0), 'away': np.maximum(self.away_side.predict(data_for_prediction), 0)}\n\n\nclass ProbabilityEstimatorNetwork:\n    \"\"\"A network of two FootballPredictor objects that is responsible for predicting the probabilities of certain outcomes.\n    \"\"\"\n    def __init__(self, voting_dict: dict, matchdays_to_drop: int) -> None:\n        \"\"\"Initializing the network\n\n        Args:\n            voting_dict (dict): A dictionary that provides the weights of long_term_from and short_term_form\n            matchdays_to_drop (int): The matchdays at the start of the season that are considered to provide redundant information to the model because the league table is not yet indicative of the teams performance due to small sample size.\n        \"\"\"\n        self.short_term_model = FootballPredictor()\n        self.long_term_model = FootballPredictor()\n        self.voting_dict = voting_dict\n        self.matchdays_to_drop = matchdays_to_drop\n    \n    def build_network(self, regressor) -> None:\n        \"\"\"Building the network\n\n        Args:\n            regressor (class): The regressor class.\n        \"\"\"\n        self.short_term_model.build_model(regressor)\n        self.long_term_model.build_model(regressor)\n        \n    def drop_matchdays(self, long_term_data: pd.DataFrame, short_term_data: pd.DataFrame) -> tuple:\n        \"\"\"Dropping the number of matchdays at the start of each season.\n\n        Args:\n            long_term_data (pd.DataFrame): A dataframe with the long term form team statistics.\n            short_term_data (pd.DataFrame): A dataframe with the short term form team statistics.\n\n        Returns: \n            tuple:\n                pd.DataFrame: A dataframe with the statistics of the long term form, having filtered out the number of matchdays to drop.\n                pd.DataFrame: A dataframe with the statistics of the short term form, having filtered out the number of matchdays to drop.\n        \"\"\"\n        filtered_long_term_data = long_term_data[(long_term_data['HM'] > self.matchdays_to_drop)&((long_term_data['AM'] > self.matchdays_to_drop))]\n        filtered_short_term_data = short_term_data[short_term_data['Match_id'].isin(filtered_long_term_data['Match_id'])]\n        return filtered_long_term_data, filtered_short_term_data\n\n    def normalize_array(self, array) -> np.array:\n        \"\"\"Normalizes the input array to [0, 1]\n\n        Args:\n            array (np.array): The array to be normalized\n\n        Returns:\n            np.array: The normalized array with values in [0, 1]\n        \"\"\"\n        scaler = MinMaxScaler()\n        normalized_array = scaler.fit_transform(array)\n        return normalized_array\n    \n    def prepare_for_prediction(self, short_term_data: pd.DataFrame, long_term_data: pd.DataFrame, for_prediction_short: pd.DataFrame, for_prediction_long: pd.DataFrame) -> tuple:\n        \"\"\"Gets the datasets in the form loaded from the .csv files and prepares them for prediction. Calls drop_matchdays() and normalize_array().\n\n        Args:\n            short_term_data (pd.DataFrame): A dataframe with the unpreprocessed short term form team statistic to train from.\n            long_term_data (pd.DataFrame): A dataframe with the unpreprocessed long term form team statistics to train from.\n            for_prediction_short (pd.DataFrame): A dataframe with the preprocessed short term form team statistic to predict.\n            for_prediction_long (pd.DataFrame): A dataframe with the preprocessed long term form team statistic to predict.\n\n        Returns:\n            tuple:\n                np.array: An array of long term form data prepared the model training\n                np.array: An array of short term form data prepared the model training\n                np.array: An array of home goals\n                np.array: An array of away goals\n                pd.DataFrame: A dataframe containing match information for the training data\n                pd.DataFrame: A dataframe containing match information for the for-prediction data\n                np.array: An array of short term form data prepared for model prediction\n                np.array: An array of long term form data prepared for model prediction\n                \n            **ATTENTION** The training data and for prediction data have slightly different naming columns due to the for prediction data being scraped and having a flactuating under/over line.\n        \"\"\"\n        long_term_data, short_term_data = self.drop_matchdays(long_term_data=long_term_data, short_term_data=short_term_data)\n        match_info = long_term_data[['Match_id', 'Date', 'HomeTeam', 'AwayTeam', 'Result', 'HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverOdds', 'UnderOdds']]\n        try:\n            for_prediction_info = for_prediction_long[['Match_id', 'Date', 'HomeTeam', 'AwayTeam', 'HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'Line', 'OverLineOdds', 'UnderLineOdds', 'Yes', 'No']]\n        except KeyError:\n            for_prediction_info = for_prediction_long[['Match_id', 'Date', 'HomeTeam', 'AwayTeam']]\n            \n        match_info = match_info.copy()\n        match_info.loc[:, 'HomeGoals'] = match_info['Result'].str.split('-').str[0].astype(int)\n        match_info.loc[:, 'AwayGoals'] = match_info['Result'].str.split('-').str[1].astype(int)\n        match_info = match_info.drop('Result', axis=1)\n        \n        home_goals_array = np.array(match_info[['HomeGoals']])\n        away_goals_array = np.array(match_info[['AwayGoals']])\n        \n        #Keeping the necessary columns\n        long_term_array = np.array(\n                                    long_term_data[\n                                       [\n                                        'HM', 'HW/M', 'HD/M', 'HL/M', 'HG/M', 'HGA/M', 'HPTS/M', 'HxG/M', 'HNPxG/M', 'HxGA/M', 'HNPxGA/M', 'HNPxGD/M', 'HPPDA', 'HOPPDA', 'HDC/M', 'HODC/M', 'HxPTS/M',\n                                       'AM', 'AW/M', 'AD/M', 'AL/M', 'AG/M', 'AGA/M', 'APTS/M', 'AxG/M', 'ANPxG/M', 'AxGA/M', 'ANPxGA/M', 'ANPxGD/M', 'APPDA', 'AOPPDA', 'ADC/M', 'AODC/M', 'AxPTS/M']\n                                       ])\n\n        short_term_array = np.array(\n                                    short_term_data[\n                                       [\n                                        'HM', 'HW/M', 'HD/M', 'HL/M', 'HG/M', 'HGA/M', 'HPTS/M', 'HxG/M', 'HNPxG/M', 'HxGA/M', 'HNPxGA/M', 'HNPxGD/M', 'HPPDA', 'HOPPDA', 'HDC/M', 'HODC/M', 'HxPTS/M',\n                                        'AM', 'AW/M', 'AD/M', 'AL/M', 'AG/M', 'AGA/M', 'APTS/M', 'AxG/M', 'ANPxG/M', 'AxGA/M', 'ANPxGA/M', 'ANPxGD/M', 'APPDA', 'AOPPDA', 'ADC/M', 'AODC/M', 'AxPTS/M']\n                                       ])\n        for_prediction_long_array = np.array(\n                                    for_prediction_long[\n                                       [\n                                        'HM', 'HW/M', 'HD/M', 'HL/M', 'HG/M', 'HGA/M', 'HPTS/M', 'HxG/M', 'HNPxG/M', 'HxGA/M', 'HNPxGA/M', 'HNPxGD/M', 'HPPDA', 'HOPPDA', 'HDC/M', 'HODC/M', 'HxPTS/M',\n                                        'AM', 'AW/M', 'AD/M', 'AL/M', 'AG/M', 'AGA/M', 'APTS/M', 'AxG/M', 'ANPxG/M', 'AxGA/M', 'ANPxGA/M', 'ANPxGD/M', 'APPDA', 'AOPPDA', 'ADC/M', 'AODC/M', 'AxPTS/M']\n                                       ])                                       \n        for_prediction_short_array = np.array(\n                                    for_prediction_short[\n                                       [\n                                        'HM', 'HW/M', 'HD/M', 'HL/M', 'HG/M', 'HGA/M', 'HPTS/M', 'HxG/M', 'HNPxG/M', 'HxGA/M', 'HNPxGA/M', 'HNPxGD/M', 'HPPDA', 'HOPPDA', 'HDC/M', 'HODC/M', 'HxPTS/M',\n                                        'AM', 'AW/M', 'AD/M', 'AL/M', 'AG/M', 'AGA/M', 'APTS/M', 'AxG/M', 'ANPxG/M', 'AxGA/M', 'ANPxGA/M', 'ANPxGD/M', 'APPDA', 'AOPPDA', 'ADC/M', 'AODC/M', 'AxPTS/M']\n                                       ])          \n        \n        for array in [long_term_array, short_term_array, for_prediction_short_array, for_prediction_long_array]:\n            array = self.normalize_array(array)\n\n        return long_term_array, short_term_array, home_goals_array, away_goals_array, match_info, for_prediction_info, for_prediction_short_array, for_prediction_long_array\n        \n    def train_network(self, short_term_data: np.array, long_term_data: np.array, home_goals: np.array, away_goals: np.array) -> None:\n        \"\"\"Trains the network using the input data\n\n        Args:\n            short_term_data (np.array): An array containing the short term form training data\n            long_term_data (np.array): An array containing the long term form training data\n            home_goals (np.array): An array containing the home goals training target values\n            away_goals (np.array): An array containing the away goals training target values\n        \"\"\"\n        self.short_term_model.train_model(train_data = short_term_data, home_goals = home_goals, away_goals = away_goals)\n        self.long_term_model.train_model(train_data = long_term_data, home_goals = home_goals, away_goals = away_goals)\n    \n    def deduct_goal_rate(self, for_prediction_short_form: np.array, for_prediction_long_form: np.array) -> dict:\n        \"\"\"Predicts the home and away goal rate using the short/long term prediction models for home and away side.\n\n        Args:\n            for_prediction_short_form (np.array): An array of short term form data prepared for model prediction\n            for_prediction_long_form (np.array): An array of long term form data prepared for model prediction\n\n        Returns:\n            dict: A dictionary containing 'home' and 'away' side goal rate values accessible by their respective keys. The goal rate is deducted by a weighted average with the weights provided by the user in the configuration file.\n        \"\"\"\n        short_term_prediction  = self.short_term_model.predict(for_prediction_short_form)\n        long_term_prediction = self.long_term_model.predict(for_prediction_long_form)\n        return {'home': (self.voting_dict['short_term']*short_term_prediction['home'] + self.voting_dict['long_term']*long_term_prediction['home']).flatten(), 'away': (self.voting_dict['short_term']*short_term_prediction['away'] + self.voting_dict['long_term']*long_term_prediction['away']).flatten()}\n    \n    def get_scoreline_probabilities(self, home_goal_rate_array: np.array, away_goal_rate_array: np.array) -> list:\n        \"\"\"Gets the unique scoreline probabilities by using the Poisson mass probability function for the deducted home and away goal rates.\n\n        Args:\n            home_goal_rate_array (np.array): An array containing the goal rates for the home sides of the predicted matches\n            away_goal_rate_array (np.array): An array containing the goal rates for the away sides of the predicted matches\n\n        Returns:\n            list: A list of arrays. Each array contains each scoreline predicted probability such that scoreline_array[home_goals][away_goals] = Probability_of_scoreline(home_goals-away_goals)\n        \"\"\"\n        max_g = 12\n        goal_values = np.arange(max_g + 1)\n        \n        poisson_home = np.zeros((len(goal_values), 1))\n        poisson_away = np.zeros((1, len(goal_values)))\n        poisson_array_list = []\n        \n        for home_rate, away_rate in zip(home_goal_rate_array, away_goal_rate_array):\n            for goal in goal_values:\n                poisson_home[goal, 0] = poisson.pmf(goal, home_rate).item()\n                poisson_away[0, goal] = poisson.pmf(goal, away_rate).item()\n            poisson_array = np.matmul(poisson_home, poisson_away)\n            poisson_array_list.append(poisson_array)\n\n        return poisson_array_list\n    \n    def get_betting_probabilities(self, scoreline_prob_list: list) -> list:\n        \"\"\"Gets the probabilities for the most popular betting results using the scoreline probability arrays\n\n        Args:\n            scoreline_prob_list (list): A list containing an array of the scoreline probabilities for each predicted match.\n\n        Returns:\n            list: A list of dictionaries. Each dictionary corresponds to a predicted match and contains the predicted probability of the respective bet shown by the key.\n        \"\"\"\n        betting_probabilities_list = []\n        for scoreline_prob_array in scoreline_prob_list:\n            rows = len(scoreline_prob_array)\n            columns = len(scoreline_prob_array[0])\n            draw = np.trace(scoreline_prob_array)\n            away_win = 0.0\n            home_win = 0.0\n            over2 = 0.0\n            under2 = 0.0\n            over3 = 0.0\n            under3 = 0.0\n            over4 = 0.0\n            under4 = 0.0\n            over1 = 0.0\n            under1 = 0.0\n            gg = 0.0\n            ng = 0.0 \n            \n            for away_goals in range(rows):\n                for home_goals in range(columns):\n                    if (home_goals>away_goals):\n                        home_win += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals>home_goals):\n                        away_win += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals+home_goals>=2):\n                        over1 += scoreline_prob_array[home_goals, away_goals]\n                    if(away_goals+home_goals<2):\n                        under1 += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals+home_goals>=3):\n                        over2 += scoreline_prob_array[home_goals, away_goals]\n                    if(away_goals+home_goals<3):\n                        under2 += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals+home_goals>=4):\n                        over3 += scoreline_prob_array[home_goals, away_goals]\n                    if(away_goals+home_goals<4):\n                        under3 += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals+home_goals>=5):\n                        over4 += scoreline_prob_array[home_goals, away_goals]\n                    if(away_goals+home_goals<5):\n                        under4 += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals==0) or (home_goals==0): \n                        ng += scoreline_prob_array[home_goals, away_goals]\n                    if (away_goals!=0) and (home_goals!=0):\n                        gg += scoreline_prob_array[home_goals, away_goals]\n            \n            betting_probabilities_list.append({'home': home_win, 'draw': draw, 'away': away_win, 'over2.5': over2, 'under2.5': under2, 'over3.5': over3, 'under3.5': under3, 'over4.5': over4, 'under4.5': under4, 'over1.5': over1, 'under1.5': under1, 'ng': ng, 'gg': gg})\n            \n        return betting_probabilities_list    \n    \n    def get_prediction_dataframe(self, for_prediction_info: pd.DataFrame, scoreline_probabilities: np.array, betting_probabilities: dict) -> pd.DataFrame:\n        \"\"\"Produces a dataframe with the predicted probabilities for each match\n\n        Args:\n            for_prediction_info (pd.DataFrame): A dataframe containing match information for the for-prediction data\n            scoreline_probabilities (np.array): A list of arrays. Each array contains each scoreline predicted probability such that scoreline_array[home_goals][away_goals] = Probability_of_scoreline(home_goals-away_goals)\n            betting_probabilities (dict): A list of dictionaries. Each dictionary corresponds to a predicted match and contains the predicted probability of the respective bet shown by the key.\n\n        Returns:\n            pd.DataFrame: A dataframe with each row corresponding to a predicted match, containing the predicted probabilities\n        \"\"\"\n        prediction_dataframe = for_prediction_info.copy()\n        prediction_dataframe['ScorelineProbability'] = [[] for _ in range(len(for_prediction_info))]\n        \n        for index, scoreline, bet in zip(range(len(for_prediction_info)), scoreline_probabilities, betting_probabilities):\n            prediction_dataframe.loc[index, 'HomeWinProbability'] = bet['home']\n            prediction_dataframe.loc[index, 'DrawProbability'] = bet['draw']\n            prediction_dataframe.loc[index, 'AwayWinProbability'] = bet['away']\n            prediction_dataframe.loc[index, 'Over2.5Probability'] = bet['over2.5']\n            prediction_dataframe.loc[index, 'Under2.5Probability'] = bet['under2.5']\n            prediction_dataframe.loc[index, 'Over3.5Probability'] = bet['over3.5']\n            prediction_dataframe.loc[index, 'Under3.5Probability'] = bet['under3.5']\n            prediction_dataframe.loc[index, 'Over4.5Probability'] = bet['over4.5']\n            prediction_dataframe.loc[index, 'Under4.5Probability'] = bet['under4.5']\n            prediction_dataframe.loc[index, 'Over1.5Probability'] = bet['over1.5']\n            prediction_dataframe.loc[index, 'Under1.5Probability'] = bet['under1.5']\n            prediction_dataframe.loc[index, 'GGProbability'] = bet['gg']\n            prediction_dataframe.loc[index, 'NGProbability'] = bet['ng']\n            # Create a dictionary\n            score_dict = {}\n\n            # Populate the dictionary with coordinates and probabilities\n            for home_goals in range(12):\n                for away_goals in range(12):\n                    key = f'{home_goals} - {away_goals}'\n                    value = scoreline[home_goals, away_goals]\n                    score_dict[key] = value\n            \n            prediction_dataframe.loc[index, 'ScorelineProbability'] = [score_dict]\n            prediction_dataframe['Date'] = pd.to_datetime(prediction_dataframe['Date'], format='%d/%m/%Y')\n            prediction_dataframe.sort_values(by='Date', inplace=True)\n            prediction_dataframe['Date'] = prediction_dataframe['Date'].dt.strftime('%d/%m/%Y')\n        \n        return prediction_dataframe\n    \n    def produce_probabilities(self, short_term_data: pd.DataFrame, long_term_data: pd.DataFrame, for_prediction_short: pd.DataFrame, for_prediction_long: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"A pipeline of functions that gets the data from the loaded files and outputs the prediction dataframe.\n\n        Args:\n            short_term_data (pd.DataFrame): A dataframe containing the short term form data to train the model, as read from the .csv file\n            long_term_data (pd.DataFrame): A dataframe containing the training long term form data to train the model, as read from the .csv file\n            for_prediction_short (pd.DataFrame): A dataframe containing the short term form data for prediction, as read from the .csv file\n            for_prediction_long (pd.DataFrame): A dataframe containing the long term form data for prediction, as read from the .csv file\n\n        Returns:\n            pd.DataFrame: A dataframe with each row corresponding to a predicted match, containing the predicted probabilities\n        \"\"\"\n        long_term_array, short_term_array, home_goals_array, away_goals_array, match_info, for_prediction_info, for_prediction_short_array, for_prediction_long_array = self.prepare_for_prediction(short_term_data=short_term_data, long_term_data=long_term_data, for_prediction_short=for_prediction_short, for_prediction_long=for_prediction_long)\n        self.train_network(short_term_data=short_term_array, long_term_data=long_term_array, home_goals=home_goals_array, away_goals=away_goals_array)\n        goal_rate = self.deduct_goal_rate(for_prediction_long_form=for_prediction_short_array, for_prediction_short_form=for_prediction_long_array)\n        scoreline_probabilities = self.get_scoreline_probabilities(home_goal_rate_array = goal_rate['home'], away_goal_rate_array = goal_rate['away'])\n        betting_probabilities = self.get_betting_probabilities(scoreline_prob_list=scoreline_probabilities)\n        prediction_dataframe = self.get_prediction_dataframe(for_prediction_info= for_prediction_info, scoreline_probabilities= scoreline_probabilities, betting_probabilities= betting_probabilities)\n        return prediction_dataframe\n    \n    def add_dummy_validation_columns(self, validation_data: dict) -> pd.DataFrame:\n        \"\"\"Adding dummy columns to the given dataframes in order to reproduce the format of the for-prediction scraped data the model is prepared to predict. Needed for the evaluation process as the evaluation uses historic odds from csv files and not scraped data.\n\n        Args:\n            validation_data (dict): A dataframe with match info and statistics in the format of the training dataframes\n\n        Returns:\n            pd.DataFrame: A dataframe with match info and statistics in the format of for-prediction dataframes, having changed the format and added dummy columns\n        \"\"\"\n        for key in ['short_term', 'long_term']:\n            validation_data[key].loc[:, 'Line'] = '2.5'\n            validation_data[key].loc[:, 'Yes'] = None \n            validation_data[key].loc[:, 'No'] = None \n            validation_data[key] = validation_data['short_term'].rename(columns={'OverOdds':'OverLineOdds', 'UnderOdds':'UnderLineOdds'})\n\n        return validation_data\n    \n    def remove_dummy_columns(self, prediction_dataframe: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"An function that reverses the format of add_dummy_validation_columns()\n\n        Args:\n            prediction_dataframe (pd.DataFrame): A dataframe with match info and statistics in the format of for-prediction dataframes\n\n        Returns:\n            pd.DataFrame: A dataframe with match info and statistics in the format of the training dataframes\n        \"\"\"\n        prediction_dataframe.drop(columns=['Line', 'Yes', 'No'], inplace=True)\n        prediction_dataframe = prediction_dataframe.rename(columns={'OverLineOdds':'OverOdds', 'UnderLineOdds':'UnderOdds'})\n        return prediction_dataframe\n    \n    def evaluate_per_season(self, validation_season: str, short_term_data: pd.DataFrame, long_term_data: pd.DataFrame, bettor: Bettor, evaluation_output: str) -> tuple:\n        \"\"\"An evaluation pipeline that gets the datasets loaded from .csv files and produces evaluation metrics and figures of the input validation_season\n\n        Args:\n            validation_season (str): The season to evaluate on. One of the available ['2017', '2018', '2019', '2020', '2021', '2022', '2023']\n            short_term_data (pd.DataFrame): The dataset as loaded from the .csv file of short term form\n            long_term_data (pd.DataFrame): The dataset as loaded from the .csv file of long term form\n            bettor (Bettor): A bettor object\n            evaluation_output (str): The path of the evaluation output figures to be saved at\n\n        Returns:\n            tuple:\n                dict: A dictionary of figures. Each figure corresponds to the results of the bettor in certain betting categories specified by the dictionary keys\n                dict: A dictionary of metrics in the format of {'Investment': initial investment value, 'NetGain': resulting net gain, 'ROI': resulting return of investment}\n        \"\"\"\n        training_data, validation_data = self.cut_validation_season(short_term_data=short_term_data, long_term_data=long_term_data, validation_season=validation_season)\n        validation_data = self.add_dummy_validation_columns(validation_data= validation_data)\n        validation_data['short_term'] = validation_data['short_term'].copy()\n        validation_data['long_term'] = validation_data['long_term'].copy()\n    \n        results = validation_data['long_term'][['Match_id', 'Result']]\n\n        prediction_dataframe = self.produce_probabilities(short_term_data= training_data['short_term'], long_term_data= training_data['long_term'], for_prediction_short=validation_data['short_term'], for_prediction_long=validation_data['long_term'])\n        prediction_dataframe = self.remove_dummy_columns(prediction_dataframe=prediction_dataframe)\n        bettor.preprocess(prediction_dataframe=prediction_dataframe, results= results)\n        \n        metrics = bettor.place_value_bets()\n        figures = bettor.produce_report_figures(validation_season = validation_season, evaluation_output=evaluation_output)\n        return figures, metrics\n\n    \n    def cut_validation_season(self, short_term_data: pd.DataFrame, long_term_data: pd.DataFrame, validation_season: str):\n        \"\"\"Gets the short/long term form data as loaded from the .csv files and cuts the season to evaluate as specified by validation_season\n\n        Args:\n            short_term_data (pd.DataFrame): A dataframe containing the short term form data as loaded from the .csv file\n            long_term_data (pd.DataFrame): A dataframe containing the long term form data as loaded from the .csv file\n            validation_season (str): The specified season to cut out of the given datasets for evaluation. One of the available ['2017', '2018', '2019', '2020', '2021', '2022', '2023']\n\n        Returns:\n            tuple: \n                dict: A dictionary with keys corresponding to long/short term form that each contain the pd.DataFrame of the training data\n                dict: A dictionary with keys corresponding to long/short term form that each contain the pd.DataFrame of the data for evaluation\n        \"\"\"\n        data = {'short_term': short_term_data, 'long_term': long_term_data}\n        data['short_term']['Date'] = pd.to_datetime(data['short_term']['Date'], format='%d/%m/%Y')\n        data['long_term']['Date'] = pd.to_datetime(data['long_term']['Date'], format='%d/%m/%Y')\n\n        # Define the start and end dates of the validation season\n        #This dating is because of the covid seasons in serie_a taking until 08-05 in 2019\n        validation_start_date = pd.to_datetime(f'{validation_season}-08-06')\n        validation_end_date = pd.to_datetime(f'{int(validation_season) + 1}-08-05')\n\n        # Filter the dataset to include only the data within the validation season\n        validation_data = {'short_term': data['short_term'][(pd.to_datetime(data['short_term']['Date'], format='%d/%m/%Y') >= validation_start_date) & (pd.to_datetime(data['short_term']['Date'], format='%d/%m/%Y') <= validation_end_date)].reset_index(drop=True),\n                           'long_term': data['long_term'][(pd.to_datetime(data['long_term']['Date'], format='%d/%m/%Y') >= validation_start_date) & (pd.to_datetime(data['long_term']['Date'], format='%d/%m/%Y') <= validation_end_date)].reset_index(drop=True)}\n\n        # Separate the training data (data before or after the validation season) and validation data\n        training_data = {'short_term': data['short_term'][(pd.to_datetime(data['short_term']['Date'], format='%d/%m/%Y') < validation_start_date) | (pd.to_datetime(data['short_term']['Date'], format='%d/%m/%Y') > validation_end_date)].reset_index(drop=True),\n                         'long_term': data['long_term'][(pd.to_datetime(data['long_term']['Date'], format='%d/%m/%Y') < validation_start_date) | (pd.to_datetime(data['long_term']['Date'], format='%d/%m/%Y') > validation_end_date)].reset_index(drop=True)}\n\n        return training_data, validation_data"}
{"type": "source_file", "path": "run_evaluation_per_season.py", "content": "from europeanfootballleaguepredictor.models.probability_estimator import FootballPredictor, ProbabilityEstimatorNetwork\nimport numpy as np\nimport pandas as pd\nfrom loguru import logger \nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom europeanfootballleaguepredictor.visualization.visualize import Visualizer\nfrom pretty_html_table import build_table\nimport argparse\nimport os\nfrom europeanfootballleaguepredictor.models.bettor import Bettor\nimport mlflow.sklearn\nimport tempfile\nimport statistics\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler \n\n\ndef main():\n    '''Parsing the configuration file'''\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    config_file_path = parser.parse_args().config\n    \n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n    \n    logger.info(config)\n    '''End of the configuration file parsing'''\n    _regressor_instance = config.regressor()\n\n        \n    net = ProbabilityEstimatorNetwork(voting_dict=config.voting_dict, matchdays_to_drop=config.matchdays_to_drop)\n    net.build_network(regressor = config.regressor)\n\n    database_handler = DatabaseHandler(league=config.league, database=config.database)\n    short_term_form, long_term_form= database_handler.get_data(table_names=[\"Preprocessed_ShortTermForm\", \"Preprocessed_LongTermForm\"])\n\n    for validation_season in config.seasons_to_gather:\n      with mlflow.start_run(run_name = f\"{_regressor_instance} | {config.league} | {validation_season}\") as run:\n        # Log model parameters\n        mlflow.log_param(\"Form_Votes\", config.voting_dict)\n        mlflow.log_param('Margin_Dict', config.bettor_kelly_cap)\n        mlflow.log_param('Regressor', _regressor_instance)\n        mlflow.log_param('League', config.league)\n        mlflow.log_param('Matchdays to drop', config.matchdays_to_drop)\n        bettor = Bettor(bank=config.bettor_bank, kelly_cap=config.bettor_kelly_cap)\n        #Evaluate the model for a specific season.\n        figures, metrics = net.evaluate_per_season(short_term_data=short_term_form, long_term_data=long_term_form, validation_season=validation_season, bettor= bettor, evaluation_output = config.evaluation_output)\n        logger.info(metrics)\n          \n        #logging metrics as a table\n        roi_df = pd.DataFrame(list(metrics['ROI'].items()), columns=['Metric', 'Value'])\n        mlflow.log_table(data=roi_df.round(2), artifact_file=f\"ROI.json\")\n          \n        # Log metrics individualy\n        for metric_name, metric_value in metrics['ROI'].items():\n            mlflow.log_metric(f\"ROI_{metric_name}\", np.round(metric_value, 2))\n\n        roi_1x2 = statistics.mean([metrics['ROI']['home_win_roi'], metrics['ROI']['draw_roi'], metrics['ROI']['away_win_roi']])\n        roi_12 = statistics.mean([metrics['ROI']['home_win_roi'], metrics['ROI']['away_win_roi']])\n        roi_ou = statistics.mean([metrics['ROI']['over2.5_roi'], metrics['ROI']['under2.5_roi']])\n        \n        mlflow.log_metric(f\"ROI_1x2\", np.round(roi_1x2, 2))\n        mlflow.log_metric(f'ROI_12', np.round(roi_12, 2))\n        mlflow.log_metric(f\"ROI_ou\", np.round(roi_ou, 2))\n          \n        # Save log the artifact\n        tmp = tempfile.NamedTemporaryFile(prefix='residuals-', suffix='.png')\n        tmp_name = tmp.name\n  \n        for name, fig in figures.items():\n            try:\n              fig.savefig(tmp_name)\n              mlflow.log_artifact(tmp_name, f\"{validation_season}/{name}.png\")\n            finally:\n              tmp.close()        \n    \nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "predictor_app.py", "content": "import gradio as gr\nfrom loguru import logger\nimport argparse\nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nimport pandas as pd\nfrom europeanfootballleaguepredictor.models.probability_estimator import ProbabilityEstimatorNetwork\nfrom europeanfootballleaguepredictor.visualization.visualize import Visualizer\nfrom pretty_html_table import build_table\nimport importlib\nimport os\nimport argparse\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler\n\n\ndef league_predictions_figure(league_name, regressor_str, matchdays_to_drop, long_form_vote):\n    \"\"\"Generates a prediction table with the model predictions of certain match outcomes\n    and a prediction figure that includes a radar plot comparing the bookmakers odds depicted as probabilities with the predicted probabilities\n    as well as a barplot with the most probable scorelines according to the model.\n\n    Args:\n        league_name (str): One of the available leagues ['EPL', 'Bundesliga', 'Ligue_1', 'La_Liga', 'Serie_A']\n        regressor_str (str): One of the available regressors ['LinearRegression', 'PoissonRegressor', 'SVR']\n        matchdays_to_drop (int): The matchdays at the start of the season that are considered to provide redundant information to the model because the league table is not yet indicative of the teams performance due to small sample size.\n        long_form_vote (int): The weight with which the model produces the predictions between long form and short form. The short_form_vote is then calculated by 1-long_form_vote. Long form and short form are dependent on the users configuration before the data collection. Defaults are long_form : season long form, short_form : 3 month form.\n\n    Returns:\n        plotly_figure: A prediction figure that includes a radar plot comparing the bookmakers odds depicted as probabilities with the predicted probabilities as well as a barplot with the most probable scorelines according to the model.\n        html_table: A prediction table with the model predictions of certain match outcomes.\n    \"\"\"\n\n    voting_dict = {'long_term': long_form_vote, 'short_term': 1-long_form_vote}\n    '''Parsing the configuration file'''\n    if (regressor_str == \"LinearRegression\") or (regressor_str == \"PoissonRegressor\"):\n        module_path = 'sklearn.linear_model'\n        regressor_module = importlib.import_module(module_path)\n        regressor = getattr(regressor_module, regressor_str)\n    if regressor_str == 'SVR':\n        module_path = 'sklearn.svm'\n        regressor_module = importlib.import_module(module_path)\n        regressor = getattr(regressor_module, regressor_str)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    config_file_path = parser.parse_args().config\n\n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n\n    logger.info(config)\n    '''End of the configuration file parsing'''\n\n    pd.set_option('display.precision', 2)\n\n    net = ProbabilityEstimatorNetwork(voting_dict=voting_dict, matchdays_to_drop=matchdays_to_drop)\n    net.build_network(regressor = regressor)\n\n    database = f'europeanfootballleaguepredictor/data/database/{league_name}_database.db'\n    database_handler = DatabaseHandler(league=config.league, database=database)\n    short_term_form, long_term_form, for_prediction_short, for_prediction_long = database_handler.get_data(table_names=[\"Preprocessed_ShortTermForm\", \"Preprocessed_LongTermForm\", \"Preprocessed_UpcomingShortTerm\", \"Preprocessed_UpcomingLongTerm\"])\n\n    probability_dataframe = net.produce_probabilities(short_term_data=short_term_form, long_term_data=long_term_form, for_prediction_short=for_prediction_short, for_prediction_long=for_prediction_long)\n\n    visualizer = Visualizer(probability_dataframe)\n    figure = visualizer.radar_scoreline_plot()\n    html_table = build_table(probability_dataframe.drop(['ScorelineProbability', 'Match_id'], axis=1), 'blue_light')\n\n    return figure, html_table\n\ndef main():\n    \"\"\"Main function of the up initializes the gradio app interface with which the user runs the model.\n    \"\"\"\n    with gr.Blocks() as iface:\n        with gr.Row():\n            drop1 = gr.Dropdown(['EPL', 'Bundesliga', 'Ligue_1', 'La_Liga', 'Serie_A'], label=\"Select League\")\n            drop3 = gr.Dropdown(['LinearRegression', 'PoissonRegressor', 'SVR'], label=\"Select regressor Type\")\n        with gr.Row():\n            with gr.Column(scale=1, min_width=600):\n                slide1 = gr.Slider(minimum=0, maximum=10, step=1, value=4, show_label=True, interactive=True, label=\"Select number of matchdays to drop\")\n                slide2 = gr.Slider(minimum=0, maximum=1, step=0.1, value=0.6, show_label=True, interactive=True, label=\"Select long form vote. Short form will be 1-long\")\n                btn = gr.Button(\"Predict\")\n            with gr.Column(scale=5, min_width=1200):\n                plot1 = gr.Plot(label='Predicted results plots')\n                table1 = gr.HTML(label='Predicted results tables')\n\n        btn.click(league_predictions_figure, inputs=[drop1, drop3, slide1, slide2], outputs=[plot1, table1])\n\n    iface.launch(height=2000)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/data/understat_gatherer.py", "content": "import numpy as np\nimport aiohttp\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\nfrom datetime import datetime as dt\nimport datetime\nfrom understat import Understat\nfrom loguru import logger\nfrom tqdm import tqdm\nimport os\nimport sqlite3\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler\n\n\n\nclass Understat_Parser():\n    \"\"\"The class responsible for using the understat api and combining the data with the data_co_uk dataset\"\"\"\n    \n    def __init__(self, league: str, dictionary : dict, database: str) -> None:\n        \"\"\"\n        Initializes the understat parser using the league and dictionary for team names used in data_co_uk dataset.\n\n        Args:\n            league (str): The league for which the understat parser will request data.\n            dictionary (dict): A team name mapping to communicate different team names between data_co_uk and understat.\n            database (str): The path to the database of the league.\n        \"\"\"\n        self.league = league\n        self.dictionary = dictionary\n        self.upcoming_fixtures_table = \"UpcomingFixtures\"\n        self.database_handler = DatabaseHandler(database=database, league=league)\n        \n    def replace_team_names(self, table_names: list, replacing_dict: dict) -> None:\n        \"\"\"\n        Replaces the team names used by data_co_uk with the format of understat.\n\n        Args:\n            table_names (list): The list of table names to replace team names.\n            replacing_dict (dict): A team name mapping to communicate different team names between data_co_uk and understat.\n        \"\"\"\n        dataframes_list = self.database_handler.get_data(table_names)\n        for dataframe in dataframes_list:\n            # Replace team names using the mapping dictionary\n            dataframe[\"HomeTeam\"] = dataframe[\"HomeTeam\"].map(replacing_dict).fillna(dataframe[\"HomeTeam\"])\n            dataframe[\"AwayTeam\"] = dataframe[\"AwayTeam\"].map(replacing_dict).fillna(dataframe[\"AwayTeam\"])\n\n        self.database_handler.save_dataframes(dataframes=dataframes_list, table_names=table_names)\n\n\n    async def get_understat_season(self, season: str, months_of_form: int, output_table_name: str) -> None:\n        \"\"\"\n        An asynchronous function that connects with the understat api and collects data to construct the required per season datasets.\n\n        Args:\n            season (str): The season for which the data gathering process takes place.\n            months_of_form (int): The number of months form to take into account when gathering data.\n            output_table_name (str): The table name of the database the results will be saved.\n        \"\"\"\n        async with aiohttp.ClientSession(cookies={'beget':'begetok'}) as session:\n            data_co_uk_table_name = f\"DataCoUk_Season{season}_{str(int(season)+1)}\"\n            self.replace_team_names(table_names = data_co_uk_table_name, replacing_dict=self.dictionary)\n            logger.info(f'Started connection with Understat. Collecting {season} for {self.league} and {months_of_form} month(s) of form.')\n            raw_dataframe_list = []\n            pd.options.mode.copy_on_write = True\n            understat = Understat(session)\n\n            #reading the file and keeping only the finished matches\n            season_dataframe = self.database_handler.get_data(table_names=data_co_uk_table_name)[0]\n            season_dataframe = season_dataframe.dropna(subset=['FTHG'])\n\n            #creating a new Result column\n            season_dataframe['Result'] = None\n            for i, x in enumerate(season_dataframe['Result']):\n                season_dataframe.loc[i, 'Result'] = f'{season_dataframe.loc[i, \"FTHG\"]}-{season_dataframe.loc[i, \"FTAG\"]}'\n\n            #keeping only the important columns and giving appropriate date format\n            try:\n                season_dataframe = season_dataframe[['Date', 'HomeTeam', 'AwayTeam', 'Result', 'B365H', 'B365D', 'B365A', 'B365>2.5', 'B365<2.5']]\n                season_dataframe = season_dataframe.rename(columns={'B365>2.5': 'OverOdds', 'B365<2.5': 'UnderOdds', 'B365H': 'HomeWinOdds', 'B365D': 'DrawOdds', 'B365A': 'AwayWinOdds'})\n            except KeyError:\n                season_dataframe = season_dataframe[['Date', 'HomeTeam', 'AwayTeam', 'Result', 'B365H', 'B365D', 'B365A', 'BbAv>2.5', 'BbAv<2.5']]\n                season_dataframe = season_dataframe.rename(columns={'BbAv>2.5': 'OverOdds', 'BbAv<2.5': 'UnderOdds', 'B365H': 'HomeWinOdds', 'B365D': 'DrawOdds', 'B365A': 'AwayWinOdds'})\n\n            try:\n                season_dataframe['Date'] = pd.to_datetime(season_dataframe['Date'], format='%d/%m/%Y')\n                season_dataframe['Date'] = season_dataframe['Date'].dt.strftime('%d/%m/%Y')\n            except ValueError:\n                season_dataframe['Date'] = pd.to_datetime(season_dataframe['Date'], format='%d/%m/%y')\n                season_dataframe['Date'] = season_dataframe['Date'].dt.strftime('%d/%m/%Y')\n\n            #get the unique dates for all the matches\n            unique_dates = season_dataframe[\"Date\"].unique()\n\n            #loop through the unique dates and gather data from understat from one day prior\n            for i, date in tqdm(enumerate(unique_dates), total=len(unique_dates), desc=logger.info('Collecting Understat data.')):\n                #formating date and subtracting a day to make sure the match has not been processed by the league table\n                try:\n                    unique_dates[i] = datetime.datetime.strptime(unique_dates[i], '%d/%m/%Y').date()\n                except ValueError:\n                    unique_dates[i] = datetime.datetime.strptime(unique_dates[i], '%d/%m/%y').date()\n                    \n                unique_dates[i] = unique_dates[i] - datetime.timedelta(days=1)\n\n                if months_of_form == None:\n                    table = await understat.get_league_table(self.league, season , end_date = str(unique_dates[i]))\n                else: \n                    start = unique_dates[i] - relativedelta(months=months_of_form)        \n                    table = await understat.get_league_table(self.league, season , end_date = str(unique_dates[i]), start_date = str(start))\n\n                Table = pd.DataFrame(table)\n                Table.columns = Table.iloc[0]\n                Table = Table[1:]\n\n                Teams = season_dataframe.loc[season_dataframe['Date'] == date]\n\n                #HomeTeam dataframe with respective stats\n                HomeStats = Table.loc[Table['Team'].isin(Teams['HomeTeam'].to_list())]\n                HomeStats['Team'] = pd.Categorical(\n                    HomeStats['Team'], \n                    Teams['HomeTeam'].to_list(), \n                    ordered=True\n                    )\n                #adding column H prefix\n                HomeStats.columns = ['H' + col for col in HomeStats.columns]\n\n                    #AwayTeam dataframe with respective stats\n                AwayStats = Table.loc[Table['Team'].isin(Teams['AwayTeam'].to_list())] \n                AwayStats['Team'] = pd.Categorical(\n                    AwayStats['Team'], \n                    Teams['AwayTeam'].to_list(), \n                    ordered=True\n                    )\n                #adding column A prefix\n                AwayStats.columns = ['A' + col for col in AwayStats.columns]\n\n                #Combining the season_dataframe with HomeStats and AwayStats\n                combined_result_home = pd.merge(Teams, HomeStats, left_on='HomeTeam', right_on='HTeam', how='outer') \n                combined_result_home_away = pd.merge(combined_result_home, AwayStats, left_on='AwayTeam', right_on='ATeam', how='outer')\n\n                raw_dataframe_list.append(combined_result_home_away)\n\n            # Concatenate all DataFrames in the list into a final DataFrame\n            final_raw = pd.concat(raw_dataframe_list, ignore_index=True)\n\n            # Save the final DataFrame to the database\n            output_table_name = f\"{output_table_name}_Season{season}_{str(int(season)+1)}\"\n            self.database_handler.save_dataframes(dataframes=final_raw, table_names=output_table_name)\n    \n    async def get_upcoming_match_stats(self, current_season: str, months_of_form_list: int) -> None:\n        \"\"\"\n        An asynchronous function that gathers the data for the upcoming matches prediction.\n\n        Args:\n            current_season (str): The current season as a string identifier. '2023' represents 2023/2024 season.\n            months_of_form_list (int): The number of months form to take into account when gathering data.\n        \"\"\"\n        async with aiohttp.ClientSession(cookies={'beget':'begetok'}) as session:\n            self.replace_team_names(table_names = self.upcoming_fixtures_table, replacing_dict=self.dictionary)\n                    \n            logger.info(f'Started connection with Understat. Collecting upcoming matches statistics')\n            pd.options.mode.copy_on_write = True\n            understat = Understat(session)\n            \n            upcoming_matches = self.database_handler.get_data(table_names=self.upcoming_fixtures_table)[0]\n            current_date = dt.now()\n\n            dataframes_list = []\n            for months_of_form in months_of_form_list:\n                if months_of_form == None:\n                    table = await understat.get_league_table(self.league, current_season , end_date = str(current_date.strftime(\"%Y-%m-%d\")))\n                else: \n                    start = current_date - relativedelta(months=months_of_form)        \n                    table = await understat.get_league_table(self.league, current_season , end_date = str(current_date.strftime(\"%Y-%m-%d\")), start_date = str(start.strftime(\"%Y-%m-%d\")))\n\n                Table = pd.DataFrame(table)\n                Table.columns = Table.iloc[0]\n                Table = Table[1:]\n\n\n                #HomeTeam dataframe with respective stats\n                HomeStats = Table.loc[Table['Team'].isin(upcoming_matches['HomeTeam'].to_list())]\n                #adding column H prefix\n                HomeStats.columns = ['H' + col for col in HomeStats.columns]\n\n                #AwayTeam dataframe with respective stats\n                AwayStats = Table.loc[Table['Team'].isin(upcoming_matches['AwayTeam'].to_list())] \n                #adding column A prefix\n                AwayStats.columns = ['A' + col for col in AwayStats.columns]\n\n                #Combining the season_dataframe with HomeStats and AwayStats\n                combined_result_home = pd.merge(upcoming_matches, HomeStats, left_on='HomeTeam', right_on='HTeam', how='outer') \n                combined_result_home_away = pd.merge(combined_result_home, AwayStats, left_on='AwayTeam', right_on='ATeam', how='outer')\n\n                dataframes_list.append(combined_result_home_away)\n            \n            self.database_handler.save_dataframes(table_names=[\"Raw_UpcomingLongTerm\", \"Raw_UpcomingShortTerm\"], dataframes=dataframes_list)        \n            logger.success('Finished understat session for upcoming matches.')"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/utils/path_handler.py", "content": "import os\nfrom loguru import logger \n\nclass PathHandler:\n    \"\"\"Handles directories\n    Args:\n        paths: A string indicating a path\n    Returns:\n        None\n    \"\"\"\n\n    def __init__(self, path: str):\n        self.path = path\n\n    def create_paths_if_not_exists(self) -> None:\n        \"\"\"Creates directories if not exist\"\"\"\n        if not os.path.exists(self.path):\n            # Create a new directory because it does not exist\n            os.makedirs(self.path)\n            logger.debug(f'Created {self.path}')\n\n    def remove_paths_if_exists(self) -> None:\n        \"\"\"Deletes directories if exist\"\"\"\n        if os.path.exists(self.path):\n            # Create a new directory because it does not exist\n            os.rmdir(self.path)"}
{"type": "source_file", "path": "run_data_collection.py", "content": "from europeanfootballleaguepredictor.data.understat_gatherer import Understat_Parser\nimport asyncio \nimport os\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom loguru import logger\nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nimport argparse\nfrom europeanfootballleaguepredictor.data.preprocessor import Preprocessor\nimport pandas as pd\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler \nimport aiohttp \nfrom aiohttp.client_exceptions import ServerDisconnectedError\nimport requests \nimport sys\n\nasync def fetch_data_with_retry(understat_parser, season, months_of_form, output_table_name, max_retries=3, retry_delay=1):\n    for attempt in range(max_retries):\n        try:\n            await understat_parser.get_understat_season(season=season, months_of_form=months_of_form, output_table_name=output_table_name)\n            logger.success(f'Succesfully gathered and saved season {season}.')\n            break  # Break out of the loop if successful\n        except (aiohttp.ClientError, ServerDisconnectedError) as e:\n            if attempt < max_retries - 1:\n                print(f\"Retry attempt {attempt + 1}/{max_retries}\")\n                await asyncio.sleep(retry_delay)  # Add a short delay before retrying\n            else:\n                logger.error(f'Max retries reached, for error {e}.')\n                sys.exit(1)\n            \ndef main():\n    '''Parsing the configuration file'''\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    parser.add_argument(\"--download\", action='store_true', help=\"Download the data from the data_co_uk website. False uses the already downloaded files.\", default=False)\n    config_file_path = parser.parse_args().config\n    \n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n    \n    logger.info(config)\n    '''End of the configuration file parsing'''\n    \n    database_handler = DatabaseHandler(league= config.league, database= config.database)\n\n    dataframe_list = [] \n    table_name_list = []\n    \n    # Download the data from the data_co_uk website if download is set to True\n    if parser.parse_args().download:\n        for season in config.seasons_to_gather:\n            # Compute the two-year seas code, e.g., '1718' for '2017', '1819' for '2018'\n            two_year_code = season[-2:] + str(int(season[-2:]) + 1).zfill(2)\n            url_base = '/'.join(config.data_co_uk_url.split('/')[:-2])\n                                                        \n            # Format the URL with the t-year season code\n            url = os.path.join(url_base, two_year_code, config.data_co_uk_url.split('/')[-1])\n    \n            response = requests.get(url)\n            if response.status_code == 200:\n                file_path = os.path.join(config.data_co_uk_path, f'E0-{season}.csv')\n                with open(file_path, 'wb') as file:\n                    file.write(response.content)\n                logger.success(f'Data for season {season} saved at {file_path}.')\n            else:\n                logger.error(f'Failed to download data for season {season}. Response code: {response.status_code}.')\n                exit(1)\n\n    # Iterate through each file in the directory\n    for file in os.listdir(config.data_co_uk_path):\n        file_path = os.path.join(config.data_co_uk_path, file)\n\n        # Extracting table name from the file name\n        season_year = int(file.split('-')[1].split('.')[0])\n        table_name = f\"DataCoUk_Season{season_year}_{season_year+1}\"\n            \n        # Read data from the CSV file using pandas\n        season_dataframe = pd.read_csv(file_path)\n        dataframe_list.append(season_dataframe)\n        table_name_list.append(table_name)\n\n    database_handler.save_dataframes(dataframes=dataframe_list, table_names=table_name_list)    \n    \n    understat_parser = Understat_Parser(league = config.league, dictionary = config.data_co_uk_dictionary, database = config.database)\n    \n    for table_name, months_of_form in zip(['Raw_LongTermForm', 'Raw_ShortTermForm'], config.months_of_form_list):\n        logger.info(f'Gathering {months_of_form} month form data for seasons in {config.seasons_to_gather}')\n        for season in config.seasons_to_gather:\n            asyncio.run(fetch_data_with_retry(understat_parser, season, months_of_form, table_name))\n    \n    \n    preprocessor = Preprocessor(league=config.league, database=config.database)\n\n    #Gathering all the seasons into two concatenated dataframes one for long term and one for short term form\n    long_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_LongTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    short_term_form_season_list = preprocessor.database_handler.get_data([f'Raw_ShortTermForm_Season{season}_{str(int(season)+1)}' for season in config.seasons_to_gather])\n    long_term_form_dataframe = pd.concat([dataframe for dataframe in long_term_form_season_list])\n    short_term_form_dataframe = pd.concat([dataframe for dataframe in short_term_form_season_list])\n    \n    preprocessed_dataframes = preprocessor.preprocessing_pipeline(data=[long_term_form_dataframe, short_term_form_dataframe])\n    preprocessor.database_handler.save_dataframes(dataframes=preprocessed_dataframes, table_names=['Preprocessed_LongTermForm', 'Preprocessed_ShortTermForm'])\n    \nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/data/database_handler.py", "content": "import sqlite3 \nimport pandas as pd \nfrom loguru import logger \nimport sys\nfrom sqlalchemy import TEXT\n\nclass DatabaseHandler:\n    \"\"\"A class for handling SQLite databases and interacting with dataframes.\"\"\"\n    def __init__(self, league:str, database: str):\n        \"\"\"\n        Initializes the DatabaseHandler.\n\n        Args:\n            league (str): The name of the league associated with the database.\n            database (str): The path to the SQLite database file.\n        \"\"\"\n        self.league = league\n        self.database = database\n    \n    def get_data(self, table_names: str or list[str]) -> list[pd.DataFrame]:\n        \"\"\"\n        Retrieves data from the specified tables in the SQLite database.\n\n        Args:\n            table_names (str or list): A single table name or a list of table names to fetch data from.\n\n        Returns:\n            list[pd.DataFrame]: A list of dataframes containing the fetched data.\n        \"\"\"\n        dataframe_list = []\n        connection = None\n\n        if isinstance(table_names, str):\n            table_names = [table_names]\n\n        try:\n            # Connect to the SQLite database\n            connection = sqlite3.connect(self.database)\n            cursor = connection.cursor()\n\n            for table_name in table_names:\n                # Check if the table exists\n                cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name=?;\", (table_name,))\n                result = cursor.fetchone()\n\n                if result:\n                    # Fetch data from the specified table\n                    query = f\"SELECT * FROM {table_name};\"\n                    dataframe_list.append(pd.read_sql_query(query, connection))\n                    logger.info(f\"Data fetched for table: {table_name}\")\n                else:\n                    logger.warning(f\"Table '{table_name}' does not exist in the database. Skipping.\")\n\n            return dataframe_list\n        \n        except sqlite3.Error as e:\n            logger.error(f\"SQLite error: {e}\")\n\n        finally:\n            # Close the database connection\n            if connection:\n                connection.close()       \n    \n    def save_dataframes(self, dataframes: list, table_names: list):\n        \"\"\"\n        Saves dataframes to the corresponding tables in the SQLite database.\n\n        Args:\n            dataframes (list): A list of dataframes to be saved.\n            table_names (list): A list of table names corresponding to the dataframes.\n        \"\"\"\n        if isinstance(table_names, str):\n            table_names = [table_names]\n        if isinstance(dataframes, pd.DataFrame):\n            dataframes = [dataframes]\n\n        if len(dataframes) != len(table_names):\n            logger.error(\"Length of dataframe_list must be equal to the length of table_names.\")\n            sys.exit(1)\n\n        try:\n            # Connect to the SQLite database\n            connection = sqlite3.connect(self.database)\n            cursor = connection.cursor()\n\n            for df, table_name in zip(dataframes, table_names):\n                try:\n                    # Save the DataFrame to the corresponding table in the database\n                    df.to_sql(table_name, connection, index=False, if_exists='replace')\n                    logger.info(f'Table {table_name} created/updated for {self.league} league.')\n\n                except Exception as e:\n                    # Print or log the DataFrame to identify the issue\n                    logger.debug(f\"DataFrame for {table_name}:\\n{df.head()}\")\n                    logger.debug(f\"Error saving DataFrame to table {table_name}: {e}\")\n\n        except sqlite3.Error as e:\n            print(f\"SQLite error: {e}\")\n\n        finally:\n            connection.commit()\n            # Close the database connection\n            if connection:\n                connection.close()     "}
{"type": "source_file", "path": "europeanfootballleaguepredictor/common/config_parser.py", "content": "import yaml \nfrom loguru import logger \nfrom dataclasses import dataclass\nimport sys\nimport importlib\nfrom sklearn.linear_model import LinearRegression, PoissonRegressor\nfrom sklearn.svm import SVR\n\nclass Config_Parser:\n    \"\"\"The configuration parser class that is responsible for loading the config.yaml, checking for some basic validity of the configuration and loading the dataclass Configuration.\n    \"\"\"\n    def __init__(self, config_path, section_name=None) -> None:\n        \"\"\"_summary_\n\n        Args:\n            config_path (_type_: str): The path of the config.yaml file\n            section_name (_type_: str, Optional): The name of the exact subsection of the .yaml file to access. Defaults to None.\n        \"\"\"\n        self.config_path = config_path\n        self.section_name = section_name\n        \n    def load_and_extract_yaml_section(self, path=None) -> dict:\n        \"\"\"Loads and extracts the exact yaml section specified when called. Else loads and extracts default configuration from self.config_path.\n\n        Args:\n            path (_type_: str, optional): The name of the exact subsection of the .yaml file to access. Defaults to None.\n\n        Returns:\n            dict: Configuration dictionary.\n        \"\"\"\n        if path is None:\n            with open(self.config_path, 'r') as file:\n                config_data = yaml.safe_load(file)\n            if self.section_name is not None and self.section_name in config_data:\n                logger.success(f'Successfully loaded {self.section_name} section from config.yaml')\n                return config_data[self.section_name]\n            if self.section_name is None:\n                logger.success(f'Successfully loaded the config.yaml')\n                return config_data\n            else:\n                logger.error(f'Section {self.section_name} not available in the config.yaml file')\n                return None  # Section not found in the YAML file\n        else:\n            with open(path, 'r') as file:\n                config = yaml.safe_load(file)\n            \n            logger.success(f'Successfully loaded {path}')\n            return config          \n    \n    def check_validity(self, config_data) -> None:\n        \"\"\"Checks the validity of certain important configuration settings.\n\n        Args:\n            config_data (_type_: dict): The configuration dictionary in the format output from load_and_extract_yaml_section().\n        \"\"\"\n        if config_data['model']['voting']['long_term_form_vote_perc'] + config_data['model']['voting']['short_term_form_vote_perc'] != 1:\n            logger.error('Voting weights do not add to 1! Please check configuration file!')\n            sys.exit(1)\n        if config_data['model']['regressor'] not in ['LinearRegression', 'PoissonRegressor', 'SVR', 'XGBRegressor']:\n            logger.error(f\"Model regressor {config_data['model']['regressor']} is invalid! Please check configuration file!\")\n            sys.exit(1)\n        if config_data['league'] not in ['EPL', 'La_Liga', 'Bundesliga', 'Serie_A', 'Ligue_1']:\n            logger.error(f\"League {config_data['league']} is invalid! Please check configuration file!\")\n            sys.exit(1)\n            \n    def load_configuration_class(self, config_data) -> dataclass:\n        \"\"\"Calls check_validity(). Imports the regressor depending on the users choice. Loads self.config dataclass with the validated configuration settings.\n\n        Args:\n            config_data (_type_: dict): The configuration dictionary in the format output from load_and_extract_yaml_section().\n\n        Returns:\n            self.config (_type_: Configuration_dataclass_object): The dataclass with the validated configuration settings.\n        \"\"\"\n        self.check_validity(config_data)\n        if (config_data['model']['regressor'] == \"LinearRegression\") or (config_data['model']['regressor'] == \"PoissonRegressor\"):\n            module_path = 'sklearn.linear_model'\n            regressor_module = importlib.import_module(module_path)\n            regressor = getattr(regressor_module, config_data['model']['regressor'])\n        if config_data['model']['regressor'] == 'SVR':\n            module_path = 'sklearn.svm'\n            regressor_module = importlib.import_module(module_path)\n            regressor = getattr(regressor_module, config_data['model']['regressor'])\n            \n        self.config = Configuration(\n                league= config_data['league'],\n                regressor = regressor,\n                bettor_bank = config_data['bettor']['initial_bank'],\n                bettor_kelly_cap = config_data['bettor']['kelly_cap'],\n                evaluation_output = config_data['data_gathering']['paths'][config_data['league']]['evaluation_output'],\n                months_of_form_list = [config_data['data_gathering']['long_term_form'], config_data['data_gathering']['short_term_form']],\n                seasons_to_gather= config_data['data_gathering']['seasons_to_gather'],\n                current_season= config_data['data_gathering']['current_season'],\n                data_co_uk_path= config_data['data_gathering']['paths'][config_data['league']]['data_co_uk_path'],\n                database = config_data['data_gathering']['paths'][config_data['league']]['database'],\n                bookmaker_url= config_data['data_gathering']['bookmaker'][config_data['league']]['url'],\n                bookmaker_dictionary= self.load_and_extract_yaml_section(path = config_data['data_gathering']['bookmaker'][config_data['league']]['dictionary_path']),\n                data_co_uk_url= config_data['data_gathering']['data_co_uk'][config_data['league']]['url'],\n                data_co_uk_dictionary= self.load_and_extract_yaml_section(path = config_data['data_gathering']['data_co_uk'][config_data['league']]['dictionary_path']),\n                fixture_download_url= config_data['data_gathering']['fixture_download'][config_data['league']]['url'],\n                fixture_download_dictionary= self.load_and_extract_yaml_section(path = config_data['data_gathering']['fixture_download'][config_data['league']]['dictionary_path']),\n                voting_dict= { 'long_term': config_data['model']['voting']['long_term_form_vote_perc'], 'short_term': config_data['model']['voting']['short_term_form_vote_perc']},\n                matchdays_to_drop = config_data['model']['matchdays_to_drop']\n        )\n        return self.config\n    \n        \n\n\n@dataclass\nclass Configuration:\n    \"\"\"A dataclass containing the configuration settings.\n    \"\"\"\n    league: str\n    regressor: object\n    bettor_bank: float\n    bettor_kelly_cap: float\n    evaluation_output: str\n    months_of_form_list: list\n    database: str\n    seasons_to_gather: list\n    current_season: str\n    data_co_uk_path: str\n    bookmaker_url: str\n    bookmaker_dictionary: dict\n    data_co_uk_url: str\n    data_co_uk_dictionary: dict\n    fixture_download_url: str\n    fixture_download_dictionary: dict\n    voting_dict: dict\n    matchdays_to_drop: int\n    "}
{"type": "source_file", "path": "europeanfootballleaguepredictor/models/base_model.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Any\n\n\nclass BaseModel(ABC):\n    \"\"\"An abstraction class to use as interface for any model classes that do not inherit from keras\"\"\"\n\n    @abstractmethod\n    def build_model(self):\n        \"\"\"A method to build the model\"\"\"\n        pass\n\n    @abstractmethod\n    def train_model(self, train_data: Any, validation_data: Any):\n        \"\"\"A method to fit the model\n        Args:\n            train_data: Train dataset\n            validation_data: Validation dataset\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def evaluate(self, evaluation_data: Any):\n        \"\"\"A method to evaluate the model\n        Args:\n            evaluation_data: Evaluation dataset\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, inference_data: Any):\n        \"\"\"A method to predict with the model\n        Args:\n            inference_data: Data to predict\n        \"\"\"\n        pass"}
{"type": "source_file", "path": "run_predictions.py", "content": "from europeanfootballleaguepredictor.models.probability_estimator import ProbabilityEstimatorNetwork\nfrom europeanfootballleaguepredictor.models.bettor import Bettor\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, PoissonRegressor\nfrom sklearn.svm import SVR\nimport pandas as pd\nfrom loguru import logger \nfrom europeanfootballleaguepredictor.common.config_parser import Config_Parser\nfrom europeanfootballleaguepredictor.utils.path_handler import PathHandler\nfrom europeanfootballleaguepredictor.visualization.visualize import Visualizer\nfrom pretty_html_table import build_table\nimport argparse\nimport os\nfrom europeanfootballleaguepredictor.data.database_handler import DatabaseHandler \n\n\"\"\"\nEuropean Football League Predictor Script\n\nThis script uses a probability estimator network to predict outcomes in the specified European football league and visualizes the predictions.\n\"\"\"\n\ndef main():\n    \"\"\"Main entry point for the script.\n\n    This function orchestrates the entire process of predicting football outcomes and visualizing the results.\n    \"\"\"\n    #Parsing the configuration file\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file (e.g., config.yaml)\", default='europeanfootballleaguepredictor/config/config.yaml')\n    config_file_path = parser.parse_args().config\n    \n    # Loading and extracting configuration data\n    config_data_parser = Config_Parser(config_file_path, None)\n    config_data = config_data_parser.load_and_extract_yaml_section()\n    config = config_data_parser.load_configuration_class(config_data)\n    \n    logger.info(config)\n\n    pd.set_option('display.precision', 2)\n    \n    database_handler = DatabaseHandler(league=config.league, database=config.database)\n    probability_estimator_network = ProbabilityEstimatorNetwork(voting_dict=config.voting_dict, matchdays_to_drop=config.matchdays_to_drop)\n    probability_estimator_network.build_network(regressor = config.regressor)\n    \n    short_term_form, long_term_form, for_prediction_short, for_prediction_long = database_handler.get_data(table_names=[\"Preprocessed_ShortTermForm\", \"Preprocessed_LongTermForm\", \"Preprocessed_UpcomingShortTerm\", \"Preprocessed_UpcomingLongTerm\"])\n    \n    probability_dataframe = probability_estimator_network.produce_probabilities(short_term_data=short_term_form, long_term_data=long_term_form, for_prediction_short=for_prediction_short, for_prediction_long=for_prediction_long)\n    logger.info(f'\\n {probability_dataframe}')\n\n    visualizer = Visualizer(probability_dataframe)\n    figure = visualizer.radar_scoreline_plot()\n    \n    # Save the interactive figure as an HTML file\n    output_handler = PathHandler(path=f'Predictions/{config.league}')\n    output_handler.create_paths_if_not_exists()\n    html_table = build_table(probability_dataframe.drop(['ScorelineProbability', 'Match_id'], axis=1), 'blue_light')\n    with open(f'Predictions/{config.league}/PredictionTable.html', 'w') as f:\n        f.write(html_table)\n    figure.write_html(f'Predictions/{config.league}/InteractiveFigure.html')\n    \nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "europeanfootballleaguepredictor/visualization/visualize.py", "content": "import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport pandas as pd\nimport numpy as np \nfrom loguru import logger \n\nclass Visualizer:\n    \"\"\"The visualizer of the predicted match probabilities\n    \"\"\"\n    def __init__(self, prediction_dataframe: pd.DataFrame) -> None:\n        \"\"\"Initializing the visualizer using the prediction dataframe\n\n        Args:\n            prediction_dataframe (pd.DataFrame): A dataframe containing match information and predicted probabilities from the model\n        \"\"\"\n        self.prediction_dataframe = prediction_dataframe.copy()\n        self.get_bookmaker_prob()\n\n    def get_bookmaker_prob(self):\n        \"\"\"Changes the bookmaker odds into probabilities\n        \"\"\"\n        try:\n            print(self.prediction_dataframe.columns)\n            self.prediction_dataframe[['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverLineOdds', 'UnderLineOdds', 'Yes', 'No']] = 1/self.prediction_dataframe[['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverLineOdds', 'UnderLineOdds', 'Yes', 'No']].astype(float)\n        except KeyError as e:\n            logger.error(e)\n            logger.warning('Bookmaker results were not processed as expected, will produce prediction figures not containing that data.')\n    \n    def produce_closest_line(self):\n        # Extracting the numerical values from the column names\n        lines = ['1.5', '2.5', '3.5']\n\n        # Calculate absolute differences between \"OverX\" and \"UnderX\" for each X\n        differences = {line: np.abs(self.prediction_dataframe[f'Over{line}Probability'] - self.prediction_dataframe[f'Under{line}Probability']) for line in lines}\n\n        # Determine the line with the minimum absolute difference for each row\n        self.prediction_dataframe['Line'] = [min(differences.keys(), key=lambda x: differences[x][i]) for i in range(len(self.prediction_dataframe))]\n\n        # Convert the \"Line\" column to string\n        self.prediction_dataframe['Line'] = self.prediction_dataframe['Line'].astype(str)\n            \n    def radar_scoreline_plot(self):\n        \"\"\"Creates the figure containing the scoreline barplot and the radar probability plot\n\n        Returns:\n            matplotlib.figure.Figure: A figure containing the scoreline barplot and the radar probability plot\n        \"\"\"\n        fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'polar'}, {'type': 'xy'}]], column_widths=[0.33, 0.67])\n        fig.update_layout(width=1500, height=650)\n        # create a list to store the buttons\n        buttons = []\n\n        # loop through each match and create two traces for Bookmaker odds and predicted values\n        for index, (date, h_team, a_team) in enumerate(zip(self.prediction_dataframe['Date'].tolist(),self.prediction_dataframe['HomeTeam'].tolist(), self.prediction_dataframe['AwayTeam'].tolist())):\n        \n            # create the trace for Bookmaker odds, try except block incase bookmaker odds are not included\n            try:\n                trace_bookmaker = go.Scatterpolar(\n                  r=100*np.round(np.array(self.prediction_dataframe.iloc[index][['HomeWinOdds', 'DrawOdds', 'AwayWinOdds', 'OverLineOdds', 'UnderLineOdds', 'Yes', 'No']].values.tolist()), 4),\n                  theta=['HomeWin', 'Draw', 'AwayWin', f\"Over{self.prediction_dataframe.iloc[index]['Line']}\", f\"Under{self.prediction_dataframe.iloc[index]['Line']}\", 'GG', 'NG'],\n                  fill='toself',\n                  name='Bookmaker Odds',\n                  marker = dict(color = 'rgb(82, 106, 131)' ),\n                  visible=False if index!=0 else True,\n                  hovertemplate='%{theta}<br>Probability: %{r:.2f}%<br>Bookmaker Probability'\n                )\n                fig.add_trace(trace_bookmaker, row=1, col=1)\n                mode = 'odds'\n            except KeyError:\n                #Produces the over under line with the closest margin\n                self.produce_closest_line()\n                mode = 'no_odds'\n            \n            # create the trace for predicted values\n            trace_predicted = go.Scatterpolar(\n                r= 100*np.round(np.array(self.prediction_dataframe.iloc[index][['HomeWinProbability', 'DrawProbability', 'AwayWinProbability', f\"Over{self.prediction_dataframe.iloc[index]['Line']}Probability\", f\"Under{self.prediction_dataframe.iloc[index]['Line']}Probability\", 'GGProbability', 'NGProbability']].values.tolist()), 4),\n                theta=['HomeWin', 'Draw', 'AwayWin', f\"Over{self.prediction_dataframe.iloc[index]['Line']}\", f\"Under{self.prediction_dataframe.iloc[index]['Line']}\", 'GG', 'NG'],\n                fill='toself',\n                name='Predicted Odds',\n                marker = dict(color ='rgb(141,211,199)'), #'rgb(217, 175, 107)'\n                visible=False if index!=0 else True,\n                hovertemplate='%{theta}<br>Probability: %{r:.2f}%<br>Predicted Probability'\n          )\n            fig.add_trace(trace_predicted, row=1, col=1)\n\n            scoreline_dict = self.prediction_dataframe.iloc[index]['ScorelineProbability'][0]\n            keys = list(scoreline_dict.keys())\n            values = list(scoreline_dict.values())\n\n            scoreline_frame = pd.DataFrame(values, columns=['P(score)'])\n            scoreline_frame['score'] = keys\n            scoreline_frame['display_score'] = scoreline_frame['score']  # New column for display in hover text\n            scoreline_frame.loc[scoreline_frame[\"P(score)\"] < 0.02, \"display_score\"] = 'Other'\n            scoreline_frame = scoreline_frame.sort_values(by=['P(score)'], ascending=False)\n\n            # Rest of your code\n            trace_goals = go.Bar(\n                x=scoreline_frame['display_score'],\n                y=100 * np.round(scoreline_frame['P(score)'], 4),\n                visible=False if index != 0 else True,\n                marker=dict(color=scoreline_frame['P(score)'], colorscale='darkmint'),\n                name='Probable Scoreline',\n                hovertemplate='Score %{customdata}<br>Probability: %{y:.2f}%',\n                customdata=scoreline_frame['score']  # Use the original score for hover text\n            )\n            fig.add_trace(trace_goals, row=1, col=2)\n             \n            # create the dictionary for the button\n            if mode=='odds':\n                button_dict = dict(\n                    method=\"restyle\",\n                    args=[{\"visible\": [False] * (3 * len(self.prediction_dataframe))}], # set all traces to false initially\n                    label= f\"<b>{h_team}-{a_team} | {str(date.strip('[]'))}</b>\" \n                )\n                # set visibility to true for the corresponding traces\n                button_dict[\"args\"][0][\"visible\"][3*index] = True # Bookmaker odds trace\n                button_dict[\"args\"][0][\"visible\"][3*index+1] = True # Predicted trace\n                button_dict[\"args\"][0][\"visible\"][3*index+2] = True # Goal trace\n\n                # set visibility to true for the corresponding traces for the first match\n                if index == 0:\n                    button_dict[\"args\"][0][\"visible\"][0] = True # Bookmaker odds trace\n                    button_dict[\"args\"][0][\"visible\"][1] = True # Predicted trace\n                    button_dict[\"args\"][0][\"visible\"][2] = True\n\n            if mode=='no_odds':\n                button_dict = dict(\n                    method=\"restyle\",\n                    args=[{\"visible\": [False] * (2 * len(self.prediction_dataframe))}], # set all traces to false initially\n                    label= f\"<b>{h_team}-{a_team} | {str(date.strip('[]'))}</b>\" \n                )\n                # set visibility to true for the corresponding traces\n                button_dict[\"args\"][0][\"visible\"][2*index] = True # Predicted trace\n                button_dict[\"args\"][0][\"visible\"][2*index+1] = True # Goal trace\n\n                # set visibility to true for the corresponding traces for the first match\n                if index == 0:\n                    button_dict[\"args\"][0][\"visible\"][0] = True # Predicted trace\n                    button_dict[\"args\"][0][\"visible\"][1] = True # Goals trace\n\n            \n            # append the button to the list\n            buttons.append(button_dict)\n\n        # update the layout with the buttons\n        fig.update_layout(\n            updatemenus=[go.layout.Updatemenu(\n                buttons=buttons,\n                active=0, # set the initial active button\n                x=0.1,\n                y=0.95,\n                direction=\"down\",\n            )],\n\n        )\n\n        return fig\n"}
