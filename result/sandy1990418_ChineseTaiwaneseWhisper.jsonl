{"repo_info": {"repo_name": "ChineseTaiwaneseWhisper", "repo_owner": "sandy1990418", "repo_url": "https://github.com/sandy1990418/ChineseTaiwaneseWhisper"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_inference.py", "content": "import pytest\nimport numpy as np\nfrom src.inference.flexible_inference import ChineseTaiwaneseASRInference\n\n\n@pytest.fixture\ndef inference():\n    return ChineseTaiwaneseASRInference(\"openai/whisper-small\", \n                                        device=\"cpu\", \n                                        use_faster_whisper=False, \n                                        language=\"zh-TW\")\n\n\ndef test_transcribe_stream(inference):\n    def dummy_audio_stream():\n        for _ in range(5):  # 5 chunks of 1 second each\n            yield np.zeros(16000, dtype=np.float32)\n\n    transcriptions = list(inference.transcribe_stream(dummy_audio_stream(), chunk_duration=1))\n    \n    assert len(transcriptions) > 0\n    for transcription in transcriptions:\n        assert isinstance(transcription, str)"}
{"type": "test_file", "path": "tests/mlflow_test.py", "content": "import time\nimport random\nfrom src.utils.mlflow_logging import mlflow_logging, setup_mlflow\nimport torch\nimport torch.nn as nn\nfrom src.utils.logging import logger\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# # Set environment variables\n# os.environ[\"USE_MLFLOW\"] = \"true\"\n# os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"  # Adjust as needed\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n@mlflow_logging(experiment_name=\"Decorator_Test\")\ndef train_model(epochs, learning_rate):\n    logger.info(\n        f\"Starting model training with {epochs} epochs and learning rate {learning_rate}\"\n    )\n    model = SimpleModel()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    criterion = nn.MSELoss()\n\n    for epoch in range(epochs):\n        # Simulate training\n        inputs = torch.randn(100, 10)\n        targets = torch.randn(100, 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        logger.debug(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n        time.sleep(0.1)  # Simulate some processing time\n\n    # Simulate evaluation\n    eval_loss = random.uniform(0.1, 0.5)\n    accuracy = random.uniform(0.7, 0.95)\n\n    logger.info(\n        f\"Training completed. Eval Loss: {eval_loss:.4f}, Accuracy: {accuracy:.4f}\"\n    )\n\n    return {\n        \"model\": model,  # 返回模型的狀態字典，而不是模型對象\n        \"train_metrics\": {\"final_loss\": loss.item()},\n        \"eval_metrics\": {\"eval_loss\": eval_loss, \"accuracy\": accuracy},\n    }\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting MLflow decorator test\")\n    try:\n        setup_mlflow()\n        result = train_model(epochs=5, learning_rate=0.01)\n        logger.info(\"Training completed. Check MLflow for logged data.\")\n    except Exception as e:\n        logger.error(f\"An error occurred during the test: {e}\")\n        logger.error(\"Make sure the MLflow server is running and accessible.\")\n"}
{"type": "test_file", "path": "tests/test_dataset.py", "content": "import pytest\nfrom src.data.dataset import ChineseTaiwaneseDataset\nfrom transformers import WhisperProcessor\n\n\n@pytest.fixture\ndef processor():\n    return WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n\n\n@pytest.fixture\ndef dataset(processor):\n    return ChineseTaiwaneseDataset(\"mozilla-foundation/common_voice_11_0\", \"test\", processor, max_samples=10)\n\n\ndef test_dataset_length(dataset):\n    assert len(dataset) == 10\n\n\ndef test_dataset_item(dataset):\n    item = dataset[0]\n    assert \"input_features\" in item\n    assert \"labels\" in item\n    assert item[\"input_features\"].dim() == 2\n    assert item[\"labels\"].dim() == 1"}
{"type": "test_file", "path": "tests/test_model.py", "content": "import pytest\nimport torch\nfrom src.models.whisper_model import load_whisper_model\n\n\n@pytest.fixture\ndef model_and_processor():\n    return load_whisper_model(\"openai/whisper-small\")\n\n\ndef test_model_output(model_and_processor):\n    model, processor = model_and_processor\n    input_features = torch.rand(1, 80, 3000)\n    input_features = input_features.to(model.device)\n    \n    with torch.no_grad():\n        output = model(input_features)\n    \n    assert output is not None\n    assert hasattr(output, 'logits')\n    assert output.logits.dim() == 3"}
{"type": "source_file", "path": "api_main.py", "content": "# main.py\nfrom fastapi import FastAPI, UploadFile, File, HTTPException\nfrom pydantic import BaseModel\nimport torch\nfrom src.inference.flexible_inference import ChineseTaiwaneseASRInference\nimport librosa\nimport io\nimport os\nfrom typing import Optional, List\nfrom uuid import UUID, uuid4\nfrom datetime import datetime\nfrom src.utils.logging import logger\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\napp = FastAPI()\n\nmodel_name = os.getenv(\"MODEL_PATH\", \"openai/whisper-small\")\n\n# Initialize ASR model\ntry:\n    logger.debug(\"Attempting to initialize ASR model\")\n    model = ChineseTaiwaneseASRInference(\n        model_path=model_name,\n        device=(\n            \"cuda\"\n            if torch.cuda.is_available()\n            and os.getenv(\"USE_GPU\", \"False\").lower() == \"true\"\n            else \"cpu\"\n        ),\n        language=os.getenv(\"LANGUAGE\", \"chinese\"),\n    )\n    logger.info(\"ASR model initialized successfully.\")\nexcept Exception as e:\n    logger.error(f\"Failed to initialize ASR model: {str(e)}\")\n    raise\n\n\nclass TranscriptionRequest(BaseModel):\n    audio_file: UploadFile\n    max_alternatives: Optional[int] = 1\n\n\nclass TranscriptionResponse(BaseModel):\n    id: UUID\n    text: str\n    confidence: float\n    timestamp: datetime\n\n\n# Simulating a database\ntranscriptions_db = {}\n\n\n@app.post(\"/transcribe\", response_model=TranscriptionResponse)\nasync def transcribe_audio(file: UploadFile = File(...)):\n    try:\n        logger.debug(\"Reading file contents\")\n        contents = await file.read()\n\n        logger.debug(\"Loading audio with librosa\")\n        audio, sr = librosa.load(io.BytesIO(contents), sr=None)\n\n        # Use your ASR model for transcription\n        transcription = model.transcribe_batch(audio, sr)[0]\n\n        # Create and store the transcription result\n        transcription_id = uuid4()\n        result = TranscriptionResponse(\n            id=transcription_id,\n            text=transcription,\n            confidence=0.95,  # Assuming a fixed confidence for simplicity\n            timestamp=datetime.now(),\n        )\n        transcriptions_db[transcription_id] = result\n\n        return result\n    except Exception as e:\n        logger.error(f\"Error during transcription: {str(e)}\")\n        raise HTTPException(\n            status_code=500, detail=\"An error occurred during transcription\"\n        )\n\n\n@app.get(\"/transcriptions\", response_model=List[TranscriptionResponse])\nasync def list_transcriptions():\n    return list(transcriptions_db.values())\n\n\n@app.get(\"/transcription/{transcription_id}\", response_model=TranscriptionResponse)\nasync def get_transcription(transcription_id: UUID):\n    if transcription_id not in transcriptions_db:\n        raise HTTPException(status_code=404, detail=\"Transcription not found\")\n    return transcriptions_db[transcription_id]\n\n\n@app.delete(\"/transcription/{transcription_id}\")\nasync def delete_transcription(transcription_id: UUID):\n    if transcription_id not in transcriptions_db:\n        raise HTTPException(status_code=404, detail=\"Transcription not found\")\n    del transcriptions_db[transcription_id]\n    return {\"message\": \"Transcription deleted successfully\"}\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n\n# if __name__ == \"__main__\":\n#     import uvicorn\n\n#     logger.info(\"Starting FastAPI application on port %s\", os.getenv(\"PORT\", 8000))\n#     uvicorn.run(app, host=\"0.0.0.0\", port=8000))\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "main.py", "content": "from inference import FusionWhisperLLaMAInference\nfrom src.utils.logging import logger\nimport librosa\n\n\ndef main():\n    # Initialize the fusion model\n    logger.info(\"Initializing FusionWhisperLLaMAInference model...\")\n    fusion_model = FusionWhisperLLaMAInference(\n        whisper_model_path=\"openai/whisper-small\",\n        llama_model_path=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n        device=\"cuda\",\n        use_peft=False,\n        language=\"chinese\",\n        use_timestamps=True,\n        lm_weight=0.1,\n    )\n\n    # Single audio file transcription\n    audio_path = \"S00001.wav\"\n    logger.info(f\"Transcribing single audio file: {audio_path}\")\n    audio, sr = librosa.load(audio_path, sr=None)\n    transcription = fusion_model.transcribe_batch(audio, sr)\n    logger.info(f\"Transcription: {transcription}\")\n\n    # Streaming transcription example\n    logger.info(\"Starting streaming transcription...\")\n\n    def audio_stream_generator(\n        audio_path, chunk_size=1600\n    ):  # 0.1 second chunks at 16kHz\n        audio, sr = librosa.load(audio_path, sr=16000)\n        for i in range(0, len(audio), chunk_size):\n            yield audio[i: i + chunk_size]\n\n    for result in fusion_model.transcribe_stream(audio_stream_generator(audio_path)):\n        if result:\n            logger.info(f\"Partial transcription: {result['transcription']}\")\n            logger.info(f\"Speed: {result['speed']:.2f}x real-time\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "scripts/infer.py", "content": "from transformers import HfArgumentParser\nimport numpy as np\nfrom typing import Dict, List\nimport soundfile as sf\nfrom src.inference.flexible_inference import ChineseTaiwaneseASRInference\nfrom src.utils.logging import logger\nimport os\nimport json\nfrom src.config import InferenceArguments\nimport tqdm\n\n\ndef get_wav_files(path: str) -> List[str]:\n    \"\"\"\n    Get all WAV files from a directory or return the file if it's a WAV file.\n    \n    Args:\n    path (str): Path to a directory or a file.\n    \n    Returns:\n    List[str]: List of WAV file paths.\n    \"\"\"\n    if os.path.isdir(path):\n        files = [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith('.wav')]\n        return files\n    elif os.path.isfile(path) and path.lower().endswith('.wav'):\n        return [path]\n    else:\n        return []\n    \n\ndef load_audio(file_path):\n    audio, sample_rate = sf.read(file_path)\n    # Convert to mono if stereo\n    # Check if the audio is stereo\n    if audio.ndim == 2 and audio.shape[1] == 2:\n        return audio[:, 0], audio[:, 1], sample_rate\n    else:\n        return audio, sample_rate\n\n\ndef transcribe_channel(\n    inference, audio: np.ndarray, sample_rate: int, channel_name: str\n) -> Dict:\n    \"\"\"\n    Transcribe a single audio channel.\n\n    Args:\n    inference: The inference object with a transcribe_batch method.\n    audio (np.ndarray): Audio data for a single channel.\n    sample_rate (int): Sample rate of the audio.\n    channel_name (str): Name of the channel ('left' or 'right').\n\n    Returns:\n    Dict: Dictionary containing channel name and transcriptions.\n    \"\"\"\n    try:\n        transcriptions = inference.transcribe_batch(audio, sample_rate)\n        return {\"channel\": channel_name, \"transcriptions\": transcriptions}\n    except Exception as e:\n        logger.error(f\"Error transcribing {channel_name} channel: {str(e)}\")\n        return {\"channel\": channel_name, \"transcriptions\": [], \"error\": str(e)}\n\n\ndef process_audio_file(inference, file_path: str) -> Dict:\n    \"\"\"\n    Process a single audio file: load, split channels, and transcribe.\n\n    Args:\n    inference: The inference object with a transcribe_batch method.\n    file_path (str): Path to the audio file.\n\n    Returns:\n    Dict: Dictionary containing file name and channel transcriptions.\n    \"\"\"\n    audio_result = load_audio(file_path)\n    if audio_result is None:\n        return {\n            \"file_name\": os.path.basename(file_path),\n            \"error\": \"Failed to load audio\",\n        }\n\n    if len(audio_result) == 3:\n        left_channel, right_channel, sample_rate = audio_result\n\n        return {\n            \"file_name\": os.path.basename(file_path),\n            \"channels\": [\n                transcribe_channel(inference, left_channel, sample_rate, \"left\"),\n                transcribe_channel(inference, right_channel, sample_rate, \"right\"),\n            ],\n        }\n    else:\n        channel, sample_rate = audio_result\n\n        return {\n            \"file_name\": os.path.basename(file_path),\n            \"channels\": [\n                transcribe_channel(inference, channel, sample_rate, \"single_channel\"),\n            ],\n        }\n\n\ndef batch_inference(\n    inference, audio_files: List[str], output_dir: str, file_name: str\n) -> None:\n    \"\"\"\n    Perform batch inference on a list of audio files and save results to a JSON file.\n\n    Args:\n    inference: The inference object with a transcribe_batch method.\n    audio_files (List[str]): List of paths to audio files.\n    output_dir (str): Directory where the JSON file will be saved.\n    \"\"\"\n    results = []\n    audio_files_dir = []\n    for file in tqdm.tqdm(audio_files):\n        if 'wav' not in file:\n            audio_files = get_wav_files(file)\n            audio_files_dir.extend(audio_files)\n        else:\n            audio_files_dir.append(file)\n\n    for file in tqdm.tqdm(audio_files_dir):\n        logger.info(f\"Processing file: {file}\")\n        result = process_audio_file(inference, file)\n        results.append(result)\n\n    output_file = os.path.join(output_dir, file_name)\n    try:\n        with open(output_file, \"a+\", encoding=\"utf-8\") as f:\n            json.dump(results, f, ensure_ascii=False, indent=4)\n        logger.info(f\"Results written to {output_file}\")\n    except Exception as e:\n        logger.error(f\"Error writing results to JSON: {str(e)}\")\n\n\ndef stream_inference(inference, audio_file):\n    def audio_generator():\n        audio, sample_rate = load_audio(audio_file)\n        chunk_size = int(sample_rate * 1)  # 1 second chunks\n        for i in range(0, len(audio), chunk_size):\n            yield audio[i: i + chunk_size]\n\n    logger.info(f\"Streaming transcription for file: {audio_file}\")\n    for transcription in inference.transcribe_stream(audio_generator()):\n        logger.info(f\"Partial transcription: {transcription}\")\n\n\ndef parse_args():\n    parser = HfArgumentParser(InferenceArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    return args\n\n\ndef main():\n    args = parse_args()\n    inference = ChineseTaiwaneseASRInference(\n        model_path=args.model_path,\n        device=args.device,\n        use_peft=args.use_peft,\n        language=args.language,\n        use_timestamps=args.use_timestamps,\n    )\n    if args.mode == \"batch\":\n        # # Process each input path (file or directory)\n        # all_audio_files = []\n        # for path in args.audio_files:\n        #     all_audio_files.extend(get_wav_files(path))\n        # if not all_audio_files:\n        #     logger.warning(\"No WAV files found in the specified paths.\")\n        #     return\n        batch_inference(inference, args.audio_files, args.output_dir, args.file_name)\n    else:\n        for audio_file in args.audio_files:\n            stream_inference(inference, audio_file)\n\n\nif __name__ == \"__main__\":\n    main()\n\n# TODO : convert model to Ctranslate2\n# TODO : Distil-model\n# TODO : Teacher-student\n# TODO : Speculative decoding\n"}
{"type": "source_file", "path": "src/config/gradio_config.py", "content": "from dataclasses import dataclass, field\n\n\n@dataclass\nclass GradioArguments:\n    cache_dir: str = field(\n        default=\"asr_transcription_streaming_cache\",\n        metadata={\"help\": \"Path to store ASR result\"}\n    )\n    cache_streaming_filename: str = field(\n        default=\"asr_log_streaming.json\",\n        metadata={\"help\": \"File name to store ASR result\"}\n    )\n    cache_batch_filename: str = field(\n        default=\"asr_log_batch.json\",\n        metadata={\"help\": \"File name to store ASR result\"}\n    )\n    language: str = field(\n        default=\"chinese\",\n        metadata={\"help\": \"Language code for the model (e.g., zh-TW for Traditional Chinese)\"}\n    )\n"}
{"type": "source_file", "path": "run_eval.py", "content": "import argparse\nimport time\nfrom time import strftime, localtime\nimport os\n\nimport evaluate\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import load_dataset\nimport librosa\nimport numpy as np\nfrom src.utils.logging import logger\nimport math\n\n# Follow open_asr_leaderboard\n# https://github.com/huggingface/open_asr_leaderboard/blob/main/speechbrain/run_eval.py\n\n\ndef get_whisper_model(model_name_or_path: str, device: str):\n    \"\"\"Load a pretrained Whisper model.\n\n    Arguments:\n    ---------\n    model_name_or_path : str\n        Name or path of the Whisper model. E.g., 'openai/whisper-small' or path to a custom model.\n    device : str\n        Device to run the model on. E.g., 'cpu' or 'cuda:0'.\n\n    Returns:\n    -------\n    tuple\n        Containing the loaded model and processor.\n    \"\"\"\n    processor = AutoProcessor.from_pretrained(model_name_or_path)\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path).to(device)\n    return model, processor\n\n\ndef transcribe_batch(model, processor, batch, device, language):\n    \"\"\"Transcribe a batch of audio.\n\n    Arguments:\n    ---------\n    model : WhisperForConditionalGeneration\n        Whisper model.\n    processor : WhisperProcessor\n        Whisper processor.\n    batch : dict\n        Batch containing audio data.\n    device : str\n        Device to run the model on.\n\n    Returns:\n    -------\n    dict\n        Dictionary containing transcription results and timing.\n    \"\"\"\n    # Load audio inputs\n    audio_inputs = batch[\"audio\"]\n    input_features = processor(\n        audio_inputs, sampling_rate=16000, return_tensors=\"pt\"\n    ).input_features.to(device)\n\n    # Start timing\n    start_time = time.time()\n    # Generate transcriptions\n    with torch.no_grad():\n        generated_ids = model.generate(input_features, language=language)\n    transcriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n\n    # End timing\n    runtime = time.time() - start_time\n    logger.info(f\"Run Time: {runtime}\")\n    # Normalize transcriptions\n    normalized_transcriptions = [normalize_text(trans) for trans in transcriptions]\n    return {\n        \"transcription_time_s\": [runtime / len(audio_inputs)] * len(audio_inputs),\n        \"predictions\": normalized_transcriptions,\n    }\n\n\ndef normalize_text(text):\n    \"\"\"Perform simple text normalization.\"\"\"\n    return text.lower().strip()\n\n\ndef main(args):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, processor = get_whisper_model(args.model_name_or_path, device)\n\n    # Load dataset\n    dataset = load_dataset(\n        args.dataset_path, args.language, split=args.split, streaming=args.streaming, trust_remote_code=True\n    )\n    if args.max_eval_samples:\n        dataset = dataset.select(range(min(args.max_eval_samples, len(dataset))))\n\n    # Prepare dataset\n    def prepare_dataset(batch):\n        audio = batch[\"audio\"]\n        batch[\"audio\"] = librosa.resample(\n            np.array(audio[\"array\"]), orig_sr=audio[\"sampling_rate\"], target_sr=16000\n        )\n        batch[\"audio_length_s\"] = len(batch[\"audio\"]) / 16000\n        batch[\"norm_text\"] = normalize_text(batch[\"sentence\"])\n        return batch\n\n    remove_columns = [\n        \"client_id\",\n        \"path\",\n        \"up_votes\",\n        \"down_votes\",\n        \"age\",\n        \"gender\",\n        \"accent\",\n        \"locale\",\n        \"segment\",\n        \"variant\",\n    ]\n    dataset = dataset.map(prepare_dataset, remove_columns=remove_columns)\n\n    # Evaluate\n    all_results = {\n        \"audio_length_s\": [],\n        \"transcription_time_s\": [],\n        \"predictions\": [],\n        \"references\": [],\n    }\n    # Calculate total number of batches if possible\n    if not args.streaming and hasattr(dataset, '__len__'):\n        total_batches = math.ceil(len(dataset) / args.batch_size)\n    else:\n        total_batches = None  # Unknown size for streaming datasets\n\n    data_iterator = dataset.iter(batch_size=args.batch_size)\n    progress_bar = tqdm(data_iterator, total=total_batches, desc=\"Evaluating\", dynamic_ncols=True)\n\n    for batch in progress_bar:\n        language = \"chinese\" if \"zh\" in args.language else args.language\n        results = transcribe_batch(model, processor, batch, device, language)\n\n        all_results[\"predictions\"].extend(results[\"predictions\"])\n        all_results[\"transcription_time_s\"].extend(results[\"transcription_time_s\"])\n       \n        all_results[\"references\"].extend(batch[\"norm_text\"])\n        all_results[\"audio_length_s\"].extend(batch[\"audio_length_s\"])\n\n    # Calculate WER\n    wer_metric = evaluate.load(\"wer\")\n    wer = wer_metric.compute(\n        references=all_results[\"references\"], predictions=all_results[\"predictions\"]\n    )\n    wer = round(100 * wer, 2)\n\n    # Calculate RTFx\n    rtfx = round(\n        sum(all_results[\"audio_length_s\"]) / sum(all_results[\"transcription_time_s\"]), 2\n    )\n\n    print(f\"WER: {wer}%, RTFx: {rtfx}\")\n\n    # Save results\n    output_dir = \"evaluation_results\"\n    os.makedirs(output_dir, exist_ok=True)\n    run_name = f\"{strftime('%Y-%m-%d', localtime(time.time()))}\"\n    \n    output_file = os.path.join(output_dir, f\"{args.dataset_path.split('/')[-1]}_{args.split}_{run_name}_results.txt\")\n    with open(output_file, \"w\") as f:\n        f.write(f\"Model: {args.model_name_or_path}\\n\")\n        f.write(f\"Split: {args.split}\\n\")\n        f.write(f\"WER: {wer}%\\n\")\n        f.write(f\"RTFx: {rtfx}\\n\")\n    print(f\"Results saved to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_name_or_path\",\n        type=str,\n        required=True,\n        help=\"Whisper model name or path\",\n    )\n    parser.add_argument(\n        \"--dataset_path\",\n        type=str,\n        default=\"mozilla-foundation/common_voice_16_1\",\n        help=\"Dataset path\",\n    )\n    parser.add_argument(\"--language\", type=str, required=True, help=\"language name\")\n    parser.add_argument(\n        \"--split\", type=str, default=\"test\", help=\"Dataset split to evaluate on\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=int,\n        default=-1,\n        help=\"Device to run evaluation on (-1 for CPU, 0 or greater for GPU)\",\n    )\n    parser.add_argument(\n        \"--batch_size\", type=int, default=32, help=\"Batch size for evaluation\"\n    )\n    parser.add_argument(\n        \"--max_eval_samples\",\n        type=int,\n        default=None,\n        help=\"Maximum number of samples to evaluate\",\n    )\n    parser.add_argument(\n        \"--streaming\", default=False, help=\"Whether to stream the dataset\"\n    )\n    args = parser.parse_args()\n\n    main(args)\n"}
{"type": "source_file", "path": "scripts/summary.py", "content": "from openai import OpenAI\nfrom typing import List, Dict, Optional, Any\nimport json\nimport os \nfrom src.utils.logging import logger\n\n\ndef read_transcript_json(file_path: str) -> List[Dict]:\n    \"\"\"\n    Read the transcript JSON file and return its content.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return json.load(file)\n\n\ndef extract_text_from_transcript(transcript: Dict) -> str:\n    \"\"\"\n    Extract the text content from the transcript, joining transcriptions from all channels.\n    \"\"\"\n    all_transcriptions = []\n    for item in transcript:\n        if 'channels' in item:\n            for channel in item['channels']:\n                transcriptions = channel.get('transcriptions', '')\n                if isinstance(transcriptions, str):\n                    all_transcriptions.append(transcriptions)\n                elif isinstance(transcriptions, list):\n                    all_transcriptions.extend(transcriptions)\n        elif 'message' in item:\n            transcriptions = item['message']\n            timestamp = item.get('timestamp', None)\n            all_transcriptions.append(f\"[{timestamp}] {transcriptions}\")\n        \n    # Join all transcriptions, removing any empty strings\n    return ' '.join(filter(bool, all_transcriptions))\n\n\ndef summarize_transcript(transcript: str) -> str:\n    \"\"\"\n    Send the transcript to OpenAI's API for summarization.\n    \"\"\"\n    # OpenAI KPI\n    with open('api_key/key.txt', 'r') as f: \n        api_key = f.read()\n\n    client = OpenAI(api_key=api_key)\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"\"\"請將以下會議內容紀錄並彙整為清晰、\\\n                完整的會議紀要，並以繁體中文返回。請運用 Chain of Thought（CoT）方法，\\\n                逐步分析和總結每個議題。請確保包含以下要點：\\\n                主要討論議題和決策\n                任何待辦事項或後續步驟\n                重要意見和建議\n                務必確保會議紀要簡潔明瞭且不遺漏任何重要信息。\\\n                最後以Markdown Code的形式提供，不需要會議時間\n                \"\"\"},\n                {\"role\": \"user\", \"content\": f\"Please summarize the following transcript:\\n\\n{transcript}\"}\n            ]\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        logger.error(f\"An error occurred while summarizing: {e}\")\n        return None\n\n\ndef process_transcript_file(file_path: str) -> str:\n    \"\"\"\n    Process a single transcript file: read, extract text, and summarize.\n    \"\"\"\n    transcript_data = read_transcript_json(file_path)\n    transcript_text = extract_text_from_transcript(transcript_data)\n    return summarize_transcript(transcript_text)\n\n# TODO: put summary to gradio_interface\n\n\ndef process_single_transcript(file_path: str) -> Optional[str]:\n    \"\"\"\n    Process a single transcript file and generate a summary.\n\n    Args:\n        file_path (str): Path to the transcript JSON file.\n\n    Returns:\n        Optional[str]: Summary of the transcript, or None if processing failed.\n    \"\"\"\n    try:\n        # Implement the logic to process the transcript file and generate summary\n        # This is a placeholder for the actual implementation\n        summary = process_transcript_file(file_path)\n        return summary\n    except Exception as e:\n        logger.error(f\"Error processing file {file_path}: {str(e)}\")\n        return None\n\n\ndef process_transcript_directory(directory: str) -> List[Dict[str, Optional[str]]]:\n    results = []\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            summary = process_single_transcript(file_path)\n            results.append({\"filename\": filename, \"summary\": summary})\n    return results\n\n\ndef process_transcripts(args: Optional[Any] = None, cache_filename: Optional[Any] = None) -> None:\n    \"\"\"\n    Process transcripts based on provided arguments.\n\n    Args:\n        args (Optional[Any]): Command line arguments. If None, use default values.\n    \"\"\"\n    if args is None:\n        transcript_dir = \"asr_transcription_streaming_cache\"\n        results = process_transcript_directory(transcript_dir)\n    else:\n        transcript_dir = args.cache_dir\n        filename = cache_filename\n        file_path = os.path.join(transcript_dir, filename)\n        summary = process_single_transcript(file_path)\n        return summary\n        # results = [{\"filename\": filename, \"summary\": summary}]\n\n    for result in results:\n        if result[\"summary\"]:\n            logger.info(f\"Summary for {result['filename']}:\")\n            logger.info(result[\"summary\"])\n            logger.info(\"\\n\" + \"=\"*50 + \"\\n\")\n        else:\n            logger.error(f\"Failed to generate summary for {result['filename']}\")\n\n\ndef main():\n    return process_transcripts()\n\n# def main(args: Optional[Any] = None):\n#     # Directory containing transcript JSON files\n\n#     if isinstance(args, type(None)):\n#         transcript_dir = \"asr_transcription_streaming_cache\"  # \"output\"\n        \n#         # Process all JSON files in the directory\n#         for filename in os.listdir(transcript_dir):\n#             if filename.endswith('.json'):\n#                 file_path = os.path.join(transcript_dir, filename)\n#                 logger.info(f\"Processing file: {filename}\")\n                \n#                 summary = process_transcript_file(file_path)\n\n#                 if summary:\n#                     logger.info(f\"Summary for {filename}:\")\n#                     logger.info(summary)\n#                     logger.info(\"\\n\" + \"=\"*50 + \"\\n\")\n#                 else:\n#                     logger.error(f\"Failed to generate summary for {filename}\")\n#     else:\n#         transcript_dir = args.cache_dir\n#         filename = args.cache_file_name\n#         file_path = os.path.join(transcript_dir, filename)\n#         logger.info(f\"Processing file: {filename}\")\n        \n#         summary = process_transcript_file(file_path)\n\n#         if summary:\n#             logger.info(f\"Summary for {filename}:\")\n#             logger.info(summary)\n#             logger.info(\"\\n\" + \"=\"*50 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "src/config/data_config.py", "content": "from dataclasses import dataclass, field\nfrom typing import Optional, List, Literal\nimport os\nimport json\n\n\n@dataclass\nclass DatasetAttr:\n    load_from: str\n    dataset_name: Optional[str] = None\n    dataset_sha1: Optional[str] = None\n    audio: Optional[str] = None\n    target: Optional[str] = None\n    language: Optional[str] = None\n    dataset_args: Optional[list] = field(default_factory=lambda: list())\n    dataset_kwargs: Optional[dict] = field(default_factory=lambda: dict(split=\"train\"))\n\n    def __repr__(self) -> str:\n        return self.dataset_name\n\n\n@dataclass\nclass DataArguments:\n    dataset: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of provided dataset(s) to use. Use commas to separate multiple datasets.\"}\n    )\n    dataset_dir: Optional[str] = field(\n        default=\"./youtube_data\",\n        metadata={\"help\": \"The name of the folder containing datasets.\"}\n    )\n    sampling_rate: Optional[int] = field(\n        default=16000,\n        metadata={\"help\": \"The frequency of the audio data.\"}\n    )\n    max_input_length: Optional[int] = field(\n        default=30,\n        metadata={\"help\": \"Maximum input length in seconds for audio clips\"}\n    )\n    do_lower_case: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Lower case the target.\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"}\n    )\n    overwrite_cache: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Overwrite the cached training and evaluation sets.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"For debugging purposes or quicker training, truncate the number \\\n            of training examples to this value if set.\"}\n    )\n    max_eval_samples: Optional[float] = field(\n        default=None,\n        metadata={\"help\": \"For debugging purposes or quicker training, truncate the number of \\\n            evaluation examples to this value if set.\"}\n    )\n    test_dataset_name: Optional[str] = field(\n        default=\"common_voice_13_test\",\n        metadata={\"help\": \"Name of the dataset to use for testing. If not specified, a \\\n            portion of the train dataset will be used based on max_eval_samples.\"}\n    )\n    timestamp: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Timestamp to control CommonVoice_13 add timestamp value.\"}\n    )\n    mix_strategy: Optional[Literal[\"concat\", \"interleave_under\", \"interleave_over\"]] = field(\n        default=\"concat\",\n        metadata={\"help\": \"Strategy to use in dataset mixing.\"}\n    )\n    interleave_probs: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Probabilities to sample data from datasets. Use commas to separate multiple datasets.\"}\n    )\n    streaming: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Enable streaming mode.\"}\n    )\n\n    def init_for_training(self):\n        dataset_names = [ds.strip() for ds in self.dataset.split(\",\")]\n        dataset_info_path = os.path.join(self.dataset_dir, \"dataset_info.json\")\n        \n        if not os.path.exists(dataset_info_path):\n            raise FileNotFoundError(f\"dataset_info.json not found in {self.dataset_dir}. \"\n                                    f\"Please ensure the file exists and the path is correct.\")\n\n        try:\n            with open(dataset_info_path, \"r\") as f:\n                dataset_info = json.load(f)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Error parsing dataset_info.json: {str(e)}. \"\n                             f\"Please check if the JSON file is correctly formatted.\")\n\n        self.dataset_list: List[DatasetAttr] = []\n        for name in dataset_names:\n            if name not in dataset_info:\n                raise ValueError(f\"Dataset '{name}' not found in dataset_info.json. \"\n                                 f\"Available datasets: {', '.join(dataset_info.keys())}\")\n\n            dataset_config = dataset_info[name]\n            if \"hf_hub_url\" in dataset_config:\n                dataset_attr = DatasetAttr(\"hf_hub\", dataset_name=dataset_config[\"hf_hub_url\"])\n            elif \"script_url\" in dataset_config:\n                dataset_attr = DatasetAttr(\"script\", dataset_name=dataset_config[\"script_url\"])\n            elif \"file_name\" in dataset_config:\n                dataset_attr = DatasetAttr(\"file\", dataset_name=dataset_config[\"file_name\"])\n            else:\n                raise ValueError(f\"Invalid configuration for dataset '{name}'. \"\n                                 f\"Must specify either 'hf_hub_url', 'script_url', or 'file_name'.\")\n\n            if \"columns\" in dataset_config:\n                dataset_attr.audio = dataset_config[\"columns\"].get(\"audio\")\n                dataset_attr.target = dataset_config[\"columns\"].get(\"target\")\n                dataset_attr.language = dataset_config[\"columns\"].get(\"language\")\n                dataset_attr.kwargs = dataset_config[\"columns\"].get(\"kwargs\", None)\n\n            dataset_attr.dataset_args = dataset_config.get(\"dataset_args\", [])\n            dataset_attr.dataset_kwargs = dataset_config.get(\"dataset_kwargs\", {})\n\n            self.dataset_list.append(dataset_attr)\n        if not self.dataset_list:\n            raise ValueError(\"No valid datasets found in the configuration.\")"}
{"type": "source_file", "path": "src/config/inference_config.py", "content": "from dataclasses import dataclass, field\nfrom typing import List\nimport torch\nimport os\n\n\n@dataclass\nclass InferenceArguments:\n    model_path: str = field(\n        metadata={\"help\": \"Path to the ASR model\"}\n    )\n    audio_files: List[str] = field(\n        default_factory=list,\n        metadata={\"help\": \"Path to audio file(s)\"}\n    )\n    mode: str = field(\n        default=\"batch\",\n        metadata={\"help\": \"Inference mode\", \"choices\": [\"batch\", \"stream\"]}\n    )\n    use_timestamps: bool = field(\n        default=False,\n        metadata={\"help\": \"Include timestamps in transcription\"}\n    )\n    use_peft: bool = field(\n        default=False,\n        metadata={\"help\": \"Use PEFT model\"}\n    )\n    language: str = field(\n        default=\"chinese\",\n        metadata={\"help\": \"Language of the audio (e.g., 'chinese', 'taiwanese')\"}\n    )\n    device: str = field(\n        default=None,\n        metadata={\"help\": \"Device to use for inference\"}\n    )\n    output_dir: str = field(\n        default=\"./output\",\n        metadata={\"help\": \"Directory to save output files\"}\n    )\n    file_name: str = field(\n        default=\"translation_result.json\",\n        metadata={\"help\": \"Directory to save output files\"}\n    )\n    \n    def __post_init__(self):\n        if self.device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        os.makedirs(self.output_dir, exist_ok=True)\n"}
{"type": "source_file", "path": "src/data/dataset_preparation.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Any\nfrom datasets import Dataset as HFDataset, Audio, Features, Value\nfrom functools import partial\nfrom src.config import DatasetAttr\nfrom src.utils import logger\nfrom src.config import DataArguments\nimport librosa\n\n\nclass DatasetPreparationStrategy(ABC):\n    @abstractmethod\n    def prepare(self) -> Any:\n        raise NotImplementedError(\"You should implement this method\")\n\n\nclass AudioDatasetPreparationStrategy(DatasetPreparationStrategy):\n    def __init__(self, dataset: HFDataset, attr: DatasetAttr, args: DataArguments):\n        self.args = args\n        self.attr = attr\n        self.dataset = dataset\n\n    def prepare(self) -> HFDataset:\n        self._rename_columns()\n        self._resample_audio()\n        self._adjust_target()\n        self._standardize_features()\n        return self.dataset\n\n    def _rename_columns(self):\n        column_mapping = {\n            self.attr.audio: \"audio\",\n            self.attr.target: \"target\",\n        }\n        self.dataset = self.dataset.rename_columns(column_mapping)\n\n    def _resample_audio(self):\n        target_sampling_rate = self.args.sampling_rate\n        if isinstance(self.dataset.features[\"audio\"], Audio):\n            current_sampling_rate = self.dataset.features[\"audio\"].sampling_rate\n            if current_sampling_rate != target_sampling_rate:\n                logger.info(\n                    f\"Resampling audio from {current_sampling_rate} Hz to {target_sampling_rate} Hz\"\n                )\n                self.dataset = self.dataset.cast_column(\n                    \"audio\", Audio(sampling_rate=target_sampling_rate)\n                )\n        else:\n            logger.info(\n                f\"Converting audio column to Audio feature with {target_sampling_rate} Hz sampling rate\"\n            )\n            self.dataset = self.dataset.cast_column(\n                \"audio\", Audio(sampling_rate=target_sampling_rate)\n            )\n\n    def _adjust_target_to_list(self, data, language):\n        if language:\n            data[\"language\"] = language\n        else:\n            raise ValueError(\"You should provide language for dataset\")\n        if isinstance(data[\"target\"], str):\n            \n            if not self.args.timestamp:\n                data[\"target\"] = [\n                    {\"start\": 0.0, \"end\": 0.0, \"text\": data[\"target\"]}\n                ]\n            else:\n                try:\n                    # TODO: 這邊有問題，不知道為什麼抓不到local的檔案\n                    data[\"target\"] = [\n                        {\n                            \"start\": 0.0,\n                            \"end\": float(\n                                round(\n                                    librosa.get_duration(\n                                        path=data[\"audio\"][\"path\"]\n                                    ),\n                                    2,\n                                )\n                            ),\n                            \"text\": data[\"target\"],\n                        }\n                    ]\n                except FileNotFoundError:\n                    data[\"target\"] = [\n                        {\n                            \"start\": 0.0,\n                            \"end\": round(data[\"duration\"], 2),\n                            \"text\": data[\"target\"],\n                        }\n                    ]\n        elif isinstance(data[\"target\"], list):\n            data[\"target\"] = [\n                {\n                    \"start\": float(target[\"start\"]),\n                    \"end\": float(target[\"end\"]),\n                    \"text\": target[\"text\"],\n                }\n                for target in data[\"target\"]\n            ]\n        else:\n            raise ValueError(\n                f\"Only support `target` type of [list, str] but get {type(data['target'])}\"\n            )\n        return data\n\n    def _adjust_target(self):\n        self.dataset = self.dataset.map(\n            partial(self._adjust_target_to_list, **{\"language\": self.attr.language}),\n            num_proc=self.args.preprocessing_num_workers,\n            desc=\"Running preprocessor on dataset\",\n            load_from_cache_file=not self.args.overwrite_cache,\n        )\n\n    def _standardize_features(self):\n        target_features = Features(\n            {\n                \"audio\": Audio(sampling_rate=self.args.sampling_rate),\n                \"target\": [\n                    {\n                        \"end\": Value(dtype=\"float64\", id=None),\n                        \"start\": Value(dtype=\"float64\", id=None),\n                        \"text\": Value(dtype=\"string\", id=None),\n                    }\n                ],\n            }\n        )\n\n        columns_to_remove = [\n            col for col in self.dataset.column_names if col not in [\"audio\", \"target\"]\n        ]\n        self.dataset = self.dataset.map(remove_columns=columns_to_remove)\n        self.dataset = self.dataset.cast(target_features)\n\n\nclass DatasetPreparationStrategyFactory:\n    @staticmethod\n    def create_strategy(\n        dataset: HFDataset, dataset_attr: DatasetAttr, args: DataArguments\n    ) -> DatasetPreparationStrategy:\n        if dataset_attr.audio == \"audio\" or dataset_attr.audio == \"audio_path\":\n            return AudioDatasetPreparationStrategy(dataset, dataset_attr, args)\n        # Add more conditions for other types of datasets\n        else:\n            raise ValueError(f\"Unsupported dataset type: {dataset_attr}\")\n\n\nclass DatasetPreparation:\n    def preprocess(\n        self, dataset: HFDataset, dataset_attr: DatasetAttr, args: DataArguments\n    ) -> HFDataset:\n        strategy = DatasetPreparationStrategyFactory.create_strategy(\n            dataset, dataset_attr, args\n        )\n        return strategy.prepare()\n"}
{"type": "source_file", "path": "scripts/gradio_interface.py", "content": "import gradio as gr\nimport numpy as np\nimport torch\nfrom src.inference.flexible_inference import ChineseTaiwaneseASRInference\nfrom scipy import signal\nimport os\nfrom datetime import datetime\nimport json\n# import time\nfrom transformers import HfArgumentParser\nfrom src.config import GradioArguments\nfrom typing import Optional, Union, Any \nfrom src.utils.logging import logger\nfrom summary import summarize_transcript, process_transcripts \n\n\nclass ASRProcessor:\n    def __init__(\n        self,\n        language: str,\n        model_choice: Optional[str] = \"OpenAI Whisper Small\",\n        use_peft: Optional[bool] = False,\n    ):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = None\n        self.language = language\n        self.initialize_model(model_choice, use_peft)\n\n    def initialize_model(self, model_choice, use_peft):\n        if model_choice == \"Custom (Finetuned)\":\n            model_path = \"./whisper-finetuned-zh-tw\"\n        elif model_choice == \"Custom (PEFT)\":\n            model_path = \"./whisper-peft-finetuned-zh-tw\"\n        else:\n            model_path = \"openai/whisper-small\"\n        \n        logger.info(f\"Right now use model : {model_choice}\")\n\n        self.model = ChineseTaiwaneseASRInference(\n            model_path, device=self.device, use_peft=use_peft, language=self.language\n        )\n\n    def reset_model(self):\n        if self.model:\n            del self.model\n            torch.cuda.empty_cache()\n        self.model = None\n\n\ndef log_to_json(\n    message: str, cache_dir: str, cache_file_name: str, channel: Optional[str] = None\n):\n    cache_dir = os.path.join(os.getcwd(), cache_dir)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    log_file = os.path.join(cache_dir, cache_file_name)\n\n    log_entry = {\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"message\": message,\n    }\n\n    if not isinstance(channel, type(None)):\n        log_entry.update(\n            {\n                \"channel\": channel,\n            }\n        )\n\n    if os.path.exists(log_file):\n        with open(log_file, \"r+\") as f:\n            try:\n                logs = json.load(f)\n            except json.JSONDecodeError:\n                logs = []\n\n            logs.append(log_entry)\n\n            f.seek(0)\n            json.dump(logs, f, ensure_ascii=False, indent=4)\n            f.truncate()\n    else:\n        with open(log_file, \"w\") as f:\n            json.dump([log_entry], f, ensure_ascii=False, indent=4)\n\n\ndef convert_audio_sampling(audio):\n    sr, y = audio\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    y = resample_audio(y, sr, 16000)\n    sr = 16000\n    if y.ndim == 2 and y.shape[1] == 2:\n        if (np.around(y[:, 0]) == np.around(y[:, 1])).all():\n            return y[:, 0], sr\n        else:\n            return y[:, 0], y[:, 1], sr\n    return y, sr\n\n\ndef resample_audio(y, orig_sr, target_sr):\n    if orig_sr != target_sr:\n        num_samples = int(len(y) * float(target_sr) / orig_sr)\n        y = signal.resample(y, num_samples)\n    return y\n\n\ndef transcribe_batch(\n    audio: Any,\n    asr_processor: object,\n    cache_dir: Union[str],\n    cache_batch_filename: str\n) -> str:\n    \"\"\"\n    Transcribe audio input using the provided ASR processor.\n\n    Args:\n        audio: Audio input (file path, bytes, or None)\n        asr_processor: ASR processor object\n        cache_dir: Directory to store cache files\n        cache_batch_filename: Name of the cache file\n\n    Returns:\n        str: Transcription result\n    \"\"\"\n    logger.info(f\"Starting transcription for {cache_batch_filename}\")\n\n    if audio is None:\n        logger.warning(\"No audio input provided.\")\n        return \"No audio input provided.\"\n    try:\n        audio_result = convert_audio_sampling(audio)\n    except Exception as e:\n        logger.error(f\"Error converting audio: {str(e)}\")\n        return f\"Error processing audio: {str(e)}\"\n\n    channels = ['left', 'right']\n    transcriptions = []\n\n    if len(audio_result) == 3:  # Stereo audio\n        for idx, channel_audio in enumerate(audio_result[:2]):\n            channel_transcription = _process_channel(\n                channel_audio, \n                audio_result[-1], \n                asr_processor, \n                cache_dir, \n                cache_batch_filename, \n                channels[idx]\n            )\n            transcriptions.append(f\"{channels[idx]}:{channel_transcription}\\n\")\n    else:  # Mono audio\n        mono_transcription = _process_channel(\n            audio_result[0], \n            audio_result[-1], \n            asr_processor, \n            cache_dir, \n            cache_batch_filename\n        )\n        transcriptions.append(mono_transcription)\n\n    final_transcription = \"\\n\".join(transcriptions)\n    logger.info(f\"Transcription completed for {cache_batch_filename}\")\n    return final_transcription\n\n\ndef _process_channel(\n    audio: Any, \n    sample_rate: int, \n    asr_processor: object, \n    cache_dir: str, \n    cache_batch_filename: str, \n    channel: str = None\n) -> str:\n    \"\"\"\n    Process a single audio channel and return its transcription.\n\n    Args:\n        audio: Audio data for the channel\n        sample_rate: Sample rate of the audio\n        asr_processor: ASR processor object\n        cache_dir: Directory to store cache files\n        cache_batch_filename: Name of the cache file\n        channel: Channel name (optional, for stereo audio)\n\n    Returns:\n        str: Transcription for the channel\n    \"\"\"\n    try:\n        transcription = asr_processor.model.transcribe_batch(audio, sample_rate)[0]\n        log_to_json(transcription, cache_dir, cache_batch_filename, channel)\n        return transcription\n    except Exception as e:\n        logger.error(f\"Error transcribing {'channel ' + channel if channel else 'audio'}: {str(e)}\")\n        return f\"Error transcribing {'channel ' + channel if channel else 'audio'}: {str(e)}\"\n\n\ndef transcribe_stream(audio, asr_processor, cache_dir, cache_streaming_filename):\n    y, sr = convert_audio_sampling(audio)\n\n    chunk_size = int(sr * 5)  # 5 second chunks\n    transcription = \"\"\n\n    for i in range(0, len(y), chunk_size):\n        chunk = y[i: i + chunk_size]\n\n        try:\n            chunk_result = next(\n                asr_processor.model.transcribe_stream([chunk], sample_rate=sr)\n            )\n\n            if isinstance(chunk_result, dict):\n                chunk_transcription = chunk_result.get(\"transcription\", \"\")\n            else:\n                chunk_transcription = chunk_result\n\n            if chunk_transcription.strip():\n                transcription += chunk_transcription + \" \"\n                log_to_json(chunk_transcription.strip(), cache_dir, cache_streaming_filename)\n                yield transcription.strip()\n            else:\n                log_to_json({\"transcription\": \"\"}, cache_dir, cache_streaming_filename)\n\n        except StopIteration:\n            logger.info(\"Transcription of chunk completed.\")\n            break\n        except Exception as e:\n            logger.error(f\"Error processing chunk: {str(e)}\")\n            yield f\"Error processing chunk: {str(e)}\"\n\n    if not transcription.strip():\n        yield \"No speech detected or transcription generated.\" \n\n\ndef create_interface(args):\n    asr_processor = ASRProcessor(args.language)\n\n    with gr.Blocks() as demo:\n        transcription_state = gr.State(\"\")\n        gr.Markdown(\"# Chinese/Taiwanese ASR Demo\")\n\n        with gr.Row():\n            with gr.Column():\n                model_choice = gr.Radio(\n                    [\"OpenAI Whisper Small\", \"Custom (Finetuned)\", \"Custom (PEFT)\"],\n                    label=\"Model Choice\",\n                    value=\"OpenAI Whisper Small\",\n                )\n                use_peft = gr.Checkbox(label=\"Use PEFT (only for custom PEFT model)\")\n                mode = gr.Radio(\n                    [\"Batch\", \"Streaming\"], label=\"Transcription Mode\", value=\"Batch\"\n                )\n\n            with gr.Column():\n                batch_audio = gr.Audio(\n                    type=\"numpy\",\n                    label=\"Batch Audio Input (Microphone or Upload)\",\n                    visible=True,\n                )\n                stream_audio = gr.Audio(\n                    sources=\"microphone\",\n                    type=\"numpy\",\n                    label=\"Streaming Audio Input (Microphone only)\",\n                    visible=False,\n                    streaming=True,\n                )\n                output_text = gr.Textbox(label=\"Transcription Output\")\n                summary_text = gr.Textbox(label=\"Summary\", visible=True)\n                transcribe_button = gr.Button(\"Transcribe\", visible=True)\n                clear_button = gr.Button(\"Clear\", visible=True)\n                summarize_button = gr.Button(\"Summarize\", visible=True)\n\n        def transcribe(audio, model_choice, use_peft):\n            return transcribe_batch(\n                audio, asr_processor, args.cache_dir, args.cache_batch_filename\n            )\n\n        def stream_transcribe(audio, model_choice, use_peft):\n            if audio is None:\n                yield \"No audio input provided.\"\n                return\n\n            try:\n                transcription_generator = transcribe_stream(\n                    audio, asr_processor, args.cache_dir, args.cache_streaming_filename\n                )\n                \n                while True:\n                    try:\n                        transcription = next(transcription_generator)\n                        yield transcription\n                    except StopIteration:\n                        logger.info(\"Transcription completed.\")\n                        break\n                    except Exception as e:\n                        logger.error(f\"Error during transcription: {str(e)}\")\n                        yield f\"An error occurred during transcription: {str(e)}\"\n                        break\n\n            except Exception as e:\n                logger.error(f\"Error in stream_transcribe: {str(e)}\")\n                yield f\"An error occurred: {str(e)}\"\n\n        def summarize_batch_audio(transcript: str, state: str) -> str:\n            summary = summarize_transcript(transcript)\n            return summary\n\n        def summarize_streaming_audio(args: Optional[Any] = args) -> str:\n            summary = process_transcripts(args, args.cache_streaming_filename)\n            return summary\n\n        def clear_output():\n            return \"\", \"\", \"\"\n\n        def update_model(model_choice, use_peft):\n            asr_processor.reset_model()\n            asr_processor.initialize_model(model_choice, use_peft)\n\n        def stop_streaming():\n            print(\"Streaming has stopped\")\n\n        transcribe_button.click(\n            fn=transcribe,\n            inputs=[batch_audio, model_choice, use_peft],\n            outputs=output_text,\n        )\n\n        stream_audio.stream(\n            fn=stream_transcribe,\n            inputs=[stream_audio, model_choice, use_peft],\n            outputs=output_text,\n        )\n        \n        stream_audio.stop_recording(\n            stop_streaming\n        ).then(\n            fn=summarize_streaming_audio,\n            inputs=[],\n            outputs=summary_text\n        )\n\n        clear_button.click(\n            fn=clear_output, \n            inputs=[], \n            outputs=[output_text, summary_text, transcription_state])\n\n        summarize_button.click(\n            fn=summarize_batch_audio,\n            inputs=[output_text, transcription_state],\n            outputs=[summary_text]\n        )\n\n        for input_component in [model_choice, use_peft]:\n            input_component.change(\n                fn=update_model, inputs=[model_choice, use_peft], outputs=None\n            )\n\n        def update_interface(mode):\n            if mode == \"Batch\":\n                return {\n                    batch_audio: gr.update(visible=True),\n                    stream_audio: gr.update(visible=False),\n                    transcribe_button: gr.update(visible=True),\n                    clear_button: gr.update(visible=True),\n                    summarize_button: gr.update(visible=True),\n                    summary_text: gr.update(visible=True)\n                }\n            else:  # Streaming\n                return {\n                    batch_audio: gr.update(visible=False),\n                    stream_audio: gr.update(visible=True),\n                    transcribe_button: gr.update(visible=False),\n                    clear_button: gr.update(visible=True),\n                    summarize_button: gr.update(visible=False),\n                    summary_text: gr.update(visible=True)\n                }\n\n        mode.change(\n            fn=update_interface,\n            inputs=[mode],\n            outputs=[batch_audio, stream_audio, transcribe_button, clear_button, summarize_button, summary_text],\n        )\n\n    return demo\n\n\ndef parse_args():\n    parser = HfArgumentParser(GradioArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    return args\n\n\ndef main():\n    args = parse_args()\n    demo = create_interface(args)\n    demo.launch(share=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/data/data_collator.py", "content": "from dataclasses import dataclass\nfrom typing import Dict, List\nimport torch\nfrom src.utils import logger\nfrom dataclasses import dataclass\nfrom typing import Any, Union\n\n\n@dataclass\nclass WhisperDataCollator:\n    processor: Any\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        model_input_name = self.processor.model_input_names[0]\n        input_features = [\n            {model_input_name: feature[model_input_name]} for feature in features\n        ]\n        batch = self.processor.feature_extractor.pad(\n            input_features, return_tensors=\"pt\"\n        )\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\n\n# @dataclass\n# class WhisperDataCollator:\n#     processor: WhisperProcessor\n\n#     def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n#         try:\n#             input_features = [feature[\"input_features\"] for feature in features]\n#             labels = [feature[\"labels\"] for feature in features]\n\n#             input_features = torch.stack(input_features)\n#             labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\n#             batch = {\n#                 \"input_features\": input_features,\n#                 \"labels\": labels,\n#             }\n\n#             return batch\n#         except Exception as e:\n#             logger.error(f\"Error in data collator: {e}\")\n#             logger.error(f\"Features causing the error: {features}\")\n#             raise\n"}
{"type": "source_file", "path": "src/inference/__init__.py", "content": "from .flexible_inference import ChineseTaiwaneseASRInference\n\n__all__ = [\n    'ChineseTaiwaneseASRInference',\n]"}
{"type": "source_file", "path": "src/config/train_config.py", "content": "from dataclasses import dataclass, field, asdict\nfrom typing import Optional, List, Union, Literal, Dict, Any\nfrom transformers import Seq2SeqTrainingArguments\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = field(\n        default=\"openai/whisper-small\",\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    language: str = field(\n        default=\"chinese\",\n        metadata={\"help\": \"Language code for the model (e.g., zh-TW for Traditional Chinese)\"}\n    )\n    use_peft: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to use PEFT for fine-tuning\"}\n    )\n    peft_method: str = field(\n        default=\"lora\",\n        metadata={\"help\": \"PEFT method to use: lora, ia3, adaption_prompt, prefix_tuning, p_tuning\"}\n    )\n    lora_r: int = field(\n        default=16,\n        metadata={\"help\": \"LoRA attention dimension\"}\n    )\n    lora_alpha: int = field(\n        default=64,\n        metadata={\"help\": \"LoRA alpha\"}\n    )\n    lora_dropout: float = field(\n        default=0.05,\n        metadata={\"help\": \"LoRA dropout\"}\n    )\n\n\n@dataclass\nclass WhisperTrainingArguments(Seq2SeqTrainingArguments):\n    output_dir: str = field(\n        default=\"./whisper-finetuned\",\n        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n    )\n    overwrite_output_dir: bool = field(\n        default=True,\n        metadata={\"help\": \"If True, overwrite the content of the output directory. Use this to continue training\\\n                   if output_dir points to a checkpoint directory.\"},\n    )\n    # auto_find_batch_size_size: bool = field(\n    #     default=True, \n    #     metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n    # )\n    per_device_train_batch_size: int = field(\n        default=16, \n        metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n    )\n    per_device_eval_batch_size: int = field(\n        default=8,\n        metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n    )\n    gradient_accumulation_steps: int = field(\n        default=4,\n        metadata={\"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"}\n    )\n    num_train_epochs: float = field(\n        default=3.0, \n        metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    warmup_steps: int = field(\n        default=100, \n        metadata={\"help\": \"Linear warmup over warmup_steps.\"}\n    )\n    warmup_ratio: float = field(\n        default=0.1, \n        metadata={\"help\": \"Linear warmup over warmup_ratio.\"}\n    )\n    lr_scheduler_type: str = field(\n        default=\"linear\", \n        metadata={\"help\": \"lr_scheduler_types.\"}\n    )\n    learning_rate: float = field(\n        default=1e-5, \n        metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    fp16: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use 16-bit (mixed) precision instead of 32-bit\"}\n    )\n    eval_strategy: str = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"}\n    )\n    save_strategy: str = field(\n        default=\"steps\",\n        metadata={\"help\": \"The saving strategy to use.\"}\n    )\n    save_steps: int = field(\n        default=50,\n        metadata={\"help\": \"Save checkpoint every X updates steps.\"}\n    )\n    eval_steps: int = field(\n        default=50,\n        metadata={\"help\": \"Run an evaluation every X steps.\"}\n    )\n    logging_steps: int = field(\n        default=50,\n        metadata={\"help\": \"Log every X updates steps.\"}\n    )\n    save_total_limit: Optional[int] = field(\n        default=-1,\n        metadata={\"help\": \"Limit the total amount of checkpoints.\"}\n    )\n    # https://github.com/huggingface/blog/issues/933\n    # metric_for_best_model: str = field(\n    #     default=\"loss\",\n    #     metadata={\"help\": \"The metric to use to compare two different models.\"}\n    # )\n    # greater_is_better: bool = field(\n    #     default=False,\n    #     metadata={\"help\": \"Whether the `metric_for_best_model` should be maximized or not.\"}\n    # )\n    load_best_model_at_end: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to load the best model found during training at the end of training.\"}\n    )\n    max_steps: int = field(\n        default=-1,\n        metadata={\"help\": \"If > 0: set total number of training steps to perform. Override num_train_epochs.\"}\n    )\n    dataloader_num_workers: int = field(\n        default=1,\n        metadata={\"help\": \"Number of subprocesses to use for data loading.\"}\n    )\n    ddp_find_unused_parameters: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"When using distributed training, the value of the flag \\\n            `find_unused_parameters` passed to `DistributedDataParallel`.\"}\n    )\n    gradient_checkpointing: bool = field(\n        default=True,\n        metadata={\"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"}\n    )\n    dataloader_pin_memory: bool = field(\n        default=True,\n        metadata={\"help\": \"If True, use dataloader_pin_memory\"}\n    )    \n    predict_with_generate: bool = field(\n        default=True,\n        metadata={\"help\": \"If True, use predict_with_generate\"}\n    )    \n    # max_grad_norm: float = field(\n    #     default=1.0, \n    #     metadata={\"help\": \"max_grad_norm.\"}\n    # )\n    eval_accumulation_steps: int = field(\n        default=1,\n        metadata={\"help\": \"Number of updates steps to accumulate before performing \\\n            a backward/update pass in evaluation.\"}\n    )\n    dataloader_num_workers: int = field(\n        default=0,\n        metadata={\"help\": \"Number of subprocesses to use for data loading (PyTorch only). \\\n            0 means that the data will be loaded in the main process.\"}\n    )\n    remove_unused_columns: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to automatically remove the columns unused by the model forward method.\"}\n    )\n    label_names: str = field(\n        default=\"labels\",\n        metadata={\"help\": \"Whether or not to automatically remove the columns unused by the model forward method.\"}\n    )\n    # predict_with_generate: bool = field(\n    #     default=False,\n    #     metadata={\"help\": \"Whether or not to automatically remove the columns unused by the model forward method.\"}\n    # )\n    return_loss: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether or not to return loss.\"}\n    )\n    \n\n@dataclass\nclass WhisperProcessorConfig:\n    # Feature Extractor Arguments\n    feature_size: int = field(\n        default=80,\n        metadata={\"help\": \"The feature dimension of the extracted features.\"}\n    )\n    padding_value: float = field(\n        default=0.0,\n        metadata={\"help\": \"The value that is used to fill the padding values.\"}\n    )\n    do_normalize: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to zero-mean and unit-variance normalize the input.\"}\n    )\n    return_attention_mask: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to return an attention mask.\"}\n    )\n    task: str = field(\n        default=\"transcribe\",\n        metadata={\"help\": \"The task token to use at the start of transcription.\"}\n    )\n    predict_timestamps: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to predict timestamps.\"}\n    )\n    return_timestamps: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to return timestamps in the decoded output.\"}\n    )\n    model_max_length: Optional[int] = field(\n        default=1024,\n        metadata={\"help\": \"The maximum length of the model inputs.\"}\n    )\n    padding: Union[bool, str] = field(\n        default=True,\n        metadata={\"help\": \"Padding strategy. Can be bool or 'max_length'.\"}\n    )\n    truncation: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to truncate sequences longer than model_max_length.\"}\n    )\n\n    # Additional Processing Arguments\n    chunk_length_s: Optional[float] = field(\n        default=30.0,\n        metadata={\"help\": \"The length of audio chunks to process in seconds.\"}\n    )\n    stride_length_s: Optional[float] = field(\n        default=None,\n        metadata={\"help\": \"The length of stride between audio chunks in seconds.\"}\n    )\n    ignore_warning: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to ignore the warning raised when the audio is too short.\"}\n    )\n    \n    # Decoding Arguments\n    skip_special_tokens: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to remove special tokens in the decoding.\"}\n    )\n    clean_up_tokenization_spaces: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to clean up the tokenization spaces.\"}\n    )\n\n    # Additional Configuration\n    forced_decoder_ids: Optional[List[List[int]]] = field(\n        default=None,\n        metadata={\"help\": \"A list of pairs of integers which indicates a mapping\\\n                           from generation indices to token indices that will be forced before sampling.\"}\n    )\n    suppress_tokens: Optional[List[int]] = field(\n        default=None,\n        metadata={\"help\": \"A list of tokens that will be suppressed at generation.\"}\n    )\n    max_new_tokens: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\"}\n    )\n\n\n@dataclass\nclass WhisperPredictionArguments:\n    do_sample: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether or not to use sampling, use greedy decoding otherwise.\"}\n    )\n    temperature: Optional[float] = field(\n        default=0.95,\n        metadata={\"help\": \"The value used to modulate the next token probabilities.\"}\n    )\n    top_p: Optional[float] = field(\n        default=0.7,\n        metadata={\"help\": \"The smallest set of most probable tokens with probabilities \\\n                  that add up to top_p or higher are kept.\"}\n    )\n    top_k: Optional[int] = field(\n        default=50,\n        metadata={\"help\": \"The number of highest probability vocabulary tokens to keep for top-k filtering.\"}\n    )\n    num_beams: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"Number of beams for beam search. 1 means no beam search.\"}\n    )\n    # max_length: Optional[int] = field(\n    #     default=None,\n    #     metadata={\"help\": \"The maximum length the generated tokens can have. It can be overridden by max_new_tokens.\"}\n    # )\n    # max_new_tokens: Optional[int] = field(\n    #     default=512,\n    #     metadata={\"help\": \"The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\"}\n    # )\n    repetition_penalty: Optional[float] = field(\n        default=1.0,\n        metadata={\"help\": \"The parameter for repetition penalty. 1.0 means no penalty.\"}\n    )\n    length_penalty: Optional[float] = field(\n        default=1.0,\n        metadata={\"help\": \"Exponential penalty to the length that is used with beam-based generation.\"}\n    )\n    metric: Optional[Literal[\"wer\", \"cer\"]] = field(\n        default=\"cer\",\n        metadata={\"help\": \"metric for hugging face evaluate module.\"}\n    )\n\n    def to_dict(self) -> Dict[str, Any]:\n        args = asdict(self)\n        # if args.get(\"max_new_tokens\", None):\n        #     args.pop(\"max_length\", None)\n        return args\n"}
{"type": "source_file", "path": "scripts/__init__.py", "content": ""}
{"type": "source_file", "path": "src/inference/flexible_inference.py", "content": "import torch\nimport numpy as np\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n# from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom peft import PeftModel, PeftConfig\nfrom src.utils.logging import logger\nfrom typing import Generator, List\nfrom collections import deque\nimport re\nimport tqdm\nimport librosa\nimport time\nfrom concurrent.futures import ThreadPoolExecutor  # , as_completed\n\n\nclass ChineseTaiwaneseASRInference:\n    def __init__(\n        self,\n        model_path: str,\n        device: str = \"cuda\",\n        use_peft: bool = False,\n        language: str = \"chinese\",\n        use_timestamps: bool = False,\n        *args,\n        **kwargs,\n    ):\n\n        self.device = device\n        self.language = language\n        self.use_timestamps = use_timestamps\n        self.executor = ThreadPoolExecutor(max_workers=4)\n        self.vad_model, utils = torch.hub.load(\n            repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True\n        )\n        self.get_speech_timestamps, _, read_audio, _, _ = utils\n\n        try:\n            if use_peft:\n                config = PeftConfig.from_pretrained(model_path)\n                self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n                    config.base_model_name_or_path\n                )\n                self.model = PeftModel.from_pretrained(self.model, model_path)\n            else:\n                self.model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path)\n\n            self.model.to(device)\n            # BUG FIX: https://medium.com/@bofenghuang7/what-i-learned-from-whisper-fine-tuning-event-2a68dab1862\n            # included in the training\n            self.model.config.forced_decoder_ids = None\n            self.model.config.suppress_tokens = []\n            # to use gradient checkpointing\n            self.model.config.use_cache = False\n\n            self.processor = AutoProcessor.from_pretrained(model_path)\n\n            # Set the language token without using forced_decoder_ids\n            self.language_token = self.processor.tokenizer.convert_tokens_to_ids(\n                f\"<|{language}|>\"\n            )\n            self.forced_decoder_ids = self.processor.get_decoder_prompt_ids(\n                language=language, task=\"transcribe\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error loading model: {e}\")\n            raise\n\n    @torch.no_grad()\n    def transcribe_batch(self, audio, sample_rate):\n        try:\n            transcriptions = []\n            if audio is None:\n                transcriptions.append(\"No valid audio input provided.\")\n            audio_chunks = self._process_audio(audio, sample_rate)\n            chunk_transcriptions = []\n            cumulative_duration = 0\n            for chunk in tqdm.tqdm(audio_chunks):\n                inputs = self.processor(\n                    chunk,\n                    return_tensors=\"pt\",\n                    truncation=False,\n                    return_attention_mask=True,\n                    sampling_rate=16000,\n                )\n\n                input_features = inputs.input_features.to(self.device)\n                if isinstance(self.language, type(None)):\n                    # Detect Language\n                    # https://discuss.huggingface.co/t/language-detection-with-whisper/26003/14\n                    language_token = self.model.generate(input_features, max_new_tokens=1)[0, 1]\n                    language_token = self.processor.tokenizer.decode(language_token)\n                    language_token = re.search(r'<\\|(.+?)\\|>', language_token).group(1)\n                else:\n                    language_token = self.language\n                \n                logger.info(f\"The Language Token is {language_token}\")\n\n                generated_ids = self.model.generate(\n                    input_features,\n                    language=language_token,  # self.language,\n                    task=\"transcribe\",\n                    return_timestamps=self.use_timestamps,\n                )\n                chunk_trans = self.processor.batch_decode(\n                    generated_ids,\n                    skip_special_tokens=True,\n                    decode_with_timestamps=self.use_timestamps,\n                )[0]\n                if self.use_timestamps:\n                    chunk_trans, cumulative_duration = self._process_timestamps(\n                        chunk_trans,\n                        cumulative_duration,\n                    )\n                chunk_transcriptions.extend(chunk_trans)\n\n            full_transcription = \"\".join(chunk_transcriptions)\n            transcriptions.append(full_transcription)\n\n            return transcriptions\n        except Exception as e:\n            logger.error(f\"Error in transcribe_batch: {e}\")\n            return [f\"Error in transcription: {str(e)}\"]\n\n    @torch.no_grad()\n    def transcribe_stream(\n        self,\n        audio_stream: Generator[np.ndarray, None, None],\n        sample_rate: int = 16000,\n        chunk_length_s: float = 30.0,\n        stride_length_s: float = 2,\n    ) -> Generator[dict, None, None]:\n        chunk_length = int(chunk_length_s * sample_rate)\n        stride_length = int(stride_length_s * sample_rate)\n        audio_buffer = deque(maxlen=chunk_length)\n        futures = []\n\n        for chunk in audio_stream:\n            audio_buffer.extend(chunk)\n\n            while len(audio_buffer) >= chunk_length:\n                # Extract the chunk\n                audio_chunk = [audio_buffer.popleft() for _ in range(chunk_length)]\n                audio_chunk = np.array(audio_chunk)\n\n                # Re-insert the last 'stride_length' samples back into the buffer for overlap\n                if stride_length > 0:\n                    overlap_samples = audio_chunk[-stride_length:]\n                    audio_buffer.extendleft(overlap_samples[::-1])\n\n                # Submit the chunk for processing\n                future = self.executor.submit(\n                    self.process_chunk, audio_chunk, sample_rate\n                )\n                futures.append(future)\n\n                # Break to allow processing and avoid high latency\n                break\n\n            # Process completed futures\n            completed_futures = [f for f in futures if f.done()]\n            for future in completed_futures:\n                result = future.result()\n                if result:\n                    yield result\n                futures.remove(future)\n\n        # Process any remaining audio in the buffer\n        if audio_buffer:\n            remaining_audio = np.array(audio_buffer)\n            result = self.process_chunk(remaining_audio, sample_rate)\n            if result:\n                yield result\n\n    def process_chunk(self, audio_chunk: np.ndarray, sample_rate: int) -> dict:\n        start_time = time.time()\n\n        # Check if the audio chunk contains speech\n        if not self.is_speech(audio_chunk, sample_rate):\n            return None\n\n        # Preprocess audio\n        audio_chunk = librosa.util.normalize(audio_chunk)\n\n        # Process audio chunk\n        input_features = self.processor(\n            audio_chunk, sampling_rate=sample_rate, return_tensors=\"pt\"\n        ).input_features\n\n        input_features = input_features.to(self.device)\n        language_token = self.model.generate(input_features, max_new_tokens=1)[0, 1]\n        language_token = self.processor.tokenizer.decode(language_token)\n        language_token = re.search(r'<\\|(.+?)\\|>', language_token).group(1)\n        \n        logger.info(f\"The Language Token is {language_token}\")\n\n        generated_ids = self.model.generate(\n            input_features,\n            forced_decoder_ids=self.forced_decoder_ids,\n            language=language_token,  # self.language,\n            return_timestamps=self.use_timestamps,\n            task=\"transcribe\",\n            num_beams=5,            # Use beam search with 5 beams\n            early_stopping=True,  \n        )\n\n        # if isinstance(generated_ids, torch.Tensor):\n        #     if generated_ids.dim() == 2:\n        #         generated_ids = generated_ids.squeeze(0)\n        #     elif generated_ids.dim() > 2:\n        #         generated_ids = generated_ids.view(-1)\n        # elif isinstance(generated_ids, list):\n        #     generated_ids = torch.tensor(generated_ids).view(-1)\n\n        # if isinstance(generated_ids, torch.Tensor):\n        #     generated_ids = (\n        #         generated_ids.squeeze(0)\n        #         if generated_ids.dim() == 2\n        #         else generated_ids.view(-1)\n        #     )\n        # elif isinstance(generated_ids, list):\n        #     generated_ids = torch.tensor(generated_ids).view(-1)\n\n        # if self.use_timestamps:  # self.processor.batch_decode(generated_ids,\n        #     transcription = self.processor.decode(generated_ids,\n        #                                           skip_special_tokens=True,\n        #                                           decode_with_timestamps=self.use_timestamps)\n        #     transcription = self._process_timestamps(transcription)\n        # else:\n        #     transcription = self.processor.decode(generated_ids,\n        #                                           skip_special_tokens=True,\n        #                                           decode_with_timestamps=self.use_timestamps)\n        transcription = self.processor.batch_decode(\n            generated_ids,\n            skip_special_tokens=True,\n            decode_with_timestamps=self.use_timestamps,\n        )[0]\n        if self.use_timestamps:\n            transcription = self._process_timestamps(transcription)[0]\n\n        end_time = time.time()\n        processing_time = end_time - start_time\n        speed = (\n            len(audio_chunk) / sample_rate / processing_time\n            if processing_time > 0\n            else 0\n        )\n\n        return {\"transcription\": transcription.strip(), \"speed\": speed}\n\n    def is_speech(self, audio_chunk: np.ndarray, sample_rate: int) -> bool:\n        audio_tensor = torch.FloatTensor(audio_chunk)\n        # Adjusted VAD parameters for better accuracy\n        speech_timestamps = self.get_speech_timestamps(\n            audio_tensor,\n            self.vad_model,\n            sampling_rate=sample_rate,\n            threshold=0.5,  # Adjusted threshold\n            min_speech_duration_ms=250,  # Minimum speech duration\n            min_silence_duration_ms=100,  # Minimum silence duration\n        )\n        return len(speech_timestamps) > 0\n\n    def _process_audio(\n        self,\n        audio,\n        sample_rate: int = 16000,\n        target_sample_rate: int = 16000,\n        chunk_length: int = 420000,\n    ) -> List[np.ndarray]:\n        \"\"\"Process and pad or trim the audio array to chunk_length.\"\"\"\n        \"\"\"Split audio into chunks of 30 seconds (480000 samples at 16kHz).\"\"\"\n        if isinstance(audio, np.ndarray):\n            audio_array = audio\n        elif isinstance(audio, tuple):\n            audio_array = audio[1] if len(audio) > 1 else audio[0]\n        else:\n            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n\n        # Resample to 16kHz if necessary\n        if sample_rate != 16000:\n            audio = librosa.resample(\n                audio, orig_sr=sample_rate, target_sr=target_sample_rate\n            )\n\n        # Normalize audio\n        audio_array = audio / np.max(np.abs(audio))\n\n        return [\n            audio_array[i: i + chunk_length]\n            for i in range(0, audio_array.shape[0], chunk_length)\n        ]\n\n    def _process_timestamps(self, transcription: str, offset: float = 0) -> str:\n        \"\"\"Process transcription with timestamps.\"\"\"\n        # Regular expression to match timestamp tokens\n        pattern = r\"<\\|(\\d+\\.\\d+)\\|>\"\n\n        # Split the transcription into segments based on timestamp tokens\n        segments = re.split(pattern, transcription)\n        segments = list(filter(None, segments))\n\n        # Process segments and timestamps\n        formatted_transcription = []\n\n        for i in range(0, len(segments) - 2, 3):\n            timestamp_start = float(segments[i]) + offset\n            text = segments[i + 1].strip()\n            try:\n                timestamp_end = float(segments[i + 2]) + offset\n            except ValueError:\n                timestamp_end = offset + 30\n\n            if text:  # Only add non-empty segments\n                text = self.remove_duplicates(text)\n                formatted_transcription.extend(\n                    [f\"[{timestamp_start:.2f}]-[{timestamp_end:.2f}]{text}\"]\n                )\n            offset = timestamp_end\n        return formatted_transcription, offset\n\n    def remove_duplicates(self, input_str):\n        # Split the input string into individual phrases\n        phrases = input_str.split(\",\")\n\n        # Remove duplicates while maintaining order\n        unique_phrases = list(dict.fromkeys(phrases))\n\n        # Join the unique phrases back into a single string\n        return \",\".join(unique_phrases)\n"}
{"type": "source_file", "path": "src/crawler/audio_process.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import List, Dict\nimport librosa\nimport json\nfrom src.utils.logging import logger\nimport os\nfrom .audio_saver import JsonAppendSaver, SoundfileSaver\n\n\nclass AudioProcessStrategy(ABC):\n    @abstractmethod\n    def process(\n        self,\n        audio_file: Path,\n        subtitle_file: Path,\n        json_path: str,\n        max_duration: float,\n    ) -> List[Dict]:\n        raise NotImplementedError(\"This method must be implemented.\")\n\n\nclass LibrosaAudioProcessStrategy(AudioProcessStrategy):\n    def __init__(self, output_dir: Path, file_prefix: str):\n        self.output_dir = output_dir\n        self.file_prefix = file_prefix\n        self.json_saver = JsonAppendSaver()\n        self.soundfile_saver = SoundfileSaver()\n\n    def process(\n        self, audio_file: Path, subtitle_file: Path, json_path: str, max_duration: float\n    ) -> List[Dict]:\n        if not Path(subtitle_file).exists():\n            logger.warning(\n                f\"Subtitle file not found for {audio_file}. Skipping this audio file.\"\n            )\n            return\n\n        audio, sr = librosa.load(audio_file, sr=None, mono=True)\n\n        split_audio_dir = os.path.join(self.output_dir, self.file_prefix, \"split_audio\")\n        os.makedirs(split_audio_dir, exist_ok=True)\n        with open(subtitle_file, \"r\", encoding=\"utf-8\") as f:\n            subtitles = json.load(f)\n\n        segments = []\n        current_segment = []\n        current_start = 0\n\n        for subtitle in subtitles:\n            if not subtitle[\"text\"].strip():\n                continue\n\n            start_time = float(subtitle[\"start\"])\n            duration = float(subtitle[\"duration\"])\n            end_time = start_time + duration\n\n            if end_time - current_start >= max_duration:\n                if current_segment:\n                    self._process_segment(\n                        current_segment[:-1],\n                        audio,\n                        sr,\n                        audio_file,\n                        split_audio_dir,\n                        segments,\n                    )\n                current_segment = [subtitle]\n                current_start = start_time\n            else:\n                current_segment.append(subtitle)\n\n        if current_segment:\n            self._process_segment(\n                current_segment, audio, sr, audio_file, split_audio_dir, segments\n            )\n        if segments:\n            self.json_saver.save(segments, json_path)\n        else:\n            logger.warning(\n                f\"No valid segments found for {audio_file}. Skipping this audio file.\"\n            )\n\n    def _process_segment(\n        self, segment, audio, sr, audio_file, split_audio_dir, segments\n    ):\n        if not segment:\n            logger.warning(\n                f\"Empty segment encountered for {audio_file}. Skipping this segment.\"\n            )\n            return\n\n        start_time = float(segment[0][\"start\"])\n        end_time = float(segment[-1][\"start\"]) + float(segment[-1][\"duration\"])\n        duration = end_time - start_time\n\n        start_sample = int(start_time * sr)\n        end_sample = int(end_time * sr)\n        split_audio = audio[start_sample:end_sample]\n\n        segment_filename = f\"{audio_file.stem}_{len(segments):04d}.wav\"\n        segment_path = os.path.join(split_audio_dir, segment_filename)\n        os.makedirs(os.path.dirname(segment_path), exist_ok=True)\n        self.soundfile_saver.save(split_audio, sr, segment_path)\n        # sf.write(segment_path, split_audio, sr)\n\n        timestamp = [\n            {\n                \"start\": float(sub[\"start\"]),\n                \"end\": float(sub[\"start\"] + sub[\"duration\"]),\n                \"text\": sub[\"text\"],\n            }\n            for sub in segment\n        ]\n\n        segments.append(\n            {\n                \"audio_path\": str(segment_path),\n                \"start\": start_time,\n                \"end\": end_time,\n                \"duration\": duration,\n                \"text\": \" \".join([s[\"text\"].strip() for s in segment]),\n                \"timestamp\": timestamp,\n            }\n        )\n\n\nclass AudioProcessStrategyFactory:\n    @staticmethod\n    def create_process_strategy(\n        output_dir: str, file_prefix: str, strategy_type: str = \"librosa\"\n    ) -> AudioProcessStrategy:\n        if strategy_type == \"librosa\":\n            return LibrosaAudioProcessStrategy(output_dir, file_prefix)\n        else:\n            raise ValueError(f\"Unknown strategy type: {strategy_type}\")\n\n\nclass AudioProcessor:\n    def __init__(self, strategy: AudioProcessStrategy):\n        self.strategy = strategy\n\n    def process_audio(\n        self,\n        audio_file: Path,\n        subtitle_file: Path,\n        json_path: str,\n        max_duration: float,\n    ) -> List[Dict]:\n        return self.strategy.process(audio_file, subtitle_file, json_path, max_duration)"}
{"type": "source_file", "path": "src/crawler/subtitle_download.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nimport json\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom src.utils import logger\nfrom typing import List, Dict\nimport openai\nfrom pydub import AudioSegment\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass SubtitleDownloadStrategy(ABC):\n    @abstractmethod\n    def download(self, video_id: str, subtitle_file: Path):\n        return NotImplementedError(\n            \"SubtitleDownloadStrategy must implement download method\"\n        )\n\n\nclass YouTubeTranscriptDownloadStrategy(SubtitleDownloadStrategy):\n    def download(\n        self,\n        video_id: str,\n        subtitle_file: Path,\n        languages: List[str] = [\"zh-TW\", \"zh-CN\", \"zh\"],\n    ):\n        \n        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)\n        with open(str(subtitle_file), \"w\", encoding=\"utf-8\") as f:\n            json.dump(transcript, f, ensure_ascii=False, indent=2)\n\n\nMAX_FILE_SIZE = 25 * 1024 * 1024  # 25 MB in bytes\nINITIAL_CHUNK_DURATION = 10 * 60 * 1000  # 10 minutes in milliseconds\n\n\nclass OpenAISubtitleDownloadStrategy(SubtitleDownloadStrategy):\n    def __init__(self):\n        logger.info(\"Initializing OpenAISubtitleDownloadStrategy, loading OpenAI client\")\n        self.client = openai.OpenAI()\n\n    def download(self, video_info: Dict[str, str], audio_file: str, subtitle_file: Path):\n        try:\n            result = self._transcribe_audio(video_info, audio_file)\n            with open(str(subtitle_file), \"w\", encoding=\"utf-8\") as f:\n                json.dump(result[\"transcript\"], f, ensure_ascii=False, indent=2)\n            return subtitle_file\n        except Exception as e:\n            logger.error(f\"Error transcribing audio: {e}\")\n            return None\n\n    def _transcribe_audio(self, video_info: Dict[str, str], audio_file: str):\n        try:\n            output_dir = '/'.join(audio_file.split(\"/\")[:-1])\n            audio = AudioSegment.from_wav(audio_file)\n            transcripts = []\n            start = 0\n            \n            while start < len(audio):\n                end = min(start + INITIAL_CHUNK_DURATION, len(audio))\n\n                while True:\n                    chunk = audio[start:end]\n                    chunk_file = os.path.join(output_dir, \"temp_chunk.mp3\")\n                    chunk.export(chunk_file, format=\"mp3\", bitrate=\"64k\")\n\n                    if os.path.getsize(chunk_file) <= MAX_FILE_SIZE:\n                        break\n\n                    end = start + (end - start) // 2\n                    os.remove(chunk_file)\n\n                    if end - start < 1000:  # Minimum 1 second chunk\n                        raise ValueError(\"Unable to create a small enough chunk\")\n\n                with open(chunk_file, \"rb\") as audio_chunk:\n                    response = self.client.audio.transcriptions.create(\n                        model=\"whisper-1\",\n                        file=audio_chunk,\n                        response_format=\"verbose_json\",\n                    )\n\n                transcripts.extend(response.segments)\n                os.remove(chunk_file)\n\n                start = end\n\n            result = {\n                \"video_id\": video_info[\"video_id\"],\n                \"video_title\": video_info[\"video_title\"],\n                \"channel_title\": video_info[\"channel_title\"],\n                \"transcript\": [\n                    {\n                        \"text\": segment[\"text\"],\n                        \"start\": segment[\"start\"],\n                        \"duration\": segment[\"end\"] - segment[\"start\"],\n                    }\n                    for segment in transcripts\n                ],\n            }\n            return result\n\n        except Exception as e:\n            logger.error(f\"Error transcribing audio: {e}\")\n            return {\"video_id\": video_info[\"video_id\"], \"error\": str(e)}\n\n\nclass FallbackSubtitleDownloadStrategy(SubtitleDownloadStrategy):\n    def __init__(self, primary_strategy: SubtitleDownloadStrategy, secondary_strategy: SubtitleDownloadStrategy):\n        self.primary_strategy = primary_strategy\n        self.secondary_strategy = secondary_strategy\n\n    def download(self, video_id: str, subtitle_file: Path, audio_file: Path):\n        try:\n            return self.primary_strategy.download(video_id, subtitle_file)\n        except Exception as e:\n            logger.error(f\"Failed to download subtitles with {self.primary_strategy.__class__.__name__}. Error: {e}\")\n            logger.info(f\"Fallback to {self.secondary_strategy.__class__.__name__}\")\n            \n            if isinstance(self.secondary_strategy, OpenAISubtitleDownloadStrategy):\n                video_info = {\n                    \"video_id\": video_id,\n                    \"video_title\": f\"video_{video_id}\",\n                    \"channel_title\": \"unknown_channel\",\n                }\n                \n                return self.secondary_strategy.download(video_info, str(audio_file), str(subtitle_file))\n\n\nclass SubtitleDownloadStrategyFactory:\n    @staticmethod\n    def create_subtitle_download_strategy() -> SubtitleDownloadStrategy:\n        return FallbackSubtitleDownloadStrategy(\n            YouTubeTranscriptDownloadStrategy(), OpenAISubtitleDownloadStrategy()\n        )"}
{"type": "source_file", "path": "src/__init__.py", "content": "from src.config import train_config\nfrom src.data import dataset, data_collator\nfrom src.model import whisper_model\nfrom src.trainers import whisper_trainer\nfrom src.inference import flexible_inference\nfrom src.utils import logger, mlflow_logging, setup_mlflow\n\n__all__ = [\n    'train_config',\n    'dataset',\n    'data_collator',\n    'whisper_model',\n    'whisper_trainer',\n    'flexible_inference',\n    'logger',\n    'mlflow_logging',\n    'setup_mlflow'\n]"}
{"type": "source_file", "path": "src/config/crawler_config.py", "content": "from dataclasses import dataclass, field\nfrom typing import Optional, List\n\n\n@dataclass\nclass CrawlerArgs:\n    # List of YouTube playlist URLs to crawl\n    playlist_urls: Optional[List[str]] = field(\n        default_factory=lambda: [],\n        metadata={\"help\": \"YouTube playlist URLs to crawl\"}\n    )\n    \n    audio_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"YouTube audio dir which are already be crawled\"}\n    )\n\n    # Directory to save audio files and dataset\n    output_dir: str = field(\n        default=\"./output\",\n        metadata={\"help\": \"Directory to save audio files and dataset\"}\n    )\n\n    # Name of the output dataset file\n    dataset_name: str = field(\n        default=\"youtube_dataset\",\n        metadata={\"help\": \"Name of the output dataset file\"}\n    )\n\n    # Path to FFmpeg executable (optional)\n    ffmpeg_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to FFmpeg executable\"}\n    )\n\n    # Prefix for audio and subtitle files\n    file_prefix: str = field(\n        default=\"youtube\",\n        metadata={\"help\": \"Prefix for audio and subtitle files\"}\n    )\n    # Prefix for audio and subtitle files\n    batch_size: int = field(\n        default=20,\n        metadata={\"help\": \"Prefix for audio and subtitle files\"}\n    )\n    max_duration: float = field(\n        default=30.0,\n        metadata={\"help\": \"Maximum duration of audio to download\"}\n    )"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\n# Fetch ReadMe\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\n# Use requirements.txt to set the install_requires\nwith open(\"requirements.txt\", encoding=\"utf-8\") as f:\n    install_requires = [line.strip() for line in f]\n\n\nsetup(\n    name=\"ChineseTaiwaneseWhisper\",  # noqa: F821\n    author=\"Sandy Chen\",\n    author_email=\"sandy1990418@gmail.com\",\n    description=\"A Chinese/Taiwanese ASR system using Whisper\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/sandy1990418/ChineseTaiwaneseWhisper/tree/master\",\n    packages=find_packages(),\n    include_package_data=True,\n    python_requires=\">=3.8\",\n    install_requires=install_requires,\n    classifiers=[\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Development Status :: 3 - Alpha\",\n    ],\n)\n"}
{"type": "source_file", "path": "src/config/__init__.py", "content": "from .inference_config import InferenceArguments\nfrom .gradio_config import GradioArguments\nfrom .crawler_config import CrawlerArgs\nfrom .train_config import ModelArguments, WhisperTrainingArguments, WhisperProcessorConfig, WhisperPredictionArguments\nfrom .data_config import DataArguments, DatasetAttr\n\n__all__ = [\n    \"InferenceArguments\",\n    \"GradioArguments\",\n    \"CrawlerArgs\",\n    \"ModelArguments\",\n    \"DataArguments\",\n    \"DatasetAttr\",\n    \"WhisperTrainingArguments\",\n    \"WhisperProcessorConfig\",\n    \"WhisperPredictionArguments\"\n]"}
{"type": "source_file", "path": "src/crawler/audio_download.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Optional\n\nimport yt_dlp\nimport requests\nimport os\nimport subprocess\nfrom src.utils.logging import logger\n\n\nclass AudioConverter:\n    @staticmethod\n    def convert(audio_file: Path, audio_type: str = \"mp3\"):\n        \"\"\"\n        Convert audio file to wav.\n\n        Args:\n            audio_file (Path): Path to the audio file.\n            audio_type (str): Audio type.\n        \"\"\"\n        wav_file = audio_file.with_suffix(\".wav\")\n        try:\n            subprocess.run(\n                [\n                    \"ffmpeg\",\n                    \"-y\",\n                    \"-i\",\n                    str(audio_file),\n                    \"-acodec\",\n                    \"pcm_s16le\",\n                    \"-ar\",\n                    \"16000\",\n                    str(wav_file),\n                ],\n                check=True,\n            )\n            audio_file = wav_file\n            os.remove(\n                str(audio_file.with_suffix(f\".{audio_type}\"))\n            )  # Remove the original mp3 file\n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Error converting mp3 to wav: {e}\")\n            return None\n\n        return wav_file\n\n\n#  abstract Download Strategy class\nclass AudioDownloadStrategy(ABC):\n    def __init__(self, audio_converter: AudioConverter):\n        self.audio_converter = audio_converter\n\n    @abstractmethod\n    def download(self, video_id: str, audio_file: Path):\n        return NotImplementedError(\"Download method must be implemented\")\n\n    def download_and_convert(self, video_id: str, audio_file: Path, audio_type: str = \"mp3\"):\n        audio_file = self.download(video_id, audio_file)\n        audio_file = self.audio_converter.convert(audio_file, audio_type)\n        return audio_file\n\n\nclass YTDLPDownloadStrategy(AudioDownloadStrategy):\n    def __init__(self, audio_converter: AudioConverter, ffmpeg_path: Optional[str] = None):\n        super().__init__(audio_converter)\n        self.ffmpeg_path = ffmpeg_path\n\n    def download(self, video_id: str, audio_file: Path):\n        \"\"\"\n        Download audio with yt-dlp. It may be prevented by youtube. If it is, try to download with cobalt.\n\n        Args:\n            video_id (str): YouTube video ID.\n            audio_file (Path): Path to save the audio file.\n\n        Returns:\n            Path: Path to the downloaded audio file.\n        \"\"\"\n        ydl_opts = {\n            \"format\": \"bestaudio/best\",\n            \"postprocessors\": [\n                {\n                    \"key\": \"FFmpegExtractAudio\",\n                    \"preferredcodec\": \"wav\",\n                    \"preferredquality\": \"192\",\n                }\n            ],\n            \"outtmpl\": audio_file,\n        }\n        if self.ffmpeg_path:\n            ydl_opts[\"ffmpeg_location\"] = self.ffmpeg_path\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            ydl.download([f\"https://www.youtube.com/watch?v={video_id}\"])\n        logger.info(f\"Download successful! {audio_file}\")\n        return audio_file\n\n\nclass CobaltDownloadStrategy(AudioDownloadStrategy):\n    def __init__(self, audio_converter: AudioConverter):\n        super().__init__(audio_converter)\n\n    def download(self, video_id: str, audio_file: Path):\n        \"\"\"\n        Download audio with cobalt.\n        Thanks to Cobalt! Your work is truly great.\n        https://github.com/imputnet/cobalt\n\n        Args:\n            video_id (str): YouTube video ID.\n            audio_file (Path): Path to save the audio file.\n\n        Returns:\n            Path: Path to the downloaded audio file.\n        \"\"\"\n        logger.info(\"Initiating download using Cobalt API.\")\n\n        url = \"https://olly.imput.net/api/json\"\n        params = {\n            \"url\": f\"https://www.youtube.com/watch?v={video_id}\",\n            \"isAudioOnly\": True,\n        }\n        headers = {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        response = requests.post(url, json=params, headers=headers)\n        if response.status_code == 200:\n            result = response.json()\n            download_url = result[\"url\"]\n            # Step 2: Download the audio content from the stream\n            logger.info(\"Start to stream download using Cobalt API.\")\n            with requests.get(download_url, stream=True) as stream_response:\n                stream_response.raise_for_status()\n                with open(audio_file, \"wb\") as file:\n                    for chunk in stream_response.iter_content(chunk_size=8192):\n                        file.write(chunk)\n\n            logger.info(\"Download successful!\")\n            return audio_file\n        else:\n            raise Exception(\n                f\"Failed to download audio with Cobalt API. Status code: {response.status_code}\"\n            )\n\n\nclass FallbackDownloadStrategy(AudioDownloadStrategy):\n    \"\"\"\n    Download strategy that tries to download with the primary strategy first.\n    If it fails, it tries to download with the secondary strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        primary_strategy: AudioDownloadStrategy,\n        secondary_strategy: AudioDownloadStrategy,\n    ):\n        super().__init__(primary_strategy.audio_converter)\n        self.primary_strategy = primary_strategy\n        self.secondary_strategy = secondary_strategy\n\n    def download(self, video_id: str, audio_file: Path) -> Path:\n        try:\n            return self.primary_strategy.download(video_id, audio_file)\n        except Exception as e:\n            logger.error(\n                f\"Failed to download audio with {self.primary_strategy.__class__.__name__}. Error: {e}\"\n            )\n            return self.secondary_strategy.download(video_id, audio_file)\n\n\nclass AudioDownloadStrategyFactory:\n    \"\"\"\n    Factory to create a download strategy.\n    \"\"\"\n\n    @staticmethod\n    def create_download_strategy(ffmpeg_path: Optional[str] = None):\n        audio_converter = AudioConverter()\n        return FallbackDownloadStrategy(\n            primary_strategy=YTDLPDownloadStrategy(audio_converter, ffmpeg_path),\n            secondary_strategy=CobaltDownloadStrategy(audio_converter),\n        )"}
{"type": "source_file", "path": "src/data/data_preprocess.py", "content": "from abc import abstractmethod, ABC\nfrom datasets import Dataset as HFDataset\nfrom src.config import DataArguments\nfrom src.utils import logger\nfrom transformers import WhisperProcessor\nimport librosa\nimport torch\nfrom typing import List, Dict, Any\nimport re\nimport opencc\nconverter = opencc.OpenCC('s2t.json')\n\n\ndef remove_punctuation(text: str or List[str]):\n    punctuation = \"!,.;:?、！，。；：？\"\n    if isinstance(text, str):\n        text = re.sub(r\"[{}]+\".format(punctuation), \"\", text).strip()\n        text = converter.convert(text)  \n        return text\n    elif isinstance(text, list):\n        result_text = []\n        for t in text:\n            t = re.sub(r\"[{}]+\".format(punctuation), \"\", t).strip()\n            t = converter.convert(t)\n            result_text.append(t)\n        return result_text\n    else:\n        raise Exception(f\"Not support this type {type(text)}\")\n\n\nclass PreporcessorStrategy(ABC):\n    @abstractmethod\n    def process(self):\n        raise NotImplementedError(\n            \"This method `PreporcessorStrategy` should be implemented.\"\n        )\n\n\nclass AudioPreprocessor(PreporcessorStrategy):\n    def process(\n        self,\n        dataset: HFDataset,\n        args: DataArguments,\n        processor: WhisperProcessor,\n        language: str,\n        **kwargs,\n    ) -> HFDataset:\n        try:\n            audio = dataset[\"audio\"]\n            if isinstance(audio, dict):\n                audio_array = audio[\"array\"]\n                sampling_rate = audio[\"sampling_rate\"]\n            else:\n                raise ValueError(f\"Unexpected audio type: {type(audio)}\")\n\n            if sampling_rate != args.sampling_rate:\n                audio_array = librosa.resample(\n                    audio_array, orig_sr=sampling_rate, target_sr=args.sampling_rate\n                )\n\n            processor.tokenizer.set_prefix_tokens(\n                language=language,\n                task=\"transcribe\",\n                predict_timestamps=args.timestamp,\n            )\n\n            dataset[\"input_features\"] = processor.feature_extractor(\n                audio_array,\n                sampling_rate=args.sampling_rate,\n                return_tensors=\"pt\",\n            ).input_features[0]\n\n            dataset[\"input_length\"] = audio_array.shape[0] / args.sampling_rate\n            return dataset\n\n        except Exception as e:\n            logger.error(f\"Error preprocessing example: {str(e)}\")\n            raise ValueError(\"`AudioPreprocessor` error raise\")\n\n\nclass TextPreprocessor(PreporcessorStrategy):\n    def process(\n        self,\n        dataset: HFDataset,\n        args: DataArguments,\n        processor: WhisperProcessor,\n        **kwargs,\n    ) -> HFDataset:\n        try:\n            target_text = dataset[\"target\"]\n            # if not args.timestamp and isinstance(target_text, list):\n            #     target_text = \" \".join(target_text)\n\n            if args.timestamp:\n                processor.tokenizer.predict_timestamps = True\n                target_text = self._process_timestamp(target_text, dataset, args)\n            else:\n                processor.tokenizer.predict_timestamps = False\n                target_text = self._process_notimestamp(target_text, dataset, args)\n\n            if args.do_lower_case:\n                target_text = target_text.lower()\n            # Tokenize the processed text\n            # self.processor.tokenizer.predict_timestamps = self.args.timestamp\n\n            dataset[\"labels\"] = processor.tokenizer(\n                target_text, return_tensors=\"pt\", add_special_tokens=True\n            ).input_ids[\n                0\n            ]  # decode_with_timestamps Remove batch dimension\n            dataset[\"label_length\"] = len(\n                processor.tokenizer(target_text, add_special_tokens=True).input_ids\n            )\n        except Exception as e:\n            logger.error(f\"Error preprocessing example: {str(e)}\")\n            raise ValueError(\"`TextPreprocessor` error raise\")\n\n        return dataset\n\n    def _process_timestamp(self, target_text, dataset, args):\n        if isinstance(target_text, list):\n            processed_text = \"\"\n            for segment in target_text:\n                # Offset\n                start_time = (\n                    segment[\"start\"]\n                    if round(segment[\"start\"] * 100) % 2 == 0\n                    else segment[\"start\"] + 0.01  \n                )\n                end_time = (\n                    segment[\"end\"]\n                    if round(segment[\"end\"] * 100) % 2 == 0\n                    else segment[\"end\"] - 0.01\n                )\n                text = segment[\"text\"]\n                text = text  # remove_punctuation(text)\n                processed_text += f\"<|{start_time:.2f}|>\"\n                processed_text += f\"{text}\"\n                processed_text += f\"<|{end_time:.2f}|>\"\n                # processed_text += f\"<|{start_time:.2f}|>{text}<|{end_time:.2f}|>\"\n            target_text = processed_text\n        else:\n            audio = dataset[\"audio\"]\n            audio_array = audio[\"array\"]\n            audio_length = len(audio_array) / args.sampling_rate\n            audio_length = (\n                audio_length\n                if round(audio_length * 100) % 2 == 0\n                else audio_length - 0.01\n            )\n            target_text = target_text  # remove_punctuation(target_text)\n            # target_text = f\"<|0.00|>{target_text}<|{audio_length:.2f}|>\"\n            target_text += \"<|0.00|>\"\n            target_text += f\"{text}\"\n            target_text += f\"<|{audio_length:.2f}|>\"\n\n        return target_text\n\n    def _process_notimestamp(self, target_text, dataset, args):\n        if isinstance(target_text, list):\n            processed_text = \"\"\n            for segment in target_text:\n                text = segment[\"text\"]\n                text = text  # remove_punctuation(text)\n                processed_text += f\"{text}\"\n            target_text = processed_text\n        else:\n            target_text = target_text  # remove_punctuation(target_text)\n            target_text = f\"{target_text}\"\n\n        return target_text\n\n\nclass ValidationStep(PreporcessorStrategy):\n    def process(self, dataset: Dict[str, Any], **kwargs) -> HFDataset:\n        if (\n            torch.isnan(dataset[\"input_features\"]).any()\n            or torch.isinf(dataset[\"input_features\"]).any()\n        ):\n            raise ValueError(\"NaN or infinity values detected in input_features\")\n\n        if torch.isnan(dataset[\"labels\"]).any() or torch.isinf(dataset[\"labels\"]).any():\n            raise ValueError(\"NaN or infinity values detected in labels\")\n\n        return dataset\n\n\nclass PipelineStrategy(ABC):\n    def __init__(self, args, processor, language):\n        self.args = args\n        self.processor = processor\n        self.language = language\n        self.steps: List[PreporcessorStrategy] = []\n\n    @abstractmethod\n    def create_pipeline(self) -> None:\n        pass\n\n    def add_step(self, step: PreporcessorStrategy) -> None:\n        self.steps.append(step)\n\n    @abstractmethod\n    def process(self, dataset: Any) -> Any:\n        pass\n\n\nclass PreprocessingPipeline(PipelineStrategy):\n    def __init__(self, args: DataArguments, processor: WhisperProcessor, language: str):\n        self.args = args\n        self.processor = processor\n        self.language = language\n\n    def create_pipeline(self) -> List[PreporcessorStrategy]:\n        self.steps = [AudioPreprocessor(), TextPreprocessor(), ValidationStep()]\n\n    def process_single(self, dataset: HFDataset) -> HFDataset:\n        for step in self.steps:\n            param = {\n                \"dataset\": dataset,\n                \"args\": self.args,\n                \"processor\": self.processor,\n                \"language\": self.language,\n            }\n            dataset = step.process(**param)\n        return dataset\n\n    def process(self, dataset: HFDataset) -> Dict:\n        self.create_pipeline()\n        dataset = self.process_single(dataset)\n        return {\n            \"input_features\": dataset[\"input_features\"],\n            \"labels\": dataset[\"labels\"],\n            \"input_length\": dataset[\"input_length\"],\n            \"label_length\": dataset[\"label_length\"],\n        }\n"}
{"type": "source_file", "path": "src/data/dataset.py", "content": "from datasets import (\n    Dataset as HFDataset,\n    concatenate_datasets,\n    interleave_datasets,\n)\nfrom transformers import WhisperProcessor\nfrom src.config import DataArguments\nfrom typing import List\nimport re\nfrom src.data import DatasetLoaderFactory, PreprocessingPipeline\nfrom src.utils.logging import logger\n\nMAX_DURATION_IN_SECONDS = 30.0\nmax_input_length = MAX_DURATION_IN_SECONDS * 16000\n\n\ndef is_audio_in_length_range(length):\n    return 0 < length < max_input_length\n\n\ndef filter_labels(labels_length):\n    \"\"\"Filter label sequences longer than max length (448)\"\"\"\n    return labels_length < 448\n\n\nclass ChineseTaiwaneseDataset:\n    def __init__(\n        self,\n        args: DataArguments,\n        processor: WhisperProcessor,\n        split: str = \"train\",\n        dataset_loader: DatasetLoaderFactory = DatasetLoaderFactory,\n        preprocessing_pipeline: PreprocessingPipeline = PreprocessingPipeline,\n        **kwargs,\n    ):\n        self.args = args\n        self.processor = processor\n        self.split = split\n        if self.split == \"train\":\n            self.args.init_for_training()\n        else:\n            self.args.dataset = self.args.test_dataset_name\n            self.args.init_for_training()\n        self.dataset_loader = dataset_loader\n        self.preprocessing_pipeline = preprocessing_pipeline\n        self.dataset = self._load_and_prepare_dataset()\n        self.kwargs = kwargs\n\n    def _load_and_prepare_dataset(self) -> HFDataset:\n        datasets = []\n\n        for dataset_attr in self.args.dataset_list:\n            logger.info(f\"Current Dataset: {dataset_attr}\")\n            strategy = self.dataset_loader(dataset_attr, self.args)\n            dataset = strategy.load_dataset()\n            datasets.append(dataset)\n\n        combined_dataset = self._concate_dataset(datasets)\n        column_names = list(next(iter(combined_dataset)).keys())\n        self.language = dataset_attr.language\n\n        pipeline = self.preprocessing_pipeline(self.args, self.processor, self.language)\n        combined_dataset = combined_dataset.map(\n            pipeline.process,\n            remove_columns=column_names,\n            num_proc=self.args.preprocessing_num_workers,\n            desc=\"Running preprocessor on dataset\",\n            batched=False,\n            load_from_cache_file=not self.args.overwrite_cache,\n        )\n        combined_dataset = self._apply_filters(combined_dataset)\n        return combined_dataset\n\n    def _apply_filters(self, dataset: HFDataset):\n        dataset = dataset.filter(\n            is_audio_in_length_range, input_columns=[\"input_length\"]\n        )\n        dataset = dataset.filter(filter_labels, input_columns=[\"label_length\"])\n\n        return dataset\n\n    def _concate_dataset(self, datasets):\n        if not datasets:\n            raise ValueError(\"No datasets were successfully loaded.\")\n\n        logger.info(\"Concatenating datasets...\")\n        combined_dataset = concatenate_datasets(datasets)\n\n        if len(self.args.dataset) == 1:\n            combined_dataset = datasets\n        elif self.args.mix_strategy == \"concat\":\n            if self.args.streaming:\n                logger.warning(\n                    \"The samples between different datasets will not be mixed in streaming mode.\"\n                )\n            combined_dataset = concatenate_datasets(datasets)\n        elif self.args.mix_strategy.startswith(\"interleave\"):\n            if not self.args.streaming:\n                logger.warning(\n                    \"We recommend using `mix_strategy=concat` in non-streaming mode.\"\n                )\n            stopping_strategy = (\n                \"first_exhausted\"\n                if self.args.mix_strategy.endswith(\"under\")\n                else \"all_exhausted\"\n            )\n            combined_dataset = interleave_datasets(\n                datasets,\n                self.args.interleave_probs,\n                stopping_strategy=stopping_strategy,\n            )\n        else:\n            raise ValueError(\"Unknown mixing strategy.\")\n\n        logger.info(f\"Combined dataset features: {combined_dataset.features} and \\\n                    Combined dataset length: {len(combined_dataset)}\")\n        \n        combined_dataset = combined_dataset.shuffle(seed=42)\n\n        if self.split == \"test\":\n            combined_dataset = combined_dataset.select(range(100))\n\n        return combined_dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx]\n\n    @classmethod\n    def create_train_and_test_datasets(\n        cls, args: DataArguments, processor: WhisperProcessor, **kwargs\n    ):\n        if args.test_dataset_name:\n            train_dataset = cls(args, processor, split=\"train\", **kwargs)\n            test_dataset = cls(args, processor, split=\"test\", **kwargs)\n\n        else:\n            if not 0 < args.max_eval_samples < 1:\n                raise ValueError(\n                    \"When no test_dataset_name is specified, \\\n                    max_eval_samples must be a float between 0 and 1, representing \\\n                    the ratio of the dataset to use for evaluation.\"\n                )\n\n            full_dataset = cls(args, processor, **kwargs)\n\n            total_samples = len(full_dataset.dataset)\n            max_eval_samples = int(total_samples * args.max_eval_samples)\n            max_train_samples = total_samples - max_eval_samples\n            train_dataset = full_dataset.select(range(max_train_samples))\n            test_dataset = full_dataset.select(range(max_train_samples, total_samples))\n\n        return train_dataset, test_dataset\n\n\ndef remove_punctuation(text: str or List[str]):\n    punctuation = \"!,.;:?、！，。；：？\"\n    if isinstance(text, str):\n        text = re.sub(r\"[{}]+\".format(punctuation), \"\", text).strip()\n        return text\n    elif isinstance(text, list):\n        result_text = []\n        for t in text:\n            t = re.sub(r\"[{}]+\".format(punctuation), \"\", t).strip()\n            result_text.append(t)\n        return result_text\n    else:\n        raise Exception(f\"Not support this type {type(text)}\")\n"}
{"type": "source_file", "path": "src/crawler/__init__.py", "content": "from .audio_download import (\n    AudioDownloadStrategy,\n    YTDLPDownloadStrategy,\n    CobaltDownloadStrategy,\n    FallbackDownloadStrategy,\n    AudioDownloadStrategyFactory,\n    AudioConverter\n)\nfrom .subtitle_download import (\n    SubtitleDownloadStrategy,\n    YouTubeTranscriptDownloadStrategy,\n    OpenAISubtitleDownloadStrategy,\n    FallbackSubtitleDownloadStrategy,\n    SubtitleDownloadStrategyFactory,\n)\nfrom .audio_saver import (\n    JsonAppendSaver,\n    HuggingFaceDatasetSaver,\n)\n\n__all__ = [\n    \"AudioDownloadStrategy\",\n    \"YTDLPDownloadStrategy\",\n    \"CobaltDownloadStrategy\",\n    \"FallbackDownloadStrategy\",\n    \"AudioDownloadStrategyFactory\",\n    \"AudioConverter\",\n    \"SubtitleDownloadStrategy\",\n    \"YouTubeTranscriptDownloadStrategy\",\n    \"OpenAISubtitleDownloadStrategy\",\n    \"FallbackSubtitleDownloadStrategy\",\n    \"SubtitleDownloadStrategyFactory\",\n    \"JsonAppendSaver\",\n    \"HuggingFaceDatasetSaver\",\n]"}
{"type": "source_file", "path": "src/data/__init__.py", "content": "from .dataset_preparation import DatasetPreparation\nfrom .dataset_loader import DatasetLoaderFactory\nfrom .data_preprocess import PreprocessingPipeline\n\n__all__ = [\n    \"DatasetPreparation\", \n    \"DatasetLoaderFactory\", \n    \"PreprocessingPipeline\"\n]\n"}
{"type": "source_file", "path": "scripts/train.py", "content": "import sys\nfrom transformers import HfArgumentParser\nfrom src.config import (\n    ModelArguments,\n    DataArguments,\n    WhisperProcessorConfig,\n    WhisperTrainingArguments,\n    WhisperPredictionArguments,\n)\nfrom src.model.whisper_model import load_whisper_model\nfrom src.data.dataset import ChineseTaiwaneseDataset\nfrom src.data.data_collator import WhisperDataCollator\nfrom src.trainers.whisper_trainer import get_trainer\nimport torch\nfrom src.utils.mlflow_logging import mlflow_logging\n\n# from peft import LoraConfig, TaskType\n\n\n@mlflow_logging(\"Whisper_Experiment\", \"lora\")\ndef main():\n    parser = HfArgumentParser(\n        (\n            ModelArguments,\n            DataArguments,\n            WhisperTrainingArguments,\n            WhisperProcessorConfig,\n            WhisperPredictionArguments,\n        )\n    )\n\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, training_args, procrssor_args, prediction_args = (\n            parser.parse_json_file(json_file=sys.argv[1])\n        )\n    else:\n        model_args, data_args, training_args, procrssor_args, prediction_args = (\n            parser.parse_args_into_dataclasses()\n        )\n    # Configure LoRA if specified\n    peft_config = None\n    if model_args.use_peft:\n        if model_args.peft_method.lower() == \"lora\":\n            peft_config = {\n                \"task_type\": None,\n                \"r\": model_args.lora_r,\n                \"lora_alpha\": model_args.lora_alpha,\n                \"lora_dropout\": model_args.lora_dropout,\n                \"bias\": \"none\",\n            }\n        else:\n            raise ValueError(f\"Unsupported PEFT method: {model_args.peft_method}\")\n\n    compute_dtype = torch.float16 if training_args.fp16 else torch.float32\n    model, processor = load_whisper_model(\n        model_args.model_name_or_path,\n        use_peft=model_args.use_peft,\n        peft_config=peft_config,\n        language=model_args.language,\n        compute_dtype=compute_dtype,\n    )\n\n    # processor.tokenizer.model_max_length = model.config.max_length\n    # model.config.forced_decoder_ids = None\n    # model.config.suppress_tokens = []\n    train_dataset_list = data_args.dataset\n    train_dataset, eval_dataset = (\n        ChineseTaiwaneseDataset.create_train_and_test_datasets(\n            data_args,\n            processor,\n        )\n    )\n\n    data_collator = WhisperDataCollator(\n        processor=processor,\n    )\n    model.config.use_cache = False\n    processor.tokenizer.predict_timestamps = data_args.timestamp\n\n    trainer = get_trainer(\n        model=model,\n        args=training_args,\n        processor_args=procrssor_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        processor=processor,\n    )\n\n    train_result = trainer.train()\n    trainer.log_metrics(\"train\", train_result.metrics)\n    trainer.save_metrics(\"train\", train_result.metrics)\n    trainer.save_state()\n    trainer.save_model(training_args.output_dir)\n    processor.save_pretrained(training_args.output_dir)\n\n    # prediction_args = prediction_args.to_dict()\n    # prediction_args[\"eos_token_id\"] = [\n    #     processor.tokenizer.eos_token_id\n    # ] + processor.tokenizer.additional_special_tokens_ids\n    # prediction_args[\"pad_token_id\"] = processor.tokenizer.pad_token_id\n    # prediction_args[\"predict_with_generate\"] = False\n    # metrics = trainer.evaluate(metric_key_prefix=\"eval\", **prediction_args)\n    # trainer.log_metrics(\"eval\", metrics)\n    # trainer.save_metrics(\"eval\", metrics)\n    # trainer.save_state()\n    return {\n        'checkpoint_dir': training_args.output_dir,\n        'base_model_name': model_args.model_name_or_path,\n        'data_config': data_args.__dict__,\n        \"train_dataset\": train_dataset_list\n    }\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/data/dataset_loader.py", "content": "from abc import ABC, abstractmethod\nfrom datasets import (\n    load_dataset,\n    Dataset as HFDataset,\n)\nfrom src.config import DatasetAttr\nimport os\nfrom src.utils.logging import logger\nfrom src.data.dataset_preparation import DatasetPreparation\nfrom src.config import DataArguments\n\n\nMAX_DURATION_IN_SECONDS = 30.0\nmax_input_length = MAX_DURATION_IN_SECONDS * 16000\n\n\ndef is_audio_in_length_range(length):\n    return 0 < length < max_input_length\n\n\ndef filter_labels(labels_length):\n    return labels_length < 448\n\n\nclass DatasetStrategy(ABC):\n    def __init__(self, dataset_attr: DatasetAttr, args: DataArguments):\n        self.dataset_attr = dataset_attr\n        self.args = args\n\n    @abstractmethod\n    def load_dataset(self, dataset_attr: DatasetAttr) -> HFDataset:\n        raise NotImplementedError(\"You should implement this method\")\n\n    def preprocess_dataset(self) -> HFDataset:\n        logger.info(f\"Loading dataset from {self.dataset_attr.dataset_name}\")\n        dataset = self.load_dataset()\n        logger.info(f\"Preprocessing dataset from {self.dataset_attr.dataset_name}\")\n        self.dataset_preprocessor = DatasetPreparation()\n        return self.dataset_preprocessor.preprocess(dataset, self.dataset_attr, self.args)\n\n\nclass HFHubDatasetStrategy(DatasetStrategy):\n    def __init__(self, dataset_attr: DatasetAttr, args: DataArguments):\n        super().__init__(dataset_attr, args)\n\n    def load_dataset(self) -> HFDataset:\n        logger.info(\"Loading dataset from Hugging Face Hub or Hugging Face Local\")\n        dataset = load_dataset(\n            self.dataset_attr.dataset_name,\n            *self.dataset_attr.dataset_args,\n            **self.dataset_attr.dataset_kwargs,\n        )\n        if self.dataset_attr.kwargs:\n            dataset = dataset.filter(lambda example: example[\"language\"] == self.dataset_attr.kwargs.get('language'))\n        return dataset\n        \n\nclass LocalDatasetStrategy(DatasetStrategy):\n    def __init__(self, dataset_attr: DatasetAttr, args: DataArguments):\n        super().__init__(dataset_attr, args)\n        \n    def load_dataset(self) -> HFDataset:\n        logger.info(\"Loading dataset from local directory\")\n        file_path = os.path.join(self.args.dataset_dir, self.dataset_attr.dataset_name)\n        dataset = load_dataset('json', data_files=file_path, **self.dataset_attr.dataset_kwargs)\n        return dataset\n\n\nclass DatasetLoaderFactory:\n    def __init__(self, dataset_attr: DatasetAttr, args: DataArguments):\n        self.dataset_attr = dataset_attr\n        self.args = args\n\n    def load_dataset(self) -> HFDataset:\n        if self.dataset_attr.load_from == \"hf_hub\":\n            return HFHubDatasetStrategy(self.dataset_attr, self.args).preprocess_dataset()\n        else:\n            return LocalDatasetStrategy(self.dataset_attr, self.args).preprocess_dataset()"}
{"type": "source_file", "path": "src/crawler/youtube_crawler.py", "content": "from pathlib import Path\nfrom typing import Optional\nfrom src.config import CrawlerArgs\nfrom src.utils.logging import logger\nfrom src.crawler.audio_download import AudioDownloadStrategyFactory\nfrom src.crawler.subtitle_download import SubtitleDownloadStrategyFactory\nfrom src.crawler.audio_process import AudioProcessStrategyFactory, AudioProcessor\nfrom src.crawler.audio_saver import HuggingFaceDatasetSaver\nimport re\nimport yt_dlp\nimport subprocess\nimport os \n\n\nclass YoutubeCrawler:\n    def __init__(self, args: CrawlerArgs):\n        self.args = args\n        self.output_dir = Path(args.output_dir)\n        self.ffmpeg_path = args.ffmpeg_path\n        self.file_prefix = args.file_prefix\n        self.batch_size = args.batch_size\n        self.max_duration = args.max_duration\n        self.ffmpeg_path = args.ffmpeg_path\n\n        self._create_output_dirs()\n\n        self.audio_download_strategy = (\n            AudioDownloadStrategyFactory.create_download_strategy(self.ffmpeg_path)\n        )\n        self.subtitle_download_strategy = (\n            SubtitleDownloadStrategyFactory.create_subtitle_download_strategy()\n        )\n        self.audio_process_strategy = (\n            AudioProcessStrategyFactory.create_process_strategy(\n                args.output_dir, args.file_prefix\n            )\n        )\n        self.audio_processor = AudioProcessor(self.audio_process_strategy)\n        self.dataset_saver = HuggingFaceDatasetSaver()\n\n    def crawl(self):\n        self._check_ffmpeg()\n        json_path = (\n            self.output_dir / self.file_prefix / f\"{self.args.dataset_name}.json\"\n        )\n\n        if self.args.playlist_urls:\n            logger.info(f\"Processing YouTube playlists: {self.args.playlist_urls}\")\n            for play_idx, playlist_url in enumerate(self.args.playlist_urls):\n                self._process_youtube_playlist(play_idx, playlist_url, json_path)\n\n        if self.args.audio_dir:\n            logger.info(f\"Processing existing audio files from: {self.args.audio_dir}\")\n            self._process_existing_audio_files(self.args.audio_dir, json_path)\n\n        logger.info(f\"All segments saved to: {json_path}\")\n\n    def _create_output_dirs(self):\n        self.audio_dir = os.path.join(self.output_dir, self.file_prefix, \"audio\")\n        self.subtitle_dir = os.path.join(self.output_dir, self.file_prefix, \"subtitles\")\n        os.makedirs(self.audio_dir, exist_ok=True)\n        os.makedirs(self.subtitle_dir, exist_ok=True)\n\n    def _check_ffmpeg(self):\n        \"\"\"Check if FFmpeg is installed and accessible.\"\"\"\n        try:\n            if self.ffmpeg_path:\n                subprocess.run(\n                    [self.ffmpeg_path, \"-version\"],\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                )\n            else:\n                subprocess.run(\n                    [\"ffmpeg\", \"-version\"],\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                )\n            return True\n        except FileNotFoundError:\n            return False\n\n    def _extract_playlist_id(self, url: str) -> Optional[str]:\n        playlist_id_match = re.search(r\"(?:list=)([a-zA-Z0-9_-]+)\", url)\n        return playlist_id_match.group(1) if playlist_id_match else None\n\n    def _process_youtube_playlist(\n        self, play_idx: int, playlist_url: str, json_path: Path\n    ):\n        playlist_id = self._extract_playlist_id(playlist_url)\n        if not playlist_id:\n            logger.error(f\"Invalid playlist URL: {playlist_url}\")\n            return\n\n        ydl_opts = {\n            \"extract_flat\": True,\n            \"quiet\": True,\n        }\n\n        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n            playlist_dict = ydl.extract_info(\n                f\"https://www.youtube.com/playlist?list={playlist_id}\", download=False\n            )\n\n        for index, video in enumerate(playlist_dict[\"entries\"]):\n            video_id = video[\"id\"]\n            logger.info(f\"Processing video: {video_id}\")\n            result = self._download_youtube_audio_and_subtitles(\n                video_id, play_idx, index\n            )\n            if result:\n                audio_file, subtitle_file = result\n                self.audio_processor.process_audio(\n                    audio_file, subtitle_file, json_path, self.max_duration\n                )\n\n            if index + 1 >= self.batch_size:\n                logger.info(f\"Reached batch size limit of {self.batch_size}. Stopping.\")\n                break\n\n    def _download_youtube_audio_and_subtitles(\n        self, video_id: str, play_idx: int, file_index: int\n    ) -> Optional[tuple[Path, Path]]:\n        file_name = f\"{self.file_prefix}_{play_idx:04d}_{file_index:04d}\"\n        audio_file = Path(os.path.join(self.audio_dir, f\"{file_name}.mp3\"))\n        subtitle_file = Path(os.path.join(self.subtitle_dir, f\"{file_name}.json\"))\n\n        if not audio_file.exists():\n            try:\n                audio_file = self.audio_download_strategy.download_and_convert(video_id, audio_file)\n            except Exception as e:\n                logger.error(f\"Error downloading audio for video {video_id}: {e}\")\n                return None\n        if not subtitle_file.exists():\n            try:\n                \n                self.subtitle_download_strategy.download(video_id, subtitle_file, audio_file)\n            except Exception as e:\n                logger.error(f\"Error downloading subtitles for video {video_id}: {e}\")\n                return None\n\n        return audio_file, subtitle_file\n\n    def _process_existing_audio_files(self, audio_dir: Path, json_path: Path):\n        audio_files = list(audio_dir.glob(\"*.wav\"))\n        for audio_file in audio_files:\n            subtitle_file = audio_file.with_suffix(\".json\")\n            if subtitle_file.exists():\n                self.audio_processor.process_audio(\n                    audio_file, subtitle_file, json_path, self.max_duration\n                )\n            else:\n                logger.warning(\n                    f\"Subtitle file not found for {audio_file}. Skipping this audio file.\"\n                )\n\n    def process_and_create_dataset(self, json_path: Path):\n        dataset = self.dataset_saver.create_dataset_from_json(json_path)\n        return dataset\n\n    def save_dataset(self, dataset, output_file: Path):\n        self.dataset_saver.convert_dataset_to_json(dataset, output_file)\n\n\ndef main():\n    from transformers import HfArgumentParser\n\n    parser = HfArgumentParser(CrawlerArgs)\n    args = parser.parse_args_into_dataclasses()[0]\n    crawler = YoutubeCrawler(args)\n    crawler.crawl()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/main.py", "content": "from inference import FusionWhisperLLaMAInference\nfrom src.utils.logging import logger\nimport librosa\n\n\ndef main():\n    # Initialize the fusion model\n    logger.info(\"Initializing FusionWhisperLLaMAInference model...\")\n    fusion_model = FusionWhisperLLaMAInference(\n        whisper_model_path=\"openai/whisper-small\",\n        llama_model_path=\"taide/Llama3-TAIDE-LX-8B-Chat-Alpha1\",\n        device=\"cuda\",\n        use_peft=False,\n        language=\"chinese\",\n        use_timestamps=True,\n        lm_weight=0.1,\n    )\n\n    # Single audio file transcription\n    audio_path = \"S00001.wav\"\n    logger.info(f\"Transcribing single audio file: {audio_path}\")\n    audio, sr = librosa.load(audio_path, sr=None)\n    transcription = fusion_model.transcribe_batch(audio, sr)\n    logger.info(f\"Transcription: {transcription}\")\n\n    # Streaming transcription example\n    logger.info(\"Starting streaming transcription...\")\n\n    def audio_stream_generator(\n        audio_path, chunk_size=1600\n    ):  # 0.1 second chunks at 16kHz\n        audio, sr = librosa.load(audio_path, sr=16000)\n        for i in range(0, len(audio), chunk_size):\n            yield audio[i: i + chunk_size]\n\n    for result in fusion_model.transcribe_stream(audio_stream_generator(audio_path)):\n        if result:\n            logger.info(f\"Partial transcription: {result['transcription']}\")\n            logger.info(f\"Speed: {result['speed']:.2f}x real-time\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/crawler/audio_saver.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport json\nimport soundfile as sf\nfrom tqdm import tqdm\nfrom datasets import Dataset, IterableDataset\nfrom collections import defaultdict\nfrom src.utils.logging import logger\n\n\nclass AudioSaver(ABC):\n    @abstractmethod\n    def save(self, audio: Any, sr: int, file_path: Path) -> None:\n        raise NotImplementedError(\n            \"AudioSaver method must be implemented by a subclass.\"\n        )\n\n\nclass JsonSaver(ABC):\n    @abstractmethod\n    def save(self, data: Any, file_path: Path) -> None:\n        raise NotImplementedError(\"JsonSaver method must be implemented by a subclass.\")\n\n\nclass HuggingFaceDatasetSaver(ABC):\n    @abstractmethod\n    def iterable_to_dataset(self, iterable_dataset: IterableDataset) -> Dataset:\n        raise NotImplementedError(\n            \"HuggingFaceDatasetSaver method must be implemented by a subclass.\"\n        )\n\n    @abstractmethod\n    def create_dataset_from_json(self, json_file: Path) -> Dataset:\n        raise NotImplementedError(\n            \"HuggingFaceDatasetSaver method must be implemented by a subclass.\"\n        )\n\n    @abstractmethod\n    def convert_dataset_to_json(self, dataset: Dataset, output_file: Path) -> None:\n        raise NotImplementedError(\n            \"HuggingFaceDatasetSaver method must be implemented by a subclass.\"\n        )\n\n\nclass SoundfileSaver(AudioSaver):\n    def save(self, audio: Any, sr: int, file_path: str) -> None:\n        sf.write(file_path, audio, sr)\n\n\nclass JsonAppendSaver(JsonSaver):\n    def save(self, data: List[Dict], file_path: str) -> None:\n        if Path(file_path).exists():\n            with open(file_path, \"r+\", encoding=\"utf-8\") as f:\n                existing_data = json.load(f)\n                existing_data.extend(data)\n                f.seek(0)\n                json.dump(existing_data, f, ensure_ascii=False, indent=2)\n                f.truncate()\n        else:\n            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, ensure_ascii=False, indent=4)\n\n\nclass HuggingFaceDatasetSaver(HuggingFaceDatasetSaver):\n    def iterable_to_dataset(self, iterable_dataset: IterableDataset) -> Dataset:\n        data_dict = defaultdict(list)\n        for item in iterable_dataset:\n            for key, value in item.items():\n                data_dict[key].append(value)\n        return Dataset.from_dict(data_dict)\n\n    def create_dataset_from_json(self, json_file: Path) -> Dataset:\n        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return Dataset.from_dict(\n            {\n                \"client_id\": [f\"{Path(item['audio_path']).name}\" for item in data],\n                \"path\": [item[\"audio_path\"] for item in data],\n                \"sentence\": [item[\"text\"] for item in data],\n                \"start\": [item[\"start\"] for item in data],\n                \"end\": [item[\"end\"] for item in data],\n                \"duration\": [item[\"end\"] - item[\"start\"] for item in data],\n            }\n        )\n\n    def convert_dataset_to_json(self, dataset: Dataset, output_file: Path) -> None:\n        data = []\n        for item in tqdm(dataset, desc=\"Converting dataset to JSON\"):\n            data.append(\n                {\n                    \"client_id\": item[\"client_id\"],\n                    \"path\": item[\"path\"],\n                    \"sentence\": item[\"sentence\"],\n                    \"start\": item[\"start\"],\n                    \"end\": item[\"end\"],\n                    \"duration\": item[\"duration\"],\n                }\n            )\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n\n        logger.info(f\"Dataset saved as JSON: {output_file}\")"}
{"type": "source_file", "path": "src/model/whisper_model.py", "content": "# from transformers import WhisperForConditionalGeneration, WhisperProcessor\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom typing import Any, Optional, List\nimport torch\nfrom transformers.modeling_utils import PreTrainedModel\nfrom src.utils.logging import logger\n\n\ndef prepare_model_for_training(\n    model: \"PreTrainedModel\",\n    output_layer_name: Optional[str] = \"lm_head\",\n    use_gradient_checkpointing: Optional[bool] = True,\n    layer_norm_names: Optional[List[str]] = [\"q_proj\", \"v_proj\"],\n) -> \"PreTrainedModel\":\n    r\"\"\"\n    Includes:\n        (1) cast the layernorm in fp32\n        (2) make output embedding layer require grads\n        (3) upcast the lm_head to fp32\n    Inspired by: https://github.com/huggingface/peft/blob/v0.2.0/src/peft/utils/other.py#L33\n    \"\"\"\n    logger.info(\"prepare_model_for_training\")\n    for name, param in model.named_parameters():\n        if param.ndim == 1 and any(\n            layer_norm_name in name for layer_norm_name in layer_norm_names\n        ):\n            param.data = param.data.to(torch.float32)\n\n    if use_gradient_checkpointing:\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        model.gradient_checkpointing_enable()\n        model.config.use_cache = (\n            False  # turn off when gradient checkpointing is enabled\n        )\n\n    logger.info(\"CastOutputToFloat\")\n    if hasattr(model, output_layer_name):\n        output_layer: torch.nn.Linear = getattr(model, output_layer_name)\n        input_dtype = output_layer.weight.dtype\n\n        class CastOutputToFloat(torch.nn.Sequential):\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\n                return super().forward(x.to(input_dtype)).to(torch.float32)\n\n        setattr(model, output_layer_name, CastOutputToFloat(output_layer))\n    return model\n\n\ndef load_whisper_model(\n    model_name_or_path: str,\n    use_peft: bool = False,\n    peft_config: dict = None,\n    language: str = \"chinese\",\n    compute_dtype: Any = None,\n):\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        model_name_or_path\n    )  # , torch_dtype=compute_dtype,\n    processor = AutoProcessor.from_pretrained(model_name_or_path)\n\n    # Set the language token\n    processor.tokenizer.set_prefix_tokens(language=language, task=\"transcribe\")\n\n    model.config.forced_decoder_ids = None\n    model.config.suppress_tokens = []\n\n    def make_inputs_require_grad(module, input, output):\n        output.requires_grad_(True)\n\n    model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n\n    if use_peft:\n        if peft_config is None:\n            peft_config = {\n                \"task_type\": TaskType.SEQ_2_SEQ_LM,\n                \"r\": 16,\n                \"lora_alpha\": 32,\n                \"lora_dropout\": 0.05,\n            }\n\n        # target_modules = []\n        # for id, (name, param) in enumerate(model.named_modules()):\n        #     if 'model.decoder' in name and ('q_proj' in name or 'v_proj' in name):\n        #         target_modules.append(name)\n        target_modules = [\n            \"q_proj\",\n            \"v_proj\",\n        ]  # [\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n        peft_config.update({\"target_modules\": target_modules})\n\n        lora_config = LoraConfig(**peft_config)\n        model = prepare_model_for_training(model)\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n\n    return model, processor\n"}
{"type": "source_file", "path": "src/model/generative_fusion_decoding.py", "content": "from collections import defaultdict\nfrom transformers import (\n    AutoModelForSpeechSeq2Seq,\n    AutoProcessor,\n    AutoModelForCausalLM,\n)\nimport torch\nfrom transformers import LlamaTokenizerFast, WhisperTokenizer\nfrom src.utils.logging import logger\n\n# TODO: A lot of bugggg!\n# https://github.com/mtkresearch/generative-fusion-decoding\n\nclass ByteTokenizer:\n    def tokenize_from_byte(self, byte_str):\n        str_part = byte_str.decode(\"utf8\", errors=\"ignore\")\n        return self(str_part, add_special_tokens=False).input_ids\n\n    def convert_ids_to_bytes(self, ids):\n        raise NotImplementedError\n\n    def get_matched_ids_from_prefix(self, byte_prefix):\n        if not hasattr(self, \"_prefix_to_ids\"):\n            self._prefix_to_ids = defaultdict(list)\n            for i in range(self.vocab_size):\n                b = self.convert_ids_to_bytes(i)\n                for j in range(1, len(b) + 1):\n                    prefix = b[:j]\n                    self._prefix_to_ids[prefix].append(i)\n        return self._prefix_to_ids.get(byte_prefix, [])\n\n    def get_alternative_ids(self, seq_ids):\n        alternative_ids = [None] * len(seq_ids)\n        prefix_from_last = b\"\"\n        pointer_from_last = 1\n        while pointer_from_last <= len(seq_ids):\n            id_to_convert = seq_ids[-pointer_from_last]\n            converted_bytes = self.convert_ids_to_bytes(id_to_convert)\n            prefix_from_last = converted_bytes + prefix_from_last\n            alternative_ids[-pointer_from_last] = self.get_matched_ids_from_prefix(\n                prefix_from_last\n            )\n            pointer_from_last += 1\n\n        return alternative_ids\n\n\nclass LlamaByteTokenizer(LlamaTokenizerFast, ByteTokenizer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.bytetokens_to_ids = {}\n        for s, i in self.get_vocab().items():\n            b = self._convert_token_to_byte(s)\n            if b in self.bytetokens_to_ids:\n                if self.bytetokens_to_ids[b] < i:\n                    self.bytetokens_to_ids[b] = i\n            else:\n                self.bytetokens_to_ids[b] = i\n\n    def convert_ids_to_bytes(self, ids):\n        if isinstance(ids, int):\n            tokens = self.convert_ids_to_tokens(ids, skip_special_tokens=False)\n            return self._convert_token_to_byte(tokens)\n        else:\n            tokens = self.convert_ids_to_tokens(ids, skip_special_tokens=False)\n            if isinstance(tokens, str):\n                return self._convert_token_to_byte(tokens)\n            return b\"\".join([self._convert_token_to_byte(t) for t in tokens])\n\n    def _convert_token_to_byte(self, token):\n        SPIECE_UNDERLINE = \"▁\"\n        if token.startswith(SPIECE_UNDERLINE) and len(token) > 1:\n            token = \" \" + token.lstrip(SPIECE_UNDERLINE)\n\n        if token.startswith(\"<0x\"):  # '<0xAB>' -> 'AB' -> b'\\xAB'\n            bs = bytes.fromhex(f\"{token[3:5]}\")\n        else:\n            bs = token.encode(\"utf8\")\n        return bs\n\n    def tokenize_from_byte(self, byte_str):\n        str_part = byte_str.decode(\"utf8\", errors=\"ignore\")\n        encoded_str_part = str_part.encode(\"utf8\")\n\n        str_part_tokenized = self(str_part, add_special_tokens=False).input_ids\n        leftover_string = byte_str[len(encoded_str_part):]\n        for byte_int in leftover_string:\n            byte_character = bytes([byte_int])\n            str_part_tokenized.append(self.bytetokens_to_ids[byte_character])\n\n        return str_part_tokenized\n\n\nclass WhisperByteTokenizer(WhisperTokenizer, ByteTokenizer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def convert_ids_to_bytes(self, ids, skip_special_tokens=True):\n        if isinstance(ids, int):\n            token = self.convert_ids_to_tokens(\n                ids, skip_special_tokens=skip_special_tokens\n            )\n            return bytes([self.byte_decoder[c] for c in token])\n        else:\n            tokens = self.convert_ids_to_tokens(\n                ids, skip_special_tokens=skip_special_tokens\n            )\n            return b\"\".join([bytes([self.byte_decoder[c] for c in s]) for s in tokens])\n\n\nclass GenerativeFusionDecoding:\n    def __init__(\n        self,\n        asr_model: AutoModelForSpeechSeq2Seq,\n        lm_model: AutoModelForCausalLM,\n        asr_processor: AutoProcessor,\n        lm_tokenizer: LlamaByteTokenizer,\n        r: float = 0.2,\n    ):\n        self.asr_model = asr_model\n        self.lm_model = lm_model\n        self.asr_processor = asr_processor\n        self.lm_tokenizer = lm_tokenizer\n        self.r = r  # fusion weight\n\n    def byte_level_probability(\n        self, model, tokenizer, input_ids_or_features, generated_ids, past_key_values, is_asr_model=False\n    ):\n        input_ids = torch.tensor([generated_ids], device=model.device)\n\n        if is_asr_model:\n            # For ASR model (Whisper)\n            outputs = model(\n                input_features=input_ids_or_features,\n                decoder_input_ids=input_ids,\n                past_key_values=past_key_values,\n            )\n        else:\n            # For LM model (LlamaForCausalLM)\n            outputs = model(\n                input_ids=input_ids,\n                past_key_values=past_key_values,\n            )\n\n        logits = outputs.logits[:, -1, :]\n        probs = torch.softmax(logits, dim=-1)\n\n        # Use ByteTokenizer methods\n        main_bytes = tokenizer.convert_ids_to_bytes(generated_ids)\n        main_prob = torch.prod(probs[0, input_ids[0]]).item()\n\n        # Get alternative token IDs that match the byte prefix\n        alternative_ids = tokenizer.get_alternative_ids(generated_ids)\n\n        alt_probs = 0\n        for alt_id_list in alternative_ids:\n            if alt_id_list:\n                for alt_id in alt_id_list:\n                    alt_prob = probs[0, alt_id].item()\n                    alt_probs += alt_prob\n\n        return main_prob + alt_probs\n\n    def fuse_probabilities(self, asr_prob, lm_prob, t, k):\n        if t < k:\n            # At the beginning of the sequence, only use ASR model probabilities\n            return asr_prob\n        else:\n            # Use fused probabilities\n            return (1 - self.r) * asr_prob + self.r * lm_prob\n\n    def decode(\n        self,\n        audio_input,\n        sampling_rate=16000,\n        prompt=None,\n        beam_size=5,\n        max_length=200,\n        k=5,\n        language=\"zh\",\n    ):\n        device = next(self.asr_model.parameters()).device\n        asr_inputs = self.asr_processor(\n            audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\"\n        ).to(device)\n\n        # Initialize decoder input IDs\n        decoder_start_token_id = self.asr_model.config.decoder_start_token_id\n        decoder_input_ids = torch.tensor([[decoder_start_token_id]], device=device)\n\n        # Get language and task-specific decoder prompt IDs\n        forced_decoder_ids = self.asr_processor.get_decoder_prompt_ids(\n            language=language, task=\"transcribe\"\n        )\n        for _, token_id in forced_decoder_ids:\n            decoder_input_ids = torch.cat(\n                [decoder_input_ids, torch.tensor([[token_id]], device=device)], dim=1\n            )\n\n        beams = [\n            {\n                \"sequence\": decoder_input_ids.squeeze(0),\n                \"score\": 0.0,\n                \"asr_state\": None,\n                \"lm_state\": None,\n            }\n        ]\n\n        for t in range(max_length):\n            candidates = []\n            for beam in beams:\n                # ASR model generation\n                asr_output = self.asr_model.generate(\n                    **asr_inputs,\n                    decoder_input_ids=beam[\"sequence\"].unsqueeze(0),\n                    output_scores=True,\n                    return_dict_in_generate=True,\n                    max_new_tokens=1,\n                    past_key_values=beam[\"asr_state\"],\n                    use_cache=True,\n                )\n\n                asr_token = asr_output.sequences[0, -1]\n\n                asr_prob = self.byte_level_probability(\n                    self.asr_model,\n                    self.asr_processor.tokenizer,\n                    asr_inputs.input_features,\n                    beam[\"sequence\"].tolist() + [asr_token.item()],\n                    asr_output.past_key_values,\n                    is_asr_model=True,\n                )\n\n                # LM model generation\n                if prompt:\n                    lm_input_ids = self.lm_tokenizer.tokenize_from_byte(prompt.encode('utf-8'))\n                else:\n                    lm_input_ids = []\n\n                # Append the beam sequence\n                lm_input_ids += beam[\"sequence\"].tolist()\n\n                # Convert to tensor\n                if len(lm_input_ids) == 0:\n                    lm_input = torch.tensor(\n                        [[self.lm_model.config.bos_token_id]], device=device\n                    )\n                else:\n                    lm_input = torch.tensor([lm_input_ids], device=device)\n\n                # Create attention_mask\n                attention_mask = torch.ones_like(lm_input)\n\n                # Set pad_token_id\n                pad_token_id = self.lm_model.config.pad_token_id\n                if pad_token_id is None:\n                    pad_token_id = self.lm_model.config.eos_token_id\n\n                lm_output = self.lm_model.generate(\n                    input_ids=lm_input,\n                    attention_mask=attention_mask,\n                    pad_token_id=pad_token_id,\n                    max_new_tokens=1,\n                    output_scores=True,\n                    return_dict_in_generate=True,\n                    past_key_values=beam[\"lm_state\"],\n                    use_cache=True,\n                )\n\n                lm_token = lm_output.sequences[0, -1]\n\n                # Prepare LM input IDs for probability computation\n                lm_input_ids += [lm_token.item()]\n\n                lm_prob = self.byte_level_probability(\n                    self.lm_model,\n                    self.lm_tokenizer,\n                    lm_input_ids,\n                    lm_input_ids,\n                    lm_output.past_key_values,\n                    is_asr_model=False,\n                )\n\n                # Fuse probabilities\n                fused_score = self.fuse_probabilities(asr_prob, lm_prob, t, k)\n\n                new_sequence = torch.cat([beam[\"sequence\"], asr_token.unsqueeze(0)])\n                breakpoint()\n                candidates.append(\n                    {\n                        \"sequence\": new_sequence,\n                        \"score\": beam[\"score\"] + fused_score,\n                        \"asr_state\": asr_output.past_key_values,\n                        \"lm_state\": lm_output.past_key_values,\n                    }\n                )\n\n            # Select the best beam_size candidates\n            beams = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:beam_size]\n\n        # Return the highest scoring sequence\n        return beams[0][\"sequence\"]\n\n\n\nif __name__ == \"__main__\":\n    # Load models and processors\n    asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\")\n    asr_processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n    asr_processor.tokenizer = WhisperByteTokenizer.from_pretrained(\n        \"openai/whisper-small\"\n    )\n\n    lm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n    lm_tokenizer = LlamaByteTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\n    # Initialize GFD\n    gfd = GenerativeFusionDecoding(asr_model, lm_model, asr_processor, lm_tokenizer)\n\n    # Perform decoding\n    audio_input = (\n        \"youtube_data/test/split_audio/test_0000_0000_0000.wav\"  # Load your audio input\n    )\n    import librosa\n\n    audio, _ = librosa.load(audio_input, sr=16000)\n    result = gfd.decode(audio)\n\n    # Output the result\n    print(asr_processor.decode(result))\n\n\n\n\nclass FusionWhisperLLaMAInference(ChineseTaiwaneseASRInference):\n    def __init__(self, \n                 whisper_model_path: str,\n                 llama_model_path: str,\n                 device: str = \"cuda\",\n                 use_peft: bool = False,\n                 language: str = \"chinese\",\n                 use_timestamps: bool = False,\n                 lm_weight: float = 0.1,\n                 *args, \n                 **kwargs):\n        super().__init__(whisper_model_path, device, use_peft, language, use_timestamps, *args, **kwargs)\n        \n        # Initialize LLaMA\n        try:\n            if not isinstance(llama_model_path, str):\n                raise ValueError(f\"llama_model_path must be a string, got {type(llama_model_path)}\")\n            \n            self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_path)\n            self.llama_model = LlamaForCausalLM.from_pretrained(llama_model_path).to(device)\n            logger.info(f\"LLaMA model loaded successfully from {llama_model_path}\")\n        except Exception as e:\n            logger.error(f\"Error loading LLaMA model: {str(e)}\")\n            raise\n\n        self.lm_weight = lm_weight\n\n    def align_token_spaces(self, whisper_tokens: List[int]) -> List[int]:\n        \"\"\"Align token spaces between Whisper and LLaMA\"\"\"\n        whisper_text = self.processor.decode(whisper_tokens)\n        return self.llama_tokenizer.encode(whisper_text, add_special_tokens=False)\n\n    def get_llm_log_probs(self, aligned_tokens: List[int]) -> torch.Tensor:\n        \"\"\"Get log probabilities from LLaMA for error correction\"\"\"\n        input_ids = torch.tensor(aligned_tokens).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            outputs = self.llama_model(input_ids)\n            log_probs = torch.log_softmax(outputs.logits, dim=-1)\n        return log_probs\n\n    def optimized_decoding(self, audio_features: torch.Tensor, max_length: int = 200) -> torch.Tensor:\n        \"\"\"Implement optimized decoding strategy with shallow fusion\"\"\"\n        input_ids = torch.tensor([[self.model.config.decoder_start_token_id]]).to(self.device)\n        \n        for _ in range(max_length):\n            with torch.no_grad():\n                outputs = self.model(\n                    input_features=audio_features,\n                    decoder_input_ids=input_ids\n                )\n            whisper_logits = outputs.logits[:, -1, :]\n            whisper_log_probs = torch.log_softmax(whisper_logits, dim=-1)\n            \n            aligned_tokens = self.align_token_spaces(input_ids[0].tolist())\n            llm_log_probs = self.get_llm_log_probs(aligned_tokens)[:, -1, :]\n            \n            combined_log_probs = whisper_log_probs + self.lm_weight * llm_log_probs\n            \n            next_token = torch.argmax(combined_log_probs, dim=-1).unsqueeze(0)\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n            \n            if next_token.item() == self.processor.tokenizer.eos_token_id:\n                break\n        \n        return input_ids\n\n    @torch.no_grad()\n    def transcribe_batch(self, audio, sample_rate):\n        try:\n            transcriptions = []\n            if audio is None:\n                transcriptions.append(\"No valid audio input provided.\")\n            audio_chunks = self._process_audio(audio, sample_rate)\n            chunk_transcriptions = []\n            cumulative_duration = 0\n            for chunk in tqdm.tqdm(audio_chunks):\n                inputs = self.processor(chunk, \n                                        return_tensors=\"pt\", \n                                        truncation=False, \n                                        return_attention_mask=True, \n                                        sampling_rate=16000)\n\n                input_features = inputs.input_features.to(self.device)\n                generated_ids = self.optimized_decoding(input_features)\n                chunk_trans = self.processor.decode(generated_ids[0], \n                                                    skip_special_tokens=True,\n                                                    decode_with_timestamps=self.use_timestamps)\n                if self.use_timestamps:\n                    chunk_trans, cumulative_duration = self._process_timestamps(chunk_trans, cumulative_duration)\n                chunk_transcriptions.extend(chunk_trans)\n\n            full_transcription = \"\".join(chunk_transcriptions)\n            transcriptions.append(full_transcription)\n            \n            return transcriptions\n        except Exception as e:\n            logger.error(f\"Error in transcribe_batch: {e}\")\n            return [f\"Error in transcription: {str(e)}\"]\n\n    def process_chunk(self, audio_chunk: np.ndarray, sample_rate: int) -> dict:\n        start_time = time.time()\n\n        if not self.is_speech(audio_chunk, sample_rate):\n            return None\n\n        audio_chunk = librosa.util.normalize(audio_chunk)\n        \n        input_features = self.processor(audio_chunk, \n                                        sampling_rate=sample_rate, \n                                        return_tensors=\"pt\").input_features\n        input_features = input_features.to(self.device)\n\n        generated_ids = self.optimized_decoding(input_features)\n\n        transcription = self.processor.decode(generated_ids[0], \n                                              skip_special_tokens=True,\n                                              decode_with_timestamps=self.use_timestamps)        \n        if self.use_timestamps:  \n            transcription = self._process_timestamps(transcription)[0]  # Only take the formatted transcription\n               \n        end_time = time.time()\n        processing_time = end_time - start_time\n        speed = len(audio_chunk) / sample_rate / processing_time if processing_time > 0 else 0\n\n        return {\"transcription\": transcription.strip(), \"speed\": speed}"}
{"type": "source_file", "path": "src/utils/logging.py", "content": "import logging\nimport sys\n\nimport colorlog\n\n\ndef create_logger(level=logging.DEBUG):\n    logger = logging.getLogger()\n    logger.setLevel(level)\n    log_config = {\n        \"DEBUG\": {\"level\": 10, \"color\": \"purple\"},\n        \"INFO\": {\"level\": 20, \"color\": \"green\"},\n        \"WARNING\": {\"level\": 30, \"color\": \"yellow\"},\n        \"ERROR\": {\"level\": 40, \"color\": \"red\"},\n    }\n    formatter = colorlog.ColoredFormatter(\n        \"%(log_color)s[%(asctime)-15s] [%(levelname)8s]%(reset)s: %(message)s\",\n        log_colors={key: conf[\"color\"] for key, conf in log_config.items()},\n    )\n\n    sh = logging.StreamHandler(sys.stdout)\n    sh.setFormatter(formatter)\n    logger.handlers.clear()\n    logger.addHandler(sh)\n    return logger\n\n\nlogger = create_logger()"}
{"type": "source_file", "path": "src/trainers/__init__.py", "content": ""}
{"type": "source_file", "path": "src/utils/__init__.py", "content": "from .logging import logger\nfrom .mlflow_logging import mlflow_logging, setup_mlflow\n\n__all__ = [\n    \"logger\",\n    \"mlflow_logging\",\n    \"setup_mlflow\"\n]"}
{"type": "source_file", "path": "src/trainers/whisper_trainer.py", "content": "from transformers import Seq2SeqTrainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl\nimport evaluate\nimport os \nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\nimport re\nfrom typing import List\n\nwer_metric = evaluate.load(\"wer\")\n\n# class WhisperTrainer(Seq2SeqTrainer):\n#     def compute_loss(self, model, inputs, return_outputs=False):\n#         input_features = inputs.get(\"input_features\")\n#         labels = inputs.get(\"labels\")\n#         outputs = model(input_features=input_features, labels=labels)\n\n#         loss = outputs.loss\n#         return (loss, outputs) if return_outputs else loss\n\n\ndef remove_punctuation(text: str or List[str]):\n    punctuation = \"!,.;:?、！，。；：？\"\n    if isinstance(text, str):\n        text = re.sub(r\"[{}]+\".format(punctuation), \"\", text).strip()\n        return text\n    elif isinstance(text, list):\n        result_text = []\n        for t in text:\n            t = re.sub(r\"[{}]+\".format(punctuation), \"\", t).strip()\n            result_text.append(t)\n        return result_text\n    else:\n        raise Exception(f\"Not support this type {type(text)}\")\n\n\nclass SavePeftModelCallback(TrainerCallback):\n    def on_save(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n\n        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n        kwargs[\"model\"].save_pretrained(peft_model_path)\n\n        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n        if os.path.exists(pytorch_model_path):\n            os.remove(pytorch_model_path)\n        return control\n\n\ndef get_trainer(model, args, processor_args, train_dataset, eval_dataset, data_collator, processor):\n\n    def make_inputs_require_grad(module, input, output):\n        output.requires_grad_(True)\n    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    # Enable gradient checkpointing\n    model.gradient_checkpointing_enable()\n    # Disable caching\n    model.config.use_cache = False\n    model.generation_config.return_timestamps = processor_args.return_timestamps\n\n    # # Verify that parameters require gradients\n    # for name, param in model.named_parameters():\n    #     if not param.requires_grad:\n    #         param.requires_grad = True\n    #         print(f\"Warning: {name} does not require gradients\")\n\n    # # Print total number of trainable parameters\n    # trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    # print(f\"Total trainable parameters: {trainable_params}\")\n\n    def compute_metrics(pred):\n        pred_ids = pred.predictions\n\n        pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n        processor.tokenizer.predict_timestamps = processor_args.return_timestamps\n        pred_str = processor.tokenizer.batch_decode(pred_ids, \n                                                    skip_special_tokens=True, \n                                                    decode_with_timestamps=processor_args.return_timestamps)\n        # we do not want to group tokens when computing the metrics\n        label_str = processor.tokenizer.batch_decode(pred.label_ids, \n                                                     skip_special_tokens=True,\n                                                     decode_with_timestamps=processor_args.return_timestamps)\n\n        # pred_str = [model.tokenizer._normalize(pred) for pred in pred_str]\n        # label_str = [model.tokenizer._normalize(label) for label in label_str]\n        # # filtering step to only evaluate the samples that correspond to non-zero references:\n        # pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n        # label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n        pred_str = remove_punctuation(pred_str)\n        label_str = remove_punctuation(label_str)\n        # we do not want to group tokens when computing the metrics\n        wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n        # loss = pred.loss.mean().item() if pred.loss is not None else None\n        return {\"wer\": wer}\n\n    return Seq2SeqTrainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        tokenizer=processor.feature_extractor,\n        callbacks=[SavePeftModelCallback],\n    )\n\n    "}
{"type": "source_file", "path": "src/utils/mlflow_logging.py", "content": "import mlflow\nimport os\nimport json\nfrom abc import ABC, abstractmethod\nfrom peft import PeftModel\nfrom src.utils.logging import logger\nimport time\nfrom time import strftime, localtime\nfrom typing import Dict\nimport functools\nfrom dotenv import load_dotenv\n\n\nload_dotenv(dotenv_path=\"./.env\")\n\nLORA_LIST = [\"lora\", \"qlora\", \"olora\"]\n\n\ndef setup_mlflow():\n    mlflow_tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    logger.info(f\"MLflow tracking URI set to: {mlflow_tracking_uri}\")\n\n\ndef get_latest_checkpoint(base_path: str) -> str:\n    if os.path.exists(os.path.join(base_path, \"adapter_config.json\")):\n        return base_path\n    checkpoints = [d for d in os.listdir(base_path) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        return None\n    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n    return os.path.join(base_path, latest_checkpoint)\n\n\ndef extract_metrics_from_trainer_state(trainer_state_path: str) -> Dict[str, float]:\n    with open(trainer_state_path, \"r\") as f:\n        trainer_state = json.load(f)\n\n    if \"log_history\" in trainer_state and trainer_state[\"log_history\"]:\n        last_log = trainer_state[\"log_history\"]\n        metrics = {\n            k: v\n            for item in last_log\n            for k, v in item.items()\n            if isinstance(v, (int, float))\n        }\n        return metrics\n\n    return {}\n\n\nclass MLflowLogger(ABC):\n    @abstractmethod\n    def log_model(\n        self,\n        lora_model: PeftModel,\n        checkpoint_dir: str,\n        base_model_name: str,\n        model_name: str,\n    ):\n        raise NotImplementedError(\n            \"If you want to customize MLflow to track your work, please implement it here.\"\n        )\n\n\nclass WhisperLoRAMLflowLogger(MLflowLogger):\n    def log_model(\n        self,\n        experiment_name: str,\n        # lora_model: PeftModel,\n        checkpoint_dir: str,\n        base_model_name: str,\n        data_config: dict,\n        train_dataset: str,\n        # model_name: str,\n    ):\n        start_time = time.time()\n        run_name = (\n            f\"{experiment_name}_{strftime('%Y-%m-%d %H:%M:%S', localtime(start_time))}\"\n        )\n\n        with mlflow.start_run(run_name=run_name):\n            logger.info(\"Log Base model name\")\n            # Log base model name\n            mlflow.log_param(\"base_model_name\", base_model_name)\n            mlflow.log_param(\"train_dataset\", train_dataset)\n            for key, value in data_config.items():\n                mlflow.log_param(key, value)\n            # Get the latest checkpoint\n            latest_checkpoint = get_latest_checkpoint(checkpoint_dir)\n            if not latest_checkpoint:\n                logger.warning(\"No checkpoint found. Logging skipped.\")\n                return\n\n            # Log LoRA configuration\n            config_path = os.path.join(latest_checkpoint, \"adapter_config.json\")\n            if os.path.exists(config_path):\n                with open(config_path, \"r\") as f:\n                    config = json.load(f)\n                    mlflow.log_params(config)\n\n            # Extract and log metrics from trainer_state.json\n            trainer_state_path = os.path.join(latest_checkpoint, \"trainer_state.json\")\n            if os.path.exists(trainer_state_path):\n                metrics = extract_metrics_from_trainer_state(trainer_state_path)\n                mlflow.log_metrics(metrics)\n\n            # Log LoRA files\n            artifact_path = \"lora_files\"\n            lora_files = [\n                \"adapter_config.json\",\n                \"adapter_model.safetensors\",\n                \"added_tokens.json\",\n                \"all_results.json\",\n                \"merges.txt\",\n                \"normalizer.json\",\n                \"preprocessor_config.json\",\n                \"README.md\",\n                \"special_tokens_map.json\",\n                \"tokenizer_config.json\",\n                \"train_results.json\",\n                \"trainer_state.json\",\n                \"training_args.bin\",\n                \"vocab.json\",\n            ]\n\n            for file in lora_files:\n                file_path = os.path.join(latest_checkpoint, file)\n                if os.path.exists(file_path):\n                    mlflow.log_artifact(file_path, artifact_path)\n\n        mlflow.end_run(status=\"FINISHED\")\n        # Register model\n        # model_uri = f\"runs:/{mlflow.active_run().info.run_id}/{artifact_path}\"\n        # registered_model = mlflow.register_model(model_uri, model_name)\n\n        # logger.info(\n        #     f\"Latest LoRA checkpoint registered with name: {model_name}, version: {registered_model.version}\"\n        # )\n\n\nclass MLflowLoggerFactory:\n    @staticmethod\n    def get_logger(finetune_type: str) -> MLflowLogger:\n        if finetune_type in LORA_LIST:\n            return WhisperLoRAMLflowLogger().log_model\n        # Add more loggers for different model types if needed\n        raise ValueError(f\"Unsupported model type: {finetune_type}\")\n\n\ndef mlflow_logging(experiment_name: str, model_type: str):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if os.getenv(\"USE_MLFLOW\", \"false\").lower() == \"true\":\n                result = func(*args, **kwargs)\n                setup_mlflow()\n                mlflow.set_experiment(experiment_name)\n                mllogger = MLflowLoggerFactory.get_logger(model_type)\n                logger.info(\"Finish Training\")\n                if (\n                    isinstance(result, dict)\n                    and \"checkpoint_dir\" in result\n                    and \"base_model_name\" in result\n                    and \"data_config\" in result\n                    and \"train_dataset\" in result\n                ):\n                    checkpoint_dir = result[\"checkpoint_dir\"]\n                    base_model_name = result[\"base_model_name\"]\n                    data_config = result[\"data_config\"]\n                    train_dataset = result[\"train_dataset\"]\n                    # model_name = f\"{experiment_name}_{model_type}_model\"\n\n                    mllogger(\n                        experiment_name=experiment_name,\n                        checkpoint_dir=checkpoint_dir,\n                        base_model_name=base_model_name,\n                        data_config=data_config,\n                        train_dataset=train_dataset,\n                    )\n                    return result\n            else:\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n# def mlflow_logging(experiment_name):\n#     def decorator(func):\n#         @functools.wraps(func)\n#         def wrapper(*args, **kwargs):\n#             if os.getenv(\"USE_MLFLOW\", \"false\").lower() == \"true\":\n#                 setup_mlflow()\n#                 try:\n#                     mlflow.set_experiment(experiment_name)\n#                     # record start time\n#                     start_time = time.time()\n#                     run_name = f\"{experiment_name} {strftime('%Y-%m-%d %H:%M:%S', localtime(start_time))}\"\n#                     with mlflow.start_run(run_name=run_name):\n#                         logger.info(\n#                             f\"Starting MLflow run for experiment: {experiment_name}\"\n#                         )\n\n#                         # Execute the original function\n#                         result = func(*args, **kwargs)\n\n#                         # Log parameters\n#                         mlflow.log_params(kwargs)\n\n#                         # record start and end time\n#                         end_time = time.time()\n#                         duration = end_time - start_time\n#                         mlflow.log_metric(\"duration\", duration)\n\n#                         # record result\n#                         if isinstance(result, dict):\n#                             if \"train_metrics\" in result:\n#                                 mlflow.log_metrics(result[\"train_metrics\"])\n#                             if \"eval_metrics\" in result:\n#                                 mlflow.log_metrics(result[\"eval_metrics\"])\n\n#                         # Log model\n#                         if 'model' in result and isinstance(result['model'], torch.nn.Module):\n#                             mlflow.pytorch.log_model(result[\"model\"], \"model\")\n#                             # Register the model\n#                             model_version = mlflow.register_model(\n#                                 f\"runs:/{mlflow.active_run().info.run_id}/model\",\n#                                 f\"{experiment_name}_model\"\n#                             )\n#                             logger.info(f\"Model registered with version: {model_version.version}\")\n#                         logger.info(\n#                             f\"MLflow run completed for experiment: {experiment_name}\"\n#                         )\n#                         return result\n#                 except mlflow.exceptions.MlflowException as e:\n#                     logger.error(f\"Failed to set up MLflow experiment: {e}\")\n#                     logger.warning(\"Continuing without MLflow logging\")\n#                     return func(*args, **kwargs)\n#             else:\n#                 logger.info(\n#                     \"MLflow logging is disabled. Running function without MLflow tracking.\"\n#                 )\n#                 return func(*args, **kwargs)\n\n#         return wrapper\n\n#     return decorator\n"}
{"type": "source_file", "path": "src/trainers/merge_model.py", "content": "import argparse\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, AutoTokenizer\n# from transformers import (\n#     WhisperForConditionalGeneration,\n#     WhisperProcessor,\n#     WhisperTokenizer,\n# )\nfrom peft import PeftModel, PeftConfig\nfrom src.utils.logging import logger\n\n\ndef merge_and_save_whisper_model(base_model_name, peft_model_path, output_dir):\n    logger.info(\"[1/7] Loading PeftConfig\")\n    config = PeftConfig.from_pretrained(peft_model_path)\n\n    logger.info(\"[2/7] Loading base Whisper model\")\n    base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        base_model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n\n    logger.info(\"[3/7] Loading LoRA adapter\")\n    model = PeftModel.from_pretrained(base_model, peft_model_path, config=config)\n\n    logger.info(\"[4/7] Merging base model and adapter\")\n    model = model.merge_and_unload()\n\n    logger.info(\"[5/7] Saving merged model\")\n    model.save_pretrained(output_dir)\n\n    logger.info(\"[6/7] Saving WhisperProcessor\")\n    processor = AutoProcessor.from_pretrained(base_model_name)\n    processor.save_pretrained(output_dir)\n\n    logger.info(\"[7/7] Saving WhisperTokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    tokenizer.save_pretrained(output_dir)\n\n    logger.info(f\"Merged model, processor, and tokenizer saved to: {output_dir}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Merge Whisper LoRA and base model and save the result\"\n    )\n    parser.add_argument(\n        \"--base_model\",\n        type=str,\n        required=True,\n        help=\"Path or name of the base Whisper model\",\n    )\n    parser.add_argument(\n        \"--peft_model\", type=str, required=True, help=\"Path to the PEFT (LoRA) model\"\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Directory to save the merged model\",\n    )\n\n    args = parser.parse_args()\n\n    merge_and_save_whisper_model(args.base_model, args.peft_model, args.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
