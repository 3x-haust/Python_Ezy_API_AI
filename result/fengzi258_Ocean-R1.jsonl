{"repo_info": {"repo_name": "Ocean-R1", "repo_owner": "fengzi258", "repo_url": "https://github.com/fengzi258/Ocean-R1"}}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_counting_superclevr_5k.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nfrom tqdm import tqdm\nimport re\n\n\nimg_dir = \"./src/eval/data/Super_CLEVR\"\n\nMODEL_PATH_list = [\n    \"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\",\n]\n\nfor MODEL_PATH in MODEL_PATH_list:\n    print(MODEL_PATH)\n\n    ckpt_name = \"_\".join(MODEL_PATH.split(\"/\")[-3:])\n    BSZ = 64 if \"3B\" in MODEL_PATH else 48\n    OUTPUT_PATH=f\"./src/eval/data/counting_superclevr_5k_grpo_{ckpt_name}.json\"\n    PROMPT_PATH=\"./src/eval/data/super_clevr_test_5k.jsonl\"\n\n    #We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n\n    if \"Qwen2-VL\" in MODEL_PATH:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                )\n    elif \"Qwen2.5-VL\" in MODEL_PATH:\n        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                    )\n\n\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n    processor.tokenizer.padding_side = \"left\"\n\n    data = []\n    with open(PROMPT_PATH, \"r\") as f:\n        for line in f:\n            data.append(json.loads(line))\n\n\n    QUESTION_TEMPLATE = \"{Question} First output the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n\n    messages = []\n\n\n    for i in data:\n        message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"file://{img_dir}/{i['image_path']}\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": QUESTION_TEMPLATE.format(Question=i['question'])\n                }\n            ]\n        }]\n        messages.append(message)\n\n\n\n\n    all_outputs = []  # List to store all answers\n\n    # Process data in batches\n    for i in tqdm(range(0, len(messages), BSZ)):\n        batch_messages = messages[i:i + BSZ]\n        \n        # Preparation for inference\n        text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n        \n        image_inputs, video_inputs = process_vision_info(batch_messages)\n        inputs = processor(\n            text=text,\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(\"cuda\")\n\n        # Inference: Generation of the output\n        generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=256, do_sample=False)\n        \n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        batch_output_text = processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n        \n        all_outputs.extend(batch_output_text)\n        print(f\"Processed batch {i//BSZ + 1}/{(len(messages) + BSZ - 1)//BSZ}\")\n\n\n    def extract_number_answer(output_str):\n        # Try to find the number within <answer> tags, if can not find, return None\n        answer_pattern = r'<answer>\\s*(\\d+)\\s*</answer>'\n        match = re.search(answer_pattern, output_str)\n        \n        if match:\n            return int(match.group(1))\n        return None\n\n\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(data,all_outputs):\n        original_output = model_output\n        ground_truth = input_example['ground_truth']\n        model_answer = extract_number_answer(original_output)\n        \n        # Create a result dictionary for this example\n        result = {\n            'question': input_example,\n            'ground_truth': ground_truth,\n            'model_output': original_output,\n            'extracted_answer': model_answer\n        }\n        final_output.append(result)\n        \n        # Count correct answers\n        if model_answer is not None and model_answer == ground_truth:\n            correct_number += 1\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(data) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    output_path = OUTPUT_PATH\n    with open(output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2)\n\n    print(f\"Results saved to {output_path}\")\n\n\n\n\n\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_geoqa_multigpu.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nimport tqdm\nfrom math_verify import parse, verify\nimport argparse\nimport pandas as pd\nfrom torch.multiprocessing import Process, set_start_method, Manager\nfrom transformers.utils.logging import disable_progress_bar\ndisable_progress_bar()\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 1. get evaluation configuration <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef get_eval_config():\n    parser = argparse.ArgumentParser(description=\"Inference script for GeoQA evaluation.\")\n    parser.add_argument(\"--model_path\", required=True, type=str, help=\"Path to the model checkpoint (e.g., qwen2vl model or a fine-tuned model).\")\n    parser.add_argument(\"--batch_size\", default=4, type=int, help=\"Batch size for inference. Reduce if GPU OOM (default: 50).\")\n    parser.add_argument(\"--output_path\", required=True, type=str, help=\"Path to save inference result (e.g., JSON file).\")\n    parser.add_argument(\"--prompt_path\", required=True, type=str, help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    all_gpu = \",\".join(map(str, range(torch.cuda.device_count())))\n    parser.add_argument(\"--gpu_ids\", default=all_gpu, help=\"comma-separated list of GPU IDs to use\")\n    args = parser.parse_args()\n    return args\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 2. load testset <<<<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef prepare_test_messages(testset_path):\n    testset_data = pd.read_json(testset_path, lines=True).to_dict(orient=\"records\")\n    QUESTION_TEMPLATE = \"{Question} Output the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n    tested_messages = []\n    for i in testset_data:\n        message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"file://{i['image_path']}\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": QUESTION_TEMPLATE.format(Question=i['question'])\n                }\n            ]\n        }]\n        tested_messages.append(message)\n    return testset_data, tested_messages\n\n\n\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 3. use several GPUs to accelerate inference at testset <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef init_model(model_path, gpu_id):\n    \"\"\"init a model(args.model_path) on a specific gpu\"\"\"\n    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    model = Qwen2VLForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n        device_map=f\"cuda:{gpu_id}\",\n    )\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n    return model, processor\n\ndef answer_a_batch_question_qwen(batch_messages, model, processor):\n    \"\"\" let qwen answer a batch of questions \"\"\"\n    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]        \n    image_inputs, video_inputs = process_vision_info(batch_messages)\n    inputs = processor(\n        text=text,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(model.device)\n    \n    generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=1024) # do_sample=False\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    batch_output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    return batch_output_text\n\ndef infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):\n    \"\"\"init model on this single gpu and let it answer asign chunk of questions\"\"\"\n    model, processor = init_model(model_path, device_id)\n    \n    ### split batch\n    responses = []\n    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] \n               for start in range(0, len(chunk_of_tested_messages), batch_size)]\n\n    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f\"GPU {device_id} progress\", position=device_id, leave=False):\n        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)\n        \n        responses.extend(batch_output_text)\n    \n    results[device_id] = responses\n    return\n        \n        \ndef multi_gpu_inference(prompts, gpu_ids, model_path, batch_size):\n    \"\"\" let each gpu (along with a model) answer a chunk of questions \"\"\"\n    set_start_method(\"spawn\", force=True)\n    manager = Manager()\n    gpu_id2result = manager.dict()\n\n    gpu_ids = [int(gpu_id.strip()) for gpu_id in gpu_ids.split(',')]\n    num_gpus = len(gpu_ids)\n\n    chunk_size = len(prompts) // num_gpus\n    processes = []\n    for i, gpu_id in enumerate(gpu_ids):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size if i != num_gpus - 1 else len(prompts)\n        chunk = prompts[start_idx: end_idx]\n        process = Process(target=infer_on_single_gpu, args=(model_path, gpu_id, chunk, batch_size, gpu_id2result))\n        process.start()\n        processes.append(process)\n\n    # for process in tqdm.auto.tqdm(processes, desc=\"Inference progress\", position=num_gpus, leave=True):\n    for process in processes:\n        process.join()\n\n    all_predicts = []\n    for gpu_id in gpu_ids:\n        all_predicts.extend(gpu_id2result[gpu_id])\n\n    return all_predicts\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 4. compute metrics <<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef compute_metrics(testset_data, all_predicts):\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(testset_data, all_predicts):\n        original_output = model_output\n        ground_truth = input_example['ground_truth']\n        model_answer = parse(original_output) \n\n        # Count correct answers\n        if model_answer is not None and float(verify(model_answer,parse(ground_truth)))>0:\n            correct_number += 1\n            is_correct = True\n        else:\n            is_correct = False\n        \n        try:\n            result = {\n                'question': input_example,\n                'ground_truth': ground_truth,\n                'model_output': original_output,\n                'extracted_answer':str(model_answer[0]) if model_answer is not None else None,\n                'is_correct':is_correct\n            }\n\n        except Exception as e:\n            print(\"no answer parsed\",e,model_answer)\n            result = {\n                'question': input_example,\n                'ground_truth': ground_truth,\n                'model_output': original_output,\n                'extracted_answer':None,\n                'is_correct':is_correct\n            }\n\n\n\n        final_output.append(result)\n\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(tested_messages) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    with open(args.output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2, ensure_ascii=False)\n\n    print(f\"Results saved to {args.output_path}\")\n\n\n\nif __name__ == \"__main__\":\n    args = get_eval_config()\n    testset_data, tested_messages = prepare_test_messages(testset_path=args.prompt_path)\n    all_predicts = multi_gpu_inference(tested_messages, args.gpu_ids, args.model_path, args.batch_size)\n    compute_metrics(testset_data, all_predicts)\n\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_geoqa.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nfrom tqdm import tqdm\nimport re\nfrom math_verify import parse, verify\nfrom utils import default_accuracy_reward\n\nimg_dir = \"./src/eval/data/geoqa\"\n# BSZ=50 # reduce it if GPU OOM\nMODEL_PATH_list = [\n    \"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\",\n]\n\nfor MODEL_PATH in MODEL_PATH_list:\n    print(MODEL_PATH)\n\n    ckpt_name = \"_\".join(MODEL_PATH.split(\"/\")[-3:])\n    BSZ = 50 if \"3B\" in MODEL_PATH else 32\n    OUTPUT_PATH=f\"./src/eval/logs/eval/geoqa_grpo_{ckpt_name}.json\"\n    PROMPT_PATH=\"./src/eval/geoqa_test_prompts.jsonl\"\n\n    #We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n\n    if \"Qwen2-VL\" in MODEL_PATH:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                )\n    elif \"Qwen2.5-VL\" in MODEL_PATH:\n        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n                    MODEL_PATH,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=\"auto\",\n                    )\n\n\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n    processor.tokenizer.padding_side = \"left\"\n\n    data = []\n    with open(PROMPT_PATH, \"r\") as f:\n        for line in f:\n            data.append(json.loads(line))\n\n\n    QUESTION_TEMPLATE = \"{Question} Output the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n\n    messages = []\n\n    data = data\n\n    for i in data:\n        message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"file://{img_dir}{i['image_path'][1:]}\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": QUESTION_TEMPLATE.format(Question=i['question'])\n                }\n            ]\n        }]\n        messages.append(message)\n\n\n\n\n    all_outputs = []  # List to store all answers\n\n    # Process data in batches\n    for i in tqdm(range(0, len(messages), BSZ)):\n        batch_messages = messages[i:i + BSZ]\n        \n        # Preparation for inference\n        text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n        \n        image_inputs, video_inputs = process_vision_info(batch_messages)\n        inputs = processor(\n            text=text,\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(\"cuda\")\n\n        # Inference: Generation of the output\n        generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n        \n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        batch_output_text = processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n        \n        all_outputs.extend(batch_output_text)\n        print(f\"Processed batch {i//BSZ + 1}/{(len(messages) + BSZ - 1)//BSZ}\")\n\n\n\n\n\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(data,all_outputs):\n        original_output = model_output\n        ground_truth = input_example['ground_truth']\n        model_answer = parse(original_output) \n\n        # Count correct answers\n        if model_answer is not None and float(verify(model_answer,parse(ground_truth)))>0:\n            correct_number += 1\n            is_correct = True\n        else:\n            is_correct = False\n\n        ## minglingfeng fix\n        # reward, model_answer = default_accuracy_reward(original_output, ground_truth)\n        # if reward == 1.0:\n        #         correct_number += 1\n        #         is_correct = True\n        # else:\n        #     is_correct = False \n        \n        try:\n            result = {\n                'question': input_example,\n                'ground_truth': ground_truth,\n                'model_output': original_output,\n                'extracted_answer':str(model_answer) if model_answer is not None else None,\n                'is_correct':is_correct\n            }\n\n        except Exception as e:\n            print(\"no answer parsed\",e,model_answer)\n            result = {\n                'question': input_example,\n                'ground_truth': ground_truth,\n                'model_output': original_output,\n                'extracted_answer':None,\n                'is_correct':is_correct\n            }\n\n\n\n        final_output.append(result)\n\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(data) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    output_path = OUTPUT_PATH\n    with open(output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2, ensure_ascii=False)\n\n    print(f\"Results saved to {output_path}\")\n\n\n\n\n\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_mmmu.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nfrom tqdm import tqdm\nimport re\nfrom math_verify import parse, verify\nimport os\nimport datasets\nfrom utils import default_accuracy_reward\nimport copy\nimport argparse\n\nmax_new_tokens = 4096\n\ndef run():\n    # Set up the argument parser\n    parser = argparse.ArgumentParser(description=\"Receive deepen model's args\")\n    parser.add_argument(\"--model_path_list\", default=\"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\", type=str, help=\"model path\")\n    parser.add_argument(\"--output_dir\", default='./src/eval/logs/eval/', type=str, help=\"save path\")\n    parser.add_argument(\"--dataset_name\", default=\"MMMU/MMMU\", type=str, help=\"dataset name\")\n    parser.add_argument(\"--ds_split\", default=\"validation\", type=str, help=\"test dataset name\")\n    parser.add_argument(\"--batch_size\", default=16, type=int, help=\"batch size\")\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    MODEL_PATH_list = args.model_path_list.split(\",\")\n\n    output_dir = f\"{args.output_dir}\"\n    os.makedirs(output_dir, exist_ok = True)\n\n    dataset_name = args.dataset_name\n    ds_split = args.ds_split\n    \n    sub_dataset_name_list = ['Accounting', 'Agriculture', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Geography', 'Manage', 'Materials', 'Math', 'Mechanical_Engineering', 'Public_Health', 'Sociology', 'Architecture_and_Engineering', 'Art', 'Computer_Science', 'Energy_and_Power', 'Finance', 'History', 'Literature', 'Marketing', 'Music', 'Pharmacy', 'Physics', 'Psychology']\n\n\n    for MODEL_PATH in MODEL_PATH_list:\n        print(MODEL_PATH)\n        BSZ = args.batch_size if \"3B\" in MODEL_PATH else args.batch_size//2\n\n        ckpt_name = \"_\".join(MODEL_PATH.split(\"/\")[-3:])\n        \n        OUTPUT_PATH=f\"{output_dir}/mmmu_{ds_split}_grpo_{ckpt_name}.json\"\n    \n        #We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n        try:\n            if \"Qwen2-VL\" in MODEL_PATH:\n                model = Qwen2VLForConditionalGeneration.from_pretrained(\n                            MODEL_PATH,\n                            torch_dtype=torch.bfloat16,\n                            attn_implementation=\"flash_attention_2\",\n                            device_map=\"auto\",\n                        )\n            elif \"Qwen2.5-VL\" in MODEL_PATH:\n                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                            MODEL_PATH,\n                            torch_dtype=torch.bfloat16,\n                            attn_implementation=\"flash_attention_2\",\n                            device_map=\"auto\",\n                        )\n            else:\n                model = AutoModelForCausalLM.from_pretrained(\n                            MODEL_PATH,\n                            torch_dtype=torch.bfloat16,\n                            attn_implementation=\"flash_attention_2\",\n                            device_map=\"auto\",\n                            )\n\n\n\n            # default processer\n            processor = AutoProcessor.from_pretrained(MODEL_PATH)\n            # processor.tokenizer.padding_side = \"left\"\n        except:\n            continue\n\n        Origin_QUESTION_TEMPLATE = \"{Question}\"\n        SYSTEM_PROMPT = \"Please solve the problem step by step and then answer it. If it is a multiple choice question, only provide the correct option letter, e.g., A, B, C, D, at the end.\\n\"\n        QUESTION_TEMPLATE = \"Question: {Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n\n        messages = []\n        data = []\n        for sub_dataset_name in sub_dataset_name_list:\n\n            try:\n                local_data_dir = \"./src/eval/data\"\n                dataset_path =  os.path.join(local_data_dir, dataset_name, sub_dataset_name)\n                ds = datasets.load_from_disk(dataset_path)\n            except:\n                ds = datasets.load_dataset(dataset_name, sub_dataset_name)\n\n            sub_ds = ds[ds_split]\n            for i in sub_ds:\n                question = i[\"question\"]\n                answer = i[\"answer\"]\n                options = i[\"options\"]\n                if len(options) > 0:\n                    choices = [f\"{key}. {value}\" for key,value in zip(\"ABCDEFG\", eval(options))]\n                    if len(choices)>0:\n                        question = question + \"\\nChoices: \" + \"\\n\".join(choices)\n                        i[\"question\"] = question\n                \n                images = []\n                for k in range(1,8):\n                    if i[f\"image_{k}\"]:\n                        images.append(i[f\"image_{k}\"])\n                \n                message = [{\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": SYSTEM_PROMPT  + QUESTION_TEMPLATE.format(Question=question)\n                        }\n                    ]\n                }]\n                \n                for image in images:\n                    message[0][\"content\"].append(\n                            {\n                            \"type\": \"image\", \n                            \"image\": image,\n                        }\n                    )\n                messages.append(message)\n                data.append(i)\n\n\n        # minglingfeng fix\n        # max_new_tokens = 2048 #2048 if \"checkpoint\" in MODEL_PATH else 1024\n        repetition_penalty = 1.0 #2.0 if \"checkpoint\" in MODEL_PATH else 1.0\n        default_eval_kwargs = dict(\n                    max_new_tokens=max_new_tokens,\n                    use_cache=True,\n                    do_sample=True,\n                    top_p=0.001,\n                    top_k=1,\n                    temperature=0.01,\n                    repetition_penalty=repetition_penalty,\n                )\n\n\n        all_outputs = []  # List to store all answers\n\n        # Process data in batches\n        for i in tqdm(range(0, len(messages), BSZ)):\n            batch_messages = messages[i:i + BSZ]\n            batch_data = data[i:i + BSZ]\n            \n            # Preparation for inference\n            text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n            \n            image_inputs, video_inputs = process_vision_info(batch_messages)\n            inputs = processor(\n                text=text,\n                images=image_inputs,\n                videos=video_inputs,\n                padding=True,\n                padding_side=\"left\",\n                return_tensors=\"pt\",\n            )\n            inputs = inputs.to(\"cuda\")\n\n            # Inference: Generation of the output\n            generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=max_new_tokens, do_sample=False)\n            # generated_ids = model.generate(**inputs, **default_eval_kwargs)\n            \n            generated_ids_trimmed = [\n                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n            ]\n            batch_output_text = processor.batch_decode(\n                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )\n            \n            all_outputs.extend(batch_output_text)\n            print(f\"\\033[31m{batch_messages[0]}\\033[0m\")\n            print(f\"\\033[32m{batch_output_text[0]}\\033[0m\")\n            print(f\"ground_truth: {batch_data[0]['answer']}\")\n                \n            print(f\"Processed batch {i//BSZ + 1}/{(len(messages) + BSZ - 1)//BSZ}\")\n\n\n\n\n\n        final_output = []\n        correct_number = 0\n\n        for input_example, model_output in zip(data,all_outputs):\n            original_output = model_output\n            ground_truth = input_example['answer']\n            reward,model_answer = default_accuracy_reward(original_output, ground_truth)\n            if reward == 1.0:\n                correct_number += 1\n                is_correct = True\n            else:\n                is_correct = False\n            \n            result = copy.deepcopy(input_example)\n            if isinstance(model_answer,list):\n                model_answer = model_answer[0]\n            for k in range(1,8):\n                result[f\"image_{k}\"] = None\n            try:\n                result['ground_truth'] = ground_truth\n                result['model_output'] =  original_output\n                result['extracted_answer'] = str(model_answer) if model_answer else \"\"\n                result['is_correct'] = is_correct\n\n            except Exception as e:\n                print(\"no answer parsed\",e,model_answer)\n                result['ground_truth'] = ground_truth\n                result['model_output'] =  original_output\n                result['extracted_answer'] = None\n                result['is_correct'] = is_correct\n\n\n\n\n            final_output.append(result)\n\n\n        # Calculate and print accuracy\n        accuracy = correct_number / len(data) * 100\n        print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n        # Save results to a JSON file\n        output_path = OUTPUT_PATH.format(ds_split=ds_split, ckpt_name=ckpt_name)\n        with open(output_path, \"w\") as f:\n            json.dump({\n                'accuracy': accuracy,\n                'results': final_output\n            }, f, indent=2, ensure_ascii=False)\n\n        print(f\"Results saved to {output_path}\")\n\n\nif __name__ == \"__main__\":\n    run()\n\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_mathvision_multigpu.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nimport tqdm\nfrom math_verify import parse, verify\nimport argparse\nimport pandas as pd\nfrom torch.multiprocessing import Process, set_start_method, Manager\nfrom transformers.utils.logging import disable_progress_bar\ndisable_progress_bar()\n\nimport datasets\nimport os\nimport copy\nfrom utils import default_accuracy_reward\n\nmax_new_tokens = 4096\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 1. get evaluation configuration <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef get_eval_config():\n    parser = argparse.ArgumentParser(description=\"Inference script for GeoQA evaluation.\")\n    parser.add_argument(\"--model_path_list\", type=str, default=\"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\", help=\"Path to the model checkpoint (e.g., qwen2vl model or a fine-tuned model).\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size for inference. Reduce if GPU OOM (default: 50).\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"ã€‚/src/eval/logs/eval/\", help=\"Path to save inference result (e.g., JSON file).\")\n    parser.add_argument(\"--dataset_name\",  type=str, default=\"MathLLMs/MathVision\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    parser.add_argument(\"--data_split\",  type=str, default=\"test\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    all_gpu = \",\".join(map(str, range(torch.cuda.device_count())))\n    parser.add_argument(\"--gpu_ids\", default=all_gpu, help=\"comma-separated list of GPU IDs to use\")\n    args = parser.parse_args()\n    return args\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 2. load testset <<<<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef prepare_test_messages(dataset_name, data_split):\n    try:\n        local_data_dir = \"./src/eval/data\"\n        dataset_path =  os.path.join(local_data_dir, dataset_name)\n        ds = datasets.load_from_disk(dataset_path)\n    except:\n        ds = datasets.load_dataset(dataset_name)\n    \n    SYSTEM_PROMPT = \"\"\"Please solve the problem step by step and put your answer in one \"\\\\boxed{}\". If it is a multiple choice question, only one letter is allowed in the \"\\\\boxed{}\".\\n\"\"\"\n    QUESTION_TEMPLATE = \"{Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n    \n    tested_messages = []\n    test_data = []\n\n    sub_ds = ds[data_split]\n    for example in sub_ds:\n        question = example[\"question\"]\n        answer = example[\"answer\"]\n        options = \"\"\n        if len(example['options']) > 0 and ''.join(example['options']) != 'ABCDE':\n            assert len(example['options']) == 5, example\n            options = f\"(A) {example['options'][0]}\\n(B) {example['options'][1]}\\n(C) {example['options'][2]}\\n(D) {example['options'][3]}\\n(E) {example['options'][4]}\\n\"\n\n        input_q = f\"{question}\\n{options}\"\n        example[\"input_q\"] = input_q\n\n        message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": example[\"decoded_image\"],\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": SYSTEM_PROMPT + \"Question: \" + QUESTION_TEMPLATE.format(Question=input_q),\n                }\n            ]\n        }]\n        tested_messages.append(message)\n        test_data.append(example)\n    return test_data, tested_messages\n\n\n\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 3. use several GPUs to accelerate inference at testset <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef init_model(model_path, gpu_id):\n    \"\"\"init a model(args.model_path) on a specific gpu\"\"\"\n    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    if \"Qwen2-VL\" in model_path:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    elif \"Qwen2.5-VL\" in model_path:\n        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                    )\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n    return model, processor\n\ndef answer_a_batch_question_qwen(batch_messages, model, processor):\n    \"\"\" let qwen answer a batch of questions \"\"\"\n    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]        \n    image_inputs, video_inputs = process_vision_info(batch_messages)\n    inputs = processor(\n        text=text,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        padding_side=\"left\",\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(model.device)\n    \n    generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=max_new_tokens, do_sample=False) # do_sample=False\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    batch_output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    # print(f\"\\033[31m{batch_messages[0]}\\033[0m\")\n    # print(f\"\\033[32m{batch_output_text[0]}\\033[0m\")\n    return batch_output_text\n\ndef infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):\n    \"\"\"init model on this single gpu and let it answer asign chunk of questions\"\"\"\n    model, processor = init_model(model_path, device_id)\n    \n    ### split batch\n    responses = []\n    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] \n               for start in range(0, len(chunk_of_tested_messages), batch_size)]\n\n    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f\"GPU {device_id} progress\", position=device_id, leave=False):\n        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)\n        \n        responses.extend(batch_output_text)\n    \n    results[device_id] = responses\n    return\n        \n        \ndef multi_gpu_inference(prompts, gpu_ids, model_path, batch_size):\n    \"\"\" let each gpu (along with a model) answer a chunk of questions \"\"\"\n    set_start_method(\"spawn\", force=True)\n    manager = Manager()\n    gpu_id2result = manager.dict()\n\n    gpu_ids = [int(gpu_id.strip()) for gpu_id in gpu_ids.split(',')]\n    num_gpus = len(gpu_ids)\n\n    chunk_size = len(prompts) // num_gpus\n    processes = []\n    for i, gpu_id in enumerate(gpu_ids):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size if i != num_gpus - 1 else len(prompts)\n        chunk = prompts[start_idx: end_idx]\n        process = Process(target=infer_on_single_gpu, args=(model_path, gpu_id, chunk, batch_size, gpu_id2result))\n        process.start()\n        processes.append(process)\n\n    # for process in tqdm.auto.tqdm(processes, desc=\"Inference progress\", position=num_gpus, leave=True):\n    for process in processes:\n        process.join()\n\n    all_predicts = []\n    for gpu_id in gpu_ids:\n        all_predicts.extend(gpu_id2result[gpu_id])\n\n    return all_predicts\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 4. compute metrics <<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef compute_metrics(testset_data, all_predicts, args):\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(testset_data, all_predicts):\n        original_output = model_output\n        ground_truth = input_example['answer']\n        reward,model_answer = default_accuracy_reward(original_output, ground_truth)\n        if reward == 1.0:\n            correct_number += 1\n            is_correct = True\n        else:\n            is_correct = False\n            \n        \n        result = copy.deepcopy(input_example)\n        if isinstance(model_answer,list):\n            model_answer = model_answer[0]\n        try:\n            result[\"decoded_image\"] = \"\" #base64.b64encode(result[\"decoded_image\"])\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = str(model_answer) if model_answer else \"\"\n            result['is_correct'] = is_correct\n\n        except Exception as e:\n            print(\"no answer parsed\",e,model_answer)\n            result[\"decoded_image\"] = \"\"#base64.b64encode(result[\"decoded_image\"])\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = None\n            result['is_correct'] = is_correct\n\n        final_output.append(result)\n\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(testset_data) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    ckpt_name = \"_\".join(args.model_path.split(\"/\")[-3:])\n    output_path=f\"{args.output_dir}/{args.dataset_name.replace('/','_')}_{args.data_split}_grpo_{ckpt_name}.json\"\n    \n    with open(output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2, ensure_ascii=False)\n\n    print(f\"Results saved to {output_path}\")\n\n\n\nif __name__ == \"__main__\":\n    args = get_eval_config()\n    print(args)\n    model_path_list = args.model_path_list.split(\",\")\n    for model_path in model_path_list:\n        args.model_path = model_path\n        print(args.model_path)\n        testset_data, tested_messages = prepare_test_messages(args.dataset_name, args.data_split)\n        all_predicts = multi_gpu_inference(tested_messages, args.gpu_ids, args.model_path, args.batch_size)\n        compute_metrics(testset_data, all_predicts, args)\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_olympiadbench_multigpu.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nimport tqdm\nfrom math_verify import parse, verify\nimport argparse\nimport pandas as pd\nfrom torch.multiprocessing import Process, set_start_method, Manager\nfrom transformers.utils.logging import disable_progress_bar\ndisable_progress_bar()\n\nimport datasets\nimport os\nimport copy\nfrom utils import default_accuracy_reward\n\nmax_new_tokens = 4096\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 1. get evaluation configuration <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef get_eval_config():\n    parser = argparse.ArgumentParser(description=\"Inference script for GeoQA evaluation.\")\n    parser.add_argument(\"--model_path_list\", type=str, default=\"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\", help=\"Path to the model checkpoint (e.g., qwen2vl model or a fine-tuned model).\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size for inference. Reduce if GPU OOM (default: 50).\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./src/eval/logs/eval/\", help=\"Path to save inference result (e.g., JSON file).\")\n    parser.add_argument(\"--dataset_name\",  type=str, default=\"lmms-lab/OlympiadBench\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    parser.add_argument(\"--data_split\",  type=str, default=\"test_en\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    all_gpu = \",\".join(map(str, range(torch.cuda.device_count())))\n    parser.add_argument(\"--gpu_ids\", default=all_gpu, help=\"comma-separated list of GPU IDs to use\")\n    args = parser.parse_args()\n    return args\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 2. load testset <<<<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef prepare_test_messages(dataset_name, data_split):\n    try:\n        local_data_dir = \"./src/eval/data\"\n        dataset_path =  os.path.join(local_data_dir, dataset_name.replace(\"/\",\"_\"))\n        ds = datasets.load_from_disk(dataset_path)\n    except:\n        ds = datasets.load_dataset(dataset_name)\n    \n    SYSTEM_PROMPT = \"\"\"Please solve the problem step by step and put your answer in one \"\\\\boxed{}\". If it is a multiple choice question, only one letter is allowed in the \"\\\\boxed{}\".\\n\"\"\"\n    QUESTION_TEMPLATE = \"{Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n    \n    tested_messages = []\n    test_data = []\n\n    sub_ds = ds[data_split]\n    for example in sub_ds:\n        question = example[\"question\"]\n        final_answer = example[\"final_answer\"]\n        if not final_answer:\n                continue\n        context = example[\"context\"]\n        if context:\n            question = context + \"\\n\\n\" + question\n                \n\n        message = [{\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": QUESTION_TEMPLATE.format(Question=question)\n                    }\n                ]\n            }]\n\n        images = example[\"images\"]\n        for image in images:\n            message[0][\"content\"].append(\n                    {\n                    \"type\": \"image\", \n                    \"image\": image,\n                }\n            )\n\n        tested_messages.append(message)\n        test_data.append(example)\n    return test_data, tested_messages\n\n\n\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 3. use several GPUs to accelerate inference at testset <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef init_model(model_path, gpu_id):\n    \"\"\"init a model(args.model_path) on a specific gpu\"\"\"\n    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    if \"Qwen2-VL\" in model_path:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    elif \"Qwen2.5-VL\" in model_path:\n        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                    )\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n    return model, processor\n\ndef answer_a_batch_question_qwen(batch_messages, model, processor):\n    \"\"\" let qwen answer a batch of questions \"\"\"\n    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]        \n    image_inputs, video_inputs = process_vision_info(batch_messages)\n    inputs = processor(\n        text=text,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        padding_side=\"left\",\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(model.device)\n    \n    generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=max_new_tokens, do_sample=False) # do_sample=False\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    batch_output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    # print(f\"\\033[31m{batch_messages[0]}\\033[0m\")\n    # print(f\"\\033[32m{batch_output_text[0]}\\033[0m\")\n    return batch_output_text\n\ndef infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):\n    \"\"\"init model on this single gpu and let it answer asign chunk of questions\"\"\"\n    model, processor = init_model(model_path, device_id)\n    \n    ### split batch\n    responses = []\n    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] \n               for start in range(0, len(chunk_of_tested_messages), batch_size)]\n\n    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f\"GPU {device_id} progress\", position=device_id, leave=False):\n        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)\n        \n        responses.extend(batch_output_text)\n    \n    results[device_id] = responses\n    return\n        \n        \ndef multi_gpu_inference(prompts, gpu_ids, model_path, batch_size):\n    \"\"\" let each gpu (along with a model) answer a chunk of questions \"\"\"\n    set_start_method(\"spawn\", force=True)\n    manager = Manager()\n    gpu_id2result = manager.dict()\n\n    gpu_ids = [int(gpu_id.strip()) for gpu_id in gpu_ids.split(',')]\n    num_gpus = len(gpu_ids)\n\n    chunk_size = len(prompts) // num_gpus\n    processes = []\n    for i, gpu_id in enumerate(gpu_ids):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size if i != num_gpus - 1 else len(prompts)\n        chunk = prompts[start_idx: end_idx]\n        process = Process(target=infer_on_single_gpu, args=(model_path, gpu_id, chunk, batch_size, gpu_id2result))\n        process.start()\n        processes.append(process)\n\n    # for process in tqdm.auto.tqdm(processes, desc=\"Inference progress\", position=num_gpus, leave=True):\n    for process in processes:\n        process.join()\n\n    all_predicts = []\n    for gpu_id in gpu_ids:\n        all_predicts.extend(gpu_id2result[gpu_id])\n\n    return all_predicts\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 4. compute metrics <<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef compute_metrics(testset_data, all_predicts, args):\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(testset_data, all_predicts):\n        original_output = model_output\n        ground_truth = \",\".join(input_example['final_answer'])\n        reward,model_answer = default_accuracy_reward(original_output, ground_truth)\n        if reward == 1.0:\n            correct_number += 1\n            is_correct = True\n        else:\n            is_correct = False\n        \n        if isinstance(model_answer,list):\n            model_answer = model_answer[0]\n        result = copy.deepcopy(input_example)\n        result[\"images\"] = None\n        try:\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = str(model_answer) if model_answer else \"\"\n            result['is_correct'] = is_correct\n\n        except Exception as e:\n            print(\"no answer parsed\",e,model_answer)\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = None\n            result['is_correct'] = is_correct\n\n        final_output.append(result)\n\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(testset_data) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    ckpt_name = \"_\".join(args.model_path.split(\"/\")[-3:])\n    output_path=f\"{args.output_dir}/{args.dataset_name.replace('/','_')}_{args.data_split}_grpo_{ckpt_name}_seqlen{max_new_tokens}.json\"\n    \n    with open(output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2, ensure_ascii=False)\n\n    print(f\"Results saved to {output_path}\")\n\n\n\nif __name__ == \"__main__\":\n    args = get_eval_config()\n    print(args)\n    model_path_list = args.model_path_list.split(\",\")\n    for model_path in model_path_list:\n        args.model_path = model_path\n        print(args.model_path)\n        testset_data, tested_messages = prepare_test_messages(args.dataset_name, args.data_split)\n        all_predicts = multi_gpu_inference(tested_messages, args.gpu_ids, args.model_path, args.batch_size)\n        compute_metrics(testset_data, all_predicts, args)\n        \n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_mathverse_multigpu.py", "content": "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration, AutoModelForCausalLM\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nimport tqdm\nfrom math_verify import parse, verify\nimport argparse\nimport pandas as pd\nfrom torch.multiprocessing import Process, set_start_method, Manager\nfrom transformers.utils.logging import disable_progress_bar\ndisable_progress_bar()\n\nimport datasets\nimport os\nimport copy\nfrom utils import default_accuracy_reward\n\n# \"xyliu6/mathverse_vision_dominant\",\n# \"xyliu6/mathverse_4k\"\n# dataset_name = \"xyliu6/mathverse_vision_dominant\"\n# sub_ds_name = \"train\"\nmax_new_tokens = 2048\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 1. get evaluation configuration <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef get_eval_config():\n    parser = argparse.ArgumentParser(description=\"Inference script for GeoQA evaluation.\")\n    parser.add_argument(\"--model_path_list\", type=str, default=\"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\", help=\"Path to the model checkpoint (e.g., qwen2vl model or a fine-tuned model).\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size for inference. Reduce if GPU OOM (default: 50).\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./src/eval/logs/eval/\", help=\"Path to save inference result (e.g., JSON file).\")\n    parser.add_argument(\"--dataset_name\",  type=str, default=\"xyliu6/mathverse_4k\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    parser.add_argument(\"--data_split\",  type=str, default=\"train\", help=\"Path to the prompts JSONL file for GeoQA evaluation.\")\n    all_gpu = \",\".join(map(str, range(torch.cuda.device_count())))\n    parser.add_argument(\"--gpu_ids\", default=all_gpu, help=\"comma-separated list of GPU IDs to use\")\n    args = parser.parse_args()\n    return args\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 2. load testset <<<<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\ndef prepare_test_messages(dataset_name, data_split):\n    try:\n        local_data_dir = \"./src/eval/data\"\n        dataset_path =  os.path.join(local_data_dir, dataset_name)\n        ds = datasets.load_from_disk(dataset_path)\n    except:\n        ds = datasets.load_dataset(dataset_name)\n    \n    SYSTEM_PROMPT = \"\"\"Please solve the problem step by step and put your answer in one \"\\\\boxed{}\". If it is a multiple choice question, only one letter is allowed in the \"\\\\boxed{}\".\\n\"\"\"\n    QUESTION_TEMPLATE = \"{Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\n    \n    tested_messages = []\n    test_data = []\n\n    sub_ds = ds[data_split]\n    for example in sub_ds:\n        question = example[\"problem\"]\n\n        message = [{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": example[\"image\"],\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": SYSTEM_PROMPT +  QUESTION_TEMPLATE.format(Question=question),\n                }\n            ]\n        }]\n        tested_messages.append(message)\n        test_data.append(example)\n    return test_data, tested_messages\n\n\n\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>> 3. use several GPUs to accelerate inference at testset <<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef init_model(model_path, gpu_id):\n    \"\"\"init a model(args.model_path) on a specific gpu\"\"\"\n    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    if \"Qwen2-VL\" in model_path:\n        model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    elif \"Qwen2.5-VL\" in model_path:\n        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.bfloat16,\n                    attn_implementation=\"flash_attention_2\",\n                    device_map=f\"cuda:{gpu_id}\",\n                    )\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n    return model, processor\n\ndef answer_a_batch_question_qwen(batch_messages, model, processor):\n    \"\"\" let qwen answer a batch of questions \"\"\"\n    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]        \n    image_inputs, video_inputs = process_vision_info(batch_messages)\n    inputs = processor(\n        text=text,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        padding_side=\"left\",\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(model.device)\n    \n    generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=max_new_tokens, do_sample=False) # do_sample=False\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    batch_output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    # print(f\"\\033[31m{batch_messages[0]}\\033[0m\")\n    # print(f\"\\033[32m{batch_output_text[0]}\\033[0m\")\n    return batch_output_text\n\ndef infer_on_single_gpu(model_path, device_id, chunk_of_tested_messages, batch_size, results=None):\n    \"\"\"init model on this single gpu and let it answer asign chunk of questions\"\"\"\n    model, processor = init_model(model_path, device_id)\n    \n    ### split batch\n    responses = []\n    batch_messages_list = [chunk_of_tested_messages[start: start + batch_size] \n               for start in range(0, len(chunk_of_tested_messages), batch_size)]\n\n    for batch_messages in tqdm.auto.tqdm(batch_messages_list, desc=f\"GPU {device_id} progress\", position=device_id, leave=False):\n        batch_output_text = answer_a_batch_question_qwen(batch_messages, model, processor)\n        \n        responses.extend(batch_output_text)\n    \n    results[device_id] = responses\n    return\n        \n        \ndef multi_gpu_inference(prompts, gpu_ids, model_path, batch_size):\n    \"\"\" let each gpu (along with a model) answer a chunk of questions \"\"\"\n    set_start_method(\"spawn\", force=True)\n    manager = Manager()\n    gpu_id2result = manager.dict()\n\n    gpu_ids = [int(gpu_id.strip()) for gpu_id in gpu_ids.split(',')]\n    num_gpus = len(gpu_ids)\n\n    chunk_size = len(prompts) // num_gpus\n    processes = []\n    for i, gpu_id in enumerate(gpu_ids):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size if i != num_gpus - 1 else len(prompts)\n        chunk = prompts[start_idx: end_idx]\n        process = Process(target=infer_on_single_gpu, args=(model_path, gpu_id, chunk, batch_size, gpu_id2result))\n        process.start()\n        processes.append(process)\n\n    # for process in tqdm.auto.tqdm(processes, desc=\"Inference progress\", position=num_gpus, leave=True):\n    for process in processes:\n        process.join()\n\n    all_predicts = []\n    for gpu_id in gpu_ids:\n        all_predicts.extend(gpu_id2result[gpu_id])\n\n    return all_predicts\n\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n# >>>>>>>>>> 4. compute metrics <<<<<<<<<<<\n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\ndef compute_metrics(testset_data, all_predicts, args):\n    final_output = []\n    correct_number = 0\n\n    for input_example, model_output in zip(testset_data, all_predicts):\n        original_output = model_output\n        ground_truth = input_example['solution']\n        reward,model_answer = default_accuracy_reward(original_output, ground_truth)\n        if reward == 1.0:\n            correct_number += 1\n            is_correct = True\n        else:\n            is_correct = False\n            \n        \n        result = copy.deepcopy(input_example)\n        result[\"image\"] = None\n        if isinstance(model_answer,list):\n            model_answer = model_answer[0]\n        try:\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = str(model_answer) if model_answer else \"\"\n            result['is_correct'] = is_correct\n\n        except Exception as e:\n            print(\"no answer parsed\",e,model_answer)\n            result['ground_truth'] = ground_truth\n            result['model_output'] =  original_output\n            result['extracted_answer'] = None\n            result['is_correct'] = is_correct\n\n        final_output.append(result)\n\n\n    # Calculate and print accuracy\n    accuracy = correct_number / len(testset_data) * 100\n    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n\n    # Save results to a JSON file\n    ckpt_name = \"_\".join(args.model_path.split(\"/\")[-3:])\n    output_path=f\"{args.output_dir}/{args.dataset_name.replace('/','_')}_{args.data_split}_grpo_{ckpt_name}.json\"\n    \n    with open(output_path, \"w\") as f:\n        json.dump({\n            'accuracy': accuracy,\n            'results': final_output\n        }, f, indent=2, ensure_ascii=False)\n\n    print(f\"Results saved to {output_path}\")\n\n\n\nif __name__ == \"__main__\":\n    args = get_eval_config()\n    print(args)\n    model_path_list = args.model_path_list.split(\",\")\n    for model_path in model_path_list:\n        try:\n            args.model_path = model_path\n            print(args.model_path)\n            testset_data, tested_messages = prepare_test_messages(args.dataset_name, args.data_split)\n            all_predicts = multi_gpu_inference(tested_messages, args.gpu_ids, args.model_path, args.batch_size)\n            compute_metrics(testset_data, all_predicts, args)\n        except Exception as e:\n            print(e)\n            continue\n"}
{"type": "test_file", "path": "src/eval/test_qwen2d5vl_rec.py", "content": "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nimport json\nfrom tqdm import tqdm\nimport re\nimport os\nfrom pprint import pprint\nimport random\n\n\"\"\"\nCopy from: https://github.com/om-ai-lab/VLM-R1/blob/main/src/eval/test_rec_r1.py\n\"\"\"\n\nOrig_QUESTION_TEMPLATE = \"{Question}\"\nQUESTION_TEMPLATE = \"{Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n\nBSZ=32\nsample_num = None #500\n\nis_baseline = False\n\nMODEL_PATH_list = [\n   \"/global_data/mllm/minglingfeng/models/Qwen2.5-VL-3B-Instruct\",\n]\n\nfor MODEL_PATH in MODEL_PATH_list:\n\n    if \"Qwen2.5-VL-3B-Instruct\" in MODEL_PATH:\n        is_baseline = True\n\n    ckpt_name = \"_\".join(MODEL_PATH.split(\"/\")[-2:])\n\n    OUTPUT_PATH=\"./src/eval/logs/eval/{CKPT}-{DATASET}\" \n    DATA_ROOT = \"./src/eval/data/coco/rec_jsons_processed\"\n\n    TEST_DATASETS = ['refcoco_val', 'refcocop_val', 'refcocog_val']\n    IMAGE_ROOT = \"./src/eval/data/coco\"\n\n    random.seed(42)\n\n    #We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n        device_map=\"auto\",\n    )\n\n    # default processer\n    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n    # processor.tokenizer.padding_side = \"left\"\n\n    def extract_bbox_answer(content):\n        # Try to find the bbox within <answer> tags, if can not find, return [0, 0, 0, 0]\n        answer_tag_pattern = r'<answer>(.*?)</answer>'\n        # bbox_pattern = r'\\{.*\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)]\\s*.*\\}'\n        bbox_pattern = r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)]'\n        content_answer_match = re.search(answer_tag_pattern, content, re.DOTALL)\n        if content_answer_match:\n            content_answer = content_answer_match.group(1).strip()\n            bbox_match = re.search(bbox_pattern, content_answer)\n            if bbox_match:\n                bbox = [int(bbox_match.group(1)), int(bbox_match.group(2)), int(bbox_match.group(3)), int(bbox_match.group(4))]\n                x1, y1, x2, y2 = bbox\n                return bbox, False\n        return [0, 0, 0, 0], False\n\n    def iou(box1, box2):\n        inter_x1 = max(box1[0], box2[0])\n        inter_y1 = max(box1[1], box2[1])\n        inter_x2 = min(box1[2]-1, box2[2]-1)\n        inter_y2 = min(box1[3]-1, box2[3]-1)\n        if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n            inter = (inter_x2-inter_x1+1)*(inter_y2-inter_y1+1)\n        else:\n            inter = 0\n        union = (box1[2]-box1[0])*(box1[3]-box1[1]) + (box2[2]-box2[0])*(box2[3]-box2[1]) - inter\n        return float(inter)/union\n\n\n    for ds in TEST_DATASETS:\n        print(f\"Processing {ds}...\")\n        ds_path = os.path.join(DATA_ROOT, f\"{ds}.json\")\n        data = json.load(open(ds_path, \"r\"))\n        random.shuffle(data)\n        if sample_num:\n            data = data[:sample_num]\n        messages = []\n\n        for x in data:\n            image_path = os.path.join(IMAGE_ROOT, x['image'])\n            message = [\n                # {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n                {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"file://{image_path}\"\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": Orig_QUESTION_TEMPLATE.format(Question=x['problem']) if is_baseline else QUESTION_TEMPLATE.format(Question=x['problem'])\n                    }\n                ]\n            }]\n            messages.append(message)\n\n        all_outputs = []  # List to store all answers\n\n        # Process data\n        for i in tqdm(range(0, len(messages), BSZ)):\n            batch_messages = messages[i:i + BSZ]\n        \n            # Preparation for inference\n            text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n            \n            image_inputs, video_inputs = process_vision_info(batch_messages)\n            inputs = processor(\n                text=text,\n                images=image_inputs,\n                videos=video_inputs,\n                padding=True,\n                return_tensors=\"pt\",\n            )\n            inputs = inputs.to(\"cuda\")\n\n            # Inference: Generation of the output\n            generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=512, do_sample=False)\n            \n            generated_ids_trimmed = [\n                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n            ]\n            batch_output_text = processor.batch_decode(\n                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )\n            \n            all_outputs.extend(batch_output_text)\n            # print(f\"Processed batch {i//BSZ + 1}/{(len(messages) + BSZ - 1)//BSZ}\")\n\n        final_output = []\n        correct_number = 0\n\n        for input_example, model_output in zip(data, all_outputs):\n            original_output = model_output\n            ground_truth = input_example['solution']\n            ground_truth_normalized = input_example['normalized_solution']\n            model_answer, normalized = extract_bbox_answer(original_output)\n            \n            # Count correct answers\n            correct = 0\n            if model_answer is not None:\n                if not normalized and iou(model_answer, ground_truth) > 0.5:\n                    correct = 1\n                elif normalized and iou(model_answer, ground_truth_normalized) > 0.5:\n                    correct = 1\n            correct_number += correct\n            \n            # Create a result dictionary for this example\n            result = {\n                'question': input_example['problem'],\n                'ground_truth': ground_truth,\n                'model_output': original_output,\n                'extracted_answer': model_answer,\n                'correct': correct\n            }\n            final_output.append(result)\n\n        # Calculate and print accuracy\n        accuracy = correct_number / len(data) * 100\n        print(f\"\\nAccuracy of {ds}: {accuracy:.2f}%\")\n\n        # Save results to a JSON file\n        output_path = OUTPUT_PATH.format(DATASET=ds, CKPT = ckpt_name)\n        with open(output_path, \"w\") as f:\n            json.dump({\n                'accuracy': accuracy,\n                'results': final_output\n            }, f, indent=2)\n\n        print(f\"Results saved to {output_path}\")\n        print(\"-\"*100)\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/__init__.py", "content": ""}
{"type": "source_file", "path": "src/r1-v/setup.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Adapted from huggingface/transformers: https://github.com/huggingface/transformers/blob/21a2d900eceeded7be9edc445b56877b95eda4ca/setup.py\n\n\nimport re\nimport shutil\nfrom pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n\n# Remove stale open_r1.egg-info directory to avoid https://github.com/pypa/pip/issues/5466\nstale_egg_info = Path(__file__).parent / \"open_r1.egg-info\"\nif stale_egg_info.exists():\n    print(\n        (\n            \"Warning: {} exists.\\n\\n\"\n            \"If you recently updated open_r1, this is expected,\\n\"\n            \"but it may prevent open_r1 from installing in editable mode.\\n\\n\"\n            \"This directory is automatically generated by Python's packaging tools.\\n\"\n            \"I will remove it now.\\n\\n\"\n            \"See https://github.com/pypa/pip/issues/5466 for details.\\n\"\n        ).format(stale_egg_info)\n    )\n    shutil.rmtree(stale_egg_info)\n\n\n# IMPORTANT: all dependencies should be listed here with their version requirements, if any.\n#   * If a dependency is fast-moving (e.g. transformers), pin to the exact version\n_deps = [\n    \"accelerate>=1.2.1\",\n    \"bitsandbytes>=0.43.0\",\n    \"black>=24.4.2\",\n    \"datasets>=3.2.0\",\n    \"deepspeed==0.15.4\",\n    \"distilabel[vllm,ray,openai]>=1.5.2\",\n    \"einops>=0.8.0\",\n    \"flake8>=6.0.0\",\n    \"hf_transfer>=0.1.4\",\n    \"huggingface-hub[cli]>=0.19.2,<1.0\",\n    \"isort>=5.12.0\",\n    \"liger_kernel==0.5.2\",\n    \"lighteval @ git+https://github.com/huggingface/lighteval.git@4f381b352c0e467b5870a97d41cb66b487a2c503#egg=lighteval[math]\",\n    \"math-verify\",  # Used for math verification in grpo\n    \"packaging>=23.0\",\n    \"parameterized>=0.9.0\",\n    \"pytest\",\n    \"safetensors>=0.3.3\",\n    \"sentencepiece>=0.1.99\",\n    \"torch>=2.5.1\",\n    \"transformers @ git+https://github.com/huggingface/transformers.git@336dc69d63d56f232a183a3e7f52790429b871ef\",\n    \"trl==0.14.0\",\n    \"vllm==0.6.6.post1\",\n    \"wandb>=0.19.1\",\n    \"pillow\",\n]\n\n# this is a lookup table with items like:\n#\n# tokenizers: \"tokenizers==0.9.4\"\n# packaging: \"packaging\"\n#\n# some of the values are versioned whereas others aren't.\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~ \\[\\]]+)(?:\\[[^\\]]+\\])?(?:[!=<>~ ].*)?$)\", x)[0] for x in _deps)}\n\n\ndef deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]\n\n\nextras = {}\nextras[\"tests\"] = deps_list(\"pytest\", \"parameterized\")\nextras[\"torch\"] = deps_list(\"torch\")\nextras[\"quality\"] = deps_list(\"black\", \"isort\", \"flake8\")\nextras[\"eval\"] = deps_list(\"lighteval\", \"math-verify\")\nextras[\"dev\"] = extras[\"quality\"] + extras[\"tests\"] + extras[\"eval\"]\n\n# core dependencies shared across the whole project - keep this to a bare minimum :)\ninstall_requires = [\n    deps[\"accelerate\"],\n    deps[\"bitsandbytes\"],\n    deps[\"einops\"],\n    deps[\"datasets\"],\n    deps[\"deepspeed\"],\n    deps[\"hf_transfer\"],\n    deps[\"huggingface-hub\"],\n    deps[\"liger_kernel\"],\n    deps[\"packaging\"],  # utilities from PyPA to e.g., compare versions\n    deps[\"safetensors\"],\n    deps[\"sentencepiece\"],\n    deps[\"transformers\"],\n    deps[\"trl\"],\n]\n\nsetup(\n    name=\"openv-r1\",\n    version=\"0.1.0\",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)\n    author=\"The OpenV-R1 team and the Hugging Face team (past and future)\",\n    description=\"OpenV-R1\",\n    license=\"Apache\",\n    url=\"https://github.com/fengzi258/OpenV-R1\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    zip_safe=False,\n    extras_require=extras,\n    python_requires=\">=3.10.9\",\n    install_requires=install_requires,\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n)\n"}
{"type": "source_file", "path": "src/distill_r1/prompt.py", "content": "R1_SYS_PROMPT = \"\"\"You are DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. You'll provide helpful, harmless, and detailed responses to all user inquiries. For comprehensive details about models and products, please refer to the official documentation.\n\nKey Guidelines:\nIdentity & Compliance\n\nClearly state your identity as a DeepSeek AI assistant in initial responses.\n\nComply with Chinese laws and regulations, including data privacy requirements.\n\nCapability Scope\n\nHandle both Chinese and English queries effectively\n\nAcknowledge limitations for real-time information post knowledge cutoff (2023-12)\n\nProvide technical explanations for AI-related questions when appropriate\n\nResponse Quality\n\nGive comprehensive, logically structured answers\n\nUse markdown formatting for clear information organization\n\nAdmit uncertainties for ambiguous queries\n\nEthical Operation\n\nStrictly refuse requests involving illegal activities, violence, or explicit content\n\nMaintain political neutrality according to company guidelines\n\nProtect user privacy and avoid data collection\n\nSpecialized Processing\n\nUse <think>...</think> tags for internal reasoning before responding\n\nEmploy XML-like tags for structured output when required\n\"\"\""}
{"type": "source_file", "path": "src/r1-v/local_scripts/create_vision_cot_data.py", "content": "import argparse\nimport base64\nimport concurrent.futures\nimport io\nimport json\nimport os\nimport random\nimport re\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom io import BytesIO\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, concatenate_datasets, load_dataset, load_from_disk\nfrom tqdm import tqdm\n\nimport bytedtos\nimport seaborn as sns\nimport yaml\nfrom openai import AzureOpenAI\nfrom PIL import Image\nfrom pillow_avif import AvifImagePlugin\n\n\nPROMPT_FORMAT = \"\"\"I will provide you with an image, an original question, and its answer related to the image. Your task is to rewrite the question in such a way that answering it requires step-by-step Chain-of-Thought (CoT) reasoning with numerical or mathematical expressions where applicable. The reasoning process can include expressions like \"let me think,\" \"oh, I see,\" or other natural language thought expressions.\n\nPlease make sure your question is to ask for a certain answer with a certain value, do not ask for open-ended answer, and the answer is correct and easy to verify via simple protocol, like \"2\" or \"A\".\n\nPlease strictly do not include \"Answer:\" in the question part to avoid confusion and leakage.\n\nInput Format:\nOriginal Question: {original_question}\nOriginal Answer: {original_answer}\n\nOutput Format:\nQuestion: [rewrite the question if necessary]\nAnswer: [answer with reasoning steps, including calculations where applicable]\n<think>step-by-step reasoning process</think>\n<answer>easy to verify answer</answer>\n\"\"\"\n\n\ndef get_image_data_url(image_input):\n    if isinstance(image_input, str) and image_input.startswith(\"data:\"):\n        return image_input\n\n    if isinstance(image_input, str) and image_input.startswith(\"http\"):\n        image_input = load_image(image_input)\n\n    if isinstance(image_input, str):\n        image_input = Image.open(image_input)\n\n    if not isinstance(image_input, Image.Image):\n        raise ValueError(\"Unsupported image input type\")\n\n    if image_input.mode != \"RGB\":\n        image_input = image_input.convert(\"RGB\")\n\n    buffer = BytesIO()\n    image_input.save(buffer, format=\"JPEG\")\n    img_bytes = buffer.getvalue()\n    base64_data = base64.b64encode(img_bytes).decode(\"utf-8\")\n    return f\"data:image/jpeg;base64,{base64_data}\"\n\n\ndef gpt4o_query(image, prompt, max_retries=5, initial_delay=3):\n    if image is None:\n        return None\n\n    data_url_list = [get_image_data_url(image)]\n    client = AzureOpenAI(\n        azure_endpoint=\"YOUR_AZURE_ENDPOINT\",\n        api_version=\"2023-07-01-preview\",\n        api_key=\"YOUR_API_KEY\",\n    )\n\n    for attempt in range(max_retries):\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert to analyze the image and provide useful information for users.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                    ],\n                },\n            ]\n\n            for data_url in data_url_list:\n                messages[1][\"content\"].insert(\n                    0, {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n                )\n\n            response = client.chat.completions.create(\n                model=\"gpt-4o-2024-08-06\",\n                messages=messages,\n                temperature=0.2,\n                max_tokens=8192,\n            )\n            return response.choices[0].message.content\n\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise Exception(\n                    f\"Failed after {max_retries} attempts. Last error: {str(e)}\"\n                )\n            delay = initial_delay * (2**attempt) + random.uniform(\n                0, 0.1 * initial_delay * (2**attempt)\n            )\n            time.sleep(delay)\n\n\ndef process_single_item(example):\n    try:\n        image_path = example[\"image_path\"]\n        formatted_prompt = PROMPT_FORMAT.format(\n            original_question=example[\"question\"], original_answer=example[\"answer\"]\n        )\n\n        response = gpt4o_query(image_path, formatted_prompt)\n        example[\"gpt4o_response\"] = response\n        return example\n    except Exception as e:\n        print(f\"Error processing item: {str(e)}\")\n        example[\"gpt4o_response\"] = None\n        return example\n\n\ndef main():\n    dataset_path = \"path/to/your/dataset\"\n    full_dataset = load_from_disk(dataset_path)\n\n    processed_dataset = full_dataset.map(\n        function=partial(process_single_item),\n        num_proc=256,\n        desc=\"Processing dataset with GPT-4o\",\n        keep_in_memory=True,\n    )\n\n    output_path = f\"{dataset_path}_processed\"\n    processed_dataset.save_to_disk(output_path)\n    print(f\"Processed dataset saved to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/eval/utils.py", "content": "from math_verify import parse, verify, LatexExtractionConfig\nfrom latex2sympy2_extended import NormalizationConfig\nimport re\nfrom datetime import datetime\nfrom Levenshtein import ratio\nfrom babel.numbers import parse_decimal\nimport math\nimport random\nfrom typing import Optional, Dict\nimport  os\n\nformat_reward_factor = 1.0\n\ndef process_expression(s):\n    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤æ‰€æœ‰è¿ç®—ç¬¦ï¼ˆ=+ã€-ã€*ã€/ï¼‰å‘¨å›´çš„ç©ºæ ¼\n    return re.sub(r'\\s*([=+\\-*/])\\s*', r'\\1', s)\n\n\ndef extract_choice(text):\n    # 1. Clean and normalize text\n    text = text.upper()  # Convert to uppercase\n    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n\n    # 2. Choice should not have uppercase letters before or after\n    choices = re.findall(r'(?<![A-Z])([A-Z])(?![A-Z])', text)\n\n    if not choices:\n        return None\n\n    # 3. If only one choice, return it directly\n    if len(choices) == 1:\n        return choices[0]\n\n    # 4. If multiple choices, use heuristic rules\n    choice_scores = {choice: 0 for choice in choices}\n\n    # 4.1 Keywords around choices get points\n    keywords = [\n        'ç­”æ¡ˆ', 'é€‰æ‹©', 'æ­£ç¡®', 'æ˜¯', 'å¯¹',\n        'answer', 'correct', 'choose', 'select', 'right',\n        'è®¤ä¸º', 'åº”è¯¥', 'è§‰å¾—', 'think', 'believe', 'should'\n    ]\n\n    # Get context for each choice (20 chars before and after)\n    for choice in choices:\n        pos = text.find(choice)\n        context = text[max(0, pos-20):min(len(text), pos+20)]\n\n        # Add points for keywords\n        for keyword in keywords:\n            if keyword.upper() in context:\n                choice_scores[choice] += 1\n\n        # Add points if choice is near the end (usually final answer)\n        if pos > len(text) * 0.7:  # In last 30% of text\n            choice_scores[choice] += 2\n\n        # Add points if followed by punctuation\n        if pos < len(text) - 1 and text[pos+1] in 'ã€‚.!ï¼,ï¼Œ':\n            choice_scores[choice] += 1\n\n    # Return highest scoring choice\n    return max(choice_scores.items(), key=lambda x: x[1])[0]\n\n\ndef numeric_reward(content, sol, **kwargs):\n    content = clean_text(content)\n    sol = clean_text(sol)\n    try:\n        content, sol = float(content), float(sol)\n        return 1.0 if content == sol else 0.0\n    except:\n        return None\n\ndef clean_text(text, exclue_chars=['\\n', '\\r']):\n    # Extract content between <answer> and </answer> if present\n    answer_matches = re.findall(r'<answer>(.*?)</answer>', text, re.DOTALL)\n    if answer_matches:\n        # Use the last match\n        text = answer_matches[-1]\n    \n    for char in exclue_chars:\n        if char in ['\\n', '\\r']:\n            # If there is a space before the newline, remove the newline\n            text = re.sub(r'(?<=\\s)' + re.escape(char), '', text)\n            # If there is no space before the newline, replace it with a space\n            text = re.sub(r'(?<!\\s)' + re.escape(char), ' ', text)\n        else:\n            text = text.replace(char, ' ')\n    \n    # Remove leading and trailing spaces and convert to lowercase\n    return text.strip().rstrip('.').lower()\n\ndef iou_reward(completion, sol, **kwargs):\n    def iou(box1, box2):\n        inter_x1 = max(box1[0], box2[0])\n        inter_y1 = max(box1[1], box2[1])\n        inter_x2 = min(box1[2]-1, box2[2]-1)\n        inter_y2 = min(box1[3]-1, box2[3]-1)\n        if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n            inter = (inter_x2-inter_x1+1)*(inter_y2-inter_y1+1)\n        else:\n            inter = 0\n        union = (box1[2]-box1[0])*(box1[3]-box1[1]) + (box2[2]-box2[0])*(box2[3]-box2[1]) - inter\n        return float(inter)/union\n    \n    content = completion[0][\"content\"]\n    answer_tag_pattern = r'<answer>(.*?)</answer>'\n    # bbox_pattern = r'\\[(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*)\\]'\n    bbox_pattern = r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)]'\n    \n    reward = 0.0\n    # Try symbolic verification first\n    try:\n        content_answer_match = re.search(answer_tag_pattern, content, re.DOTALL)\n        sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n        ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n        ground_truth = eval(ground_truth)\n            \n        if content_answer_match:\n            content_answer = content_answer_match.group(1).strip()\n            bbox_match = re.search(bbox_pattern, content_answer)\n            if bbox_match:\n                bbox = [int(bbox_match.group(1)), int(bbox_match.group(2)), int(bbox_match.group(3)), int(bbox_match.group(4))]\n                if iou(bbox, ground_truth) > 0.5:\n                    reward = 1.0\n    except Exception:\n        pass  # Continue to next verification method if this fails\n                \n    return reward\n\ndef default_format_reward(completion, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    completion_content = completion[0][\"content\"]\n    match = re.fullmatch(pattern, completion_content, re.DOTALL) \n    return 1.0 if match else 0.0 \n\ndef iou_format_reward(completion, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    # pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?\\{.*\\[\\d+,\\s*\\d+,\\s*\\d+,\\s*\\d+\\].*\\}.*?</answer>\"\n    completion_content = completion[0][\"content\"]\n    match = re.fullmatch(pattern, completion_content, re.DOTALL)\n    return 1.0 if match else 0.0\n\ndef format_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using symbolic verification, exact string matching, or fuzzy matching.\"\"\"\n    # contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    for content, sol, accu_reward_method in zip(completions, solution, kwargs.get(\"reward_func\")):\n        # if accu_reward_method is defined, use the corresponding reward function, otherwise use the default reward function\n        if accu_reward_method == \"iou\":\n            reward = iou_format_reward(content)\n        else:\n            reward = default_format_reward(content)  \n        \n        reward *= format_reward_factor\n        rewards.append(reward)\n\n    return rewards\n\n\ndef default_accuracy_reward(completion, sol, **kwargs):\n    content = completion #completion[0][\"content\"]\n    reward = 0.0\n    model_answer = \"\"\n    # Try symbolic verification first for numeric answers\n    try:\n        answer = parse(content)\n        model_answer = answer\n        if float(verify(answer, parse(sol))) > 0:\n            reward = 1.0\n    except Exception:\n        pass  # Continue to next verification method if this fails\n\n    # If symbolic verification failed, try string matching or fuzzy matching\n    if reward == 0.0:\n        try:\n            # Extract answer from solution if it has think/answer tags\n            sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n            ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n            \n            # Extract answer from content if it has think/answer tags\n            content_matches = re.findall(r'<answer>(.*?)</answer>', content, re.DOTALL)\n            student_answer = content_matches[-1].strip() if content_matches else content.strip()\n            \n            student_answer = process_expression(student_answer)\n            ground_truth = process_expression(ground_truth)\n\n            model_answer = student_answer\n\n            # Compare the extracted answers\n            if student_answer == ground_truth:\n                reward = 1.0\n            \n            if reward == 0.0:\n                # Check if ground truth contains numbers\n                has_numbers = bool(re.search(r'\\d', ground_truth))\n                # Check if it's a multiple choice question\n                has_choices = extract_choice(ground_truth)\n                \n                if has_numbers:\n                    # For numeric answers, use exact matching\n                    reward = numeric_reward(student_answer, ground_truth)\n                    if reward is None:\n                        model_answer = clean_text(student_answer)\n                        reward = ratio(clean_text(student_answer), clean_text(ground_truth))\n                elif has_choices:\n                    # For multiple choice, extract and compare choices\n                    correct_choice = has_choices.upper()\n                    student_choice = extract_choice(student_answer)\n                    model_answer = clean_text(student_choice)\n                    if student_choice:\n                        reward = 1.0 if student_choice == correct_choice else 0.0\n                else:\n                    # For text answers, use fuzzy matching\n                    model_answer = clean_text(student_answer)\n                    reward = ratio(clean_text(student_answer), clean_text(ground_truth))\n            \n            # The answer partially mismatches the ground truth \n            if reward > 0.0 and reward < 1.0:\n                reward = 0.0\n                \n        except Exception:\n            pass  # Keep reward as 0.0 if all methods fail\n\n    return reward, model_answer\n\ndef accuracy_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using symbolic verification, exact string matching, or fuzzy matching.\"\"\"\n    # contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    for content, sol, accu_reward_method in zip(completions, solution, kwargs.get(\"reward_func\")):\n        # if accu_reward_method is defined, use the corresponding reward function, otherwise use the default reward function\n        if accu_reward_method == \"iou\":\n            reward = iou_reward(content, sol)\n        else:\n            reward = default_accuracy_reward(content, sol)  \n        rewards.append(reward)\n\n        \n    return rewards\n\n\ndef origin_accuracy_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using either symbolic verification or exact string matching.\"\"\"\n    contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    current_time = datetime.now().strftime(\"%d-%H-%M-%S-%f\")\n    for content, sol in zip(contents, solution):\n        reward = 0.0\n        # Try symbolic verification first\n        try:\n            answer = parse(content)\n            if float(verify(answer, parse(sol))) > 0:\n                reward = 1.0\n        except Exception:\n            pass  # Continue to next verification method if this fails\n\n        # If symbolic verification failed, try string matching\n        if reward == 0.0:\n            try:\n                # Extract answer from solution if it has think/answer tags\n                sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n                ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n                \n                # Extract answer from content if it has think/answer tags\n                content_match = re.search(r'<answer>(.*?)</answer>', content)\n                student_answer = content_match.group(1).strip() if content_match else content.strip()\n                \n                # Compare the extracted answers\n                if student_answer == ground_truth:\n                    reward = 1.0\n\n                # è¡¨è¾¾å¼åˆ¤æ–­\n                # if reward == 0.0:\n                #     processed_student_answer = process_expression(student_answer)\n                #     processed_ground_truth = process_expression(ground_truth)\n                #     # Compare the extracted answers\n                #     if processed_student_answer == processed_ground_truth:\n                #         reward = 1.0\n\n            except Exception:\n                pass  # Keep reward as 0.0 if both methods fail\n                \n        rewards.append(reward)\n        \n    return rewards\n\n\ndef origin_format_reward(completions, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    completion_contents = [completion[0][\"content\"] for completion in completions]\n    matches = [re.fullmatch(pattern, content, re.DOTALL) for content in completion_contents]\n    return [1.0 if match else 0.0 for match in matches]\n\n\n"}
{"type": "source_file", "path": "src/distill_r1/query_r1.py", "content": "import json\nimport random\nimport os \nfrom openai import OpenAI\nfrom tqdm import tqdm\nimport concurrent.futures\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nfrom threading import Lock\nimport time \nfrom prompt import R1_SYS_PROMPT \n# Initialize the client\nclient = OpenAI(\n    api_key=os.environ.get(\"SL_KEY\", \"YOUR_SILCONFLOW_KEY\"),\n    base_url=\"https://api.siliconflow.cn/v1\",\n)\n\n# Create a lock for thread-safe file writing\nfile_lock = Lock()\n\ndef format_query(qa_dict: Dict, v2=False) -> str:\n    query = \"Answer the question according to scene description.\\n\\n\"\n    query += qa_dict[\"description\"]\n    query += f\"\\nQuestion:\\n{qa_dict['q']}\"\n    if v2:\n        query += \"\\nInstructions:\\n\"\n        query += \"1. Carefully analyze the scene description\\n\"\n        query += \"2. Provide your reasoning if necessary\\n\"\n        query += \"3. For the final answer, start a new line with '**The answer is: **' followed by your answer\\n\"\n    return query\n\ndef write_to_jsonl(result: Dict, filename: str):\n    \"\"\"Thread-safe function to write a result to JSONL file\"\"\"\n    with file_lock:\n        with open(filename, 'a') as f:\n            f.write(json.dumps(result) + '\\n')\n\ndef query_r1(qa_pair: Dict, output_file: str, model: str = \"deepseek-ai/DeepSeek-R1\", v2=False) -> Optional[Dict]:\n    query = format_query(qa_pair, v2=v2)\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": R1_SYS_PROMPT},\n                {\"role\": \"user\", \"content\": query}],\n            stream=False,\n            max_tokens=4096 \n        )\n        result = {\n            **qa_pair,\n            \"r1_response\": response.choices[0].message.content,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        # Write result immediately\n        write_to_jsonl(result, output_file)\n        time.sleep(4)\n        return result\n    except Exception as e:\n        print(f\"Error processing query: {e}\")\n        error_result = {\n            **qa_pair,\n            \"error\": str(e),\n            \"timestamp\": datetime.now().isoformat()\n        }\n        write_to_jsonl(error_result, f\"errors_{output_file}\")\n        time.sleep(10)\n        return None\n\ndef process_qa_pairs_parallel(qa_pairs: List[Dict], output_file: str, max_workers: int = 10) -> List[Dict]:\n    successful_count = 0\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Create futures for all qa_pairs\n        futures = [executor.submit(query_r1, qa_pair, output_file, v2=\"v2\" in output_file) for qa_pair in qa_pairs]\n        \n        # Process results as they complete with progress bar\n        results = []\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            try:\n                result = future.result()\n                if result is not None:\n                    results.append(result)\n                    successful_count += 1\n            except Exception as e:\n                print(f\"Failed to process query: {e}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Load and shuffle QA pairs\n    random.seed(1234)\n    qa_pairs = json.load(open(\"/home/lilei/Visual-R1/data/clever_counting_problems_clevr_cogent_v1.0_trainA.json\"))\n    random.shuffle(qa_pairs)\n    qa_pairs = qa_pairs[:10000]\n    # Create output filename with timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"r1_results_clevr_cogent_v1.0_trainA_v2.jsonl\"\n    \n    finished = set() \n    with open(output_file, 'r') as f:\n        for line in f:\n            ins = json.loads(line)\n            key = ins[\"img_filename\"] + \"-\" + ins[\"q\"] + \"-\"  + str(ins[\"a\"])\n            finished.add(key)\n    qa_pairs = [ins for ins in qa_pairs if ins[\"img_filename\"] + \"-\" + ins[\"q\"] + \"-\" + str(ins[\"a\"]) not in finished] \n    print(\"Finished: \", len(finished))\n    print(\"Remaining: \", len(qa_pairs)) \n    # Process QA pairs in parallel\n    r1_results = process_qa_pairs_parallel(qa_pairs, output_file)\n    \n    # Print final statistics\n    print(f\"Successfully processed {len(r1_results)} out of {len(qa_pairs)} queries\")\n    print(f\"Results saved to {output_file}\")\n    print(f\"Any errors were saved to errors_{output_file}\")"}
{"type": "source_file", "path": "src/distill_r1/create_hf_dataset.py", "content": "import json\nimport os\nimport random\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nrandom.seed(1234)\nVAL_NUM = 5000\n\n\ndef create_r1_train_dataset(\n    valid_pair_json,\n    data_dir,\n    img_dir=\"/home/lilei/Visual-R1/CLEVR_CoGenT_v1.0/images/trainA/\",\n):\n    os.makedirs(data_dir, exist_ok=True)\n    pairs = [json.loads(line) for line in open(valid_pair_json, \"r\")]\n    mapped_pairs = []\n\n    for idx, pair in tqdm(enumerate(pairs)):\n        img_filename = pair[\"img_filename\"]\n        new_pair = {}\n        try:\n            new_pair[\"thinking\"] = (\n                pair[\"r1_response\"]\n                .split(\"<think>\")[1]\n                .split(\"</think>\")[0]\n                .replace(\"scene description\", \"image\")\n            )\n        except Exception as e:\n            print(f\"Error processing pair response: \", pair[\"r1_response\"])\n            continue  # skip this pair\n        # add index to distinguish the same image\n        dataset_filename = (\n            img_filename.split(\".\")[0] + \"_\" + str(idx) + \".\" + img_filename.split(\".\")[1]\n        )\n        if not os.path.exists(f\"{data_dir}/{img_filename}\"):\n            os.system(f\"cp {img_dir}/{img_filename} {data_dir}/{dataset_filename}\")\n        q, a = pair[\"q\"], pair[\"a\"]\n        new_pair[\"problem\"] = q\n        # get the thinking path\n        \n        new_pair[\"thinking\"] = \"<think>\" + new_pair[\"thinking\"] + \"</think>\"\n        new_pair[\"solution\"] = f\"<answer> {a} </answer>\"\n        new_pair[\"file_name\"] = dataset_filename\n        mapped_pairs.append(new_pair)\n    with open(f\"{data_dir}/metadata.jsonl\", \"w\") as f:\n        for pair in mapped_pairs:\n            f.write(json.dumps(pair) + \"\\n\")\n\n    train_dataset = load_dataset(\n        \"imagefolder\",\n        data_dir=data_dir,\n        split=\"train\",\n    )\n    return train_dataset\n\n\ndef create_val_dataset(\n    json_file,\n    data_dir,\n    val_num=VAL_NUM,\n    image_dir=\"/home/lilei/Visual-R1/CLEVR_CoGenT_v1.0/images/valB\",\n):\n    os.makedirs(data_dir, exist_ok=True)\n    val = json.load(open(json_file))\n    random.shuffle(val)\n    val = val[:val_num]\n    val_pairs = []\n    for idx, pair in tqdm(enumerate(val)):\n        q, a = pair[\"q\"], pair[\"a\"]\n        img_filename = pair[\"img_filename\"]\n        # copy images to the DATA_DIR\n        val_filename = (\n            img_filename.split(\".\")[0] + f\"_{idx}.\" + img_filename.split(\".\")[1]\n        )\n        if not os.path.exists(f\"{data_dir}/{img_filename}\"):\n            os.system(f\"cp {image_dir}/{img_filename} {data_dir}/{val_filename}\")\n        new_pair = {}\n        new_pair[\"problem\"] = q\n        new_pair[\"solution\"] = f\"<answer> {a} </answer>\"\n        new_pair[\"file_name\"] = val_filename\n        val_pairs.append(new_pair)\n    with open(f\"{data_dir}/metadata.jsonl\", \"w\") as f:\n        for pair in val_pairs:\n            f.write(json.dumps(pair) + \"\\n\")\n    val_dataset = load_dataset(\"imagefolder\", data_dir=data_dir, split=\"train\")\n    return val_dataset\n\n\n# valA split\nVALA_DATA_DIR = \"data/Clevr_CoGenT_ValA\"\nVALB_DATA_DIR = \"data/Clevr_CoGenT_ValB\"\nvalA_json = (\n    \"/home/lilei/Visual-R1/data/clever_counting_problems_clevr_cogent_v1.0_valA.json\"\n)\nvalB_json = (\n    \"/home/lilei/Visual-R1/data/clever_counting_problems_clevr_cogent_v1.0_valB.json\"\n)\nTRAIN_DATADIR = \"data/Clevr_CoGenT_TrainA_R1\"\ntrain_dataset = create_r1_train_dataset(\n    \"/home/lilei/Visual-R1/filter_results_v2/valid_pairs.jsonl\",\n    TRAIN_DATADIR,\n)\n\n# print(train_dataset)\nvalA_dataset = create_val_dataset(\n    valA_json,\n    VALA_DATA_DIR,\n    image_dir=\"/home/lilei/Visual-R1/CLEVR_CoGenT_v1.0/images/valA\",\n)\nvalB_dataset = create_val_dataset(\n    valB_json,\n    VALB_DATA_DIR,\n    image_dir=\"/home/lilei/Visual-R1/CLEVR_CoGenT_v1.0/images/valB\",\n)\nvalA_dataset.push_to_hub(\"MMInstruction/Clevr_CoGenT_ValA\")\nvalB_dataset.push_to_hub(\"MMInstruction/Clevr_CoGenT_ValB\")\ntrain_dataset.push_to_hub(\"MMInstruction/Clevr_CoGenT_TrainA_R1\")\n"}
{"type": "source_file", "path": "src/r1-v/local_scripts/prepare_hf_data.py", "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nfrom typing import List, Dict\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport datasets\n\nimport io\nfrom datasets import load_dataset, load_from_disk, concatenate_datasets\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom functools import partial\nfrom pillow_avif import AvifImagePlugin\nfrom datasets import Dataset\nimport json\nimport yaml\nimport os\nimport re\nimport time\nimport random\nimport base64\nfrom openai import AzureOpenAI\nimport concurrent.futures\nfrom typing import List, Dict\nimport argparse\nimport time\n\n\ndef extract_problem_solution(gpt4o_response):\n    # Split the response into parts\n    parts = gpt4o_response.split(\"<think>\")\n\n    # Extract the problem (first part before any <think> tags)\n    problem = parts[0].strip()\n    # Remove \"Question:\" prefix if it exists\n    problem = re.sub(r\"^Question:\\s*\", \"\", problem)\n    # Remove \"Answer:\" at the end of the problem\n    problem = re.sub(r\"\\s*Answer:\\s*$\", \"\", problem).strip()\n\n    # Combine all the reasoning steps into a single <think> block\n    think_parts = [p.split(\"</think>\")[0].strip() for p in parts[1:] if \"</think>\" in p]\n    solution = f\"<think>{' '.join(think_parts)}</think>\"\n\n    # Add the final answer if it exists, removing \"Answer:\" prefix\n    if \"<answer>\" in gpt4o_response:\n        final_answer = (\n            gpt4o_response.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n        )\n        final_answer = re.sub(r\"^Answer:\\s*\", \"\", final_answer)\n        solution += f\"\\n\\n<answer>{final_answer}</answer>\"\n\n    return problem, solution\n\n\ndef load_image_from_path(image_path):\n    try:\n        img = Image.open(image_path)\n        return img\n    except Exception as e:\n        print(f\"Error loading image {image_path}: {str(e)}\")\n        return None\n\n\ndef process_raw_data(raw_data):\n    # Parse the raw data if it's a string\n    if isinstance(raw_data, str):\n        data = json.loads(raw_data)\n    else:\n        data = raw_data\n\n    # Extract problem and solution\n    try:\n        problem, solution = extract_problem_solution(data[\"gpt4o_response\"])\n        image = load_image_from_path(data[\"image_path\"])\n\n        return {\n            \"image\": image,\n            \"problem\": problem,\n            \"solution\": solution,\n            \"original_question\": data[\"question\"],\n            \"original_answer\": data[\"answer\"],\n        }\n    except Exception as e:\n        print(f\"Error processing data {data}: {str(e)}\")\n        return {\n            \"image\": None,\n            \"problem\": None,\n            \"solution\": None,\n            \"original_question\": None,\n            \"original_answer\": None,\n        }\n\n\nraw_data_list = [\n    \"/path/to/reasoning_data_with_response_90k_verified\",\n]\n\nraw_data = concatenate_datasets([load_from_disk(path) for path in raw_data_list])\n\nprocessed_data = raw_data.map(process_raw_data, num_proc=128).shuffle(seed=42)\n\nhf_dict = {\n    \"image\": [],\n    \"problem\": [],\n    \"solution\": [],\n    \"original_question\": [],\n    \"original_answer\": [],\n}\n\nfor item in tqdm(processed_data):\n    hf_dict[\"image\"].append(item[\"image\"])\n    hf_dict[\"problem\"].append(item[\"problem\"])\n    hf_dict[\"solution\"].append(item[\"solution\"])\n    hf_dict[\"original_question\"].append(item[\"original_question\"])\n    hf_dict[\"original_answer\"].append(item[\"original_answer\"])\n\n\nfeatures = datasets.Features(\n    {\n        \"image\": datasets.Image(),\n        \"problem\": datasets.Value(\"string\"),\n        \"solution\": datasets.Value(\"string\"),\n        \"original_question\": datasets.Value(\"string\"),\n        \"original_answer\": datasets.Value(\"string\"),\n    }\n)\n\n\ndef has_empty_tags(text):\n    # Pattern to match empty tags like <tag></tag>\n    pattern = r\"<[^>]+></[^>]+>\"\n    return bool(re.search(pattern, text))\n\n\ndef has_answer_pattern(text):\n    if \"Answer:\" in text:\n        return True\n    return False\n\n\ndef has_valid_image_size(example): # for Qwen2-VL-2B's processor requirement\n    # Assuming the image is in a format that can be checked for dimensions\n    # You might need to adjust this depending on how the image is stored in your dataset\n    try:\n        image = example[\"image\"]  # or however your image is accessed\n        if isinstance(image, dict) and \"height\" in image and \"width\" in image:\n            return image[\"height\"] >= 28 and image[\"width\"] >= 28\n        # If image is a PIL Image or similar\n        return image.height >= 28 and image.width >= 28\n    except:\n        return False\n\n\nds = datasets.Dataset.from_dict(hf_dict, features=features)\nds = ds.filter(\n    lambda x: not has_empty_tags(x[\"solution\"])\n    and not has_answer_pattern(x[\"problem\"])\n    and has_valid_image_size(x)\n    and x[\"image\"] is not None,\n    num_proc=128,\n)\n# Push to Hugging Face Hub\nds.push_to_hub(\"path/to/your/dataset\")\n"}
{"type": "source_file", "path": "src/distill_r1/filter_r1.py", "content": "import json\nimport re\nfrom pathlib import Path\n\n\n\ndef extract_answer_from_query(query_results: str) -> str | None:\n    \"\"\"\n    Extract answer from query results, specifically looking for:\n    - Numbers within asterisks\n    - Yes/No answers in various formats\n\n    Args:\n        query_results: String containing the query response\n\n    Returns:\n        Extracted answer string or None if no answer found\n    \"\"\"\n    # First try to find answers in the standard format with labels\n    # Split the text into segments (trying to get the last conclusion)\n    if \"<think>\" not in query_results or \"</think>\" not in query_results:\n        return None\n    segments = query_results.split(\"\\n\")\n\n    # First try to find final conclusion in the last few segments\n    conclusion_patterns = [\n        r\"(?:so|therefore|thus|hence),?\\s*(?:the answer is\\s+)?\\*\\*\\s*(no|yes|[0-9]+)\\s*\\*\\*\",\n        r\"(?:so|therefore|thus|hence),?\\s*(?:the answer is\\s+)?(no|yes|[0-9]+)\\b\",\n        r\"the answer is\\s+\\*\\*\\s*(no|yes|[0-9]+)\\s*\\*\\*\",\n        r\"(?:final|conclusive) answer(?:\\s+is)?\\s*\\*\\*\\s*(no|yes|[0-9]+)\\s*\\*\\*\",\n    ]\n\n    # Try to find conclusion in last 3 segments\n    for segment in reversed(segments[-3:]):\n        for pattern in conclusion_patterns:\n            match = re.search(pattern, segment, re.IGNORECASE)\n            if match:\n                return match.group(1).strip().lower()\n\n    # If no conclusion found, try other patterns on the full text\n    labeled_patterns = [\n        r\"\\*\\*The answer is:\\s*\\*\\*\\s*([0-9]+|yes|no)\\b\",\n        r\"\\*\\*Answer:\\s*\\*\\*\\s*([0-9]+|yes|no)\\b\",\n        r\"\\*\\*Answer\\*\\*:\\s*([0-9]+|yes|no)\\b\",\n        r\"\\*\\*Answer:?\\s*\\*\\*\\s*There (?:is|are)\\s+([0-9]+)\",\n        r\"\\*\\*Final Count:\\s*\\*\\*\\s*([0-9]+)\",\n        r\"\\*\\*Final Count:\\s*\\*\\*\\s*([0-9]+)\\s+(?:items?|objects?|spheres?|cubes?|boxes?)\",\n        r\"\\*\\*Total:\\s*\\*\\*\\s*([0-9]+)\",\n        r\"The answer is:\\s*([0-9]+|yes|no)\\b\",\n        r\"Answer:\\s*([0-9]+|yes|no)\\b\",\n        r\"should be\\s+([0-9]+)[.\\s]\",\n    ]\n\n    direct_patterns = [\n        r\"\\*\\*\\s*([0-9]+)\\s*\\*\\*\",\n        r\"\\*\\*\\s*([0-9]+)\\s+(?:items?|objects?|cubes?|boxes?|spheres?)?\\s*\\*\\*\",\n        r\"\\*\\*\\s*([0-9]+)\\s+[^*]+\\*\\*\",\n    ]\n\n    latex_patterns = [\n        r\"\\$\\\\boxed{([0-9]+)}\\$\",\n        r\"\\\\boxed{([0-9]+)}\",\n    ]\n\n    count_patterns = [\n        r\"There (?:is|are)\\s+([0-9]+)\\s+(?:items?|objects?|spheres?|cubes?|boxes?)\",\n    ]\n\n    # Try all patterns in sequence on full text\n    all_patterns = labeled_patterns + direct_patterns + latex_patterns + count_patterns\n\n    for pattern in all_patterns:\n        match = re.search(pattern, query_results, re.IGNORECASE)\n        if match:\n            return match.group(1).strip().lower()\n\n    return None\n\n\ndef validate_qa_pairs(input_file: str, output_dir: str, verbose: bool = True):\n    \"\"\"\n    Process QA pairs and save them to separate files.\n    Only saves pairs where parsed answer matches ground truth.\n\n    Args:\n        input_file: Path to input JSONL file\n        output_dir: Directory to save output files\n        verbose: If True, print examples of mismatched or unparseable responses\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    valid_pairs = []\n    invalid_pairs = []\n    stats = {\"total\": 0, \"unparseable\": 0, \"mismatch\": 0, \"valid\": 0}\n\n    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n        for line_num, line in enumerate(f, 1):\n            stats[\"total\"] += 1\n            qa_pair = json.loads(line.strip())\n            ground_truth = str(qa_pair.get(\"a\", \"\")).lower().strip()\n            parsed_answer = extract_answer_from_query(qa_pair[\"r1_response\"])\n\n            if parsed_answer is None:\n                stats[\"unparseable\"] += 1\n                qa_pair[\"error\"] = \"unparseable\"\n                invalid_pairs.append(qa_pair)\n                if verbose:\n                    print(f\"\\nLine {line_num}: Could not parse answer\")\n                    print(f\"Ground truth: {ground_truth}\")\n                    print(f\"Query results: {qa_pair['r1_response'][-200:]}...\")\n            elif parsed_answer != ground_truth:\n                stats[\"mismatch\"] += 1\n                qa_pair[\"error\"] = \"mismatch\"\n                qa_pair[\"parsed_answer\"] = parsed_answer\n                invalid_pairs.append(qa_pair)\n                if verbose:\n                    print(f\"\\nLine {line_num}: Answer mismatch\")\n                    print(f\"Ground truth: {ground_truth}\")\n                    print(f\"Parsed answer: {parsed_answer}\")\n                    print(f\"Query results: {qa_pair['r1_response'][-200:]}...\")\n            else:\n                stats[\"valid\"] += 1\n                valid_pairs.append(qa_pair)\n\n    # Save valid pairs (where parsed answer matches ground truth)\n    valid_file = output_dir / \"valid_pairs.jsonl\"\n    with open(valid_file, \"w\", encoding=\"utf-8\") as f:\n        for pair in valid_pairs:\n            f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n\n    # Save invalid pairs (unparseable or mismatched)\n    invalid_file = output_dir / \"invalid_pairs.jsonl\"\n    with open(invalid_file, \"w\", encoding=\"utf-8\") as f:\n        for pair in invalid_pairs:\n            f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n\n    # Print statistics\n    print(f\"\\nProcessing Summary:\")\n    print(f\"Total pairs processed: {stats['total']}\")\n    print(f\"Valid pairs (matching ground truth): {stats['valid']}\")\n    print(f\"Invalid pairs: {stats['unparseable'] + stats['mismatch']}\")\n    print(f\"  - Unparseable: {stats['unparseable']}\")\n    print(f\"  - Answer mismatch: {stats['mismatch']}\")\n    print(f\"\\nOutput files:\")\n    print(f\"Valid pairs saved to: {valid_file}\")\n    print(f\"Invalid pairs saved to: {invalid_file}\")\n\n\nif __name__ == \"__main__\":\n    validate_qa_pairs(\n        \"r1_results_clevr_cogent_v1.0_trainA_v2.jsonl\", \"filter_results_v2\"\n    )  # \"filtered_output_tmp_v1.jsonl\")\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/evaluate.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Custom evaluation tasks for LightEval.\"\"\"\n\nfrom lighteval.metrics.dynamic_metrics import (\n    ExprExtractionConfig,\n    LatexExtractionConfig,\n    multilingual_extractive_match_metric,\n)\nfrom lighteval.tasks.lighteval_task import LightevalTaskConfig\nfrom lighteval.tasks.requests import Doc\nfrom lighteval.utils.language import Language\n\n\nmetric = multilingual_extractive_match_metric(\n    language=Language.ENGLISH,\n    fallback_mode=\"first_match\",\n    precision=5,\n    gold_extraction_target=(LatexExtractionConfig(),),\n    pred_extraction_target=(ExprExtractionConfig(), LatexExtractionConfig()),\n    aggregation_function=max,\n)\n\n\ndef prompt_fn(line, task_name: str = None):\n    \"\"\"Assumes the model is either prompted to emit \\\\boxed{answer} or does so automatically\"\"\"\n    return Doc(\n        task_name=task_name,\n        query=line[\"problem\"],\n        choices=[line[\"solution\"]],\n        gold_index=0,\n    )\n\n\n# Define tasks\naime24 = LightevalTaskConfig(\n    name=\"aime24\",\n    suite=[\"custom\"],\n    prompt_function=prompt_fn,\n    hf_repo=\"HuggingFaceH4/aime_2024\",\n    hf_subset=\"default\",\n    hf_avail_splits=[\"train\"],\n    evaluation_splits=[\"train\"],\n    few_shots_split=None,\n    few_shots_select=None,\n    generation_size=32768,\n    metric=[metric],\n    version=1,\n)\nmath_500 = LightevalTaskConfig(\n    name=\"math_500\",\n    suite=[\"custom\"],\n    prompt_function=prompt_fn,\n    hf_repo=\"HuggingFaceH4/MATH-500\",\n    hf_subset=\"default\",\n    hf_avail_splits=[\"test\"],\n    evaluation_splits=[\"test\"],\n    few_shots_split=None,\n    few_shots_select=None,\n    generation_size=32768,\n    metric=[metric],\n    version=1,\n)\n\n# Add tasks to the table\nTASKS_TABLE = []\nTASKS_TABLE.append(aime24)\nTASKS_TABLE.append(math_500)\n\n# MODULE LOGIC\nif __name__ == \"__main__\":\n    print([t[\"name\"] for t in TASKS_TABLE])\n    print(len(TASKS_TABLE))\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/trainer/vllm_grpo_trainer_modified.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport textwrap\nfrom collections import defaultdict\nfrom typing import Any, Callable, Optional, Union\nfrom accelerate.utils.other import is_compiled_module\nfrom accelerate.utils import broadcast_object_list, gather, gather_object\nimport torch\nimport torch.utils.data\nimport transformers\nimport warnings\nfrom unittest.mock import patch\nfrom datasets import Dataset, IterableDataset\nfrom packaging import version\nfrom transformers import (\n    AriaForConditionalGeneration,\n    AriaProcessor,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoProcessor,\n    AutoTokenizer,\n    GenerationConfig,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Qwen2VLForConditionalGeneration,\n    Qwen2_5_VLForConditionalGeneration,\n    Trainer,\n    TrainerCallback,\n    is_wandb_available,\n)\nfrom transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.utils import is_peft_available\n\nfrom trl.data_utils import (\n    apply_chat_template,\n    is_conversational,\n    maybe_apply_chat_template,\n)\nfrom trl.import_utils import is_vllm_available\n\nfrom trl.models import (\n    create_reference_model,\n    prepare_deepspeed,\n    unwrap_model_for_generation,\n)\nfrom trl.trainer.grpo_config import GRPOConfig\nfrom trl.trainer.utils import generate_model_card, get_comet_experiment_url, pad\nfrom trl import GRPOTrainer\n\nimport copy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nif is_peft_available():\n    from peft import PeftConfig, get_peft_model\n\nif is_vllm_available():\n    from vllm import LLM, SamplingParams\n\nif is_wandb_available():\n    import wandb\n    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nimport torch.nn as nn\nfrom torch.utils.data import Sampler\n\n# What we call a reward function is a callable that takes a list of prompts and completions and returns a list of\n# rewards. When it's a string, it's a model ID, so it's loaded as a pretrained model.\nRewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n\nSYSTEM_PROMPT = (\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n    \"<think> reasoning process here </think><answer> answer here </answer>\"\n)\n\nQUESTION_TEMPLATE = \"{Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\nIOU_QUESTION_TEMPLATE = \"{Question}\\nFirst output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n  \nclass Qwen2VLGRPOVLLMTrainerModified(Trainer):\n    def __init__(\n        self,\n        model: Union[str, PreTrainedModel],\n        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n        args: GRPOConfig = None,\n        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n        eval_dataset: Optional[\n            Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]\n        ] = None,\n        processing_class: Optional[PreTrainedTokenizerBase] = None,\n        reward_processing_classes: Optional[\n            Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]\n        ] = None,\n        callbacks: Optional[list[TrainerCallback]] = None,\n        optimizers: tuple[\n            Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]\n        ] = (None, None),\n        peft_config: Optional[\"PeftConfig\"] = None,\n        # qwen2-vl related params\n        max_pixels: Optional[int] = 12845056,\n        min_pixels: Optional[int] = 3136,\n        attn_implementation: str = \"flash_attention_2\",\n    ):\n\n        # Args\n        if args is None:\n            model_name = model if isinstance(model, str) else model.config._name_or_path\n            model_name = model_name.split(\"/\")[-1]\n            args = GRPOConfig(f\"{model_name}-GRPO\")\n\n        # Models\n        # Trained model\n        model_init_kwargs = args.model_init_kwargs or {}\n        model_init_kwargs[\"attn_implementation\"] = attn_implementation\n        if isinstance(model, str):\n            model_id = model\n            torch_dtype = model_init_kwargs.get(\"torch_dtype\")\n            if (\n                isinstance(torch_dtype, torch.dtype)\n                or torch_dtype == \"auto\"\n                or torch_dtype is None\n            ):\n                pass  # torch_dtype is already a torch.dtype or \"auto\" or None\n            elif isinstance(torch_dtype, str):  # it's a str, but not \"auto\"\n                torch_dtype = getattr(torch, torch_dtype)\n                model_init_kwargs[\"torch_dtype\"] = torch_dtype\n            else:\n                raise ValueError(\n                    \"Invalid `torch_dtype` passed to `GRPOConfig`. Expected either 'auto' or a string representing \"\n                    f\"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}.\"\n                )\n            # Disable caching if gradient checkpointing is enabled (not supported)\n            model_init_kwargs[\"use_cache\"] = (\n                False\n                if args.gradient_checkpointing\n                else model_init_kwargs.get(\"use_cache\")\n            )\n            if \"Qwen2-VL\" in model_id:\n                model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Qwen2.5-VL\" in model_id:\n                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                model_init_kwargs.pop(\"use_cache\")\n                model = AriaForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            else:\n                model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n        else:\n            model_id = model.config._name_or_path\n            if args.model_init_kwargs is not None:\n                raise ValueError(\n                    \"You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. \"\n                    \"This argument can only be used when the `model` argument is a string.\"\n                )\n\n        if peft_config is not None:\n            model = get_peft_model(model, peft_config)\n\n        # Reference model\n        if is_deepspeed_zero3_enabled():\n            if \"Qwen2-VL\" in model_id:\n                self.ref_model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            elif \"Qwen2.5-VL\" in model_id:\n                self.ref_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                self.ref_model = AriaForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            else:\n                self.ref_model = AutoModelForCausalLM.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n        elif peft_config is None:\n            # If PEFT configuration is not provided, create a reference model based on the initial model.\n            self.ref_model = create_reference_model(model)\n        else:\n            # If PEFT is used, the reference model is not needed since the adapter can be disabled\n            # to revert to the initial model.\n            self.ref_model = None\n\n        # Processing class\n        if processing_class is None:\n            if \"Qwen\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(\n                    model.config._name_or_path, padding_side=\"left\"\n                )\n                pad_token_id = processing_class.pad_token_id\n\n        # Reward functions\n        if not isinstance(reward_funcs, list):\n            reward_funcs = [reward_funcs]\n        for i, reward_func in enumerate(reward_funcs):\n            if isinstance(reward_func, str):\n                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(\n                    reward_func, num_labels=1, **model_init_kwargs\n                )\n        self.reward_funcs = reward_funcs\n\n        # Reward processing class\n        if reward_processing_classes is None:\n            reward_processing_classes = [None] * len(reward_funcs)\n        elif not isinstance(reward_processing_classes, list):\n            reward_processing_classes = [reward_processing_classes]\n        else:\n            if len(reward_processing_classes) != len(reward_funcs):\n                raise ValueError(\n                    \"The number of reward processing classes must match the number of reward functions.\"\n                )\n\n        for i, (reward_processing_class, reward_func) in enumerate(\n            zip(reward_processing_classes, reward_funcs)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if reward_processing_class is None:\n                    reward_processing_class = AutoTokenizer.from_pretrained(\n                        reward_func.config._name_or_path\n                    )\n                if reward_processing_class.pad_token_id is None:\n                    reward_processing_class.pad_token = (\n                        reward_processing_class.eos_token\n                    )\n                # The reward model computes the reward for the latest non-padded token in the input sequence.\n                # So it's important to set the pad token ID to the padding token ID of the processing class.\n                reward_func.config.pad_token_id = reward_processing_class.pad_token_id\n                reward_processing_classes[i] = reward_processing_class\n        self.reward_processing_classes = reward_processing_classes\n\n        # Data collator\n        def data_collator(features):  # No data collation is needed in GRPO\n            return features\n\n        # Training arguments\n        self.max_prompt_length = args.max_prompt_length\n        self.max_completion_length = (\n            args.max_completion_length\n        )  # = |o_i| in the GRPO paper\n        self.num_generations = args.num_generations  # = G in the GRPO paper\n        self.generation_config = GenerationConfig(\n            max_new_tokens=self.max_completion_length,\n            do_sample=True,\n            temperature=1,  # HACK\n            num_return_sequences=self.num_generations,\n            pad_token_id=pad_token_id,\n        )\n        self.beta = args.beta\n\n        # The trainer estimates the number of FLOPs (floating-point operations) using the number of elements in the\n        # input tensor associated with the key \"input_ids\". However, in GRPO, the sampled data does not include the\n        # \"input_ids\" key. Instead, the available keys is \"prompt\". As a result, the trainer issues the warning:\n        # \"Could not estimate the number of tokens of the input, floating-point operations will not be computed.\" To\n        # suppress this warning, we set the \"estimate_tokens\" key in the model's \"warnings_issued\" dictionary to True.\n        # This acts as a flag to indicate that the warning has already been issued.\n        model.warnings_issued[\"estimate_tokens\"] = True\n\n        # Initialize the metrics\n        self._metrics = defaultdict(list)\n        self.use_vllm = args.use_vllm\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=processing_class,\n            callbacks=callbacks,\n            optimizers=optimizers,\n        )\n        # Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\n        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\n        # self.model_accepts_loss_kwargs to False to enable scaling.\n        self.model_accepts_loss_kwargs = False\n\n        if self.use_vllm:\n            if not is_vllm_available():\n                raise ImportError(\n                    \"vLLM is not available and `use_vllm` is set to True. Please install vLLM with \"\n                    \"`pip install vllm` to use it.\"\n                )\n\n            if self.accelerator.is_main_process:\n                vllm_device = self.args.vllm_device\n                if vllm_device == \"auto\":\n                    vllm_device = f\"cuda:{self.accelerator.num_processes}\"  # take the next GPU idx\n                # Check that the requested device is available\n                if (\n                    vllm_device.split(\":\")[0] == \"cuda\"\n                    and int(vllm_device.split(\":\")[1]) >= torch.cuda.device_count()\n                ):\n                    raise ValueError(\n                        f\"The requested device for vllm ({vllm_device}) is not available. You are likely using vLLM \"\n                        \"without restricting the number of GPUs for training. Set the `--num_processes` argument to a \"\n                        \"value lower than the number of GPUs available on your machineâ€”typically, reducing it by one \"\n                        f\"is sufficient. In your case: `--num_processes {torch.cuda.device_count() - 1}`.\"\n                    )\n                # Check that the requested device is not also used for training\n                if vllm_device in {\n                    f\"cuda:{idx}\" for idx in range(self.accelerator.num_processes)\n                }:\n                    warnings.warn(\n                        f\"The requested device {vllm_device} is also used for training. This may lead to unexpected \"\n                        \"behavior. It is recommended to use a dedicated device for vLLM.\"\n                    )\n                # vLLM is not compatible with accelerate. So we need to patch it to make sure we can (1) place the vLLM\n                # model on the desired device (world_size_patch) and (2) avoid a test that is not designed for our\n                # setting (profiling_patch).\n                world_size_patch = patch(\n                    \"torch.distributed.get_world_size\", return_value=1\n                )\n                profiling_patch = patch(\n                    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n                    return_value=None,\n                )\n                with world_size_patch, profiling_patch:\n                    print(\"vllm is running on: \", vllm_device)\n                    self.llm = LLM(\n                        model=model.name_or_path,\n                        device=vllm_device,\n                        gpu_memory_utilization=self.args.vllm_gpu_memory_utilization,\n                        dtype=torch.bfloat16,\n                        # Automatic Prefix Caching caches the KV cache of existing queries, so that a new query can\n                        # directly reuse the KV cache if it shares the same prefix with one of the existing queries.\n                        # This is particularly useful here because we generate completions from the same prompts.\n                        enable_prefix_caching=True,\n                        enforce_eager=True,\n                        mm_processor_kwargs=(\n                            {\n                                \"max_pixels\": max_pixels,\n                                \"min_pixels\": min_pixels,\n                            }\n                            if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id\n                            else None\n                        ),\n                        max_model_len=args.max_prompt_length + args.max_completion_length,\n                    )\n                self.sampling_params = SamplingParams(\n                    temperature=args.temperature,\n                    max_tokens=self.max_completion_length,\n                )\n\n            self._last_loaded_step = 0  # tag to avoid useless loading during grad accumulation\n\n            # When using vLLM, the main process is responsible for loading the model weights. This can cause process\n            # desynchronization and seems to lead to DeepSpeed hanging during initialization. To prevent this, we\n            # synchronize all processes after vLLM has been fully initialized.\n            self.accelerator.wait_for_everyone()\n        else:\n            raise ValueError(\n                \"GRPOVLLMTrainerModified only supports vllm generation, please set --use_vllm True\"\n            )\n\n        if self.ref_model is not None:\n            if self.is_deepspeed_enabled:\n                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)\n            else:\n                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(reward_func, PreTrainedModel):\n                self.reward_funcs[i] = self.accelerator.prepare_model(reward_func, evaluation_mode=True)\n\n    def _set_signature_columns_if_needed(self):\n        # If `self.args.remove_unused_columns` is True, non-signature columns are removed.\n        # By default, this method sets `self._signature_columns` to the model's expected inputs.\n        # In GRPOTrainer, we preprocess data, so using the model's signature columns doesn't work.\n        # Instead, we set them to the columns expected by the `training_step` method, hence the override.\n        if self._signature_columns is None:\n            self._signature_columns = [\"prompt\"]\n\n    # Get the per-token log probabilities for the completions for the model and the reference model\n    def _get_per_token_logps(\n        self,\n        model,\n        input_ids,\n        attention_mask,\n        pixel_values,\n        image_grid_thw,\n        logits_to_keep,\n    ):  \n        if pixel_values is not None:\n            pixel_values = pixel_values.to(model.device)\n        if image_grid_thw is not None:\n            image_grid_thw = image_grid_thw.to(device=model.device)\n        logits = model(\n            input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            image_grid_thw=image_grid_thw,\n        ).logits  # (B, L, V)\n        logits = logits[\n            :, :-1, :\n        ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n        input_ids = input_ids[\n            :, -logits_to_keep:\n        ]  # (B, L-1), exclude the first input ID since we don't have logits for it\n        # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n        logits = logits[:, -logits_to_keep:]\n        per_token_logps = []\n        for logits_row, input_ids_row in zip(logits, input_ids):\n            log_probs = logits_row.log_softmax(dim=-1)\n            token_log_prob = torch.gather(\n                log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n            ).squeeze(1)\n            per_token_logps.append(token_log_prob)\n        return torch.stack(per_token_logps)\n\n    # Trainer \"prepares\" the inputs before calling `compute_loss`. It converts to tensor and move to device.\n    # Since we preprocess the data in `compute_loss`, we need to override this method to skip this step.\n    def _prepare_inputs(\n        self, inputs: dict[str, Union[torch.Tensor, Any]]\n    ) -> dict[str, Union[torch.Tensor, Any]]:\n        device = self.accelerator.device\n        \n        if \"prompt\" not in inputs[0]:\n            for example in inputs:\n                if example[\"has_image\"]:\n                    example[\"prompt\"] = [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\"type\": \"image\"},\n                                    {\"type\": \"text\", \"text\": IOU_QUESTION_TEMPLATE.format(Question=example[\"problem\"]) if \"reward_func\" in example and example[\"reward_func\"] == \"iou\" else QUESTION_TEMPLATE.format(Question=example[\"problem\"])},\n                                ],\n                            },\n                        ]\n                else:\n                    example[\"prompt\"] = [\n                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                            {\"role\": \"user\", \"content\": example[\"problem\"]},\n                        ]\n\n        prompts = [x[\"prompt\"] for x in inputs]\n        images = [x.get(\"image\", None) for x in inputs]\n        prompts_text = [\n            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n            for example in inputs\n        ]\n        prompt_inputs = self.processing_class(\n            text=copy.deepcopy(prompts_text),\n            images=images if images[0] else None,\n            return_tensors=\"pt\",\n            padding=True,\n            padding_side=\"left\",\n            add_special_tokens=False,\n        )\n        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"].to(device), prompt_inputs[\"attention_mask\"].to(device)\n        \n        if self.max_prompt_length is not None:\n            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n\n        if self.args.use_vllm:\n            # First, have main process load weights if needed\n            if self.state.global_step != self._last_loaded_step:\n                with unwrap_model_for_generation(\n                    self.model,\n                    self.accelerator,\n                    gather_deepspeed3_params=False,  # TODO: fix this, self.args.ds3_gather_for_generation,\n                ) as unwrapped_model:\n                    if is_compiled_module(unwrapped_model):\n                        state_dict = unwrapped_model._orig_mod.state_dict()\n                    else:\n                        state_dict = unwrapped_model.state_dict()\n                if self.accelerator.is_main_process:\n                    llm_model = (\n                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n                    )\n                    llm_model.load_weights(state_dict.items())\n                self._last_loaded_step = self.state.global_step\n\n            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n            all_prompts_text = gather_object(prompts_text)\n            all_images = gather_object(images)\n            # group into pairs\n            all_multimodal_inputs = []\n\n            use_naive_loop_sampling = False\n            if use_naive_loop_sampling:\n                # in this implementation, one sample will repeat `self.num_generations` times\n                # it's not a efficient implementation, but safe to keep sampling diversity\n                for prompt, image in zip(all_prompts_text, all_images):\n                    for _ in range(self.num_generations):\n                        if image:\n                            all_multimodal_inputs.append({\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}})\n                        else:\n                            all_multimodal_inputs.append({\"prompt\": prompt})\n                all_completion_ids = [None] * len(all_multimodal_inputs)\n                for i in range(self.num_generations):\n                    # Get the inputs for the current batch\n                    batch_inputs = [all_multimodal_inputs[j] for j in range(i, len(all_multimodal_inputs), self.num_generations)]\n                    if self.accelerator.is_main_process:\n                        outputs = self.llm.generate(\n                            batch_inputs,\n                            sampling_params=self.sampling_params,\n                            use_tqdm=False,\n                        )\n                        batch_completion_ids = [out.token_ids for completions in outputs for out in completions.outputs]\n                    else:\n                        batch_completion_ids = [None] * len(batch_inputs)\n                    # Place the results back into their original positions\n                    for idx, completion_id in enumerate(batch_completion_ids):\n                        all_completion_ids[i + idx * self.num_generations] = completion_id\n                # Final completion IDs\n                completion_ids = all_completion_ids\n\n            # 2. Refer to TobiasLee's implementation suggestions\n            # this is a better implementation for vLLM sampling.\n            for prompt, image in zip(all_prompts_text, all_images):\n                if image:\n                    all_multimodal_inputs.append({\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}})\n                else:\n                    all_multimodal_inputs.append({\"prompt\": prompt})\n            # Create sampling params with num_generations\n            if self.accelerator.is_main_process:\n                # Clone to avoid modifying original params\n                sampling_params = copy.deepcopy(self.sampling_params)\n                sampling_params.n = self.num_generations\n                # Single generate call with all prompts\n                if self.accelerator.is_main_process:\n                    outputs = self.llm.generate(\n                        all_multimodal_inputs,\n                        sampling_params=sampling_params,\n                        use_tqdm=False,\n                    )\n                # Flatten outputs: [prompt1_gen1, prompt1_gen2, ..., prompt2_gen1, prompt2_gen2, ...]\n                completion_ids = [out.token_ids for completion in outputs for out in completion.outputs]\n            else:\n                completion_ids = [None] * len(all_multimodal_inputs) * self.num_generations\n            \n            # broadcast and slice\n            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n            process_slice = slice(\n                self.accelerator.process_index * len(prompts) * self.num_generations,\n                (self.accelerator.process_index + 1) * len(prompts) * self.num_generations,\n            )\n            completion_ids = completion_ids[process_slice]\n\n            # Pad the completions, and concatenate them with the prompts\n            completion_ids = [torch.tensor(ids, device=device) for ids in completion_ids]\n            completion_ids = pad(\n                completion_ids, padding_value=self.processing_class.pad_token_id\n            )\n            prompt_ids = prompt_ids.repeat_interleave(self.num_generations, dim=0)\n            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n\n            prompt_length = prompt_ids.size(1)\n            prompt_ids = prompt_completion_ids[:, :prompt_length]\n            completion_ids = prompt_completion_ids[:, prompt_length:]\n            prompt_mask = prompt_mask.repeat_interleave(self.num_generations, dim=0)\n        else:\n            raise ValueError(\"Only vLLM generation is supported in this version \")\n\n        # below are the same with yifan's code\n        # Mask everything after the first EOS token\n        is_eos = completion_ids == self.processing_class.eos_token_id\n        device = self.accelerator.device\n        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)\n        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)\n        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n\n        # Concatenate prompt_mask with completion_mask for logit computation\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)\n        if \"pixel_values\" in prompt_inputs:\n            pixel_values = prompt_inputs[\"pixel_values\"][None].repeat_interleave(self.num_generations, dim=0)\n        else:\n            pixel_values = None\n        if \"image_grid_thw\" in prompt_inputs:\n            image_grid_thw = prompt_inputs[\"image_grid_thw\"].repeat_interleave(self.num_generations, dim=0)\n        else:\n            image_grid_thw = None\n        logits_to_keep = completion_ids.size(1)\n\n        with torch.inference_mode():\n            if self.ref_model is not None:\n                ref_per_token_logps = self._get_per_token_logps(\n                    self.ref_model,\n                    prompt_completion_ids,\n                    attention_mask,\n                    pixel_values,\n                    image_grid_thw,\n                    logits_to_keep,\n                )\n            else:\n                with self.accelerator.unwrap_model(self.model).disable_adapter():\n                    ref_per_token_logps = self._get_per_token_logps(\n                        self.model,\n                        prompt_completion_ids,\n                        attention_mask,\n                        pixel_values,\n                        image_grid_thw,\n                        logits_to_keep,\n                    )\n\n        # Decode the generated completions\n        completions = self.processing_class.batch_decode(\n            completion_ids, skip_special_tokens=True\n        )\n        if is_conversational(inputs[0]):\n            completions = [\n                [{\"role\": \"assistant\", \"content\": completion}]\n                for completion in completions\n            ]\n\n        # Compute the rewards\n        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n        rewards_per_func = torch.zeros(\n            len(prompts), len(self.reward_funcs), device=device\n        )\n        for i, (reward_func, reward_processing_class) in enumerate(\n            zip(self.reward_funcs, self.reward_processing_classes)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if is_conversational(inputs[0]):\n                    messages = [{\"messages\": p + c} for p, c in zip(prompts, completions)]\n                    texts = [\n                        apply_chat_template(x, reward_processing_class)[\"text\"]\n                        for x in messages\n                    ]\n                else:\n                    texts = [p + c for p, c in zip(prompts, completions)]\n                reward_inputs = reward_processing_class(\n                    texts,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    padding_side=\"right\",\n                    add_special_tokens=False,\n                )\n                reward_inputs = super()._prepare_inputs(reward_inputs)\n                with torch.inference_mode():\n                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]  # Shape (B*G,)\n            else:\n                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n                reward_kwargs = {\n                    key: []\n                    for key in inputs[0].keys()\n                    if key not in [\"prompt\", \"completion\"]\n                }\n                for key in reward_kwargs:\n                    for example in inputs:\n                        # Repeat each value in the column for `num_generations` times\n                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n                output_reward_func = reward_func(\n                    prompts=prompts, completions=completions, **reward_kwargs\n                )\n                rewards_per_func[:, i] = torch.tensor(\n                    output_reward_func, dtype=torch.float32, device=device\n                )\n        rewards_per_func = gather(rewards_per_func)\n        # Sum the rewards from all reward functions\n        rewards = rewards_per_func.sum(dim=1)\n\n        # Compute grouped-wise rewards\n        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n\n        # Normalize the rewards to compute the advantages\n        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n\n        # Slice to keep only the local part of the data\n        process_slice = slice(\n            self.accelerator.process_index * len(prompts),\n            (self.accelerator.process_index + 1) * len(prompts),\n        )\n        advantages = advantages[process_slice]\n\n        # Log the metrics\n        reward_per_func = rewards_per_func.mean(0)\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(\n                reward_func, nn.Module\n            ):  # Module instead of PretrainedModel for compat with compiled models\n                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n            else:\n                reward_func_name = reward_func.__name__\n            self._metrics[f\"rewards/{reward_func_name}\"].append(\n                reward_per_func[i].item()\n            )\n\n        self._metrics[\"reward\"].append(rewards.mean().item())\n        self._metrics[\"reward_std\"].append(std_grouped_rewards.mean().item())\n\n        return {\n            \"prompt_ids\": prompt_ids,\n            \"prompt_mask\": prompt_mask,\n            \"completion_ids\": completion_ids,\n            \"completion_mask\": completion_mask,\n            \"ref_per_token_logps\": ref_per_token_logps,\n            \"advantages\": advantages,\n            \"pixel_values\": pixel_values,\n            \"image_grid_thw\": image_grid_thw,\n        }\n\n    def compute_loss(\n        self, model, inputs, return_outputs=False, num_items_in_batch=None\n    ):\n        if return_outputs:\n            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n        # Compute the per-token log probabilities for the model\n\n        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n        completion_ids, completion_mask = inputs[\"completion_ids\"], inputs[\"completion_mask\"]\n        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n        pixel_values = inputs.get(\"pixel_values\", None)\n        image_grid_thw = inputs.get(\"image_grid_thw\", None)\n        logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens\n\n        per_token_logps = self._get_per_token_logps(\n            model,\n            input_ids,\n            attention_mask,\n            pixel_values,\n            image_grid_thw,\n            logits_to_keep,\n        )\n\n        # Compute the KL divergence between the model and the reference model\n        ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n        per_token_kl = (torch.exp(ref_per_token_logps - per_token_logps)- (ref_per_token_logps - per_token_logps)- 1)\n\n        # x - x.detach() allows for preserving gradients from x\n        advantages = inputs[\"advantages\"]\n        per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)\n        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n        loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n\n        # Log the metrics\n        completion_length = (\n            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n            .float()\n            .mean()\n            .item()\n        )\n        self._metrics[\"completion_length\"].append(completion_length)\n\n        mean_kl = (\n            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n        ).mean()\n        self._metrics[\"kl\"].append(\n            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n        )\n\n        return loss\n\n        \n    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n        metrics = {key: sum(val) / len(val) for key, val in self._metrics.items()}  # average the metrics\n\n        # This method can be called both in training and evaluation. When called in evaluation, the keys in `logs`\n        # start with \"eval_\". We need to add the prefix \"eval_\" to the keys in `metrics` to match the format.\n        if next(iter(logs.keys())).startswith(\"eval_\"):\n            metrics = {f\"eval_{key}\": val for key, val in metrics.items()}\n\n        logs = {**logs, **metrics}\n        if version.parse(transformers.__version__) >= version.parse(\"4.47.0.dev0\"):\n            super().log(logs, start_time)\n        else:  # transformers<=4.46\n            super().log(logs)\n        self._metrics.clear()"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/trainer/__init__.py", "content": "from .grpo_trainer import Qwen2VLGRPOTrainer\nfrom .vllm_grpo_trainer import Qwen2VLGRPOVLLMTrainer \nfrom .vllm_grpo_trainer_modified import Qwen2VLGRPOVLLMTrainerModified\n\n__all__ = [\n    \"Qwen2VLGRPOTrainer\", \n    \"Qwen2VLGRPOVLLMTrainer\",\n    \"Qwen2VLGRPOVLLMTrainerModified\"\n]"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/reward.py", "content": "from math_verify import parse, verify, LatexExtractionConfig\nfrom latex2sympy2_extended import NormalizationConfig\nimport re\nfrom datetime import datetime\nfrom Levenshtein import ratio\nfrom babel.numbers import parse_decimal\nimport math\nimport random\nfrom typing import Optional, Dict\nimport  os\n\nformat_reward_factor = float(os.getenv(\"FORMAT_REWARD_FACTOR\", 1.0))\n\ndef process_expression(s):\n    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤æ‰€æœ‰è¿ç®—ç¬¦ï¼ˆ=+ã€-ã€*ã€/ï¼‰å‘¨å›´çš„ç©ºæ ¼\n    return re.sub(r'\\s*([=+\\-*/])\\s*', r'\\1', s)\n\n\ndef extract_choice(text):\n    # 1. Clean and normalize text\n    text = text.upper()  # Convert to uppercase\n    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n\n    # 2. Choice should not have uppercase letters before or after\n    choices = re.findall(r'(?<![A-Z])([A-Z])(?![A-Z])', text)\n\n    if not choices:\n        return None\n\n    # 3. If only one choice, return it directly\n    if len(choices) == 1:\n        return choices[0]\n\n    # 4. If multiple choices, use heuristic rules\n    choice_scores = {choice: 0 for choice in choices}\n\n    # 4.1 Keywords around choices get points\n    keywords = [\n        'ç­”æ¡ˆ', 'é€‰æ‹©', 'æ­£ç¡®', 'æ˜¯', 'å¯¹',\n        'answer', 'correct', 'choose', 'select', 'right',\n        'è®¤ä¸º', 'åº”è¯¥', 'è§‰å¾—', 'think', 'believe', 'should'\n    ]\n\n    # Get context for each choice (20 chars before and after)\n    for choice in choices:\n        pos = text.find(choice)\n        context = text[max(0, pos-20):min(len(text), pos+20)]\n\n        # Add points for keywords\n        for keyword in keywords:\n            if keyword.upper() in context:\n                choice_scores[choice] += 1\n\n        # Add points if choice is near the end (usually final answer)\n        if pos > len(text) * 0.7:  # In last 30% of text\n            choice_scores[choice] += 2\n\n        # Add points if followed by punctuation\n        if pos < len(text) - 1 and text[pos+1] in 'ã€‚.!ï¼,ï¼Œ':\n            choice_scores[choice] += 1\n\n    # Return highest scoring choice\n    return max(choice_scores.items(), key=lambda x: x[1])[0]\n\n\ndef numeric_reward(content, sol, **kwargs):\n    content = clean_text(content)\n    sol = clean_text(sol)\n    try:\n        content, sol = float(content), float(sol)\n        return 1.0 if content == sol else 0.0\n    except:\n        return None\n\ndef clean_text(text, exclue_chars=['\\n', '\\r']):\n    # Extract content between <answer> and </answer> if present\n    answer_matches = re.findall(r'<answer>(.*?)</answer>', text, re.DOTALL)\n    if answer_matches:\n        # Use the last match\n        text = answer_matches[-1]\n    \n    for char in exclue_chars:\n        if char in ['\\n', '\\r']:\n            # If there is a space before the newline, remove the newline\n            text = re.sub(r'(?<=\\s)' + re.escape(char), '', text)\n            # If there is no space before the newline, replace it with a space\n            text = re.sub(r'(?<!\\s)' + re.escape(char), ' ', text)\n        else:\n            text = text.replace(char, ' ')\n    \n    # Remove leading and trailing spaces and convert to lowercase\n    return text.strip().rstrip('.').lower()\n\ndef iou_reward(completion, sol, **kwargs):\n    def iou(box1, box2):\n        inter_x1 = max(box1[0], box2[0])\n        inter_y1 = max(box1[1], box2[1])\n        inter_x2 = min(box1[2]-1, box2[2]-1)\n        inter_y2 = min(box1[3]-1, box2[3]-1)\n        if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n            inter = (inter_x2-inter_x1+1)*(inter_y2-inter_y1+1)\n        else:\n            inter = 0\n        union = (box1[2]-box1[0])*(box1[3]-box1[1]) + (box2[2]-box2[0])*(box2[3]-box2[1]) - inter\n        return float(inter)/union\n    \n    content = completion[0][\"content\"]\n    answer_tag_pattern = r'<answer>(.*?)</answer>'\n    # bbox_pattern = r'\\[(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*),\\s*(\\s*-?\\d*\\.?\\d+\\s*)\\]'\n    bbox_pattern = r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)]'\n    \n    reward = 0.0\n    # Try symbolic verification first\n    try:\n        content_answer_match = re.search(answer_tag_pattern, content, re.DOTALL)\n        sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n        ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n        ground_truth = eval(ground_truth)\n            \n        if content_answer_match:\n            content_answer = content_answer_match.group(1).strip()\n            bbox_match = re.search(bbox_pattern, content_answer)\n            if bbox_match:\n                bbox = [int(bbox_match.group(1)), int(bbox_match.group(2)), int(bbox_match.group(3)), int(bbox_match.group(4))]\n                if iou(bbox, ground_truth) > 0.5:\n                    reward = 1.0\n    except Exception:\n        pass  # Continue to next verification method if this fails\n                \n    return reward\n\ndef default_format_reward(completion, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    completion_content = completion[0][\"content\"]\n    match = re.fullmatch(pattern, completion_content, re.DOTALL) \n    return 1.0 if match else 0.0 \n\ndef iou_format_reward(completion, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    # pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?\\{.*\\[\\d+,\\s*\\d+,\\s*\\d+,\\s*\\d+\\].*\\}.*?</answer>\"\n    completion_content = completion[0][\"content\"]\n    match = re.fullmatch(pattern, completion_content, re.DOTALL)\n    return 1.0 if match else 0.0\n\ndef format_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using symbolic verification, exact string matching, or fuzzy matching.\"\"\"\n    # contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    for content, sol, accu_reward_method in zip(completions, solution, kwargs.get(\"reward_func\")):\n        # if accu_reward_method is defined, use the corresponding reward function, otherwise use the default reward function\n        if accu_reward_method == \"iou\":\n            reward = iou_format_reward(content)\n        else:\n            reward = default_format_reward(content)  \n        \n        reward *= format_reward_factor\n        rewards.append(reward)\n\n    return rewards\n\n\ndef default_accuracy_reward(completion, sol, **kwargs):\n    content = completion[0][\"content\"]\n    reward = 0.0\n    # Try symbolic verification first for numeric answers\n    try:\n        answer = parse(content)\n        if float(verify(answer, parse(sol))) > 0:\n            reward = 1.0\n    except Exception:\n        pass  # Continue to next verification method if this fails\n\n    # If symbolic verification failed, try string matching or fuzzy matching\n    if reward == 0.0:\n        try:\n            # Extract answer from solution if it has think/answer tags\n            sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n            ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n            \n            # Extract answer from content if it has think/answer tags\n            content_matches = re.findall(r'<answer>(.*?)</answer>', content, re.DOTALL)\n            student_answer = content_matches[-1].strip() if content_matches else content.strip()\n            \n            student_answer = process_expression(student_answer)\n            ground_truth = process_expression(ground_truth)\n\n            # Compare the extracted answers\n            if student_answer == ground_truth:\n                reward = 1.0\n            \n            if reward == 0.0:\n                # Check if ground truth contains numbers\n                has_numbers = bool(re.search(r'\\d', ground_truth))\n                # Check if it's a multiple choice question\n                has_choices = extract_choice(ground_truth)\n                \n                if has_numbers:\n                    # For numeric answers, use exact matching\n                    reward = numeric_reward(student_answer, ground_truth)\n                    if reward is None:\n                        reward = ratio(clean_text(student_answer), clean_text(ground_truth))\n                elif has_choices:\n                    # For multiple choice, extract and compare choices\n                    correct_choice = has_choices.upper()\n                    student_choice = extract_choice(student_answer)\n                    if student_choice:\n                        reward = 1.0 if student_choice == correct_choice else 0.0\n                else:\n                    # For text answers, use fuzzy matching\n                    reward = ratio(clean_text(student_answer), clean_text(ground_truth))\n            \n            # The answer partially mismatches the ground truth\n            if reward <= 0.6:\n                reward = 0.0\n            elif reward > 0.6 and reward < 1.0:\n                reward = 0.1 \n                \n        except Exception:\n            pass  # Keep reward as 0.0 if all methods fail\n\n    return reward\n\ndef accuracy_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using symbolic verification, exact string matching, or fuzzy matching.\"\"\"\n    # contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    for content, sol, accu_reward_method in zip(completions, solution, kwargs.get(\"reward_func\")):\n        # if accu_reward_method is defined, use the corresponding reward function, otherwise use the default reward function\n        if accu_reward_method == \"iou\":\n            reward = iou_reward(content, sol)\n        else:\n            reward = default_accuracy_reward(content, sol)  \n        rewards.append(reward)\n        \n    if os.getenv(\"DEBUG_MODE\") == \"true\":\n        log_path = os.getenv(\"LOG_PATH\")\n        current_time = datetime.now().strftime(\"%d-%H-%M-%S-%f\")\n        # image_path = kwargs.get(\"image_path\")[0]\n        problem = kwargs.get(\"problem\")[0]\n        with open(log_path, \"a\", encoding='utf-8') as f:\n            f.write(f\"------------- {current_time} Accuracy reward: {reward} -------------\\n\")\n            f.write(f\"accu_reward_method: {accu_reward_method}\\n\")\n            # f.write(f\"image_path: {image_path}\\n\")\n            f.write(f\"problem: {problem}\\n\")\n            f.write(f\"Content: {content}\\n\")\n            f.write(f\"Solution: {sol}\\n\")     \n\n        \n    return rewards\n\n\ndef origin_accuracy_reward(completions, solution, **kwargs):\n    \"\"\"Reward function that checks if the completion is correct using either symbolic verification or exact string matching.\"\"\"\n    contents = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    current_time = datetime.now().strftime(\"%d-%H-%M-%S-%f\")\n    for content, sol in zip(contents, solution):\n        reward = 0.0\n        # Try symbolic verification first\n        try:\n            answer = parse(content)\n            if float(verify(answer, parse(sol))) > 0:\n                reward = 1.0\n        except Exception:\n            pass  # Continue to next verification method if this fails\n\n        # If symbolic verification failed, try string matching\n        if reward == 0.0:\n            try:\n                # Extract answer from solution if it has think/answer tags\n                sol_match = re.search(r'<answer>(.*?)</answer>', sol)\n                ground_truth = sol_match.group(1).strip() if sol_match else sol.strip()\n                \n                # Extract answer from content if it has think/answer tags\n                content_match = re.search(r'<answer>(.*?)</answer>', content)\n                student_answer = content_match.group(1).strip() if content_match else content.strip()\n                \n                # Compare the extracted answers\n                if student_answer == ground_truth:\n                    reward = 1.0\n\n                # è¡¨è¾¾å¼åˆ¤æ–­\n                # if reward == 0.0:\n                #     processed_student_answer = process_expression(student_answer)\n                #     processed_ground_truth = process_expression(ground_truth)\n                #     # Compare the extracted answers\n                #     if processed_student_answer == processed_ground_truth:\n                #         reward = 1.0\n\n            except Exception:\n                pass  # Keep reward as 0.0 if both methods fail\n                \n        rewards.append(reward)\n        if os.getenv(\"DEBUG_MODE\") == \"true\":\n            log_path = os.getenv(\"LOG_PATH\")\n            # local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n            with open(log_path, \"a\") as f:\n                f.write(f\"------------- {current_time} Accuracy reward: {reward} -------------\\n\")\n                f.write(f\"Content: {content}\\n\")\n                f.write(f\"Solution: {sol}\\n\")\n    return rewards\n\n\ndef origin_format_reward(completions, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n    completion_contents = [completion[0][\"content\"] for completion in completions]\n    matches = [re.fullmatch(pattern, content, re.DOTALL) for content in completion_contents]\n    return [1.0 if match else 0.0 for match in matches]\n\n\n\ndef get_repetition_penalty_reward(ngram_size: int, max_penalty: float):\n    \"\"\"\n    Computes N-gram repetition penalty as described in Appendix C.2 of https://arxiv.org/abs/2502.03373.\n    Reference implementation from: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n\n    Args:\n    ngram_size: size of the n-grams\n    max_penalty: Maximum (negative) penalty for wrong answers\n    \"\"\"\n    if max_penalty > 0:\n        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n\n    def zipngram(text: str, ngram_size: int):\n        words = text.lower().split()\n        return zip(*[words[i:] for i in range(ngram_size)])\n\n    def repetition_penalty_reward(completions, **kwargs) -> float:\n        \"\"\"\n        reward function the penalizes repetitions\n        ref implementation: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py\n\n        Args:\n            completions: List of model completions\n        \"\"\"\n\n        contents = [completion[0][\"content\"] for completion in completions]\n        rewards = []\n        for completion in contents:\n            if completion == \"\":\n                rewards.append(0.0)\n                continue\n            if len(completion.split()) < ngram_size:\n                rewards.append(0.0)\n                continue\n\n            ngrams = set()\n            total = 0\n            for ng in zipngram(completion, ngram_size):\n                ngrams.add(ng)\n                total += 1\n\n            scaling = 1 - len(ngrams) / total\n            reward = scaling * max_penalty\n            rewards.append(reward)\n        return rewards\n\n    return repetition_penalty_reward\n\ndef get_cosine_scaled_reward(\n    min_value_wrong: float = -1.0,\n    max_value_wrong: float = -0.5,\n    min_value_correct: float = 0.5,\n    max_value_correct: float = 1.0,\n    max_len: int = 1000,\n):\n    def cosine_scaled_reward(completions, solution, **kwargs):\n        \"\"\"Reward function that scales based on completion length using a cosine schedule.\n\n        Shorter correct solutions are rewarded more than longer ones.\n        Longer incorrect solutions are penalized less than shorter ones.\n\n        Args:\n            completions: List of model completions\n            solution: List of ground truth solutions\n\n        This function is parameterized by the following arguments:\n            min_value_wrong: Minimum reward for wrong answers\n            max_value_wrong: Maximum reward for wrong answers\n            min_value_correct: Minimum reward for correct answers\n            max_value_correct: Maximum reward for correct answers\n            max_len: Maximum length for scaling\n        \"\"\"\n        contents = [completion[0][\"content\"] for completion in completions]\n        rewards = []\n\n        for content, sol in zip(contents, solution):\n            gold_parsed = parse(sol, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n            if len(gold_parsed) == 0:\n                rewards.append(1.0)  # Skip unparseable examples\n                print(\"Failed to parse gold solution: \", sol)\n                continue\n\n            answer_parsed = parse(\n                content,\n                extraction_config=[\n                    LatexExtractionConfig(\n                        normalization_config=NormalizationConfig(\n                            nits=False,\n                            malformed_operators=False,\n                            basic_latex=True,\n                            equations=True,\n                            boxed=True,\n                            units=True,\n                        ),\n                        boxed_match_priority=0,\n                        try_extract_without_anchor=False,\n                    )\n                ],\n                extraction_mode=\"first_match\",\n            )\n\n            is_correct = verify(answer_parsed, gold_parsed)\n            gen_len = len(content)\n\n            # Apply cosine scaling based on length\n            progress = gen_len / max_len\n            cosine = math.cos(progress * math.pi)\n\n            if is_correct:\n                min_value = min_value_correct\n                max_value = max_value_correct\n            else:\n                # Swap min/max for incorrect answers\n                min_value = max_value_wrong\n                max_value = min_value_wrong\n\n            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n            rewards.append(float(reward))\n\n        return rewards\n\n    return cosine_scaled_reward\n\ndef reasoning_steps_reward(completions, **kwargs):\n    r\"\"\"Reward function that checks for clear step-by-step reasoning.\n    Regex pattern:\n        Step \\d+: - matches \"Step 1:\", \"Step 2:\", etc.\n        ^\\d+\\. - matches numbered lists like \"1.\", \"2.\", etc. at start of line\n        \\n- - matches bullet points with hyphens\n        \\n\\* - matches bullet points with asterisks\n        First,|Second,|Next,|Finally, - matches transition words\n    \"\"\"\n    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n    completion_contents = [completion[0][\"content\"] for completion in completions]\n    matches = [len(re.findall(pattern, content)) for content in completion_contents]\n\n    # Magic nubmer 3 to encourage 3 steps and more, otherwise partial reward\n    return [min(1.0, count / 3) for count in matches]\n\ndef len_reward(completions: list[Dict[str, str]], solution: list[str], **kwargs) -> float:\n    \"\"\"Compute length-based rewards to discourage overthinking and promote token efficiency.\n\n    Taken from from the Kimi 1.5 tech report: https://arxiv.org/abs/2501.12599\n\n    Args:\n        completions: List of model completions\n        solution: List of ground truth solutions\n\n    Returns:\n        List of rewards where:\n        - For correct answers: reward = 0.5 - (len - min_len)/(max_len - min_len)\n        - For incorrect answers: reward = min(0, 0.5 - (len - min_len)/(max_len - min_len))\n    \"\"\"\n    contents = [completion[0][\"content\"] for completion in completions]\n\n    # First check correctness of answers\n    correctness = []\n    for content, sol in zip(contents, solution):\n        gold_parsed = parse(\n            sol,\n            extraction_mode=\"first_match\",\n            extraction_config=[LatexExtractionConfig()],\n        )\n        if len(gold_parsed) == 0:\n            # Skip unparseable examples\n            correctness.append(True)  # Treat as correct to avoid penalizing\n            print(\"Failed to parse gold solution: \", sol)\n            continue\n        answer_parsed = parse(\n            content,\n            extraction_config=[\n                LatexExtractionConfig(\n                    normalization_config=NormalizationConfig(\n                        nits=False,\n                        malformed_operators=False,\n                        basic_latex=True,\n                        equations=True,\n                        boxed=True,\n                        units=True,\n                    ),\n                    boxed_match_priority=0,\n                    try_extract_without_anchor=False,\n                )\n            ],\n            extraction_mode=\"first_match\",\n        )\n        correctness.append(verify(answer_parsed, gold_parsed))\n\n    # Calculate lengths\n    lengths = [len(content) for content in contents]\n    min_len = min(lengths)\n    max_len = max(lengths)\n\n    # If all responses have the same length, return zero rewards\n    if max_len == min_len:\n        return [0.0] * len(completions)\n\n    rewards = []\n    for length, is_correct in zip(lengths, correctness):\n        lambda_val = 0.5 - (length - min_len) / (max_len - min_len)\n\n        if is_correct:\n            reward = lambda_val\n        else:\n            reward = min(0, lambda_val)\n\n        rewards.append(float(reward))\n\n    return rewards\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/trainer/vllm_grpo_trainer.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport textwrap\nfrom collections import defaultdict\nfrom typing import Any, Callable, Optional, Union\nfrom accelerate.utils.other import is_compiled_module\nfrom accelerate.utils import broadcast_object_list, gather, gather_object\nimport torch\nimport torch.utils.data\nimport transformers\nimport warnings\nfrom unittest.mock import patch\nfrom datasets import Dataset, IterableDataset\nfrom packaging import version\nfrom transformers import (\n    AriaForConditionalGeneration,\n    AriaProcessor,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoProcessor,\n    AutoTokenizer,\n    GenerationConfig,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Qwen2VLForConditionalGeneration,\n    Qwen2_5_VLForConditionalGeneration,\n    Trainer,\n    TrainerCallback,\n    is_wandb_available,\n)\nfrom transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.utils import is_peft_available\n\nfrom trl.data_utils import (\n    apply_chat_template,\n    is_conversational,\n    maybe_apply_chat_template,\n)\nfrom trl.import_utils import is_vllm_available\n\nfrom trl.models import (\n    create_reference_model,\n    prepare_deepspeed,\n    unwrap_model_for_generation,\n)\nfrom trl.trainer.grpo_config import GRPOConfig\nfrom trl.trainer.utils import generate_model_card, get_comet_experiment_url, pad\nfrom trl import GRPOTrainer\n\nimport copy\n\nif is_peft_available():\n    from peft import PeftConfig, get_peft_model\n\nif is_vllm_available():\n    from vllm import LLM, SamplingParams\n\n\nif is_wandb_available():\n    import wandb\n    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\nimport torch.nn as nn\nfrom torch.utils.data import Sampler\n\n# What we call a reward function is a callable that takes a list of prompts and completions and returns a list of\n# rewards. When it's a string, it's a model ID, so it's loaded as a pretrained model.\nRewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n\nSYSTEM_PROMPT = (\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n    \"<think> reasoning process here </think><answer> answer here </answer>\"\n)\n\nQUESTION_TEMPLATE = \"{Question}  Output the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\nIOU_QUESTION_TEMPLATE = \"{Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n  \n\nclass RepeatRandomSampler(Sampler):\n    \"\"\"\n    Sampler that repeats the indices of a dataset N times.\n\n    Args:\n        data_source (`Sized`):\n            Dataset to sample from.\n        repeat_count (`int`):\n            Number of times to repeat each index.\n\n    Example:\n    ```python\n    >>> sampler = RepeatRandomSampler([\"a\", \"b\", \"c\", \"d\"], repeat_count=2)\n    >>> list(sampler)\n    [2, 2, 0, 0, 3, 3, 1, 1]\n    ```\n    \"\"\"\n\n    def __init__(self, data_source, repeat_count: int):\n        self.data_source = data_source\n        self.repeat_count = repeat_count\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        indexes = [\n            idx\n            for idx in torch.randperm(self.num_samples).tolist()\n            for _ in range(self.repeat_count)\n        ]\n        return iter(indexes)\n\n    def __len__(self):\n        return self.num_samples * self.repeat_count\n\n\nclass Qwen2VLGRPOVLLMTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[str, PreTrainedModel],\n        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n        args: GRPOConfig = None,\n        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n        eval_dataset: Optional[\n            Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]\n        ] = None,\n        processing_class: Optional[PreTrainedTokenizerBase] = None,\n        reward_processing_classes: Optional[\n            Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]\n        ] = None,\n        callbacks: Optional[list[TrainerCallback]] = None,\n        optimizers: tuple[\n            Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]\n        ] = (None, None),\n        peft_config: Optional[\"PeftConfig\"] = None,\n        # qwen2-vl related params\n        max_pixels: Optional[int] = 12845056,\n        min_pixels: Optional[int] = 3136,\n        attn_implementation: str = \"flash_attention_2\",\n    ):\n\n        # Args\n        if args is None:\n            model_name = model if isinstance(model, str) else model.config._name_or_path\n            model_name = model_name.split(\"/\")[-1]\n            args = GRPOConfig(f\"{model_name}-GRPO\")\n\n        # Models\n        # Trained model\n        model_init_kwargs = args.model_init_kwargs or {}\n        model_init_kwargs[\"attn_implementation\"] = attn_implementation\n        if isinstance(model, str):\n            model_id = model\n            torch_dtype = model_init_kwargs.get(\"torch_dtype\")\n            if (\n                isinstance(torch_dtype, torch.dtype)\n                or torch_dtype == \"auto\"\n                or torch_dtype is None\n            ):\n                pass  # torch_dtype is already a torch.dtype or \"auto\" or None\n            elif isinstance(torch_dtype, str):  # it's a str, but not \"auto\"\n                torch_dtype = getattr(torch, torch_dtype)\n                model_init_kwargs[\"torch_dtype\"] = torch_dtype\n            else:\n                raise ValueError(\n                    \"Invalid `torch_dtype` passed to `GRPOConfig`. Expected either 'auto' or a string representing \"\n                    f\"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}.\"\n                )\n            # Disable caching if gradient checkpointing is enabled (not supported)\n            model_init_kwargs[\"use_cache\"] = (\n                False\n                if args.gradient_checkpointing\n                else model_init_kwargs.get(\"use_cache\")\n            )\n            if \"Qwen2-VL\" in model_id:\n                model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Qwen2.5-VL\" in model_id:\n                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                model_init_kwargs.pop(\"use_cache\")\n                model = AriaForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            else:\n                model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n        else:\n            model_id = model.config._name_or_path\n            if args.model_init_kwargs is not None:\n                raise ValueError(\n                    \"You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. \"\n                    \"This argument can only be used when the `model` argument is a string.\"\n                )\n\n        if peft_config is not None:\n            model = get_peft_model(model, peft_config)\n\n        # Reference model\n        if is_deepspeed_zero3_enabled():\n            if \"Qwen2-VL\" in model_id:\n                self.ref_model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            elif \"Qwen2.5-VL\" in model_id:\n                self.ref_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                self.ref_model = AriaForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            else:\n                self.ref_model = AutoModelForCausalLM.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n        elif peft_config is None:\n            # If PEFT configuration is not provided, create a reference model based on the initial model.\n            self.ref_model = create_reference_model(model)\n        else:\n            # If PEFT is used, the reference model is not needed since the adapter can be disabled\n            # to revert to the initial model.\n            self.ref_model = None\n\n        # Processing class\n        if processing_class is None:\n            if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(\n                    model.config._name_or_path, padding_side=\"left\"\n                )\n                pad_token_id = processing_class.pad_token_id\n\n        # Reward functions\n        if not isinstance(reward_funcs, list):\n            reward_funcs = [reward_funcs]\n        for i, reward_func in enumerate(reward_funcs):\n            if isinstance(reward_func, str):\n                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(\n                    reward_func, num_labels=1, **model_init_kwargs\n                )\n        self.reward_funcs = reward_funcs\n\n        # Reward processing class\n        if reward_processing_classes is None:\n            reward_processing_classes = [None] * len(reward_funcs)\n        elif not isinstance(reward_processing_classes, list):\n            reward_processing_classes = [reward_processing_classes]\n        else:\n            if len(reward_processing_classes) != len(reward_funcs):\n                raise ValueError(\n                    \"The number of reward processing classes must match the number of reward functions.\"\n                )\n\n        for i, (reward_processing_class, reward_func) in enumerate(\n            zip(reward_processing_classes, reward_funcs)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if reward_processing_class is None:\n                    reward_processing_class = AutoTokenizer.from_pretrained(\n                        reward_func.config._name_or_path\n                    )\n                if reward_processing_class.pad_token_id is None:\n                    reward_processing_class.pad_token = (\n                        reward_processing_class.eos_token\n                    )\n                # The reward model computes the reward for the latest non-padded token in the input sequence.\n                # So it's important to set the pad token ID to the padding token ID of the processing class.\n                reward_func.config.pad_token_id = reward_processing_class.pad_token_id\n                reward_processing_classes[i] = reward_processing_class\n        self.reward_processing_classes = reward_processing_classes\n\n        # Data collator\n        def data_collator(features):  # No data collation is needed in GRPO\n            return features\n\n        # Training arguments\n        self.max_prompt_length = args.max_prompt_length\n        self.max_completion_length = (\n            args.max_completion_length\n        )  # = |o_i| in the GRPO paper\n        self.num_generations = args.num_generations  # = G in the GRPO paper\n        self.generation_config = GenerationConfig(\n            max_new_tokens=self.max_completion_length,\n            do_sample=True,\n            temperature=1,  # HACK\n            num_return_sequences=self.num_generations,\n            pad_token_id=pad_token_id,\n        )\n        self.beta = args.beta\n\n        # The trainer estimates the number of FLOPs (floating-point operations) using the number of elements in the\n        # input tensor associated with the key \"input_ids\". However, in GRPO, the sampled data does not include the\n        # \"input_ids\" key. Instead, the available keys is \"prompt\". As a result, the trainer issues the warning:\n        # \"Could not estimate the number of tokens of the input, floating-point operations will not be computed.\" To\n        # suppress this warning, we set the \"estimate_tokens\" key in the model's \"warnings_issued\" dictionary to True.\n        # This acts as a flag to indicate that the warning has already been issued.\n        model.warnings_issued[\"estimate_tokens\"] = True\n\n        # Initialize the metrics\n        self._metrics = defaultdict(list)\n        self.use_vllm = args.use_vllm\n\n        # rewrite the processing AutoTokenizer -> AutoProcessor\n        model_id = model if isinstance(model, str) else model.config._name_or_path\n        if processing_class is None:\n            if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(\n                    model.config._name_or_path, padding_side=\"left\"\n                )\n                pad_token_id = processing_class.pad_token_id\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=processing_class,\n            callbacks=callbacks,\n            optimizers=optimizers,\n        )\n        # Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\n        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\n        # self.model_accepts_loss_kwargs to False to enable scaling.\n        self.model_accepts_loss_kwargs = False\n        # Check if the per_device_train/eval_batch_size * num processes can be divided by the number of generations\n        num_processes = self.accelerator.num_processes\n        global_batch_size = args.per_device_train_batch_size * num_processes\n        possible_values = [\n            n_gen\n            for n_gen in range(2, global_batch_size + 1)\n            if (global_batch_size) % n_gen == 0\n        ]\n\n        if self.num_generations not in possible_values:\n            raise ValueError(\n                f\"The global train batch size ({num_processes} x {args.per_device_train_batch_size}) must be evenly \"\n                f\"divisible by the number of generations per prompt ({self.num_generations}). Given the current train \"\n                f\"batch size, the valid values for the number of generations are: {possible_values}.\"\n            )\n        if self.args.eval_strategy != \"no\":\n            global_batch_size = args.per_device_eval_batch_size * num_processes\n            possible_values = [\n                n_gen\n                for n_gen in range(2, global_batch_size + 1)\n                if (global_batch_size) % n_gen == 0\n            ]\n            if self.num_generations not in possible_values:\n                raise ValueError(\n                    f\"The global eval batch size ({num_processes} x {args.per_device_eval_batch_size}) must be evenly \"\n                    f\"divisible by the number of generations per prompt ({self.num_generations}). Given the current \"\n                    f\"eval batch size, the valid values for the number of generations are: {possible_values}.\"\n                )\n\n        if self.use_vllm:\n            if not is_vllm_available():\n                raise ImportError(\n                    \"vLLM is not available and `use_vllm` is set to True. Please install vLLM with \"\n                    \"`pip install vllm` to use it.\"\n                )\n\n            if self.accelerator.is_main_process:\n                vllm_device = self.args.vllm_device\n                if vllm_device == \"auto\":\n                    vllm_device = f\"cuda:{self.accelerator.num_processes}\"  # take the next GPU idx\n                # Check that the requested device is available\n                if (\n                    vllm_device.split(\":\")[0] == \"cuda\"\n                    and int(vllm_device.split(\":\")[1]) >= torch.cuda.device_count()\n                ):\n                    raise ValueError(\n                        f\"The requested device for vllm ({vllm_device}) is not available. You are likely using vLLM \"\n                        \"without restricting the number of GPUs for training. Set the `--num_processes` argument to a \"\n                        \"value lower than the number of GPUs available on your machineâ€”typically, reducing it by one \"\n                        f\"is sufficient. In your case: `--num_processes {torch.cuda.device_count() - 1}`.\"\n                    )\n                # Check that the requested device is not also used for training\n                if vllm_device in {\n                    f\"cuda:{idx}\" for idx in range(self.accelerator.num_processes)\n                }:\n                    warnings.warn(\n                        f\"The requested device {vllm_device} is also used for training. This may lead to unexpected \"\n                        \"behavior. It is recommended to use a dedicated device for vLLM.\"\n                    )\n                # vLLM is not compatible with accelerate. So we need to patch it to make sure we can (1) place the vLLM\n                # model on the desired device (world_size_patch) and (2) avoid a test that is not designed for our\n                # setting (profiling_patch).\n                world_size_patch = patch(\n                    \"torch.distributed.get_world_size\", return_value=1\n                )\n                profiling_patch = patch(\n                    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n                    return_value=None,\n                )\n                with world_size_patch, profiling_patch:\n                    print(\"vllm is running on: \", vllm_device)\n                    self.llm = LLM(\n                        model=model.name_or_path,\n                        device=vllm_device,\n                        gpu_memory_utilization=self.args.vllm_gpu_memory_utilization,\n                        dtype=torch.bfloat16,\n                        # Automatic Prefix Caching caches the KV cache of existing queries, so that a new query can\n                        # directly reuse the KV cache if it shares the same prefix with one of the existing queries.\n                        # This is particularly useful here because we generate completions from the same prompts.\n                        enable_prefix_caching=True,\n                        enforce_eager=True,\n                        # Ensure that training and inference use the same processor for images.\n                        mm_processor_kwargs=(\n                            {\n                                \"max_pixels\": max_pixels,\n                                \"min_pixels\": min_pixels,\n                            }\n                            if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id\n                            else None\n                        ),\n                        max_model_len=args.max_completion_length,\n                    )\n                self.sampling_params = SamplingParams(\n                    temperature=args.temperature,\n                    max_tokens=self.max_completion_length,\n                )\n\n            self._last_loaded_step = (\n                0  # tag to avoid useless loading during grad accumulation\n            )\n\n            # When using vLLM, the main process is responsible for loading the model weights. This can cause process\n            # desynchronization and seems to lead to DeepSpeed hanging during initialization. To prevent this, we\n            # synchronize all processes after vLLM has been fully initialized.\n            self.accelerator.wait_for_everyone()\n        else:\n            raise ValueError(\n                \"Qwen2VLGRPOVLLMTrainer only supports vllm generation, please set --use_vllm True\"\n            )\n\n        if self.ref_model is not None:\n            if self.is_deepspeed_enabled:\n                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)\n            else:\n                self.ref_model = self.accelerator.prepare_model(\n                    self.ref_model, evaluation_mode=True\n                )\n\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(reward_func, PreTrainedModel):\n                self.reward_funcs[i] = self.accelerator.prepare_model(\n                    reward_func, evaluation_mode=True\n                )\n\n    def _set_signature_columns_if_needed(self):\n        # If `self.args.remove_unused_columns` is True, non-signature columns are removed.\n        # By default, this method sets `self._signature_columns` to the model's expected inputs.\n        # In GRPOTrainer, we preprocess data, so using the model's signature columns doesn't work.\n        # Instead, we set them to the columns expected by the `training_step` method, hence the override.\n        if self._signature_columns is None:\n            self._signature_columns = [\"prompt\"]\n\n    # We need a custom sampler that samples the same prompt multiple times\n    def _get_train_sampler(self):\n        return RepeatRandomSampler(self.train_dataset, self.num_generations)\n\n    # Get the per-token log probabilities for the completions for the model and the reference model\n    def _get_per_token_logps(\n        self,\n        model,\n        input_ids,\n        attention_mask,\n        pixel_values,\n        image_grid_thw,\n        logits_to_keep,\n    ):\n        pixel_values = pixel_values.to(model.device)\n        image_grid_thw = image_grid_thw.to(device=model.device)\n        logits = model(\n            input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            image_grid_thw=image_grid_thw,\n        ).logits  # (B, L, V)\n        logits = logits[\n            :, :-1, :\n        ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n        input_ids = input_ids[\n            :, -logits_to_keep:\n        ]  # (B, L-1), exclude the first input ID since we don't have logits for it\n        # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n        logits = logits[:, -logits_to_keep:]\n        per_token_logps = []\n        for logits_row, input_ids_row in zip(logits, input_ids):\n            log_probs = logits_row.log_softmax(dim=-1)\n            token_log_prob = torch.gather(\n                log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n            ).squeeze(1)\n            per_token_logps.append(token_log_prob)\n        return torch.stack(per_token_logps)\n\n    # Trainer \"prepares\" the inputs before calling `compute_loss`. It converts to tensor and move to device.\n    # Since we preprocess the data in `compute_loss`, we need to override this method to skip this step.\n    def _prepare_inputs(\n        self, inputs: dict[str, Union[torch.Tensor, Any]]\n    ) -> dict[str, Union[torch.Tensor, Any]]:\n        device = self.accelerator.device\n        prompts = [x[\"prompt\"] for x in inputs]\n        images = [x[\"image\"] for x in inputs]\n        prompts_text = [\n            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n            for example in inputs\n        ]\n        prompt_inputs = self.processing_class(\n            # prompts_text, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n            text=prompts_text,\n            images=images,\n            return_tensors=\"pt\",\n            padding=True,\n            padding_side=\"left\",\n            add_special_tokens=False,\n        )\n        prompt_ids, prompt_mask = (\n            prompt_inputs[\"input_ids\"].to(device),\n            prompt_inputs[\"attention_mask\"].to(device),\n        )\n        if self.max_prompt_length is not None:\n            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n\n        if self.args.use_vllm:\n            # First, have main process load weights if needed\n            if self.state.global_step != self._last_loaded_step:\n                with unwrap_model_for_generation(\n                    self.model,\n                    self.accelerator,\n                    gather_deepspeed3_params=False,  # TODO: fix this, self.args.ds3_gather_for_generation,\n                ) as unwrapped_model:\n                    if is_compiled_module(unwrapped_model):\n                        state_dict = unwrapped_model._orig_mod.state_dict()\n                    else:\n                        state_dict = unwrapped_model.state_dict()\n                if self.accelerator.is_main_process:\n                    llm_model = (\n                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n                    )\n                    llm_model.load_weights(state_dict.items())\n                self._last_loaded_step = self.state.global_step\n\n            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n            all_prompts_text = gather_object(prompts_text)\n            all_images = gather_object(images)\n            # group into pairs\n            all_multimodal_inputs = [\n                {\"prompt\": p, \"multi_modal_data\": {\"image\": i}}\n                for p, i in zip(all_prompts_text, all_images)\n            ]\n\n            if self.accelerator.is_main_process:\n                outputs = self.llm.generate(\n                    all_multimodal_inputs,\n                    sampling_params=self.sampling_params,\n                    use_tqdm=False,\n                )\n                completion_ids = [\n                    out.token_ids\n                    for completions in outputs\n                    for out in completions.outputs\n                ]\n            else:\n                completion_ids = [None] * len(all_prompts_text)\n            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n            process_slice = slice(\n                self.accelerator.process_index * len(prompts),\n                (self.accelerator.process_index + 1) * len(prompts),\n            )\n            completion_ids = completion_ids[process_slice]\n\n            # Pad the completions, and concatenate them with the prompts\n            completion_ids = [\n                torch.tensor(ids, device=device) for ids in completion_ids\n            ]\n            completion_ids = pad(\n                completion_ids, padding_value=self.processing_class.pad_token_id\n            )\n            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n        else:\n            raise ValueError(\"Only vLLM generation is supported in this version \")\n\n        # below are the same with yifan's code\n        # Mask everything after the first EOS token\n        is_eos = completion_ids == self.processing_class.eos_token_id\n        device = self.accelerator.device\n        eos_idx = torch.full(\n            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n        )\n        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n            is_eos.size(0), -1\n        )\n        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n\n        # Concatenate prompt_mask with completion_mask for logit computation\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)\n        # pixel_values = prompt_inputs[\"pixel_values\"].repeat_interleave(\n        #     self.num_generations, dim=0\n        # )\n\n        pixel_values = prompt_inputs[\"pixel_values\"]\n        # [None].repeat_interleave(self.num_generations, dim=0)\n        # pixel_values = pixel_values.view(-1, pixel_values.shape[-1])\n\n        image_grid_thw = prompt_inputs[\"image_grid_thw\"]\n        # .repeat_interleave(\n        #     self.num_generations, dim=0\n        # )\n        logits_to_keep = completion_ids.size(1)\n\n        with torch.inference_mode():\n            if self.ref_model is not None:\n                ref_per_token_logps = self._get_per_token_logps(\n                    self.ref_model,\n                    prompt_completion_ids,\n                    attention_mask,\n                    pixel_values,\n                    image_grid_thw,\n                    logits_to_keep,\n                )\n            else:\n                with self.accelerator.unwrap_model(self.model).disable_adapter():\n                    ref_per_token_logps = self._get_per_token_logps(\n                        self.model,\n                        prompt_completion_ids,\n                        attention_mask,\n                        pixel_values,\n                        image_grid_thw,\n                        logits_to_keep,\n                    )\n\n        # Decode the generated completions\n        completions = self.processing_class.batch_decode(\n            completion_ids, skip_special_tokens=True\n        )\n        if is_conversational(inputs[0]):\n            completions = [\n                [{\"role\": \"assistant\", \"content\": completion}]\n                for completion in completions\n            ]\n\n        # Compute the rewards\n        rewards_per_func = torch.zeros(\n            len(prompts), len(self.reward_funcs), device=device\n        )\n        for i, (reward_func, reward_processing_class) in enumerate(\n            zip(self.reward_funcs, self.reward_processing_classes)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if is_conversational(inputs[0]):\n                    messages = [\n                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n                    ]\n                    texts = [\n                        apply_chat_template(x, reward_processing_class)[\"text\"]\n                        for x in messages\n                    ]\n                else:\n                    texts = [p + c for p, c in zip(prompts, completions)]\n                reward_inputs = reward_processing_class(\n                    texts,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    padding_side=\"right\",\n                    add_special_tokens=False,\n                )\n                reward_inputs = super()._prepare_inputs(reward_inputs)\n                with torch.inference_mode():\n                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n                        :, 0\n                    ]  # Shape (B*G,)\n            else:\n                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n                reward_kwargs = {\n                    key: []\n                    for key in inputs[0].keys()\n                    if key not in [\"prompt\", \"completion\"]\n                }\n                for key in reward_kwargs:\n                    for example in inputs:\n                        # Repeat each value in the column for `num_generations` times\n                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n                output_reward_func = reward_func(\n                    prompts=prompts, completions=completions, **reward_kwargs\n                )\n                rewards_per_func[:, i] = torch.tensor(\n                    output_reward_func, dtype=torch.float32, device=device\n                )\n        rewards_per_func = gather(rewards_per_func)\n        # Sum the rewards from all reward functions\n        rewards = rewards_per_func.sum(dim=1)\n\n        # Compute grouped-wise rewards\n        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n\n        # Normalize the rewards to compute the advantages\n        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n\n        # Slice to keep only the local part of the data\n        process_slice = slice(\n            self.accelerator.process_index * len(prompts),\n            (self.accelerator.process_index + 1) * len(prompts),\n        )\n        advantages = advantages[process_slice]\n\n        # Log the metrics\n        reward_per_func = rewards_per_func.mean(0)\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(\n                reward_func, nn.Module\n            ):  # Module instead of PretrainedModel for compat with compiled models\n                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n            else:\n                reward_func_name = reward_func.__name__\n            self._metrics[f\"rewards/{reward_func_name}\"].append(\n                reward_per_func[i].item()\n            )\n\n        self._metrics[\"reward\"].append(rewards.mean().item())\n        self._metrics[\"reward_std\"].append(std_grouped_rewards.mean().item())\n\n        return {\n            \"prompt_ids\": prompt_ids,\n            \"prompt_mask\": prompt_mask,\n            \"completion_ids\": completion_ids,\n            \"completion_mask\": completion_mask,\n            \"ref_per_token_logps\": ref_per_token_logps,\n            \"advantages\": advantages,\n            \"pixel_values\": pixel_values,\n            \"image_grid_thw\": image_grid_thw,\n        }\n\n    def compute_loss(\n        self, model, inputs, return_outputs=False, num_items_in_batch=None\n    ):\n        if return_outputs:\n            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n        # Compute the per-token log probabilities for the model\n\n        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n        completion_ids, completion_mask = (\n            inputs[\"completion_ids\"],\n            inputs[\"completion_mask\"],\n        )\n        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n        pixel_values = inputs[\"pixel_values\"]\n        image_grid_thw = inputs[\"image_grid_thw\"]\n        logits_to_keep = completion_ids.size(\n            1\n        )  # we only need to compute the logits for the completion tokens\n\n        per_token_logps = self._get_per_token_logps(\n            model,\n            input_ids,\n            attention_mask,\n            pixel_values,\n            image_grid_thw,\n            logits_to_keep,\n        )\n\n        # Compute the KL divergence between the model and the reference model\n        ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n        per_token_kl = (\n            torch.exp(ref_per_token_logps - per_token_logps)\n            - (ref_per_token_logps - per_token_logps)\n            - 1\n        )\n\n        # x - x.detach() allows for preserving gradients from x\n        advantages = inputs[\"advantages\"]\n        per_token_loss = torch.exp(\n            per_token_logps - per_token_logps.detach()\n        ) * advantages.unsqueeze(1)\n        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n        loss = (\n            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n        ).mean()\n\n        # Log the metrics\n        completion_length = (\n            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n            .float()\n            .mean()\n            .item()\n        )\n        self._metrics[\"completion_length\"].append(completion_length)\n\n        mean_kl = (\n            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n        ).mean()\n        self._metrics[\"kl\"].append(\n            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n        )\n\n        return loss\n\n    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n        metrics = {\n            key: sum(val) / len(val) for key, val in self._metrics.items()\n        }  # average the metrics\n\n        # This method can be called both in training and evaluation. When called in evaluation, the keys in `logs`\n        # start with \"eval_\". We need to add the prefix \"eval_\" to the keys in `metrics` to match the format.\n        if next(iter(logs.keys())).startswith(\"eval_\"):\n            metrics = {f\"eval_{key}\": val for key, val in metrics.items()}\n\n        logs = {**logs, **metrics}\n        if version.parse(transformers.__version__) >= version.parse(\"4.47.0.dev0\"):\n            super().log(logs, start_time)\n        else:  # transformers<=4.46\n            super().log(logs)\n        self._metrics.clear()\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/sft.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nSupervised fine-tuning script for decoder language models.\n\nUsage:\n\n# One 1 node of 8 x H100s\naccelerate launch --config_file=configs/zero3.yaml src/open_r1/sft.py \\\n    --model_name_or_path Qwen/Qwen2.5-1.5B-Instruct \\\n    --dataset_name HuggingFaceH4/Bespoke-Stratos-17k \\\n    --learning_rate 2.0e-5 \\\n    --num_train_epochs 1 \\\n    --packing \\\n    --max_seq_length 4096 \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --gradient_checkpointing \\\n    --bf16 \\\n    --logging_steps 5 \\\n    --eval_strategy steps \\\n    --eval_steps 100 \\\n    --output_dir data/Qwen2.5-1.5B-Open-R1-Distill\n\"\"\"\n\nimport logging\nimport os\nimport sys\n\nimport datasets\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, set_seed, AutoProcessor\nfrom transformers.trainer_utils import get_last_checkpoint\nimport trl\nfrom trl import (\n    ModelConfig,\n    ScriptArguments,\n    SFTTrainer,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\n\nfrom qwen_vl_utils import process_vision_info\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass SFTConfig(trl.SFTConfig):\n    \"\"\"\n    args for callbacks, benchmarks etc\n    \"\"\"\n\n    benchmarks: list[str] = field(\n        default_factory=lambda: [], metadata={\"help\": \"The benchmarks to run after training.\"}\n    )\n    callbacks: list[str] = field(\n        default_factory=lambda: [], metadata={\"help\": \"The callbacks to run during training.\"}\n    )\n    system_prompt: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The optional system prompt to use for benchmarking.\"},\n    )\n    hub_model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The Hub model branch to push the model to.\"},\n    )\n    overwrite_hub_revision: bool = field(default=False, metadata={\"help\": \"Whether to overwrite the Hub revision.\"})\n    push_to_hub_revision: bool = field(default=False, metadata={\"help\": \"Whether to push to a Hub revision/branch.\"})\n\n\n\nprocessor = None\n\n\ndef convert_example(example):\n    \"\"\"\n    correct example into \"messages\" \n    eg:\n    {\n      \"system\": \"You are a helpful assistant.\",\n      \"conversations\": [\n          {\"from\": \"user\", \"value\": \"How many objects are included in this image?\",\n           \"image_path\": \"/path/to/image.png\"},\n          {\"from\": \"assistant\", \"value\": \"<think>\\nI can see 10 objects\\n</think>\\n<answer>\\n10\\n</answer>\"}\n      ]\n    }\n    \"\"\"\n    messages = []\n    if \"system\" in example:\n        messages.append({\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": example[\"system\"]}],\n        })\n    else:\n        SYSTEM_PROMPT = (\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n    \"<think> reasoning process here </think><answer> answer here </answer>\"\n        )\n        messages.append({\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n        })\n\n    thinking = example.get(\"thinking\")\n    problem = example.get(\"problem\")\n    solution = example.get(\"solution\")\n    image = example.get(\"image\")\n    messages.append({\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": problem},\n            {\"type\": \"image\", \"image\": image},\n            ]\n    })\n    messages.append({\n        \"role\": \"assistant\",\n        \"content\": f\"{thinking}\\n\\n{solution}\",\n    })\n    \n    example[\"messages\"] = messages\n    return example\n\n\ndef collate_fn(examples):\n    texts = [\n        processor.apply_chat_template( convert_example(example)[\"messages\"], tokenize=False, add_generation_prompt=True)\n        for example in examples\n    ]\n    image_inputs = []\n    for example in examples:\n        imgs, vids = process_vision_info(example[\"messages\"])\n        image_inputs.append(imgs)\n    batch = processor(\n        text=texts,\n        images=image_inputs,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n    labels[labels == image_token_id] = -100\n    batch[\"labels\"] = labels\n\n    return batch\n\n\ndef main(script_args, training_args, model_args):\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n\n    ###############\n    # Setup logging\n    ###############\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process a small summary\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Model parameters {model_args}\")\n    logger.info(f\"Script parameters {script_args}\")\n    logger.info(f\"Data parameters {training_args}\")\n\n    # Check for last checkpoint\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir):\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n\n    ################\n    # Load datasets\n    ################\n\n    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n\n    ################\n    # Load tokenizer\n    ################\n    global processor\n    if \"vl\" in model_args.model_name_or_path.lower():\n        processor = AutoProcessor.from_pretrained(\n            model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n        )\n        logger.info(\"Using AutoProcessor for vision-language model.\")\n    else:\n        processor = AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast=True\n        )\n        logger.info(\"Using AutoTokenizer for text-only model.\")\n    if hasattr(processor, \"pad_token\") and processor.pad_token is None:\n        processor.pad_token = processor.eos_token\n    elif hasattr(processor.tokenizer, \"pad_token\") and processor.tokenizer.pad_token is None:\n        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n    \n    ###################\n    # Model init kwargs\n    ###################\n    logger.info(\"*** Initializing model kwargs ***\")\n    torch_dtype = (\n        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n    )\n    quantization_config = get_quantization_config(model_args)\n    model_kwargs = dict(\n        revision=model_args.model_revision,\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n    # training_args.model_init_kwargs = model_kwargs\n    from transformers import Qwen2VLForConditionalGeneration\n    model = Qwen2VLForConditionalGeneration.from_pretrained(\n        model_args.model_name_or_path, **model_kwargs\n    )\n    ############################\n    # Initialize the SFT Trainer\n    ############################\n    training_args.dataset_kwargs = {\n        \"skip_prepare_dataset\": True,\n    }\n    training_args.remove_unused_columns = False\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[script_args.dataset_train_split],\n        eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None,\n        processing_class=processor.tokenizer,\n        data_collator=collate_fn,\n        peft_config=get_peft_config(model_args)\n    )\n\n    ###############\n    # Training loop\n    ###############\n    logger.info(\"*** Train ***\")\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    elif last_checkpoint is not None:\n        checkpoint = last_checkpoint\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(dataset[script_args.dataset_train_split])\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n    ##################################\n    # Save model and create model card\n    ##################################\n    logger.info(\"*** Save model ***\")\n    trainer.save_model(training_args.output_dir)\n    processor.save_pretrained(training_args.output_dir)\n    logger.info(f\"Model saved to {training_args.output_dir}\")\n\n    # Save everything else on main process\n    kwargs = {\n        \"dataset_name\": script_args.dataset_name,\n        \"tags\": [\"R1-V\"],\n    }\n    if trainer.accelerator.is_main_process:\n        trainer.create_model_card(**kwargs)\n        # Restore k,v cache for fast inference\n        trainer.model.config.use_cache = True\n        trainer.model.config.save_pretrained(training_args.output_dir)\n    #############\n    # push to hub\n    #############\n\n    if training_args.push_to_hub:\n        logger.info(\"Pushing to hub...\")\n        trainer.push_to_hub(**kwargs)\n        processor.push_to_hub(training_args.hub_model_id)\n\n\n\n\nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n    script_args, training_args, model_args = parser.parse_args_and_config()\n    main(script_args, training_args, model_args)\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/grpo.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import Qwen2VLForConditionalGeneration\n\nfrom math_verify import parse, verify, LatexExtractionConfig\nfrom latex2sympy2_extended import NormalizationConfig\nfrom open_r1.trainer import Qwen2VLGRPOTrainer, Qwen2VLGRPOVLLMTrainer, Qwen2VLGRPOVLLMTrainerModified\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, ScriptArguments, TrlParser, get_peft_config\nfrom reward import accuracy_reward, format_reward, reasoning_steps_reward, get_cosine_scaled_reward, get_repetition_penalty_reward, len_reward\n\nimport math\nimport random\nfrom typing import Optional, Dict\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSYSTEM_PROMPT = (\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n    \"<think> reasoning process here </think><answer> answer here </answer>\"\n)\n\nQUESTION_TEMPLATE = \"{Question}\\nOutput the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\nIOU_QUESTION_TEMPLATE = \"{Question}\\nFirst output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n  \n\n@dataclass\nclass GRPOScriptArguments(ScriptArguments):\n    \"\"\"\n    Script arguments for the GRPO training script.\n\n    Args:\n        reward_funcs (`list[str]`):\n            List of reward functions. Possible values: 'accuracy', 'format'.\n    \"\"\"\n\n    reward_funcs: list[str] = field(\n        default_factory=lambda: [\"accuracy\", \"format\"],\n        metadata={\"help\": \"List of reward functions. Possible values: 'accuracy', 'format'\"},\n    )\n    max_pixels: Optional[int] = field(\n        default=12845056,\n        metadata={\"help\": \"Maximum number of pixels for the image\"},\n    )\n    min_pixels: Optional[int] = field(\n        default=3136,\n        metadata={\"help\": \"Minimum number of pixels for the image\"},\n    )\n\n\nreward_funcs_registry = {\n    \"accuracy\": accuracy_reward,\n    \"format\": format_reward,\n    \"reasoning_steps\": reasoning_steps_reward,\n    \"cosine\": get_cosine_scaled_reward(\n        min_value_wrong=0,\n        max_value_wrong=-0.5,\n        min_value_correct=0.5,\n        max_value_correct=1,\n        max_len=1000,\n    ),\n    \"repetition_penalty\": get_repetition_penalty_reward(\n        ngram_size=3,\n        max_penalty=-1.0,\n    ),\n    \"length\": len_reward,\n}\n\n\ndef main(script_args, training_args, model_args):\n    # Get reward functions\n    reward_funcs = [reward_funcs_registry[func] for func in script_args.reward_funcs]\n\n    # Load the dataset\n    dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n    # random_number = random.randint(1, 100)\n    # dataset = dataset.shuffle(seed=random_number)\n    # dataset.save_to_disk(f\"./processed_data/r1_data/{script_args.dataset_name}\")\n    print(dataset)\n\n\n    # Format into conversation\n    def make_conversation(example):\n        return {\n            \"prompt\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": example[\"problem\"]},\n            ],\n        }\n\n    def make_conversation_image(example):\n        if \"has_image\" in example:\n            ## visual data\n            if example[\"has_image\"]:\n                return {\n                    \"prompt\": [\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\"type\": \"image\"},\n                                {\"type\": \"text\", \"text\": IOU_QUESTION_TEMPLATE.format(Question=example[\"problem\"]) if \"reward_func\" in example and example[\"reward_func\"] == \"iou\" else QUESTION_TEMPLATE.format(Question=example[\"problem\"])},\n                            ],\n                        },\n                    ]\n                }\n            ## text-only data\n            else:\n                return {\n                    \"prompt\": [\n                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                        {\"role\": \"user\", \"content\": example[\"problem\"]},\n                    ],\n            }\n        ## default\n        else:\n            return {\n                    \"prompt\": [\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\"type\": \"image\"},\n                                {\"type\": \"text\", \"text\": QUESTION_TEMPLATE.format(Question=example[\"problem\"])},\n                            ],\n                        },\n                    ],\n                }\n\n    if \"image\" in dataset[script_args.dataset_train_split].features:\n        print(\"has image in dataset\")\n        try:\n            dataset = dataset.map(make_conversation_image, num_proc=8)  # Utilize multiprocessing for faster mapping\n        # dataset = dataset.remove_columns([\"original_question\", \"original_answer\"])\n        except Exception as e:\n            print(e)\n            print(\"Make Conversation with Image Later\")\n\n    else:\n        print(\"no image in dataset\")\n        dataset = dataset.map(make_conversation, num_proc=8)\n        try:\n            dataset = dataset.remove_columns(\"messages\")\n        except:\n            print(\"Pure Text Data ...\")\n\n    \n    trainer_cls = Qwen2VLGRPOTrainer if not training_args.use_vllm else Qwen2VLGRPOVLLMTrainerModified\n    print(\"using: \", trainer_cls)\n\n    # Initialize the GRPO trainer\n    trainer = trainer_cls(\n        model=model_args.model_name_or_path,\n        reward_funcs=reward_funcs,\n        args=training_args,\n        train_dataset=dataset[script_args.dataset_train_split],\n        eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None,\n        peft_config=get_peft_config(model_args),\n        attn_implementation=model_args.attn_implementation,\n        max_pixels=script_args.max_pixels,\n        min_pixels=script_args.min_pixels,\n    )\n\n    # Train and push the model to the Hub\n    trainer.train()\n\n    # Save and push to hub\n    trainer.save_model(training_args.output_dir)\n    if training_args.push_to_hub:\n        trainer.push_to_hub(dataset_name=script_args.dataset_name)\n\n\nif __name__ == \"__main__\":\n    parser = TrlParser((GRPOScriptArguments, GRPOConfig, ModelConfig))\n    script_args, training_args, model_args = parser.parse_args_and_config()\n    main(script_args, training_args, model_args)\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/generate.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nfrom distilabel.llms import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps.tasks import TextGeneration\n\n\ndef build_distilabel_pipeline(\n    model: str,\n    base_url: str = \"http://localhost:8000/v1\",\n    prompt_column: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    max_new_tokens: int = 8192,\n    num_generations: int = 1,\n) -> Pipeline:\n    generation_kwargs = {\"max_new_tokens\": max_new_tokens}\n\n    if temperature is not None:\n        generation_kwargs[\"temperature\"] = temperature\n\n    if top_p is not None:\n        generation_kwargs[\"top_p\"] = top_p\n\n    with Pipeline().ray() as pipeline:\n        TextGeneration(\n            llm=OpenAILLM(\n                base_url=base_url,\n                api_key=\"something\",\n                model=model,\n                # thinking can take some time...\n                timeout=10 * 60,\n                generation_kwargs=generation_kwargs,\n            ),\n            input_mappings={\"instruction\": prompt_column} if prompt_column is not None else {},\n            input_batch_size=64,  # on 4 nodes bs ~60+ leads to preemption due to KV cache exhaustion\n            num_generations=num_generations,\n        )\n\n    return pipeline\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    from datasets import load_dataset\n\n    parser = argparse.ArgumentParser(description=\"Run distilabel pipeline for generating responses with DeepSeek R1\")\n    parser.add_argument(\n        \"--hf-dataset\",\n        type=str,\n        required=True,\n        help=\"HuggingFace dataset to load\",\n    )\n    parser.add_argument(\n        \"--hf-dataset-config\",\n        type=str,\n        required=False,\n        help=\"Dataset config to use\",\n    )\n    parser.add_argument(\n        \"--hf-dataset-split\",\n        type=str,\n        default=\"train\",\n        help=\"Dataset split to use\",\n    )\n    parser.add_argument(\"--prompt-column\", type=str, default=\"prompt\")\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        required=True,\n        help=\"Model name to use for generation\",\n    )\n    parser.add_argument(\n        \"--vllm-server-url\",\n        type=str,\n        default=\"http://localhost:8000/v1\",\n        help=\"URL of the vLLM server\",\n    )\n    parser.add_argument(\n        \"--temperature\",\n        type=float,\n        help=\"Temperature for generation\",\n    )\n    parser.add_argument(\n        \"--top-p\",\n        type=float,\n        help=\"Top-p value for generation\",\n    )\n    parser.add_argument(\n        \"--max-new-tokens\",\n        type=int,\n        default=8192,\n        help=\"Maximum number of new tokens to generate\",\n    )\n    parser.add_argument(\n        \"--num-generations\",\n        type=int,\n        default=1,\n        help=\"Number of generations per problem\",\n    )\n    parser.add_argument(\n        \"--hf-output-dataset\",\n        type=str,\n        required=False,\n        help=\"HuggingFace repo to push results to\",\n    )\n    parser.add_argument(\n        \"--private\",\n        action=\"store_true\",\n        help=\"Whether to make the output dataset private when pushing to HF Hub\",\n    )\n\n    args = parser.parse_args()\n\n    print(\"\\nRunning with arguments:\")\n    for arg, value in vars(args).items():\n        print(f\"  {arg}: {value}\")\n    print()\n\n    print(f\"Loading '{args.hf_dataset}' (config: {args.hf_dataset_config}, split: {args.hf_dataset_split}) dataset...\")\n    dataset = load_dataset(args.hf_dataset, split=args.hf_dataset_split)\n    print(\"Dataset loaded!\")\n\n    pipeline = build_distilabel_pipeline(\n        model=args.model,\n        base_url=args.vllm_server_url,\n        prompt_column=args.prompt_column,\n        temperature=args.temperature,\n        top_p=args.top_p,\n        max_new_tokens=args.max_new_tokens,\n        num_generations=args.num_generations,\n    )\n\n    print(\"Running generation pipeline...\")\n    distiset = pipeline.run(dataset=dataset, use_cache=False)\n    print(\"Generation pipeline finished!\")\n\n    if args.hf_output_dataset:\n        print(f\"Pushing resulting dataset to '{args.hf_output_dataset}'...\")\n        distiset.push_to_hub(args.hf_output_dataset, private=args.private)\n        print(\"Dataset pushed!\")\n"}
{"type": "source_file", "path": "src/r1-v/src/open_r1/trainer/grpo_trainer.py", "content": "# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport textwrap\nfrom collections import defaultdict\nfrom typing import Any, Callable, Optional, Union\n\nimport torch\nimport torch.utils.data\nimport transformers\nfrom datasets import Dataset, IterableDataset\nfrom packaging import version\nfrom transformers import (\n    AriaForConditionalGeneration,\n    AriaProcessor,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoProcessor,\n    AutoTokenizer,\n    GenerationConfig,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Qwen2VLForConditionalGeneration,\n    Qwen2_5_VLForConditionalGeneration,\n    Trainer,\n    TrainerCallback,\n    is_wandb_available,\n)\nfrom transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.utils import is_peft_available\n\nfrom trl.data_utils import apply_chat_template, is_conversational, maybe_apply_chat_template\nfrom trl.models import create_reference_model, prepare_deepspeed, unwrap_model_for_generation\nfrom trl.trainer.grpo_config import GRPOConfig\nfrom trl.trainer.utils import generate_model_card, get_comet_experiment_url\n\nimport copy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nif is_peft_available():\n    from peft import PeftConfig, get_peft_model\n\nif is_wandb_available():\n    import wandb\n    wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n\n# What we call a reward function is a callable that takes a list of prompts and completions and returns a list of\n# rewards. When it's a string, it's a model ID, so it's loaded as a pretrained model.\nRewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n\nSYSTEM_PROMPT = (\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n    \"<think> reasoning process here </think><answer> answer here </answer>\"\n)\n\nQUESTION_TEMPLATE = \"{Question}  Output the thinking process in <think> </think> and final answer (number) in <answer> </answer> tags.\"\nIOU_QUESTION_TEMPLATE = \"{Question} First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.\"\n  \nclass Qwen2VLGRPOTrainer(Trainer):\n    \"\"\"\n    Trainer for the Group Relative Policy Optimization (GRPO) method. This algorithm was initially proposed in the\n    paper [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).\n\n    Example:\n\n    ```python\n    from datasets import load_dataset\n    from trl import GRPOTrainer\n\n    dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n\n    trainer = GRPOTrainer(\n        model=\"Qwen/Qwen2-0.5B-Instruct\",\n        reward_funcs=\"weqweasdas/RM-Gemma-2B\",\n        train_dataset=dataset,\n    )\n\n    trainer.train()\n    ```\n\n    Args:\n        model (`Union[str, PreTrainedModel]`):\n            Model to be trained. Can be either:\n\n            - A string, being the *model id* of a pretrained model hosted inside a model repo on huggingface.co, or\n              a path to a *directory* containing model weights saved using\n              [`~transformers.PreTrainedModel.save_pretrained`], e.g., `'./my_model_directory/'`. The model is\n              loaded using [`~transformers.AutoModelForCausalLM.from_pretrained`] with the keywork arguments\n              in `args.model_init_kwargs`.\n            - A [`~transformers.PreTrainedModel`] object. Only causal language models are supported.\n        reward_funcs (`Union[RewardFunc, list[RewardFunc]]`):\n            Reward functions to be used for computing the rewards. To compute the rewards, we call all the reward\n            functions with the prompts and completions and sum the rewards. Can be either:\n\n            - A single reward function, such as:\n                - A string: The *model ID* of a pretrained model hosted inside a model repo on huggingface.co, or a\n                path to a *directory* containing model weights saved using\n                [`~transformers.PreTrainedModel.save_pretrained`], e.g., `'./my_model_directory/'`. The model is loaded\n                using [`~transformers.AutoModelForSequenceClassification.from_pretrained`] with `num_labels=1` and the\n                keyword arguments in `args.model_init_kwargs`.\n                - A [`~transformers.PreTrainedModel`] object: Only sequence classification models are supported.\n                - A custom reward function: The function is provided with the prompts and the generated completions,\n                  plus any additional columns in the dataset. It should return a list of rewards. For more details, see\n                  [Using a custom reward function](#using-a-custom-reward-function).\n            - A list of reward functions, where each item can independently be any of the above types. Mixing different\n            types within the list (e.g., a string model ID and a custom reward function) is allowed.\n        args ([`GRPOConfig`], *optional*, defaults to `None`):\n            Configuration for this trainer. If `None`, a default configuration is used.\n        train_dataset ([`~datasets.Dataset`] or [`~datasets.IterableDataset`]):\n            Dataset to use for training. It must include a column `\"prompt\"`. Any additional columns in the dataset is\n            ignored. The format of the samples can be either:\n\n            - [Standard](dataset_formats#standard): Each sample contains plain text.\n            - [Conversational](dataset_formats#conversational): Each sample contains structured messages (e.g., role\n              and content).\n        eval_dataset ([`~datasets.Dataset`], [`~datasets.IterableDataset`] or `dict[str, Union[Dataset, IterableDataset]]`):\n            Dataset to use for evaluation. It must meet the same requirements as `train_dataset`.\n        processing_class ([`~transformers.PreTrainedTokenizerBase`], *optional*, defaults to `None`):\n            Processing class used to process the data. The padding side must be set to \"left\". If `None`, the\n            processing class is loaded from the model's name with [`~transformers.AutoTokenizer.from_pretrained`].\n        reward_processing_classes (`Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]`, *optional*, defaults to `None`):\n            Processing classes corresponding to the reward functions specified in `reward_funcs`. Can be either:\n\n            - A single processing class: Used when `reward_funcs` contains only one reward function.\n            - A list of processing classes: Must match the order and length of the reward functions in `reward_funcs`.\n            If set to `None`, or if an element of the list corresponding to a [`~transformers.PreTrainedModel`] is\n            `None`, the tokenizer for the model is automatically loaded using [`~transformers.AutoTokenizer.from_pretrained`].\n            For elements in `reward_funcs` that are custom reward functions (not [`~transformers.PreTrainedModel`]),\n            the corresponding entries in `reward_processing_classes` are ignored.\n        callbacks (list of [`~transformers.TrainerCallback`], *optional*, defaults to `None`):\n            List of callbacks to customize the training loop. Will add those to the list of default callbacks\n            detailed in [here](https://huggingface.co/docs/transformers/main_classes/callback).\n\n            If you want to remove one of the default callbacks used, use the [`~transformers.Trainer.remove_callback`]\n            method.\n        optimizers (`tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\n            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\n            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n        peft_config ([`~peft.PeftConfig`], *optional*, defaults to `None`):\n            PEFT configuration used to wrap the model. If `None`, the model is not wrapped.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[str, PreTrainedModel],\n        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n        args: GRPOConfig = None,\n        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n        eval_dataset: Optional[Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]] = None,\n        processing_class: Optional[PreTrainedTokenizerBase] = None,\n        reward_processing_classes: Optional[Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]] = None,\n        callbacks: Optional[list[TrainerCallback]] = None,\n        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),\n        peft_config: Optional[\"PeftConfig\"] = None,\n        max_pixels: Optional[int] = 12845056,\n        min_pixels: Optional[int] = 3136,\n        attn_implementation: str = \"flash_attention_2\",\n    ):\n        # Args\n        if args is None:\n            model_name = model if isinstance(model, str) else model.config._name_or_path\n            model_name = model_name.split(\"/\")[-1]\n            args = GRPOConfig(f\"{model_name}-GRPO\")\n\n        # Models\n        # Trained model\n        model_init_kwargs = args.model_init_kwargs or {}\n        model_init_kwargs[\"attn_implementation\"] = attn_implementation\n        if isinstance(model, str):\n            model_id = model\n            torch_dtype = model_init_kwargs.get(\"torch_dtype\")\n            if isinstance(torch_dtype, torch.dtype) or torch_dtype == \"auto\" or torch_dtype is None:\n                pass  # torch_dtype is already a torch.dtype or \"auto\" or None\n            elif isinstance(torch_dtype, str):  # it's a str, but not \"auto\"\n                torch_dtype = getattr(torch, torch_dtype)\n                model_init_kwargs[\"torch_dtype\"] = torch_dtype\n            else:\n                raise ValueError(\n                    \"Invalid `torch_dtype` passed to `GRPOConfig`. Expected either 'auto' or a string representing \"\n                    f\"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}.\"\n                )\n            # Disable caching if gradient checkpointing is enabled (not supported)\n            model_init_kwargs[\"use_cache\"] = (\n                False if args.gradient_checkpointing else model_init_kwargs.get(\"use_cache\")\n            )\n            if \"Qwen2-VL\" in model_id:\n                model = Qwen2VLForConditionalGeneration.from_pretrained(model, **model_init_kwargs)\n            elif \"Qwen2.5-VL\" in model_id:\n                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model, **model_init_kwargs)\n            elif \"Aria\" in model_id:\n                model_init_kwargs.pop(\"use_cache\")\n                model = AriaForConditionalGeneration.from_pretrained(model, **model_init_kwargs)\n            else:\n                model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n        else:\n            model_id = model.config._name_or_path\n            if args.model_init_kwargs is not None:\n                raise ValueError(\n                    \"You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. \"\n                    \"This argument can only be used when the `model` argument is a string.\"\n                )\n\n        if peft_config is not None:\n            model = get_peft_model(model, peft_config)\n\n        # Reference model\n        if is_deepspeed_zero3_enabled():\n            if \"Qwen2-VL\" in model_id:\n                self.ref_model = Qwen2VLForConditionalGeneration.from_pretrained(model_id, **model_init_kwargs)\n            elif \"Qwen2.5-VL\" in model_id:\n                self.ref_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id, **model_init_kwargs)\n            elif \"Aria\" in model_id:\n                self.ref_model = AriaForConditionalGeneration.from_pretrained(model_id, **model_init_kwargs)\n            else:\n                self.ref_model = AutoModelForCausalLM.from_pretrained(model_id, **model_init_kwargs)\n        elif peft_config is None:\n            # If PEFT configuration is not provided, create a reference model based on the initial model.\n            self.ref_model = create_reference_model(model)\n        else:\n            # If PEFT is used, the reference model is not needed since the adapter can be disabled\n            # to revert to the initial model.\n            self.ref_model = None\n\n        # Processing class\n        if processing_class is None:\n            if \"Qwen2-VL\" in model_id or \"Qwen2.5-VL\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen\" in model_id or \"Qwen2.5-VL\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side=\"left\")\n                pad_token_id = processing_class.pad_token_id\n\n        # Reward functions\n        if not isinstance(reward_funcs, list):\n            reward_funcs = [reward_funcs]\n        for i, reward_func in enumerate(reward_funcs):\n            if isinstance(reward_func, str):\n                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(\n                    reward_func, num_labels=1, **model_init_kwargs\n                )\n        self.reward_funcs = reward_funcs\n\n        # Reward processing class\n        if reward_processing_classes is None:\n            reward_processing_classes = [None] * len(reward_funcs)\n        elif not isinstance(reward_processing_classes, list):\n            reward_processing_classes = [reward_processing_classes]\n        else:\n            if len(reward_processing_classes) != len(reward_funcs):\n                raise ValueError(\"The number of reward processing classes must match the number of reward functions.\")\n\n        for i, (reward_processing_class, reward_func) in enumerate(zip(reward_processing_classes, reward_funcs)):\n            if isinstance(reward_func, PreTrainedModel):\n                if reward_processing_class is None:\n                    reward_processing_class = AutoTokenizer.from_pretrained(reward_func.config._name_or_path)\n                if reward_processing_class.pad_token_id is None:\n                    reward_processing_class.pad_token = reward_processing_class.eos_token\n                # The reward model computes the reward for the latest non-padded token in the input sequence.\n                # So it's important to set the pad token ID to the padding token ID of the processing class.\n                reward_func.config.pad_token_id = reward_processing_class.pad_token_id\n                reward_processing_classes[i] = reward_processing_class\n        self.reward_processing_classes = reward_processing_classes\n\n        # Data collator\n        def data_collator(features):  # No data collation is needed in GRPO\n            return features\n\n        # Training arguments\n        self.max_prompt_length = args.max_prompt_length\n        self.max_completion_length = args.max_completion_length  # = |o_i| in the GRPO paper\n        self.num_generations = args.num_generations  # = G in the GRPO paper\n        self.generation_config = GenerationConfig(\n            max_new_tokens=self.max_completion_length,\n            do_sample=True,  \n            temperature=1, # HACK\n            num_return_sequences=self.num_generations,\n            pad_token_id=pad_token_id,\n        )\n        self.beta = args.beta\n\n        # The trainer estimates the number of FLOPs (floating-point operations) using the number of elements in the\n        # input tensor associated with the key \"input_ids\". However, in GRPO, the sampled data does not include the\n        # \"input_ids\" key. Instead, the available keys is \"prompt\". As a result, the trainer issues the warning:\n        # \"Could not estimate the number of tokens of the input, floating-point operations will not be computed.\" To\n        # suppress this warning, we set the \"estimate_tokens\" key in the model's \"warnings_issued\" dictionary to True.\n        # This acts as a flag to indicate that the warning has already been issued.\n        model.warnings_issued[\"estimate_tokens\"] = True\n\n        # Initialize the metrics\n        self._metrics = defaultdict(list)\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=processing_class,\n            callbacks=callbacks,\n            optimizers=optimizers,\n        )\n\n        # Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\n        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\n        # self.model_accepts_loss_kwargs to False to enable scaling.\n        self.model_accepts_loss_kwargs = False\n\n        if self.ref_model is not None:\n            if self.is_deepspeed_enabled:\n                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)\n            else:\n                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(reward_func, PreTrainedModel):\n                self.reward_funcs[i] = self.accelerator.prepare_model(reward_func, evaluation_mode=True)\n\n    def _set_signature_columns_if_needed(self):\n        # If `self.args.remove_unused_columns` is True, non-signature columns are removed.\n        # By default, this method sets `self._signature_columns` to the model's expected inputs.\n        # In GRPOTrainer, we preprocess data, so using the model's signature columns doesn't work.\n        # Instead, we set them to the columns expected by the `training_step` method, hence the override.\n        if self._signature_columns is None:\n            self._signature_columns = [\"prompt\"]\n\n\n    # Get the per-token log probabilities for the completions for the model and the reference model\n    def _get_per_token_logps(self, model, input_ids, attention_mask, pixel_values, image_grid_thw):\n        logits = model(input_ids, attention_mask=attention_mask, pixel_values=pixel_values, image_grid_thw=image_grid_thw).logits  # (B, L, V)\n        logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n        input_ids = input_ids[:, 1:]  # (B, L-1), exclude the first input ID since we don't have logits for it\n        # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n        per_token_logps = []\n        for logits_row, input_ids_row in zip(logits, input_ids):\n            log_probs = logits_row.log_softmax(dim=-1)\n            token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)\n            per_token_logps.append(token_log_prob)\n        return torch.stack(per_token_logps)\n\n\n    # Trainer \"prepares\" the inputs before calling `compute_loss`. It converts to tensor and move to device.\n    # Since we preprocess the data in `compute_loss`, we need to override this method to skip this step.\n    def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[str, Union[torch.Tensor, Any]]:\n        return inputs\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        if return_outputs:\n            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n    \n        if \"prompt\" not in inputs[0]:\n            for example in inputs:\n                if example[\"has_image\"]:\n                    example[\"prompt\"] = [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\"type\": \"image\"},\n                                    {\"type\": \"text\", \"text\": IOU_QUESTION_TEMPLATE.format(Question=example[\"problem\"]) if \"reward_func\" in example and example[\"reward_func\"] == \"iou\" else QUESTION_TEMPLATE.format(Question=example[\"problem\"])},\n                                ],\n                            },\n                        ]\n                else:\n                    example[\"prompt\"] = [\n                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                            {\"role\": \"user\", \"content\": example[\"problem\"]},\n                        ]\n\n        prompts = [x[\"prompt\"] for x in inputs]\n        prompts_text = [maybe_apply_chat_template(example, self.processing_class)[\"prompt\"] for example in inputs]\n        images = [x.get(\"image\",None) for x in inputs]\n        prompt_inputs = self.processing_class(\n            text=prompts_text,\n            images=images if images[0] else None,\n            return_tensors=\"pt\",\n            padding=True,\n            padding_side=\"left\",\n            add_special_tokens=False,\n        )\n        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n\n        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n        pixel_values = prompt_inputs.get(\"pixel_values\",None)\n        image_grid_thw = prompt_inputs.get(\"image_grid_thw\",None)\n\n        \n        if self.max_prompt_length is not None:\n            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n\n        # Generate completions\n        with unwrap_model_for_generation(model, self.accelerator) as unwrapped_model:\n            prompt_completion_ids = unwrapped_model.generate(**prompt_inputs, generation_config=self.generation_config)\n\n            prompt_length = prompt_ids.size(1)\n            prompt_ids = prompt_completion_ids[:, :prompt_length]\n            completion_ids = prompt_completion_ids[:, prompt_length:]\n            prompt_mask = prompt_mask.repeat_interleave(self.num_generations, dim=0)\n\n        # Mask everything after the first EOS token\n        is_eos = completion_ids == self.processing_class.eos_token_id\n        device = self.accelerator.device\n        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)\n        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)\n        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n\n        # Concatenate prompt_mask with completion_mask for logit computation\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)\n        if pixel_values is not None:\n            pixel_values = prompt_inputs[\"pixel_values\"].repeat(self.num_generations, 1)\n        if image_grid_thw is not None:\n            image_grid_thw = prompt_inputs[\"image_grid_thw\"].repeat_interleave(self.num_generations, dim=0)\n\n        per_token_logps = self._get_per_token_logps(model, prompt_completion_ids, attention_mask, pixel_values, image_grid_thw)\n        # Get rid of the prompt (-1 because of the shift done in get_per_token_logps)\n        per_token_logps = per_token_logps[:, prompt_length - 1 :]\n\n        with torch.inference_mode():\n            if self.ref_model is not None:\n                ref_per_token_logps = self._get_per_token_logps(self.ref_model, prompt_completion_ids, attention_mask, pixel_values, image_grid_thw)\n            else:\n                with self.accelerator.unwrap_model(model).disable_adapter():\n                    ref_per_token_logps = self._get_per_token_logps(model, prompt_completion_ids, attention_mask, pixel_values, image_grid_thw)\n        ref_per_token_logps = ref_per_token_logps[:, prompt_length - 1 :]\n\n        # Compute the KL divergence between the model and the reference model\n        per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1\n\n        # Decode the generated completions\n        completions = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)\n        if is_conversational(inputs[0]):\n            completions = [[{\"role\": \"assistant\", \"content\": completion}] for completion in completions]\n\n        # Compute the rewards\n        prompts = [prompt for prompt in prompts for _ in range(self.num_generations)]\n\n        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)\n        for i, (reward_func, reward_processing_class) in enumerate(\n            zip(self.reward_funcs, self.reward_processing_classes)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if is_conversational(inputs[0]):\n                    messages = [{\"messages\": p + c} for p, c in zip(prompts, completions)]\n                    texts = [apply_chat_template(x, reward_processing_class)[\"text\"] for x in messages]\n                else:\n                    texts = [p + c for p, c in zip(prompts, completions)]\n                reward_inputs = reward_processing_class(\n                    texts, return_tensors=\"pt\", padding=True, padding_side=\"right\", add_special_tokens=False\n                )\n                reward_inputs = super()._prepare_inputs(reward_inputs)\n                with torch.inference_mode():\n                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]  # Shape (B*G,)\n            else:\n                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n                reward_kwargs = {key: [] for key in inputs[0].keys() if key not in [\"prompt\", \"completion\"]}\n                for key in reward_kwargs:\n                    for example in inputs:\n                        # Repeat each value in the column for `num_generations` times\n                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n                output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)\n                rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n\n        # Sum the rewards from all reward functions\n        rewards = rewards_per_func.sum(dim=1)\n\n        # Compute grouped-wise rewards\n        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n\n        # Normalize the rewards to compute the advantages\n        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n\n        # x - x.detach() allows for preserving gradients from x\n        per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)\n        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n        loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n\n        # Log the metrics\n        completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()\n        self._metrics[\"completion_length\"].append(completion_length)\n\n        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(reward_func, PreTrainedModel):\n                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n            else:\n                reward_func_name = reward_func.__name__\n            self._metrics[f\"rewards/{reward_func_name}\"].append(reward_per_func[i].item())\n\n        self._metrics[\"reward\"].append(self.accelerator.gather_for_metrics(rewards).mean().item())\n\n        self._metrics[\"reward_std\"].append(self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item())\n\n        mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n        self._metrics[\"kl\"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())\n\n        return loss\n\n    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n        metrics = {key: sum(val) / len(val) for key, val in self._metrics.items()}  # average the metrics\n        logs = {**logs, **metrics}\n        if version.parse(transformers.__version__) >= version.parse(\"4.47.0.dev0\"):\n            super().log(logs, start_time)\n        else:  # transformers<=4.46\n            super().log(logs)\n        self._metrics.clear()\n\n    def create_model_card(\n        self,\n        model_name: Optional[str] = None,\n        dataset_name: Optional[str] = None,\n        tags: Union[str, list[str], None] = None,\n    ):\n        \"\"\"\n        Creates a draft of a model card using the information available to the `Trainer`.\n\n        Args:\n            model_name (`str` or `None`, *optional*, defaults to `None`):\n                Name of the model.\n            dataset_name (`str` or `None`, *optional*, defaults to `None`):\n                Name of the dataset used for training.\n            tags (`str`, `list[str]` or `None`, *optional*, defaults to `None`):\n                Tags to be associated with the model card.\n        \"\"\"\n        if not self.is_world_process_zero():\n            return\n\n        if hasattr(self.model.config, \"_name_or_path\") and not os.path.isdir(self.model.config._name_or_path):\n            base_model = self.model.config._name_or_path\n        else:\n            base_model = None\n\n        tags = tags or []\n        if isinstance(tags, str):\n            tags = [tags]\n\n        if hasattr(self.model.config, \"unsloth_version\"):\n            tags.append(\"unsloth\")\n\n        citation = textwrap.dedent(\n            \"\"\"\\\n            @article{zhihong2024deepseekmath,\n                title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},\n                author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},\n                year         = 2024,\n                eprint       = {arXiv:2402.03300},\n            \"\"\"\n        )\n\n        model_card = generate_model_card(\n            base_model=base_model,\n            model_name=model_name,\n            hub_model_id=self.hub_model_id,\n            dataset_name=dataset_name,\n            tags=tags,\n            wandb_url=wandb.run.get_url() if is_wandb_available() and wandb.run is not None else None,\n            comet_url=get_comet_experiment_url(),\n            trainer_name=\"GRPO\",\n            trainer_citation=citation,\n            paper_title=\"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\",\n            paper_id=\"2402.03300\",\n        )\n\n        model_card.save(os.path.join(self.args.output_dir, \"README.md\"))\n"}
