{"repo_info": {"repo_name": "ML-Model-CI", "repo_owner": "cap-ntu", "repo_url": "https://github.com/cap-ntu/ML-Model-CI"}}
{"type": "test_file", "path": "tests/test_keras_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Jiang Shanshan\nEmail: univeroner@gmail.com\nDate: 2021/2/28\n\n\"\"\"\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom pathlib import Path\n\nimport onnx\nimport onnxruntime\nimport tensorflow as tf\nimport numpy as np\nfrom modelci.types.trtis_objects import ModelInputFormat\n\nfrom modelci.types.bo import IOShape\n\nfrom modelci.hub.converter import convert\n\n\nclass TestKerasConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.keras_model = tf.keras.applications.ResNet50()\n        cls.sample_input = np.random.random((1, 224, 224, 3)).astype('float32')\n        cls.keras_model_predict = cls.keras_model.predict(cls.sample_input)\n        cls.tfs_model_path = Path(tempfile.gettempdir() + '/ResNet50/tensorflow-tfs/image_classification/1')\n        cls.trt_model_path = Path(tempfile.gettempdir() + '/ResNet50/tensorflow-trt/image_classification/1')\n        cls.inputs = [IOShape([-1, 224, 224, 3], dtype=float, name='input_1', format=ModelInputFormat.FORMAT_NHWC)]\n        cls.outputs = [IOShape([-1, 1000], dtype=float, name='probs')]\n\n    def test_keras_to_onnx(self):\n        onnx_model = convert(self.keras_model, 'keras', 'onnx')\n        onnx.checker.check_model(onnx_model)\n        ort_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n        ort_inputs = {ort_session.get_inputs()[0].name: self.sample_input}\n        onnx_model_predict = ort_session.run(None, ort_inputs)\n        np.testing.assert_allclose(onnx_model_predict[0], self.keras_model_predict, rtol=1e-05, atol=1e-05)\n\n    def test_keras_to_tfs(self):\n        convert(self.keras_model, 'tensorflow', 'tfs', save_path=self.tfs_model_path)\n        tfs_model = tf.keras.models.load_model(str(self.tfs_model_path))\n        tfs_model_predict = tfs_model.predict(self.sample_input)\n        np.testing.assert_allclose(tfs_model_predict, self.keras_model_predict, atol=1e-6)\n\n\n    @classmethod\n    def tearDownClass(cls):\n        if os.path.exists(str(cls.tfs_model_path)):\n            shutil.rmtree(cls.tfs_model_path)\n        if os.path.exists(str(cls.tfs_model_path) + '.zip'):\n            os.remove(str(cls.tfs_model_path) + '.zip')\n\n    if __name__ == '__main__':\n        unittest.main()"}
{"type": "test_file", "path": "tests/test_lightgbm_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport lightgbm as lgb\nimport numpy as np\nimport onnx\nimport onnxruntime\nimport torch\n\nfrom modelci.types.bo import IOShape\nfrom modelci.hub.converter import convert\nimport unittest\n\n\nclass TestLightgbmConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        num_classes = 2\n        X = np.random.rand(100000, 28).astype(np.float32)\n        y = np.random.randint(num_classes, size=100000)\n        cls.lgbm_model = lgb.LGBMClassifier()\n        cls.lgbm_model.fit(X, y)\n        cls.inputs = [IOShape(shape=[-1, 28], dtype=X.dtype, name='input_0')]\n        cls.sample_input = X[0:2, :]\n        cls.lgbm_model_out = cls.lgbm_model.predict(cls.sample_input)\n        cls.lgbm_model_probs = cls.lgbm_model.predict_proba(cls.sample_input)\n\n    # noinspection DuplicatedCode\n    def test_lightgbm_to_onnx(self):\n        onnx_model = convert(self.lgbm_model, 'lightgbm', 'onnx', inputs=self.inputs, opset=9, optimize=False)\n        onnx.checker.check_model(onnx_model)\n        ort_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n        ort_inputs = {ort_session.get_inputs()[0].name: self.sample_input}\n        onnx_model_out, onnx_model_probs = ort_session.run(None, ort_inputs)\n        np.testing.assert_array_equal(onnx_model_out, self.lgbm_model_out)\n        np.testing.assert_allclose(np.array([list(item.values()) for item in onnx_model_probs]), self.lgbm_model_probs, rtol=1e-05, atol=1e-05)\n\n    def test_lightgbm_to_torch(self):\n        model = convert(self.lgbm_model,'lightgbm', 'pytorch')\n        classes, probs = model(torch.from_numpy(self.sample_input))\n        np.testing.assert_array_equal(classes.numpy(), self.lgbm_model_out)\n        np.testing.assert_allclose(probs.numpy(), self.lgbm_model_probs, rtol=1e-05, atol=1e-05)\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "tests/test_model_api.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: USER\nEmail: yli056@e.ntu.edu.sg\nDate: 10/14/2020\n\"\"\"\nfrom http import HTTPStatus\nfrom pathlib import Path\n\nimport requests\n\nfrom modelci.config import app_settings\nfrom modelci.hub.registrar import download_model_from_url\n\nPath(f\"{str(Path.home())}/.modelci/ResNet50/pytorch-pytorch/image_classification\").mkdir(parents=True, exist_ok=True)\ndownload_model_from_url(\n    'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    f'{str(Path.home())}/.modelci/ResNet50/pytorch-pytorch/image_classification/1.pth'\n)\n\n\ndef test_get_all_models():\n    response = requests.get(f'{app_settings.api_v1_prefix}/model')\n    assert response.status_code == HTTPStatus.OK\n    assert response.json() == []\n\n\ndef test_publish_model():\n    payload = {'convert': True, 'profile': False}\n    form_data = {'architecture': 'ResNet50', 'framework': '1', 'engine': '7', 'version': '1', 'dataset': 'ImageNet',\n                 'metric': \"{'acc': 0.76}\", 'task': '0',\n                 'inputs': \"[{'shape': [-1, 3, 224, 224], 'dtype': 11, 'name': 'input', 'format': 0}]\",\n                 'outputs': \"[{'shape': [-1, 1000], 'dtype': 11, 'name': 'output', 'format': 0}]\"}\n    files = []\n    weights_file = f'{str(Path.home())}/.modelci/ResNet50/pytorch-pytorch/image_classification/1.pth'\n    files.append((\"files\", (weights_file, open(Path(weights_file), 'rb'), 'application/example')))\n    response = requests.post(f'{app_settings.api_v1_prefix}/model/', params=payload, data=form_data,\n                             files=files)\n    assert response.status_code == HTTPStatus.CREATED\n    assert '\"status\":true' in response.text\n\n\ndef test_get_model_by_id():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    response = requests.get(f'{app_settings.api_v1_prefix}/model/{model_id}')\n    assert response.status_code == HTTPStatus.OK\n    assert model_id in response.text\n\n\ndef test_update_model():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    response = requests.patch(f'{app_settings.api_v1_prefix}/model/{model_id}',\n                              json={'framework': 'TensorFlow'})\n    assert response.status_code == HTTPStatus.OK\n    assert '\"framework\":0' in response.text\n\n\ndef test_delete_model():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    response = requests.delete(f'{app_settings.api_v1_prefix}/model/{model_id}')\n    assert response.status_code == HTTPStatus.NO_CONTENT\n"}
{"type": "test_file", "path": "tests/test_model_service.py", "content": "from modelci.persistence import mongo\nfrom modelci.persistence.service import ModelService\nfrom modelci.types.bo import (\n    DynamicProfileResultBO,\n    ProfileMemory,\n    ProfileLatency,\n    ProfileThroughput,\n    ModelBO,\n    Framework,\n    Engine,\n    ModelVersion,\n    IOShape,\n    Weight,\n    StaticProfileResultBO,\n    InfoTuple,\n    Task,\n    Metric,\n    ModelStatus\n)\nfrom modelci.types.trtis_objects import ModelInputFormat\n\n\ndef test_init():\n    mongo.db.drop_database('test')\n\n\ndef test_register_model():\n    model = ModelBO(\n        'ResNet50',\n        framework=Framework.PYTORCH,\n        engine=Engine.PYTORCH,\n        version=ModelVersion(1),\n        dataset='ImageNet',\n        metric={Metric.ACC: 0.80},\n        task=Task.IMAGE_CLASSIFICATION,\n        inputs=[IOShape([-1, 3, 224, 224], dtype=float, format=ModelInputFormat.FORMAT_NCHW)],\n        outputs=[IOShape([-1, 1000], dtype=int)],\n        model_status=[ModelStatus.PUBLISHED],\n        weight=Weight(bytes([123]))\n    )\n\n    assert ModelService.post_model(model)\n\n\ndef test_get_model_by_name():\n    models = ModelService.get_models('ResNet50')\n\n    # check length\n    assert len(models) == 1\n    # check name\n    for model in models:\n        assert model.architecture == 'ResNet50'\n\n\ndef test_get_model_by_task():\n    models = ModelService.get_models_by_task(Task.IMAGE_CLASSIFICATION)\n\n    # check length\n    assert len(models) == 1\n    # check name\n    for model in models:\n        assert model.task == Task.IMAGE_CLASSIFICATION\n\n\ndef test_get_model_by_id():\n    model_bo = ModelService.get_models('ResNet50')[0]\n    model = ModelService.get_model_by_id(model_bo.id)\n\n    # check model id\n    assert model.id == model_bo.id\n\n\ndef test_update_model():\n    model = ModelService.get_models('ResNet50')[0]\n    model.metric[Metric.ACC] = 0.9\n    model.weight.weight = bytes([123, 255])\n\n    # check if update success\n    assert ModelService.update_model(model)\n\n    model_ = ModelService.get_models('ResNet50')[0]\n\n    # check updated model\n    assert abs(model_.metric[Metric.ACC] - 0.9) < 1e-6\n    assert model_.weight.weight == model.weight.weight\n\n\ndef test_register_static_profiling_result():\n    model = ModelService.get_models('ResNet50')[0]\n    spr = StaticProfileResultBO(5000, 200000, 200000, 10000, 10000, 10000)\n    assert ModelService.register_static_profiling_result(model.id, spr)\n\n\ndef test_register_dynamic_profiling_result():\n    model = ModelService.get_models('ResNet50')[0]\n    dummy_info_tuple = InfoTuple(avg=1, p50=1, p95=1, p99=1)\n    dpr = DynamicProfileResultBO(\n        device_id='gpu:01',\n        device_name='Tesla K40c',\n        batch=1,\n        memory=ProfileMemory(1000, 1000, 0.5),\n        latency=ProfileLatency(\n            init_latency=dummy_info_tuple,\n            preprocess_latency=dummy_info_tuple,\n            inference_latency=dummy_info_tuple,\n            postprocess_latency=dummy_info_tuple,\n        ),\n        throughput=ProfileThroughput(\n            batch_formation_throughput=1,\n            preprocess_throughput=1,\n            inference_throughput=1,\n            postprocess_throughput=1,\n        )\n    )\n    assert ModelService.append_dynamic_profiling_result(model.id, dpr)\n\n\ndef test_update_dynamic_profiling_result():\n    model = ModelService.get_models('ResNet50')[0]\n    dummy_info_tuple = InfoTuple(avg=1, p50=1, p95=1, p99=1)\n    updated_info_tuple = InfoTuple(avg=1, p50=2, p95=1, p99=1)\n    dpr = DynamicProfileResultBO(\n        device_id='gpu:01',\n        device_name='Tesla K40c',\n        batch=1,\n        memory=ProfileMemory(1000, 2000, 0.5),\n        latency=ProfileLatency(\n            init_latency=dummy_info_tuple,\n            preprocess_latency=dummy_info_tuple,\n            inference_latency=updated_info_tuple,\n            postprocess_latency=dummy_info_tuple,\n        ),\n        throughput=ProfileThroughput(\n            batch_formation_throughput=1,\n            preprocess_throughput=1,\n            inference_throughput=1,\n            postprocess_throughput=1,\n        )\n    )\n    # check update\n    assert ModelService.update_dynamic_profiling_result(model.id, dpr)\n    # check result\n    model = ModelService.get_models('ResNet50')[0]\n    assert model.profile_result.dynamic_results[0].memory.memory_usage == 2000\n    assert model.profile_result.dynamic_results[0].latency.inference_latency.p50 == 2\n\n\ndef test_delete_dynamic_profiling_result():\n    model = ModelService.get_models('ResNet50')[0]\n    dummy_info_tuple1 = InfoTuple(avg=1, p50=1, p95=1, p99=2)\n    dummy_info_tuple2 = InfoTuple(avg=1, p50=1, p95=1, p99=1)\n\n    dpr = DynamicProfileResultBO(\n        device_id='gpu:02',\n        device_name='Tesla K40c',\n        batch=1,\n        memory=ProfileMemory(1000, 1000, 0.5),\n        latency=ProfileLatency(\n            init_latency=dummy_info_tuple1,\n            preprocess_latency=dummy_info_tuple2,\n            inference_latency=dummy_info_tuple2,\n            postprocess_latency=dummy_info_tuple2,\n        ),\n        throughput=ProfileThroughput(\n            batch_formation_throughput=1,\n            preprocess_throughput=1,\n            inference_throughput=1,\n            postprocess_throughput=1,\n        )\n    )\n    ModelService.append_dynamic_profiling_result(model.id, dpr)\n\n    # reload\n    model = ModelService.get_models('ResNet50')[0]\n    dpr_bo = model.profile_result.dynamic_results[0]\n    dpr_bo2 = model.profile_result.dynamic_results[1]\n\n    # check delete\n    assert ModelService.delete_dynamic_profiling_result(model.id, dpr_bo.ip, dpr_bo.device_id)\n\n    # check result\n    model = ModelService.get_models('ResNet50')[0]\n    assert len(model.profile_result.dynamic_results) == 1\n\n    dpr_left = model.profile_result.dynamic_results[0]\n    assert dpr_bo2.latency.init_latency.avg == dpr_left.latency.init_latency.avg\n\n\ndef test_delete_model():\n    model = ModelService.get_models('ResNet50')[0]\n    assert ModelService.delete_model_by_id(model.id)\n\n\ndef test_drop_test_database():\n    mongo.db.drop_database('test')\n"}
{"type": "test_file", "path": "tests/test_modelhub_cli.py", "content": "# test ModelCI CLI with unitest\nfrom pathlib import Path\n\nimport requests\nimport torch\nfrom typer.testing import CliRunner\nimport torchvision\nfrom modelci.config import app_settings\nfrom modelci.cli.modelhub import app\n\nrunner = CliRunner()\nfile_dir = f\"{str(Path.home())}/.modelci/ResNet50/PyTorch-PYTORCH/Image_Classification\"\nPath(file_dir).mkdir(parents=True, exist_ok=True)\nfile_path = file_dir + \"/1.pth\"\n\n\ndef test_get():\n    result = runner.invoke(app, [\n        'get',\n        'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n        f'{str(Path.home())}/.modelci/ResNet50/PyTorch-PYTORCH/Image_Classification/1.pth'\n    ])\n    assert result.exit_code == 0\n    assert \"model downloaded successfully\" in result.stdout\n\n\ndef test_publish():\n    result = runner.invoke(app, [\n        'publish', '-f', 'example/resnet50.yml'\n    ])\n    assert result.exit_code == 0\n    assert \"\\'status\\': True\" in result.stdout\n\n\ndef test_ls():\n    result = runner.invoke(app, ['ls'])\n    assert result.exit_code == 0\n\n\ndef test_detail():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    result = runner.invoke(app, ['detail', model_id])\n    assert result.exit_code == 0\n\n\ndef test_update():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    result = runner.invoke(app, ['update', model_id, '--version', '2'])\n    assert result.exit_code == 0\n\n\ndef test_delete():\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    result = runner.invoke(app, ['delete', model_id])\n    assert result.exit_code == 0\n    assert f\"Model {model_id} deleted\\n\" == result.output\n\n\ndef test_convert():\n    torch_model = torchvision.models.resnet50(pretrained=False)\n    torch_model.load_state_dict(torch.load(file_path))\n    torch.save(torch_model, file_path)\n    result = runner.invoke(app, [\n        'convert', '-f', 'example/resnet50.yml'\n    ])\n    assert result.exit_code == 0\n\n\ndef test_profile():\n    runner.invoke(app, [\n        'publish', '-f', 'example/resnet50_torchscript.yml'\n    ])\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    torchscript_id = None\n    for model in model_list:\n        if model['engine'] == \"TORCHSCRIPT\":\n            torchscript_id = model['id']\n            break\n    result = runner.invoke(app, [\n        'profile', torchscript_id, '-d', 'cpu'\n    ])\n    with requests.get(f'{app_settings.api_v1_prefix}/model/') as r:\n        model_list = r.json()\n    model_id = model_list[0][\"id\"]\n    runner.invoke(app, ['delete', model_id])\n    assert result.exit_code == 0\n\n\n\n"}
{"type": "test_file", "path": "tests/test_pytorch_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nimport os\nimport unittest\nfrom pathlib import Path\n\nimport torch\nimport onnx\nimport torchvision\n\nfrom modelci.types.bo import IOShape\n\nfrom modelci.types.trtis_objects import ModelInputFormat\nimport numpy as np\nimport tempfile\nfrom modelci.hub.converter import convert\nimport onnxruntime as rt\n\n\nclass TestPytorchConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.torch_model = torchvision.models.alexnet(pretrained=True)\n        cls.inputs = [IOShape([1, 3, 224, 224], dtype=float, name='INPUT__0', format=ModelInputFormat.FORMAT_NCHW)]\n        cls.outputs = [IOShape([-1, 1000], dtype=float, name='probs')]\n        cls.X = torch.rand(1, 3, 224, 224, dtype=torch.float32)\n        cls.onnx_path = Path(tempfile.gettempdir() + '/test.onnx')\n        cls.torchscript_path = Path(tempfile.gettempdir() + '/test_torchscript.zip')\n\n    def test_torch_to_onnx(self):\n        convert(self.torch_model, 'pytorch', 'onnx', save_path=self.onnx_path, inputs=self.inputs, outputs=self.outputs, opset=11)\n        onnx_model = onnx.load(self.onnx_path)\n        # TODO add checker after upgrade ONNX version to 1.7\n        sess = rt.InferenceSession(onnx_model.SerializeToString())\n        onnx_model_predict = sess.run(['probs'], {'INPUT__0': self.X.numpy()})[0].flatten()\n        self.torch_model.eval()\n        torch_model_predict = self.torch_model(self.X)[0].data.numpy()\n        np.testing.assert_allclose(onnx_model_predict, torch_model_predict, rtol=1e-05, atol=1e-05)\n\n    def test_torch_to_torchscript(self):\n        convert(self.torch_model, 'pytorch', 'torchscript', save_path=self.torchscript_path)\n        torchscript_model = torch.jit.load(str(self.torchscript_path))\n        torchscript_model_predict = torchscript_model(self.X)[0].data.numpy()\n        self.torch_model.eval()\n        torch_model_predict = self.torch_model(self.X)[0].data.numpy()\n        np.testing.assert_allclose(torchscript_model_predict, torch_model_predict, rtol=1e-05, atol=1e-05)\n\n    @classmethod\n    def tearDownClass(cls):\n        if os.path.exists(cls.onnx_path):\n            os.remove(cls.onnx_path)\n        if os.path.exists(cls.torchscript_path):\n            os.remove(cls.torchscript_path)\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "tests/test_onnx_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport onnxruntime as rt\nimport torch\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\n\nfrom modelci.hub.converter import convert\nfrom modelci.types.bo import IOShape\n\n\nclass TestONNXConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        X_bc, y_bc = load_breast_cancer(return_X_y=True)\n        nrows = 15000\n        X_bc: np.ndarray = X_bc[0:nrows]\n        y_bc: np.ndarray = y_bc[0:nrows]\n        model = lgb.LGBMRegressor(n_estimators=3, min_child_samples=1)\n        model.fit(X_bc, y_bc)\n        inputs_bc = [IOShape(shape=[-1, X_bc.shape[1]], dtype=float, name='input_0')]\n        cls.onnx_model = convert(model, 'lightgbm', 'onnx', inputs=inputs_bc, optimize=False)\n        sess = rt.InferenceSession(cls.onnx_model.SerializeToString())\n        cls.sample_input = torch.rand(2, X_bc.shape[1], dtype=torch.float32)\n        cls.onnx_model_predict = sess.run(None, {'input_0': cls.sample_input.numpy()})[0].flatten()\n\n    # noinspection DuplicatedCode\n    def test_onnx_to_pytorch(self):\n        torch_model = convert(self.onnx_model, 'onnx', 'pytorch')\n        torch_model.eval()\n        torch_model_predict = torch_model(self.sample_input).data.numpy().flatten()\n        np.testing.assert_allclose(self.onnx_model_predict, torch_model_predict, rtol=1e-05, atol=1e-05)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_sklearn_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport onnx\nimport onnxruntime\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom modelci.hub.converter import convert\nfrom modelci.types.bo import IOShape\n\n\nclass TestSklearnConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        X_bc, y_bc = load_breast_cancer(return_X_y=True)\n        nrows = 15000\n        X_bc: np.ndarray = X_bc[0:nrows]\n        y_bc: np.ndarray = y_bc[0:nrows]\n\n        cls.sklearn_model = RandomForestClassifier(n_estimators=10, max_depth=10)\n        cls.sklearn_model.fit(X_bc, y_bc)\n        cls.inputs_bc = [IOShape(shape=[-1, X_bc.shape[1]], dtype=float, name='input_0')]\n        cls.sample_input = X_bc[0:2, :].astype(np.float32)\n        cls.sklearn_model_out = cls.sklearn_model.predict(cls.sample_input)\n        cls.sklearn_model_probs = cls.sklearn_model.predict_proba(cls.sample_input)\n\n    # noinspection DuplicatedCode\n    def test_sklearn_to_onnx(self):\n        onnx_model = convert(self.sklearn_model, 'sklearn', 'onnx', inputs=self.inputs_bc, optimize=False)\n        onnx.checker.check_model(onnx_model)\n        ort_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n        ort_inputs = {ort_session.get_inputs()[0].name: self.sample_input}\n        onnx_model_out, onnx_model_probs = ort_session.run(None, ort_inputs)\n        np.testing.assert_array_equal(onnx_model_out, self.sklearn_model_out)\n        np.testing.assert_allclose(np.array([list(item.values()) for item in onnx_model_probs]), self.sklearn_model_probs, rtol=1e-05, atol=1e-05)\n\n    def test_sklearn_to_torch(self):\n        model = convert(self.sklearn_model, 'sklearn', 'pytorch', extra_config={'tree_implementation': 'gemm'})\n        torch_model_out, torch_model_probs = model(self.sample_input)\n        np.testing.assert_array_equal(torch_model_out.numpy(), self.sklearn_model_out)\n        np.testing.assert_allclose(torch_model_probs.numpy(), self.sklearn_model_probs, rtol=1e-05, atol=1e-05)\n\n    if __name__ == '__main__':\n        unittest.main()"}
{"type": "test_file", "path": "tests/test_xgboost_conversion.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport onnx\nimport onnxruntime\nimport xgboost as xgt\n\nfrom modelci.hub.converter import convert\nfrom modelci.types.bo import IOShape\n\n\nclass TestXgboostConverter(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        num_classes = 2\n        X = np.random.rand(100000, 28).astype(np.float32)\n        y = np.random.randint(num_classes, size=100000)\n        cls.xgboost_model = xgt.XGBClassifier()\n        cls.xgboost_model.fit(X, y)\n        cls.inputs = [IOShape(shape=[-1, 28], dtype=X.dtype, name='input_0')]\n        cls.sample_input = X[0:2, :]\n        cls.xgboost_model_out = cls.xgboost_model.predict(cls.sample_input)\n        cls.xgboost_model_probs = cls.xgboost_model.predict_proba(cls.sample_input)\n\n    def test_xgboost_to_onnx(self):\n        onnx_model = convert(self.xgboost_model, 'xgboost', 'onnx', inputs=self.inputs)\n        onnx.checker.check_model(onnx_model)\n        ort_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n        ort_inputs = {ort_session.get_inputs()[0].name: self.sample_input}\n        onnx_model_out, onnx_model_probs = ort_session.run(None, ort_inputs)\n        np.testing.assert_array_equal(onnx_model_out, self.xgboost_model_out)\n        np.testing.assert_allclose(np.array(onnx_model_probs), self.xgboost_model_probs, rtol=1e-05, atol=1e-05)\n\n    def test_xgboost_to_torch(self):\n        model = convert(self.xgboost_model, 'xgboost', 'pytorch', inputs=self.inputs)\n        torch_model_out, torch_model_probs = model(self.sample_input)\n        np.testing.assert_array_equal(torch_model_out.numpy(), self.xgboost_model_out)\n        np.testing.assert_allclose(torch_model_probs.numpy(), self.xgboost_model_probs, rtol=1e-05, atol=1e-05)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "source_file", "path": "example/sample_mrcnn.py", "content": "\"\"\"\nMask R-CNN registering, managing, serving and profiling code demo using ModelCI.\n\"\"\"\n\nfrom PIL import Image\n\nfrom modelci.hub.client.tfs_client import CVTFSClient\nfrom modelci.hub.manager import retrieve_model, register_model_from_yaml\nfrom modelci.hub.profiler import Profiler\nfrom modelci.types.bo import Engine, Framework\n\nif __name__ == \"__main__\":\n    test_img = Image.open(\"path to the test data\")\n\n    register_model_from_yaml(\"path to your yaml file\")  # register your model in the database\n    model_info = retrieve_model(  # retrieve model information\n        architecture_name='MRCNN',\n        framework=Framework.TENSORFLOW,\n        engine=Engine.TFS\n    )[0]\n\n    tfs_client = CVTFSClient(\n        test_img, batch_num=100, batch_size=32, asynchronous=True, model_info=model_info\n    )\n\n    profiler = Profiler(model_info=model_info, server_name='tfs', inspector=tfs_client)\n    profiler.diagnose(device='cuda:0')  # profile batch size 32\n"}
{"type": "source_file", "path": "modelci/app/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/19/2020\n\"\"\"\nimport multiprocessing as mp\nimport os\nimport signal\nfrom pathlib import Path\n\nfrom modelci.config import app_settings\nfrom modelci.utils import Logger\nfrom modelci.utils.misc import check_process_running\n\nlogger = Logger('modelci backend', welcome=False)\ndefault_log_file = Path.home() / 'tmp/modelci.log'\ndefault_log_file.parent.mkdir(exist_ok=True)\n\n\ndef start():\n    \"\"\"Run a ModelCI backend server with Uvicorn.\"\"\"\n    from modelci.app.main import _app_start_detach\n\n    # check if the process is running\n    pid = check_process_running(app_settings.server_port)\n    if not pid:\n        backend_process = mp.Process(target=_app_start_detach, args=(default_log_file,))\n        backend_process.start()\n\n        logger.info(f'Uvicorn server listening on {app_settings.server_url}, check full log at {default_log_file}')\n    else:\n        logger.warning(f'Unable to started server. A process with pid={pid} is already listening on '\n                       f'port {app_settings.server_port}. '\n                       'Please check if your Uvicorn server has started.')\n\n\ndef stop():\n    \"\"\"Stop the ModelCI backend server.\"\"\"\n    # get backend process pid\n    pid = check_process_running(app_settings.server_port)\n    if pid:\n        os.killpg(os.getpgid(pid), signal.SIGTERM)\n        logger.info(f'The Uvicorn server with pid={pid} stopped.')\n    else:\n        logger.warning(f'No process is listening on port {app_settings.server_port}')\n\n\nif __name__ == '__main__':\n    start()\n"}
{"type": "source_file", "path": "modelci/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 9/19/2020\n\"\"\"\nfrom . import data_engine\nfrom . import metrics\nfrom . import monitor\nfrom . import types\nfrom . import utils\n\n__all__ = ['data_engine', 'metrics', 'monitor', 'types', 'utils']\n"}
{"type": "source_file", "path": "modelci/app/experimental/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: yuanmingleee\nEmail: \nDate: 1/29/2021\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/experimental/api.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/20/2020\n\"\"\"\nfrom fastapi import APIRouter\n\nfrom modelci.app.experimental.endpoints import cv_tuner\nfrom modelci.app.experimental.endpoints import model_structure, trainer\n\napi_router = APIRouter()\napi_router.include_router(cv_tuner.router, prefix='/cv-tuner', tags=['[*exp] cv-tuner'])\napi_router.include_router(model_structure.router, prefix='/structure', tags=['[*exp] structure'])\napi_router.include_router(trainer.router, prefix='/train', tags=['[*exp] train'])\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/29/2021\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/cv_tuner.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: yuanmingleee\nEmail: \nDate: 1/29/2021\n\"\"\"\n\nimport torch\nfrom fastapi import APIRouter\n\nfrom modelci.experimental.model.model_structure import Structure, Operation\nfrom modelci.hub.registrar import register_model\nfrom modelci.hub.manager import get_remote_model_weight\nfrom modelci.hub.utils import generate_path_plain\nfrom modelci.persistence.service import ModelService\nfrom modelci.types.bo import ModelVersion, Engine, IOShape, ModelStatus\nfrom modelci.types.models.mlmodel import MLModel\nfrom modelci.types.type_conversion import model_data_type_to_torch, type_to_data_type\nfrom modelci.utils.exceptions import ModelStructureError\n\nrouter = APIRouter()\n\n\n@router.patch('/finetune/{id}')\ndef update_finetune_model_as_new(id: str, updated_layer: Structure, dry_run: bool = False):  # noqa\n    \"\"\"\n    Temporary function for finetune CV models. The function's functionality is overlapped with\n    `update_model_structure_as_new`. Please use the `update_model_structure_as_new` in next release.\n\n    Examples:\n        Fine-tune the model by modify the layer with name 'fc' (last layer). The layer\n        has a changed argument out_features = 10. op_='M' indicates the operation to this layer ('fc')\n        is 'Modify'. There is no changes in layer connections.\n        Therefore, the structure change summary is\n            [M] fc: (...) out_features=10\n\n        >>> from collections import OrderedDict\n        >>> structure_data = {\n        ...     'layer': OrderedDict({'fc': {'out_features': 10, 'op_': 'M', 'type_': 'torch.nn.Linear'}})\n        ... }\n        >>> update_finetune_model_as_new(id=..., updated_layer=Structure.parse_obj(structure_data))\n\n    Args:\n        id (str): ID of the model to be updated.\n        updated_layer (Structure): Contains layers to be fine-tuned.\n        dry_run (bool): Test run for verify if the provided parameter (i.e. model specified in `id`\n            and updated layers) is valid.\n\n    Returns:\n\n    \"\"\"\n    if len(updated_layer.layer.items()) == 0:\n        return True\n    model = ModelService.get_model_by_id(id)\n    if model.engine != Engine.PYTORCH:\n        raise ValueError(f'model {id} is not supported for editing. '\n                         f'Currently only support model with engine=PYTORCH')\n    # download model as local cache\n    cache_path = get_remote_model_weight(model=model)\n    net = torch.load(cache_path)\n\n    for layer_name, layer_param in updated_layer.layer.items():\n        layer_op = getattr(layer_param, 'op_')\n\n        # update layer\n        if layer_op == Operation.MODIFY:\n\n            # check if the layer name exists\n            # TODO check if layer path exists eg.\"layer1.0.conv1\"\n            if not hasattr(net, layer_name):\n                raise ModelStructureError(f'Structure layer name `{layer_name}` not found in model {id}.')\n            net_layer = getattr(net, layer_name)\n\n            # check if the provided type matches the original type\n            layer_type = type(net_layer)\n            layer_type_provided = eval(layer_param.type_.value)  # nosec\n            if layer_type is not layer_type_provided:\n                raise ModelStructureError(f'Expect `{layer_name}.type_` to be {layer_type}, '\n                                          f'but got {layer_type_provided}')\n\n            # get layer parameters\n            layer_param_old = layer_param.parse_layer_obj(net_layer)\n            layer_param_data = layer_param_old.dict(exclude_none=True, exclude={'type_', 'op_'})\n\n            layer_param_update_data = layer_param.dict(exclude_none=True, exclude={'type_', 'op_'})\n            # replace 'null' with None. See reason :class:`ModelLayer`.\n            for k, v in layer_param_update_data.items():\n                if v == 'null':\n                    layer_param_update_data[k] = None\n\n            # update the layer parameters\n            layer_param_data.update(layer_param_update_data)\n            layer = layer_type(**layer_param_data)\n            setattr(net, layer_name, layer)\n\n        else:\n            # if layer_op is Operation.ADD,\n            #     1. check if the layer name not exists\n            #     2. add a layer\n            #     3. change the `forward` function according to the connections\n            # if layer_op is Operation.DELETE,\n            #     1. check if the layer exists\n            #     2. delete the layer\n            #     3. change the `forward` function\n            raise ValueError('Operation not permitted. Please use `update_model_structure_as_new`.')\n\n    input_tensors = list()\n    bs = 1\n    for input_ in model.inputs:\n        input_tensor = torch.rand(bs, *input_.shape[1:]).type(model_data_type_to_torch(input_.dtype))\n        input_tensors.append(input_tensor)\n\n    # parse output tensors\n    output_shapes = list()\n    output_tensors = net(*input_tensors)\n    if not isinstance(output_tensors, (list, tuple)):\n        output_tensors = (output_tensors,)\n    for output_tensor in output_tensors:\n        output_shape = IOShape(shape=[bs, *output_tensor.shape[1:]], dtype=type_to_data_type(output_tensor.dtype))\n        output_shapes.append(output_shape)\n\n    if not dry_run:\n        # TODO return validation result for dry_run mode\n        # TODO apply Semantic Versioning https://semver.org/\n        # TODO reslove duplicate model version problem in a more efficient way\n        version = ModelVersion(model.version.ver + 1)\n        previous_models = ModelService.get_models(\n            architecture=model.architecture,\n            task=model.task,\n            framework=model.framework,\n            engine=Engine.NONE\n        )\n        if len(previous_models):\n            last_version = max(previous_models, key=lambda k: k.version.ver).version.ver\n            version = ModelVersion(last_version + 1)\n\n        saved_path = generate_path_plain(\n            architecture=model.architecture,\n            task=model.task,\n            framework=model.framework,\n            engine=Engine.NONE,\n            version=version\n        )\n        saved_path.parent.mkdir(parents=True, exist_ok=True)\n        torch.save(model,saved_path.with_suffix('.pt') )\n        mlmodelin = MLModel(\n            dataset='',\n            metric={key: 0 for key in model.metric.keys()},\n            task=model.task,\n            inputs=model.inputs,\n            outputs=output_shapes,\n            architecture=model.name,\n            framework=model.framework,\n            engine=Engine.NONE,\n            model_status=[ModelStatus.DRAFT],\n            parent_model_id=model.id,\n            version=version,\n            weight=saved_path\n        )\n        register_model(\n            mlmodelin,\n            convert=False, profile=False\n        )\n\n        model_bo = ModelService.get_models(\n            architecture=model.architecture,\n            task=model.task,\n            framework=model.framework,\n            engine=Engine.NONE,\n            version=version\n        )[0]\n\n        return {'id': model_bo.id}\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/drl_tuner.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/29/2021\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/model_structure.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/31/2021\n\nML model structure related API\n\"\"\"\nimport torch\nfrom fastapi import APIRouter\nfrom modelci.hub.manager import get_remote_model_weight\n\nfrom modelci.types.bo import Engine\n\nfrom modelci.persistence.service import ModelService\n\nfrom modelci.experimental.model.model_structure import Structure\n\nrouter = APIRouter()\n\n\n@router.get('/{id}', response_model=Structure)\nasync def get_model_structure(id: str):  # noqa\n    \"\"\"\n    Get model structure as a model structure graph (connection between layer as edge, layers as nodes)\n\n    Arguments:\n        id (str): Model object ID.\n    \"\"\"\n    # return model DAG\n    raise NotImplementedError('Method `get_model_structure` not implemented.')\n\n\n@router.patch('/{id}')  # TODO: add response_model\ndef update_model_structure_as_new(id: str, structure: Structure, dry_run: bool = False):  # noqa\n    \"\"\"\n    TODO: Update model structure by adjusting layers (add, modify, delete) or rewiring the\n        connections between layers.\n\n    Examples:\n        Fine-tune the model by modify the layer with name 'fc' (last layer). The layer\n        has a changed argument out_features = 10. op_='M' indicates the operation to this layer ('fc')\n        is 'Modify'. There is no changes in layer connections.\n        Therefore, the structure change summary is\n            [M] fc: (...) out_features=10\n\n        >>> from collections import OrderedDict\n        >>> structure_data = {\n        ...     'layer': OrderedDict({'fc': {'out_features': 10, 'op_': 'M', 'type_': 'torch.nn.Linear'}})\n        ... }\n        >>> update_model_structure_as_new(id=..., structure=Structure.parse_obj(structure_data))\n\n        Use original model as a feature extractor. The new model delete the last layer named 'fc', and add two\n        layers as following:\n            fc1: (nn.Linear) in_features=1024, out_features=512\n            fc2: (nn.Linear) in_features=512, out_features=10\n        The node change summary is\n            [D] fc\n            [A] fc1: (nn.Linear) in_features=1024, out_features=512\n            [A] fc2: (nn.Linear) in_features=512, out_features=10\n        Besides, we have connection changes:\n            [D] conv1 -> fc\n            [A] conv1 -> fc1\n            [A] fc1 -> fc2\n\n        >>>\n        ... structure_data = {\n        ...     'layer': {\n        ...         'fc': {'op_': 'D'},\n        ...         'fc1': {'in_features': 1024, 'out_features': 512, 'type_': 'torch.nn.Linear', 'op_': 'A'},\n        ...         'fc2': {'in_features': 512, 'out_features': 10, 'type_': 'torch.nn.Linear', 'op_': 'A'},\n        ...     },\n        ...     'connection': {\n        ...         'conv1': {'fc': 'D', 'fc1': 'A'},\n        ...         'fc1': {'fc2': 'A'},\n        ...     }\n        ... }\n\n    Args:\n        id (str): Model object ID of the original structure.\n        structure: A model structure graph indicating changed layer (node) and layer connection (edge).\n        dry_run (bool): Dry run update for validation.\n\n    Returns:\n\n    \"\"\"\n    raise NotImplementedError('Method `update_model_structure_as_new` not implemented.')\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/nlp_tuner.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/29/2021\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/handler.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/22/2020\n\"\"\"\nfrom datetime import datetime\n\nfrom fastapi import Request\nfrom starlette.responses import JSONResponse\n\nfrom modelci.app.main import app\nfrom modelci.persistence.exceptions import DoesNotExistException, BadRequestValueException, ServiceException\n\n\n@app.exception_handler(ServiceException)\nasync def service_exception_handler(request: Request, exc: ServiceException):\n    return JSONResponse(\n        status_code=500,\n        content={'message': exc.message, 'status': 500, 'timestamp': datetime.now().timestamp()}\n    )\n\n\n@app.exception_handler(DoesNotExistException)\nasync def does_not_exist_exception_handler(request: Request, exc: DoesNotExistException):\n    return JSONResponse(\n        status_code=404,\n        content={'message': exc.message, 'status': 404, 'timestamp': datetime.now().timestamp()}\n    )\n\n\n@app.exception_handler(BadRequestValueException)\nasync def bad_request_value_exception_handler(request: Request, exc: BadRequestValueException):\n    return JSONResponse(\n        status_code=400,\n        content={'message': exc.message, 'status': 400, 'timestamp': datetime.now().timestamp()}\n    )\n"}
{"type": "source_file", "path": "modelci/app/main.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/19/2020\n\"\"\"\nimport os\nimport sys\nfrom typing import Optional\n\nimport uvicorn\nfrom fastapi import FastAPI\nfrom starlette.middleware.cors import CORSMiddleware\n\nfrom modelci import config\nfrom modelci.app.experimental.api import api_router as api_rounter_exp\nfrom modelci.app.v1.api import api_router\n\nsettings = config.AppSettings()\napp = FastAPI(title=settings.project_name, openapi_url=\"/api/v1/openapi.json\")\n\n# CORS\norigins = []\n\n# Set all CORS enabled origins\nif settings.backend_cors_origins:\n    origins_raw = settings.backend_cors_origins.split(\",\")\n    for origin in origins_raw:\n        use_origin = origin.strip().replace('\"', '')\n        origins.append(use_origin)\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\napp.include_router(api_router, prefix=config.API_V1_STR)\napp.include_router(api_rounter_exp, prefix=config.API_EXP_STR)\n\n\ndef _app_start_detach(output_file: Optional[str] = None):\n    \"\"\"Start FastAPI as a detached process.\n\n    This is a double fork approach.\n\n    Reference:\n        https://stackoverflow.com/a/49123627\n    \"\"\"\n\n    if os.fork() != 0:  # do a double fork,\n        return\n\n    if output_file:\n        # redirect stdout, stderr to a file\n        output_file = open(output_file, 'a')\n        sys.stdout = output_file\n        sys.stderr = output_file\n    uvicorn.run(app, host=settings.server_host, port=settings.server_port)\n    if output_file:\n        output_file.close()\n\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=settings.server_host, port=settings.server_port)\n"}
{"type": "source_file", "path": "modelci/app/v1/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/20/2020\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/v1/api.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/20/2020\n\"\"\"\nfrom fastapi import APIRouter\n\nfrom modelci.app.v1.endpoints import model\nfrom modelci.app.v1.endpoints import visualizer\nfrom modelci.app.v1.endpoints import profiler\n\napi_router = APIRouter()\napi_router.include_router(model.router, prefix='/model', tags=['model'])\napi_router.include_router(visualizer.router, prefix='/visualizer', tags=['visualizer'])\napi_router.include_router(profiler.router, prefix='/profiler', tags=['profiler'])"}
{"type": "source_file", "path": "modelci/app/v1/endpoints/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/20/2020\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/app/v1/endpoints/model.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/20/2020\n\"\"\"\nimport asyncio\nimport http\nimport json\nimport shutil\nfrom pathlib import Path\nfrom typing import List\n\nfrom fastapi import APIRouter, File, UploadFile, Depends\nfrom fastapi.exceptions import RequestValidationError, HTTPException\nfrom pydantic.error_wrappers import ErrorWrapper\nfrom starlette.responses import JSONResponse\n\nfrom modelci.hub.registrar import register_model\nfrom modelci.persistence.service_ import get_by_id, get_models, update_model, delete_model, exists_by_id\nfrom modelci.types.models.mlmodel import MLModel, BaseMLModel, ModelUpdateSchema, Framework, Engine, Task\n\nrouter = APIRouter()\n\n\n@router.get('/')\ndef get_all_models(architecture: str = None, framework: Framework = None, engine: Engine = None, task: Task = None,\n                  version: int = None):\n    models = get_models(architecture=architecture, framework=framework, engine=engine, task=task, version=version)\n    content = list(map( lambda item: json.loads(item.json(by_alias=False)), models))\n    return JSONResponse(content=content)\n\n\n@router.get('/{id}')\ndef get_model(*, id: str):  # noqa\n    # Due to FastAPI use default json encoder before customer encoder, we have to rely on\n    # Pydantic BaseModel.json and convert it back\n    # Check https://github.com/tiangolo/fastapi/blob/master/fastapi/encoders.py#L118 to see if this\n    # issue is fixed.\n    content = json.loads(get_by_id(id).json(by_alias=False))\n    return JSONResponse(content=content)\n\n\n@router.patch('/{id}', response_model=MLModel)\ndef update(id: str, schema: ModelUpdateSchema):\n    if not exists_by_id(id):\n        raise HTTPException(\n            status_code=404,\n            detail=f'Model ID {id} does not exist. You may change the ID',\n        )\n    return update_model(id, schema)\n\n\n@router.delete('/{id}', status_code=http.HTTPStatus.NO_CONTENT)\ndef delete(id: str):\n    if not exists_by_id(id):\n        raise HTTPException(\n            status_code=404,\n            detail=f'Model ID {id} does not exist. You may change the ID',\n        )\n    delete_model(id)\n\n\n@router.post('/', status_code=201)\nasync def publish_model(\n        model: BaseMLModel = Depends(BaseMLModel.as_form),\n        files: List[UploadFile] = File(\n            [],\n            description='This field can be set with empty value. In such settings, the publish is a dry run to'\n                        'validate the `ml_model_in_form` field. You are recommend to try a dry run to find input'\n                        'errors before you send the wight file(s) to the server.'\n        ),\n        convert: bool = True,\n        profile: bool = False\n):\n    \"\"\"Publish model to the model hub. The model weight file(s) as well as its meta information (e.g.\n    architecture, framework, and serving engine) will be stored into the model hub.\n\n    The publish API will also automatically convert the published model into other deployable format such as\n    TorchScript and ONNX. After successfully converted, original model and its generated models will be profiled\n    on the underlying devices in the clusters, and collects, aggregates, and processes running model performance.\n\n    Args:\n        model (MLModel): Model meta information.\n        files (List[UploadFile]): A list of model weight files. The files are organized accordingly. Their file name\n            contains relative path to their common parent directory.\n            If the files is empty value, a dry-run to this API is conducted for parameter checks. No information\n            will be saved into model hub in this case.\n        convert (bool): Flag for auto configuration.\n        profile (bool): Flag for auto profiling.\n\n    Returns:\n        A message response, with IDs of all published model. The format of the return is:\n        ```\n        {\n          \"data\": {\"id\": [\"603e6a1f5a62b08bc0a2a7f2\", \"603e6a383b00cbe9bfee7277\"]},\n          \"status\": true\n        }\n        ```\n        Specially, if the dry-run test passed, it will return a status True:\n        ```\n        {\n          \"status\": true\n        }\n        ```\n    \"\"\"\n    # save the posted files as local cache\n    loop = asyncio.get_event_loop()\n    saved_path = model.saved_path\n    if len(files) == 0:\n        # conduct dry run for parameter check only.\n        return {'status': True}\n    if len(files) == 1:\n        file = files[0]\n        suffix = Path(file.filename).suffix\n        try:\n            # create directory\n            if len(suffix) == 0:\n                error = ErrorWrapper(\n                    ValueError(f'Expect a suffix for file {file.filename}, got None.'), loc='files[0]'\n                )\n                raise RequestValidationError([error])\n            saved_path = saved_path.with_suffix(suffix)\n            saved_path.parent.mkdir(exist_ok=True, parents=True)\n\n            # save file\n            await file.seek(0)\n            with open(saved_path, 'wb') as buffer:\n                await loop.run_in_executor(None, shutil.copyfileobj, file.file, buffer)\n        finally:\n            await file.close()\n    else:\n        raise NotImplementedError('`publish_model` not implemented for multiple files upload.')\n        # zip the files\n\n    model = MLModel(**model.dict(), weight=saved_path)\n    models = register_model(model=model, convert=convert, profile=profile)\n    return {\n        'data': {'id': [str(model.id) for model in models], },\n        'status': True,\n        'model_path': saved_path\n    }"}
{"type": "source_file", "path": "modelci/app/v1/endpoints/profiler.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Xing Di\nDate: 2021/1/15\n\n\"\"\"\nfrom fastapi import APIRouter, HTTPException\nfrom modelci.persistence.service_ import exists_by_id, profile_model\n\nrouter = APIRouter()\n\n\n@router.get('/{model_id}', status_code=201)\ndef profile(model_id: str, device: str='cuda', batch_size: int=1):\n    if not exists_by_id(model_id):\n        raise HTTPException(\n            status_code=404,\n            detail=f'Model ID {model_id} does not exist. You may change the ID',\n        )\n    profile_result = profile_model(model_id, device, batch_size)\n    return profile_result\n"}
{"type": "source_file", "path": "modelci/cli/__init__.py", "content": "#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport click\nimport typer\n\nfrom modelci.cli import modelhub\nfrom modelci.cli._fixup import _get_click_type_wrapper, _generate_enum_convertor_wrapper\nfrom modelci.cli.service import service\n\n# Fixup for typer argument and options annotations\ntyper.main.get_click_type = _get_click_type_wrapper(typer.main.get_click_type)\ntyper.main.generate_enum_convertor = _generate_enum_convertor_wrapper(typer.main.generate_enum_convertor)\n\napp = typer.Typer()\n\n\n@app.callback()\ndef callback():\n    \"\"\"\n    A complete MLOps platform for managing, converting and profiling models and\n    then deploying models as cloud services (MLaaS)\n    \"\"\"\n\n\napp.add_typer(modelhub.app, name='modelhub')\ntyper_click_object: click.Group = typer.main.get_command(app)  # noqa\ntyper_click_object.add_command(service)\n\nif __name__ == '__main__':\n    typer_click_object()\n"}
{"type": "source_file", "path": "modelci/cli/archive/model_cli.py", "content": "#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\n\n# TODO(ZHZ): remove the file after moving all functions to model_manager.py\nimport click\nimport requests\n\nfrom modelci.config import app_settings\nfrom modelci.types.models import MLModel\nfrom modelci.ui import model_view\nfrom modelci.utils.misc import remove_dict_null\n\n\n@click.command()\n@click.argument('name', type=click.STRING, required=False)\n@click.option(\n    '-f', '--framework',\n    type=click.Choice(['TensorFlow', 'PyTorch'], case_sensitive=False),\n    help='Model framework.'\n)\n@click.option(\n    '-e', '--engine',\n    type=click.Choice(['NONE', 'TFS', 'TORCHSCRIPT', 'ONNX', 'TRT', 'TVM', 'CUSTOMIZED'], case_sensitive=False),\n    help='Model serving engine.'\n)\n@click.option('-v', '--version', type=click.INT, help='Model version.')\n@click.option('-a', '--all', 'list_all', type=click.BOOL, is_flag=True, help='Show all models.')\n@click.option('-q', '--quiet', type=click.BOOL, is_flag=True, help='Only show numeric IDs.')\ndef models(name, framework, engine, version, list_all, quiet):\n    payload = remove_dict_null({'name': name, 'framework': framework, 'engine': engine, 'version': version})\n    with requests.get(f'{app_settings.api_v1_prefix}/model', params=payload) as r:\n        model_list = r.json()\n        model_view([MLModel.parse_obj(model) for model in model_list], list_all=list_all, quiet=quiet)\n\n\n\n\n\n"}
{"type": "source_file", "path": "modelci/cli/modelps.py", "content": "#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License."}
{"type": "source_file", "path": "modelci/cli/service.py", "content": "#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n\nimport click\n\nfrom modelci.app import (start as app_start, stop as app_stop)\nfrom modelci.utils import Logger\nfrom modelci.utils.docker_container_manager import DockerContainerManager\n\nlogger = Logger(__name__, welcome=False)\n\n\n@click.group()\ndef service():\n    pass\n\n\n@service.command(\"init\")\n@click.option('--gpu', default=False, type=click.BOOL, is_flag=True)\ndef start_leader_server(gpu=False):\n    \"\"\"start the system on a leader server in a cluster.\n    initialize necessary services such as database and monitor\n\n    Args:\n        gpu (bool, optional): [description]. Defaults to False.\n    \"\"\"\n    # todo: lazy docker start\n    container_conn = DockerContainerManager(enable_gpu=gpu)\n    if not container_conn.start():\n        container_conn.connect()\n    # FIXME: app not started because pytorch imported when this function is called.\n    #     PyTorch and subprocess have conflicts.\n    app_start()\n\n\n@service.command(\"stop\")\ndef stop_leader_server():\n    \"\"\"shutdown all jobs and save a screenshot, then stop the leader server\n    it will broadcast a message to all follower workers\n    \"\"\"\n    container_conn = DockerContainerManager()\n    container_conn.stop()\n    app_stop()\n\n\n@service.command(\"clean\")\ndef remove_services():\n    \"\"\"stop all services and remove downloaded docker images\n    \"\"\"\n    container_conn = DockerContainerManager()\n    container_conn.stop()\n    app_stop()\n    container_conn.remove_all()\n\n\n@service.command(\"connect\")\n@click.argument(\"ip_address\")\ndef connect_leader_server(ip_address=\"localhost\"):\n    \"\"\"connect to the leader server to team use\n\n    Args:\n        ip_address (str, optional): [description]. Defaults to \"localhost\".\n\n    Raises:\n        NotImplementedError: [description]\n    \"\"\"\n\n    raise NotImplementedError\n\n\n@service.command(\"disconnect\")\ndef disconnect_leader_server():\n    \"\"\"disconnect the follower worker from the leader server\n\n    Raises:\n        NotImplementedError: [description]\n    \"\"\"\n    raise NotImplementedError\n"}
{"type": "source_file", "path": "modelci/controller/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/29/2020\n\"\"\"\nimport atexit\n\nfrom modelci.controller.executor import JobExecutor\n\njob_executor = JobExecutor()\njob_executor.start()\n\n\n@atexit.register\ndef terminate_controllers():\n    job_executor.join()\n    print('Exiting job executor.')\n\n\n__all__ = ['job_executor']\n"}
{"type": "source_file", "path": "modelci/cli/_fixup.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 3/3/2021\n\nFixup for typer to support more data type as annotations.\n\"\"\"\nimport ast\nimport inspect\nfrom typing import Any\n\nimport click\nimport typer\nfrom pydantic import ValidationError, BaseModel\nfrom pydantic.utils import lenient_issubclass\n\nfrom modelci.types.models.common import NamedEnum\nfrom modelci.utils.misc import isgeneric\n\n\nclass PydanticModelParamType(click.ParamType):\n    \"\"\"Customized type `pydantic.BaseModel` for typer parameter annotation.\n\n    The input format of this type is the JSON string, whose format can be parsed by\n    :meth:`BaseModel.parse_dict`.\n\n    Examples:\n        This fixup enables the parameter type annotation like:\n        >>> from pydantic import BaseModel\n        ...\n        ... class ParamType(BaseModel):\n        ...     some_param: int\n        ...     other_param: str\n        ...\n        ...\n        ... def main(param: ParamType = typer.Argument(...))\n        ...     typer.echo(param.dict())\n        ...\n        ...\n        ... if __name__ == '__main__':\n        ...     typer.run(main)\n\n        Then you can run the program:\n        ```\n        $ python main.py `'\"some_param\": 1, \"other_param\": \"string\"'`\n        ```\n        The expected result is\n        ```\n        {\"some_param\": 1, \"other_param\": \"string\"}\n        ```\n    \"\"\"\n\n    name = 'pydantic model Json'\n\n    def __init__(self, annotation):\n        self._annotation = annotation\n\n    def convert(self, value, param, ctx):\n        try:\n            return self._annotation.parse_raw(value)\n        except ValidationError as exc:\n            typer.echo(exc, err=True, color=True)\n            raise typer.Exit(code=1)\n\n\nclass DictParamType(click.ParamType):\n    \"\"\"Customized type `Dict[Any, Any]` for typer parameter annotation.\n\n    The input format of this type is the JSON string that can be converted to a Python dictionary.\n\n    Examples:\n        This fixup enables the parameter type annotation:\n        >>> from typing import Dict\n        ...\n        ... def main(str_int_dict: Dict[str, int] = typer.Argument(...))\n        ...     typer.echo(str_int_dict)\n        ...\n        ...\n        ... if __name__ == '__main__':\n        ...     typer.run(main)\n        You can run the program:\n        ```shell script\n        $ python main.py '{\"item1\": 1, \"item2\": 2}'\n        ```\n        The expected result is\n        ```\n        {\"item1\": 1, \"item2\": 2}\n        ```\n    \"\"\"\n    name = 'dict'\n\n    def __init__(self, annotation):\n        self._annotation = annotation\n\n    @staticmethod\n    def _get_parser(type_):\n        if inspect.isclass(type_) and issubclass(type_, BaseModel):\n            return type_.parse_obj\n        else:\n            return type_\n\n    def convert(self, value, param, ctx):\n        key_type, value_type = self._annotation.__args__\n\n        try:\n            return {\n                self._get_parser(key_type)(k): self._get_parser(value_type)(v)\n                for k, v in ast.literal_eval(str(value)).items()\n            }\n        except ValidationError as exc:\n            typer.echo(exc, err=True, color=True)\n            raise typer.Exit(422)\n\n\ndef _get_click_type_wrapper(get_click_type):\n    \"\"\"Wrapper for fixup `typer.get_click_type` function.\n\n    It fixes up typer support for more data type as argument and option annotations.\n\n    Args:\n        get_click_type: The function :meth:`typer.get_click_type`.\n    \"\"\"\n\n    def supersede_get_click_type(\n            *, annotation: Any, parameter_info: typer.main.ParameterInfo\n    ) -> click.ParamType:\n        \"\"\"Fixup for typer to support more customized class for type hint. Such hints can be argument and option\n        annotations.\n\n        Originated from https://github.com/tiangolo/typer/issues/111#issuecomment-635512182.\n        \"\"\"\n\n        if isgeneric(annotation) and annotation.__origin__ is dict:\n            return DictParamType(annotation)\n        elif lenient_issubclass(annotation, BaseModel):\n            return PydanticModelParamType(annotation)\n        elif lenient_issubclass(annotation, NamedEnum):\n            return click.Choice(\n                [item.name for item in annotation],\n                case_sensitive=parameter_info.case_sensitive,\n            )\n        else:\n            return get_click_type(annotation=annotation, parameter_info=parameter_info)\n\n    return supersede_get_click_type\n\n\ndef _generate_enum_convertor_wrapper(enum_converter_factory):\n    \"\"\"Wrapper for fixup `typer.generate_enum_convertor` function.\n\n    The original version of `generate_enum_convertor` will break the `IntEnum`, which utilized by ModelCI.\n    By wrapping such function over the origin function, the typer can support `IntEnum` as argument and option\n    annotations.\n\n    Args:\n        enum_converter_factory: The function :meth:`typer.generate_enum_convertor`.\n    \"\"\"\n\n    def generate_named_enum_converter(enum):\n        def convertor(value: Any) -> Any:\n            if value is not None:\n                low = str(value).lower()\n                if low in lower_val_map:\n                    key = lower_val_map[low]\n                    return enum(key)\n\n        if enum is not None:\n            if issubclass(enum, NamedEnum):\n                lower_val_map = {str(val.name).lower(): val for val in enum}\n                return convertor\n            else:\n                return enum_converter_factory\n\n    return generate_named_enum_converter\n"}
{"type": "source_file", "path": "modelci/cli/modelhub.py", "content": "#  Copyright (c) NTU_CAP 2021. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at:\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n#  or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom http import HTTPStatus\nfrom pathlib import Path\nfrom shutil import copy2, make_archive\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport typer\nimport yaml\nfrom pydantic import ValidationError\nimport modelci.persistence.service_ as ModelDB\nfrom modelci.hub.converter import generate_model_family\n\nfrom modelci.config import app_settings\nfrom modelci.hub.utils import parse_path_plain\nfrom modelci.types.models.common import Framework, Engine, Task, Metric, IOShape, ModelStatus\nfrom modelci.types.models.mlmodel import MLModelFromYaml, MLModel, ModelUpdateSchema\nfrom modelci.ui import model_view, model_detailed_view\nfrom modelci.utils import Logger\nfrom modelci.utils.misc import remove_dict_null\n\nlogger = Logger(__name__, welcome=False)\n\napp = typer.Typer()\n\n\n@app.callback()\ndef modelhub():\n    pass\n\n\n@app.command()\ndef publish(\n        file_or_dir: Optional[Path] = typer.Argument(None, help='Model weight files', exists=True),\n        architecture: Optional[str] = typer.Option(None, '-n', '--name', help='Architecture'),\n        framework: Optional[Framework] = typer.Option(None, '-fw', '--framework', help='Framework'),\n        engine: Optional[Engine] = typer.Option(None, '-e', '--engine', help='Engine'),\n        version: Optional[int] = typer.Option(None, '-v', '--version', min=1, help='Version number'),\n        task: Optional[Task] = typer.Option(None, '-t', '--task', help='Task'),\n        dataset: Optional[str] = typer.Option(None, '-d', '--dataset', help='Dataset name'),\n        metric: Dict[Metric, float] = typer.Option(\n            '{}',\n            help='Metrics in the form of mapping JSON string. The map type is '\n                 '`Dict[types.models.mlmodel.Metric, float]`. An example is \\'{\"acc\": 0.76}.\\'',\n        ),\n        inputs: List[IOShape] = typer.Option(\n            [],\n            '-i', '--input',\n            help='List of shape definitions for input tensors. An example of one shape definition is '\n                 '\\'{\"name\": \"input\", \"shape\": [-1, 3, 224, 224], \"dtype\": \"TYPE_FP32\", \"format\": \"FORMAT_NCHW\"}\\'',\n        ),\n        outputs: List[IOShape] = typer.Option(\n            [],\n            '-o', '--output',\n            help='List of shape definitions for output tensors. An example of one shape definition is '\n                 '\\'{\"name\": \"output\", \"shape\": [-1, 1000], \"dtype\": \"TYPE_FP32\"}\\'',\n        ),\n        convert: Optional[bool] = typer.Option(\n            True,\n            '-c', '--convert',\n            is_flag=True,\n            help='Convert the model to other possible format.',\n        ),\n        profile: Optional[bool] = typer.Option(\n            False,\n            '-p', '--profile',\n            is_flag=True,\n            help='Profile the published model(s).',\n        ),\n        yaml_file: Optional[Path] = typer.Option(\n            None, '-f', '--yaml-file', exists=True, file_okay=True,\n            help='Path to configuration YAML file. You should either set the `yaml_file` field or fields '\n                 '(`FILE_OR_DIR`, `--name`, `--framework`, `--engine`, `--version`, `--task`, `--dataset`,'\n                 '`--metric`, `--input`, `--output`).'\n        ),\n):\n    meta_info = (file_or_dir, architecture, framework, engine, version, task, dataset, metric, inputs, outputs)\n    # check either using parameters, or using YAML\n    if yaml_file and not any(meta_info):\n        with open(yaml_file) as f:\n            model_config = yaml.safe_load(f)\n        try:\n            model_yaml = MLModelFromYaml.parse_obj(model_config)\n        except ValidationError as exc:\n            typer.echo(exc, err=True, color=True)\n            raise typer.Exit(422)\n    elif not yaml_file and all(meta_info):\n        model_yaml = MLModelFromYaml(\n            weight=file_or_dir, architecture=architecture, framework=framework, engine=engine, version=version,  # noqa\n            dataset=dataset, metric=metric, task=task, inputs=inputs, outputs=outputs, convert=convert, profile=profile\n        )\n    else:\n        typer.echo('Incorrect parameter, you should set either YAML_FILE, or all of the (FILE_OR_DIR, --name,'\n                   '--framework, --engine, --version, --task, --dataset, --metric, --input, --output)')\n        raise typer.Exit(422)\n\n    # build request parameters\n    payload = {'convert': model_yaml.convert, 'profile': model_yaml.profile}\n    data = model_yaml.dict(use_enum_values=True, exclude_none=True, exclude={'convert', 'profile', 'weight'})\n    form_data = {k: str(v) for k, v in data.items()}\n    file_or_dir = model_yaml.weight\n\n    # read weight files\n    files = list()\n    key = 'files'\n    try:\n        # read weight file\n        if file_or_dir.is_dir():\n            for file in filter(Path.is_file, file_or_dir.rglob('*')):\n                name = Path(file).relative_to(file_or_dir.parent)\n                files.append((key, (str(name), open(file, 'rb'), 'application/example')))\n        else:\n            files.append((key, (file_or_dir.name, open(file_or_dir, 'rb'), 'application/example')))\n        with requests.post(\n                f'{app_settings.api_v1_prefix}/model/',\n                params=payload, data=form_data, files=files,\n        ) as r:\n            typer.echo(r.json(), color=True)\n    finally:\n        for file in files:\n            file[1][1].close()\n\n\n@app.command('ls')\ndef list_models(\n        architecture: Optional[str] = typer.Option(None, '-n', '--name', help='Model architecture name'),\n        framework: Optional[Framework] = typer.Option(None, '-fw', '--framework', case_sensitive=False,\n                                                      help='Framework'),\n        engine: Optional[Engine] = typer.Option(None, '-e', '--engine', case_sensitive=False, help='Serving engine'),\n        version: Optional[int] = typer.Option(None, '-v', '--version', help='Version'),\n        list_all: Optional[bool] = typer.Option(\n            False,\n            '-a', '--all', is_flag=True,\n            help='Display queried models. otherwise, only partial result will be shown.'\n        ),\n):\n    \"\"\"Show a table that lists all models published in MLModelCI\"\"\"\n\n    payload = remove_dict_null(\n        {'architecture': architecture, 'framework': framework, 'engine': engine, 'version': version}\n    )\n    with requests.get(f'{app_settings.api_v1_prefix}/model', params=payload) as r:\n        model_list = r.json()\n        model_view([MLModel.parse_obj(model) for model in model_list], list_all=list_all)\n\n\n@app.command()\ndef download():\n    \"\"\"Download model from model hub. (Not implemented).\"\"\"\n    raise NotImplementedError\n\n\n@app.command('get')\ndef download_model_from_url(\n        url: str = typer.Argument(..., help='The link to a model'),\n        path: Path = typer.Argument(..., file_okay=True, help='The saved path and file name.')\n):\n    \"\"\"Download a model weight file from an online URL.\"\"\"\n\n    from modelci.hub.registrar import download_model_from_url\n\n    download_model_from_url(url, path)\n    typer.echo(f'{path} model downloaded successfully.')\n\n\n@app.command('export')\ndef export(\n        name: str = typer.Option(..., '-n', '--name', help='Architecture'),\n        framework: Optional[Framework] = typer.Option(None, '-fw', '--framework', case_sensitive=False,\n                                                      help='Framework'),\n        trt: Optional[bool] = typer.Option(\n            False,\n            is_flag=True,\n            help='Flag for exporting models served by TensorRT. Please make sure you have TensorRT installed in your '\n                 'machine before set this flag.')\n):\n    \"\"\"\n    Export model from PyTorch hub / TensorFlow hub and try convert the model into various format for different serving\n    engines.\n    \"\"\"\n    from modelci.hub.init_data import export_model\n\n    export_model(model_name=name, framework=framework, enable_trt=trt)\n\n\n@app.command('detail')\ndef detail(model_id: str = typer.Argument(..., help='Model ID')):\n    \"\"\"Show a single model.\"\"\"\n    with requests.get(f'{app_settings.api_v1_prefix}/model/{model_id}') as r:\n        data = r.json()\n        model_detailed_view(MLModel.parse_obj(data))\n\n\n@app.command('update')\ndef update(\n        model_id: str = typer.Argument(..., help='Model ID'),\n        architecture: Optional[str] = typer.Option(None, '-n', '--name', help='Architecture'),\n        framework: Optional[Framework] = typer.Option(None, '-fw', '--framework', help='Framework'),\n        engine: Optional[Engine] = typer.Option(None, '-e', '--engine', help='Engine'),\n        version: Optional[int] = typer.Option(None, '-v', '--version', min=1, help='Version number'),\n        task: Optional[Task] = typer.Option(None, '-t', '--task', help='Task'),\n        dataset: Optional[str] = typer.Option(None, '-d', '--dataset', help='Dataset name'),\n        metric: Optional[Dict[Metric, float]] = typer.Option(\n            None,\n            help='Metrics in the form of mapping JSON string. The map type is '\n                 '`Dict[types.models.mlmodel.Metric, float]`. An example is \\'{\"acc\": 0.76}.\\'',\n        ),\n        inputs: Optional[List[IOShape]] = typer.Option(\n            [],\n            '-i', '--input',\n            help='List of shape definitions for input tensors. An example of one shape definition is '\n                 '\\'{\"name\": \"input\", \"shape\": [-1, 3, 224, 224], \"dtype\": \"TYPE_FP32\", \"format\": \"FORMAT_NCHW\"}\\'',\n        ),\n        outputs: Optional[List[IOShape]] = typer.Option(\n            [],\n            '-o', '--output',\n            help='List of shape definitions for output tensors. An example of one shape definition is '\n                 '\\'{\"name\": \"output\", \"shape\": [-1, 1000], \"dtype\": \"TYPE_FP32\"}\\'',\n        )\n):\n    model = ModelUpdateSchema(\n        architecture=architecture, framework=framework, engine=engine, version=version,  # noqa\n        dataset=dataset, metric=metric, task=task, inputs=inputs, outputs=outputs\n    )\n\n    with requests.patch(f'{app_settings.api_v1_prefix}/model/{model_id}',\n                        data=model.json(exclude_defaults=True)) as r:\n        data = r.json()\n        model_detailed_view(MLModel.parse_obj(data))\n\n\n@app.command('delete')\ndef delete(model_id: str = typer.Argument(..., help='Model ID')):\n    with requests.delete(f'{app_settings.api_v1_prefix}/model/{model_id}') as r:\n        if r.status_code == HTTPStatus.NO_CONTENT:\n            typer.echo(f\"Model {model_id} deleted\")\n\n\n@app.command('convert')\ndef convert(\n        id: str = typer.Option(None, '-i', '--id', help='ID of model.'),\n        yaml_file: Optional[Path] = typer.Option(\n            None, '-f', '--yaml-file', exists=True, file_okay=True,\n            help='Path to configuration YAML file. You should either set the `yaml_file` field or fields '\n                 '(`FILE_OR_DIR`, `--name`, `--framework`, `--engine`, `--version`, `--task`, `--dataset`,'\n                 '`--metric`, `--input`, `--output`).'\n        ),\n        register: bool = typer.Option(False, '-r', '--register', is_flag=True, help='register the converted models to modelhub, default false')\n):\n    model = None\n    if id is None and yaml_file is None:\n        raise ValueError('WARNING: Please assign a way to find the target model! details refer to --help')\n    if id is not None and yaml_file is not None:\n        raise ValueError('WARNING: Do not use -id and -path at the same time!')\n    elif id is not None and yaml_file is None:\n        if ModelDB.exists_by_id(id):\n            model = ModelDB.get_by_id(id)\n        else:\n            typer.echo(f\"model id: {id} does not exist in modelhub\")\n    elif id is None and yaml_file is not None:\n        # get MLModel from yaml file\n        with open(yaml_file) as f:\n            model_config = yaml.safe_load(f)\n        model_yaml = MLModelFromYaml.parse_obj(model_config)\n        model_in_saved_path = model_yaml.saved_path\n        if model_in_saved_path != model_yaml.weight:\n            copy2(model_yaml.weight, model_in_saved_path)\n        if model_yaml.engine == Engine.TFS:\n            weight_dir = model_yaml.weight\n            make_archive(weight_dir.with_suffix('.zip'), 'zip', weight_dir)\n\n        model_data = model_yaml.dict(exclude_none=True, exclude={'convert', 'profile'})\n        model = MLModel.parse_obj(model_data)\n\n    # auto execute all possible convert and return a list of save paths of every converted model\n    generated_dir_list = generate_model_family(model)\n    typer.echo(f\"Converted models are save in: {generated_dir_list}\")\n    if register:\n        model_data = model.dict(exclude={'weight', 'id', 'model_status', 'engine'})\n        for model_dir in generated_dir_list:\n            parse_result = parse_path_plain(model_dir)\n            engine = parse_result['engine']\n            model_cvt = MLModel(**model_data, weight=model_dir, engine=engine, model_status=[ModelStatus.CONVERTED])\n            ModelDB.save(model_cvt)\n            typer.echo(f\"converted {engine} are successfully registered in Modelhub\")\n\n\n@app.command('profile')\ndef profile(\n        model_id: str = typer.Argument(..., help='Model ID'),\n        device: str = typer.Option(\"cuda\", '-d', '--device', help='device to pre-deploy the model.'),\n        batch_size: int = typer.Option(None, '-b', '--batchsize', help='batchsize of the test input')\n):\n    args = {'id': model_id, 'device': device, 'batch_size': batch_size}\n    with requests.get(f'{app_settings.api_v1_prefix}/profiler/{model_id}', params=args) as r:\n        if r.status_code == 201:\n            typer.echo(\"Profile successfully! Results are showed below:\")\n            json_response = json.dumps(r.json(), sort_keys=True, indent=4, separators=(',', ':'))\n            typer.echo(json_response)\n        else:\n            raise ConnectionError(\"Can not connect to profile api!\")\n"}
{"type": "source_file", "path": "modelci/config.py", "content": "import os\nfrom pathlib import Path\n\nfrom pydantic import BaseSettings, SecretStr, SecretBytes\n\n# API\nAPI_V1_STR = '/api/v1'\nAPI_EXP_STR = '/api/exp'\n\n\nclass DBSettings(BaseSettings):\n    mongo_host: str = 'localhost'\n    mongo_port: int = 27017\n    mongo_username: str = 'modelci'\n    mongo_password: SecretStr = SecretStr('modelci@2020')\n    mongo_db: str = 'modelci'\n    mongo_auth_source: str = 'modelci'\n    auth_mechanism: str = 'SCRAM-SHA-256'\n\n    class Config:\n        env_file = Path(__file__).absolute().parent / '.env'\n\n\nclass ServiceSettings(BaseSettings):\n    mongo_host: str = 'localhost'\n    mongo_port: int = 27017\n\n    # cAdvisor configuration\n    cadvisor_port: int = 8080\n\n    # Node exporter configuration\n    node_exporter_port: int = 9400\n\n    class Config:\n        env_file = Path(__file__).absolute().parent / '.env'\n\n\nclass AppSettings(BaseSettings):\n    project_name: str = 'ModelCI'\n    backend_cors_origins: str = '*'\n    server_host: str = 'localhost'\n    server_port: int = 8000\n    secret_key: SecretBytes = SecretBytes(os.urandom(32))\n    access_token_expire_minutes: int = 60 * 24 * 8  # 60 minutes * 24 hours * 8 days = 8 days\n\n    class Config:\n        env_file = Path(__file__).absolute().parent / '.env'\n\n    @property\n    def server_url(self):\n        return f'http://{self.server_host}:{self.server_port}'\n\n    @property\n    def api_v1_prefix(self):\n        return f'http://{self.server_host}:{self.server_port}{API_V1_STR}'\n\n\nservice_settings = ServiceSettings()\ndb_settings = DBSettings()\napp_settings = AppSettings()\n"}
{"type": "source_file", "path": "modelci/app/experimental/endpoints/trainer.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/15/2021\n\"\"\"\nfrom typing import List\n\nfrom fastapi import APIRouter\nfrom starlette.responses import JSONResponse\n\nfrom modelci.experimental.curd.model_train import save, get_by_id, get_all, delete_by_id, delete_all\nfrom modelci.experimental.finetuner.trainer import PyTorchTrainer\nfrom modelci.experimental.model.model_train import TrainingJob, TrainingJobIn\n\nrouter = APIRouter()\n\n\n@router.post('/', status_code=201)\ndef create_training_job(training_job: TrainingJobIn):\n    \"\"\"\n    Create a training job data object, and save it into the database. Then, submit the created training job\n    (with job ID generated by database) to the training job coordinator.\n    TODO return training job as soon as created\n\n    Args:\n        training_job (TrainingJobIn): Training job to be submitted.\n\n    Returns:\n        Submitted training job data class object.\n    \"\"\"\n    id_ = save(training_job_in=training_job)\n    if id_ is not None:\n        training_job = get_by_id(id_)\n        trainer = PyTorchTrainer.from_training_job(training_job)\n        trainer.start()\n        trainer.join()\n    return {'id': str(id_)}\n\n\n@router.get('/')\ndef get_all_training_jobs() -> List[TrainingJob]:\n    return get_all()\n\n\n@router.get('/{id}')\ndef get_training_job(id: str) -> TrainingJob:\n    \"\"\"\n    Get a training job.\n\n    Args:\n        id (str): Training job ID.\n\n    Returns:\n        int: Affected number of records.\n    \"\"\"\n    return get_by_id(id)\n\n\n@router.delete('/{id}')\ndef delete_training_job(id: str):\n    \"\"\"\n    Delete a training job.\n\n    Args:\n        id (str): Training job ID.\n\n    Returns:\n        int: Affected number of records.\n    \"\"\"\n    if bool(delete_by_id(id)):\n        return JSONResponse(status_code=204)\n    else:\n        return JSONResponse(status_code=400, content={'message': 'Failed in deletion.'})\n\n\n@router.delete('/')\ndef delete_all_training_job():\n    count = delete_all()\n    return JSONResponse(status_code=204, content={'deleted': count})\n"}
{"type": "source_file", "path": "modelci/app/v1/endpoints/visualizer.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Jiang Shanshan\nEmail: univeroner@gmail.com\nDate: 2021/1/15\n\n\"\"\"\n\nfrom fastapi import APIRouter\nfrom modelci.persistence.service import ModelService\nfrom modelci.types.bo.model_objects import Engine\nfrom torchviz import make_dot\nimport torch\n\nrouter = APIRouter()\n\n\n@router.get('/{id}')\ndef generate_model_graph(*, id: str):  # noqa\n    model_bo = ModelService.get_model_by_id(id)\n    dot_graph = ''\n    if model_bo.engine == Engine.PYTORCH:\n        pytorch_model = torch.load(model_bo.saved_path)\n        sample_data = torch.zeros(1, *model_bo.inputs[0].shape[1:], dtype=torch.float, requires_grad=False)\n        out = pytorch_model(sample_data)\n        dot_graph = make_dot(out, params=dict(list(pytorch_model.named_parameters()) + [('x', sample_data)]))\n\n    return {'dot': str(dot_graph)}\n"}
{"type": "source_file", "path": "modelci/experimental/finetuner/coordinator.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2/2/2021\n\"\"\"\nfrom typing import Dict\n\nfrom modelci.experimental.finetuner.trainer import BaseTrainer\n\n\nclass Coordinator(object):\n\n    def __init__(self):\n        self.pool: Dict[str, BaseTrainer] = dict()\n\n    def get_job_by_id(self, job_id: str):\n        return self.pool.get(job_id, None)\n\n    def submit_job(self, trainer: BaseTrainer):\n        self.pool[trainer.id] = trainer\n        trainer.start()\n\n    def delete_job_by_id(self, job_id: str):\n        trainer = self.pool.pop(job_id, None)\n        if trainer is not None:\n            trainer.terminate()\n\n    def delete_all(self):\n        for trainer in self.pool.values():\n            trainer.terminate()\n"}
{"type": "source_file", "path": "modelci/experimental/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/31/2021\n\nModelCI's experimental APIs.\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/data_engine/postprocessor/__init__.py", "content": "from .image_classification import postprocess as image_classification_postprocessor\n\n__all__ = ['image_classification_postprocessor']\n"}
{"type": "source_file", "path": "modelci/data_engine/preprocessor/__init__.py", "content": "from .image_classification import preprocess as image_classification_preprocessor\n\n__all__ = ['image_classification_preprocessor']\n"}
{"type": "source_file", "path": "modelci/data_engine/postprocessor/image_classification.py", "content": "def postprocess(results, filenames, batch_size):\n    \"\"\"\n    Post-process results to show classifications.\n    \"\"\"\n    if len(results) != 1:\n        raise Exception(\"expected 1 result, got {}\".format(len(results)))\n\n    batched_result = list(results.values())[0]\n    if len(batched_result) != batch_size:\n        raise Exception(\"expected {} results, got {}\".format(batch_size, len(batched_result)))\n    if len(filenames) != batch_size:\n        raise Exception(\"expected {} filenames, got {}\".format(batch_size, len(filenames)))\n\n    for (index, result) in enumerate(batched_result):\n        print(\"Image '{}':\".format(filenames[index]))\n        for cls in result:\n            print(\"    {} ({}) = {}\".format(cls[0], cls[2], cls[1]))\n"}
{"type": "source_file", "path": "modelci/controller/controller.py", "content": "import collections\nimport time\nfrom threading import Thread\n\nimport GPUtil\n\nfrom modelci.hub import Profiler\nfrom modelci.monitor import GPUNodeExporter\n\n\nclass UtilMonitor(Thread):\n    \"\"\"\n    Monitor for GPU Utilization\n    \"\"\"\n\n    def __init__(self, device, profiler: Profiler, delay, util_level, memory_level):\n        \"\"\"\n        Init the GPU Utilization Monitor Thread.\n\n        :param delay: time period to get the information.\n        :param util_level: The utilization level that trigger profiling.\n        :param memory_level: The memory usage level that trigger profiling.\n        :param profiler: The instance of model profiler.\n        :param device: GPU device to test.\n        \"\"\"\n        super(UtilMonitor, self).__init__()\n        self.stopped = False\n        self.delay = delay\n        self.memory_level = memory_level\n        self.util_level = util_level\n        self.profiler = profiler\n        self.exporter = GPUNodeExporter()\n        self.device = device\n\n        if self.exporter is None:\n            raise TypeError(\n                'Failed to get GPU relative information from node exporter, please make sure the service has started.')\n\n    def run(self):\n        while not self.stopped:\n            available_devices = self.exporter.get_idle_gpu(util_level=self.util_level,\n                                                           memory_level=self.memory_level)\n\n            if self.device.id in available_devices:\n                self.profiler.auto_diagnose(available_devices=[self.device])\n\n            time.sleep(self.delay)\n\n    def stop(self):\n        self.stopped = True\n\n\ndef auto_model_profiling(model_info, server_name, device_util_thd=0.01, device_memory_thd=0.01, period=10):\n    \"\"\"\n    Start model profiling automatically.\n\n    :param model_info: model information object saved by ModelCI.\n    :param server_name: serving docker container's name.\n    :param device_util_thd: The utilization level that trigger profiling.\n    :param device_memory_thd: The memory usage level that trigger profiling.\n    :param period: time period to get the information.\n    :return: None\n    \"\"\"\n\n    different_kind_devices = collections.OrderedDict()\n    for gpu in GPUtil.getGPUs():\n        if gpu.name not in different_kind_devices:\n            different_kind_devices[gpu.name] = gpu\n\n    for device in list(different_kind_devices.values()):\n        profiler = Profiler(model_info=model_info, server_name=server_name)\n        monitor = UtilMonitor(device, profiler, period, device_util_thd, device_memory_thd)\n        monitor.start()\n\n\ndef auto_device_placement():\n    raise NotImplementedError('Method `auto_device_placement` is not implemented.')\n"}
{"type": "source_file", "path": "modelci/experimental/finetuner/trainer.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/12/2021\n\"\"\"\nimport abc\nimport threading\nfrom concurrent.futures import Future\nfrom concurrent.futures.thread import ThreadPoolExecutor\nfrom typing import Optional\n\nimport pytorch_lightning as pl\nimport torch\n\nimport modelci.experimental.curd.model_train as model_train_curd\nfrom modelci.experimental.finetuner import OUTPUT_DIR\nfrom modelci.experimental.finetuner.pytorch_datamodule import PyTorchDataModule\nfrom modelci.experimental.finetuner.transfer_learning import freeze, FineTuneModule\nfrom modelci.experimental.model.model_train import TrainingJob, TrainingJobUpdate\nfrom modelci.hub.manager import get_remote_model_weight\nfrom modelci.persistence.service import ModelService\nfrom modelci.types.bo import Engine, ModelStatus\nfrom modelci.types.vo import Status\n\n\nclass BaseTrainer(abc.ABC):\n    \"\"\"Trainer interface.\"\"\"\n\n    def __init__(self, id):\n        self.id = id\n\n    @classmethod\n    @abc.abstractmethod\n    def from_training_job(cls, training_job: TrainingJob):\n        raise NotImplementedError('Method `from_training_job` not implemented.')\n\n    @abc.abstractmethod\n    def start(self):\n        raise NotImplementedError('Method `start` not implemented.')\n\n    @abc.abstractmethod\n    def join(self, timeout=None):\n        \"\"\"Wait for the trainer to finish training.\"\"\"\n\n        raise NotImplementedError('Method `join` not implemented.')\n\n    @abc.abstractmethod\n    def terminate(self):\n        raise NotImplementedError('Method `terminate` not implemented.')\n\n    @abc.abstractmethod\n    def resume_soon(self):\n        \"\"\"Resume from a pause.\"\"\"\n\n        raise NotImplementedError('Method `resume_soon` not implemented.')\n\n    @abc.abstractmethod\n    def pause_soon(self):\n        \"\"\"Pause training for a while\"\"\"\n        raise NotImplementedError('Method `pause_soon` not implemented.')\n\n    @abc.abstractmethod\n    def export_model(self):\n        raise NotImplementedError('Method `export_model` not implemented.')\n\n    @abc.abstractmethod\n    def set_device(self):\n        raise NotImplementedError('Method `set_device` not implemented.')\n\n\nclass PyTorchTrainer(BaseTrainer):\n    \"\"\"\n    PyTorch Trainer utilize :class:`pytorch_lighting.Trainer` as the engine.\n    \"\"\"\n\n    def __init__(\n            self,\n            model: pl.LightningModule,\n            model_id: str,\n            id: str = None,\n            data_loader_kwargs: dict = None,\n            trainer_kwargs: dict = None,\n    ):\n        super().__init__(id)\n        self.model = model\n        trainer_kwargs = trainer_kwargs or dict()\n        self.model_id = model_id\n        self.trainer_engine = pl.Trainer(**trainer_kwargs)\n        self._data_loader_kwargs = data_loader_kwargs or dict()\n\n        self._executor = ThreadPoolExecutor(max_workers=1)\n        self._event_pause = threading.Event()\n        self._task: Optional[Future] = None\n\n    @classmethod\n    def from_training_job(cls, training_job: TrainingJob) -> 'PyTorchTrainer':\n        # TODO: only support fine-tune\n\n        model_bo = ModelService.get_model_by_id(training_job.model)\n        if model_bo.engine != Engine.NONE:\n            raise ValueError(f'Model engine expected `{Engine.NONE}`, but got {model_bo.engine}.')\n\n        # download local cache\n        cache_path = get_remote_model_weight(model_bo)\n        net = torch.load(cache_path)\n        freeze(module=net, n=-1, train_bn=True)\n\n        # build pytorch lightning module\n        fine_tune_module_kwargs = {\n            'net': net,\n            'loss': eval(str(training_job.loss_function))(),  # nosec\n            'batch_size': training_job.data_module.batch_size,\n            'num_workers': training_job.data_module.num_workers,\n        }\n        if training_job.optimizer_property.lr:\n            fine_tune_module_kwargs['lr'] = training_job.optimizer_property.lr\n        if training_job.lr_scheduler_property.gamma:\n            fine_tune_module_kwargs['lr_scheduler_gamma'] = training_job.lr_scheduler_property.gamma\n        if training_job.lr_scheduler_property.step_size:\n            fine_tune_module_kwargs['step_size'] = training_job.lr_scheduler_property.step_size\n        model = FineTuneModule(**fine_tune_module_kwargs)\n        data_module = PyTorchDataModule(**training_job.data_module.dict(exclude_none=True))\n\n        trainer_kwargs = training_job.dict(exclude_none=True, include={'min_epochs', 'max_epochs'})\n        trainer = cls(\n            id=training_job.id,\n            model=model,\n            data_loader_kwargs={'datamodule': data_module},\n            trainer_kwargs={\n                'default_root_dir': training_job.data_module.data_dir or OUTPUT_DIR,\n                'weights_summary': None,\n                'progress_bar_refresh_rate': 1,\n                'num_sanity_val_steps': 0,\n                'gpus': 1,  # TODO: set GPU number\n                **trainer_kwargs,\n            },\n            model_id=training_job.model\n        )\n        return trainer\n\n    def start(self):\n        def training_done_callback(future):\n            model_train_curd.update(TrainingJobUpdate(_id=self.id, status=Status.PASS))\n            # TODO: save to database and update model_status, engine\n            print(self.export_model())\n\n        self._task = self._executor.submit(self.trainer_engine.fit, self.model, **self._data_loader_kwargs)\n        self._task.add_done_callback(training_done_callback)\n        model_train_curd.update(TrainingJobUpdate(_id=self.id, status=Status.RUNNING))\n\n        model_bo = ModelService.get_model_by_id(self.model_id)\n        model_bo.model_status.remove(ModelStatus.DRAFT)\n        model_bo.model_status.append(ModelStatus.TRAINING)\n        ModelService.update_model(model_bo)\n\n    def join(self, timeout=None):\n        if self._task:\n            self._task.result(timeout=timeout)\n\n    def terminate(self):\n        if self._task:\n            # trigger pytorch lighting training graceful shutdown via a ^C\n            self._task.set_exception(KeyboardInterrupt())\n            model_train_curd.update(TrainingJobUpdate(_id=self.id, status=Status.FAIL))\n            model_bo = ModelService.get_model_by_id(self.model_id)\n            model_bo.model_status.remove(ModelStatus.TRAINING)\n            model_bo.model_status.append(ModelStatus.DRAFT)\n            ModelService.update_model(model_bo)\n\n    def export_model(self):\n        return self.model.net.cpu()\n\n    def resume_soon(self):\n        if self._event_pause.is_set():\n            self._event_pause.clear()\n            return True\n        return False\n\n    def pause_soon(self):\n        if not self._event_pause.is_set():\n            self._event_pause.set()\n            return True\n        return False\n\n    def set_device(self):\n        pass\n"}
{"type": "source_file", "path": "modelci/experimental/mongo_client.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2/1/2021\n\"\"\"\nfrom typing import Optional\n\nimport pymongo\n\nfrom modelci.config import db_settings\n\n\nclass MongoClient(pymongo.MongoClient):\n    def __init__(\n            self,\n            host: str = db_settings.mongo_host,\n            port: str = db_settings.mongo_port,\n            document_class: type = dict,\n            tz_aware: bool = True,\n            connect: bool = None,\n            type_registry=None,\n            username: str = db_settings.mongo_username,\n            password: Optional[str] = db_settings.mongo_password.get_secret_value(),\n            authSource: str = db_settings.mongo_auth_source,\n            authMechanism: str = db_settings.auth_mechanism,\n            **kwargs\n    ):\n        \"\"\"\n        MongoDB Client wrapper with defined configuration.\n\n        Use this class just as `pymongo.MongoClient`. We inject some database related configuration into such\n        MongoDB Client wrapper, for ease of config management.\n        \"\"\"\n        super().__init__(\n            host=host,\n            port=port,\n            document_class=document_class,\n            tz_aware=tz_aware,\n            connect=connect,\n            type_registry=type_registry,\n            username=username,\n            password=password,\n            authSource=authSource,\n            authMechanism=authMechanism,\n            **kwargs,\n        )\n"}
{"type": "source_file", "path": "modelci/experimental/curd/model_train.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2/1/2021\n\"\"\"\nfrom typing import List\n\nfrom bson import ObjectId\n\nfrom modelci.config import db_settings\nfrom modelci.experimental.model.model_train import TrainingJob, TrainingJobIn, TrainingJobUpdate\nfrom modelci.experimental.mongo_client import MongoClient\nfrom modelci.persistence.exceptions import ServiceException\nfrom modelci.persistence.model_dao import ModelDAO\n\n_db = MongoClient()[db_settings.mongo_db]\n_collection = _db['training_job']\n\n\ndef exists_by_id(id: str) -> bool:\n    count = _collection.count_documents(filter={'_id': ObjectId(id)}, limit=1)\n    return bool(count)\n\n\ndef get_by_id(id: str) -> TrainingJob:\n    # exists by ID\n    if not bool(_collection.count_documents(filter={'_id': ObjectId(id)}, limit=1)):\n        raise ValueError(f'id {id} not found.')\n\n    document = _collection.find_one(filter={'_id': ObjectId(id)})\n    training_job = TrainingJob(**document)\n    return training_job\n\n\ndef get_all() -> List[TrainingJob]:\n    cursor = _collection.find({})\n    training_jobs = list(map(lambda d: TrainingJob(**d), cursor))\n    return training_jobs\n\n\ndef save(training_job_in: TrainingJobIn) -> str:\n    model_id = training_job_in.model\n    if not ModelDAO.exists_by_id(ObjectId(model_id)):\n        raise ServiceException(f'Model with ID {model_id} not exist.')\n\n    training_job = TrainingJob(**training_job_in.dict(exclude_none=True))\n    return _collection.insert_one(training_job.dict(exclude_none=True)).inserted_id\n\n\ndef update(training_job: TrainingJobUpdate) -> int:\n    # exists by ID\n    if not bool(_collection.count_documents(filter={'_id': ObjectId(training_job.id)}, limit=1)):\n        raise ValueError(f'id {training_job.id} not found.')\n\n    # check model ID\n    if training_job.model and not ModelDAO.exists_by_id(ObjectId(training_job.model)):\n        raise ServiceException(f'Model with ID {training_job.model} not exist.')\n\n    # save update\n    update_data = training_job.dict(exclude_unset=True)\n    result = _collection.update_one(filter={'_id': ObjectId(training_job.id)}, update={'$set': update_data})\n    return result.modified_count\n\n\ndef delete_by_id(id: str) -> TrainingJob:\n    document: dict = _collection.find_one_and_delete(filter={'_id': ObjectId(id)})\n    return TrainingJob(**document)\n\n\ndef delete_all() -> int:\n    return _collection.delete_many(filter={}).deleted_count\n"}
{"type": "source_file", "path": "modelci/experimental/model/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/31/2021\n\nData model definition.\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/experimental/model/common.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2/1/2021\n\nCommon Pydantic Class\n\"\"\"\nfrom bson import ObjectId\n\n\nclass ObjectIdStr(str):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        if isinstance(v, str):\n            v = ObjectId(v)\n        if not isinstance(v, ObjectId):\n            raise ValueError(\"Not a valid ObjectId\")\n        return str(v)\n"}
{"type": "source_file", "path": "modelci/experimental/model/model_train.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/31/2021\n\nML model training parameters.\n\"\"\"\nimport abc\nfrom enum import Enum\nfrom typing import Optional, Union, Tuple, List\n\nfrom pydantic import BaseModel, PositiveInt, PositiveFloat, root_validator, validator, confloat\n\nfrom modelci.experimental.model.common import ObjectIdStr\nfrom modelci.types.vo import Status\n\n\nclass DataModuleProperty(BaseModel):\n    dataset_name: str\n    batch_size: PositiveInt\n    num_workers: Optional[int] = 1\n    data_dir: Optional[str]\n\n\nclass OptimizerType(Enum):\n    SGD = 'SGD'\n    ADAGRAD = 'Adagrad'\n    ADAM = 'Adam'\n\n\n# noinspection PyUnresolvedReferences\nclass OptimizerPropertyBase(BaseModel, abc.ABC):\n    \"\"\"\n    Base class of optimizer property. It provides parameters to initialize optimizer.\n\n    Attributes:\n        __required_type__ (OptimizerType): By overriding this attribute, the subclass type can be automatically\n            determined by :meth:`check_required_type`.\n    \"\"\"\n    lr: Optional[PositiveFloat]\n\n    __required_type__: OptimizerType\n\n    @root_validator(pre=True)\n    def check_required_type(cls, values):  # pylint: disable=no-self-use\n        required_type = values.get('__required_type__')\n        if isinstance(required_type, str):\n            required_type = OptimizerType(required_type)\n        if required_type != cls.__required_type__:\n            raise ValueError(f'Given type {required_type}, required {cls.__required_type__}')\n        return values\n\n\nclass SGDProperty(OptimizerPropertyBase):\n    momentum: Optional[confloat(ge=0)]\n\n    __required_type__ = OptimizerType.SGD\n\n\nclass AdagradProperty(OptimizerPropertyBase):\n    lr_decay: Optional[PositiveFloat]\n    weight_decay: Optional[confloat(ge=0)]\n    eps: Optional[PositiveFloat]\n\n    __required_type__ = OptimizerType.ADAGRAD\n\n\nclass AdamProperty(OptimizerPropertyBase):\n    betas: Optional[Tuple[PositiveFloat, PositiveFloat]]\n    eps: Optional[confloat(ge=0)]\n    weight_decay: Optional[confloat(ge=0)]\n    amsgrad: Optional[bool]\n\n    __required_type__ = OptimizerType.ADAM\n\n\n_OptimizerProperty = Union[SGDProperty, AdagradProperty, AdamProperty]\n\n\nclass LRSchedulerType(Enum):\n    STEP_LR = 'StepLR'\n    MULTI_STEP_LR = 'MultiStepLR'\n    EXPONENTIAL_LR = 'ExponentialLR'\n\n\n# noinspection PyUnresolvedReferences\nclass LRSchedulerPropertyBase(BaseModel, abc.ABC):\n    \"\"\"\n    Base class of learning rate scheduler property. It provides parameters to initialize LR scheduler.\n\n    Attributes:\n        __required_type__ (LRSchedulerType): By overriding this attribute, the subclass type can be automatically\n            determined by :meth:`check_required_type`.\n    \"\"\"\n    last_epoch: Optional[int]\n    verbose: Optional[bool]\n\n    __required_type__: LRSchedulerType\n\n    @root_validator(pre=True)\n    def check_required_type(cls, values):  # pylint: disable=no-self-use\n        required_type = values.get('__required_type__')\n        if isinstance(required_type, str):\n            required_type = LRSchedulerType(required_type)\n        if required_type != cls.__required_type__:\n            raise ValueError(f'Given type {required_type}, required {cls.__required_type__}')\n        return values\n\n\nclass StepLRProperty(LRSchedulerPropertyBase):\n    step_size: PositiveInt\n    gamma: Optional[PositiveFloat]\n\n    __required_type__ = LRSchedulerType.STEP_LR\n\n\nclass MultiStepLRProperty(LRSchedulerPropertyBase):\n    milestones: List[PositiveInt]\n    gamma: Optional[PositiveFloat]\n\n    __required_type__ = LRSchedulerType.MULTI_STEP_LR\n\n    @validator('milestones')\n    def check_list_increasing(cls, v):  # pylint: disable=no-self-use\n        if not all(i < j for i, j in zip(v, v[1:])):\n            raise ValueError(f'List {v} is not strictly increasing.')\n\n\nclass ExponentialLRProperty(LRSchedulerPropertyBase):\n    gamma: Optional[PositiveFloat]\n\n    __required_type__ = LRSchedulerType.EXPONENTIAL_LR\n\n\n_LRSchedulerProperty = Union[StepLRProperty, MultiStepLRProperty, ExponentialLRProperty]\n\n\nclass LossFunctionType(Enum):\n    L1_Loss = 'torch.nn.L1Loss'\n    MSE_Loss = 'torch.nn.MSELoss'\n    CROSS_ENTROPY_LOSS = 'torch.nn.CrossEntropyLoss'\n\n\nclass TrainingJob(BaseModel):\n    id: Optional[ObjectIdStr]\n    model: ObjectIdStr\n    data_module: DataModuleProperty\n    min_epochs: Optional[PositiveInt]\n    max_epochs: PositiveInt\n    optimizer_type: OptimizerType\n    optimizer_property: _OptimizerProperty\n    lr_scheduler_type: LRSchedulerType\n    lr_scheduler_property: _LRSchedulerProperty\n    loss_function: LossFunctionType\n    status: Optional[Status] = Status.UNKNOWN\n    valdation_accuracy: Optional[float] = None\n\n    @root_validator(pre=True)\n    def optimizer_type_inject(cls, values):  # pylint: disable=no-self-use\n        \"\"\"\n        Inject type for `optimizer_property` based on the value provided in `optimizer_type`.\n        \"\"\"\n        test_type, test_prop = values.get('optimizer_type'), values.get('optimizer_property')\n        if test_type is None or test_prop is None:\n            # not checking here\n            return values\n\n        if isinstance(test_prop, dict):\n            test_prop['__required_type__'] = test_type\n        elif isinstance(test_prop, OptimizerPropertyBase):\n            if test_prop.__required_type__.value != test_type:\n                raise TypeError(f'`optimizer_property` has incorrect type {type(test_prop)} as '\n                                f'defined in `optimizer_type`: {test_type}.')\n        else:\n            raise TypeError(\n                f'Cannot parse type, expected one of [`dict`, `OptimizerPropertyBase`], got {type(test_prop)}.'\n            )\n\n        return values\n\n    @root_validator(pre=True)\n    def lr_scheduler_type_inject(cls, values):  # pylint: disable=no-self-use\n        \"\"\"\n        Inject type for `lr_scheduler_property` based on the value provided in `lr_scheduler_type`.\n        \"\"\"\n        test_type, test_prop = values.get('lr_scheduler_type'), values.get('lr_scheduler_property')\n        if test_type is None or test_prop is None:\n            # not checking here\n            return values\n\n        if isinstance(test_prop, dict):\n            test_prop['__required_type__'] = test_type\n        elif isinstance(test_prop, LRSchedulerPropertyBase):\n            if test_prop.__required_type__.value != test_type:\n                raise TypeError(f'`lr_scheduler_property` has incorrect type {type(test_prop)} as '\n                                f'defined in `lr_scheduler_type`: {test_type}.')\n        else:\n            raise TypeError(\n                f'Cannot parse type, expected one of [`dict`, `LRSchedulerPropertyBase`], got {type(test_prop)}.'\n            )\n\n        return values\n\n    class Config:\n        use_enum_values = True\n        fields = {'id': '_id'}\n\n\nclass TrainingJobIn(BaseModel):\n    model: ObjectIdStr\n    data_module: DataModuleProperty\n    min_epochs: Optional[PositiveInt]\n    max_epochs: PositiveInt\n    optimizer_type: OptimizerType\n    optimizer_property: _OptimizerProperty\n    lr_scheduler_type: LRSchedulerType\n    lr_scheduler_property: _LRSchedulerProperty\n    loss_function: LossFunctionType\n\n    @root_validator(pre=True)\n    def optimizer_type_inject(cls, values):  # pylint: disable=no-self-use\n        \"\"\"\n        Inject type for `optimizer_property` based on the value provided in `optimizer_type`.\n        \"\"\"\n        test_type, test_prop = values.get('optimizer_type'), values.get('optimizer_property')\n        if isinstance(test_prop, dict):\n            test_prop['__required_type__'] = test_type\n        elif isinstance(test_prop, OptimizerPropertyBase):\n            if test_prop.__required_type__.value != test_type:\n                raise TypeError(f'`optimizer_property` has incorrect type {type(test_prop)} as '\n                                f'defined in `optimizer_type`: {test_type}.')\n        else:\n            raise TypeError(\n                f'Cannot parse type, expected one of [`dict`, `OptimizerProperty`], got {type(test_prop)}.'\n            )\n\n        return values\n\n    @root_validator(pre=True)\n    def lr_scheduler_type_inject(cls, values):  # pylint: disable=no-self-use\n        \"\"\"\n        Inject type for `lr_scheduler_property` based on the value provided in `lr_scheduler_type`.\n        \"\"\"\n        test_type, test_prop = values.get('lr_scheduler_type'), values.get('lr_scheduler_property')\n        if isinstance(test_prop, dict):\n            test_prop['__required_type__'] = test_type\n        elif isinstance(test_prop, LRSchedulerPropertyBase):\n            if test_prop.__required_type__.value != test_type:\n                raise TypeError(f'`lr_scheduler_property` has incorrect type {type(test_prop)} as '\n                                f'defined in `lr_scheduler_type`: {test_type}.')\n        else:\n            raise TypeError(\n                f'Cannot parse type, expected one of [`dict`, `LRSchedulerPropertyBase`], got {type(test_prop)}.'\n            )\n\n        return values\n\n    class Config:\n        use_enum_values = True\n\n\nclass TrainingJobUpdate(TrainingJob):\n    \"\"\"\n    TODO: more optional fields for update\n    \"\"\"\n    model: Optional[ObjectIdStr]\n    data_module: Optional[DataModuleProperty]\n    max_epochs: Optional[PositiveInt]\n    optimizer_type: Optional[OptimizerType]\n    optimizer_property: Optional[_OptimizerProperty]\n    lr_scheduler_type: Optional[LRSchedulerType]\n    lr_scheduler_property: Optional[_LRSchedulerProperty]\n    loss_function: Optional[LossFunctionType]\n"}
{"type": "source_file", "path": "modelci/experimental/curd/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2/1/2021\n\nCRUD functions for data access.\n\"\"\"\n"}
{"type": "source_file", "path": "modelci/controller/executor.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 6/29/2020\n\"\"\"\nfrom queue import Queue\nfrom threading import Thread\nfrom typing import Union, Optional\n\nfrom docker.models.containers import Container\n\nfrom modelci.hub.profiler import Profiler\nfrom modelci.metrics.benchmark.metric import BaseModelInspector\nfrom modelci.persistence.service import ModelService\nfrom modelci.types.bo import ModelBO, Status\n\n\nclass Job(object):\n    def __init__(\n            self,\n            client: BaseModelInspector,\n            device: str,\n            model_info: ModelBO,\n            container_name: str = None\n    ):\n        self.client = client\n        self.device = device\n        self.model = model_info\n        self.container_name = container_name\n\n\nclass JobExecutor(Thread):\n    _instance = None\n    _queue_finish_flag = object()\n\n    def __init__(self, q_size: int = 200):\n        if self._instance is not None:\n            return\n        super().__init__()\n        self.job_queue: Queue[Union[Job, object]] = Queue(maxsize=q_size)\n        self._hold_container: Queue[Container] = Queue(maxsize=10)\n        self._instance = self\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            return super().__new__(cls, *args, **kwargs)\n        else:\n            return cls._instance\n\n    def submit(self, job: Job):\n        \"\"\"Submit a profiling job to the executor.\"\"\"\n        self.job_queue.put(job)\n\n    def join(self, timeout: Optional[float] = ...) -> None:\n        \"\"\"The executor stops accepting new coming jobs and joins to its parent.\n\n        This function should be called before `join`. Otherwise, the executor will never stop.\n\n        TODO:\n            1. Save exit when there is an exception. Try excepthook in python 3.8\n            2. Change failed profiling model status to fail.\n        \"\"\"\n        self.job_queue.put(self._queue_finish_flag)\n        return super().join(timeout)\n\n    def run(self) -> None:\n        from modelci.hub.deployer.dispatcher import serve\n\n        for job in iter(self.job_queue.get, None):\n            # exit the queue\n            if job is self._queue_finish_flag:\n                break\n            # start a new container if container not started\n            if job.container_name is None:\n                container = serve(save_path=job.model.saved_path, device=job.device)\n                container_name = container.name\n                # remember to clean-up the created container\n                self._hold_container.put(container)\n            else:\n                container_name = job.container_name\n            # change model status\n            job.model.status = Status.RUNNING\n            ModelService.update_model(job.model)\n\n            profiler = Profiler(model_info=job.model, server_name=container_name, inspector=job.client)\n            dpr = profiler.diagnose(device=job.device)\n            ModelService.append_dynamic_profiling_result(job.model.id, dynamic_result=dpr)\n\n            # set model status to pass\n            job.model.status = Status.PASS\n            ModelService.update_model(job.model)\n\n            if job.container_name is None:\n                # get holding container\n                self._hold_container.get().stop()\n"}
{"type": "source_file", "path": "modelci/experimental/finetuner/transfer_learning.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2021/1/22\n\nComputer vision example on Transfer Learning.\n\nThis computer vision example illustrates how one could fine-tune a pre-trained\nnetwork (by default, a ResNet50 is used) using pytorch-lightning. For the sake\nof this example, the 'cats and dogs dataset' (~60MB, see `DATA_URL` below) and\nthe proposed network (denoted by `TransferLearningModel`, see below) is\ntrained for 15 epochs.\n\nThe training consists of three stages.\n\nFrom epoch 0 to 4, the feature extractor (the pre-trained network) is frozen except\nmaybe for the BatchNorm layers (depending on whether `train_bn = True`). The BatchNorm\nlayers (if `train_bn = True`) and the parameters of the classifier are trained as a\nsingle parameters group with lr = 1e-2.\n\nFrom epoch 5 to 9, the last two layer groups of the pre-trained network are unfrozen\nand added to the optimizer as a new parameter group with lr = 1e-4 (while lr = 1e-3\nfor the first parameter group in the optimizer).\n\nEventually, from epoch 10, all the remaining layer groups of the pre-trained network\nare unfrozen and added to the optimizer as a third parameter group. From epoch 10,\nthe parameters of the pre-trained network are trained with lr = 1e-5 while those of\nthe classifier is trained with lr = 1e-4.\n\nNote:\n    See: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n\nReference:\n    https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/computer_vision_fine_tuning.py\n\"\"\"\n\nfrom typing import Generator, Optional, Callable\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import optim\nfrom torch.nn import Module\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.optim.optimizer import Optimizer\n\nBN_TYPES = (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)\n\n\n#  --- Utility functions ---\n\n\ndef _make_trainable(module: Module) -> None:\n    \"\"\"Unfreezes a given module.\n\n    Args:\n        module: The module to unfreeze\n    \"\"\"\n    for param in module.parameters():\n        param.requires_grad = True\n    module.train()\n\n\ndef _recursive_freeze(module: Module, train_bn: bool = True) -> None:\n    \"\"\"Freezes the layers of a given module.\n\n    Args:\n        module: The module to freeze\n        train_bn: If True, leave the BatchNorm layers in training mode\n    \"\"\"\n    children = list(module.children())\n    if not children:\n        if not (isinstance(module, BN_TYPES) and train_bn):\n            for param in module.parameters():\n                param.requires_grad = False\n            module.eval()\n        else:\n            # Make the BN layers trainable\n            _make_trainable(module)\n    else:\n        for child in children:\n            _recursive_freeze(module=child, train_bn=train_bn)\n\n\ndef freeze(module: Module, n: Optional[int] = None, train_bn: bool = True) -> None:\n    \"\"\"Freezes the layers up to index n (if n is not None).\n\n    Args:\n        module: The module to freeze (at least partially)\n        n: Max depth at which we stop freezing the layers. If None, all\n            the layers of the given module will be frozen.\n        train_bn: If True, leave the BatchNorm layers in training mode\n    \"\"\"\n    children = list(module.children())\n    n_max = len(children) if n is None else int(n)\n\n    for child in children[:n_max]:\n        _recursive_freeze(module=child, train_bn=train_bn)\n\n    for child in children[n_max:]:\n        _make_trainable(module=child)\n\n\ndef filter_params(module: Module, train_bn: bool = True) -> Generator:\n    \"\"\"Yields the trainable parameters of a given module.\n\n    Args:\n        module: A given module\n        train_bn: If True, leave the BatchNorm layers in training mode\n\n    Returns:\n        Generator\n    \"\"\"\n    children = list(module.children())\n    if not children:\n        if not (isinstance(module, BN_TYPES) and train_bn):\n            for param in module.parameters():\n                if param.requires_grad:\n                    yield param\n    else:\n        for child in children:\n            for param in filter_params(module=child, train_bn=train_bn):\n                yield param\n\n\ndef _unfreeze_and_add_param_group(\n        module: Module, optimizer: Optimizer, lr: Optional[float] = None, train_bn: bool = True\n):\n    \"\"\"Unfreezes a module and adds its parameters to an optimizer.\"\"\"\n    _make_trainable(module)\n    params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\n    optimizer.add_param_group(\n        {\n            \"params\": filter_params(module=module, train_bn=train_bn),\n            \"lr\": params_lr / 10.0,\n        }\n    )\n\n\n#  --- Pytorch-lightning module ---\n\n\nclass FineTuneModule(pl.LightningModule):\n    \"\"\"Transfer Learning with pre-trained model\"\"\"\n\n    def __init__(\n            self,\n            net: torch.nn.Module,\n            loss: Callable,\n            batch_size: int = 8,\n            lr: float = 1e-2,\n            lr_scheduler_gamma: float = 1e-1,\n            step_size: int = 7,\n            num_workers: int = 6,\n            **kwargs,\n    ) -> None:\n        super().__init__()\n        self.net = net\n        self.loss = loss\n        self.batch_size = batch_size\n        self.lr = lr\n        self.lr_scheduler_gamma = lr_scheduler_gamma\n        self.step_size = step_size\n        self.num_workers = num_workers\n\n        self.train_acc = pl.metrics.Accuracy()\n        self.valid_acc = pl.metrics.Accuracy()\n        self.save_hyperparameters('loss', 'batch_size', 'lr', 'lr_scheduler_gamma', 'step_size', 'num_workers')\n\n    def forward(self, x):\n\n        return self.net(x)\n\n    def training_step(self, batch, batch_idx):\n\n        # 1. Forward pass:\n        x, y = batch\n        outputs = self.forward(x)\n        preds = torch.argmax(outputs, dim=1)\n\n        # 2. Compute loss & accuracy:\n        train_loss = self.loss(outputs, y)\n        accuracy = self.train_acc(preds, y)\n\n        # 3. Outputs:\n        tqdm_dict = {'train_loss': train_loss, 'train_acc': accuracy}\n        self.log_dict(tqdm_dict, prog_bar=True)\n        return {\"loss\": train_loss}\n\n    def training_epoch_end(self, outputs):\n        \"\"\"Compute and log training loss and accuracy at the epoch level.\"\"\"\n\n        train_loss_mean = torch.stack([output['loss'] for output in outputs]).mean()\n        train_acc_mean = self.train_acc.compute()\n        self.log_dict({'train_loss': train_loss_mean, 'train_acc': train_acc_mean, 'step': self.current_epoch})\n\n    def validation_step(self, batch, batch_idx):\n\n        # 1. Forward pass:\n        x, y = batch\n        outputs = self.forward(x)\n        preds = torch.argmax(outputs, dim=1)\n\n        # 2. Compute loss & accuracy:\n        val_loss = self.loss(outputs, y)\n        accuracy = self.valid_acc(preds, y)\n\n        return {\"val_loss\": val_loss, 'val_acc': accuracy}\n\n    def validation_epoch_end(self, outputs):\n        \"\"\"Compute and log validation loss and accuracy at the epoch level.\"\"\"\n\n        val_loss_mean = torch.stack([output['val_loss'] for output in outputs]).mean()\n        train_acc_mean = self.valid_acc.compute()\n        log_dict = {'val_loss': val_loss_mean, 'val_acc': train_acc_mean}\n        self.log_dict(log_dict, prog_bar=True)\n        self.log_dict({'step': self.current_epoch})\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n\n        scheduler = StepLR(optimizer, step_size=self.step_size, gamma=self.lr_scheduler_gamma)\n\n        return [optimizer], [scheduler]\n"}
{"type": "source_file", "path": "modelci/experimental/finetuner/resnet_fine_tune_example.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/25/2021\n\"\"\"\nimport argparse\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\n\nimport torch\n\nfrom modelci.experimental.finetuner.pytorch_datamodule import PyTorchDataModule\nfrom modelci.experimental.finetuner.trainer import PyTorchTrainer\nfrom modelci.experimental.finetuner.transfer_learning import freeze, FineTuneModule\n\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Train the model.\n\n    Args:\n        args: Model hyper-parameters\n\n    Note:\n        For the sake of the example, the images dataset will be downloaded\n        to a temporary directory.\n    \"\"\"\n\n    with TemporaryDirectory(dir=args.root_data_path):\n        # TODO: Done in editor (L247-251)\n        net = torch.hub.load('pytorch/vision:v0.6.0', args.backbone, pretrained=True)\n        freeze(module=net, train_bn=True)\n\n        num_ftrs = net.fc.in_features\n        net.fc = torch.nn.Linear(num_ftrs, 10)\n\n        model = FineTuneModule(**vars(args), net=net, loss=torch.nn.CrossEntropyLoss())\n        data_module = PyTorchDataModule('CIFAR10', batch_size=args.batch_size, data_dir=args.root_data_path)\n\n        trainer = PyTorchTrainer(\n            model=model,\n            data_loader_kwargs={'datamodule': data_module},\n            trainer_kwargs={\n                'weights_summary': None,\n                'progress_bar_refresh_rate': 1,\n                'num_sanity_val_steps': 0,\n                'gpus': args.gpus,\n                'min_epochs': args.nb_epochs,\n                'max_epochs': args.nb_epochs,\n            }\n        )\n        trainer.start()\n        trainer.join()\n\n\ndef get_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument(\n        \"--root-data-path\",\n        metavar=\"DIR\",\n        type=str,\n        default=Path.cwd().as_posix(),\n        help=\"Root directory where to download the data\",\n        dest=\"root_data_path\",\n    )\n    parser.add_argument(\n        \"--backbone\",\n        default=\"resnet50\",\n        type=str,\n        metavar=\"BK\",\n        help=\"Name (as in ``torchvision.models``) of the feature extractor\",\n    )\n    parser.add_argument(\n        \"--epochs\", default=15, type=int, metavar=\"N\", help=\"total number of epochs\", dest=\"nb_epochs\"\n    )\n    parser.add_argument(\"--batch-size\", default=128, type=int, metavar=\"B\", help=\"batch size\", dest=\"batch_size\")\n    parser.add_argument(\"--gpus\", type=int, default=1, help=\"number of gpus to use\")\n    parser.add_argument(\n        \"--lr\", \"--learning-rate\", default=1e-2, type=float, metavar=\"LR\", help=\"initial learning rate\", dest=\"lr\"\n    )\n    parser.add_argument(\n        \"--lr-scheduler-gamma\",\n        default=1e-1,\n        type=float,\n        metavar=\"LRG\",\n        help=\"Factor by which the learning rate is reduced at each milestone\",\n    )\n    parser.add_argument(\n        \"--step-size\",\n        default=7,\n        type=int,\n        metavar=\"SS\",\n        help=\"Step size used in step scheduler\",\n    )\n    parser.add_argument(\n        \"--num-workers\", default=6, type=int, metavar=\"W\", help=\"number of CPU workers\", dest=\"num_workers\"\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    main(get_args())\n"}
{"type": "source_file", "path": "modelci/experimental/finetuner/pytorch_datamodule.py", "content": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Jiang Shanshan\nEmail: univeroner@gmail.com\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 2021/1/20\n\nReferences:\n    https://pytorch-lightning.readthedocs.io/en/stable/datamodules.html\n\"\"\"\nimport warnings\n\nimport pytorch_lightning as pl\nimport torchvision\nfrom torch.utils.data import random_split, DataLoader\nfrom torchvision.transforms import transforms\n\n# transforms\nfrom modelci.experimental.finetuner import OUTPUT_DIR\n\nINPUT_SIZE = (224, 224)\nTRAIN_VAL_SPLIT_RATIO = 0.7\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(INPUT_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(INPUT_SIZE),\n    transforms.CenterCrop(INPUT_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n# noinspection PyAbstractClass\nclass PyTorchDataModule(pl.LightningDataModule):\n\n    def __init__(self, dataset_name: str, batch_size: int = 8, num_workers=2, data_dir=OUTPUT_DIR):\n        \"\"\"\n\n        Args:\n            dataset_name (str): name of vision dataset to be load, full lists of supported datasets:\n                https://pytorch.org/docs/stable/torchvision/datasets.html\n            batch_size (int): samples per batch to load\n        \"\"\"\n        super().__init__()\n        self.data_dir = data_dir\n        self.dataset_name = dataset_name\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        if hasattr(torchvision.datasets, self.dataset_name):\n            self.dataset = getattr(torchvision.datasets, self.dataset_name)\n        else:\n            # TODO: support user local dataset folder\n            warnings.warn('If you are using customized dataset, please specify a dataset type.')\n            raise ValueError(f'torchvision.datasets does not have dataset name {self.dataset_name}')\n\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n\n    def prepare_data(self, *args, **kwargs):\n        self.dataset(root=self.data_dir, train=True, download=True)\n        self.dataset(root=self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # split dataset\n        if stage == 'fit':\n            full = self.dataset(root=self.data_dir, train=True, transform=train_transforms)\n            train_size = int(len(full) * TRAIN_VAL_SPLIT_RATIO)\n            test_size = len(full) - train_size\n            self.train_dataset, self.val_dataset = random_split(full, [train_size, test_size])\n\n        if stage == 'test':\n            self.test_dataset = self.dataset(root=self.data_dir, train=False, transform=test_transforms)\n\n    # return the dataloader for each split\n    def train_dataloader(self):\n        train = DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n        return train\n\n    def val_dataloader(self):\n        val = DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n        return val\n\n    def test_dataloader(self):\n        test = DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n        return test\n"}
{"type": "source_file", "path": "modelci/data_engine/__init__.py", "content": ""}
{"type": "source_file", "path": "modelci/experimental/finetuner/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/12/2021\n\"\"\"\nfrom pathlib import Path\n\nOUTPUT_DIR = Path.home() / 'tmp'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"}
{"type": "source_file", "path": "modelci/experimental/model/model_structure.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 1/27/2021\n\nML model structure definitions.\n\"\"\"\nimport abc\nimport inspect\nfrom enum import Enum\nfrom typing import Optional, Union, Tuple, Dict, OrderedDict\n\nfrom pydantic import BaseModel, PositiveInt, conint, PositiveFloat, Field, validator\nfrom typing_extensions import Literal\n\n\nclass Operation(Enum):\n    \"\"\"\n    Operation enum to the layer or connection. There are three kinds of operations: ``'A'`` for add the specific\n    layer / connection, ``'D'`` for delete the specific layer / connection, ``M`` for modify the layer /\n    connection, and ``E`` for no operation.\n    \"\"\"\n    ADD = 'A'\n    DELETE = 'D'\n    MODIFY = 'M'\n    EMPTY = 'E'\n\n\nclass LayerType(Enum):\n    \"\"\"\n    Enum of the supported layer type. This is to hint which class of layer the provided data is converted to.\n    \"\"\"\n\n    LINEAR = 'torch.nn.Linear'\n    CONV_1D = 'torch.nn.Conv1d'\n    CONV_2D = 'torch.nn.Conv2d'\n    RELU = 'torch.nn.ReLU'\n    TANH = 'torch.nn.Tanh'\n    BN_1D = 'torch.nn.BatchNorm1d'\n    BN_2D = 'torch.nn.BatchNorm2d'\n    MP_1D = 'torch.nn.MaxPool1d'\n    MP_2D = 'torch.nn.MaxPool2d'\n    AAP_1D = 'torch.nn.AdaptiveAvgPool1d'\n    AAP_2D = 'torch.nn.AdaptiveAvgPool2d'\n\n\nclass ModelLayer(BaseModel, abc.ABC):\n    # noinspection PyUnresolvedReferences\n    \"\"\"\n    Layer of the model structure.\n\n    For layer attributes need to be set :code:`None`, use :code:`'null'` instead. This is for the reason of\n    updated parameters with value :code:`None` will be viewed as not set. So we take special care to the\n    desired :code:`None`, replacing it with :code:`'null'`.\n\n    Attributes:\n        op_ (Operation): Operation to the layer.\n        type_ (LayerType): Indicates the type of this layer. This field also provides hint for :class:`pydantic`\n            model conversion.\n        __required_type__ (LayerType): By overriding this attributes, we can use :meth:`check_layer_type` to\n            provide validation of the sub classes.\n    \"\"\"\n\n    op_: Operation\n    type_: LayerType\n\n    __required_type__: LayerType\n\n    @classmethod\n    def parse_layer_obj(cls, layer_obj):\n        \"\"\"\n        Parse from a ML layer object.\n\n        This function will inspect the required parameters to build the layer, and try to obtain its\n        parameter value from the layer object. The default parameter parser is python default\n        :code:`getattr`, which assume we can get the value from the same-named attribute of the\n        layer object.\n\n        For parameter cannot parsed with default parser, set a function with the format:\n        :code:`__{parameter_name}_parser__(layer_obj: Any) -> Any`.\n        Has the following signature:\n            Input Arguments:\n            * layer_obj : Any\n                The layer object to be parsed.\n            Return Arguments:\n            * Any\n                The parsed value of the given parameter.\n\n        TODO:\n            Signature checking for __{parameter_name}_parser__\n        \"\"\"\n        kwargs = {'op_': Operation.EMPTY, 'type_': cls.__required_type__}\n        signature = inspect.signature(layer_obj.__init__)\n        for param in signature.parameters:\n            parser = getattr(cls, f'__{param}_parser__', lambda obj: getattr(obj, param))\n            kwargs[param] = parser(layer_obj)\n\n        return cls(**kwargs)\n\n    @validator('type_')\n    def check_layer_type(cls, layer_type: LayerType) -> LayerType:  # noqa\n        \"\"\"\n        Checks layer type value provided is the same as the required value.\n        This is to generate validator for check :code:`layer_type` field of subclasses of :class:`ModelLayer`.\n        \"\"\"\n        if layer_type != cls.__required_type__:\n            raise ValueError(f'Expected {cls.__required_type__} but got {layer_type}')\n        return layer_type\n\n\nclass Linear(ModelLayer):\n    in_features: Optional[PositiveInt]\n    out_features: Optional[PositiveInt]\n    bias: Optional[bool]\n\n    __required_type__ = LayerType.LINEAR\n\n    @staticmethod\n    def __bias_parser__(layer_obj):\n        return layer_obj.bias is not None\n\n\nclass _ConvNd(ModelLayer, abc.ABC):\n    in_channels: Optional[PositiveInt]\n    out_channels: Optional[PositiveInt]\n    kernel_size: Optional[Union[PositiveInt, Tuple[PositiveInt, ...]]]\n    stride: Optional[Union[PositiveInt, Tuple[PositiveInt, ...]]]\n    padding: Optional[Union[conint(ge=0), Tuple[conint(ge=0), ...]]]\n    dilation: Optional[Union[PositiveInt, Tuple[PositiveInt, ...]]]\n    groups: PositiveInt\n    bias: bool\n    padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular']\n\n    @staticmethod\n    def __bias_parser__(layer_obj):\n        return layer_obj.bias is not None\n\n\nclass Conv1d(_ConvNd):\n    __required_type__ = LayerType.CONV_1D\n\n\nclass Conv2d(_ConvNd):\n    __required_type__ = LayerType.CONV_2D\n\n\nclass ReLU(ModelLayer):\n    inplace: Optional[bool]\n\n    __required_type__ = LayerType.RELU\n\n\nclass Tanh(ModelLayer):\n    __required_type__ = LayerType.TANH\n\n\nclass _BatchNorm(ModelLayer, abc.ABC):\n    num_features: Optional[PositiveInt]\n    eps: Optional[PositiveFloat]\n    momentum: Optional[Union[PositiveFloat, Literal['null']]]\n    affine: Optional[bool]\n    track_running_stats: Optional[bool]\n\n\nclass BatchNorm1d(_BatchNorm):\n    __required_type__ = LayerType.BN_1D\n\n\nclass BatchNorm2d(_BatchNorm):\n    __required_type__ = LayerType.BN_2D\n\n\nclass _MaxPool(ModelLayer, abc.ABC):\n    kernel_size: Union[PositiveInt, Tuple[PositiveInt, ...]]\n    stride: Optional[Union[PositiveInt, Tuple[PositiveInt, ...]]] = None\n    padding: Union[conint(ge=0), Tuple[conint(ge=0), ...]] = 0\n    dilation: Union[PositiveInt, Tuple[PositiveInt, ...]] = 1\n    return_indices: bool = False\n    ceil_mode: bool = False\n\n\nclass MaxPool1d(_MaxPool):\n    __required_type__ = LayerType.MP_1D\n\n\nclass MaxPool2d(_MaxPool):\n    __required_type__ = LayerType.MP_2D\n\n\nclass _AdaptiveAvgPool(ModelLayer, abc.ABC):\n    output_size: Union[PositiveInt, Tuple[PositiveInt, ...]]\n\n\nclass AdaptiveAvgPool1d(_AdaptiveAvgPool):\n    __required_type__ = LayerType.AAP_1D\n\n\nclass AdaptiveAvgPool2d(_AdaptiveAvgPool):\n    __required_type__ = LayerType.AAP_2D\n\n\n_LayerType = Union[Linear, Conv1d, Conv2d, ReLU, Tanh, BatchNorm1d, BatchNorm2d, MaxPool1d, MaxPool2d,\n                   AdaptiveAvgPool1d, AdaptiveAvgPool2d]\n\n\nclass Structure(BaseModel):\n    # noinspection PyUnresolvedReferences\n    \"\"\"\n    Indicate a ML model structure using a graph data structure.\n    :attr:`layer` is the graph node, representing a layer of the model. :attr:`connection` is the graph edge,\n    representing which two layers are connected, and the directions of tensor pass.\n\n    Attributes:\n        layer (OrderedDict[str, _LayerType]): Layer mapping, the key is layer name, and the value is layer\n            attributes. See :class:`ModelLayer` for reference.\n        connection (Optional[Dict[str, Dict[str, Operation]]]): The connection (:attr:`connection`) maps\n            the starting layer name, to the ending layer name with a connection operation.\n\n    Examples::\n\n        >>> from collections import OrderedDict\n        >>> # add a nn.Linear layer named 'fc1' with in_features=1024, out_features=10\n        >>> layer_mapping = OrderedDict({\n        ...     'fc1': LinearLayer(in_features=1024, out_features=10, type_=LayerType.LINEAR, op_=Operation.ADD),\n        ... })\n        >>> # connection example for add connection from 'conv1' to 'fc1'\n        >>> connection_mapping = {'conv1': {'fc1': Operation.ADD}}\n        >>> struct = Structure(layer=layer_mapping, connection=connection_mapping)\n        >>> print(struct)\n        layer={'fc1': LinearLayer(in_features=1024, out_features=10, bias=None)}\n        connection={'conv1': {'fc1': <Operation.ADD: 'A'>}}\n        >>> # Other than using the model object, we can pass in a plain dictionary,\n        ... # and utilize `Structure.parse_obj`.\n        >>> structure_data = {\n        ...     'layer': {'fc': {'in_features': 1024, 'out_features': 10, 'type_': 'torch.nn.Linear', 'op_': 'A'}},\n        ...     'connection': {'conv1': {'fc1': 'A'}}\n        ... }\n        >>> Structure.parse_obj(structure_data)\n        Structure(layer={'fc': LinearLayer(in_features=1024, out_features=10, bias=None)},\n        connection={'conv1': {'fc1': <Operation.ADD: 'A'>}})\n\n    \"\"\"\n    layer: OrderedDict[str, _LayerType] = Field(\n        default_factory=OrderedDict,\n        example={'fc': {'out_features': 10, 'type_': 'torch.nn.Linear', 'op_': 'M'}}\n    )\n    connection: Optional[Dict[str, Dict[str, Operation]]] = Field(\n        default_factory=dict,\n        example={'conv1': {'fc1': 'A'}}\n    )\n"}
{"type": "source_file", "path": "modelci/hub/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 9/19/2020\n\"\"\"\n__all__ = ['converter', 'manager', 'client', 'utils', 'profiler']\n"}
{"type": "source_file", "path": "modelci/hub/client/__init__.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: Li Yuanming\nEmail: yli056@e.ntu.edu.sg\nDate: 9/19/2020\n\"\"\"\nfrom .onnx_client import CVONNXClient\nfrom .tfs_client import CVTFSClient\nfrom .torch_client import CVTorchClient\nfrom .trt_client import CVTRTClient\n\n__all__ = ['CVONNXClient', 'CVTFSClient', 'CVTRTClient', 'CVTorchClient']\n"}
{"type": "source_file", "path": "modelci/data_engine/preprocessor/image_classification.py", "content": "import numpy as np\nfrom PIL import Image\n\nfrom modelci.hub.utils import TensorRTModelInputFormat\nfrom modelci.types.trtis_objects import DataType\n\n\ndef preprocess(\n        img: Image.Image,\n        format: TensorRTModelInputFormat,\n        dtype: DataType,\n        c: int,\n        h: int,\n        w: int,\n        scaling: str\n):\n    \"\"\"\n    Pre-process an image to meet the size, type and format\n    requirements specified by the parameters.\n\n    Arguments:\n        img (Image.Image): Image object to be predicted.\n        format (TensorRTModelInputFormat): Format of input tensor.\n        dtype (DataType): Data type of input tensor.\n        c (int): Channel size.\n        h (int): Height size.\n        w (int): Weight size.\n        scaling (str): Image scaling algorithm. Supported one of `'INCEPTION'`, `'VGG'` and `None`.\n    \"\"\"\n    if c == 1:\n        sample_img = img.convert('L')\n    else:\n        sample_img = img.convert('RGB')\n\n    resized_img = sample_img.resize((w, h), Image.BILINEAR)\n    resized = np.array(resized_img)\n    if resized.ndim == 2:\n        resized = resized[:, :, np.newaxis]\n\n    typed = resized.astype(dtype)\n\n    if scaling == 'INCEPTION':\n        scaled = (typed / 128) - 1\n    elif scaling == 'VGG':\n        if c == 1:\n            scaled = typed - np.asarray((128,), dtype=dtype)\n        else:\n            scaled = typed - np.asarray((123, 117, 104), dtype=dtype)\n    else:\n        scaled = typed\n\n    # Swap to CHW if necessary\n    if format == TensorRTModelInputFormat.FORMAT_NCHW:\n        ordered = np.transpose(scaled, (2, 0, 1))\n    else:\n        ordered = scaled\n\n    # Channels are in RGB order. Currently model configuration data\n    # doesn't provide any information as to other channel orderings\n    # (like BGR) so we just assume RGB.\n    return ordered\n"}
