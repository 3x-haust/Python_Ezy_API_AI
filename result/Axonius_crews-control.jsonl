{"repo_info": {"repo_name": "crews-control", "repo_owner": "Axonius", "repo_url": "https://github.com/Axonius/crews-control"}}
{"type": "source_file", "path": "execution/consts.py", "content": "import typing\nimport os\n\nEXECUTION_CONFIG_PATH: typing.Final[str] = \"execution.yaml\"\nBENCHMARK_CONFIG_PATH: typing.Final[str] = \"benchmark.yaml\"\nOUTPUT_DIRECTORY_PATH: str = 'output'\nEXIT_ON_ERROR = os.getenv('EXIT_ON_ERROR', 'False').lower() == 'true'\n"}
{"type": "source_file", "path": "execution/crews/builder.py", "content": "import hashlib\nimport os\nimport typing\nfrom pathlib import Path\nimport time\nimport rich\nfrom crewai import Task, Agent, Crew\nfrom execution.contexts import load_crew_contexts\nfrom execution.consts import EXIT_ON_ERROR\nfrom tools.index import get_tool\nfrom utils import is_safe_path\nimport re\n\nclass NoAgentFoundError(Exception):\n    pass\n\nclass NoTaskFoundError(Exception):\n    pass\n\nclass CrewRunner:\n    def __init__(\n        self,\n        project_name: str,\n        crew_name: str,\n        crew_config: dict,\n        user_inputs: dict,\n        previous_crews_results: dict,\n        llm,\n        embedding_model,\n        should_export_results: bool = True,\n        ignore_cache: bool = False,\n    ):\n        self._crew_name: str = crew_name\n        self._user_input: dict = user_inputs\n        self._crew_config: dict = crew_config\n        self._project_name: str = project_name\n        self._previous_results: dict = previous_crews_results\n        self._llm, self._embedding_model = llm, embedding_model\n        self._crew_context: typing.Optional[dict] = None\n        self._ignore_cache: bool = ignore_cache\n\n        # evaluate paths\n        for key, value in (crew_config.get('context') or {}).items():\n            crew_config['context'][key] = self._evaluate_input(value)\n\n        # load crew context\n        self._crew_context: dict = load_crew_contexts(project_name, crew_config)\n\n        # output file\n        self._should_export_results: bool = should_export_results\n\n        # validate results\n        self._validate_results: str = self._evaluate_input(crew_config.get('validate_results') or '')\n\n        # evaluate context inputs\n        self.validate_crew_parameters()\n\n    def validate_crew_parameters(self):\n        \"\"\"Validate internal parameters that crew needs from 3rd parties.\"\"\"\n        # validations\n        if 'agents' not in self._crew_config:\n            raise NoAgentFoundError('Crew config must have at least one agent.')\n        if 'tasks' not in self._crew_config:\n            raise NoTaskFoundError('Crew config must have at least one task.')\n        # check no intersection between _crew_context and _user_input\n        if set(self._crew_context.keys()) & set(self._user_input.keys()) & set(self._previous_results.keys()):\n            raise ValueError('Crew context and user input must not have any intersection.')\n\n    def _evaluate_input(self, user_input: str) -> str:\n        try:\n            user_input = self._strip_sha256(user_input)\n\n            return user_input.format(\n                **(self._crew_context or {}),\n                **(self._user_input or {}),\n                **(self._previous_results or {})\n            )\n        except ValueError as e:\n            raise ValueError(f'\\nError evaluating input: {e}\\nUser input:\\n---\\n{user_input}\\n---\\n')\n\n    def _strip_sha256(self, user_input: str) -> str:\n        sha256_pattern = re.compile(r'\\{sha256:(\\w+)\\}')\n        return sha256_pattern.sub(r'{\\1}', user_input)\n\n    def _replace_sha256(self, user_input: str) -> str:\n        # Define a regex pattern to find {sha256:(\\w+)}\n        sha256_pattern = re.compile(r'\\{sha256:(\\w+)\\}')\n        \n        def replace_match(match):\n            var_name = match.group(1)\n            # Get the variable value from the context\n            var_value = (self._crew_context or {}).get(var_name) or \\\n                        (self._user_input or {}).get(var_name) or \\\n                        (self._previous_results or {}).get(var_name)\n            if var_value is None:\n                raise ValueError(f\"Variable '{var_name}' not found in context for SHA-256 hashing.\")\n            # Compute SHA-256 hash\n            hash_object = hashlib.sha256(var_value.encode())\n            return hash_object.hexdigest()\n        \n        # Replace {sha256:<variable>} patterns with their SHA-256 hash\n        return sha256_pattern.sub(replace_match, user_input)\n\n    def _evaluate_for_output_file(self, user_input: str) -> str:\n        # First, handle SHA-256 replacements\n        user_input = self._replace_sha256(user_input)\n        # Then, perform regular formatting\n        return self._evaluate_input(user_input)\n\n    @property\n    def _output_file(self) -> str:\n        # Automatically evaluate when accessing the _output_file property\n        return self._evaluate_for_output_file(self._crew_config.get('output_naming_template') or '').replace('/', '-')\n\n    def _get_tool_id(self, scope: typing.Optional[str] = None) -> str:\n        if scope is None:\n            return hashlib.md5(f'{self._crew_name}{list(self._user_input.values())}'.lower().encode()).hexdigest()\n        return hashlib.md5(f'{self._crew_name}{scope}{list(self._user_input.values())}'.lower().encode()).hexdigest()\n\n    def _get_agent(self, agent_name: str, agent_scope: typing.Optional[str] = None) -> Agent:\n        agent_config: dict = self._crew_config['agents'].get(agent_name)\n        try:\n            return Agent(\n                role=self._evaluate_input(agent_config['role']),\n                goal=self._evaluate_input(agent_config['goal']),\n                tools=[\n                    get_tool(tool, task_id=self._get_tool_id(agent_scope))\n                    for tool in agent_config.get('tools') or []\n                ],\n                backstory=self._evaluate_input(agent_config['backstory']),\n                allow_delegation=False,\n                llm=self._llm,\n                embedding_model=self._embedding_model,\n                verbose=True,\n                memory=True,\n            )\n        except ValueError as e:\n            raise ValueError(f'Error evaluating agent: {agent_name}. Error: {e}')\n\n    def _get_crew_tasks(self) -> list[Task]:\n        return [\n            Task(\n                description=self._evaluate_input(task_context['description']),\n                expected_output=self._evaluate_input(task_context['expected_output']),\n                tools=[\n                    get_tool(tool, task_id=self._get_tool_id(task_name))\n                    for tool in task_context.get('tools') or []\n                ],\n                agent=self._get_agent(agent_name=task_context['agent'], agent_scope=task_name),\n            )\n            for task_name, task_context in self._crew_config['tasks'].items()\n        ]\n\n    def _generate_agents(self) -> list[Agent]:\n        return [\n            self._get_agent(agent_name)\n            for agent_name, agent_config in list(self._crew_config.get('agents').items()) or []\n        ]\n\n    def _export_results(self, results: str):\n        if self._should_export_results:\n            export_path: Path = self._get_export_path()\n            rich.print(\n                f'[green bold]'\n                f'Writing {self._crew_name} result into <{export_path}>'\n                f'[/green bold]'\n            )\n            if not export_path.parent.exists():\n                export_path.parent.mkdir(parents=True, exist_ok=True)\n            export_path.write_text(results)\n        else:\n            rich.print(f\"[green bold]Crew <{self._crew_name}> result:\\n{results}\\n\\n[/green bold]\")\n\n    def _get_export_path(self) -> Path:\n        if not is_safe_path(Path.cwd() / 'projects' / self._project_name / 'output',\n                            Path.cwd() / 'projects' / self._project_name / 'output' / self._output_file):\n            rich.print(f\"[red bold]Error: Directory traversal detected in output file {self._output_file}[/red bold]\")\n            os._exit(1)\n        return Path.cwd() / 'projects' / self._project_name / 'output' / self._output_file\n\n    def run_crew(self) -> str:\n        export_path: Path = self._get_export_path()\n        if not self._ignore_cache and export_path.exists():\n            return export_path.read_text()\n\n        max_retries = 5\n        retry_count = 0\n        backoff_factor = 2\n\n        while retry_count < max_retries:\n            try:\n                results: str = Crew(\n                    agents=self._generate_agents(),\n                    tasks=self._get_crew_tasks(),\n                    verbose=2\n                ).kickoff()\n                self._export_results(results)\n                return results\n\n            except Exception as e:\n                error_code = self._extract_error_code(e)  # Implement this method to extract the error code\n                if error_code == \"429\":  # Check for rate limit error code\n                    retry_count += 1\n                    wait_time = backoff_factor ** retry_count\n                    rich.print(f\"[yellow bold]Rate limit error encountered. Retrying in {wait_time} seconds...[/yellow bold]\")\n                    rich.print(f\"[yellow bold]Exception details: {e}[/yellow bold]\")\n                    time.sleep(wait_time)\n                else:\n                    rich.print(f\"[red bold]Error occurred while running crew <{self._crew_name}>[/red bold]\")\n                    rich.print(f\"[red bold]Error: {e}[/red bold]\")\n                    if EXIT_ON_ERROR:\n                        os._exit(1)\n                    return str(e)\n\n        rich.print(f\"[red bold]Exceeded maximum retries. Aborting...[/red bold]\")\n        return \"Rate limit error: Exceeded maximum retries\"\n\n    def _extract_error_code(self, exception: Exception) -> str:\n        # Example implementation - adjust based on your actual exception structure\n        if hasattr(exception, 'response') and hasattr(exception.response, 'status_code'):\n            return str(exception.response.status_code)\n        return \"\""}
{"type": "source_file", "path": "execution/inputs.py", "content": "import rich\nfrom rich.pretty import Pretty\n\ndef get_user_inputs(execution_config: dict) -> dict:\n    \"\"\"Get inputs from the user.\"\"\"\n    if 'user_inputs' not in execution_config:\n        return {}\n\n    user_inputs: dict[str, str] = {}\n    for task, description in execution_config['user_inputs'].items():\n        while True:\n            user_input = input(f'Please enter {description[\"title\"]}: ')\n            if user_input:\n                if (\n                    execution_config['user_inputs'][task].get('enum') and\n                    user_input not in execution_config['user_inputs'][task]['enum']\n                ):\n                    rich.print(\n                        f'[red]Invalid {task} entered: {user_input}.'\n                        f' Please enter one of {execution_config[\"user_inputs\"][task][\"enum\"]}[/red]'\n                    )\n                    continue\n                break\n            if 'enum' in description:\n                rich.print(\n                    f'[red]Invalid {task} entered: {user_input}.'\n                    f' Please enter one of {execution_config[\"user_inputs\"][task][\"enum\"]}[/red]'\n                )\n            else:\n                break\n        user_inputs[task] = user_input\n    rich.print(f'[white]User inputs: {user_inputs}[/white]')\n    return user_inputs\n\n\ndef validate_user_inputs(user_inputs: dict, execution_config: dict):\n    \"\"\"Validate the user inputs.\"\"\"\n    rich.print(\"Validating user inputs:\")\n    rich.print(Pretty(user_inputs, indent_guides=True,expand_all=True))\n    rich.print(\"with execution config:\")\n    rich.print(Pretty(execution_config[\"user_inputs\"], indent_guides=True,expand_all=True))\n\n    for user_input, descriptor in execution_config['user_inputs'].items():\n        if not user_inputs.get(user_input):\n            if not descriptor.get('optional'):\n                raise ValueError(f'{user_input} is required')\n\n            # set optional inputs to empty string\n            user_inputs[user_input] = ''\n\n        if (\n            descriptor.get('enum') and\n            user_inputs.get(user_input) not in descriptor['enum']\n        ):\n            raise ValueError(\n                f'Invalid {user_input} entered: {user_inputs.get(user_input)}.'\n                f' Please enter one of {descriptor[\"enum\"]}'\n            )\n"}
{"type": "source_file", "path": "execution/contexts.py", "content": "import abc\nimport os\nfrom pathlib import Path\nfrom utils import is_safe_path\nimport rich\n\nCONTEXT_DIRECTORY_PATH = 'context'\n\n\nclass IFileReader(abc.ABC):\n    @abc.abstractmethod\n    def read(self, context_name: str, project_name: str) -> str:\n        pass\n\n\nclass ContextFileReader(IFileReader):\n    def __init__(self, context_directory_path: str = CONTEXT_DIRECTORY_PATH):\n        self.context_directory_path = context_directory_path\n\n    def read(self, context_name: str, project_name: str) -> str:\n        if not context_name:\n            raise ValueError('context name is required.')\n\n        if not is_safe_path(Path.cwd() / 'projects', Path.cwd() / 'projects' / project_name):\n            rich.print(f\"[red bold]Error: Directory traversal detected in project name {project_name}[/red bold]\")\n            os._exit(1)\n\n        path = Path.cwd() / 'projects' / project_name / self.context_directory_path\n        if not path.exists():\n            raise FileNotFoundError(f'Context directory not found at {path}.')\n        return (path / context_name).read_text()\n\n\ndef load_crew_contexts(\n    project_name: str,\n    crew_config: dict,\n    context_reader: IFileReader = ContextFileReader()\n) -> dict[str, str]:\n    \"\"\"Load the contexts of all crews.\"\"\"\n    context: dict[str, str] = {}\n    for context_name, context_file in (crew_config.get('context') or {}).items():\n        context[context_name] = context_reader.read(context_file, project_name)\n    return context\n"}
{"type": "source_file", "path": "create-project.py", "content": "import shutil\nimport argparse\nfrom pathlib import Path\nfrom rich.console import Console\nfrom utils import is_safe_path\nimport rich\nimport os\n\nconsole = Console()\n\ndef strip_code_block(yaml_content):\n    if yaml_content.startswith(\"```yaml\") and yaml_content.endswith(\"```\"):\n        yaml_content = yaml_content[7:-3].strip()\n    return yaml_content\n\ndef create_project_folder(yaml_file, project_name):\n    projects_dir = Path.cwd() / \"projects\"\n    projects_dir.mkdir(parents=True, exist_ok=True)\n\n    project_folder: Path = projects_dir / project_name\n\n    if not is_safe_path(projects_dir, project_folder):\n        rich.print(f\"[bold red]Error: Path traversal detected in project name {project_name}[/bold red]\")\n        os._exit(1)\n\n    project_folder.mkdir(exist_ok=True)\n\n    destination_file = project_folder / \"execution.yaml\"\n\n    with open(yaml_file, 'r') as file:\n        yaml_content = file.read()\n    \n    yaml_content = strip_code_block(yaml_content)\n\n    with open(destination_file, 'w') as file:\n        file.write(yaml_content)\n\n    console.print(f\"[green]Project folder '{project_name}' created successfully.[/green]\")\n    console.print(f\"[blue]YAML file copied to '{destination_file}'.[/blue]\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Create a project folder and copy a YAML file to it.\")\n    parser.add_argument(\"yaml_file\", type=Path, help=\"The path to the YAML input file.\")\n    parser.add_argument(\"project_name\", type=str, help=\"The name of the project.\")\n    \n    args: argparse.ArgumentParser = parser.parse_args()\n    \n    if not args.yaml_file.is_file():\n        console.print(f\"[red]Error: The file '{args.yaml_file}' does not exist.[/red]\")\n        exit(1)\n\n    create_project_folder(args.yaml_file, args.project_name)\n"}
{"type": "source_file", "path": "execution/graph.py", "content": "import networkx as nx\n\n\ndef get_crews_execution_order(execution_config: dict) -> list[str]:\n    \"\"\"Get the order of execution of the crews by the 'depends_on' key.\n\n    Structure:\n    {\n        'research': {}\n        'crews': {\n            'fetch': {\n                'depends_on': ['research']\n            },\n            'parse': {\n                'depends_on': ['fetch', 'research']\n            }\n        }\n    }\n\n    > The order of execution of the crews is: research, fetch, parse\n\n    Run the crew only if the crews it depends on have been executed.\n    Check that the resulted graph is Directed Acyclic Graph (DAG).\n    \"\"\"\n    G = nx.DiGraph()\n    crews = execution_config['crews']\n\n    for crew in crews:\n        G.add_node(crew)\n\n    for crew, crew_config in crews.items():\n        for dependency in crew_config.get('depends_on') or []:\n            G.add_edge(dependency, crew)  # Directed edge: dependency -> crew\n\n    if not nx.is_directed_acyclic_graph(G):\n        raise nx.NetworkXUnfeasible(\"The graph is not a Directed Acyclic Graph (DAG).\")\n\n    execution_order = list(nx.topological_sort(G))\n\n    return execution_order\n"}
{"type": "source_file", "path": "tools/custom/fetch_file_content_tool.py", "content": "import subprocess\nfrom crewai_tools import BaseTool\nfrom pathlib import Path\nclass GitFileContentQueryTool(BaseTool):\n    \"\"\"A tool that fetches file content from a local git repository.\"\"\"\n    name: str = \"GitFileContentQueryTool\"\n    description: str = (\n        \"This tool fetches the content of a file from a local git repository.\"\n    )\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def _run(self, file_path: str, repo_path: str) -> str:\n        \"\"\"Use the GitFileContentQueryTool.\"\"\"\n        return self.fetch_file_from_git(file_path=file_path, repo_path=repo_path)\n\n    def fetch_file_from_git(self, file_path: str, repo_path: str) -> str:\n        \"\"\"\n        Fetches the content of a file from a local git repository.\n\n        Args:\n            file_path (str): The path to the file within the repository.\n            repo_path (str): The path to the git repository.\n\n        Returns:\n            str: The content of the file as a string, or an error message if the operation fails.\n\n        Raises:\n            FileNotFoundError: If the git repository does not exist at the provided path.\n            subprocess.CalledProcessError: If the git command fails.\n        \"\"\"\n        # Construct the full path to the file in the git repository\n        full_file_path = Path(repo_path) / file_path\n\n        # Construct the git command to show the file content\n        git_command = [\"git\", \"-C\", repo_path, \"show\", f\"HEAD:{file_path}\"]\n\n        try:\n            # Execute the git command using subprocess\n            result = subprocess.run(git_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, text=True)\n\n            # Return the standard output which contains the file content\n            return result.stdout\n        except FileNotFoundError:\n            # Raise an error if the git repository does not exist\n            raise FileNotFoundError(f\"The specified repository path does not exist: {repo_path}\")\n        except subprocess.CalledProcessError as e:\n            # Return the standard error output if the git command fails\n            return e.stderr\n"}
{"type": "source_file", "path": "projects/tool-generator/context/example_tool.py", "content": "import os\nfrom crewai_tools import BaseTool\nfrom utils import get_embedchain_settings\nfrom embedchain import App\nfrom typing import Optional\n\nclass WebsiteContentQueryTool(BaseTool):\n    \"\"\"A tool that fetches website content, adds it to a vector database, and queries it.\"\"\"\n    name: str = \"WebsiteContentQueryTool\"\n    app: Optional[App] = None\n    description: str = (\n        \"This tool fetches the content of a website, adds it to a vector database, and queries the vector database for a given query string.\"\n    )\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(self, app: 'App', **kwargs):\n        super().__init__(**kwargs)\n        if app and isinstance(app, App):\n            self.app = app\n\n    def _run(self, url: str, query: str) -> str:\n        \"\"\"Use the WebsiteContentQueryTool.\"\"\"\n        return self.query_website_content(url=url, query=query)\n\n    def query_website_content(self, url: str, query: str) -> str:\n        \"\"\"\n        Fetches the content of a website, adds it to a vector database, and queries the vector database for a given query string.\n\n        Parameters:\n        url (str): The URL of the website to fetch content from.\n        query (str): The query string to search in the vector database.\n\n        Returns:\n        str: The result from the vector database query.\n\n        Raises:\n        Exception: If there is an issue with fetching website content or querying the vector database.\n        \"\"\"\n        # Fetch the content of the website\n        try:\n            if not self.app:\n                config = get_embedchain_settings(task_id='shared',\n                                                 llm_name=os.getenv('LLM_NAME'),\n                                                 embedder_name=os.getenv('EMBEDDER_NAME'))\n                self.app = App.from_config(config=config)\n            self.app.add(url, data_type='web_page')\n            results = self.app.query(query)\n        except Exception as e:\n            raise Exception(f\"Failed to fetch website content: {e}\")\n\n        # Return the result from the vector database\n        return str(results)\n"}
{"type": "source_file", "path": "tools/custom/git_search_tool.py", "content": "import subprocess\nfrom crewai_tools import BaseTool\nfrom pathlib import Path\nclass GitSearchTool(BaseTool):\n    \"\"\"A tool that searches for a query string within a local git repository.\"\"\"\n    name: str = \"GitSearchTool\"\n    description: str = (\n        \"\"\"\n        This tool executes a git search command in a local git repository folder using the provided query string.\n        The syntax for the query string is the same as the syntax for the `git grep` command.\n        \"\"\"\n    )\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def _run(self, query: str, repo_path: str) -> str:\n        \"\"\"Use the GitSearchTool.\"\"\"\n        return self.git_search(query=query, repo_path=repo_path)\n\n    def git_search(self, query: str, repo_path: str) -> str:\n        \"\"\"\n        Executes a git search command in the local git repository folder using the provided query string.\n\n        Parameters:\n        query (str): The query string to search for in the git repository. The syntax is the same as the `git grep` command.\n        repo_path (str): The path to the local git repository.\n\n        Returns:\n        str: The result of the git search command as a string.\n\n        Raises:\n        Exception: If there is an issue with executing the git search command.\n        \"\"\"\n        try:\n            # Construct the git search command\n            repo_path = Path(repo_path).resolve()\n            command = ['git', '-C', repo_path, 'grep', '-n', query]\n\n            # Execute the command using subprocess\n            result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n            # Check if the command was successful\n            if result.returncode == 0:\n                # Return the standard output of the git command\n                return result.stdout\n            else:\n                # Return the standard error if the command failed\n                return result.stderr\n        except Exception as e:\n            # Return the exception message if an error occurs\n            raise Exception(f\"Failed to execute git search: {e}\")\n\n"}
{"type": "source_file", "path": "tools/custom/pr_details.py", "content": "from typing import Type, Any\nfrom pydantic.v1 import BaseModel, Field\nfrom crewai_tools import BaseTool\nfrom github import Github\nimport requests\nimport os\n\nclass GitHubPRDetailsSchema(BaseModel):\n    \"\"\"Input schema for GitHub PR Details Fetch Tool.\"\"\"\n    gh_repo: str = Field(..., description=\"Full name of the repository (e.g., 'user/repo')\")\n    pr_number: int = Field(..., description=\"Number of the pull request\")\n\nclass GitHubPRDetailsTool(BaseTool):\n    name: str = \"Fetch GitHub PR Details\"\n    description: str = \"A tool that fetches details of a specific pull request from GitHub.\"\n    args_schema: Type[BaseModel] = GitHubPRDetailsSchema\n    gh_repo: str = \"default/repo\"  # Default GitHub repository\n    pr_number: int = 1             # Default PR number\n\n    def _run(self, **kwargs: Any) -> Any:\n        # Fetching GitHub repository and PR number from the provided arguments or defaults\n        gh_repo = kwargs.get('gh_repo', self.gh_repo)\n        pr_number = kwargs.get('pr_number', self.pr_number)\n\n        # Initializing GitHub client with an environment variable token\n        gh = Github(os.getenv('GITHUB_TOKEN'))\n        \n        # Getting the repository and pull request\n        repo = gh.get_repo(gh_repo)\n        pr = repo.get_pull(pr_number)\n\n        # Headers for the request to fetch diff content\n        headers = {\n            'Authorization': f'Bearer {os.getenv('GITHUB_TOKEN')}',\n            'Accept': 'application/vnd.github.diff'  # Media type for diff content\n        }\n\n        # Making a GET request to the diff_url with the necessary headers\n        diff_response = requests.get(f'https://api.github.com/repos/{gh_repo}/pulls/{pr_number}.diff',\n                                     headers=headers)\n\n        if diff_response.status_code == 200:\n            diff_content = diff_response.text\n            # Excluding certain files from the diff content (WIP: support pagination for large diffs)\n            excluded_files = [] # e.g., 'requirements.txt' (may have many hashes causing large diffs)\n            diff_lines = diff_content.split('\\n')\n            filtered_diff_lines = []\n            skip_file = False\n\n            for line in diff_lines:\n                if line.startswith('diff --git'):\n                    file_name = line.split(' ')[-1].replace('b/', '')\n                    skip_file = file_name in excluded_files\n\n                if not skip_file:\n                    filtered_diff_lines.append(line)\n\n            diff_content = '\\n'.join(filtered_diff_lines)\n        else:\n            diff_content = f\"Failed to fetch diff: HTTP {diff_response.status_code} - {diff_response.reason}\"\n\n        # Constructing the details of the pull request\n        pr_details = {\n            'title': pr.title,\n            'description': pr.body,\n            'diff_url': pr.diff_url,\n            'diff_content': diff_content,\n            'comments': [{'user': comment.user.login, 'body': comment.body} for comment in pr.get_issue_comments()],\n            'review_comments': [{'user': comment.user.login, 'body': comment.body} for comment in pr.get_review_comments()]\n        }\n\n        return pr_details\n"}
{"type": "source_file", "path": "models.py", "content": "import os\nfrom pathlib import Path\nimport pydantic\nimport yaml\nfrom execution.consts import BENCHMARK_CONFIG_PATH\nfrom execution.consts import EXIT_ON_ERROR\nfrom utils import is_safe_path\nimport rich\nimport rich.padding\n\nclass RuntimeSettings(pydantic.BaseModel):\n    project_name: str\n    benchmark_mode: bool = False\n    ignore_cache: bool = False\n\n    def load_benchmark_file(self) -> dict:\n        \"\"\"Load the benchmark file for the project.\n\n        File should be located at `projects/{project_name}/benchmark.yml`\n\n        Represents all the executions that should be run, and the user inputs for each execution (external inputs).\n\n        Structure:\n            ```yaml\n                executions:\n                  - user_inputs:\n                    x: y\n                    ...\n            ```\n        \"\"\"\n        try:\n            if not is_safe_path(Path.cwd() / 'projects', Path.cwd() / 'projects' / self.project_name / BENCHMARK_CONFIG_PATH):\n                if EXIT_ON_ERROR:\n                    rich.print(\n                        rich.padding.Padding(\n                            f\"[bold red]Error: Directory traversal detected in project name[/bold red]\",\n                            (2, 4),\n                            expand=True,\n                            style=\"bold red\",\n                        )\n                    )\n                    os._exit(1)\n                else:\n                    raise FileNotFoundError(\"Directory traversal detected in project name\")\n            \n            with open(\n                Path.cwd() / 'projects' / self.project_name / BENCHMARK_CONFIG_PATH, 'r'\n            ) as file:\n                return yaml.safe_load(file)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Benchmark file not found for project {self.project_name}\")\n"}
{"type": "source_file", "path": "execution/orchestrator.py", "content": "from pathlib import Path\n\nimport rich\nimport yaml\n\nfrom execution.consts import EXECUTION_CONFIG_PATH\nfrom execution.crews.builder import CrewRunner\nfrom execution.graph import get_crews_execution_order\nfrom utils import get_clients\nfrom utils import sanitize_filename\nfrom utils import is_safe_path\nimport os\nfrom utils import validate_env_vars\nvalidate_env_vars('LLM_NAME', 'EMBEDDER_NAME')\n\ndef execute_crews(project_name: str,\n                  user_inputs: dict = None,\n                  validations: dict = None,\n                  ignore_cache: bool = False):\n    \"\"\"Execute crews in the order defined in the execution config.\"\"\"\n    if not user_inputs:\n        user_inputs = {}\n\n    if not is_safe_path(Path.cwd() / 'projects', Path.cwd() / 'projects' / project_name):\n        rich.print(\n            f\"[bold red]Error: Path traversal detected in project name: {project_name}[/bold red]\"\n        )\n        os._exit(1)\n\n    execution_config: dict = get_execution_config(project_name)\n    llm_name: str = os.getenv('LLM_NAME')\n    embedder_name: str = os.getenv('EMBEDDER_NAME')\n    llm, embedding_model = get_clients(llm_name, embedder_name)\n    execution_order: list[str] = get_crews_execution_order(execution_config)\n\n    rich.print(\n        f'[bold white]'\n        f'Starting execution with the following order: {execution_order}\\n'\n        f'[/bold white]'\n    )\n\n    crews_results: dict = {}\n    for acting_crew in execution_order:\n        crew_config: dict = execution_config['crews'][acting_crew]\n        rich.print(f\"[white bold]Running crew <{acting_crew}> [/white bold]\")\n        result: str = CrewRunner(\n            project_name=project_name,\n            crew_name=acting_crew,\n            crew_config=crew_config,\n            user_inputs=user_inputs,\n            previous_crews_results=crews_results,\n            llm=llm,\n            embedding_model=embedding_model,\n            should_export_results=(execution_config.get('settings') or {}).get('output_results'),\n            ignore_cache=ignore_cache,\n        ).run_crew()\n        crews_results[acting_crew] = result\n        if validations and acting_crew in validations:\n            from crewai import Task, Agent, Crew\n            import textwrap\n            validations_compare_to = validations[acting_crew]['compare_to']\n            compare_to_filename: Path = (\n                Path.cwd()\n                / 'projects'\n                / project_name\n                / 'validations'\n                / validations_compare_to\n            )\n            if compare_to_filename.exists():\n                if is_safe_path(Path.cwd() / 'projects' / project_name, compare_to_filename):\n                    with open(compare_to_filename, 'r') as file:\n                        # validations_compare_to is a filename, overwrite var with its content to be used below\n                        validations_compare_to = file.read()\n                    validation_results_filename: Path = Path(f'{compare_to_filename}.result') # no need to sanitize filename or check path traversal as just adding an extension to validated path.\n                else:\n                    rich.print(\n                        f\"[bold red]Error: Path traversal detected in {compare_to_filename}[/bold red]\"\n                    )\n                    os._exit(1)\n            else:\n                input_values_filename = f'{sanitize_filename(\"_\".join(user_inputs.values()))}.result'\n                validation_results_filename: Path = Path(\n                    Path.cwd()\n                    / 'projects'\n                    / project_name\n                    / 'validations'\n                    / input_values_filename\n                )\n                if not is_safe_path(Path.cwd() / 'projects' / project_name, validation_results_filename):\n                    rich.print(\n                        f\"[bold red]Error: Path traversal detected in {validation_results_filename}[/bold red]\"\n                    )\n                    os._exit(1)\n            \n            metrics = validations[acting_crew]['metrics']\n\n            agent = Agent(\n                role = 'Software QA Engineer',\n                goal = 'Validate the results of the crew',\n                backstory = \"\"\"You are a Software QA Engineer who is responsible for validating the results of the crew.\"\"\",\n                tools = [],\n                llm = llm,\n            )\n            task =Task(\n                description = textwrap.dedent(f\"\"\"\\\n                    IMPORTANT INSTRUCTIONS:\n                    -----------------------\n                    - output MUST be in json format without any additional text (output is used by other tools - !!!NOT ENCLOSED IN JSON CODE BLOCK!!!).\n                    - output MUST contain a boolean result for each check.\n                    - output MUST NOT include any text other than the json object!!\n\n                    for each of the following checks:\n                    <<<<METRICS_START_MARKER>>>>\n                    {metrics}\n                    <<<<METRICS_END_MARKER>>>>\n                    compare the result with the expected output and indicate for each check if it succeeded or not.\n\n                    <<<<RESULT_START_MARKER>>>>\n                    {result}\n                    <<<<RESULT_END_MARKER>>>>\n\n                    <<<<EXPECTED_OUTPUT_START_MARKER>>>>\n                    {validations_compare_to}\n                    <<<<EXPECTED_OUTPUT_END_MARKER>>>>\n                \"\"\"),\n                expected_output = textwrap.dedent(\n                    f\"\"\"direct json string (not enclosed in json code-block) with the following structure (\n                        failure requires reason, success does not):\n                        -----------------------\n                        {{check_endpoint: {{res: false, reason: \"the version of the API endpoint URL. The result uses `/v3/admin/users/` while the expected output uses `/v2/admin/users/`\"}}, check_something_else: {{res: false, reason: 'succinct reason for failue'}}, check_another_thing: {{res: true}}...}}\n                        -----------------------\n                        \n                        IMPORTANT INSTRUCTIONS:\n                        -----------------------\n                        - Your response MUST be in json format without any additional text (output is used by other tools - !!!NOT ENCLOSED IN JSON CODE BLOCK!!!).\n                        - Example response is the text above enclosed between horizontal lines (without the lines).\n                        - Ensure the output is a direct json string (not enclosed in json code-block).\n                        - Ensure there is no text before or after the json object.\n                        - You MUST provide comparison reason for each failed check - i.e., what is the difference between the actual and expected output for the specific check.\n                        - Reason MUST be succinct and clear.\n                        \"\"\"),\n                tools = [],\n                agent = agent,\n            )\n            crew = Crew(\n                agents = [agent],\n                tasks = [task],\n                verbose = 2,\n            )\n            validation_result = crew.kickoff()\n            if not validation_results_filename.parent.exists():\n                validation_results_filename.parent.mkdir(parents=True)\n\n            if not is_safe_path(Path.cwd() / 'projects' / project_name, validation_results_filename):\n                rich.print(\n                    f\"[bold red]Error: Path traversal detected in {validation_results_filename}[/bold red]\"\n                )\n                os._exit(1)\n\n            with open(validation_results_filename, 'w') as file:\n                file.write(validation_result)\n\n\ndef get_execution_config(project_name: str) -> dict:\n    if not is_safe_path(Path.cwd() / 'projects', Path.cwd() / 'projects' / project_name / EXECUTION_CONFIG_PATH):\n        rich.print(\n            f\"[bold red]Error: Directory traversal detected in project name: {project_name} [/bold red]\"\n        )\n        os._exit(1)\n\n    with open(\n        Path.cwd() / 'projects' / project_name / EXECUTION_CONFIG_PATH, 'r'\n    ) as file:\n        execution_config: dict = yaml.safe_load(file)\n    return execution_config\n"}
{"type": "source_file", "path": "main.py", "content": "import crews_control\n\nif __name__ == \"__main__\":\n    crews_control.main()\n"}
{"type": "source_file", "path": "tools/custom/create_jira_issue.py", "content": "from typing import Any, Type\nfrom crewai_tools import BaseTool\nfrom pydantic.v1 import BaseModel, Field\nfrom jira import JIRA\nimport os\n\nclass JiraTicketSchema(BaseModel):\n    \"\"\"Input schema for Jira Ticket Creation Tool.\"\"\"\n    summary: str = Field(..., description=\"Summary of the Jira ticket\")\n    description: str = Field(..., description=\"Description of the Jira ticket\")\n    status: str = Field(..., description=\"Status of the Jira ticket\")\n\nclass JiraTicketCreationTool(BaseTool):\n    name: str = \"Create Jira Ticket\"\n    description: str = \"A tool that creates a Jira ticket.\"\n    args_schema: Type[BaseModel] = JiraTicketSchema\n    summary: str = \"Default Summary\"  # Default summary for the Jira ticket\n    description: str = \"Default Description\"  # Default description for the Jira ticket\n    status: str = \"New\"  # Default status for the Jira ticket\n\n    def _run(self, **kwargs: Any) -> Any:\n        summary = kwargs.get('summary', self.summary)\n        description = kwargs.get('description', self.description)\n        status = kwargs.get('status', self.status)\n        jira_server = os.getenv('JIRA_INSTANCE_URL')\n        jira_username = os.getenv('JIRA_USERNAME')\n        jira_password = os.getenv('JIRA_API_TOKEN')\n        jira_project_key = os.getenv('JIRA_CREATE_ISSUE_PROJECT_KEY')\n\n        jira = JIRA(server=jira_server, basic_auth=(jira_username, jira_password))\n\n        try:\n            # Create the issue\n            new_issue = jira.create_issue(project=jira_project_key,\n                                           summary=summary,\n                                           description=description,\n                                           issuetype={'name': 'Task'})\n            print(f\"Jira ticket created successfully: {new_issue}\")\n            transitions = jira.transitions(new_issue)\n            available_transitions = [(t['name'], t['id']) for t in transitions]\n            print(f\"Available transitions: {available_transitions}\")\n            transition_map = dict(available_transitions)\n            transition_id = transition_map.get(status)\n\n            if transition_id:\n                jira.transition_issue(new_issue, transition_id)\n                print(f\"Issue transitioned to {status}\")\n            else:\n                print(f\"No transition found with name {status}\")\n\n            print(f\"Jira ticket status updated to: {status}\")\n            return {'ticket_key': new_issue.key,\n                    'status': status,\n                    'link': f'{jira_server}/browse/{new_issue.key}'}\n        except Exception as e:\n            print(f\"Error creating Jira ticket: {e}\")\n            return {'error': str(e), 'status': 'Failed'}\n"}
{"type": "source_file", "path": "crews_control.py", "content": "from dotenv import load_dotenv\nload_dotenv()\n\nimport argparse\nimport rich\nfrom rich.padding import Padding\nimport os\nfrom execution.inputs import get_user_inputs, validate_user_inputs\nfrom execution.orchestrator import execute_crews, get_execution_config\nfrom models import RuntimeSettings\nfrom pathlib import Path\nfrom execution.consts import EXECUTION_CONFIG_PATH\nfrom utils import EnvironmentVariableNotSetError, is_safe_path\n\nclass KeyValueAction(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, {})\n        for value in values:\n            key, value = value.split('=')\n            getattr(namespace, self.dest)[key] = value\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(\"crews_control\")\n\n    parser.add_argument(\"--project-name\", help=\"The name of the project to run.\", type=str)\n    parser.add_argument(\"--ignore-cache\", help=\"Ignore the cache and run all crews\", action=\"store_true\")\n    parser.add_argument('--params', nargs='+', action=KeyValueAction, help='List of key=value pairs')\n    \n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--list-tools\", help=\"List available tools\", action=\"store_true\")\n    group.add_argument(\"--list-models\", help=\"List available models\", action=\"store_true\")\n    group.add_argument(\"--list-projects\", help=\"List available projects\", action=\"store_true\")\n    group.add_argument(\"--benchmark\", help=\"Run the project from benchmark file (`benchmark.yml`)\", action=\"store_true\")\n    \n    args = parser.parse_args()\n\n    # Ensure project name is provided if not listing\n    if not (args.list_tools or args.list_models or args.list_projects):\n        if not args.project_name:\n            parser.error(\"--project-name is required for execution and benchmark\")\n\n    return args\n\ndef handle_list_arguments(args):\n    if args.list_tools:\n        from utils import list_tools\n        list_tools()\n        os._exit(0)\n    elif args.list_models:\n        from utils import list_models\n        list_models()\n        os._exit(0)\n    elif args.list_projects:\n        from utils import list_projects\n        list_projects()\n        os._exit(0)\n\ndef display_error(message):\n    rich.print(Padding(f\"[bold red]Error: {message}[/bold red]\", (2, 4), expand=True, style=\"bold red\"))\n    os._exit(1)\n\ndef display_message(message):\n    rich.print(Padding(f\"[bold white]{message}[/bold white]\", (2, 4), expand=True, style=\"bold white\"))\n\ndef execute_project(runtime_settings, execution_config, user_inputs=None, validations=None):\n    try:\n        validate_user_inputs(user_inputs=user_inputs or {}, execution_config=execution_config)\n    except ValueError as e:\n        display_error(str(e))\n    execute_crews(\n        project_name=runtime_settings.project_name,\n        user_inputs=user_inputs or get_user_inputs(execution_config),\n        validations=validations,\n        ignore_cache=runtime_settings.ignore_cache\n    )\n\ndef main():\n    args = parse_arguments()\n    handle_list_arguments(args)\n\n    runtime_settings = RuntimeSettings(\n        project_name=args.project_name,\n        benchmark_mode=args.benchmark,\n        ignore_cache=args.ignore_cache\n    )\n\n    project_path = Path.cwd() / 'projects' / runtime_settings.project_name\n    if not project_path.exists():\n        display_error(f\"Project {runtime_settings.project_name} not found\")\n\n    try:\n        execution_config = get_execution_config(project_name=runtime_settings.project_name)\n    except FileNotFoundError:\n        display_error(f\"{EXECUTION_CONFIG_PATH} file not found for project {runtime_settings.project_name}\")\n\n    display_message(f\"Welcome to {runtime_settings.project_name}™\")\n\n    try:\n        if runtime_settings.benchmark_mode:\n            from utils import report_success_percentage\n            benchmark_settings = runtime_settings.load_benchmark_file()\n            for index, execution in enumerate(benchmark_settings.get('executions') or []):\n                rich.print(f\"[grey]Running benchmark execution: <{index}>[/grey]\")\n                execute_project(runtime_settings, execution_config, execution.get('user_inputs'), execution.get('validations'))\n        elif args.params:\n            user_inputs = {k: v for k, v in args.params.items()}\n            execute_project(runtime_settings, execution_config, user_inputs)\n        else:\n            # Interactive mode: get user inputs interactively\n            user_inputs = get_user_inputs(execution_config)\n            execute_project(runtime_settings, execution_config, user_inputs)\n    except (FileNotFoundError, EnvironmentVariableNotSetError) as e:\n        display_error(str(e))\n\n    if runtime_settings.benchmark_mode:\n        if not is_safe_path(Path.cwd() / 'projects', Path.cwd() / 'projects' / runtime_settings.project_name / 'validations'):\n            display_error(f\"Path traversal detected in project name {runtime_settings.project_name}\")\n        \n        report_success_percentage(f\"projects/{runtime_settings.project_name}/validations\")\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "tools/custom/human.py", "content": "# https://github.com/langchain-ai/langchain/commit/4c087e2bf77c520f300a5cec5424660ad740f41f\n\"\"\"Tool for asking human input.\"\"\"\n\nfrom typing import Callable\nfrom pydantic import Field\nfrom langchain.tools.base import BaseTool\n\n\ndef _print_func(text: str) -> None:\n    print(\"\\n\")\n    print(text)\n\ndef input_func() -> str:\n    print(\"Insert your text. Press Ctrl-D (or Ctrl-Z on Windows) to end.\")\n    contents = []\n    while True:\n        try:\n            line = input()\n        except EOFError:\n            break\n        contents.append(line)\n    return \"\\n\".join(contents)\n\nclass HumanTool(BaseTool):\n    \"\"\"Tool that adds the capability to ask user for multi line input.\"\"\"\n\n    name = \"HumanTool\"\n    description = (\n        \"You can ask a human for guidance when you think you\"\n        \" got stuck or you are not sure what to do next.\"\n        \" The input should be a question for the human.\"\n        \" This tool version is suitable when you need answers that span over\"\n        \" several lines.\"\n    )\n    prompt_func: Callable[[str], None] = _print_func\n    input_func: Callable[[], str] = input_func\n\n    def _run(self, query: str) -> str:\n        \"\"\"Use the Multi Line Human input tool.\"\"\"\n        self.prompt_func(query)\n        return self.input_func()\n\n    async def _arun(self, query: str) -> str:\n        \"\"\"Use the Multi Line Human tool asynchronously.\"\"\"\n        raise NotImplementedError(\"Human tool does not support async\")\n"}
{"type": "source_file", "path": "tools/custom/find_method_implementation.py", "content": "from typing import Type, Any\nfrom crewai_tools import BaseTool\nfrom pydantic.v1 import BaseModel, Field\nfrom github import Github, Repository\nimport os\nimport ast\n\nclass FindMethodImplementationSchema(BaseModel):\n    \"\"\"Input schema for Find Method Implementation Tool.\"\"\"\n    repo_name: str = Field(..., description=\"Full name of the repository (e.g., 'user/repo')\")\n    initial_class_name: str = Field(..., description=\"Initial class name to start the search from\")\n    method_name: str = Field(..., description=\"Method name to search for\")\n    branch: str = Field(default='main', description=\"Branch to search in\")\n\nclass FindMethodImplementationTool(BaseTool):\n    name: str = \"Find Method Implementation Tool\"\n    description: str = \"A tool that searches for the actual implementation of a method in a class hierarchy.\"\n    args_schema: Type[BaseModel] = FindMethodImplementationSchema\n\n    def _run(self, **kwargs: Any) -> Any:\n        repo_name = kwargs.get('repo_name')\n        initial_class_name = kwargs.get('initial_class_name')\n        method_name = kwargs.get('method_name')\n        branch = kwargs.get('branch', 'main')\n\n        gh = Github(os.getenv('GITHUB_TOKEN'))\n        repo = gh.get_repo(repo_name)\n        result = self.find(github=gh,\n                           repo=repo,\n                           initial_class_name=initial_class_name,\n                           method_name=method_name,\n                           branch=branch)\n        return result\n\n    def find(self,\n             github: Github,\n             repo: Repository,\n             initial_class_name: str,\n             method_name: str,\n             branch: str='main') -> str:\n        \"\"\"\n        Recursively search for the actual implementation of a method in a class hierarchy using AST\n        and returns a formatted string containing both the method source and class name.\n        \"\"\"\n        current_class_name = initial_class_name\n\n        while current_class_name:\n            class_node, class_content, file_path = fetch_class_definition(github=github,\n                                                                          repo=repo,\n                                                                          class_name=current_class_name,\n                                                                          branch=branch)\n            if class_node:\n                for node in ast.iter_child_nodes(class_node):\n                    if isinstance(node, ast.FunctionDef) and node.name == method_name:\n                        source = ast.get_source_segment(class_content, node)\n                        return f\"Method implementation found in class `{current_class_name}` in file `{file_path}`:\\n\" \\\n                               f\"------------------\\n\" \\\n                               f\"{source}\\n\" \\\n                               f\"------------------\"\n                # Navigate to parent class if method not found\n                for base in class_node.bases:\n                    if isinstance(base, ast.Name):\n                        current_class_name = base.id\n                        break\n                else:\n                    break  # No valid parent class to search in\n            else:\n                break\n        return f\"Method implementation not found in any class derived from {initial_class_name}.\"\n\n# The following functions are used by the tool and should be included in the same module.\n\ndef find_class_file(github: Github, repo: Repository, class_name: str, branch: str='main'):\n    \"\"\"\n    Search for a file containing the specified class definition using GitHub's search API.\n    \n    :param repo: Repository object.\n    :param class_name: Class name to search for.\n    :param branch: Branch name to fetch from.\n    :return: File path containing the class definition, or None if not found.\n    \"\"\"\n    query = f'repo:{repo.full_name} \"class {class_name}(\" language:python'\n    results = github.search_code(query, order='desc')\n    for file in results:\n        if file.path.endswith('.py'):\n            return file.path\n    return None\n\ndef get_ast_from_code(code: str):\n    try:\n        return ast.parse(code)\n    except SyntaxError:\n        return None\n\ndef find_class_in_ast(ast_tree: ast.AST, class_name: str):\n    \"\"\"\n    Find an AST node for the specified class within the given AST tree.\n    \"\"\"\n    for node in ast.walk(ast_tree):\n        if isinstance(node, ast.ClassDef) and node.name == class_name:\n            return node\n    return None\n\ndef fetch_class_definition(github: Github, repo: Repository, class_name: str, branch: str='main') -> tuple:\n    \"\"\"\n    Fetch the class definition from a file located by searching the repository and parse it to find the AST node of the class.\n    \n    :param repo: Repository object.\n    :param class_name: Class name to fetch.\n    :param branch: Branch name to fetch from.\n    :return: Tuple containing the AST node of the class, the class content, and the file path.\n    \"\"\"\n    file_path = find_class_file(github=github, repo=repo, class_name=class_name, branch=branch)\n    if file_path:\n        contents = repo.get_contents(file_path, ref=branch)\n        file_content = contents.decoded_content.decode('utf-8')\n        ast_tree = get_ast_from_code(file_content)\n        class_node = find_class_in_ast(ast_tree, class_name)\n        if class_node:\n            return (class_node, file_content, file_path)\n    return (None, None, None)"}
{"type": "source_file", "path": "tools/custom/github_search.py", "content": "from crewai_tools import BaseTool\nfrom github import Github, GithubException\nimport os\nimport time\nimport ast\nimport datetime\n\nMAX_CONTENT_LEN = 10000\nSNIPPET_LEN = 1000\n\nclass GitHubSearchTool(BaseTool):\n    \"\"\"A tool that searches for code snippets in a GitHub repository.\"\"\"\n    name: str = \"GitHubSearchTool\"\n    description: str = (\n        \"\"\"\n**Tool Name: GitHubSearchTool**\n\n**Description:**\nUse this tool to search for specific query terms within a file in a GitHub repository. Follow these rules and examples to ensure correct usage:\n\n1. **General Rules:**\n   - Always include at least one search term when searching source code.\n   - Avoid using wildcard characters: `. , : ; / \\\\ ` ' \" = * ! ? # $ & + ^ | ~ < > ( ) { } [ ] @`.\n   - Use qualifiers to refine your search:\n     - **in:file** - Search within file contents. Example: `\"octocat in:file\"`\n     - **in:path** - Search within file paths. Example: `\"octocat in:path\"`\n     - **in:file,path** - Search within both file contents and paths. Example: `\"octocat in:file,path\"`\n\n2. **Path Qualifiers:**\n   - **path:/** - Search files at the root level. Example: `\"octocat filename:readme path:/\"`\n   - **path:DIRECTORY** - Search files in a specific directory. Example: `\"form path:cgi-bin language:perl\"`\n   - **path:PATH/TO/DIRECTORY** - Search files in a specific directory and its subdirectories. Example: `\"console path:app/public language:javascript\"`\n\n3. **Language Qualifiers:**\n   - Specify the language to refine your search. Example: `\"element language:xml\"`\n\n4. **Size Qualifiers:**\n   - Filter results based on file size. Example: `\"function size:>10000 language:python\"`\n\n5. **Filename Qualifiers:**\n   - Search for files with a specific name. Example: `\"filename:linguist\"`\n   - Combine with path and language qualifiers. Example: `\"filename:test_helper path:test language:ruby\"`\n\n6. **Extension Qualifiers:**\n   - Search for files with a specific extension. Example: `\"icon size:>200000 extension:css\"`\n\n**Examples:**\n- **Simple Search:** `\"COPY filename:Dockerfile repo:Axonius/crews-control\"`\n- **Complex Search:** `\"COPY in:file filename:Dockerfile path:/src repo:Axonius/crews-control\"`\n\n**Error Handling:**\n- If you encounter a query parsing error (422), check for disallowed special characters and ensure the query includes at least one valid search term.\n        \"\"\"\n        )\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n    \n    def _run(self, repo_name: str, search_query: str) -> str:\n        \"\"\"Use the GitHubSearchTool.\"\"\"\n        gh = Github(os.getenv('GITHUB_TOKEN'))\n        query = f'{search_query} repo:{repo_name}'\n        return self.execute_search(query=query, gh=gh)\n    \n    def execute_search(self, gh: Github, query: str) -> str:\n        try:\n            search_result = gh.search_code(query)\n            code_results = []\n            if search_result.totalCount > 10:\n                error_message = 'Too many results. Please narrow down the search. Returning without file content.'\n                for item in search_result:\n                    if item.path.endswith('.py'):\n                        classes, methods = self.parse_python_code('')\n                    else:\n                        classes, methods = [], []\n                    code_results.append({\n                        'filename': item.path,\n                        'content': error_message,\n                        'classes': classes,\n                        'methods': methods\n                    })\n            else:\n                for item in search_result:\n                    file_content = item.decoded_content.decode('utf-8')\n                    if item.path.endswith('.py'):\n                        classes, methods = self.parse_python_code(file_content)\n                    else:\n                        classes, methods = [], []\n\n                    if len(file_content) <= MAX_CONTENT_LEN:\n                        content = file_content\n                    else:\n                        if search_result.totalCount > 1:\n                            content = 'file content too large - narrow search to this file only!'\n                        else:\n                            content = file_content[:SNIPPET_LEN] + '\\n\\n...content too large - showing snippet only.'\n\n                    code_results.append({\n                        'filename': item.path,\n                        'content': content,\n                        'classes': classes,\n                        'methods': methods\n                    })\n            \n            return str(code_results)\n        except GithubException as e:\n            if e.status == 403 and 'rate limit' in e.data['message'].lower():\n                print(\"Rate limit exceeded. Handling...\")\n                return self.handle_rate_limit(gh=gh, query=query)\n            else:\n                raise\n\n    def parse_python_code(self, code):\n        tree = ast.parse(code)\n        classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n        methods = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]\n        return classes, methods\n\n    def utc_to_local(self, utc_dt):\n        epoch = time.mktime(utc_dt.timetuple())\n        offset = datetime.datetime.fromtimestamp(epoch) - datetime.datetime.utcfromtimestamp(epoch)\n        return utc_dt + offset\n\n    def handle_rate_limit(self, gh: Github, query: str) -> str:\n        current_time = time.time()\n        local_reset_timestamp: datetime = time.mktime(self.utc_to_local(gh.get_rate_limit().search.reset).timetuple())\n        print(f\"Time until reset: {local_reset_timestamp - current_time} seconds\")\n\n        sleep_time = local_reset_timestamp - current_time + 10  # adding 10 seconds to ensure the limit is reset\n\n        if sleep_time > 0:\n            print(f\"Rate limit exceeded. Sleeping for {sleep_time} seconds.\")\n            time.sleep(sleep_time)\n        else:\n            print(f\"Calculated negative sleep time: {sleep_time} seconds. Reset time might have already passed.\")\n\n        print(\"Retrying the request...\")\n        return self.execute_search(query)\n"}
{"type": "source_file", "path": "utils.py", "content": "import os\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_groq import ChatGroq\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nimport json\nfrom pathlib import Path\nimport re\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nclass EnvironmentVariableNotSetError(Exception):\n    pass\n\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"Sanitize the filename by replacing non-alphanumeric characters with underscores.\"\"\"\n    return re.sub(r'[^a-zA-Z0-9]', '_', filename).lower()\n\ndef is_safe_path(base_dir: Path, path: Path) -> bool:\n    \"\"\"Check if the resolved path is within the base directory to prevent path traversal.\"\"\"\n    try:\n        return path.resolve().is_relative_to(base_dir.resolve())\n    except ValueError:\n        return False\n\ndef validate_env_vars(*vars):\n    # Handle single list or tuple containing a list\n    if len(vars) == 1 and isinstance(vars[0], list):\n        vars = vars[0]\n\n    for var in vars:\n        if os.getenv(var) is None or os.getenv(var) == \"\":\n            raise EnvironmentVariableNotSetError(f\"Environment variable '{var}' is not set.\")\n\ndef create_llm_client(config):\n    provider = config['provider']\n    validate_env_vars(config['required_vars'])\n    \n    if provider == 'groq':\n        return ChatGroq(\n            model=os.getenv(\"GROQ_MODEL_NAME\"),\n            api_key=os.getenv(\"GROQ_API_KEY\"),\n            streaming=config.get('stream', True),\n            max_tokens=config.get('max_tokens', 8192),\n            model_name=os.getenv('GROQ_MODEL_NAME'),\n        )\n    elif provider == 'anthropic':\n        return ChatAnthropic(\n            model=os.getenv(\"ANTHROPIC_MODEL_NAME\"),\n            temperature=config.get('temperature', 0.7),\n            max_tokens=config.get('max_tokens', 1024),\n            timeout=None,\n            max_retries=2,\n        )\n    elif provider == 'azure_openai':\n        return AzureChatOpenAI(\n            temperature=config.get('temperature', 0),\n            openai_api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n            azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        )\n    elif provider == 'openai':\n        from langchain_openai import ChatOpenAI\n        return ChatOpenAI(\n            temperature=config.get('temperature', 0),\n            model=os.getenv(\"OPENAI_MODEL_NAME\"),\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n        )\n    # Add more LLM providers here as needed\n    else:\n        raise ValueError(f\"Unsupported LLM provider: {provider}\")\n\ndef create_embedder_client(config):\n    provider = config['provider']\n    \n    if provider == 'gpt4all':\n        return GPT4AllEmbeddings(provider=\"gpt4all\")\n    elif provider == 'azure_openai':\n        import warnings\n        from langchain_core._api.deprecation import LangChainDeprecationWarning\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The class `AzureChatOpenAI` was deprecated\"\n        )\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The class `AzureOpenAIEmbeddings` was deprecated\"\n        )\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The method `BaseChatModel.__call__` was deprecated\"\n        )\n        validate_env_vars(config['required_vars'])\n        return AzureOpenAIEmbeddings(\n            azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n            openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n        )\n    elif provider == 'huggingface':\n        import warnings\n        warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\")\n\n        return HuggingFaceEmbeddings(\n            model_name=config['config']['model'],\n        )\n    elif provider == 'openai':\n        from langchain_openai import OpenAIEmbeddings\n        import warnings\n        from langchain_core._api.deprecation import LangChainDeprecationWarning\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The class ChatOpenAI was deprecated\"\n        )\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The class OpenAIEmbeddings was deprecated\"\n        )\n        warnings.filterwarnings(\n            \"ignore\",\n            category=LangChainDeprecationWarning,\n            message=\"The method BaseChatModel.__call__ was deprecated\"\n        )\n        validate_env_vars(config['required_vars'])\n        return OpenAIEmbeddings(\n            model=os.getenv(\"OPENAI_EMBEDDING_MODEL_NAME\"),\n            openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n        )\n    # Add more embedder providers here as needed\n    else:\n        raise ValueError(f\"Unsupported embedder provider: {provider}\")\n\ndef get_clients(llm_name: str, embedder_name: str):\n    llm_config_path = Path('config') / 'llms' / f'{llm_name}.json'\n    embedder_config_path = Path('config') / 'embedders' / f'{embedder_name}.json'\n    \n    llm_config = load_config(llm_config_path)\n    embedder_config = load_config(embedder_config_path)\n    \n    llm_client = create_llm_client(llm_config)\n    embedder_client = create_embedder_client(embedder_config)\n    \n    return llm_client, embedder_client\n\ndef load_config(file_path):\n    with open(file_path, 'r') as file:\n        return json.load(file)\n\ndef get_embedchain_settings(task_id: str, llm_name: str, embedder_name: str) -> dict:\n    llm_config_path = Path('config') / 'llms' / f'{llm_name}.json'\n    embedder_config_path = Path('config') / 'embedders' / f'{embedder_name}.json'\n    \n    llm_config = load_config(llm_config_path)\n    embedder_config = load_config(embedder_config_path)\n\n    if llm_name == 'azure_openai':\n        llm, embedder = get_clients(llm_name=llm_name, embedder_name=embedder_name)\n        llm_config['config']['deployment_name'] = getattr(llm, 'deployment_name')\n        llm_config['config']['api_key'] = getattr(llm, 'openai_api_key')\n        embedder_config['config']['deployment_name'] = getattr(embedder, 'deployment')\n        embedder_config['config']['api_key'] = getattr(embedder, 'openai_api_key')\n    elif llm_name == 'openai':\n        llm, embedder = get_clients(llm_name=llm_name, embedder_name=embedder_name)\n        llm_config['config']['api_key'] = getattr(llm, 'openai_api_key')\n        embedder_config['config']['api_key'] = getattr(embedder, 'openai_api_key')\n\n    return {\n        'llm': {\n            'provider': llm_config['provider'],\n            'config': llm_config['config'],\n        },\n        'embedder': {\n            'provider': embedder_config['provider'],\n            'config': embedder_config['config'],\n        },\n        'app': {\n            'config': {\n                'id': task_id,\n                'collect_metrics': False,\n            },\n        },\n    }\n\n\ndef get_validated_input(prompt: str, valid_options: list[str] = None) -> str:\n    while True:\n        user_input = input(prompt).strip()\n        if not user_input:\n            print(\"Input cannot be empty.\")\n        elif valid_options is not None and user_input not in valid_options:\n            print(f\"Please enter a valid option from {valid_options}\")\n        else:\n            return user_input\n\ndef report_success_percentage(folder_path):\n    total_files = 0\n    success_files = 0\n    failed_details = []\n\n    # Iterate through all files in the given directory\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".result\"):\n            total_files += 1\n            path: Path = Path.cwd() / folder_path / filename\n            with open(path, 'r') as file:\n                try:\n                    data = json.load(file)\n                except json.JSONDecodeError:\n                    print(f\"Error reading file {filename}\")\n                    failed_details.append((filename, [\"Error reading file\"]))\n                    continue\n\n                # Check each metric in the file\n                has_failure = False\n                file_failures = []\n                for metric, verdict in data.items():\n                    if not verdict.get(\"res\", False):  # Check if the result is False or absent\n                        has_failure = True\n                        reason = verdict.get(\"reason\", \"No reason provided\")\n                        file_failures.append(f\"{metric}: {reason}\")\n\n                if has_failure:\n                    failed_details.append((filename, file_failures))\n                else:\n                    success_files += 1\n\n    # Calculate success percentage\n    if total_files > 0:\n        success_percentage = (success_files / total_files) * 100\n    else:\n        success_percentage = 100  # Default to 100% if no .result files are found\n\n    print(f\"Success percentage: {success_percentage:.2f}%\")\n    if failed_details:\n        print(\"Failed files and reasons:\")\n        for filename, failures in failed_details:\n            print(f\"{filename}:\")\n            for failure in failures:\n                print(f\"  {failure}\")\n    else:\n        print(\"All files succeeded!\")\n\ndef list_models():\n    def list_model_files(directory, model_type):\n        print(\"-\" * 30)\n        print(f\"Available {model_type} models:\")\n        print(\"-\" * 30)\n        path = Path('config') / directory\n        if not path.exists():\n            print(f\"The directory {path} does not exist.\")\n            return\n\n        for file in path.glob('*.json'):\n            print(f'- {file.stem}')\n        print()\n\n    list_model_files('llms', 'LLM')\n    list_model_files('embedders', 'Embedder')\n\ndef list_tools():\n    print(\"-\" * 30)\n    print(\"Available tools:\")\n    print(\"-\" * 30)\n    from tools.index import _TOOLS_MAP\n    for tool_name in _TOOLS_MAP.keys():\n        print(f'- {tool_name}')\n\ndef list_projects():\n    print(\"-\" * 30)\n    print(\"Available projects:\")\n    print(\"-\" * 30)\n    path = Path('projects')\n    if not path.exists():\n        print(f\"The directory {path} does not exist.\")\n        return\n\n    for project in path.iterdir():\n        if project.is_dir():\n            print(f'- {project.name}')\n    print()\n\n"}
{"type": "source_file", "path": "tools/index.py", "content": "import typing\nfrom typing import Callable\nimport os\n\nfrom crewai_tools.tools.directory_search_tool.directory_search_tool import DirectorySearchTool\nfrom crewai_tools.tools.serper_dev_tool.serper_dev_tool import SerperDevTool\nfrom crewai_tools import SeleniumScrapingTool\n\nfrom langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\nfrom langchain_community.utilities.jira import JiraAPIWrapper\nfrom tools.custom.github_search import GitHubSearchTool\nfrom tools.custom.find_method_implementation import FindMethodImplementationTool\nfrom tools.custom.pr_details import GitHubPRDetailsTool\nfrom tools.custom.create_jira_issue import JiraTicketCreationTool\nfrom tools.custom.website_search_tool import WebsiteContentQueryTool\nfrom tools.custom.human import HumanTool\nfrom tools.custom.website_search_tool import WebsiteContentQueryTool\nfrom tools.custom.git_search_tool import GitSearchTool\nfrom tools.custom.fetch_file_content_tool import GitFileContentQueryTool\nfrom embedchain import App\n\nfrom langchain.agents import load_tools\nfrom utils import validate_env_vars, EnvironmentVariableNotSetError\nfrom utils import get_embedchain_settings\nimport rich\nfrom rich.padding import Padding\n\ntools_requiring_app = {\n    'website_search',\n    'directory_search',\n    'serper',\n}\n\n_TOOLS_MAP: dict[str, Callable] = {\n    'serper': lambda app: SerperDevTool(app=app),\n    'website_search': lambda app: WebsiteContentQueryTool(app=app),\n    'human': lambda: HumanTool(),\n    'read_file': lambda: load_tools(['read_file'])[0],\n    'directory_search': lambda app: DirectorySearchTool(app=app),\n    'jql_query': lambda: jira_toolkit.get_tools()[0], # 'JQL Query\n    'selenium': lambda: SeleniumScrapingTool(),\n    'github_search': lambda: GitHubSearchTool(),\n    'fetch_pr_content': lambda: GitHubPRDetailsTool(),\n    'FindMethodImplementationTool': lambda: FindMethodImplementationTool(),\n    'create_issue': lambda: JiraTicketCreationTool(),\n    'git_search': lambda: GitSearchTool(),\n    'fetch_file_content': lambda: GitFileContentQueryTool(),\n}\n\nrequired_vars = [\n    \"JIRA_API_TOKEN\",\n    \"JIRA_USERNAME\",\n    \"JIRA_INSTANCE_URL\",\n    \"JIRA_CREATE_ISSUE_PROJECT_KEY\",\n    \"GITHUB_TOKEN\",\n    \"SERPER_API_KEY\",\n    \"LLM_NAME\",\n    \"EMBEDDER_NAME\",\n]\n\ntry:\n    validate_env_vars(*required_vars)\nexcept EnvironmentVariableNotSetError as e:\n        rich.print(\n            Padding(\n                f\"[bold red]Error: {str(e)}[/bold red]\",\n                (2, 4),\n                expand=True,\n                style=\"bold red\",\n            )\n        )\n        os._exit(1)\n\njira = JiraAPIWrapper(\n    jira_api_token=os.getenv('JIRA_API_TOKEN'),\n    jira_username=os.getenv('JIRA_USERNAME'),\n    jira_instance_url=os.getenv('JIRA_INSTANCE_URL')\n)\njira_toolkit = JiraToolkit.from_jira_api_wrapper(jira)\n\ndef get_tool(tool_name: str, task_id: typing.Optional[str] = None) -> Callable:\n    try:\n        if tool_name in tools_requiring_app:\n                app = App.from_config(config=get_embedchain_settings(task_id=task_id or 'shared',\n                                                                    llm_name=os.getenv('LLM_NAME'),\n                                                                    embedder_name=os.getenv('EMBEDDER_NAME')))\n                return _TOOLS_MAP[tool_name](app=app)\n        else:\n            return _TOOLS_MAP[tool_name]()\n    except KeyError as e:\n        raise ValueError(f\"Tool '{tool_name}' not found: {e}\")\n    except Exception as e:\n        raise Exception(f\"Failed to get tool: {e}\")\n"}
{"type": "source_file", "path": "tools/custom/website_search_tool.py", "content": "import os\nfrom crewai_tools import BaseTool\nfrom utils import get_embedchain_settings\nfrom embedchain import App\nfrom typing import Optional\n\nclass WebsiteContentQueryTool(BaseTool):\n    \"\"\"A tool that fetches website content, adds it to a vector database, and queries it.\"\"\"\n    name: str = \"WebsiteContentQueryTool\"\n    app: Optional[App] = None\n    description: str = (\n        \"This tool fetches the content of a website, adds it to a vector database, and queries the vector database for a given query string.\"\n    )\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(self, app: 'App', **kwargs):\n        super().__init__(**kwargs)\n        if app and isinstance(app, App):\n            self.app = app\n\n    def _run(self, url: str, query: str) -> str:\n        \"\"\"Use the WebsiteContentQueryTool.\"\"\"\n        return self.query_website_content(url=url, query=query)\n\n    def query_website_content(self, url: str, query: str) -> str:\n        \"\"\"\n        Fetches the content of a website, adds it to a vector database, and queries the vector database for a given query string.\n\n        Parameters:\n        url (str): The URL of the website to fetch content from.\n        query (str): The query string to search in the vector database.\n\n        Returns:\n        str: The result from the vector database query.\n\n        Raises:\n        Exception: If there is an issue with fetching website content or querying the vector database.\n        \"\"\"\n        # Fetch the content of the website\n        try:\n            if not self.app:\n                config = get_embedchain_settings(task_id='shared',\n                                                 llm_name=os.getenv('LLM_NAME'),\n                                                 embedder_name=os.getenv('EMBEDDER_NAME'))\n                self.app = App.from_config(config=config)\n            self.app.add(url, data_type='web_page')\n            results = self.app.query(query)\n        except Exception as e:\n            raise Exception(f\"Failed to fetch website content: {e}\")\n\n        # Return the result from the vector database\n        return str(results)\n"}
