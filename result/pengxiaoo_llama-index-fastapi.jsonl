{"repo_info": {"repo_name": "llama-index-fastapi", "repo_owner": "pengxiaoo", "repo_url": "https://github.com/pengxiaoo/llama-index-fastapi"}}
{"type": "test_file", "path": "app/tests/__init__.py", "content": "import contextvars\n\nrequest_id: contextvars.ContextVar = contextvars.ContextVar(\"request_id\")\n"}
{"type": "test_file", "path": "app/tests/_test_streaming_chat.py", "content": "import json\nimport requests\n\nurl = \"http://127.0.0.1:8081/api/v1/chat/streaming\"\nbody = {\n  \"conversation_id\": \"1\",\n  \"role\": \"user\",\n  \"content\": \"How many players in table tennis game?\"\n}\n\nwith requests.post(url, data=json.dumps(body), stream=True) as r:\n    for chunk in r.iter_content(1024):  # or, for line in r.iter_lines():\n        print(chunk)\n"}
{"type": "test_file", "path": "app/tests/test_qa.py", "content": "import unittest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nfrom app.data.messages.qa import (\n    QuestionAnsweringRequest,\n    QuestionAnsweringResponse,\n)\nfrom app.data.models.qa import Source, get_default_answer\nfrom app.llama_index_server.chat_message_dao import ChatMessageDao\nfrom app.tests.test_base import BaseTest\n\n\nclass QaTest(BaseTest):\n    client = TestClient(app=app)\n    ROOT = \"/api/v1\"\n    ROUTER_QA = \"qa\"\n    ROUTER_ADMIN = \"admin\"\n    chat_message_dao = ChatMessageDao()\n\n    def test_ask_questions_not_relevant(self):\n        data = QuestionAnsweringRequest.ConfigDict.json_schema_extra[\n            \"example_not_relevant\"\n        ]\n        response = self.client.post(url=f\"{self.ROOT}/{self.ROUTER_QA}/query\", json=data)\n        response = QuestionAnsweringResponse(**response.json())\n        self.assertEqual(get_default_answer(), response.data.answer)\n        self.check_document(doc_id=data[\"question\"], from_knowledge_base=False)\n        self.doc_id = data[\"question\"]\n\n    def test_ask_questions_relevant_and_in_knowledge_base(self):\n        data = QuestionAnsweringRequest.ConfigDict.json_schema_extra[\n            \"example_relevant_and_in_knowledge_base\"\n        ]\n        response = self.client.post(url=f\"{self.ROOT}/{self.ROUTER_QA}/query\", json=data)\n        json_dict = response.json()\n        response = QuestionAnsweringResponse(**json_dict)\n        self.assertNotEqual(response.data.answer, get_default_answer())\n        self.assertEqual(response.data.source, Source.KNOWLEDGE_BASE)\n        self.assertIsNotNone(response.data.matched_question)\n        self.check_document(doc_id=response.data.matched_question, from_knowledge_base=True)\n        self.doc_id = response.data.matched_question\n\n    def test_ask_questions_relevant_but_not_in_knowledge_base(self):\n        data = QuestionAnsweringRequest.ConfigDict.json_schema_extra[\n            \"example_relevant_but_not_in_knowledge_base\"\n        ]\n        response = self.client.post(url=f\"{self.ROOT}/{self.ROUTER_QA}/query\", json=data)\n        json_dict = response.json()\n        response = QuestionAnsweringResponse(**json_dict)\n        self.assertNotEqual(response.data.answer, get_default_answer())\n        self.check_document(doc_id=data[\"question\"], from_knowledge_base=False)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "app/tests/conftest.py", "content": "import os\n\n\ndef pytest_configure():\n    os.environ[\"IS_LOCAL_TEST\"] = \"True\"\n"}
{"type": "test_file", "path": "app/tests/test_chat.py", "content": "import unittest\nfrom fastapi.testclient import TestClient\nfrom llama_index.core.llms import MessageRole\nfrom app.main import app\nfrom app.data.models.mongodb import Message\nfrom app.tests.test_base import BaseTest\n\n\nclass ChatTest(BaseTest):\n    client = TestClient(app=app)\n    ROOT = \"/api/v1\"\n    ROUTER_CHAT = \"chat\"\n    CSV_PATH = \"../llama_index_server/documents/golf-knowledge-base.csv\"\n\n    def tearDown(self):\n        super().tearDown()\n        self.chat_message_dao.delete_many({\"conversation_id\": self.conversation_id})\n\n    def test_non_streaming_question_irrelevant(self):\n        query = \"how to play football?\"\n        self.conversation_id = \"test_non_streaming_question_irrelevant\"\n        body = {\n            \"conversation_id\": self.conversation_id,\n            \"role\": \"user\",\n            \"content\": query,\n        }\n        response = self.client.post(\n            url=f\"{self.ROOT}/{self.ROUTER_CHAT}/non-streaming\", json=body\n        )\n        self.assertEqual(response.status_code, 200)\n        json_response = response.json()\n        message = Message(**json_response[\"data\"])\n        self.assertEqual(message.role, MessageRole.ASSISTANT)\n        self.assertEqual(message.conversation_id, self.conversation_id)\n\n    def test_non_streaming_chat_history(self):\n        self.conversation_id = \"test_non_streaming_chat_history\"\n        body = {\n            \"conversation_id\": self.conversation_id,\n            \"role\": \"user\",\n            \"content\": \"hi, my name is Christopher\",\n        }\n        response = self.client.post(\n            url=f\"{self.ROOT}/{self.ROUTER_CHAT}/non-streaming\", json=body\n        )\n        self.assertEqual(response.status_code, 200)\n        body[\"content\"] = \"what is my name?\"\n        response = self.client.post(\n            url=f\"{self.ROOT}/{self.ROUTER_CHAT}/non-streaming\", json=body\n        )\n        json_response = response.json()\n        self.assertEqual(response.status_code, 200)\n        message = Message(**json_response[\"data\"])\n        self.assertIn(\"Christopher\", message.content)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "app/tests/test_base.py", "content": "import unittest\nfrom fastapi.testclient import TestClient\nimport base64\nfrom app.main import app\nfrom app.data.messages.qa import (\n    DocumentRequest,\n    DocumentResponse,\n)\nfrom app.data.models.qa import Source\nfrom app.utils import data_consts\nfrom app.llama_index_server.chat_message_dao import ChatMessageDao\n\n\nclass BaseTest(unittest.TestCase):\n    client = TestClient(app=app)\n    ROOT = \"/api/v1\"\n    ROUTER_QA = \"qa\"\n    ROUTER_ADMIN = \"admin\"\n    chat_message_dao = ChatMessageDao()\n\n    @staticmethod\n    def create_authorization_header(username: str, password: str) -> dict:\n        credentials = f\"{username}:{password}\"\n        encoded_credentials = base64.b64encode(credentials.encode()).decode()\n        return {\"Authorization\": f\"Basic {encoded_credentials}\"}\n\n    def setUp(self):\n        self.doc_id = None\n        auth_header = self.create_authorization_header(data_consts.EXPECTED_USERNAME, data_consts.EXPECTED_PASSWORD)\n        response = self.client.post(url=f\"{self.ROOT}/{self.ROUTER_ADMIN}/cleanup\", headers=auth_header)\n        self.assertEqual(response.status_code, 200)\n\n    def tearDown(self):\n        if self.doc_id:\n            self.delete_doc(self.doc_id)\n\n    def delete_doc(self, doc_id):\n        auth_header = self.create_authorization_header(data_consts.EXPECTED_USERNAME, data_consts.EXPECTED_PASSWORD)\n        response = self.client.delete(url=f\"{self.ROOT}/{self.ROUTER_ADMIN}/documents/{doc_id}\", headers=auth_header)\n        self.assertEqual(response.status_code, 200)\n\n    def check_document(self, doc_id, from_knowledge_base):\n        data = DocumentRequest.ConfigDict.json_schema_extra\n        data[\"doc_id\"] = doc_id\n        response = self.client.post(url=f\"{self.ROOT}/{self.ROUTER_QA}/document\", json=data)\n        response = DocumentResponse(**response.json())\n        self.assertIsNotNone(response.data)\n        if from_knowledge_base:\n            self.assertTrue(response.data.source == Source.KNOWLEDGE_BASE)\n        else:\n            self.assertTrue(response.data.source != Source.KNOWLEDGE_BASE)\n"}
{"type": "source_file", "path": "app/data/models/__init__.py", "content": ""}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/data/__init__.py", "content": ""}
{"type": "source_file", "path": "app/data/models/qa.py", "content": "from enum import Enum\nfrom pydantic import Field, BaseModel\nfrom typing import Optional\nfrom llama_index.core import Document\nfrom app.utils import data_util\n\nIRRELEVANT_QUESTION = {\n    \"default_answer_id\": \"irrelevant_question\",\n    \"default_answer\": \"This question is not relevant to golf, please ask a question related to golf.\",\n}\n\n\ndef get_default_answer_id():\n    return IRRELEVANT_QUESTION[\"default_answer_id\"]\n\n\ndef get_default_answer():\n    return IRRELEVANT_QUESTION[\"default_answer\"]\n\n\nclass Source(str, Enum):\n    KNOWLEDGE_BASE = \"knowledge-base\"\n    USER_ASKED = \"user-asked\"\n    CHATGPT35 = \"gpt-3.5-turbo\"\n    CHATGPT4 = \"gpt-4\"\n    CLAUDE_2 = \"claude-2\"\n\n\nclass Answer(BaseModel):\n    category: Optional[str] = Field(None, description=\"Category of the question, if it can be recognized\")\n    question: str = Field(..., description=\"the original question\")\n    matched_question: Optional[str] = Field(None, description=\"matched question, if any\")\n    source: Source = Field(..., description=\"Source of the answer\")\n    answer: str = Field(..., description=\"answer to the question\")\n\n    @staticmethod\n    def normalize_answer_for_irrelevant_question(answer: str):\n        if answer == IRRELEVANT_QUESTION[\"default_answer_id\"]:\n            return IRRELEVANT_QUESTION[\"default_answer\"]\n        else:\n            return answer\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.answer = self.normalize_answer_for_irrelevant_question(self.answer)\n\n    def to_llama_index_document(self):\n        return Document(\n            doc_id=data_util.get_doc_id(self.question),\n            text=self.question,\n            excluded_llm_metadata_keys=[\"category\", \"source\", \"answer\"],\n            metadata={\n                \"category\": self.category,\n                \"source\": self.source.value,\n                \"answer\": self.answer,\n            },\n        )\n"}
{"type": "source_file", "path": "app/llama_index_server/chat_message_dao.py", "content": "from typing import List\nimport pymongo\nfrom llama_index.core.llms import ChatMessage\nfrom app.utils.mongo_dao import MongoDao\nfrom app.utils import data_consts\nfrom app.data.models.mongodb import Message\nfrom app.utils.log_util import logger\n\nCHAT_HISTORY_LIMIT = 20\n\n\nclass ChatMessageDao(MongoDao):\n    def __init__(self,\n                 mongo_uri=data_consts.MONGO_URI,\n                 db_name=Message.db_name(),\n                 collection_name=Message.collection_name(),\n                 ):\n        super().__init__(mongo_uri, db_name, collection_name)\n\n    def get_chat_history(self, conversation_id: str) -> List[Message]:\n        messages = self.find(\n            query={\"conversation_id\": conversation_id, },\n            limit=CHAT_HISTORY_LIMIT,\n            sort=[(\"timestamp\", pymongo.DESCENDING)],\n        )\n        if messages is None:\n            return []\n        else:\n            messages = list(messages)\n            messages = [Message(**m) for m in messages]\n            messages.sort(key=lambda m: m.timestamp)\n            logger.info(f\"Found message history size: {len(messages)}\")\n            return messages\n\n    def save_chat_history(self, conversation_id: str, chat_message: ChatMessage):\n        message = Message.from_chat_message(conversation_id, chat_message)\n        self.insert_one(message)\n"}
{"type": "source_file", "path": "app/data/messages/status_code.py", "content": "from enum import Enum\n\n\nclass StatusCode(str, Enum):\n    \"\"\"\n    \"status_code\" in the response is a high level descriptive string indicating the status of the request.\n    \"msg\" in the response gives more low level details.\n    \"\"\"\n\n    SUCCEEDED = \"SUCCEEDED\"\n\n    ERROR_INPUT_FORMAT = \"ERROR_INPUT_FORMAT\"\n    ERROR_OPENAI = \"ERROR_OPENAI\"\n    ERROR_TIMEOUT = \"ERROR_TIMEOUT\"\n    ERROR_ALREADY_EXISTS = \"ERROR_ALREADY_EXISTS\"\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "app/data/models/mongodb.py", "content": "from pydantic import Field, BaseModel\nfrom typing import List, Optional\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom app.utils import data_util\nfrom app.data.models.qa import Source\n\n\nclass CollectionModel(BaseModel):\n    \"\"\"\n        Base class for all the collections stored in MongoDB.\n    \"\"\"\n\n    @staticmethod\n    def db_name():\n        return \"ai_bot\"\n\n    @staticmethod\n    def collection_name():\n        return None\n\n\nclass LlamaIndexDocumentMeta(CollectionModel):\n    \"\"\"\n    meta data of llama index document.\n\n    In llama index, a `Document` is a container around any data source.\n    reference: https://docs.llamaindex.ai/en/stable/getting_started/concepts.html\n    \"\"\"\n    \"\"\"\n    Indexes:\n        doc_id(primary)\n    \"\"\"\n    doc_id: str = Field(..., description=\"Global unique id of the document\")\n    question: str = Field(..., description=\"the original question\")\n    matched_question: Optional[str] = Field(None, description=\"matched question, if any\")\n    category: Optional[str] = Field(None, description=\"Category of the question, if it can be recognized\")\n    source: Source = Field(..., description=\"Source of the answer\")\n    answer: str = Field(..., description=\"answer to the question\")\n    insert_timestamp: int = Field(..., description=\"The timestamp when the document is inserted, in milliseconds\")\n    query_timestamps: List[int] = Field([], description=\"The timestamps when the document is queried\")\n\n    @staticmethod\n    def collection_name():\n        return \"llama_index_document_meta\"\n\n    @staticmethod\n    def from_answer(answer):\n        doc_meta = LlamaIndexDocumentMeta(\n            doc_id=data_util.get_doc_id(answer.question),\n            question=answer.question,\n            matched_question=answer.matched_question,\n            source=answer.source.value,\n            category=answer.category,\n            answer=answer.answer,\n            insert_timestamp=data_util.get_current_milliseconds(),\n            query_timestamps=[],\n        )\n        return doc_meta\n\n    def __init__(self, **data):\n        if \"doc_id\" not in data:\n            data[\"doc_id\"] = data_util.get_doc_id(data[\"question\"])\n        super().__init__(**data)\n\n\nclass LlamaIndexDocumentMetaReadable(LlamaIndexDocumentMeta):\n    insert_time: str = Field(..., description=\"The time when the document is inserted, in human readable format\")\n    last_query_time: str = Field(\"\", description=\"The time when the document is last queried\")\n    query_count_7_days: int = Field(0, description=\"How many times the document is queried in last 7 days\")\n\n    def __init__(self, **data):\n        data[\"insert_time\"] = data_util.milliseconds_to_human_readable(data[\"insert_timestamp\"])\n        super().__init__(**data)\n        if len(self.query_timestamps) > 0:\n            self.query_timestamps.sort()\n            self.last_query_time = data_util.milliseconds_to_human_readable(self.query_timestamps[-1])\n        self.query_count_7_days = len([t for t in self.query_timestamps if\n                                       t > data_util.get_current_milliseconds() - 7 * data_util.MILLISECONDS_PER_DAY])\n\n\nclass Message(CollectionModel):\n    conversation_id: str = Field(..., description=\"Unique id of the conversation\")\n    role: MessageRole = Field(..., description=\"Author of the chat message\")\n    content: str = Field(..., description=\"Content of the chat message\")\n    timestamp: int = Field(..., description=\"Time when this chat message was sent, in milliseconds\")\n    time: Optional[str] = Field(None, description=\"Time when this chat message was sent, in human readable format\")\n\n    @staticmethod\n    def collection_name():\n        return \"chat_message\"\n\n    @staticmethod\n    def from_chat_message(conversation_id: str, chat_message: ChatMessage):\n        return Message(\n            conversation_id=conversation_id,\n            role=chat_message.role,\n            content=chat_message.content,\n            timestamp=data_util.get_current_milliseconds(),\n        )\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self.time = data_util.milliseconds_to_human_readable(self.timestamp)\n"}
{"type": "source_file", "path": "app/data/messages/chat.py", "content": "from llama_index.core.llms import ChatMessage\nfrom pydantic import Field, BaseModel\nfrom llama_index.core.llms import MessageRole\nfrom app.data.models.mongodb import Message\n\n\nclass ChatRequest(BaseModel):\n    conversation_id: str = Field(..., description=\"Unique id of the conversation\")\n    content: str = Field(..., description=\"Content of the chat message\")\n\n    def to_chat_message(self) -> ChatMessage:\n        return ChatMessage(\n            role=MessageRole.USER,\n            content=self.content,\n        )\n\n\nclass ChatResponse(BaseModel):\n    data: Message = Field(..., description=\"response from the chatbot\")\n"}
{"type": "source_file", "path": "app/data/messages/qa.py", "content": "from pydantic import Field, BaseModel\nfrom typing import Optional\nfrom app.data.models.qa import Answer\nfrom app.data.messages.response import BaseResponseModel\nfrom app.data.models.mongodb import LlamaIndexDocumentMetaReadable\n\n\nclass QuestionAnsweringRequest(BaseModel):\n    question: str = Field(..., description=\"question to be answered\")\n\n    class ConfigDict:\n        json_schema_extra = {\n            \"example_relevant_and_in_knowledge_base\": {\n                \"question\": \"How do I achieve consistent ball contact?\"\n            },\n            \"example_relevant_but_not_in_knowledge_base\": {\n                \"question\": \"How much money it will cost if I buy a full set of golf clubs?\"\n            },\n            \"example_not_relevant\": {\"question\": \"how to become a football player?\"},\n        }\n\n\nclass QuestionAnsweringResponse(BaseResponseModel):\n    data: Optional[Answer] = Field(None, description=\"answer to the question\")\n\n\nclass DocumentRequest(BaseModel):\n    doc_id: str = Field(..., description=\"document id\")\n    fuzzy: bool = Field(False, description=\"whether to use fuzzy search\")\n\n    class ConfigDict:\n        json_schema_extra = {\n            \"doc_id\": \"doc_id_1\",\n            \"fuzzy\": False,\n        }\n\n\nclass DocumentResponse(BaseResponseModel):\n    data: Optional[LlamaIndexDocumentMetaReadable] = Field(None, description=\"document data\")\n\n\nclass DeleteDocumentResponse(BaseResponseModel):\n    \"\"\"DeleteDocumentResponse\"\"\"\n"}
{"type": "source_file", "path": "app/llama_index_server/__init__.py", "content": ""}
{"type": "source_file", "path": "app/data/messages/__init__.py", "content": ""}
{"type": "source_file", "path": "app/data/messages/response.py", "content": "from pydantic import BaseModel\nfrom fastapi import HTTPException\nfrom app.data.messages.status_code import StatusCode\n\n\nclass BaseResponseModel(BaseModel):\n    \"\"\"\n    Base class for all the responses\n    \"\"\"\n\n    status_code: StatusCode = StatusCode.SUCCEEDED\n    msg: str = \"success\"\n\n    def __init__(self, **data):\n        # only return necessary information to the frontend\n        msg = data.get(\"msg\")\n        if msg and \": \" in msg:\n            msg = msg.split(\": \")[-1].rstrip(\".\")\n            data[\"msg\"] = msg\n        super().__init__(**data)\n\n\nclass CustomHTTPException(HTTPException):\n    \"\"\"\n    Base class for all the exceptions\n    \"\"\"\n\n    custom_status_code: str = None\n\n    def __init__(\n        self,\n        http_status_code: int = 500,\n        custom_status_code: str = None,\n        detail: str = None,\n    ):\n        if detail and \": \" in detail:\n            detail = detail.split(\": \")[-1].rstrip(\".\")\n        self.custom_status_code = custom_status_code\n        super().__init__(status_code=http_status_code, detail=detail)\n"}
{"type": "source_file", "path": "app/routers/qa.py", "content": "from fastapi import APIRouter\nimport asyncio\nfrom app.data.messages.qa import (\n    QuestionAnsweringRequest,\n    QuestionAnsweringResponse,\n    DocumentRequest,\n    DocumentResponse,\n)\nfrom app.llama_index_server import index_server\nfrom app.utils.log_util import logger\nfrom app.utils.data_consts import API_TIMEOUT\n\nqa_router = APIRouter(\n    prefix=\"/qa\",\n    tags=[\"question answering\"],\n)\n\n\n@qa_router.post(\n    \"/query\",\n    response_model=QuestionAnsweringResponse,\n    description=\"ask questions related to golf, return a standard answer if there is a good match in the knowledge \"\n                \"base, otherwise turn to chatgpt for an answer. if the question is not related to golf at all, \"\n                \"return a default answer telling the user to ask another question\",\n)\nasync def answer_question(req: QuestionAnsweringRequest):\n    logger.info(\"answer question from user\")\n    query_text = req.question\n    answer = await asyncio.wait_for(index_server.query_index(query_text), timeout=API_TIMEOUT)\n    return QuestionAnsweringResponse(data=answer)\n\n\n@qa_router.post(\n    \"/document\",\n    response_model=DocumentResponse,\n    description=\"check what's inside the document. mainly for testing\",\n)\nasync def get_document(req: DocumentRequest):\n    logger.info(f\"get document for doc_id {req.doc_id}, fuzzy search: {req.fuzzy}\")\n    document = await asyncio.wait_for(index_server.get_document(req), timeout=API_TIMEOUT)\n    return DocumentResponse(data=document)\n"}
{"type": "source_file", "path": "app/llama_index_server/document_meta_dao.py", "content": "from app.utils.mongo_dao import MongoDao\nfrom app.utils.log_util import logger\nfrom app.utils.data_util import get_current_milliseconds, MILLISECONDS_PER_DAY\nfrom app.utils import data_consts\nfrom app.data.models.mongodb import LlamaIndexDocumentMeta\nfrom app.data.models.qa import Source\n\n\nclass DocumentMetaDao(MongoDao):\n    def __init__(self,\n                 mongo_uri=data_consts.MONGO_URI,\n                 db_name=LlamaIndexDocumentMeta.db_name(),\n                 collection_name=LlamaIndexDocumentMeta.collection_name(),\n                 size_limit=data_consts.DOCUMENT_META_LIMIT,\n                 ):\n        super().__init__(mongo_uri, db_name, collection_name, size_limit)\n\n    def prune(self):\n        \"\"\"\n        remove all documents that satisfy the following conditions:\n            1. not from knowledge base, and\n            2. inserted more than 7 days ago, and\n            3. not queried in the last 7 days\n        \"\"\"\n        logger.info(f\"current doc size: {self.doc_size()}, pruning...\")\n        seven_days_ago = get_current_milliseconds() - 7 * MILLISECONDS_PER_DAY\n        query = {\n            \"source\": {\"$ne\": Source.KNOWLEDGE_BASE.value},\n            \"insert_timestamp\": {\"$lt\": seven_days_ago},\n            \"query_timestamps\": {\n                \"$not\": {\n                    \"$elemMatch\": {\"$gte\": seven_days_ago}\n                }\n            }\n        }\n        projection = {\n            \"_id\": 0,\n            \"doc_id\": 1,\n        }\n        pruned_doc_ids = []\n        for doc in self.find(query, projection):\n            pruned_doc_ids.append(doc[\"doc_id\"])\n        if len(pruned_doc_ids) > 0:\n            self.delete_many(query)\n        return pruned_doc_ids\n\n    def cleanup_for_test(self):\n        query = {\n            \"source\": {\"$ne\": Source.KNOWLEDGE_BASE.value},\n        }\n        super().delete_many(query)\n"}
{"type": "source_file", "path": "app/routers/chatbot.py", "content": "from fastapi import APIRouter\nimport asyncio\nfrom app.data.messages.chat import ChatRequest, ChatResponse\nfrom app.llama_index_server import index_server\nfrom app.utils.log_util import logger\nfrom app.utils.data_consts import API_TIMEOUT\n\nchatbot_router = APIRouter(\n    prefix=\"/chat\",\n    tags=[\"chatbot\"],\n)\n\n\n@chatbot_router.post(\n    \"/non-streaming\",\n    response_model=ChatResponse,\n    description=\"Chat with the ai bot in a non streaming way.\")\nasync def chat(request: ChatRequest):\n    logger.info(\"Non streaming chat\")\n    conversation_id = request.conversation_id\n    message = await asyncio.wait_for(index_server.chat(request.content, conversation_id), timeout=API_TIMEOUT)\n    return ChatResponse(data=message)\n"}
{"type": "source_file", "path": "app/llama_index_server/index_storage.py", "content": "import os\nfrom contextlib import contextmanager\nfrom multiprocessing import Lock\nfrom typing import Tuple\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core import (\n    Settings,\n    load_index_from_storage,\n    StorageContext,\n    VectorStoreIndex,\n)\nfrom app.data.models.qa import Source, Answer\nfrom app.data.models.mongodb import LlamaIndexDocumentMeta\nfrom app.utils.log_util import logger\nfrom app.utils import data_util, csv_util\nfrom app.llama_index_server.document_meta_dao import DocumentMetaDao\n\nCURRENT_DIR = os.path.dirname(__file__)\nPARENT_DIR = os.path.dirname(CURRENT_DIR)\nLLAMA_INDEX_HOME = os.path.join(PARENT_DIR, \"llama_index_server\")\nos.environ[\"LLAMA_INDEX_CACHE_DIR\"] = f\"{LLAMA_INDEX_HOME}/llama_index_cache\"\nINDEX_PATH = f\"{LLAMA_INDEX_HOME}/saved_index\"\nCSV_PATH = os.path.join(PARENT_DIR, f\"{LLAMA_INDEX_HOME}/documents/golf-knowledge-base.csv\")\nPERSIST_INTERVAL = 3600\n\n\nclass IndexStorage:\n    def __init__(self):\n        self._current_model = Source.CHATGPT35\n        logger.info(\"initializing index and mongo ...\")\n        self._index, self._mongo = self.initialize_index()\n        logger.info(\"initializing index and mongo done\")\n        self._lock = Lock()\n        self._last_persist_time = 0\n        self._chat_engine_record = {}\n\n    @property\n    def chat_engine_record(self):\n        return self._chat_engine_record\n\n    @property\n    def current_model(self):\n        return self._current_model\n\n    def mongo(self):\n        return self._mongo\n\n    def index(self):\n        return self._index\n\n    @contextmanager\n    def lock(self):\n        # for the write operations on self._index\n        with self._lock:\n            yield\n\n    def delete_doc(self, doc_id):\n        \"\"\"remove from both index and mongo\"\"\"\n        with self.lock():\n            self._index.delete_ref_doc(doc_id, delete_from_docstore=True)\n            self._index.storage_context.persist(persist_dir=INDEX_PATH)\n            return self._mongo.delete_one({\"doc_id\": doc_id})\n\n    def add_doc(self, answer: Answer):\n        \"\"\"add to both index and mongo\"\"\"\n        with self.lock():\n            doc = answer.to_llama_index_document()\n            self._index.insert(doc)\n            current_time = data_util.get_current_seconds()\n            if current_time - self._last_persist_time >= PERSIST_INTERVAL:\n                self._index.storage_context.persist(persist_dir=INDEX_PATH)\n                self._last_persist_time = current_time\n            doc_meta = LlamaIndexDocumentMeta.from_answer(answer)\n            pruned_doc_ids = self._mongo.upsert_one({\"doc_id\": doc.doc_id}, doc_meta, need_prune=True)\n            if len(pruned_doc_ids) > 0:\n                for pruned_doc_id in pruned_doc_ids:\n                    self._index.delete_ref_doc(pruned_doc_id, delete_from_docstore=True)\n                self._index.storage_context.persist(persist_dir=INDEX_PATH)\n\n    def initialize_index(self) -> Tuple[BaseIndex, DocumentMetaDao]:\n        llm = OpenAI(temperature=0.1, model=self._current_model)\n        Settings.llm = llm\n        mongo = DocumentMetaDao()\n        if os.path.exists(INDEX_PATH) and os.path.exists(INDEX_PATH + \"/docstore.json\"):\n            logger.info(f\"Loading index from dir: {INDEX_PATH}\")\n            index = load_index_from_storage(\n                StorageContext.from_defaults(persist_dir=INDEX_PATH),\n            )\n        else:\n            data_util.assert_true(os.path.exists(CSV_PATH), f\"csv file not found: {CSV_PATH}\")\n            standard_answers = csv_util.load_standard_answers_from_csv(CSV_PATH)\n            documents = [answer.to_llama_index_document() for answer in standard_answers]\n            index = VectorStoreIndex.from_documents(documents)\n            index.storage_context.persist(persist_dir=INDEX_PATH)\n            doc_metas = [LlamaIndexDocumentMeta.from_answer(answer).model_dump() for answer in standard_answers]\n            mongo.bulk_upsert(doc_metas, primary_keys=[\"doc_id\"])\n        logger.info(f\"Stored docs size: {mongo.doc_size()}\")\n        return index, mongo\n\n\nindex_storage = IndexStorage()\n"}
{"type": "source_file", "path": "app/utils/api-docs/swagger_html.py", "content": "#!/usr/bin/python\n#\n#  Copyright 2017 Otto Seiskari\n#  Licensed under the Apache License, Version 2.0.\n#  See http://www.apache.org/licenses/LICENSE-2.0 for the full text.\n#\n#  This file is based on\n#  https://github.com/swagger-api/swagger-ui/blob/4f1772f6544699bc748299bd65f7ae2112777abc/dist/index.html\n#  (Copyright 2017 SmartBear Software, Licensed under Apache 2.0)\n#\n\"\"\"\nUsage:\n    python swagger-yaml-to-html.py < /path/to/api.yaml > doc.html\n\"\"\"\nimport yaml\nimport json\nimport sys\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Swagger UI</title>\n  <link href=\"https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700\" rel=\"stylesheet\">\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdnjs.cloudflare.com/ajax/libs/swagger-ui/5.1.0/swagger-ui.css\" >\n  <style>\n    html\n    {\n      box-sizing: border-box;\n      overflow: -moz-scrollbars-vertical;\n      overflow-y: scroll;\n    }\n    *,\n    *:before,\n    *:after\n    {\n      box-sizing: inherit;\n    }\n    body {\n      margin:0;\n      background: #fafafa;\n    }\n  </style>\n</head>\n<body>\n<div id=\"swagger-ui\"></div>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/swagger-ui/5.1.0/swagger-ui-bundle.js\"> </script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/swagger-ui/5.1.0/swagger-ui-standalone-preset.js\"> </script>\n<script>\nwindow.onload = function() {\n  var spec = %s;\n  // Build a system\n  const ui = SwaggerUIBundle({\n    spec: spec,\n    dom_id: '#swagger-ui',\n    deepLinking: true,\n    presets: [\n      SwaggerUIBundle.presets.apis,\n      SwaggerUIStandalonePreset\n    ],\n    plugins: [\n      SwaggerUIBundle.plugins.DownloadUrl\n    ],\n    layout: \"StandaloneLayout\"\n  })\n  window.ui = ui\n}\n</script>\n</body>\n</html>\n\"\"\"\n\nspec = yaml.load(sys.stdin, Loader=yaml.FullLoader)\nsys.stdout.write(TEMPLATE % json.dumps(spec))\n"}
{"type": "source_file", "path": "app/routers/admin.py", "content": "from fastapi import APIRouter, Path, Depends\nfrom fastapi.security import HTTPBasicCredentials\nfrom app.data.messages.qa import DeleteDocumentResponse\nfrom app.llama_index_server import index_server\nfrom app.utils.log_util import logger\nfrom app.utils import auth_util\n\nadmin_router = APIRouter(\n    prefix=\"/admin\",\n    tags=[\"(admin) high priority admin operations, usually for testing and debugging\"],\n)\n\n\n@admin_router.delete(\n    \"/documents/{doc_id}\",\n    response_model=DeleteDocumentResponse,\n    description=\"delete a document by doc_id. only for testing\",\n)\nasync def delete_doc(doc_id: str = Path(..., title=\"The ID of the document to delete\"),\n                     credentials: HTTPBasicCredentials = Depends(auth_util.verify_credentials)):\n    logger.info(f\"Delete doc for {doc_id}\")\n    deleted_count = index_server.delete_doc(doc_id)\n    return DeleteDocumentResponse(msg=f\"Deleting {doc_id}. Deleted count = {deleted_count}\")\n\n\n@admin_router.post(\n    \"/cleanup\",\n    response_model=DeleteDocumentResponse,\n    description=\"cleanup all the user query meta data in mongodb. only for testing\",\n)\nasync def cleanup_for_test(credentials: HTTPBasicCredentials = Depends(auth_util.verify_credentials)):\n    logger.info(f\"Cleanup for test\")\n    index_server.cleanup_for_test()\n    return DeleteDocumentResponse(msg=f\"Successfully cleanup\")\n"}
{"type": "source_file", "path": "app/llama_index_server/my_query_engine_tool.py", "content": "from typing import Any\nfrom llama_index.core.tools import QueryEngineTool\nfrom llama_index.core.tools.types import ToolOutput\n\nDEFAULT_NAME = \"my_query_engine_tool\"\nDEFAULT_DESCRIPTION = \"\"\"The only difference between MyQueryEngineTool and QueryEngineTool is: \nMyQueryEngineTool utilizes source_nodes while QueryEngineTool doesn't\n\"\"\"\nMATCHED_MARK = \"Matched:\"\n\n\ndef get_matched_question(response):\n    response_str = str(response)\n    if not response_str or response_str == \"None\" or response_str == \"\":\n        source_nodes = response.source_nodes\n        if len(source_nodes) > 0:\n            matched_node = source_nodes[0]\n            matched_question = matched_node.text\n            return f\"{MATCHED_MARK}{matched_question}\"\n    else:\n        return str(response)\n\n\nclass MyQueryEngineTool(QueryEngineTool):\n\n    def call(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        tool_output = super().call(*args, **kwargs)\n        matched_question = get_matched_question(tool_output.raw_output)\n        tool_output.content = matched_question\n        return tool_output\n\n    async def acall(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        tool_output = super().call(*args, **kwargs)\n        matched_question = get_matched_question(tool_output.raw_output)\n        tool_output.content = matched_question\n        return tool_output\n"}
{"type": "source_file", "path": "app/routers/__init__.py", "content": "import contextvars\n\nrequest_id: contextvars.ContextVar = contextvars.ContextVar(\"request_id\")\n"}
{"type": "source_file", "path": "app/main.py", "content": "from fastapi import FastAPI, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom app.utils.openapi import patch_openapi\nfrom app.data.messages.status_code import StatusCode\nimport asyncio\nfrom openai import OpenAIError\nfrom app.routers.qa import qa_router\nfrom app.routers.admin import admin_router\nfrom app.routers.chatbot import chatbot_router\nfrom app.utils.log_util import logger\nimport uvicorn\nimport time\n\napp = FastAPI(\n    title=\"Api Definitions for Question Answering\",\n    servers=[\n        {\n            \"url\": \"http://127.0.0.1:8081\",\n            \"description\": \"Local test environment\",\n        },\n    ],\n    version=\"0.0.1\",\n)\n\n# Enable CORS for *\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.middleware(\"timing\")\nasync def add_response_timing_header(request: Request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    end_time = time.time()\n    response.headers[\"X-Response-Time\"] = str((end_time - start_time) * 1000)\n    return response\n\n\n# Remove 422 error in the api docs\npatch_openapi(app)\nprefix = \"/api/v1\"\napp.include_router(qa_router, prefix=prefix)\napp.include_router(chatbot_router, prefix=prefix)\napp.include_router(admin_router, prefix=prefix)\n\n\ndef handle_error_msg(request, error_msg, error_code=None):\n    request_url = str(request.url)\n    error_msg = f\"client error in {request_url}: {error_msg}\"\n    logger.error(error_msg)\n    result = error_msg.split(\":\")[-1].strip()\n    return result\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc):\n    msg = exc.errors()[0][\"msg\"]\n    error_msg = handle_error_msg(request, msg)\n    return JSONResponse(\n        status_code=400,\n        content={\n            \"status_code\": StatusCode.ERROR_INPUT_FORMAT,\n            \"msg\": error_msg,\n        },\n    )\n\n\n@app.exception_handler(OpenAIError)\nasync def openai_exception_handler(request: Request, exc: OpenAIError):\n    request_url = str(request.url)\n    logger.error(f\"OpenAIError: {exc} in request_url: {request_url}\")\n    return JSONResponse(status_code=200, content={\n        \"status_code\": StatusCode.ERROR_OPENAI,\n        \"msg\": {exc},\n    })\n\n\n@app.exception_handler(asyncio.TimeoutError)\nasync def timeout_exception_handler(request: Request, exc: asyncio.TimeoutError):\n    request_url = str(request.url)\n    logger.error(f\"TimeoutError: {exc} in request_url: {request_url}\")\n    return JSONResponse(status_code=200, content={\n        \"status_code\": StatusCode.ERROR_TIMEOUT,\n        \"msg\": f\"TimeoutError during request url: {request_url}\",\n    })\n\n\ndef main(host=\"127.0.0.1\", port=8081):\n    # show if there is any python process running bounded to the port\n    # ps -fA | grep python\n    logger.info(\"Start api server\")\n    uvicorn.run(\"app.main:app\", host=host, port=port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "app/llama_index_server/index_server.py", "content": "from typing import Union\nfrom llama_index.core import Prompt\nfrom llama_index.core.response_synthesizers import get_response_synthesizer, ResponseMode\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.agent.openai import OpenAIAgent\nfrom llama_index.llms.openai import OpenAI\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom app.data.messages.qa import DocumentRequest\nfrom app.data.models.qa import Source, Answer, get_default_answer_id, get_default_answer\nfrom app.data.models.mongodb import (\n    LlamaIndexDocumentMeta,\n    LlamaIndexDocumentMetaReadable,\n    Message,\n)\nfrom app.utils.log_util import logger\nfrom app.utils import data_util\nfrom app.llama_index_server.chat_message_dao import ChatMessageDao\nfrom app.llama_index_server.index_storage import index_storage\nfrom app.llama_index_server.my_query_engine_tool import MyQueryEngineTool, MATCHED_MARK\n\nexecutor = ThreadPoolExecutor(max_workers=100)\nSIMILARITY_CUTOFF = 0.85\nPROMPT_TEMPLATE_FOR_QUERY_ENGINE = (\n    \"Assume you are an experienced golf coach glad to answer questions from golfer beginners, \"\n    \"if the question has anything to do with golf, or golf knowledge, or golfer population, \"\n    \"please give short, simple, accurate, precise answer to the question, \"\n    \"limited to 80 words maximum. If the question has nothing to do with golf at all, please answer \"\n    f\"'{get_default_answer_id()}'.\\n\"\n    \"The question is: {query_str}\\n\"\n)\nSYSTEM_PROMPT_TEMPLATE_FOR_CHAT_ENGINE = (\n    \"Your are an expert Q&A system that can find relevant information using the tools at your disposal, and you have \"\n    \"great knowledge about golf.\\n\"\n    \"The tools can access a set of typical questions a golf beginner might ask.\\n\"\n    \"If the user's query matches one of those typical questions, stop and return the matched question immediately.\\n\"\n    \"If the user's query doesn't match any of those typical questions, \"\n    \"please give short, simple, accurate, precise answer to the question, limited to 80 words maximum.\\n\"\n    \"You may need to combine the chat history to fully understand the query of the user.\\n\"\n)\nchat_message_dao = ChatMessageDao()\n\n\ndef get_local_query_engine():\n    \"\"\"\n    strictly limited to local knowledge base. our local knowledge base is a list of standard questions which are indexed in vector store,\n    while the standard answers are stored in mongodb through DocumentMetaDao.\n    there is a one-to-one mapping between each standard question and a standard answer.\n    we may update or optimize the standard answers in mongodb frequently, but usually we don't update the standard questions.\n    if a query matches one of the standard questions, we can find the respective standard answer from mongodb.\n    \"\"\"\n    index = index_storage.index()\n    return index.as_query_engine(\n        response_synthesizer=get_response_synthesizer(\n            response_mode=ResponseMode.NO_TEXT\n        ),\n        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=SIMILARITY_CUTOFF)],\n    )\n\n\ndef get_matched_question_from_local_query_engine(query_text):\n    local_query_engine = get_local_query_engine()\n    local_query_response = local_query_engine.query(query_text)\n    if len(local_query_response.source_nodes) > 0:\n        matched_node = local_query_response.source_nodes[0]\n        matched_question = matched_node.text\n        logger.debug(f\"Found matched question from index: {matched_question}\")\n        return matched_question\n    else:\n        return None\n\n\ndef get_doc_meta(text):\n    matched_doc_id = data_util.get_doc_id(text)\n    mongo = index_storage.mongo()\n    doc_meta = mongo.find_one({\"doc_id\": matched_doc_id})\n    doc_meta = LlamaIndexDocumentMeta(**doc_meta) if doc_meta else None\n    return matched_doc_id, doc_meta\n\n\ndef get_llm_query_engine():\n    index = index_storage.index()\n    qa_template = Prompt(PROMPT_TEMPLATE_FOR_QUERY_ENGINE)\n    return index.as_query_engine(text_qa_template=qa_template)\n\n\nasync def query_index(query_text, only_for_meta=False) -> Union[Answer, LlamaIndexDocumentMeta, None]:\n    data_util.assert_not_none(query_text, \"query cannot be none\")\n    logger.info(f\"Query test: {query_text}\")\n    loop = asyncio.get_running_loop()\n    # first search locally\n    matched_question = await loop.run_in_executor(executor, get_matched_question_from_local_query_engine, query_text)\n    if matched_question:\n        matched_doc_id, doc_meta = get_doc_meta(matched_question)\n        if doc_meta:\n            logger.debug(f\"An matched doc meta found from mongodb: {doc_meta}\")\n            doc_meta.query_timestamps.append(data_util.get_current_milliseconds())\n            index_storage.mongo().upsert_one({\"doc_id\": matched_doc_id}, doc_meta)\n            if only_for_meta:\n                return doc_meta\n            else:\n                return Answer(\n                    category=doc_meta.category,\n                    question=query_text,\n                    matched_question=matched_question,\n                    source=Source.KNOWLEDGE_BASE if doc_meta.source == Source.KNOWLEDGE_BASE else Source.USER_ASKED,\n                    answer=doc_meta.answer,\n                )\n        else:\n            # means the document meta has been removed from mongodb. for example by pruning\n            logger.warning(f\"'{matched_doc_id}' is not found in mongodb\")\n            if only_for_meta:\n                return None\n    # if not found, turn to LLM\n    llm_query_engine = get_llm_query_engine()\n    response = await loop.run_in_executor(executor, llm_query_engine.query, query_text)\n    # save the question-answer pair to index\n    answer = Answer(\n        category=None,\n        question=query_text,\n        source=index_storage.current_model,\n        answer=str(response),\n    )\n    index_storage.add_doc(answer)\n    return answer\n\n\ndef delete_doc(doc_id):\n    data_util.assert_not_none(doc_id, \"doc_id cannot be none\")\n    logger.info(f\"Delete document with doc id: {doc_id}\")\n    return index_storage.delete_doc(doc_id)\n\n\nasync def get_document(req: DocumentRequest):\n    doc_meta = index_storage.mongo().find_one({\"doc_id\": req.doc_id})\n    if doc_meta:\n        return LlamaIndexDocumentMetaReadable(**doc_meta)\n    elif req.fuzzy:\n        doc_meta = await query_index(req.doc_id, only_for_meta=True)\n        if doc_meta:\n            doc_meta.matched_question = doc_meta.question\n            doc_meta.question = doc_meta.doc_id = req.doc_id\n            return LlamaIndexDocumentMetaReadable(**doc_meta.model_dump())\n    return None\n\n\ndef cleanup_for_test():\n    return index_storage.mongo().cleanup_for_test()\n\n\ndef get_chat_engine(conversation_id: str, streaming: bool = False):\n    local_query_engine = get_local_query_engine()\n    query_engine_tools = [\n        MyQueryEngineTool.from_defaults(\n            query_engine=local_query_engine,\n            name=\"local_query_engine\",\n            description=\"Queries from a knowledge base consists of typical questions that a golf beginner might ask\",\n        )\n    ]\n    chat_llm = OpenAI(\n        temperature=0,\n        model=index_storage.current_model,\n        streaming=streaming,\n        max_tokens=100,\n    )\n    chat_history = chat_message_dao.get_chat_history(conversation_id)\n    chat_history = [ChatMessage(role=c.role, content=c.content) for c in chat_history]\n    return OpenAIAgent.from_tools(\n        tools=query_engine_tools,\n        llm=chat_llm,\n        chat_history=chat_history,\n        verbose=True,\n        system_prompt=SYSTEM_PROMPT_TEMPLATE_FOR_CHAT_ENGINE,\n    )\n\n\ndef get_response_text_from_chat(agent_chat_response):\n    sources = agent_chat_response.sources\n    if len(sources) > 0:\n        source_content = sources[0].content\n        if MATCHED_MARK in source_content:\n            return source_content.replace(MATCHED_MARK, \"\").strip()\n    return agent_chat_response.response\n\n\nasync def chat(query_text: str, conversation_id: str) -> Message:\n    # we will not index chat messages in vector store, but will save them in mongodb\n    data_util.assert_not_none(query_text, \"query content cannot be none\")\n    user_message = ChatMessage(role=MessageRole.USER, content=query_text)\n    # save immediately, since the following steps may take a while and throw exceptions\n    chat_message_dao.save_chat_history(conversation_id, user_message)\n    chat_engine = get_chat_engine(conversation_id)\n    loop = asyncio.get_running_loop()\n    agent_chat_response = await loop.run_in_executor(executor, chat_engine.chat, query_text)\n    response_text = get_response_text_from_chat(agent_chat_response)\n    response_text = get_default_answer() if get_default_answer_id() in response_text else response_text\n    matched_doc_id, doc_meta = get_doc_meta(response_text)\n    if doc_meta:\n        logger.debug(f\"An matched doc meta found from mongodb: {doc_meta}\")\n        doc_meta.query_timestamps.append(data_util.get_current_milliseconds())\n        index_storage.mongo().update_one({\"doc_id\": matched_doc_id}, doc_meta)\n        bot_message = ChatMessage(role=MessageRole.ASSISTANT, content=doc_meta.answer)\n    else:\n        # means the chat engine cannot find a matched doc meta from mongodb\n        logger.warning(f\"'{matched_doc_id}' is not found in mongodb\")\n        bot_message = ChatMessage(role=MessageRole.ASSISTANT, content=response_text)\n    chat_message_dao.save_chat_history(conversation_id, bot_message)\n    return Message.from_chat_message(conversation_id, bot_message)\n"}
{"type": "source_file", "path": "app/utils/api-docs/extract_openapi.py", "content": "import argparse\nimport json\nimport sys\nimport yaml\nfrom uvicorn.importer import import_from_string\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"app\",       help='App import string. Eg. \"main:app\"', default=\"main:app\")\nparser.add_argument(\"--app-dir\", help=\"Directory containing the app\", default=None)\nparser.add_argument(\"--out\",     help=\"Output file ending in .json or .yaml\", default=\"openapi.json\")\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n\n    if args.app_dir is not None:\n        print(f\"adding {args.app_dir} to sys.path\")\n        sys.path.insert(0, args.app_dir)\n\n    print(f\"importing app from {args.app}\")\n    app = import_from_string(args.app)\n    openapi = app.openapi()\n    version = openapi.get(\"openapi\", \"unknown version\")\n\n    print(f\"writing openapi spec v{version}\")\n    with open(args.out, \"w\") as f:\n        if args.out.endswith(\".json\"):\n            json.dump(openapi, f, indent=2)\n        else:\n            yaml.dump(openapi, f, sort_keys=False)\n\n    print(f\"spec written to {args.out}\")\n"}
{"type": "source_file", "path": "app/utils/__init__.py", "content": "import contextvars\n\nrequest_id: contextvars.ContextVar = contextvars.ContextVar(\"request_id\")\n"}
{"type": "source_file", "path": "app/utils/api-docs/redoc_html.py", "content": "\"\"\"\nScript to export the ReDoc documentation page into a standalone HTML file.\nCreated by https://github.com/pawamoy on https://github.com/Redocly/redoc/issues/726#issuecomment-645414239\n\"\"\"\nimport yaml\nimport json\nimport sys\n\n\"\"\"\nUsage:\n    python redoc-html.py < /path/to/api.yaml > doc.html\n\"\"\"\n\nTEMPLATE = \"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n    <title>Api Definitions for Traveler's App</title>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <link rel=\"shortcut icon\" href=\"https://fastapi.tiangolo.com/img/favicon.png\">\n    <style>\n        body {\n            margin: 0;\n            padding: 0;\n        }\n    </style>\n    <style data-styled=\"\" data-styled-version=\"4.4.1\"></style>\n</head>\n<body>\n    <div id=\"redoc-container\"></div>\n    <script src=\"https://cdn.jsdelivr.net/npm/redoc/bundles/redoc.standalone.js\"> </script>\n    <script>\n        var spec = %s;\n        Redoc.init(spec, {}, document.getElementById(\"redoc-container\"));\n    </script>\n</body>\n</html>\n\"\"\"\n\nspec = yaml.load(sys.stdin, Loader=yaml.FullLoader)\nsys.stdout.write(TEMPLATE % json.dumps(spec))\n"}
{"type": "source_file", "path": "app/utils/log_util.py", "content": "import logging\nimport os\nimport sys\n\nLOG_LEVEL = os.environ.get(\"QA_SERVICE_LOG_LEVEL\", \"DEBUG\")\n\ndefault_formatter = logging.Formatter(\n    \"[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d:%(funcName)s] %(message)s\",\n)\nstream_handler = logging.StreamHandler(stream=sys.stderr)\nstream_handler.setLevel(LOG_LEVEL)\nstream_handler.setFormatter(default_formatter)\nlogger = logging.getLogger(__name__)\nlogger.addHandler(stream_handler)\nlogger.setLevel(LOG_LEVEL)\nlogger.propagate = False\n"}
{"type": "source_file", "path": "app/utils/data_util.py", "content": "from typing import Any, List\nimport time\nfrom datetime import datetime\n\nTIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nMILLISECONDS_PER_DAY = 24 * 60 * 60 * 1000\n\n\ndef get_current_seconds():\n    return int(time.time())\n\n\ndef get_current_milliseconds():\n    return int(time.time() * 1000)\n\n\ndef milliseconds_to_human_readable(milliseconds):\n    return time.strftime(TIME_FORMAT, time.localtime(milliseconds / 1000))\n\n\nclass CustomClientError(ValueError):\n    msg: str\n\n    def __init__(self, msg, *args, **kwargs):\n        self.msg = msg\n        super().__init__(args, kwargs)\n\n\ndef assert_not_none(value, msg=None):\n    if value is None:\n        msg = \"value should not be None\" if not msg else msg\n        raise CustomClientError(msg)\n\n\ndef assert_true(value, msg=None):\n    if value is not True:\n        msg = \"value should be true\" if not msg else msg\n        raise CustomClientError(msg)\n\n\ndef now():\n    # dynamodb does not support datetime type, so we use isoformat() to convert datetime to string.\n    return datetime.now().isoformat()\n\n\ndef get_doc_id(text: str):\n    return text\n\n\ndef is_empty(value: Any):\n    return value is None or value == \"\" or value == [] or value == {}\n\n\ndef not_empty(value: Any):\n    return not is_empty(value)\n\n\ndef del_if_exists(data: dict, keys: List[str]):\n    for key in keys:\n        if key in data:\n            del data[key]\n\n\ndef chunks(long_list, chunk_size):\n    for i in range(0, len(long_list), chunk_size):\n        yield long_list[i: i + chunk_size]\n"}
{"type": "source_file", "path": "app/utils/mongo_dao.py", "content": "from pymongo import MongoClient\nfrom pymongo.collection import Collection\nfrom pymongo.operations import ReplaceOne\nfrom app.data.models.mongodb import CollectionModel\nfrom app.utils.log_util import logger\n\n\nclass MongoDao:\n    \"\"\"\n    Base Data Access Object for MongoDB.\n    \"\"\"\n\n    def __init__(\n            self,\n            mongo_uri,\n            db_name,\n            collection_name,\n            size_limit=0,\n    ):\n        self._client = MongoClient(mongo_uri)\n        self._db = self._client[db_name]\n        self._collection: Collection = self._db[collection_name]\n        if size_limit > 0:\n            self._size_limit = size_limit\n        else:\n            self._size_limit = 0\n\n    def insert_one(self, doc: CollectionModel):\n        logger.info(f\"Insert data\")\n        self._collection.insert_one(doc.model_dump())\n\n    def upsert_one(self, query, doc: CollectionModel, need_prune=False):\n        logger.info(f\"Upsert one: query = {query}\")\n        self._collection.update_one(\n            query,\n            {\"$set\": doc.model_dump()},\n            upsert=True,\n        )\n        pruned_ids = []\n        if need_prune and 0 < self._size_limit < self.doc_size():\n            pruned_ids = self.prune()\n        return pruned_ids\n\n    def update_one(self, query, doc: CollectionModel):\n        logger.info(f\"Update one: query = {query}\")\n        self._collection.update_one(\n            query,\n            {\"$set\": doc.model_dump()},\n            upsert=False,\n        )\n\n    def bulk_upsert(self, docs, primary_keys):\n        operations = [ReplaceOne(\n            filter={primary_key: doc[primary_key] for primary_key in primary_keys},\n            replacement=doc,\n            upsert=True\n        ) for doc in docs]\n        result = self._collection.bulk_write(operations, ordered=False)\n        logger.info(f\"Bulk upsert {len(docs)} docs, result = {result}\")\n\n    def find(self, query, projection=None, limit=0, sort=None, **kwargs):\n        logger.info(\n            f\"Find: query = {query}, projection = {projection}, limit = {limit}, sort = {sort}\"\n        )\n        return self._collection.find(\n            query, projection=projection, limit=limit, sort=sort, **kwargs\n        )\n\n    def find_one(self, query):\n        logger.info(f\"Find one: query = {query}\")\n        doc = self._collection.find_one(query)\n        return doc\n\n    def delete_one(self, query):\n        delete_result = self._collection.delete_one(query)\n        logger.info(f\"Delete one: query = {query}, delete_result = {delete_result}\")\n        return delete_result.deleted_count\n\n    def delete_many(self, query):\n        delete_result = self._collection.delete_many(\n            query,\n        )\n        deleted_count = delete_result.deleted_count\n        logger.info(f\"Delete many with query = {query}, deleted_count = {deleted_count}\")\n        return deleted_count\n\n    def doc_size(self):\n        return self._collection.count_documents({})\n\n    def prune(self):\n        return []\n\n    def cleanup_for_test(self):\n        pass\n"}
{"type": "source_file", "path": "app/utils/csv_util.py", "content": "import csv\nfrom app.data.models.qa import Source, Answer\n\n\ndef load_standard_answers_from_csv(csv_file_path) -> list[Answer]:\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        standard_answers = []\n        for row in reader:\n            answer = Answer(\n                category=row[\"category\"],\n                question=row[\"question\"],\n                answer=row[\"answer\"],\n                source=Source.KNOWLEDGE_BASE,\n            )\n            standard_answers.append(answer)\n        return standard_answers\n"}
{"type": "source_file", "path": "app/utils/openapi.py", "content": "from fastapi import FastAPI\nfrom fastapi.openapi.utils import get_openapi\n\n\ndef patch_openapi(app: FastAPI):\n    def wrapper():\n        if not app.openapi_schema:\n            app.openapi_schema = get_openapi(\n                title=app.title,\n                version=app.version,\n                openapi_version=app.openapi_version,\n                description=app.description,\n                terms_of_service=app.terms_of_service,\n                contact=app.contact,\n                license_info=app.license_info,\n                routes=app.routes,\n                tags=app.openapi_tags,\n                servers=app.servers,\n            )\n            for _, method_item in app.openapi_schema.get(\"paths\").items():\n                for _, param in method_item.items():\n                    responses = param.get(\"responses\")\n                    # remove 422 response, also can remove other status code\n                    if \"422\" in responses:\n                        del responses[\"422\"]\n        return app.openapi_schema\n\n    app.openapi = wrapper\n"}
{"type": "source_file", "path": "app/utils/auth_util.py", "content": "from fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\nimport secrets\nfrom app.utils import data_consts\n\nsecurity = HTTPBasic()\n\n\ndef verify_credentials(credentials: HTTPBasicCredentials = Depends(security)):\n    correct_username = secrets.compare_digest(credentials.username, data_consts.EXPECTED_USERNAME)\n    correct_password = secrets.compare_digest(credentials.password, data_consts.EXPECTED_PASSWORD)\n    if not (correct_username and correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Basic\"},\n        )\n    return credentials\n"}
{"type": "source_file", "path": "app/utils/data_consts.py", "content": "import os\n\nEXPECTED_USERNAME = os.environ.get(\"AI_BOT_ADMIN_USERNAME\", \"your-username\")\nEXPECTED_PASSWORD = os.environ.get(\"AI_BOT_ADMIN_PASSWORD\", \"your-password\")\nMONGO_URI = os.environ.get(\"AI_BOT_MONGO_URI\", \"mongodb://localhost:27017\")\nDOCUMENT_META_LIMIT = os.environ.get(\"AI_BOT_DOCUMENT_META_LIMIT\", 10000)\nAPI_TIMEOUT = 10\n"}
