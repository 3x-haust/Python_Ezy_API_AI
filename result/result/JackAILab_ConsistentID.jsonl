{"repo_info": {"repo_name": "ConsistentID", "repo_owner": "JackAILab", "repo_url": "https://github.com/JackAILab/ConsistentID"}}
{"type": "source_file", "path": "app.py", "content": "import gradio as gr\nimport torch\nimport os\nimport glob\nfrom datetime import datetime\nfrom PIL import Image\nfrom diffusers.utils import load_image\nfrom diffusers import EulerDiscreteScheduler\nfrom pipline_StableDiffusion_ConsistentID import ConsistentIDStableDiffusionPipeline\nimport sys\n\n# Gets the absolute path of the current script\nscript_directory = os.path.dirname(os.path.realpath(__file__))\n\n# The GPU peak consumption is about 6G.\ndef process(inputImage,prompt,negative_prompt):\n\n    device = \"cuda\"\n    ### Download the model from huggingface and put it locally, then place the model in a local directory and specify the directory location.\n    base_model_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\n    consistentID_path = \"JackAILab/ConsistentID/ConsistentID-v1.bin\"\n\n    ### Load base model\n    pipe = ConsistentIDStableDiffusionPipeline.from_pretrained(\n        base_model_path, \n        torch_dtype=torch.float16, \n        use_safetensors=True, \n        variant=\"fp16\"\n    ).to(device)\n\n    ### Load consistentID_model checkpoint\n    pipe.load_ConsistentID_model(\n        os.path.dirname(consistentID_path),\n        subfolder=\"\",\n        weight_name=os.path.basename(consistentID_path),\n        trigger_word=\"img\",\n    )     \n    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n    \n    # hyper-parameter\n    select_images = load_image(Image.fromarray(inputImage))\n    num_steps = 50\n    merge_steps = 30\n    \n\n    if prompt == \"\":\n        prompt = \"A man, in a forest, adventuring\"\n\n    if negative_prompt == \"\":\n        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\n    #Extend Prompt\n    prompt = \"cinematic photo,\" + prompt + \", 50mm photograph, half-length portrait, film, bokeh, professional, 4k, highly detailed\"\n\n    negtive_prompt_group=\"((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), [out of frame], extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))). out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\"\n    negative_prompt = negative_prompt + negtive_prompt_group\n    \n    seed = torch.randint(0, 1000, (1,)).item()\n    generator = torch.Generator(device=device).manual_seed(seed)\n\n    images = pipe(\n        prompt=prompt,\n        width=512,    \n        height=512,\n        input_id_images=select_images,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=1,\n        num_inference_steps=num_steps,\n        start_merge_step=merge_steps,\n        generator=generator,\n    ).images[0]\n\n    current_date = datetime.today()\n\n    output_dir = script_directory + f\"/images/gradio_outputs\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    images.save(os.path.join(output_dir, f\"{current_date}-{seed}.jpg\"))\n\n    return os.path.join(output_dir, f\"{current_date}-{seed}.jpg\")\n\n\niface = gr.Interface(\n    fn=process,\n    inputs=[\n        gr.Image(label=\"Upload Image\"), \n        gr.Textbox(label=\"prompt\",placeholder=\"A man, in a forest, adventuring\"),\n        gr.Textbox(label=\"negative prompt\",placeholder=\"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"),\n    ],\n    outputs=[\n        gr.Image(label=\"Output\"), \n    ],\n    title=\"ConsistentID Demo\",\n    description=\"Put reference portrait below\"\n)\n\niface.launch(share=True)\n"}
{"type": "source_file", "path": "data/FGID_mask.py", "content": "'''\nOfficial implementation of FGID data (facial mask) production script\nAuthor: Jiehui Huang\nHugging Face Demo: https://huggingface.co/spaces/JackAILab/ConsistentID\nProject: https://ssugarwh.github.io/consistentid.github.io/\n'''\n\nimport os\nimport json\nimport torch\nimport torchvision.transforms as transforms\nimport cv2\nimport os.path as osp\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom models.BiSeNet.model import BiSeNet\nfrom PIL import Image\n\n# file_name_length = -5 # .jpeg\nfile_name_length = -4 # .jpg / .png\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path=None, \\\n                    color_save_path=None, json_save_path=None):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1): # num_of_class=17 pi=1~16\n        index = np.where(vis_parsing_anno == pi) # index[0/1].shape.(11675,) \n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi] # vis_parsing_anno_color.shape.(512, 512, 3)\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    if save_im:\n        cv2.imwrite(color_save_path[:file_name_length] +'_color.png', vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100]) \n        cv2.imwrite(save_path[:file_name_length] +'_mask.png', vis_parsing_anno) \n\n\ndef evaluate(respth=\"./parsing_mask_IMG\", color_respth=\"./parsing_color_IMG\", resize_dspth=\"./resize_IMG\", \\\n             dspth='./origin_IMG', jp=\"./all_JSON\", cp='./79999_iter.pth'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = cp\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        total_files = sum(len(files) for _, _, files in os.walk(dspth))\n        for root, dirs, files in os.walk(dspth):\n            for image_file in tqdm(files, total=total_files, desc=\"Processing Files\"):\n                img_path = osp.join(root, image_file)\n                img = Image.open(img_path)\n                image = img.resize((512, 512), Image.BILINEAR)\n                relative_path = osp.relpath(root, dspth)\n                resize_dspth_path = osp.join(resize_dspth, relative_path, image_file[:file_name_length] +'_resize.png') # image_file  '0062963.png'\n                os.makedirs(os.path.dirname(resize_dspth_path), exist_ok=True)\n                image.save(resize_dspth_path) # save origin resized IMGs\n\n                ### Cut image\n                img = to_tensor(image)\n                img = torch.unsqueeze(img, 0)\n                img = img.cuda()\n                out = net(img)[0]\n                parsing = out.squeeze(0).cpu().numpy().argmax(0)\n\n                ### Create storage path\n                result_path = osp.join(respth, relative_path, image_file)\n                color_result_path = osp.join(color_respth, relative_path, image_file) # relative_path: 15\n                if file_name_length==-5:\n                    json_path = osp.join(jp, relative_path, image_file.replace(\"jpeg\", \"json\")) \n                elif file_name_length==-4:\n                    json_path = osp.join(jp, relative_path, image_file.replace(\"jpg\", \"json\")) \n                    json_path = osp.join(jp, relative_path, image_file.replace(\"png\", \"json\")) # save png or jpg\n\n                ### Create subfolder path\n                os.makedirs(osp.dirname(result_path), exist_ok=True)\n                os.makedirs(osp.dirname(color_result_path), exist_ok=True)\n                os.makedirs(osp.dirname(json_path), exist_ok=True)\n\n                ### Save json files\n                json_data = {\n                    \"origin_IMG\": osp.join(relative_path, image_file), # '15/0062963.png'\n                    \"resize_IMG\": osp.join(relative_path, image_file[:file_name_length] +'_resize.png'), # '15/0062963_resize.png'\n                    \"parsing_color_IMG\": osp.join(relative_path, image_file[:file_name_length] +'_color.png'), # '15/0062963_color.png'\n                    \"parsing_mask_IMG\": osp.join(relative_path, image_file[:file_name_length] +'_mask.png'),                            \n                }\n                with open(json_path, 'w') as json_file:\n                    json.dump(json_data, json_file)\n                print(f\"JSON File Saved Success at {json_file}!\")\n\n                ### save origin fine_mask color IMGs\n                vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=result_path, color_save_path=color_result_path, json_save_path=json_path)\n\n\n\nif __name__ == \"__main__\":\n    \n    ### Specify folder path\n    origin_img_path = \"./FGID_Origin\" # read_path for customized IMGs\n    pretrain_model_path = \"JackAILab/ConsistentID/face_parsing.pth\" # read_path for pretrained face-parsing model\n\n    resize_IMG_path = \"./FGID_resize\" # save_path for customized resized IMGs\n    parsing_mask_IM_path = \"./FGID_parsing_mask\" # save_path for fine_mask gray IMGs\n    parsing_color_IM_path = \"./FGID_parsing_color\" # save_path for fine_mask color IMGs\n    json_save_path = \"./FGID_JSON\" # save_path for fine_mask information\n\n    evaluate(respth=parsing_mask_IM_path, color_respth=parsing_color_IM_path, resize_dspth=resize_IMG_path, \\\n            dspth=origin_IMG_path, jp=json_save_path, cp=pretrain_model_path)\n\n\n\n\n''' \nAdditional instructions:\n\n(1) Specify folder path, then CUDA_VISIBLE_DEVICES=0 python ./FGID_mask.py\n\n(2) Time consumption: 10w IMGs / 15 hours on single RTX3090\n\n(3) JSON data should be constructed as follows:\n{\n\"origin_IMG\": \"15/0062963.png\", \n\"resize_IMG\": \"15/0062963_resize.png\", \n\"parsing_color_IMG\": \"15/0062963_color.png\", \n\"parsing_mask_IMG\": \"15/0062963_mask.png\"\n}\n\n(4) Reference table of facial parts corresponding to the Mak value of Face-parsing:\n\n|  Mask  | Face part name         | RGB Color           |\n|--------|------------------------|---------------------|\n| 1      | Face                   | [255, 0, 0]         |\n| 2      | Left_eyebrow           | [255, 85, 0]        |\n| 3      | Right_eyebrow          | [255, 170, 0]       |\n| 4      | Left_eye               | [255, 0, 85]        |\n| 5      | Right_eye              | [255, 0, 170]       |\n| 6      | Hair                   | [0, 0, 255]         |\n| 7      | Left_ear               | [85, 0, 255]        |\n| 8      | Right_ear              | [170, 0, 255]       |\n| 9      | Mouth_external_contour | [0, 255, 85]        |\n| 10     | Nose                   | [0, 255, 0]         |\n| 11     | Mouth_inner_contour    | [0, 255, 170]       |\n| 12     | Upper_lip              | [85, 255, 0]        |\n| 13     | Lower_lip              | [170, 255, 0]       |\n| 14     | Neck                   | [0, 85, 255]        |\n| 15     | Neck_inner_contour     | [0, 170, 255]       |\n| 16     | Cloth                  | [255, 255, 0]       |\n| 17     | Hat                    | [255, 0, 255]       |\n| 18     | Earring                | [255, 85, 255]      |\n| 19     | Necklace               | [255, 255, 85]      |\n| 20     | Glasses                | [255, 170, 255]     |\n| 21     | Hand                   | [255, 0, 255]       |\n| 22     | Wristband              | [0, 255, 255]       |\n| 23     | Clothes_upper          | [85, 255, 255]      |\n| 24     | Clothes_lower          | [170, 255, 255]     |\n| 25     | Other                  | [0, 0, 0]           |\n\n'''\n\n\n\n"}
{"type": "source_file", "path": "data/FGID_faceid_embeds.py", "content": "'''\nOfficial implementation of FGID data (facial caption) production script\nAuthor: Jiehui Huang\nHugging Face Demo: https://huggingface.co/spaces/JackAILab/ConsistentID\nProject: https://ssugarwh.github.io/consistentid.github.io/\n'''\n\nimport os\nimport cv2\nimport torch\nimport json\n\nfrom tqdm import tqdm\nfrom insightface.app import FaceAnalysis\n### Initialize the FaceAnalysis model\napp = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n\ndef process_faceid(origin_path=None, resize_path=None,faceid_path=None, json_path=None):\n\n    ### ========================== resize extraction ==========================\n    fail_count = 0\n    success_count = 0\n\n    total_files = sum(len(files) for _, _, files in os.walk(resize_path))\n\n    for root, dirs, files in os.walk(resize_path):\n        for image_file in tqdm(files, total=total_files, desc=\"Processing Files\"):\n            image_path = os.path.join(root, image_file) # '023_Origin.jpeg'\n            image = cv2.imread(image_path)\n            faces = app.get(image) \n            relative_path = os.path.relpath(root, resize_path) \n        \n            image_id = os.path.splitext(image_file)[0] # '023_Origin'\n            # Save faceid_embeds\n            if faces == []:   \n                ### TODO The prior extraction of FaceID is unstable and a stronger ID prior structure can be used.\n                fail_count += 1\n                print(f\"fail_count resize number is {fail_count}, the current fail img is {image_id}\")\n                continue            \n            else:\n                faceid_embed_file = os.path.join(faceid_path, relative_path, image_id+'_faceid.bin')\n                ### Ensure the directory exists\n                os.makedirs(os.path.join(faceid_path, relative_path), exist_ok=True)\n                faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n                torch.save(faceid_embeds, faceid_embed_file)\n                success_count += 1 \n                print(f\"success_count resize number is {success_count}, the current success img is {os.path.join(relative_path,image_id)}\")\n            \n            ### To update the corresponding JSON file, you need to first run the FGID_mask.py\n            json_file_path = os.path.join(json_path, relative_path, image_id.replace('_resize', '')+'.json')\n\n            with open(json_file_path, 'r+') as json_file:\n                data = json.load(json_file)\n                data['id_embed_file_resize'] = os.path.join(relative_path, image_id+'_faceid.bin')\n                json_file.seek(0)\n                json.dump(data, json_file)\n                json_file.truncate()\n\n            # print(f\"Success_count number is {success_count}, Saved faceid_embeds: {faceid_embed_path}, Updated JSON: {json_file_path}\")\n\n    ### ========================== origin again ==========================\n    fail_count = 0\n    success_count = 0\n\n    total_files = sum(len(files) for _, _, files in os.walk(origin_path))\n\n    for root, dirs, files in os.walk(origin_path):\n        for image_file in tqdm(files, total=total_files, desc=\"Processing Files\"):\n            image_path = os.path.join(root, image_file) # '023_Origin.jpeg'\n            image = cv2.imread(image_path)\n            faces = app.get(image)\n            relative_path = os.path.relpath(root, origin_path)\n            \n            image_id = os.path.splitext(image_file)[0] # '023_Origin'\n            if faces == []:\n                fail_count += 1\n                continue            \n            else:\n                faceid_embed_file = os.path.join(faceid_path, relative_path, image_id+'_faceid.bin')\n                ### Ensure the directory exists\n                os.makedirs(os.path.join(faceid_path, relative_path), exist_ok=True)\n                faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n                torch.save(faceid_embeds, faceid_embed_file)\n                success_count += 1 \n                print(f\"success_count origin number is {success_count}, the current success img is {os.path.join(relative_path,image_id)}\")\n            \n            json_file_path = os.path.join(json_path, relative_path, image_id+'.json')\n            with open(json_file_path, 'r+') as json_file:\n                data = json.load(json_file)\n                data['id_embed_file_origin'] = os.path.join(relative_path, image_id+'_faceid.bin')\n                json_file.seek(0) \n                json.dump(data, json_file)\n                json_file.truncate()\n\n            # print(f\"Success_count number is {success_count}, Saved faceid_embeds: {faceid_embed_path}, Updated JSON: {json_file_path}\")\n\nif __name__ == \"__main__\":\n    \n    ### Specify folder path\n    origin_img_path = \"./FGID_Origin\" # read_path for customized IMGs\n    resize_IMG_path = \"./FGID_resize\" # read_path for customized resized IMGs (generated in FGID_mask.py)\n\n    faceid_path = \"./FGID_faceID\" # save_path for faceid embedding (.bin)\n    json_save_path = \"./FGID_JSON\" # save_path for faceid information\n\n    process_faceid(origin_path=origin_img_path, resize_path=resize_IMG_path, \\\n                    faceid_path=faceid_path, json_path=json_save_path)\n\n'''\nAdditional instructions:\n\n(1) Specify folder path, then CUDA_VISIBLE_DEVICES=0 python ./FGID_faceid_embeds.py\n\n(2) Note: Run FGID_mask.py first, and then FGID_faceid_embeds.py to ensure that the json data is created smoothly.\n\n(3) JSON data should be constructed as follows:\n{\n\"origin_IMG\": \"15/0062963.png\", \n\"resize_IMG\": \"15/0062963_resize.png\", \n\"parsing_color_IMG\": \"15/0062963_color.png\", \n\"parsing_mask_IMG\": \"15/0062963_mask.png\"\n\"id_embed_file_resize\": \"15/0061175_resize_faceid.bin\", ### may not exist, and the training process uses ZERO embedding instead.\n\"id_embed_file_origin\": \"15/0061175_faceid.bin\", ### may not exist\n\"vqa_llva\": \"The image features a young woman with short hair, wearing a black shirt and a black jacket. \n            She has a smiling expression on her face, and her eyes are open. The woman appears to be the main subject of the photo.\",\n\"vqa_llva_face_caption\": \"The person in the image has a short haircut, which gives her a modern and stylish appearance. \n            She has a small nose, which is a prominent feature of her face. Her eyes are large and wide-set, adding to her striking facial features. \n            The woman has a thin mouth and a small chin, which further accentuates her overall look. Her ears are small and subtle, blending in with her hairstyle.\"\n}\n'''\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "data/FGID_caption.py", "content": "'''\nOfficial implementation of FGID data (facial caption) production script\nAuthor: Jiehui Huang\nHugging Face Demo: https://huggingface.co/spaces/JackAILab/ConsistentID\nProject: https://ssugarwh.github.io/consistentid.github.io/\n'''\n\nimport os\nimport json\nimport sys\nsys.path.append(\"./Llava1.5/LLaVA\")\n# cd ConsistentID/models/LLaVA1.5\n# git clone https://github.com/haotian-liu/LLaVA.git\n\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path\nfrom llava.eval.run_llava import eval_model\nfrom tqdm import tqdm\n\nmodel_path = \"liuhaotian/llava-v1.5-7b\" \nprompt1 = \"Please describe the people in the image, including their gender, \\\n        age, clothing, facial expressions, and any other distinguishing features.\" ### item[\"vqa_llva\"]\nprompt2 = \"Describe this person's facial features for me, including \\\n        face, ears, eyes, nose, and mouth.\" ### item[\"vqa_llva_more_face_detail\"]\n\n### Prompt generated by LLVA1.5 to capture caption\nimage_file = \"./demo_IMG.jpg\"\n\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path=model_path,\n    model_base=None,\n    model_name=get_model_name_from_path(model_path),\n    load_4bit=True # Save GPU memory\n) # device=\"cuda\" default\n\ndef process_caption(image_folder=None, json_folder=None):\n\n    total_files = sum(len(files) for _, _, files in os.walk(image_folder))\n\n    for root, dirs, files in os.walk(image_folder):\n        for image_file in tqdm(files, total=total_files, desc=\"Processing Files\"):\n            if image_file.endswith(('.jpg', '.jpeg', '.png')):\n\n                image_path = os.path.join(root, image_file)\n\n                args = type('Args', (), {\n                    \"model_path\": model_path,\n                    \"model_base\": None,\n                    \"model_name\": get_model_name_from_path(model_path),\n                    \"query\": prompt1,\n                    \"conv_mode\": None,\n                    \"image_file\": image_path,  ### Loop through different files\n                    \"sep\": \",\",\n                    \"temperature\": 0,\n                    \"top_p\": None,\n                    \"num_beams\": 1,\n                    \"max_new_tokens\": 512\n                })()\n\n                generated_text_vqa1 = eval_model(args, tokenizer, model, image_processor) \n                image_id = os.path.splitext(image_file)[0]\n                relative_path = os.path.relpath(root, image_folder)\n\n                args = type('Args', (), {\n                    \"model_path\": model_path,\n                    \"model_base\": None,\n                    \"model_name\": get_model_name_from_path(model_path),\n                    \"query\": prompt2,\n                    \"conv_mode\": None,\n                    \"image_file\": image_path,  ### Loop through different files\n                    \"sep\": \",\",\n                    \"temperature\": 0,\n                    \"top_p\": None,\n                    \"num_beams\": 1,\n                    \"max_new_tokens\": 512\n                })()\n\n                generated_text_vqa2 = eval_model(args, tokenizer, model,image_processor)  \n\n                ### Build JSON data\n                json_file_path = os.path.join(json_folder, relative_path, image_id + '.json')\n                data1 = {\"vqa_llva\": generated_text_vqa1}\n                data2 = {\"vqa_llva_face_caption\": generated_text_vqa2}\n\n                with open(json_file_path, 'r') as f:\n                    data = json.load(f)\n                    if \"vqa_llva\" in data:\n                        print(\"The key vqa_llva exists in the JSON file.\")\n                        continue\n                    else:\n                        print(\"Key vqa_llva does not exist in the JSON file.\")\n                \n                ### To update the corresponding JSON file, you need to first run the FGID_mask.py file\n                with open(json_file_path, 'r+') as json_file:\n                    data1 = json.load(json_file)\n                    data1['vqa_llva'] = generated_text_vqa1\n                    json_file.seek(0) \n                    json.dump(data1, json_file)\n                    json_file.truncate()\n                with open(json_file_path, 'r+') as json_file:\n                    data2 = json.load(json_file)\n                    data2['vqa_llva_more_face_detail'] = generated_text_vqa2\n                    json_file.seek(0) \n                    json.dump(data2, json_file)\n                    json_file.truncate()\n\n                print(f\"Processed {image_file}, JSON file saved at: {json_file_path}\")\n\n\nif __name__ == \"__main__\":\n    \n    ### Specify folder path\n    origin_img_path = \"./FGID_Origin\" # read_path for customized IMGs\n    json_save_path = \"./FGID_JSON\" # save_path for fine_caption information\n\n    process_caption(image_folder=origin_img_path, json_folder=json_save_path)\n\n\n\n'''\nAdditional instructions:\n\n(1) Specify folder path, then CUDA_VISIBLE_DEVICES=0 python ./FGID_caption.py\n\n(2) Note: Run FGID_mask.py first, and then FGID_caption.py to ensure that the json data is created smoothly.\n\n(3) JSON data should be constructed as follows:\n{\n\"origin_IMG\": \"15/0062963.png\", \n\"resize_IMG\": \"15/0062963_resize.png\", \n\"parsing_color_IMG\": \"15/0062963_color.png\", \n\"parsing_mask_IMG\": \"15/0062963_mask.png\"\n\"vqa_llva\": \"The image features a young woman with short hair, wearing a black shirt and a black jacket. \n            She has a smiling expression on her face, and her eyes are open. The woman appears to be the main subject of the photo.\",\n\"vqa_llva_face_caption\": \"The person in the image has a short haircut, which gives her a modern and stylish appearance. \n            She has a small nose, which is a prominent feature of her face. Her eyes are large and wide-set, adding to her striking facial features. \n            The woman has a thin mouth and a small chin, which further accentuates her overall look. Her ears are small and subtle, blending in with her hairstyle.\"\n}\n\n'''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/__init__.py", "content": "#__init__.py\n# from BiSeNet.model import *"}
{"type": "source_file", "path": "models/BiSeNet/face_dataset.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nimport os.path as osp\nimport os\nfrom PIL import Image\nimport numpy as np\nimport json\nimport cv2\n\nfrom transform import *\n\n\n\nclass FaceMask(Dataset):\n    def __init__(self, rootpth, cropsize=(640, 480), mode='train', *args, **kwargs):\n        super(FaceMask, self).__init__(*args, **kwargs)\n        assert mode in ('train', 'val', 'test')\n        self.mode = mode\n        self.ignore_lb = 255\n        self.rootpth = rootpth\n\n        self.imgs = os.listdir(os.path.join(self.rootpth, 'CelebA-HQ-img'))\n\n        #  pre-processing\n        self.to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ])\n        self.trans_train = Compose([\n            ColorJitter(\n                brightness=0.5,\n                contrast=0.5,\n                saturation=0.5),\n            HorizontalFlip(),\n            RandomScale((0.75, 1.0, 1.25, 1.5, 1.75, 2.0)),\n            RandomCrop(cropsize)\n            ])\n\n    def __getitem__(self, idx):\n        impth = self.imgs[idx]\n        img = Image.open(osp.join(self.rootpth, 'CelebA-HQ-img', impth))\n        img = img.resize((512, 512), Image.BILINEAR)\n        label = Image.open(osp.join(self.rootpth, 'mask', impth[:-3]+'png')).convert('P')\n        # print(np.unique(np.array(label)))\n        if self.mode == 'train':\n            im_lb = dict(im=img, lb=label)\n            im_lb = self.trans_train(im_lb)\n            img, label = im_lb['im'], im_lb['lb']\n        img = self.to_tensor(img)\n        label = np.array(label).astype(np.int64)[np.newaxis, :]\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nif __name__ == \"__main__\":\n    face_data = '/home/zll/data/CelebAMask-HQ/CelebA-HQ-img'\n    face_sep_mask = '/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno'\n    mask_path = '/home/zll/data/CelebAMask-HQ/mask'\n    counter = 0\n    total = 0\n    for i in range(15):\n        # files = os.listdir(osp.join(face_sep_mask, str(i)))\n\n        atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',\n                'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']\n\n        for j in range(i*2000, (i+1)*2000):\n\n            mask = np.zeros((512, 512))\n\n            for l, att in enumerate(atts, 1):\n                total += 1\n                file_name = ''.join([str(j).rjust(5, '0'), '_', att, '.png'])\n                path = osp.join(face_sep_mask, str(i), file_name)\n\n                if os.path.exists(path):\n                    counter += 1\n                    sep_mask = np.array(Image.open(path).convert('P'))\n                    # print(np.unique(sep_mask))\n\n                    mask[sep_mask == 225] = l\n            cv2.imwrite('{}/{}.png'.format(mask_path, j), mask)\n            print(j)\n\n    print(counter, total)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "attention.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom diffusers.models.lora import LoRALinearLayer\nfrom functions import AttentionMLP\nfrom diffusers.utils.import_utils import is_xformers_available\nif is_xformers_available():\n    import xformers\n\nclass FuseModule(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.mlp1 = MLP(embed_dim * 2, embed_dim, embed_dim, use_residual=False)\n        self.mlp2 = MLP(embed_dim, embed_dim, embed_dim, use_residual=True)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def fuse_fn(self, prompt_embeds, id_embeds):\n        stacked_id_embeds = torch.cat([prompt_embeds, id_embeds], dim=-1)\n        stacked_id_embeds = self.mlp1(stacked_id_embeds) + prompt_embeds\n        stacked_id_embeds = self.mlp2(stacked_id_embeds)\n        stacked_id_embeds = self.layer_norm(stacked_id_embeds)\n        return stacked_id_embeds\n\n    def forward(\n        self,\n        prompt_embeds,\n        id_embeds,\n        class_tokens_mask,\n        valid_id_mask,\n    ) -> torch.Tensor:\n        \n        id_embeds = id_embeds.to(prompt_embeds.dtype)\n        batch_size, max_num_inputs = id_embeds.shape[:2]\n        seq_length = prompt_embeds.shape[1]\n        flat_id_embeds = id_embeds.view(-1, id_embeds.shape[-2], id_embeds.shape[-1])\n\n        valid_id_embeds = flat_id_embeds[valid_id_mask.flatten()]\n\n        prompt_embeds = prompt_embeds.view(-1, prompt_embeds.shape[-1])\n        class_tokens_mask = class_tokens_mask.view(-1)\n        valid_id_embeds = valid_id_embeds.view(-1, valid_id_embeds.shape[-1])\n        image_token_embeds = prompt_embeds[class_tokens_mask]\n        stacked_id_embeds = self.fuse_fn(image_token_embeds, valid_id_embeds)\n        assert class_tokens_mask.sum() == stacked_id_embeds.shape[0], f\"{class_tokens_mask.sum()} != {stacked_id_embeds.shape[0]}\"\n        prompt_embeds.masked_scatter_(class_tokens_mask[:, None], stacked_id_embeds.to(prompt_embeds.dtype)) \n        updated_prompt_embeds = prompt_embeds.view(batch_size, seq_length, -1)\n\n        return updated_prompt_embeds\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim, use_residual=True):\n        super().__init__()\n        if use_residual:\n            assert in_dim == out_dim\n        self.layernorm = nn.LayerNorm(in_dim)\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n        self.use_residual = use_residual\n        self.act_fn = nn.GELU()\n\n    def forward(self, x):\n\n        residual = x\n        x = self.layernorm(x)\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.fc2(x)\n        if self.use_residual:\n            x = x + residual\n        return x\n\nclass FacialEncoder(nn.Module):\n    def __init__(self,image_CLIPModel_encoder=None,embedding_dim=1280, output_dim=768, embed_dim=768):\n        super().__init__()\n        self.visual_projection = AttentionMLP(embedding_dim=embedding_dim, output_dim=output_dim)\n        self.fuse_module = FuseModule(embed_dim=embed_dim)\n\n    def forward(self, prompt_embeds, multi_image_embeds, class_tokens_mask, valid_id_mask):\n        \n        bs, num_inputs, token_length, image_dim = multi_image_embeds.shape\n        multi_image_embeds_view = multi_image_embeds.view(bs * num_inputs, token_length, image_dim)\n\n        id_embeds = self.visual_projection(multi_image_embeds_view)\n        id_embeds = id_embeds.view(bs, num_inputs, 1, -1)\n\n        updated_prompt_embeds = self.fuse_module(prompt_embeds, id_embeds, class_tokens_mask, valid_id_mask)\n\n        return updated_prompt_embeds\n      \nclass Consistent_AttProcessor(nn.Module):\n    \n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n        rank=4,\n        network_alpha=None,\n        lora_scale=1.0,\n    ):\n        super().__init__()\n        \n        self.rank = rank\n        self.lora_scale = lora_scale\n        \n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        if is_xformers_available():\n            ### xformers\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n    \n\nclass Consistent_IPAttProcessor(nn.Module):\n\n    def __init__(\n            self, \n            hidden_size, \n            cross_attention_dim=None, \n            rank=4, \n            network_alpha=None, \n            lora_scale=1.0, \n            scale=1.0, \n            num_tokens=4):\n        super().__init__()\n        \n        self.rank = rank\n        self.lora_scale = lora_scale\n        self.num_tokens = num_tokens\n\n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        \n        \n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self, \n        attn,\n        hidden_states, \n        encoder_hidden_states=None, \n        attention_mask=None, \n        scale=1.0,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        \n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n        \n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n        ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n\n        ip_hidden_states = F.scaled_dot_product_attention(\n            query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n\n        ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        ip_hidden_states = ip_hidden_states.to(query.dtype)\n\n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n"}
{"type": "source_file", "path": "evaluation/style_template.py", "content": "style_list = [\n    {\n        \"name\": \"Mars\",\n        \"prompt\": \"{prompt}, Post-apocalyptic. Mars Colony, Scavengers roam the wastelands searching for valuable resources, rovers, bright morning sunlight shining, (detailed) (intricate) (8k) (HDR) (cinematic lighting) (sharp focus)\",\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n    },   \n    {\n        \"name\": \"Neonpunk\",\n        \"prompt\": \"neonpunk style {prompt} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\",\n        \"negative_prompt\": \"painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\",\n    },         \n    {\n        \"name\": \"Fantasy art\",\n        \"prompt\": \"ethereal fantasy concept art of  {prompt} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\",\n        \"negative_prompt\": \"photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\",\n    },\n    {\n        \"name\": \"Enhance\",\n        \"prompt\": \"breathtaking {prompt} . award-winning, professional, highly detailed\",\n        \"negative_prompt\": \"ugly, deformed, noisy, blurry, distorted, grainy\",\n    },\n    {\n        \"name\": \"Comic book\",\n        \"prompt\": \"comic {prompt} . graphic illustration, comic art, graphic novel art, vibrant, highly detailed\",\n        \"negative_prompt\": \"photograph, deformed, glitch, noisy, realistic, stock photo\",\n    },\n    {\n        \"name\": \"Lowpoly\",\n        \"prompt\": \"low-poly style {prompt} . low-poly game art, polygon mesh, jagged, blocky, wireframe edges, centered composition\",\n        \"negative_prompt\": \"noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\",\n    },\n    {\n        \"name\": \"Line art\",\n        \"prompt\": \"line art drawing {prompt} . professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n        \"negative_prompt\": \"anime, photorealistic, 35mm film, deformed, glitch, blurry, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, mutated, realism, realistic, impressionism, expressionism, oil, acrylic\",\n    },\n    {\n        \"name\": \"Watercolor\",\n        \"prompt\": \"watercolor painting, {prompt}. vibrant, beautiful, painterly, detailed, textural, artistic\",\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, anime, photorealistic, 35mm film, deformed, glitch, low contrast, noisy\",\n    },\n    {\n        \"name\": \"Vibrant Color\",\n        \"prompt\": \"vibrant colorful, ink sketch|vector|2d colors, at nightfall, sharp focus, {prompt}, highly detailed, sharp focus, the clouds,colorful,ultra sharpness\",\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n    },\n    {\n        \"name\": \"Jungle\",\n        \"prompt\": 'waist-up \"{prompt} in a Jungle\" by Syd Mead, tangerine cold color palette, muted colors, detailed, 8k,photo r3al,dripping paint,3d toon style,3d style,Movie Still',\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n    },    \n    {\n        \"name\": \"Snow\",\n        \"prompt\": \"cinema 4d render, {prompt}, high contrast, vibrant and saturated, sico style, surrounded by magical glow,floating ice shards, snow crystals, cold, windy background, frozen natural landscape in background  cinematic atmosphere,highly detailed, sharp focus, intricate design, 3d, unreal engine, octane render, CG best quality, highres, photorealistic, dramatic lighting, artstation, concept art, cinematic, epic Steven Spielberg movie still, sharp focus, smoke, sparks, art by pascal blanche and greg rutkowski and repin, trending on artstation, hyperrealism painting, matte painting, 4k resolution\",\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n    },        \n    {\n        \"name\": \"Film Noir\",\n        \"prompt\": \"film noir style, ink sketch|vector, {prompt} highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\",\n        \"negative_prompt\": \"(lowres, low quality, worst quality:1.2), (text:1.2), watermark, (frame:1.2), deformed, ugly, deformed eyes, blur, out of focus, blurry, deformed cat, deformed, photo, anthropomorphic cat, monochrome, photo, pet collar, gun, weapon, blue, 3d, drones, drone, buildings in background, green\",\n    },   \n    {\n        \"name\": \"Cinematic\",\n        \"prompt\": \"cinematic still {prompt} . emotional, harmonious, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n        \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\",\n    },\n    {\n        \"name\": \"Disney Charactor\",\n        \"prompt\": \"A Pixar animation character of {prompt} . pixar-style, studio anime, Disney, high-quality\",\n        \"negative_prompt\": \"lowres, bad anatomy, bad hands, text, bad eyes, bad arms, bad legs, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, blurry, grayscale, noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\",\n    },\n    {\n        \"name\": \"Digital Art\",\n        \"prompt\": \"concept art {prompt} . digital artwork, illustrative, painterly, matte painting, highly detailed\",\n        \"negative_prompt\": \"photo, photorealistic, realism, ugly\",\n    },\n    {\n        \"name\": \"Photographic (Default)\",\n        \"prompt\": \"cinematic photo {prompt} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\n        \"negative_prompt\": \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\",\n    },     \n    {\n        \"name\": \"(No style)\",\n        \"prompt\": \"{prompt}\",\n        \"negative_prompt\": \"\",\n    },    \n]\n\nstyles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}"}
{"type": "source_file", "path": "evaluation/convert_weights.py", "content": "import torch\nfrom safetensors import safe_open\n\n### Used to remove redundant parameters and minimize the model.\nckpt = \"./pytorch_model.bin\"\nstate_dict = torch.load(ckpt, map_location=\"cuda\")\n\nimage_proj_sd = {}\nadapter_modules = {}\nFacialEncoder = {}\n\n# open('./model_struct.txt', 'w').write('\\n'.join([name for name in state_dict.keys()])) ### view model structures\n\nfor k in state_dict:\n    if k.startswith(\"unet\"):\n        pass ### unet freezed\n    elif k.startswith(\"image_proj_model\"):\n        image_proj_sd[k.replace(\"image_proj_model.\", \"\")] = state_dict[k]\n    elif k.startswith(\"adapter_modules\"):   \n        adapter_modules[k.replace(\"adapter_modules.\", \"\")] = state_dict[k]\n    elif k.startswith(\"FacialEncoder\"):\n        FacialEncoder[k.replace(\"FacialEncoder.\", \"\")] = state_dict[k]\n\nstate_dict_path = \"./ConsistentID-v1.bin\"\ntorch.save({\"image_proj_model\": image_proj_sd, \"adapter_modules\": adapter_modules, \"FacialEncoder\": FacialEncoder}, state_dict_path)\nprint(f\"Sucessful saved at: {state_dict_path}\")\n\n\n\n\n"}
{"type": "source_file", "path": "data/FGID_fuse_JSON.py", "content": "'''\nOfficial implementation of FGID data (facial caption) production script\nAuthor: Jiehui Huang\nHugging Face Demo: https://huggingface.co/spaces/JackAILab/ConsistentID\nProject: https://ssugarwh.github.io/consistentid.github.io/\n'''\n\nimport os\nimport json\n\nfrom tqdm import tqdm\n\njson_folder_path = \"./FGID_JSON\" # read_path for all FGID json data\noutput_file_path = \"./JSON_all.json\" # save_path for all FGID json data\n\nall_data = []\n\n\ntotal_files = sum(len(files) for _, _, files in os.walk(json_folder_path))\nfor root, dirs, files in os.walk(json_folder_path):\n    for file_name in tqdm(files, total=total_files, desc=\"Processing Files\"): # (('.png', '.jpg', '.jpeg')):\n        json_path = os.path.join(root, file_name)\n        with open(json_path, 'r') as json_file:\n            json_data = json.load(json_file)\n            all_data.append(json_data)\n\n### Write all data to a new JSON file\nwith open(output_file_path, 'w') as output_file:\n    json.dump(all_data, output_file)\n\nprint(f\"Merged all JSON files into {output_file_path}\")\n\n\n\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/makeup.py", "content": "import cv2\nimport os\nimport numpy as np\nfrom skimage.filters import gaussian\n\n\ndef sharpen(img):\n    img = img * 1.0\n    gauss_out = gaussian(img, sigma=5, multichannel=True)\n\n    alpha = 1.5\n    img_out = (img - gauss_out) * alpha + img\n\n    img_out = img_out / 255.0\n\n    mask_1 = img_out < 0\n    mask_2 = img_out > 1\n\n    img_out = img_out * (1 - mask_1)\n    img_out = img_out * (1 - mask_2) + mask_2\n    img_out = np.clip(img_out, 0, 1)\n    img_out = img_out * 255\n    return np.array(img_out, dtype=np.uint8)\n\n\ndef hair(image, parsing, part=17, color=[230, 50, 20]):\n    b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n    tar_color = np.zeros_like(image)\n    tar_color[:, :, 0] = b\n    tar_color[:, :, 1] = g\n    tar_color[:, :, 2] = r\n\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    tar_hsv = cv2.cvtColor(tar_color, cv2.COLOR_BGR2HSV)\n\n    if part == 12 or part == 13:\n        image_hsv[:, :, 0:2] = tar_hsv[:, :, 0:2]\n    else:\n        image_hsv[:, :, 0:1] = tar_hsv[:, :, 0:1]\n\n    changed = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2BGR)\n\n    if part == 17:\n        changed = sharpen(changed)\n\n    changed[parsing != part] = image[parsing != part]\n    # changed = cv2.resize(changed, (512, 512))\n    return changed\n\n#\n# def lip(image, parsing, part=17, color=[230, 50, 20]):\n#     b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n#     tar_color = np.zeros_like(image)\n#     tar_color[:, :, 0] = b\n#     tar_color[:, :, 1] = g\n#     tar_color[:, :, 2] = r\n#\n#     image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n#     il, ia, ib = cv2.split(image_lab)\n#\n#     tar_lab = cv2.cvtColor(tar_color, cv2.COLOR_BGR2Lab)\n#     tl, ta, tb = cv2.split(tar_lab)\n#\n#     image_lab[:, :, 0] = np.clip(il - np.mean(il) + tl, 0, 100)\n#     image_lab[:, :, 1] = np.clip(ia - np.mean(ia) + ta, -127, 128)\n#     image_lab[:, :, 2] = np.clip(ib - np.mean(ib) + tb, -127, 128)\n#\n#\n#     changed = cv2.cvtColor(image_lab, cv2.COLOR_Lab2BGR)\n#\n#     if part == 17:\n#         changed = sharpen(changed)\n#\n#     changed[parsing != part] = image[parsing != part]\n#     # changed = cv2.resize(changed, (512, 512))\n#     return changed\n\n\nif __name__ == '__main__':\n    # 1  face\n    # 10 nose\n    # 11 teeth\n    # 12 upper lip\n    # 13 lower lip\n    # 17 hair\n    num = 116\n    table = {\n        'hair': 17,\n        'upper_lip': 12,\n        'lower_lip': 13\n    }\n    image_path = '/home/zll/data/CelebAMask-HQ/test-img/{}.jpg'.format(num)\n    parsing_path = 'res/test_res/{}.png'.format(num)\n\n    image = cv2.imread(image_path)\n    ori = image.copy()\n    parsing = np.array(cv2.imread(parsing_path, 0))\n    parsing = cv2.resize(parsing, image.shape[0:2], interpolation=cv2.INTER_NEAREST)\n\n    parts = [table['hair'], table['upper_lip'], table['lower_lip']]\n    # colors = [[20, 20, 200], [100, 100, 230], [100, 100, 230]]\n    colors = [[100, 200, 100]]\n    for part, color in zip(parts, colors):\n        image = hair(image, parsing, part, color)\n    cv2.imwrite('res/makeup/116_ori.png', cv2.resize(ori, (512, 512)))\n    cv2.imwrite('res/makeup/116_2.png', cv2.resize(image, (512, 512)))\n\n    cv2.imshow('image', cv2.resize(ori, (512, 512)))\n    cv2.imshow('color', cv2.resize(image, (512, 512)))\n\n    # cv2.imshow('image', ori)\n    # cv2.imshow('color', image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/logger.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport os.path as osp\nimport time\nimport sys\nimport logging\n\nimport torch.distributed as dist\n\n\ndef setup_logger(logpth):\n    logfile = 'BiSeNet-{}.log'.format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n    logfile = osp.join(logpth, logfile)\n    FORMAT = '%(levelname)s %(filename)s(%(lineno)d): %(message)s'\n    log_level = logging.INFO\n    if dist.is_initialized() and not dist.get_rank()==0:\n        log_level = logging.ERROR\n    logging.basicConfig(level=log_level, format=FORMAT, filename=logfile)\n    logging.root.addHandler(logging.StreamHandler())\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/model.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom models.BiSeNet.resnet import Resnet18\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\n### This is not used, since I replace this with the resnet feature with the same size\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        ## here self.sp is deleted\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == \"__main__\":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/deeplab.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\nfrom models._util import try_index\nfrom .bn import ABN\n\n\nclass DeeplabV3(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=256,\n                 dilations=(12, 24, 36),\n                 norm_act=ABN,\n                 pooling_size=None):\n        super(DeeplabV3, self).__init__()\n        self.pooling_size = pooling_size\n\n        self.map_convs = nn.ModuleList([\n            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[0], padding=dilations[0]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[1], padding=dilations[1]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[2], padding=dilations[2])\n        ])\n        self.map_bn = norm_act(hidden_channels * 4)\n\n        self.global_pooling_conv = nn.Conv2d(in_channels, hidden_channels, 1, bias=False)\n        self.global_pooling_bn = norm_act(hidden_channels)\n\n        self.red_conv = nn.Conv2d(hidden_channels * 4, out_channels, 1, bias=False)\n        self.pool_red_conv = nn.Conv2d(hidden_channels, out_channels, 1, bias=False)\n        self.red_bn = norm_act(out_channels)\n\n        self.reset_parameters(self.map_bn.activation, self.map_bn.slope)\n\n    def reset_parameters(self, activation, slope):\n        gain = nn.init.calculate_gain(activation, slope)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight.data, gain)\n                if hasattr(m, \"bias\") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, ABN):\n                if hasattr(m, \"weight\") and m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if hasattr(m, \"bias\") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Map convolutions\n        out = torch.cat([m(x) for m in self.map_convs], dim=1)\n        out = self.map_bn(out)\n        out = self.red_conv(out)\n\n        # Global pooling\n        pool = self._global_pooling(x)\n        pool = self.global_pooling_conv(pool)\n        pool = self.global_pooling_bn(pool)\n        pool = self.pool_red_conv(pool)\n        if self.training or self.pooling_size is None:\n            pool = pool.repeat(1, 1, x.size(2), x.size(3))\n\n        out += pool\n        out = self.red_bn(out)\n        return out\n\n    def _global_pooling(self, x):\n        if self.training or self.pooling_size is None:\n            pool = x.view(x.size(0), x.size(1), -1).mean(dim=-1)\n            pool = pool.view(x.size(0), x.size(1), 1, 1)\n        else:\n            pooling_size = (min(try_index(self.pooling_size, 0), x.shape[2]),\n                            min(try_index(self.pooling_size, 1), x.shape[3]))\n            padding = (\n                (pooling_size[1] - 1) // 2,\n                (pooling_size[1] - 1) // 2 if pooling_size[1] % 2 == 1 else (pooling_size[1] - 1) // 2 + 1,\n                (pooling_size[0] - 1) // 2,\n                (pooling_size[0] - 1) // 2 if pooling_size[0] % 2 == 1 else (pooling_size[0] - 1) // 2 + 1\n            )\n\n            pool = functional.avg_pool2d(x, pooling_size, stride=1)\n            pool = functional.pad(pool, pad=padding, mode=\"replicate\")\n        return pool\n"}
{"type": "source_file", "path": "functions.py", "content": "import numpy as np\nimport math\nimport types\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport re\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\nfrom PIL import Image\n\ndef extract_first_sentence(text):\n    end_index = text.find('.')\n    if end_index != -1:\n        first_sentence = text[:end_index + 1]\n        return first_sentence.strip()\n    else:\n        return text.strip()\n    \nimport re\ndef remove_duplicate_keywords(text, keywords): ### This function can continue to be optimized\n    keyword_counts = {}\n\n    words = re.findall(r'\\b\\w+\\b|[.,;!?]', text)\n\n    for keyword in keywords:\n        keyword_counts[keyword] = 0\n        for i, word in enumerate(words):\n            if word.lower() == keyword.lower():\n                keyword_counts[keyword] += 1\n                if keyword_counts[keyword] > 1:\n                    words[i] = \"\"\n    processed_text = \" \".join(words)\n\n    return processed_text\n\ndef process_text_with_markers(text, parsing_mask_list):\n    keywords = [\"face\", \"ears\", \"eyes\", \"nose\", \"mouth\"]\n    text = remove_duplicate_keywords(text, keywords)\n    key_parsing_mask_markers = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n    mapping = {\n        \"Face\": \"face\",\n        \"Left_Ear\": \"ears\",\n        \"Right_Ear\": \"ears\",\n        \"Left_Eye\": \"eyes\",\n        \"Right_Eye\": \"eyes\",\n        \"Nose\": \"nose\",\n        \"Upper_Lip\": \"mouth\",\n        \"Lower_Lip\": \"mouth\",\n    }\n    facial_features_align = []\n    markers_align = []\n    for key in key_parsing_mask_markers:\n        if key in parsing_mask_list:\n            mapped_key = mapping.get(key, key.lower())\n            if mapped_key not in facial_features_align:\n                facial_features_align.append(mapped_key)\n                markers_align.append(\"<|\"+mapped_key+\"|>\")\n\n    text_marked = text\n    align_parsing_mask_list = parsing_mask_list\n    for feature, marker in zip(facial_features_align[::-1], markers_align[::-1]):\n        pattern = rf'\\b{feature}\\b'  \n        text_marked_new = re.sub(pattern, f'{feature} {marker}', text_marked, count=1)\n        if text_marked == text_marked_new:\n            for key, value in mapping.items():\n                if value == feature:\n                    if key in align_parsing_mask_list:\n                        del align_parsing_mask_list[key]   \n\n        text_marked = text_marked_new \n\n    text_marked = text_marked.replace('\\n', '')\n\n    ordered_text = []\n    text_none_makers = []\n    facial_marked_count = 0\n    skip_count = 0\n    for marker in markers_align:\n        start_idx = text_marked.find(marker)\n        end_idx = start_idx + len(marker)\n\n        while start_idx > 0 and text_marked[start_idx - 1] not in [\",\", \".\", \";\"]:\n            start_idx -= 1\n\n        while end_idx < len(text_marked) and text_marked[end_idx] not in [\",\", \".\", \";\"]:\n            end_idx += 1\n\n        context = text_marked[start_idx:end_idx].strip()\n        if context == \"\":\n            text_none_makers.append(text_marked[:end_idx])\n        else:\n            if skip_count!=0:\n                skip_count -= 1 \n                continue\n            else:\n                ordered_text.append(context + \",\") \n                text_delete_makers = text_marked[:start_idx] + text_marked[end_idx:]\n                text_marked = text_delete_makers\n                facial_marked_count += 1\n\n    align_marked_text = \" \".join(ordered_text)\n    replace_list = [\"<|face|>\", \"<|ears|>\", \"<|nose|>\", \"<|eyes|>\", \"<|mouth|>\"] \n    for item in replace_list:\n        align_marked_text = align_marked_text.replace(item, \"<|facial|>\")\n\n    return align_marked_text, align_parsing_mask_list\n\ndef tokenize_and_mask_noun_phrases_ends(text, image_token_id, facial_token_id, tokenizer):\n    input_ids = tokenizer.encode(text)\n    image_noun_phrase_end_mask = [False for _ in input_ids] \n    facial_noun_phrase_end_mask = [False for _ in input_ids]\n    clean_input_ids = []\n    clean_index = 0\n    image_num = 0\n\n    for i, id in enumerate(input_ids):\n        if id == image_token_id:\n            image_noun_phrase_end_mask[clean_index + image_num - 1] = True\n            image_num += 1\n        elif id == facial_token_id:\n            facial_noun_phrase_end_mask[clean_index - 1] = True   \n        else:\n            clean_input_ids.append(id)\n            clean_index += 1\n\n    max_len = tokenizer.model_max_length \n\n    if len(clean_input_ids) > max_len:\n        clean_input_ids = clean_input_ids[:max_len]\n    else:\n        clean_input_ids = clean_input_ids + [tokenizer.pad_token_id] * (\n            max_len - len(clean_input_ids)\n        )\n\n    if len(image_noun_phrase_end_mask) > max_len: \n        image_noun_phrase_end_mask = image_noun_phrase_end_mask[:max_len]\n    else:\n        image_noun_phrase_end_mask = image_noun_phrase_end_mask + [False] * (\n            max_len - len(image_noun_phrase_end_mask)\n        )\n\n    if len(facial_noun_phrase_end_mask) > max_len: \n        facial_noun_phrase_end_mask = facial_noun_phrase_end_mask[:max_len]\n    else:\n        facial_noun_phrase_end_mask = facial_noun_phrase_end_mask + [False] * (\n            max_len - len(facial_noun_phrase_end_mask)\n        )        \n\n    clean_input_ids = torch.tensor(clean_input_ids, dtype=torch.long)\n    image_noun_phrase_end_mask = torch.tensor(image_noun_phrase_end_mask, dtype=torch.bool)\n    facial_noun_phrase_end_mask = torch.tensor(facial_noun_phrase_end_mask, dtype=torch.bool)\n    \n    return clean_input_ids.unsqueeze(0), image_noun_phrase_end_mask.unsqueeze(0), facial_noun_phrase_end_mask.unsqueeze(0)\n\ndef prepare_image_token_idx(image_token_mask, facial_token_mask, max_num_objects=2, max_num_facials=5):\n    image_token_idx = torch.nonzero(image_token_mask, as_tuple=True)[1]\n    image_token_idx_mask = torch.ones_like(image_token_idx, dtype=torch.bool)\n    if len(image_token_idx) < max_num_objects:\n        image_token_idx = torch.cat(\n            [ \n                image_token_idx,\n                torch.zeros(max_num_objects - len(image_token_idx), dtype=torch.long),\n            ]\n        )\n        image_token_idx_mask = torch.cat(\n            [ \n                image_token_idx_mask,\n                torch.zeros(\n                    max_num_objects - len(image_token_idx_mask),\n                    dtype=torch.bool,\n                ),\n            ]\n        )\n\n    facial_token_idx = torch.nonzero(facial_token_mask, as_tuple=True)[1]\n    facial_token_idx_mask = torch.ones_like(facial_token_idx, dtype=torch.bool)     \n    if len(facial_token_idx) < max_num_facials:\n        facial_token_idx = torch.cat(\n            [ \n                facial_token_idx,\n                torch.zeros(max_num_facials - len(facial_token_idx), dtype=torch.long),\n            ]\n        )\n        facial_token_idx_mask = torch.cat(\n            [ \n                facial_token_idx_mask,\n                torch.zeros(\n                    max_num_facials - len(facial_token_idx_mask),\n                    dtype=torch.bool,\n                ),\n            ]\n        )\n\n    image_token_idx = image_token_idx.unsqueeze(0)\n    image_token_idx_mask = image_token_idx_mask.unsqueeze(0)\n    \n    facial_token_idx = facial_token_idx.unsqueeze(0)\n    facial_token_idx_mask = facial_token_idx_mask.unsqueeze(0)\n\n    return image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask\n\ndef get_object_localization_loss_for_one_layer(\n    cross_attention_scores,\n    object_segmaps,\n    object_token_idx,\n    object_token_idx_mask,\n    loss_fn,\n):\n    bxh, num_noise_latents, num_text_tokens = cross_attention_scores.shape\n    b, max_num_objects, _, _ = object_segmaps.shape\n    size = int(num_noise_latents**0.5)\n\n    object_segmaps = F.interpolate(object_segmaps, size=(size, size), mode=\"bilinear\", antialias=True)\n\n    object_segmaps = object_segmaps.view(\n        b, max_num_objects, -1\n    )\n\n    num_heads = bxh // b\n    cross_attention_scores = cross_attention_scores.view(b, num_heads, num_noise_latents, num_text_tokens)\n\n    \n    object_token_attn_prob = torch.gather(\n        cross_attention_scores,\n        dim=3,\n        index=object_token_idx.view(b, 1, 1, max_num_objects).expand(\n            b, num_heads, num_noise_latents, max_num_objects\n        ),\n    )\n    object_segmaps = (\n        object_segmaps.permute(0, 2, 1)\n        .unsqueeze(1)\n        .expand(b, num_heads, num_noise_latents, max_num_objects)\n    )\n    loss = loss_fn(object_token_attn_prob, object_segmaps)\n\n    loss = loss * object_token_idx_mask.view(b, 1, max_num_objects)\n    object_token_cnt = object_token_idx_mask.sum(dim=1).view(b, 1) + 1e-5\n    loss = (loss.sum(dim=2) / object_token_cnt).mean()\n\n    return loss\n\n\ndef get_object_localization_loss(\n    cross_attention_scores,\n    object_segmaps,\n    image_token_idx,\n    image_token_idx_mask,\n    loss_fn,\n):  \n    num_layers = len(cross_attention_scores)\n    loss = 0\n    for k, v in cross_attention_scores.items():\n        layer_loss = get_object_localization_loss_for_one_layer(\n            v, object_segmaps, image_token_idx, image_token_idx_mask, loss_fn\n        )\n        loss += layer_loss\n    return loss / num_layers\n\ndef unet_store_cross_attention_scores(unet, attention_scores, layers=5):\n    from diffusers.models.attention_processor import Attention\n\n    UNET_LAYER_NAMES = [ \n        \"down_blocks.0\",\n        \"down_blocks.1\",\n        \"down_blocks.2\",\n        \"mid_block\",\n        \"up_blocks.1\",\n        \"up_blocks.2\",\n        \"up_blocks.3\",\n    ]\n\n    start_layer = (len(UNET_LAYER_NAMES) - layers) // 2\n    end_layer = start_layer + layers   \n    applicable_layers = UNET_LAYER_NAMES[start_layer:end_layer]\n\n    def make_new_get_attention_scores_fn(name):\n        def new_get_attention_scores(module, query, key, attention_mask=None):\n            attention_probs = module.old_get_attention_scores(\n                query, key, attention_mask\n            )\n            attention_scores[name] = attention_probs\n            return attention_probs\n\n        return new_get_attention_scores \n\n    for name, module in unet.named_modules():\n        if isinstance(module, Attention) and \"attn1\" in name:\n            if not any(layer in name for layer in applicable_layers):\n                continue\n \n            module.old_get_attention_scores = module.get_attention_scores\n            module.get_attention_scores = types.MethodType(\n                make_new_get_attention_scores_fn(name), module\n            )\n    return unet\n    \nclass BalancedL1Loss(nn.Module):\n    def __init__(self, threshold=1.0, normalize=False):\n        super().__init__()\n        self.threshold = threshold\n        self.normalize = normalize\n\n    def forward(self, object_token_attn_prob, object_segmaps):\n        if self.normalize:\n            object_token_attn_prob = object_token_attn_prob / (\n                object_token_attn_prob.max(dim=2, keepdim=True)[0] + 1e-5\n            )\n        background_segmaps = 1 - object_segmaps\n        background_segmaps_sum = background_segmaps.sum(dim=2) + 1e-5\n        object_segmaps_sum = object_segmaps.sum(dim=2) + 1e-5\n\n        background_loss = (object_token_attn_prob * background_segmaps).sum(\n            dim=2\n        ) / background_segmaps_sum\n\n        object_loss = (object_token_attn_prob * object_segmaps).sum(\n            dim=2\n        ) / object_segmaps_sum\n\n        return background_loss - object_loss\n\ndef fetch_mask_raw_image(raw_image, mask_image):\n\n    mask_image = mask_image.resize(raw_image.size)\n    mask_raw_image = Image.composite(raw_image, Image.new('RGB', raw_image.size, (0, 0, 0)), mask_image) \n\n    return mask_raw_image\n\nmapping_table = [\n    {\"Mask Value\": 0, \"Body Part\": \"Background\", \"RGB Color\": [0, 0, 0]},\n    {\"Mask Value\": 1, \"Body Part\": \"Face\", \"RGB Color\": [255, 0, 0]},\n    {\"Mask Value\": 2, \"Body Part\": \"Left_Eyebrow\", \"RGB Color\": [255, 85, 0]},\n    {\"Mask Value\": 3, \"Body Part\": \"Right_Eyebrow\", \"RGB Color\": [255, 170, 0]},\n    {\"Mask Value\": 4, \"Body Part\": \"Left_Eye\", \"RGB Color\": [255, 0, 85]},\n    {\"Mask Value\": 5, \"Body Part\": \"Right_Eye\", \"RGB Color\": [255, 0, 170]},\n    {\"Mask Value\": 6, \"Body Part\": \"Hair\", \"RGB Color\": [0, 0, 255]},\n    {\"Mask Value\": 7, \"Body Part\": \"Left_Ear\", \"RGB Color\": [85, 0, 255]},\n    {\"Mask Value\": 8, \"Body Part\": \"Right_Ear\", \"RGB Color\": [170, 0, 255]},\n    {\"Mask Value\": 9, \"Body Part\": \"Mouth_External Contour\", \"RGB Color\": [0, 255, 85]},\n    {\"Mask Value\": 10, \"Body Part\": \"Nose\", \"RGB Color\": [0, 255, 0]},\n    {\"Mask Value\": 11, \"Body Part\": \"Mouth_Inner_Contour\", \"RGB Color\": [0, 255, 170]},\n    {\"Mask Value\": 12, \"Body Part\": \"Upper_Lip\", \"RGB Color\": [85, 255, 0]}, \n    {\"Mask Value\": 13, \"Body Part\": \"Lower_Lip\", \"RGB Color\": [170, 255, 0]},\n    {\"Mask Value\": 14, \"Body Part\": \"Neck\", \"RGB Color\": [0, 85, 255]},\n    {\"Mask Value\": 15, \"Body Part\": \"Neck_Inner Contour\", \"RGB Color\": [0, 170, 255]},\n    {\"Mask Value\": 16, \"Body Part\": \"Cloth\", \"RGB Color\": [255, 255, 0]},\n    {\"Mask Value\": 17, \"Body Part\": \"Hat\", \"RGB Color\": [255, 0, 255]},\n    {\"Mask Value\": 18, \"Body Part\": \"Earring\", \"RGB Color\": [255, 85, 255]},\n    {\"Mask Value\": 19, \"Body Part\": \"Necklace\", \"RGB Color\": [255, 255, 85]},\n    {\"Mask Value\": 20, \"Body Part\": \"Glasses\", \"RGB Color\": [255, 170, 255]},\n    {\"Mask Value\": 21, \"Body Part\": \"Hand\", \"RGB Color\": [255, 0, 255]},\n    {\"Mask Value\": 22, \"Body Part\": \"Wristband\", \"RGB Color\": [0, 255, 255]},\n    {\"Mask Value\": 23, \"Body Part\": \"Clothes_Upper\", \"RGB Color\": [85, 255, 255]},\n    {\"Mask Value\": 24, \"Body Part\": \"Clothes_Lower\", \"RGB Color\": [170, 255, 255]}\n]\n\ndef masks_for_unique_values(image_raw_mask):\n\n    image_array = np.array(image_raw_mask)\n    unique_values, counts = np.unique(image_array, return_counts=True)\n    masks_dict = {}\n    for value in unique_values:\n        binary_image = np.uint8(image_array == value) * 255\n    \n        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        mask = np.zeros_like(image_array)\n\n        for contour in contours:\n            cv2.drawContours(mask, [contour], -1, (255), thickness=cv2.FILLED)\n        \n        if value == 0:\n            body_part=\"WithoutBackground\"\n            mask2 = np.where(mask == 255, 0, 255).astype(mask.dtype)\n            masks_dict[body_part] = Image.fromarray(mask2)\n            \n        body_part = next((entry[\"Body Part\"] for entry in mapping_table if entry[\"Mask Value\"] == value), f\"Unknown_{value}\")\n        if body_part.startswith(\"Unknown_\"):\n            continue            \n\n        masks_dict[body_part] = Image.fromarray(mask)\n    \n    return masks_dict\n\n# FFN\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n\n\ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    x = x.view(bs, length, heads, -1)\n    x = x.transpose(1, 2)\n    x = x.reshape(bs, heads, length, -1)\n    return x\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, n2, D)\n        \"\"\"\n\n        x = self.norm1(x)\n        latents = self.norm2(latents)\n\n        b, l, _ = latents.shape\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        q = reshape_tensor(q, self.heads)\n        k = reshape_tensor(k, self.heads)\n        v = reshape_tensor(v, self.heads)\n\n        # attention\n        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n        weight = (q * scale) @ (k * scale).transpose(-2, -1)\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        out = weight @ v\n\n        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n\n        return self.to_out(out)\n\nclass FacePerceiverResampler(torch.nn.Module):\n    def __init__(\n        self,\n        *,\n        dim=768,\n        depth=4,\n        dim_head=64,\n        heads=16,\n        embedding_dim=1280,\n        output_dim=768,\n        ff_mult=4,\n    ):\n        super().__init__()\n        \n        self.proj_in = torch.nn.Linear(embedding_dim, dim)\n        self.proj_out = torch.nn.Linear(dim, output_dim)\n        self.norm_out = torch.nn.LayerNorm(output_dim)\n        self.layers = torch.nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                torch.nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, latents, x):\n        x = self.proj_in(x)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n  \nclass ProjPlusModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, clip_embeddings_dim=1280, num_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n        self.perceiver_resampler = FacePerceiverResampler(\n            dim=cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=cross_attention_dim // 64,\n            embedding_dim=clip_embeddings_dim,\n            output_dim=cross_attention_dim,\n            ff_mult=4,\n        )\n        \n    def forward(self, id_embeds, clip_embeds, shortcut=False, scale=1.0):\n\n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x) \n        out = self.perceiver_resampler(x, clip_embeds)\n        if shortcut:\n            out = x + scale * out\n        return out\n    \nclass AttentionMLP(nn.Module):\n    def __init__(\n        self,\n        dtype=torch.float16,\n        dim=1024,\n        depth=8,\n        dim_head=64,\n        heads=16,\n        single_num_tokens=1,\n        embedding_dim=1280,\n        output_dim=768,\n        ff_mult=4,\n        max_seq_len: int = 257*2,\n        apply_pos_emb: bool = False,\n        num_latents_mean_pooled: int = 0,\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, embedding_dim) if apply_pos_emb else None\n\n        self.single_num_tokens = single_num_tokens\n        self.latents = nn.Parameter(torch.randn(1, self.single_num_tokens, dim) / dim**0.5)\n\n        self.proj_in = nn.Linear(embedding_dim, dim)\n\n        self.proj_out = nn.Linear(dim, output_dim)\n        self.norm_out = nn.LayerNorm(output_dim)\n\n        self.to_latents_from_mean_pooled_seq = (\n            nn.Sequential(\n                nn.LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange(\"b (n d) -> b n d\", n=num_latents_mean_pooled),\n            )\n            if num_latents_mean_pooled > 0\n            else None\n        )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, x):\n        if self.pos_emb is not None:\n            n, device = x.shape[1], x.device\n            pos_emb = self.pos_emb(torch.arange(n, device=device))\n            x = x + pos_emb\n\n        latents = self.latents.repeat(x.size(0), 1, 1)\n\n        x = self.proj_in(x)\n\n        if self.to_latents_from_mean_pooled_seq:\n            meanpooled_seq = masked_mean(x, dim=1, mask=torch.ones(x.shape[:2], device=x.device, dtype=torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim=-2)\n\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\ndef masked_mean(t, *, dim, mask=None):\n    if mask is None:\n        return t.mean(dim=dim)\n\n    denom = mask.sum(dim=dim, keepdim=True)\n    mask = rearrange(mask, \"b n -> b n 1\")\n    masked_t = t.masked_fill(~mask, 0.0)\n\n    return masked_t.sum(dim=dim) / denom.clamp(min=1e-5)\n\n\n"}
{"type": "source_file", "path": "demo/inpaint_demo.py", "content": "import torch\nimport os\n# from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\nfrom diffusers import ControlNetModel, DDIMScheduler\nfrom pipelines.StableDIffusionInpaint_ConsistentID import StableDiffusionInpaintConsistentIDPipeline\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\n\n# Set device and define model paths\n# Check if GPU is available\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"GPU is available. Using GPU:\", torch.cuda.get_device_name(device))\nelse:\n    device = torch.device('cpu')\n    print(\"GPU is not available. Using CPU.\")\nbase_model_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\nconsistentID_path = \"JackAILab/ConsistentID/ConsistentID-v1.bin\"\n\n# Load initial and mask images\n# init_image_url = \" \" # TODO need to be checked\n# mask_image_url = \" \"\ninit_image = load_image(init_image_url)\nmask_image = load_image(mask_image_url)\n\n# Resize images\nselect_images = init_image.resize((512, 512))\nmask_image = mask_image.resize((512, 512))\n\n# Create control image using Canny edge detection\ndef make_canny_condition(image):\n    image = np.array(image)\n    image = cv2.Canny(image, 100, 200)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    image = Image.fromarray(image)\n    return image\n\ncontrol_image = make_canny_condition(init_image)\n\n# Load control model for inpainting\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_inpaint\", \n    torch_dtype=torch.float16,\n).to(device) \n\n# Load base model\npipe = StableDiffusionInpaintConsistentIDPipeline.from_pretrained(\n    base_model_path, \n    controlnet=controlnet, \n    torch_dtype=torch.float16, \n    use_safetensors=True, \n    variant=\"fp16\",\n).to(device)\n\n# Load ConsistentID model checkpoint\npipe.load_ConsistentID_model(\n    os.path.dirname(consistentID_path),\n    subfolder=\"\",\n    weight_name=os.path.basename(consistentID_path),\n    trigger_word=\"img\",\n)\n\n# Set up scheduler\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\npipe.to(device)\n\n# Set generator with seed\ngenerator = torch.Generator(device=device).manual_seed(2024)\n# hyper-parameter\nnum_steps = 50\nmerge_steps = 30\n# Define prompt and parameters\nprompt = \"cinematic photo, A man, in a forest, adventuring, 50mm photograph, half-length portrait, film, bokeh, professional, 4k, highly detailed\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry, ((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), [out of frame], extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))). out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\"\n\n# Generate the image\nimages = pipe(\n    prompt=prompt,\n    width=512,    \n    height=768,\n    strength=1,\n    mask_image=mask_image,\n    input_id_images=select_images,\n    control_image=control_image,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=1,\n    num_inference_steps=num_steps,\n    start_merge_step=merge_steps,\n    generator=generator,\n).images[0]\n\n# Save the resulting image\nimages.save(\"./result.jpg\")\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/bn.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom .functions import *\n\n\nclass ABN(nn.Module):\n    \"\"\"Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    \"\"\"\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=\"leaky_relu\", slope=0.01):\n        \"\"\"Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        \"\"\"\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \\\n              ' affine={affine}, activation={activation}'\n        if self.activation == \"leaky_relu\":\n            rep += ', slope={slope})'\n        else:\n            rep += ')'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    \"\"\"InPlace Activated Batch Normalization\"\"\"\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=\"leaky_relu\", slope=0.01):\n        \"\"\"Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        \"\"\"\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    \"\"\"InPlace Activated Batch Normalization with cross-GPU synchronization\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n    \"\"\"\n\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \\\n              ' affine={affine}, activation={activation}'\n        if self.activation == \"leaky_relu\":\n            rep += ', slope={slope})'\n        else:\n            rep += ')'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\n"}
{"type": "source_file", "path": "demo/controlnet_demo.py", "content": "# Import necessary libraries\nimport torch\nimport os\nfrom diffusers import ControlNetModel, DDIMScheduler\nfrom pipelines.StableDIffusionControlNetInpaint_ConsistentID import StableDiffusionControlNetInpaintConsistentIDPipeline\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\n\n# Set device and define model paths\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"GPU is available. Using GPU:\", torch.cuda.get_device_name(device))\nelse:\n    device = torch.device('cpu')\n    print(\"GPU is not available. Using CPU.\")\n\nbase_model_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\nconsistentID_path = \"JackAILab/ConsistentID/ConsistentID-v1.bin\"\n\n# Load initial and mask images\n# init_image_url = \" \" # TODO need to be checked\n# mask_image_url = \" \"\ninit_image = load_image(init_image_url)\nmask_image = load_image(mask_image_url)\n\n# Resize images\ninit_image = init_image.resize((512, 512))\nmask_image = mask_image.resize((512, 512))\n\n# Create control image using Canny edge detection\ndef make_canny_condition(image):\n    image = np.array(image)\n    image = cv2.Canny(image, 100, 200)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    image = Image.fromarray(image)\n    return image\n\ncontrol_image = make_canny_condition(init_image)\n\n# Load control model for inpainting\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_inpaint\", \n    torch_dtype=torch.float16,\n).to(device) \n\n# Load base model\npipe = StableDiffusionControlNetInpaintConsistentIDPipeline.from_pretrained(\n    base_model_path, \n    controlnet=controlnet, \n    torch_dtype=torch.float16, \n    use_safetensors=True, \n    variant=\"fp16\",\n).to(device)\n\n# Load ConsistentID model checkpoint\npipe.load_ConsistentID_model(\n    os.path.dirname(consistentID_path),\n    subfolder=\"\",\n    weight_name=os.path.basename(consistentID_path),\n    trigger_word=\"img\",\n)\n\n# Set up scheduler\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\npipe.to(device)\n\n# Set generator with seed\ngenerator = torch.Generator(device=device).manual_seed(2024)\n\n# Hyper-parameters\nnum_steps = 50\nmerge_steps = 30\n\n# Define prompt and parameters\nprompt = \"cinematic photo, A man, in a forest, adventuring, 50mm photograph, half-length portrait, film, bokeh, professional, 4k, highly detailed\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry, ((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), [out of frame], extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))). out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\"\n\n# Generate the image\nimages = pipe(\n    prompt=prompt,\n    width=512,    \n    height=768,\n    controlnet_conditioning_scale=0.5,\n    mask_image=mask_image,\n    control_image=control_image,\n    input_id_images=init_image,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=1,\n    num_inference_steps=num_steps,\n    start_merge_step=merge_steps,\n    generator=generator,\n).images[0]\n\n# Save the resulting image\nimages.save(\"./result.jpg\")\n"}
{"type": "source_file", "path": "models/BiSeNet/evaluate.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path='vis_results/parsing_map_on_im.jpg'):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    # Save result or not\n    if save_im:\n        cv2.imwrite(save_path, vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n\n    # return vis_im\n\ndef evaluate(respth='./res/test_res', dspth='./data', cp='model_final_diss.pth'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = osp.join('res/cp', cp)\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        for image_path in os.listdir(dspth):\n            img = Image.open(osp.join(dspth, image_path))\n            image = img.resize((512, 512), Image.BILINEAR)\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.cuda()\n            out = net(img)[0]\n            parsing = out.squeeze(0).cpu().numpy().argmax(0)\n\n            vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=osp.join(respth, image_path))\n\n\n\n\n\n\n\nif __name__ == \"__main__\":\n    setup_logger('./res')\n    evaluate()\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/functions.py", "content": "from os import path\nimport torch \nimport torch.distributed as dist\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), \"src\")\n_backend = load(name=\"inplace_abn\",\n                extra_cflags=[\"-O3\"],\n                sources=[path.join(_src_path, f) for f in [\n                    \"inplace_abn.cpp\",\n                    \"inplace_abn_cpu.cpp\",\n                    \"inplace_abn_cuda.cu\",\n                    \"inplace_abn_cuda_half.cu\"\n                ]],\n                extra_cuda_cflags=[\"--expt-extended-lambda\"])\n\n# Activation names\nACT_RELU = \"relu\"\nACT_LEAKY_RELU = \"leaky_relu\"\nACT_ELU = \"elu\"\nACT_NONE = \"none\"\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(\"CUDA Error encountered in {}\".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz * weight.sign() if ctx.affine else None\n        dbias = edz if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01, equal_batches=True):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        ctx.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        #count = _count_samples(x)\n        batch_size = x.new_tensor([x.shape[0]],dtype=torch.long)\n\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n            if ctx.world_size>1:\n                # get global batch size\n                if equal_batches:\n                    batch_size *= ctx.world_size\n                else:\n                    dist.all_reduce(batch_size, dist.ReduceOp.SUM)\n\n                ctx.factor = x.shape[0]/float(batch_size.item())\n\n                mean_all = mean.clone() * ctx.factor\n                dist.all_reduce(mean_all, dist.ReduceOp.SUM)\n\n                var_all = (var + (mean - mean_all) ** 2) * ctx.factor\n                dist.all_reduce(var_all, dist.ReduceOp.SUM)\n\n                mean = mean_all\n                var = var_all\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            count = batch_size.item() * x.view(x.shape[0],x.shape[1],-1).shape[-1]\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * (float(count) / (count - 1)))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n            edz_local = edz.clone()\n            eydz_local = eydz.clone()\n\n            if ctx.world_size>1:\n                edz *= ctx.factor\n                dist.all_reduce(edz, dist.ReduceOp.SUM)\n\n                eydz *= ctx.factor\n                dist.all_reduce(eydz, dist.ReduceOp.SUM)\n        else:\n            edz_local = edz = dz.new_zeros(dz.size(1))\n            eydz_local = eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz_local * weight.sign() if ctx.affine else None\n        dbias = edz_local if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [\"inplace_abn\", \"inplace_abn_sync\", \"ACT_RELU\", \"ACT_LEAKY_RELU\", \"ACT_ELU\", \"ACT_NONE\"]\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/misc.py", "content": "import torch.nn as nn\nimport torch\nimport torch.distributed as dist\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n\nclass SingleGPU(nn.Module):\n    def __init__(self, module):\n        super(SingleGPU, self).__init__()\n        self.module=module\n\n    def forward(self, input):\n        return self.module(input.cuda(non_blocking=True))\n\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/dense.py", "content": "from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (\"bn\", norm_act(in_channels)),\n                (\"conv\", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (\"bn\", norm_act(self.growth * bottleneck_factor)),\n                (\"conv\", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n"}
{"type": "source_file", "path": "models/BiSeNet/optimizer.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport logging\n\nlogger = logging.getLogger()\n\nclass Optimizer(object):\n    def __init__(self,\n                model,\n                lr0,\n                momentum,\n                wd,\n                warmup_steps,\n                warmup_start_lr,\n                max_iter,\n                power,\n                *args, **kwargs):\n        self.warmup_steps = warmup_steps\n        self.warmup_start_lr = warmup_start_lr\n        self.lr0 = lr0\n        self.lr = self.lr0\n        self.max_iter = float(max_iter)\n        self.power = power\n        self.it = 0\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = model.get_params()\n        param_list = [\n                {'params': wd_params},\n                {'params': nowd_params, 'weight_decay': 0},\n                {'params': lr_mul_wd_params, 'lr_mul': True},\n                {'params': lr_mul_nowd_params, 'weight_decay': 0, 'lr_mul': True}]\n        self.optim = torch.optim.SGD(\n                param_list,\n                lr = lr0,\n                momentum = momentum,\n                weight_decay = wd)\n        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n\n\n    def get_lr(self):\n        if self.it <= self.warmup_steps:\n            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n        else:\n            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n            lr = self.lr0 * factor\n        return lr\n\n\n    def step(self):\n        self.lr = self.get_lr()\n        for pg in self.optim.param_groups:\n            if pg.get('lr_mul', False):\n                pg['lr'] = self.lr * 10\n            else:\n                pg['lr'] = self.lr\n        if self.optim.defaults.get('lr_mul', False):\n            self.optim.defaults['lr'] = self.lr * 10\n        else:\n            self.optim.defaults['lr'] = self.lr\n        self.it += 1\n        self.optim.step()\n        if self.it == self.warmup_steps+2:\n            logger.info('==> warmup done, start to implement poly lr strategy')\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/__init__.py", "content": "from .bn import ABN, InPlaceABN, InPlaceABNSync\nfrom .functions import ACT_RELU, ACT_LEAKY_RELU, ACT_ELU, ACT_NONE\nfrom .misc import GlobalAvgPool2d, SingleGPU\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule\n"}
{"type": "source_file", "path": "infer_SDXL.py", "content": "import torch\nimport os\nfrom diffusers.utils import load_image\nfrom diffusers import EulerDiscreteScheduler\nfrom pipline_StableDiffusionXL_ConsistentID import ConsistentIDStableDiffusionXLPipeline\nimport sys\nfrom PIL import Image\nimport numpy as np\nimport argparse\n\ndef infer(base_model=None, star_name=None, prompt=None, face_caption=None):\n    # import base SD model and pretrained ConsistentID model\n    device = \"cuda\"\n    base_model_path = base_model # \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\n    consistentID_path = \"JackAILab/ConsistentID/ConsistentID_SDXL-v1.bin\"\n    image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n    bise_net_cp = \"JackAILab/ConsistentID/face_parsing.pth\" ### Please specify the specific path.\n\n    ### Load base model\n    pipe = ConsistentIDStableDiffusionXLPipeline.from_pretrained(\n        base_model_path, \n        torch_dtype=torch.float16, \n        safety_checker=None, # use_safetensors=True, \n        variant=\"fp16\"\n    ).to(device)\n\n    ### Load consistentID_model checkpoint\n    pipe.load_ConsistentID_model(\n        os.path.dirname(consistentID_path),\n        image_encoder_path=image_encoder_path,\n        bise_net_cp=bise_net_cp,\n        subfolder=\"\",\n        weight_name=os.path.basename(consistentID_path),\n        trigger_word=\"img\",\n    )     \n\n    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n\n    # lora_model_name = os.path.basename(lora_path)\n    # pipe.load_lora_weights(os.path.dirname(lora_path), weight_name=lora_model_name) # trigger: HTA\n    ### If there's a specific adapter name defined for this LoRA, use it; otherwise, the default might work.\n    ### Ensure 'adapter_name' matches what you intend to use or remove if not needed in your setup.\n    # pipe.set_adapter_settings(adapter_name=\"your_adapter_name_here\") # Uncomment and adjust as necessary\n    # pipe.set_adapters(,[\"ConsistentID\", \"more_art-full\"] adapter_weights=[1.0, 0.5])\n    ### Fuse the loaded LoRA into the pipeline\n    # pipe.fuse_lora()\n\n    ### input image\n    input_image_path = f\"./examples/{star_name}.jpg\"\n    select_images = load_image(input_image_path)\n\n    # hyper-parameter\n    num_steps = 50\n    merge_steps = 30\n    negative_prompt = '(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth'\n    \n    generator = torch.Generator(device=device).manual_seed(222)\n\n    images = pipe(\n        prompt=prompt,\n        width=864,    \n        height=1152, ## 1024 896\n        input_id_images=select_images,\n        input_image_path=input_image_path,\n        negative_prompt=negative_prompt,\n        num_images_per_prompt=1,\n        num_inference_steps=num_steps,\n        start_merge_step=merge_steps,\n        generator=generator,\n    ).images[0]\n\n    save_image_path = f\"./{base_model}_{star_name}.png\"\n    if not os.path.exists(os.path.dirname(save_image_path)):\n        os.makedirs(os.path.dirname(save_image_path))\n\n    images.save(save_image_path)\n    print(f\"IMAGE saved at : {save_image_path}\")\n\n\nif __name__ == \"__main__\":\n    \n    ### set parameters\n    parser = argparse.ArgumentParser(description=\"Parse image processing paths.\")\n    parser.add_argument(\"--base_model\", type=str, default=\"SG161222/Realistic_Vision_V6.0_B1_noVAE\",\n                        help=\"Path to the origin images.\")\n    parser.add_argument(\"--star_name\", type=str, default=\"scarlett_johansson\", ### albert_einstein  scarlett_johansson\n                        help=\"Path to the origin images.\") \n    parser.add_argument(\"--prompt\", type=str, default=\"A woman wearing a santa hat\",\n                        help=\"\")                                                                        \n    args = parser.parse_args()\n\n    infer(model_name=args.model_name, base_model=args.base_model, star_name=args.star_name, prompt=args.prompt, face_caption=face_caption)\n\n\n\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/loss.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nclass OhemCELoss(nn.Module):\n    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n        super(OhemCELoss, self).__init__()\n        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n        self.n_min = n_min\n        self.ignore_lb = ignore_lb\n        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n\n    def forward(self, logits, labels):\n        N, C, H, W = logits.size()\n        loss = self.criteria(logits, labels).view(-1)\n        loss, _ = torch.sort(loss, descending=True)\n        if loss[self.n_min] > self.thresh:\n            loss = loss[loss>self.thresh]\n        else:\n            loss = loss[:self.n_min]\n        return torch.mean(loss)\n\n\nclass SoftmaxFocalLoss(nn.Module):\n    def __init__(self, gamma, ignore_lb=255, *args, **kwargs):\n        super(SoftmaxFocalLoss, self).__init__()\n        self.gamma = gamma\n        self.nll = nn.NLLLoss(ignore_index=ignore_lb)\n\n    def forward(self, logits, labels):\n        scores = F.softmax(logits, dim=1)\n        factor = torch.pow(1.-scores, self.gamma)\n        log_score = F.log_softmax(logits, dim=1)\n        log_score = factor * log_score\n        loss = self.nll(log_score, labels)\n        return loss\n\n\nif __name__ == '__main__':\n    torch.manual_seed(15)\n    criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    net1 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net1.cuda()\n    net1.train()\n    net2 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net2.cuda()\n    net2.train()\n\n    with torch.no_grad():\n        inten = torch.randn(16, 3, 20, 20).cuda()\n        lbs = torch.randint(0, 19, [16, 20, 20]).cuda()\n        lbs[1, :, :] = 255\n\n    logits1 = net1(inten)\n    logits1 = F.interpolate(logits1, inten.size()[2:], mode='bilinear')\n    logits2 = net2(inten)\n    logits2 = F.interpolate(logits2, inten.size()[2:], mode='bilinear')\n\n    loss1 = criteria1(logits1, lbs)\n    loss2 = criteria2(logits2, lbs)\n    loss = loss1 + loss2\n    print(loss.detach().cpu())\n    loss.backward()\n"}
{"type": "source_file", "path": "evaluation/eval_function.py", "content": "from PIL import Image\nimport numpy as np\nimport os\nimport torch\nfrom torchvision.io import read_image, ImageReadMode\nimport glob\nimport json\nimport numpy as np\nimport random\nfrom copy import deepcopy\n\ndef crop_image(image_pil, threshold=10):\n\n    # image = Image.open(image_path)\n    image = image_pil\n    gray_image = image.convert('L')\n    gray_array = np.array(gray_image)\n\n    # find black\n    non_black_indices = np.where(gray_array > threshold)\n    top = np.min(non_black_indices[0])\n    bottom = np.max(non_black_indices[0])\n    left = np.min(non_black_indices[1])\n    right = np.max(non_black_indices[1])\n\n    cropped_image = image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef prepare_image_token_idx(image_token_mask, max_num_objects):\n\n    image_token_idx = torch.nonzero(image_token_mask, as_tuple=True)[1]\n    image_token_idx_mask = torch.ones_like(image_token_idx, dtype=torch.bool)\n    if len(image_token_idx) < max_num_objects:\n        image_token_idx = torch.cat(\n            [ \n                image_token_idx,\n                torch.zeros(max_num_objects - len(image_token_idx), dtype=torch.long),\n            ]\n        )\n        image_token_idx_mask = torch.cat(\n            [ \n                image_token_idx_mask,\n                torch.zeros(\n                    max_num_objects - len(image_token_idx_mask),\n                    dtype=torch.bool,\n                ),\n            ]\n        )\n    \n    image_token_idx = image_token_idx.unsqueeze(0)\n    image_token_idx_mask = image_token_idx_mask.unsqueeze(0)\n    \n    return image_token_idx, image_token_idx_mask\n\n\ndef white_balance_correction(image):\n\n    image_float = image.astype(np.float64)\n\n    mean_r = np.mean(image_float[:, :, 0])\n    mean_g = np.mean(image_float[:, :, 1])\n    mean_b = np.mean(image_float[:, :, 2])\n\n    offset_r = 128 - mean_r\n    offset_g = 128 - mean_g\n    offset_b = 128 - mean_b\n\n    image_balanced = image_float + [offset_r, offset_g, offset_b]\n\n    image_balanced = np.clip(image_balanced, 0, 255).astype(np.uint8)\n\n    return image_balanced\n \ndef get_object_transforms(args):\n    if args.no_object_augmentation:\n        pre_augmentations = []\n        augmentations = []\n    else:\n        pre_augmentations = [\n            (\n                \"zoomin\",\n                T.RandomApply([RandomZoomIn(min_zoom=1.0, max_zoom=2.0)], p=0.5),\n            ),\n        ]\n\n        augmentations = [\n            (\n                \"rotate\",\n                T.RandomApply(\n                    [\n                        T.RandomAffine(\n                            degrees=30, interpolation=T.InterpolationMode.BILINEAR\n                        )\n                    ],\n                    p=0.75,\n                ),\n            ),\n            (\"jitter\", T.RandomApply([T.ColorJitter(0.5, 0.5, 0.5, 0.5)], p=0.5)),\n            (\"blur\", T.RandomApply([T.GaussianBlur(5, sigma=(0.1, 2.0))], p=0.5)),\n            (\"gray\", T.RandomGrayscale(p=0.1)),\n            (\"flip\", T.RandomHorizontalFlip()),\n            (\"elastic\", T.RandomApply([T.ElasticTransform()], p=0.5)),\n        ]\n\n    object_transforms = torch.nn.Sequential(\n        OrderedDict(\n            [\n                *pre_augmentations,\n                (\"pad_to_square\", PadToSquare(fill=0, padding_mode=\"constant\")),\n                (\n                    \"resize\",\n                    T.Resize(\n                        (args.resolution, args.resolution),\n                        interpolation=T.InterpolationMode.BILINEAR,\n                        antialias=True,\n                    ),\n                ),\n                *augmentations,\n                (\"convert_to_float\", T.ConvertImageDtype(torch.float32)),\n            ]\n        )\n    )\n    return object_transforms\n    \nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms as T\nimport torch\nfrom collections import OrderedDict\n\nclass PadToSquare(torch.nn.Module):\n    def __init__(self, fill=0, padding_mode=\"constant\"):\n        super().__init__()\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    def forward(self, image: torch.Tensor):\n        _, h, w = image.shape\n        if h == w:\n            return image\n        elif h > w:\n            padding = (h - w) // 2\n            image = torch.nn.functional.pad(\n                image,\n                (padding, padding, 0, 0),\n                self.padding_mode,\n                self.fill,\n            )\n        else:\n            padding = (w - h) // 2\n            image = torch.nn.functional.pad(\n                image,\n                (0, 0, padding, padding),\n                self.padding_mode,\n                self.fill,\n            )\n        return image\n\nclass CropTopSquare(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, image: torch.Tensor):\n        _, h, w = image.shape\n        if h <= w:\n            return image\n        return image[:, :w, :]\n\nclass RandomZoomIn(torch.nn.Module):\n    def __init__(self, min_zoom=1.0, max_zoom=1.5):\n        super().__init__()\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n\n    def forward(self, image: torch.Tensor):\n        zoom = torch.rand(1) * (self.max_zoom - self.min_zoom) + self.min_zoom\n        original_shape = image.shape\n        image = T.functional.resize(\n            image,\n            (int(zoom * image.shape[1]), int(zoom * image.shape[2])),\n            interpolation=T.InterpolationMode.BILINEAR,\n            antialias=True,\n        )\n        image = CropTopSquare()(image)\n        return image\n\nimport sys\nsys.path.append(\"/mnt/data/sysu/Users/huangjiehui/projects/ConsistentID/\")\nfrom models.BiSeNet.model import BiSeNet\nimport json\nimport os\nimport os.path as osp\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\nimport torch\nimport numpy as np\nfrom PIL import Image\n\ndef resize_tensor(input_tensor, size):\n\n    numpy_image = np.transpose(input_tensor.cpu().numpy(), (1, 2, 0))\n\n    pil_image = Image.fromarray(np.uint8(numpy_image))\n\n    resized_image = pil_image.resize(size, Image.BILINEAR)\n\n    resized_numpy_image = np.array(resized_image)\n    resized_numpy_image = np.transpose(resized_numpy_image, (2, 0, 1))\n\n    resized_image_tensor = torch.tensor(resized_numpy_image, dtype=input_tensor.dtype)\n\n    return resized_image_tensor\n\ndef parsing_face_mask(raw_image_refer, cp='/mnt/data/sysu/Users/huangjiehui/pretrained_model/ConsistentID/face_parsing.pth'):\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = cp\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    to_pil = transforms.ToPILImage()\n\n    with torch.no_grad():\n\n        image = raw_image_refer.resize((512, 512), Image.BILINEAR)\n        image_resize_PIL = image\n        img = to_tensor(image)\n\n        img = torch.unsqueeze(img, 0)\n        img = img.float().cuda()\n        out = net(img)[0]\n        parsing_anno = out.squeeze(0).cpu().numpy().argmax(0)\n\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n    \n    im = np.array(image_resize_PIL)\n    vis_im = im.copy().astype(np.uint8)\n    stride=1\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    vis_parsing_anno_color = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    return vis_parsing_anno_color, vis_parsing_anno\n\ndef fetch_mask_raw_image(raw_image, mask_image):\n    \n    mask_image = mask_image.resize(raw_image.size)\n    mask_raw_image = Image.composite(raw_image, Image.new('RGB', raw_image.size, (0, 0, 0)), mask_image) \n\n    return mask_raw_image\n\nmapping_table = [\n    {\"Mask Value\": 0, \"Body Part\": \"Background\", \"RGB Color\": [0, 0, 0]},\n    {\"Mask Value\": 1, \"Body Part\": \"Face\", \"RGB Color\": [255, 0, 0]},\n    {\"Mask Value\": 2, \"Body Part\": \"Left_Eyebrow\", \"RGB Color\": [255, 85, 0]},\n    {\"Mask Value\": 3, \"Body Part\": \"Right_Eyebrow\", \"RGB Color\": [255, 170, 0]},\n    {\"Mask Value\": 4, \"Body Part\": \"Left_Eye\", \"RGB Color\": [255, 0, 85]},\n    {\"Mask Value\": 5, \"Body Part\": \"Right_Eye\", \"RGB Color\": [255, 0, 170]},\n    {\"Mask Value\": 6, \"Body Part\": \"Hair\", \"RGB Color\": [0, 0, 255]},\n    {\"Mask Value\": 7, \"Body Part\": \"Left_Ear\", \"RGB Color\": [85, 0, 255]},\n    {\"Mask Value\": 8, \"Body Part\": \"Right_Ear\", \"RGB Color\": [170, 0, 255]},\n    {\"Mask Value\": 9, \"Body Part\": \"Mouth_External Contour\", \"RGB Color\": [0, 255, 85]},\n    {\"Mask Value\": 10, \"Body Part\": \"Nose\", \"RGB Color\": [0, 255, 0]},\n    {\"Mask Value\": 11, \"Body Part\": \"Mouth_Inner_Contour\", \"RGB Color\": [0, 255, 170]},\n    {\"Mask Value\": 12, \"Body Part\": \"Upper_Lip\", \"RGB Color\": [85, 255, 0]}, \n    {\"Mask Value\": 13, \"Body Part\": \"Lower_Lip\", \"RGB Color\": [170, 255, 0]},\n    {\"Mask Value\": 14, \"Body Part\": \"Neck\", \"RGB Color\": [0, 85, 255]},\n    {\"Mask Value\": 15, \"Body Part\": \"Neck_Inner Contour\", \"RGB Color\": [0, 170, 255]},\n    {\"Mask Value\": 16, \"Body Part\": \"Cloth\", \"RGB Color\": [255, 255, 0]},\n    {\"Mask Value\": 17, \"Body Part\": \"Hat\", \"RGB Color\": [255, 0, 255]},\n    {\"Mask Value\": 18, \"Body Part\": \"Earring\", \"RGB Color\": [255, 85, 255]},\n    {\"Mask Value\": 19, \"Body Part\": \"Necklace\", \"RGB Color\": [255, 255, 85]},\n    {\"Mask Value\": 20, \"Body Part\": \"Glasses\", \"RGB Color\": [255, 170, 255]},\n    {\"Mask Value\": 21, \"Body Part\": \"Hand\", \"RGB Color\": [255, 0, 255]},\n    {\"Mask Value\": 22, \"Body Part\": \"Wristband\", \"RGB Color\": [0, 255, 255]},\n    {\"Mask Value\": 23, \"Body Part\": \"Clothes_Upper\", \"RGB Color\": [85, 255, 255]},\n    {\"Mask Value\": 24, \"Body Part\": \"Clothes_Lower\", \"RGB Color\": [170, 255, 255]}\n]\n\ndef masks_for_unique_values(image_raw_mask):\n\n    image_array = np.array(image_raw_mask)\n    unique_values, counts = np.unique(image_array, return_counts=True)\n    masks_dict = {}\n\n    for value in unique_values:\n        binary_image = np.uint8(image_array == value) * 255\n        \n        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        mask = np.zeros_like(image_array)\n\n        for contour in contours:\n            cv2.drawContours(mask, [contour], -1, (255), thickness=cv2.FILLED)\n\n        if value == 0:\n            body_part=\"WithoutBackground\"\n            mask2 = np.where(mask == 255, 0, 255).astype(mask.dtype)\n            masks_dict[body_part] = Image.fromarray(mask2)\n\n        body_part = next((entry[\"Body Part\"] for entry in mapping_table if entry[\"Mask Value\"] == value), f\"Unknown_{value}\")\n\n        if body_part.startswith(\"Unknown_\"):\n            continue            \n\n        masks_dict[body_part] = Image.fromarray(mask)\n    \n    return masks_dict\n\nimport torch\nimport torch.nn as nn\nfrom transformers.models.clip.modeling_clip import CLIPVisionModelWithProjection\nfrom transformers.models.clip.configuration_clip import CLIPVisionConfig\nfrom transformers import PretrainedConfig\n\nVISION_CONFIG_DICT = {\n    \"hidden_size\": 1024,\n    \"intermediate_size\": 4096,\n    \"num_attention_heads\": 16,\n    \"num_hidden_layers\": 24,\n    \"patch_size\": 14,\n    \"projection_dim\": 768\n}\n\nimport re\ndef remove_duplicate_keywords(text, keywords):\n    keyword_counts = {}\n\n    words = re.findall(r'\\b\\w+\\b|[.,;!?]', text)\n\n    for keyword in keywords:\n        keyword_counts[keyword] = 0\n\n        for i, word in enumerate(words):\n            if word.lower() == keyword.lower():\n                keyword_counts[keyword] += 1\n\n                if keyword_counts[keyword] > 1:\n                    words[i] = \"\"\n\n    processed_text = \" \".join(words)\n\n    return processed_text\n\ndef process_text_with_markers(text, parsing_mask_list):\n\n    keywords = [\"face\", \"eyes\", \"ears\", \"nose\", \"mouth\"]\n\n    text = remove_duplicate_keywords(text, keywords)\n\n    key_parsing_mask_markers = [\"Face\", \"Left_Eye\", \"Right_Eye\", \"Left_Ear\", \"Right_Ear\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n    mapping = {\n        \"Face\": \"face\",\n        \"Left_Eye\": \"eyes\",\n        \"Right_Eye\": \"eyes\",\n        \"Left_Ear\": \"ears\",\n        \"Right_Ear\": \"ears\",        \n        \"Nose\": \"nose\",\n        \"Upper_Lip\": \"mouth\",\n        \"Lower_Lip\": \"mouth\",\n    }\n    facial_features_align = []\n    markers_align = []\n    for key in key_parsing_mask_markers:\n        if key in parsing_mask_list:\n            mapped_key = mapping.get(key, key.lower())\n            if mapped_key not in facial_features_align:\n                facial_features_align.append(mapped_key)\n                markers_align.append(\"<|\"+mapped_key+\"|>\")\n\n    # (2)\n    text_marked = text\n    align_parsing_mask_list = parsing_mask_list\n    for feature, marker in zip(facial_features_align[::-1], markers_align[::-1]):\n        pattern = rf'\\b{feature}\\b' # feature  \"face\", \"ears\", \"nose\", \"eyes\", \"mouth\" \n    \n        text_marked_new = re.sub(pattern, f'{feature} {marker}', text_marked, count=1)\n        if text_marked == text_marked_new:\n            for key, value in mapping.items():\n                if value == feature:\n                    if key in align_parsing_mask_list:\n                        del align_parsing_mask_list[key]   \n\n        text_marked = text_marked_new \n\n    text_marked = text_marked.replace('\\n', '')\n\n    # (3)\n    ordered_text = []\n    text_none_makers = []\n    facial_marked_count = 0 \n    skip_count = 0\n    for marker in markers_align: # markers_align ['<|face|>', '<|eyes|>', '<|nose|>', '<|mouth|>']\n        start_idx = text_marked.find(marker)\n        end_idx = start_idx + len(marker)\n\n        while start_idx > 0 and text_marked[start_idx - 1] not in [\",\", \".\", \";\"]: # [\",\", \".\", \";\"] [ \".\" ]\n            start_idx -= 1\n\n        while end_idx < len(text_marked) and text_marked[end_idx] not in [\",\", \".\", \";\"]: # [\",\", \".\", \";\"] [ \".\" ]\n            end_idx += 1\n\n        context = text_marked[start_idx:end_idx].strip()\n        if context == \"\":\n            text_none_makers.append(text_marked[:end_idx])\n            # print(f\"The facial part of {marker} can not be found in caption!\\r\\n\")\n        else:\n            if skip_count!=0:\n                skip_count -= 1 \n                continue\n            else:\n                ordered_text.append(context + \",\")\n                text_delete_makers = text_marked[:start_idx] + text_marked[end_idx:]\n                text_marked = text_delete_makers\n                facial_marked_count += 1\n                # print(f\"Current successful matched special token in the facial text number is: {facial_marked_count}\")\n                # print(f\"The current marked text is: {ordered_text} \\r\\n\")\n\n    align_marked_text = \" \".join(ordered_text)\n    replace_list = [\"<|face|>\", \"<|eyes|>\", \"<|ears|>\", \"<|nose|>\", \"<|mouth|>\"] \n    for item in replace_list:\n        align_marked_text = align_marked_text.replace(item, \"<|facial|>\")\n\n    # print(f\"The final aligned facial text is: {align_marked_text}\")\n\n    return align_marked_text, align_parsing_mask_list\n\n\ndef tokenize_and_mask_noun_phrases_ends(text, image_token_id, facial_token_id, tokenizer):\n\n    input_ids = tokenizer.encode(text)\n    image_noun_phrase_end_mask = [False for _ in input_ids] \n    facial_noun_phrase_end_mask = [False for _ in input_ids]\n    clean_input_ids = []\n    clean_index = 0\n    image_num = 0\n\n    for i, id in enumerate(input_ids):\n        if id == image_token_id:\n            image_noun_phrase_end_mask[clean_index + image_num - 1] = True\n            image_num += 1\n        elif id == facial_token_id:\n            facial_noun_phrase_end_mask[clean_index - 1] = True        \n        else:\n            clean_input_ids.append(id)\n            clean_index += 1\n\n    max_len = tokenizer.model_max_length \n\n    if len(clean_input_ids) > max_len:\n        clean_input_ids = clean_input_ids[:max_len]\n    else:\n        clean_input_ids = clean_input_ids + [tokenizer.pad_token_id] * (\n            max_len - len(clean_input_ids)\n        )\n\n    if len(image_noun_phrase_end_mask) > max_len: \n        image_noun_phrase_end_mask = image_noun_phrase_end_mask[:max_len]\n    else:\n        image_noun_phrase_end_mask = image_noun_phrase_end_mask + [False] * (\n            max_len - len(image_noun_phrase_end_mask)\n        )\n\n    if len(facial_noun_phrase_end_mask) > max_len: \n        facial_noun_phrase_end_mask = facial_noun_phrase_end_mask[:max_len]\n    else:\n        facial_noun_phrase_end_mask = facial_noun_phrase_end_mask + [False] * (\n            max_len - len(facial_noun_phrase_end_mask)\n        )        \n\n    clean_input_ids = torch.tensor(clean_input_ids, dtype=torch.long)\n    image_noun_phrase_end_mask = torch.tensor(image_noun_phrase_end_mask, dtype=torch.bool)\n    facial_noun_phrase_end_mask = torch.tensor(facial_noun_phrase_end_mask, dtype=torch.bool)\n\n    return clean_input_ids.unsqueeze(0), image_noun_phrase_end_mask.unsqueeze(0), facial_noun_phrase_end_mask.unsqueeze(0)\n\ndef prepare_image_token_idx(image_token_mask, facial_token_mask, max_num_objects=2, max_num_facials=5):\n\n    image_token_idx = torch.nonzero(image_token_mask, as_tuple=True)[1]\n    image_token_idx_mask = torch.ones_like(image_token_idx, dtype=torch.bool)\n    if len(image_token_idx) < max_num_objects: \n        image_token_idx = torch.cat(\n            [ \n                image_token_idx,\n                torch.zeros(max_num_objects - len(image_token_idx), dtype=torch.long),\n            ]\n        )\n        image_token_idx_mask = torch.cat(\n            [ \n                image_token_idx_mask,\n                torch.zeros(\n                    max_num_objects - len(image_token_idx_mask),\n                    dtype=torch.bool,\n                ),\n            ]\n        )\n\n    facial_token_idx = torch.nonzero(facial_token_mask, as_tuple=True)[1]\n    facial_token_idx_mask = torch.ones_like(facial_token_idx, dtype=torch.bool)     \n    if len(facial_token_idx) < max_num_facials:\n        facial_token_idx = torch.cat(\n            [ \n                facial_token_idx,\n                torch.zeros(max_num_facials - len(facial_token_idx), dtype=torch.long),\n            ]\n        )\n        facial_token_idx_mask = torch.cat(\n            [ \n                facial_token_idx_mask,\n                torch.zeros(\n                    max_num_facials - len(facial_token_idx_mask),\n                    dtype=torch.bool,\n                ),\n            ]\n        )\n\n    image_token_idx = image_token_idx.unsqueeze(0)\n    image_token_idx_mask = image_token_idx_mask.unsqueeze(0)\n    \n    facial_token_idx = facial_token_idx.unsqueeze(0)\n    facial_token_idx_mask = facial_token_idx_mask.unsqueeze(0)\n\n    return image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask \n\nclass Transform_ID():\n    def __init__(self,):\n        super().__init__()\n        pass\n    \n    def process_image(self, image):\n\n        if image.size(0) > 3:\n            image_list = [image]\n            processed_image = torch.cat(image_list, dim=0)\n        else:\n            image_list = [image] * 2\n\n        processed_image = torch.cat(image_list, dim=0)\n\n        return processed_image\n\n    def process_clip_image(self, clip_image):\n\n        if clip_image.size(0) > 1:\n            processed_clip_image = torch.cat([clip_image, clip_image], dim=0)\n        else:\n            processed_clip_image = clip_image\n\n        return processed_clip_image\n\n    def process_pasring_mask(self, parsing_mask):\n\n        processed_parsing_mask=[]\n\n        return processed_parsing_mask\n\n    def process_faceid(self, faceid):\n\n        processed_faceid=[]   \n\n        return processed_faceid\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim, use_residual=True):\n        super().__init__()\n        if use_residual:\n            assert in_dim == out_dim\n        self.layernorm = nn.LayerNorm(in_dim)\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n        self.use_residual = use_residual\n        self.act_fn = nn.GELU()\n\n    def forward(self, x):\n        residual = x\n        x = self.layernorm(x)\n        x = self.fc1(x)\n        x = self.act_fn(x)\n        x = self.fc2(x)\n        if self.use_residual:\n            x = x + residual\n        return x\n\n\nclass FuseModule(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.mlp1 = MLP(embed_dim * 2, embed_dim, embed_dim, use_residual=False)\n        self.mlp2 = MLP(embed_dim, embed_dim, embed_dim, use_residual=True)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def fuse_fn(self, prompt_embeds, id_embeds):\n        stacked_id_embeds = torch.cat([prompt_embeds, id_embeds], dim=-1)\n        stacked_id_embeds = self.mlp1(stacked_id_embeds) + prompt_embeds\n        stacked_id_embeds = self.mlp2(stacked_id_embeds)\n        stacked_id_embeds = self.layer_norm(stacked_id_embeds)\n        return stacked_id_embeds\n\n    def forward(\n        self,\n        prompt_embeds,\n        id_embeds,\n        class_tokens_mask,\n    ) -> torch.Tensor:\n        # id_embeds shape: [b, max_num_inputs, 1, 2048]\n        id_embeds = id_embeds.to(prompt_embeds.dtype)\n        num_inputs = class_tokens_mask.sum().unsqueeze(0) # TODO: check for training case\n        batch_size, max_num_inputs = id_embeds.shape[:2]\n        # seq_length: 77\n        seq_length = prompt_embeds.shape[1]\n        # flat_id_embeds shape: [b*max_num_inputs, 1, 2048]\n        flat_id_embeds = id_embeds.view(\n            -1, id_embeds.shape[-2], id_embeds.shape[-1]\n        )\n        # valid_id_mask [b*max_num_inputs]\n        valid_id_mask = (\n            torch.arange(max_num_inputs, device=flat_id_embeds.device)[None, :]\n            < num_inputs[:, None]\n        )\n        valid_id_embeds = flat_id_embeds[valid_id_mask.flatten()]\n\n        prompt_embeds = prompt_embeds.view(-1, prompt_embeds.shape[-1])\n        class_tokens_mask = class_tokens_mask.view(-1)\n        valid_id_embeds = valid_id_embeds.view(-1, valid_id_embeds.shape[-1])\n        # slice out the image token embeddings\n        image_token_embeds = prompt_embeds[class_tokens_mask]\n        stacked_id_embeds = self.fuse_fn(image_token_embeds, valid_id_embeds)\n        assert class_tokens_mask.sum() == stacked_id_embeds.shape[0], f\"{class_tokens_mask.sum()} != {stacked_id_embeds.shape[0]}\"\n        prompt_embeds.masked_scatter_(class_tokens_mask[:, None], stacked_id_embeds.to(prompt_embeds.dtype))\n        updated_prompt_embeds = prompt_embeds.view(batch_size, seq_length, -1)\n        return updated_prompt_embeds\n\nclass PhotoMakerIDEncoder(CLIPVisionModelWithProjection):\n    def __init__(self):\n        super().__init__(CLIPVisionConfig(**VISION_CONFIG_DICT))\n        self.visual_projection_2 = nn.Linear(1024, 1280, bias=False)\n        self.fuse_module = FuseModule(2048)\n\n    def forward(self, id_pixel_values, prompt_embeds, class_tokens_mask):\n        b, num_inputs, c, h, w = id_pixel_values.shape\n        id_pixel_values = id_pixel_values.view(b * num_inputs, c, h, w)\n\n        shared_id_embeds = self.vision_model(id_pixel_values)[1]\n        id_embeds = self.visual_projection(shared_id_embeds)\n        id_embeds_2 = self.visual_projection_2(shared_id_embeds)\n\n        id_embeds = id_embeds.view(b, num_inputs, 1, -1)\n        id_embeds_2 = id_embeds_2.view(b, num_inputs, 1, -1)    \n\n        id_embeds = torch.cat((id_embeds, id_embeds_2), dim=-1)\n        updated_prompt_embeds = self.fuse_module(prompt_embeds, id_embeds, class_tokens_mask)\n\n        return updated_prompt_embeds\n\n\nif __name__ == \"__main__\":\n    PhotoMakerIDEncoder()"}
{"type": "source_file", "path": "infer.py", "content": "import torch\nimport os\nfrom diffusers.utils import load_image\nfrom diffusers import EulerDiscreteScheduler\nfrom pipline_StableDiffusion_ConsistentID import ConsistentIDStableDiffusionPipeline\nimport sys\n\n\n### Download the model from huggingface and put it locally, then place the model in a local directory and specify the directory location.\ndevice = \"cuda\"\nbase_model_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\nconsistentID_path = \"JackAILab/ConsistentID/ConsistentID-v1.bin\" \nimage_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\nbise_net_cp = \"JackAILab/ConsistentID/face_parsing.pth\" ### Download this model manually and specify the path\n\n### Load base model\npipe = ConsistentIDStableDiffusionPipeline.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.float16,\n    variant=\"fp16\"\n).to(device)\n\n### Load consistentID_model checkpoint\npipe.load_ConsistentID_model(\n    os.path.dirname(consistentID_path),\n    image_encoder_path=image_encoder_path,\n    bise_net_cp=bise_net_cp,\n    subfolder=\"\",\n    weight_name=os.path.basename(consistentID_path),\n    trigger_word=\"img\",\n)     \n\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n\n### Experimental feature, using LoRA modules in community\n# lora_model_name = os.path.basename(lora_path)\n# pipe.load_lora_weights(os.path.dirname(lora_path), weight_name=lora_model_name) # trigger: HTA\n### If there's a specific adapter name defined for this LoRA, use it; otherwise, the default might work.\n### Ensure 'adapter_name' matches what you intend to use or remove if not needed in your setup.\n# pipe.set_adapter_settings(adapter_name=\"your_adapter_name_here\") # Uncomment and adjust as necessary\n# pipe.set_adapters(,[\"ConsistentID\", \"more_art-full\"] adapter_weights=[1.0, 0.5]) # TODO\n### Fuse the loaded LoRA into the pipeline\n# pipe.fuse_lora()\n\n### input image \nselect_images = load_image(\"./examples/albert_einstein.jpg\")\n# hyper-parameter\nnum_steps = 50\nmerge_steps = 30\n# Prompt\nprompt = \"A man, in a forest, adventuring\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n\n### Extend Prompt\nprompt = \"cinematic photo,\" + prompt + \", 50mm photograph, half-length portrait, film, bokeh, professional, 4k, highly detailed\"\nnegtive_prompt_group=\"((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), [out of frame], extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))). out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\"\nnegative_prompt = negative_prompt + negtive_prompt_group\n\ngenerator = torch.Generator(device=device).manual_seed(2024)\n\nimages = pipe(\n    prompt=prompt,\n    width=512,    \n    height=768,\n    input_id_images=select_images,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=1,\n    num_inference_steps=num_steps,\n    start_merge_step=merge_steps,\n    generator=generator,\n).images[0]\n\nimages.save(\"./result.jpg\")\n\n\n"}
{"type": "source_file", "path": "models/BiSeNet/modules/residual.py", "content": "from collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        \"\"\"Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        \"\"\"\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(\"channels must contain either two or three values\")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(\"groups > 1 are only valid if len(channels) == 3\")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (\"conv1\", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (\"bn2\", norm_act(channels[0])),\n                (\"conv2\", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(\"dropout\", dropout())] + layers[2:]\n        else:\n            layers = [\n                (\"conv1\", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (\"bn2\", norm_act(channels[0])),\n                (\"conv2\", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (\"bn3\", norm_act(channels[1])),\n                (\"conv3\", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(\"dropout\", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, \"proj_conv\"):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n"}
{"type": "source_file", "path": "train_SDXL.py", "content": "import os\nfrom pathlib import Path\nimport itertools\nimport time\nimport torch\nimport torch.nn.functional as F\nimport math\nimport torchvision.transforms as T\nimport gc\n\nfrom einops import rearrange\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration\nfrom diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\nfrom transformers import CLIPModel, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n\nimport torchvision.transforms.functional as TF\n\nfrom tqdm import tqdm \nimport torch.nn as nn \n\nimport sys\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\nparent_dir = os.path.dirname(current_dir)\nsys.path.append(parent_dir)\n\nfrom utils_SDXL import parse_args, collate_fn, MyDataset\nfrom attention import FacialEncoder, Consistent_IPAttProcessor, Consistent_AttProcessor\nfrom functions import MLPProjModel, ProjPlusModel, BalancedL1Loss, unet_store_cross_attention_scores, get_object_localization_loss\n\n# exp_name\nexp_name = 'ConsistentID_SDXL'\ninitial_epoch = 0\n\nclass ConsistentID(torch.nn.Module):\n    \"\"\"ConsistentID\"\"\"\n    def __init__(self, unet, image_proj_model, adapter_modules, image_CLIPModel_encoder=None):\n        super().__init__()\n        self.unet = unet\n        self.image_proj_model = image_proj_model\n        self.adapter_modules = adapter_modules\n\n        self.FacialEncoder = FacialEncoder(image_CLIPModel_encoder)\n        self.cross_attention_scores = {}\n        self.localization_layers  = 3\n        self.facial_weight = 0.01\n        self.mask_loss_prob = 0.5\n\n        self.unet = unet_store_cross_attention_scores( \n            self.unet, self.cross_attention_scores, self.localization_layers \n        )\n\n        self.object_localization_loss_fn = BalancedL1Loss(threshold=1.0, normalize=True)    \n        # self.load_from_checkpoint(ckpt_path=\"./ConsistentID.bin\")\n\n    def forward(self, noisy_latents, timesteps, prompt_embeds, image_embeds, faceid_embeds, \\\n                unet_added_cond_kwargs, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask, \\\n                noise, batch, parsing_mask_lists, facial_masks, facial_token_idxs, facial_token_idx_masks): \n\n        faceid_tokens = self.image_proj_model(faceid_embeds, image_embeds)\n\n        prompt_id_embeds = self.FacialEncoder(prompt_embeds, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)\n\n        prompt_id_embeds = torch.cat([prompt_id_embeds, faceid_tokens], dim=1)\n\n        noise_pred = self.unet(noisy_latents, timesteps, prompt_id_embeds, added_cond_kwargs=unet_added_cond_kwargs).sample\n\n        target=noise\n        pred=noise_pred\n        loss_dict = {\"background_loss\": 0}\n\n        if torch.rand(1) < self.mask_loss_prob:\n            try:\n                mask_list = [TF.to_tensor(image['WithoutBackground']).unsqueeze(0) for image in parsing_mask_lists]\n                mask_stacked = torch.cat(mask_list, dim=0)\n\n                mask_final = F.interpolate(mask_stacked,size=(pred.shape[-2], pred.shape[-1]),mode=\"bilinear\",align_corners=False,)\n                pred = pred * mask_final.to(pred.device, dtype=pred.dtype)\n                target = target * mask_final.to(target.device, dtype=target.dtype)\n\n                background_loss = F.mse_loss(pred.float(), target.float(), reduction=\"mean\")\n                loss_dict[\"background_loss\"] = background_loss\n\n            except:\n                print(f\"The fail 'Background' of parsing_mask_lists: {parsing_mask_lists}\")\n\n\n        predict_loss = F.mse_loss(pred.float(), target.float(), reduction=\"mean\")\n        loss_dict[\"predict_loss\"] = predict_loss\n        loss_dict[\"facial_loss\"] = 0\n\n        object_segmaps = facial_masks \n        image_token_idx = facial_token_idxs \n        image_token_idx_mask = facial_token_idx_masks\n        facial_loss = get_object_localization_loss(\n            self.cross_attention_scores,\n            object_segmaps,\n            image_token_idx,\n            image_token_idx_mask,\n            self.object_localization_loss_fn,\n        )\n            \n        facial_loss = self.facial_weight * facial_loss\n        loss_dict[\"facial_loss\"]=facial_loss\n\n        return pred, loss_dict\n    \n    def load_from_checkpoint(self, ckpt_path: str):\n\n        # Calculate original checksums\n        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n        orig_FacialEncoder_sum = torch.sum(torch.stack([torch.sum(p) for p in self.FacialEncoder.parameters()]))\n\n        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj_model\"], strict=True)\n        self.adapter_modules.load_state_dict(state_dict[\"adapter_modules\"], strict=True)\n        self.FacialEncoder.load_state_dict(state_dict[\"FacialEncoder\"], strict=True)\n\n        # Calculate new checksums\n        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n        new_FacialEncoder_sum = torch.sum(torch.stack([torch.sum(p) for p in self.FacialEncoder.parameters()]))\n\n        # # Verify if the weights have changed\n        assert orig_ip_proj_sum != new_ip_proj_sum, \"Weights of image_proj_model did not change!\"\n        assert orig_adapter_sum != new_adapter_sum, \"Weights of adapter_modules did not change!\"\n        assert orig_FacialEncoder_sum != new_FacialEncoder_sum, \"Weights of adapter_modules did not change!\"\n\n        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n\ndef main():\n\n    args = parse_args()\n\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n\n    accelerator = Accelerator(\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        project_config=accelerator_project_config,\n    )\n    \n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n    tokenizer_2 = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\")\n    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder_2\")\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n\n    # freeze parameters of models to save more memory\n    unet.requires_grad_(False)\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    text_encoder_2.requires_grad_(False)\n    image_encoder.requires_grad_(False)\n\n    image_proj_model = ProjPlusModel(\n        cross_attention_dim=unet.config.cross_attention_dim,\n        id_embeddings_dim=512,\n        clip_embeddings_dim=image_encoder.config.hidden_size,\n        num_tokens=args.num_tokens,\n    )\n\n    weight_dtype = torch.float16\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"fp32\":\n        weight_dtype = torch.float32\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n    elif accelerator.mixed_precision == \"fp8\":\n        weight_dtype = torch.float8        \n    elif accelerator.mixed_precision == 'no':\n        weight_dtype = torch.float32  \n\n    vae.to(accelerator.device, dtype=weight_dtype)\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\n    image_encoder.to(accelerator.device, dtype=weight_dtype)\n    text_encoder_2.to(accelerator.device, dtype=weight_dtype)\n    image_proj_model.to(accelerator.device, dtype=weight_dtype)\n\n    # init adapter modules\n    lora_rank = 128 ### important TODO\n    attn_procs = {}\n    unet_sd = unet.state_dict()\n    for name in unet.attn_processors.keys():\n        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n        if name.startswith(\"mid_block\"):\n            hidden_size = unet.config.block_out_channels[-1]\n        elif name.startswith(\"up_blocks\"):\n            block_id = int(name[len(\"up_blocks.\")])\n            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n        elif name.startswith(\"down_blocks\"):\n            block_id = int(name[len(\"down_blocks.\")])\n            hidden_size = unet.config.block_out_channels[block_id]  \n        if cross_attention_dim is None:\n            attn_procs[name] = Consistent_AttProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)\n        else:\n            layer_name = name.split(\".processor\")[0]\n            weights = {\n                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"], \n            }\n            attn_procs[name] = Consistent_IPAttProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)\n            attn_procs[name].load_state_dict(weights, strict=False)\n\n    unet.set_attn_processor(attn_procs)\n    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n\n    consistentID_model = ConsistentID(unet, image_proj_model, adapter_modules)\n\n    optimizer_cls = torch.optim.AdamW\n\n    unet_params = list([p for p in consistentID_model.unet.parameters() if p.requires_grad])\n    other_params = list(\n        [p for n, p in consistentID_model.named_parameters() if p.requires_grad and \"unet\" not in n]\n    )\n    parameters = unet_params + other_params\n\n    optimizer = optimizer_cls(\n        [\n            {\"params\": unet_params, \"lr\": 1e-4*1.0 },\n            {\"params\": other_params, \"lr\": 1e-4},\n        ],\n        betas=(0.9, 0.999),\n        weight_decay=1e-2,\n        eps=1e-08,\n    )\n\n    # dataloader\n    train_dataset = MyDataset(args.data_json_file, args.data_json_mutiID_file, tokenizer=tokenizer, tokenizer_2=tokenizer_2, size=args.resolution, \\\n                              image_root_path=args.data_root_path, faceid_root_path=args.faceid_root_path, parsing_root_path=args.parsing_root_path)\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset,\n        shuffle=True,\n        collate_fn=collate_fn,\n        batch_size=args.train_batch_size,\n        num_workers=args.dataloader_num_workers,\n    )\n    \n    # Prepare everything with our `accelerator`.\n    consistentID_model, optimizer, train_dataloader = accelerator.prepare(consistentID_model, optimizer, train_dataloader)\n\n    # # Train\n    for epoch in range(initial_epoch, args.num_train_epochs):\n        begin = time.perf_counter()\n        global_step = 0\n        progress_bar = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{args.num_train_epochs}\", total=len(train_dataloader), disable=not accelerator.is_local_main_process,)\n\n        for step, batch in enumerate(train_dataloader):\n            if any(\"error\" in item for item in batch):\n                print(\"Skipping batch with invalid data\")\n                continue            \n            load_data_time = time.perf_counter() - begin\n            with accelerator.accumulate(consistentID_model):\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"images\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n                    latents = latents * vae.config.scaling_factor\n                    latents = latents.to(accelerator.device, dtype=weight_dtype)\n\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n                timesteps = timesteps.long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                with torch.no_grad():\n                    clip_images = batch[\"clip_images\"]\n                    image_embeds = image_encoder(clip_images.to(accelerator.device, dtype=weight_dtype), output_hidden_states=True).hidden_states[-2] # .image_embeds\n\n                    # level 3\n                    hidden_states = []\n                    facial_clip_images = batch[\"facial_clip_images\"]                    \n                    for facial_clip_image in facial_clip_images:\n                        hidden_state = image_encoder(facial_clip_image.to(accelerator.device, dtype=weight_dtype), output_hidden_states=True).hidden_states[-2]\n                        hidden_states.append(hidden_state)\n                    multi_facial_embeds = torch.stack(hidden_states)\n\n                with torch.no_grad():\n                    encoder_output = text_encoder(batch['text_input_id_alls'].to(accelerator.device), output_hidden_states=True)\n                    text_embeds = encoder_output.hidden_states[-2]\n                    encoder_output_2 = text_encoder_2(batch['text_input_id_all2s'].to(accelerator.device), output_hidden_states=True)\n                    pooled_text_embeds = encoder_output_2[0]\n                    text_embeds_2 = encoder_output_2.hidden_states[-2]\n                    prompt_embeds = torch.concat([text_embeds, text_embeds_2], dim=-1) # concat     \n                \n                add_time_ids = [\n                    batch[\"original_sizes\"].to(accelerator.device),\n                    batch[\"crop_coords_top_lefts\"].to(accelerator.device),\n                    batch[\"target_sizes\"].to(accelerator.device),\n                ]\n                add_time_ids = torch.cat(add_time_ids, dim=1).to(accelerator.device, dtype=weight_dtype)\n                unet_added_cond_kwargs = {\"text_embeds\": pooled_text_embeds, \"time_ids\": add_time_ids}\n\n                faceid_embeds = batch[\"face_id_embeds\"].to(accelerator.device, dtype=weight_dtype)\n\n                # level 3\n                facial_token_masks = batch[\"facial_token_masks\"]\n                valid_facial_token_idx_mask = batch[\"facial_token_idx_masks\"]\n\n                parsing_mask_lists = batch[\"parsing_mask_lists\"]\n                facial_masks = batch[\"facial_masks\"]\n                facial_token_idxs = batch[\"facial_token_idxs\"]\n                facial_token_idx_masks = batch[\"facial_token_idx_masks\"]\n\n                noise_pred, loss_dict = consistentID_model(noisy_latents, timesteps, prompt_embeds, image_embeds, faceid_embeds, \\\n                            unet_added_cond_kwargs, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask,\n                            noise, batch, parsing_mask_lists, facial_masks, facial_token_idxs, facial_token_idx_masks)\n\n                predict_loss = loss_dict[\"predict_loss\"]\n                facial_loss = loss_dict[\"facial_loss\"]\n\n                background_loss = loss_dict[\"background_loss\"]\n\n                loss = predict_loss + facial_loss\n\n                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n                      \n                # Backpropagate\n                accelerator.backward(loss)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if accelerator.is_main_process:\n                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}, predict_loss: {}, facial_loss: {}, background_loss: {}\".format(\n                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss, predict_loss, facial_loss, background_loss))\n                    \n            global_step += 1 \n \n            progress_bar.set_description(f\"{exp_name}Epoch {epoch + 1}/{args.num_train_epochs} - Step {step}/{len(train_dataloader)}\")\n\n            if global_step % args.save_steps == 0:\n                save_path = os.path.join(args.output_dir, f\"{exp_name}_Epoch{epoch+1}-{global_step}\")\n                if not os.path.exists(save_path):\n                    try:\n                        os.makedirs(save_path)\n                    except:\n                        print(f\"The path can not be make {save_path}!!!\")\n                \n                save_path_pth = save_path + \"/ConsistentID_SDXL.pth\"\n                torch.save(consistentID_model.state_dict(), save_path_pth)\n\n            if global_step == len(train_dataloader)-1: # save the lastest\n                save_path_pth = os.path.join(args.output_dir, f\"{exp_name}_checkpoint-lasted\") + \"/ConsistentID_SDXL.pth\"\n                if not os.path.exists(os.path.dirname(save_path_pth)):\n                    try:\n                        os.makedirs(os.path.dirname(save_path_pth))\n                    except:\n                        print(f\"The path can not be make {save_path}!!!\")                \n                \n                torch.save(consistentID_model.state_dict(), save_path_pth) \n\n            begin = time.perf_counter()\n\n                     \nif __name__ == \"__main__\":\n    main()    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "pipline_StableDiffusion_ConsistentID.py", "content": "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport cv2\nimport PIL\nimport numpy as np \nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom insightface.app import FaceAnalysis \n### insight-face installation can be found at https://github.com/deepinsight/insightface\nfrom safetensors import safe_open\nfrom huggingface_hub.utils import validate_hf_hub_args\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\nfrom diffusers.utils import _get_model_file\nfrom functions import process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nfrom functions import ProjPlusModel, masks_for_unique_values\nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor, FacialEncoder\n\n### Model can be imported from https://github.com/zllrunning/face-parsing.PyTorch?tab=readme-ov-file\n### We use the ckpt of 79999_iter.pth: https://drive.google.com/open?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812\n### Thanks for the open source of face-parsing model.\nfrom models.BiSeNet.model import BiSeNet\n\nPipelineImageInput = Union[\n    PIL.Image.Image,\n    torch.FloatTensor,\n    List[PIL.Image.Image],\n    List[torch.FloatTensor],\n]\n\n### Download the pretrained model from huggingface and put it locally, then place the model in a local directory and specify the directory location.\nclass ConsistentIDStableDiffusionPipeline(StableDiffusionPipeline):\n    \n    @validate_hf_hub_args\n    def load_ConsistentID_model(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        weight_name: str,\n        subfolder: str = '',\n        trigger_word_ID: str = '<|image|>',\n        trigger_word_facial: str = '<|facial|>',\n        image_encoder_path: str = 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K',  \n        bise_net_cp: str = 'JackAILab/ConsistentID/face_parsing.pth', \n        torch_dtype = torch.float16,\n        num_tokens = 4,\n        lora_rank= 128,\n        **kwargs,\n    ):\n        self.lora_rank = lora_rank \n        self.torch_dtype = torch_dtype\n        self.num_tokens = num_tokens\n        self.set_ip_adapter()\n        self.image_encoder_path = image_encoder_path\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=self.torch_dtype\n        )   \n        self.clip_image_processor = CLIPImageProcessor()\n        self.id_image_processor = CLIPImageProcessor()\n        self.crop_size = 512\n\n        # FaceID\n        self.app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        self.app.prepare(ctx_id=0, det_size=(640, 640))\n\n        ### BiSeNet\n        self.bise_net = BiSeNet(n_classes = 19)\n        self.bise_net.cuda()\n        self.bise_net_cp= bise_net_cp\n        self.bise_net.load_state_dict(torch.load(self.bise_net_cp))\n        self.bise_net.eval()\n        # Colors for all 20 parts\n        self.part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                    [255, 0, 85], [255, 0, 170],\n                    [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                    [0, 255, 85], [0, 255, 170],\n                    [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                    [0, 85, 255], [0, 170, 255],\n                    [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                    [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                    [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n        \n        ### LLVA (Optional)\n        self.llva_model_path = \"liuhaotian/llava-v1.5-13b\" # TODO \n        # IMPORTANT! Download the openai/clip-vit-large-patch14-336 model and specify the model path in config.json (\"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\").\n        self.llva_prompt = \"Describe this person's facial features for me, including face, ears, eyes, nose, and mouth.\" \n        self.llva_tokenizer, self.llva_model, self.llva_image_processor, self.llva_context_len = None,None,None,None #load_pretrained_model(self.llva_model_path)\n\n        self.image_proj_model = ProjPlusModel(\n            cross_attention_dim=self.unet.config.cross_attention_dim, \n            id_embeddings_dim=512,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size, \n            num_tokens=self.num_tokens,  # 4 - inspirsed by IPAdapter and Midjourney\n        ).to(self.device, dtype=self.torch_dtype)\n        self.FacialEncoder = FacialEncoder().to(self.device, dtype=self.torch_dtype)\n\n        # Load the main state dict first.\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", None)\n        token = kwargs.pop(\"token\", None)\n        revision = kwargs.pop(\"revision\", None)\n\n        user_agent = {\n            \"file_type\": \"attn_procs_weights\",\n            \"framework\": \"pytorch\",\n        }\n\n        if not isinstance(pretrained_model_name_or_path_or_dict, dict):\n            model_file = _get_model_file(\n                pretrained_model_name_or_path_or_dict,\n                weights_name=weight_name,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=token,\n                revision=revision,\n                subfolder=subfolder,\n                user_agent=user_agent,\n            )\n            if weight_name.endswith(\".safetensors\"):\n                state_dict = {\"id_encoder\": {}, \"lora_weights\": {}}\n                with safe_open(model_file, framework=\"pt\", device=\"cpu\") as f:\n                    for key in f.keys():\n                        if key.startswith(\"id_encoder.\"):\n                            state_dict[\"id_encoder\"][key.replace(\"id_encoder.\", \"\")] = f.get_tensor(key)\n                        elif key.startswith(\"lora_weights.\"):\n                            state_dict[\"lora_weights\"][key.replace(\"lora_weights.\", \"\")] = f.get_tensor(key)\n            else:\n                state_dict = torch.load(model_file, map_location=\"cpu\")\n        else:\n            state_dict = pretrained_model_name_or_path_or_dict\n    \n        self.trigger_word_ID = trigger_word_ID\n        self.trigger_word_facial = trigger_word_facial\n\n        self.FacialEncoder.load_state_dict(state_dict[\"FacialEncoder\"], strict=True)\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"], strict=True)\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"adapter_modules\"], strict=True)\n        print(f\"Successfully loaded weights from checkpoint\")\n\n        # Add trigger word token\n        if self.tokenizer is not None: \n            self.tokenizer.add_tokens([self.trigger_word_ID], special_tokens=True)\n            self.tokenizer.add_tokens([self.trigger_word_facial], special_tokens=True)\n\n    def set_ip_adapter(self):\n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = Consistent_AttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n                ).to(self.device, dtype=self.torch_dtype)\n            else:\n                attn_procs[name] = Consistent_IPAttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        \n        unet.set_attn_processor(attn_procs)\n\n    @torch.inference_mode()\n    def get_facial_embeds(self, prompt_embeds, negative_prompt_embeds, facial_clip_images, facial_token_masks, valid_facial_token_idx_mask):\n        \n        hidden_states = []\n        uncond_hidden_states = []\n        for facial_clip_image in facial_clip_images:\n            hidden_state = self.image_encoder(facial_clip_image.to(self.device, dtype=self.torch_dtype), output_hidden_states=True).hidden_states[-2]\n            uncond_hidden_state = self.image_encoder(torch.zeros_like(facial_clip_image, dtype=self.torch_dtype).to(self.device), output_hidden_states=True).hidden_states[-2]\n            hidden_states.append(hidden_state)\n            uncond_hidden_states.append(uncond_hidden_state)\n        multi_facial_embeds = torch.stack(hidden_states)       \n        uncond_multi_facial_embeds = torch.stack(uncond_hidden_states)   \n\n        # condition \n        facial_prompt_embeds = self.FacialEncoder(prompt_embeds, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n\n        # uncondition \n        uncond_facial_prompt_embeds = self.FacialEncoder(negative_prompt_embeds, uncond_multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n\n        return facial_prompt_embeds, uncond_facial_prompt_embeds        \n\n    @torch.inference_mode()   \n    def get_image_embeds(self, faceid_embeds, face_image, s_scale, shortcut=False):\n\n        clip_image = self.clip_image_processor(images=face_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=self.torch_dtype)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        uncond_clip_image_embeds = self.image_encoder(torch.zeros_like(clip_image), output_hidden_states=True).hidden_states[-2]\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_tokens = self.image_proj_model(faceid_embeds, clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds), uncond_clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        \n        return image_prompt_tokens, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, Consistent_IPAttProcessor):\n                attn_processor.scale = scale\n\n    @torch.inference_mode()\n    def get_prepare_faceid(self, face_image):\n        faceid_image = np.array(face_image)\n        faces = self.app.get(faceid_image)\n        if faces==[]:\n            faceid_embeds = torch.zeros_like(torch.empty((1, 512)))\n        else:\n            faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n            ### TODO The prior extraction of FaceID is unstable and a stronger ID prior structure can be used.\n            \n        return faceid_embeds\n\n    @torch.inference_mode()\n    def parsing_face_mask(self, raw_image_refer):\n\n        to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n        to_pil = transforms.ToPILImage()\n\n        with torch.no_grad():\n            image = raw_image_refer.resize((512, 512), Image.BILINEAR)\n            image_resize_PIL = image\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.float().cuda()\n            out = self.bise_net(img)[0]\n            parsing_anno = out.squeeze(0).cpu().numpy().argmax(0)\n        \n        im = np.array(image_resize_PIL)\n        vis_im = im.copy().astype(np.uint8)\n        stride=1\n        vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n        vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n        vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n        num_of_class = np.max(vis_parsing_anno)\n\n        for pi in range(1, num_of_class + 1): # num_of_class=17 pi=1~16\n            index = np.where(vis_parsing_anno == pi) \n            vis_parsing_anno_color[index[0], index[1], :] = self.part_colors[pi] \n\n        vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n        vis_parsing_anno_color = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n        return vis_parsing_anno_color, vis_parsing_anno\n\n    @torch.inference_mode()\n    def get_prepare_llva_caption(self, input_image_file, model_path=None, prompt=None):\n        \n        ### Optional: Use the LLaVA\n        # args = type('Args', (), {\n        #     \"model_path\": self.llva_model_path,\n        #     \"model_base\": None,\n        #     \"model_name\": get_model_name_from_path(self.llva_model_path),\n        #     \"query\": self.llva_prompt,\n        #     \"conv_mode\": None,\n        #     \"image_file\": input_image_file,\n        #     \"sep\": \",\",\n        #     \"temperature\": 0,\n        #     \"top_p\": None,\n        #     \"num_beams\": 1,\n        #     \"max_new_tokens\": 512\n        # })() \n        # face_caption = eval_model(args, self.llva_tokenizer, self.llva_model, self.llva_image_processor)\n\n        ### Use built-in template\n        face_caption = \"The person has one face, one nose, two eyes, two ears, and one mouth.\"\n\n        return face_caption\n\n    @torch.inference_mode()\n    def get_prepare_facemask(self, input_image_file):\n\n        vis_parsing_anno_color, vis_parsing_anno = self.parsing_face_mask(input_image_file)\n        parsing_mask_list = masks_for_unique_values(vis_parsing_anno) \n\n        key_parsing_mask_list = {}\n        key_list = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n        processed_keys = set()\n        for key, mask_image in parsing_mask_list.items():\n            if key in key_list:\n                if \"_\" in key:\n                    prefix = key.split(\"_\")[1]\n                    if prefix in processed_keys:                   \n                        continue\n                    else:            \n                        key_parsing_mask_list[key] = mask_image \n                        processed_keys.add(prefix)  \n            \n                key_parsing_mask_list[key] = mask_image            \n\n        return key_parsing_mask_list, vis_parsing_anno_color\n\n    def encode_prompt_with_trigger_word(\n        self,\n        prompt: str,\n        face_caption: str,\n        key_parsing_mask_list = None,\n        image_token = \"<|image|>\", \n        facial_token = \"<|facial|>\",\n        max_num_facials = 5,\n        num_id_images: int = 1,\n        device: Optional[torch.device] = None,\n    ):\n        device = device or self._execution_device\n\n        face_caption_align, key_parsing_mask_list_align = process_text_with_markers(face_caption, key_parsing_mask_list) \n        \n        prompt_face = prompt + \"Detail:\" + face_caption_align\n\n        max_text_length=330      \n        if len(self.tokenizer(prompt_face, max_length=self.tokenizer.model_max_length, padding=\"max_length\",truncation=False,return_tensors=\"pt\").input_ids[0])!=77:\n            prompt_face = \"Detail:\" + face_caption_align + \" Caption:\" + prompt\n        \n        if len(face_caption)>max_text_length:\n            prompt_face = prompt\n            face_caption_align =  \"\"\n  \n        prompt_text_only = prompt_face.replace(\"<|facial|>\", \"\").replace(\"<|image|>\", \"\")\n        tokenizer = self.tokenizer\n        facial_token_id = tokenizer.convert_tokens_to_ids(facial_token)\n        image_token_id = None\n\n        clean_input_id, image_token_mask, facial_token_mask = tokenize_and_mask_noun_phrases_ends(\n        prompt_face, image_token_id, facial_token_id, tokenizer) \n\n        image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask = prepare_image_token_idx(\n            image_token_mask, facial_token_mask, num_id_images, max_num_facials )\n\n        return prompt_text_only, clean_input_id, key_parsing_mask_list_align, facial_token_mask, facial_token_idx, facial_token_idx_mask\n\n    @torch.inference_mode()\n    def get_prepare_clip_image(self, input_image_file, key_parsing_mask_list, image_size=512, max_num_facials=5, change_facial=True):\n        \n        facial_mask = []\n        facial_clip_image = []\n        transform_mask = transforms.Compose([transforms.CenterCrop(size=image_size), transforms.ToTensor(),])\n        clip_image_processor = CLIPImageProcessor()\n\n        num_facial_part = len(key_parsing_mask_list)\n\n        for key in key_parsing_mask_list:\n            key_mask=key_parsing_mask_list[key]\n            facial_mask.append(transform_mask(key_mask))\n            key_mask_raw_image = fetch_mask_raw_image(input_image_file,key_mask)\n            parsing_clip_image = clip_image_processor(images=key_mask_raw_image, return_tensors=\"pt\").pixel_values\n            facial_clip_image.append(parsing_clip_image)\n\n        padding_ficial_clip_image = torch.zeros_like(torch.zeros([1, 3, 224, 224]))\n        padding_ficial_mask = torch.zeros_like(torch.zeros([1, image_size, image_size]))\n\n        if num_facial_part < max_num_facials:\n            facial_clip_image += [torch.zeros_like(padding_ficial_clip_image) for _ in range(max_num_facials - num_facial_part) ]\n            facial_mask += [ torch.zeros_like(padding_ficial_mask) for _ in range(max_num_facials - num_facial_part)]\n\n        facial_clip_image = torch.stack(facial_clip_image, dim=1).squeeze(0)\n        facial_mask = torch.stack(facial_mask, dim=0).squeeze(dim=1)\n\n        return facial_clip_image, facial_mask\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        original_size: Optional[Tuple[int, int]] = None,\n        target_size: Optional[Tuple[int, int]] = None,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        input_id_images: PipelineImageInput = None,\n        start_merge_step: int = 0,\n        class_tokens_mask: Optional[torch.LongTensor] = None,\n        prompt_embeds_text_only: Optional[torch.FloatTensor] = None,\n    ):\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            height,\n            width,\n            callback_steps,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n        )\n        if not isinstance(input_id_images, list):\n            input_id_images = [input_id_images]\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n        do_classifier_free_guidance = guidance_scale >= 1.0\n        input_image_file = input_id_images[0]\n\n        faceid_embeds = self.get_prepare_faceid(face_image=input_image_file)\n        face_caption = self.get_prepare_llva_caption(input_image_file)\n        key_parsing_mask_list, vis_parsing_anno_color = self.get_prepare_facemask(input_image_file)\n\n        assert do_classifier_free_guidance\n\n        # 3. Encode input prompt\n        num_id_images = len(input_id_images)\n\n        (\n            prompt_text_only,\n            clean_input_id,\n            key_parsing_mask_list_align,\n            facial_token_mask,\n            facial_token_idx,\n            facial_token_idx_mask,\n        ) = self.encode_prompt_with_trigger_word(\n            prompt = prompt,\n            face_caption = face_caption,\n            # prompt_2=None,  \n            key_parsing_mask_list=key_parsing_mask_list,\n            device=device,\n            max_num_facials = 5,\n            num_id_images= num_id_images,\n            # prompt_embeds= None,\n            # pooled_prompt_embeds= None,\n            # class_tokens_mask= None,\n        )\n\n        # 4. Encode input prompt without the trigger word for delayed conditioning\n        encoder_hidden_states = self.text_encoder(clean_input_id.to(device))[0] \n\n        prompt_embeds = self._encode_prompt(\n            prompt_text_only,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            do_classifier_free_guidance=True,\n            negative_prompt=negative_prompt,\n        )\n        negative_encoder_hidden_states_text_only = prompt_embeds[0:num_images_per_prompt]\n        encoder_hidden_states_text_only = prompt_embeds[num_images_per_prompt:]\n\n        # 5. Prepare the input ID images\n        prompt_tokens_faceid, uncond_prompt_tokens_faceid = self.get_image_embeds(faceid_embeds, face_image=input_image_file, s_scale=1.0, shortcut=False)\n\n        facial_clip_image, facial_mask = self.get_prepare_clip_image(input_image_file, key_parsing_mask_list_align, image_size=512, max_num_facials=5)\n        facial_clip_images = facial_clip_image.unsqueeze(0).to(device, dtype=self.torch_dtype)\n        facial_token_mask = facial_token_mask.to(device)\n        facial_token_idx_mask = facial_token_idx_mask.to(device)\n        negative_encoder_hidden_states = negative_encoder_hidden_states_text_only\n\n        cross_attention_kwargs = {}\n\n        # 6. Get the update text embedding\n        prompt_embeds_facial, uncond_prompt_embeds_facial = self.get_facial_embeds(encoder_hidden_states, negative_encoder_hidden_states, \\\n                                                            facial_clip_images, facial_token_mask, facial_token_idx_mask)\n\n        prompt_embeds = torch.cat([prompt_embeds_facial, prompt_tokens_faceid], dim=1)\n        negative_prompt_embeds = torch.cat([uncond_prompt_embeds_facial, uncond_prompt_tokens_faceid], dim=1)\n\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n        )        \n        prompt_embeds_text_only = torch.cat([encoder_hidden_states_text_only, prompt_tokens_faceid], dim=1)\n        prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_text_only], dim=0)\n\n        # 7. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 8. Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n        (\n            null_prompt_embeds,\n            augmented_prompt_embeds,\n            text_prompt_embeds,\n        ) = prompt_embeds.chunk(3)\n\n        # 9. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                latent_model_input = (\n                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n                \n                if i <= start_merge_step:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, text_prompt_embeds], dim=0\n                    )\n                else:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, augmented_prompt_embeds], dim=0\n                    )\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=current_prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n                else:\n                    assert 0, \"Not Implemented\"\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, **extra_step_kwargs\n                ).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        if output_type == \"latent\":\n            image = latents\n            has_nsfw_concept = None\n        elif output_type == \"pil\":\n            # 9.1 Post-processing\n            image = self.decode_latents(latents)\n\n            # 9.2 Run safety checker\n            image, has_nsfw_concept = self.run_safety_checker(\n                image, device, prompt_embeds.dtype\n            )\n\n            # 9.3 Convert to PIL\n            image = self.numpy_to_pil(image)\n        else:\n            # 9.1 Post-processing\n            image = self.decode_latents(latents)\n\n            # 9.2 Run safety checker\n            image, has_nsfw_concept = self.run_safety_checker(\n                image, device, prompt_embeds.dtype\n            )\n\n        # Offload last model to CPU\n        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n            self.final_offload_hook.offload()\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(\n            images=image, nsfw_content_detected=has_nsfw_concept\n        )\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "pipelines/StableDIffusionControlNetInpaint_ConsistentID.py", "content": "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport cv2\nimport PIL\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom insightface.app import FaceAnalysis\nfrom safetensors import safe_open\nfrom huggingface_hub.utils import validate_hf_hub_args\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom diffusers import StableDiffusionControlNetInpaintPipeline\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\nfrom diffusers.utils import _get_model_file\nfrom diffusers.utils.torch_utils import is_compiled_module, randn_tensor\nfrom diffusers.utils import (\n    USE_PEFT_BACKEND,\n    deprecate,\n    logging,\n    replace_example_docstring,\n    scale_lora_layers,\n    unscale_lora_layers,\n)\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel, ControlNetModel\nfrom functions import process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nfrom functions import ProjPlusModel, masks_for_unique_values\nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor, FacialEncoder\nfrom .BaseConsistentID import BaseConsistentIDPipeline\nfrom models.BiSeNet.model import BiSeNet\n\nPipelineImageInput = Union[\n    PIL.Image.Image,\n    torch.FloatTensor,\n    List[PIL.Image.Image],\n    List[torch.FloatTensor],\n]\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> # !pip install transformers accelerate\n        >>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\n        >>> from diffusers.utils import load_image\n        >>> import numpy as np\n        >>> import torch\n\n        >>> init_image = load_image(\n        ...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png\"\n        ... )\n        >>> init_image = init_image.resize((512, 512))\n\n        >>> generator = torch.Generator(device=\"cpu\").manual_seed(1)\n\n        >>> mask_image = load_image(\n        ...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png\"\n        ... )\n        >>> mask_image = mask_image.resize((512, 512))\n\n\n        >>> def make_canny_condition(image):\n        ...     image = np.array(image)\n        ...     image = cv2.Canny(image, 100, 200)\n        ...     image = image[:, :, None]\n        ...     image = np.concatenate([image, image, image], axis=2)\n        ...     image = Image.fromarray(image)\n        ...     return image\n\n\n        >>> control_image = make_canny_condition(init_image)\n\n        >>> controlnet = ControlNetModel.from_pretrained(\n        ...     \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16\n        ... )\n        >>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n        ...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n        ... )\n\n        >>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        >>> pipe.enable_model_cpu_offload()\n\n        >>> # generate image\n        >>> image = pipe(\n        ...     \"a handsome man with ray-ban sunglasses\",\n        ...     num_inference_steps=20,\n        ...     generator=generator,\n        ...     eta=1.0,\n        ...     image=init_image,\n        ...     mask_image=mask_image,\n        ...     control_image=control_image,\n        ... ).images[0]\n        ```\n\"\"\"\n\nclass StableDiffusionControlNetInpaintConsistentIDPipeline(StableDiffusionControlNetInpaintPipeline, BaseConsistentIDPipeline):\n\n    @torch.no_grad()\n    # @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        image: PipelineImageInput = None,\n        mask_image: PipelineImageInput = None,\n        control_image: PipelineImageInput = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        strength: float = 1.0,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        original_size: Optional[Tuple[int, int]] = None,\n        target_size: Optional[Tuple[int, int]] = None,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        input_id_images: PipelineImageInput = None,\n        start_merge_step: int = 0,\n        prompt_embeds_text_only: Optional[torch.FloatTensor] = None,\n        controlnet_conditioning_scale: Union[float, List[float]] = 0.5,\n        control_guidance_start: Union[float, List[float]] = 0.0,\n        control_guidance_end: Union[float, List[float]] = 1.0,\n    ):\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n        \n        controlnet = self.controlnet._orig_mod if is_compiled_module(self.controlnet) else self.controlnet\n\n        # align format for control guidance\n        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n            mult = len(controlnet.nets) if isinstance(controlnet, MultiControlNetModel) else 1\n            control_guidance_start, control_guidance_end = (\n                mult * [control_guidance_start],\n                mult * [control_guidance_end],\n            )\n\n        # 1. Check inputs\n        self.check_inputs(\n            prompt,\n            control_image,\n            mask_image,\n            height,\n            width,\n            callback_steps,\n            output_type,\n            negative_prompt,\n            prompt_embeds,\n            negative_prompt_embeds,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            control_guidance_start=control_guidance_start,\n            control_guidance_end=control_guidance_end,\n        )\n        if not isinstance(input_id_images, list):\n            input_id_images = [input_id_images]\n        \n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n        do_classifier_free_guidance = guidance_scale >= 1.0\n        input_image_file = input_id_images[0]\n\n        faceid_embeds = self.get_prepare_faceid(face_image=input_image_file)\n        face_caption = self.get_prepare_llva_caption(input_image_file)\n        key_parsing_mask_list, vis_parsing_anno_color = self.get_prepare_facemask(input_image_file)\n\n        assert do_classifier_free_guidance\n\n        # 3. Encode input prompt\n        num_id_images = len(input_id_images)\n\n        (\n            prompt_text_only,\n            clean_input_id,\n            key_parsing_mask_list_align,\n            facial_token_mask,\n            facial_token_idx,\n            facial_token_idx_mask,\n        ) = self.encode_prompt_with_trigger_word(\n            prompt = prompt,\n            face_caption = face_caption,\n            # prompt_2=None,  \n            key_parsing_mask_list=key_parsing_mask_list,\n            device=device,\n            max_num_facials = 5,\n            num_id_images= num_id_images,\n            # prompt_embeds= None,\n            # pooled_prompt_embeds= None,\n            # class_tokens_mask= None,\n        )\n\n        # 4. Encode input prompt without the trigger word for delayed conditioning\n        encoder_hidden_states = self.text_encoder(clean_input_id.to(device))[0] \n\n        prompt_embeds = self._encode_prompt(\n            prompt_text_only,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            do_classifier_free_guidance=True,\n            negative_prompt=negative_prompt,\n        )\n        negative_encoder_hidden_states_text_only = prompt_embeds[0:num_images_per_prompt]\n        encoder_hidden_states_text_only = prompt_embeds[num_images_per_prompt:]\n\n        # 5. Prepare the input ID images\n        prompt_tokens_faceid, uncond_prompt_tokens_faceid = self.get_image_embeds(faceid_embeds, face_image=input_image_file, s_scale=1.0, shortcut=False)\n\n        facial_clip_image, facial_mask = self.get_prepare_clip_image(input_image_file, key_parsing_mask_list_align, image_size=512, max_num_facials=5)\n        facial_clip_images = facial_clip_image.unsqueeze(0).to(device, dtype=self.torch_dtype)\n        facial_token_mask = facial_token_mask.to(device)\n        facial_token_idx_mask = facial_token_idx_mask.to(device)\n        negative_encoder_hidden_states = negative_encoder_hidden_states_text_only\n\n        cross_attention_kwargs = {}\n\n        # 6. Get the update text embedding\n        prompt_embeds_facial, uncond_prompt_embeds_facial = self.get_facial_embeds(encoder_hidden_states, negative_encoder_hidden_states, \\\n                                                            facial_clip_images, facial_token_mask, facial_token_idx_mask)\n\n        prompt_embeds = torch.cat([prompt_embeds_facial, prompt_tokens_faceid], dim=1)\n        negative_prompt_embeds = torch.cat([uncond_prompt_embeds_facial, uncond_prompt_tokens_faceid], dim=1)\n\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n        )        \n        prompt_embeds_text_only = torch.cat([encoder_hidden_states_text_only, prompt_tokens_faceid], dim=1)\n        prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_text_only], dim=0)\n\n        # 7. Prepare images\n        init_image = self.image_processor.preprocess(\n            input_image_file, height=height, width=width, # crops_coords=None, resize_mode='default'\n        )\n        init_image = init_image.to(dtype=torch.float32)\n\n        mask = self.mask_processor.preprocess(\n            mask_image, height=height, width=width, # resize_mode='default', crops_coords=None\n        )\n\n        masked_image = init_image * (mask < 0.5)\n        _, _, height, width = init_image.shape\n\n        if isinstance(controlnet, ControlNetModel):\n            control_image = self.prepare_control_image(\n                image=control_image,\n                width=width,\n                height=height,\n                batch_size=batch_size * num_images_per_prompt,\n                num_images_per_prompt=num_images_per_prompt,\n                device=device,\n                dtype=controlnet.dtype,\n                # crops_coords=None,\n                # resize_mode='default',\n                do_classifier_free_guidance=do_classifier_free_guidance,\n                guess_mode=True,\n            )\n        elif isinstance(controlnet, MultiControlNetModel):\n            control_images = []\n\n            for control_image_ in control_image:\n                control_image_ = self.prepare_control_image(\n                    image=control_image_,\n                    width=width,\n                    height=height,\n                    batch_size=batch_size * num_images_per_prompt,\n                    num_images_per_prompt=num_images_per_prompt,\n                    device=device,\n                    dtype=controlnet.dtype,\n                    crops_coords=None,\n                    resize_mode='default',\n                    do_classifier_free_guidance=do_classifier_free_guidance,\n                    guess_mode=True,\n                )\n\n                control_images.append(control_image_)\n\n            control_image = control_images\n        else:\n            assert False\n\n        # 8. Pprepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps, num_inference_steps = self.get_timesteps(\n            num_inference_steps=num_inference_steps, strength=strength, device=device\n        )\n        # at which timestep to set the initial noise (n.b. 50% if strength is 0.5)\n        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n        # create a boolean to check if the strength is set to 1. if so then initialise the latents with pure noise\n        is_strength_max = strength == 1.0\n        self._num_timesteps = len(timesteps)\n\n        # 9.Prepare latent variables\n        num_channels_latents = self.vae.config.latent_channels\n        num_channels_unet = self.unet.config.in_channels\n        return_image_latents = num_channels_unet == 4\n        latents_outputs = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n            image=init_image,\n            timestep=latent_timestep,\n            is_strength_max=is_strength_max,\n            return_noise=True,\n            return_image_latents=return_image_latents,\n        )\n\n        if return_image_latents:\n            latents, noise, image_latents = latents_outputs\n        else:\n            latents, noise = latents_outputs\n\n        # 10. Prepare mask latent variables\n        mask, masked_image_latents = self.prepare_mask_latents(\n            mask,\n            masked_image,\n            batch_size * num_images_per_prompt,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            do_classifier_free_guidance,\n        )\n\n        # 11. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n        \n        (\n            null_prompt_embeds,\n            augmented_prompt_embeds,\n            text_prompt_embeds,\n        ) = prompt_embeds.chunk(3)\n\n        # 12. Create tensor stating which controlnets to keep\n        controlnet_keep = []\n        for i in range(len(timesteps)):\n            keeps = [\n                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n                for s, e in zip(control_guidance_start, control_guidance_end)\n            ]\n            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n\n        # 13. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                if i <= start_merge_step:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, text_prompt_embeds], dim=0\n                    )\n                else:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, augmented_prompt_embeds], dim=0\n                    )\n                \n                if do_classifier_free_guidance:\n                    control_model_input = latents\n                    control_model_input = self.scheduler.scale_model_input(control_model_input, t)\n                    controlnet_prompt_embeds = current_prompt_embeds.chunk(2)[1]\n                else:\n                    control_model_input = latent_model_input\n                    controlnet_prompt_embeds = current_prompt_embeds\n\n                if isinstance(controlnet_keep[i], list):\n                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n                else:\n                    controlnet_cond_scale = controlnet_conditioning_scale\n                    if isinstance(controlnet_cond_scale, list):\n                        controlnet_cond_scale = controlnet_cond_scale[0]\n                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n                \n                down_block_res_samples, mid_block_res_sample = self.controlnet(\n                    control_model_input,\n                    t,\n                    encoder_hidden_states=controlnet_prompt_embeds,\n                    controlnet_cond=control_image,\n                    conditioning_scale=cond_scale,\n                    return_dict=False,\n                )\n\n                # predict the noise residual\n                if num_channels_unet == 9:\n                    latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=current_prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                    down_block_additional_residuals=down_block_res_samples,\n                    mid_block_additional_residual=mid_block_res_sample,\n                ).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                else:\n                    assert 0, 'Not Implemented'\n                \n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                if num_channels_unet == 4:\n                    init_latents_proper = image_latents\n                    if do_classifier_free_guidance:\n                        init_mask, _ = mask.chunk(2)\n                    else:\n                        init_mask = mask\n\n                    if i < len(timesteps) - 1:\n                        noise_timestep = timesteps[i + 1]\n                        init_latents_proper = self.scheduler.add_noise(\n                            init_latents_proper, noise, torch.tensor([noise_timestep])\n                        )\n                    latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n                \n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n\n        # If we do sequential model offloading, let's offload unet and controlnet\n        # manually for max memory savings\n        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n            self.unet.to(\"cpu\")\n            torch.cuda.empty_cache()\n\n        if not output_type == \"latent\":\n            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[\n                0\n            ]\n            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n        else:\n            image = latents\n            has_nsfw_concept = None\n\n        if has_nsfw_concept is None:\n            do_denormalize = [True] * image.shape[0]\n        else:\n            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n"}
{"type": "source_file", "path": "models/BiSeNet/resnet.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\nresnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_chan),\n                )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = F.relu(self.bn1(residual))\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum-1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x) # 1/8\n        feat16 = self.layer3(feat8) # 1/16\n        feat32 = self.layer4(feat16) # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if 'fc' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module,  nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nif __name__ == \"__main__\":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()\n"}
{"type": "source_file", "path": "models/BiSeNet/prepropess_data.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport os.path as osp\nimport os\nimport cv2\nfrom transform import *\nfrom PIL import Image\n\nface_data = '/home/zll/data/CelebAMask-HQ/CelebA-HQ-img'\nface_sep_mask = '/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno'\nmask_path = '/home/zll/data/CelebAMask-HQ/mask'\ncounter = 0\ntotal = 0\nfor i in range(15):\n\n    atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',\n            'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']\n\n    for j in range(i * 2000, (i + 1) * 2000):\n\n        mask = np.zeros((512, 512))\n\n        for l, att in enumerate(atts, 1):\n            total += 1\n            file_name = ''.join([str(j).rjust(5, '0'), '_', att, '.png'])\n            path = osp.join(face_sep_mask, str(i), file_name)\n\n            if os.path.exists(path):\n                counter += 1\n                sep_mask = np.array(Image.open(path).convert('P'))\n                # print(np.unique(sep_mask))\n\n                mask[sep_mask == 225] = l\n        cv2.imwrite('{}/{}.png'.format(mask_path, j), mask)\n        print(j)\n\nprint(counter, total)"}
{"type": "source_file", "path": "models/LLaVA/demo_llva_use.py", "content": "import sys\nsys.path.append(\"./Llava1.5/LLaVA\")\n# Detailed model can be viewed at https://github.com/haotian-liu/LLaVA\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path\nfrom llava.eval.run_llava import eval_model\n\nmodel_path = \"./pretrained_model/llava-v1.5-7b\" # \"liuhaotian/llava-v1.5-7b\"\nprompt = \"Please describe the people in the image, including their gender, age, clothing, facial expressions, and any other distinguishing features.\"\nimage_file = \"./demo.png\"\n\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path=model_path,\n    model_base=None,\n    model_name=get_model_name_from_path(model_path),\n    # load_4bit=True\n) # device=\"cuda\"\n\nargs = type('Args', (), {\n    \"model_path\": model_path,\n    \"model_base\": None,\n    \"model_name\": get_model_name_from_path(model_path),\n    \"query\": prompt,\n    \"conv_mode\": None,\n    \"image_file\": image_file,\n    \"sep\": \",\",\n    \"temperature\": 0,\n    \"top_p\": None,\n    \"num_beams\": 1,\n    \"max_new_tokens\": 512\n})()\n\noutputs = eval_model(args, tokenizer, model, image_processor)\nprint(f\"The caption is: {outputs}\")\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "pipelines/BaseConsistentID.py", "content": "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport cv2\nimport PIL\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom insightface.app import FaceAnalysis\nfrom safetensors import safe_open\nfrom huggingface_hub.utils import validate_hf_hub_args\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom diffusers.utils import _get_model_file\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel, ControlNetModel\nfrom functions import process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nfrom functions import ProjPlusModel, masks_for_unique_values\nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor, FacialEncoder\n\nfrom models.BiSeNet.model import BiSeNet\n\nPipelineImageInput = Union[\n    PIL.Image.Image,\n    torch.FloatTensor,\n    List[PIL.Image.Image],\n    List[torch.FloatTensor],\n]\n\nclass BaseConsistentIDPipeline:\n\n    @validate_hf_hub_args\n    def load_ConsistentID_model(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        weight_name: str,\n        subfolder: str = '',\n        trigger_word_ID: str = '<|image|>',\n        trigger_word_facial: str = '<|facial|>',\n        image_encoder_path: str = 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K',  \n        torch_dtype = torch.float16,\n        num_tokens = 4,\n        lora_rank= 128,\n        **kwargs,\n    ):\n        self.lora_rank = lora_rank \n        self.torch_dtype = torch_dtype\n        self.num_tokens = num_tokens\n        self.set_ip_adapter()\n        self.image_encoder_path = image_encoder_path\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=self.torch_dtype\n        )   \n        self.clip_image_processor = CLIPImageProcessor()\n        self.id_image_processor = CLIPImageProcessor()\n        self.crop_size = 512\n\n        # FaceID\n        self.app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n        self.app.prepare(ctx_id=0, det_size=(640, 640))\n\n        ### BiSeNet\n        self.bise_net = BiSeNet(n_classes = 19)\n        self.bise_net.cuda()\n        self.bise_net_cp='JackAILab/ConsistentID/face_parsing.pth' \n        self.bise_net.load_state_dict(torch.load(self.bise_net_cp))\n        self.bise_net.eval()\n        # Colors for all 20 parts\n        self.part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                    [255, 0, 85], [255, 0, 170],\n                    [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                    [0, 255, 85], [0, 255, 170],\n                    [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                    [0, 85, 255], [0, 170, 255],\n                    [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                    [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                    [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n        \n        ### LLVA (Optional)\n        self.llva_model_path = \"liuhaotian/llava-v1.5-13b\" # TODO \n        # IMPORTANT! Download the openai/clip-vit-large-patch14-336 model and specify the model path in config.json (\"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\").\n        self.llva_prompt = \"Describe this person's facial features for me, including face, ears, eyes, nose, and mouth.\" \n        self.llva_tokenizer, self.llva_model, self.llva_image_processor, self.llva_context_len = None,None,None,None #load_pretrained_model(self.llva_model_path)\n\n        self.image_proj_model = ProjPlusModel(\n            cross_attention_dim=self.unet.config.cross_attention_dim, \n            id_embeddings_dim=512,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size, \n            num_tokens=self.num_tokens,  # 4 - inspirsed by IPAdapter and Midjourney\n        ).to(self.device, dtype=self.torch_dtype)\n        self.FacialEncoder = FacialEncoder(self.image_encoder).to(self.device, dtype=self.torch_dtype)\n\n        # Load the main state dict first.\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", None)\n        token = kwargs.pop(\"token\", None)\n        revision = kwargs.pop(\"revision\", None)\n\n        user_agent = {\n            \"file_type\": \"attn_procs_weights\",\n            \"framework\": \"pytorch\",\n        }\n\n        if not isinstance(pretrained_model_name_or_path_or_dict, dict):\n            model_file = _get_model_file(\n                pretrained_model_name_or_path_or_dict,\n                weights_name=weight_name,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=token,\n                revision=revision,\n                subfolder=subfolder,\n                user_agent=user_agent,\n            )\n            if weight_name.endswith(\".safetensors\"):\n                state_dict = {\"id_encoder\": {}, \"lora_weights\": {}}\n                with safe_open(model_file, framework=\"pt\", device=\"cpu\") as f:\n                    for key in f.keys():\n                        if key.startswith(\"id_encoder.\"):\n                            state_dict[\"id_encoder\"][key.replace(\"id_encoder.\", \"\")] = f.get_tensor(key)\n                        elif key.startswith(\"lora_weights.\"):\n                            state_dict[\"lora_weights\"][key.replace(\"lora_weights.\", \"\")] = f.get_tensor(key)\n            else:\n                state_dict = torch.load(model_file, map_location=\"cpu\")\n        else:\n            state_dict = pretrained_model_name_or_path_or_dict\n    \n        self.trigger_word_ID = trigger_word_ID\n        self.trigger_word_facial = trigger_word_facial\n\n        self.FacialEncoder.load_state_dict(state_dict[\"FacialEncoder\"], strict=True)\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"], strict=True)\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"adapter_modules\"], strict=True)\n        print(f\"Successfully loaded weights from checkpoint\")\n\n        # Add trigger word token\n        if self.tokenizer is not None: \n            self.tokenizer.add_tokens([self.trigger_word_ID], special_tokens=True)\n            self.tokenizer.add_tokens([self.trigger_word_facial], special_tokens=True)\n\n    def set_ip_adapter(self):\n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = Consistent_AttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n                ).to(self.device, dtype=self.torch_dtype)\n            else:\n                attn_procs[name] = Consistent_IPAttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        \n        unet.set_attn_processor(attn_procs)\n\n    @torch.inference_mode()\n    def get_facial_embeds(self, prompt_embeds, negative_prompt_embeds, facial_clip_images, facial_token_masks, valid_facial_token_idx_mask):\n        \n        hidden_states = []\n        uncond_hidden_states = []\n        for facial_clip_image in facial_clip_images:\n            hidden_state = self.image_encoder(facial_clip_image.to(self.device, dtype=self.torch_dtype), output_hidden_states=True).hidden_states[-2]\n            uncond_hidden_state = self.image_encoder(torch.zeros_like(facial_clip_image, dtype=self.torch_dtype).to(self.device), output_hidden_states=True).hidden_states[-2]\n            hidden_states.append(hidden_state)\n            uncond_hidden_states.append(uncond_hidden_state)\n        multi_facial_embeds = torch.stack(hidden_states)       \n        uncond_multi_facial_embeds = torch.stack(uncond_hidden_states)   \n\n        # condition \n        facial_prompt_embeds = self.FacialEncoder(prompt_embeds, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n\n        # uncondition \n        uncond_facial_prompt_embeds = self.FacialEncoder(negative_prompt_embeds, uncond_multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n\n        return facial_prompt_embeds, uncond_facial_prompt_embeds        \n\n    @torch.inference_mode()   \n    def get_image_embeds(self, faceid_embeds, face_image, s_scale, shortcut=False):\n\n        clip_image = self.clip_image_processor(images=face_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=self.torch_dtype)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        uncond_clip_image_embeds = self.image_encoder(torch.zeros_like(clip_image), output_hidden_states=True).hidden_states[-2]\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_tokens = self.image_proj_model(faceid_embeds, clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds), uncond_clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        \n        return image_prompt_tokens, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, Consistent_IPAttProcessor):\n                attn_processor.scale = scale\n\n    @torch.inference_mode()\n    def get_prepare_faceid(self, face_image):\n        faceid_image = np.array(face_image)\n        faces = self.app.get(faceid_image)\n        if faces==[]:\n            faceid_embeds = torch.zeros_like(torch.empty((1, 512)))\n        else:\n            faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n\n        return faceid_embeds\n\n    @torch.inference_mode()\n    def parsing_face_mask(self, raw_image_refer):\n\n        to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n        to_pil = transforms.ToPILImage()\n\n        with torch.no_grad():\n            image = raw_image_refer.resize((512, 512), Image.BILINEAR)\n            image_resize_PIL = image\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.float().cuda()\n            out = self.bise_net(img)[0]\n            parsing_anno = out.squeeze(0).cpu().numpy().argmax(0)\n        \n        im = np.array(image_resize_PIL)\n        vis_im = im.copy().astype(np.uint8)\n        stride=1\n        vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n        vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n        vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n        num_of_class = np.max(vis_parsing_anno)\n\n        for pi in range(1, num_of_class + 1): # num_of_class=17 pi=1~16\n            index = np.where(vis_parsing_anno == pi) \n            vis_parsing_anno_color[index[0], index[1], :] = self.part_colors[pi] \n\n        vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n        vis_parsing_anno_color = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n        return vis_parsing_anno_color, vis_parsing_anno\n\n    @torch.inference_mode()\n    def get_prepare_llva_caption(self, input_image_file, model_path=None, prompt=None):\n        \n        ### Optional: Use the LLaVA\n        # args = type('Args', (), {\n        #     \"model_path\": self.llva_model_path,\n        #     \"model_base\": None,\n        #     \"model_name\": get_model_name_from_path(self.llva_model_path),\n        #     \"query\": self.llva_prompt,\n        #     \"conv_mode\": None,\n        #     \"image_file\": input_image_file,\n        #     \"sep\": \",\",\n        #     \"temperature\": 0,\n        #     \"top_p\": None,\n        #     \"num_beams\": 1,\n        #     \"max_new_tokens\": 512\n        # })() \n        # face_caption = eval_model(args, self.llva_tokenizer, self.llva_model, self.llva_image_processor)\n\n        ### Use built-in template\n        face_caption = \"The person has one nose, two eyes, two ears, and a mouth.\"\n\n        return face_caption\n\n    @torch.inference_mode()\n    def get_prepare_facemask(self, input_image_file):\n\n        vis_parsing_anno_color, vis_parsing_anno = self.parsing_face_mask(input_image_file)\n        parsing_mask_list = masks_for_unique_values(vis_parsing_anno) \n\n        key_parsing_mask_list = {}\n        key_list = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n        processed_keys = set()\n        for key, mask_image in parsing_mask_list.items():\n            if key in key_list:\n                if \"_\" in key:\n                    prefix = key.split(\"_\")[1]\n                    if prefix in processed_keys:                   \n                        continue\n                    else:            \n                        key_parsing_mask_list[key] = mask_image \n                        processed_keys.add(prefix)  \n            \n                key_parsing_mask_list[key] = mask_image            \n\n        return key_parsing_mask_list, vis_parsing_anno_color\n\n    def encode_prompt_with_trigger_word(\n        self,\n        prompt: str,\n        face_caption: str,\n        key_parsing_mask_list = None,\n        image_token = \"<|image|>\", \n        facial_token = \"<|facial|>\",\n        max_num_facials = 5,\n        num_id_images: int = 1,\n        device: Optional[torch.device] = None,\n    ):\n        device = device or self._execution_device\n\n        face_caption_align, key_parsing_mask_list_align = process_text_with_markers(face_caption, key_parsing_mask_list) \n        \n        prompt_face = prompt + \"Detail:\" + face_caption_align\n\n        max_text_length=330      \n        if len(self.tokenizer(prompt_face, max_length=self.tokenizer.model_max_length, padding=\"max_length\",truncation=False,return_tensors=\"pt\").input_ids[0])!=77:\n            prompt_face = \"Detail:\" + face_caption_align + \" Caption:\" + prompt\n        \n        if len(face_caption)>max_text_length:\n            prompt_face = prompt\n            face_caption_align =  \"\"\n  \n        prompt_text_only = prompt_face.replace(\"<|facial|>\", \"\").replace(\"<|image|>\", \"\")\n        tokenizer = self.tokenizer\n        facial_token_id = tokenizer.convert_tokens_to_ids(facial_token)\n        image_token_id = None\n\n        clean_input_id, image_token_mask, facial_token_mask = tokenize_and_mask_noun_phrases_ends(\n        prompt_face, image_token_id, facial_token_id, tokenizer) \n\n        image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask = prepare_image_token_idx(\n            image_token_mask, facial_token_mask, num_id_images, max_num_facials )\n\n        return prompt_text_only, clean_input_id, key_parsing_mask_list_align, facial_token_mask, facial_token_idx, facial_token_idx_mask\n\n    @torch.inference_mode()\n    def get_prepare_clip_image(self, input_image_file, key_parsing_mask_list, image_size=512, max_num_facials=5, change_facial=True):\n        \n        facial_mask = []\n        facial_clip_image = []\n        transform_mask = transforms.Compose([transforms.CenterCrop(size=image_size), transforms.ToTensor(),])\n        clip_image_processor = CLIPImageProcessor()\n\n        num_facial_part = len(key_parsing_mask_list)\n\n        for key in key_parsing_mask_list:\n            key_mask=key_parsing_mask_list[key]\n            facial_mask.append(transform_mask(key_mask))\n            key_mask_raw_image = fetch_mask_raw_image(input_image_file,key_mask)\n            parsing_clip_image = clip_image_processor(images=key_mask_raw_image, return_tensors=\"pt\").pixel_values\n            facial_clip_image.append(parsing_clip_image)\n\n        padding_ficial_clip_image = torch.zeros_like(torch.zeros([1, 3, 224, 224]))\n        padding_ficial_mask = torch.zeros_like(torch.zeros([1, image_size, image_size]))\n\n        if num_facial_part < max_num_facials:\n            facial_clip_image += [torch.zeros_like(padding_ficial_clip_image) for _ in range(max_num_facials - num_facial_part) ]\n            facial_mask += [ torch.zeros_like(padding_ficial_mask) for _ in range(max_num_facials - num_facial_part)]\n\n        facial_clip_image = torch.stack(facial_clip_image, dim=1).squeeze(0)\n        facial_mask = torch.stack(facial_mask, dim=0).squeeze(dim=1)\n\n        return facial_clip_image, facial_mask\n"}
{"type": "source_file", "path": "train.py", "content": "import os\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nfrom pathlib import Path\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration\nfrom diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\nfrom transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\nfrom tqdm import tqdm \nfrom utils import parse_args, collate_fn, MyDataset \nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor\nfrom attention import FacialEncoder\nfrom functions import ProjPlusModel, BalancedL1Loss, unet_store_cross_attention_scores, get_object_localization_loss \n\nexp_name = 'ConsistentID' \ninitial_epoch = 0\n\nclass ConsistentID(torch.nn.Module):\n    \"\"\"ConsistentID\"\"\"\n    def __init__(self, unet, image_proj_model, adapter_modules, facial_encoder):\n        super().__init__()\n        self.unet = unet\n        self.image_proj_model = image_proj_model\n        self.adapter_modules = adapter_modules\n        self.FacialEncoder = facial_encoder\n\n        ### attention loss\n        self.cross_attention_scores = {}\n        self.localization_layers  = 5\n        self.facial_weight = 0.01\n        self.mask_loss_prob = 0.5\n        self.unet = unet_store_cross_attention_scores(\n            self.unet, self.cross_attention_scores, self.localization_layers \n        )\n        self.object_localization_loss_fn = BalancedL1Loss(threshold=1.0, normalize=True)    \n\n    def forward(self, noisy_latents, timesteps, prompt_embeds, image_embeds, faceid_embeds, \\\n                multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask, \\\n                noise, parsing_mask_lists, facial_masks, facial_token_idxs, facial_token_idx_masks): \n\n        ### Overall Feature\n        faceid_tokens = self.image_proj_model(faceid_embeds, image_embeds) \n\n        ### Fine-grained Feature\n        prompt_id_embeds = self.FacialEncoder(prompt_embeds, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask) \n\n        ### Final Features\n        prompt_id_embeds = torch.cat([prompt_id_embeds, faceid_tokens], dim=1)\n        noise_pred = self.unet(noisy_latents, timesteps, prompt_id_embeds).sample \n        \n        #### Random Mask     \n        target=noise\n        pred=noise_pred\n        loss_dict = {\"background_loss\": 0}\n        if torch.rand(1) < self.mask_loss_prob:\n            try:\n                mask_list = [TF.to_tensor(image['WithoutBackground']).unsqueeze(0) for image in parsing_mask_lists] \n                mask_stacked = torch.cat(mask_list, dim=0)\n                mask_final = F.interpolate(mask_stacked,size=(pred.shape[-2], pred.shape[-1]),mode=\"bilinear\",align_corners=False,) \n                pred = pred * mask_final.to(pred.device, dtype=pred.dtype)\n                target = target * mask_final.to(target.device, dtype=target.dtype)\n\n                background_loss = F.mse_loss(pred.float(), target.float(), reduction=\"mean\")\n                loss_dict[\"background_loss\"] = background_loss\n            except:\n                print(f\"The fail 'Background' of parsing_mask_lists: {parsing_mask_lists}\")\n\n        predict_loss = F.mse_loss(pred.float(), target.float(), reduction=\"mean\") \n        loss_dict[\"predict_loss\"] = predict_loss\n        \n        ### Attention Loss\n        loss_dict[\"facial_loss\"] = 0\n        object_segmaps = facial_masks\n        image_token_idx = facial_token_idxs\n        image_token_idx_mask = facial_token_idx_masks\n        facial_loss = get_object_localization_loss(\n            self.cross_attention_scores,\n            object_segmaps,\n            image_token_idx,\n            image_token_idx_mask,\n            self.object_localization_loss_fn,\n        )\n            \n        facial_loss = self.facial_weight * facial_loss \n        loss_dict[\"facial_loss\"]=facial_loss\n\n        return pred, loss_dict\n\ndef main():\n\n    args = parse_args()\n\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n\n    accelerator = Accelerator(\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        project_config=accelerator_project_config,\n    )\n    \n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load scheduler, tokenizer and models.\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n\n    # freeze parameters of models to save more memory\n    unet.requires_grad_(False)\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    image_encoder.requires_grad_(False)\n\n    # Projection\n    image_proj_model = ProjPlusModel(\n        cross_attention_dim=768,\n        id_embeddings_dim=512,\n        clip_embeddings_dim=image_encoder.config.hidden_size,\n        num_tokens=args.num_tokens,\n    )\n\n    ### Facial Encoder\n    facial_encoder = FacialEncoder(image_CLIPModel_encoder=None)\n\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"fp32\": ### TODO\n        weight_dtype = torch.float32\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    unet.to(accelerator.device, dtype=weight_dtype) \n    vae.to(accelerator.device, dtype=weight_dtype)\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\n    image_encoder.to(accelerator.device, dtype=weight_dtype) \n    image_proj_model.to(accelerator.device, dtype=weight_dtype) \n\n    # init adapter modules\n    lora_rank = 128\n    attn_procs = {}\n    unet_sd = unet.state_dict()\n\n    for name in unet.attn_processors.keys():\n        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim \n        if name.startswith(\"mid_block\"):\n            hidden_size = unet.config.block_out_channels[-1]\n        elif name.startswith(\"up_blocks\"):\n            block_id = int(name[len(\"up_blocks.\")])\n            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n        elif name.startswith(\"down_blocks\"):\n            block_id = int(name[len(\"down_blocks.\")])\n            hidden_size = unet.config.block_out_channels[block_id]  \n        if cross_attention_dim is None:  \n            attn_procs[name] = Consistent_AttProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)  \n        else: \n            layer_name = name.split(\".processor\")[0] \n            weights = {\n                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"], \n                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"], \n            }\n            attn_procs[name] = Consistent_IPAttProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)  \n            attn_procs[name].load_state_dict(weights, strict=False)\n\n    unet.set_attn_processor(attn_procs) \n    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values()) \n\n    consistentID_model = ConsistentID(unet, image_proj_model, adapter_modules, facial_encoder)\n\n    optimizer_cls = torch.optim.AdamW\n    unet_params = list([p for p in consistentID_model.unet.parameters() if p.requires_grad]) \n    other_params = list( \n        [p for n, p in consistentID_model.named_parameters() if p.requires_grad and \"unet\" not in n]\n    )\n\n    optimizer = optimizer_cls( \n        [\n            {\"params\": unet_params, \"lr\": 1e-4*1.0 },\n            {\"params\": other_params, \"lr\": 1e-4},\n        ],\n        betas=(0.9, 0.999),\n        weight_decay=1e-2,\n        eps=1e-08,\n    )\n\n    # dataloader\n    train_dataset = MyDataset(args.data_json_file, args.data_json_mutiID_file, tokenizer=tokenizer, size=args.resolution, \\\n                              image_root_path=args.data_root_path, faceid_root_path=args.faceid_root_path, parsing_root_path=args.parsing_root_path)\n    \n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset,\n        shuffle=True,\n        collate_fn=collate_fn,\n        batch_size=args.train_batch_size,\n        num_workers=args.dataloader_num_workers,\n    )\n    \n    consistentID_model, optimizer, train_dataloader = accelerator.prepare(consistentID_model, optimizer, train_dataloader)\n\n    for epoch in range(initial_epoch, args.num_train_epochs):\n        begin = time.perf_counter()\n        global_step = 0\n        progress_bar = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{args.num_train_epochs}\", total=len(train_dataloader), disable=not accelerator.is_local_main_process,)\n\n        for step, batch in enumerate(train_dataloader):\n            load_data_time = time.perf_counter() - begin\n            \n            lora_rank = 128\n            attn_procs = {}\n            unet_sd = unet.state_dict()\n            \n            with accelerator.accumulate(consistentID_model):\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"images\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n                    latents = latents * vae.config.scaling_factor\n\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n                timesteps = timesteps.long()\n\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                with torch.no_grad():\n                    clip_images = batch[\"clip_images\"]\n                    image_embeds = image_encoder(clip_images.to(accelerator.device, dtype=weight_dtype), output_hidden_states=True).hidden_states[-2]\n\n                    hidden_states = []\n                    facial_clip_images = batch[\"facial_clip_images\"]                    \n                    for facial_clip_image in facial_clip_images:\n                        hidden_state = image_encoder(facial_clip_image.to(accelerator.device, dtype=weight_dtype), output_hidden_states=True).hidden_states[-2]\n                        hidden_states.append(hidden_state)\n                    multi_facial_embeds = torch.stack(hidden_states)\n\n                with torch.no_grad(): \n                    clean_input_ids = batch[\"clean_input_ids\"]\n                    prompt_embeds = text_encoder(clean_input_ids.to(accelerator.device))[0]\n\n                faceid_embeds = batch[\"face_id_embeds\"].to(accelerator.device, dtype=weight_dtype)\n\n                facial_token_masks = batch[\"facial_token_masks\"]\n                valid_facial_token_idx_mask = batch[\"facial_token_idx_masks\"] \n\n                parsing_mask_lists = batch[\"parsing_mask_lists\"]\n                facial_masks = batch[\"facial_masks\"] \n                facial_token_idxs = batch[\"facial_token_idxs\"]\n                facial_token_idx_masks = batch[\"facial_token_idx_masks\"]\n\n                noise_pred, loss_dict = consistentID_model(noisy_latents, timesteps, prompt_embeds, image_embeds, faceid_embeds, \\\n                            multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask,\n                            noise, parsing_mask_lists, facial_masks, facial_token_idxs, facial_token_idx_masks)\n\n                predict_loss = loss_dict[\"predict_loss\"] \n                facial_loss = loss_dict[\"facial_loss\"]\n\n                background_loss = loss_dict[\"background_loss\"]\n\n                loss = predict_loss + facial_loss\n                \n                # Gather the losses across all processes for logging (if we use distributed training).\n                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()  \n                      \n                # Backpropagate\n                accelerator.backward(loss)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if accelerator.is_main_process:\n                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}, predict_loss: {}, facial_loss: {}, background_loss: {}\".format(\n                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss, predict_loss, facial_loss, background_loss))\n                    \n            global_step += 1 \n            \n            progress_bar.set_description(f\"{exp_name}Epoch {epoch + 1}/{args.num_train_epochs} - Step {step}/{len(train_dataloader)}\")\n            \n            if global_step % args.save_steps == 0:\n                save_path = os.path.join(args.output_dir, f\"{exp_name}_Epoch{epoch+1}-{global_step}\")\n                accelerator.save_state(save_path)\n                \n            begin = time.perf_counter()\n                     \nif __name__ == \"__main__\":\n    main()    \n\n"}
{"type": "source_file", "path": "models/BiSeNet/train.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\nfrom loss import OhemCELoss\nfrom evaluate import evaluate\nfrom optimizer import Optimizer\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = './res'\nif not osp.exists(respth):\n    os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            '--local_rank',\n            dest = 'local_rank',\n            type = int,\n            default = -1,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = 'nccl',\n                init_method = 'tcp://127.0.0.1:33241',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    # dataset\n    n_classes = 19\n    n_img_per_gpu = 16\n    n_workers = 8\n    cropsize = [448, 448]\n    data_root = '/home/zll/data/CelebAMask-HQ/'\n\n    ds = FaceMask(data_root, cropsize=cropsize, mode='train')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    # model\n    ignore_idx = -100\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    net.train()\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n    score_thres = 0.7\n    n_min = n_img_per_gpu * cropsize[0] * cropsize[1]//16\n    LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            model = net.module,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0] == n_img_per_gpu:\n                raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = LossP(out, lb)\n        loss2 = Loss2(out16, lb)\n        loss3 = Loss3(out32, lb)\n        loss = lossp + loss2 + loss3\n        loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n\n        #  print training log message\n        if (it+1) % msg_iter == 0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = ', '.join([\n                    'it: {it}/{max_it}',\n                    'lr: {lr:4f}',\n                    'loss: {loss:.4f}',\n                    'eta: {eta}',\n                    'time: {time:.4f}',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n        if dist.get_rank() == 0:\n            if (it+1) % 5000 == 0:\n                state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n                if dist.get_rank() == 0:\n                    torch.save(state, './res/cp/{}_iter.pth'.format(it))\n                evaluate(dspth='/home/zll/data/CelebAMask-HQ/test-img', cp='{}_iter.pth'.format(it))\n\n    #  dump the final model\n    save_pth = osp.join(respth, 'model_final_diss.pth')\n    # net.cpu()\n    state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n    if dist.get_rank() == 0:\n        torch.save(state, save_pth)\n    logger.info('training done, model saved to: {}'.format(save_pth))\n\n\nif __name__ == \"__main__\":\n    train()\n"}
{"type": "source_file", "path": "utils_SDXL.py", "content": "import argparse\nimport os\nimport json\nimport random\nimport torch\nfrom PIL import Image\nfrom transformers import CLIPImageProcessor\nfrom torchvision import transforms\nfrom functions_SDXL import crop_image, extract_first_sentence, process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nimport numpy as np\nimport pdb\n\n# Dataset\nclass MyDataset(torch.utils.data.Dataset):\n\n    def __init__(self, json_file, json_mutiID_file, tokenizer, tokenizer_2, size=1024, center_crop=True, t_drop_rate=0.05, i_drop_rate=0.05, ti_drop_rate=0.05, \\\n                image_root_path=\"\", faceid_root_path=\"\", parsing_root_path=\"\", image_token=\"<|image|>\", facial_token=\"<|facial|>\",\n                image_token_face=\"<|face|>\",image_token_ear=\"<|ear|>\",image_token_nose=\"<|nose|>\",image_token_eye=\"<|eye|>\",image_token_mouth=\"<|mouth|>\"):\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.tokenizer_2 = tokenizer_2\n        self.size = size\n        self.center_crop = center_crop\n        self.i_drop_rate = i_drop_rate\n        self.t_drop_rate = t_drop_rate\n        self.ti_drop_rate = ti_drop_rate\n\n        self.image_root_path = image_root_path\n        self.faceid_root_path = faceid_root_path\n        self.parsing_root_path = parsing_root_path\n\n        self.data = json.load(open(json_file))\n                                   \n        self.transform = transforms.Compose([\n            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(self.size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ])\n        self.transform_mask = transforms.Compose([\n            transforms.CenterCrop(self.size),\n            transforms.ToTensor(),\n        ])        \n        self.clip_image_processor = CLIPImageProcessor()\n        self.image_token = image_token\n        tokenizer.add_tokens([image_token], special_tokens=True)\n        self.image_token_id = tokenizer.convert_tokens_to_ids(image_token)\n        self.facial_token = facial_token\n        tokenizer.add_tokens([facial_token], special_tokens=True)\n        self.facial_token_id = tokenizer.convert_tokens_to_ids(facial_token)\n        self.max_num_facials = 5 \n        json_name = json_file.split(\"/\")[-2]\n        self.text_path = f\"/mnt/data/sysu/Users/huangjiehui/projects/ConsistentID/SDXL_train/results/logs/train_log_{json_name}.txt\"\n        with open(self.text_path, 'w') as f:\n            f.write(f'Training following data: {json_file}\\n')\n\n        \n    def __getitem__(self, idx):\n\n        item = self.data[idx]\n\n        try:\n            text_origin = item[\"vqa_llva_more_face_detail\"]\n            image_file = item[\"origin_IMG\"]\n            parsing_mask = item[\"parsing_mask_IMG\"]\n\n            image_raw_mask = Image.open(os.path.join(self.parsing_root_path, parsing_mask)) \n            parsing_mask_list = masks_for_unique_values(image_raw_mask) \n            if \"id_embed_file_origin\" in item:\n                faceid_file = item[\"id_embed_file_origin\"]\n            else:\n                faceid_file = None\n                \n            # level 2\n            raw_image = Image.open(os.path.join(self.image_root_path, image_file)) \n            # level 1\n            if faceid_file==None or faceid_file==\"\":\n                face_id_embed = torch.zeros_like(torch.empty((1, 512)))\n                with open(self.text_path, 'a') as f:\n                    f.write(str(item[\"resize_IMG\"]) + '\\n')\n                    f.write(\"This IMG do not have faceid\\n\")         \n            else:\n                face_id_embed = torch.load(os.path.join(self.faceid_root_path, faceid_file))\n        except Exception as e:\n            image_file = \"00000/49250326_resize.png\"\n            raw_image = Image.open(\"./49250326_resize.png\") \n            item[\"vqa_llva\"] = \"The image features a woman sitting in a chair, wearing a black dress and a green top. She is holding a microphone and appears to be smiling. The woman is the main subject of the image, and her facial expression and attire suggest that she might be a performer or an interviewer.\"\n            text_origin = \"The woman in the image has a beautiful face with a prominent nose, large eyes, and a small mouth. She has long, curly hair that falls down her back. Her ears are positioned on the sides of her head, and she is wearing a black shirt.\"\n            face_id_embed = torch.load(\"./49250326_faceid.bin\")\n            image_raw_mask = Image.open(\"./49250326_mask.png\") \n            parsing_mask_list = masks_for_unique_values(image_raw_mask) \n\n            with open(self.text_path, 'a') as f:\n                error_message = f\"Error: {type(e).__name__}, Message: {str(e)}\"\n                try:\n                    f.write(str(item['origin_IMG']) + '\\n')\n                    f.write(f\"{error_message}\\n\")\n                except:\n                    f.write(f\"{error_message}\\n\")\n\n        # original size\n        original_width, original_height = raw_image.size\n        original_size = torch.tensor([original_height, original_width])\n\n        # random crop\n        image_tensor = self.transform(raw_image.convert(\"RGB\"))            \n        delta_h = image_tensor.shape[1] - self.size\n        delta_w = image_tensor.shape[2] - self.size\n        assert not all([delta_h, delta_w])\n        \n        if self.center_crop:\n            top = delta_h // 2\n            left = delta_w // 2\n        else:\n            top = np.random.randint(0, delta_h + 1)\n            left = np.random.randint(0, delta_w + 1)\n        image = transforms.functional.crop(\n            image_tensor, top=top, left=left, height=self.size, width=self.size\n        )\n        crop_coords_top_left = torch.tensor([top, left]) \n        raw_image = raw_image.resize((self.size, self.size))\n\n        key_parsing_mask_list = {}\n        key_list = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n        processed_keys = set()\n        for key, mask_image in parsing_mask_list.items():\n            if key in key_list:\n                if \"_\" in key:\n                    prefix = key.split(\"_\")[1]\n                    if prefix in processed_keys:                   \n                        continue\n                    else:         \n                        key_parsing_mask_list[key] = mask_image \n                        processed_keys.add(prefix)               \n                key_parsing_mask_list[key] = mask_image            \n\n        clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n        try:\n            body_raw_image = fetch_mask_raw_image(raw_image,parsing_mask_list[\"WithoutBackground\"])\n        except:\n            body_raw_image = raw_image\n            with open(self.text_path, 'a') as f:\n                f.write(str(image_file) + '\\n')             \n            print(f\"Fail to extract body part name is:{image_file}!\")\n\n        body_image = self.transform(body_raw_image.convert(\"RGB\"))\n        body_clip_image = self.clip_image_processor(images=body_raw_image, return_tensors=\"pt\").pixel_values \n        multi_image = torch.cat([image, body_image], dim=0)\n        multi_clip_image = torch.cat([clip_image, body_clip_image], dim=1)\n\n        max_text_length=340\n        text_face, key_parsing_mask_list = process_text_with_markers(text_origin, key_parsing_mask_list)\n        text = \"Caption:\" + extract_first_sentence(item[\"vqa_llva\"]) + \" Detail:\" + text_face + item[\"vqa_llva\"][len(extract_first_sentence(item[\"vqa_llva\"])):-1]\n\n        if len(self.tokenizer(text,max_length=self.tokenizer.model_max_length, padding=\"max_length\",truncation=False,return_tensors=\"pt\").input_ids[0])!=77:\n            text = \"Detail:\" + text_face + \" Caption:\" + item[\"vqa_llva\"]\n\n        if len(text_face)>max_text_length:\n            text = item[\"vqa_llva\"]\n            text_path = \"/mnt/data/sysu/Users/huangjiehui/projects/ConsistentID/SDXL_train/results/logs/train_mask_log_v2.txt\"\n            with open(text_path, 'a') as f:\n                f.write(str(text_face) + '\\n')                       \n\n        drop_image_embed = 0\n\n        prob = random.random()\n        if prob < 0.1:\n            text = \"\"\n            multi_clip_image=torch.zeros_like(multi_clip_image)  \n            clip_image=torch.zeros_like(clip_image) \n        elif prob < 0.1 + 0:\n            multi_clip_image=torch.zeros_like(multi_clip_image)  \n            clip_image=torch.zeros_like(clip_image)\n        else:\n            pass\n\n        text_input_id_all = self.tokenizer(\n            text.replace(\"<|facial|>\",\"\"),\n            max_length=self.tokenizer.model_max_length, # 77\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).input_ids\n\n        text_input_id_all2 = self.tokenizer_2(\n            text.replace(\"<|facial|>\",\"\"),\n            max_length=self.tokenizer_2.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).input_ids\n\n        # 4.2 ================================= image_token_idx image_token_idx_mask =================================\n        clean_input_id, image_token_mask, facial_token_mask = tokenize_and_mask_noun_phrases_ends( # input_idspaddingtest_caption ---- image_token_mask  \n            text, self.image_token_id, self.facial_token_id, self.tokenizer\n        )\n\n        max_num_objects=2\n        max_num_facials=5\n        image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask = prepare_image_token_idx(\n            image_token_mask, facial_token_mask, max_num_objects, max_num_facials # self.max_num_facials\n        )\n        \n        facial_mask = []\n        facial_clip_image = []\n        num_facial_part = len(key_parsing_mask_list)\n        for key in key_parsing_mask_list:\n            key_mask=key_parsing_mask_list[key]\n            facial_mask.append(self.transform_mask(key_mask))\n            key_mask_raw_image = fetch_mask_raw_image(raw_image,key_mask)\n           \n            parsing_clip_image = self.clip_image_processor(images=key_mask_raw_image, return_tensors=\"pt\").pixel_values\n            facial_clip_image.append(parsing_clip_image)            \n\n        padding_ficial_clip_image = torch.zeros_like(torch.zeros([1, 3, 224, 224]))\n        padding_ficial_mask = torch.zeros_like(torch.zeros([1, self.size, self.size]))\n        \n        if num_facial_part < self.max_num_facials:\n            facial_clip_image += [torch.zeros_like(padding_ficial_clip_image) for _ in range(self.max_num_facials - num_facial_part) ]\n            facial_mask += [ torch.zeros_like(padding_ficial_mask) for _ in range(self.max_num_facials - num_facial_part)]\n\n        facial_clip_image = torch.stack(facial_clip_image, dim=1).squeeze(0)\n        facial_mask = torch.stack(facial_mask, dim=0).squeeze(dim=1)\n\n        return {\n            \"image\": image,\n            \"multi_image\": multi_image,\n            \"facial_clip_image\": facial_clip_image,\n            \"facial_mask\": facial_mask,\n            \"clean_input_id\": clean_input_id,\n            \"text_input_id_all\": text_input_id_all,\n            \"text_input_id_all2\": text_input_id_all2,\n            \"image_token_idx\": image_token_idx,\n            \"image_token_idx_mask\": image_token_idx_mask,\n            \"image_token_mask\": image_token_mask,\n            \"facial_token_idx\": facial_token_idx, \n            \"facial_token_idx_mask\": facial_token_idx_mask,\n            \"facial_token_mask\": facial_token_mask,\n            \"original_size\": original_size,\n            \"crop_coords_top_left\": crop_coords_top_left,\n            \"target_size\": torch.tensor([self.size, self.size]),\n            \"text_prompt\": text,\n            \"text_origin\": text_origin,\n            \"clip_image\": clip_image,\n            \"multi_clip_image\": multi_clip_image,\n            \"face_id_embed\": face_id_embed,\n            \"drop_image_embed\": drop_image_embed,\n            \"parsing_mask_list\": parsing_mask_list,\n            \"key_parsing_mask_list\": key_parsing_mask_list\n        }\n\n    def __len__(self):\n        return len(self.data)\n\ndef collate_fn(data):\n\n    images = torch.stack([example[\"image\"] for example in data])\n    multi_images = torch.stack([example[\"multi_image\"] for example in data])\n    facial_clip_images = torch.stack([example[\"facial_clip_image\"] for example in data])\n    facial_masks = torch.stack([example[\"facial_mask\"] for example in data])\n\n    clean_input_ids = torch.cat([example[\"clean_input_id\"] for example in data], dim=0)\n    text_input_id_alls = torch.cat([example[\"text_input_id_all\"] for example in data], dim=0)\n    text_input_id_all2s = torch.cat([example[\"text_input_id_all2\"] for example in data], dim=0)\n    image_token_idxs = torch.cat([example[\"image_token_idx\"] for example in data], dim=0) \n    image_token_idx_masks = torch.cat([example[\"image_token_idx_mask\"] for example in data], dim=0)  \n    image_token_masks = torch.cat([example[\"image_token_mask\"] for example in data], dim=0)\n    facial_token_masks = torch.cat([example[\"facial_token_mask\"] for example in data], dim=0)\n    facial_token_idxs = torch.cat([example[\"facial_token_idx\"] for example in data], dim=0)\n    facial_token_idx_masks = torch.cat([example[\"facial_token_idx_mask\"] for example in data], dim=0)   \n\n    original_sizes = torch.stack([example[\"original_size\"] for example in data])\n    crop_coords_top_lefts = torch.stack([example[\"crop_coords_top_left\"] for example in data])\n    target_sizes = torch.stack([example[\"target_size\"] for example in data])\n    text_prompts = [example[\"text_prompt\"] for example in data]\n    text_origins = [example[\"text_origin\"] for example in data]\n    clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n    multi_clip_images = torch.cat([example[\"multi_clip_image\"] for example in data], dim=0) \n    face_id_embeds = torch.stack([example[\"face_id_embed\"] for example in data])\n    drop_image_embeds = [example[\"drop_image_embed\"] for example in data]\n    parsing_mask_lists = [example[\"parsing_mask_list\"] for example in data]\n    key_parsing_mask_lists = [example[\"key_parsing_mask_list\"] for example in data]\n\n    return {\n        \"images\": images,\n        \"multi_images\": multi_images,\n        \"facial_clip_images\": facial_clip_images,\n        \"facial_masks\": facial_masks,\n        \"clean_input_ids\": clean_input_ids,\n        \"text_input_id_alls\": text_input_id_alls,\n        \"text_input_id_all2s\": text_input_id_all2s,\n        \"image_token_idxs\": image_token_idxs, \n        \"image_token_idx_masks\": image_token_idx_masks,  \n        \"image_token_masks\": image_token_masks,\n        \"facial_token_masks\": facial_token_masks,\n        \"facial_token_idxs\": facial_token_idxs, \n        \"facial_token_idx_masks\": facial_token_idx_masks,  \n        \"original_sizes\": original_sizes,\n        \"crop_coords_top_lefts\": crop_coords_top_lefts,\n        \"target_sizes\": target_sizes,\n        \"text_prompts\": text_prompts,\n        \"text_origins\": text_origins, \n        \"clip_images\": clip_images,\n        \"multi_clip_images\": multi_clip_images,\n        \"face_id_embeds\": face_id_embeds,   \n        \"drop_image_embeds\": drop_image_embeds,\n        \"parsing_mask_lists\": parsing_mask_lists,\n        \"key_parsing_mask_lists\": key_parsing_mask_lists\n    }\n    \n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=\"./stable-diffusion-xl-base-1.0\",\n        required=False,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--num_tokens\",\n        type=int,\n        default=4,\n        help=\"Number of tokens to query from the CLIP image encoding.\",\n    )\n    parser.add_argument(\n        \"--data_json_file\",\n        type=str,\n        default=\"./FGID/JSON_train.json\",\n        required=False,\n        help=\"Training data\",\n    )\n    parser.add_argument(\n        \"--data_json_mutiID_file\",\n        type=str,\n        default=\"./FGID/MultiID/JSON_mutiID_all.json\",\n        required=False,\n        help=\"Training data\",\n    )        \n    parser.add_argument(\n        \"--data_root_path\",\n        type=str,\n        default=\"./FGID/ffhq_celebA/origin_IMG\",\n        required=False,\n        help=\"Training data root path\",\n    )\n    parser.add_argument(\n        \"--faceid_root_path\",\n        type=str,\n        default=\"./FGID/ffhq_celebA/all_faceID\",\n        required=False,\n        help=\"Training data root path\",\n    )\n    parser.add_argument(\n        \"--parsing_root_path\",\n        type=str,\n        default=\"./FGID/ffhq_celebA/parsing_mask_IMG\",\n        required=False,\n        help=\"Training data root path\",\n    )        \n    parser.add_argument(\n        \"--image_encoder_path\",\n        type=str,\n        default=\"/mnt/data/sysu/Users/huangjiehui/pretrained_model/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n        required=False,\n        help=\"Path to CLIP image encoder\",\n    )\n    parser.add_argument(\n        \"--image_encoder2_path\",\n        type=str,\n        default=\"/mnt/data/sysu/Users/huangjiehui/pretrained_model/clip-vit-large-patch14\",\n        required=False,\n        help=\"Path to CLIP image encoder\",\n    )    \n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"/mnt/data/sysu/Users/huangjiehui/projects/ConsistentID/SDXL_train/results/outputs/\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--logging_dir\",\n        type=str,\n        default=\"/mnt/data/sysu/Users/huangjiehui/projects/ConsistentID/SDXL_train/results/logs\",\n        help=(\n            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n        ),\n    )\n    parser.add_argument(\n        \"--mask_loss_prob\",\n        type=float,\n        default=0.5,\n    )       \n    parser.add_argument(\n        \"--facial_weight\", # facial_weight attn_reg_weight\n        type=float,\n        default=0.01,\n    )           \n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=1024,\n        help=(\n            \"The resolution for input images\"\n        ),\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Learning rate to use.\",\n    )\n    parser.add_argument(\"--localization_layers\", type=int, default=5) # Add from fastcomposer\n    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\n        \"--dataloader_num_workers\",\n        type=int,\n        default=0,\n        help=(\n            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n        ),\n    )\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=100, # 2000\n        help=(\n            \"Save a checkpoint of the training state every X updates\"  \n        ),\n    )\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=None,\n        choices=[\"no\", \"fp8\", \"fp16\", \"bf16\", \"fp32\"],\n        help=(\n            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n        ),\n    )\n    parser.add_argument(\n        \"--report_to\",\n        type=str,\n        default=\"tensorboard\",\n        help=(\n            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n    \n    parser.add_argument(\n        \"--train_text_encoder\",\n        type=bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--train_image_encoder\",\n        type=bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--image_encoder_trainable_layers\",\n        type=int,\n        default=2,\n    )\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    return args\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "pipelines/StableDIffusionInpaint_ConsistentID.py", "content": "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport cv2\nimport PIL\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom insightface.app import FaceAnalysis\nfrom safetensors import safe_open\nfrom huggingface_hub.utils import validate_hf_hub_args\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom diffusers import StableDiffusionInpaintPipeline\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\nfrom diffusers.utils import _get_model_file\nfrom diffusers.utils.torch_utils import is_compiled_module, randn_tensor\nfrom diffusers.utils import (\n    USE_PEFT_BACKEND,\n    deprecate,\n    logging,\n    replace_example_docstring,\n    scale_lora_layers,\n    unscale_lora_layers,\n)\nfrom diffusers.pipelines.controlnet.multicontrolnet import MultiControlNetModel, ControlNetModel\nfrom functions import process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nfrom functions import ProjPlusModel, masks_for_unique_values\nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor, FacialEncoder\nfrom .BaseConsistentID import BaseConsistentIDPipeline\nfrom models.BiSeNet.model import BiSeNet\n\nPipelineImageInput = Union[\n    PIL.Image.Image,\n    torch.FloatTensor,\n    List[PIL.Image.Image],\n    List[torch.FloatTensor],\n]\n\nEXAMPLE_DOC_STRING = \"\"\"\n    Examples:\n        ```py\n        >>> # !pip install transformers accelerate\n        >>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\n        >>> from diffusers.utils import load_image\n        >>> import numpy as np\n        >>> import torch\n\n        >>> init_image = load_image(\n        ...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png\"\n        ... )\n        >>> init_image = init_image.resize((512, 512))\n\n        >>> generator = torch.Generator(device=\"cpu\").manual_seed(1)\n\n        >>> mask_image = load_image(\n        ...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png\"\n        ... )\n        >>> mask_image = mask_image.resize((512, 512))\n\n\n        >>> def make_canny_condition(image):\n        ...     image = np.array(image)\n        ...     image = cv2.Canny(image, 100, 200)\n        ...     image = image[:, :, None]\n        ...     image = np.concatenate([image, image, image], axis=2)\n        ...     image = Image.fromarray(image)\n        ...     return image\n\n\n        >>> control_image = make_canny_condition(init_image)\n\n        >>> controlnet = ControlNetModel.from_pretrained(\n        ...     \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16\n        ... )\n        >>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n        ...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n        ... )\n\n        >>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        >>> pipe.enable_model_cpu_offload()\n\n        >>> # generate image\n        >>> image = pipe(\n        ...     \"a handsome man with ray-ban sunglasses\",\n        ...     num_inference_steps=20,\n        ...     generator=generator,\n        ...     eta=1.0,\n        ...     image=init_image,\n        ...     mask_image=mask_image,\n        ...     control_image=control_image,\n        ... ).images[0]\n        ```\n\"\"\"\n\nclass StableDiffusionInpaintConsistentIDPipeline(StableDiffusionInpaintPipeline, BaseConsistentIDPipeline):\n\n    @torch.no_grad()\n    # @replace_example_docstring(EXAMPLE_DOC_STRING)\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        image: PipelineImageInput = None,\n        mask_image: PipelineImageInput = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        strength: float = 1.0,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        original_size: Optional[Tuple[int, int]] = None,\n        target_size: Optional[Tuple[int, int]] = None,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        input_id_images: PipelineImageInput = None,\n        start_merge_step: int = 0,\n        prompt_embeds_text_only: Optional[torch.FloatTensor] = None,\n    ):\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n        \n        # 1. Check inputs\n        self.check_inputs(\n            prompt=prompt,\n            height=height,\n            width=width,\n            strength=strength,\n            callback_steps=callback_steps,\n            negative_prompt=negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n        )\n        if not isinstance(input_id_images, list):\n            input_id_images = [input_id_images]\n        \n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n        do_classifier_free_guidance = guidance_scale >= 1.0\n        input_image_file = input_id_images[0]\n\n        faceid_embeds = self.get_prepare_faceid(face_image=input_image_file)\n        face_caption = self.get_prepare_llva_caption(input_image_file)\n        key_parsing_mask_list, vis_parsing_anno_color = self.get_prepare_facemask(input_image_file)\n\n        assert do_classifier_free_guidance\n\n        # 3. Encode input prompt\n        num_id_images = len(input_id_images)\n\n        (\n            prompt_text_only,\n            clean_input_id,\n            key_parsing_mask_list_align,\n            facial_token_mask,\n            facial_token_idx,\n            facial_token_idx_mask,\n        ) = self.encode_prompt_with_trigger_word(\n            prompt = prompt,\n            face_caption = face_caption,\n            # prompt_2=None,  \n            key_parsing_mask_list=key_parsing_mask_list,\n            device=device,\n            max_num_facials = 5,\n            num_id_images= num_id_images,\n            # prompt_embeds= None,\n            # pooled_prompt_embeds= None,\n            # class_tokens_mask= None,\n        )\n\n        # 4. Encode input prompt without the trigger word for delayed conditioning\n        encoder_hidden_states = self.text_encoder(clean_input_id.to(device))[0] \n\n        prompt_embeds = self._encode_prompt(\n            prompt_text_only,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            do_classifier_free_guidance=True,\n            negative_prompt=negative_prompt,\n        )\n        negative_encoder_hidden_states_text_only = prompt_embeds[0:num_images_per_prompt]\n        encoder_hidden_states_text_only = prompt_embeds[num_images_per_prompt:]\n\n        # 5. Prepare the input ID images\n        prompt_tokens_faceid, uncond_prompt_tokens_faceid = self.get_image_embeds(faceid_embeds, face_image=input_image_file, s_scale=1.0, shortcut=False)\n\n        facial_clip_image, facial_mask = self.get_prepare_clip_image(input_image_file, key_parsing_mask_list_align, image_size=512, max_num_facials=5)\n        facial_clip_images = facial_clip_image.unsqueeze(0).to(device, dtype=self.torch_dtype)\n        facial_token_mask = facial_token_mask.to(device)\n        facial_token_idx_mask = facial_token_idx_mask.to(device)\n        negative_encoder_hidden_states = negative_encoder_hidden_states_text_only\n\n        cross_attention_kwargs = {}\n\n        # 6. Get the update text embedding\n        prompt_embeds_facial, uncond_prompt_embeds_facial = self.get_facial_embeds(encoder_hidden_states, negative_encoder_hidden_states, \\\n                                                            facial_clip_images, facial_token_mask, facial_token_idx_mask)\n\n        prompt_embeds = torch.cat([prompt_embeds_facial, prompt_tokens_faceid], dim=1)\n        negative_prompt_embeds = torch.cat([uncond_prompt_embeds_facial, uncond_prompt_tokens_faceid], dim=1)\n\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n        )        \n        prompt_embeds_text_only = torch.cat([encoder_hidden_states_text_only, prompt_tokens_faceid], dim=1)\n        prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_text_only], dim=0)\n\n        # 7. Prepare images\n        init_image = self.image_processor.preprocess(\n            input_image_file, height=height, width=width,  # resize_mode='default', crops_coords=None\n        )\n        init_image = init_image.to(dtype=torch.float32)\n\n        mask = self.mask_processor.preprocess(\n            mask_image, height=height, width=width, # resize_mode='default', crops_coords=None\n        )\n\n        masked_image = init_image * (mask < 0.5)\n        _, _, height, width = init_image.shape\n\n        # 8. Pprepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps, num_inference_steps = self.get_timesteps(\n            num_inference_steps=num_inference_steps, strength=strength, device=device\n        )\n        # at which timestep to set the initial noise (n.b. 50% if strength is 0.5)\n        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n        # create a boolean to check if the strength is set to 1. if so then initialise the latents with pure noise\n        is_strength_max = strength == 1.0\n        self._num_timesteps = len(timesteps)\n\n        # 9.Prepare latent variables\n        num_channels_latents = self.vae.config.latent_channels\n        num_channels_unet = self.unet.config.in_channels\n        return_image_latents = num_channels_unet == 4\n        latents_outputs = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n            image=init_image,\n            timestep=latent_timestep,\n            is_strength_max=is_strength_max,\n            return_noise=True,\n            return_image_latents=return_image_latents,\n        )\n\n        if return_image_latents:\n            latents, noise, image_latents = latents_outputs\n        else:\n            latents, noise = latents_outputs\n\n        # 10. Prepare mask latent variables\n        mask, masked_image_latents = self.prepare_mask_latents(\n            mask,\n            masked_image,\n            batch_size * num_images_per_prompt,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            do_classifier_free_guidance,\n        )\n\n        # 11. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n        \n        (\n            null_prompt_embeds,\n            augmented_prompt_embeds,\n            text_prompt_embeds,\n        ) = prompt_embeds.chunk(3)\n\n        # 12. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                if i <= start_merge_step:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, text_prompt_embeds], dim=0\n                    )\n                else:\n                    current_prompt_embeds = torch.cat(\n                        [null_prompt_embeds, augmented_prompt_embeds], dim=0\n                    )\n                \n                # predict the noise residual\n                if num_channels_unet == 9:\n                    latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=current_prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n                else:\n                    assert 0, 'Not Implemented'\n                \n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                if num_channels_unet == 4:\n                    init_latents_proper = image_latents\n                    if do_classifier_free_guidance:\n                        init_mask, _ = mask.chunk(2)\n                    else:\n                        init_mask = mask\n\n                    if i < len(timesteps) - 1:\n                        noise_timestep = timesteps[i + 1]\n                        init_latents_proper = self.scheduler.add_noise(\n                            init_latents_proper, noise, torch.tensor([noise_timestep])\n                        )\n                    latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n                \n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n                        callback(step_idx, t, latents)\n\n        # If we do sequential model offloading, let's offload unet and controlnet\n        # manually for max memory savings\n        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n            self.unet.to(\"cpu\")\n            torch.cuda.empty_cache()\n\n        if not output_type == \"latent\":\n            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[\n                0\n            ]\n            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n        else:\n            image = latents\n            has_nsfw_concept = None\n\n        if has_nsfw_concept is None:\n            do_denormalize = [True] * image.shape[0]\n        else:\n            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n\n        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n"}
{"type": "source_file", "path": "utils.py", "content": "import argparse\nimport os\nimport json\nimport random\nimport torch\nfrom PIL import Image\nfrom transformers import CLIPImageProcessor\nfrom torchvision import transforms\nfrom functions import extract_first_sentence, process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\n\n# Dataset\nclass MyDataset(torch.utils.data.Dataset):\n\n    def __init__(self, json_file, json_mutiID_file, tokenizer, size=512, t_drop_rate=0.05, i_drop_rate=0.05, ti_drop_rate=0.05, \\\n                image_root_path=\"\", faceid_root_path=\"\", parsing_root_path=\"\", image_token=\"<|image|>\", facial_token=\"<|facial|>\",):\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.size = size\n        self.i_drop_rate = i_drop_rate\n        self.t_drop_rate = t_drop_rate\n        self.ti_drop_rate = ti_drop_rate\n\n        self.image_root_path = image_root_path\n        self.faceid_root_path = faceid_root_path\n        self.parsing_root_path = parsing_root_path\n\n        self.data = json.load(open(json_file))\n                                   \n        self.transform = transforms.Compose([\n            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(self.size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ])\n        self.transform_mask = transforms.Compose([\n            transforms.CenterCrop(self.size),\n            transforms.ToTensor(),\n        ])        \n        self.clip_image_processor = CLIPImageProcessor()\n\n        self.image_token = image_token\n        tokenizer.add_tokens([image_token], special_tokens=True)\n        self.image_token_id = tokenizer.convert_tokens_to_ids(image_token)\n\n        self.facial_token = facial_token\n        tokenizer.add_tokens([facial_token], special_tokens=True)\n        self.facial_token_id = tokenizer.convert_tokens_to_ids(facial_token)\n        self.max_num_facials = 5\n        \n    def __getitem__(self, idx):\n\n        item = self.data[idx]\n        text_origin = item[\"vqa_llva_more_face_detail\"]\n        image_file = item[\"resize_IMG\"]\n        parsing_mask = item[\"parsing_mask_IMG\"]\n\n        image_raw_mask = Image.open(os.path.join(self.parsing_root_path, parsing_mask)) \n        parsing_mask_list = masks_for_unique_values(image_raw_mask) \n        if \"id_embed_file_origin\" in item:\n            faceid_file = item[\"id_embed_file_origin\"]\n        elif \"id_embed_file_resize\" in item:  \n            faceid_file = item[\"id_embed_file_resize\"]\n        else:\n            faceid_file = None  \n\n        raw_image = Image.open(os.path.join(self.image_root_path, image_file)) \n\n        if faceid_file is None:\n            face_id_embed = torch.zeros_like(torch.empty((1, 512)))\n        else:\n            face_id_embed = torch.load(os.path.join(self.faceid_root_path, faceid_file))\n\n        key_parsing_mask_list = {}\n        key_list = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n        processed_keys = set()  \n        for key, mask_image in parsing_mask_list.items():\n            if key in key_list:\n                if \"_\" in key:  \n                    prefix = key.split(\"_\")[1]\n                    if prefix in processed_keys:                   \n                        continue\n                    else:              \n                        key_parsing_mask_list[key] = mask_image \n                        processed_keys.add(prefix)          \n                key_parsing_mask_list[key] = mask_image            \n\n        image = self.transform(raw_image.convert(\"RGB\"))\n        clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n        \n        body_raw_image = fetch_mask_raw_image(raw_image,parsing_mask_list[\"WithoutBackground\"])\n        body_image = self.transform(body_raw_image.convert(\"RGB\"))\n        body_clip_image = self.clip_image_processor(images=body_raw_image, return_tensors=\"pt\").pixel_values \n        multi_image = torch.cat([image, body_image], dim=0)\n        multi_clip_image = torch.cat([clip_image, body_clip_image], dim=1) ### TODO multiID\n\n        text_face, key_parsing_mask_list = process_text_with_markers(text_origin, key_parsing_mask_list)\n        text = \"Caption:\" + extract_first_sentence(item[\"vqa_llva\"]) + \" Detail:\" + text_face + item[\"vqa_llva\"][len(extract_first_sentence(item[\"vqa_llva\"])):-1]\n        if len(self.tokenizer(text,max_length=self.tokenizer.model_max_length, padding=\"max_length\",truncation=False,return_tensors=\"pt\").input_ids[0])!=77:\n            text = \"Detail:\" + text_face + \" Caption:\" + item[\"vqa_llva\"]\n\n        max_text_length = 340 # Prevent Facial text from being too long and causing errors.\n        if len(text_face)>max_text_length:\n            text = item[\"vqa_llva\"]\n            text_path = \"./long_caption_log.txt\"\n            with open(text_path, 'a') as f:\n                f.write(str(text_face) + '\\n') # A small portion of the text descriptions obtained through llava are too long. \n        \n        drop_image_embed = 0\n\n        prob = random.random()\n        if prob < 0.1:\n            text = \"\"\n            multi_clip_image=torch.zeros_like(multi_clip_image)  \n            clip_image=torch.zeros_like(clip_image) \n        elif prob < 0.1 + 0:\n            multi_clip_image=torch.zeros_like(multi_clip_image)  \n            clip_image=torch.zeros_like(clip_image)\n        else:\n            pass\n\n        text_input_id_all = self.tokenizer(\n            text.replace(\"<|facial|>\",\"\"),\n            max_length=self.tokenizer.model_max_length, # 77\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).input_ids\n\n        clean_input_id, image_token_mask, facial_token_mask = tokenize_and_mask_noun_phrases_ends(\n            text, self.image_token_id, self.facial_token_id, self.tokenizer\n        )\n\n        max_num_objects=2\n        max_num_facials=5\n        image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask = prepare_image_token_idx(\n            image_token_mask, facial_token_mask, max_num_objects, max_num_facials\n        )\n\n        facial_mask = []\n        facial_clip_image = []\n        num_facial_part = len(key_parsing_mask_list)\n        for key in key_parsing_mask_list:\n            key_mask=key_parsing_mask_list[key]\n            facial_mask.append(self.transform_mask(key_mask))\n            key_mask_raw_image = fetch_mask_raw_image(raw_image,key_mask)\n           \n            parsing_clip_image = self.clip_image_processor(images=key_mask_raw_image, return_tensors=\"pt\").pixel_values\n            facial_clip_image.append(parsing_clip_image)        \n\n        padding_ficial_clip_image = torch.zeros_like(torch.zeros([1, 3, 224, 224]))\n        padding_ficial_mask = torch.zeros_like(torch.zeros([1, self.size, self.size]))\n        \n        if num_facial_part < self.max_num_facials:\n            facial_clip_image += [torch.zeros_like(padding_ficial_clip_image) for _ in range(self.max_num_facials - num_facial_part) ]\n            facial_mask += [ torch.zeros_like(padding_ficial_mask) for _ in range(self.max_num_facials - num_facial_part)]\n\n        facial_clip_image = torch.stack(facial_clip_image, dim=1).squeeze(0)\n        facial_mask = torch.stack(facial_mask, dim=0).squeeze(dim=1)\n\n        return {\n            \"image\": image,\n            \"multi_image\": multi_image,\n            \"facial_clip_image\": facial_clip_image,\n            \"facial_mask\": facial_mask,\n            \"clean_input_id\": clean_input_id,\n            \"text_input_id_all\": text_input_id_all,   \n            \"facial_token_idx\": facial_token_idx, \n            \"facial_token_idx_mask\": facial_token_idx_mask,\n            \"facial_token_mask\": facial_token_mask,\n            \"text_prompt\": text,\n            \"clip_image\": clip_image,\n            \"multi_clip_image\": multi_clip_image,\n            \"face_id_embed\": face_id_embed,\n            \"drop_image_embed\": drop_image_embed,\n            \"parsing_mask_list\": parsing_mask_list,\n            \"key_parsing_mask_list\": key_parsing_mask_list\n        }\n\n    def __len__(self):\n        return len(self.data)\n\ndef collate_fn(data):\n    images = torch.stack([example[\"image\"] for example in data])\n    multi_images = torch.stack([example[\"multi_image\"] for example in data])\n    facial_clip_images = torch.stack([example[\"facial_clip_image\"] for example in data])\n    facial_masks = torch.stack([example[\"facial_mask\"] for example in data])\n    clean_input_ids = torch.cat([example[\"clean_input_id\"] for example in data], dim=0)\n    text_input_id_alls = torch.cat([example[\"text_input_id_all\"] for example in data], dim=0)\n    facial_token_masks = torch.cat([example[\"facial_token_mask\"] for example in data], dim=0)\n    facial_token_idxs = torch.cat([example[\"facial_token_idx\"] for example in data], dim=0)\n    facial_token_idx_masks = torch.cat([example[\"facial_token_idx_mask\"] for example in data], dim=0)   \n    text_prompts = [example[\"text_prompt\"] for example in data]\n    clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n    multi_clip_images = torch.cat([example[\"multi_clip_image\"] for example in data], dim=0) \n    face_id_embeds = torch.stack([example[\"face_id_embed\"] for example in data])\n    drop_image_embeds = [example[\"drop_image_embed\"] for example in data]\n    parsing_mask_lists = [example[\"parsing_mask_list\"] for example in data]\n    key_parsing_mask_lists = [example[\"key_parsing_mask_list\"] for example in data]\n\n    return {\n        \"images\": images,\n        \"multi_images\": multi_images,\n        \"facial_clip_images\": facial_clip_images,\n        \"facial_masks\": facial_masks,\n        \"clean_input_ids\": clean_input_ids,\n        \"text_input_id_alls\": text_input_id_alls,\n        \"facial_token_masks\": facial_token_masks,\n        \"facial_token_idxs\": facial_token_idxs, \n        \"facial_token_idx_masks\": facial_token_idx_masks,  \n        \"text_prompts\": text_prompts,\n        \"clip_images\": clip_images,\n        \"multi_clip_images\": multi_clip_images,\n        \"face_id_embeds\": face_id_embeds,   \n        \"drop_image_embeds\": drop_image_embeds,\n        \"parsing_mask_lists\": parsing_mask_lists,\n        \"key_parsing_mask_lists\": key_parsing_mask_lists\n    }\n    \ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=\"runwayml/stable-diffusion-v1-5\", \n        required=False,\n    )\n    parser.add_argument(\n        \"--num_tokens\",\n        type=int,\n        default=4,\n    )\n    parser.add_argument(\n        \"--data_json_file\",\n        type=str,\n        default=\"\",\n        required=False,\n    )\n    parser.add_argument(\n        \"--data_json_mutiID_file\",\n        type=str,\n        default=\"\",\n        required=False,\n    )    \n    parser.add_argument(\n        \"--image_encoder_path\",\n        type=str,\n        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n        required=False,\n        help=\"Path to CLIP image encoder\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"./ConsistentID\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--logging_dir\",\n        type=str,\n        default=\"./ConsistentID\",\n    )\n    parser.add_argument(\n        \"--mask_loss_prob\",\n        type=float,\n        default=0.5,\n    )       \n    parser.add_argument(\n        \"--facial_weight\",\n        type=float,\n        default=0.01,\n    )           \n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images\"\n        ),\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Learning rate to use.\",\n    )\n    parser.add_argument(\"--localization_layers\", type=int, default=5)\n    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\n        \"--dataloader_num_workers\",\n        type=int,\n        default=0,\n    )\n    parser.add_argument(\n        \"--save_steps\",\n        type=int,\n        default=100,\n    )\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=None,\n        choices=[\"no\", \"fp16\", \"bf16\"],\n    )\n    parser.add_argument(\n        \"--report_to\",\n        type=str,\n        default=\"tensorboard\",\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n    \n    parser.add_argument(\n        \"--train_text_encoder\",\n        type=bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--train_image_encoder\",\n        type=bool,\n        default=True,\n    )\n    parser.add_argument(\n        \"--image_encoder_trainable_layers\",\n        type=int,\n        default=2,\n    )\n    parser.add_argument(\n        \"--data_root_path\",\n        type=str,\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--faceid_root_path\",\n        type=str,\n        default=\"\",\n    )\n    parser.add_argument(\n        \"--parsing_root_path\",\n        type=str,\n        default=\"\",\n    )\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    return args\n\n"}
{"type": "source_file", "path": "models/BiSeNet/transform.py", "content": "#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nfrom PIL import Image\nimport PIL.ImageEnhance as ImageEnhance\nimport random\nimport numpy as np\n\nclass RandomCrop(object):\n    def __init__(self, size, *args, **kwargs):\n        self.size = size\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        assert im.size == lb.size\n        W, H = self.size\n        w, h = im.size\n\n        if (W, H) == (w, h): return dict(im=im, lb=lb)\n        if w < W or h < H:\n            scale = float(W) / w if w < h else float(H) / h\n            w, h = int(scale * w + 1), int(scale * h + 1)\n            im = im.resize((w, h), Image.BILINEAR)\n            lb = lb.resize((w, h), Image.NEAREST)\n        sw, sh = random.random() * (w - W), random.random() * (h - H)\n        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n        return dict(\n                im = im.crop(crop),\n                lb = lb.crop(crop)\n                    )\n\n\nclass HorizontalFlip(object):\n    def __init__(self, p=0.5, *args, **kwargs):\n        self.p = p\n\n    def __call__(self, im_lb):\n        if random.random() > self.p:\n            return im_lb\n        else:\n            im = im_lb['im']\n            lb = im_lb['lb']\n\n            # atts = [1 'skin', 2 'l_brow', 3 'r_brow', 4 'l_eye', 5 'r_eye', 6 'eye_g', 7 'l_ear', 8 'r_ear', 9 'ear_r',\n            #         10 'nose', 11 'mouth', 12 'u_lip', 13 'l_lip', 14 'neck', 15 'neck_l', 16 'cloth', 17 'hair', 18 'hat']\n\n            flip_lb = np.array(lb)\n            flip_lb[lb == 2] = 3\n            flip_lb[lb == 3] = 2\n            flip_lb[lb == 4] = 5\n            flip_lb[lb == 5] = 4\n            flip_lb[lb == 7] = 8\n            flip_lb[lb == 8] = 7\n            flip_lb = Image.fromarray(flip_lb)\n            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n                        lb = flip_lb.transpose(Image.FLIP_LEFT_RIGHT),\n                    )\n\n\nclass RandomScale(object):\n    def __init__(self, scales=(1, ), *args, **kwargs):\n        self.scales = scales\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        W, H = im.size\n        scale = random.choice(self.scales)\n        w, h = int(W * scale), int(H * scale)\n        return dict(im = im.resize((w, h), Image.BILINEAR),\n                    lb = lb.resize((w, h), Image.NEAREST),\n                )\n\n\nclass ColorJitter(object):\n    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n        if not brightness is None and brightness>0:\n            self.brightness = [max(1-brightness, 0), 1+brightness]\n        if not contrast is None and contrast>0:\n            self.contrast = [max(1-contrast, 0), 1+contrast]\n        if not saturation is None and saturation>0:\n            self.saturation = [max(1-saturation, 0), 1+saturation]\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n        im = ImageEnhance.Color(im).enhance(r_saturation)\n        return dict(im = im,\n                    lb = lb,\n                )\n\n\nclass MultiScale(object):\n    def __init__(self, scales):\n        self.scales = scales\n\n    def __call__(self, img):\n        W, H = img.size\n        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n        imgs = []\n        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n        return imgs\n\n\nclass Compose(object):\n    def __init__(self, do_list):\n        self.do_list = do_list\n\n    def __call__(self, im_lb):\n        for comp in self.do_list:\n            im_lb = comp(im_lb)\n        return im_lb\n\n\n\n\nif __name__ == '__main__':\n    flip = HorizontalFlip(p = 1)\n    crop = RandomCrop((321, 321))\n    rscales = RandomScale((0.75, 1.0, 1.5, 1.75, 2.0))\n    img = Image.open('data/img.jpg')\n    lb = Image.open('data/label.png')\n"}
{"type": "source_file", "path": "pipline_StableDiffusionXL_ConsistentID.py", "content": "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\nimport cv2\nimport PIL\nimport numpy as np \nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom insightface.app import FaceAnalysis \n### insight-face installation can be found at https://github.com/deepinsight/insightface\nfrom safetensors import safe_open\nfrom huggingface_hub.utils import validate_hf_hub_args\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\nfrom diffusers.utils import _get_model_file\nfrom functions import process_text_with_markers, masks_for_unique_values, fetch_mask_raw_image, tokenize_and_mask_noun_phrases_ends, prepare_image_token_idx\nfrom functions import ProjPlusModel, masks_for_unique_values\nfrom attention import Consistent_IPAttProcessor, Consistent_AttProcessor, FacialEncoder\n### Model can be imported from https://github.com/zllrunning/face-parsing.PyTorch?tab=readme-ov-file\n### We use the ckpt of 79999_iter.pth: https://drive.google.com/open?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812\n### Thanks for the open source of face-parsing model.\nfrom models.BiSeNet.model import BiSeNet # resnet tensorflow\nimport pdb\n######################################\n########## add for sdxl\n######################################\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n######################################\n########## add for llava\n######################################\n# import sys\n# sys.path.append(\"./Llava1.5/LLaVA\")\n# from llava.model.builder import load_pretrained_model\n# from llava.mm_utils import get_model_name_from_path\n# from llava.eval.run_llava import eval_model\n\nPipelineImageInput = Union[\n    PIL.Image.Image,\n    torch.FloatTensor,\n    List[PIL.Image.Image],\n    List[torch.FloatTensor],\n]\n\n\nclass ConsistentIDStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n    \n    @validate_hf_hub_args\n    def load_ConsistentID_model(\n        self,\n        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n        weight_name: str,\n        subfolder: str = '',\n        trigger_word_ID: str = '<|image|>',\n        trigger_word_facial: str = '<|facial|>',\n        image_encoder_path: str = 'laion/CLIP-ViT-H-14-laion2B-s32B-b79K',   # Import CLIP pretrained model\n        bise_net_cp: str = 'JackAILab/ConsistentID/face_parsing.pth',\n        torch_dtype = torch.float16,\n        num_tokens = 4,\n        lora_rank= 128,\n        **kwargs,\n    ):\n        self.lora_rank = lora_rank \n        self.torch_dtype = torch_dtype\n        self.num_tokens = num_tokens\n        self.set_ip_adapter()\n        self.image_encoder_path = image_encoder_path\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=self.torch_dtype\n        )   \n        self.clip_image_processor = CLIPImageProcessor()\n        self.id_image_processor = CLIPImageProcessor()\n        self.crop_size = 512\n\n        # FaceID\n        self.app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider']) ### root=\"/root/.insightface/models/buffalo_l\"\n        self.app.prepare(ctx_id=0, det_size=(512, 512)) ### (640, 640)\n\n        ### BiSeNet\n        self.bise_net = BiSeNet(n_classes = 19)\n        self.bise_net.cuda()\n        self.bise_net_cp= bise_net_cp # Import BiSeNet model\n        self.bise_net.load_state_dict(torch.load(self.bise_net_cp)) # , map_location=\"cpu\"\n        self.bise_net.eval()\n        # Colors for all 20 parts\n        self.part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                    [255, 0, 85], [255, 0, 170],\n                    [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                    [0, 255, 85], [0, 255, 170],\n                    [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                    [0, 85, 255], [0, 170, 255],\n                    [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                    [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                    [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n        \n        ### LLVA Optional\n        self.llva_model_path = \"\" #TODO import llava weights\n        self.llva_prompt = \"Describe this person's facial features for me, including face, ears, eyes, nose, and mouth.\" \n        self.llva_tokenizer, self.llva_model, self.llva_image_processor, self.llva_context_len = None,None,None,None #load_pretrained_model(self.llva_model_path)\n\n        self.FacialEncoder = FacialEncoder(self.image_encoder, embedding_dim=1280, output_dim=2048, embed_dim=2048).to(self.device, dtype=self.torch_dtype)\n\n        # Load the main state dict first.\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", None)\n        token = kwargs.pop(\"token\", None)\n        revision = kwargs.pop(\"revision\", None)\n\n        user_agent = {\n            \"file_type\": \"attn_procs_weights\",\n            \"framework\": \"pytorch\",\n        }\n\n        if not isinstance(pretrained_model_name_or_path_or_dict, dict):\n            model_file = _get_model_file(\n                pretrained_model_name_or_path_or_dict,\n                weights_name=weight_name,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=token,\n                revision=revision,\n                subfolder=subfolder,\n                user_agent=user_agent,\n            )\n            if weight_name.endswith(\".safetensors\"):\n                state_dict = {\"image_proj_model\": {}, \"adapter_modules\": {}, \"FacialEncoder\": {}}\n                with safe_open(model_file, framework=\"pt\", device=\"cpu\") as f:\n                    for key in f.keys():\n                        if key.startswith(\"unet\"):\n                            pass\n                        elif key.startswith(\"image_proj_model\"):\n                            state_dict[\"image_proj_model\"][key.replace(\"image_proj_model.\", \"\")] = f.get_tensor(key)\n                        elif key.startswith(\"adapter_modules\"):   \n                            state_dict[\"adapter_modules\"][key.replace(\"adapter_modules.\", \"\")] = f.get_tensor(key)\n                        elif key.startswith(\"FacialEncoder\"):\n                            state_dict[\"FacialEncoder\"][key.replace(\"FacialEncoder.\", \"\")] = f.get_tensor(key)    \n            else:\n                state_dict = torch.load(model_file, map_location=\"cuda\")\n        else:\n            state_dict = pretrained_model_name_or_path_or_dict\n        \n\n        self.trigger_word_ID = trigger_word_ID\n        self.trigger_word_facial = trigger_word_facial\n\n        self.image_proj_model = ProjPlusModel(\n            cross_attention_dim=self.unet.config.cross_attention_dim, \n            id_embeddings_dim=512,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size, \n            num_tokens=self.num_tokens,  # 4\n        ).to(self.device, dtype=self.torch_dtype)\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj_model\"], strict=True)\n\n        ip_layers = torch.nn.ModuleList(self.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"adapter_modules\"], strict=True)\n        self.FacialEncoder.load_state_dict(state_dict[\"FacialEncoder\"], strict=True)\n        print(f\"Successfully loaded weights from checkpoint\")\n\n        # Add trigger word token\n        if self.tokenizer is not None: \n            self.tokenizer.add_tokens([self.trigger_word_ID], special_tokens=True)\n            self.tokenizer.add_tokens([self.trigger_word_facial], special_tokens=True)\n\n        ######################################\n        ########## add for sdxl\n        ######################################\n        ### (1) load lora into models\n        # print(f\"Loading ConsistentID components lora_weights from [{pretrained_model_name_or_path_or_dict}]\")\n        # self.load_lora_weights(state_dict[\"lora_weights\"], adapter_name=\"photomaker\")\n\n        ### (2) Add trigger word token for tokenizer_2\n        self.tokenizer_2.add_tokens([self.trigger_word_ID], special_tokens=True)\n\n    def set_ip_adapter(self):\n        unet = self.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = Consistent_AttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n                ).to(self.device, dtype=self.torch_dtype)\n            else:\n                attn_procs[name] = Consistent_IPAttProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        \n        unet.set_attn_processor(attn_procs)\n\n    @torch.inference_mode()\n    def get_facial_embeds(self, prompt_embeds, negative_prompt_embeds, facial_clip_images, facial_token_masks, valid_facial_token_idx_mask):\n        \n        hidden_states = []\n        uncond_hidden_states = []\n        for facial_clip_image in facial_clip_images:\n            hidden_state = self.image_encoder(facial_clip_image.to(self.device, dtype=self.torch_dtype), output_hidden_states=True).hidden_states[-2]\n            uncond_hidden_state = self.image_encoder(torch.zeros_like(facial_clip_image, dtype=self.torch_dtype).to(self.device), output_hidden_states=True).hidden_states[-2]\n            hidden_states.append(hidden_state)\n            uncond_hidden_states.append(uncond_hidden_state)\n        multi_facial_embeds = torch.stack(hidden_states)       \n        uncond_multi_facial_embeds = torch.stack(uncond_hidden_states)   \n\n        # condition \n        facial_prompt_embeds = self.FacialEncoder(prompt_embeds, multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n\n        # uncondition \n        uncond_facial_prompt_embeds = self.FacialEncoder(negative_prompt_embeds, uncond_multi_facial_embeds, facial_token_masks, valid_facial_token_idx_mask)  \n        \n        return facial_prompt_embeds, uncond_facial_prompt_embeds    \n\n    @torch.inference_mode()   \n    def get_image_embeds(self, faceid_embeds, face_image, s_scale=1.0, shortcut=False):\n\n        clip_image = self.clip_image_processor(images=face_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=self.torch_dtype)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        uncond_clip_image_embeds = self.image_encoder(torch.zeros_like(clip_image), output_hidden_states=True).hidden_states[-2]\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_tokens = self.image_proj_model(faceid_embeds, clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds), uncond_clip_image_embeds, shortcut=shortcut, scale=s_scale)\n\n        return image_prompt_tokens, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, Consistent_IPAttProcessor):\n                attn_processor.scale = scale\n\n    @torch.inference_mode()\n    def get_prepare_faceid(self, input_image_path=None):\n        faceid_image = cv2.imread(input_image_path)\n        face_info = self.app.get(faceid_image)\n        if face_info==[]:\n            faceid_embeds = torch.zeros_like(torch.empty((1, 512)))\n        else:\n            faceid_embeds = torch.from_numpy(face_info[0].normed_embedding).unsqueeze(0)\n\n        # print(f\"faceid_embeds is : {faceid_embeds}\")\n        return faceid_embeds\n\n    @torch.inference_mode()\n    def parsing_face_mask(self, raw_image_refer):\n\n        to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n        to_pil = transforms.ToPILImage()\n\n        with torch.no_grad():\n            ### change sdxl\n            image = raw_image_refer.resize((1280, 1280), Image.BILINEAR)\n            image_resize_PIL = image\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.float().cuda()\n            out = self.bise_net(img)[0]\n            parsing_anno = out.squeeze(0).cpu().numpy().argmax(0)\n        \n        im = np.array(image_resize_PIL)\n        vis_im = im.copy().astype(np.uint8)\n        stride=1\n        vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n        vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n        vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n        num_of_class = np.max(vis_parsing_anno)\n\n        for pi in range(1, num_of_class + 1): # num_of_class=17 pi=1~16\n            index = np.where(vis_parsing_anno == pi) \n            vis_parsing_anno_color[index[0], index[1], :] = self.part_colors[pi] \n\n        vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n        vis_parsing_anno_color = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n        return vis_parsing_anno_color, vis_parsing_anno\n\n    @torch.inference_mode()\n    def get_prepare_llva_caption(self, input_image_file, model_path=None, prompt=None):\n        \n        ### Optional: Use the LLaVA\n        # args = type('Args', (), {\n        #     \"model_path\": self.llva_model_path,\n        #     \"model_base\": None,\n        #     \"model_name\": get_model_name_from_path(self.llva_model_path),\n        #     \"query\": self.llva_prompt,\n        #     \"conv_mode\": None,\n        #     \"image_file\": input_image_file,\n        #     \"sep\": \",\",\n        #     \"temperature\": 0,\n        #     \"top_p\": None,\n        #     \"num_beams\": 1,\n        #     \"max_new_tokens\": 512\n        # })() \n        # face_caption = eval_model(args, self.llva_tokenizer, self.llva_model, self.llva_image_processor)\n\n        ### Use built-in template\n        face_caption = \"The person has one face, one nose, two eyes, two ears, and a mouth.\"\n\n        return face_caption\n\n    @torch.inference_mode()\n    def get_prepare_facemask(self, input_image_file):\n\n        vis_parsing_anno_color, vis_parsing_anno = self.parsing_face_mask(input_image_file)\n        parsing_mask_list = masks_for_unique_values(vis_parsing_anno) \n\n        key_parsing_mask_list = {}\n        key_list = [\"Face\", \"Left_Ear\", \"Right_Ear\", \"Left_Eye\", \"Right_Eye\", \"Nose\", \"Upper_Lip\", \"Lower_Lip\"]\n        processed_keys = set()\n        for key, mask_image in parsing_mask_list.items():\n            if key in key_list:\n                if \"_\" in key:\n                    prefix = key.split(\"_\")[1]\n                    if prefix in processed_keys:                   \n                        continue\n                    else:            \n                        key_parsing_mask_list[key] = mask_image \n                        processed_keys.add(prefix)  \n            \n                key_parsing_mask_list[key] = mask_image            \n\n        return key_parsing_mask_list, vis_parsing_anno_color\n\n    def encode_prompt_with_trigger_word(\n        self,\n        prompt: str,\n        face_caption: str,\n        key_parsing_mask_list = None,\n        image_token = \"<|image|>\", \n        facial_token = \"<|facial|>\",\n        max_num_facials = 5,\n        num_id_images: int = 1,\n        device: Optional[torch.device] = None,\n    ):\n        device = device or self._execution_device\n\n        # pdb.set_trace()\n        face_caption_align, key_parsing_mask_list_align = process_text_with_markers(face_caption, key_parsing_mask_list) \n        \n        prompt_face = prompt + \"; Detail:\" + face_caption_align\n\n        max_text_length=330      \n        if len(self.tokenizer(prompt_face, max_length=self.tokenizer.model_max_length, padding=\"max_length\",truncation=False,return_tensors=\"pt\").input_ids[0])!=77:\n            prompt_face = \"; Detail:\" + face_caption_align + \" Caption:\" + prompt\n        \n        if len(face_caption)>max_text_length:\n            prompt_face = prompt\n            face_caption_align =  \"\"\n\n        prompt_text_only = prompt_face.replace(\"<|facial|>\", \"\").replace(\"<|image|>\", \"\")\n        tokenizer = self.tokenizer\n        facial_token_id = tokenizer.convert_tokens_to_ids(facial_token)\n        image_token_id = None\n\n        clean_input_id, image_token_mask, facial_token_mask = tokenize_and_mask_noun_phrases_ends(\n        prompt_face, image_token_id, facial_token_id, tokenizer) \n\n        image_token_idx, image_token_idx_mask, facial_token_idx, facial_token_idx_mask = prepare_image_token_idx(\n            image_token_mask, facial_token_mask, num_id_images, max_num_facials )\n\n        ######################################\n        ########## add for sdxl\n        ######################################\n        tokenizer_2 = self.tokenizer_2\n        facial_token_id2 = tokenizer.convert_tokens_to_ids(facial_token)\n        image_token_id2 = None\n        clean_input_id2, image_token_mask2, facial_token_mask2 = tokenize_and_mask_noun_phrases_ends(\n        prompt_face, image_token_id2, facial_token_id2, tokenizer_2) \n\n        image_token_idx2, image_token_idx_mask2, facial_token_idx2, facial_token_idx_mask2 = prepare_image_token_idx(\n            image_token_mask2, facial_token_mask2, num_id_images, max_num_facials )\n\n        return prompt_text_only, clean_input_id, clean_input_id2, key_parsing_mask_list_align, facial_token_mask, facial_token_idx, facial_token_idx_mask\n\n    @torch.inference_mode()\n    def get_prepare_clip_image(self, input_image_file, key_parsing_mask_list, image_size=512, max_num_facials=5, change_facial=True):\n        \n        facial_mask = []\n        facial_clip_image = []\n        transform_mask = transforms.Compose([transforms.CenterCrop(size=image_size), transforms.ToTensor(),])\n        clip_image_processor = CLIPImageProcessor()\n\n        num_facial_part = len(key_parsing_mask_list)\n\n        for key in key_parsing_mask_list:\n            key_mask=key_parsing_mask_list[key]\n            facial_mask.append(transform_mask(key_mask))\n            key_mask_raw_image = fetch_mask_raw_image(input_image_file,key_mask)\n            parsing_clip_image = clip_image_processor(images=key_mask_raw_image, return_tensors=\"pt\").pixel_values\n            facial_clip_image.append(parsing_clip_image)\n\n        padding_ficial_clip_image = torch.zeros_like(torch.zeros([1, 3, 224, 224]))\n        padding_ficial_mask = torch.zeros_like(torch.zeros([1, image_size, image_size]))\n\n        if num_facial_part < max_num_facials:\n            facial_clip_image += [torch.zeros_like(padding_ficial_clip_image) for _ in range(max_num_facials - num_facial_part) ]\n            facial_mask += [ torch.zeros_like(padding_ficial_mask) for _ in range(max_num_facials - num_facial_part)]\n\n        facial_clip_image = torch.stack(facial_clip_image, dim=1).squeeze(0)\n        facial_mask = torch.stack(facial_mask, dim=0).squeeze(dim=1)\n\n        return facial_clip_image, facial_mask\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        face_caption: Union[str, List[str]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        original_size: Optional[Tuple[int, int]] = None,\n        target_size: Optional[Tuple[int, int]] = None,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        input_id_images: PipelineImageInput = None,\n        input_image_path: PipelineImageInput = None,\n        start_merge_step: int = 0,\n        class_tokens_mask: Optional[torch.LongTensor] = None,\n        prompt_embeds_text_only: Optional[torch.FloatTensor] = None,\n        ### add for sdxl\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,   \n        prompt_2: Optional[Union[str, List[str]]] = None,     \n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,        \n        pooled_prompt_embeds_text_only: Optional[torch.FloatTensor] = None,   \n        guidance_rescale: float = 7.5             \n    ):\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n\n        # 1. Check inputs. Raise error if not correct\n        # self.check_inputs(\n        #     prompt,\n        #     height,\n        #     width,\n        #     callback_steps,\n        #     negative_prompt,\n        #     prompt_embeds,\n        #     negative_prompt_embeds,\n        # )\n\n        if not isinstance(input_id_images, list):\n            input_id_images = [input_id_images]\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n        do_classifier_free_guidance = guidance_scale >= 1.0\n        input_image_file = input_id_images[0]\n\n        faceid_embeds = self.get_prepare_faceid(input_image_path=input_image_path)\n        face_caption = self.get_prepare_llva_caption(input_image_file=input_image_file)\n        key_parsing_mask_list, vis_parsing_anno_color = self.get_prepare_facemask(input_image_file)\n\n        assert do_classifier_free_guidance\n\n        # 3. Encode input prompt\n        num_id_images = len(input_id_images)\n\n        (\n            prompt_text_only,\n            clean_input_id,\n            clean_input_id2, ### add for sdxl\n            key_parsing_mask_list_align,\n            facial_token_mask,\n            facial_token_idx,\n            facial_token_idx_mask,\n        ) = self.encode_prompt_with_trigger_word(\n            prompt = prompt,\n            face_caption = face_caption,\n            key_parsing_mask_list=key_parsing_mask_list,\n            device=device,\n            max_num_facials = 5,\n            num_id_images= num_id_images,\n        )\n\n        # 4. Encode input prompt without the trigger word for delayed conditioning\n        text_embeds = self.text_encoder(clean_input_id.to(device), output_hidden_states=True).hidden_states[-2]\n        ######################################\n        ########## add for sdxl : add pooled_text_embeds\n        ######################################\n        ### (4-1)\n        encoder_output_2 = self.text_encoder_2(clean_input_id2.to(device), output_hidden_states=True)\n        pooled_text_embeds = encoder_output_2[0]\n        text_embeds_2 = encoder_output_2.hidden_states[-2]\n        \n        ### (4-2)\n        encoder_hidden_states = torch.concat([text_embeds, text_embeds_2], dim=-1) # concat  \n\n        ### (4-3)\n        if self.text_encoder_2 is None:\n            text_encoder_projection_dim = int(pooled_text_embeds.shape[-1])\n        else:\n            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n        add_time_ids = self._get_add_time_ids(\n            original_size,\n            crops_coords_top_left,\n            target_size,\n            dtype=self.torch_dtype,\n            text_encoder_projection_dim=text_encoder_projection_dim,\n        )\n        add_time_ids = torch.cat([add_time_ids, add_time_ids], dim=0) ### add_time_ids.Size([2, 6])\n        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n        \n        ######################################\n        ########## add for sdxl : add pooled_prompt_embeds\n        ######################################\n        text_encoder_lora_scale = (\n            cross_attention_kwargs.get(\"scale\", None) if cross_attention_kwargs is not None else None\n        )        \n        (\n            prompt_embeds,\n            negative_prompt_embeds, \n            pooled_prompt_embeds_text_only,\n            negative_pooled_prompt_embeds,  \n        )= self.encode_prompt(\n            prompt=prompt,\n            prompt_2=prompt_2,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n            negative_prompt_2=negative_prompt_2,\n            prompt_embeds=prompt_embeds_text_only,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds_text_only,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds, \n            lora_scale=text_encoder_lora_scale,       \n        )\n\n        # 5. Prepare the input ID images\n        prompt_tokens_faceid, uncond_prompt_tokens_faceid = self.get_image_embeds(faceid_embeds, face_image=input_image_file, s_scale=1.0, shortcut=True)\n\n        facial_clip_image, facial_mask = self.get_prepare_clip_image(input_image_file, key_parsing_mask_list_align, image_size=1280, max_num_facials=5)\n        facial_clip_images = facial_clip_image.unsqueeze(0).to(device, dtype=self.torch_dtype)\n        facial_token_mask = facial_token_mask.to(device)\n        facial_token_idx_mask = facial_token_idx_mask.to(device)\n\n        cross_attention_kwargs = {}\n\n        # 6. Get the update text embedding\n        prompt_embeds_facial, uncond_prompt_embeds_facial = self.get_facial_embeds(encoder_hidden_states, negative_prompt_embeds, \\\n                                                            facial_clip_images, facial_token_mask, facial_token_idx_mask)\n\n        ########## text_facial embeds\n        prompt_embeds_facial = torch.cat([prompt_embeds_facial, prompt_tokens_faceid], dim=1)\n        negative_prompt_embeds_facial = torch.cat([uncond_prompt_embeds_facial, uncond_prompt_tokens_faceid], dim=1)\n\n        ########## text_only embeds\n        prompt_embeds_text_only = torch.cat([prompt_embeds, prompt_tokens_faceid], dim=1)\n        negative_prompt_embeds_text_only = torch.cat([negative_prompt_embeds, uncond_prompt_tokens_faceid], dim=1)\n\n        # 7. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 8. Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 9. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                latent_model_input = (\n                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n                \n                ######################################\n                ########## add for sdxl : add unet_added_cond_kwargs\n                ######################################                  \n                if i <= start_merge_step:\n                    current_prompt_embeds = torch.cat(\n                        [negative_prompt_embeds_text_only, prompt_embeds_text_only], dim=0\n                    )\n                    add_text_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds_text_only], dim=0)\n                else:\n                    current_prompt_embeds = torch.cat(\n                        [negative_prompt_embeds_facial, prompt_embeds_facial], dim=0\n                    )\n                    add_text_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_text_embeds], dim=0)\n\n                unet_added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=current_prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                    added_cond_kwargs=unet_added_cond_kwargs,\n                    # return_dict=False, ### [0]\n                ).sample \n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (\n                        noise_pred_text - noise_pred_uncond\n                    )\n                else:\n                    assert 0, \"Not Implemented\"\n\n                # if do_classifier_free_guidance and guidance_rescale > 0.0:\n                #     # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf \n                #     noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale) ### TODO optimal noise and LCM\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(\n                    noise_pred, t, latents, **extra_step_kwargs\n                ).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or (\n                    (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0\n                ):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n        \n        # make sure the VAE is in float32 mode, as it overflows in float16\n        if self.vae.dtype == torch.float16 and self.vae.config.force_upcast:\n            self.upcast_vae()\n            latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n\n        if not output_type == \"latent\":\n            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n        else:\n            image = latents\n            return StableDiffusionXLPipelineOutput(images=image)\n\n        # apply watermark if available\n        # if self.watermark is not None:\n        #     image = self.watermark.apply_watermark(image)\n\n        image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return StableDiffusionXLPipelineOutput(images=image)\n\n\n\n\n\n\n\n\n\n"}
