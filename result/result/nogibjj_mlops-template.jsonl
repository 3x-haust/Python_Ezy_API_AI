{"repo_info": {"repo_name": "mlops-template", "repo_owner": "nogibjj", "repo_url": "https://github.com/nogibjj/mlops-template"}}
{"type": "test_file", "path": "test_main.py", "content": "\"\"\"\nTest goes here\n\n\"\"\"\n\nfrom mylib.calculator import add\n\n\ndef test_add():\n    assert add(1, 2) == 3\n"}
{"type": "source_file", "path": "bentoml/hello.py", "content": "import bentoml\n\nfrom sklearn import svm\nfrom sklearn import datasets\n\n# Load training data set\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Train the model\nclf = svm.SVC(gamma='scale')\nclf.fit(X, y)\n\n# Save model to the BentoML local model store\nsaved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\nprint(f\"Model saved: {saved_model}\")\n"}
{"type": "source_file", "path": "examples/quickstart/service.py", "content": "import numpy as np\n\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n\n@svc.api(\n    input=NumpyNdarray.from_sample(\n        np.array([[4.9, 3.0, 1.4, 0.2]], dtype=np.double), enforce_shape=False\n    ),\n    output=NumpyNdarray(),\n)\nasync def classify(input_series: np.ndarray) -> np.ndarray:\n    return await iris_clf_runner.predict.async_run(input_series)\n"}
{"type": "source_file", "path": "hugging-face/hf_whisper.py", "content": "#!/usr/bin/env python\n# \"\"\"Create OpenAI Whisper command-line tool using Hugging Face's transformers library.\"\"\"\n\nfrom transformers import pipeline\nimport click\n\n\n# Create a function that reads a sample audio file and transcribes it using openai's whisper\ndef traudio(filename, model=\"openai/whisper-tiny.en\"):\n    with open(filename, \"rb\") as f:\n        _ = f.read()  # this needs to be fixed\n    print(f\"Transcribing {filename}...\")\n    pipe = pipeline(\"automatic-speech-recognition\", model=model)\n    results = pipe(filename)\n    return results\n\n\n# create click group\n@click.group()\ndef cli():\n    \"\"\"A cli for openai whisper\"\"\"\n\n\n# create a click command that transcribes\n@cli.command(\"transcribe\")\n@click.option(\n    \"--model\", default=\"openai/whisper-tiny.en\", help=\"Model to use for transcription\"\n)\n@click.argument(\"filename\", default=\"utils/four-score.m4a\")\ndef whispercli(filename, model):\n    \"\"\"Transcribe audio using openai whisper\"\"\"\n    results = traudio(filename, model)\n    # print out each label and its score in a tabular format with colors\n    for result in results:\n        click.secho(f\"{result['text']}\", fg=\"green\")\n\n\nif __name__ == \"__main__\":\n    cli()\n"}
{"type": "source_file", "path": "hugging-face/zero_shot_classification.py", "content": "#!/usr/bin/env python\n\"\"\"Create Zero-shot classification command-line tool using Hugging Face's transformers library.\"\"\"\n\nfrom transformers import pipeline\nimport click\n\n# Create a function that reads a file\ndef read_file(filename):\n    with open(filename, encoding=\"utf-8\") as myfile:\n        return myfile.read()\n\n\n# create a function that grabs candidate labels from a file\ndef read_labels(kw_file):\n    return read_file(kw_file).splitlines()\n\n\n# create a function that reads a file performs zero-shot classification\ndef classify(text, labels, model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"):\n    classifier = pipeline(\"zero-shot-classification\", model=model)\n    results = classifier(text, labels, multi_label=False)\n    return results\n\n\n# create click group\n@click.group()\ndef cli():\n    \"\"\"A cli for zero-shot classification\"\"\"\n\n\n# create a click command that performs zero-shot classification\n@cli.command(\"classify\")\n@click.argument(\"filename\", default=\"four-score.m4a.txt\")\n@click.argument(\"kw_file\", default=\"keywords.txt\")\ndef classifycli(filename, kw_file):\n    \"\"\"Classify text using zero-shot classification\"\"\"\n    text = read_file(filename)\n    labels = read_labels(kw_file)  # needs to be a sequence\n    results = classify(text, labels)\n    # print out each label and its score in a tabular format with colors\n    for label, score in zip(results[\"labels\"], results[\"scores\"]):\n        click.secho(f\"{label}\\t{score:.2f}\", fg=\"green\")\n\n\nif __name__ == \"__main__\":\n    cli()\n"}
{"type": "source_file", "path": "examples/quickstart/locustfile.py", "content": "import time\n\nimport grpc\nimport numpy as np\nfrom locust import task\nfrom locust import User\nfrom locust import between\nfrom locust import HttpUser\nfrom sklearn import datasets\n\nfrom bentoml.grpc.v1 import service_pb2 as pb\nfrom bentoml.grpc.v1 import service_pb2_grpc as services\n\ntest_data = datasets.load_iris().data\nnum_of_rows = test_data.shape[0]\nmax_batch_size = 10\n\n\nclass IrisHttpUser(HttpUser):\n    \"\"\"\n    Usage:\n        Run the iris_classifier service in production mode:\n\n            bentoml serve-http iris_classifier:latest --production\n\n        Start locust load testing client with:\n\n            locust --class-picker -H http://localhost:3000\n\n        Open browser at http://0.0.0.0:8089, adjust desired number of users and spawn\n        rate for the load test from the Web UI and start swarming.\n    \"\"\"\n\n    @task\n    def classify(self):\n        start = np.random.choice(num_of_rows - max_batch_size)\n        end = start + np.random.choice(max_batch_size) + 1\n\n        input_data = test_data[start:end]\n        self.client.post(\"/classify\", json=input_data.tolist())\n\n    wait_time = between(0.01, 2)\n\n\nclass GrpcUser(User):\n    abstract = True\n\n    stub_class = None\n\n    def __init__(self, environment):\n        super().__init__(environment)\n        self.environment = environment\n\n    def on_start(self):\n        self.channel = grpc.insecure_channel(self.host)\n        self.stub = services.BentoServiceStub(self.channel)\n\n\nclass IrisGrpcUser(GrpcUser):\n    \"\"\"\n    Implementation is inspired by https://docs.locust.io/en/stable/testing-other-systems.html\n\n    Usage:\n        Run the iris_classifier service in production mode:\n\n            bentoml serve-grpc iris_classifier:latest --production\n\n        Start locust load testing client with:\n\n            locust --class-picker -H localhost:3000\n\n        Open browser at http://0.0.0.0:8089, adjust desired number of users and spawn\n        rate for the load test from the Web UI and start swarming.\n    \"\"\"\n\n    @task\n    def classify(self):\n        start = np.random.choice(num_of_rows - max_batch_size)\n        end = start + np.random.choice(max_batch_size) + 1\n        input_data = test_data[start:end]\n        request_meta = {\n            \"request_type\": \"grpc\",\n            \"name\": \"classify\",\n            \"start_time\": time.time(),\n            \"response_length\": 0,\n            \"exception\": None,\n            \"context\": None,\n            \"response\": None,\n        }\n        start_perf_counter = time.perf_counter()\n        try:\n            request_meta[\"response\"] = self.stub.Call(\n                request=pb.Request(\n                    api_name=request_meta[\"name\"],\n                    ndarray=pb.NDArray(\n                        dtype=pb.NDArray.DTYPE_FLOAT,\n                        # shape=(1, 4),\n                        shape=(len(input_data), 4),\n                        # float_values=[5.9, 3, 5.1, 1.8],\n                        float_values=input_data.flatten(),\n                    ),\n                )\n            )\n        except grpc.RpcError as e:\n            request_meta[\"exception\"] = e\n        request_meta[\"response_time\"] = (\n            time.perf_counter() - start_perf_counter\n        ) * 1000\n        self.environment.events.request.fire(**request_meta)\n\n    wait_time = between(0.01, 2)\n"}
{"type": "source_file", "path": "examples/quickstart/query.py", "content": "import requests\n\nresult = requests.post(\n   \"http://127.0.0.1:3000/classify\",\n   headers={\"content-type\": \"application/json\"},\n   data=\"[[5.9, 3, 5.1, 1.8]]\",\n).text\n\nprint(result)"}
{"type": "source_file", "path": "examples/quickstart/train.py", "content": "import logging\n\nfrom sklearn import svm\nfrom sklearn import datasets\n\nimport bentoml\n\nlogging.basicConfig(level=logging.WARN)\n\nif __name__ == \"__main__\":\n\n    # Load training data\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n\n    # Model Training\n    clf = svm.SVC()\n    clf.fit(X, y)\n\n    # Save model to BentoML local model store\n    saved_model = bentoml.sklearn.save_model(\n        \"iris_clf\", clf, signatures={\"predict\": {\"batchable\": True, \"batch_dim\": 0}}\n    )\n    print(f\"Model saved: {saved_model}\")\n"}
{"type": "source_file", "path": "hugging-face/hf_fine_tune_hello_world.py", "content": "#!/usr/bin/env python\n\n\"\"\"\nFine Tuning Example with HuggingFace\n\nBased on official tutorial\n\"\"\"\n\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset, load_metric\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport numpy as np\n\n# Load the dataset\ndataset = load_dataset(\"yelp_review_full\")\ndataset[\"train\"][100]\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Load the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\nmetric = load_metric(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# can use if needed to reduce memory usage and training time\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train() # train the model"}
{"type": "source_file", "path": "hugging-face/download_hf_model.py", "content": "\"\"\"\nAfter running zip it to save\nzip -r summarizeApp.zip summarizeApp \n\"\"\"\n\nfrom transformers import pipeline\n\nmodel = pipeline(\n    \"summarization\",\n    model=\"sshleifer/distilbart-cnn-12-6\",\n    revision=\"a4f8f3e\",\n)\nmodel.save_pretrained(\"summarizeApp\")"}
{"type": "source_file", "path": "hugging-face/load_model.py", "content": "\"\"\"Loading a model from a file.\"\"\"\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"summarizeApp\")\ntokenizer = AutoTokenizer.from_pretrained(\"summarizeApp\")\n#open the file with utf-8 encoding\nwith open(\"input.txt\", encoding=\"utf-8\") as f:\n    text = f.read()\n\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")\noutputs = model.generate(input_ids)\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(decoded)"}
{"type": "source_file", "path": "utils/quickstart_tf2.py", "content": "#!/usr/bin/env python\n\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\npredictions = model(x_train[:1]).numpy()\npredictions\ntf.nn.softmax(predictions).numpy()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_fn(y_train[:1], predictions).numpy()\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test,  y_test, verbose=2)\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\nprobability_model(x_test[:5])"}
{"type": "source_file", "path": "mylib/calculator.py", "content": "\"\"\"\nCalculations library\n\"\"\"\n\n\ndef add(a, b):\n    return a + b\n\n\ndef subtract(a, b):\n    return a - b\n\n\ndef multiply(a, b):\n    return a * b\n\n\ndef divide(a, b):\n    return a / b"}
{"type": "source_file", "path": "utils/verify_pytorch.py", "content": "import torch\n\nif torch.cuda.is_available():\n    print(\"CUDA is available\")\n    print(\"CUDA version: {}\".format(torch.version.cuda))\n    print(\"PyTorch version: {}\".format(torch.__version__))\n    print(\"cuDNN version: {}\".format(torch.backends.cudnn.version()))\n    print(\"Number of CUDA devices: {}\".format(torch.cuda.device_count()))\n    print(\"Current CUDA device: {}\".format(torch.cuda.current_device()))\n    print(\"Device name: {}\".format(torch.cuda.get_device_name(torch.cuda.current_device())))\nelse:\n    print(\"CUDA is not available\")"}
{"type": "source_file", "path": "mylib/__init__.py", "content": ""}
{"type": "source_file", "path": "utils/verify_tf.py", "content": "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"}
{"type": "source_file", "path": "main.py", "content": "\"\"\"\nMain cli or app entry point\n\"\"\"\n\nfrom mylib.calculator import add\nimport click\n\n\n@click.command(\"add\")\n@click.argument(\"a\", type=int)\n@click.argument(\"b\", type=int)\ndef add_cli(a, b):\n    click.echo(add(a, b))\n\n\nif __name__ == \"__main__\":\n    # pylint: disable=no-value-for-parameter\n    add_cli()\n"}
{"type": "source_file", "path": "utils/kw_extract.py", "content": "#!/usr/bin/env python3\n\n\"\"\"\nMain cli or app entry point\n\"\"\"\nimport yake\nimport click\n\n# create a function that reads a file\ndef read_file(filename):\n    with open(filename, encoding=\"utf-8\") as myfile:\n        return myfile.read()\n\n\n# def extract keywords\ndef extract_keywords(text):\n    kw_extractor = yake.KeywordExtractor()\n    keywords = kw_extractor.extract_keywords(text)\n    return keywords\n\n\n# create a function that makes hash tags\ndef make_hashtags(keywords):\n    hashtags = []\n    for keyword in keywords:\n        hashtags.append(\"#\" + keyword[0].replace(\" \", \"\"))\n    return hashtags\n\n#create a function returns only the top N keywords\ndef top_n_keywords(keywords, n=5):\n    return keywords[:n]\n\n@click.group()\ndef cli():\n    \"\"\"A cli for keyword extraction\"\"\"\n\n#print the top N keywords\n@cli.command(\"extract\")\n@click.argument(\"filename\", default=\"text.txt\")\n@click.option(\"--n\", default=5, help=\"Number of keywords to extract\")\ndef extract(filename, n):\n    \"\"\"Extract keywords from a file\"\"\"\n    text = read_file(filename)\n    keywords = extract_keywords(text)\n    top_keywords = top_n_keywords(keywords, n)\n    for keyword in top_keywords:\n        print(keyword[0])\n\n\n@cli.command(\"hashtags\")\n@click.argument(\"filename\", default=\"text.txt\")\ndef hashtagscli(filename):\n    \"\"\"Extract keywords from a file and make hashtags\"\"\"\n    text = read_file(filename)\n    keywords = extract_keywords(text)\n    hashtags = make_hashtags(keywords)\n    click.echo(hashtags)\n\n\nif __name__ == \"__main__\":\n    cli()"}
{"type": "source_file", "path": "utils/quickstart_pytorch.py", "content": "\"\"\"\n`Learn the Basics <intro.html>`_ ||\n**Quickstart** ||\n`Tensors <tensorqs_tutorial.html>`_ ||\n`Datasets & DataLoaders <data_tutorial.html>`_ ||\n`Transforms <transforms_tutorial.html>`_ ||\n`Build Model <buildmodel_tutorial.html>`_ ||\n`Autograd <autogradqs_tutorial.html>`_ ||\n`Optimization <optimization_tutorial.html>`_ ||\n`Save & Load Model <saveloadrun_tutorial.html>`_\n\nQuickstart\n===================\nThis section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n\nWorking with data\n-----------------\nPyTorch has two `primitives to work with data <https://pytorch.org/docs/stable/data.html>`_:\n``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\nthe ``Dataset``.\n\n\"\"\"\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n######################################################################\n# PyTorch offers domain-specific libraries such as `TorchText <https://pytorch.org/text/stable/index.html>`_,\n# `TorchVision <https://pytorch.org/vision/stable/index.html>`_, and `TorchAudio <https://pytorch.org/audio/stable/index.html>`_,\n# all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n#\n# The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n# CIFAR, COCO (`full list here <https://pytorch.org/vision/stable/datasets.html>`_). In this tutorial, we\n# use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n# ``target_transform`` to modify the samples and labels respectively.\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n######################################################################\n# We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n# automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n# in the dataloader iterable will return a batch of 64 features and labels.\n\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\n######################################################################\n# Read more about `loading data in PyTorch <data_tutorial.html>`_.\n#\n\n######################################################################\n# --------------\n#\n\n################################\n# Creating Models\n# ------------------\n# To define a neural network in PyTorch, we create a class that inherits\n# from `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_. We define the layers of the network\n# in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n# operations in the neural network, we move it to the GPU if available.\n\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\n######################################################################\n# Read more about `building neural networks in PyTorch <buildmodel_tutorial.html>`_.\n#\n\n\n######################################################################\n# --------------\n#\n\n\n#####################################################################\n# Optimizing the Model Parameters\n# ----------------------------------------\n# To train a model, we need a `loss function <https://pytorch.org/docs/stable/nn.html#loss-functions>`_\n# and an `optimizer <https://pytorch.org/docs/stable/optim.html>`_.\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n\n#######################################################################\n# In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n# backpropagates the prediction error to adjust the model's parameters.\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n##############################################################################\n# We also check the model's performance against the test dataset to ensure it is learning.\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\n##############################################################################\n# The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n# parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n# accuracy increase and the loss decrease with every epoch.\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\n######################################################################\n# Read more about `Training your model <optimization_tutorial.html>`_.\n#\n\n######################################################################\n# --------------\n#\n\n######################################################################\n# Saving Models\n# -------------\n# A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n\ntorch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")\n\n\n\n######################################################################\n# Loading Models\n# ----------------------------\n#\n# The process for loading a model includes re-creating the model structure and loading\n# the state dictionary into it.\n\nmodel = NeuralNetwork()\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\n#############################################################\n# This model can now be used to make predictions.\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\n\n######################################################################\n# Read more about `Saving & Loading your model <saveloadrun_tutorial.html>`_.\n#"}
