{"repo_info": {"repo_name": "ProtoLLM", "repo_owner": "aimclub", "repo_url": "https://github.com/aimclub/ProtoLLM"}}
{"type": "test_file", "path": "protollm_tools/llm-agents-api/tests/conftest.py", "content": "import json\nimport os\nimport chromadb\nimport numpy as np\nimport pytest\nfrom fastapi.testclient import TestClient\nimport psycopg2\nfrom protollm_agents.entrypoint import Entrypoint\nfrom protollm_agents.sdk.models import CompletionModel, ChatModel, TokenizerModel, EmbeddingAPIModel\n\n@pytest.fixture(scope=\"session\")\ndef docker_compose_command() -> str:\n    return \"docker compose\"\n\n# @pytest.fixture(scope=\"session\")\n# def docker_setup():\n#     pass\n\n# @pytest.fixture(scope=\"session\")\n# def docker_cleanup():\n#     pass\n\n@pytest.fixture(scope=\"session\")\ndef docker_compose_file() -> str:\n    return \"tests/docker-compose.test.yml\"\n\n\n@pytest.fixture(scope=\"session\")\ndef docker_compose_project_name() -> str:\n    return \"test-protollm-agents\"\n\n\n@pytest.fixture(scope=\"session\")\ndef test_config_path():\n    return \"tests/config.test.yml\"\n\n\ndef is_responsive_db(host, port):\n    try:\n        conn = psycopg2.connect(host=host, port=port, user=\"test\", password=\"test\", dbname=\"test\")\n        conn.close()\n        return True\n    except psycopg2.OperationalError:\n        return False\n\ndef is_responsive_vectorstore(host, port):\n    try:\n        chroma_client = chromadb.HttpClient(host=host, port=port)\n        chroma_client.list_collections()\n        return True\n    except Exception:\n        return False\n\n@pytest.fixture(scope=\"session\")\ndef test_client(test_config_path, docker_services, docker_ip):\n    port_db = docker_services.port_for(\"db\", 5432)\n    docker_services.wait_until_responsive(\n        timeout=30.0, pause=0.1, check=lambda: is_responsive_db(host=docker_ip, port=port_db)\n    )\n    port_vectorstore = docker_services.port_for(\"vectorstore\", 8000)\n    docker_services.wait_until_responsive(\n        timeout=30.0, pause=0.1, check=lambda: is_responsive_vectorstore(host=docker_ip, port=port_vectorstore)\n    )\n    models = [\n        CompletionModel(\n            name=\"planner_llm\",\n            model=\"/model\",\n            temperature=0.01,\n            top_p=0.95,\n            streaming=False,\n            url=f\"http://{os.getenv('PYTEST_COMPLETION_MODEL_HOST')}:{os.getenv('PYTEST_COMPLETION_MODEL_PORT')}/v1\",\n            api_key=os.getenv('PYTEST_COMPLETION_MODEL_API_KEY'),\n        ),\n        CompletionModel(\n            name=\"generator_llm\",\n            model=\"/model\",\n            temperature=0.01,\n            top_p=0.95,\n            streaming=True,\n            url=f\"http://{os.getenv('PYTEST_COMPLETION_MODEL_HOST')}:{os.getenv('PYTEST_COMPLETION_MODEL_PORT')}/v1\",\n            api_key=os.getenv('PYTEST_COMPLETION_MODEL_API_KEY'),\n        ),\n        ChatModel(\n            name=\"router_llm\",\n            model=\"/model\",\n            temperature=0.01,\n            top_p=0.95,\n            streaming=True,\n            url=f\"http://{os.getenv('PYTEST_CHAT_MODEL_HOST')}:{os.getenv('PYTEST_CHAT_MODEL_PORT')}/v1\",\n            api_key=os.getenv('PYTEST_CHAT_MODEL_API_KEY'),\n        ),\n        TokenizerModel(\n            name=\"qwen_2.5\",\n            path_or_repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n        ),\n        EmbeddingAPIModel(\n            name=\"e5-mistral-7b-instruct\",\n            model=\"/models/e5-mistral-7b-instruct\",\n            url=f\"http://{os.getenv('PYTEST_EMBEDDING_MODEL_HOST')}:{os.getenv('PYTEST_EMBEDDING_MODEL_PORT')}/v1\",\n            api_key=os.getenv('PYTEST_EMBEDDING_MODEL_API_KEY'),\n            check_embedding_ctx_length=False,\n            tiktoken_enabled=False,\n        ),\n    ]\n\n    entrypoint = Entrypoint(config_path=test_config_path, models=models)\n    with TestClient(entrypoint.app) as client:\n        yield client\n\n\n@pytest.fixture(scope=\"function\")\ndef create_collections(request, docker_services, docker_ip):\n    port = docker_services.port_for(\"vectorstore\", 8000)\n    collection_datas = {\n        \"test_collection_1\": \"col_data_ed.json\",\n        \"test_collection_2\": \"col_data_env.json\",\n        \"test_collection_3\": None,\n    }\n    chroma_client = chromadb.HttpClient(host=docker_ip, port=port)\n    for collection_name, collection_data_path in collection_datas.items():\n        if collection_name in [col.name for col in chroma_client.list_collections()]:\n            chroma_client.delete_collection(name=collection_name)\n        collection = chroma_client.create_collection(name=collection_name)\n        if collection_data_path is None:\n            ids, documents, embeddings, metadatas = [], [], [], []\n            for path in collection_datas.values():\n                if path is not None:\n                    with open(os.path.join(request.config.rootdir, \"tests\", \"docs\", path), \"r\") as f:\n                        data = json.load(f)\n                        ids.extend(data.get(\"ids\"))\n                        documents.extend(data.get(\"documents\")) \n                        embeddings.extend(data.get(\"embeddings\"))\n                        metadatas.extend(data.get(\"metadatas\"))\n        else:\n            with open(os.path.join(request.config.rootdir, \"tests\", \"docs\", collection_data_path), \"r\") as f:\n                collection_data = json.load(f)\n            ids = collection_data.get(\"ids\")\n            documents = collection_data.get(\"documents\")\n            embeddings = np.array(collection_data.get(\"embeddings\"))\n            metadatas = collection_data.get(\"metadatas\")\n        collection.add(\n            documents=documents,\n            embeddings=embeddings,\n            ids=ids,\n            metadatas=metadatas,\n        )\n        \n    yield collection_datas.keys()\n    for collection_name in collection_datas.keys():\n        chroma_client.delete_collection(name=collection_name)"}
{"type": "test_file", "path": "protollm-synthetic/tests/test_summarization_chain.py", "content": "import unittest\nimport os\nfrom protollm_synthetic.synthetic_pipelines.chains import SummarisationChain\nfrom protollm_synthetic.utils import VLLMChatOpenAI, Dataset\nimport pandas as pd\nimport asyncio\n\nclass TestSummarizationChain(unittest.TestCase):\n    def test_summarization_chain_on_list_of_texts(self):\n        # Sample input: a list of texts\n        texts = [\n            \"The quick brown fox jumps over the lazy dog.\",\n            \"Artificial intelligence is transforming the world.\",\n            \"Python is a popular programming language.\"\n        ]\n\n        df = pd.DataFrame(texts, columns=[\"content\"])\n        df.to_json(\"tmp_data/tmp_sample_summarization_dataset.json\", index=False)\n\n        dataset = Dataset(path=\"tmp_data/tmp_sample_summarization_dataset.json\")\n        # Expected output: a list of summaries\n        expected_summaries = [\n            \"The fox jumps over the dog.\",\n            \"AI is changing the world.\",\n            \"Python is a popular language.\"\n        ]\n\n        qwen2vl_api_key = os.environ.get(\"QWEN2VL_OPENAI_API_KEY\")\n        qwen2vl_api_base = os.environ.get(\"QWEN2VL_OPENAI_API_BASE\")\n\n        llm=VLLMChatOpenAI(\n                api_key=qwen2vl_api_key,\n                base_url=qwen2vl_api_base,\n                model=\"/model\",\n                max_tokens=2048,\n                # max_concurrency=10\n            )   \n\n        summarisation_chain = SummarisationChain(llm=llm)\n        actual_summaries = asyncio.run(summarisation_chain.run(dataset, n_examples=3))\n        \n        # Assert that the actual summaries match the expected summaries\n        self.assertEqual(len(actual_summaries), len(expected_summaries))\n        # self.assertEqual(actual_summaries, expected_summaries)\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/__init__.py", "content": "\n"}
{"type": "test_file", "path": "protollm_tools/llm-agents-api/tests/test_api.py", "content": "import uuid\nimport pytest\n\ndef test_get_agents(docker_services, test_client):\n    response = test_client.get(\"/\")\n    assert response.status_code == 200\n    assert isinstance(response.json(), list)\n    for agent in response.json():\n        assert agent.get(\"name\") in [\"Ensemble\", \"Router\", \"rag_environment\", \"rag_education\", \"rag_union\"]\n        assert \"description\" in agent\n        assert \"arguments\" in agent\n        assert isinstance(agent.get(\"arguments\"), dict)\n    assert len(response.json()) == 5\n\n@pytest.mark.parametrize(\"agent_id, agent_name\", [\n    (\"07dd7db1-075a-4391-b537-6fbca4d5a5f6\", \"rag_environment\"),\n    (\"3208a446-d847-45a8-a724-159fa87334b9\", \"rag_education\"),\n    (\"2fb8e8f0-bd05-5eca-8e4d-376ede293e52\", \"rag_union\"),\n])\ndef test_get_agent_by_id(docker_services, test_client, agent_id, agent_name):\n    response = test_client.get(f\"/{agent_id}\")\n    assert response.status_code == 200\n    assert isinstance(response.json(), dict)\n    assert response.json().get(\"name\") == agent_name\n\ndef test_get_agent_by_id_not_found(docker_services, test_client):\n    response = test_client.get(f\"/{uuid.uuid4()}\")\n    assert response.status_code == 404\n\ndef test_ensemble_websocket(docker_services, test_client, create_collections):\n    with test_client.websocket_connect(\"/ensemble\") as websocket:\n        json_data = {\n            \"dialogue_id\": str(uuid.uuid4()),\n            \"chat_history\":[],\n            \"query\":\"Какие целевые показатели госпрограмм по образованию и защите окружающей среды?\"\n        }\n        websocket.send_json(json_data)\n        is_eos = False\n        events = set()\n        while not is_eos:\n            data = websocket.receive_json()\n            print(data.get('event_id'))\n            event_name = data.get('name')\n            assert event_name != 'error'\n            is_eos = data.get(\"is_eos\", False)\n            events.add(event_name)\n        assert events == set({'answer', 'tool_answer', 'retrieval'})\n\ndef test_router_websocket(docker_services, test_client, create_collections):\n    with test_client.websocket_connect(\"/router\") as websocket:\n        json_data = {\n            \"dialogue_id\": str(uuid.uuid4()),\n            \"chat_history\":[],\n            \"query\":\"Какие целевые показатели госпрограмм по образованию?\"\n        }\n        websocket.send_json(json_data)\n        is_eos = False\n        events = set()\n        while not is_eos:\n            data = websocket.receive_json()\n            event_name = data.get('name')\n            assert event_name != 'error'\n            is_eos = data.get(\"is_eos\", False)\n            events.add(event_name)\n        assert events == set({'answer', 'tool_answer', 'retrieval'})\n\n@pytest.mark.parametrize(\"agent_id\", [\n    \"07dd7db1-075a-4391-b537-6fbca4d5a5f6\", \"3208a446-d847-45a8-a724-159fa87334b9\", \"2fb8e8f0-bd05-5eca-8e4d-376ede293e52\"\n])\ndef test_rag_websocket(docker_services, test_client, agent_id, create_collections):\n    with test_client.websocket_connect(\"/agent\") as websocket:\n        json_data = {\n            \"dialogue_id\": str(uuid.uuid4()),\n            \"agent_id\": agent_id,\n            \"chat_history\":[],\n            \"query\":\"Какие целевые показатели госпрограмм по образованию и защите окружающей среды?\",\n            \"run_params\": {}\n        }\n        websocket.send_json(json_data)\n        is_eos = False\n        events = set()\n        while not is_eos:\n            data = websocket.receive_json()\n            event_name = data.get('name')\n            assert event_name != 'error'\n            is_eos = data.get(\"is_eos\", False)\n            events.add(event_name)\n        assert events == set({'answer', 'retrieval'})\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/conftest.py", "content": "import pytest\n\nfrom protollm_api.config import Config\n\n\n@pytest.fixture(scope=\"module\")\ndef test_local_config():\n    return Config()\n\n\n@pytest.fixture(scope=\"module\")\ndef test_real_config():\n    return Config.read_from_env()\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/integration/test_local_Redis.py", "content": "import asyncio\nimport uuid\n\nimport pytest\nfrom protollm_sdk.models.job_context_models import ResponseModel\nfrom protollm_sdk.object_interface.redis_wrapper import RedisWrapper\n\nfrom protollm_api.backend.broker import get_result\n\n\n@pytest.fixture(scope=\"module\")\ndef redis_client(test_local_config):\n    assert test_local_config.redis_host == \"localhost\"\n    client = RedisWrapper(test_local_config.redis_host, test_local_config.redis_port)\n    return client\n\n\n@pytest.mark.asyncio\nasync def test_get_result_from_local_redis(test_local_config, redis_client):\n    task_id = str(uuid.uuid4())\n    redis_key = f\"{test_local_config.redis_prefix}:{task_id}\"\n    expected_content = {'content': 'success'}\n\n    result_task = asyncio.create_task(get_result(test_local_config, task_id, redis_client))\n\n    await asyncio.sleep(1)\n\n    redis_client.save_item(redis_key, expected_content)\n\n    response = await result_task\n\n    assert isinstance(response, ResponseModel)\n    assert response.content == 'success'\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/integration/test_local_RMQ.py", "content": "import json\nimport uuid\n\nimport pika\nimport pytest\nfrom protollm_sdk.models.job_context_models import (ChatCompletionModel, PromptMeta, ChatCompletionUnit,\n                                                      ChatCompletionTransactionModel, PromptTypes)\nfrom protollm_sdk.object_interface import RabbitMQWrapper\n\nfrom protollm_api.backend.broker import send_task\n\n@pytest.fixture(scope=\"module\")\ndef rabbit_client(test_local_config):\n    assert test_local_config.rabbit_host == \"localhost\"\n    client = RabbitMQWrapper(test_local_config.rabbit_host,\n                             test_local_config.rabbit_port,\n                             test_local_config.rabbit_login,\n                             test_local_config.rabbit_password)\n    return client\n\n\n@pytest.fixture(scope=\"module\")\ndef rabbitmq_connection(test_local_config):\n    assert test_local_config.rabbit_host == \"localhost\"\n    connection = pika.BlockingConnection(\n        pika.ConnectionParameters(\n            host=test_local_config.rabbit_host,\n            port=test_local_config.rabbit_port,\n            virtual_host='/',\n            credentials=pika.PlainCredentials(\n                username=test_local_config.rabbit_login,\n                password=test_local_config.rabbit_password\n            )\n        )\n    )\n    channel = connection.channel()\n\n    yield channel\n\n    connection.close()\n\n\n@pytest.mark.asyncio\nasync def test_task_in_queue(test_local_config, rabbitmq_connection, rabbit_client):\n    queue_name = \"test_priority_queue\"\n    prompt = ChatCompletionModel(\n        job_id=str(uuid.uuid4()),\n        priority=3,\n        meta=PromptMeta(),\n        messages=[ChatCompletionUnit(role=\"user\", content=\"test request\")]\n    )\n    transaction = ChatCompletionTransactionModel(prompt=prompt, prompt_type=PromptTypes.CHAT_COMPLETION.value)\n\n    await send_task(test_local_config, queue_name, transaction, rabbit_client)\n\n    method_frame, header_frame, body = rabbitmq_connection.basic_get(queue=queue_name, auto_ack=True)\n\n    assert method_frame is not None\n    task = json.loads(body)\n\n    assert task[\"id\"] == prompt.job_id\n    assert task[\"task\"] == \"generate\"\n\n    kwargs = task[\"kwargs\"]\n    assert kwargs[\"prompt_type\"] == transaction.prompt_type\n\n    resp_prompt = kwargs[\"prompt\"]\n    assert resp_prompt[\"job_id\"] == prompt.job_id\n    assert resp_prompt[\"priority\"] == prompt.priority\n\n    meta = resp_prompt[\"meta\"]\n    assert meta[\"temperature\"] == prompt.meta.temperature\n    assert meta[\"tokens_limit\"] == prompt.meta.tokens_limit\n    assert meta[\"stop_words\"] == prompt.meta.stop_words\n    assert meta[\"model\"] == prompt.meta.model\n\n    message = resp_prompt[\"messages\"]\n    assert len(message) == 1\n    assert message[0][\"content\"] == prompt.messages[0].content\n    assert message[0][\"role\"] == prompt.messages[0].role\n\n    method_frame, header_frame, body = rabbitmq_connection.basic_get(queue=queue_name, auto_ack=True)\n    assert method_frame is None, \"There is more then one task in queue\"\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/integration/test_with_llm.py", "content": "import uuid\n\nimport pytest\nfrom protollm_sdk.models.job_context_models import (\n    ChatCompletionModel, PromptMeta, ChatCompletionUnit,\n    ChatCompletionTransactionModel, PromptTypes\n)\nfrom protollm_sdk.models.job_context_models import ResponseModel\nfrom protollm_sdk.object_interface.redis_wrapper import RedisWrapper\n\nfrom protollm_api.backend.broker import get_result\nfrom protollm_api.backend.broker import send_task\n\n\n@pytest.fixture(scope=\"module\")\ndef redis_client(test_real_config):\n    assert test_real_config.redis_host != \"localhost\"\n    client = RedisWrapper(test_real_config.redis_host, test_real_config.redis_port)\n    return client\n\n\n@pytest.mark.asyncio\n@pytest.mark.skip(reason=\"Test waits infinitely in GitHub Action\")\nasync def test_task_in_queue(test_real_config, redis_client):\n    task_id = str(uuid.uuid4())\n    prompt = ChatCompletionModel(\n        job_id=task_id,\n        meta=PromptMeta(),\n        messages=[ChatCompletionUnit(role=\"user\", content=\"Сколько будет 2+2*2?\")]\n    )\n    transaction = ChatCompletionTransactionModel(prompt=prompt, prompt_type=PromptTypes.CHAT_COMPLETION.value)\n\n    await send_task(test_real_config, test_real_config.queue_name, transaction)\n\n    result = await get_result(test_real_config, task_id, redis_client)\n\n    assert isinstance(result, ResponseModel)\n    assert result.content != \"\"\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/unit/test_brocker.py", "content": "import json\nimport uuid\nfrom unittest.mock import AsyncMock, patch, MagicMock, ANY\n\nimport pytest\nfrom protollm_sdk.models.job_context_models import ResponseModel, ChatCompletionTransactionModel, ChatCompletionModel, \\\n    PromptMeta, ChatCompletionUnit, PromptTypes\n\nfrom protollm_api.backend.broker import send_task, get_result\n\n\n@pytest.mark.asyncio\nasync def test_send_task(test_local_config):\n    prompt = ChatCompletionModel(\n        job_id=str(uuid.uuid4()),\n        priority=None,\n        meta=PromptMeta(),\n        messages=[ChatCompletionUnit(role=\"user\", content=\"test request\")]\n    )\n    transaction = ChatCompletionTransactionModel(prompt=prompt, prompt_type=PromptTypes.CHAT_COMPLETION.value)\n\n    mock_rabbit = MagicMock()\n\n    await send_task(test_local_config, test_local_config.queue_name, transaction, mock_rabbit)\n\n    mock_rabbit.publish_message.assert_called_once_with(test_local_config.queue_name, ANY, True)\n\n\n@pytest.mark.asyncio\nasync def test_get_result(test_local_config):\n    redis_mock = MagicMock()\n    redis_mock.wait_item = AsyncMock(return_value=json.dumps({\"content\": \"return test success\"}).encode())\n    task_id = str(uuid.uuid4())\n\n    response = await get_result(test_local_config, task_id, redis_mock)\n\n    redis_mock.wait_item.assert_called_once_with(f\"{test_local_config.redis_prefix}:{task_id}\", timeout=90)\n    assert response == ResponseModel(content=\"return test success\")\n\n\n@pytest.mark.asyncio\nasync def test_get_result_with_exception(test_local_config):\n    redis_mock = MagicMock()\n    redis_mock.wait_item = AsyncMock(\n        side_effect=[Exception(\"Redis error\"), json.dumps({\"content\": \"return test success\"}).encode()])\n    task_id = str(uuid.uuid4())\n\n    response = await get_result(test_local_config, task_id, redis_mock)\n\n    assert redis_mock.wait_item.call_count == 2\n    assert response == ResponseModel(content=\"return test success\")\n"}
{"type": "test_file", "path": "protollm_tools/llm-api/tests/unit/test_endpoints.py", "content": "import json\nfrom unittest.mock import AsyncMock, patch, ANY\n\nimport pytest\nfrom fastapi import FastAPI\nfrom httpx import AsyncClient, ASGITransport\nfrom protollm_sdk.models.job_context_models import ResponseModel, PromptTransactionModel, PromptModel, \\\n    PromptTypes, ChatCompletionModel, ChatCompletionTransactionModel\nfrom protollm_sdk.object_interface import RabbitMQWrapper\n\nfrom protollm_api.backend.endpoints import get_router\n\n\n@pytest.fixture\ndef test_app(test_local_config):\n    app = FastAPI()\n    app.include_router(get_router(test_local_config))\n    return app\n\n\n@pytest.mark.asyncio\nasync def test_generate_endpoint(test_app, test_local_config):\n    async with AsyncClient(transport=ASGITransport(app=test_app), base_url=\"http://testserver\") as client:\n        with patch(\"protollm_api.backend.endpoints.send_task\", new_callable=AsyncMock) as send_task_mock, \\\n                patch(\"protollm_api.backend.endpoints.get_result\", new_callable=AsyncMock) as get_result_mock:\n            get_result_mock.return_value = ResponseModel(content=\"Test Response\")\n\n            prompt = {\n                \"job_id\": \"test-job-id\",\n                \"priority\": 3,\n                \"meta\": {\n                    \"temperature\": 0.2,\n                    \"tokens_limit\": 0,\n                    \"stop_words\": [\n                        \"string\"\n                    ],\n                    \"model\": \"string\"\n                },\n                \"content\": \"string\"\n            }\n\n            response = await client.post(\n                \"/generate\",\n                json=prompt\n            )\n\n            prompt_data = PromptModel.model_validate_json(json.dumps(prompt))\n            transaction_model = ChatCompletionTransactionModel(\n                prompt=ChatCompletionModel.from_prompt_model(prompt_data),\n                prompt_type=PromptTypes.CHAT_COMPLETION.value\n            )\n            send_task_mock.assert_called_once_with(test_local_config, \"llm-api-queue\", transaction_model, ANY)\n\n            get_result_mock.assert_called_once_with(test_local_config, \"test-job-id\", ANY)\n\n            assert response.status_code == 200\n            assert response.json() == {\"content\": \"Test Response\"}\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion_endpoint(test_app, test_local_config):\n    async with AsyncClient(transport=ASGITransport(app=test_app), base_url=\"http://testserver\") as client:\n        with patch(\"protollm_api.backend.endpoints.send_task\", new_callable=AsyncMock) as send_task_mock, \\\n                patch(\"protollm_api.backend.endpoints.get_result\", new_callable=AsyncMock) as get_result_mock:\n            get_result_mock.return_value = ResponseModel(content=\"Test Response\")\n\n            prompt = {\n                \"job_id\": \"test-job-id\",\n                \"meta\": {\n                    \"temperature\": 0.2,\n                    \"tokens_limit\": 0,\n                    \"stop_words\": [\n                        \"string\"\n                    ],\n                    \"model\": \"string\"\n                },\n                \"messages\": [{\"role\": \"user\", \"content\": \"string\"}]\n            }\n\n            response = await client.post(\n                \"/chat_completion\",\n                json=prompt\n            )\n\n            prompt_data = ChatCompletionModel.model_validate_json(json.dumps(prompt))\n            transaction_model = ChatCompletionTransactionModel(\n                prompt=prompt_data,\n                prompt_type=PromptTypes.CHAT_COMPLETION.value\n            )\n            send_task_mock.assert_called_once_with(test_local_config, \"llm-api-queue\", transaction_model, ANY)\n\n            get_result_mock.assert_called_once_with(test_local_config, \"test-job-id\", ANY)\n\n            assert response.status_code == 200\n            assert response.json() == {\"content\": \"Test Response\"}\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/__init__.py", "content": ""}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/celery/test_app.py", "content": "import logging\nimport uuid\n\nimport pytest\n\nfrom protollm_sdk.celery.app import task_test, abstract_task\nfrom protollm_sdk.celery.job import ResultStorageJob\nfrom protollm_sdk.jobs.job import Job\nfrom protollm_sdk.jobs.utility import construct_job_context\n\n\n@pytest.fixture\ndef result_storage():\n    return {\"question\": \"What is the ultimate question answer?\",\n            \"answers\": \"42\"}\n\n@pytest.mark.ci\ndef test_task_test_unknown_job_class(caplog):\n    task_id = str(uuid.uuid4())\n    task_class = \"unknown_class\"\n\n    with pytest.raises(Exception, match=f\"Unknown job class: '{task_class}'\"):\n        task_test(task_class=task_class, task_id=task_id)\n\n    assert f\"Error in task '{task_id}'. Unknown job class: '{task_class}'.\" in caplog.text\n\n@pytest.mark.local\ndef test_task_test_known_job_class(caplog, result_storage):\n    caplog.set_level(logging.INFO)\n    task_id = str(uuid.uuid4())\n    task_class = ResultStorageJob.__name__\n\n    result = task_test(task_class=task_class, task_id=task_id, kwargs=result_storage)\n\n    assert f\"Starting task '{task_id}'. Job '{task_class}'.\" in caplog.text\n    assert result is None\n\n\nclass DummyJob(Job):\n    def __init__(self):\n        self.ran = False\n\n    def run(self, task_id, ctx, **kwargs):\n        self.ran = True\n        self.task_id = task_id\n        self.ctx = ctx\n        self.kwargs = kwargs\n\n\n@pytest.fixture\ndef dummy_job():\n    return DummyJob()\n\n@pytest.mark.ci\ndef test_abstract_task_class_input(caplog, dummy_job):\n    caplog.set_level(\"INFO\")\n\n    task_id = \"test_task_id\"\n    task_class = DummyJob\n\n    abstract_task(task_class=task_class, task_id=task_id, test_arg=\"value\")\n\n    assert f\"Starting task '{task_id}'. Job 'DummyJob'.\" in caplog.text\n\n    assert construct_job_context(\"DummyJob\", abstract_task) is not None\n\n    job_instance = task_class()\n    job_instance.run(task_id=task_id, ctx=construct_job_context(\"DummyJob\", abstract_task), test_arg=\"value\")\n    assert job_instance.ran is True\n    assert job_instance.task_id == task_id\n    assert job_instance.kwargs == {\"test_arg\": \"value\"}\n\n@pytest.mark.ci\ndef test_abstract_task_instance_input(caplog, dummy_job):\n    caplog.set_level(\"INFO\")\n\n    task_id = \"test_task_id\"\n\n    abstract_task(task_class=dummy_job, task_id=task_id, test_arg=\"value\")\n\n    assert f\"Starting task '{task_id}'. Job 'DummyJob'.\" in caplog.text\n\n    assert construct_job_context(\"DummyJob\", abstract_task) is not None\n\n    assert dummy_job.ran is True\n    assert dummy_job.task_id == task_id\n    assert dummy_job.kwargs == {\"test_arg\": \"value\"}\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_job_api.py", "content": "import uuid\n\nimport pytest\n\nfrom protollm_sdk.jobs.utility import construct_job_context\nfrom protollm_sdk.models.job_context_models import PromptModel, ChatCompletionModel\n\n@pytest.fixture\ndef llm_request():\n    random_id = str(uuid.uuid4())\n    prompt_msg = \"What has a head like cat, feet like a kat, tail like a cat, but isn't a cat?\"\n    meta = {\"temperature\": 0.5,\n            \"tokens_limit\": 1000,\n            \"stop_words\": [\"Stop\"]}\n    llm_request = {\"job_id\": random_id,\n                   \"meta\": meta,\n                   \"content\": prompt_msg}\n    request = PromptModel(**llm_request)\n    return request\n\n@pytest.fixture\ndef llm_job_context(llm_request):\n    jc = construct_job_context(llm_request.job_id)\n    return jc\n\n@pytest.mark.local\ndef test_api_interface(llm_request, llm_job_context):\n    response = llm_job_context.llm_api.inference(llm_request)\n    print(response)\n\n@pytest.mark.local\ndef test_api_interface_with_queue_name(llm_request, llm_job_context):\n    response = llm_job_context.llm_api.inference(llm_request, \"wq_outer_vsegpt\")\n    print(response)\n\n@pytest.mark.local\ndef test_api_chat_completion(llm_request, llm_job_context):\n    response = llm_job_context.llm_api.chat_completion(ChatCompletionModel.from_prompt_model(llm_request))\n    print(response)\n\n@pytest.mark.local\ndef test_api_chat_completion_with_queue_name(llm_request, llm_job_context):\n    response = llm_job_context.llm_api.chat_completion(ChatCompletionModel.from_prompt_model(llm_request), \"wq_outer_vsegpt\")\n    print(response)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/__init__.py", "content": ""}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_job.py", "content": "import uuid\n\nimport pytest\n\nfrom protollm_sdk.celery.app import task_test\nfrom protollm_sdk.celery.job import (\n    LLMAPIJob, TextEmbedderJob, ResultStorageJob, VectorDBJob\n)\nfrom protollm_sdk.models.job_context_models import LLMResponse, TextEmbedderResponse\n\n\n@pytest.fixture\ndef llm_request():\n    random_id = uuid.uuid4()\n    prompt_msg = \"What has a head like cat, feet like a kat, tail like a cat, but isn't a cat?\"\n    meta = {\"temperature\": 0.5,\n            \"tokens_limit\": 10,\n            \"stop_words\": [\"Stop\"]}\n    llm_request = {\"job_id\": str(random_id),\n                   \"meta\": meta,\n                   \"content\": prompt_msg}\n    return llm_request\n\n\n@pytest.fixture\ndef text_embedder_request():\n    return {\"job_id\": \"0\",\n            \"inputs\": \"Everybody steals and throws, they cut each other and hang each other... \"\n                      \"In general, normal civilized life is going on. McDonald's everywhere. \"\n                      \"I don't see them here, by the way. That can't be good.\",\n            \"truncate\": False}\n\n\n@pytest.fixture\ndef result_storage():\n    return {\"question\": \"What is the ultimate question answer?\",\n            \"answers\": \"42\"}\n\n\n@pytest.mark.skip(reason=\"LLM die sometimes because stupid questions\")\ndef test_llm_request(llm_request):\n    result = task_test.apply_async(args=(LLMAPIJob.__name__, llm_request[\"job_id\"]), kwargs=llm_request)\n    r = result.get()\n    res = LLMResponse(job_id=llm_request[\"job_id\"], text=r.content)\n    assert isinstance(res, LLMResponse)\n\n@pytest.mark.local\ndef test_text_embedder_request(text_embedder_request):\n    random_id = uuid.uuid4()\n    result = task_test.apply_async(args=(TextEmbedderJob.__name__, random_id), kwargs=text_embedder_request)\n    assert isinstance(result.get(), TextEmbedderResponse)\n\n@pytest.mark.local\ndef test_result_storage(result_storage):\n    random_id = uuid.uuid4()\n    task_test.apply_async(args=(ResultStorageJob.__name__, random_id), kwargs=result_storage)\n\n@pytest.mark.skip(reason=\"We don't have local vector DB\")\ndef test_ping_vector_db():\n    random_id = uuid.uuid4()\n    result = task_test.apply_async(args=(VectorDBJob.__name__, random_id))\n    r = result.get()\n    assert isinstance(r, dict)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_job_invoke.py", "content": "from unittest import mock\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom protollm_sdk.jobs.job_invoke import BlockingJobResult, WorkerJobResult, JobInvoker, InvokeType\n\n\n@pytest.fixture\ndef blocking_job_result():\n    \"\"\"\n    Fixture to create a BlockingJobResult with mocked storage and result functionality.\n    \"\"\"\n    mock_storage = MagicMock()\n\n    mock_storage.for_job.return_value.load_dict.return_value = {\"status\": \"success\", \"job_id\": \"12345\"}\n\n    job_result = BlockingJobResult(storage=mock_storage, job_name=\"test_job\", job_id=\"12345\")\n\n    return job_result, mock_storage\n\n\n@pytest.fixture\ndef worker_job_result():\n    \"\"\"\n    Fixture to create a BlockingJobResult with mocked storage and result functionality.\n    \"\"\"\n    mock_storage = MagicMock()\n\n    mock_storage.for_job.return_value.load_dict.return_value = {\"status\": \"success\", \"job_id\": \"12345\"}\n\n    job_result = WorkerJobResult(storage=mock_storage, job_name=\"test_job\", job_id=\"12345\")\n\n    return job_result, mock_storage\n\n\n@pytest.fixture\ndef job_invoker():\n    mock_task = MagicMock()\n    mock_storage = MagicMock()\n\n    invoker = JobInvoker(\n        abstract_celery_task=mock_task,\n        result_storage=mock_storage,\n        invoke_type=InvokeType.Worker\n    )\n\n    return invoker, mock_task, mock_storage\n\n\n# ---------------------------- Functional test of BlockingJobResult ----------------------------\n\n@pytest.mark.ci\ndef test_blocking_job_result_initialization(blocking_job_result):\n    \"\"\"\n    Test that BlockingJobResult initializes correctly and calls _get_result.\n    \"\"\"\n    blocking_job_result, mock_storage = blocking_job_result\n\n    assert blocking_job_result.job_id == \"12345\"\n    assert isinstance(blocking_job_result.result, dict)\n    mock_storage.for_job.assert_called_once_with(\"test_job\")\n\n@pytest.mark.ci\ndef test_blocking_job_result_get_result(blocking_job_result):\n    \"\"\"\n    Test that get_result returns the correct result.\n    \"\"\"\n    blocking_job_result, mock_storage = blocking_job_result\n\n    result = blocking_job_result.get_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    mock_storage.for_job().load_dict.assert_called_once_with(\"12345\")\n\n@pytest.mark.ci\n@patch('time.sleep', return_value=None)\ndef test_blocking_job_result_timeout(mock_sleep, blocking_job_result):\n    \"\"\"\n    Test that BlockingJobResult raises TimeoutError after the timeout is reached.\n    \"\"\"\n    blocking_job_result, mock_storage = blocking_job_result\n\n    mock_storage.for_job().load_dict.return_value = None\n\n    with patch('time.monotonic', side_effect=[0, 1, 2, 300]):  # Simulate waiting 0/1/2/300 seconds\n        with pytest.raises(TimeoutError, match=\"Couldn't retrieve result in 5min 0sec\"):\n            blocking_job_result._get_result()\n\n    assert mock_sleep.call_count > 0\n\n@pytest.mark.ci\n@patch('time.sleep', return_value=None)\ndef test_blocking_job_result_retries_and_succeeds(mock_sleep, blocking_job_result):\n    \"\"\"\n    Test that BlockingJobResult retries the result retrieval and eventually succeeds.\n    \"\"\"\n    blocking_job_result, mock_storage = blocking_job_result\n\n    mock_storage.for_job().load_dict.side_effect = [None, None, None, {\"status\": \"success\", \"job_id\": \"12345\"}]\n\n    result = blocking_job_result._get_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    assert mock_storage.for_job().load_dict.call_count == 5\n\n@pytest.mark.ci\ndef test_blocking_job_ping_result(blocking_job_result):\n    \"\"\"\n    Test that _ping_result calls the storage and returns the correct result.\n    \"\"\"\n    blocking_job_result, mock_storage = blocking_job_result\n\n    mock_storage.load_dict.return_value = {\"status\": \"success\", \"job_id\": \"12345\"}\n\n    result = blocking_job_result._ping_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    mock_storage.for_job().load_dict.assert_called_with(\"12345\")\n    assert mock_storage.for_job().load_dict.call_count == 2\n\n\n# ---------------------------- Functional test of WorkerJobResult ----------------------------\n\n@pytest.mark.ci\ndef test_worker_job_result_initialization(worker_job_result):\n    \"\"\"\n    Test that WorkerJobResult initializes correctly and calls _get_result.\n    \"\"\"\n    worker_job_result, mock_storage = worker_job_result\n\n    assert worker_job_result.job_id == \"12345\"\n    mock_storage.for_job.assert_called_once_with(\"test_job\")\n\n@pytest.mark.ci\ndef test_worker_job_result_get_result(worker_job_result):\n    \"\"\"\n    Test that get_result returns the correct result.\n    \"\"\"\n    worker_job_result, mock_storage = worker_job_result\n\n    result = worker_job_result.get_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    mock_storage.for_job().load_dict.assert_called_once_with(\"12345\")\n\n@pytest.mark.ci\n@patch('time.sleep', return_value=None)\ndef test_worker_job_result_timeout(mock_sleep, worker_job_result):\n    \"\"\"\n    Test that BlockingJobResult raises TimeoutError after the timeout is reached.\n    \"\"\"\n    worker_job_result, mock_storage = worker_job_result\n\n    mock_storage.for_job().load_dict.return_value = None\n\n    with patch('time.monotonic', side_effect=[0, 1, 2, 300, 600]):  # Simulate waiting 0/1/2/300 seconds\n        with pytest.raises(TimeoutError, match=\"Couldn't retrieve result in 10min 0sec\"):\n            worker_job_result.get_result()\n\n    assert mock_sleep.call_count > 0\n\n@pytest.mark.ci\n@patch('time.sleep', return_value=None)\ndef test_worker_job_result_retries_and_succeeds(mock_sleep, worker_job_result):\n    \"\"\"\n    Test that BlockingJobResult retries the result retrieval and eventually succeeds.\n    \"\"\"\n    worker_job_result, mock_storage = worker_job_result\n\n    mock_storage.for_job().load_dict.side_effect = [None, None, None, {\"status\": \"success\", \"job_id\": \"12345\"}]\n\n    result = worker_job_result.get_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    assert mock_storage.for_job().load_dict.call_count == 4\n\n@pytest.mark.ci\ndef test_worker_job_ping_result(worker_job_result):\n    \"\"\"\n    Test that _ping_result calls the storage and returns the correct result.\n    \"\"\"\n    worker_job_result, mock_storage = worker_job_result\n\n    mock_storage.load_dict.return_value = {\"status\": \"success\", \"job_id\": \"12345\"}\n\n    result = worker_job_result._ping_result()\n\n    assert result == {\"status\": \"success\", \"job_id\": \"12345\"}\n    mock_storage.for_job().load_dict.assert_called_with(\"12345\")\n    assert mock_storage.for_job().load_dict.call_count == 1\n\n\n# ---------------------------- Functional test of JobInvoke ----------------------------\n\n@pytest.mark.ci\ndef test_job_invoker_worker_success(job_invoker):\n    \"\"\"\n    Test JobInvoker with InvokeType.Worker.\n    \"\"\"\n    invoker, mock_task, mock_storage = job_invoker\n\n    mock_job_class = MagicMock()\n\n    result = invoker.invoke(mock_job_class)\n\n    mock_task.apply_async.assert_called_once_with(args=(mock_job_class, mock.ANY), kwargs={})\n    assert isinstance(result, WorkerJobResult)\n\n@pytest.mark.ci\ndef test_job_invoker_blocking_success(job_invoker):\n    \"\"\"\n    Test JobInvoker with InvokeType.Blocking.\n    \"\"\"\n    invoker, mock_task, mock_storage = job_invoker\n\n    invoker._invoke_type = InvokeType.Blocking\n\n    mock_job_class = MagicMock()\n\n    result = invoker.invoke(mock_job_class)\n\n    mock_task.assert_called_once_with(mock_job_class, mock.ANY)\n    assert isinstance(result, BlockingJobResult)\n\n@pytest.mark.ci\ndef test_job_invoker_no_task(job_invoker):\n    \"\"\"\n    Test JobInvoker when no task is provided.\n    \"\"\"\n    invoker, mock_task, mock_storage = job_invoker\n\n    invoker._task = None\n\n    mock_job_class = MagicMock()\n\n    with pytest.raises(Exception, match=\"Calling JobInvoker without abstract task.\"):\n        invoker.invoke(mock_job_class)\n\n@pytest.mark.ci\ndef test_job_invoker_unknown_invoke_type(job_invoker):\n    \"\"\"\n    Test JobInvoker with an unknown InvokeType.\n    \"\"\"\n    invoker, mock_task, mock_storage = job_invoker\n\n    invoker._invoke_type = \"UnknownType\"\n\n    mock_job_class = MagicMock()\n\n    with pytest.raises(NotImplementedError, match=\"Unknown invoke type\"):\n        invoker.invoke(mock_job_class)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_result_storage.py", "content": "import json\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom protollm_sdk.jobs.result_storage import ResultStorage\n\n\n@pytest.fixture\ndef mock_redis():\n    with patch(\"redis.Redis.from_url\", return_value=MagicMock()) as mock_redis:\n        yield mock_redis\n\n\n@pytest.fixture\ndef result_storage():\n    return ResultStorage(redis_host=\"localhost\", redis_port=6379, prefix=\"test_job\")\n\n@pytest.mark.ci\ndef test_for_job(result_storage):\n    \"\"\"\n    Test that for_job correctly creates a new ResultStorage instance with a new job prefix.\n    \"\"\"\n    with patch('protollm_sdk.jobs.result_storage.ResultStorage') as mock_result_storage:\n        new_storage = result_storage.for_job(\"new_job\")\n\n        mock_result_storage.assert_called_once_with(redis_host=\"redis://localhost:6379\", redis_port=None,\n                                                    prefix=\"new_job\")\n\n        assert new_storage is mock_result_storage.return_value\n\n@pytest.mark.ci\ndef test_build_key(result_storage):\n    \"\"\"\n    Test the build_key method for formatting keys.\n    \"\"\"\n    key = result_storage.build_key(job_id=\"12345\", prefix=\"test_prefix\")\n    assert key == \"test_prefix:12345\"\n\n    key_without_prefix = result_storage.build_key(job_id=\"12345\", prefix=None)\n    assert key_without_prefix == \"12345\"\n\n@pytest.mark.ci\ndef test_save_dict_success(mock_redis, result_storage):\n    \"\"\"\n    Test saving a dictionary to Redis.\n    \"\"\"\n    mock_redis_instance = mock_redis.return_value\n\n    result_storage.save_dict(\"12345\", {\"status\": \"success\"})\n\n    mock_redis_instance.set.assert_called_once_with(\"test_job:12345\", json.dumps({\"status\": \"success\"}))\n\n@pytest.mark.ci\ndef test_save_dict_failure(mock_redis, result_storage):\n    \"\"\"\n    Test failure when saving a dictionary to Redis.\n    \"\"\"\n    mock_redis_instance = mock_redis.return_value\n\n    mock_redis_instance.set.side_effect = Exception(\"Redis Error\")\n\n    with pytest.raises(Exception, match=\"Saving the result with the key test_job:12345 has been interrupted\"):\n        result_storage.save_dict(\"12345\", {\"status\": \"success\"})\n\n    mock_redis_instance.set.assert_called_once_with(\"test_job:12345\", json.dumps({\"status\": \"success\"}))\n\n@pytest.mark.ci\ndef test_load_bytes_or_str_dict_success(mock_redis, result_storage):\n    \"\"\"\n    Test loading a dictionary from Redis.\n    \"\"\"\n    mock_redis_instance = mock_redis.return_value\n\n    mock_redis_instance.get.return_value = json.dumps({\"status\": \"success\"})\n\n    result = result_storage.load_dict(\"12345\")\n\n    mock_redis_instance.get.assert_called_once_with(\"test_job:12345\")\n\n    assert result == {\"status\": \"success\"}\n\n@pytest.mark.ci\ndef test_load_not_bytes_or_str_dict_success(mock_redis, result_storage):\n    \"\"\"\n    Test loading a dictionary from Redis.\n    \"\"\"\n    mock_redis_instance = mock_redis.return_value\n\n    mock_redis_instance.get.return_value = 123\n\n    result = result_storage.load_dict(\"12345\")\n\n    mock_redis_instance.get.assert_called_once_with(\"test_job:12345\")\n\n    assert result == 123\n\n@pytest.mark.ci\ndef test_load_dict_failure(mock_redis, result_storage):\n    \"\"\"\n    Test failure when loading a dictionary from Redis.\n    \"\"\"\n    mock_redis_instance = mock_redis.return_value\n\n    mock_redis_instance.get.side_effect = Exception(\"Redis Error\")\n\n    with pytest.raises(Exception, match=\"Failed to load the result with the key test_job:12345 from redis.\"):\n        result_storage.load_dict(\"12345\")\n\n    mock_redis_instance.get.assert_called_once_with(\"test_job:12345\")\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_text_embedder.py", "content": "import uuid\nfrom unittest.mock import patch\n\nimport httpx\nimport pytest\n\nfrom protollm_sdk.config import Config\nfrom protollm_sdk.jobs.text_embedder import TextEmbedder\nfrom protollm_sdk.models.job_context_models import TextEmbedderRequest, TextEmbedderResponse\n\n\n# ---------------------------- Initialisations tests ----------------------------\n@pytest.mark.ci\n@pytest.mark.parametrize(\n    \"host, port, expected_path\",\n    [\n        (\"localhost\", 8080, \"http://localhost:8080\"),\n        (\"example.com\", None, \"http://example.com\"),\n        (\"127.0.0.1\", 9942, \"http://127.0.0.1:9942\"),\n        (\"text-embedder.com\", \"6672\", \"http://text-embedder.com:6672\"),\n    ]\n)\ndef test_text_embedder_initialization(host, port, expected_path):\n    \"\"\"\n    Tests that the URL is correctly initialized with different values of text_emb_host and text_emb_port.\n    \"\"\"\n    text_embedder = TextEmbedder(text_emb_host=host, text_emb_port=port)\n    assert text_embedder.path == expected_path\n    assert isinstance(text_embedder.client, httpx.Client)\n\n@pytest.mark.ci\ndef test_text_embedder_timeout_default():\n    \"\"\"\n    Tests that the default timeout value is correctly set.\n    \"\"\"\n    text_embedder = TextEmbedder(text_emb_host=\"localhost\", text_emb_port=8080)\n    assert text_embedder.timeout_sec == 10 * 60  # Default timeout is 10 minutes\n\n@pytest.mark.ci\n@pytest.mark.parametrize(\n    \"custom_timeout\",\n    [30, 60, 120]\n)\ndef test_text_embedder_custom_timeout(custom_timeout):\n    \"\"\"\n    Tests that the custom timeout value is correctly set.\n    \"\"\"\n    text_embedder = TextEmbedder(text_emb_host=\"localhost\", text_emb_port=8080, timeout_sec=custom_timeout)\n    assert text_embedder.timeout_sec == custom_timeout\n\n@pytest.mark.ci\ndef test_text_embedder_client_initialization():\n    \"\"\"\n    Tests that httpx.Client is initialized when the object is created.\n    \"\"\"\n    text_embedder = TextEmbedder(text_emb_host=\"localhost\", text_emb_port=8080)\n    assert isinstance(text_embedder.client, httpx.Client)\n\n\n# ---------------------------- Fixtures ----------------------------\n\n@pytest.fixture\ndef text_embedder():\n    \"\"\"\n    Fixture to create a TextEmbedder instance using environment variables.\n    \"\"\"\n    return TextEmbedder(text_emb_host=Config.text_embedder_host, text_emb_port=Config.text_embedder_port)\n\n\n@pytest.fixture\ndef text_embedder_request():\n    \"\"\"\n    Fixture to create a sample TextEmbedderRequest for testing.\n    \"\"\"\n    data = {\n        \"job_id\": str(uuid.uuid4()),\n        \"inputs\": \"A Greek was crossing the river when he saw a crab in the water.\",\n        \"truncate\": False\n    }\n    return TextEmbedderRequest(**data)\n\n\n# ---------------------------- Function Tests ----------------------------\n@pytest.mark.local\ndef test_text_embedder_inference(text_embedder, text_embedder_request):\n    \"\"\"\n    Tests that the inference method returns a valid TextEmbedderResponse.\n    \"\"\"\n    res = text_embedder.inference(text_embedder_request)\n    assert isinstance(res, TextEmbedderResponse)\n\n@pytest.mark.ci\ndef test_text_embedder_connection_error(text_embedder, text_embedder_request):\n    \"\"\"\n    Tests that a ConnectionError is raised when the server returns a 500 status code.\n    \"\"\"\n    with patch.object(text_embedder.client, 'post') as mock_post:\n        mock_post.return_value.status_code = 500\n        with pytest.raises(Exception, match='The embedding was interrupted. Error: The LLM server is not available.'):\n            text_embedder.inference(text_embedder_request)\n\n@pytest.mark.ci\ndef test_text_embedder_validation_error(text_embedder, text_embedder_request):\n    \"\"\"\n    Tests that a ValueError is raised when the server returns a 422 status code.\n    \"\"\"\n    with patch.object(text_embedder.client, 'post') as mock_post:\n        mock_post.return_value.status_code = 422\n        mock_post.return_value.json.return_value = {\"detail\": \"Validation failed\"}\n        with pytest.raises(Exception,\n                           match=\"The embedding was interrupted. Error: Data model validation error. {'detail': 'Validation failed'}\"):\n            text_embedder.inference(text_embedder_request)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_utility.py", "content": "import os\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom protollm_sdk.config import Config\nfrom protollm_sdk.jobs.job_context import JobContext\nfrom protollm_sdk.jobs.job_invoke import InvokeType\nfrom protollm_sdk.jobs.utility import construct_job_context\n\n@pytest.mark.ci\ndef test_construct_job_context_real():\n    \"\"\"\n    Test construct_job_context function with real environment variables and connections.\n    This test assumes that all services (Redis, LLM API, Text Embedder, etc.) are properly running.\n    \"\"\"\n    job_name = \"test_job\"\n    job_context = construct_job_context(job_name)\n\n    assert isinstance(job_context, JobContext)\n\n    assert job_context.llm_api is not None\n    assert job_context.outer_llm_api is not None\n    assert job_context.text_embedder is not None\n    assert job_context.result_storage is not None\n    assert job_context.vector_db is not None\n    assert job_context.job_invoker is not None\n\n@pytest.mark.ci\ndef test_construct_job_context_with_invoke_type_worker():\n    \"\"\"\n    Test construct_job_context with a missing environment variable.\n    \"\"\"\n    with patch.dict(os.environ, {\"JOB_INVOCATION_TYPE\": \"worker\"}):\n        assert os.getenv(\"JOB_INVOCATION_TYPE\") == \"worker\"\n\n        job_name = \"test_job\"\n        job_context = construct_job_context(job_name)\n        assert job_context.job_invoker._invoke_type == InvokeType.Worker\n\n@pytest.mark.ci\ndef test_construct_job_context_with_invoke_type_blocking():\n    \"\"\"\n    Test construct_job_context with a missing environment variable.\n    \"\"\"\n    with patch.dict(os.environ, {\"JOB_INVOCATION_TYPE\": \"blocking\"}):\n        assert os.getenv(\"JOB_INVOCATION_TYPE\") == \"blocking\"\n        Config.reload_invocation_type()\n        job_name = \"test_job\"\n        job_context = construct_job_context(job_name)\n        assert job_context.job_invoker._invoke_type == InvokeType.Blocking\n\n    Config.reload_invocation_type()\n\n@pytest.mark.ci\ndef test_construct_job_context_with_wrong_invoke_type():\n    \"\"\"\n    Test construct_job_context with a missing environment variable.\n    \"\"\"\n    with patch.dict(os.environ, {\"JOB_INVOCATION_TYPE\": \"cringe\"}):\n        assert os.getenv(\"JOB_INVOCATION_TYPE\") == \"cringe\"\n        Config.reload_invocation_type()\n\n        with pytest.raises(ValueError, match=\"Found unknown invocation type 'cringe'.\"):\n            job_name = \"test_job\"\n            construct_job_context(job_name)\n\n    Config.reload_invocation_type()\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_vector_db.py", "content": "from unittest.mock import patch, MagicMock\nfrom urllib.parse import urljoin\n\nimport httpx\nimport pytest\n\nfrom protollm_sdk.jobs.vector_db import VectorDB\n\n@pytest.mark.ci\ndef test_vector_db_initialization_without_port():\n    \"\"\"\n    Test that VectorDB is initialized correctly without a port.\n    \"\"\"\n    vector_db = VectorDB(\"localhost\")\n    assert vector_db.url == \"http://localhost\"\n    assert isinstance(vector_db.client, httpx.Client)\n\n@pytest.mark.ci\ndef test_vector_db_initialization_with_port():\n    \"\"\"\n    Test that VectorDB is initialized correctly with a port.\n    \"\"\"\n    vector_db_with_port = VectorDB(\"localhost\", 8080)\n    assert vector_db_with_port.url == \"http://localhost:8080\"\n    assert isinstance(vector_db_with_port.client, httpx.Client)\n\n@pytest.mark.ci\n@patch('httpx.Client.get')\ndef test_vector_db_api_v1(mock_get):\n    \"\"\"\n    Test the api_v1 method of VectorDB class.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.json.return_value = {\"status\": \"success\"}\n    mock_get.return_value = mock_response\n\n    vector_db = VectorDB(\"localhost\", 8080)\n\n    response = vector_db.api_v1()\n\n    mock_get.assert_called_once_with(\n        urljoin(\"http://localhost:8080\", \"/api/v1\"),\n        headers={\"Content-type\": \"application/json\"},\n        timeout=10 * 60\n    )\n\n    assert response == {\"status\": \"success\"}\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/models/test_job_context.py", "content": "import pytest\n\nfrom protollm_sdk.models.job_context_models import PromptModel, PromptMeta, ChatCompletionModel\n\n@pytest.mark.ci\ndef test_from_prompt_model():\n    prompt_model = PromptModel(\n        job_id=\"test_job_123\",\n        meta=PromptMeta(\n            temperature=0.5,\n            tokens_limit=100,\n            stop_words=[\"stop\", \"words\"],\n            model=\"gpt-3\"\n        ),\n        content=\"This is a test prompt\"\n    )\n\n    chat_completion = ChatCompletionModel.from_prompt_model(prompt_model)\n\n    assert chat_completion.job_id == prompt_model.job_id\n    assert chat_completion.meta == prompt_model.meta\n\n    assert len(chat_completion.messages) == 1\n\n    assert chat_completion.messages[0].role == \"user\"\n    assert chat_completion.messages[0].content == prompt_model.content\n\n    assert chat_completion.meta.temperature == 0.5\n    assert chat_completion.meta.tokens_limit == 100\n    assert chat_completion.meta.stop_words == [\"stop\", \"words\"]\n    assert chat_completion.meta.model == \"gpt-3\"\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_llm_api.py", "content": "from unittest.mock import patch, MagicMock\nfrom urllib.parse import urljoin\n\nimport pytest\n\nfrom protollm_sdk.jobs.llm_api import LLMAPI\nfrom protollm_sdk.models.job_context_models import PromptModel, ResponseModel, ChatCompletionModel\n\n\n@pytest.fixture\ndef llm_api():\n    \"\"\"\n    Fixture for creating an LLMAPI object.\n    \"\"\"\n    return LLMAPI(\"localhost\", 8080)\n\n\n@pytest.fixture\ndef mock_post():\n    \"\"\"\n    Fixture for mocking the httpx.Client.post method.\n    \"\"\"\n    with patch('httpx.Client.post') as mock_post:\n        yield mock_post\n\n@pytest.mark.ci\ndef test_llmapi_initialization():\n    \"\"\"\n    Test that LLMAPI is initialized correctly with and without a port.\n    \"\"\"\n    llm_api_no_port = LLMAPI(\"localhost\")\n    assert llm_api_no_port.path == \"http://localhost\"\n\n    llm_api_with_port = LLMAPI(\"localhost\", 8080)\n    assert llm_api_with_port.path == \"http://localhost:8080\"\n\n@pytest.mark.ci\ndef test_llmapi_inference_success(mock_post, llm_api):\n    \"\"\"\n    Test inference method of LLMAPI for a successful response.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {\"content\": \"Test success\"}\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n    mock_request.model_dump_json.return_value = '{\"prompt\": \"test prompt\"}'\n\n    response = llm_api.inference(mock_request)\n\n    mock_post.assert_called_once_with(\n        urljoin(\"http://localhost:8080\", \"/generate\"),\n        headers={\"Content-type\": \"application/json\"},\n        params={},\n        data='{\"prompt\": \"test prompt\"}',\n        timeout=10 * 60\n    )\n\n    assert isinstance(response, ResponseModel)\n\n@pytest.mark.ci\ndef test_llmapi_inference_server_error(mock_post, llm_api):\n    \"\"\"\n    Test inference method of LLMAPI for a 500 error.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 500\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n\n    with pytest.raises(Exception,\n                       match='The response generation has been interrupted. Error: The LLM server is not available..'):\n        llm_api.inference(mock_request)\n\n@pytest.mark.ci\ndef test_llmapi_inference_validation_error(mock_post, llm_api):\n    \"\"\"\n    Test inference method of LLMAPI for a 422 validation error.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 422\n    mock_response.json.return_value = {\"detail\": \"Validation failed\"}\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n\n    with pytest.raises(Exception,\n                       match=\"The response generation has been interrupted. Error: Data model validation error..\"):\n        llm_api.inference(mock_request)\n\n@pytest.mark.ci\ndef test_llmapi_chat_completion_success(mock_post, llm_api):\n    \"\"\"\n    Test chat_completion method of LLMAPI for a successful response.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {\"content\": \"Test chat success\"}\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=ChatCompletionModel)\n    mock_request.model_dump_json.return_value = '{\"chat\": \"test chat\"}'\n\n    response = llm_api.chat_completion(mock_request)\n\n    mock_post.assert_called_once_with(\n        urljoin(\"http://localhost:8080\", \"/chat_completion\"),\n        headers={\"Content-type\": \"application/json\"},\n        params={},\n        data='{\"chat\": \"test chat\"}',\n        timeout=10 * 60\n    )\n\n    assert isinstance(response, ResponseModel)\n\n@pytest.mark.ci\ndef test_llmapi_chat_completion_server_error(mock_post, llm_api):\n    \"\"\"\n    Test inference method of LLMAPI for a 500 error.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 500\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n\n    with pytest.raises(Exception,\n                       match='The response generation has been interrupted. Error: The LLM server is not available..'):\n        llm_api.chat_completion(mock_request)\n\n@pytest.mark.ci\ndef test_llmapi_chat_completion_validation_error(mock_post, llm_api):\n    \"\"\"\n    Test inference method of LLMAPI for a 422 validation error.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.status_code = 422\n    mock_response.json.return_value = {\"detail\": \"Validation failed\"}\n    mock_post.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n\n    with pytest.raises(Exception,\n                       match=\"The response generation has been interrupted. Error: Data model validation error..\"):\n        llm_api.chat_completion(mock_request)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/integration/__init__.py", "content": ""}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/integration/test_redis_wrapper.py", "content": "import asyncio\nimport json\nimport time\n\nimport pytest\n\nfrom protollm_sdk.object_interface import RedisWrapper\n\n\n@pytest.fixture\ndef redis_wrapper():\n    return RedisWrapper(redis_host=\"localhost\", redis_port=6379)\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_redis(redis_wrapper):\n    with redis_wrapper._get_redis() as redis:\n        redis.flushall()\n\n@pytest.mark.local\ndef test_save_item_publishes_message(redis_wrapper):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    with redis_wrapper._get_redis() as redis:\n        pubsub = redis.pubsub()\n        pubsub.subscribe(key)\n\n        redis_wrapper.save_item(key, item)\n\n        message = None\n        timeout = 5\n        start_time = time.time()\n\n        while time.time() - start_time < timeout:\n            message = pubsub.get_message(ignore_subscribe_messages=True, timeout=0.1)\n            if message:\n                break\n\n        assert message is not None, \"Сообщение не получено из канала в течение таймаута\"\n        assert message[\"channel\"] == key.encode(\"utf-8\")\n        assert message[\"data\"] == b\"set\"\n\n@pytest.mark.local\ndef test_get_item_raises_exception(redis_wrapper):\n    \"\"\"Тестирует, что get_item выбрасывает исключение при ошибке.\"\"\"\n    key = \"test_key\"\n\n    redis_wrapper.url = \"redis://invalid_host:6379\"\n    with pytest.raises(Exception, match=f\"The receipt of the element with the {key} prefix was interrupted\"):\n        redis_wrapper.get_item(key)\n\n@pytest.mark.local\ndef test_save_and_get_item(redis_wrapper):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    redis_wrapper.save_item(key, item)\n    retrieved_item = redis_wrapper.get_item(key)\n    assert retrieved_item == b'{\"field\": \"value\"}'\n    assert json.loads(retrieved_item) == item\n\n@pytest.mark.local\ndef test_save_item_raises_exception(redis_wrapper):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    redis_wrapper.url = \"redis://invalid_host:6379\"\n    with pytest.raises(Exception, match=f\"Saving the result with the {key} prefix has been interrupted\"):\n        redis_wrapper.save_item(key, item)\n\n@pytest.mark.local\ndef test_check_key(redis_wrapper):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    assert redis_wrapper.check_key(key) is False\n    redis_wrapper.save_item(key, item)\n    assert redis_wrapper.check_key(key) is True\n\n@pytest.mark.local\ndef test_check_key_raises_exception(redis_wrapper):\n    key = \"test_key\"\n    redis_wrapper.url = \"redis://invalid_host:6379\"\n    with pytest.raises(Exception, match=f\"An error occurred while processing the {key} key\"):\n        redis_wrapper.check_key(key)\n\n@pytest.mark.local\n@pytest.mark.asyncio\nasync def test_wait_item(redis_wrapper):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n    wait_task = asyncio.create_task(redis_wrapper.wait_item(key, timeout=5))\n    await asyncio.sleep(1)\n\n    with redis_wrapper._get_redis() as redis:\n        redis.set(key, json.dumps(item))\n        redis.publish(key, 'set')\n\n    result = await wait_task\n    assert result == b'{\"field\": \"value\"}'\n\n@pytest.mark.local\n@pytest.mark.asyncio\nasync def test_wait_item_raises_exception(redis_wrapper):\n    key = \"test_key\"\n    redis_wrapper.url = \"redis://invalid_host:6379\"\n    with pytest.raises(Exception, match=f\"The receipt of the element with the {key} prefix was interrupted\"):\n        await redis_wrapper.wait_item(key, timeout=1)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/integration/test_rabbit_mq_wrapper.py", "content": "import pika\nimport pytest\nimport json\nfrom time import sleep\nimport threading\nfrom protollm_sdk.object_interface import RabbitMQWrapper\n\n@pytest.fixture(scope=\"module\")\ndef rabbit_wrapper():\n    \"\"\"\n    Fixture to initialize RabbitMQWrapper with local RabbitMQ.\n    \"\"\"\n    rabbit_host = \"localhost\"\n    rabbit_port = 5672\n    rabbit_user = \"admin\"\n    rabbit_password = \"admin\"\n    wrapper = RabbitMQWrapper(\n        rabbit_host=rabbit_host,\n        rabbit_port=rabbit_port,\n        rabbit_user=rabbit_user,\n        rabbit_password=rabbit_password,\n    )\n    return wrapper\n\n@pytest.fixture(autouse=True)\ndef cleanup_queues(rabbit_wrapper):\n    \"\"\"\n    Fixture to clean up all queues before each test.\n    \"\"\"\n    with rabbit_wrapper.get_channel() as channel:\n        channel.queue_declare(queue=\"test_priority_queue\", durable=True, arguments={\"x-max-priority\": 5})\n        channel.queue_purge(\"test_priority_queue\")\n\n@pytest.mark.local\ndef test_publish_message_with_priority(rabbit_wrapper):\n    \"\"\"\n    Tests successful message publishing to a queue with priority.\n    \"\"\"\n    queue_name = \"test_priority_queue\"\n    priority = 3\n    message = {\"kwargs\": {\"prompt\": {\"priority\": 3}}}\n\n    rabbit_wrapper.publish_message(queue_name, message)\n\n    with rabbit_wrapper.get_channel() as channel:\n        method_frame, header_frame, body = channel.basic_get(queue_name, auto_ack=True)\n        assert method_frame is not None, \"Message not found in the queue\"\n        assert json.loads(body) == message, \"Message in the queue does not match the sent message\"\n        assert header_frame.priority == priority, f\"Expected priority {priority}, but got {header_frame.priority}\"\n\n\n@pytest.mark.local\ndef test_consume_message(rabbit_wrapper):\n    \"\"\"\n    Tests successful message consumption from a queue and stopping consuming.\n    \"\"\"\n    queue_name = \"test_priority_queue\"\n    priority = 3\n    message = {\"kwargs\": {\"prompt\": {\"priority\": 3}}}\n\n    consumed_messages = []\n\n    def callback(ch, method, properties, body):\n        consumed_messages.append(json.loads(body))\n\n    consuming_thread = threading.Thread(\n        target=rabbit_wrapper.consume_messages, args=(queue_name, callback)\n    )\n    consuming_thread.daemon = True\n    consuming_thread.start()\n\n    rabbit_wrapper.publish_message(queue_name, message)\n\n    sleep(2)\n\n    assert len(consumed_messages) == 1, \"Message was not consumed\"\n    assert consumed_messages[0] == message, \"Consumed message does not match the sent message\"\n\n@pytest.mark.local\ndef test_publish_message_exception(rabbit_wrapper):\n    \"\"\"\n    Tests exception handling during message publishing.\n    \"\"\"\n    queue_name = \"test_queue\"\n    message = {\"key\": \"value\"}\n\n    invalid_wrapper = RabbitMQWrapper(\n        rabbit_host=\"invalid_host\",\n        rabbit_port=5672,\n        rabbit_user=\"guest\",\n        rabbit_password=\"guest\",\n    )\n    with pytest.raises(Exception, match=\"Failed to publish message\"):\n        invalid_wrapper.publish_message(queue_name, message)\n\n@pytest.mark.local\ndef test_consume_message_exception(rabbit_wrapper):\n    \"\"\"\n    Tests exception handling during message consumption.\n    \"\"\"\n    queue_name = \"test_queue\"\n\n    invalid_wrapper = RabbitMQWrapper(\n        rabbit_host=\"invalid_host\",\n        rabbit_port=5672,\n        rabbit_user=\"guest\",\n        rabbit_password=\"guest\",\n    )\n\n    def callback(ch, method, properties, body):\n        pass\n\n    with pytest.raises(Exception, match=\"Failed to consume messages\"):\n        invalid_wrapper.consume_messages(queue_name, callback)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/unit/test_rabbit_mq_wrapper.py", "content": "import pika\nimport pytest\nimport json\nfrom unittest.mock import MagicMock, patch\nfrom protollm_sdk.object_interface import RabbitMQWrapper\n\n@pytest.fixture\ndef mock_pika():\n    with patch(\"pika.BlockingConnection\") as mock_connection:\n        mock_channel = MagicMock()\n        mock_connection.return_value.channel.return_value = mock_channel\n        yield mock_channel\n\n@pytest.fixture\ndef rabbit_wrapper(mock_pika):\n    return RabbitMQWrapper(\n        rabbit_host=\"localhost\",\n        rabbit_port=5672,\n        rabbit_user=\"admin\",\n        rabbit_password=\"admin\",\n    )\n\n@pytest.mark.ci\ndef test_publish_message_with_priority(rabbit_wrapper, mock_pika):\n    \"\"\"\n    Tests successful message publishing to a queue with priority.\n    \"\"\"\n    queue_name = \"test_queue\"\n    message = {\"kwargs\": {\"prompt\": {\"priority\": 3}}}\n\n\n    rabbit_wrapper.publish_message(queue_name, message)\n\n    mock_pika.queue_declare.assert_called_once_with(\n        queue=queue_name,\n        durable=True,\n        arguments={\"x-max-priority\": 5}\n    )\n\n    mock_pika.basic_publish.assert_called_once_with(\n        exchange=\"\",\n        routing_key=queue_name,\n        body=json.dumps(message),\n        properties=pika.BasicProperties(\n            delivery_mode=2,\n            priority=3\n        ),\n    )\n\n@pytest.mark.ci\ndef test_consume_message(rabbit_wrapper, mock_pika):\n    queue_name = \"test_queue\"\n    callback = MagicMock()\n\n    rabbit_wrapper.consume_messages(queue_name, callback)\n\n    mock_pika.queue_declare.assert_called_once_with(queue=queue_name, durable=True, arguments={'x-max-priority': 5})\n    mock_pika.basic_consume.assert_called_once_with(\n        queue=queue_name,\n        on_message_callback=callback,\n        auto_ack=True,\n    )\n    mock_pika.start_consuming.assert_called_once()\n\n@pytest.mark.ci\ndef test_publish_message_exception(mock_pika):\n    mock_pika.basic_publish.side_effect = Exception(\"Mocked exception\")\n    rabbit_wrapper = RabbitMQWrapper(\n        rabbit_host=\"localhost\",\n        rabbit_port=5672,\n        rabbit_user=\"admin\",\n        rabbit_password=\"admin\",\n    )\n\n    queue_name = \"test_queue\"\n    message = {\"key\": \"value\"}\n\n    with pytest.raises(Exception, match=\"Failed to publish message\"):\n        rabbit_wrapper.publish_message(queue_name, message)\n\n@pytest.mark.ci\ndef test_consume_message_exception(mock_pika):\n    mock_pika.start_consuming.side_effect = Exception(\"Mocked exception\")\n    rabbit_wrapper = RabbitMQWrapper(\n        rabbit_host=\"localhost\",\n        rabbit_port=5672,\n        rabbit_user=\"admin\",\n        rabbit_password=\"admin\",\n    )\n\n    queue_name = \"test_queue\"\n    callback = MagicMock()\n\n    with pytest.raises(Exception, match=\"Failed to consume messages\"):\n        rabbit_wrapper.consume_messages(queue_name, callback)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/unit/__init__.py", "content": ""}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/object_interface/unit/test_redis_wrapper.py", "content": "import asyncio\nimport json\nimport time\nfrom unittest.mock import MagicMock, patch, AsyncMock, Mock\nimport pytest\n\nfrom protollm_sdk.object_interface import RedisWrapper\n\n@pytest.fixture\ndef mock_redis():\n    with patch(\"redis.Redis.from_url\", return_value=MagicMock()) as mock_redis:\n        yield mock_redis\n\n@pytest.fixture\ndef mock_async_redis():\n    with patch(\"aioredis.from_url\", return_value=AsyncMock()) as mock_redis:\n        yield mock_redis\n\n@pytest.fixture\ndef redis_wrapper():\n    return RedisWrapper(redis_host=\"localhost\", redis_port=6379)\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_redis(redis_wrapper):\n    with patch.object(redis_wrapper, '_get_redis', return_value=MagicMock()) as mock_redis:\n        mock_redis().flushall()\n\n@pytest.mark.ci\ndef test_save_item_publishes_message(redis_wrapper, mock_redis):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    mock_redis_instance = mock_redis.return_value\n    pubsub = MagicMock()\n    mock_redis_instance.pubsub.return_value = pubsub\n    pubsub.subscribe.return_value = None\n\n    mock_message = {\n        \"channel\": key.encode(\"utf-8\"),\n        \"data\": b\"set\"\n    }\n    pubsub.get_message.return_value = mock_message\n\n    redis_wrapper.save_item(key, item)\n\n    message = None\n    timeout = 5\n    start_time = time.time()\n\n    while time.time() - start_time < timeout:\n        message = pubsub.get_message(ignore_subscribe_messages=True, timeout=0.1)\n        if message:\n            break\n\n    assert message is not None, \"Message was not received from the channel within the timeout\"\n    assert message[\"channel\"] == key.encode(\"utf-8\")\n    assert message[\"data\"] == b\"set\"\n\n@pytest.mark.ci\ndef test_get_item_raises_exception(redis_wrapper, mock_redis):\n    \"\"\"Tests that get_item raises an exception on error.\"\"\"\n    key = \"test_key\"\n\n    mock_redis_instance = mock_redis.return_value\n    mock_redis_instance.get.side_effect = Exception(f\"The receipt of the element with the {key} prefix was interrupted\")\n\n    with pytest.raises(Exception, match=f\"The receipt of the element with the {key} prefix was interrupted\"):\n        redis_wrapper.get_item(key)\n\n@pytest.mark.ci\ndef test_save_and_get_item(redis_wrapper, mock_redis):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    mock_redis_instance = mock_redis.return_value\n    mock_redis_instance.set.return_value = True\n    redis_wrapper.save_item(key, item)\n\n    mock_redis_instance.get.return_value = json.dumps(item).encode('utf-8')\n    retrieved_item = redis_wrapper.get_item(key)\n    assert retrieved_item == b'{\"field\": \"value\"}'\n    assert json.loads(retrieved_item) == item\n\n@pytest.mark.ci\ndef test_save_item_raises_exception(redis_wrapper, mock_redis):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    mock_redis_instance = mock_redis.return_value\n    mock_redis_instance.set.side_effect = Exception(f\"Saving the result with the {key} prefix has been interrupted\")\n\n    with pytest.raises(Exception, match=f\"Saving the result with the {key} prefix has been interrupted\"):\n        redis_wrapper.save_item(key, item)\n\n@pytest.mark.ci\ndef test_check_key(redis_wrapper, mock_redis):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    mock_redis_instance = mock_redis.return_value\n    mock_redis_instance.get.return_value = None\n\n    assert redis_wrapper.check_key(key) is False\n    redis_wrapper.save_item(key, item)\n    mock_redis_instance.get.return_value = json.dumps(item).encode('utf-8')\n    assert redis_wrapper.check_key(key) is True\n\n@pytest.mark.ci\ndef test_check_key_raises_exception(redis_wrapper, mock_redis):\n    key = \"test_key\"\n\n    mock_redis_instance = mock_redis.return_value\n    mock_redis_instance.get.side_effect = Exception(f\"An error occurred while processing the {key} key\")\n\n    with pytest.raises(Exception, match=f\"An error occurred while processing the {key} key\"):\n        redis_wrapper.check_key(key)\n\n@pytest.mark.ci\n@pytest.mark.asyncio\nasync def test_wait_item(redis_wrapper, mock_async_redis):\n    key = \"test_key\"\n    item = {\"field\": \"value\"}\n\n    mock_pubsub = Mock()\n    mock_message = {\n        \"channel\": key.encode(\"utf-8\"),\n        \"data\": b\"set\",\n    }\n    mock_pubsub.subscribe = AsyncMock(return_value=None)\n    mock_pubsub.get_message = AsyncMock(\n        side_effect=[None, mock_message]\n    )\n\n    mock_redis_instance = mock_async_redis.return_value\n    mock_redis_instance.pubsub = mock_pubsub\n    mock_redis_instance.get = AsyncMock(return_value=json.dumps(item).encode(\"utf-8\"))\n    mock_redis_instance.pubsub.side_effect = lambda: mock_pubsub\n\n    async def mock_aenter(_):\n        return mock_redis_instance\n\n    async def mock_aexit(obj, exc_type, exc, tb):\n        return None\n\n    mock_async_redis.__aenter__ = mock_aenter\n    mock_async_redis.__aexit__ = mock_aexit\n\n    result = await redis_wrapper.wait_item(key, timeout=5)\n\n    assert result == b'{\"field\": \"value\"}'\n\n    mock_pubsub.subscribe.assert_called_once_with(f\"{key}\")\n    mock_pubsub.get_message.assert_called()\n\n\n\n@pytest.mark.ci\n@pytest.mark.asyncio\nasync def test_wait_item_raises_exception(redis_wrapper, mock_async_redis):\n    key = \"test_key\"\n\n    mock_pubsub = Mock()\n    mock_message = {\n        \"channel\": key.encode(\"utf-8\"),\n        \"data\": b\"set\",\n    }\n    mock_pubsub.subscribe = AsyncMock(return_value=None)\n    mock_pubsub.get_message = AsyncMock(\n        side_effect=[None, mock_message]\n    )\n\n    mock_redis_instance = mock_async_redis.return_value\n    mock_redis_instance.pubsub = mock_pubsub\n    mock_redis_instance.get = AsyncMock(\n        side_effect=Exception(f\"The receipt of the test element with the {key} prefix was interrupted\"))\n    mock_redis_instance.pubsub.side_effect = lambda: mock_pubsub\n\n    async def mock_aenter(_):\n        return mock_redis_instance\n\n    async def mock_aexit(obj, exc_type, exc, tb):\n        return None\n\n    mock_async_redis.__aenter__ = mock_aenter\n    mock_async_redis.__aexit__ = mock_aexit\n\n    with pytest.raises(Exception, match=f\"The receipt of the test element with the {key} prefix was interrupted\"):\n        await redis_wrapper.wait_item(key, timeout=1)\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/test_utils.py", "content": "import uuid\n\nfrom protollm_sdk.models.job_context_models import ResponseModel\nfrom protollm_sdk.object_interface import RedisWrapper\nfrom protollm_sdk.utils.reddis import get_reddis_wrapper, load_result\n\n\ndef test_get_reddis_wrapper():\n    redis_wrapper = get_reddis_wrapper()\n    assert isinstance(redis_wrapper, RedisWrapper)\n\n\ndef test_load_result():\n    job_id = str(uuid.uuid4())\n    prefix = None\n    redis = get_reddis_wrapper()\n    redis.save_item(job_id, {\"content\": \"value\"})\n\n    result = load_result(redis, job_id, prefix)\n\n    assert ResponseModel.model_validate_json(result.decode()) == ResponseModel(content=\"value\")\n"}
{"type": "test_file", "path": "tests/mock_chat_model.py", "content": "import logging\nfrom typing import List, Optional, Any, Dict\n\nimport requests\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    BaseMessage,\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    ChatResult,\n    ChatGeneration,\n)\nfrom pydantic import PrivateAttr\n\n\nclass MockChatModel(BaseChatModel):\n    api_key: str\n    base_url: str\n    model: str\n    temperature: float = 0.5\n    max_tokens: int = 3000\n    logging_level: int = logging.INFO\n\n    _logger: logging.Logger = PrivateAttr()\n\n    class Config:\n        arbitrary_types_allowed = True  # Allows arbitrary types like the logger\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._logger = logging.getLogger(__name__)\n        logging.basicConfig(level=self.logging_level)\n\n    @property\n    def _llm_type(self) -> str:\n        return \"llama31\"\n\n    def _prepare_headers(self) -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _prepare_context(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n        role_map = {\n            HumanMessage: \"user\",\n            AIMessage: \"assistant\",\n            SystemMessage: \"system\"\n        }\n\n        return [{\"role\": role_map.get(type(message), \"user\"), \"content\": message.content} for message in messages]\n\n    def _prepare_payload(\n            self,\n            context: List[Dict[str, str]],\n            stop: Optional[List[str]] = None,\n            **kwargs: Any\n    ) -> Dict[str, Any]:\n        payload = {\n            \"model\": self.model,\n            \"messages\": context,\n            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n        }\n        if stop is not None:\n            payload[\"stop\"] = stop\n        return payload\n\n    def _generate(\n            self,\n            messages: List[BaseMessage],\n            stop: Optional[List[str]] = None,\n            **kwargs: Any,\n    ) -> ChatResult:\n        ai_message = AIMessage(content='Action:```{ \"action\": \"add_numbers\", \"action_input\": { \"a\": 15, \"b\": 27 } }```')\n        generation = ChatGeneration(message=ai_message)\n        return ChatResult(generations=[generation])\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_agent.py", "content": "from langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor,\n    tool,\n)\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n\nfrom tests.mock_chat_model import MockChatModel\n\n\ndef test_from_prompt_model():\n\n    @tool\n    def add_numbers(a: int, b: int) -> int:\n        \"\"\"Adds two numbers.\"\"\"\n        return a + b\n\n    @tool\n    def multiply_numbers(a: int, b: int) -> int:\n        \"\"\"Multiplies two numbers.\"\"\"\n        return a * b\n\n    # List of tools\n    tools = [add_numbers, multiply_numbers]\n\n    # Create the system and human prompts\n    system_prompt = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n    \n    {tools}\n    \n    Use a JSON blob to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\n    \n    Valid \"action\" values: \"Final Answer\" or {tool_names}\n    \n    Provide only ONE action per JSON blob, as shown:\n    \n    {{ \"action\": $TOOL_NAME, \"action_input\": $INPUT }}\n    \n    Follow this format:\n    \n    Question: input question to answer\n    Thought: consider previous and subsequent steps\n    Action: $JSON_BLOB\n    \n    Observation: action result\n    ... (repeat Thought/Action/Observation N times)\n    Thought: I know what to respond\n    Action: {{ \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" }}\n    \n    \n    Begin! Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools if necessary. \n    Respond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation'''\n\n    human_prompt = '''{input}\n    {agent_scratchpad}\n    (Reminder to respond in a JSON blob no matter what)'''\n\n    system_message = SystemMessagePromptTemplate.from_template(\n        system_prompt,\n        input_variables=[\"tools\", \"tool_names\"],\n    )\n    human_message = HumanMessagePromptTemplate.from_template(\n        human_prompt,\n        input_variables=[\"input\", \"agent_scratchpad\"],\n    )\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            system_message,\n            MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n            human_message,\n        ]\n    )\n\n    llm = MockChatModel(\n        api_key=\"API_KEY\",\n        base_url=\"BASE_URL\",\n        model=\"MODEL\",\n        temperature=0.5,\n        max_tokens=3000,\n    )\n\n    # Create the structured chat agent\n    agent = create_structured_chat_agent(\n        llm=llm,\n        tools=tools,\n        prompt=prompt,\n        stop_sequence=True,\n    )\n\n    # Create the AgentExecutor\n    agent_executor = AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        verbose=True,\n        return_intermediate_steps=True,  # Set to True if you want intermediate steps\n        output_keys=[\"output\"],\n    )\n\n    user_question = \"What is the sum and product of 15 and 27?\"\n\n    response = agent_executor.invoke({\"input\": user_question})\n\n    final_answer = response[\"output\"]\n\n    assert final_answer is not None\n"}
{"type": "test_file", "path": "tests/test_connector.py", "content": "from unittest.mock import patch\n\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_core.tools import tool\nfrom langchain_gigachat import GigaChat\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\nimport pytest\n\nfrom protollm.connectors import create_llm_connector\nfrom protollm.connectors import CustomChatOpenAI\nfrom protollm.connectors import ChatRESTServer\n\n\n@pytest.fixture\ndef custom_chat_openai_without_fc_and_so():\n    return CustomChatOpenAI(model_name=\"test_model\", api_key=\"test\")\n\n\n@pytest.fixture\ndef custom_chat_openai_with_fc_and_so():\n    return CustomChatOpenAI(model_name=\"test\", api_key=\"test\")\n\n\n@pytest.fixture\ndef final_messages_for_tool_calling():\n    return [\n        SystemMessage(content=\"\"\"You have access to the following functions:\n\nFunction name: example_function\nDescription: Example function\nParameters: {}\n\nThere are the following 4 function call options:\n- str of the form <<tool_name>>: call <<tool_name>> tool.\n- 'auto': automatically select a tool (including no tool).\n- 'none': don't call a tool.\n- 'any' or 'required' or 'True': at least one tool have to be called.\n\nUser-selected option - auto\n\nIf you choose to call a function ONLY reply in the following format with no prefix or suffix:\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\"\"\"),\n        HumanMessage(content=\"Call example_function\")\n    ]\n\n\n@pytest.fixture\ndef dict_schema():\n    return {\n        \"title\": \"example_schema\",\n        \"description\": \"Example schema for test\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\n                \"type\": \"string\",\n                \"description\": \"Person name\",\n            },\n            \"age\": {\n                \"type\": \"int\",\n                \"description\": \"Person age\",\n            },\n        },\n        \"required\": [\"name\", \"age\"],\n    }\n\n\nclass ExampleModel(BaseModel):\n    \"\"\"Example\"\"\"\n    name: str = Field(description=\"Person name\")\n    age: int = Field(description=\"Person age\")\n\n\ndef test_connector():\n    conn = ChatRESTServer()\n    conn.base_url = 'mock'\n    chat = conn.create_chat(messages=[HumanMessage('M1'), HumanMessage('M2'), HumanMessage('M3')])\n    assert chat is not None\n\n\n# Basic invoke\ndef test_invoke_basic(custom_chat_openai_with_fc_and_so):\n\n    mock_response = AIMessage(content=\"Hello, world!\")\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = custom_chat_openai_with_fc_and_so.invoke(\"Hello\")\n        assert result.content == \"Hello, world!\"\n\n\ndef test_invoke_special(custom_chat_openai_without_fc_and_so):\n\n    mock_response = AIMessage(content=\"Hello, world!\")\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = custom_chat_openai_without_fc_and_so.invoke(\"Hello\")\n        assert result.content == \"Hello, world!\"\n\n\n# Function calling tests for models that doesn't support it out-of-the-box\ndef test_function_calling_with_dict(custom_chat_openai_without_fc_and_so, final_messages_for_tool_calling):\n    tools = [{\"name\": \"example_function\", \"description\": \"Example function\", \"parameters\": {}}]\n    choice_mode = \"auto\"\n    \n    model_with_tools = custom_chat_openai_without_fc_and_so.bind_tools(tools=tools, tool_choice=choice_mode)\n\n    mock_response = AIMessage(content='<function=example_function>{\"param\": \"value\"}</function>')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response) as mocked_invoke:\n        result = model_with_tools.invoke(\"Call example_function\")\n        mocked_invoke.assert_called_with(final_messages_for_tool_calling)\n        assert hasattr(result, 'tool_calls')\n        assert len(result.tool_calls) == 1\n        assert result.tool_calls[0]['name'] == \"example_function\"\n        \n\ndef test_function_calling_with_tool(custom_chat_openai_without_fc_and_so, final_messages_for_tool_calling):\n    @tool\n    def example_function():\n        \"\"\"Example function\"\"\"\n        pass\n    \n    model_with_tools = custom_chat_openai_without_fc_and_so.bind_tools(tools=[example_function], tool_choice=\"auto\")\n\n    mock_response = AIMessage(content='<function=example_function>{\"param\": \"value\"}</function>')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response) as mocked_invoke:\n        result = model_with_tools.invoke(\"Call example_function\")\n        mocked_invoke.assert_called_with(final_messages_for_tool_calling)\n        assert hasattr(result, 'tool_calls')\n        assert len(result.tool_calls) == 1\n        assert result.tool_calls[0]['name'] == \"example_function\"\n        \n\n# Function calling tests for models that support it out-of-the-box\ndef test_function_calling_with_dict_out_of_the_box(custom_chat_openai_with_fc_and_so):\n    tools = [{\"name\": \"example_function\", \"description\": \"Example function\", \"parameters\": {}}]\n    choice_mode = \"auto\"\n    \n    model_with_tools = custom_chat_openai_with_fc_and_so.bind_tools(tools=tools, tool_choice=choice_mode)\n    \n    mock_response = AIMessage(\n        content=\"\",\n        additional_kwargs={\n            \"tool_calls\": [\n                {\n                    'id': '1',\n                    'function': {\n                        'arguments': '{\"param\":\"value\"}',\n                        'name': 'example_function'\n                    },\n                    'type': 'function'\n                }\n            ],\n        }\n    )\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = model_with_tools.invoke(\"Call example_function\")\n        assert hasattr(result, 'tool_calls')\n        assert len(result.tool_calls) == 1\n        assert result.tool_calls[0]['name'] == \"example_function\"\n\n\ndef test_function_calling_with_tool_out_of_the_box(custom_chat_openai_with_fc_and_so):\n    @tool\n    def example_function():\n        \"\"\"Example function\"\"\"\n        pass\n    \n    model_with_tools = custom_chat_openai_with_fc_and_so.bind_tools(tools=[example_function], tool_choice=\"auto\")\n    \n    mock_response = AIMessage(\n        content=\"\",\n        additional_kwargs={\n            \"tool_calls\": [\n                {\n                    'id': '1',\n                    'function': {\n                        'arguments': '{\"param\":\"value\"}',\n                        'name': 'example_function'\n                    },\n                    'type': 'function'\n                }\n            ],\n        }\n    )\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = model_with_tools.invoke(\"Call example_function\")\n        assert hasattr(result, 'tool_calls')\n        assert len(result.tool_calls) == 1\n        assert result.tool_calls[0]['name'] == \"example_function\"\n\n\n# Structured output tests for models that doesn't support it out-of-the-box yet\ndef test_structured_output_pydantic(custom_chat_openai_without_fc_and_so):\n    model_with_so = custom_chat_openai_without_fc_and_so.with_structured_output(schema=ExampleModel)\n    \n    final_messages = [\n        SystemMessage(content=\"Generate a JSON object that matches one of the following schemas:\\n\\n\"\n                              \"{'description': 'Example', 'properties': {'name': {'description': 'Person name', \"\n                              \"'title': 'Name', 'type': 'string'}, 'age': {'description': 'Person age', \"\n                              \"'title': 'Age', 'type': 'integer'}}, 'required': ['name',\"\n                              \" 'age'], 'title': 'ExampleModel', 'type': 'object'}\\n\\n\"\n                              \"Your response must contain ONLY valid JSON, parsable by a standard JSON parser. Do not\"\n                              \" include any additional text, explanations, or comments.\"),\n        HumanMessage(content=\"Generate structured output\")\n    ]\n\n    mock_response = AIMessage(content='{\"name\": \"John\", \"age\": \"30\"}')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response) as mocked_invoke:\n        result = model_with_so.invoke(\"Generate structured output\")\n        mocked_invoke.assert_called_with(final_messages)\n        assert isinstance(result, ExampleModel)\n        assert result.name == \"John\"\n        assert result.age == 30\n        \n\ndef test_structured_output_dict(custom_chat_openai_without_fc_and_so, dict_schema):\n    \n    final_messages=[\n        SystemMessage(content=\"Generate a JSON object that matches one of the following schemas:\\n\\n\"\n                              \"{'title': 'example_schema', 'description': 'Example schema for test', 'type': 'object',\"\n                              \" 'properties': {'name': {'type': 'string', 'description': 'Person name'}, 'age':\"\n                              \" {'type': 'int', 'description': 'Person age'}}, 'required': ['name', 'age']}\\n\\n\"\n                              \"Your response must contain ONLY valid JSON, parsable by a standard JSON parser. Do not\"\n                              \" include any additional text, explanations, or comments.\"),\n        HumanMessage(content=\"Generate structured output\")\n    ]\n    \n    model_with_so = custom_chat_openai_without_fc_and_so.with_structured_output(schema=dict_schema)\n    mock_response = AIMessage(content='{\"name\": \"John\", \"age\": 30}')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response) as mocked_invoke:\n        result = model_with_so.invoke(\"Generate structured output\")\n        mocked_invoke.assert_called_with(final_messages)\n        assert isinstance(result, dict)\n        assert result[\"name\"] == \"John\"\n        assert result[\"age\"] == 30\n\n\ndef test_structured_output_error_pydantic(custom_chat_openai_without_fc_and_so):\n    model_with_so = custom_chat_openai_without_fc_and_so.with_structured_output(schema=ExampleModel)\n\n    mock_response = AIMessage(content='{\"name\": \"John\"}')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        with pytest.raises(ValueError):\n            model_with_so.invoke(\"Generate structured output\")\n            \n\ndef test_structured_output_error_dict(custom_chat_openai_without_fc_and_so, dict_schema):\n    \n    model_with_so = custom_chat_openai_without_fc_and_so.with_structured_output(schema=dict_schema)\n\n    mock_response = AIMessage(content='{\"name\": \"John\":}')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        with pytest.raises(ValueError):\n            model_with_so.invoke(\"Generate structured output\")\n\n\n# Structured output tests for models that support it out-of-the-box\ndef test_structured_output_pydantic_out_of_the_box(custom_chat_openai_with_fc_and_so):\n    model_with_so = custom_chat_openai_with_fc_and_so.with_structured_output(schema=ExampleModel)\n    \n    mock_response = AIMessage(\n            content=\"\",\n            additional_kwargs={\n                \"tool_calls\": [\n                    {\n                        'id': '1',\n                        'function': {\n                            'arguments': '{\"name\":\"test\",\"age\":\"30\"}',\n                            'name': 'ExampleModel'\n                        },\n                        'type': 'function'\n                    }\n                ],\n                \"parsed\": ExampleModel.model_validate_json('{\"name\": \"John\", \"age\": 30}')\n            },\n        )\n        \n        # 'parsed': ExampleModel.model_validate_json('{\"name\": \"John\", \"age\": 30}')\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = model_with_so.invoke(\"Generate structured output\")\n        assert isinstance(result, ExampleModel)\n        assert result.name == \"John\"\n        assert result.age == 30\n\n\ndef test_structured_output_dict_out_of_the_box(custom_chat_openai_with_fc_and_so):\n    dict_schema = {\n        \"title\": \"example_schema\",\n        \"description\": \"Example schema for test\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\n                \"type\": \"string\",\n                \"description\": \"Person name\",\n            },\n            \"age\": {\n                \"type\": \"int\",\n                \"description\": \"Person age\",\n            },\n        },\n        \"required\": [\"name\", \"age\"],\n    }\n    \n    model_with_so = custom_chat_openai_with_fc_and_so.with_structured_output(schema=dict_schema)\n    mock_response = '{\"name\": \"John\", \"age\": 30}'\n    with patch.object(CustomChatOpenAI, '_super_invoke', return_value=mock_response):\n        result = model_with_so.invoke(\"Generate structured output\")\n        assert isinstance(result, dict)\n        assert result[\"name\"] == \"John\"\n        assert result[\"age\"] == 30\n\n\ndef test_vsegpt_connector(monkeypatch):\n    model_url = \"https://api.vsegpt.ru/v1;meta-llama/llama-3.1-70b-instruct\"\n    test_api_key = \"test_vsegpt_key\"\n    monkeypatch.setenv(\"VSE_GPT_KEY\", test_api_key)\n    connector = create_llm_connector(model_url)\n    assert isinstance(connector, CustomChatOpenAI)\n\n\n@patch(\"protollm.connectors.connector_creator.get_access_token\", return_value=\"test_gigachat_token\")\ndef test_gigachat_connector(mock_get_token):\n    model_url = \"https://gigachat.devices.sberbank.ru/api/v1;Gigachat\"\n    connector = create_llm_connector(model_url)\n    assert isinstance(connector, GigaChat)\n\n\ndef test_openai_connector(monkeypatch):\n    model_url = \"https://api.openai.com/v1;gpt-4o\"\n    test_api_key = \"test_openai_key\"\n    monkeypatch.setenv(\"OPENAI_KEY\", test_api_key)\n    connector = create_llm_connector(model_url)\n    assert isinstance(connector, ChatOpenAI)\n\n\ndef test_ollama_connector():\n    model_url = \"ollama;http://localhost:11434;llama3.2\"\n    connector = create_llm_connector(model_url)\n    assert isinstance(connector, ChatOllama)\n\n\ndef test_test_model_connector():\n    model_url = \"test_model\"\n    connector = create_llm_connector(model_url)\n    assert isinstance(connector, CustomChatOpenAI)\n\n\ndef test_unsupported_provider():\n    model_url = \"https://unknown.provider/v1;some-model\"\n    with pytest.raises(ValueError) as exc_info:\n        create_llm_connector(model_url)\n    assert \"Unsupported provider URL\" in str(exc_info.value)\n\n\n@pytest.mark.parametrize(\"invalid_url\", [\n    \"invalid_url_without_semicolon\",\n    \"https://api.vsegpt.ru/v1\",\n    \";;\",\n])\ndef test_invalid_url_format(invalid_url):\n    with pytest.raises(ValueError):\n        create_llm_connector(invalid_url)\n"}
{"type": "test_file", "path": "tests/test_metrics.py", "content": "from typing import Optional\nfrom unittest.mock import patch\n\nfrom deepeval.test_case import LLMTestCase\nfrom langchain_core.messages import AIMessage\nfrom pydantic import BaseModel, Field\n\nfrom protollm.metrics.deepeval_connector import DeepEvalConnector\nfrom protollm.metrics.evaluation_metrics import correctness_metric\n\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\n\ndef test_metric_connector():\n    model = DeepEvalConnector()\n    mock_response = AIMessage(content=\"Hello, world!\")\n    with patch.object(model, 'generate', return_value=mock_response):\n        result = model.generate(\"Hello\")\n        assert result.content == \"Hello, world!\"\n\n\ndef test_metric_connector_with_schema():\n    model = DeepEvalConnector()\n    mock_response = Joke.model_validate_json('{\"setup\": \"test\", \"punchline\": \"test\", \"score\": \"7\"}')\n    with patch.object(model, 'generate', return_value=mock_response):\n        response = model.generate(prompt=\"Tell me a joke\", schema=Joke)\n        assert issubclass(type(response), BaseModel)\n\n\ndef test_correctness_metric():\n    test_case = LLMTestCase(\n        input=\"The dog chased the cat up the tree, who ran up the tree?\",\n        actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n        expected_output=\"The cat.\"\n    )\n    \n    with (\n        patch.object(\n            correctness_metric, \"_generate_evaluation_steps\", return_value=[\"first step\", \"second step\"]\n        ),\n        patch.object(\n            correctness_metric,\"evaluate\", return_value=(1.0, \"all good\")\n        ) as mocked_evaluate,\n    ):\n        correctness_metric.measure(test_case)\n        mocked_evaluate.assert_called_with(test_case)\n        assert isinstance(correctness_metric.score, float)\n        assert isinstance(correctness_metric.reason, str)\n"}
{"type": "test_file", "path": "tests/validation/api_check.py", "content": "from fastapi import FastAPI\nfrom protollm_tools.llm-api.protollm_api.config import Config\nfrom protollm_tools.llm-api.backend.endpoints import get_router\n\napp = FastAPI()\n\nconfig = Config.read_from_env()\n\napp.include_router(get_router(config))\n\n'''\ncurl -X POST \"http://localhost:8000/generate\" -H \"Content-Type: application/json\" -d '{\n  \"job_id\": \"12345\",\n  \"meta\": {\n    \"temperature\": 0.5,\n    \"tokens_limit\": 1000,\n    \"stop_words\": [\"stop\"],\n    \"model\": \"gpt-model\"\n  },\n  \"content\": \"What is AI?\"\n}'\n\ncurl -X POST \"http://localhost:8000/chat_completion\" -H \"Content-Type: application/json\" -d '{\n  \"job_id\": \"12345\",\n  \"meta\": {\n    \"temperature\": 0.5,\n    \"tokens_limit\": 1000,\n    \"stop_words\": [\"stop\"],\n    \"model\": \"gpt-model\"\n  },\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"What is AI?\"},\n    {\"role\": \"assistant\", \"content\": \"Artificial Intelligence is...\"}\n  ]}'\n'''"}
{"type": "test_file", "path": "tests/validation/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/validation/complex_check.py", "content": "import os\nimport uuid\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nfrom dataclasses_json import dataclass_json\nfrom deepeval.metrics import GEval\nfrom deepeval.models.base_model import DeepEvalBaseLLM\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom openai._types import NOT_GIVEN\nfrom protollm_sdk.jobs.llm_api import LLMAPI\nfrom protollm_sdk.jobs.outer_llm_api import OuterLLMAPI\nfrom protollm_sdk.models.job_context_models import PromptModel\n\n\n@dataclass_json\n@dataclass\nclass Message:\n    \"\"\"Template for message in following format {\"role\", \"content\"}.\"\"\"\n\n    role: str\n    content: str\n\n    def to_dict(self) -> Dict:\n        return {\"role\": self.role, \"content\": self.content}\n\n\nclass VseGPTConnector(DeepEvalBaseLLM):\n    \"\"\"Implementation of Evaluation agent based on large language model for Assistant's answers evaluation.\"\"\"\n\n    def __init__(\n            self,\n            model: str,\n            sys_prompt: str = \"\",\n            base_url=\"https://api.vsegpt.ru/v1\",\n    ):\n        \"\"\"Initialize instance with evaluation LLM.\n\n        Args:\n            model: Evaluation model's name\n            sys_prompt: predefined rules for model\n            base_url: URL where models are available\n        \"\"\"\n        load_dotenv(\"./config.env\")\n        self._sys_prompt = sys_prompt\n        self._model_name = model\n        self.base_url = base_url\n        self.model = self.load_model()\n\n    def load_model(self) -> OpenAI:\n        \"\"\"Load model's instance.\"\"\"\n        # TODO extend pull of possible LLMs (Not only just OpenAI's models)\n        return OpenAI(api_key=os.environ.get(\"VSE_GPT_KEY\"), base_url=self.base_url)\n\n    def generate(\n            self,\n            prompt: str,\n            context: str | None = None,\n            temperature: float = 0.015,\n            chat_history: list[Message] | None = None,\n            *args,\n            **kwargs,\n    ) -> str:\n        \"\"\"Get a response form LLM to given question.\n\n        Args:\n            prompt (str): User's question, the model must answer.\n            context (str, optional): Supplementary information, may be used for answer.\n            temperature (float, optional): Determines randomness and diversity of generated answers.\n            The higher the temperature, the more diverse the answer is. Defaults to .015.\n\n        Returns:\n            str: Model's response for user's question.\n        \"\"\"\n        usr_msg_template = (\n            prompt if context is None else f\"Вопрос:{prompt} Контекст:{context}\"\n        )\n        if chat_history is None:\n            chat_history = []\n\n        messages = [\n            {\"role\": \"system\", \"content\": self._sys_prompt},\n            *(msg.to_dict() for msg in chat_history),\n            {\"role\": \"user\", \"content\": usr_msg_template},\n        ]\n        response_format = kwargs.get(\"schema\", NOT_GIVEN)\n        response = self.model.chat.completions.create(\n            model=self._model_name,\n            messages=messages,\n            temperature=temperature,\n            n=1,\n            max_tokens=8182,\n            response_format=response_format,\n        )\n        return response.choices[0].message.content\n\n    async def a_generate(\n            self,\n            prompt: str,\n            context: str | None = None,\n            temperature: float = 0.015,\n            chat_history: list[Message] | None = None,\n            *args,\n            **kwargs,\n    ) -> str:\n        return self.generate(\n            prompt, context, temperature, chat_history, *args, **kwargs\n        )\n\n    def get_model_name(self, *args, **kwargs) -> str:\n        return \"Implementation of custom LLM for evaluation.\"\n\n\nmodel = VseGPTConnector(model=\"openai/gpt-4o-mini\")\nmetrics_init_params = {\n    \"model\": model,\n    \"verbose_mode\": False,\n    \"async_mode\": False,\n}\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=(\n        \"1. Correctness and Relevance:\"\n        \"- Compare the actual response against the expected response. Determine the\"\n        \" extent to which the actual response captures the key elements and concepts of\"\n        \" the expected response.\"\n        \"- Assign higher scores to actual responses that accurately reflect the core\"\n        \" information of the expected response, even if only partial.\"\n        \"2. Numerical Accuracy and Interpretation:\"\n        \"- Pay particular attention to any numerical values present in the expected\"\n        \" response. Verify that these values are correctly included in the actual\"\n        \" response and accurately interpreted within the context.\"\n        \"- Ensure that units of measurement, scales, and numerical relationships are\"\n        \" preserved and correctly conveyed.\"\n        \"3. Allowance for Partial Information:\"\n        \"- Do not heavily penalize the actual response for incompleteness if it covers\"\n        \" significant aspects of the expected response. Prioritize the correctness of\"\n        \" provided information over total completeness.\"\n        \"4. Handling of Extraneous Information:\"\n        \"- While additional information not present in the expected response should not\"\n        \" necessarily reduce score, ensure that such additions do not introduce\"\n        \" inaccuracies or deviate from the context of the expected response.\"\n    ),\n    evaluation_params=[\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT,\n    ],\n    **metrics_init_params,\n)\n\n\ndef local_llm(question: str, meta: dict, host: str, port: str | int):\n    llmapi = LLMAPI(llm_api_host=host, llm_api_port=port)\n    llm_request = PromptModel(job_id=str(uuid.uuid4()), meta=meta, content=question)\n    res = llmapi.inference(llm_request)\n    return res.content\n\n\ndef outer_llm(question: str, meta: dict, key: str):\n    llmapi = OuterLLMAPI(key)\n    llm_request = PromptModel(job_id=str(uuid.uuid4()), meta=meta, content=question)\n    res = llmapi.inference(llm_request)\n    return res.content\n\n\nif __name__ == \"__main__\":\n    load_dotenv(\"config.env\")\n    q = \"Что необходимо обеспечить в рамках формирования улично-дорожной сети (УДС) Санкт-Петербурга?\"\n    context = \"В рамках указанной задачи необходимо обеспечить формирование опорного каркаса улично-дорожной сети (далее - УДС) Санкт-Петербурга за счет развития современной транспортной инфраструктуры, повышения связности объектов транспортной инфраструктуры, увеличения пропускной способности, развития магистралей непрерывного движения и магистралей с улучшенными условиями движения, вылетных магистралей, строительства многоуровневых развязок и транспортно-пересадочных узлов.\"\n    question = f\"Question: {q}; Context: {context}\"\n\n    host = \"URL\"\n    port = 6672\n    meta = {\"temperature\": 0.05, \"tokens_limit\": 4096, \"stop_words\": None}\n    key = os.environ.get(\"KEY\")\n    local_llm_ans = local_llm(question, meta, host, port)\n    outer_llm_ans = outer_llm(question, meta, key)\n\n    metric_local = correctness_metric.measure(\n        LLMTestCase(\n            input=question,\n            actual_output=local_llm_ans,\n            expected_output=\"В рамках формирования улично-дорожной сети (УДС) Санкт-Петербурга необходимо обеспечить формирование опорного каркаса УДС за счет развития современной транспортной инфраструктуры, повышения связности объектов транспортной инфраструктуры, увеличения пропускной способности, развития магистралей непрерывного движения и магистралей с улучшенными условиями движения, строительства многоуровневых развязок и транспортно-пересадочных узлов.\",\n            retrieval_context=None,\n        )\n    )\n    metric_outer = correctness_metric.measure(\n        LLMTestCase(\n            input=question,\n            actual_output=outer_llm_ans,\n            expected_output=\"В рамках формирования улично-дорожной сети (УДС) Санкт-Петербурга \"\n                            \"необходимо обеспечить формирование опорного каркаса УДС за счет развития \"\n                            \"современной транспортной инфраструктуры, повышения связности объектов \"\n                            \"транспортной инфраструктуры, увеличения пропускной способности, развития \"\n                            \"магистралей непрерывного движения и магистралей с улучшенными условиями движения, \"\n                            \"строительства многоуровневых развязок и транспортно-пересадочных узлов.\",\n            retrieval_context=None,\n        )\n    )\n\n    print(f\"Question: {q}\")\n    if metric_local < metric_outer:\n        print(f\"Ответ VseGPT LLM: \\n {outer_llm_ans}\")\n        print(f\"Metric outer: {metric_outer}\")\n    else:\n        print(f\"Ответ локальной LLM: \\n {local_llm_ans}\")\n        print(f\"Metric local: {metric_local}\")\n"}
{"type": "test_file", "path": "tests/validation/complex_check_ens.py", "content": "import logging\n\nfrom protollm_agents.entrypoint import Entrypoint\n\n\nlogging.basicConfig(\nlevel=logging.DEBUG,\nformat=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\",\nhandlers=[logging.StreamHandler()],\n)\nlogger = logging.getLogger(__name__)\n\n\nif __name__ == \"__main__\":\n    epoint = Entrypoint(config_path=\"./examples/admin-config.yml\")\n    epoint.run()\n"}
{"type": "test_file", "path": "tests/validation/ens_check.py", "content": "import logging\nfrom protollm_tools.llm-agents-api.protollm_agents.entrypoint import Entrypoint\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\", handlers=[logging.StreamHandler()], )\nlogger = logging.getLogger(name)\nif __name__ == \"__main__\":\n    epoint = Entrypoint(config_path=\"./examples/admin-config.yml\")\n    epoint.run()\n"}
{"type": "test_file", "path": "tests/validation/fail_check.py", "content": "from protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\nfrom langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor,\n    tool,\n)\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.tools.render import render_text_description_and_args\nfrom langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n\n\n# Define tools using the @tool decorator\n@tool\ndef add_numbers(a: int, b: int) -> int:\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply_numbers(a: int, b: int) -> int:\n    \"\"\"Multiplies two numbers.\"\"\"\n    return a * b\n\n# List of tools\ntools = [add_numbers, multiply_numbers]\n\n# Create the system and human prompts\nsystem_prompt = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n\n{tools}\n\nUse a JSON blob to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\n\nValid \"action\" values: \"Final Answer\" or {tool_names}\n\nProvide only ONE action per JSON blob, as shown:\n\n{{ \"action\": $TOOL_NAME, \"action_input\": $INPUT }}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction: $JSON_BLOB\n\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction: {{ \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" }}\n\n\nBegin! Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation'''\n\nhuman_prompt = '''{input}\n{agent_scratchpad}\n(Reminder to respond in a JSON blob no matter what)'''\n\nsystem_message = SystemMessagePromptTemplate.from_template(\n    system_prompt,\n    input_variables=[\"tools\", \"tool_names\"],\n)\nhuman_message = HumanMessagePromptTemplate.from_template(\n    human_prompt,\n    input_variables=[\"input\", \"agent_scratchpad\"],\n)\n\n# Create the ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages(\n    [\n        system_message,\n        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n        human_message,\n    ]\n)\n\n# Initialize the custom LLM\nllm = Llama31ChatModel(\n    api_key=\"WRONG_API_KEY\",\n    base_url=\" WRONG_BASE_URL \",\n    model=\"WRONG_MODEL_NAME\",\n    temperature=0.5,\n    max_tokens=3000,\n)\n\n# Create the structured chat agent\nagent = create_structured_chat_agent(\n    llm=llm,\n    tools=tools,\n    prompt=prompt,\n    stop_sequence=True,\n)\n\n# Create the AgentExecutor\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    return_intermediate_steps=True,  # Set to True if you want intermediate steps\n    output_keys=[\"output\"],\n)\n\n# Example usage of the agent\nif __name__ == \"__main__\":\n    # Question for the agent\n    user_question = \"What is the sum and product of 15 and 27?\"\n    # Use invoke instead of run\n    response = agent_executor.invoke({\"input\": user_question})\n\n    # Access the output\n    final_answer = response[\"output\"]\n\n    # Print the final answer\n    print(f\"Agent's Response: \\n {final_answer}\")\n"}
{"type": "test_file", "path": "tests/validation/rag_check.py", "content": "import uuid\nfrom protollm_tools.sdk.protollm_sdk.jobs.utility import construct_job_context\nfrom protollm_tools.sdk.protollm_sdk.utils.reddis import get_reddis_wrapper, load_result\nfrom protollm.rags.jobs import RAGJob\n\n# Шаг 1. Инициализация уникального номера идентификации\njob_id = str(uuid.uuid4())\n# Шаг 2.  Инициализация переменных доступа к БД и SDK\njob_name = \"fast_validation\"\nctx = construct_job_context(job_name)\n# Шаг 3. Запуск поиска релевантных документов\nRAGJob().run(job_id, ctx, user_prompt='Какой бывает арматура железобетонных конструкций?', use_advanced_rag=False)\n# Шаг 4. Получение ответа модели из базы данных.\nrd = get_reddis_wrapper()\nresult = load_result(rd, job_id, job_name)\n"}
{"type": "test_file", "path": "tests/validation/repro_check.py", "content": "from langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor,\n)\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import (\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom protollm_tools.sdk.protollm_sdk.jobs.job import Job\nfrom protollm_tools.sdk.protollm_sdk.jobs.job_context import JobContext\n\nfrom protollm_tools.llm_agents_api.models import StairsLLMAgentResult\nfrom protollm_tools.llm_agents_api.config import CUSTOM_USER_MESSAGE, CUSTOM_SYSTEM_MESSAGE\nfrom protollm_tools.llm_agents_api.parse_result import parse_intermediate_steps\nfrom protollm_tools.llm_agents_api.tools import (\n    query_database_rag,\n    get_time,\n    get_resource,\n    restore_works_edges,\n    start_schedule,\n    extract_scheduling_params\n)\nfrom protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\n\nclass StairsLLMAgentJob(Job):\n    \"\"\"\n    Job class representing the main execution of the Stairs LLM Agent.\n\n    Attributes:\n        tools (list): List of tools available to the agent.\n        prompt (ChatPromptTemplate): Chat prompt template used to structure the conversation.\n        llm (Llama31ChatModel): Language model instance used by the agent for generating responses.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the StairsLLMAgentJob with tools, prompt, and language model.\"\"\"\n        super().__init__()\n        self.tools = [\n            query_database_rag,\n            get_time, get_resource,\n            restore_works_edges,\n            start_schedule,\n            extract_scheduling_params\n        ]\n        self.prompt = ChatPromptTemplate.from_messages(\n            [\n                SystemMessagePromptTemplate.from_template(\n                    CUSTOM_SYSTEM_MESSAGE,\n                    input_variables=[\"tools\", \"tool_names\"],\n                ),\n                MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n                HumanMessagePromptTemplate.from_template(\n                    CUSTOM_USER_MESSAGE,\n                    input_variables=[\"input\", \"agent_scratchpad\"],\n                ),\n            ]\n        )\n\n        self.llm = Llama31ChatModel()\n\n    def run(self, job_id: str, ctx: JobContext, **kwargs):\n        \"\"\"Executes the agent job, generating and saving the result.\n\n        Args:\n            job_id (str): Unique identifier for the job execution.\n            ctx (JobContext): Context object for managing the job's lifecycle and data storage.\n            **kwargs: Additional parameters which includes:\n                request (str): The user query string.\n                is_project (bool): Flag indicating if the request is a project.\n                is_scheduling (bool): Flag indicating if the request is for scheduling.\n\n        Returns:\n            None. Saves the job result directly into the provided job context.\n        \"\"\"\n        request: str = kwargs.get(\"request\", \"\")\n        is_project: bool or None = kwargs.get(\"is_project\", None)\n        request += \"\" if is_project is None else f\" is_project={is_project}\"\n        is_scheduling: bool or None = kwargs.get(\"is_scheduling\", None)\n        request += \"\" if is_scheduling is None else f\" is_scheduling={is_scheduling}\"\n\n        agent_request = {\"input\": request}  # , \"context\": {\"is_project\": is_project, \"is_scheduling\": is_scheduling}}\n\n        agent = create_structured_chat_agent(\n            llm=self.llm,\n            tools=self.tools,\n            prompt=self.prompt,\n            stop_sequence=True,\n        )\n        agent_executor = AgentExecutor.from_agent_and_tools(\n            agent=agent,\n            tools=self.tools,\n            verbose=True,\n            return_intermediate_steps=True,\n            output_keys=[\"output\"],\n        )\n        result = agent_executor.invoke(agent_request)\n\n        parsed_result = parse_intermediate_steps(result)\n        dumped_result = parsed_result.model_dump()\n        ctx.result_storage.save_dict(job_id, dumped_result)\n        print(\"written to redis: \", parsed_result)\n\n\nimport uuid\n\nfrom protollm_tools.sdk.protollm_sdk.jobs.utility import construct_job_context\nfrom protollm_tools.sdk.protollm_sdk.utils.reddis import get_reddis_wrapper, load_result\n\nfrom protollm_tools.llm_agents_api.jobs import StairsLLMAgentJob\n\ndef prepare_test_data(request: str, is_project: str or None, is_scheduling: str or None, tools: list[str]) -> dict:\n    return {\n        \"request\": request,\n        \"is_project\": is_project,\n        \"is_scheduling\": is_scheduling,\n        \"tools\": tools\n    }\n\nSCHEDULE_QUERY = \"Запусти планирования для проекта\"\n\nSCHEDULE_TEST = prepare_test_data(SCHEDULE_QUERY, True, False, [\"start_schedule\"])\n\ndef main():\n    job_id = str(uuid.uuid4())\n    ctx = construct_job_context(\"agent\")\n    job = StairsLLMAgentJob()\n    request, is_project, is_scheduling, _ = SCHEDULE_TEST\n    job.run(job_id=job_id, ctx=ctx, request=request, is_project=True, is_scheduling=False)\n    rd = get_reddis_wrapper()\n    result = str(load_result(rd, job_id, \"agent\"))\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "protollm_tools/sdk/tests/protollm_sdk/job/test_outer_llm_api.py", "content": "from unittest.mock import patch, MagicMock\n\nimport pytest\nfrom httpx import URL\n\nfrom protollm_sdk.jobs.outer_llm_api import OuterLLMAPI\nfrom protollm_sdk.models.job_context_models import PromptModel, ResponseModel, ChatCompletionModel, \\\n    ChatCompletionUnit, PromptMeta\n\n\n@pytest.fixture\ndef outer_llm_api():\n    return OuterLLMAPI(\"key\")\n\n\n@pytest.fixture\ndef mock_openai():\n    with patch('protollm_sdk.jobs.outer_llm_api.OpenAI') as mock_openai:\n        mock_openai().chat.completions.create = MagicMock()\n        yield mock_openai().chat.completions.create\n\n@pytest.mark.ci\ndef test_outer_llmapi_initialization():\n    outer_llm_api = OuterLLMAPI(\"key\")\n    assert outer_llm_api.model == \"openai/gpt-4o-2024-08-06\"\n    assert outer_llm_api.timeout_sec == 10 * 60\n    assert isinstance(outer_llm_api.client.api_key, str)\n    assert len(outer_llm_api.client.api_key) == len(\n        \"key\")\n    assert outer_llm_api.client.base_url == URL(\"https://api.vsegpt.ru/v1/\")\n\n@pytest.mark.ci\ndef test_outer_llmapi_inference_success(mock_openai, outer_llm_api):\n    \"\"\"\n    Test inference method of OuterLLMAPI for a successful response.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.choices = [MagicMock(message=MagicMock(content=\"Test success\"))]\n    mock_openai.return_value = mock_response\n\n    mock_request = MagicMock(spec=PromptModel)\n    mock_request.content = \"test prompt\"\n    mock_meta = MagicMock()\n    mock_meta.temperature = 0.7\n    mock_meta.tokens_limit = 100\n    mock_meta.model = None\n    mock_request.meta = mock_meta  # Добавляем meta к запросу\n\n    response = outer_llm_api.inference(mock_request)\n\n    mock_openai.assert_called_once_with(\n        model=\"openai/gpt-4o-2024-08-06\",\n        messages=[{\"role\": \"user\", \"content\": \"test prompt\"}],\n        temperature=0.7,\n        n=1,\n        max_tokens=100,\n        timeout=10 * 60\n    )\n\n    assert isinstance(response, ResponseModel)\n    assert response.content == \"Test success\"\n\n@pytest.mark.ci\ndef test_outer_llmapi_inference_server_error(mock_openai, outer_llm_api):\n    \"\"\"\n    Test inference method of OuterLLMAPI for a 500 error.\n    \"\"\"\n    mock_openai.side_effect = Exception(\"The LLM server is not available.\")\n\n    mock_request = MagicMock(spec=PromptModel)\n    mock_request.content = \"test prompt\"\n    mock_meta = MagicMock()\n    mock_meta.temperature = 0.7\n    mock_meta.tokens_limit = 100\n    mock_meta.model = None\n    mock_request.meta = mock_meta\n\n    with pytest.raises(Exception,\n                       match=\"The response generation has been interrupted. Error: The LLM server is not available.\"):\n        outer_llm_api.inference(mock_request)\n\n@pytest.mark.ci\ndef test_outer_llmapi_chat_completion_success(mock_openai, outer_llm_api):\n    \"\"\"\n    Test chat_completion method of OuterLLMAPI for a successful response.\n    \"\"\"\n    mock_response = MagicMock()\n    mock_response.choices = [MagicMock(message=MagicMock(content=\"Test chat success\"))]\n    mock_openai.return_value = mock_response\n\n    mock_request = MagicMock(spec=ChatCompletionModel)\n\n    mock_request.messages = [ChatCompletionUnit(role=\"user\", content=\"test chat\")]\n\n    mock_meta = MagicMock(spec=PromptMeta)\n    mock_meta.temperature = 0.7\n    mock_meta.tokens_limit = 100\n    mock_meta.model = None\n    mock_request.meta = mock_meta\n\n    response = outer_llm_api.chat_completion(mock_request)\n\n    mock_openai.assert_called_once_with(\n        model=\"openai/gpt-4o-2024-08-06\",\n        messages=[{\"role\": \"user\", \"content\": \"test chat\"}],\n        temperature=0.7,\n        n=1,\n        max_tokens=100,\n        timeout=10 * 60\n    )\n\n    assert isinstance(response, ResponseModel)\n    assert response.content == \"Test chat success\"\n\n@pytest.mark.ci\ndef test_outer_llmapi_chat_completion_server_error(mock_openai, outer_llm_api):\n    \"\"\"\n    Test chat_completion method of OuterLLMAPI for a 500 error.\n    \"\"\"\n    mock_openai.side_effect = Exception(\"The LLM server is not available.\")\n\n    mock_request = MagicMock(spec=ChatCompletionModel)\n\n    mock_request.messages = [ChatCompletionUnit(role=\"user\", content=\"test chat\")]\n\n    mock_meta = MagicMock(spec=PromptMeta)\n    mock_meta.temperature = 0.7\n    mock_meta.tokens_limit = 100\n    mock_meta.model = None\n    mock_request.meta = mock_meta\n\n    with pytest.raises(Exception,\n                       match=\"The response generation has been interrupted. Error: The LLM server is not available.\"):\n        outer_llm_api.chat_completion(mock_request)\n"}
{"type": "source_file", "path": "examples/llama31_usage_example.py", "content": "from langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor,\n    tool,\n)\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n\nfrom protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\n\n\n# Define tools using the @tool decorator\n@tool\ndef add_numbers(a: int, b: int) -> int:\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply_numbers(a: int, b: int) -> int:\n    \"\"\"Multiplies two numbers.\"\"\"\n    return a * b\n\n# List of tools\ntools = [add_numbers, multiply_numbers]\n\n# Create the system and human prompts\nsystem_prompt = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n\n{tools}\n\nUse a JSON blob to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\n\nValid \"action\" values: \"Final Answer\" or {tool_names}\n\nProvide only ONE action per JSON blob, as shown:\n\n{{ \"action\": $TOOL_NAME, \"action_input\": $INPUT }}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction: $JSON_BLOB\n\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction: {{ \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" }}\n\n\nBegin! Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation'''\n\nhuman_prompt = '''{input}\n{agent_scratchpad}\n(Reminder to respond in a JSON blob no matter what)'''\n\nsystem_message = SystemMessagePromptTemplate.from_template(\n    system_prompt,\n    input_variables=[\"tools\", \"tool_names\"],\n)\nhuman_message = HumanMessagePromptTemplate.from_template(\n    human_prompt,\n    input_variables=[\"input\", \"agent_scratchpad\"],\n)\n\n# Create the ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages(\n    [\n        system_message,\n        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n        human_message,\n    ]\n)\n\n# Initialize the custom LLM\nllm = Llama31ChatModel(\n    api_key=\"\",\n    base_url=\"\",\n    model=\"meta-llama/llama-3.1-70b-instruct\",\n    temperature=0.5,\n    max_tokens=3000,\n)\n\n# Create the structured chat agent\nagent = create_structured_chat_agent(\n    llm=llm,\n    tools=tools,\n    prompt=prompt,\n    stop_sequence=True,\n)\n\n# Create the AgentExecutor\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    return_intermediate_steps=True,  # Set to True if you want intermediate steps\n    output_keys=[\"output\"],\n)\n\n# Example usage of the agent\nif __name__ == \"__main__\":\n    # Question for the agent\n    user_question = \"What is the sum and product of 15 and 27?\"\n    # Use invoke instead of run\n    response = agent_executor.invoke({\"input\": user_question})\n\n    # Access the output\n    final_answer = response[\"output\"]\n\n    # Print the final answer\n    print(f\"Agent's Response: \\n {final_answer}\")\n"}
{"type": "source_file", "path": "examples/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/connector_creator_usage_example.py", "content": "import os\nimport logging\nfrom typing import Optional\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom protollm.connectors import create_llm_connector\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n\ndef basic_call_example(url_with_name: str):\n    \"\"\"\n    Example of using a model to get an answer.\n\n    Args:\n        url_with_name: Model URL combined with the model name\n    \"\"\"\n    try:\n        model = create_llm_connector(url_with_name, temperature=0.015, top_p=0.95)\n        res = model.invoke(\"Tell me a joke\")\n        logging.info(res.content)\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n    \n\n# Some models do not support explicit function calls, so the system prompt will be used for this. If it is not\n# specified, it will be generated from the tool description and response format. If specified, it will be\n# supplemented.\ndef function_call_example_with_functions(url_with_name: str):\n    \"\"\"\n    Example of using a function to create a connector for function calls. Tools are defined as functions with the\n    @tool decorator from LangChain.\n\n    Args:\n        url_with_name: Model URL combined with the model name\n    \"\"\"\n    model = create_llm_connector(url_with_name)\n    mssgs = [\n        SystemMessage(content=\"\"),\n        HumanMessage(content=\"Build a plan for placing new schools with a budget of 5 billion rubles.\"),\n    ]\n\n    @tool\n    def territory_by_budget(is_best_one: bool, budget: int | None, service_type: str) -> str:\n        \"\"\"\n        Get potential territories for building a new service of a given type, considering the budget (amount in\n        rubles).  This function should be used if the discussion involves placement, creation, construction, or erection\n        of new services, including parks. Possible service types are strictly in the following list: ['school',\n        'clinic', 'kindergarten', 'park']. The budget is optional. If the user does not specify a budget, the parameter\n        will remain empty (None). Do not set default values yourself.\n        Required arguments: [\"service_type\"]\n\n        Args:\n            is_best_one (bool): Flag indicating whether to select the best territory.\n            budget (int | None): Budget amount in rubles.\n            service_type (str): The new service being planned for construction. Possible service types are strictly in\n                the following list: ['school', 'clinic', 'kindergarten', 'park']. Select the type that the user is\n                interested in constructing.\n\n        Returns:\n            str: Analysis result.\n        \"\"\"\n        return f\"The best territory for {service_type} with a budget of {budget} has been found.\"\n\n    @tool\n    def parks_by_budget(budget: int | None) -> str:\n        \"\"\"\n        Get parks suitable for improvement, considering the specified budget (amount in rubles). This function is used\n        only if the discussion involves improving existing parks, not creating new ones. The budget is optional. If the\n        user does not specify a budget, the parameter will remain empty (None). Do not set default values yourself.\n        Required arguments: []\n\n        Args:\n            budget (int | None): Budget amount in rubles.\n\n        Returns:\n            str: Analysis result.\n        \"\"\"\n        return f\"Parks for improvement with a budget of {budget} have been found.\"\n\n    tools_as_functions = [territory_by_budget, parks_by_budget]\n    model_with_tools = model.bind_tools(tools=tools_as_functions, tool_choice=\"auto\")\n    try:\n        res = model_with_tools.invoke(mssgs)\n        logging.info(res.content)\n        logging.info(res.tool_calls)\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n\n\ndef function_call_example_with_dicts(url_with_name: str):\n    \"\"\"\n    Example of using a function to create a connector for function calls. Tools are defined as dictionaries.\n\n    Args:\n        url_with_name: Model URL combined with the model name\n    \"\"\"\n    model = create_llm_connector(url_with_name)\n    mssgs = [\n        SystemMessage(content=\"\"),\n        HumanMessage(content=\"Build a plan for placing new schools with a budget of 5 billion rubles.\"),\n    ]\n\n    tools_as_dicts = [\n        {\n            \"name\": \"territory_by_budget\",\n            \"description\": (\n                \"Get potential territories for building a new service of a given type, considering the budget \"\n                \"(amount in rubles). This function should be used if the discussion involves placement, creation, \"\n                \"construction, or erection of new services, including parks. Possible service types are strictly in \"\n                \"the following list: ['school', 'clinic', 'kindergarten', 'park']. The budget is optional. If the user \"\n                \"does not specify a budget, the parameter will remain empty (None). Do not set default values yourself.\"\n            ),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"is_best_one\": {\n                        \"type\": \"boolean\",\n                        \"description\": (\n                            \"Flag indicating whether to select the best territory.\"\n                        ),\n                    },\n                    \"budget\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Budget amount in rubles.\",\n                    },\n                    \"service_type\": {\n                        \"type\": \"string\",\n                        \"description\": (\n                            \"The new service being planned for construction. Possible service types are strictly in \"\n                            \"the following list: ['school', 'clinic', 'kindergarten', 'park']. Select the type that \"\n                            \"the user is interested in constructing.\"\n                        ),\n                    },\n                },\n                \"required\": [\"service_type\"],\n            },\n        },\n        {\n            \"name\": \"parks_by_budget\",\n            \"description\": (\n                \"Get parks suitable for improvement, considering the specified budget (amount in rubles). \"\n                \"This function is used only if the discussion involves improving existing parks, not creating new ones. \"\n                \"The budget is optional. If the user does not specify a budget, the parameter will remain empty (None). \"\n                \"Do not set default values yourself.\"\n            ),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"budget\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Budget amount. May be specified in the request. Default value is None.\",\n                    },\n                },\n                \"required\": [],\n            },\n        },\n    ]\n\n    model_with_tools = model.bind_tools(tools=tools_as_dicts, tool_choice=\"auto\")\n    try:\n        res = model_with_tools.invoke(mssgs)\n        logging.info(res.content)\n        logging.info(res.tool_calls)\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n\n\ndef structured_output_example_with_dict(url_with_name: str):\n    \"\"\"\n    Example of using a model to produce a structured response with a dictionary schema.\n\n    Args:\n        url_with_name: Model URL combined with the model name\n    \"\"\"\n    model = create_llm_connector(url_with_name)\n\n    Joke = {\n        \"title\": \"joke\",\n        \"description\": \"Joke to tell user.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"setup\": {\n                \"type\": \"string\",\n                \"description\": \"The setup of the joke\",\n            },\n            \"punchline\": {\n                \"type\": \"string\",\n                \"description\": \"The punchline to the joke\",\n            },\n            \"rating\": {\n                \"type\": \"integer\",\n                \"description\": \"How funny the joke is, from 1 to 10\",\n                \"default\": None,\n            },\n        },\n        \"required\": [\"setup\", \"punchline\"],\n    }\n\n    structured_model = model.with_structured_output(schema=Joke)\n    try:\n        res = structured_model.invoke(\"Tell me a joke about cats\")\n        logging.info(res)\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n\n\ndef structured_output_example_with_pydantic(url_with_name: str):\n    \"\"\"\n    Example of using a model to produce a structured response with a Pydantic class schema.\n\n    Args:\n        url_with_name: Model URL combined with the model name\n    \"\"\"\n    model = create_llm_connector(url_with_name)\n\n    class Joke(BaseModel):\n        \"\"\"Joke to tell user.\"\"\"\n        setup: str = Field(description=\"The setup of the joke\")\n        punchline: str = Field(description=\"The punchline to the joke\")\n        rating: Optional[int] = Field(\n            default=None, description=\"How funny the joke is, from 1 to 10\"\n        )\n\n    structured_model = model.with_structured_output(schema=Joke)\n    try:\n        res = structured_model.invoke(\"Tell me a joke about cats\")\n        logging.info(res)\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n\n\nif __name__ == \"__main__\":\n    load_dotenv(\"../config.env\") # Change path to your config file if needed or pass URL with name directly\n    \n    # model_url_and_name = os.getenv(\"LLAMA_URL\")\n    # model_url_and_name = os.getenv(\"GIGACHAT_URL\")\n    # model_url_and_name = os.getenv(\"DEEPSEEK_URL\")\n    # model_url_and_name = os.getenv(\"DEEPSEEK_R1_URL\")\n    # model_url_and_name = os.getenv(\"GPT4_URL\")\n    # model_url_and_name = os.getenv(\"OPENAI_URL\")\n    model_url_and_name = os.getenv(\"OLLAMA_URL\")\n    \n    # Uncomment the example you want to run\n    basic_call_example(model_url_and_name)\n    function_call_example_with_functions(model_url_and_name)\n    function_call_example_with_dicts(model_url_and_name)\n    structured_output_example_with_dict(model_url_and_name)\n    structured_output_example_with_pydantic(model_url_and_name)\n    "}
{"type": "source_file", "path": "examples/real_world/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/metrics_usage_examples.py", "content": "# You can use correctness_metric from ProtoLLM or import desired directly from deepeval.\n# The correctness metric is demonstrative, but has been shown to be good for determining the correctness of an answer.\n# You can define a new one, with a different criterion, if necessary.\n#\n# In the second case, you may also need to import a connector object for deepeval metrics to work. This can be done\n# as follows:\n#\n# `from protollm.metrics import model_for_metrics`\n#\n# Also make sure that you set the model URL and model_name in the same format as for a normal LLM connector\n# (URL;model_name).\n#\n# Detailed documentation on metrics is available at the following URL:\n# https://docs.confident-ai.com/docs/metrics-introduction\n\nimport logging\n\nfrom deepeval.metrics import AnswerRelevancyMetric, ToolCorrectnessMetric\nfrom deepeval.test_case import LLMTestCase, ToolCall\n\nfrom protollm.metrics import correctness_metric, model_for_metrics\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nanswer_relevancy = AnswerRelevancyMetric(model=model_for_metrics, async_mode=False)\ntool_correctness = ToolCorrectnessMetric()\n\nif __name__ == \"__main__\":\n    # ===================================metrics using LLM=============================================\n    # Create test case for metric\n    test_case = LLMTestCase(\n        input=\"What if these shoes don't fit?\",\n        actual_output=\"We offer a 30-day full refund at no extra cost.\",\n        expected_output=\"You are eligible for a 30 day full refund at no extra cost.\"\n    )\n\n    answer_relevancy.measure(test_case) # Evaluate metric\n    logging.info(f\"Answer relevancy score {answer_relevancy.score}\")\n    logging.info(f\"Answer relevancy reason: {answer_relevancy.reason}\")\n    \n    correctness_metric.measure(test_case) # Evaluate metric\n    logging.info(f\"Correctness score {correctness_metric.score}\")\n    logging.info(f\"Correctness reason: {correctness_metric.reason}\")\n    \n    # ===================================metrics not using LLM=========================================\n    # Create test case for metric\n    test_case = LLMTestCase(\n        input=\"What if these shoes don't fit?\",\n        actual_output=\"We offer a 30-day full refund at no extra cost.\",\n        # Replace this with the tools that was actually used by your LLM agent\n        tools_called=[ToolCall(name=\"WebSearch\", input_parameters={}), ToolCall(name=\"ToolQuery\", input_parameters={})],\n        expected_tools=[ToolCall(name=\"WebSearch\", input_parameters={})],\n    )\n    \n    tool_correctness.measure(test_case)\n    logging.info(f\"Tool correctness score {tool_correctness.score}\")\n    logging.info(f\"Tool correctness reason: {tool_correctness.reason}\")\n"}
{"type": "source_file", "path": "examples/real_world/urbanistics/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm-synthetic/examples/summarisation_example.py", "content": "from protollm_synthetic.synthetic_pipelines.chains import SummarisationChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport pandas as pd\nimport os\nimport asyncio\n\ntexts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Artificial intelligence is transforming the world.\",\n    \"Python is a popular programming language.\"\n]\n\ndf = pd.DataFrame(texts, columns=[\"content\"])\ndf.to_json(\"tmp_data/tmp_sample_summarization_dataset.json\", index=False)\n\ndataset = Dataset(path=\"tmp_data/tmp_sample_summarization_dataset.json\")\n# Expected output: a list of summaries\nexpected_summaries = [\n    \"The fox jumps over the dog.\",\n    \"AI is changing the world.\",\n    \"Python is a popular language.\"\n]\n\nqwen_large_api_key = os.environ.get(\"OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"OPENAI_API_BASE\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n        # max_concurrency=10\n    )\n\nsummarisation_chain = SummarisationChain(llm=llm)\nactual_summaries = asyncio.run(summarisation_chain.run(dataset, n_examples=3))\nprint(actual_summaries)"}
{"type": "source_file", "path": "protollm-synthetic/examples/free_query_example.py", "content": "import os\nfrom protollm_synthetic.synthetic_pipelines.chains import FreeQueryChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport json\nimport asyncio\n\n# создаем небольшой датасет с задачей извлечения событий из текста\ntexts = [\n\"Сегодня хорошая погода в Москве\",\n\"Завтра в 11 будет важный созвон с заказчиком из Италии\",\n\"Я записан в бассейн 30.12.2024 в 12 и 31.12.2024 в 13. Удивляюсь, как мне это удалось\",\n\"Через неделю будет вечеринка в клубе 'Золотой' в 23:00\",\n\"Сегодня в 10:00 я занялся спортом\",\n\"19.01.2025 я записан к стоматологу\"\n]\n\nsolutions = [\n    \"[{'date': '22.12.2024', 'event': 'хорошая погода в Москве'}]\",\n    \"[{'date': '23.12.2024', 'time': '11:00', 'event': 'созвон с заказчиком из Италии'}]\",\n    \"[{'date': '30.12.2024', 'time': '12:00', 'event': 'запись в бассейн'}, {'date': '31.12.2024', 'time': '13:00', 'event': 'запись в бассейн'}]\",\n    None,\n    None,\n    None,\n    ]\n\ndata_dict = {'content': texts, 'solution': solutions}\n\nwith open('tmp_data/sample_data_free_instruction.json', 'w', encoding='utf-8') as file:\n    json.dump(data_dict, file, ensure_ascii=False)\n\ndataset = Dataset(data_col='content', labels_col='solution', path='tmp_data/sample_data_free_instruction.json')\n\nqwen_large_api_key = os.environ.get(\"OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"OPENAI_API_BASE\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n        # max_concurrency=10\n    )\n\nfree_query_chain = FreeQueryChain(llm=llm)\nasyncio.run(free_query_chain.run(dataset, n_examples=3))\n\nprint(free_query_chain.data)\n"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/synthetic_pipelines/prompts.py", "content": "from typing import List\n\n# Summarisation\n\ndef generate_summary_system_prompt() -> str:\n    return f\"\"\"Summarize the following text in a concise manner, in the same language as the original text\n    Pay attention that you should save only importaint information so the final summary is short and concise.\"\"\"\n\ndef generate_summary_human_prompt() -> str:\n    return \"Text that should be summarised:\\n\\n{text}\\n\\n Summary:\"\n\ndef generate_detailed_summary_prompt(text: str) -> str:\n    return \"\"\"Provide a detailed summary of the following text, highlighting key points and important details, in the same language as the original text.\"\"\"\n\ndef generate_bullet_point_summary_system_prompt(text: str) -> str  :\n    return f\"Summarize the following text using bullet points for clarity, in the same language as the original text:\\n\\n{text}\\n\\nBullet Point Summary:\"\n\ndef generate_question_based_summary_prompt(text: str) -> str    :\n    return f\"Summarize the following text by answering the question: What are the main ideas and conclusions, in the same language as the original text?\\n\\n{text}\\n\\nSummary:\"\n\ndef generate_summary_with_length_constraint_prompt(text: str, max_length: int) -> str:\n    return f\"Summarize the following text in no more than {max_length} words, in the same language as the original text:\\n\\n{text}\\n\\nSummary:\"\n\n# Aspect summarisation\n\ndef generate_aspect_summarisation_prompt(aspect: str) -> str:\n    return f\"\"\"You are a professional specialist in {aspect}.\n    Summarise the following text by identifying and summarising the main aspects, \n    Strictly follow the rules below:\n    - Do not include any information that is not related to the aspect. If the text is not related to the aspect, return an empty string.\n    - Use the same language as the original text.\n    - Final summary should be short and concise. Shorter than the original text.\n    \"\"\"\n\n# test generation\n\ndef generate_quiz_system_prompt() -> str:\n    return \"\"\"You are a professional teacher and you know that students learn better when they complete quizes with multiple choices on the material they are learning.\n    You will be provided with a text and you need to generate a quiz for it.\n    Strictly follow the rules below:\n    - Generate up to 5 questions and answers for a given text.\n    - The questions should be relevant to the text. It means that the question can be answered after reading the text.\n    - Each question should have 4 options with one or more correct answers.\n    - The correct options should be marked with a special symbol '(X)' after the option.\n    - The questions should not be obvious so students should have to think to choose the correct options.\n    - The questions should be diverse and cover different aspects of the text.\n    - The questions should be in the same language as the text.\n    The response should be in the following format:\n    {response_format_description}\n    \"\"\"\n\ndef generate_quiz_human_prompt() -> str:\n    return \"\"\"Text that should be used for quiz generation: \n    {text}\n    \"\"\"\n\n# RAG\n\ndef generate_rag_system_prompt() -> str:\n    return \"\"\"Create a set of qestion and answer pairs to check the quality of a RAG system.\n    User will provide a text for you to create a set of questions and corresponding answers.\n    When generating follow the rules below:\n    - Generate 5 questions and answers\n    - The questions should be relevant to the text. It means that the question can be answered after reading the text.\n    - The questions should be of different categories: \"easy\" - simple question asking about some fact from the text, \"medium\" - question require to analyse and synthesise information from the text e.g. comparision or reasoning, \"hard\" - requires deep understanding on application level and the answer is not obvious.\n    - The questions should be diverse and cover different aspects of the text.\n    - The questions should be in the same language as the text. \n    Return answer in the following format:\n    {response_format_description}\n    \"\"\"\n\ndef generate_rag_human_prompt() -> str:\n    return \"\"\"Text that should be used for question and answers generation: \n    {text}\n    \"\"\"\n\n# Instruction design\n\ndef generate_instruction_design_prompt(tasks: List[str], solutions: List[str]) -> str:\n    \"\"\"\n    Generates a prompt for the LLM to design an instruction for itself to accomplish the given tasks.\n\n    :param tasks: A list of tasks that need to be accomplished.\n    :param solutions: A list of solutions corresponding to each task.\n    :return: A formatted string prompt for the LLM.\n    \"\"\"\n    task_solution_pairs = \"\\n\".join(\n        f\"Task: {task}\\nSolution: {solution}\" for task, solution in zip(tasks, solutions)\n    )\n    return (\n        f\"Design an instruction for yourself to accomplish the following tasks based on the provided solutions:\\n\\n\"\n        f\"{task_solution_pairs}\\n\\nInstruction:\"\n    )\n\ndef generate_instruction_one_shot_system_prompt() -> str:\n    return \"\"\"Design an instruction for yourself to transform the following text to the provided result.\n    You will be provided with some text and result that user want to obtain from the text.\n    You should define the instruction following the rules below:\n    - The instruction should be concise and to the point.\n    - Define all the special conditions that should be met to transform the text to the result.\n    Return answer in the following format:\n    {response_format_description}\"\"\"\n\ndef generate_instruction_one_shot_human_prompt() -> str:\n    return \"\"\"Text that should be transformed:\n    {text}\n    Result that should be obtained:\n    {result}\"\"\"\n\ndef merge_instructions() -> str:\n    return \"\"\"Merge the following instructions into one instruction according to the set of presented fields.\n    You will be provided with several instructions to transform text into the predefined resultand you need to merge them into one json instruction.\n    Save as much details as possible to save the quality of instructions.\n    Return answer in the following format:\n    {response_format_description}\"\"\"\n\ndef merge_instructions_human_prompt() -> str:\n    return \"\"\"Instructions to merge in json format:\n    {text}\"\"\"\n\n# Augmentation\n\ndef paraphrase_text() -> str:\n    return \"Paraphrase the following text in a concise manner, in the same language as the original text:\\n\\n{text}\\n\\nParaphrased Text:\"\n\n# Evaluation\n\ndef generate_summary_evaluation_system_prompt() -> str:\n    return \"\"\"Evaluate the quality of the following summary and provide a score from 1 to 10 with feedback.\n    Return answer in the following format: \n    {response_format_description}\"\"\"\n\ndef generate_aspect_summarisation_evaluation_system_prompt(aspect: str) -> str:\n    return f\"\"\"Evaluate the quality of the following aspect summarisation on a topic of {aspect} and provide a score from 1 to 10 with feedback.\n    \"\"\" + \"\"\"Return answer in the following format: \n    {response_format_description}\"\"\"\n\ndef check_summary_quality_human_prompt() -> str:\n    return \"Initial text: {text}\\n\\nSummary: {summary}\\n\"\n\n# Genetic Evolution\n\ndef generate_genetic_evolution_prompt() -> str:\n    return \"\"\"You are a genetic algorithm that evolves the instruction to answer the question.\n    You will be provided with an instruction and a question.\n    You need to evolve the instruction to answer the question.\n    \"\"\""}
{"type": "source_file", "path": "examples/real_world/chemical_pipeline/llama31_chemical_example.py", "content": "\"\"\"\nExample of using an agent in a chemical pipeline with tools for drug generation (by API).\n\nProcess:\n- Reading from a file with example queries. \n- Pipeline starts. \n- Results are written to the same file.\n\"\"\"\nfrom langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor,\n    tool\n)\nimport requests\nimport json\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\nimport pandas as pd\nfrom protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\nfrom examples.real_world.chemical_pipeline.validate_tools import validate_decompose, compute_metrics, validate_conductor\n\n\ndef make_markdown_table(props: dict) -> str:\n    \"\"\"Create a table in Markdown format dynamically based on dict keys.\n\n    Args:\n        props (dict): properties of molecules\n\n    Returns:\n        str: table with properties\n    \"\"\"\n    # get all the keys for column headers\n    headers = list(props.keys())\n\n    # prepare the header row\n    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\"\n    markdown_table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n    # get the number of rows (assuming all lists in the dictionary are the same length)\n    num_rows = len(next(iter(props.values())))\n\n    # fill the table rows dynamically based on the keys\n    for i in range(num_rows):\n        row = [\n            str(props[key][i]) for key in headers\n        ]\n        markdown_table += \"| \" + \" | \".join(row) + \" |\\n\"\n\n    return markdown_table\n\n\n# Define tools using the @tool decorator\n@tool\ndef request_mols_generation(num: int) -> list:\n    \"\"\"Generates random molecules.\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"RNDM\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_alzheimer(num: int) -> list:\n    \"\"\"Generation of drug molecules for the treatment of Alzheimer's disease. GSK-3beta inhibitors with high activity. \\\n    These molecules can bind to GSK-3beta protein, molecules has low brain-blood barrier permeability\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Alzhmr\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_multiple_sclerosis(num: int) -> list:\n    \"\"\"Generation of molecules for the treatment of multiple sclerosis.\\\n            There are high activity tyrosine-protein kinase BTK inhibitors or highly potent non-covalent \\\n            BTK tyrosine kinase inhibitors from the TEC family of tyrosine kinases that have the potential \\\n            to affect B cells as a therapeutic target for the treatment of multiple sclerosis.\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Sklrz\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n\n@tool\ndef gen_mols_dyslipidemia(num: int) -> list:\n    \"\"\"\n    Generation of molecules for the treatment of dyslipidemia.\n    Molecules that inhibit Proprotein Convertase Subtilisin/Kexin Type 9 with enhanced bioavailability and \n    the ability to cross the BBB. Molecules have affinity to the protein ATP citrate synthase, enhances \n    reverse cholesterol transport via ABCA1 upregulation\n    , inhibits HMG-CoA reductase with improved safety profile compared to statins. It can be  \n    PCSK9 inhibitors to enhance LDL receptor recycling and reduce LDL cholesterol levels.\n\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Dslpdm\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_acquired_drug_resistance(num: int) -> list:\n    \"\"\"\n    Generation of molecules for acquired drug resistance. \n    Molecules that selectively induce apoptosis in drug-resistant tumor cells.\n    It significantly enhances the activity of existing therapeutic agents against drug-resistant pathogens.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"TBLET\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_lung_cancer(num: int) -> list:\n    \"\"\"\n    Generation of molecules for the treatment of lung cancer. \n    Molecules are inhibitors of KRAS protein with G12C mutation. \n    The molecules are selective, meaning they should not bind with HRAS and NRAS proteins.\n    Its target KRAS proteins with all possible mutations, including G12A/C/D/F/V/S, G13C/D, \n    V14I, L19F, Q22K, D33E, Q61H, K117N and A146V/T.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Cnsr\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_parkinson(num: int) -> list:\n    \"\"\"\n    Generation of molecules for parkinson.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Prkns\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n# List of tools\ntools = [gen_mols_parkinson, gen_mols_lung_cancer, gen_mols_acquired_drug_resistance,\n         gen_mols_dyslipidemia, gen_mols_multiple_sclerosis, gen_mols_alzheimer, request_mols_generation]\n\n# Create the system and human prompts\nsystem_prompt = '''\nRespond to the human as helpfully and accurately as possible. You have access to the following tools:\n\n{tools}\n\nUse a JSON blob to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\n\nValid \"action\" values: \"Final Answer\" or {tool_names}\n\nProvide only ONE action per JSON blob, as shown:\n\n{{ \"action\": $TOOL_NAME, \"action_input\": $INPUT }}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction: $JSON_BLOB\n\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction: {{ \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" }}\n\n\nBegin! Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools if necessary. \nRespond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation\nIn the \"Final Answer\" you must ALWAYS display all generated molecules!!!\nFor example answer must consist table (!):\n| Molecules | QED | Synthetic Accessibility | PAINS | SureChEMBL | Glaxo | Brenk | BBB | IC50 |\n\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Fc1ccc2c(c1)CCc1ccccc1-2 | 0.6064732613170888 \n| 1.721973678244476 | 0 | 0 | 0 | 0 | 1 | 0 |\\n| O=C(Nc1ccc(C(=O)c2ccccc2)cc1)c1ccc(F)cc1 | 0.728441789442482 \n| 1.4782662488060723 | 0 | 0 | 0 | 0 | 1 | 1 |\\n| O=C(Nc1ccccc1)c1ccc(NS(=O)(=O)c2ccc3c(c2)CCC3=O)cc1 | \n0.6727786031171711 | 1.9616124655434675 | 0 | 0 | 0 | 0 | 0 | 0 |\\n| Cc1ccc(C)c(-n2c(=O)c3ccccc3n(Cc3ccccc3)c2=O)c1 \n| 0.5601042919484651 | 1.920664623176684 | 0 | 0 | 0 | 0 | 1 | 1 |\\n| Cc1ccc2c(c1)N(C(=O)CN1C(=O)NC3(CCCc4ccccc43)C1=O)CC2 \n| 0.8031696199670261 | 3.3073398307371438 | 0 | 0 | 0 | 1 | 1 | 0 |\"\n'''\n\nhuman_prompt = '''{input}\n{agent_scratchpad}\n(Reminder to respond in a JSON blob no matter what)'''\n\nsystem_message = SystemMessagePromptTemplate.from_template(\n    system_prompt,\n    input_variables=[\"tools\", \"tool_names\"],\n)\nhuman_message = HumanMessagePromptTemplate.from_template(\n    human_prompt,\n    input_variables=[\"input\", \"agent_scratchpad\"],\n)\n\n# Create the ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages(\n    [\n        system_message,\n        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n        human_message,\n    ]\n)\n\n# Initialize the custom LLM\nllm = Llama31ChatModel(\n    api_key='API_KEY_VSE_GPT',\n    base_url=\"https://api.vsegpt.ru/v1\",\n    model=\"meta-llama/llama-3.1-70b-instruct\",\n    temperature=0.5,\n    max_tokens=5000\n)\n\n# Create the structured chat agent\nagent = create_structured_chat_agent(\n    llm=llm,\n    tools=tools,\n    prompt=prompt,\n    stop_sequence=True\n)\n\n# Create the AgentExecutor\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    return_intermediate_steps=True,\n    output_keys=[\"output\"],\n    early_stopping_method=\"generate\"\n)\n\n# Example usage of the agent\nif __name__ == \"__main__\":\n    path = 'examples/chemical_pipeline/queries_responses_chemical.xlsx'\n    questions = pd.read_excel(path).values.tolist()\n    \n    for i, q in enumerate(questions):\n        print('Task № ', i)\n        response = agent_executor.invoke({\n            \"input\": q[1]\n        })\n        \n        validate_decompose(i, response[\"intermediate_steps\"], path)\n        for n, tools_pred in enumerate(response[\"intermediate_steps\"]):\n            name_tool = tools_pred[0].tool\n            func = {'name': name_tool}\n            validate_conductor(i, func, n, path)\n            \n        # Access the output\n        final_answer = response[\"output\"]\n        # Print the final answer\n        print(f\"Agent's Response: \\n {final_answer}\")"}
{"type": "source_file", "path": "protollm-synthetic/examples/aspect_summarisation_example.py", "content": "from protollm_synthetic.synthetic_pipelines.chains import AspectSummarisationChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport pandas as pd\nimport os\nimport asyncio\n\ntexts = [\n    \"The quick brown fox jumps over the lazy dog. The fox is a cunning animal as some politicians.\",\n    \"Artificial intelligence is transforming the world. AI is a powerful technology and influence much on the world politics.\",\n    \"Python is a popular programming language.\"\n]\n\ndf = pd.DataFrame(texts, columns=[\"content\"])\ndf.to_json(\"tmp_data/tmp_sample_summarization_dataset.json\", index=False)\n\ndataset = Dataset(path=\"tmp_data/tmp_sample_summarization_dataset.json\")\n# Expected output: a list of summaries\nexpected_summaries = [\n    \"The fox jumps over the dog.\",\n    \"AI is changing the world.\",\n    \"Python is a popular language.\"\n]\n\naspect = \"politics\"\n\nqwen_large_api_key = os.environ.get(\"OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"OPENAI_API_BASE\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n        # max_concurrency=10\n    )\n\naspect_summarisation_chain = AspectSummarisationChain(llm=llm)\nactual_summaries = asyncio.run(aspect_summarisation_chain.run(dataset, aspect=aspect, n_examples=3))\nprint(actual_summaries)"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/synthetic_pipelines/genetic_evolver.py", "content": "import random\nfrom typing import List\n\nclass GeneticEvolver:\n    def __init__(self, initial_population: List[str], \n                 generations: int = 10, mutation_rate: float = 0.1):\n        self.initial_population = initial_population\n        self.generations = generations\n        self.mutation_rate = mutation_rate\n\n    def evolve(self):\n        population = self.initial_population\n        for generation in range(self.generations):\n            fitness_scores = [self.evaluate_prompt(prompt) for prompt in population]\n            selected_parents = self.select_parents(population, fitness_scores)\n            new_population = []\n            while len(new_population) < len(population):\n                parent1, parent2 = random.sample(selected_parents, 2)\n                child = self.crossover(parent1, parent2)\n                if random.random() < self.mutation_rate:\n                    child = self.mutate_prompt(child)\n                new_population.append(child)\n\n    def crossover(self, parent1, parent2):\n        # Simple crossover: concatenate the first half of parent1 with the second half of parent2\n        return parent1[:len(parent1)//2] + parent2[len(parent2)//2:]\n    \n    def mutate_prompt(self, prompt):\n        # Example mutation: add a random word\n        words = [\"optimize\", \"enhance\", \"improve\", \"boost\"]\n        return prompt + \" \" + random.choice(words)\n\n    # Define a function to evaluate the prompt using an LLM-based evaluation function\n    def evaluate_prompt(self, prompt):\n        # Placeholder for LLM-based evaluation logic\n    # Return a score representing the prompt's success\n        return random.uniform(0, 1)  # Example: random score for demonstration\n    \n    def select_parents(self, population, fitness_scores):\n        # Select top 50% based on fitness\n        sorted_population = [x for _, x in sorted(zip(fitness_scores, population), reverse=True)]\n        return sorted_population[:len(population)//2]\n\n\n"}
{"type": "source_file", "path": "examples/real_world/chemical_pipeline/validate_tools.py", "content": "import pandas as pd\n\ndict_for_map_many_func = {\n    \"alzheimer\": \"gen_mols_alzheimer\",\n    \"dyslipidemia\": \"gen_mols_dyslipidemia\",\n    \"lung cancer\": \"gen_mols_lung_cancer\",\n    \"sclerosis\": \"gen_mols_multiple_sclerosis\",\n    \"Drug_Resistance\": \"gen_mols_acquired_drug_resistance\",\n    \"Parkinson\": \"gen_mols_parkinson\",\n    \"nothing\": \"nothing\"\n}\n\n\ndef validate_decompose(\n    idx: int, \n    decompose_lst: list, \n    validation_path=\"experiment3.xlsx\"\n    ) -> bool:\n    \n    lines = pd.read_excel(validation_path)\n    columns = lines.columns\n    lines = lines.values.tolist()\n\n    num_tasks_true = len(lines[idx][0].split(\",\"))\n    lines[idx][2] = decompose_lst\n    \n    if len(decompose_lst) == num_tasks_true:\n        lines[idx][3] = True\n        \n        pd.DataFrame(\n            lines, columns=columns\n        ).to_excel(validation_path, index=False)\n        return True\n    else:\n        lines[idx][3] = False\n        pd.DataFrame(\n            lines, columns=columns\n        ).to_excel(validation_path, index=False)\n        return False\n\n\ndef validate_conductor(idx: int, func: dict, sub_task_number: int, path_total_val=\"experiment3_example.xlsx\") -> bool:\n    \"\"\"\n    Validate conductors agent answer. File must consist of next columns = \n    'case', 'content', 'decomposers_tasks', 'is_correct_context', 'task 1', \n    'task 2', 'task 3', 'task 4', 'task 5'\n\n    Parameters\n    ----------\n    idx : int\n        Number of line for validation\n    func : dict\n        Dict with function name and parameters\n    sub_task_number : int\n        Number of subtask (from decompose agent)\n\n    Returns\n    -------\n    answer : bool\n        Validation passed or not\n    \"\"\"\n    lines = pd.read_excel(path_total_val)\n    columns = lines.columns\n    lines = lines.values.tolist()\n    \n    try:\n        target_name = lines[idx][0].split(\", \")[sub_task_number]\n    except:\n        target_name = 'nothing'\n    if isinstance(func, bool):\n        return False\n\n    # if call chat model for answer in free form (сos no such case exists in the file)\n    if func[\"name\"].replace(\" \", \"\") == \"make_answer_chat_model\":\n        lines[idx][4 + sub_task_number] = func[\"name\"].replace(\" \", \"\")\n        pd.DataFrame(\n            lines,\n            columns=columns,\n        ).to_excel(path_total_val, index=False)\n        return False\n    else:\n        if func[\"name\"].replace(\" \", \"\") == dict_for_map_many_func[target_name]:\n            lines[idx][4 + sub_task_number] = func[\"name\"].replace(\" \", \"\")\n            pd.DataFrame(\n                lines,\n                columns=columns,\n            ).to_excel(path_total_val, index=False)\n            return True\n        else:\n            lines[idx][4 + sub_task_number] = func[\"name\"].replace(\" \", \"\")\n            pd.DataFrame(\n                lines,\n                columns=columns,\n            ).to_excel(path_total_val, index=False)\n            return False\n        \n        \ndef compute_metrics(model_name: str = 'no_name_model', file_path: str = 'experiment3_example.xlsx'):\n    \"\"\"\n    Compute pipeline metrics\n\n    Parameters\n    ----------\n\n    file_path : str\n        Path to excel file with next columns:\n        case, content, decompose_tasks, is_correct, task 1, task 2, task 3, task 4\n    model_name : str\n        Name of model with which testing was carried out\n    \"\"\"\n    just_1_case_in_all_smpls = True\n    dfrm = pd.read_excel(file_path)\n\n    number_subtasks = 0\n    number_tasks = 0\n\n    correct_subtasks = 0\n    correct_tasks = 0\n    \n    decomposer_true = 0\n\n    # add zeros columns for result\n    dfrm[\"conductors_score\"] = 0\n    dfrm[\"score_from\"] = 0\n    dfrm[\"total_score\"] = 0\n    columns = dfrm.columns\n\n    lst = dfrm.values.tolist()\n\n    for row in lst:\n        try:\n            cases = (\n                row[0]\n                .replace(\"Parkinson \", \"Parkinson\")\n                .replace(\"Drug_Resistance \", \"Drug_Resistance\")\n                .split(\", \")\n            )\n            decomposer_true += row[3]\n            \n            row[11 - 1] = len(cases)\n            \n            # for every subtask in main task(query)\n            for n, case in enumerate(cases):\n                is_correct = dict_for_map_many_func[case] == row[4 + n]\n                row[10 - 1], correct_subtasks = (\n                    row[10 - 1] + int(is_correct),\n                    correct_subtasks + int(is_correct),\n                )\n\n            # if all subtasks are defined correctly\n            if row[10 - 1] == row[11 - 1]:\n                correct_tasks += 1\n                row[12 - 1] = 1\n            else:\n                row[12 - 1] = 0\n            \n            if just_1_case_in_all_smpls:\n                if len(cases) > 1:\n                    just_1_case_in_all_smpls = False\n                \n            number_subtasks, number_tasks = number_subtasks + len(cases), number_tasks + 1\n\n        except:\n            continue\n\n    pd.DataFrame(lst, columns=columns).to_excel(\n        f\"result.xlsx\", index=False\n    )\n\n    if not(just_1_case_in_all_smpls):\n        print(\n            \"Percentage true subtasks (accuracy of whole pipeline): \",\n            100 / (number_subtasks) * correct_subtasks,\n        )\n        print(\"Percentage true tasks by Decomposer: \", 100 / number_tasks * decomposer_true)\n        print(\"Percentage true tasks by Conductor: \", 100 / (number_subtasks) * correct_subtasks)\n    else:\n        print(\"Percentage true tasks: \", 100 / (number_tasks) * correct_tasks)\n"}
{"type": "source_file", "path": "examples/real_world/chemical_multi_agent_system/tools.py", "content": "from langchain.agents import tool\n\nimport requests\nimport json\n\ndef make_markdown_table(props: dict) -> str:\n    \"\"\"Create a table in Markdown format dynamically based on dict keys.\n\n    Args:\n        props (dict): properties of molecules\n\n    Returns:\n        str: table with properties\n    \"\"\"\n    # get all the keys for column headers\n    headers = list(props.keys())\n\n    # prepare the header row\n    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\"\n    markdown_table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n    # get the number of rows (assuming all lists in the dictionary are the same length)\n    num_rows = len(next(iter(props.values())))\n\n    # fill the table rows dynamically based on the keys\n    for i in range(num_rows):\n        row = [\n            str(props[key][i]) for key in headers\n        ]\n        markdown_table += \"| \" + \" | \".join(row) + \" |\\n\"\n\n    return markdown_table\n\n# Define tools using the @tool decorator\n@tool\ndef request_mols_generation(num: int) -> list:\n    \"\"\"Generates random molecules.\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"RNDM\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_alzheimer(num: int) -> list:\n    \"\"\"Generation of drug molecules for the treatment of Alzheimer's disease. GSK-3beta inhibitors with high activity. \\\n    These molecules can bind to GSK-3beta protein, molecules has low brain-blood barrier permeability\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Alzhmr\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_multiple_sclerosis(num: int) -> list:\n    \"\"\"Generation of molecules for the treatment of multiple sclerosis.\\\n            There are high activity tyrosine-protein kinase BTK inhibitors or highly potent non-covalent \\\n            BTK tyrosine kinase inhibitors from the TEC family of tyrosine kinases that have the potential \\\n            to affect B cells as a therapeutic target for the treatment of multiple sclerosis.\n\n    Args:\n        num (int): number of molecules to generate\n\n    Returns:\n        list: list of generated molecules\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Sklrz\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n\n@tool\ndef gen_mols_dyslipidemia(num: int) -> list:\n    \"\"\"\n    Generation of molecules for the treatment of dyslipidemia.\n    Molecules that inhibit Proprotein Convertase Subtilisin/Kexin Type 9 with enhanced bioavailability and \n    the ability to cross the BBB. Molecules have affinity to the protein ATP citrate synthase, enhances \n    reverse cholesterol transport via ABCA1 upregulation\n    , inhibits HMG-CoA reductase with improved safety profile compared to statins. It can be  \n    PCSK9 inhibitors to enhance LDL receptor recycling and reduce LDL cholesterol levels.\n\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Dslpdm\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_acquired_drug_resistance(num: int) -> list:\n    \"\"\"\n    Generation of molecules for acquired drug resistance. \n    Molecules that selectively induce apoptosis in drug-resistant tumor cells.\n    It significantly enhances the activity of existing therapeutic agents against drug-resistant pathogens.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"TBLET\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_lung_cancer(num: int) -> list:\n    \"\"\"\n    Generation of molecules for the treatment of lung cancer. \n    Molecules are inhibitors of KRAS protein with G12C mutation. \n    The molecules are selective, meaning they should not bind with HRAS and NRAS proteins.\n    Its target KRAS proteins with all possible mutations, including G12A/C/D/F/V/S, G13C/D, \n    V14I, L19F, Q22K, D33E, Q61H, K117N and A146V/T.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Cnsr\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans\n\n@tool\ndef gen_mols_parkinson(num: int) -> list:\n    \"\"\"\n    Generation of molecules for parkinson.\n    \"\"\"\n    params = {\n        \"numb_mol\": num,\n        \"cuda\": True,\n        \"mean_\": 0.0,\n        \"std_\": 1.0,\n        \"case_\": \"Prkns\"\n    }\n    resp = requests.post('http://10.32.2.2:81/case_generator', data=json.dumps(params))\n\n    ans = make_markdown_table(json.loads(resp.json()))\n\n    return ans"}
{"type": "source_file", "path": "examples/real_world/urbanistics/synthetic_rag_query_example.py", "content": "import logging\nimport os\nfrom samplefactory.synthetic_pipelines.chains import RAGChain\nfrom samplefactory.utils import Dataset, VLLMChatOpenAI\nimport asyncio\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# path = 'tmp_data/sample_data_city_rag.json'\npath = 'tmp_data/sample_data_rag_spb.json'\ndataset = Dataset(data_col='content', path=path)\n\nqwen_large_api_key = os.environ.get(\"QWEN_OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"QWEN_OPENAI_API_BASE\")\n\nlogger.info(\"Initializing LLM connection\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n    )\n\nrag_chain = RAGChain(llm=llm)\n\nlogger.info(\"Starting generating\")\nasyncio.run(rag_chain.run(dataset, \n                          n_examples=5))\n\nlogger.info(\"Saving results\")\npath = 'tmp_data/sample_data_city_rag_generated.json'\n\n# An alternative way to save data\n# rag_chain.save_chain_output('tmp_data/sample_data_city_rag_generated.json')\n\ndf = rag_chain.data.explode('generated')\ndf['question'] = df['generated'].apply(lambda x: x['question'])\ndf['answer'] = df['generated'].apply(lambda x: x['answer'])\ndf = df[['content', 'question', 'answer']]\n\nlogger.info(f\"Writing result to {path}\")\ndf.to_json(path, orient=\"records\")\n\nlogger.info(\"Generation successfully finished\")\n"}
{"type": "source_file", "path": "protollm/metrics/evaluation_metrics.py", "content": "from deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\n\nfrom .deepeval_connector import DeepEvalConnector\n\nmodel_for_metrics = DeepEvalConnector()\n\n# Custom metric for evaluating the correctness of an answer\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=(\n        \"1. Correctness and Relevance:\"\n        \"- Compare the actual response against the expected response. Determine the\"\n        \" extent to which the actual response captures the key elements and concepts of\"\n        \" the expected response.\"\n        \"- Assign higher scores to actual responses that accurately reflect the core\"\n        \" information of the expected response, even if only partial.\"\n        \"2. Numerical Accuracy and Interpretation:\"\n        \"- Pay particular attention to any numerical values present in the expected\"\n        \" response. Verify that these values are correctly included in the actual\"\n        \" response and accurately interpreted within the context.\"\n        \"- Ensure that units of measurement, scales, and numerical relationships are\"\n        \" preserved and correctly conveyed.\"\n        \"3. Allowance for Partial Information:\"\n        \"- Do not heavily penalize the actual response for incompleteness if it covers\"\n        \" significant aspects of the expected response. Prioritize the correctness of\"\n        \" provided information over total completeness.\"\n        \"4. Handling of Extraneous Information:\"\n        \"- While additional information not present in the expected response should not\"\n        \" necessarily reduce score, ensure that such additions do not introduce\"\n        \" inaccuracies or deviate from the context of the expected response.\"\n    ),\n    evaluation_params=[\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT,\n    ],\n    model=model_for_metrics,\n    async_mode=False\n)\n"}
{"type": "source_file", "path": "examples/real_world/chemical_multi_agent_system/prompting.py", "content": "system_prompt_conductor = '''\nRespond to the human as helpfully and accurately as possible. You have access to the following tools:\n\n{tools}\n\nUse a JSON blob to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\n\nValid \"action\" values: \"Final Answer\" or {tool_names}\n\nProvide only ONE action per JSON blob, as shown:\n\n{{ \"action\": $TOOL_NAME, \"action_input\": $INPUT }}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction: $JSON_BLOB\n\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction: {{ \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" }}\n\n\nBegin! Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools if necessary. \nRespond directly if appropriate. Format is Action:```$JSON_BLOB``` then Observation\nIn the \"Final Answer\" you must ALWAYS display all generated molecules!!!\nFor example answer must consist table (!):\n| Molecules | QED | Synthetic Accessibility | PAINS | SureChEMBL | Glaxo | Brenk | BBB | IC50 |\n\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Fc1ccc2c(c1)CCc1ccccc1-2 | 0.6064732613170888 \n| 1.721973678244476 | 0 | 0 | 0 | 0 | 1 | 0 |\\n| O=C(Nc1ccc(C(=O)c2ccccc2)cc1)c1ccc(F)cc1 | 0.728441789442482 \n| 1.4782662488060723 | 0 | 0 | 0 | 0 | 1 | 1 |\\n| O=C(Nc1ccccc1)c1ccc(NS(=O)(=O)c2ccc3c(c2)CCC3=O)cc1 | \n0.6727786031171711 | 1.9616124655434675 | 0 | 0 | 0 | 0 | 0 | 0 |\\n| Cc1ccc(C)c(-n2c(=O)c3ccccc3n(Cc3ccccc3)c2=O)c1 \n| 0.5601042919484651 | 1.920664623176684 | 0 | 0 | 0 | 0 | 1 | 1 |\\n| Cc1ccc2c(c1)N(C(=O)CN1C(=O)NC3(CCCc4ccccc43)C1=O)CC2 \n| 0.8031696199670261 | 3.3073398307371438 | 0 | 0 | 0 | 1 | 1 | 0 |\"\n'''\nsystem_prompt_decomposer = \\\n\"\"\"\nRespond to the human as helpfully and accurately as possible. You must decompose the input questions into tasks.\n\nUse a JSON to specify a tool by providing an \"action\" key (tool name) and an \"action_input\" key (tool input).\nValid \"action\" values: \"Final Answer\". Action is always == \"Final Answer\".\nValid number of tasks: 1-5.\n\nFollow this format:\nQuestion: input questions to answer\n{ \"action\": \"Final Answer\", \"action_input\": \"[task1, task2, task3...]\" }\n\nExample:\nQuestion: Generate molecule for Alzheimer. Generate 3 molecules for Parkinson\n{ \"action\": \"Final Answer\", \"action_input\": \"['Generate molecule for Alzheimer', 'Generate 3 molecules for Parkinson']\" }\n\nBegin! Reminder to ALWAYS respond with a valid JSON of a single action.\nIn the \"Final Answer\" you must ALWAYS display in list!\n\"\"\"\n\nhuman_prompt = '''{input}\n{agent_scratchpad}\n(Reminder to respond in a JSON blob no matter what)'''"}
{"type": "source_file", "path": "protollm/rags/pipeline/__init__.py", "content": "from protollm.rags.pipeline.etl_pipeline import DocsLoadPipeline, DocsTransformPipeline, DocsExtractPipeline\n"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/synthetic_pipelines/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm-synthetic/examples/rag_example.py", "content": "import os\nimport os\nimport json\nimport logging\nfrom protollm_synthetic.synthetic_pipelines.chains import RAGChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport asyncio\n\nimport logging\nfrom protollm_synthetic.synthetic_pipelines.chains import RAGChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport asyncio\n\n\n# Сохраняем набор данных \ntexts = [\n    \"\"\"Формирование Стратегии 2030 осуществлялось на основе анализа устойчивых тенденций социально-экономического развития Санкт-Петербурга, а также с учетом результатов социально-экономического развития Санкт-Петербурга в 2012-2013 годах.\nПредставленная в Стратегии 2030 система целей социально-экономического развития Санкт-Петербурга структурирована по четырем уровням: генеральная цель определяет 4 стратегических направления, в рамках которых сформированы 17 стратегических целей, исходя из содержания которых сформулированы программно-целевые установки. Всего в Стратегии 2030 сформулированы 114 целей социально-экономического развития Санкт-Петербурга различных уровней, каждой из которых, кроме генеральной, соответствуют целевые показатели, характеризующие степень их достижения.\nГенеральной целью Стратегии 2030 является обеспечение стабильного улучшения качества жизни горожан и повышение глобальной конкурентоспособности Санкт-Петербурга на основе реализации национальных приоритетов развития, обеспечения устойчивого экономического роста и использования результатов инновационно-технологической деятельности.\n\"\"\",\n    \"\"\"Задачи государственной программы\nОбеспечение приоритета профилактики в сфере охраны здоровья и развития первичной медико-санитарной помощи.\nПовышение эффективности оказания специализированной, включая высокотехнологичную, медицинской помощи, скорой, в том числе скорой специализированной, медицинской помощи, медицинской эвакуации и паллиативной медицинской помощи.\nРазвитие и внедрение инновационных методов диагностики, профилактики и лечения.\nПовышение эффективности службы родовспоможения и детства.Развитие медицинской реабилитации населения и совершенствование системы санаторно-курортного лечения, в том числе детей.\nОбеспечение медицинской помощью неизлечимых больных, в том числе детей. Обеспечение системы здравоохранения высококвалифицированными и мотивированными кадрами.\"\"\"\n    \"\"\"Региональные проекты, реализуемые в рамках государственной программы. Развитие экспорта медицинских услуг (город федерального значения Санкт-Петербург)\nБорьба с онкологическими заболеваниями (город федерального значения Санкт-Петербург)\nРазвитие системы оказания первичной медико-санитарной помощи (город федерального значения Санкт-Петербург)\nОбеспечение медицинских организаций системы здравоохранения квалифицированными кадрами (город федерального значения Санкт-Петербург)\nБорьба с сердечно-сосудистыми заболеваниями (город федерального значения Санкт-Петербург)\nМодернизация первичного звена здравоохранения Российской Федерации (город федерального значения Санкт-Петербург)\nСтаршее поколение (город федерального значения Санкт-Петербург)\nСоздание единого цифрового контура в здравоохранении на основе единой государственной информационной системы здравоохранения (ЕГИСЗ) (город федерального значения Санкт-Петербург)\nРазвитие детского здравоохранения, включая создание современной инфраструктуры оказания медицинской помощи детям (город федерального значения Санкт-Петербург)\nФормирование системы мотивации граждан к здоровому образу жизни, включая здоровое питание и отказ от вредных привычек (город федерального значения Санкт-Петербург)\n\"\"\",\n    \"\"\"Ожидаемые результаты реализации государственной программы\nК 2029 году должен сложиться качественно новый уровень состояния жилищной сферы, характеризуемый следующими целевыми ориентирами:\nоказание государственного содействия в улучшении жилищных условий в форме предоставления социальных выплат за счет средств бюджета Санкт-Петербурга и федерального бюджета в отношении 26113 петербургских семей;\nрасселение 6,08 тыс.кв.м аварийного жилищного фонда, признанного таковым до 01.01.2017, в рамках реализации первого этапа расселения аварийного жилищного фонда, и по второму этапу расселения аварийного жилищного фонда, признанного таковым в период с 01.01.2017 до 01.01.2022, переселение из него 0,38 тыс. человек и расселение 1,15 тыс.кв.м аварийного жилищного фонда, признанного таковым после 01.01.2017, переселение из него 0,113 тыс. человек;\nформирование государственного жилищного фонда Санкт-Петербурга общей площадью 602,67 тыс.кв.м для предоставления жилых помещений 10,6 тыс. семей;\nпроведение капитального ремонта общего имущества по необходимым видам работ, включая мероприятия в области энергосбережения и повышения энергетической эффективности, в 81,5% многоквартирных домов от общего количества домов, включенных в региональную программу капитального ремонта общего имущества в многоквартирных домах в Санкт-Петербурге (далее - региональная программа);\nповышение степени удовлетворенности населения Санкт-Петербурга уровнем жилищно-коммунального обслуживания до 67%;\nдостижение уровня доступности оплаты жилищно-коммунальных услуг гражданами.\n\"\"\",\n    \"\"\"Расселение многоквартирных домов, признанных после 01.01.2017 аварийными и подлежащими сносу или реконструкции, осуществляется на основании распоряжений ЖК о признании многоквартирных домов аварийными и подлежащими сносу или реконструкции. С 01.01.2017 в Санкт-Петербурге были признаны аварийными и подлежащими сносу или реконструкции, а также подлежащими расселению 48 многоквартирных домов, в которых проживало 190 семей (556 человек), из них по состоянию на 01.12.2023 завершено расселение 31 аварийного многоквартирного дома, 157 семей (443 человека) переселены в благоустроенные жилые помещения.\nВ целях обеспечения социальной поддержки детей-сирот и детей, оставшихся без попечения родителей, данная категория подлежит обеспечению специализированными жилыми помещениями. Детям-сиротам как не имеющим закрепленного жилья, так и тем, у кого такое жилое помещение есть, однако проживание в нем по ряду обстоятельств невозможно, предоставляются отдельные квартиры по договорам найма специализированного жилищного фонда на пять лет. Жилищный фонд для этих целей сформирован из квартир в домах бюджетного строительства, приобретаемых в собственность Санкт-Петербурга, а также квартир свободного жилищного фонда районов Санкт-Петербурга. В период с 2021 по 2023 год 2656 детей-сирот обеспечено отдельными квартирами специализированного жилищного фонда Санкт-Петербурга.\n\"\"\",\n    \"\"\"Ожидаемые результаты реализации государственной программы\nСозданы условия для повышения уровня производительности труда в промышленности и роста конкурентоспособности предприятий;\nосуществлено ускоренное развитие высокотехнологичного сектора промышленности Санкт-Петербурга;\nобеспечено сохранение позиций Санкт-Петербурга как одного из ведущих промышленных регионов Российской Федерации, увеличение объема отгруженной продукции и ее доли в общем объеме отгруженной продукции по стране до 5,5 процентов;\nобеспечено повышение инновационной активности предприятий; население Санкт-Петербурга обеспечено качественными и безопасными продуктами питания\n\"\"\",\n    \"\"\"Ожидаемые результаты реализации государственной программы\nВыполнение обязательств государства по социальной поддержке отдельных категорий граждан;\nснижение бедности среди получателей мер социальной поддержки на основе расширения сферы применения адресного принципа ее предоставления;\nрасширение масштабов предоставления мер социальной поддержки отдельным категориям граждан в денежной форме;\nсохранение размера оплаты труда социальных работников государственных учреждений социальной защиты населения Санкт-Петербурга на уровне не ниже 100% среднемесячного дохода от трудовой деятельности в Санкт-Петербурге;\nудовлетворение потребностей всех категорий граждан в социальном обслуживании;\nобеспечение поддержки и содействие социальной адаптации граждан, попавших в трудную жизненную ситуацию или находящихся в социально опасном положении;\nсоздание прозрачной и конкурентной среды в сфере социального обслуживания населения;\nрост рождаемости; преобладание семейных форм устройства детей, оставшихся без попечения родителей; расширение охвата детей-инвалидов социальным обслуживанием;\nразвитие конкуренции на рынке социального обслуживания; создание прозрачной и конкурентной системы государственной поддержки СО НКО; обеспечение эффективности и финансовой устойчивости СО НКО;\nувеличение объемов социальных услуг, оказываемых негосударственными организациями, в том числе СО НКО;\nрасширение возможностей граждан пожилого возраста для социальной интеграции в общество; развитие рекреационных поселений в границах территорий ведения жителями Санкт-Петербурга садоводства для собственных нужд\n\"\"\"\n    \"\"\"Социальная защита населения представляет собой систему правовых, экономических, организационных и иных мер, гарантированных государством отдельным категориям населения. Категории граждан - получателей социальной поддержки и (или) социальных услуг, виды и формы социальной поддержки и условия ее предоставления определены законодательством Российской Федерации, законодательством Санкт-Петербурга, иными нормативными правовыми актами.\nГосударственная политика Санкт-Петербурга в области социальной защиты населения формируется в соответствии с положениями Конституции Российской Федерации, в которой определено, что в Российской Федерации обеспечивается государственная поддержка семьи, материнства, отцовства и детства, инвалидов и пожилых граждан, развивается система социальных служб, устанавливаются государственные пенсии, пособия и иные гарантии социальной защиты. При этом Конституцией Российской Федерации установлено, что в совместном ведении Российской Федерации и субъектов Российской Федерации находятся вопросы социальной защиты, включая социальное обеспечение, защиты семьи, материнства, отцовства и детства; защиты института брака как союза мужчины и женщины; создания условий для достойного воспитания детей в семье, а также для осуществления совершеннолетними детьми обязанности заботиться о родителях.\nНа развитие института социальной защиты населения оказывает влияние ряд следующих факторов:\nэкономические (уровень и темпы экономического развития, занятость и доходы населения, состояние государственных финансов, уровень развития производительных сил);\nдемографические (сокращение рождаемости, увеличение продолжительности жизни);\"\"\",\n    \"\"\"Промышленность Санкт-Петербурга является основой экономики Санкт-Петербурга, главным источником доходов бюджета Санкт-Петербурга.\nНа долю промышленности в Санкт-Петербурге приходится 12,7 процента валового регионального продукта (по данным за 2022 год).\nВклад промышленности в формирование доходной части бюджета Санкт-Петербурга является наибольшим: промышленные предприятия по итогам 2023 года обеспечили 39,7 процента налоговых поступлений в бюджетную систему Российской Федерации, 15,0 процента поступлений в бюджет Санкт-Петербурга (по оценке КППИТ на основе данных Управления Федеральной налоговой службы по Санкт-Петербургу).\nРазвитие промышленности как базового сектора экономики оказывает влияние на различные аспекты социально-экономического развития региона, в том числе на доходы бюджета, занятость и уровень благосостояния населения, решение социальных задач, состояние потребительского рынка.\nОдним из основных факторов обеспечения стратегической конкурентоспособности и необходимым условием устойчивого развития промышленности Санкт-Петербурга является наличие в Санкт-Петербурге значительного инновационного потенциала.\nСанкт-Петербург находится в центре передового инновационного развития и на протяжении нескольких лет занимает лидирующие позиции в различных рейтингах, в том числе международных. Начиная с 2014 года Санкт-Петербург входит в тройку лидеров в Рейтинге инновационных регионов России, разработанном ассоциацией экономического взаимодействия субъектов Российской Федерации \"Ассоциация инновационных регионов России\" совместно с Министерством экономического развития Российской Федерации (далее - Рейтинг), и в 2018 году Санкт-Петербург в Рейтинге занял первое место.\nСогласно ежегодному Рейтингу инновационного развития субъектов Российской Федерации, который проводится Институтом статистических исследований и экономики знаний Национального исследовательского университета \"Высшая школа экономики\" в рамках деятельности Российской кластерной обсерватории, Санкт-Петербург на протяжении ряда лет располагается в первой тройке инновационных регионов.\nС учетом указанных предпосылок, целей и задач социально-экономического развития Санкт-Петербурга на долгосрочную перспективу инновационное развитие Санкт-Петербурга определено как одно из приоритетных направлений.\nЭффективным сектором экономики, включающим в себя предприятия пищевой и перерабатывающей промышленности и предприятия сельского хозяйства, является агропромышленный комплекс Санкт-Петербурга, который призван решать одну из важнейших задач, - содействие обеспечению продовольственной безопасности Санкт-Петербурга.\nХарактеристики текущего состояния промышленности, инновационной деятельности и агропромышленного комплекса Санкт-Петербурга с указанием основных проблем приведены в соответствующих разделах подпрограмм государственной программы.\n\"\"\",\n    \"\"\"Ожидаемые результаты реализации государственной программы\nСозданы условия для повышения уровня производительности труда в промышленности и роста конкурентоспособности предприятий;\nосуществлено ускоренное развитие высокотехнологичного сектора промышленности Санкт-Петербурга;\nобеспечено сохранение позиций Санкт-Петербурга как одного из ведущих промышленных регионов Российской Федерации, увеличение объема отгруженной продукции и ее доли в общем объеме отгруженной продукции по стране до 5,5 процентов;\nобеспечено повышение инновационной активности предприятий; население Санкт-Петербурга обеспечено качественными и безопасными продуктами питания\"\"\"\n]\n\npath = 'tmp_data/sample_data_city_rag.json'\n\ndata_dict = {'content': texts}\n\nwith open('tmp_data/sample_data_rag_spb.json', 'w', encoding='utf-8') as file:\n    json.dump(data_dict, file, ensure_ascii=False)\n\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\npath = 'tmp_data/sample_data_rag_spb.json'\ndataset = Dataset(data_col='content', path=path)\n\nqwen_large_api_key = os.environ.get(\"OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"OPENAI_API_BASE\")\n\nlogger.info(\"Initializing LLM connection\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n    )\n\nrag_chain = RAGChain(llm=llm)\n\nlogger.info(\"Starting generating\")\nasyncio.run(rag_chain.run(dataset, \n                          n_examples=5))\n\nlogger.info(\"Saving results\")\npath = 'tmp_data/sample_data_city_rag_generated.json'\n\n# An alternative way to save data\n# rag_chain.save_chain_output('tmp_data/sample_data_city_rag_generated.json')\n\ndf = rag_chain.data.explode('generated')\ndf['question'] = df['generated'].apply(lambda x: x['question'])\ndf['answer'] = df['generated'].apply(lambda x: x['answer'])\ndf = df[['content', 'question', 'answer']]\n\nlogger.info(f\"Writing result to {path}\")\ndf.to_json(path, orient=\"records\")\n\nlogger.info(\"Generation successfully finished\")\nlogger.info(\"Generation successfully finished\")"}
{"type": "source_file", "path": "protollm/rags/pipeline/docs_processing/entities.py", "content": "from enum import Enum\nfrom json import load\nfrom typing import Iterator\n\nfrom langchain_core.document_loaders import BaseLoader\nfrom langchain_core.documents import Document\nfrom langchain_core.load import load as ln_load\n\nfrom protollm.raw_data_processing.docs_transformers import ChunkMerger, RecursiveSplitter\nfrom protollm.raw_data_processing.docs_transformers.metadata_sentence_splitter import DivMetadataSentencesSplitter\nfrom protollm.raw_data_processing.docs_transformers.key_words_splitter import MultiMetadataAppender\n\n\ntransformer_object_dict = {\n    'recursive_character': RecursiveSplitter,\n    # 'list_hierarchy': ListHierarchySplitter,\n    'hierarchical_merger': ChunkMerger,\n    'div_sentence_splitter': DivMetadataSentencesSplitter,\n    'keyword_appender': MultiMetadataAppender\n}\n\n\nclass LoaderType(str, Enum):\n    docx = 'docx'\n    doc = 'doc'\n    odt = 'odt'\n    rtf = 'rtf'\n    pdf = 'pdf'\n    directory = 'directory'\n    zip = 'zip'\n    json = 'json'\n\n\nclass LangChainDocumentLoader(BaseLoader):\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n\n    def lazy_load(self) -> Iterator[Document]:\n        with open(self.file_path, 'r') as f:\n            for i, doc_dict in load(f).items():\n                yield ln_load(doc_dict)\n"}
{"type": "source_file", "path": "examples/real_world/chemical_multi_agent_system/chain.py", "content": "\"\"\"\nExample of multi-agents chemical pipeline with tools for drug generation (by API).\nThere are 2 agents.\n\nProcess:\n- Reading from a file with example queries. \n- Pipeline starts. \n- Decomposer define tasks.\n- Conductor-executor agent define and run tools, reflects on everyone tool response and return answer.\n\"\"\"\nfrom langchain.agents import (\n    create_structured_chat_agent,\n    AgentExecutor\n)\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\nimport pandas as pd\nfrom protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\nfrom examples.real_world.chemical_multi_agent_system.tools import gen_mols_parkinson, gen_mols_lung_cancer, gen_mols_acquired_drug_resistance, \\\n         gen_mols_dyslipidemia, gen_mols_multiple_sclerosis, gen_mols_alzheimer, request_mols_generation\nfrom examples.real_world.chemical_multi_agent_system.prompting import system_prompt_conductor, system_prompt_decomposer, human_prompt\n\n\ntools = [gen_mols_parkinson, gen_mols_lung_cancer, gen_mols_acquired_drug_resistance,\n         gen_mols_dyslipidemia, gen_mols_multiple_sclerosis, gen_mols_alzheimer, request_mols_generation]\n\n\nclass Chain:\n    def __init__(self, key: str):\n        self.llm = Llama31ChatModel(\n            api_key=key, \n            base_url=\"https://api.vsegpt.ru/v1\",\n            model=\"meta-llama/llama-3.1-70b-instruct\",\n            temperature=0.5, max_tokens=5000\n        )\n        self.agents_meta = {\n            'conductor': {\n                'prompt': system_prompt_conductor,\n                'variables': [\"tools\", \"tool_names\"]},\n            'decomposer': {\n                'prompt': system_prompt_decomposer,\n                'variables': []\n            }}\n        self.conductor = self._create_agent_executor('conductor')\n        self.decomposer = self.llm\n    \n    def _complite_prompt_from_template(self, text: str, input_variables: list = [\"tools\", \"tool_names\"]):\n        system_message = SystemMessagePromptTemplate.from_template(\n            text,\n            input_variables=input_variables\n        )\n        human_message = HumanMessagePromptTemplate.from_template(\n            human_prompt,\n            input_variables=[\"input\", \"agent_scratchpad\"]\n        )\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                system_message,\n                MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n                human_message\n            ]\n        )\n        return prompt\n    \n    def _complite_prompt_no_template(self, system_txt: str, user_txt: str):\n        messages = [\n            SystemMessage(\n                content=system_txt\n            ),\n            HumanMessage(\n                content=user_txt\n            )\n        ]\n        return messages\n    \n    def _create_agent_executor(self, agent_name: str):\n        agent = create_structured_chat_agent(\n            llm=self.llm,\n            tools=tools,\n            prompt=self._complite_prompt_from_template(\n                self.agents_meta[agent_name]['prompt'], \n                self.agents_meta[agent_name]['variables']\n            ),\n            stop_sequence=True\n        )\n        executor = AgentExecutor.from_agent_and_tools(\n            agent=agent,\n            tools=tools,\n            verbose=True,\n            return_intermediate_steps=True,\n            output_keys=[\"output\"],\n            early_stopping_method=\"generate\"\n        )\n        return executor\n    \n    def run_chain(self, user_msg: str):\n        input = self._complite_prompt_no_template(self.agents_meta['decomposer']['prompt'], user_msg)\n        tasks = eval(eval(self.decomposer.invoke(input).content)['action_input'])\n        for n, task in enumerate(tasks):\n            answer = self.conductor.invoke({\n                \"input\": task\n            })[\"output\"]\n            print('Answer, part №: ', n + 1)\n            print(answer)\n\n# run pipeline on test data\nif __name__ == \"__main__\":\n    path = 'examples/chemical_pipeline/queries_responses_chemical.xlsx'\n    questions = pd.read_excel(path).values.tolist()\n    chain = Chain(key='KEY_HERE')\n    \n    for i, q in enumerate(questions):\n        print('Task № ', i)\n        chain.run_chain(q[1])\n"}
{"type": "source_file", "path": "protollm/ensembles_ma/collect_results.py", "content": "from enum import Enum\nimport uuid\nimport requests\nfrom websockets import ConnectionClosed\nimport websockets.sync.client as wsclient\nimport json\nimport click\nimport pandas as pd\n\n\nclass AnswerType(str, Enum):\n    RETRIEVAL = 'retrieval'\n    ANSWER = 'answer'\n    ERROR = 'error'\n\ndef parse_ws_response(response):\n    response_body = json.loads(response)\n    match (name := response_body.get('name')):\n        case AnswerType.RETRIEVAL | AnswerType.ANSWER:\n            return name, response_body['result']\n        case AnswerType.ERROR:\n            raise Exception(response_body.get('detail'))\n        case _:\n            return None, None\n        \n\n@click.command()\n@click.option(\"--basepath\", type=str, default=\"0.0.0.0:8080\")\n@click.option(\"--output\", type=str, default=\"agents_responses.csv\")\ndef main(basepath, output):\n    run_uid = str(uuid.uuid4())\n    questions = [\n        \"Какой объем финансирования программы политики защиты окружающей среды\",\n        \"Какая разница в объеме финансирования программ защиты окружающей среды и образования?\",\n        \"Кто ответственный исполнитель программы политики защиты окружающей среды?\",\n        \"Какие целевые показатели госпрограмм по образованию и защите окружающей среды?\",\n        \"Сколько подпрограмм в госполитике по защите окружающей среды?\",\n        \"Какие приоритеты политики в плане обращений с твердыми коммунальными отходами?\",\n        \"какой объем финансирования программы образования в 2017 году?\",\n    ]\n\n    response = requests.get(f\"http://{basepath}/\", params={\"agent_type\": \"streaming\"})\n    assert response.status_code == 200, \"Failed to get agents\"\n    response = response.json()\n    assert len(response) > 0, \"No agents found\"\n    agents_ids = {agent['agent_id']: agent['name'] for agent in response if \"rag\" in agent['name']}\n\n    agents_responses = list()\n    for question in questions:\n        question_columns = dict(question=question)\n        for agent_id, agent_name in agents_ids.items():\n            click.echo(f\"Collecting response from {agent_name}, {question=}\")\n            with wsclient.connect(f\"ws://{basepath}/agent\") as ws:\n                request_payload = {\n                    \"dialogue_id\": run_uid,\n                    \"agent_id\": agent_id,\n                    \"chat_history\":[],\n                    \"query\": question,\n                    \"run_params\": {}\n                }\n                ws.send(json.dumps(request_payload))\n                try:\n                    while True:\n                        response = ws.recv()\n                        response_type, response_data = parse_ws_response(response)\n                        if response_type == AnswerType.RETRIEVAL:\n                            question_columns[f'docs_{agent_name}'] = response_data\n                        elif response_type == AnswerType.ANSWER:\n                            question_columns[f'answer_{agent_name}'] = response_data\n                except ConnectionClosed:\n                    pass\n        click.echo(\"Finished collecting RAG agents responses\")\n\n        for endpoint in ('router', 'ensemble'):\n            click.echo(f\"Collecting response from {endpoint}, {question=}\")\n            with wsclient.connect(f\"ws://{basepath}/{endpoint}\") as ws:\n                request_payload = {\n                    \"dialogue_id\": run_uid,\n                    \"chat_history\":[],\n                    \"query\": question,\n                }\n                ws.send(json.dumps(request_payload))\n                try:\n                    while True:\n                        response = ws.recv()\n                        response_type, response_data = parse_ws_response(response)\n                        if response_type == AnswerType.RETRIEVAL:\n                            question_columns[f'docs_{endpoint}'] = response_data\n                        elif response_type == AnswerType.ANSWER:\n                            question_columns[f'answer_{endpoint}'] = response_data\n                except ConnectionClosed:\n                    pass\n        click.echo(\"Finished collecting router and ensemble responses\")\n        click.echo(f\"Collected question_columns: {question_columns}\")\n        agents_responses.append(question_columns)\n    click.echo(\"Finished collecting agents responses\")\n\n    df = pd.DataFrame().from_records(agents_responses)\n    df.to_csv(output, index=False)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "examples/real_world/chemical_multi_agent_system/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm-synthetic/examples/quiz_example.py", "content": "import os\nfrom protollm_synthetic.synthetic_pipelines.chains import QuizChain\nfrom protollm_synthetic.utils import Dataset, VLLMChatOpenAI\nimport json\nimport asyncio\n\n# создаем игрушечный датасет из вырезок случайных статей википедии\ntexts = [\n    \"\"\"Фэрфилд (англ. Fairfield) — город в США, расположенный в восточной части штата Техас, административный центр округа Фристон. По данным переписи за 2010 год число жителей составляло 2951 человек, по оценке Бюро переписи США в 2018 году в городе проживало 2922 человек. Изначально место называлось Маунд-Прейри, однако, когда поселение выбрали административным центром округа, оно было переименовано в Фэрфилд. Плодородные земли, лесные запасы, запасы пресной воды и близость к реке Тринити привлекли поселенцев из восточных штатов. В 1851 году открылось городское почтовое отделение, спустя год в городе работали три магазина, два отеля и тюрьма. В 1853 году была основана масонская ложа. В 1854 году открылась школа для девочек, школа для мальчиков открылась в 1856 году. Первая еженедельная газета Texas Pioneer издавалась с 1857 года.\"\"\",\n    \"\"\"Филип Джеймс Плейсер Янгхазбанд (англ. Philip James Placer Younghusband; 4 августа 1987, Ашфорд) — английский и филиппинский футболист, нападающий. Рекордсмен по числу игр и голов за сборную Филиппин. Братья Янгхазбанды являются воспитанниками футбольной академии \"Челси\". Фил начинал играть на позиции нападающего. В сезоне 2003/04 он стал лучшим бомбардиром юношеской команды \"Челси\", а в 2004/05 лучшим бомбардиром молодёжной команды. В сезоне 2005/06 сыграл 21 матч и забил 5 голов за резерв \"Челси\". В августе 2007 года был отдан в аренду в клуб датской Суперлиги \"Эсбьерг\", однако ни одного матча за эту команду так и не провёл. Летом 2008 года контракт Янгхазбанда с \"Челси\" истёк, и он уехал на Филиппины.\"\"\",\n    \"\"\"Модель вложенного множества (англ. Nested set model) — это способ представления вложенных множеств[англ.] (известных как деревья или иерархии) в реляционных базах данных. Стандартная реляционная алгебра и реляционное исчисление, а также операции SQL, основанные на них, не могут напрямую выразить все желаемые операции над иерархиями. Вложенная модель множества является решением этой проблемы. Альтернативным решением является выражение иерархии как отношения родитель-потомок. Селко назвал это списком смежности. Если иерархия может иметь произвольную глубину, то список смежности не допускает выражения операций, как сравнение содержимого иерархий двух элементов или определение того, находится ли элемент где-то в подиерархии другого элемента. Когда иерархия имеет фиксированную или ограниченную глубину, операции возможны, но дорогостоящие, из-за необходимости выполнения на каждом уровне одного реляционного соединения. Это часто называют проблемой спецификации материалов.\"\"\"\n]\n\ndata_dict = {'content': texts}\n\nwith open('tmp_data/sample_data_rag.json', 'w', encoding='utf-8') as file:\n    json.dump(data_dict, file, ensure_ascii=False)\n\ndataset = Dataset(data_col='content', path='tmp_data/sample_data_rag.json')\n\nqwen_large_api_key = os.environ.get(\"OPENAI_API_KEY\")\nqwen_large_api_base = os.environ.get(\"OPENAI_API_BASE\")\n\nllm=VLLMChatOpenAI(\n        api_key=qwen_large_api_key,\n        base_url=qwen_large_api_base,\n        model=\"/model\",\n        max_tokens=2048,\n        # max_concurrency=10\n    )\n\nquiz_chain = QuizChain(llm=llm)\nasyncio.run(quiz_chain.run(dataset, n_examples=3))  \n\nprint(quiz_chain.generated)"}
{"type": "source_file", "path": "protollm/metrics/deepeval_connector.py", "content": "import os\n\nfrom deepeval.models.base_model import DeepEvalBaseLLM\nfrom langchain_core.language_models.chat_models import BaseChatModel\nfrom pydantic import BaseModel\nfrom openai._types import NOT_GIVEN\n\nfrom ..connectors import create_llm_connector\n\n\nclass DeepEvalConnector(DeepEvalBaseLLM):\n    \"\"\"Implementation of Evaluation agent based on large language model for Assistant's answers evaluation.\n\n    Uses the LangChain's ChatModel to make requests to a compatible API. Must inherit from the base class and\n    implement a set of methods.\n    The vsegpt.ru service is used by default, so in the configuration file it is necessary to specify the API key of\n    this service and the model name, as in the general case.\n    \"\"\"\n\n    def __init__(self, sys_prompt: str = \"\", *args, **kwargs):\n        \"\"\"Initialize instance with evaluation LLM.\n\n        Args:\n            sys_prompt: predefined rules for model\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self._sys_prompt = sys_prompt\n        self.model = self.load_model()\n\n    @staticmethod\n    def load_model() -> BaseChatModel:\n        \"\"\"Returns LangChain's ChatModel for requests\"\"\"\n        return create_llm_connector(os.getenv(\"DEEPEVAL_LLM_URL\", \"test_model\"))\n\n    def generate(\n            self,\n            prompt: str,\n            *args,\n            **kwargs,\n    ) -> str | BaseModel:\n        \"\"\"Get a response from LLM to given question.\n\n        Args:\n            prompt (str): Query, the model must answer.\n\n        Returns:\n            str: Model's response.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self._sys_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        response_format = kwargs.get(\"schema\", NOT_GIVEN)\n        if response_format == NOT_GIVEN:\n            return self.model.invoke(messages).content\n        else:\n            struct_llm = self.model.with_structured_output(schema=response_format, method=\"json_mode\")\n            return struct_llm.invoke(messages)\n\n    async def a_generate(\n            self,\n            prompt: str,\n            *args,\n            **kwargs,\n    ) -> str:\n        \"\"\"Same as synchronous generate method just because it must be implemented\"\"\"\n        return self.generate(\n            prompt, *args, **kwargs\n        )\n\n    def get_model_name(self, *args, **kwargs) -> str:\n        \"\"\"Returns a description of what the class is about\"\"\"\n        return \"Implementation of custom LLM connector using OpenAI compatible API for evaluation.\"\n"}
{"type": "source_file", "path": "examples/real_world/urbanistics/rag_example.py", "content": "import chromadb\nimport os\nimport uuid\nfrom dotenv import load_dotenv\nfrom langchain_community.embeddings.huggingface_hub import HuggingFaceHubEmbeddings\n\nfrom protollm_sdk.models.job_context_models import PromptModel\nfrom protollm_sdk.jobs.outer_llm_api import OuterLLMAPI\nfrom protollm.rags.rag_core.retriever import DocRetriever, DocsSearcherModels\n\nfrom protollm.definitions import CONFIG_PATH\n\n\ndef init_chroma_client():\n    host, port = os.environ.get(\"CHROMA_DEFAULT_SETTINGS\").split(':')\n    return chromadb.HttpClient(\n        host=host,\n        port=int(port),\n        settings=chromadb.Settings(),\n    )\n\n\ndef proto_view(\n    query: str,\n    collection: str,\n    k: int = 1,\n    embedding_function: HuggingFaceHubEmbeddings = None,\n) -> list:\n    # Returns k chunks that are closest to the query\n    embedding_host = os.environ.get(\"EMBEDDING_HOST\")\n    embedding_function = HuggingFaceHubEmbeddings(model=embedding_host)\n    chroma_client = init_chroma_client()\n\n    docs_searcher_models = DocsSearcherModels(embedding_model=embedding_function, chroma_client=chroma_client)\n    retriever = DocRetriever(top_k=k,\n                             docs_searcher_models=docs_searcher_models,\n                             )\n\n    return retriever.retrieve_top(collection_name=collection, query=query)\n\n\ndef outer_llm(question: str,\n              meta: dict,\n              key: str):\n    llmapi = OuterLLMAPI(key)\n    llm_request = PromptModel(job_id=str(uuid.uuid4()),\n                              meta=meta,\n                              content=question)\n    res = llmapi.inference(llm_request)\n    return res.content\n\n\nif __name__ == \"__main__\":\n    load_dotenv(CONFIG_PATH)\n\n    # Настройки БЯМ\n    meta = {\"temperature\": 0.05,\n            \"tokens_limit\": 4096,\n            \"stop_words\": None}\n    key = os.environ.get(\"VSE_GPT_KEY\")\n\n\n    # Название коллекции в БД\n    collection_name = \"strategy-spb\"\n\n    # Вопрос\n    question = 'Какие задачи Стратегия ставит в области энергосбережения?'\n\n    # Извлечение контекста из БД\n    context = proto_view(question, collection_name)\n    context = f'Вопрос: {question} Контекст: {context[0].page_content}'\n\n    # Получение ответа от БЯМ\n    print(f'Ответ VseGPT LLM: \\n {outer_llm(context, meta, key)}')\n"}
{"type": "source_file", "path": "protollm/metrics/__init__.py", "content": "from .evaluation_metrics import correctness_metric, model_for_metrics\n"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/synthetic_pipelines/chains.py", "content": "from typing import List\nimport json\nimport copy\nfrom datetime import datetime\nimport logging\nfrom protollm_synthetic.synthetic_pipelines.prompts import (generate_summary_system_prompt, generate_summary_evaluation_system_prompt,\n                                                        generate_rag_system_prompt, check_summary_quality_human_prompt,\n                                                        generate_rag_human_prompt, generate_aspect_summarisation_prompt,\n                                                        generate_summary_human_prompt, generate_aspect_summarisation_evaluation_system_prompt,\n                                                        generate_quiz_system_prompt, generate_quiz_human_prompt,\n                                                        generate_instruction_one_shot_system_prompt, generate_instruction_one_shot_human_prompt,\n                                                        merge_instructions, merge_instructions_human_prompt)\nfrom protollm_synthetic.utils import Dataset\nimport numpy as np\nimport asyncio\nfrom typing import List, Optional, Dict, Any, TypeVar, cast\n\nfrom langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom langchain.output_parsers import RetryOutputParser\nfrom langchain_core.exceptions import OutputParserException\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.prompt_values import PromptValue\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import (Runnable, RunnablePassthrough, \n                                      RunnableParallel, RunnableLambda)\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom openai import APIConnectionError\nfrom protollm_synthetic.synthetic_pipelines.genetic_evolver import GeneticEvolver\n\nimport random\nfrom protollm_synthetic.synthetic_pipelines.schemes import (SummaryQualitySchema, \n                                                       RAGScheme, AspectSummarisationQualitySchema,\n                                                       QuizScheme, FreeQueryScheme, FreeQueryMerger)\n\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nTT = TypeVar(\"TT\")\n\nclass BaseChain:\n    def __init__(self, llm: BaseChatModel, max_retry_attempts: int = 3):\n        self._max_retry_attempts = max_retry_attempts\n        self.llm = llm\n        self.data = None\n\n    def _enhance_prompt_with_cot(self, prompt: str) -> str:\n        return (\n            prompt +\n            \"\\nLet's approach the task step by step:\"\n            \"\\n1. First, analyze the important elements for the task.\"\n            \"\\n2. Then, analyze their interconnections.\"\n            \"\\n3. Finally, form the conclusion in the required format.\"\n            \"\\n\\nThe response should be in JSON format according to the schema.\\n\\n\"\n        )\n    \n    def _enhance_prompt_with_tot(self, prompt: str) -> str:\n        return (\n            prompt +\n            \"\\nConsider several solutions before forming the final conclusion. \"\n            \"Ensure the final answer matches the specified JSON format.\"\n        )\n\n    def _prepare_prompt(self, system_template, human_template, x: Dict[str, Any]) -> PromptValue:\n        prompt = ChatPromptTemplate(\n            messages=[\n                SystemMessagePromptTemplate.from_template([{\"text\": system_template.replace('\"',\"\")}]),\n                HumanMessagePromptTemplate.from_template([{\"text\": human_template}])\n            ]\n        )\n\n        args = x.copy()\n        if \"date\" not in args:\n            args[\"date\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        prompt_value = prompt.invoke(args)\n        return prompt_value\n\n    async def _run_chain_with_retries(self,\n                                      chain: Runnable,\n                                      chain_kwargs: Dict[str, Any],\n                                      max_retry_attempts: Optional[int] = None) -> Optional[Any]:\n        retry_attempts = max_retry_attempts or self._max_retry_attempts\n        retry_delay = 2\n        attempt = 0\n        result = None\n\n        while attempt < retry_attempts:\n            try:\n                result = await chain.ainvoke(chain_kwargs)\n            except OutputParserException:\n                logger.warning(\"Parsing error occurred. Interrupting execution.\")\n                raise\n            except APIConnectionError:\n                logger.warning(\n                    f\"Encountered APIConnectionError. Attempt: {attempt}. Will retry after delay {retry_delay} seconds\",\n                    exc_info=True\n                )\n                await asyncio.sleep(retry_delay)\n            except Exception as e:\n                logger.warning(f\"Unexpected error during chain execution: {str(e)}\", exc_info=False)\n                \n            attempt += 1\n\n        return result\n\n    def create_chain(self,\n                          system_template: str,\n                          human_template: str,\n                          parser: Optional[PydanticOutputParser] = None,\n                          max_parsing_retries: int = 3,\n                          use_chain_of_thought: bool = False,\n                          use_tree_of_thought: bool = False) -> Runnable:\n        \"\"\"Helper method to create chains that process text documents\"\"\"\n        if use_chain_of_thought:\n            human_template = self._enhance_prompt_with_cot(human_template)\n        \n        if use_tree_of_thought:\n            system_template = self._enhance_prompt_with_tot(system_template)\n\n        prompt = RunnableLambda(lambda x: self._prepare_prompt(system_template, human_template, x))\n\n        chain = (\n            prompt\n            | RunnableParallel(completion=self.llm, prompt_value=RunnablePassthrough())\n        )\n\n        if parser:\n            retry_planner_parser = RetryOutputParser.from_llm(\n                parser=parser,\n                llm=self.llm,\n                prompt=PromptTemplate.from_template(\"{prompt}\"),\n                max_retries=max_parsing_retries\n            )\n\n            def _do_parsing_retrying(x: dict):\n                result = None\n                completion = x['completion'].content\n                prompt_value = x['prompt_value']\n\n                try:\n                    result = retry_planner_parser.parse_with_prompt(completion=completion, prompt_value=prompt_value)\n                except OutputParserException:\n                    prompt_value = cast(PromptValue, x['prompt_value'])\n                    logger.warning(\"Proceeding without result due to parser errors (even after retrying). \"\n                                   \"Prompt - %s\" % prompt_value)\n\n                return result\n\n            chain = (\n                RunnableLambda(lambda x: {**x, \"response_format_description\": parser.get_format_instructions()})\n                | chain\n                | RunnableLambda(_do_parsing_retrying, name=\"retry_planner_lambda\")\n            )\n\n        return chain\n\n    def save_chain_output(self, path: str):\n        if 'generated' in self.data.columns:  \n            recordings = self.data.to_dict()\n            with open(path, 'w', encoding='utf-8') as file:\n                json.dump(recordings, file, ensure_ascii=False)\n        else:\n            logger.warning(\"No output to save. Chain has not been run.\")\n\n    def _sample_data(self, n_samples: int):\n        \"\"\"Sample a specified number of examples from the DataFrame.\"\"\"\n        if self.data is not None:\n            n_samples = min(n_samples, len(self.data))\n            self.data = self.data.sample(n=n_samples, random_state=random.randint(0, 10000)).reset_index(drop=True)  # Ensure we do not sample more than available\n        else:\n            logger.warning(\"No data to sample. Chain has not been run.\")\n\n\nclass SummarisationChain(BaseChain):\n    def __init__(self, llm: Optional[BaseChatModel] = None):\n        super().__init__(llm=llm)\n\n    async def _generate_summaries(self, data: List[str]) -> List[str]:\n        chain = self.create_chain(system_template=generate_summary_system_prompt(), \n                                  human_template=generate_summary_human_prompt(),)\n        summaries = await chain.abatch(inputs=[{'text': i} for i in data])\n        return summaries\n\n    async def _evaluate_summary_quality(self, summary: List[str], text: List[str]) -> List[SummaryQualitySchema]:\n        parser = PydanticOutputParser(pydantic_object=SummaryQualitySchema)\n        chain = self.create_chain(\n            system_template=generate_summary_evaluation_system_prompt(), \n            human_template=check_summary_quality_human_prompt(), \n            parser=parser\n        )\n        \n        # TODO: Use _run_chain_with_retries to handle retries\n        response = await chain.abatch(inputs=[{'text': i, 'summary': j} for i, j in zip(text, summary)])\n        return response\n    \n    async def postprocess(self, summaries: List[str], data: List[str]) -> Dataset:\n        quality = await self._evaluate_summary_quality(summaries, data)\n        logger.info(f\"Summary Quality: {np.mean([int(sample.score) for sample in quality])}\")\n        # select only data indices with quality score above 0.8\n        good_indices = [i for i in range(len(quality)) if quality[i].score > 0.8]\n        return good_indices\n    \n    async def _generate_genetic_evolution(self, dataset: Dataset) -> str:\n        # TODO: implement genetic evolution\n        genetic_evolver = GeneticEvolver(initial_population=dataset.data[dataset.data_col].tolist(), \n                                        generations=10, mutation_rate=0.1)\n        results = genetic_evolver.evolve()\n        return results\n    \n    async def run(self, \n            dataset: Dataset, \n            n_examples: int = 3,\n            do_postprocess: bool = True,\n            do_genetic_evolution: bool = False\n            ) -> str:\n        data = dataset.data[dataset.data_col].tolist()\n        # sampling without changing the order of the data\n        idx = random.sample(range(len(data)), n_examples)\n        data = [data[i] for i in sorted(idx)]\n        logger.info(f\"Running summarisation chain on {n_examples} examples\")\n        summaries = await self._generate_summaries(data)\n        summaries = [i['completion'].content for i in summaries]\n        dataset.data['generated'] = summaries\n        self.generated = dataset.data\n        logger.info(f\"Summary examples: {random.sample(summaries, 1)}\")\n        if do_postprocess:\n            good_indices = await self.postprocess(summaries, data)\n            dataset.data = dataset.data.iloc[good_indices]\n        if len(dataset.data) > 10 and do_genetic_evolution:\n            genetic_evolution = await self._generate_genetic_evolution(dataset)\n            dataset.data['generated'] = genetic_evolution\n        return summaries\n    \n# TODO: Implement aspect summarisation chain\nclass AspectSummarisationChain(BaseChain):\n    def __init__(self, llm: BaseChatModel):\n        if llm is None:\n            raise ValueError(\"LLM must be provided for AspectSummarisationChain\")\n        super().__init__(llm=llm)\n\n    async def _generate_aspect_summaries(self, \n                                         data: List[str], \n                                         aspect: str) -> List[str]:\n        chain = self.create_chain(system_template=generate_aspect_summarisation_prompt(aspect=aspect), \n                                  human_template=generate_summary_human_prompt(),)\n        summaries = await chain.abatch(inputs=[{'text': i} for i in data])\n        return summaries\n\n    async def _evaluate_summary_quality(self, summary: List[str], text: List[str], aspect: str) -> List[SummaryQualitySchema]:\n        parser = PydanticOutputParser(pydantic_object=AspectSummarisationQualitySchema)\n        chain = self.create_chain(\n            system_template=generate_aspect_summarisation_evaluation_system_prompt(aspect=aspect), \n            human_template=check_summary_quality_human_prompt(), \n            parser=parser\n        )\n        \n        # TODO: Use _run_chain_with_retries to handle retries\n        response = await chain.abatch(inputs=[{'text': i, 'summary': j} for i, j in zip(text, summary)])\n\n        return response\n\n    async def run(self, \n                  dataset: Dataset, \n                  aspect: str,\n                  n_examples: int = 3,\n                  do_genetic_evolution: bool = False\n                  ) -> str:\n        data = self._sample_data(dataset.data, n_examples)  # Use the new sampling method\n        logger.info(f\"Running aspect summarisation chain on {n_examples} examples\")\n        summaries = await self._generate_aspect_summaries(data[dataset.data_col].tolist(), aspect=aspect)\n        summaries = [i['completion'].content for i in summaries]\n        dataset.data['generated'] = summaries\n        self.generated = dataset.data\n        logger.info(f\"Summary examples: {random.sample(summaries, 1)}\")\n        quality = await self._evaluate_summary_quality(summaries, data[dataset.data_col].tolist(), aspect=aspect)\n        logger.info(f\"Summary Quality: {np.mean([int(sample.score) for sample in quality])}\")\n        if len(dataset.data) > 10 and do_genetic_evolution:\n            genetic_evolution = await self._generate_genetic_evolution(dataset)\n            dataset.data['generated'] = genetic_evolution\n        return summaries\n    \n    # TODO: Implement postprocessing of the dataset\n    def postprocess(self, dataset: Dataset) -> Dataset:\n        dataset.data['generated'] = [i['completion'].content for i in self.generated]\n        return dataset\n\nclass RAGChain(BaseChain):\n    def __init__(self, llm: BaseChatModel):\n        if llm is None:\n            raise ValueError(\"LLM must be provided for RAGChain\")\n        super().__init__(llm=llm)\n\n    async def _generate_rag_examples(self, data: List[str]) -> str:\n        parser = PydanticOutputParser(pydantic_object=RAGScheme)\n        chain = self.create_chain(\n            system_template=generate_rag_system_prompt(), \n            human_template=generate_rag_human_prompt(),\n            parser=parser\n        )\n        rag_examples = await chain.abatch(inputs=[{'text': i} for i in data])\n        return rag_examples\n\n    async def run(self, \n                  dataset: Dataset, \n                  n_examples: int = 3,\n                  do_genetic_evolution: bool = False\n                  ) -> str:\n        self.data = copy.deepcopy(dataset.data)\n        self._sample_data(n_samples=n_examples)  # Use the new sampling method\n        logger.info(f\"Running RAG chain on {n_examples} examples\")\n        rag_examples = await self._generate_rag_examples(self.data[dataset.data_col].tolist())\n        logger.info(f\"RAG num examples generated: {len(rag_examples)}\")\n        # logger.debug(f\"RAG examples: {random.sample(rag_examples, 1)}\")\n        print(len(rag_examples), len(self.data))\n        self.data['generated'] = [\n            [\n                {\n                    'question': rag_examples[i].context[j].question,\n                    'answer': rag_examples[i].context[j].answer,\n                    'difficulty': rag_examples[i].context[j].difficulty\n                }\n                for j in range(len(rag_examples[i].context))\n                if rag_examples[i].context[j] is not None\n             ]\n            for i in range(len(rag_examples))\n            if rag_examples[i] is not None\n        ]\n        if len(dataset.data) > 10 and do_genetic_evolution:\n            genetic_evolution = await self._generate_genetic_evolution(dataset)\n            dataset.data['generated'] = genetic_evolution\n\n        return rag_examples\n\n\nclass QuizChain(BaseChain):\n    def __init__(self, llm: BaseChatModel):\n        super().__init__(llm=llm)\n\n\n    async def _generate_quiz(self, dataset: Dataset) -> str:\n        parser = PydanticOutputParser(pydantic_object=QuizScheme)\n        chain = self.create_chain(system_template=generate_quiz_system_prompt(), \n                                  human_template=generate_quiz_human_prompt(),\n                                  parser=parser)\n        quiz = await chain.abatch(inputs=[{'text': i} for i in dataset.data[dataset.data_col].tolist()])\n        return quiz\n    \n    def postprocess(self, dataset: pd.DataFrame) -> Dataset:\n        raise NotImplementedError(\"Not implemented\")\n    \n    async def run(self, \n                  dataset: Dataset, \n                  n_examples: int = 3,\n                  do_postprocess: bool = False,\n                  do_genetic_evolution: bool = False\n                  ) -> str:\n        self.data = copy.deepcopy(dataset.data)\n        self._sample_data(n_samples=n_examples)\n        logger.info(f\"Running quiz chain on {n_examples} examples\")\n        quiz = await self._generate_quiz(self.data[dataset.data_col].tolist())\n        self.data['generated'] = quiz\n        if do_postprocess:\n            dataset = self.postprocess(self.data)\n        if len(dataset.data) > 10 and do_genetic_evolution:\n            genetic_evolution = await self._generate_genetic_evolution(dataset)\n            dataset.data['generated'] = genetic_evolution\n        return quiz\n\nclass FreeQueryChain(BaseChain):\n    def __init__(self, llm: Optional[BaseChatModel] = None):\n        super().__init__(llm=llm)\n\n    async def _generate_free_query_instruction(self, examples: List[str], solutions: List[str]) -> List[FreeQueryScheme]:\n        parser = PydanticOutputParser(pydantic_object=FreeQueryScheme)\n        # TODO: add few-shot instruction prompt\n        chain = self.create_chain(system_template=generate_instruction_one_shot_system_prompt(), \n                                  human_template=generate_instruction_one_shot_human_prompt(),\n                                  parser=parser)\n        free_query_instruction = await chain.abatch(inputs=[{'text': i, 'result': j} for i, j in zip(examples, solutions)])\n        return free_query_instruction\n    \n    async def _combine_free_query_instruction(self, free_query_instruction: List[FreeQueryScheme]) -> str:\n        parser = PydanticOutputParser(pydantic_object=FreeQueryMerger)\n        chain = self.create_chain(system_template=merge_instructions(), \n                                  human_template=merge_instructions_human_prompt(),\n                                  parser=parser)\n        \n        merged_instruction = await chain.abatch(inputs=[{'text': [json.dumps(i.model_dump()) for i in free_query_instruction]}])\n        return merged_instruction\n    \n    async def _generate_result_with_instruction(self, instruction: FreeQueryMerger, \n                                                text: List[str], num_retries: int = 3) -> str:\n        complete_system_prompt = f\"\"\"{instruction.system_role_part} \\n\\n\n        {instruction.system_general_instruction_part} \\n\\n  \n        {instruction.system_specifics_instruction_part} \\n\\n\n        {instruction.system_output_format_part}\"\"\"\n        human_prompt = \"\"\"Text that should be transformed: {text}\"\"\"\n        chain = self.create_chain(system_template=complete_system_prompt, \n                                  human_template=human_prompt,)\n        for i in range(3):\n            try:\n                result = await chain.abatch(inputs=[{'text': i} for i in text])\n                return result\n            except Exception as e:\n                logger.error(f\"Error during result generation: {e}\")\n        return result\n\n    async def run(self, dataset: Dataset, \n            n_examples: int = 3, \n            initial_instruction: str = None) -> str:\n        self.data = copy.deepcopy(dataset.data)\n        self._sample_data(n_samples=n_examples)\n        logger.info(f\"Running free query chain on {n_examples} examples: Instruction Extraction\")\n        free_query_instruction = await self._generate_free_query_instruction(dataset.labeled_data[dataset.data_col].tolist(), \n                                                                       dataset.labeled_data[dataset.labels_col].tolist())\n        logger.info(f\"Running free query chain on {n_examples} examples: Instruction Merging\")\n        merged_instruction = await self._combine_free_query_instruction(free_query_instruction)\n        logger.info(f\"Running free query chain on {n_examples} examples: Result Generation\")\n        result = await self._generate_result_with_instruction(merged_instruction[0], dataset.data[dataset.data_col].tolist())\n        self.data['generated'] = result\n"}
{"type": "source_file", "path": "protollm/rags/pipeline/docs_processing/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm/rags/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/synthetic_pipelines/schemes.py", "content": "from pydantic import BaseModel, Field\n\nfrom typing import List\n\nclass SummaryQualitySchema(BaseModel):\n    is_informative: bool = Field(\n        ...,\n        description=\"Does the summary contain all the necessary information from the text?\"\n    )\n    is_concise: bool = Field(\n        ...,\n        description=\"Is the summary concise and to the point?\"\n    )\n    is_language_correct: bool = Field(\n        ...,\n        description=\"Is the summary in the same language as the original text?\"\n    )\n    score: int = Field(\n        ...,\n        description=\"Based on the evaluated criteria of the summary, score the resulting summary from 1 to 10\"\n    )\n\nclass AspectSummarisationQualitySchema(BaseModel):\n    is_aspect_covered: bool = Field(\n        ...,\n        description=\"Does the aspect summarisation cover the aspect?\"\n    )\n    is_aspect_informative: bool = Field(\n        ...,\n        description=\"Is the aspect summarisation informative?\"\n    )   \n    is_aspect_concise: bool = Field(\n        ...,\n        description=\"Is the aspect summarisation concise?\"\n    )\n    is_language_correct: bool = Field(\n        ...,\n        description=\"Is the aspect summarisation in the same language as the original text?\"\n    )\n    score: int = Field(\n        ...,\n        description=\"Based on the evaluated criteria of the aspect summarisation, score the resulting aspect summarisation from 1 to 10\"\n    )\n\nclass RAGExample(BaseModel):\n    context: str = Field(\n        ...,\n        description=\"The context that the RAG system should use to answer the question\"\n    )\n    question: str = Field(\n        ...,\n        description=\"The question that the RAG system should answer\"\n    )\n    difficulty: str = Field(\n        ...,\n        description=\"The difficulty of the question\"\n    )\n    answer: str = Field(\n        ...,\n        description=\"The answer to the question\"\n    )\n\nclass RAGScheme(BaseModel):\n    context: List[RAGExample] = Field(\n        ...,\n        description=\"The context that the RAG system should use to answer the question\"\n    )\n\nclass QuizExample(BaseModel):\n    question: str = Field(\n        ...,\n        description=\"The quiz question on a provided text\"\n    )\n    options: List[str] = Field(\n        ...,\n        description=\"The 4 options that are presented to the student. Mark the correct options\"\n    )\n\nclass QuizScheme(BaseModel):\n    quiz: List[QuizExample] = Field(\n        ...,\n        description=\"The quiz for students on a given text\"\n    )\n\nclass FreeQueryScheme(BaseModel):\n    system_role_part: str = Field(\n        ...,\n        description=\"Define a role that llm will play in solving the task, e.g. 'You are a professional teacher who is generating free-answerquestions for students to check their knowledge'\"\n    )\n    system_general_instruction_part: str = Field(\n        ...,\n        description=\"Define a general instruction for llm to follow, e.g. 'You should generate a question that is easy to understand and follow and a correct answer based on the provided text'\"\n    )\n    system_specifics_instruction_part: str = Field(\n        ...,\n        description=\"Define a set of strict rules and domain specific information for llm to know and follow, e.g. '1. The question should be in the same language as the text, 2. The question should be easy to understand and follow, 3. The question should be a free-answer question'\"\n    )\n    system_output_format_part: str = Field(\n        ...,\n        description=\"Define the output format of the llm, e.g. 'The output should be a json object with the following fields: question, options, correct_options'\"\n    )\n\n\nclass FreeQueryMerger(BaseModel):\n    system_role_part: str = Field(\n        ...,\n        description=\"Merge the proposed role parts into one role\"\n    )\n    system_general_instruction_part: str = Field(\n        ...,\n        description=\"Merge the proposed general instruction parts into one general instruction\"\n    )\n    system_specifics_instruction_part: str = Field(\n        ...,\n        description=\"Merge the proposed specifics instruction parts into one detailed instruction\"\n    )\n    system_output_format_part: str = Field(\n        ...,\n        description=\"Merge the proposed output format parts into one output format\"\n    )       \n\n\n"}
{"type": "source_file", "path": "protollm/rags/pipeline/docs_processing/exceptions.py", "content": "class PathIsNotAssigned(Exception):\n    def __init__(self, message):\n        super.__init__(message)\n\n\nclass PipelineError(Exception):\n    def __init__(self, message):\n        super().__init__(message)\n\n\nclass FileExtensionError(Exception):\n    def __init__(self, message):\n        super().__init__(message)\n\n\nclass TransformerNameError(Exception):\n    def __init__(self, message):\n        super().__init__(message)\n\n\nclass LoaderNameError(Exception):\n    def __init__(self, message):\n        super().__init__(message)"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm/rags/jobs.py", "content": "import logging\nimport uuid\nfrom typing import Optional, List, Any, Callable\n\nimport chromadb\nfrom langchain_community.embeddings.huggingface_hub import HuggingFaceHubEmbeddings\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models import LLM\n\nfrom protollm_sdk.jobs.job import Job\nfrom protollm_sdk.jobs.job_context import JobContext\nfrom protollm_sdk.models.job_context_models import PromptModel, PromptMeta, ResponseModel\n\nfrom protollm.templates.prompt_templates.rag_prompt_templates import EOS, EOT\nfrom protollm.rags.rag_core.retriever import DocsSearcherModels, DocRetriever, RetrievingPipeline\nfrom protollm.rags.rag_core.utils import run_multiple_rag\nfrom protollm.rags.settings.chroma_settings import settings\n\nfrom protollm.raw_data_processing.docs_transformers.key_words_splitter import KeywordExtractor\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n\n\ndef get_simple_retriever(docs_searcher_models: DocsSearcherModels, top_k: int,\n                         preprocess_query: Optional[Callable[[str], str]] = None) -> DocRetriever:\n    return DocRetriever(top_k, docs_searcher_models, preprocess_query)\n\n\ndef get_advanced_retriever(docs_searcher_models: DocsSearcherModels, top_ks: list[int] = None):\n    if top_ks is None:\n        top_ks = [100, 50, 10]\n    file_name_retriever = get_simple_retriever(docs_searcher_models, top_ks[0])\n    keywords_retriever = get_simple_retriever(docs_searcher_models, top_ks[1],\n                                              lambda query: KeywordExtractor().get_object_action_pair(query)[0])\n    content_retriever = get_simple_retriever(docs_searcher_models, top_ks[2])\n    return [file_name_retriever, keywords_retriever, content_retriever]\n\n\ndef get_pipelines_retriever(list_retrievers: list[list[DocRetriever]], list_collection_names: list[list[str]]) \\\n        -> list[RetrievingPipeline]:\n    return [RetrievingPipeline().set_retrievers(retrievers).set_collection_names(collection_names)\n            for retrievers, collection_names in zip(list_retrievers, list_collection_names)]\n\n\nclass CustomLLM(LLM):\n    ctx: JobContext = None\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        temperature = kwargs.get(\"temperature\", 0)\n        tokens_limit = kwargs.get(\"tokens_limit\", 8000)\n        stop_words = kwargs.get(\"stop_words\", [EOS, EOT])\n\n        return self.ctx.outer_llm_api.inference(\n            PromptModel(\n                job_id=str(uuid.uuid4()),\n                content=prompt,\n                meta=PromptMeta(temperature=temperature, tokens_limit=tokens_limit, stop_words=stop_words)\n            )\n        ).content\n\n    def _llm_type(self) -> str:\n        return 'custom'\n\n\nclass RAGJob(Job):\n    def run(self, job_id: str, ctx: JobContext, **kwargs: Any):\n        \"\"\"\n        Run RAG pipeline and save the LLM response to outer database\n\n        :param user_prompt: input question\n        :param use_advanced_rag:\n        :return: None\n        \"\"\"\n        user_prompt: str | list[str] = kwargs['user_prompt']\n        use_advanced_rag: bool = kwargs.get('use_advanced_rag', False)\n        client = chromadb.HttpClient(host=settings.chroma_host, port=settings.chroma_port)\n        llm = CustomLLM(ctx=ctx, n=10)\n        encoder = HuggingFaceHubEmbeddings(huggingfacehub_api_token='any-token', model=settings.embedding_host)\n        docs_searcher_models = DocsSearcherModels(embedding_model=encoder, chroma_client=client)\n\n        logger.info('Necessary setting setup')\n\n        simple_pipeline = get_pipelines_retriever([[get_simple_retriever(docs_searcher_models, 10)]], [[settings.collection_name]])\n        advanced_pipeline = get_pipelines_retriever(\n            [[get_simple_retriever(docs_searcher_models, 10)], get_advanced_retriever(docs_searcher_models)],\n            [[settings.collection_name], settings.collection_names_for_advance]\n        )\n\n        pipelines = simple_pipeline if not use_advanced_rag else advanced_pipeline\n\n        # Run RAG process and obtain the final LLM response\n        llm_response = run_multiple_rag(user_prompt, llm, pipelines)\n        logger.info(f'LLM responses the follow text: \"{llm_response}\"')\n\n        # Save result to DB\n        ctx.result_storage.save_dict(job_id, ResponseModel(content=llm_response).model_dump())\n"}
{"type": "source_file", "path": "protollm/rags/pipeline/docs_processing/models.py", "content": "from typing import Any, List\n\nfrom pydantic import BaseModel\n\n\nclass ConfigLoader(BaseModel):\n    file_path: str = ''\n    save_path: str = ''\n    loader_name: str\n    parsing_params: dict[str, Any] = dict()\n\n\nclass ConfigSplitter(BaseModel):\n    splitter_name: str | None = None\n    splitter_params: dict[str, Any] = dict()\n\n\nclass ConfigFile(BaseModel):\n    loader: ConfigLoader\n    splitter: List[ConfigSplitter] = []\n    tokenizer: str | None = None\n"}
{"type": "source_file", "path": "protollm/rags/pipeline/docs_processing/utils.py", "content": "import os\nfrom pathlib import Path\nfrom typing import Any\n\nfrom protollm.raw_data_processing.docs_parsers.loaders import PDFLoader, WordDocumentLoader, ZipLoader, \\\n    RecursiveDirectoryLoader\nfrom langchain_core.document_loaders import BaseLoader\n\nfrom protollm.rags.pipeline.docs_processing.entities import LoaderType, LangChainDocumentLoader\nfrom protollm.rags.pipeline.docs_processing.exceptions import FileExtensionError, PathIsNotAssigned\n\n\ndef get_loader(**loader_params) -> BaseLoader:\n    document_path = loader_params.get('file_path', '')\n    if not isinstance(document_path, (str, Path)) or document_path == '':\n        raise PathIsNotAssigned('Input file (directory) path is not assigned')\n    doc_extension = str(document_path).lower().split('.')[-1]\n    if os.path.isdir(document_path):\n        doc_extension = LoaderType.directory\n    match doc_extension:\n        case LoaderType.pdf:\n            return PDFLoader(**loader_params)\n        case LoaderType.json:\n            return LangChainDocumentLoader(**loader_params)\n        case LoaderType.docx | LoaderType.doc | LoaderType.rtf | LoaderType.odt:\n            return WordDocumentLoader(**loader_params)\n\n    parsing_scheme = loader_params.pop('parsing_scheme', 'lines')\n    extract_images = loader_params.pop('extract_images', False)\n    extract_tables = loader_params.pop('extract_tables', False)\n    parse_formulas = loader_params.pop('parse_formulas', False)\n    remove_service_info = loader_params.pop('remove_service_info', False)\n    loader_params = dict(\n        pdf_parsing_scheme=parsing_scheme,\n        pdf_extract_images=extract_images,\n        pdf_extract_tables=extract_tables,\n        pdf_parse_formulas=parse_formulas,\n        pdf_remove_service_info=remove_service_info,\n        word_doc_parsing_scheme=parsing_scheme,\n        word_doc_extract_images=extract_images,\n        word_doc_extract_tables=extract_tables,\n        word_doc_parse_formulas=parse_formulas,\n        word_doc_remove_service_info=remove_service_info,\n        **loader_params,\n    )\n    match doc_extension:\n        case LoaderType.zip:\n            return ZipLoader(**loader_params)\n        case LoaderType.directory:\n            return RecursiveDirectoryLoader(**loader_params)\n        case _:\n            raise FileExtensionError(f'File with extension {doc_extension} has not been implemented yet.')\n"}
{"type": "source_file", "path": "protollm-synthetic/protollm_synthetic/utils.py", "content": "from typing import Any, Optional, List\nimport os\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.language_models import LanguageModelInput  \n\nclass Dataset:\n    \"\"\"\n    A class to load and manipulate datasets.\n    \"\"\"\n    def __init__(self, path: str, \n                 labels_col: bool = False,\n                 data_col: str = \"content\",\n                 max_chunk_size: int = 200000):\n        self.path = path\n        self.labels_col = labels_col\n        self.data_col = data_col\n        self.max_chunk_size = max_chunk_size\n        self.data = None\n        self.labeled_data = None\n        self.load()\n\n    def load(self):\n        # check if file exists\n        if not os.path.exists(self.path):\n            raise FileNotFoundError(f\"File {self.path} does not exist\")\n        \n        # load data\n        if self.path.endswith(\".json\"):\n            self.data = pd.read_json(self.path)\n        elif self.path.endswith(\".csv\"):\n            self.data = pd.read_csv(self.path)\n        else:\n            raise ValueError(f\"Unsupported file type: {self.path}\")\n\n        # Verify data_col exists in the DataFrame\n        if self.data_col not in self.data.columns:\n            available_cols = list(self.data.columns)\n            raise ValueError(f\"Column '{self.data_col}' not found in dataset. Available columns: {available_cols}\")\n\n        if self.labels_col:\n            self.get_labeled_data()\n\n        # Checking that some elements in data_col are larger than max_chunk_size\n        if self.data[self.data_col].apply(lambda x: len(x) > self.max_chunk_size).any():\n            self.chunk_context(self.data_col, self.max_chunk_size)\n\n\n    def get_labeled_data(self):\n        self.labeled_data = self.data[self.data[self.labels_col].notna()]\n        self.data = self.data[self.data[self.labels_col].isna()]\n\n    def train_test_split(self, test_size: float = 0.2):\n        raise NotImplementedError(\"Not implemented\")\n\n    def chunk_context(self, column_name: str, max_chunk_size: int):\n        \"\"\"\n        Splits the text in the specified column into chunks if the text size exceeds max_chunk_size.\n\n        :param column_name: The name of the column to process.\n        :param max_chunk_size: The maximum size of each chunk.\n        \"\"\"\n        if column_name not in self.data.columns:\n            raise ValueError(f\"Column {column_name} does not exist in the dataset\")\n\n        def split_into_chunks(text, max_size):\n            # Split text into chunks of max_size\n            return [text[i:i + max_size] for i in range(0, len(text), max_size)]\n\n        self.data[column_name] = self.data[column_name].apply(\n            lambda x: split_into_chunks(x, max_chunk_size) if isinstance(x, str) and len(x) > max_chunk_size else x\n        )\n\n\nclass VLLMChatOpenAI(ChatOpenAI):\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        # Call the parent class method to get the initial payload\n        request_payload = super()._get_request_payload(input_, stop=stop, **kwargs)\n        \n        # Check for deprecated 'max_completion_tokens' and replace it with 'max_tokens'\n        if \"max_completion_tokens\" in request_payload:\n            request_payload[\"max_tokens\"] = request_payload.pop(\"max_completion_tokens\")\n        \n        return request_payload"}
{"type": "source_file", "path": "protollm/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm/rags/pipeline/etl_pipeline.py", "content": "import logging\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import Iterable, Optional, Union\n\nfrom langchain_community.vectorstores import utils\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import VectorStore\n\nfrom protollm.rags.pipeline.docs_processing.utils import get_loader\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n\n\nclass DocsExtractPipeline:\n    def __init__(self, pipeline_settings: 'PipelineSettings'):\n        self._pipeline_settings = pipeline_settings\n\n    def update_loader(self, **kwargs) -> 'DocsExtractPipeline':\n        self._pipeline_settings.update_loader_params(kwargs)\n        return self\n\n    def load_docs(self, docs_collection_path: Optional[Union[str, Path]] = None,\n                  byte_content: Optional[bytes] = None):\n        logger.info('Initialize parsing process')\n        if docs_collection_path is not None:\n            self.update_loader(file_path=docs_collection_path)\n\n        loader = get_loader(byte_content=byte_content, **self._pipeline_settings.loader_params)\n        return loader.lazy_load()\n\n    def go_to_next_step(self, docs_collection_path: Optional[Union[str, Path]] = None,\n                        byte_content: Optional[bytes] = None) -> 'DocsTransformPipeline':\n        return DocsTransformPipeline(self._pipeline_settings, self.load_docs(docs_collection_path, byte_content))\n\n\nclass DocsTransformPipeline:\n    def __init__(self, pipeline_settings: 'PipelineSettings', docs_generator: Iterable[Document]):\n        self._pipeline_settings = pipeline_settings\n        self._docs_generator = docs_generator\n\n    def update_docs_transformers(self, **kwargs) -> 'DocsTransformPipeline':\n        self._pipeline_settings.update_transformer_params(kwargs)\n        return self\n\n    def transform(self, batch_size: int = 100) -> Iterable[Document]:\n        logger.info('Initialize transformation process')\n        transformers = self._pipeline_settings.transformers\n        batch_size = max(batch_size, 1)\n        while docs_batch := list(islice(self._docs_generator, batch_size)):\n            for transformer in transformers:\n                docs_batch = transformer.transform_documents(docs_batch)\n            yield from docs_batch\n\n    def go_to_next_step(self, batch_size: int = 100) -> 'DocsLoadPipeline':\n        return DocsLoadPipeline(self.transform(batch_size))\n\n\nclass DocsLoadPipeline:\n    def __init__(self, docs_generator: Iterable[Document]):\n        self._docs_generator = docs_generator\n\n    def load(self, store: VectorStore, loading_batch_size: int = 32) -> None:\n        logger.info('Initialize loading process')\n\n        # Add processed documents to the store\n        loading_batch_size = max(loading_batch_size, 1)\n        while docs_batch := list(islice(self._docs_generator, loading_batch_size)):\n            # TODO: replace filtering. It loses important metadatas. Use DocsTransformer to transform metadata\n            store.add_documents(utils.filter_complex_metadata(docs_batch))\n\n"}
{"type": "source_file", "path": "examples/real_world/chemical_pipeline/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm/rags/rag_core/__init__.py", "content": ""}
{"type": "source_file", "path": "protollm/definitions.py", "content": "import os\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nCONFIG_PATH = os.path.join(ROOT_DIR, 'config.env')\n"}
{"type": "source_file", "path": "protollm/agents/__init__.py", "content": "from protollm.agents.llama31_agents import Llama31ChatModel\n"}
{"type": "source_file", "path": "protollm/connectors/__init__.py", "content": "from .connector_creator import create_llm_connector, CustomChatOpenAI\nfrom .rest_server import ChatRESTServer"}
{"type": "source_file", "path": "protollm/connectors/connector_creator.py", "content": "import json\nimport os\nfrom typing import Any, Dict, List\nimport re\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import AIMessage, SystemMessage, HumanMessage\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.runnables import Runnable\nfrom langchain_gigachat import GigaChat\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, ValidationError\n\nfrom protollm.connectors.utils import (get_access_token,\n                                       models_without_function_calling,\n                                       models_without_structured_output)\nfrom protollm.definitions import CONFIG_PATH\n\n\nload_dotenv(CONFIG_PATH)\n\n\nclass CustomChatOpenAI(ChatOpenAI):\n    \"\"\"\n    A class that extends ChatOpenAI class from LangChain to support LLama and other models that do not support\n    function calls or structured output by default. This is implemented through custom processing of tool calls and JSON\n    schemas for a known list of models.\n    \n    Methods:\n        __init__(*args: Any, **kwargs: Any): Initializes the instance with parent configuration and custom handlers\n        invoke(messages: str | list, *args, **kwargs) -> AIMessage | dict | BaseModel: Processes input messages with\n            custom tool call handling and structured output parsing\n        bind_tools(*args, **kwargs: Any) -> Runnable: Enables function calling capability by binding tool definitions\n        with_structured_output(*args, **kwargs: Any) -> Runnable: Configures structured output format using JSON schemas\n            or Pydantic models\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self._response_format = None\n        self._tool_choice_mode = None\n        self._tools = None\n\n    def invoke(self, messages: str | list, *args, **kwargs) -> AIMessage | dict | BaseModel:\n        \n        if self._requires_custom_handling_for_tools() and self._tools:\n            system_prompt = self._generate_system_prompt_with_tools()\n            messages = self._handle_system_prompt(messages, system_prompt)\n        \n        if self._requires_custom_handling_for_structured_output() and self._response_format:\n            system_prompt = self._generate_system_prompt_with_schema()\n            messages = self._handle_system_prompt(messages, system_prompt)\n\n        response = self._super_invoke(messages, *args, **kwargs)\n\n        match response:\n            case AIMessage() if (\"<function=\" in response.content):\n                tool_calls = self._parse_function_calls(response.content)\n                if tool_calls:\n                    response.tool_calls = tool_calls\n                    response.content = \"\"\n            case AIMessage() if self._response_format:\n                response = self._parse_custom_structure(response)\n\n        return response\n    \n    def _super_invoke(self, messages, *args, **kwargs):\n        return super().invoke(messages, *args, **kwargs)\n\n    def bind_tools(self, *args, **kwargs: Any) -> Runnable:\n        if self._requires_custom_handling_for_tools():\n            self._tools = kwargs.get(\"tools\", [])\n            self._tool_choice_mode = kwargs.get(\"tool_choice\", \"auto\")\n            return self\n        else:\n            return super().bind_tools(*args, **kwargs)\n        \n    def with_structured_output(self, *args, **kwargs: Any) -> Runnable:\n        if self._requires_custom_handling_for_structured_output():\n            self._response_format = kwargs.get(\"schema\", [])\n            return self\n        else:\n            return super().with_structured_output(*args, **kwargs)\n\n    def _generate_system_prompt_with_tools(self) -> str:\n        \"\"\"\n        Generates a system prompt with function descriptions and instructions for the model.\n        \n        Returns:\n            System prompt with instructions for calling functions and descriptions of the functions themselves.\n        \n        Raises:\n            ValueError: If tools in an unsupported format have been passed.\n        \"\"\"\n        tool_descriptions = []\n        match self._tool_choice_mode:\n            case \"auto\" | None | \"any\" | \"required\" | True:\n                tool_choice_mode = str(self._tool_choice_mode)\n            case _:\n                tool_choice_mode = f\"<<{self._tool_choice_mode}>>\"\n        for tool in self._tools:\n            match tool:\n                case dict():\n                    tool_descriptions.append(\n                        f\"Function name: {tool['name']}\\n\"\n                        f\"Description: {tool['description']}\\n\"\n                        f\"Parameters: {json.dumps(tool['parameters'], ensure_ascii=False)}\"\n                    )\n                case BaseTool():\n                    tool_descriptions.append(\n                        f\"Function name: {tool.name}\\n\"\n                        f\"Description: {tool.description}\\n\"\n                        f\"Parameters: {json.dumps(tool.args, ensure_ascii=False)}\"\n                    )\n                case _:\n                    raise ValueError(\n                        \"Unsupported tool type. Try using a dictionary or function with the @tool decorator as tools\"\n                    )\n        tool_prefix = \"You have access to the following functions:\\n\\n\"\n        tool_instructions = (\n            \"There are the following 4 function call options:\\n\"\n            \"- str of the form <<tool_name>>: call <<tool_name>> tool.\\n\"\n            \"- 'auto': automatically select a tool (including no tool).\\n\"\n            \"- 'none': don't call a tool.\\n\"\n            \"- 'any' or 'required' or 'True': at least one tool have to be called.\\n\\n\"\n            f\"User-selected option - {tool_choice_mode}\\n\\n\"\n            \"If you choose to call a function ONLY reply in the following format with no prefix or suffix:\\n\"\n            '<function=example_function_name>{\"example_name\": \"example_value\"}</function>'\n        )\n        return tool_prefix + \"\\n\\n\".join(tool_descriptions) + \"\\n\\n\" + tool_instructions\n    \n    def _generate_system_prompt_with_schema(self) -> str:\n        \"\"\"\n        Generates a system prompt with response format descriptions and instructions for the model.\n        \n        Returns:\n            A system prompt with instructions for structured output and descriptions of the response formats themselves.\n            \n        Raises:\n            ValueError: If the structure descriptions for the response were passed in an unsupported format.\n        \"\"\"\n        schema_descriptions = []\n        match self._response_format:\n            case list():\n                schemas = self._response_format\n            case _:\n                schemas = [self._response_format]\n        for schema in schemas:\n            match schema:\n                case dict():\n                    schema_descriptions.append(str(schema))\n                case _ if issubclass(schema, BaseModel):\n                    schema_descriptions.append(str(schema.model_json_schema()))\n                case _:\n                    raise ValueError(\n                        \"Unsupported schema type. Try using a description of the answer structure as a dictionary or\"\n                        \" Pydantic model.\"\n                    )\n        schema_prefix = \"Generate a JSON object that matches one of the following schemas:\\n\\n\"\n        schema_instructions = (\n            \"Your response must contain ONLY valid JSON, parsable by a standard JSON parser. Do not include any\"\n            \" additional text, explanations, or comments.\"\n        )\n        return schema_prefix + \"\\n\\n\".join(schema_descriptions) + \"\\n\\n\" + schema_instructions\n\n    def _requires_custom_handling_for_tools(self) -> bool:\n        \"\"\"\n        Determines whether additional processing for tool calling is required for the current model.\n        \"\"\"\n        return any(model_name in self.model_name.lower() for model_name in models_without_function_calling)\n    \n    def _requires_custom_handling_for_structured_output(self) -> bool:\n        \"\"\"\n        Determines whether additional processing for structured output is required for the current model.\n        \"\"\"\n        return any(model_name in self.model_name.lower() for model_name in models_without_structured_output)\n    \n    def _parse_custom_structure(self, response_from_model) -> dict | BaseModel | None:\n        \"\"\"\n        Parses the model response into a dictionary or Pydantic class\n        \n        Args:\n            response_from_model: response of a model that does not support structured output by default\n        \n        Raises:\n            ValueError: If a structured response is not obtained\n        \"\"\"\n        match [self._response_format][0]:\n            case dict():\n                try:\n                    return json.loads(response_from_model.content)\n                except json.JSONDecodeError as e:\n                    raise ValueError(\n                        \"Failed to return structured output. There may have been a problem with loading JSON from the\"\n                        f\" model.\\n{e}\"\n                    )\n            case _ if issubclass([self._response_format][0], BaseModel):\n                for schema in [self._response_format]:\n                    try:\n                        return schema.model_validate_json(response_from_model.content)\n                    except ValidationError:\n                        continue\n                raise ValueError(\n                    \"Failed to return structured output. There may have been a problem with validating JSON from the\"\n                    \" model.\"\n                )\n        \n    @staticmethod\n    def _parse_function_calls(content: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Parses LLM answer (HTML string) to extract function calls.\n\n        Args:\n            content: model response as an HTML string\n\n        Returns:\n            A list of dictionaries in tool_calls format\n            \n        Raises:\n            ValueError: If the arguments for a function call are returned in an incorrect format\n        \"\"\"\n        tool_calls = []\n        pattern = r\"<function=(.*?)>(.*?)</function>\"\n        matches = re.findall(pattern, content, re.DOTALL)\n\n        for match in matches:\n            function_name, function_args = match\n            try:\n                arguments = json.loads(function_args)\n            except json.JSONDecodeError as e:\n                raise ValueError(f\"Error when decoding function arguments: {e}\")\n\n            tool_call = {\n                \"id\": f\"call_{len(tool_calls) + 1}\",\n                \"type\": \"tool_call\",\n                \"name\": function_name,\n                \"args\": arguments\n            }\n            tool_calls.append(tool_call)\n\n        return tool_calls\n    \n    @staticmethod\n    def _handle_system_prompt(msgs, sys_prompt):\n        match msgs:\n            case str():\n                return [SystemMessage(content=sys_prompt), HumanMessage(content=msgs)]\n            case list():\n                if not any(isinstance(msg, SystemMessage) for msg in msgs):\n                    msgs.insert(0, SystemMessage(content=sys_prompt))\n                else:\n                    idx = next((index for index, obj in enumerate(msgs) if isinstance(obj, SystemMessage)), 0)\n                    msgs[idx].content += \"\\n\\n\" + sys_prompt\n        return msgs\n\n\ndef create_llm_connector(model_url: str, *args: Any, **kwargs: Any) -> CustomChatOpenAI | GigaChat | ChatOpenAI:\n    \"\"\"Creates the proper connector for a given LLM service URL.\n\n    Args:\n        model_url: The LLM endpoint for making requests; should be in the format 'base_url;model_endpoint or name'\n            - for vsegpt.ru service for example: 'https://api.vsegpt.ru/v1;meta-llama/llama-3.1-70b-instruct'\n            - for Gigachat models family: 'https://gigachat.devices.sberbank.ru/api/v1/chat/completions;Gigachat'\n              for Gigachat model you should also install certificates from 'НУЦ Минцифры' -\n              instructions - 'https://developers.sber.ru/docs/ru/gigachat/certificates'\n            - for OpenAI for example: 'https://api.openai.com/v1;gpt-4o'\n            - for Ollama for example: 'ollama;http://localhost:11434;llama3.2'\n\n    Returns:\n        The ChatModel object from 'langchain' that can be used to make requests to the LLM service,\n        use tools, get structured output.\n    \"\"\"\n    if \"vsegpt\" in model_url:\n        base_url, model_name = model_url.split(\";\")\n        api_key = os.getenv(\"VSE_GPT_KEY\")\n        return CustomChatOpenAI(model_name=model_name, base_url=base_url, api_key=api_key, *args, **kwargs)\n    elif \"gigachat\" in model_url:\n        model_name = model_url.split(\";\")[1]\n        access_token = get_access_token()\n        return GigaChat(model=model_name, access_token=access_token, *args, **kwargs)\n    elif \"api.openai\" in model_url:\n        model_name = model_url.split(\";\")[1]\n        return ChatOpenAI(model=model_name, api_key=os.getenv(\"OPENAI_KEY\"), *args, **kwargs)\n    elif \"ollama\" in model_url:\n        url_and_name = model_url.split(\";\")\n        return ChatOllama(model=url_and_name[2], base_url=url_and_name[1], *args, **kwargs)\n    elif model_url == \"test_model\":\n        return CustomChatOpenAI(model_name=model_url, api_key=\"test\")\n    else:\n        raise ValueError(\"Unsupported provider URL\")\n    # Possible to add another LangChain compatible connector\n"}
{"type": "source_file", "path": "protollm/connectors/rest_server.py", "content": "import json\nfrom typing import Any, Dict, List, Optional, Union\n\nimport requests\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.messages import (AIMessage, BaseMessage, HumanMessage,\n                                     SystemMessage)\nfrom langchain_core.outputs import ChatGeneration, ChatResult\n\n\nclass ChatRESTServer(BaseChatModel):\n    model: Optional[str] = 'llama3'\n    base_url: str = 'http://10.32.2.2:8672'\n    timeout: Optional[int] = None\n    \"\"\"Timeout for the request stream\"\"\"\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"chat-rest-server\"\n\n    def _convert_messages_to_rest_server_messages(\n            self, messages: List[BaseMessage]\n    ) -> List[Dict[str, Union[str, List[str]]]]:\n        chat_messages: List = []\n        for message in messages:\n            role = \"\"\n        match message:\n            case HumanMessage():\n                role = \"user\"\n            case AIMessage():\n                role = \"assistant\"\n            case SystemMessage():\n                role = \"system\"\n            case _:\n                raise ValueError(\"Received unsupported message type.\")\n\n\n        content = \"\"\n        if isinstance(message.content, str):\n            content = message.content\n        else:\n            raise ValueError(\n                \"Unsupported message content type. \"\n                \"Must have type 'text' \"\n            )\n        chat_messages.append(\n            {\n                \"role\": role,\n                \"content\": content\n            }\n        )\n        return chat_messages\n\n    def create_chat(\n            self,\n            messages: List[BaseMessage],\n            stop: Optional[List[str]] = None,\n            **kwargs: Any,\n    ) -> Dict[str, Any]:\n        payload = {\n            \"model\": self.model,\n            \"messages\": self._convert_messages_to_rest_server_messages(\n                messages)\n        }\n\n        if self.base_url=='mock':\n            return {'test':'test'}\n\n        response = requests.post(\n            url=f'{self.base_url}/v1/chat/completions',\n            headers={\"Content-Type\": \"application/json\"},\n            json=payload,\n            timeout=self.timeout\n        )\n        response.encoding = \"utf-8\"\n        match response.status_code:\n            case 200:\n                pass  # Status code is 200, no action needed\n            case 404:\n                raise ValueError(\n                    \"CustomWeb call failed with status code 404. \"\n                    \"Maybe you need to connect to the corporate network.\"\n                )\n            case _:\n                optional_detail = response.text\n                raise ValueError(\n                    f\"CustomWeb call failed with status code \"\n                    f\"{response.status_code}. \"\n                    f\"Details: {optional_detail}\"\n                )\n        return json.loads(response.text)\n\n    def _generate(\n            self,\n            messages: List[BaseMessage],\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n    ) -> ChatResult:\n        response = self._create_chat(messages, stop, **kwargs)\n        chat_generation = ChatGeneration(\n            message=AIMessage(\n                content=response['choices'][0]['message']['content']),\n            generation_info=response,\n        )\n        return ChatResult(generations=[chat_generation])\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Return a dictionary of identifying parameters.\n\n        This information is used by the LangChain callback system, which\n        is used for tracing purposes make it possible to monitor LLMs.\n        \"\"\"\n        return {\n            \"model_name\": self.model,\n            \"url\": self.base_url\n        }\n"}
{"type": "source_file", "path": "protollm/connectors/utils.py", "content": "import json\nimport os\nimport uuid\n\nimport requests\n\n\ndef get_access_token() -> str:\n    \"\"\"\n    Gets the access token by the authorisation key specified in the config.\n    The token is valid for 30 minutes.\n\n    Returns:\n        Access token for Gigachat API\n    \"\"\"\n    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n    request_id = uuid.uuid4()\n    authorization_key = os.getenv(\"AUTHORIZATION_KEY\")\n    \n    payload = {\n        'scope': 'GIGACHAT_API_PERS'\n    }\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json',\n        'RqUID': f'{request_id}',\n        'Authorization': f'Basic {authorization_key}'\n    }\n    \n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    return json.loads(response.text)['access_token']\n\n\n# List of models that do NOT support calling functions out-of-the-box yet\nmodels_without_function_calling = [\"r1\", \"deepseek-chat-alt\", \"test_model\"]\n\n# List of models that do NOT support structured outputs out-of-the-box yet\nmodels_without_structured_output = [\"deepseek-chat\", \"deepseek-chat-alt\", \"llama-3.3-70b-instruct\", \"test_model\"]"}
{"type": "source_file", "path": "protollm/agents/llama31_agents/llama31_agent.py", "content": "import logging\nfrom typing import List, Optional, Any, Dict\n\nimport requests\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    BaseMessage,\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    ChatResult,\n    ChatGeneration,\n)\nfrom pydantic import PrivateAttr\n\n\n# Define the custom LLM class\nclass Llama31ChatModel(BaseChatModel):\n    api_key: str\n    base_url: str\n    model: str\n    temperature: float = 0.5\n    max_tokens: int = 3000\n    logging_level: int = logging.INFO\n\n    _logger: logging.Logger = PrivateAttr()\n\n    class Config:\n        arbitrary_types_allowed = True  # Allows arbitrary types like the logger\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        self._logger = logging.getLogger(__name__)\n        logging.basicConfig(level=self.logging_level)\n\n    @property\n    def _llm_type(self) -> str:\n        return \"llama31\"\n    \n    def _prepare_headers(self) -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _prepare_context(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n        role_map = {\n            HumanMessage: \"user\",\n            AIMessage: \"assistant\",\n            SystemMessage: \"system\"\n        }\n        \n        return [{\"role\": role_map.get(type(message), \"user\"), \"content\": message.content} for message in messages]\n\n    def _prepare_payload(\n        self,\n        context: List[Dict[str, str]],\n        stop: Optional[List[str]] = None,\n        **kwargs: Any\n    ) -> Dict[str, Any]:\n        payload = {\n            \"model\": self.model,\n            \"messages\": context,\n            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n        }\n        if stop is not None:\n            payload[\"stop\"] = stop\n        return payload\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        headers = self._prepare_headers()\n        context = self._prepare_context(messages)\n        payload = self._prepare_payload(context, stop, **kwargs)\n\n        try:\n            response = requests.post(\n                f\"{self.base_url}/chat/completions\",\n                headers=headers,\n                json=payload,\n            )\n            response.raise_for_status()\n            assistant_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n            self._logger.info(\"REQUEST: %s\", payload)\n            self._logger.info(\"RESPONSE: %s\", assistant_response)\n            ai_message = AIMessage(content=assistant_response)\n            generation = ChatGeneration(message=ai_message)\n            return ChatResult(generations=[generation])\n        except requests.RequestException as e:\n            self._logger.error(\"API request failed: %s\", e)\n            raise RuntimeError(f\"API request failed: {e}\")\n"}
{"type": "source_file", "path": "protollm/agents/llama31_agents/__init__.py", "content": "from protollm.agents.llama31_agents.llama31_agent import Llama31ChatModel\n"}
