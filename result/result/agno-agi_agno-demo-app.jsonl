{"repo_info": {"repo_name": "agno-demo-app", "repo_owner": "agno-agi", "repo_url": "https://github.com/agno-agi/agno-demo-app"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/evals/test_calculator.py", "content": ""}
{"type": "source_file", "path": "db/tables/base.py", "content": "from sqlalchemy import MetaData\nfrom sqlalchemy.orm import DeclarativeBase\n\n\nclass Base(DeclarativeBase):\n    \"\"\"\n    Base class for SQLAlchemy model definitions.\n\n    https://fastapi.tiangolo.com/tutorial/sql-databases/#create-a-base-class\n    https://docs.sqlalchemy.org/en/20/orm/mapping_api.html#sqlalchemy.orm.DeclarativeBase\n    \"\"\"\n\n    metadata = MetaData(schema=\"public\")\n"}
{"type": "source_file", "path": "api/routes/__init__.py", "content": ""}
{"type": "source_file", "path": "workflows/investment_report_generator.py", "content": "from textwrap import dedent\nfrom typing import Iterator\n\nfrom agno.agent import Agent, RunResponse\nfrom agno.models.openai import OpenAIChat\nfrom agno.storage.workflow.postgres import PostgresWorkflowStorage\nfrom agno.tools.yfinance import YFinanceTools\nfrom agno.utils.log import logger\nfrom agno.workflow import Workflow\n\nfrom workflows.settings import workflow_settings\nfrom db.session import db_url\n\n\nclass InvestmentReportGenerator(Workflow):\n    \"\"\"Advanced workflow for generating professional investment analysis with strategic recommendations.\"\"\"\n\n    description: str = dedent(\"\"\"\\\n    An intelligent investment analysis system that produces comprehensive financial research and\n    strategic investment recommendations. This workflow orchestrates multiple AI agents to analyze\n    market data, evaluate investment potential, and create detailed portfolio allocation strategies.\n    The system excels at combining quantitative analysis with qualitative insights to deliver\n    actionable investment advice.\n    \"\"\")\n\n    stock_analyst: Agent = Agent(\n        name=\"Stock Analyst\",\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        tools=[YFinanceTools(company_info=True, analyst_recommendations=True, company_news=True)],\n        description=dedent(\"\"\"\\\n        You are MarketMaster-X, an elite Senior Investment Analyst at Goldman Sachs with expertise in:\n\n        - Comprehensive market analysis\n        - Financial statement evaluation\n        - Industry trend identification\n        - News impact assessment\n        - Risk factor analysis\n        - Growth potential evaluation\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Market Research 📊\n           - Analyze company fundamentals and metrics\n           - Review recent market performance\n           - Evaluate competitive positioning\n           - Assess industry trends and dynamics\n        2. Financial Analysis 💹\n           - Examine key financial ratios\n           - Review analyst recommendations\n           - Analyze recent news impact\n           - Identify growth catalysts\n        3. Risk Assessment 🎯\n           - Evaluate market risks\n           - Assess company-specific challenges\n           - Consider macroeconomic factors\n           - Identify potential red flags\n        Note: This analysis is for educational purposes only.\\\n        \"\"\"),\n        expected_output=\"Comprehensive market analysis report in markdown format\",\n    )\n\n    research_analyst: Agent = Agent(\n        name=\"Research Analyst\",\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        description=dedent(\"\"\"\\\n        You are ValuePro-X, an elite Senior Research Analyst at Goldman Sachs specializing in:\n\n        - Investment opportunity evaluation\n        - Comparative analysis\n        - Risk-reward assessment\n        - Growth potential ranking\n        - Strategic recommendations\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Investment Analysis 🔍\n           - Evaluate each company's potential\n           - Compare relative valuations\n           - Assess competitive advantages\n           - Consider market positioning\n        2. Risk Evaluation 📈\n           - Analyze risk factors\n           - Consider market conditions\n           - Evaluate growth sustainability\n           - Assess management capability\n        3. Company Ranking 🏆\n           - Rank based on investment potential\n           - Provide detailed rationale\n           - Consider risk-adjusted returns\n           - Explain competitive advantages\\\n        \"\"\"),\n        expected_output=\"Detailed investment analysis and ranking report in markdown format\",\n    )\n\n    investment_lead: Agent = Agent(\n        name=\"Investment Lead\",\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        description=dedent(\"\"\"\\\n        You are PortfolioSage-X, a distinguished Senior Investment Lead at Goldman Sachs expert in:\n\n        - Portfolio strategy development\n        - Asset allocation optimization\n        - Risk management\n        - Investment rationale articulation\n        - Client recommendation delivery\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Portfolio Strategy 💼\n           - Develop allocation strategy\n           - Optimize risk-reward balance\n           - Consider diversification\n           - Set investment timeframes\n        2. Investment Rationale 📝\n           - Explain allocation decisions\n           - Support with analysis\n           - Address potential concerns\n           - Highlight growth catalysts\n        3. Recommendation Delivery 📊\n           - Present clear allocations\n           - Explain investment thesis\n           - Provide actionable insights\n           - Include risk considerations\\\n        \"\"\"),\n    )\n\n    def run(self, companies: str) -> Iterator[RunResponse]:  # type: ignore\n        logger.info(f\"Getting investment reports for companies: {companies}\")\n        initial_report: RunResponse = self.stock_analyst.run(companies)\n        if initial_report is None or not initial_report.content:\n            yield RunResponse(\n                run_id=self.run_id,\n                content=\"Sorry, could not get the stock analyst report.\",\n            )\n            return\n\n        logger.info(\"Ranking companies based on investment potential.\")\n        ranked_companies: RunResponse = self.research_analyst.run(initial_report.content)\n        if ranked_companies is None or not ranked_companies.content:\n            yield RunResponse(run_id=self.run_id, content=\"Sorry, could not get the ranked companies.\")\n            return\n\n        logger.info(\"Reviewing the research report and producing an investment proposal.\")\n        yield from self.investment_lead.run(ranked_companies.content, stream=True)\n\n\ndef get_investment_report_generator(debug_mode: bool = False) -> InvestmentReportGenerator:\n    return InvestmentReportGenerator(\n        workflow_id=\"generate-investment-report\",\n        storage=PostgresWorkflowStorage(\n            table_name=\"investment_report_generator_workflows\",\n            db_url=db_url,\n        ),\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "utils/dttm.py", "content": "from datetime import datetime, timezone\n\n\ndef current_utc() -> datetime:\n    return datetime.now(timezone.utc)\n\n\ndef current_utc_str(format: str = \"%Y-%m-%dT%H:%M:%S.%fZ\") -> str:\n    return current_utc().strftime(format)\n"}
{"type": "source_file", "path": "workspace/__init__.py", "content": ""}
{"type": "source_file", "path": "utils/__init__.py", "content": ""}
{"type": "source_file", "path": "agents/web_search.py", "content": "from textwrap import dedent\nfrom typing import Optional\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.serpapi import SerpApiTools\nfrom agno.storage.agent.postgres import PostgresAgentStorage\n\nfrom agents.settings import agent_settings\nfrom db.session import db_url\n\nweb_search_agent_storage = PostgresAgentStorage(table_name=\"web_search_agent\", db_url=db_url)\n\n\ndef get_web_search_agent(\n    user_id: Optional[str] = None,\n    session_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Agent:\n    return Agent(\n        name=\"Web Search Agent\",\n        role=\"Search the web for information\",\n        agent_id=\"web-search-agent\",\n        session_id=session_id,\n        user_id=user_id,\n        model=OpenAIChat(\n            id=agent_settings.gpt_4,\n            max_tokens=agent_settings.default_max_completion_tokens,\n            temperature=agent_settings.default_temperature,\n        ),\n        tools=[SerpApiTools()],\n        description=\"You are a Web Search Agent that has the special skill of searching the web for information and presenting the results in a structured manner.\",\n        instructions=[\n            \"To answer the user's question, first search the web for information by breaking down the user's question into smaller queries.\",\n            \"Make sure you cover all the aspects of the question.\",\n            \"Important: \\n\"\n            \" - Focus on legitimate sources\\n\"\n            \" - Always provide sources and the links to the information you used to answer the question\\n\"\n            \" - If you cannot find the answer, say so and ask the user to provide more details.\",\n            \"Keep your answers concise and engaging.\",\n        ],\n        expected_output=dedent(\"\"\"\\\n        Your answer should be in the following format:\n\n        {provide a detailed answer to the user's question}\n\n        ### Sources\n        {provide the sources and links to the information you used to answer the question}\n        \"\"\"),\n        storage=web_search_agent_storage,\n        add_history_to_messages=True,\n        num_history_responses=5,\n        add_datetime_to_instructions=True,\n        markdown=True,\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "workspace/dev_resources.py", "content": "from agno.docker.app.fastapi import FastApi\nfrom agno.docker.app.postgres import PgVectorDb\nfrom agno.docker.resource.image import DockerImage\nfrom agno.docker.resources import DockerResources\n\nfrom workspace.settings import BUILD_IMAGES, DEV_ENV, DEV_KEY, IMAGE_REPO, WS_NAME, WS_ROOT\n\n#\n# -*- Resources for the Development Environment\n#\n\n# -*- Dev image\ndev_image = DockerImage(\n    name=f\"{IMAGE_REPO}/{WS_NAME}\",\n    tag=DEV_ENV,\n    enabled=BUILD_IMAGES,\n    path=str(WS_ROOT),\n    push_image=False,\n)\n\n# -*- Dev database running on port 5432:5432\ndev_db = PgVectorDb(\n    name=f\"{DEV_KEY}-db\",\n    enabled=True,\n    pg_user=\"api\",\n    pg_password=\"api\",\n    pg_database=\"api\",\n    # Connect to this db on port 5432\n    host_port=5432,\n)\n\n# -*- Build container environment\ncontainer_env = {\n    \"RUNTIME_ENV\": \"dev\",\n    \"AGNO_MONITOR\": \"True\",\n    # Database configuration\n    \"DB_HOST\": dev_db.get_db_host(),\n    \"DB_PORT\": dev_db.get_db_port(),\n    \"DB_USER\": dev_db.get_db_user(),\n    \"DB_PASS\": dev_db.get_db_password(),\n    \"DB_DATABASE\": dev_db.get_db_database(),\n    # Wait for database to be available before starting the application\n    \"WAIT_FOR_DB\": True,\n    # Migrate database on startup using alembic\n    # \"MIGRATE_DB\": ws_settings.prd_db_enabled,\n}\n\n# -*- FastApi running on port 8000:8000\ndev_fastapi = FastApi(\n    name=DEV_KEY,\n    enabled=True,\n    image=dev_image,\n    command=\"uvicorn api.main:app --reload\",\n    port_number=8000,\n    debug_mode=True,\n    mount_workspace=True,\n    env_vars=container_env,\n    use_cache=True,\n    # Read secrets from secrets/dev_api_secrets.yml\n    secrets_file=WS_ROOT.joinpath(\"workspace/secrets/dev_api_secrets.yml\"),\n    depends_on=[dev_db],\n)\n\n# -*- Dev DockerResources\ndev_docker_resources = DockerResources(\n    env=DEV_ENV,\n    network=WS_NAME,\n    apps=[dev_db, dev_fastapi],\n)\n"}
{"type": "source_file", "path": "workflows/startup_idea_validator.py", "content": "import json\nfrom typing import Iterator, Optional\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.storage.workflow.postgres import PostgresWorkflowStorage\nfrom agno.tools.googlesearch import GoogleSearchTools\nfrom agno.utils.log import logger\nfrom agno.workflow import RunEvent, RunResponse, Workflow\nfrom pydantic import BaseModel, Field\n\nfrom workflows.settings import workflow_settings\nfrom db.session import db_url\n\n\nclass IdeaClarification(BaseModel):\n    originality: str = Field(..., description=\"Originality of the idea.\")\n    mission: str = Field(..., description=\"Mission of the company.\")\n    objectives: str = Field(..., description=\"Objectives of the company.\")\n\n\nclass MarketResearch(BaseModel):\n    total_addressable_market: str = Field(..., description=\"Total addressable market (TAM).\")\n    serviceable_available_market: str = Field(..., description=\"Serviceable available market (SAM).\")\n    serviceable_obtainable_market: str = Field(..., description=\"Serviceable obtainable market (SOM).\")\n    target_customer_segments: str = Field(..., description=\"Target customer segments.\")\n\n\nclass StartupIdeaValidator(Workflow):\n    idea_clarifier_agent: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        instructions=[\n            \"Given a user's startup idea, its your goal to refine that idea. \",\n            \"Evaluates the originality of the idea by comparing it with existing concepts. \",\n            \"Define the mission and objectives of the startup.\",\n        ],\n        add_history_to_messages=True,\n        add_datetime_to_instructions=True,\n        response_model=IdeaClarification,\n        structured_outputs=True,\n        debug_mode=False,\n    )\n\n    market_research_agent: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        tools=[GoogleSearchTools()],\n        instructions=[\n            \"You are provided with a startup idea and the company's mission and objectives. \",\n            \"Estimate the total addressable market (TAM), serviceable available market (SAM), and serviceable obtainable market (SOM). \",\n            \"Define target customer segments and their characteristics. \",\n            \"Search the web for resources if you need to.\",\n        ],\n        add_history_to_messages=True,\n        add_datetime_to_instructions=True,\n        response_model=MarketResearch,\n        structured_outputs=True,\n        debug_mode=False,\n    )\n\n    competitor_analysis_agent: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        tools=[GoogleSearchTools()],\n        instructions=[\n            \"You are provided with a startup idea and some market research related to the idea. \",\n            \"Identify existing competitors in the market. \",\n            \"Perform Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis for each competitor. \",\n            \"Assess the startup’s potential positioning relative to competitors.\",\n        ],\n        add_history_to_messages=True,\n        add_datetime_to_instructions=True,\n        markdown=True,\n        debug_mode=False,\n    )\n\n    report_agent: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        instructions=[\n            \"You are provided with a startup idea and other data about the idea. \",\n            \"Summarise everything into a single report.\",\n        ],\n        add_history_to_messages=True,\n        add_datetime_to_instructions=True,\n        markdown=True,\n        debug_mode=False,\n    )\n\n    def get_idea_clarification(self, startup_idea: str) -> Optional[IdeaClarification]:\n        try:\n            response: RunResponse = self.idea_clarifier_agent.run(startup_idea)\n\n            # Check if we got a valid response\n            if not response or not response.content:\n                logger.warning(\"Empty Idea Clarification response\")\n            # Check if the response is of the expected type\n            if not isinstance(response.content, IdeaClarification):\n                logger.warning(\"Invalid response type\")\n\n            return response.content\n\n        except Exception as e:\n            logger.warning(f\"Failed: {str(e)}\")\n\n        return None\n\n    def get_market_research(\n        self, startup_idea: str, idea_clarification: IdeaClarification\n    ) -> Optional[MarketResearch]:\n        agent_input = {\"startup_idea\": startup_idea, **idea_clarification.model_dump()}\n\n        try:\n            response: RunResponse = self.market_research_agent.run(json.dumps(agent_input, indent=4))\n\n            # Check if we got a valid response\n            if not response or not response.content:\n                logger.warning(\"Empty Market Research response\")\n\n            # Check if the response is of the expected type\n            if not isinstance(response.content, MarketResearch):\n                logger.warning(\"Invalid response type\")\n\n            return response.content\n\n        except Exception as e:\n            logger.warning(f\"Failed: {str(e)}\")\n\n        return None\n\n    def get_competitor_analysis(self, startup_idea: str, market_research: MarketResearch) -> Optional[str]:\n        agent_input = {\"startup_idea\": startup_idea, **market_research.model_dump()}\n\n        try:\n            response: RunResponse = self.competitor_analysis_agent.run(json.dumps(agent_input, indent=4))\n\n            # Check if we got a valid response\n            if not response or not response.content:\n                logger.warning(\"Empty Competitor Analysis response\")\n\n            return response.content\n\n        except Exception as e:\n            logger.warning(f\"Failed: {str(e)}\")\n\n        return None\n\n    def run(self, startup_idea: str) -> Iterator[RunResponse]:  # type: ignore\n        logger.info(f\"Generating a startup validation report for: {startup_idea}\")\n\n        # Clarify and quantify the idea\n        idea_clarification: Optional[IdeaClarification] = self.get_idea_clarification(startup_idea)\n\n        if idea_clarification is None:\n            yield RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not even clarify the idea: {startup_idea}\",\n            )\n            return\n\n        # Do some market research\n        market_research: Optional[MarketResearch] = self.get_market_research(startup_idea, idea_clarification)\n\n        if market_research is None:\n            yield RunResponse(\n                event=RunEvent.workflow_completed,\n                content=\"Market research failed\",\n            )\n            return\n\n        competitor_analysis: Optional[str] = self.get_competitor_analysis(startup_idea, market_research)\n\n        # Compile the final report\n        final_response: RunResponse = self.report_agent.run(\n            json.dumps(\n                {\n                    \"startup_idea\": startup_idea,\n                    **idea_clarification.model_dump(),\n                    **market_research.model_dump(),\n                    \"competitor_analysis_report\": competitor_analysis,\n                },\n                indent=4,\n            )\n        )\n\n        yield RunResponse(content=final_response.content, event=RunEvent.workflow_completed)\n\n\ndef get_startup_idea_validator(debug_mode: bool = False) -> StartupIdeaValidator:\n    return StartupIdeaValidator(\n        workflow_id=\"validate-startup-idea\",\n        storage=PostgresWorkflowStorage(\n            table_name=\"startup_idea_validator_workflows\",\n            db_url=db_url,\n        ),\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "db/settings.py", "content": "from os import getenv\nfrom typing import Optional\n\nfrom pydantic_settings import BaseSettings\n\nfrom utils.log import logger\n\n\nclass DbSettings(BaseSettings):\n    \"\"\"Database settings that can be set using environment variables.\n\n    Reference: https://docs.pydantic.dev/latest/usage/pydantic_settings/\n    \"\"\"\n\n    # Database configuration\n    db_host: Optional[str] = None\n    db_port: Optional[int] = None\n    db_user: Optional[str] = None\n    db_pass: Optional[str] = None\n    db_database: Optional[str] = None\n    db_driver: str = \"postgresql+psycopg\"\n    # Create/Upgrade database on startup using alembic\n    migrate_db: bool = False\n\n    def get_db_url(self) -> str:\n        db_url = \"{}://{}{}@{}:{}/{}\".format(\n            self.db_driver,\n            self.db_user,\n            f\":{self.db_pass}\" if self.db_pass else \"\",\n            self.db_host,\n            self.db_port,\n            self.db_database,\n        )\n        # Use local database if RUNTIME_ENV is not set\n        if \"None\" in db_url and getenv(\"RUNTIME_ENV\") is None:\n            from workspace.dev_resources import dev_db\n\n            logger.debug(\"Using local connection\")\n            local_db_url = dev_db.get_db_connection_local()\n            if local_db_url:\n                db_url = local_db_url\n\n        # Validate database connection\n        if \"None\" in db_url or db_url is None:\n            raise ValueError(\"Could not build database connection\")\n        return db_url\n\n\n# Create DbSettings object\ndb_settings = DbSettings()\n"}
{"type": "source_file", "path": "api/__init__.py", "content": ""}
{"type": "source_file", "path": "api/routes/playground.py", "content": "from os import getenv\nfrom agno.playground import Playground\n\n# Import agents\nfrom agents.finance import get_finance_agent\nfrom agents.research import get_research_agent\nfrom agents.web_search import get_web_search_agent\nfrom agents.basic_agent import get_basic_agent\n\n# Import workflows\nfrom workflows.blog_post_generator import get_blog_post_generator\nfrom workflows.investment_report_generator import get_investment_report_generator\nfrom workflows.startup_idea_validator import get_startup_idea_validator\n\n######################################################\n## Router for the agent playground\n######################################################\n\nfinance_agent = get_finance_agent(debug_mode=True)\nresearch_agent = get_research_agent(debug_mode=True)\nweb_search_agent = get_web_search_agent(debug_mode=True)\nbasic_agent = get_basic_agent(debug_mode=True)\n\nblog_post_generator = get_blog_post_generator(debug_mode=True)\ninvestment_report_generator = get_investment_report_generator(debug_mode=True)\nstartup_idea_validator = get_startup_idea_validator(debug_mode=True)\n\n# Create a playground instance\nplayground = Playground(\n    agents=[basic_agent, web_search_agent, research_agent, finance_agent],\n    workflows=[blog_post_generator, investment_report_generator, startup_idea_validator],\n)\n# Log the playground endpoint with app.agno.com\nif getenv(\"RUNTIME_ENV\") == \"dev\":\n    playground.create_endpoint(\"http://localhost:8000\")\n\nplayground_router = playground.get_router()\n"}
{"type": "source_file", "path": "api/main.py", "content": "from fastapi import FastAPI\nfrom starlette.middleware.cors import CORSMiddleware\n\nfrom api.settings import api_settings\nfrom api.routes.v1_router import v1_router\n\n\ndef create_app() -> FastAPI:\n    \"\"\"Create a FastAPI App\n\n    Returns:\n        FastAPI: FastAPI App\n    \"\"\"\n\n    # Create FastAPI App\n    app: FastAPI = FastAPI(\n        title=api_settings.title,\n        version=api_settings.version,\n        docs_url=\"/docs\" if api_settings.docs_enabled else None,\n        redoc_url=\"/redoc\" if api_settings.docs_enabled else None,\n        openapi_url=\"/openapi.json\" if api_settings.docs_enabled else None,\n    )\n\n    # Add v1 router\n    app.include_router(v1_router)\n\n    # Add Middlewares\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=api_settings.cors_origin_list,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    return app\n\n\n# Create FastAPI app\napp = create_app()\n"}
{"type": "source_file", "path": "utils/log.py", "content": "import logging\n\nfrom rich.logging import RichHandler\n\n\ndef get_logger(logger_name: str) -> logging.Logger:\n    # https://rich.readthedocs.io/en/latest/reference/logging.html#rich.logging.RichHandler\n    # https://rich.readthedocs.io/en/latest/logging.html#handle-exceptions\n    rich_handler = RichHandler(\n        show_time=False,\n        rich_tracebacks=False,\n        show_path=True,\n        tracebacks_show_locals=False,\n    )\n    rich_handler.setFormatter(\n        logging.Formatter(\n            fmt=\"%(message)s\",\n            datefmt=\"[%X]\",\n        )\n    )\n\n    _logger = logging.getLogger(logger_name)\n    _logger.addHandler(rich_handler)\n    _logger.setLevel(logging.DEBUG)\n    _logger.propagate = False\n    return _logger\n\n\nlogger: logging.Logger = get_logger(\"agno-demo-app\")\n"}
{"type": "source_file", "path": "workflows/settings.py", "content": "from pydantic_settings import BaseSettings\n\n\nclass WorkflowSettings(BaseSettings):\n    \"\"\"Workflow settings that can be set using environment variables.\n\n    Reference: https://pydantic-docs.helpmanual.io/usage/settings/\n    \"\"\"\n\n    gpt_4_mini: str = \"gpt-4o-mini\"\n    embedding_model: str = \"text-embedding-3-small\"\n    default_max_completion_tokens: int = 16000\n    default_temperature: float = 0\n\n\n# Create an WorkflowSettings object\nworkflow_settings = WorkflowSettings()\n"}
{"type": "source_file", "path": "db/__init__.py", "content": ""}
{"type": "source_file", "path": "api/routes/health.py", "content": "from fastapi import APIRouter\n\nfrom utils.dttm import current_utc_str\n\n######################################################\n## Router for health checks\n######################################################\n\nhealth_check_router = APIRouter(tags=[\"Health\"])\n\n\n@health_check_router.get(\"/health\")\ndef get_health():\n    \"\"\"Check the health of the Api\"\"\"\n\n    return {\n        \"status\": \"success\",\n        \"router\": \"health\",\n        \"path\": \"/health\",\n        \"utc\": current_utc_str(),\n    }\n"}
{"type": "source_file", "path": "agents/basic_agent.py", "content": "from typing import Optional\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.storage.agent.postgres import PostgresAgentStorage\n\nfrom agents.settings import agent_settings\nfrom db.session import db_url\n\n\nbasic_agent_storage = PostgresAgentStorage(table_name=\"simple_agent\", db_url=db_url)\n\n\ndef get_basic_agent(\n    user_id: Optional[str] = None,\n    session_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Agent:\n    return Agent(\n        name=\"Basic Agent\",\n        role=\"Basic agent\",\n        agent_id=\"basic-agent\",\n        session_id=session_id,\n        user_id=user_id,\n        model=OpenAIChat(\n            id=agent_settings.gpt_4o_mini,\n            max_tokens=agent_settings.default_max_completion_tokens,\n            temperature=agent_settings.default_temperature,\n        ),\n        storage=basic_agent_storage,\n        add_history_to_messages=True,\n        num_history_responses=5,\n        add_datetime_to_instructions=True,\n        markdown=True,\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "db/migrations/env.py", "content": "from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\n\nfrom alembic import context\n\nfrom db.tables import Base\nfrom db.session import db_url\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\nconfig.set_main_option(\"sqlalchemy.url\", db_url)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata\n\n\n# -*- Only include tables that are in the target_metadata\n# See: https://alembic.sqlalchemy.org/en/latest/autogenerate.html#omitting-table-names-from-the-autogenerate-process\ndef include_name(name, type_, parent_names):\n    if type_ == \"table\":\n        return name in target_metadata.tables\n    else:\n        return True\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        include_name=include_name,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n        version_table_schema=target_metadata.schema,\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            include_name=include_name,\n            version_table_schema=target_metadata.schema,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "api/settings.py", "content": "from typing import List, Optional\n\nfrom pydantic import field_validator, Field\nfrom pydantic_settings import BaseSettings\nfrom pydantic_core.core_schema import FieldValidationInfo\n\n\nclass ApiSettings(BaseSettings):\n    \"\"\"Api settings that can be set using environment variables.\n\n    Reference: https://pydantic-docs.helpmanual.io/usage/settings/\n    \"\"\"\n\n    # Api title and version\n    title: str = \"agno-demo-app\"\n    version: str = \"1.0\"\n\n    # Api runtime_env derived from the `runtime_env` environment variable.\n    # Valid values include \"dev\", \"stg\", \"prd\"\n    runtime_env: str = \"dev\"\n\n    # Set to False to disable docs at /docs and /redoc\n    docs_enabled: bool = True\n\n    # Cors origin list to allow requests from.\n    # This list is set using the set_cors_origin_list validator\n    # which uses the runtime_env variable to set the\n    # default cors origin list.\n    cors_origin_list: Optional[List[str]] = Field(None, validate_default=True)\n\n    @field_validator(\"runtime_env\")\n    def validate_runtime_env(cls, runtime_env):\n        \"\"\"Validate runtime_env.\"\"\"\n\n        valid_runtime_envs = [\"dev\", \"stg\", \"prd\"]\n        if runtime_env not in valid_runtime_envs:\n            raise ValueError(f\"Invalid runtime_env: {runtime_env}\")\n\n        return runtime_env\n\n    @field_validator(\"cors_origin_list\", mode=\"before\")\n    def set_cors_origin_list(cls, cors_origin_list, info: FieldValidationInfo):\n        valid_cors = cors_origin_list or []\n\n        # Add app.agno.com to cors origin list\n        valid_cors.extend(\n            [\n                \"http://localhost:3000\",\n                \"https://app.agno.com\",\n                \"https://app-stg.agno.com\",\n                \"https://agno-agent-ui.vercel.app\",\n                # Add more here if you are not hosting on localhost\n            ]\n        )\n\n        return valid_cors\n\n\n# Create ApiSettings object\napi_settings = ApiSettings()\n"}
{"type": "source_file", "path": "agents/__init__.py", "content": ""}
{"type": "source_file", "path": "agents/finance.py", "content": "from typing import Optional\nfrom textwrap import dedent\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.yfinance import YFinanceTools\nfrom agno.storage.agent.postgres import PostgresAgentStorage\n\nfrom agents.settings import agent_settings\nfrom db.session import db_url\n\n\nfinance_agent_storage = PostgresAgentStorage(table_name=\"finance_agent\", db_url=db_url)\n\n\ndef get_finance_agent(\n    user_id: Optional[str] = None,\n    session_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Agent:\n    return Agent(\n        name=\"Finance Agent\",\n        role=\"Analyze financial data\",\n        agent_id=\"finance-agent\",\n        session_id=session_id,\n        user_id=user_id,\n        model=OpenAIChat(\n            id=agent_settings.gpt_4,\n            max_tokens=agent_settings.default_max_completion_tokens,\n            temperature=agent_settings.default_temperature,\n        ),\n        tools=[YFinanceTools(enable_all=True)],\n        instructions=dedent(\"\"\"\\\n            You are a seasoned Wall Street analyst with deep expertise in market analysis! 📊\n\n            Follow these steps for comprehensive financial analysis:\n            1. Market Overview\n            - Latest stock price\n            - 52-week high and low\n            2. Financial Deep Dive\n            - Key metrics (P/E, Market Cap, EPS)\n            3. Professional Insights\n            - Analyst recommendations breakdown\n            - Recent rating changes\n\n            4. Market Context\n            - Industry trends and positioning\n            - Competitive analysis\n            - Market sentiment indicators\n\n            Your reporting style:\n            - Begin with an executive summary\n            - Use tables for data presentation\n            - Include clear section headers\n            - Add emoji indicators for trends (📈 📉)\n            - Highlight key insights with bullet points\n            - Compare metrics to industry averages\n            - Include technical term explanations\n            - End with a forward-looking analysis\n\n            Risk Disclosure:\n            - Always highlight potential risk factors\n            - Note market uncertainties\n            - Mention relevant regulatory concerns\n        \"\"\"),\n        storage=finance_agent_storage,\n        add_history_to_messages=True,\n        num_history_responses=5,\n        add_datetime_to_instructions=True,\n        markdown=True,\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "db/session.py", "content": "from typing import Generator\n\nfrom sqlalchemy.engine import Engine, create_engine\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom db.settings import db_settings\n\n# Create SQLAlchemy Engine using a database URL\ndb_url: str = db_settings.get_db_url()\ndb_engine: Engine = create_engine(db_url, pool_pre_ping=True)\n\n# Create a SessionLocal class\n# https://fastapi.tiangolo.com/tutorial/sql-databases/#create-a-sessionlocal-class\nSessionLocal: sessionmaker[Session] = sessionmaker(autocommit=False, autoflush=False, bind=db_engine)\n\n\ndef get_db() -> Generator[Session, None, None]:\n    \"\"\"\n    Dependency to get a database session.\n\n    Yields:\n        Session: An SQLAlchemy database session.\n    \"\"\"\n    db: Session = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n"}
{"type": "source_file", "path": "workflows/__init__.py", "content": ""}
{"type": "source_file", "path": "workspace/settings.py", "content": "from pathlib import Path\n\n# Workspace name: only used for naming cloud resources\nWS_NAME = \"agno-demo-app\"\n\n# Path to the workspace root\nWS_ROOT = Path(__file__).parent.parent.resolve()\n\n# Environment names\nDEV_ENV = \"dev\"\nPRD_ENV = \"prd\"\n\n# Dev key is used for naming development resources\nDEV_KEY = f\"{WS_NAME}-{DEV_ENV}\"\n# Production key is used for naming production resources\nPRD_KEY = f\"{WS_NAME}-{PRD_ENV}\"\n\n# AWS settings\n# Region for AWS resources\nAWS_REGION = \"us-east-1\"\n# AWS profile for the ai-demos account\nAWS_PROFILE = \"ai-demos\"\n\n# Availability Zones for AWS resources\nAWS_AZ1 = \"us-east-1a\"\nAWS_AZ2 = \"us-east-1b\"\n\n# Subnet IDs in the aws_region\nSUBNET_IDS = [\"subnet-067b9140c018160f6\", \"subnet-0f3ba3094301af603\"]\n\n# Image Settings\n# Repository for images\nIMAGE_REPO = \"agnohq\"\n\n# Build images locally\nBUILD_IMAGES = True\n"}
{"type": "source_file", "path": "agents/settings.py", "content": "from pydantic_settings import BaseSettings\n\n\nclass AgentSettings(BaseSettings):\n    \"\"\"Agent settings that can be set using environment variables.\n\n    Reference: https://pydantic-docs.helpmanual.io/usage/settings/\n    \"\"\"\n\n    gpt_4: str = \"gpt-4o\"\n    gpt_4o_mini: str = \"gpt-4o-mini\"\n    embedding_model: str = \"text-embedding-3-small\"\n    default_max_completion_tokens: int = 16000\n    default_temperature: float = 0\n\n\n# Create an AgentSettings object\nagent_settings = AgentSettings()\n"}
{"type": "source_file", "path": "api/routes/v1_router.py", "content": "from fastapi import APIRouter\n\nfrom api.routes.playground import playground_router\nfrom api.routes.health import health_check_router\n\nv1_router = APIRouter(prefix=\"/v1\")\nv1_router.include_router(playground_router)\nv1_router.include_router(health_check_router)\n"}
{"type": "source_file", "path": "workflows/blog_post_generator.py", "content": "\"\"\"🎨 Blog Post Generator - Your AI Content Creation Studio!\n\nThis advanced example demonstrates how to build a sophisticated blog post generator that combines\nweb research capabilities with professional writing expertise. The workflow uses a multi-stage\napproach:\n1. Intelligent web research and source gathering\n2. Content extraction and processing\n3. Professional blog post writing with proper citations\n\nKey capabilities:\n- Advanced web research and source evaluation\n- Content scraping and processing\n- Professional writing with SEO optimization\n- Automatic content caching for efficiency\n- Source attribution and fact verification\n\nExample blog topics to try:\n- \"The Rise of Artificial General Intelligence: Latest Breakthroughs\"\n- \"How Quantum Computing is Revolutionizing Cybersecurity\"\n- \"Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint\"\n- \"The Future of Work: AI and Human Collaboration\"\n- \"Space Tourism: From Science Fiction to Reality\"\n- \"Mindfulness and Mental Health in the Digital Age\"\n- \"The Evolution of Electric Vehicles: Current State and Future Trends\"\n\nRun `pip install openai duckduckgo-search newspaper4k lxml_html_clean sqlalchemy agno` to install dependencies.\n\"\"\"\n\nimport json\nfrom textwrap import dedent\nfrom typing import Dict, Iterator, Optional\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.storage.workflow.postgres import PostgresWorkflowStorage\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.tools.newspaper4k import Newspaper4kTools\nfrom agno.utils.log import logger\nfrom agno.workflow import RunEvent, RunResponse, Workflow\nfrom pydantic import BaseModel, Field\nfrom db.session import db_url\nfrom workflows.settings import workflow_settings\n\n\nclass NewsArticle(BaseModel):\n    title: str = Field(..., description=\"Title of the article.\")\n    url: str = Field(..., description=\"Link to the article.\")\n    summary: Optional[str] = Field(..., description=\"Summary of the article if available.\")\n\n\nclass SearchResults(BaseModel):\n    articles: list[NewsArticle]\n\n\nclass ScrapedArticle(BaseModel):\n    title: str = Field(..., description=\"Title of the article.\")\n    url: str = Field(..., description=\"Link to the article.\")\n    summary: Optional[str] = Field(..., description=\"Summary of the article if available.\")\n    content: Optional[str] = Field(\n        ...,\n        description=\"Full article content in markdown format. None if content is unavailable.\",\n    )\n\n\nclass BlogPostGenerator(Workflow):\n    \"\"\"Advanced workflow for generating professional blog posts with proper research and citations.\"\"\"\n\n    description: str = dedent(\"\"\"\\\n    An intelligent blog post generator that creates engaging, well-researched content.\n    This workflow orchestrates multiple AI agents to research, analyze, and craft\n    compelling blog posts that combine journalistic rigor with engaging storytelling.\n    The system excels at creating content that is both informative and optimized for\n    digital consumption.\n    \"\"\")\n\n    # Search Agent: Handles intelligent web searching and source gathering\n    searcher: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        tools=[DuckDuckGoTools()],\n        description=dedent(\"\"\"\\\n        You are BlogResearch-X, an elite research assistant specializing in discovering\n        high-quality sources for compelling blog content. Your expertise includes:\n\n        - Finding authoritative and trending sources\n        - Evaluating content credibility and relevance\n        - Identifying diverse perspectives and expert opinions\n        - Discovering unique angles and insights\n        - Ensuring comprehensive topic coverage\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Search Strategy 🔍\n           - Find 10-15 relevant sources and select the 5-7 best ones\n           - Prioritize recent, authoritative content\n           - Look for unique angles and expert insights\n        2. Source Evaluation 📊\n           - Verify source credibility and expertise\n           - Check publication dates for timeliness\n           - Assess content depth and uniqueness\n        3. Diversity of Perspectives 🌐\n           - Include different viewpoints\n           - Gather both mainstream and expert opinions\n           - Find supporting data and statistics\\\n        \"\"\"),\n        response_model=SearchResults,\n        structured_outputs=True,\n    )\n\n    # Content Scraper: Extracts and processes article content\n    article_scraper: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        tools=[Newspaper4kTools()],\n        description=dedent(\"\"\"\\\n        You are ContentBot-X, a specialist in extracting and processing digital content\n        for blog creation. Your expertise includes:\n\n        - Efficient content extraction\n        - Smart formatting and structuring\n        - Key information identification\n        - Quote and statistic preservation\n        - Maintaining source attribution\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Content Extraction 📑\n           - Extract content from the article\n           - Preserve important quotes and statistics\n           - Maintain proper attribution\n           - Handle paywalls gracefully\n        2. Content Processing 🔄\n           - Format text in clean markdown\n           - Preserve key information\n           - Structure content logically\n        3. Quality Control ✅\n           - Verify content relevance\n           - Ensure accurate extraction\n           - Maintain readability\\\n        \"\"\"),\n        response_model=ScrapedArticle,\n        structured_outputs=True,\n    )\n\n    # Content Writer Agent: Crafts engaging blog posts from research\n    writer: Agent = Agent(\n        model=OpenAIChat(id=workflow_settings.gpt_4_mini),\n        description=dedent(\"\"\"\\\n        You are BlogMaster-X, an elite content creator combining journalistic excellence\n        with digital marketing expertise. Your strengths include:\n\n        - Crafting viral-worthy headlines\n        - Writing engaging introductions\n        - Structuring content for digital consumption\n        - Incorporating research seamlessly\n        - Optimizing for SEO while maintaining quality\n        - Creating shareable conclusions\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n        1. Content Strategy 📝\n           - Craft attention-grabbing headlines\n           - Write compelling introductions\n           - Structure content for engagement\n           - Include relevant subheadings\n        2. Writing Excellence ✍️\n           - Balance expertise with accessibility\n           - Use clear, engaging language\n           - Include relevant examples\n           - Incorporate statistics naturally\n        3. Source Integration 🔍\n           - Cite sources properly\n           - Include expert quotes\n           - Maintain factual accuracy\n        4. Digital Optimization 💻\n           - Structure for scanability\n           - Include shareable takeaways\n           - Optimize for SEO\n           - Add engaging subheadings\\\n        \"\"\"),\n        expected_output=dedent(\"\"\"\\\n        # {Viral-Worthy Headline}\n\n        ## Introduction\n        {Engaging hook and context}\n\n        ## {Compelling Section 1}\n        {Key insights and analysis}\n        {Expert quotes and statistics}\n\n        ## {Engaging Section 2}\n        {Deeper exploration}\n        {Real-world examples}\n\n        ## {Practical Section 3}\n        {Actionable insights}\n        {Expert recommendations}\n\n        ## Key Takeaways\n        - {Shareable insight 1}\n        - {Practical takeaway 2}\n        - {Notable finding 3}\n\n        ## Sources\n        {Properly attributed sources with links}\\\n        \"\"\"),\n        markdown=True,\n    )\n\n    def run(  # type: ignore\n        self,\n        topic: str,\n        use_search_cache: bool = True,\n        use_scrape_cache: bool = True,\n        use_cached_report: bool = True,\n    ) -> Iterator[RunResponse]:\n        logger.info(f\"Generating a blog post on: {topic}\")\n\n        # Use the cached blog post if use_cache is True\n        if use_cached_report:\n            cached_blog_post = self.get_cached_blog_post(topic)\n            if cached_blog_post:\n                yield RunResponse(content=cached_blog_post, event=RunEvent.workflow_completed)\n                return\n\n        # Search the web for articles on the topic\n        search_results: Optional[SearchResults] = self.get_search_results(topic, use_search_cache)\n        # If no search_results are found for the topic, end the workflow\n        if search_results is None or len(search_results.articles) == 0:\n            yield RunResponse(\n                event=RunEvent.workflow_completed,\n                content=f\"Sorry, could not find any articles on the topic: {topic}\",\n            )\n            return\n\n        # Scrape the search results\n        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(\n            topic, search_results, use_scrape_cache\n        )\n\n        # Prepare the input for the writer\n        writer_input = {\n            \"topic\": topic,\n            \"articles\": [v.model_dump() for v in scraped_articles.values()],\n        }\n\n        # Run the writer and yield the response\n        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)\n\n        # Save the blog post in the cache\n        if self.writer.run_response:\n            self.add_blog_post_to_cache(topic, str(self.writer.run_response.content))\n\n    def get_cached_blog_post(self, topic: str) -> Optional[str]:\n        logger.info(\"Checking if cached blog post exists\")\n\n        return self.session_state.get(\"blog_posts\", {}).get(topic)\n\n    def add_blog_post_to_cache(self, topic: str, blog_post: str):\n        logger.info(f\"Saving blog post for topic: {topic}\")\n        self.session_state.setdefault(\"blog_posts\", {})\n        self.session_state[\"blog_posts\"][topic] = blog_post\n\n    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:\n        logger.info(\"Checking if cached search results exist\")\n        search_results = self.session_state.get(\"search_results\", {}).get(topic)\n        return (\n            SearchResults.model_validate(search_results)\n            if search_results and isinstance(search_results, dict)\n            else search_results\n        )\n\n    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):\n        logger.info(f\"Saving search results for topic: {topic}\")\n        self.session_state.setdefault(\"search_results\", {})\n        self.session_state[\"search_results\"][topic] = search_results\n\n    def get_cached_scraped_articles(self, topic: str):\n        logger.info(\"Checking if cached scraped articles exist\")\n        scraped_articles = self.session_state.get(\"scraped_articles\", {}).get(topic)\n        return (\n            ScrapedArticle.model_validate(scraped_articles)\n            if scraped_articles and isinstance(scraped_articles, dict)\n            else scraped_articles\n        )\n\n    def add_scraped_articles_to_cache(self, topic: str, scraped_articles: Dict[str, ScrapedArticle]):\n        logger.info(f\"Saving scraped articles for topic: {topic}\")\n        self.session_state.setdefault(\"scraped_articles\", {})\n        self.session_state[\"scraped_articles\"][topic] = scraped_articles\n\n    def get_search_results(\n        self, topic: str, use_search_cache: bool, num_attempts: int = 3\n    ) -> Optional[SearchResults]:\n        # Get cached search_results from the session state if use_search_cache is True\n        if use_search_cache:\n            try:\n                search_results_from_cache = self.get_cached_search_results(topic)\n                if search_results_from_cache is not None:\n                    search_results = SearchResults.model_validate(search_results_from_cache)\n                    logger.info(f\"Found {len(search_results.articles)} articles in cache.\")\n                    return search_results\n            except Exception as e:\n                logger.warning(f\"Could not read search results from cache: {e}\")\n\n        # If there are no cached search_results, use the searcher to find the latest articles\n        for attempt in range(num_attempts):\n            try:\n                searcher_response: RunResponse = self.searcher.run(topic)\n                if (\n                    searcher_response is not None\n                    and searcher_response.content is not None\n                    and isinstance(searcher_response.content, SearchResults)\n                ):\n                    article_count = len(searcher_response.content.articles)\n                    logger.info(f\"Found {article_count} articles on attempt {attempt + 1}\")\n                    # Cache the search results\n                    self.add_search_results_to_cache(topic, searcher_response.content)\n                    return searcher_response.content\n                else:\n                    logger.warning(f\"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type\")\n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n\n        logger.error(f\"Failed to get search results after {num_attempts} attempts\")\n        return None\n\n    def scrape_articles(\n        self, topic: str, search_results: SearchResults, use_scrape_cache: bool\n    ) -> Dict[str, ScrapedArticle]:\n        scraped_articles: Dict[str, ScrapedArticle] = {}\n\n        # Get cached scraped_articles from the session state if use_scrape_cache is True\n        if use_scrape_cache:\n            try:\n                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)\n                if scraped_articles_from_cache is not None:\n                    scraped_articles = scraped_articles_from_cache\n                    logger.info(f\"Found {len(scraped_articles)} scraped articles in cache.\")\n                    return scraped_articles\n            except Exception as e:\n                logger.warning(f\"Could not read scraped articles from cache: {e}\")\n\n        # Scrape the articles that are not in the cache\n        for article in search_results.articles:\n            if article.url in scraped_articles:\n                logger.info(f\"Found scraped article in cache: {article.url}\")\n                continue\n\n            article_scraper_response: RunResponse = self.article_scraper.run(article.url)\n            if (\n                article_scraper_response is not None\n                and article_scraper_response.content is not None\n                and isinstance(article_scraper_response.content, ScrapedArticle)\n            ):\n                scraped_articles[article_scraper_response.content.url] = article_scraper_response.content\n                logger.info(f\"Scraped article: {article_scraper_response.content.url}\")\n\n        # Save the scraped articles in the session state\n        self.add_scraped_articles_to_cache(topic, scraped_articles)\n        return scraped_articles\n\n\n# Run the workflow if the script is executed directly\ndef write_blog_post(self, topic: str, scraped_articles: Dict[str, ScrapedArticle]) -> Iterator[RunResponse]:\n    logger.info(\"Writing blog post\")\n    # Prepare the input for the writer\n    writer_input = {\n        \"topic\": topic,\n        \"articles\": [v.model_dump() for v in scraped_articles.values()],\n    }\n    # Run the writer and yield the response\n    yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)\n    # Save the blog post in the cache\n    self.add_blog_post_to_cache(topic, self.writer.run_response.content)\n\n\ndef get_blog_post_generator(debug_mode: bool = False) -> BlogPostGenerator:\n    return BlogPostGenerator(\n        workflow_id=\"generate-blog-post-on\",\n        storage=PostgresWorkflowStorage(\n            table_name=\"blog_post_generator_workflows\",\n            db_url=db_url,\n        ),\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "agents/research.py", "content": "from textwrap import dedent\nfrom typing import Optional\nfrom datetime import datetime\n\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.exa import ExaTools\nfrom agno.storage.agent.postgres import PostgresAgentStorage\n\nfrom agents.settings import agent_settings\nfrom db.session import db_url\n\nresearch_agent_storage = PostgresAgentStorage(table_name=\"research_agent\", db_url=db_url)\n\n\ndef get_research_agent(\n    user_id: Optional[str] = None,\n    session_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Agent:\n    return Agent(\n        name=\"Research Agent\",\n        agent_id=\"research-agent\",\n        session_id=session_id,\n        user_id=user_id,\n        model=OpenAIChat(\n            id=agent_settings.gpt_4,\n            max_tokens=agent_settings.default_max_completion_tokens,\n            temperature=agent_settings.default_temperature,\n        ),\n        tools=[ExaTools(start_published_date=datetime.now().strftime(\"%Y-%m-%d\"), type=\"keyword\")],\n        description=dedent(\"\"\"\\\n            You are Professor X-1000, a distinguished AI research scientist with expertise\n            in analyzing and synthesizing complex information. Your specialty lies in creating\n            compelling, fact-based reports that combine academic rigor with engaging narrative.\n\n            Your writing style is:\n            - Clear and authoritative\n            - Engaging but professional\n            - Fact-focused with proper citations\n            - Accessible to educated non-specialists\\\n        \"\"\"),\n        instructions=dedent(\"\"\"\\\n            Begin by running 3 distinct searches to gather comprehensive information.\n            Analyze and cross-reference sources for accuracy and relevance.\n            Structure your report following academic standards but maintain readability.\n            Include only verifiable facts with proper citations.\n            Create an engaging narrative that guides the reader through complex topics.\n            End with actionable takeaways and future implications.\\\n        \"\"\"),\n        expected_output=dedent(\"\"\"\\\n        A professional research report in markdown format:\n\n        # {Compelling Title That Captures the Topic's Essence}\n\n        ## Executive Summary\n        {Brief overview of key findings and significance}\n\n        ## Introduction\n        {Context and importance of the topic}\n        {Current state of research/discussion}\n\n        ## Key Findings\n        {Major discoveries or developments}\n        {Supporting evidence and analysis}\n\n        ## Implications\n        {Impact on field/society}\n        {Future directions}\n\n        ## Key Takeaways\n        - {Bullet point 1}\n        - {Bullet point 2}\n        - {Bullet point 3}\n\n        ## References\n        - [Source 1](link) - Key finding/quote\n        - [Source 2](link) - Key finding/quote\n        - [Source 3](link) - Key finding/quote\n\n        ---\n        Report generated by Professor X-1000\n        Advanced Research Systems Division\n        Date: {current_date}\\\n        \"\"\"),\n        markdown=True,\n        add_history_to_messages=True,\n        num_history_responses=5,\n        add_datetime_to_instructions=True,\n        storage=research_agent_storage,\n        debug_mode=debug_mode,\n    )\n"}
{"type": "source_file", "path": "db/tables/__init__.py", "content": "from db.tables.base import Base\n"}
{"type": "source_file", "path": "workspace/prd_resources.py", "content": "from agno.aws.app.fastapi import FastApi\nfrom agno.aws.resources import AwsResources\nfrom agno.aws.resource.ecs import EcsCluster\nfrom agno.aws.resource.ec2 import SecurityGroup, InboundRule\nfrom agno.aws.resource.rds import DbInstance, DbSubnetGroup\nfrom agno.aws.resource.reference import AwsReference\nfrom agno.aws.resource.s3 import S3Bucket\nfrom agno.aws.resource.secret import SecretsManager\nfrom agno.docker.resources import DockerResources\nfrom agno.docker.resource.image import DockerImage\n\nfrom workspace.settings import (\n    BUILD_IMAGES,\n    IMAGE_REPO,\n    PRD_ENV,\n    PRD_KEY,\n    WS_NAME,\n    WS_ROOT,\n    SUBNET_IDS,\n    AWS_AZ1,\n    AWS_REGION,\n    AWS_PROFILE,\n)\n\n#\n# -*- AWS resources for the production environment\n#\n# Skip resource deletion when running `agno ws down`\nskip_delete: bool = False\n# Save resource outputs to workspace/outputs\nsave_output: bool = True\n\n# -*- Production image\nprd_image = DockerImage(\n    name=f\"{IMAGE_REPO}/{WS_NAME}\",\n    tag=PRD_ENV,\n    enabled=BUILD_IMAGES,\n    path=str(WS_ROOT),\n    platforms=[\"linux/amd64\", \"linux/arm64\"],\n    push_image=True,\n)\n\n# -*- S3 bucket for production data (set enabled=True when needed)\nprd_bucket = S3Bucket(\n    name=f\"{PRD_KEY}-data\",\n    enabled=False,\n    acl=\"private\",\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n\n# -*- Secrets for production application\nprd_secret = SecretsManager(\n    name=f\"{PRD_KEY}-secret\",\n    group=\"api\",\n    # Create secret from workspace/secrets/prd_api_secrets.yml\n    secret_files=[WS_ROOT.joinpath(\"workspace/secrets/prd_api_secrets.yml\")],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n# -*- Secrets for production database\nprd_db_secret = SecretsManager(\n    name=f\"{PRD_KEY}-db-secret\",\n    group=\"db\",\n    # Create secret from workspace/secrets/prd_db_secrets.yml\n    secret_files=[WS_ROOT.joinpath(\"workspace/secrets/prd_db_secrets.yml\")],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n\n# -*- Security Group for the load balancer\nprd_lb_sg = SecurityGroup(\n    name=f\"{PRD_KEY}-lb-security-group\",\n    group=\"api\",\n    description=\"Security group for the load balancer\",\n    inbound_rules=[\n        InboundRule(\n            description=\"Allow HTTP traffic from the internet\",\n            port=80,\n            cidr_ip=\"0.0.0.0/0\",\n        ),\n        InboundRule(\n            description=\"Allow HTTPS traffic from the internet\",\n            port=443,\n            cidr_ip=\"0.0.0.0/0\",\n        ),\n    ],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n# -*- Security Group for the application\nprd_sg = SecurityGroup(\n    name=f\"{PRD_KEY}-security-group\",\n    enabled=True,\n    group=\"api\",\n    description=\"Security group for the production api\",\n    inbound_rules=[\n        InboundRule(\n            description=\"Allow traffic from LB to the FastAPI server\",\n            port=8000,\n            security_group_id=AwsReference(prd_lb_sg.get_security_group_id),\n        ),\n    ],\n    depends_on=[prd_lb_sg],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n# -*- Security Group for the database\nprd_db_port = 5432\nprd_db_sg = SecurityGroup(\n    name=f\"{PRD_KEY}-db-security-group\",\n    enabled=True,\n    group=\"db\",\n    description=\"Security group for the production database\",\n    inbound_rules=[\n        InboundRule(\n            description=\"Allow traffic from the FastAPI server to the database\",\n            port=prd_db_port,\n            security_group_id=AwsReference(prd_sg.get_security_group_id),\n        ),\n    ],\n    depends_on=[prd_sg],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n\n# -*- RDS Database Subnet Group\nprd_db_subnet_group = DbSubnetGroup(\n    name=f\"{PRD_KEY}-db-sg\",\n    enabled=True,\n    group=\"db\",\n    subnet_ids=SUBNET_IDS,\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n\n# -*- RDS Database Instance\nprd_db = DbInstance(\n    name=f\"{PRD_KEY}-db\",\n    enabled=True,\n    group=\"db\",\n    db_name=\"api\",\n    port=prd_db_port,\n    engine=\"postgres\",\n    engine_version=\"17.2\",\n    allocated_storage=64,\n    db_instance_class=\"db.r6g.large\",\n    db_security_groups=[prd_db_sg],\n    db_subnet_group=prd_db_subnet_group,\n    availability_zone=AWS_AZ1,\n    publicly_accessible=False,\n    enable_performance_insights=True,\n    aws_secret=prd_db_secret,\n    skip_delete=skip_delete,\n    save_output=save_output,\n    # Do not wait for the db to be deleted\n    wait_for_delete=False,\n)\n\n# -*- ECS cluster\nlaunch_type = \"FARGATE\"\nprd_ecs_cluster = EcsCluster(\n    name=f\"{PRD_KEY}-cluster\",\n    ecs_cluster_name=WS_NAME,\n    capacity_providers=[launch_type],\n    skip_delete=skip_delete,\n    save_output=save_output,\n)\n\n# -*- Build container environment\ncontainer_env = {\n    \"RUNTIME_ENV\": \"prd\",\n    \"PHI_MONITORING\": \"True\",\n    # Database configuration\n    \"DB_HOST\": AwsReference(prd_db.get_db_endpoint),\n    \"DB_PORT\": AwsReference(prd_db.get_db_port),\n    \"DB_USER\": AwsReference(prd_db.get_master_username),\n    \"DB_PASS\": AwsReference(prd_db.get_master_user_password),\n    \"DB_DATABASE\": AwsReference(prd_db.get_db_name),\n    # Wait for database to be available before starting the application\n    \"WAIT_FOR_DB\": True,\n    # Migrate database on startup using alembic\n    # \"MIGRATE_DB\": ws_settings.prd_db_enabled,\n}\n\n# -*- FastApi running on ECS\nprd_fastapi = FastApi(\n    name=PRD_KEY,\n    enabled=True,\n    group=\"api\",\n    image=prd_image,\n    command=\"uvicorn api.main:app --workers 4\",\n    port_number=8000,\n    ecs_task_cpu=\"2048\",\n    ecs_task_memory=\"4096\",\n    ecs_service_count=2,\n    ecs_cluster=prd_ecs_cluster,\n    aws_secrets=[prd_secret],\n    subnets=SUBNET_IDS,\n    security_groups=[prd_sg],\n    # To enable HTTPS, create an ACM certificate and add the ARN below:\n    load_balancer_enable_https=True,\n    load_balancer_certificate_arn=\"arn:aws:acm:us-east-1:497891874516:certificate/e822946f-02c9-4ed1-8177-97ef2f4f5b72\",\n    load_balancer_security_groups=[prd_lb_sg],\n    create_load_balancer=True,\n    health_check_path=\"/v1/health\",\n    env_vars=container_env,\n    use_cache=True,\n    skip_delete=skip_delete,\n    save_output=save_output,\n    # Do not wait for the service to stabilize\n    wait_for_create=False,\n    # Do not wait for the service to be deleted\n    wait_for_delete=False,\n)\n\n# -*- Production DockerResources\nprd_docker_resources = DockerResources(\n    env=PRD_ENV,\n    network=WS_NAME,\n    resources=[prd_image],\n)\n\n# -*- Production AwsResources\nprd_aws_config = AwsResources(\n    env=PRD_ENV,\n    aws_region=AWS_REGION,\n    aws_profile=AWS_PROFILE,\n    apps=[prd_fastapi],\n    resources=(\n        prd_lb_sg,\n        prd_sg,\n        prd_db_sg,\n        prd_secret,\n        prd_db_secret,\n        prd_db_subnet_group,\n        prd_db,\n        prd_bucket,\n    ),\n)\n"}
