{"repo_info": {"repo_name": "GCNet", "repo_owner": "zeroQiaoba", "repo_url": "https://github.com/zeroQiaoba/GCNet"}}
{"type": "test_file", "path": "baseline-cpmnet/test_lianzheng.py", "content": "import os\nimport time\nimport warnings\nimport numpy as np\nfrom util.util import read_cmumosi_data\nfrom util.get_sn import get_sn\nfrom util.model import CPMNets\nimport util.classfiy as classfiy\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport sys\nsys.path.append('../')\nimport config\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--lsd-dim', type=int, default=512, help='dimensionality of the latent space data [default: 512]')\n    parser.add_argument('--epochs-train', type=int, default=20, metavar='N', help='number of epochs to train [default: 20]')\n    parser.add_argument('--epochs-test', type=int, default=100, metavar='N', help='number of epochs to test [default: 50]')\n    parser.add_argument('--lamb', type=float, default=10., help='trade off parameter [default: 10]')\n    parser.add_argument('--missing-rate', type=float, default=0.0, help='view missing rate [default: 0]')\n    parser.add_argument('--normalize', action='store_true', default=False, help='whether normalize input features')\n    parser.add_argument('--dataset', type=str, default='cmumosi', help='input dataset')\n    parser.add_argument('--test_mask', type=str, default=None, help='test under same mask for fair comparision')\n    args = parser.parse_args()\n\n    print (f'========= starting ============')\n    folder_f1 = []\n    folder_acc = []\n    folder_recon = []\n    folder_save = []\n    if args.dataset in ['cmumosi', 'cmumosei']:\n        num_folder = 1\n    elif args.dataset == 'iemocapfour':\n        num_folder = 5\n    elif args.dataset == 'iemocapsix':\n        num_folder = 5\n\n    for index in range(num_folder):\n        print (f'>>>>> Cross-validation: training on the {index+1} folder >>>>>')\n\n        # read data\n        trainData, testData, view_num = read_cmumosi_data(f'data/{args.dataset}/{index+1}', args.normalize)\n        outdim_size = [trainData.data[str(i)].shape[1] for i in range(view_num)] # two view feature dim: [adim, vdim, tdim]\n\n        # set layer size\n        layer_size = [[outdim_size[i]] for i in range(view_num)]  # [[adim], [vdim], [tdim]]\n        epoch = [args.epochs_train, args.epochs_test]  # [20, 100]\n        learning_rate = [0.001, 0.01]\n\n        # Randomly generated missing matrix [under same missing rate]\n        Sn = get_sn(view_num, trainData.num_examples + testData.num_examples, args.missing_rate) # [samplenum, 2]\n        Sn_train = Sn[np.arange(trainData.num_examples)]\n        Sn_test = Sn[np.arange(testData.num_examples) + trainData.num_examples]\n        ######################################################\n        if args.test_mask != None:\n            print (f'using predefined mask!!')\n            name2mask = np.load(args.test_mask, allow_pickle=True)['name2mask'].tolist()\n            Sn_test = []\n            for name in testData.names:\n                mask = name2mask[name] # (A, L, V)\n                mask = [mask[0], mask[2], mask[1]] # (A, V, L)\n                Sn_test.append(mask)\n            Sn_test = np.array(Sn_test) # [sample_num, 3]\n        else:\n            print (f'using random initialized mask!!')\n        ######################################################\n\n        # Model building\n        model = CPMNets(view_num, trainData.num_examples, testData.num_examples, layer_size, args.lsd_dim, \n                        learning_rate, args.lamb)\n\n        # train: use h -> optimize on y and v [using args.missing_rate]\n        model.train(trainData.data, Sn_train, trainData.labels.reshape(trainData.num_examples), epoch[0])\n        H_train = model.get_h_train()\n\n        # test: use h -> optimize on v [using all missing rate]\n        Imputation_loss = model.test(testData.data, Sn_test, testData.labels.reshape(testData.num_examples), epoch[1])\n        H_test = model.get_h_test()\n\n        ## gain prediction results for testData\n        total_pred = classfiy.ave(H_train, H_test, trainData.labels)\n        total_label = testData.labels\n        f1 = f1_score(total_label, total_pred, average='weighted')\n        acc = accuracy_score(total_label, total_pred)\n\n        folder_acc.append(acc)\n        folder_f1.append(f1)\n        folder_recon.append(Imputation_loss / testData.num_examples)\n        folder_save.append({'test_labels': total_label, 'test_preds': total_pred, 'test_hiddens': H_test, 'test_names': testData.names})\n        print (f'>>>>> Finish: training on the {index+1} folder >>>>>')\n\n\n    ## save results\n    # gain suffix_name\n    suffix_name = f'{args.dataset}_cpmnet_mask:{args.missing_rate:.1f}'\n\n    mean_f1 = np.mean(np.array(folder_f1))\n    mean_acc = np.mean(np.array(folder_acc))\n    mean_recon = np.mean(np.array(folder_recon))\n    res_name = f'f1:{mean_f1:2.2%}_acc:{mean_acc:2.2%}_reconloss:{mean_recon:.4f}'\n\n    save_root = config.MODEL_DIR\n    if not os.path.exists(save_root): os.makedirs(save_root)\n    save_path = f'{save_root}/{suffix_name}_{res_name}_{time.time()}.npz'\n    print (f'save results in {save_path}')\n    np.savez_compressed(save_path,\n                        args=np.array(args, dtype=object),\n                        folder_save=np.array(folder_save, dtype=object) # save non-structure type\n                        )\n    print (f' =========== finish =========== ')\n\n"}
{"type": "test_file", "path": "baseline-mmin/opts/test_opts.py", "content": "from .base_opts import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    \"\"\"This class includes training options.\n\n    It also includes shared options defined in BaseOptions.\n    \"\"\"\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n        parser.add_argument('--method', type=str, default='mean', help='How to calculate final test result, [concat, mean]')\n        parser.add_argument('--simple', action='store_true', help='simple print information')\n        self.isTrain = False\n        return parser\n"}
{"type": "source_file", "path": "baseline-cpmnet/util/get_sn.py", "content": "import numpy as np\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef get_sn(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return:Sn\n    \"\"\"\n    one_rate = 1-missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\ndef save_Sn(Sn, str_name):\n    np.savetxt(str_name + '.csv', Sn, delimiter=',')\n\n\ndef load_Sn(str_name):\n    return np.loadtxt(str_name + '.csv', delimiter=',')\n"}
{"type": "source_file", "path": "baseline-cca/dccae.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nimport os\nimport cv2\nimport glob\nimport time\nimport tqdm\nimport random\nfrom functools import partial\n\nfrom apex import amp\nimport matplotlib.pyplot as plt\n\nimport gzip\n\nimport os\nfrom sklearn import svm\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport numpy as np\nfrom numpy.random import randint\n\nfrom util import *\n\nimport sys\nsys.path.append('../')\nimport config\n\ndef loss(H1, H2, H3, outdim_size, use_all_singular_values=False):\n\n    loss1 = pairloss(H1, H2, outdim_size, use_all_singular_values)\n    loss2 = pairloss(H1, H3, outdim_size, use_all_singular_values)\n    loss3 = pairloss(H2, H3, outdim_size, use_all_singular_values)\n\n    return loss1 + loss2 + loss3\n\ndef reconstruction_loss(r, x):\n    #return F.l1_loss(r, x)\n    return F.mse_loss(r, x)\n\ndef pairloss(H1, H2, outdim_size, use_all_singular_values=False):\n    r1 = 1e-3\n    r2 = 1e-3\n    eps = 1e-9\n\n    H1, H2 = H1.t(), H2.t()\n\n    o1 = o2 = H1.size(0)\n\n    m = H1.size(1)\n\n    H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n    H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n\n    SigmaHat12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.t())\n    SigmaHat11 = (1.0 / (m - 1)) * torch.matmul(H1bar, H1bar.t()) + r1 * torch.eye(o1, device=H1.device)\n    SigmaHat22 = (1.0 / (m - 1)) * torch.matmul(H2bar, H2bar.t()) + r2 * torch.eye(o2, device=H1.device)\n\n    # Calculating the root inverse of covariance matrices by using eigen decomposition\n    [D1, V1] = torch.symeig(SigmaHat11, eigenvectors=True)\n    [D2, V2] = torch.symeig(SigmaHat22, eigenvectors=True)\n\n    # Added to increase stability\n    posInd1 = torch.gt(D1, eps).nonzero()[:, 0]\n    D1 = D1[posInd1]\n    V1 = V1[:, posInd1]\n    posInd2 = torch.gt(D2, eps).nonzero()[:, 0]\n    D2 = D2[posInd2]\n    V2 = V2[:, posInd2]\n\n    SigmaHat11RootInv = torch.matmul(torch.matmul(V1, torch.diag(D1 ** -0.5)), V1.t())\n    SigmaHat22RootInv = torch.matmul(torch.matmul(V2, torch.diag(D2 ** -0.5)), V2.t())\n\n    Tval = torch.matmul(torch.matmul(SigmaHat11RootInv, SigmaHat12), SigmaHat22RootInv)\n\n    if use_all_singular_values:\n        # all singular values are used to calculate the correlation\n        tmp = torch.matmul(Tval.t(), Tval)\n        corr = torch.trace(torch.sqrt(tmp))\n        # assert torch.isnan(corr).item() == 0\n    else:\n        # just the top outdim_size singular values are used\n        trace_TT = torch.matmul(Tval.t(), Tval)\n        trace_TT = torch.add(trace_TT, (torch.eye(trace_TT.shape[0])*r1).to(H1.device)) # regularization for more stability\n        U, V = torch.symeig(trace_TT, eigenvectors=True)\n        U = torch.where(U>eps, U, (torch.ones(U.shape)*eps).to(H1.device))\n        U = U.topk(outdim_size)[0]\n        corr = torch.sum(torch.sqrt(U))\n\n    return - corr\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, chan):\n        super().__init__()\n        layers = nn.ModuleList([])\n        cin = chan[0]\n        for cout in chan[1:-1]:\n            layers.append(nn.Linear(cin, cout))\n            layers.append(nn.BatchNorm1d(cout))\n            layers.append(nn.ReLU(True))\n            cin = cout\n        layers.append(nn.Linear(cin, chan[-1]))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\n\nclass DeepCCA(nn.Module):\n    def __init__(self, chan1, chan2, chan3):\n        super().__init__()\n\n        self.encoder1 = MLP(chan1)\n        self.decoder1 = MLP(chan1[::-1])\n\n        self.encoder2 = MLP(chan2)\n        self.decoder2 = MLP(chan2[::-1])\n\n        self.encoder3 = MLP(chan3)\n        self.decoder3 = MLP(chan3[::-1])\n        \n    def forward(self, x1, x2, x3):\n\n        z1 = self.encoder1(x1)\n        z2 = self.encoder2(x2)\n        z3 = self.encoder2(x3)\n\n        r1 = self.decoder1(z1)\n        r2 = self.decoder2(z2)\n        r3 = self.decoder2(z3)\n\n        return z1, z2, z3, r1, r2, r3\n\n\n\nclass NoisyMNIST(Dataset):\n    def __init__(self, X1, X2, X3, y):\n        self.X1 = X1\n        self.X2 = X2\n        self.X3 = X3\n        self.y = y\n    \n    def __getitem__(self, i):\n        data = {\n            'x1': self.X1[i],\n            'x2': self.X2[i],\n            'x3': self.X3[i],\n            'y': self.y[i]\n        }\n\n        return data\n    \n    def __len__(self):\n        #return 100\n        return self.X1.shape[0]\n\n\nclass Trainer(object):\n    def __init__(self, \n                 name, # name of this experiment\n                 model, # network \n                 objective, # loss function\n                 optimizer=None, # optimizer\n                 lr_scheduler=None, # scheduler\n                 metrics=[], # metrics for evaluation\n                 local_rank=0, # which GPU am I\n                 world_size=1, # total num of GPUs\n                 device=None, # device to use, usually setting to None is OK. (auto choose device)\n                 mute=False, # whether to mute all print\n                 resume=True, # resume all information to continue training\n                 opt_level='O0', # amp optimize level\n                 eval_interval=1, # eval once every $ epoch\n                 max_keep_ckpt=3, # max num of saved ckpts in disk\n                 workspace='workspace', # workspace to save logs & ckpts\n                 best_mode='min', # the smaller/larger result, the better\n                 use_checkpoint=\"latest\", # which ckpt to use at init time\n                 scheduler_update_every_step=False, # whether to call scheduler.step() after every train step\n                 ):\n        \n        self.name = name\n        self.mute = mute\n        self.model = model\n        self.resume = resume\n        self.objective = objective\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.metrics = metrics\n        self.local_rank = local_rank\n        self.world_size = world_size\n        self.workspace = workspace\n        self.opt_level = opt_level\n        self.best_mode = best_mode\n        self.max_keep_ckpt = max_keep_ckpt\n        self.eval_interval = eval_interval\n        self.use_checkpoint = use_checkpoint\n        self.time_stamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.scheduler_update_every_step = scheduler_update_every_step\n        self.device = device if device is not None else torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n\n        self.model.to(self.device)\n        if isinstance(self.objective, nn.Module):\n            self.objective.to(self.device)\n\n        if optimizer is None:\n            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=5e-4) # naive adam\n\n        if lr_scheduler is None:\n            self.lr_scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda epoch: 1) # fake scheduler\n\n        self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level=self.opt_level, verbosity=0)\n\n        # variable init\n        self.epoch = 1\n        self.global_step = 0\n        self.local_step = 0\n        self.stats = {\n            \"loss\": [],\n            \"valid_loss\": [],\n            \"results\": [], # metrics[0], or valid_loss\n            \"checkpoints\": [], # record path of saved ckpt, to automatically remove old ckpt\n            \"best_result\": None,\n            }\n\n        # auto fix\n        if len(metrics) == 0:\n            self.best_mode = 'min'\n\n        # workspace prepare\n        self.log_ptr = None\n        if self.workspace is not None:\n            os.makedirs(self.workspace, exist_ok=True)        \n            self.log_path = os.path.join(workspace, \"log.txt\")\n            self.log_ptr = open(self.log_path, \"a+\")\n\n            self.ckpt_path = os.path.join(self.workspace, 'checkpoints')\n            self.best_path = f\"{self.ckpt_path}/{self.name}_best.pth.tar\"\n            os.makedirs(self.ckpt_path, exist_ok=True)\n            \n        self.log(f'[INFO] Trainer: {self.name} | {self.time_stamp} | {self.device} | {self.workspace}')\n        self.log(f'[INFO] #parameters: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n\n        if self.workspace is not None:\n            if self.use_checkpoint == \"scratch\":\n                self.log(\"[INFO] Train from scratch\")\n            elif self.use_checkpoint == \"latest\":\n                self.log(\"[INFO] Loading latest checkpoint ...\")\n                self.load_checkpoint()\n            elif self.use_checkpoint == \"best\":\n                if os.path.exists(self.best_path):\n                    self.log(\"[INFO] Loading best checkpoint ...\")\n                    self.load_checkpoint(self.best_path)\n                else:\n                    self.log(\"[INFO] Best checkpoint not found, loading latest ...\")\n                    self.load_checkpoint()\n            else: # path to ckpt\n                self.log(f\"[INFO] Loading {self.use_checkpoint} ...\")\n                self.load_checkpoint(self.use_checkpoint)\n\n        # extra\n        self.Z1 = []\n        self.Z2 = []\n        self.Z3 = []\n\n    def __del__(self):\n        if self.log_ptr: \n            self.log_ptr.close()\n\n    def log(self, *args):\n        if not self.mute: \n            print(*args)\n        if self.log_ptr: \n            print(*args, file=self.log_ptr)    \n\n    ### ------------------------------\t\n\n    def train_step(self, data):\n        x1, x2, x3 = data['x1'], data['x2'], data['x3']\n        z1, z2, z3, r1, r2, r3 = self.model(x1, x2, x3)\n        loss = self.objective(z1, z2, z3) + (reconstruction_loss(r1, x1) + reconstruction_loss(r2, x2) + reconstruction_loss(r3, x3)) * 0.001\n        return (z1, z2, z3), None, loss\n\n    def eval_step(self, data):\n        (z1, z2, z3), truths, loss = self.train_step(data)\n        self.Z1.append(z1) # [B, fout]\n        self.Z2.append(z2)\n        self.Z3.append(z3)\n        return (z1, z2, z3), truths, loss\n\n    ### ------------------------------\n\n    def train(self, train_loader, valid_loader, max_epochs):\n       \n        for epoch in range(self.epoch, max_epochs + 1):\n            self.epoch = epoch\n            self.train_one_epoch(train_loader)\n            if self.workspace is not None:\n                self.save_checkpoint(True if epoch == max_epochs else False) # save full at last epoch\n            if self.epoch % self.eval_interval == 0:\n                self.evaluate_one_epoch(valid_loader)\n\n    def evaluate(self, loader):\n        if os.path.exists(self.best_path):\n            self.load_checkpoint(self.best_path)\n        else:\n            self.load_checkpoint()\n        self.Z1 = []\n        self.Z2 = []\n        self.Z3 = []\n        self.evaluate_one_epoch(loader)\n        self.Z1 = torch.cat(self.Z1, dim=0).detach().cpu().numpy()\n        self.Z2 = torch.cat(self.Z2, dim=0).detach().cpu().numpy()\n        self.Z3 = torch.cat(self.Z3, dim=0).detach().cpu().numpy()\n\n        # return transformed results !\n        return self.Z1, self.Z2, self.Z3\n\n    def prepare_data(self, data):\n        if isinstance(data, list):\n            for i, v in enumerate(data):\n                if isinstance(v, np.ndarray):\n                    data[i] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[i] = v.to(self.device)\n        elif isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray):\n                    data[k] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[k] = v.to(self.device)\n        elif isinstance(data, np.ndarray):\n            data = torch.from_numpy(data).to(self.device)\n        else: # is_tensor, or other similar objects that has `to`\n            data = data.to(self.device)\n\n        return data\n\n    def train_one_epoch(self, loader):\n\n        total_loss = []\n        for metric in self.metrics:\n            metric.clear()\n\n        self.model.train()\n        \n        self.local_step = 0\n\n        for data in loader:\n            \n            self.local_step += 1\n            self.global_step += 1\n            \n            data = self.prepare_data(data)\n            preds, truths, loss = self.train_step(data)\n\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            if self.scheduler_update_every_step:\n                self.lr_scheduler.step()\n\n            total_loss.append(loss.item())\n            for metric in self.metrics:\n                metric.update(preds, truths)\n                        \n                \n        average_loss = np.mean(total_loss)\n        self.stats[\"loss\"].append(average_loss)\n\n        for metric in self.metrics:\n            self.log(metric.report())\n            metric.clear()\n\n        if not self.scheduler_update_every_step:\n            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                self.lr_scheduler.step(average_loss)\n            else:\n                self.lr_scheduler.step()\n\n        self.log(f\"==> Finished Epoch {self.epoch}, average_loss={average_loss:.4f}\")\n\n\n    def evaluate_one_epoch(self, loader):\n\n        total_loss = []\n        for metric in self.metrics:\n            metric.clear()\n\n        self.model.eval()\n\n        with torch.no_grad():\n            self.local_step = 0\n            for data in loader:    \n                self.local_step += 1\n                \n                data = self.prepare_data(data)\n                preds, truths, loss = self.eval_step(data)\n                \n                total_loss.append(loss.item())            \n                for metric in self.metrics:\n                    metric.update(preds, truths)\n\n        average_loss = np.mean(total_loss)\n        self.stats[\"valid_loss\"].append(average_loss)\n\n        if len(self.metrics) > 0:\n            result = self.metrics[0].measure()\n            self.stats[\"results\"].append(result if self.best_mode == 'min' else - result) # if max mode, use -result\n        else:\n            self.stats[\"results\"].append(average_loss) # if no metric, choose best by min loss\n\n        for metric in self.metrics:\n            self.log(metric.report())\n            metric.clear()\n\n\n    def save_checkpoint(self, full=False):\n        file_path = f\"{self.ckpt_path}/{self.name}_ep{self.epoch:04d}.pth.tar\"\n\n        self.stats[\"checkpoints\"].append(file_path)\n\n        if len(self.stats[\"checkpoints\"]) > self.max_keep_ckpt:\n            old_ckpt = self.stats[\"checkpoints\"].pop(0)\n            if os.path.exists(old_ckpt):\n                os.remove(old_ckpt)\n\n        state = {\n            'epoch': self.epoch,\n            'stats': self.stats,\n            'model': self.model.state_dict(),\n        }\n\n        if full:\n            state['amp'] = amp.state_dict()\n            state['optimizer'] = self.optimizer.state_dict()\n            state['lr_scheduler'] = self.lr_scheduler.state_dict()\n        \n        torch.save(state, file_path)\n        \n        if len(self.stats[\"results\"]) > 0:\n            if self.stats[\"best_result\"] is None or self.stats[\"results\"][-1] < self.stats[\"best_result\"]:\n                self.stats[\"best_result\"] = self.stats[\"results\"][-1]\n                torch.save(state, self.best_path)\n            \n    def load_checkpoint(self, checkpoint=None):\n        if checkpoint is None:\n            checkpoint_list = sorted(glob.glob(f'{self.ckpt_path}/{self.name}_ep*.pth.tar'))\n            if checkpoint_list:\n                checkpoint = checkpoint_list[-1]\n            else:\n                self.log(\"[WARN] No checkpoint found, model randomly initialized.\")\n                return\n\n        print(os.getcwd())\n\n        checkpoint_dict = torch.load(checkpoint, map_location=self.device)\n\n        self.model.load_state_dict(checkpoint_dict['model'])\n        if self.resume:\n            self.stats = checkpoint_dict['stats']\n            self.epoch = checkpoint_dict['epoch'] + 1\n            if 'optimizer' in checkpoint_dict:\n                self.optimizer.load_state_dict(checkpoint_dict['optimizer'])\n            if 'lr_scheduler' in checkpoint_dict:\n                self.lr_scheduler.load_state_dict(checkpoint_dict['lr_scheduler'])\n            if 'amp' in checkpoint_dict:\n                amp.load_state_dict(checkpoint_dict['amp'])\n\n        self.log(\"[INFO] Checkpoint Loaded Successfully.\")\n\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--n-components', type=int, default=10, help='number of cca component')\n    parser.add_argument('--n-hidden', type=int, default=1024, help='number of hidden in DCCA')\n    parser.add_argument('--max-epoch', type=int, default=20, help='max epoch in training')\n    parser.add_argument('--missing-rate', type=float, default=0.0, help='view missing rate [default: 0]')\n    parser.add_argument('--normalize', action='store_true', default=False, help='whether normalize input features')\n    parser.add_argument('--dataset', type=str, default='cmumosi', help='input dataset')\n    parser.add_argument('--test_mask', type=str, default=None, help='test under same mask for fair comparision')\n    args = parser.parse_args()\n    print (args)\n\n    print (f'========= starting ============')\n    folder_acc = []\n    folder_f1 = []\n    folder_save = []\n    if args.dataset in ['cmumosi', 'cmumosei']:\n        num_folder = 1\n    elif args.dataset == 'iemocapfour':\n        num_folder = 5\n    elif args.dataset == 'iemocapsix':\n        num_folder = 5\n\n    for index in range(num_folder):\n        print (f'>>>>> Cross-validation: training on the {index+1} folder >>>>>')\n\n        print (f' >>>>> read data >>>>> ')\n        X_train, y_train, X_valid, y_valid, X_test, y_test, name_test = read_cmumosi_data(f'../baseline-cpmnet/data/{args.dataset}/{index+1}', args.normalize)\n        trainNum = len(y_train)\n        validNum = len(y_valid)\n        testNum = len(y_test)\n        print (f'train number: {trainNum};   valid number: {validNum};   test number: {testNum}')\n        X0_train, X1_train, X2_train = X_train\n        X0_valid, X1_valid, X2_valid = X_valid\n        X0_test, X1_test, X2_test = X_test\n        view0Dim = len(X0_train[0])\n        view1Dim = len(X1_train[0])\n        view2Dim = len(X2_train[0])\n        print (f'view 0: {view0Dim};  view 1: {view1Dim};  view 2: {view2Dim}')\n\n        # load random mask\n        samplenum = trainNum + validNum + testNum\n        Sn = get_sn(3, samplenum, args.missing_rate) # [samplenum, 3]\n        Sn_train = Sn[np.arange(trainNum)]\n        Sn_valid = Sn[np.arange(validNum) + trainNum]\n        Sn_test =  Sn[np.arange(testNum) + trainNum + validNum]\n        ######################################################\n        if args.test_mask != None:\n            print (f'using predefined mask!!')\n            name2mask = np.load(args.test_mask, allow_pickle=True)['name2mask'].tolist()\n            Sn_test = []\n            for name in name_test:\n                mask = name2mask[name] # (A, L, V)\n                mask = [mask[0], mask[2], mask[1]] # (A, V, L)\n                Sn_test.append(mask)\n            Sn_test = np.array(Sn_test) # [sample_num, 3]\n        else:\n            print (f'using random initialized mask!!')\n        ######################################################\n\n        # pad features in masked part\n        X0_train, X0_valid, X0_test = padmeanV1(X0_train, Sn_train[:,0], X0_valid, Sn_valid[:,0], X0_test, Sn_test[:,0])\n        X1_train, X1_valid, X1_test = padmeanV1(X1_train, Sn_train[:,1], X1_valid, Sn_valid[:,1], X1_test, Sn_test[:,1])\n        X2_train, X2_valid, X2_test = padmeanV1(X2_train, Sn_train[:,2], X2_valid, Sn_valid[:,2], X2_test, Sn_test[:,2])\n\n\n        print (f' >>>>> build model >>>>> ')\n        # model [for two view inputs]\n        model = DeepCCA([view0Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components], \n                        [view1Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components],\n                        [view2Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components]\n                        )\n        # loss\n        objective = partial(loss, outdim_size=args.n_components, use_all_singular_values=False)\n\n        # data loader\n        train_dataset = NoisyMNIST(X0_train.astype('float32'), X1_train.astype('float32'), X2_train.astype('float32'), y_train)\n        valid_dataset = NoisyMNIST(X0_valid.astype('float32'), X1_valid.astype('float32'), X2_valid.astype('float32'), y_valid)\n        test_dataset = NoisyMNIST(X0_test.astype('float32'), X1_test.astype('float32'), X2_test.astype('float32'), y_test)\n\n        batch_size = 1024\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False)\n        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n        # trainer\n        trainer = Trainer(f'DCCAE_{args.dataset}', model, objective, optimizer=optimizer, use_checkpoint='scratch')\n        trainer.train(train_loader, valid_loader, args.max_epoch)\n        train_loader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False) # ordered with y_train\n        Z1_train, Z2_train, Z3_train = trainer.evaluate(train_loader2)\n        Z1_valid, Z2_valid, Z3_valid = trainer.evaluate(valid_loader)\n        Z1_test, Z2_test, Z3_test = trainer.evaluate(test_loader)\n\n        Z_train = np.concatenate([Z1_train, Z2_train, Z3_train], axis=1) # [samplenum, dim]\n        Z_valid = np.concatenate([Z1_valid, Z2_valid, Z3_valid], axis=1) # [samplenum, dim]\n        Z_test  = np.concatenate([Z1_test, Z2_test, Z3_test], axis=1)  # [samplenum, dim]\n\n        # SVM classify\n        print (f' >>>>> training classifier >>>>> ')\n        clf = svm.LinearSVC(C=0.01, dual=False)\n        clf.fit(Z_train, y_train)\n        total_pred = clf.predict(Z_test)\n        total_label = y_test\n        f1 = f1_score(total_label, total_pred, average='weighted')\n        acc = accuracy_score(total_label, total_pred)\n\n        folder_acc.append(acc)\n        folder_f1.append(f1)\n        folder_save.append({'test_labels': total_label, 'test_preds': total_pred, 'test_hiddens': Z_test, 'test_names': name_test})\n\n        print (f'>>>>> Finish: training on the {index+1} folder >>>>>')\n\n\n    ## save results\n    suffix_name = f'{args.dataset}_dccae_mask:{args.missing_rate:.1f}'\n\n    mean_f1 = np.mean(np.array(folder_f1))\n    mean_acc = np.mean(np.array(folder_acc))\n    res_name = f'f1:{mean_f1:2.2%}_acc:{mean_acc:2.2%}'\n\n    save_root = config.MODEL_DIR\n    if not os.path.exists(save_root): os.makedirs(save_root)\n    save_path = f'{save_root}/{suffix_name}_{res_name}_{time.time()}.npz'\n    np.savez_compressed(save_path,\n                        args=np.array(args, dtype=object),\n                        folder_save=np.array(folder_save, dtype=object) # save non-structure type\n                        )\n    print (f' =========== finish =========== ')\n\n"}
{"type": "source_file", "path": "baseline-cpmnet/change_format.py", "content": "import re\nimport os\nimport copy\nimport tqdm\nimport glob\nimport json\nimport math\nimport shutil\nimport random\nimport pickle\nimport numpy as np\nimport soundfile as sf\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ndef name2feat(feature_root):\n\n    ## gain (names)\n    names = os.listdir(feature_root)\n\n    ## (names, speakers) => features\n    features = []\n    feature_dim = -1\n    for ii, name in enumerate(names):\n        feature = []\n        feature_path = os.path.join(feature_root, name) # folder or npy\n        # print (f'process name: {name}  {ii+1}/{len(names)}')\n        if os.path.isfile(feature_path): # for .npy\n            single_feature = np.load(feature_path)\n            single_feature = single_feature.squeeze() # [Dim, ] or [Time, Dim]\n            feature.append(single_feature)\n            feature_dim = max(feature_dim, single_feature.shape[-1])\n        else: ## exists dir, faces\n            facenames = os.listdir(feature_path)\n            for facename in sorted(facenames):\n                facefeat = np.load(os.path.join(feature_path, facename))\n                feature_dim = max(feature_dim, facefeat.shape[-1])\n                feature.append(facefeat)\n        # sequeeze features\n        single_feature = np.array(feature).squeeze()\n        if len(single_feature) == 0:\n            single_feature = np.zeros((feature_dim, ))\n        elif len(single_feature.shape) == 2:\n            single_feature = np.mean(single_feature, axis=0)\n        features.append(single_feature)\n\n    ## save (names, features)\n    print (f'Input feature {os.path.basename(feature_root)} ===> dim is {feature_dim}; No. sample is {len(names)}')\n    assert len(names) == len(features), f'Error: len(names) != len(features)'\n    name2feats = {}\n    for ii in range(len(names)):\n        name = names[ii]\n        if name.endswith('.npy') or name.endswith('.npz'):\n            name = name[:-4]\n        name2feats[name] = features[ii]\n\n    ## return name2feats\n    return name2feats\n\n\n\n#########################################################\n## Process for cmumosi\n#########################################################\ndef change_feat_format_cmumosi():\n\n    label_pkl = '../dataset/CMUMOSI/CMUMOSI_features_raw_2way.pkl'\n    feat_root = '../dataset/CMUMOSI/features'\n    save_root = './data/cmumosi'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4'\n    videoIDs, videoLabels, videoSpeakers, videoSentence, trainVid, valVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n\n    for (item1, item2) in [(trainVid, 'trn'), (valVid, 'val'), (testVid, 'tst')]:\n        all_A = []\n        all_V = []\n        all_L = []\n        all_labels = []\n        all_names = []\n\n        ## read all_names and all_labels\n        for vid in tqdm.tqdm(item1):\n            names = videoIDs[vid]\n            labels = videoLabels[vid]\n\n            ## change labels\n            for ii, label in enumerate(labels):\n                if label == 0:\n                    continue\n                elif label > 0:\n                    all_labels.append(1)\n                    all_names.append(names[ii])\n                else:\n                    all_labels.append(0)\n                    all_names.append(names[ii])\n\n        ## all_names -> feat\n        for name in all_names:\n            featA = name2featA[name]\n            featV = name2featV[name]\n            featL = name2featL[name]\n            all_A.append(featA)\n            all_V.append(featV)\n            all_L.append(featL)\n        all_A = np.array(all_A).astype('float32')\n        all_V = np.array(all_V).astype('float32')\n        all_L = np.array(all_L).astype('float32')\n\n        ## save path\n        save_path = f\"{save_root}/1/{item2}.npz\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.savez(save_path, name = all_names, label = all_labels, audio = all_A, video = all_V, text = all_L)\n\n\n\n\n#########################################################\n## Process for cmumosi\n#########################################################\ndef change_feat_format_cmumosei():\n\n    label_pkl = '../dataset/CMUMOSEI/CMUMOSEI_features_raw_2way.pkl'\n    feat_root = '../dataset/CMUMOSEI/features'\n    save_root = './data/cmumosei'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentence, trainVid, valVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n\n    for (item1, item2) in [(trainVid, 'trn'), (valVid, 'val'), (testVid, 'tst')]:\n        all_A = []\n        all_V = []\n        all_L = []\n        all_labels = []\n        all_names = []\n\n        ## read all_names and all_labels\n        for vid in tqdm.tqdm(item1):\n            names = videoIDs[vid]\n            labels = videoLabels[vid]\n\n            ## change labels\n            for ii, label in enumerate(labels):\n                if label == 0:\n                    continue\n                elif label > 0:\n                    all_labels.append(1)\n                    all_names.append(names[ii])\n                else:\n                    all_labels.append(0)\n                    all_names.append(names[ii])\n\n        ## all_names -> feat\n        for name in all_names:\n            featA = name2featA[name]\n            featV = name2featV[name]\n            featL = name2featL[name]\n            all_A.append(featA)\n            all_V.append(featV)\n            all_L.append(featL)\n        all_A = np.array(all_A).astype('float32')\n        all_V = np.array(all_V).astype('float32')\n        all_L = np.array(all_L).astype('float32')\n\n        ## save path\n        save_path = f\"{save_root}/1/{item2}.npz\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.savez(save_path, name = all_names, label = all_labels, audio = all_A, video = all_V, text = all_L)\n\n\n\n#########################################################\n## Process for iemocapfour\n#########################################################\ndef change_feat_format_iemocapfour():\n    label_pkl = '../dataset/IEMOCAP/IEMOCAP_features_raw_4way.pkl'\n    feat_root = '../dataset/IEMOCAP/features'\n    save_root = './data/iemocapfour'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentences, trainVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n    ## generate five folders\n    num_folder = 5\n    vids = sorted(list(trainVid | testVid))\n\n    session_to_idx = {}\n    for idx, vid in enumerate(vids):\n        session = int(vid[4]) - 1\n        if session not in session_to_idx: session_to_idx[session] = []\n        session_to_idx[session].append(idx)\n    assert len(session_to_idx) == num_folder, f'Must split into five folder'\n\n    train_test_idxs = []\n    for ii in range(num_folder): # ii in [0, 4]\n        test_idxs = session_to_idx[ii]\n        train_idxs = []\n        for jj in range(num_folder):\n            if jj != ii: train_idxs.extend(session_to_idx[jj])\n        train_test_idxs.append([train_idxs, test_idxs])\n\n    ## for each folder\n    for ii in range(len(train_test_idxs)):\n        train_idxs = train_test_idxs[ii][0]\n        test_idxs = train_test_idxs[ii][1]\n        trainVid = np.array(vids)[train_idxs]\n        testVid = np.array(vids)[test_idxs]\n\n        for (item1, item2) in [(trainVid, 'trn'), (testVid, 'val'), (testVid, 'tst')]:\n            ## change to utterance-level feats\n            all_A = []\n            all_V = []\n            all_L = []\n            all_labels = []\n            all_names = []\n            for vid in tqdm.tqdm(item1):\n                all_names.extend(videoIDs[vid])\n                all_labels.extend(videoLabels[vid])\n\n            ## all_names -> feat\n            for name in all_names:\n                featA = name2featA[name]\n                featV = name2featV[name]\n                featL = name2featL[name]\n                all_A.append(featA)\n                all_V.append(featV)\n                all_L.append(featL)\n            all_A = np.array(all_A).astype('float32')\n            all_V = np.array(all_V).astype('float32')\n            all_L = np.array(all_L).astype('float32')\n\n            save_path = f\"{save_root}/{ii+1}/{item2}.npz\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.savez(save_path, name = all_names, label = all_labels, audio = all_A, video = all_V, text = all_L)\n\n\n\n#########################################################\n## Process for iemocapfour\n#########################################################\ndef change_feat_format_iemocapsix():\n    label_pkl = '../dataset/IEMOCAP/IEMOCAP_features_raw_6way.pkl'\n    feat_root = '../dataset/IEMOCAP/features'\n    save_root = './data/iemocapsix'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentences, trainVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n    ## generate five folders\n    num_folder = 5\n    vids = sorted(list(trainVid | testVid))\n\n    session_to_idx = {}\n    for idx, vid in enumerate(vids):\n        session = int(vid[4]) - 1\n        if session not in session_to_idx: session_to_idx[session] = []\n        session_to_idx[session].append(idx)\n    assert len(session_to_idx) == num_folder, f'Must split into five folder'\n\n    train_test_idxs = []\n    for ii in range(num_folder): # ii in [0, 4]\n        test_idxs = session_to_idx[ii]\n        train_idxs = []\n        for jj in range(num_folder):\n            if jj != ii: train_idxs.extend(session_to_idx[jj])\n        train_test_idxs.append([train_idxs, test_idxs])\n\n    ## for each folder\n    for ii in range(len(train_test_idxs)):\n        train_idxs = train_test_idxs[ii][0]\n        test_idxs = train_test_idxs[ii][1]\n        trainVid = np.array(vids)[train_idxs]\n        testVid = np.array(vids)[test_idxs]\n\n        for (item1, item2) in [(trainVid, 'trn'), (testVid, 'val'), (testVid, 'tst')]:\n            ## change to utterance-level feats\n            all_A = []\n            all_V = []\n            all_L = []\n            all_labels = []\n            all_names = []\n            for vid in tqdm.tqdm(item1):\n                all_names.extend(videoIDs[vid])\n                all_labels.extend(videoLabels[vid])\n\n            ## all_names -> feat\n            for name in all_names:\n                featA = name2featA[name]\n                featV = name2featV[name]\n                featL = name2featL[name]\n                all_A.append(featA)\n                all_V.append(featV)\n                all_L.append(featL)\n            all_A = np.array(all_A).astype('float32')\n            all_V = np.array(all_V).astype('float32')\n            all_L = np.array(all_L).astype('float32')\n\n            save_path = f\"{save_root}/{ii+1}/{item2}.npz\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.savez(save_path, name = all_names, label = all_labels, audio = all_A, video = all_V, text = all_L)\n\n\nif __name__ == '__main__':\n    import fire\n    fire.Fire()"}
{"type": "source_file", "path": "baseline-mmin/auto/combine_results.py", "content": "import os\nimport numpy as np\n\ndef read_results(file):\n    ans = []\n    lines = open(file).readlines()\n    for line in lines:\n        if not line.startswith('0'): continue\n        ans.append(list(map(lambda x: float(x), line.strip().split('\\t'))))\n           \n    data = np.array(ans).astype(np.float)\n    assert data.shape[0] == 24\n    val_data = data[0: 10]\n    tst_data = data[12: 22]\n    return val_data, tst_data\n\ndef combine(result1, result2):\n    result = result1 * (result1>=result2) + result2 * (result1<result2)\n    return result\n\ndef combine_file(file1, file2, output):\n    val_data1, tst_data1 = read_results(file1)\n    val_data2, tst_data2 = read_results(file2)\n    val_data = combine(val_data1, val_data2)\n    val_mean = np.expand_dims(np.mean(val_data, axis=0), 0)\n    val_std = np.expand_dims(np.std(val_data, axis=0), 0)\n    val_data = np.vstack([val_data, val_mean, val_std])\n    tst_data = combine(tst_data1, tst_data2)\n    tst_mean = np.expand_dims(np.mean(tst_data, axis=0), 0)\n    tst_std = np.expand_dims(np.std(tst_data, axis=0), 0)\n    tst_data = np.vstack([tst_data, tst_mean, tst_std])\n    f = open(output, 'w')\n    f.write(output.split('/')[-1] + '\\n')\n    f.write('val:\\n')\n    for d in val_data:\n        line = '\\t'.join(list(map(lambda x:'{:.4f}'.format(x), d))) + '\\n'\n        f.write(line)\n\n    val_mean = val_mean[0]\n    val_std = val_std[0]\n    acc = '{:.4f}±{:.4f}'.format(val_mean[0], val_std[0])\n    uar = '{:.4f}±{:.4f}'.format(val_mean[1], val_std[1])\n    f1 = '{:.4f}±{:.4f}'.format(val_mean[2], val_std[2])\n    f.write('VAL result:\\nacc %s uar %s f1 %s\\n\\n' % (acc, uar, f1))\n\n    \n    f.write('tst:\\n')\n    for d in tst_data:\n        line = '\\t'.join(list(map(lambda x:'{:.4f}'.format(x), d))) + '\\n'\n        f.write(line)\n    \n    tst_mean = tst_mean[0]\n    tst_std = tst_std[0]\n    acc = '{:.4f}±{:.4f}'.format(tst_mean[0], tst_std[0])\n    uar = '{:.4f}±{:.4f}'.format(tst_mean[1], tst_std[1])\n    f1 = '{:.4f}±{:.4f}'.format(tst_mean[2], tst_std[2])\n    f.write('TEST result:\\nacc %s uar %s f1 %s\\n' % (acc, uar, f1))\n    \n# print(val_data)\n# print(tst_data)\nroot = 'today_tasks/results'\nsave_root = 'today_tasks/results_combine'\n# run_idx1 = 'A_ts_Adnn512,256,128_lossBCE_kd1.0_temp2.0_ce0.0_mmd0.0_run1'\n# run_idx2 = 'A_ts_Adnn512,256,128_lossBCE_kd1.0_temp2.0_ce0.0_mmd0.0_run2'\n# out = 'A_ts_Adnn512,256,128_lossBCE_kd1.0_temp2.0_ce0.0_mmd0.0'\n# combine_file(os.path.join(root, run_idx1), os.path.join(root, run_idx2), os.path.join(save_root, out))\ntotal_file = os.listdir(root)\nname_set = set()\nfor file in total_file:\n    name = '_'.join(file.split('_')[:-1])\n    name_set.add(name)\n\nfor name in name_set:\n    run_idx1 = name + '_run1'\n    run_idx2 = name + '_run2'\n    out = name\n    combine_file(os.path.join(root, run_idx1), os.path.join(root, run_idx2), os.path.join(save_root, out))"}
{"type": "source_file", "path": "baseline-mmin/data/iemocapsix_miss_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom data.base_dataset import BaseDataset\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n## copy from cpm-net\ndef random_mask(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return: Sn [alldata_len, view_num]\n    \"\"\"\n    # print (f'==== generate random mask ====')\n    one_rate = 1-missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        # print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        # print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        # print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\n\nclass IEMOCAPSIXMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.mask_rate = opt.mask_rate\n        self.dataset = opt.dataset_mode.split('_')[0]\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAPSIX_config.json')))\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        # make missing index\n        samplenum = len(self.label)\n        self.maskmatrix = random_mask(3, samplenum, self.mask_rate) # [samplenum, view_num]\n\n        self.manual_collate_fn = False\n\n    def __getitem__(self, index):\n        \n        maskseq = self.maskmatrix[index] # (3, )\n        assert np.sum(maskseq) >= 0.999999 # at least one view is not masked\n        missing_index = torch.LongTensor(maskseq) # (3, ) [1,1,1]; maskrate=1=>[0,0,0]\n\n        int2name = self.int2name[index]\n        if self.dataset in ['cmumosi']:\n            label = torch.tensor(self.label[index]).float()\n        elif self.dataset in ['iemocapfour', 'iemocapsix']:\n            label = torch.tensor(self.label[index]).long()\n        A_feat = torch.tensor(self.all_A[index]).float()\n        V_feat = torch.tensor(self.all_V[index]).float()\n        L_feat = torch.tensor(self.all_L[index]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index\n        }\n    \n    def __len__(self):\n        return len(self.label)\n"}
{"type": "source_file", "path": "baseline-cpmnet/util/classfiy.py", "content": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef convert_to_one_hot(y, C):\n    return np.eye(C)[y.reshape(-1)]\n\n\ndef vote(lsd1, lsd2, label, n=1):\n    \"\"\"Sometimes the prediction accuracy will be higher in this way.\n    :param lsd1: train set's latent space data\n    :param lsd2: test set's latent space data\n    :param label: label of train set\n    :param n: Similar to K in k-nearest neighbors algorithm\n    :return: Predicted label\n    \"\"\"\n    F_h_h = np.dot(lsd2, np.transpose(lsd1))\n    gt_list = []\n    label = label.reshape(len(label), 1)\n    for num in range(n):\n        F_h_h_argmax = np.argmax(F_h_h, axis=1)\n        F_h_h_onehot = convert_to_one_hot(F_h_h_argmax, len(label))\n        F_h_h = F_h_h - np.multiply(F_h_h, F_h_h_onehot)\n        gt_list.append(np.dot(F_h_h_onehot, label))\n    gt_ = np.array(gt_list).transpose(2, 1, 0)[0].astype(np.int64)\n    count_list = []\n    count_list.append([np.argmax(np.bincount(gt_[i])) for i in range(lsd2.shape[0])])\n    gt_pre = np.array(count_list)\n    return gt_pre.transpose()\n\ndef ave(lsd1, lsd2, label):\n    \"\"\"In most cases, this method is used to predict the highest accuracy.\n    :param lsd1: train set's latent space data\n    :param lsd2: test set's latent space data\n    :param label: label of train set\n    :return: Predicted label\n    \"\"\"\n    F_h_h = np.dot(lsd2, np.transpose(lsd1)) # 每个测试样本和所有训练样本的距离\n    label = label.reshape(len(label), 1) - 1\n    enc = OneHotEncoder()\n    a = enc.fit_transform(label)\n    label_onehot = a.toarray()               # 每个测试样本的onehot标签\n    label_num = np.sum(label_onehot, axis=0) # 没有样本数量【训练集】\n    F_h_h_sum = np.dot(F_h_h, label_onehot)\n    F_h_h_mean = F_h_h_sum / label_num       # 每个类的中性距离\n    label_pre = np.argmax(F_h_h_mean, axis=1) + 1\n    return label_pre                         # 计算预测标签结果\n"}
{"type": "source_file", "path": "baseline-mmin/data/cmumosi_multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass CMUMOSIMultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'CMUMOSI_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        # self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = False ## for utterance level features\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        ###############\n        # label = torch.tensor(self.label[index])\n        label = torch.tensor(self.label[index]).float()\n        ###############\n        # process A_feat\n        A_feat = torch.FloatTensor(self.all_A[index])\n        # process V_feat \n        V_feat = torch.FloatTensor(self.all_V[index])\n        # proveee L_feat\n        L_feat = torch.FloatTensor(self.all_L[index])\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert_large\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MSPMultimodalDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/data/melspec_dataset.py", "content": "import os\nimport json\nimport torch\nimport numpy as np\nimport h5py\nimport PIL\nfrom torchvision.transforms import transforms\n\nfrom data.base_dataset import BaseDataset\n\n\nclass MelspecDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--spec_aug', action='store_true', help='whether to do specaug')\n        parser.add_argument('--time_mask', type=float, default=0.1, help='specaug parameter time mask')\n        parser.add_argument('--freq_mask', type=float, default=0.1, help='specaug parameter freq mask')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAP_config.json')))\n        \n        # load feature\n        self.all_A = h5py.File(os.path.join(config['feature_root'], 'A', 'melspec.h5'), 'r')\n\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n\n        # loading setting\n        self.spec_aug = opt.spec_aug\n        self.time_mask = opt.time_mask\n        self.freq_mask = opt.freq_mask\n        \n        self.manual_collate_fn = False\n\n    def get_transform(self):\n        if self.set_name == 'trn':\n            _transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(0.5),\n                transforms.ToTensor()\n            ])\n        else:\n            _transform = transforms.Compose([\n                transforms.ToTensor()\n            ])\n        \n        return _transform\n\n    def process_melspec(self, melspec):\n        if not hasattr(self, \"transform\"):\n            self.transform = self.get_transform()\n        \n        image = melspec\n        time_dim, base_dim = image.shape[1], image.shape[0]\n        crop = np.random.randint(0, time_dim - base_dim)\n        image = image[:, crop:crop + base_dim, ...]\n\n        if self.set_name == 'trn' and self.spec_aug:\n            freq_mask_begin = int(np.random.uniform(0, 1 - self.freq_mask) * base_dim)\n            image[freq_mask_begin:freq_mask_begin + int(self.freq_mask * base_dim), ...] = 0\n            time_mask_begin = int(np.random.uniform(0, 1 - self.time_mask) * base_dim)\n            image[:, time_mask_begin:time_mask_begin + int(self.time_mask * base_dim), ...] = 0\n\n        image = PIL.Image.fromarray(image[...,0], mode='L')\n        image = self.transform(image).div_(255)\n        return image.float()\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index][0].decode()\n        label = torch.tensor(self.label[index])\n        # process A_feat\n        A_feat = self.all_A[int2name]\n        A_feat = self.process_melspec(A_feat)\n        return {\n            'A_feat': A_feat, \n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        spec_aug = True\n        time_mask = 0.1\n        freq_mask = 0.1\n    \n    opt = test()\n    a = MelspecDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/models/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "baseline-mmin/data/iemocapfour_multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass IEMOCAPFOURMultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        self.dataset = opt.dataset_mode.split('_')[0]\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAPFOUR_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        # self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = False ## for utterance level features\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        ###############\n        if self.dataset in ['cmumosi']:\n            label = torch.tensor(self.label[index]).float()\n        elif self.dataset in ['iemocapfour', 'iemocapsix']:\n            label = torch.tensor(self.label[index]).long()\n        ###############\n        # process A_feat\n        A_feat = torch.FloatTensor(self.all_A[index])\n        # process V_feat \n        V_feat = torch.FloatTensor(self.all_V[index])\n        # proveee L_feat\n        L_feat = torch.FloatTensor(self.all_L[index])\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n"}
{"type": "source_file", "path": "baseline-mmin/models/utils/load_pretrained.py", "content": "import os\nimport json\nfrom .config import OptConfig\n\ndef load_from_opt_record(file_path):\n    opt_content = json.load(open(file_path, 'r'))\n    opt = OptConfig()\n    opt.load(opt_content)\n    return opt\n\ndef load_pretrained_model(model_class, checkpoints_dir, cv, gpu_ids):\n    path = os.path.join(checkpoints_dir, str(cv))\n    config_path = os.path.join(checkpoints_dir, 'train_opt.conf')\n    config = load_from_opt_record(config_path)\n    config.isTrain = False                             # teacher model should be in test mode\n    config.gpu_ids = gpu_ids                       # set gpu to the same\n    model = model_class(config)\n    model.cuda()\n    model.load_networks_cv(path)\n    model.eval()\n    return model\n"}
{"type": "source_file", "path": "baseline-mmin/models/uttf_dataaug_model.py", "content": "\nimport torch\nimport os\nimport json\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\n\n\nclass UttFDataAugModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--modality', type=str, help='which modality to use for model')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE']\n        self.modality = opt.modality\n        self.model_names = ['C']\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        cls_input_size = opt.embd_size_a * int(\"A\" in self.modality) + \\\n                         opt.embd_size_v * int(\"V\" in self.modality) + \\\n                         opt.embd_size_l * int(\"L\" in self.modality)\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n        \n        # acoustic model\n        if 'A' in self.modality:\n            self.model_names.append('A')\n            self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n            \n        # lexical model\n        if 'L' in self.modality:\n            self.model_names.append('L')\n            self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n            \n        # visual model\n        if 'V' in self.modality:\n            self.model_names.append('V')\n            self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, opt.embd_method_v)\n            \n        if self.isTrain:\n            self.criterion_ce = torch.nn.CrossEntropyLoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=opt.weight_decay)\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device)\n        lexical = input['L_feat'].float().to(self.device)\n        visual = input['V_feat'].float().to(self.device)\n        if self.isTrain:\n            self.label = input['label'].to(self.device)\n            self.missing_index = input['missing_index'].long().to(self.device)\n            # A modality\n            if 'A' in self.modality:\n                self.A_miss_index = self.missing_index[:, 0].unsqueeze(1).unsqueeze(2)\n                self.A_miss = acoustic * self.A_miss_index\n            # L modality\n            if 'L' in self.modality:\n                self.L_miss_index = self.missing_index[:, 2].unsqueeze(1).unsqueeze(2)\n                self.L_miss = lexical * self.L_miss_index\n            # V modality\n            if 'V' in self.modality:\n                self.V_miss_index = self.missing_index[:, 1].unsqueeze(1).unsqueeze(2)\n                self.V_miss = visual * self.V_miss_index\n        else:\n            self.A_miss = acoustic\n            self.V_miss = visual\n            self.L_miss = lexical\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        final_embd = []\n        if 'A' in self.modality:\n            self.feat_A_miss = self.netA(self.A_miss)\n            final_embd.append(self.feat_A_miss)\n\n        if 'L' in self.modality:\n            self.feat_L_miss = self.netL(self.L_miss)\n            final_embd.append(self.feat_L_miss)\n\n        if 'V' in self.modality:\n            self.feat_V_miss = self.netV(self.V_miss)\n            final_embd.append(self.feat_V_miss)\n        \n        # if 'A' in self.modality:\n        #     self.feat_A = self.netA(self.acoustic)\n        #     final_embd.append(self.feat_A)\n\n        # if 'L' in self.modality:\n        #     self.feat_L = self.netL(self.lexical)\n        #     final_embd.append(self.feat_L)\n        \n        # if 'V' in self.modality:\n        #     self.feat_V = self.netV(self.visual)\n        #     final_embd.append(self.feat_V)\n        \n        # get model outputs\n        self.feat = torch.cat(final_embd, dim=-1)\n        self.logits, self.ef_fusion_feat = self.netC(self.feat)\n        self.pred = F.softmax(self.logits, dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.criterion_ce(self.logits, self.label)\n        loss = self.loss_CE\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5.0) # 0.1\n\n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/data/msp_miss_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass MSPMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'MSP_config.json')))\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        # make missing index\n        if set_name != 'trn':           # val && tst\n            self.missing_index = torch.tensor([\n                [1,0,0], # AZZ\n                [0,1,0], # ZVZ\n                [0,0,1], # ZZL\n                [1,1,0], # AVZ\n                [1,0,1], # AZL\n                [0,1,1], # ZVL\n                # [1,1,1]  # AVL\n            ] * len(self.label)).long()\n            self.miss_type = ['azz', 'zvz', 'zzl', 'avz', 'azl', 'zvl'] * len(self.label)\n        else:                           # trn\n            self.missing_index = [\n                [1,0,0], # AZZ\n                [0,1,0], # ZVZ\n                [0,0,1], # ZZL\n                [1,1,0], # AVZ\n                [1,0,1], # AZL\n                [0,1,1], # ZVL\n                # [1,1,1]  # AVL\n            ]\n            self.miss_type = ['azz', 'zvz', 'zzl', 'avz', 'azl', 'zvl']\n        self.manual_collate_fn = False\n\n    def __getitem__(self, index):\n        if self.set_name != 'trn':\n            feat_idx = index // 6         # totally 6 missing types\n            missing_index = self.missing_index[index]         \n            miss_type = self.miss_type[index]\n        else:\n            feat_idx = index\n            missing_index = torch.tensor(random.choice(self.missing_index)).long()\n            miss_type = random.choice(self.miss_type)\n        \n        int2name = self.int2name[feat_idx]\n        label = torch.tensor(self.label[feat_idx])\n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[feat_idx]).float()\n        # process V_feat \n        V_feat = torch.from_numpy(self.all_V[feat_idx]).float()\n        # proveee L_feat\n        L_feat = torch.from_numpy(self.all_L[feat_idx]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index,\n            'miss_type': miss_type\n        } if self.set_name == 'trn' else{\n            'A_feat': A_feat * missing_index[0], \n            'V_feat': V_feat * missing_index[1],\n            'L_feat': L_feat * missing_index[2],\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index,\n            'miss_type': miss_type\n        }\n    \n    def __len__(self):\n        return len(self.missing_index) if self.set_name != 'trn' else len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MSPMissDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label', 'miss_type']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/models/base_model.py", "content": "import os\nimport torch\nfrom collections import OrderedDict\nfrom abc import ABC, abstractmethod\nfrom .networks import tools\n\n\nclass BaseModel(ABC):\n    \"\"\"This class is an abstract base class (ABC) for models.\n    To create a subclass, you need to implement the following five functions:\n        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n        -- <set_input>:                     unpack data from dataset and apply preprocessing.\n        -- <forward>:                       produce intermediate results.\n        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n    \"\"\"\n\n    def __init__(self, opt):\n        \"\"\"Initialize the BaseModel class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\n        When creating your custom class, you need to implement your own initialization.\n        In this fucntion, you should first call <BaseModel.__init__(self, opt)>\n        Then, you need to define four lists:\n            -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n            -- self.model_names (str list):         specify the images that you want to display and save.\n            -- self.visual_names (str list):        define networks used in our training.\n            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n        \"\"\"\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')  # get device name: CPU or GPU\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir\n        if opt.cuda_benchmark: # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.\n            torch.backends.cudnn.benchmark = True\n        self.loss_names = []\n        self.model_names = []\n        self.optimizers = []\n        self.metric = 0  # used for learning rate policy 'plateau'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        \"\"\"Add new model-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"\n        return parser\n\n    @abstractmethod\n    def set_input(self, input):\n        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): includes the data itself and its metadata information.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        pass\n\n    @abstractmethod\n    def optimize_parameters(self):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        pass\n\n    def setup(self, opt):\n        \"\"\"Load and print networks; create schedulers\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        if self.isTrain:\n            self.schedulers = [tools.get_scheduler(optimizer, opt) for optimizer in self.optimizers] # Adam\n            for name in self.model_names: # mmin: ['A', 'V', 'L', 'C', 'AE', 'AE_cycle'] ## fully: ['C', 'A', 'L', 'V']\n                net = getattr(self, 'net' + name)\n                net = tools.init_net(net, opt.init_type, opt.init_gain, opt.gpu_ids)\n                setattr(self, 'net' + name, net)\n        else:\n            self.eval()\n\n        self.print_networks(opt.verbose)\n        self.post_process()\n\n    def cuda(self):\n        assert(torch.cuda.is_available())\n        for name in self.model_names:\n            net = getattr(self, 'net' + name)\n            net.to(self.gpu_ids[0])\n            net = torch.nn.DataParallel(net, self.gpu_ids)  # multi-GPUs\n\n    def eval(self):\n        \"\"\"Make models eval mode during test time\"\"\"\n        self.isTrain = False\n        for name in self.model_names: # ['A', 'V', 'L', 'C']\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n    \n    def train(self):\n        \"\"\"Make models back to train mode after test time\"\"\"\n        self.isTrain = True\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.train()\n\n    def test(self):\n        \"\"\"Forward function used in test time.\n\n        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\n        It also calls <compute_visuals> to produce additional visualization results\n        \"\"\"\n        with torch.no_grad():\n            self.forward()\n\n    def compute_visuals(self):\n        \"\"\"Calculate additional output images for visdom and HTML visualization\"\"\"\n        pass\n\n    def update_learning_rate(self, logger):\n        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n        for scheduler in self.schedulers:\n            if self.opt.lr_policy == 'plateau':\n                scheduler.step(self.metric)\n            else:\n                scheduler.step()\n\n        lr = self.optimizers[0].param_groups[0]['lr']\n        # print('learning rate = %.7f' % lr)\n        logger.info('learning rate = %.7f' % lr)\n\n    def get_current_visuals(self):\n        \"\"\"Return visualization images. train.py will display these images with visdom, and save the images to a HTML\"\"\"\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    def get_current_losses(self):\n        \"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"\n        errors_ret = OrderedDict()\n        for name in self.loss_names: # ['CE', 'mse', 'cycle']\n            if isinstance(name, str):\n                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\n        return errors_ret\n\n    def save_networks(self, epoch):\n        \"\"\"Save all the networks to the disk.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n        \"\"\"\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def load_networks(self, epoch):\n        \"\"\"Load all the networks from the disk.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n        \"\"\"\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                state_dict = torch.load(load_path, map_location=self.device)\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                net.load_state_dict(state_dict)\n    \n    def load_networks_cv(self, folder_path):\n        \"\"\"Load all the networks from cv folder.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n        \"\"\"\n        checkpoints = list(filter(lambda x: x.endswith('.pth'), os.listdir(folder_path)))\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = list(filter(lambda x: x.split('.')[0].endswith('net_'+name), checkpoints))\n                assert len(load_filename) == 1, 'In folder: {}, Exists file {}'.format(folder_path, load_filename)\n                load_filename = load_filename[0]\n                load_path = os.path.join(folder_path, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                state_dict = torch.load(load_path, map_location=self.device)\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                net.load_state_dict(state_dict)\n\n    def print_networks(self, verbose):\n        \"\"\"Print the total number of parameters in the network and (if verbose) network architecture\n\n        Parameters:\n            verbose (bool) -- if verbose: print the network architecture\n        \"\"\"\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    def set_requires_grad(self, nets, requires_grad=False):\n        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n        Parameters:\n            nets (network list)   -- a list of networks\n            requires_grad (bool)  -- whether the networks require gradients or not\n        \"\"\"\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n\n    def post_process(self):\n        pass"}
{"type": "source_file", "path": "baseline-mmin/models/networks/classifier.py", "content": "import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, fc1_size, output_size, dropout_rate):\n        super(LSTMClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.fc1_size = fc1_size\n        self.output_size = output_size\n        self.dropout_rate = dropout_rate\n\n        # defining modules - two layer bidirectional LSTM with layer norm in between\n        self.rnn1 = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n        self.rnn2 = nn.LSTM(2 * hidden_size, hidden_size, bidirectional=True, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size * 4, fc1_size)\n        self.fc2 = nn.Linear(fc1_size, output_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_rate)\n        self.layer_norm = nn.LayerNorm((hidden_size * 2, ))\n        self.bn = nn.BatchNorm1d(hidden_size * 4)\n\n    def extract_features(self, sequence, lengths, rnn1, rnn2, layer_norm):\n        packed_sequence = pack_padded_sequence(sequence, lengths, batch_first=True, enforce_sorted=False)\n        packed_h1, (final_h1, _) = rnn1(packed_sequence)\n        padded_h1, _ = pad_packed_sequence(packed_h1, batch_first=True)\n        normed_h1 = layer_norm(padded_h1)\n        packed_normed_h1 = pack_padded_sequence(normed_h1, lengths, batch_first=True, enforce_sorted=False)\n        _, (final_h2, _) = rnn2(packed_normed_h1)\n        return final_h1, final_h2\n\n    def rnn_flow(self, x, lengths):\n        batch_size = lengths.size(0)\n        h1, h2 = self.extract_features(x, lengths, self.rnn1, self.rnn2, self.layer_norm)\n        h = torch.cat((h1, h2), dim=2).permute(1, 0, 2).contiguous().view(batch_size, -1)\n        return self.bn(h)\n\n    def mask2length(self, mask):\n        ''' mask [batch_size, seq_length, feat_size]\n        '''\n        _mask = torch.mean(mask, dim=-1).long() # [batch_size, seq_len]\n        length = torch.sum(_mask, dim=-1)       # [batch_size,]\n        return length \n\n    def forward(self, x, mask):\n        lengths = self.mask2length(mask)\n        h = self.rnn_flow(x, lengths)\n        h = self.fc1(h)\n        h = self.dropout(h)\n        h = self.relu(h)\n        o = self.fc2(h)\n        return o, h\n\nclass SimpleClassifier(nn.Module):\n    ''' Linear classifier, use embedding as input\n        Linear approximation, should append with softmax\n    '''\n    def __init__(self, embd_size, output_dim, dropout):\n        super(SimpleClassifier, self).__init__()\n        self.dropout = dropout\n        self.C = nn.Linear(embd_size, output_dim)\n        self.dropout_op = nn.Dropout(dropout)\n\n    def forward(self, x):\n        if self.dropout > 0:\n            x = self.dropout_op(x)\n        return self.C(x)\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n    \nclass FcClassifier(nn.Module):\n    def __init__(self, input_dim, layers, output_dim, dropout=0.3, use_bn=False):\n        ''' Fully Connect classifier\n            Parameters:\n            --------------------------\n            input_dim: input feature dim\n            layers: [x1, x2, x3] will create 3 layers with x1, x2, x3 hidden nodes respectively.\n            output_dim: output feature dim\n            activation: activation function\n            dropout: dropout rate\n        '''\n        super().__init__()\n        self.all_layers = []\n        for i in range(0, len(layers)):\n            self.all_layers.append(nn.Linear(input_dim, layers[i]))\n            self.all_layers.append(nn.ReLU())\n            if use_bn:\n                self.all_layers.append(nn.BatchNorm1d(layers[i]))\n            if dropout > 0:\n                self.all_layers.append(nn.Dropout(dropout))\n            input_dim = layers[i]\n        \n        if len(layers) == 0:\n            layers.append(input_dim)\n            self.all_layers.append(Identity())\n        \n        self.fc_out = nn.Linear(layers[-1], output_dim)\n        self.module = nn.Sequential(*self.all_layers)\n    \n    def forward(self, x):\n        feat = self.module(x)\n        out = self.fc_out(feat)\n        return out, feat\n\nclass EF_model_AL(nn.Module):\n    def __init__(self, fc_classifier, lstm_classifier, out_dim_a, out_dim_v, fusion_size, num_class, dropout):\n        ''' Early fusion model classifier\n            Parameters:\n            --------------------------\n            fc_classifier: acoustic classifier\n            lstm_classifier: lexical classifier\n            out_dim_a: fc_classifier output dim\n            out_dim_v: lstm_classifier output dim\n            fusion_size: output_size for fusion model\n            num_class: class number\n            dropout: dropout rate\n        '''\n        super(EF_model_AL, self).__init__()\n        self.fc_classifier = fc_classifier\n        self.lstm_classifier = lstm_classifier\n        self.out_dim = out_dim_a + out_dim_v\n        self.dropout = nn.Dropout(dropout)\n        self.num_class = num_class\n        self.fusion_size = fusion_size\n        # self.out = nn.Sequential(\n        #     nn.Linear(self.out_dim, self.fusion_size),\n        #     nn.ReLU(),\n        #     nn.Linear(self.fusion_size, self.num_class),\n        # )\n        self.out1 = nn.Linear(self.out_dim, self.fusion_size)\n        self.relu = nn.ReLU()\n        self.out2 = nn.Linear(self.fusion_size, self.num_class)\n\n    def forward(self, A_feat, L_feat, L_mask):\n        _, A_out = self.fc_classifier(A_feat)\n        _, L_out = self.lstm_classifier(L_feat, L_mask)\n        feat = torch.cat([A_out, L_out], dim=-1)\n        feat = self.dropout(feat)\n        feat = self.relu(self.out1(feat))\n        out = self.out2(self.dropout(feat))\n        return out, feat\n    \n\nclass MaxPoolFc(nn.Module):\n    def __init__(self, hidden_size, num_class=4):\n        super(MaxPoolFc, self).__init__()\n        self.hidden_size = hidden_size\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, num_class),\n            nn.ReLU()\n        )\n    \n    def forward(self, x):\n        ''' x shape => [batch_size, seq_len, hidden_size]\n        '''\n        batch_size, seq_len, hidden_size = x.size()\n        x = x.view(batch_size, hidden_size, seq_len)\n        # print(x.size())\n        out = torch.max_pool1d(x, kernel_size=seq_len)\n        out = out.squeeze()\n        out = self.fc(out)\n        \n        return out\n\nif __name__ == '__main__':\n    a = FcClassifier(256, [128], 4)\n    print(a)"}
{"type": "source_file", "path": "baseline-mmin/models/networks/tools.py", "content": "import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport numpy as np\nimport functools\nfrom torch.optim import lr_scheduler\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\n\ndef get_norm_layer(norm_type='instance'):\n    \"\"\"Return a normalization layer\n\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    \"\"\"\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif norm_type == 'layer':\n        norm_layer = functools.partial(nn.LayerNorm, eps=1e-6, elementwise_affine=True)\n    elif norm_type == 'none':\n        norm_layer = lambda x: Identity()\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    \"\"\"Return a learning rate scheduler\n\n    Parameters:\n        optimizer          -- the optimizer of the network\n        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n\n    For 'linear', we keep the same learning rate for the first <opt.niter> epochs\n    and linearly decay the rate to zero over the next <opt.niter_decay> epochs.\n    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    \"\"\"\n    if opt.lr_policy == 'linear':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    elif opt.lr_policy == 'cosine':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type='normal', init_gain=0.02):\n    \"\"\"Initialize network weights.\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    \"\"\"\n    def init_func(m):  # define the initialization function\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)  # apply the initialization function <init_func>\n\n\ndef init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n    Parameters:\n        net (network)      -- the network to be initialized\n        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Return an initialized network.\n    \"\"\"\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n    init_weights(net, init_type, init_gain=init_gain)\n    return net\n\n\ndef diagnose_network(net, name='network'):\n    \"\"\"Calculate and print the mean of average absolute(gradients)\n\n    Parameters:\n        net (torch network) -- Torch network\n        name (str) -- the name of the network\n    \"\"\"\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\nclass MidLayerFeatureExtractor(object):\n    def __init__(self, layer):\n        self.layer = layer\n        self.feature = None\n        self.layer.register_forward_hook(self.hook)\n        self.device = None\n    \n    def hook(self, module, input, output):\n        # default tensor on cpu\n        self.is_empty = True\n        self.feature = output.clone()\n        self.is_empty = False\n        # self.is_empty = True\n        # self.feature = output\n        # self.is_empty = False\n    \n    def extract(self):\n        assert not self.is_empty, 'Synic Error in MidLayerFeatureExtractor, \\\n                this may caused by calling extract method before the hooked module has execute forward method'\n        return self.feature\n    \nclass MultiLayerFeatureExtractor(object):\n    def __init__(self, net, layers):\n        '''\n        Parameter:\n        -----------------\n        net: torch.nn.Modules\n        layers: str, something like \"C.fc[0], module[1]\"\n                which will get mid layer features in net.C.fc[0] and net.module[1] respectively\n        '''\n        self.net = net\n        self.layer_names = layers.strip().split(',')\n        self.layers = [self.str2layer(layer_name) for layer_name in self.layer_names]\n        self.extractors = [MidLayerFeatureExtractor(layer) for layer in self.layers]\n\n    def str2layer(self, name):\n        modules = name.split('.')\n        layer = self.net\n        for module in modules:\n            if '[' and ']' in module:\n                sequential_name = module[:module.find('[')]\n                target_module_num = int(module[module.find('[')+1:module.find(']')])\n                layer = getattr(layer, sequential_name)\n                layer = layer[target_module_num]\n            else:\n                layer = getattr(layer, module)\n        \n        return layer\n    \n    def extract(self):\n        ans = [extractor.extract() for extractor in self.extractors]\n        return ans\n   \n        \n    \n\n    \n\n\n\n"}
{"type": "source_file", "path": "baseline-mmin/data/cmumosi_miss_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom data.base_dataset import BaseDataset\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n## copy from cpm-net\ndef random_mask(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return: Sn [alldata_len, view_num]\n    \"\"\"\n    # print (f'==== generate random mask ====')\n    one_rate = 1 - missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        # print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        # print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        # print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\n\nclass CMUMOSIMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.mask_rate = opt.mask_rate\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'CMUMOSI_config.json')))\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        # make missing index\n        samplenum = len(self.label)\n        self.maskmatrix = random_mask(3, samplenum, self.mask_rate) # [samplenum, view_num]\n\n        self.manual_collate_fn = False\n\n    def __getitem__(self, index):\n        \n        maskseq = self.maskmatrix[index] # (3, )\n        missing_index = torch.LongTensor(maskseq) # (3, ) [1,1,1]; maskrate=1=>[0,0,0]\n\n        int2name = self.int2name[index]\n        ###############\n        # label = torch.tensor(self.label[index])\n        label = torch.tensor(self.label[index]).float()\n        ###############\n        A_feat = torch.tensor(self.all_A[index]).float()\n        V_feat = torch.tensor(self.all_V[index]).float()\n        L_feat = torch.tensor(self.all_L[index]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n"}
{"type": "source_file", "path": "baseline-mmin/models/mmin_old_model.py", "content": "import torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import ResidualAE\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINOldModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--n_blocks', type=int, default=3, help='number of AE blocks')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        parser.add_argument('--cycle_weight', type=float, default=1.0, help='weight of cycle loss')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE', 'mse', 'cycle']\n        self.model_names = ['A', 'V', 'L', 'C', 'AE', 'AE_cycle']\n        \n        # acoustic model\n        self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n        # lexical model\n        self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n        # visual model\n        self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, opt.embd_method_v)\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        self.netAE = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        self.netAE_cycle = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        cls_input_size = AE_layers[-1] * opt.n_blocks\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.miss2_rate = 0.5\n\n        if self.isTrain:\n            self.load_pretrained_encoder(opt)\n            self.criterion_ce = torch.nn.CrossEntropyLoss()\n            self.criterion_mse = torch.nn.MSELoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n            self.cycle_weight = opt.cycle_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def load_pretrained_encoder(self, opt):\n        print('Init parameter from {}'.format(opt.pretrained_path))\n        pretrained_path = os.path.join(opt.pretrained_path, str(opt.cvNo))\n        pretrained_config_path = os.path.join(opt.pretrained_path, 'train_opt.conf')\n        pretrained_config = self.load_from_opt_record(pretrained_config_path)\n        pretrained_config.isTrain = False                             # teacher model should be in test mode\n        pretrained_config.gpu_ids = opt.gpu_ids                       # set gpu to the same\n        self.pretrained_encoder = UttFusionModel(pretrained_config)\n        self.pretrained_encoder.load_networks_cv(pretrained_path)\n        self.pretrained_encoder.cuda()\n        self.pretrained_encoder.eval()\n    \n    def post_process(self):\n        # called after model.setup()\n        def transform_key_for_parallel(state_dict):\n            return OrderedDict([('module.'+key, value) for key, value in state_dict.items()])\n        if self.isTrain:\n            print('[ Init ] Load parameters from pretrained encoder network')\n            f = lambda x: transform_key_for_parallel(x)\n            self.netA.load_state_dict(f(self.pretrained_encoder.netA.state_dict()))\n            self.netV.load_state_dict(f(self.pretrained_encoder.netV.state_dict()))\n            self.netL.load_state_dict(f(self.pretrained_encoder.netL.state_dict()))\n        \n    def load_from_opt_record(self, file_path):\n        opt_content = json.load(open(file_path, 'r'))\n        opt = OptConfig()\n        opt.load(opt_content)\n        return opt\n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device)\n        lexical = input['L_feat'].float().to(self.device)\n        visual = input['V_feat'].float().to(self.device)\n        if self.isTrain:\n            self.label = input['label'].to(self.device)\n            batch_size = acoustic.size(0)\n            self.missing_index = torch.zeros([batch_size]).long().to(self.device)\n            self.miss_index_matrix = torch.zeros([batch_size, 3]).to(self.device)\n            self.missing_index = self.missing_index.random_(0, 3)\n            self.missing_index = self.miss_index_matrix.scatter_(1, self.missing_index.unsqueeze(1), 1).long()\n            self.missing_index[:int((1-self.miss2_rate) * batch_size)] = -1 * \\\n                            (self.missing_index[:int((1-self.miss2_rate) * batch_size)] - 1) # 前(1-self.miss2_rate)的数据取反, \n\n            # A modality\n            self.A_miss_index = self.missing_index[:, 0].unsqueeze(1).unsqueeze(2)\n            self.A_miss = acoustic * self.A_miss_index\n            self.A_reverse = acoustic * -1 * (self.A_miss_index - 1)\n            # L modality\n            self.L_miss_index = self.missing_index[:, 2].unsqueeze(1).unsqueeze(2)\n            self.L_miss = lexical * self.L_miss_index\n            self.L_reverse = lexical * -1 * (self.L_miss_index - 1)\n            # V modality\n            self.V_miss_index = self.missing_index[:, 1].unsqueeze(1).unsqueeze(2)\n            self.V_miss = visual * self.V_miss_index\n            self.V_reverse = visual * -1 * (self.V_miss_index - 1)\n        else:\n            self.A_miss = acoustic\n            self.V_miss = visual\n            self.L_miss = lexical\n\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        # get utt level representattion\n        self.feat_A_miss = self.netA(self.A_miss)\n        self.feat_L_miss = self.netL(self.L_miss)\n        self.feat_V_miss = self.netV(self.V_miss)\n        # fusion miss\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        # calc reconstruction of teacher's output\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        self.recon_cycle, self.latent_cycle = self.netAE_cycle(self.recon_fusion)\n        # get fusion outputs for missing modality\n        self.logits, _ = self.netC(self.latent)\n        self.pred = F.softmax(self.logits, dim=-1)\n        # for training \n        if self.isTrain:\n            with torch.no_grad():\n                self.T_embd_A = self.pretrained_encoder.netA(self.A_reverse)\n                self.T_embd_L = self.pretrained_encoder.netL(self.L_reverse)\n                self.T_embd_V = self.pretrained_encoder.netV(self.V_reverse)\n                self.T_embds = torch.cat([self.T_embd_A, self.T_embd_L, self.T_embd_V], dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.ce_weight * self.criterion_ce(self.logits, self.label)\n        self.loss_mse = self.mse_weight * self.criterion_mse(self.T_embds, self.recon_fusion)\n        self.loss_cycle = self.cycle_weight * self.criterion_mse(self.feat_fusion_miss.detach(), self.recon_cycle)\n        loss = self.loss_CE + self.loss_mse + self.loss_cycle\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n            \n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/auto/task_generate.py", "content": "import os\n\ndef make_grid(params):\n    total_length = 1\n    for key, value in params.items():\n        total_length *= len(value)\n    \n    ans = []\n    for _ in range(total_length):\n        ans.append({})\n    \n    combo_num = total_length\n    for key, value in params.items():\n        combo_num = combo_num // len(value)\n        for i in range(0, total_length, combo_num):\n            for j in range(combo_num):\n                ans[i+j][key] = value[i//combo_num%len(value)]\n\n    return ans\n\ndef make_task(parameters):\n    # tuned hyper-parameters\n    param_grid = make_grid(parameters)\n    template = 'sh ' + task_script + ' ' + ' '.join(['{' + key + '}' for key in parameters.keys()])\n\n    total_cmd = []\n    for param in param_grid:\n        cmd = template.format(**param)\n        total_cmd.append(cmd)\n    \n    # 平均分配gpu\n    cmd_with_gpu = []\n    for i in range(len(avialable_gpus)):\n        task_num = len(total_cmd) / len(avialable_gpus)\n        cmds = total_cmd[int(i*task_num):int((i+1)*task_num)]\n        for cmd in cmds:\n            cmd_with_gpu.append(cmd + ' ' + str(avialable_gpus[i]))\n    \n    for i in range(num_sessions):\n        session_name = '{}_{}'.format(screen_name, i)\n        task_file = os.path.join(auto_script_dir, f'{i}_task.sh')\n        f = open(task_file, 'w')\n        f.write('screen -dmS {}\\n'.format(session_name))\n        task_num = len(cmd_with_gpu) / num_sessions\n        cmds = cmd_with_gpu[int(i*task_num):int((i+1)*task_num)]\n        for cmd in cmds:\n            _cmd = \"screen -x -S {} -p 0 -X stuff '{}\\n'\\n\".format(session_name, cmd)\n            f.write(_cmd)\n        f.write(\"screen -x -S {} -p 0 -X stuff 'exit\\n'\\n\".format(session_name))\n   \nif __name__ == '__main__':\n    auto_script_dir = 'auto/tmp'                                # 生成脚本路径\n    script_root = 'auto/scripts'\n    task_script = script_root + '/' + 'mmin.sh'           # 执行script路径\n    avialable_gpus = [0,1,2,3,4,5]                                  # 可用GPU有哪些\n    num_sessions = 6                                            # 一共开多少个session同时执行\n    avialable_gpus = avialable_gpus[:num_sessions]\n    screen_name = 'mmin'\n    parameters = {                                              # 一共有哪些参数\n        'mse_weight': [0.1, 0.15, 0.2],\n        'cycle_weight': [0.05, 0.1, 0.2],\n        'run_idx': [1, 2]\n    }\n    make_task(parameters)\n\n    for i in range(num_sessions):\n        cmd = 'sh {}/{}_task.sh'.format(auto_script_dir, i)\n        print(cmd)\n        os.system(cmd)"}
{"type": "source_file", "path": "baseline-mmin/data/multimodal_miss_dataset.py", "content": "import os\nimport json\nimport random\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass MultimodalMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--A_type', type=str, help='which audio feat to use')\n        parser.add_argument('--V_type', type=str, help='which visual feat to use')\n        parser.add_argument('--L_type', type=str, help='which lexical feat to use')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAP_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.A_type = opt.A_type\n        self.all_A = h5py.File(os.path.join(config['feature_root'], 'A', f'{self.A_type}.h5'), 'r')\n        if self.A_type == 'comparE':\n            self.mean_std = h5py.File(os.path.join(config['feature_root'], 'A', 'comparE_mean_std.h5'), 'r')\n            self.mean = torch.from_numpy(self.mean_std[str(cvNo)]['mean'][()]).unsqueeze(0).float()\n            self.std = torch.from_numpy(self.mean_std[str(cvNo)]['std'][()]).unsqueeze(0).float()\n        self.V_type = opt.V_type\n        self.all_V = h5py.File(os.path.join(config['feature_root'], 'V', f'{self.V_type}.h5'), 'r')\n        self.L_type = opt.L_type\n        self.all_L = h5py.File(os.path.join(config['feature_root'], 'L', f'{self.L_type}.h5'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        # make missing index\n        if set_name != 'trn':           # val && tst\n            self.missing_index = torch.tensor([\n                [1,0,0], # AZZ\n                [0,1,0], # ZVZ\n                [0,0,1], # ZZL\n                [1,1,0], # AVZ\n                [1,0,1], # AZL\n                [0,1,1], # ZVL\n                # [1,1,1]  # AVL\n            ] * len(self.label)).long()\n            self.miss_type = ['azz', 'zvz', 'zzl', 'avz', 'azl', 'zvl'] * len(self.label)\n        else:                           # trn\n            self.missing_index = [\n                [1,0,0], # AZZ\n                [0,1,0], # ZVZ\n                [0,0,1], # ZZL\n                [1,1,0], # AVZ\n                [1,0,1], # AZL\n                [0,1,1], # ZVL\n                # [1,1,1]  # AVL\n            ]\n            self.miss_type = ['azz', 'zvz', 'zzl', 'avz', 'azl', 'zvl']\n        \n        # set collate function\n        self.manual_collate_fn = True\n\n    def __getitem__(self, index):\n        if self.set_name != 'trn':\n            feat_idx = index // 6         # totally 6 missing types\n            missing_index = self.missing_index[index]         \n            miss_type = self.miss_type[index]\n        else:  # 'trn'\n            feat_idx = index\n            missing_index = torch.tensor(random.choice(self.missing_index)).long() # random choice: [1,0,0]\n            miss_type = random.choice(self.miss_type)                              # miss_type\n        \n        int2name = self.int2name[feat_idx][0].decode()\n        label = torch.tensor(self.label[feat_idx])\n        \n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[int2name][()]).float()\n        if self.A_type == 'comparE':\n            A_feat = self.normalize_on_utt(A_feat) if self.norm_method == 'utt' else self.normalize_on_trn(A_feat)\n        # process V_feat \n        V_feat = torch.from_numpy(self.all_V[int2name][()]).float()\n        # proveee L_feat\n        L_feat = torch.from_numpy(self.all_L[int2name][()]).float()\n        \n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index,\n            'miss_type': miss_type\n        } if self.set_name == 'trn' else{\n            'A_feat': A_feat * missing_index[0], \n            'V_feat': V_feat * missing_index[1],\n            'L_feat': L_feat * missing_index[2],\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index,\n            'miss_type': miss_type\n        }\n    \n    def __len__(self):\n        return len(self.missing_index) if self.set_name != 'trn' else len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        missing_index = torch.cat([sample['missing_index'].unsqueeze(0) for sample in batch], axis=0)\n        miss_type = [sample['miss_type'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name,\n            'missing_index': missing_index,\n            'miss_type': miss_type\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert_large\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MultimodalMissDataset(opt, set_name='trn')\n    # print()\n    # data = next(iter(a))\n    # for k, v in data.items():\n    #     if k not in ['int2name', 'label']:\n    #         print(k, v.shape, torch.sum(v))\n    #     else:\n    #         print(k, v)\n    print('Reading from dataloader:')\n    x = [a[i] for i in range(128)]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(_x['missing_index'], _x['miss_type'])\n    # for i, _x in enumerate(x):\n    #     print(i, ':')\n    #     for k, v in _x.items():\n    #         if k not in ['int2name', 'label', 'missing_index']:\n    #             print(k, v.shape, torch.sum(v))\n    #         else:\n    #             print(k, v)\n    # print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label', 'miss_type']:\n            print(k, v.shape, torch.sum(v))\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/models/networks/lstm.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LSTMEncoder(nn.Module):\n    ''' one directional LSTM encoder\n    '''\n    def __init__(self, input_size, hidden_size, embd_method='last'):\n        super(LSTMEncoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.rnn = nn.LSTM(self.input_size, self.hidden_size, batch_first=True)\n        assert embd_method in ['maxpool', 'attention', 'last']\n        self.embd_method = embd_method\n        \n        if self.embd_method == 'attention':\n            self.attention_vector_weight = nn.Parameter(torch.Tensor(hidden_size, 1))\n            self.attention_layer = nn.Sequential(\n                nn.Linear(self.hidden_size, self.hidden_size),\n                nn.Tanh(),\n            )\n            self.softmax = nn.Softmax(dim=-1)\n\n    def embd_attention(self, r_out, h_n):\n        ''''\n        参考这篇博客的实现:\n        https://blog.csdn.net/dendi_hust/article/details/94435919\n        https://blog.csdn.net/fkyyly/article/details/82501126\n        论文：Hierarchical Attention Networks for Document Classification\n        formulation:  lstm_output*softmax(u * tanh(W*lstm_output + Bias)\n        W and Bias 是映射函数，其中 Bias 可加可不加\n        u 是 attention vector 大小等于 hidden size\n        '''\n        hidden_reps = self.attention_layer(r_out)                       # [batch_size, seq_len, hidden_size]\n        atten_weight = (hidden_reps @ self.attention_vector_weight)     # [batch_size, seq_len, 1]\n        atten_weight = self.softmax(atten_weight)                       # [batch_size, seq_len, 1]\n        # [batch_size, seq_len, hidden_size] * [batch_size, seq_len, 1]  =  [batch_size, seq_len, hidden_size]\n        sentence_vector = torch.sum(r_out * atten_weight, dim=1)       # [batch_size, hidden_size]\n        return sentence_vector\n\n    def embd_maxpool(self, r_out, h_n):\n        # embd = self.maxpool(r_out.transpose(1,2))   # r_out.size()=>[batch_size, seq_len, hidden_size]\n                                                    # r_out.transpose(1, 2) => [batch_size, hidden_size, seq_len]\n        in_feat = r_out.transpose(1,2)\n        embd = F.max_pool1d(in_feat, in_feat.size(2), in_feat.size(2))\n        return embd.squeeze()\n\n    def embd_last(self, r_out, h_n):\n        #Just for  one layer and single direction\n        return h_n.squeeze()\n\n    def forward(self, x):\n        '''\n        r_out shape: seq_len, batch, num_directions * hidden_size\n        hn and hc shape: num_layers * num_directions, batch, hidden_size\n        '''\n        r_out, (h_n, h_c) = self.rnn(x)\n        embd = getattr(self, 'embd_'+self.embd_method)(r_out, h_n)\n        return embd"}
{"type": "source_file", "path": "baseline-mmin/models/__init__.py", "content": "\"\"\"This package contains modules related to objective functions, optimizations, and network architectures.\n\nTo add a custom model class called 'dummy', you need to add a file called 'dummy_model.py' and define a subclass DummyModel inherited from BaseModel.\nYou need to implement the following five functions:\n    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n    -- <set_input>:                     unpack data from dataset and apply preprocessing.\n    -- <forward>:                       produce intermediate results.\n    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n\nIn the function <__init__>, you need to define four lists:\n    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n    -- self.model_names (str list):         define networks used in our training.\n    -- self.visual_names (str list):        specify the images that you want to display and save.\n    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\n\nNow you can use the model class by specifying flag '--model dummy'.\nSee our template model class 'template_model.py' for more details.\n\"\"\"\n\nimport importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    \"\"\"Import the module \"models/[model_name]_model.py\".\n\n    In the file, the class called DatasetNameModel() will\n    be instantiated. It has to be a subclass of BaseModel,\n    and it is case-insensitive.\n    \"\"\"\n    model_filename = \"models.\" + model_name + \"_model\" # 'models.mmin_model'\n    modellib = importlib.import_module(model_filename)\n    model = None\n    target_model_name = model_name.replace('_', '') + 'model'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    \"\"\"Create a model given the option.\n\n    This function warps the class CustomDatasetDataLoader.\n    This is the main interface between this package and 'train.py'/'test.py'\n\n    Example:\n        >>> from models import create_model\n        >>> model = create_model(opt)\n    \"\"\"\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    print(\"model [%s] was created\" % type(instance).__name__)\n    return instance\n"}
{"type": "source_file", "path": "baseline-mmin/models/mmin_ablation_model.py", "content": "\nimport torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import ResidualAE_test\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINAblationModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--n_blocks', type=int, default=3, help='number of AE blocks')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        parser.add_argument('--cycle_weight', type=float, default=1.0, help='weight of cycle loss')\n        parser.add_argument('--case', type=str, default='raw', help='weight of cycle loss')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE', 'mse', 'cycle']\n        self.model_names = ['A', 'V', 'L', 'C', 'AE', 'AE_cycle']\n        \n        # acoustic model\n        self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n        # lexical model\n        self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n        # visual model\n        self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, opt.embd_method_v)\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        self.netAE = ResidualAE_test(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False, case=opt.case)\n        self.netAE_cycle = ResidualAE_test(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False, case=opt.case)\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        cls_input_size = AE_layers[-1] * opt.n_blocks\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        if self.isTrain:\n            self.load_pretrained_encoder(opt)\n            self.criterion_ce = torch.nn.CrossEntropyLoss()\n            self.criterion_mse = torch.nn.MSELoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n            self.cycle_weight = opt.cycle_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def load_pretrained_encoder(self, opt):\n        print('Init parameter from {}'.format(opt.pretrained_path))\n        pretrained_path = os.path.join(opt.pretrained_path, str(opt.cvNo))\n        pretrained_config_path = os.path.join(opt.pretrained_path, 'train_opt.conf')\n        pretrained_config = self.load_from_opt_record(pretrained_config_path)\n        pretrained_config.isTrain = False                             # teacher model should be in test mode\n        pretrained_config.gpu_ids = opt.gpu_ids                       # set gpu to the same\n        self.pretrained_encoder = UttFusionModel(pretrained_config)\n        self.pretrained_encoder.load_networks_cv(pretrained_path)\n        self.pretrained_encoder.cuda()\n        self.pretrained_encoder.eval()\n    \n    def post_process(self):\n        # called after model.setup()\n        def transform_key_for_parallel(state_dict):\n            return OrderedDict([('module.'+key, value) for key, value in state_dict.items()])\n        if self.isTrain:\n            print('[ Init ] Load parameters from pretrained encoder network')\n            f = lambda x: transform_key_for_parallel(x)\n            self.netA.load_state_dict(f(self.pretrained_encoder.netA.state_dict()))\n            self.netV.load_state_dict(f(self.pretrained_encoder.netV.state_dict()))\n            self.netL.load_state_dict(f(self.pretrained_encoder.netL.state_dict()))\n        \n    def load_from_opt_record(self, file_path):\n        opt_content = json.load(open(file_path, 'r'))\n        opt = OptConfig()\n        opt.load(opt_content)\n        return opt\n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device)\n        lexical = input['L_feat'].float().to(self.device)\n        visual = input['V_feat'].float().to(self.device)\n        if self.isTrain:\n            self.label = input['label'].to(self.device)\n            self.missing_index = input['missing_index'].long().to(self.device)\n            # A modality\n            self.A_miss_index = self.missing_index[:, 0].unsqueeze(1).unsqueeze(2)\n            self.A_miss = acoustic * self.A_miss_index\n            self.A_reverse = acoustic * -1 * (self.A_miss_index - 1)\n            # L modality\n            self.L_miss_index = self.missing_index[:, 2].unsqueeze(1).unsqueeze(2)\n            self.L_miss = lexical * self.L_miss_index\n            self.L_reverse = lexical * -1 * (self.L_miss_index - 1)\n            # V modality\n            self.V_miss_index = self.missing_index[:, 1].unsqueeze(1).unsqueeze(2)\n            self.V_miss = visual * self.V_miss_index\n            self.V_reverse = visual * -1 * (self.V_miss_index - 1)\n        else:\n            self.A_miss = acoustic\n            self.V_miss = visual\n            self.L_miss = lexical\n\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        # get utt level representattion\n        self.feat_A_miss = self.netA(self.A_miss)\n        self.feat_L_miss = self.netL(self.L_miss)\n        self.feat_V_miss = self.netV(self.V_miss)\n        # fusion miss\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        # calc reconstruction of teacher's output\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        self.recon_cycle, self.latent_cycle = self.netAE_cycle(self.recon_fusion)\n        # get fusion outputs for missing modality\n        self.logits, _ = self.netC(self.latent)\n        self.pred = F.softmax(self.logits, dim=-1)\n        # for training \n        if self.isTrain:\n            with torch.no_grad():\n                self.T_embd_A = self.pretrained_encoder.netA(self.A_reverse)\n                self.T_embd_L = self.pretrained_encoder.netL(self.L_reverse)\n                self.T_embd_V = self.pretrained_encoder.netV(self.V_reverse)\n                self.T_embds = torch.cat([self.T_embd_A, self.T_embd_L, self.T_embd_V], dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.ce_weight * self.criterion_ce(self.logits, self.label)\n        self.loss_mse = self.mse_weight * self.criterion_mse(self.T_embds, self.recon_fusion)\n        self.loss_cycle = self.cycle_weight * self.criterion_mse(self.feat_fusion_miss.detach(), self.recon_cycle)\n        loss = self.loss_CE + self.loss_mse + self.loss_cycle\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n            \n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/data/__init__.py", "content": "\"\"\"This package includes all the modules related to data loading and preprocessing\n\n To add a custom dataset class called 'dummy', you need to add a file called 'dummy_dataset.py' and define a subclass 'DummyDataset' inherited from BaseDataset.\n You need to implement four functions:\n    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).\n    -- <__len__>:                       return the size of dataset.\n    -- <__getitem__>:                   get a data point from data loader.\n    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.\n\nNow you can use the dataset class by specifying flag '--dataset_mode dummy'.\nSee our template dataset class 'template_dataset.py' for more details.\n\"\"\"\nimport importlib\nimport torch.utils.data\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    \"\"\"Import the module \"data/[dataset_name]_dataset.py\".\n\n    In the file, the class called DatasetNameDataset() will\n    be instantiated. It has to be a subclass of BaseDataset,\n    and it is case-insensitive.\n    \"\"\"\n    dataset_filename = \"data.\" + dataset_name + \"_dataset\"\n    datasetlib = importlib.import_module(dataset_filename)\n\n    dataset = None\n    target_dataset_name = dataset_name.replace('_', '') + 'dataset'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        raise NotImplementedError(\"In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase.\" % (dataset_filename, target_dataset_name))\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):\n    \"\"\"Return the static method <modify_commandline_options> of the dataset class.\"\"\"\n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    \"\"\"Create a dataloader given the option.\n\n    This function wraps the class CustomDatasetDataLoader.\n        This is the main interface between this package and 'train.py'/'test.py'\n\n    Example:\n        >>> from data import create_dataset\n        >>> dataset = create_dataset(opt)\n    \"\"\"\n    data_loader = CustomDatasetDataLoader(opt)\n    return data_loader\n\n# opt, set_name=['trn', 'val', 'tst']\ndef create_dataset_with_args(opt, **kwargs):\n    \"\"\"Create two dataloader given the option, dataset may have additional args.\n    This function wraps the class CustomDatasetDataLoader.\n        This is the main interface between this package and 'train.py'/'test.py'\n    Example:\n        >>> from data import create_split_dataset\n        >>> dataset = create_dataset(opt, set_name=['trn', 'val', 'tst'])\n        This will create 3 datasets, each one get different parameters\n        for the specific dataset class, __init__ func must get one parameter\n        eg: dataset.__init__(self, set_name='trn'): ....\n    \"\"\"\n    _kwargs = []\n    for key in kwargs: # 'set_name'\n        value = kwargs[key] # 'trn', 'val', 'tst'\n        if not isinstance(value, (list, tuple)):\n            value = [value]\n        lens = len(value) # lens = 3\n        _kwargs += list(map(lambda x: {}, range(lens))) if len(_kwargs) == 0 else []\n        for i, v in enumerate(value):\n            _kwargs[i][key] = v \n    \n    # _kwargs: [{'set_name': 'trn'}, {'set_name': 'val'}, {'set_name': 'tst'}]    \n    dataloaders = tuple(map(lambda x: CustomDatasetDataLoader(opt, **x), _kwargs))\n    return dataloaders if len(dataloaders) > 1 else dataloaders[0]\n\n\nclass CustomDatasetDataLoader():\n    \"\"\"Wrapper class of Dataset class that performs multi-threaded data loading\"\"\"\n    ## kwargs: [{'set_name': 'trn'}, {'set_name': 'val'}, {'set_name': 'tst'}]\n    def __init__(self, opt, **kwargs):\n        \"\"\"Initialize this class\n        Step 1: create a dataset instance given the name [dataset_mode]\n        Step 2: create a multi-threaded data loader.\n        \"\"\"\n        self.opt = opt\n        dataset_class = find_dataset_using_name(opt.dataset_mode) # 'data.multimodal_dataset.MultimodalDataset'\n        self.dataset = dataset_class(opt, **kwargs)\n        # print(\"dataset [%s] was created\" % type(self.dataset).__name__)\n        \n        ''' Whether to use manual collate function defined in dataset.collate_fn'''\n        if self.dataset.manual_collate_fn: \n            self.dataloader = torch.utils.data.DataLoader(\n                self.dataset,\n                batch_size=opt.batch_size,\n                shuffle=not opt.serial_batches,\n                num_workers=int(opt.num_threads),\n                drop_last=False,\n                collate_fn=self.dataset.collate_fn\n            )\n\n        else:\n            self.dataloader = torch.utils.data.DataLoader(\n                self.dataset,\n                batch_size=opt.batch_size,\n                shuffle=not opt.serial_batches,\n                num_workers=int(opt.num_threads),\n                drop_last=False\n            )\n\n    def __len__(self):\n        \"\"\"Return the number of data in the dataset\"\"\"\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        \"\"\"Return a batch of data\"\"\"\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n"}
{"type": "source_file", "path": "baseline-mmin/data/iemocapfour_miss_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom data.base_dataset import BaseDataset\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n## copy from cpm-net\ndef random_mask(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return: Sn [alldata_len, view_num]\n    \"\"\"\n    # print (f'==== generate random mask ====')\n    one_rate = 1-missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        # print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        # print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        # print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\n\nclass IEMOCAPFOURMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings\n        cvNo = opt.cvNo\n        self.mask_rate = opt.mask_rate\n        self.dataset = opt.dataset_mode.split('_')[0]\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAPFOUR_config.json')))\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        ####################################\n        self.maskmatrix, self.name2mask = [], []\n        if set_name == 'tst' and opt.test_mask is not None: # make missing index\n            print (f'using predefined mask!!')\n            self.name2mask = np.load(opt.test_mask, allow_pickle=True)['name2mask'].tolist()\n        else: ## gain outside mask for testing set\n            samplenum = len(self.label)\n            print (f'using random initialized mask!!')\n            self.maskmatrix = random_mask(3, samplenum, self.mask_rate) # [samplenum, view_num]\n        ####################################\n\n        self.manual_collate_fn = False\n\n    def __getitem__(self, index):\n        \n        int2name = self.int2name[index]\n\n        ####################################\n        if len(self.maskmatrix) > 0:\n            maskseq = self.maskmatrix[index] # (3, ) (A, V, L)\n        elif len(self.name2mask) > 0:\n            maskseq = self.name2mask[int2name] # (A, L, V)\n            maskseq = [maskseq[0], maskseq[2], maskseq[1]] # (A, V, L)\n        missing_index = torch.LongTensor(maskseq) # (3, ) [1,1,1]; maskrate=1=>[0,0,0]\n        ####################################\n        \n        if self.dataset in ['cmumosi']:\n            label = torch.tensor(self.label[index]).float()\n        elif self.dataset in ['iemocapfour', 'iemocapsix']:\n            label = torch.tensor(self.label[index]).long()\n\n        A_feat = torch.tensor(self.all_A[index]).float()\n        V_feat = torch.tensor(self.all_V[index]).float()\n        L_feat = torch.tensor(self.all_L[index]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index\n        }\n    \n    def __len__(self):\n        return len(self.label)\n   \n"}
{"type": "source_file", "path": "baseline-mmin/models/mmin_model.py", "content": "import torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import ResidualAE\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--n_blocks', type=int, default=3, help='number of AE blocks')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        parser.add_argument('--cycle_weight', type=float, default=1.0, help='weight of cycle loss')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE', 'mse', 'cycle']\n        self.model_names = ['A', 'V', 'L', 'C', 'AE', 'AE_cycle'] # name of each layer\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n\n        # acoustic model\n        #############################\n        # self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n        self.netA = FcClassifier(opt.input_dim_a, cls_layers, output_dim=opt.embd_size_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n        #############################\n\n        # lexical model\n        #############################\n        # self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n        self.netL = FcClassifier(opt.input_dim_l, cls_layers, output_dim=opt.embd_size_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n        #############################\n\n        # visual model\n        #############################\n        # self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, embd_method=opt.embd_method_v)\n        self.netV = FcClassifier(opt.input_dim_v, cls_layers, output_dim=opt.embd_size_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n        #############################\n\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        self.netAE = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        self.netAE_cycle = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        cls_input_size = AE_layers[-1] * opt.n_blocks\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        if self.isTrain:\n            self.load_pretrained_encoder(opt)\n            #############################\n            dataset = opt.dataset_mode.split('_')[0]\n            if dataset in ['cmumosi', 'cmumosei']:   self.criterion_ce = torch.nn.MSELoss()\n            if dataset in ['boxoflies', 'iemocapfour', 'iemocapsix']: self.criterion_ce = torch.nn.CrossEntropyLoss()\n            #############################\n            self.criterion_mse = torch.nn.MSELoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n            self.cycle_weight = opt.cycle_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def load_pretrained_encoder(self, opt):\n        print('Init parameter from {}'.format(opt.pretrained_path))\n        pretrained_path = os.path.join(opt.pretrained_path, str(opt.cvNo))\n        pretrained_config_path = os.path.join(opt.pretrained_path, 'train_opt.conf')\n        pretrained_config = self.load_from_opt_record(pretrained_config_path)\n        pretrained_config.isTrain = False                             # teacher model should be in test mode\n        pretrained_config.gpu_ids = opt.gpu_ids                       # set gpu to the same\n        self.pretrained_encoder = UttFusionModel(pretrained_config)\n        self.pretrained_encoder.load_networks_cv(pretrained_path)\n        self.pretrained_encoder.cuda()\n        self.pretrained_encoder.eval()\n    \n    def post_process(self):\n        # called after model.setup()\n        def transform_key_for_parallel(state_dict):\n            return OrderedDict([('module.'+key, value) for key, value in state_dict.items()])\n        if self.isTrain:\n            print('[ Init ] Load parameters from pretrained encoder network')\n            f = lambda x: transform_key_for_parallel(x)\n            self.netA.load_state_dict(f(self.pretrained_encoder.netA.state_dict()))\n            self.netV.load_state_dict(f(self.pretrained_encoder.netV.state_dict()))\n            self.netL.load_state_dict(f(self.pretrained_encoder.netL.state_dict()))\n        \n    def load_from_opt_record(self, file_path):\n        opt_content = json.load(open(file_path, 'r'))\n        opt = OptConfig()\n        opt.load(opt_content)\n        return opt\n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device) # [256, 512]\n        lexical = input['L_feat'].float().to(self.device)  # [256, 1024]\n        visual = input['V_feat'].float().to(self.device)   # [256, 1024]\n        self.label = input['label'].to(self.device)    # [256]\n        self.missing_index = input['missing_index'].long().to(self.device) # [256, 3]\n        # A modality\n        #############################################\n        self.A_miss_index = self.missing_index[:, 0].unsqueeze(1) # [256, 1]\n        self.A_miss = acoustic * self.A_miss_index\n        self.A_reverse = acoustic * -1 * (self.A_miss_index - 1)\n        # L modality\n        self.L_miss_index = self.missing_index[:, 2].unsqueeze(1)\n        self.L_miss = lexical * self.L_miss_index\n        self.L_reverse = lexical * -1 * (self.L_miss_index - 1)\n        # V modality\n        self.V_miss_index = self.missing_index[:, 1].unsqueeze(1)\n        self.V_miss = visual * self.V_miss_index\n        self.V_reverse = visual * -1 * (self.V_miss_index - 1)\n        #############################################\n\n\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        # get utt level representattion\n        ###############################################\n        self.feat_A_miss, _ = self.netA(self.A_miss)\n        self.feat_L_miss, _ = self.netL(self.L_miss)\n        self.feat_V_miss, _ = self.netV(self.V_miss)\n        # self.feat_A_miss = self.netA(self.A_miss)\n        # self.feat_L_miss = self.netL(self.L_miss)\n        # self.feat_V_miss = self.netV(self.V_miss)\n        ###############################################\n        # fusion miss\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        # calc reconstruction of teacher's output\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        self.recon_cycle, self.latent_cycle = self.netAE_cycle(self.recon_fusion)\n        # get fusion outputs for missing modality\n        self.hiddens = self.latent\n        self.logits, _ = self.netC(self.latent)\n        #############################\n        # self.pred = F.softmax(self.logits, dim=-1)\n        self.logits = self.logits.squeeze()\n        self.pred = self.logits\n        self.loss_recon = torch.zeros(1)\n        #############################\n        # for training \n        if self.isTrain:\n            with torch.no_grad():\n                ###############################################\n                # self.T_embd_A = self.pretrained_encoder.netA(self.A_reverse)\n                # self.T_embd_L = self.pretrained_encoder.netL(self.L_reverse)\n                # self.T_embd_V = self.pretrained_encoder.netV(self.V_reverse)\n                self.T_embd_A, _ = self.pretrained_encoder.netA(self.A_reverse)\n                self.T_embd_L, _ = self.pretrained_encoder.netL(self.L_reverse)\n                self.T_embd_V, _ = self.pretrained_encoder.netV(self.V_reverse)\n                ###############################################\n                self.T_embds = torch.cat([self.T_embd_A, self.T_embd_L, self.T_embd_V], dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.ce_weight * self.criterion_ce(self.logits, self.label)\n        self.loss_mse = self.mse_weight * self.criterion_mse(self.T_embds, self.recon_fusion)\n        self.loss_cycle = self.cycle_weight * self.criterion_mse(self.feat_fusion_miss.detach(), self.recon_cycle)\n        loss = self.loss_CE + self.loss_mse + self.loss_cycle\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n            \n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/data/cmumosei_miss_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom data.base_dataset import BaseDataset\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n## copy from cpm-net\ndef random_mask(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return: Sn [alldata_len, view_num]\n    \"\"\"\n    # print (f'==== generate random mask ====')\n    one_rate = 1-missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        # print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        # print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        # print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\n\nclass CMUMOSEIMissDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.mask_rate = opt.mask_rate\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'CMUMOSEI_config.json')))\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        # make missing index\n        samplenum = len(self.label)\n        self.maskmatrix = random_mask(3, samplenum, self.mask_rate) # [samplenum, view_num]\n\n        self.manual_collate_fn = False\n\n    def __getitem__(self, index):\n        \n        maskseq = self.maskmatrix[index] # (3, )\n        missing_index = torch.LongTensor(maskseq) # (3, ) [1,1,1]; maskrate=1=>[0,0,0]\n\n        int2name = self.int2name[index]\n        ###############\n        # label = torch.tensor(self.label[index])\n        label = torch.tensor(self.label[index]).float()\n        ###############\n        A_feat = torch.tensor(self.all_A[index]).float()\n        V_feat = torch.tensor(self.all_V[index]).float()\n        L_feat = torch.tensor(self.all_L[index]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name,\n            'missing_index': missing_index\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    "}
{"type": "source_file", "path": "baseline-mmin/data/comparE_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass ComparEDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, default=4, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAP_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.all_A = h5py.File(os.path.join(config['feature_root'], 'A', 'comparE.h5'), 'r')\n        self.mean_std = h5py.File(os.path.join(config['feature_root'], 'A', 'comparE_mean_std.h5'), 'r')\n        self.mean = torch.from_numpy(self.mean_std[str(cvNo)]['mean'][()]).unsqueeze(0).float()\n        self.std = torch.from_numpy(self.mean_std[str(cvNo)]['std'][()]).unsqueeze(0).float()\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = True\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index][0].decode()\n        label = torch.tensor(self.label[index])\n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[int2name][()]).float()\n        A_feat = self.normalize_on_utt(A_feat) if self.norm_method == 'utt' else self.normalize_on_trn(A_feat)\n        return {\n            'A_feat': A_feat, \n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        # A = pack_padded_sequence(A, lengths=lengths, batch_first=True, enforce_sorted=False)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        norm_method = 'trn'\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = ComparEDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/models/mmin_CRA_model.py", "content": "import torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import ResidualAE\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINCRAModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        parser.add_argument('--n_blocks', type=int, default=3, help='number of AE blocks')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        self.dataset = opt.dataset_mode.split('_')[0]\n        self.loss_names = ['ce', 'recon']\n        self.model_names = ['A', 'AA', 'V', 'VV', 'L', 'LL', 'C', 'AE']\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        \n        # acoustic model\n        self.netA = FcClassifier(opt.input_dim_a, cls_layers, output_dim=opt.embd_size_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netAA = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # lexical model\n        self.netL = FcClassifier(opt.input_dim_l, cls_layers, output_dim=opt.embd_size_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netLL = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # visual model\n        self.netV = FcClassifier(opt.input_dim_v, cls_layers, output_dim=opt.embd_size_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netVV = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        self.netAE = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        # cls_input_size = AE_layers[-1] * opt.n_blocks\n        self.netC = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        if self.isTrain:\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device)\n        lexical = input['L_feat'].float().to(self.device)\n        visual = input['V_feat'].float().to(self.device)\n\n        self.label = input['label'].to(self.device)\n        self.missing_index = input['missing_index'].long().to(self.device)\n        # A modality\n        self.A_miss_index = self.missing_index[:, 0].unsqueeze(1)\n        self.A_miss = acoustic * self.A_miss_index\n        self.A_reverse = acoustic * -1 * (self.A_miss_index - 1)\n        self.A_full = acoustic\n        # L modality\n        self.L_miss_index = self.missing_index[:, 2].unsqueeze(1)\n        self.L_miss = lexical * self.L_miss_index\n        self.L_reverse = lexical * -1 * (self.L_miss_index - 1)\n        self.L_full = lexical\n        # V modality\n        self.V_miss_index = self.missing_index[:, 1].unsqueeze(1)\n        self.V_miss = visual * self.V_miss_index\n        self.V_reverse = visual * -1 * (self.V_miss_index - 1)\n        self.V_full = visual\n        \n\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        \n        ## recon\n        self.feat_A_miss, _ = self.netA(self.A_miss)\n        self.feat_L_miss, _ = self.netL(self.L_miss)\n        self.feat_V_miss, _ = self.netV(self.V_miss)\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        self.A_rec, _ = self.netAA(self.recon_fusion)\n        self.L_rec, _ = self.netLL(self.recon_fusion)\n        self.V_rec, _ = self.netVV(self.recon_fusion)\n\n        ## classifier\n        self.hiddens = self.recon_fusion\n        self.logits, _ = self.netC(self.recon_fusion)\n        self.logits = self.logits.squeeze()\n        self.pred = self.logits\n\n       ## calculate cls loss\n        if self.dataset in ['cmumosi', 'cmumosei']:                    criterion_ce = torch.nn.MSELoss()\n        if self.dataset in ['boxoflies', 'iemocapfour', 'iemocapsix']: criterion_ce = torch.nn.CrossEntropyLoss()\n        self.loss_ce = criterion_ce(self.logits, self.label)\n        ## calculate recon loss [if miss, the calculate recon loss; if exist, no recon loss]\n        recon_loss = torch.nn.MSELoss(reduction='none')\n        loss_recon1 = recon_loss(self.A_rec, self.A_full) * -1 * (self.A_miss_index - 1) # 1 (exist), 0 (miss)  [batch, featdim]\n        loss_recon2 = recon_loss(self.L_rec, self.L_full) * -1 * (self.L_miss_index - 1) # 1 (exist), 0 (miss)\n        loss_recon3 = recon_loss(self.V_rec, self.V_full) * -1 * (self.V_miss_index - 1) # 1 (exist), 0 (miss)\n        loss_recon1 = torch.sum(loss_recon1) / self.A_full.shape[1]                   # each dimension delta\n        loss_recon2 = torch.sum(loss_recon2) / self.L_full.shape[1]                   # each dimension delta\n        loss_recon3 = torch.sum(loss_recon3) / self.V_full.shape[1]                   # each dimension delta\n        self.loss_recon = loss_recon1 + loss_recon2 + loss_recon3\n        ## merge all loss\n        self.loss = self.ce_weight * self.loss_ce + self.mse_weight * self.loss_recon\n\n\n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n\n\n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/change_format.py", "content": "import re\nimport os\nimport copy\nimport tqdm\nimport glob\nimport json\nimport math\nimport shutil\nimport random\nimport pickle\nimport numpy as np\nimport soundfile as sf\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\ndef name2feat(feature_root):\n\n    ## gain (names)\n    names = os.listdir(feature_root)\n\n    ## (names, speakers) => features\n    features = []\n    feature_dim = -1\n    for ii, name in tqdm.tqdm(enumerate(names)):\n        feature = []\n        feature_path = os.path.join(feature_root, name) # folder or npy\n        # print (f'process name: {name}  {ii+1}/{len(names)}')\n        if os.path.isfile(feature_path): # for .npy\n            single_feature = np.load(feature_path)\n            single_feature = single_feature.squeeze() # [Dim, ] or [Time, Dim]\n            feature.append(single_feature)\n            feature_dim = max(feature_dim, single_feature.shape[-1])\n        else: ## exists dir, faces\n            facenames = os.listdir(feature_path)\n            for facename in sorted(facenames):\n                facefeat = np.load(os.path.join(feature_path, facename))\n                feature_dim = max(feature_dim, facefeat.shape[-1])\n                feature.append(facefeat)\n        # sequeeze features\n        single_feature = np.array(feature).squeeze()\n        if len(single_feature) == 0:\n            single_feature = np.zeros((feature_dim, ))\n        elif len(single_feature.shape) == 2:\n            single_feature = np.mean(single_feature, axis=0)\n        features.append(single_feature)\n\n    ## save (names, features)\n    print (f'Input feature {os.path.basename(feature_root)} ===> dim is {feature_dim}; No. sample is {len(names)}')\n    assert len(names) == len(features), f'Error: len(names) != len(features)'\n    name2feats = {}\n    for ii in range(len(names)):\n        name = names[ii]\n        if name.endswith('.npy') or name.endswith('.npz'):\n            name = name[:-4]\n        name2feats[name] = features[ii]\n\n    ## return name2feats\n    return name2feats\n\n\n\n#########################################################\n## Process for cmumosi\n#########################################################\ndef change_feat_format_cmumosei():\n\n    label_pkl = '../dataset/CMUMOSEI/CMUMOSEI_features_raw_2way.pkl'\n    feat_root = '../dataset/CMUMOSEI/features'\n    save_root = './CMUMOSEI_features_2021'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentence, trainVid, valVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n    for (item1, item2) in [(trainVid, 'trn'), (valVid, 'val'), (testVid, 'tst')]:\n        all_A = []\n        all_V = []\n        all_L = []\n        label = []\n        int2name = []\n        for vid in tqdm.tqdm(item1):\n            int2name.extend(videoIDs[vid])\n            label.extend(videoLabels[vid])\n            for ii in range(len(videoIDs[vid])):\n                name = videoIDs[vid][ii]\n                featA = name2featA[name]\n                featV = name2featV[name]\n                featL = name2featL[name]\n                all_A.append(featA)\n                all_V.append(featV)\n                all_L.append(featL)\n        all_A = np.array(all_A)\n        all_V = np.array(all_V)\n        all_L = np.array(all_L)\n\n        save_path = f\"{save_root}/A/1/{item2}.npy\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.save(save_path, all_A)\n\n        save_path = f\"{save_root}/V/1/{item2}.npy\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.save(save_path, all_V)\n\n        save_path = f\"{save_root}/L/1/{item2}.npy\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.save(save_path, all_L)\n\n        save_path = f\"{save_root}/target/1/{item2}_label.npy\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.save(save_path, label)\n\n        save_path = f\"{save_root}/target/1/{item2}_int2name.npy\"\n        save_temp = os.path.split(save_path)[0]\n        if not os.path.exists(save_temp): os.makedirs(save_temp)\n        np.save(save_path, int2name)\n\n\n\n#########################################################\n## Process for iemocapfour\n#########################################################\ndef change_feat_format_iemocapfour():\n    label_pkl = '../dataset/IEMOCAP/IEMOCAP_features_raw_4way.pkl'\n    feat_root = '../dataset/IEMOCAP/features'\n    save_root = './IEMOCAPFOUR_features_2021'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentences, trainVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n    ## generate five folders\n    num_folder = 5\n    vids = sorted(list(trainVid | testVid))\n\n    session_to_idx = {}\n    for idx, vid in enumerate(vids):\n        session = int(vid[4]) - 1\n        if session not in session_to_idx: session_to_idx[session] = []\n        session_to_idx[session].append(idx)\n    assert len(session_to_idx) == num_folder, f'Must split into five folder'\n\n    train_test_idxs = []\n    for ii in range(num_folder): # ii in [0, 4]\n        test_idxs = session_to_idx[ii]\n        train_idxs = []\n        for jj in range(num_folder):\n            if jj != ii: train_idxs.extend(session_to_idx[jj])\n        train_test_idxs.append([train_idxs, test_idxs])\n\n    ## for each folder\n    for ii in range(len(train_test_idxs)):\n        train_idxs = train_test_idxs[ii][0]\n        test_idxs = train_test_idxs[ii][1]\n        trainVid = np.array(vids)[train_idxs]\n        testVid = np.array(vids)[test_idxs]\n\n        for (item1, item2) in [(trainVid, 'trn'), (testVid, 'val'), (testVid, 'tst')]:\n            ## change to utterance-level feats\n            all_A = []\n            all_V = []\n            all_L = []\n            label = []\n            int2name = []\n            for vid in tqdm.tqdm(item1):\n                int2name.extend(videoIDs[vid])\n                label.extend(videoLabels[vid])\n                for jj in range(len(videoIDs[vid])):\n                    name = videoIDs[vid][jj]\n                    featA = name2featA[name]\n                    featV = name2featV[name]\n                    featL = name2featL[name]\n                    all_A.append(featA)\n                    all_V.append(featV)\n                    all_L.append(featL)\n            all_A = np.array(all_A)\n            all_V = np.array(all_V)\n            all_L = np.array(all_L)\n\n            save_path = f\"{save_root}/A/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_A)\n\n            save_path = f\"{save_root}/V/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_V)\n\n            save_path = f\"{save_root}/L/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_L)\n\n            save_path = f\"{save_root}/target/{ii+1}/{item2}_label.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, label)\n\n            save_path = f\"{save_root}/target/{ii+1}/{item2}_int2name.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, int2name)\n\n\n\n#########################################################\n## Process for iemocapfour\n#########################################################\ndef change_feat_format_iemocapsix():\n    label_pkl = '../dataset/IEMOCAP/IEMOCAP_features_raw_6way.pkl'\n    feat_root = '../dataset/IEMOCAP/features'\n    save_root = './IEMOCAPSIX_features_2021'\n    nameA = 'wav2vec-large-c-UTT'\n    nameV = 'manet_UTT'\n    nameL = 'deberta-large-4-UTT'\n    videoIDs, videoLabels, videoSpeakers, videoSentences, trainVid, testVid = pickle.load(open(label_pkl, \"rb\"), encoding='latin1')\n    featrootA = os.path.join(feat_root, nameA)\n    featrootV = os.path.join(feat_root, nameV)\n    featrootL = os.path.join(feat_root, nameL)\n    name2featA = name2feat(featrootA)\n    name2featV = name2feat(featrootV)\n    name2featL = name2feat(featrootL)\n\n    ## generate five folders\n    num_folder = 5\n    vids = sorted(list(trainVid | testVid))\n\n    session_to_idx = {}\n    for idx, vid in enumerate(vids):\n        session = int(vid[4]) - 1\n        if session not in session_to_idx: session_to_idx[session] = []\n        session_to_idx[session].append(idx)\n    assert len(session_to_idx) == num_folder, f'Must split into five folder'\n\n    train_test_idxs = []\n    for ii in range(num_folder): # ii in [0, 4]\n        test_idxs = session_to_idx[ii]\n        train_idxs = []\n        for jj in range(num_folder):\n            if jj != ii: train_idxs.extend(session_to_idx[jj])\n        train_test_idxs.append([train_idxs, test_idxs])\n\n    ## for each folder\n    for ii in range(len(train_test_idxs)):\n        train_idxs = train_test_idxs[ii][0]\n        test_idxs = train_test_idxs[ii][1]\n        trainVid = np.array(vids)[train_idxs]\n        testVid = np.array(vids)[test_idxs]\n\n        for (item1, item2) in [(trainVid, 'trn'), (testVid, 'val'), (testVid, 'tst')]:\n            ## change to utterance-level feats\n            all_A = []\n            all_V = []\n            all_L = []\n            label = []\n            int2name = []\n            for vid in tqdm.tqdm(item1):\n                int2name.extend(videoIDs[vid])\n                label.extend(videoLabels[vid])\n                for jj in range(len(videoIDs[vid])):\n                    name = videoIDs[vid][jj]\n                    featA = name2featA[name]\n                    featV = name2featV[name]\n                    featL = name2featL[name]\n                    all_A.append(featA)\n                    all_V.append(featV)\n                    all_L.append(featL)\n            all_A = np.array(all_A)\n            all_V = np.array(all_V)\n            all_L = np.array(all_L)\n\n            save_path = f\"{save_root}/A/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_A)\n\n            save_path = f\"{save_root}/V/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_V)\n\n            save_path = f\"{save_root}/L/{ii+1}/{item2}.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, all_L)\n\n            save_path = f\"{save_root}/target/{ii+1}/{item2}_label.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, label)\n\n            save_path = f\"{save_root}/target/{ii+1}/{item2}_int2name.npy\"\n            save_temp = os.path.split(save_path)[0]\n            if not os.path.exists(save_temp): os.makedirs(save_temp)\n            np.save(save_path, int2name)\n\n\n\nif __name__ == '__main__':\n    import fire\n    fire.Fire()\n\n\n"}
{"type": "source_file", "path": "baseline-cca/util.py", "content": "import scipy.io as sio\nimport numpy as np\nimport math\nimport os\nfrom numpy.random import shuffle\nimport numpy as np\nfrom numpy.random import randint\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef Normalize(data):\n    \"\"\"\n    :param data:Input data\n    :return:normalized data\n    \"\"\"\n    m = np.mean(data)\n    mx = np.max(data)\n    mn = np.min(data)\n    return (data - m) / (mx - mn)\n\n\ndef read_cmumosi_data(data_root, normalize=True):\n    \"\"\"read data and spilt it train set and test set evenly\n    :param data_root: data root\n    :return:dataset and view number\n    \"\"\"\n\n    trn_path = os.path.join(data_root, 'trn.npz')\n    val_path = os.path.join(data_root, 'val.npz')\n    tst_path = os.path.join(data_root, 'tst.npz')\n    \n    ### read train data\n    X_train = []\n    names_train = np.load(trn_path)['name']\n    labels_train = np.load(trn_path)['label']\n    audio = np.load(trn_path)['audio']\n    video = np.load(trn_path)['video']\n    text = np.load(trn_path)['text']\n    X_train.append(audio)\n    X_train.append(video)\n    X_train.append(text)\n\n\n    ### read train data\n    X_val = []\n    names_val = np.load(val_path)['name']\n    labels_val = np.load(val_path)['label']\n    audio = np.load(val_path)['audio']\n    video = np.load(val_path)['video']\n    text = np.load(val_path)['text']\n    X_val.append(audio)\n    X_val.append(video)\n    X_val.append(text)\n\n\n    ### read test data\n    X_test = []\n    names_test = np.load(tst_path)['name']\n    labels_test = np.load(tst_path)['label']\n    audio = np.load(tst_path)['audio']\n    video = np.load(tst_path)['video']\n    text = np.load(tst_path)['text']\n    X_test.append(audio)\n    X_test.append(video)\n    X_test.append(text)\n\n\n    ## normalize each view\n    if normalize:\n        view_number = 3\n        for v_num in range(view_number):\n            X_train[v_num] = Normalize(X_train[v_num]) # [2, samplenum, dim]\n            X_val[v_num] = Normalize(X_val[v_num]) # [2, samplenum, dim]\n            X_test[v_num] = Normalize(X_test[v_num])   # [2, samplenum, dim]\n\n    return X_train, labels_train, X_val, labels_val, X_test, labels_test, names_test\n\n\ndef get_sn(view_num, alldata_len, missing_rate):\n    \"\"\"Randomly generate incomplete data information, simulate partial view data with complete view data\n    :param view_num:view number\n    :param alldata_len:number of samples\n    :param missing_rate:Defined in section 3.2 of the paper\n    :return:Sn\n    \"\"\"\n    one_rate = 1-missing_rate      # missing_rate: 0.8; one_rate: 0.2\n\n    if one_rate <= (1 / view_num): # \n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # only select one view [avoid all zero input]\n        return view_preserve # [samplenum, viewnum=2] => one value set=1, others=0\n\n    if one_rate == 1:\n        matrix = randint(1, 2, size=(alldata_len, view_num)) # [samplenum, viewnum=2] => all ones\n        return matrix\n\n    ## for one_rate between [1 / view_num, 1] => can have multi view input\n    ## ensure at least one of them is avaliable \n    ## since some sample is overlapped, which increase difficulties\n    error = 1\n    while error >= 0.005:\n\n        ## gain initial view_preserve\n        enc = OneHotEncoder(categories=[np.arange(view_num)])\n        view_preserve = enc.fit_transform(randint(0, view_num, size=(alldata_len, 1))).toarray() # [samplenum, viewnum=2] => one value set=1, others=0\n\n        ## further generate one_num samples\n        one_num = view_num * alldata_len * one_rate - alldata_len  # left one_num after previous step\n        ratio = one_num / (view_num * alldata_len)                 # now processed ratio\n        # print (f'first ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int) # based on ratio => matrix_iter\n        a = np.sum(((matrix_iter + view_preserve) > 1).astype(np.int)) # a: overlap number\n        one_num_iter = one_num / (1 - a / one_num)\n        ratio = one_num_iter / (view_num * alldata_len)\n        # print (f'second ratio: {ratio}')\n        matrix_iter = (randint(0, 100, size=(alldata_len, view_num)) < int(ratio * 100)).astype(np.int)\n        matrix = ((matrix_iter + view_preserve) > 0).astype(np.int)\n        ratio = np.sum(matrix) / (view_num * alldata_len)\n        # print (f'third ratio: {ratio}')\n        error = abs(one_rate - ratio)\n        \n    return matrix\n\n\ndef padmeanV1(X_train, Sn_train, X_valid, Sn_valid, X_test, Sn_test):\n    \n    # gain number\n    trainNum = len(X_train)\n    validNum = len(X_valid)\n    testNum = len(X_test)\n\n    # (X, Sn) -> 计算可见样本的均值\n    # X: [samplenum, dim]\n    # Sn: [samplenum, ]\n    X = np.concatenate([X_train, X_valid, X_test], axis=0)\n    Sn = np.concatenate([Sn_train, Sn_valid, Sn_test], axis=0)\n    meanfeat = np.mean(X[Sn==1], axis=0)\n\n    # (X, Sn, meanfeat) -> Xnew\n    X_new = []\n    for ii in range(len(X)):\n        if Sn[ii] == 0:\n            X_new.append(meanfeat)\n        else:\n            X_new.append(X[ii])\n    X_new = np.array(X_new)\n\n    # split into train, valid, test\n    X_trainNew = X_new[:trainNum, :]\n    X_validNew = X_new[trainNum:trainNum+validNum, :]\n    X_testNew = X_new[trainNum+validNum:, :]\n    assert len(X_testNew) == testNum\n\n    return X_trainNew, X_validNew, X_testNew\n\n\ndef padmeanV2(X_train, Sn_train, y_train, X_valid, Sn_valid, y_valid, X_test, Sn_test, y_test):\n    \n    # gain number\n    trainNum = len(X_train)\n    validNum = len(X_valid)\n    testNum = len(X_test)\n\n    # 可见样本中，每类的均值\n    X = np.concatenate([X_train, X_valid, X_test], axis=0)\n    Sn = np.concatenate([Sn_train, Sn_valid, Sn_test], axis=0)\n    y = np.concatenate([y_train, y_valid, y_test], axis=0)\n    n_classes = max(y) - min(y) + 1\n    cls2feat = {}\n    for ii in range(len(X)):\n        y_sample = y[ii]\n        Sn_sample = Sn[ii]\n        X_sample = X[ii]\n        if y_sample not in cls2feat: cls2feat[y_sample] = []\n        if Sn_sample == 1:\n            cls2feat[y_sample].append(X_sample)\n\n    cls2mean = {}\n    for label in cls2feat:\n        feats = cls2feat[label]\n        meanfeat = np.mean(feats, axis=0)\n        cls2mean[label] = meanfeat\n\n\n    # calculate X_new\n    X_new = []\n    for ii in range(len(X)):\n        y_sample = y[ii]\n        Sn_sample = Sn[ii]\n        X_sample = X[ii]\n        if Sn_sample == 0:\n            X_new.append(cls2mean[y_sample])\n        else:\n            X_new.append(X_sample)\n    X_new = np.array(X_new)\n\n    # split into train, valid, test\n    X_trainNew = X_new[:trainNum, :]\n    X_validNew = X_new[trainNum:trainNum+validNum, :]\n    X_testNew = X_new[trainNum+validNum:, :]\n    assert len(X_testNew) == testNum\n\n    return X_trainNew, X_validNew, X_testNew\n\n\ndef padmeanV3(X_train, Sn_train, X_valid, Sn_valid, X_test, Sn_test):\n    \n    # gain number\n    trainNum = len(X_train)\n    validNum = len(X_valid)\n    testNum = len(X_test)\n\n    # (X, Sn) -> 计算可见样本的均值\n    # X: [samplenum, dim]\n    # Sn: [samplenum, ]\n    X = np.concatenate([X_train, X_valid, X_test], axis=0)\n    Sn = np.concatenate([Sn_train, Sn_valid, Sn_test], axis=0)\n    meanfeat = np.mean(X_train[Sn_train==1], axis=0)\n\n    # (X, Sn, meanfeat) -> Xnew\n    X_new = []\n    for ii in range(len(X)):\n        if Sn[ii] == 0:\n            X_new.append(meanfeat)\n        else:\n            X_new.append(X[ii])\n    X_new = np.array(X_new)\n\n    # split into train, valid, test\n    X_trainNew = X_new[:trainNum, :]\n    X_validNew = X_new[trainNum:trainNum+validNum, :]\n    X_testNew = X_new[trainNum+validNum:, :]\n    assert len(X_testNew) == testNum\n\n    return X_trainNew, X_validNew, X_testNew\n\n\ndef padmeanV4(X_train, Sn_train, y_train, X_valid, Sn_valid, y_valid, X_test, Sn_test, y_test):\n    \n    # gain number\n    trainNum = len(X_train)\n    validNum = len(X_valid)\n    testNum = len(X_test)\n\n    # 可见样本中，每类的均值\n    X = np.concatenate([X_train, X_valid, X_test], axis=0)\n    Sn = np.concatenate([Sn_train, Sn_valid, Sn_test], axis=0)\n    y = np.concatenate([y_train, y_valid, y_test], axis=0)\n    n_classes = max(y) - min(y) + 1\n    cls2feat = {}\n    for ii in range(len(X_train)):\n        y_sample = y_train[ii]\n        Sn_sample = Sn_train[ii]\n        X_sample = X_train[ii]\n        if y_sample not in cls2feat: cls2feat[y_sample] = []\n        if Sn_sample == 1:\n            cls2feat[y_sample].append(X_sample)\n\n    cls2mean = {}\n    for label in cls2feat:\n        feats = cls2feat[label]\n        meanfeat = np.mean(feats, axis=0)\n        cls2mean[label] = meanfeat\n\n\n    # calculate X_new\n    X_new = []\n    for ii in range(len(X)):\n        y_sample = y[ii]\n        Sn_sample = Sn[ii]\n        X_sample = X[ii]\n        if Sn_sample == 0:\n            X_new.append(cls2mean[y_sample])\n        else:\n            X_new.append(X_sample)\n    X_new = np.array(X_new)\n\n    # split into train, valid, test\n    X_trainNew = X_new[:trainNum, :]\n    X_validNew = X_new[trainNum:trainNum+validNum, :]\n    X_testNew = X_new[trainNum+validNum:, :]\n    assert len(X_testNew) == testNum\n\n    return X_trainNew, X_validNew, X_testNew"}
{"type": "source_file", "path": "baseline-mmin/models/lstm_audio_model.py", "content": "\nimport torch\nimport os\nimport json\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.fc_encoder import FcEncoder\n\n\nclass LSTMAudioModel(BaseModel):\n    '''\n    A: DNN\n    V: denseface + LSTM + maxpool\n    L: bert + textcnn\n    '''\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim', type=int, default=130)\n        parser.add_argument('--cls_layers', type=str, default='256,128')\n        parser.add_argument('--hidden_size', type=int, default=256)\n        parser.add_argument('--embd_method', type=str, default='maxpool')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE']\n        self.model_names = ['A', 'C']\n        self.netA = LSTMEncoder(opt.input_dim, opt.hidden_size, embd_method=opt.embd_method)\n        cls_layers = [int(x) for x in opt.cls_layers.split(',')] + [opt.output_dim]\n        self.netC = FcEncoder(opt.hidden_size, cls_layers, dropout=0.3)\n            \n        if self.isTrain:\n            self.criterion_ce = torch.nn.CrossEntropyLoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.998)) # 0.999\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        self.A_feat = input['A_feat'].to(self.device)\n        self.label = input['label'].to(self.device)\n        self.input = input\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        self.feat = self.netA(self.A_feat)\n        self.logits = self.netC(self.feat)\n        self.pred = F.softmax(self.logits, dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.criterion_ce(self.logits, self.label)\n        loss = self.loss_CE\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5.0) # 0.1\n\n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        self.forward()   \n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/models/utt_fusion_model.py", "content": "import torch\nimport os\nimport json\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\n\n\nclass UttFusionModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='visual input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--modality', type=str, help='which modality to use for model')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE']\n        self.modality = opt.modality # 'AVL'\n        self.model_names = ['C']\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(','))) # [128, 128]\n        cls_input_size = opt.embd_size_a * int(\"A\" in self.modality) + \\\n                         opt.embd_size_v * int(\"V\" in self.modality) + \\\n                         opt.embd_size_l * int(\"L\" in self.modality)        # [384]\n\n        ## input dim: concatenation\n        ## layer: cls_layers\n        ## output_dim: output_dim [4]\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n        \n        # acoustic model [frame level]\n        if 'A' in self.modality:\n            self.model_names.append('A')\n            #############################\n            # self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n            self.netA = FcClassifier(opt.input_dim_a, cls_layers, output_dim=opt.embd_size_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n            #############################\n            \n        # lexical model [sentence level]\n        if 'L' in self.modality:\n            self.model_names.append('L')\n            #############################\n            # self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n            self.netL = FcClassifier(opt.input_dim_l, cls_layers, output_dim=opt.embd_size_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n            #############################\n            \n        # visual model [frame level]\n        if 'V' in self.modality:\n            self.model_names.append('V')\n            #############################\n            # self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, embd_method=opt.embd_method_v)\n            self.netV = FcClassifier(opt.input_dim_v, cls_layers, output_dim=opt.embd_size_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n            #############################\n        \n        ## self.model_names: ['C', 'A', 'L', 'V']\n        if self.isTrain:\n            #############################\n            dataset = opt.dataset_mode.split('_')[0]\n            if dataset in ['cmumosi', 'cmumosei']:   self.criterion_ce = torch.nn.MSELoss()\n            if dataset in ['boxoflies', 'iemocapfour', 'iemocapsix']: self.criterion_ce = torch.nn.CrossEntropyLoss()\n            #############################\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names] # all parameters\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim # classes: 4\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        if 'A' in self.modality:\n            self.acoustic = input['A_feat'].float().to(self.device)\n        if 'L' in self.modality:\n            self.lexical = input['L_feat'].float().to(self.device)\n        if 'V' in self.modality:\n            self.visual = input['V_feat'].float().to(self.device)\n        \n        self.label = input['label'].to(self.device)\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        final_embd = []\n        if 'A' in self.modality:\n            self.feat_A, _ = self.netA(self.acoustic) # self.acoustic: [256, 155, 130]\n            final_embd.append(self.feat_A)\n\n        if 'L' in self.modality:\n            self.feat_L, _ = self.netL(self.lexical)\n            final_embd.append(self.feat_L)\n        \n        if 'V' in self.modality:\n            self.feat_V, _ = self.netV(self.visual)\n            final_embd.append(self.feat_V)\n        \n        # get model outputs\n        self.feat = torch.cat(final_embd, dim=-1) # [batch, dim*3]\n        self.logits, self.ef_fusion_feat = self.netC(self.feat)\n        #############################\n        # self.pred = F.softmax(self.logits, dim=-1)\n        self.logits = self.logits.squeeze()\n        self.pred = self.logits\n        #############################\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.criterion_ce(self.logits, self.label)\n        loss = self.loss_CE\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5.0) # 0.1\n\n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-cca/dcca.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nimport os\nimport cv2\nimport glob\nimport time\nimport tqdm\nfrom functools import partial\n\nfrom apex import amp\nimport matplotlib.pyplot as plt\n\nimport gzip\nimport random\n\nimport os\nfrom sklearn import svm\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport numpy as np\nfrom numpy.random import randint\n\nfrom util import *\n\nimport sys\nsys.path.append('../')\nimport config\n\n## input two hidden and calculate loss \ndef loss(H1, H2, H3, outdim_size, use_all_singular_values=False):\n\n    loss1 = pairloss(H1, H2, outdim_size, use_all_singular_values)\n    loss2 = pairloss(H1, H3, outdim_size, use_all_singular_values)\n    loss3 = pairloss(H2, H3, outdim_size, use_all_singular_values)\n\n    return loss1 + loss2 + loss3\n\n\ndef pairloss(H1, H2, outdim_size, use_all_singular_values=False):\n    r1 = 1e-3\n    r2 = 1e-3\n    eps = 1e-9\n\n    H1, H2 = H1.t(), H2.t()\n\n    o1 = o2 = H1.size(0)\n\n    m = H1.size(1)\n\n    H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n    H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n\n    SigmaHat12 = (1.0 / (m - 1)) * torch.matmul(H1bar, H2bar.t())\n    SigmaHat11 = (1.0 / (m - 1)) * torch.matmul(H1bar, H1bar.t()) + r1 * torch.eye(o1, device=H1.device)\n    SigmaHat22 = (1.0 / (m - 1)) * torch.matmul(H2bar, H2bar.t()) + r2 * torch.eye(o2, device=H1.device)\n\n    # Calculating the root inverse of covariance matrices by using eigen decomposition\n    [D1, V1] = torch.symeig(SigmaHat11, eigenvectors=True)\n    [D2, V2] = torch.symeig(SigmaHat22, eigenvectors=True)\n\n    # Added to increase stability\n    posInd1 = torch.gt(D1, eps).nonzero()[:, 0]\n    D1 = D1[posInd1]\n    V1 = V1[:, posInd1]\n    posInd2 = torch.gt(D2, eps).nonzero()[:, 0]\n    D2 = D2[posInd2]\n    V2 = V2[:, posInd2]\n\n    SigmaHat11RootInv = torch.matmul(torch.matmul(V1, torch.diag(D1 ** -0.5)), V1.t())\n    SigmaHat22RootInv = torch.matmul(torch.matmul(V2, torch.diag(D2 ** -0.5)), V2.t())\n\n    Tval = torch.matmul(torch.matmul(SigmaHat11RootInv, SigmaHat12), SigmaHat22RootInv)\n\n    if use_all_singular_values:\n        # all singular values are used to calculate the correlation\n        tmp = torch.matmul(Tval.t(), Tval)\n        corr = torch.trace(torch.sqrt(tmp))\n        # assert torch.isnan(corr).item() == 0\n    else:\n        # just the top outdim_size singular values are used\n        trace_TT = torch.matmul(Tval.t(), Tval)\n        trace_TT = torch.add(trace_TT, (torch.eye(trace_TT.shape[0])*r1).to(H1.device)) # regularization for more stability\n        U, V = torch.symeig(trace_TT, eigenvectors=True)\n        U = torch.where(U>eps, U, (torch.ones(U.shape)*eps).to(H1.device))\n        U = U.topk(outdim_size)[0]\n        corr = torch.sum(torch.sqrt(U))\n\n    return -corr\n\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, chan):\n        super().__init__()\n        layers = nn.ModuleList([])\n        cin = chan[0]\n        for cout in chan[1:-1]:\n            layers.append(nn.Linear(cin, cout))\n            layers.append(nn.BatchNorm1d(cout))\n            layers.append(nn.ReLU(True))\n            cin = cout\n        layers.append(nn.Linear(cin, chan[-1]))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass DeepCCA(nn.Module):\n    def __init__(self, chan1, chan2, chan3):\n        super().__init__()\n\n        self.model1 = MLP(chan1)\n        self.model2 = MLP(chan2)\n        self.model3 = MLP(chan3)\n        \n    def forward(self, x1, x2, x3):\n        # feature * batch_size\n        z1 = self.model1(x1)\n        z2 = self.model2(x2)\n        z3 = self.model2(x3)\n\n        return z1, z2, z3\n\n\n\n\n## read data\nclass NoisyMNIST(Dataset):\n    def __init__(self, X1, X2, X3, y):\n        self.X1 = X1\n        self.X2 = X2\n        self.X3 = X3\n        self.y = y\n    \n    def __getitem__(self, i):\n        data = {\n            'x1': self.X1[i],\n            'x2': self.X2[i],\n            'x3': self.X3[i],\n            'y': self.y[i]\n        }\n\n        return data\n    \n    def __len__(self):\n        #return 100\n        return self.X1.shape[0]\n\n\n\nclass Trainer(object):\n    def __init__(self, \n                 name, # name of this experiment\n                 model, # network \n                 objective, # loss function\n                 optimizer=None, # optimizer\n                 lr_scheduler=None, # scheduler\n                 metrics=[], # metrics for evaluation\n                 local_rank=0, # which GPU am I\n                 world_size=1, # total num of GPUs\n                 device=None, # device to use, usually setting to None is OK. (auto choose device)\n                 mute=False, # whether to mute all print\n                 opt_level='O0', # amp optimize level\n                 eval_interval=1, # eval once every $ epoch\n                 max_keep_ckpt=3, # max num of saved ckpts in disk\n                 workspace='workspace', # workspace to save logs & ckpts\n                 best_mode='min', # the smaller/larger result, the better\n                 use_checkpoint=\"latest\", # which ckpt to use at init time\n                 scheduler_update_every_step=False, # whether to call scheduler.step() after every train step\n                 ):\n        \n        self.name = name\n        self.mute = mute\n        self.model = model\n        self.objective = objective\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.metrics = metrics\n        self.local_rank = local_rank\n        self.world_size = world_size\n        self.workspace = workspace\n        self.opt_level = opt_level\n        self.best_mode = best_mode\n        self.max_keep_ckpt = max_keep_ckpt\n        self.eval_interval = eval_interval\n        self.use_checkpoint = use_checkpoint\n        self.time_stamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.scheduler_update_every_step = scheduler_update_every_step\n        self.device = device if device is not None else torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n\n        self.model.to(self.device)\n        if isinstance(self.objective, nn.Module):\n            self.objective.to(self.device)\n\n        if optimizer is None:\n            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=5e-4) # naive adam\n\n        if lr_scheduler is None:\n            self.lr_scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda epoch: 1) # fake scheduler\n\n        self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level=self.opt_level, verbosity=0)\n\n        # variable init\n        self.epoch = 1\n        self.global_step = 0\n        self.local_step = 0\n        self.stats = {\n            \"loss\": [],\n            \"valid_loss\": [],\n            \"results\": [], # metrics[0], or valid_loss\n            \"checkpoints\": [], # record path of saved ckpt, to automatically remove old ckpt\n            \"best_result\": None,\n            }\n\n        # auto fix\n        if len(metrics) == 0:\n            self.best_mode = 'min'\n\n        # workspace prepare\n        self.log_ptr = None\n        if self.workspace is not None:\n            os.makedirs(self.workspace, exist_ok=True)        \n            self.log_path = os.path.join(workspace, \"log.txt\")\n            self.log_ptr = open(self.log_path, \"a+\")\n\n            self.ckpt_path = os.path.join(self.workspace, 'checkpoints')\n            self.best_path = f\"{self.ckpt_path}/{self.name}_best.pth.tar\"\n            os.makedirs(self.ckpt_path, exist_ok=True)\n            \n        self.log(f'[INFO] Trainer: {self.name} | {self.time_stamp} | {self.device} | {self.workspace}')\n        self.log(f'[INFO] #parameters: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n\n        if self.workspace is not None:\n            if self.use_checkpoint == \"scratch\":\n                self.log(\"[INFO] Train from scratch\")\n            elif self.use_checkpoint == \"latest\":\n                self.log(\"[INFO] Loading latest checkpoint ...\")\n                self.load_checkpoint()\n            elif self.use_checkpoint == \"best\":\n                if os.path.exists(self.best_path):\n                    self.log(\"[INFO] Loading best checkpoint ...\")\n                    self.load_checkpoint(self.best_path)\n                else:\n                    self.log(\"[INFO] Best checkpoint not found, loading latest ...\")\n                    self.load_checkpoint()\n            else: # path to ckpt\n                self.log(f\"[INFO] Loading {self.use_checkpoint} ...\")\n                self.load_checkpoint(self.use_checkpoint)\n\n        # extra\n        self.Z1 = []\n        self.Z2 = []\n        self.Z3 = []\n\n    def __del__(self):\n        if self.log_ptr: \n            self.log_ptr.close()\n\n    def log(self, *args):\n        if not self.mute: \n            print(*args)\n        if self.log_ptr: \n            print(*args, file=self.log_ptr)    \n\n\n    ## 将不同类型的数据转成tensor向量\n    def prepare_data(self, data):\n        if isinstance(data, list):\n            for i, v in enumerate(data):\n                if isinstance(v, np.ndarray):\n                    data[i] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[i] = v.to(self.device)\n        elif isinstance(data, dict):\n            for k, v in data.items():\n                if isinstance(v, np.ndarray):\n                    data[k] = torch.from_numpy(v).to(self.device)\n                if torch.is_tensor(v):\n                    data[k] = v.to(self.device)\n        elif isinstance(data, np.ndarray):\n            data = torch.from_numpy(data).to(self.device)\n        else: # is_tensor, or other similar objects that has `to`\n            data = data.to(self.device)\n        return data\n\n\n    ### ------------------------------  \n\n    def train_step(self, data):\n        x1, x2, x3 = data['x1'], data['x2'], data['x3']\n        z1, z2, z3 = self.model(x1, x2, x3)\n        loss = self.objective(z1, z2, z3)\n        return (z1, z2, z3), None, loss\n\n    def eval_step(self, data):\n        (z1, z2, z3), truths, loss = self.train_step(data)\n        self.Z1.append(z1) # [B, fout]\n        self.Z2.append(z2)\n        self.Z3.append(z3)\n        return (z1, z2, z3), truths, loss\n\n    ### ------------------------------\n\n    def train_one_epoch(self, loader):\n\n        total_loss = []\n        self.model.train()\n        self.local_step = 0\n\n        for data in loader:\n            \n            self.local_step += 1\n            self.global_step += 1\n            \n            data = self.prepare_data(data)\n            preds, truths, loss = self.train_step(data)\n\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            if self.scheduler_update_every_step:\n                self.lr_scheduler.step()\n\n            total_loss.append(loss.item())\n\n        average_loss = np.mean(total_loss)\n        self.stats[\"loss\"].append(average_loss)\n\n        if not self.scheduler_update_every_step:\n            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                self.lr_scheduler.step(average_loss)\n            else:\n                self.lr_scheduler.step()\n\n        self.log(f\"==> Finished Epoch {self.epoch}, average_loss={average_loss:.4f}\")\n\n\n    def evaluate_one_epoch(self, loader):\n\n        total_loss = []\n        self.model.eval()\n\n        with torch.no_grad():\n            self.local_step = 0\n            for data in loader:    \n                self.local_step += 1\n                data = self.prepare_data(data)\n                preds, truths, loss = self.eval_step(data)\n                total_loss.append(loss.item())            \n\n        average_loss = np.mean(total_loss)\n\n    ### ------------------------------\n\n    def train(self, train_loader, valid_loader, max_epochs):\n        for epoch in range(self.epoch, max_epochs + 1):\n            self.epoch = epoch\n            self.train_one_epoch(train_loader)\n            if self.workspace is not None:\n                self.save_checkpoint(True if epoch == max_epochs else False) # save full at last epoch\n            if self.epoch % self.eval_interval == 0:\n                self.evaluate_one_epoch(valid_loader)\n\n\n    def evaluate(self, loader):\n        if os.path.exists(self.best_path):\n            self.load_checkpoint(self.best_path)\n        else:\n            self.load_checkpoint()\n        self.Z1 = []\n        self.Z2 = []\n        self.Z3 = []\n        self.evaluate_one_epoch(loader)\n        self.Z1 = torch.cat(self.Z1, dim=0).detach().cpu().numpy()\n        self.Z2 = torch.cat(self.Z2, dim=0).detach().cpu().numpy()\n        self.Z3 = torch.cat(self.Z3, dim=0).detach().cpu().numpy()\n        return self.Z1, self.Z2, self.Z3\n\n    ### ------------------------------\n\n    def save_checkpoint(self, full=False):\n        file_path = f\"{self.ckpt_path}/{self.name}_ep{self.epoch:04d}.pth.tar\"\n\n        self.stats[\"checkpoints\"].append(file_path)\n\n        if len(self.stats[\"checkpoints\"]) > self.max_keep_ckpt:\n            old_ckpt = self.stats[\"checkpoints\"].pop(0)\n            if os.path.exists(old_ckpt):\n                os.remove(old_ckpt)\n\n        state = {\n            'epoch': self.epoch,\n            'stats': self.stats,\n            'model': self.model.state_dict(),\n        }\n\n        if full:\n            state['amp'] = amp.state_dict()\n            state['optimizer'] = self.optimizer.state_dict()\n            state['lr_scheduler'] = self.lr_scheduler.state_dict()\n        \n        torch.save(state, file_path)\n        \n        if len(self.stats[\"results\"]) > 0:\n            if self.stats[\"best_result\"] is None or self.stats[\"results\"][-1] < self.stats[\"best_result\"]:\n                self.stats[\"best_result\"] = self.stats[\"results\"][-1]\n                torch.save(state, self.best_path)\n            \n\n    def load_checkpoint(self, checkpoint=None):\n        if checkpoint is None:\n            checkpoint_list = sorted(glob.glob(f'{self.ckpt_path}/{self.name}_ep*.pth.tar'))\n            if checkpoint_list:\n                checkpoint = checkpoint_list[-1]\n            else:\n                self.log(\"[WARN] No checkpoint found, model randomly initialized.\")\n                return\n\n        checkpoint_dict = torch.load(checkpoint, map_location=self.device)\n\n        self.model.load_state_dict(checkpoint_dict['model'])\n        self.stats = checkpoint_dict['stats']\n        self.epoch = checkpoint_dict['epoch'] + 1\n        if 'optimizer' in checkpoint_dict:\n            self.optimizer.load_state_dict(checkpoint_dict['optimizer'])\n        if 'lr_scheduler' in checkpoint_dict:\n            self.lr_scheduler.load_state_dict(checkpoint_dict['lr_scheduler'])\n        if 'amp' in checkpoint_dict:\n            amp.load_state_dict(checkpoint_dict['amp'])\n\n        self.log(\"[INFO] Checkpoint Loaded Successfully.\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--n-components', type=int, default=10, help='number of cca component')\n    parser.add_argument('--n-hidden', type=int, default=1024, help='number of hidden in DCCA')\n    parser.add_argument('--max-epoch', type=int, default=20, help='max epoch in training')\n    parser.add_argument('--missing-rate', type=float, default=0.0, help='view missing rate [default: 0]')\n    parser.add_argument('--normalize', action='store_true', default=False, help='whether normalize input features')\n    parser.add_argument('--dataset', type=str, default='cmumosi', help='input dataset')\n    parser.add_argument('--test_mask', type=str, default=None, help='test under same mask for fair comparision')\n    args = parser.parse_args()\n    print (args)\n\n    print (f'========= starting ============')\n    folder_acc = []\n    folder_f1 = []\n    folder_save = []\n    if args.dataset in ['cmumosi', 'cmumosei']:\n        num_folder = 1\n    elif args.dataset == 'iemocapfour':\n        num_folder = 5\n    elif args.dataset == 'iemocapsix':\n        num_folder = 5\n\n    for index in range(num_folder):\n        print (f'>>>>> Cross-validation: training on the {index+1} folder >>>>>')\n\n        print (f' >>>>> read data >>>>> ')\n        X_train, y_train, X_valid, y_valid, X_test, y_test, name_test = read_cmumosi_data(f'../baseline-cpmnet/data/{args.dataset}/{index+1}', args.normalize)\n        trainNum = len(y_train)\n        validNum = len(y_valid)\n        testNum = len(y_test)\n        print (f'train number: {trainNum};   valid number: {validNum};   test number: {testNum}')\n        X0_train, X1_train, X2_train = X_train\n        X0_valid, X1_valid, X2_valid = X_valid\n        X0_test, X1_test, X2_test = X_test\n        view0Dim = len(X0_train[0])\n        view1Dim = len(X1_train[0])\n        view2Dim = len(X2_train[0])\n        print (f'view 0: {view0Dim};  view 1: {view1Dim};  view 2: {view2Dim}')\n\n        # load random mask\n        samplenum = trainNum + validNum + testNum\n        Sn = get_sn(3, samplenum, args.missing_rate) # [samplenum, 3]\n        Sn_train = Sn[np.arange(trainNum)]\n        Sn_valid = Sn[np.arange(validNum) + trainNum]\n        Sn_test =  Sn[np.arange(testNum) + trainNum + validNum]\n        ######################################################\n        if args.test_mask != None:\n            print (f'using predefined mask!!')\n            name2mask = np.load(args.test_mask, allow_pickle=True)['name2mask'].tolist()\n            Sn_test = []\n            for name in name_test:\n                mask = name2mask[name] # (A, L, V)\n                mask = [mask[0], mask[2], mask[1]] # (A, V, L)\n                Sn_test.append(mask)\n            Sn_test = np.array(Sn_test) # [sample_num, 3]\n        else:\n            print (f'using random initialized mask!!')\n        ######################################################\n\n        # pad features in masked part\n        X0_train, X0_valid, X0_test = padmeanV1(X0_train, Sn_train[:,0], X0_valid, Sn_valid[:,0], X0_test, Sn_test[:,0])\n        X1_train, X1_valid, X1_test = padmeanV1(X1_train, Sn_train[:,1], X1_valid, Sn_valid[:,1], X1_test, Sn_test[:,1])\n        X2_train, X2_valid, X2_test = padmeanV1(X2_train, Sn_train[:,2], X2_valid, Sn_valid[:,2], X2_test, Sn_test[:,2])\n\n\n        print (f' >>>>> build model >>>>> ')\n        # model [for two view inputs]\n        model = DeepCCA([view0Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components], \n                        [view1Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components],\n                        [view2Dim, args.n_hidden, args.n_hidden, args.n_hidden, args.n_components]\n                        )\n\n        # loss function\n        objective = partial(loss, outdim_size=args.n_components, use_all_singular_values=False)\n\n        # data loader\n        train_dataset = NoisyMNIST(X0_train, X1_train, X2_train, y_train)\n        valid_dataset = NoisyMNIST(X0_valid, X1_valid, X2_valid, y_valid)\n        test_dataset = NoisyMNIST(X0_test, X1_test, X2_test, y_test)\n\n        batch_size = 1024\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=False)\n        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n        # trainer\n        trainer = Trainer(f'DCCA_{args.dataset}', model, objective, optimizer=optimizer, use_checkpoint='scratch')\n        trainer.train(train_loader, valid_loader, args.max_epoch)\n        train_loader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False) # ordered with y_train\n        Z1_train, Z2_train, Z3_train = trainer.evaluate(train_loader2)\n        Z1_valid, Z2_valid, Z3_valid = trainer.evaluate(valid_loader)\n        Z1_test, Z2_test, Z3_test = trainer.evaluate(test_loader)\n\n        Z_train = np.concatenate([Z1_train, Z2_train, Z3_train], axis=1) # [samplenum, dim]\n        Z_valid = np.concatenate([Z1_valid, Z2_valid, Z3_valid], axis=1) # [samplenum, dim]\n        Z_test  = np.concatenate([Z1_test, Z2_test, Z3_test], axis=1)  # [samplenum, dim]\n\n        # SVM classify\n        print (f' >>>>> training classifier >>>>> ')\n        clf = svm.LinearSVC(C=0.01, dual=False)\n        clf.fit(Z_train, y_train)\n        total_pred = clf.predict(Z_test)\n        total_label = y_test\n        f1 = f1_score(total_label, total_pred, average='weighted')\n        acc = accuracy_score(total_label, total_pred)\n\n        folder_acc.append(acc)\n        folder_f1.append(f1)\n        folder_save.append({'test_labels': total_label, 'test_preds': total_pred, 'test_hiddens': Z_test, 'test_names': name_test})\n\n        print (f'>>>>> Finish: training on the {index+1} folder >>>>>')\n\n\n    ## save results\n    suffix_name = f'{args.dataset}_dcca_mask:{args.missing_rate:.1f}'\n\n    mean_f1 = np.mean(np.array(folder_f1))\n    mean_acc = np.mean(np.array(folder_acc))\n    res_name = f'f1:{mean_f1:2.2%}_acc:{mean_acc:2.2%}'\n\n    save_root = config.MODEL_DIR\n    if not os.path.exists(save_root): os.makedirs(save_root)\n    save_path = f'{save_root}/{suffix_name}_{res_name}_{time.time()}.npz'\n    np.savez_compressed(save_path,\n                        args=np.array(args, dtype=object),\n                        folder_save=np.array(folder_save, dtype=object) # save non-structure type\n                        )\n    print (f' =========== finish =========== ')\n    "}
{"type": "source_file", "path": "baseline-mmin/models/mmin_no_cycle_model.py", "content": "\nimport torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import ResidualAE\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINNoCycleModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--n_blocks', type=int, default=3, help='number of AE blocks')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        parser.add_argument('--cycle_weight', type=float, default=1.0, help='weight of cycle loss')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        # our expriment is on 10 fold setting, teacher is on 5 fold setting, the train set should match\n        self.loss_names = ['CE', 'mse']\n        self.model_names = ['A', 'V', 'L', 'C', 'AE']\n        \n        # acoustic model\n        self.netA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)\n        # lexical model\n        self.netL = TextCNN(opt.input_dim_l, opt.embd_size_l)\n        # visual model\n        self.netV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, opt.embd_method_v)\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        self.netAE = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        self.netAE_cycle = ResidualAE(AE_layers, opt.n_blocks, AE_input_dim, dropout=0, use_bn=False)\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        cls_input_size = AE_layers[-1] * opt.n_blocks\n        self.netC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        if self.isTrain:\n            self.load_pretrained_encoder(opt)\n            self.criterion_ce = torch.nn.CrossEntropyLoss()\n            self.criterion_mse = torch.nn.MSELoss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n            self.cycle_weight = opt.cycle_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n    def load_pretrained_encoder(self, opt):\n        print('Init parameter from {}'.format(opt.pretrained_path))\n        pretrained_path = os.path.join(opt.pretrained_path, str(opt.cvNo))\n        pretrained_config_path = os.path.join(opt.pretrained_path, 'train_opt.conf')\n        pretrained_config = self.load_from_opt_record(pretrained_config_path)\n        pretrained_config.isTrain = False                             # teacher model should be in test mode\n        pretrained_config.gpu_ids = opt.gpu_ids                       # set gpu to the same\n        self.pretrained_encoder = UttFusionModel(pretrained_config)\n        self.pretrained_encoder.load_networks_cv(pretrained_path)\n        self.pretrained_encoder.cuda()\n        self.pretrained_encoder.eval()\n    \n    def post_process(self):\n        # called after model.setup()\n        def transform_key_for_parallel(state_dict):\n            return OrderedDict([('module.'+key, value) for key, value in state_dict.items()])\n        \n        print('[ Init ] Load parameters from pretrained encoder network')\n        f = lambda x: transform_key_for_parallel(x)\n        self.netA.load_state_dict(f(self.pretrained_encoder.netA.state_dict()))\n        self.netV.load_state_dict(f(self.pretrained_encoder.netV.state_dict()))\n        self.netL.load_state_dict(f(self.pretrained_encoder.netL.state_dict()))\n    \n    def load_from_opt_record(self, file_path):\n        opt_content = json.load(open(file_path, 'r'))\n        opt = OptConfig()\n        opt.load(opt_content)\n        return opt\n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        self.acoustic = input['A_feat'].float().to(self.device)\n        self.lexical = input['L_feat'].float().to(self.device)\n        self.visual = input['V_feat'].float().to(self.device)\n        self.missing_index = input['missing_index'].long().to(self.device)\n        self.label = input['label'].to(self.device)\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        # A modality\n        self.A_miss_index = self.missing_index[:, 0].unsqueeze(1).unsqueeze(2)\n        self.feat_A_miss = self.netA(self.acoustic * self.A_miss_index)\n        # L modality\n        self.L_miss_index = self.missing_index[:, 2].unsqueeze(1).unsqueeze(2)\n        self.feat_L_miss = self.netL(self.lexical * self.L_miss_index)\n        # V modality\n        self.V_miss_index = self.missing_index[:, 1].unsqueeze(1).unsqueeze(2)\n        self.feat_V_miss = self.netV(self.visual * self.V_miss_index)\n        # fusion miss\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        # calc reconstruction of teacher's output\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        # get fusion outputs for missing modality\n        self.logits, _ = self.netC(self.latent)\n        self.pred = F.softmax(self.logits, dim=-1)\n        # for training \n        if self.isTrain:\n            with torch.no_grad():\n                self.T_embd_A = self.pretrained_encoder.netA(self.acoustic)\n                self.T_embd_L = self.pretrained_encoder.netL(self.lexical)\n                self.T_embd_V = self.pretrained_encoder.netV(self.visual)\n                self.T_embds = torch.cat([self.T_embd_A, self.T_embd_L, self.T_embd_V], dim=-1)\n        \n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss_CE = self.ce_weight * self.criterion_ce(self.logits, self.label)\n        self.loss_mse = self.mse_weight * self.criterion_mse(self.T_embds, self.recon_fusion)\n        loss = self.loss_CE + self.loss_mse\n        loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n            \n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/models/networks/textcnn.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TextCNN(nn.Module):\n    def __init__(self, input_dim, emb_size=128, in_channels=1, out_channels=128, kernel_heights=[3,4,5], dropout=0.5):\n        super().__init__()\n        '''\n        cat((conv1-relu+conv2-relu+conv3-relu)+maxpool) + dropout, and to trans\n        '''\n        self.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], input_dim), stride=1, padding=0)\n        self.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], input_dim), stride=1, padding=0)\n        self.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], input_dim), stride=1, padding=0)\n        self.dropout = nn.Dropout(dropout)\n        self.embd = nn.Sequential(\n            nn.Linear(len(kernel_heights)*out_channels, emb_size),\n            nn.ReLU(inplace=True),\n        )\n\n    def conv_block(self, input, conv_layer):\n        conv_out = conv_layer(input)# conv_out.size() = (batch_size, out_channels, dim, 1)\n        activation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2) # maxpool_out.size() = (batch_size, out_channels)\n        return max_out\n\n    def forward(self, frame_x):\n        batch_size, seq_len, feat_dim = frame_x.size()\n        frame_x = frame_x.view(batch_size, 1, seq_len, feat_dim)\n        max_out1 = self.conv_block(frame_x, self.conv1)\n        max_out2 = self.conv_block(frame_x, self.conv2)\n        max_out3 = self.conv_block(frame_x, self.conv3)\n        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n        fc_in = self.dropout(all_out)\n        embd = self.embd(fc_in)\n        return embd"}
{"type": "source_file", "path": "baseline-mmin/models/networks/__init__.py", "content": "''' Contains network files. '''"}
{"type": "source_file", "path": "baseline-mmin/models/mmin_AE_model.py", "content": "import torch\nimport os\nimport json\nfrom collections import OrderedDict\nimport torch.nn.functional as F\nfrom models.base_model import BaseModel\nfrom models.networks.fc import FcEncoder\nfrom models.networks.lstm import LSTMEncoder\nfrom models.networks.textcnn import TextCNN\nfrom models.networks.classifier import FcClassifier\nfrom models.networks.autoencoder import SimpleFcAE\nfrom models.utt_fusion_model import UttFusionModel\nfrom .utils.config import OptConfig\n\n\nclass MMINAEModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--input_dim_a', type=int, default=130, help='acoustic input dim')\n        parser.add_argument('--input_dim_l', type=int, default=1024, help='lexical input dim')\n        parser.add_argument('--input_dim_v', type=int, default=384, help='lexical input dim')\n        parser.add_argument('--embd_size_a', default=128, type=int, help='audio model embedding size')\n        parser.add_argument('--embd_size_l', default=128, type=int, help='text model embedding size')\n        parser.add_argument('--embd_size_v', default=128, type=int, help='visual model embedding size')\n        parser.add_argument('--embd_method_a', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='audio embedding method,last,mean or atten')\n        parser.add_argument('--embd_method_v', default='maxpool', type=str, choices=['last', 'maxpool', 'attention'], \\\n            help='visual embedding method,last,mean or atten')\n        parser.add_argument('--AE_layers', type=str, default='128,64,32', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--cls_layers', type=str, default='128,128', help='256,128 for 2 layers with 256, 128 nodes respectively')\n        parser.add_argument('--dropout_rate', type=float, default=0.3, help='rate of dropout')\n        parser.add_argument('--bn', action='store_true', help='if specified, use bn layers in FC')\n        parser.add_argument('--pretrained_path', type=str, help='where to load pretrained encoder network')\n        parser.add_argument('--ce_weight', type=float, default=1.0, help='weight of ce loss')\n        parser.add_argument('--mse_weight', type=float, default=1.0, help='weight of mse loss')\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the LSTM autoencoder class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        super().__init__(opt)\n        self.dataset = opt.dataset_mode.split('_')[0]\n        self.loss_names = ['ce', 'recon']\n        self.model_names = ['A', 'AA', 'V', 'VV', 'L', 'LL', 'C', 'AE']\n        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))\n        AE_input_dim = opt.embd_size_a + opt.embd_size_v + opt.embd_size_l\n        \n        # acoustic model\n        self.netA = FcClassifier(opt.input_dim_a, cls_layers, output_dim=opt.embd_size_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netAA = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_a, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # lexical model\n        self.netL = FcClassifier(opt.input_dim_l, cls_layers, output_dim=opt.embd_size_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netLL = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_l, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # visual model\n        self.netV = FcClassifier(opt.input_dim_v, cls_layers, output_dim=opt.embd_size_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n        self.netVV = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.input_dim_v, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        # AE model\n        AE_layers = list(map(lambda x: int(x), opt.AE_layers.split(',')))\n        self.netAE = SimpleFcAE(AE_layers, AE_input_dim, dropout=0, use_bn=False)\n        # cls_input_size = AE_layers[-1]\n        self.netC = FcClassifier(AE_input_dim, cls_layers, output_dim=opt.output_dim, dropout=opt.dropout_rate, use_bn=opt.bn)\n\n        if self.isTrain:\n            paremeters = [{'params': getattr(self, 'net'+net).parameters()} for net in self.model_names]\n            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer)\n            self.output_dim = opt.output_dim\n            self.ce_weight = opt.ce_weight\n            self.mse_weight = opt.mse_weight\n\n        # modify save_dir\n        self.save_dir = os.path.join(self.save_dir, str(opt.cvNo))\n        if not os.path.exists(self.save_dir):\n            os.mkdir(self.save_dir)\n    \n\n    def set_input(self, input):\n        \"\"\"\n        Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n        \"\"\"\n        acoustic = input['A_feat'].float().to(self.device)\n        lexical = input['L_feat'].float().to(self.device)\n        visual = input['V_feat'].float().to(self.device)\n        self.label = input['label'].to(self.device)\n        self.missing_index = input['missing_index'].long().to(self.device)\n        # A modality\n        self.A_miss_index = self.missing_index[:, 0].unsqueeze(1)\n        self.A_miss = acoustic * self.A_miss_index               # 1 (exist), 0 (miss)\n        self.A_reverse = acoustic * -1 * (self.A_miss_index - 1) # 0, 1\n        self.A_full = acoustic\n        # L modality\n        self.L_miss_index = self.missing_index[:, 2].unsqueeze(1)\n        self.L_miss = lexical * self.L_miss_index\n        self.L_reverse = lexical * -1 * (self.L_miss_index - 1)\n        self.L_full = lexical\n        # V modality\n        self.V_miss_index = self.missing_index[:, 1].unsqueeze(1)\n        self.V_miss = visual * self.V_miss_index\n        self.V_reverse = visual * -1 * (self.V_miss_index - 1)\n        self.V_full = visual\n\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n\n        ## recon\n        self.feat_A_miss, _ = self.netA(self.A_miss)\n        self.feat_L_miss, _ = self.netL(self.L_miss)\n        self.feat_V_miss, _ = self.netV(self.V_miss)\n        self.feat_fusion_miss = torch.cat([self.feat_A_miss, self.feat_L_miss, self.feat_V_miss], dim=-1)\n        self.recon_fusion, self.latent = self.netAE(self.feat_fusion_miss)\n        self.A_rec, _ = self.netAA(self.recon_fusion)\n        self.L_rec, _ = self.netLL(self.recon_fusion)\n        self.V_rec, _ = self.netVV(self.recon_fusion)\n\n        ## classifier\n        self.hiddens = self.recon_fusion\n        self.logits, _ = self.netC(self.recon_fusion)\n        self.logits = self.logits.squeeze()\n        self.pred = self.logits\n\n        ## calculate cls loss\n        if self.dataset in ['cmumosi', 'cmumosei']:                    criterion_ce = torch.nn.MSELoss()\n        if self.dataset in ['boxoflies', 'iemocapfour', 'iemocapsix']: criterion_ce = torch.nn.CrossEntropyLoss()\n        self.loss_ce = criterion_ce(self.logits, self.label)\n        ## calculate recon loss [if miss, the calculate recon loss; if exist, no recon loss]\n        recon_loss = torch.nn.MSELoss(reduction='none')\n        loss_recon1 = recon_loss(self.A_rec, self.A_full) * -1 * (self.A_miss_index - 1) # 1 (exist), 0 (miss)  [batch, featdim]\n        loss_recon2 = recon_loss(self.L_rec, self.L_full) * -1 * (self.L_miss_index - 1) # 1 (exist), 0 (miss)\n        loss_recon3 = recon_loss(self.V_rec, self.V_full) * -1 * (self.V_miss_index - 1) # 1 (exist), 0 (miss)\n        loss_recon1 = torch.sum(loss_recon1) / self.A_full.shape[1]                   # each dimension delta\n        loss_recon2 = torch.sum(loss_recon2) / self.L_full.shape[1]                   # each dimension delta\n        loss_recon3 = torch.sum(loss_recon3) / self.V_full.shape[1]                   # each dimension delta\n        self.loss_recon = loss_recon1 + loss_recon2 + loss_recon3\n        ## merge all loss\n        self.loss = self.ce_weight * self.loss_ce + self.mse_weight * self.loss_recon\n       \n\n    def backward(self):\n        \"\"\"Calculate the loss for back propagation\"\"\"\n        self.loss.backward()\n        for model in self.model_names:\n            torch.nn.utils.clip_grad_norm_(getattr(self, 'net'+model).parameters(), 5)\n\n    def optimize_parameters(self, epoch):\n        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n        # forward\n        self.forward()   \n        # backward\n        self.optimizer.zero_grad()  \n        self.backward()            \n        self.optimizer.step() \n"}
{"type": "source_file", "path": "baseline-mmin/data/cmumosei_multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass CMUMOSEIMultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'CMUMOSEI_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        # self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = False ## for utterance level features\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        ###############\n        # label = torch.tensor(self.label[index])\n        label = torch.tensor(self.label[index]).float()\n        ###############\n        # process A_feat\n        A_feat = torch.FloatTensor(self.all_A[index])\n        # process V_feat \n        V_feat = torch.FloatTensor(self.all_V[index])\n        # proveee L_feat\n        L_feat = torch.FloatTensor(self.all_L[index])\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert_large\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MSPMultimodalDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-cpmnet/util/model.py", "content": "import util.classfiy as classfiy\nimport tensorflow as tf\nimport numpy as np\nfrom numpy.random import shuffle\nfrom util.util import xavier_init\n\n\nclass CPMNets():\n    \"\"\"build model\n    \"\"\"\n    def __init__(self, view_num, trainLen, testLen, layer_size, lsd_dim=128, learning_rate=[0.001, 0.001], lamb=1):\n        \"\"\"\n        :param learning_rate:learning rate of network and h\n        :param view_num:view number\n        :param layer_size:node of each net\n        :param lsd_dim:latent space dimensionality\n        :param trainLen:training dataset samples\n        :param testLen:testing dataset samples\n        \"\"\"\n        # initialize parameter\n        self.view_num = view_num      # 2\n        self.layer_size = layer_size  # [[4096], [4096]]\n        self.lsd_dim = lsd_dim        # 512\n        self.trainLen = trainLen      # train sample\n        self.testLen = testLen        # test sample\n        self.lamb = lamb              # 10\n\n        # initialize latent space data\n        self.h_train, self.h_train_update = self.H_init('train') # [samplenum, lsd_dim]\n        self.h_test, self.h_test_update = self.H_init('test')    # [samplenum, lsd_dim]\n        self.h = tf.concat([self.h_train, self.h_test], axis=0)  # [samplenum, lsd_dim]\n        self.h_index = tf.placeholder(tf.int32, shape=[None, 1], name='h_index')\n        self.h_temp = tf.gather_nd(self.h, self.h_index)  # select sample according to self.h_index\n\n        # initialize the input data: input + sn(mask)\n        self.input = dict()\n        self.sn = dict()\n        for v_num in range(self.view_num):\n            self.input[str(v_num)] = tf.placeholder(tf.float32, shape=[None, self.layer_size[v_num][-1]],\n                                                    name='input' + str(v_num))\n            self.sn[str(v_num)] = tf.placeholder(tf.float32, shape=[None, 1], name='sn' + str(v_num))\n\n        # ground truth\n        self.gt = tf.placeholder(tf.int32, shape=[None], name='gt')\n\n        # bulid the model\n        self.train_op, self.loss = self.bulid_model([self.h_train_update, self.h_test_update], learning_rate)\n\n        # open session\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n        self.sess.run(tf.global_variables_initializer())\n\n    ## h_update: [self.h_train_update, self.h_test_update]\n    ## learning_rate: [0.001, 0.01] (0.001 for recon; 0.01 for all loss)\n    def bulid_model(self, h_update, learning_rate):\n\n        ## 在损失计算的时候，输入都是self.h_temp\n        ## net: map self.h_temp into view v_num\n        net = dict()\n        for v_num in range(self.view_num):\n            net[str(v_num)] = self.Encoding_net(self.h_temp, v_num)\n        reco_loss = self.reconstruction_loss(net)\n        imputation_loss = self.imputation_loss(net)\n        class_loss = self.classification_loss()\n        all_loss = tf.add(reco_loss, self.lamb * class_loss)\n\n        # train net operator [固定h，优化重建过程中的参数]\n        # train the network to minimize reconstruction loss\n        train_net_op = tf.train.AdamOptimizer(learning_rate[0]) \\\n            .minimize(reco_loss, var_list=tf.get_collection('weight'))\n\n        ## focus on training data 【固定参数，优化训练过程中的h】\n        # train the latent space data to minimize reconstruction loss and classification loss\n        train_hn_op = tf.train.AdamOptimizer(learning_rate[1]) \\\n            .minimize(all_loss, var_list=h_update[0])\n\n        ## focus on testing data 【固定参数，优化训练过程中的h】\n        # adjust the latent space data\n        adj_hn_op = tf.train.AdamOptimizer(learning_rate[0]) \\\n            .minimize(reco_loss, var_list=h_update[1])\n        return [train_net_op, train_hn_op, adj_hn_op], [reco_loss, class_loss, all_loss, imputation_loss]\n\n    ## initial hidden features\n    def H_init(self, a):\n        with tf.variable_scope('H' + a):\n            if a == 'train':\n                h = tf.Variable(xavier_init(self.trainLen, self.lsd_dim))\n            elif a == 'test':\n                h = tf.Variable(xavier_init(self.testLen, self.lsd_dim))\n            h_update = tf.trainable_variables(scope='H' + a)\n        return h, h_update\n\n    ## Target: map h into v\n    ## h: select from self.h via index, [samplenum, lsd_dim]\n    ## v: index of view, int => \"self.layer_size[v] is the fc layers of view v\"\n    def Encoding_net(self, h, v):\n        weight = self.initialize_weight(self.layer_size[v])\n        layer = tf.matmul(h, weight['w0']) + weight['b0']\n        for num in range(1, len(self.layer_size[v])):\n            layer = tf.nn.dropout(tf.matmul(layer, weight['w' + str(num)]) + weight['b' + str(num)], 0.9)\n        return layer\n\n    ## gain input->latent mapping functions\n    def initialize_weight(self, dims_net):\n        all_weight = dict()\n        with tf.variable_scope('weight'):\n            all_weight['w0'] = tf.Variable(xavier_init(self.lsd_dim, dims_net[0])) # [512, 4096]\n            all_weight['b0'] = tf.Variable(tf.zeros([dims_net[0]])) # [4096, ]\n            tf.add_to_collection(\"weight\", all_weight['w' + str(0)])\n            tf.add_to_collection(\"weight\", all_weight['b' + str(0)])\n            for num in range(1, len(dims_net)): # if dims_net has two layers, such as [4096, 4096]\n                all_weight['w' + str(num)] = tf.Variable(xavier_init(dims_net[num - 1], dims_net[num]))\n                all_weight['b' + str(num)] = tf.Variable(tf.zeros([dims_net[num]]))\n                tf.add_to_collection(\"weight\", all_weight['w' + str(num)])\n                tf.add_to_collection(\"weight\", all_weight['b' + str(num)])\n        return all_weight\n\n    ## net: map self.h_temp into view v_num\n    ## self.input[str(num)]: origin input for view v_num\n    ## self.sn[str(num)]: mask for loss calculation 只重构看到的部分，缺失的部分不管\n    def reconstruction_loss(self, net):\n        loss = 0\n        for v_num in range(self.view_num):\n            loss = loss + tf.reduce_sum(\n                tf.pow(tf.subtract(net[str(v_num)], self.input[str(v_num)]), 2.0) * self.sn[str(v_num)] # 1(exist) 0(miss)\n            )\n        return loss\n\n    def imputation_loss(self, net):\n        loss = 0\n        for v_num in range(self.view_num): # 1(exist) 0(miss)\n            view_loss = tf.pow(tf.subtract(net[str(v_num)], self.input[str(v_num)]), 2.0) * (1 - self.sn[str(v_num)])\n            loss = loss + tf.reduce_sum(view_loss) / self.layer_size[v_num][-1]\n        return loss\n\n    ## 这个和原始论文中计算的方法一样\n    ## 注意：分类过程中是没有参数的\n    def classification_loss(self):\n        ## 计算h_temp中两两维度为512dim，样本之间的相关性\n        F_h_h = tf.matmul(self.h_temp, tf.transpose(self.h_temp)) # [samplenum, samplenum]\n        F_hn_hn = tf.diag_part(F_h_h) # [samplenum] 返回对角线的值\n        F_h_h = tf.subtract(F_h_h, tf.matrix_diag(F_hn_hn)) # [samplenum, samplenum]\n\n        classes = tf.reduce_max(self.gt) - tf.reduce_min(self.gt) + 1\n        label_onehot = tf.one_hot(self.gt - 1, classes)  # [samplenum, classes]\n        label_num = tf.reduce_sum(label_onehot, 0, keep_dims=True)  # label_num for each class\n\n        F_h_h_sum = tf.matmul(F_h_h, label_onehot)\n        label_num_broadcast = tf.tile(label_num, [self.trainLen, 1]) - label_onehot\n        F_h_h_mean = tf.divide(F_h_h_sum, label_num_broadcast)\n\n        gt_ = tf.cast(tf.argmax(F_h_h_mean, axis=1), tf.int32) + 1  # gt begin from 1\n        F_h_h_mean_max = tf.reduce_max(F_h_h_mean, axis=1, keep_dims=False)\n\n        theta = tf.cast(tf.not_equal(self.gt, gt_), tf.float32)\n        F_h_hn_mean_ = tf.multiply(F_h_h_mean, label_onehot) # 按元素相乘\n        F_h_hn_mean = tf.reduce_sum(F_h_hn_mean_, axis=1, name='F_h_hn_mean') # F_h_hn_mean 真值对应的距离\n\n        return tf.reduce_sum(tf.nn.relu(tf.add(theta, tf.subtract(F_h_h_mean_max, F_h_hn_mean))))\n\n    # data['0'], data['1']\n    # sn: [samplenum, 2]\n    # gt: [samplenum, ]\n    def train(self, data, sn, gt, epoch, step=[5, 5]):\n        global Reconstruction_LOSS\n        index = np.array([x for x in range(self.trainLen)])\n        shuffle(index)\n        sn = sn[index]\n        gt = gt[index]\n        feed_dict = {self.input[str(v_num)]: data[str(v_num)][index] for v_num in range(self.view_num)}\n        feed_dict.update({self.sn[str(i)]: sn[:, i].reshape(self.trainLen, 1) for i in range(self.view_num)})\n        feed_dict.update({self.gt: gt})\n        feed_dict.update({self.h_index: index.reshape((self.trainLen, 1))})\n        for iter in range(epoch):\n            # update the network\n            for i in range(step[0]):\n                _, Reconstruction_LOSS, Classification_LOSS = self.sess.run(\n                    [self.train_op[0], self.loss[0], self.loss[1]], feed_dict=feed_dict)\n            # update the h\n            for i in range(step[1]):\n                _, Reconstruction_LOSS, Classification_LOSS = self.sess.run(\n                    [self.train_op[1], self.loss[0], self.loss[1]], feed_dict=feed_dict)\n            output = \"Epoch : {:.0f}  ===> Reconstruction Loss = {:.4f}, Classification Loss = {:.4f} \" \\\n                .format((iter + 1), Reconstruction_LOSS, Classification_LOSS)\n            print(output)\n\n    ## input['0']; input['1']; sn['0']; sn['1']\n    ## h_index: [a, a+1, ...., a+n] => 默认train在test前面\n    def test(self, data, sn, gt, epoch):\n        feed_dict = {self.input[str(v_num)]: data[str(v_num)] for v_num in range(self.view_num)}\n        feed_dict.update({self.sn[str(i)]: sn[:, i].reshape(self.testLen, 1) for i in range(self.view_num)})\n        feed_dict.update({self.gt: gt})\n        feed_dict.update({self.h_index: np.array([x for x in range(self.testLen)]).reshape(self.testLen, 1) + self.trainLen})\n\n        # update the h\n        for iter in range(epoch):\n            for i in range(5):\n                _, Reconstruction_LOSS, Imputation_LOSS = self.sess.run(\n                    [self.train_op[2], self.loss[0], self.loss[3]], feed_dict=feed_dict)\n            output = \"Epoch : {:.0f}  ===> Reconstruction Loss = {:.4f}\" \\\n                .format((iter + 1), Reconstruction_LOSS)\n            print(output)\n        return Imputation_LOSS\n\n\n    def get_h_train(self):\n        lsd = self.sess.run(self.h)\n        return lsd[0:self.trainLen]\n\n\n    def get_h_test(self):\n        lsd = self.sess.run(self.h)\n        return lsd[self.trainLen:]\n\n"}
{"type": "source_file", "path": "baseline-mmin/models/networks/autoencoder.py", "content": "import torch\nimport torch.nn as nn\nimport random\nimport copy\nimport torch.nn.functional as F\n\nclass BaseAutoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        latent_vector = self.encoder(x)\n        reconstructed = self.decoder(latent_vector)\n        return reconstructed, latent_vector\n\n\nclass LSTMAutoencoder(nn.Module):\n    ''' Conditioned LSTM autoencoder\n    '''\n    def __init__(self, opt):\n        super().__init__()\n        self.input_size = opt.input_size\n        self.hidden_size = opt.hidden_size\n        self.embedding_size = opt.embedding_size\n        self.false_teacher_rate = opt.false_teacher_rate # use the label instead of the output of previous time step\n        super().__init__()\n        self.encoder = nn.LSTMCell(self.input_size, self.hidden_size)\n        self.enc_fc = nn.Linear(self.hidden_size, self.embedding_size)\n        self.decoder = nn.LSTMCell(self.hidden_size + self.input_size, self.input_size)\n        self.dec_fc = nn.Linear(self.embedding_size, self.hidden_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        ''' x.size() = [batch, timestamp, dim]\n        '''\n        # timestamp_size = x.size(1)\n        # inverse_range = range(timestamp_size-1, -1, -1)\n        # inverse_x = x[:, inverse_range, :]\n        outputs = []\n        o_t_enc = torch.zeros(x.size(0), self.hidden_size).cuda()\n        h_t_enc = torch.zeros(x.size(0), self.hidden_size).cuda()\n        o_t_dec = torch.zeros(x.size(0), self.input_size).cuda()\n        h_t_dec = torch.zeros(x.size(0), self.input_size).cuda()\n\n        for i, input_t in enumerate(x.chunk(x.size(1), dim=1)):\n            input_t = input_t.squeeze(1)\n            o_t_enc, h_t_enc = self.encoder(input_t, (o_t_enc, h_t_enc))\n\n        embd = self.relu(self.enc_fc(h_t_enc))\n        dec_first_hidden = self.relu(self.dec_fc(embd))\n        dec_first_zeros = torch.zeros(x.size(0), self.input_size).cuda()\n        dec_input = torch.cat((dec_first_hidden, dec_first_zeros), dim=1)\n\n        for i in range(x.size(1)):\n            o_t_dec, h_t_dec = self.decoder(dec_input, (o_t_dec, h_t_dec))\n            if self.training and random.random() < self.false_teacher_rate:\n                dec_input = torch.cat((dec_first_hidden, x[:, -i-1, :]), dim=1)\n            else:\n                dec_input = torch.cat((dec_first_hidden, h_t_dec), dim=1)\n            outputs.append(h_t_dec)\n        \n        outputs.reverse()\n        outputs = torch.stack(outputs, 1)\n        # print(outputs.shape)\n        return outputs, embd\n\n\n\n\nclass ResidualAE(nn.Module):\n    ''' Residual autoencoder using fc layers\n        layers should be something like [128, 64, 32]\n        eg:[128,64,32]-> add: [(input_dim, 128), (128, 64), (64, 32), (32, 64), (64, 128), (128, input_dim)]\n                          concat: [(input_dim, 128), (128, 64), (64, 32), (32, 64), (128, 128), (256, input_dim)]\n    '''\n    def __init__(self, layers, n_blocks, input_dim, dropout=0.5, use_bn=False):\n        super(ResidualAE, self).__init__()\n        self.use_bn = use_bn\n        self.dropout = dropout\n        self.n_blocks = n_blocks\n        self.input_dim = input_dim\n        self.transition = nn.Sequential(\n            nn.Linear(input_dim, input_dim),\n            nn.ReLU(),\n            nn.Linear(input_dim, input_dim)\n        )\n        for i in range(n_blocks):\n            setattr(self, 'encoder_' + str(i), self.get_encoder(layers))\n            setattr(self, 'decoder_' + str(i), self.get_decoder(layers))\n    \n    def get_encoder(self, layers):\n        all_layers = []\n        input_dim = self.input_dim\n        for i in range(0, len(layers)):\n            all_layers.append(nn.Linear(input_dim, layers[i]))\n            all_layers.append(nn.LeakyReLU())\n            if self.use_bn:\n                all_layers.append(nn.BatchNorm1d(layers[i]))\n            if self.dropout > 0:\n                all_layers.append(nn.Dropout(self.dropout))\n            input_dim = layers[i]\n        # delete the activation layer of the last layer\n        decline_num = 1 + int(self.use_bn) + int(self.dropout > 0)\n        all_layers = all_layers[:-decline_num]\n        return nn.Sequential(*all_layers)\n    \n    def get_decoder(self, layers):\n        all_layers = []\n        decoder_layer = copy.deepcopy(layers)\n        decoder_layer.reverse()\n        decoder_layer.append(self.input_dim)\n        for i in range(0, len(decoder_layer)-2):\n            all_layers.append(nn.Linear(decoder_layer[i], decoder_layer[i+1]))\n            all_layers.append(nn.ReLU())\n            if self.use_bn:\n                all_layers.append(nn.BatchNorm1d(decoder_layer[i]))\n            if self.dropout > 0:\n                all_layers.append(nn.Dropout(self.dropout))\n        \n        all_layers.append(nn.Linear(decoder_layer[-2], decoder_layer[-1]))\n        return nn.Sequential(*all_layers)\n\n    \n    def forward(self, x):\n        x_in = x\n        x_out = x.clone().fill_(0)\n        latents = []\n        for i in range(self.n_blocks):\n            encoder = getattr(self, 'encoder_' + str(i))\n            decoder = getattr(self, 'decoder_' + str(i))\n            x_in = x_in + x_out\n            latent = encoder(x_in)\n            x_out = decoder(latent)\n            latents.append(latent)\n        latents = torch.cat(latents, dim=-1)\n        return self.transition(x_in+x_out), latents\n\n\n\nclass ResidualUnetAE(nn.Module):\n    ''' Residual autoencoder using fc layers\n    '''\n    def __init__(self, layers, n_blocks, input_dim, dropout=0.5, use_bn=False, fusion='concat'):\n        ''' Unet是对称的, 所以layers只用写一半就好 \n            eg:[128,64,32]-> add: [(input_dim, 128), (128, 64), (64, 32), (32, 64), (64, 128), (128, input_dim)]\n                          concat: [(input_dim, 128), (128, 64), (64, 32), (32, 64), (128, 128), (256, input_dim)]\n        '''\n        super(ResidualUnetAE, self).__init__()\n        self.use_bn = use_bn\n        self.dropout = dropout\n        self.n_blocks = n_blocks\n        self.input_dim = input_dim\n        self.layers = layers\n        self.fusion = fusion\n        if self.fusion == 'concat':\n            self.expand_num = 2\n        elif self.fusion == 'add':\n            self.expand_num = 1\n        else:\n            raise NotImplementedError('Only concat and add is available')\n    \n        # self.encoder = self.get_encoder(layers)\n        for i in range(self.n_blocks):\n            setattr(self, 'encoder_'+str(i), self.get_encoder(layers))\n            setattr(self, 'decoder_'+str(i), self.get_decoder(layers))\n\n    \n    def get_encoder(self, layers):\n        encoder = []\n        input_dim = self.input_dim\n        for i in range(0, len(layers)):\n            layer = []\n            layer.append(nn.Linear(input_dim, layers[i]))\n            layer.append(nn.ReLU())\n            if self.use_bn:\n                layer.append(nn.BatchNorm1d(layers[i]))\n            if self.dropout > 0:\n                layer.append(nn.Dropout(self.dropout))\n            layer = nn.Sequential(*layer)\n            encoder.append(layer)\n            input_dim = layers[i]\n        encoder = nn.Sequential(*encoder)\n        return encoder\n    \n    def get_decoder(self, layers):\n        decoder = []\n        # first layer don't need to fusion outputs\n        first_layer = []\n        first_layer.append(nn.Linear(layers[-1], layers[-2]))\n        if self.use_bn:\n            first_layer.append(nn.BatchNorm1d(layers[-1] * self.expand_num))\n        if self.dropout > 0:\n            first_layer.append(nn.Dropout(self.dropout))\n        decoder.append(nn.Sequential(*first_layer))\n    \n        for i in range(len(layers)-2, 0, -1):\n            layer = []\n            layer.append(nn.Linear(layers[i]*self.expand_num, layers[i-1]))\n            layer.append(nn.ReLU())\n            if self.use_bn:\n                layer.append(nn.BatchNorm1d(layers[i] * self.expand_num))\n            if self.dropout > 0:\n                layer.append(nn.Dropout(self.dropout))\n            layer = nn.Sequential(*layer)\n            decoder.append(layer)\n        \n        decoder.append(\n            nn.Sequential(\n                nn.Linear(layers[0] * self.expand_num, self.input_dim),\n                nn.ReLU()\n            )\n        )\n        decoder = nn.Sequential(*decoder)\n        return decoder\n    \n    def forward_AE_block(self, x, block_num):\n        encoder = getattr(self, 'encoder_' + str(block_num))\n        decoder = getattr(self, 'decoder_' + str(block_num))\n        encoder_out_lookup = {}\n        x_in = x\n        for i in range(len(self.layers)):\n            x_out = encoder[i](x_in)\n            encoder_out_lookup[i] = x_out.clone()\n            x_in = x_out\n        \n        for i in range(len(self.layers)):\n            encoder_out_num = len(self.layers) -1 - i\n            encoder_out = encoder_out_lookup[encoder_out_num]\n            if i == 0:\n                pass\n            elif self.fusion == 'concat':\n                x_in = torch.cat([x_in, encoder_out], dim=-1)\n            elif self.fusion == 'add':\n                x_in = x_in + encoder_out\n            \n            x_out = decoder[i](x_in)\n            x_in = x_out\n        \n        # print(decoder[-1])\n        # print(x_in.shape)\n        # print(x_out.shape)\n        # x_out = decoder[-1](x_in)\n        return x_out\n    \n    def forward(self, x):\n        x_in = x\n        x_out = x.clone().fill_(0)\n        output = {}\n        for i in range(self.n_blocks):\n            x_in = x_in + x_out\n            x_out = self.forward_AE_block(x_in, i)\n            output[i] = x_out.clone()\n\n        return x_out, output\n\nclass SimpleFcAE(nn.Module):\n    def __init__(self, layers, input_dim, dropout=0.5, use_bn=False):\n        ''' Parameters:\n            --------------------------\n            input_dim: input feature dim\n            layers: [x1, x2, x3] will create 3 layers with x1, x2, x3 hidden nodes respectively.\n            dropout: dropout rate\n            use_bn: use batchnorm or not\n        '''\n        super().__init__()\n        self.input_dim = input_dim\n        self.dropout = dropout\n        self.use_bn = use_bn\n        self.encoder = self.get_encoder(layers)\n        self.decoder = self.get_decoder(layers)\n        \n    def get_encoder(self, layers):\n        all_layers = []\n        input_dim = self.input_dim\n        for i in range(0, len(layers)):\n            all_layers.append(nn.Linear(input_dim, layers[i]))\n            all_layers.append(nn.LeakyReLU())\n            if self.use_bn:\n                all_layers.append(nn.BatchNorm1d(layers[i]))\n            if self.dropout > 0:\n                all_layers.append(nn.Dropout(self.dropout))\n            input_dim = layers[i]\n        # delete the activation layer of the last layer\n        # decline_num = 1 + int(self.use_bn) + int(self.dropout > 0)\n        # all_layers = all_layers[:-decline_num]\n        return nn.Sequential(*all_layers)\n    \n    def get_decoder(self, layers):\n        all_layers = []\n        decoder_layer = copy.deepcopy(layers)\n        decoder_layer.reverse()\n        decoder_layer.append(self.input_dim)\n        for i in range(0, len(decoder_layer)-1):\n            all_layers.append(nn.Linear(decoder_layer[i], decoder_layer[i+1]))\n            all_layers.append(nn.ReLU()) if i == len(decoder_layer)-2 else all_layers.append(nn.LeakyReLU()) \n            if self.use_bn:\n                all_layers.append(nn.BatchNorm1d(decoder_layer[i]))\n            if self.dropout > 0:\n                all_layers.append(nn.Dropout(self.dropout))\n        \n        # all_layers.append(nn.Linear(decoder_layer[-2], decoder_layer[-1]))\n        return nn.Sequential(*all_layers)\n    \n    def forward(self, x):\n        ## make layers to a whole module\n        latent = self.encoder(x)\n        recon = self.decoder(latent)\n        return recon, latent\n"}
{"type": "source_file", "path": "baseline-cca/cca.py", "content": "import os\nfrom sklearn import svm\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score, f1_score\nimport time\nimport numpy as np\nfrom numpy.random import randint\n\nfrom util import *\n\nimport sys\nsys.path.append('../')\nimport config\n\nclass CCA:\n    def __init__(self, n_components=1, r1=1e-4, r2=1e-4):\n        self.n_components = n_components\n        self.r1 = r1\n        self.r2 = r2\n        self.w = [None, None]\n        self.m = [None, None]\n\n    # (50000, 784)\n    def fit(self, X1, X2):\n        N = X1.shape[0]  # 50000\n        f1 = X1.shape[1] # 784\n        f2 = X2.shape[1] # 784\n\n        self.m[0] = np.mean(X1, axis=0, keepdims=True) # [1, f1]\n        self.m[1] = np.mean(X2, axis=0, keepdims=True)\n        H1bar = X1 - self.m[0] # zero mean X1\n        H2bar = X2 - self.m[1] # zero mean X2\n\n        SigmaHat12 = (1.0 / (N - 1)) * np.dot(H1bar.T, H2bar)\n        SigmaHat11 = (1.0 / (N - 1)) * np.dot(H1bar.T, H1bar) + self.r1 * np.identity(f1) # 防止矩阵不可逆\n        SigmaHat22 = (1.0 / (N - 1)) * np.dot(H2bar.T, H2bar) + self.r2 * np.identity(f2) # 防止矩阵不可逆\n\n        [D1, V1] = np.linalg.eigh(SigmaHat11) # 计算特征值和特征相向量\n        [D2, V2] = np.linalg.eigh(SigmaHat22)\n        SigmaHat11RootInv = np.dot(np.dot(V1, np.diag(D1 ** -0.5)), V1.T)\n        SigmaHat22RootInv = np.dot(np.dot(V2, np.diag(D2 ** -0.5)), V2.T)\n\n        Tval = np.dot(np.dot(SigmaHat11RootInv, SigmaHat12), SigmaHat22RootInv)\n\n        [U, D, V] = np.linalg.svd(Tval)\n        V = V.T\n        self.w[0] = np.dot(SigmaHat11RootInv, U[:, 0:self.n_components])\n        self.w[1] = np.dot(SigmaHat22RootInv, V[:, 0:self.n_components])\n        D = D[0:self.n_components]\n\n    def _get_result(self, x, idx):\n        result = x - self.m[idx].reshape([1, -1]).repeat(len(x), axis=0)\n        result = np.dot(result, self.w[idx])\n        return result\n\n    def test(self, X1, X2):\n        return self._get_result(X1, 0), self._get_result(X2, 1)\n\n\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--n-components', type=int, default=10, help='number of cca component')\n    parser.add_argument('--missing-rate', type=float, default=0.0, help='view missing rate [default: 0]')\n    parser.add_argument('--normalize', action='store_true', default=False, help='whether normalize input features')\n    parser.add_argument('--dataset', type=str, default='cmumosi', help='input dataset')\n    parser.add_argument('--test_mask', type=str, default=None, help='test under same mask for fair comparision')\n    args = parser.parse_args()\n\n\n    print (f'========= starting ============')\n    folder_acc = []\n    folder_f1 = []\n    folder_save = []\n    if args.dataset in ['cmumosi', 'cmumosei']:\n        num_folder = 1\n    elif args.dataset == 'iemocapfour':\n        num_folder = 5\n    elif args.dataset == 'iemocapsix':\n        num_folder = 5\n\n    for index in range(num_folder):\n        print (f'>>>>> Cross-validation: training on the {index+1} folder >>>>>')\n\n        ## read data\n        print (f' >>>>> read data >>>>> ')\n        X_train, y_train, X_valid, y_valid, X_test, y_test, name_test = read_cmumosi_data(f'../baseline-cpmnet/data/{args.dataset}/{index+1}', args.normalize)\n        trainNum = len(y_train)\n        validNum = len(y_valid)\n        testNum = len(y_test)\n        print (f'train number: {trainNum};   valid number: {validNum};   test number: {testNum}')\n        X0_train, X1_train, X2_train = X_train\n        X0_valid, X1_valid, X2_valid = X_valid\n        X0_test, X1_test, X2_test = X_test\n        print (f'view 0: {len(X0_train[0])};  view 1: {len(X1_train[0])};  view 2: {len(X2_train[0])}')\n\n        # load random mask\n        samplenum = trainNum + validNum + testNum\n        Sn = get_sn(3, samplenum, args.missing_rate) # [samplenum, 3] (A, V, L)\n        Sn_train = Sn[np.arange(trainNum)]\n        Sn_valid = Sn[np.arange(validNum) + trainNum]\n        Sn_test =  Sn[np.arange(testNum) + trainNum + validNum]\n        ######################################################\n        if args.test_mask != None:\n            print (f'using predefined mask!!')\n            name2mask = np.load(args.test_mask, allow_pickle=True)['name2mask'].tolist()\n            Sn_test = []\n            for name in name_test:\n                mask = name2mask[name] # (A, L, V)\n                mask = [mask[0], mask[2], mask[1]] # (A, V, L)\n                Sn_test.append(mask)\n            Sn_test = np.array(Sn_test) # [sample_num, 3]\n        else:\n            print (f'using random initialized mask!!')\n        ######################################################\n\n\n        # pad features in masked part\n        X0_train, X0_valid, X0_test = padmeanV1(X0_train, Sn_train[:,0], X0_valid, Sn_valid[:,0], X0_test, Sn_test[:,0])\n        X1_train, X1_valid, X1_test = padmeanV1(X1_train, Sn_train[:,1], X1_valid, Sn_valid[:,1], X1_test, Sn_test[:,1])\n        X2_train, X2_valid, X2_test = padmeanV1(X2_train, Sn_train[:,2], X2_valid, Sn_valid[:,2], X2_test, Sn_test[:,2])\n\n\n        # CCA feature extraction\n        print (f' >>>>> extract features >>>>> ')\n        Z_trains = []\n        Z_valids = []\n        Z_tests = []\n\n        model = CCA(n_components=args.n_components)\n        model.fit(X0_train, X1_train)\n        Z0_train, Z1_train = model.test(X0_train, X1_train)\n        Z0_valid, Z1_valid = model.test(X0_valid, X1_valid)\n        Z0_test, Z1_test = model.test(X0_test, X1_test)\n        Z_trains.extend([Z0_train, Z1_train])\n        Z_valids.extend([Z0_valid, Z1_valid])\n        Z_tests.extend([Z0_test, Z1_test])\n\n        model = CCA(n_components=args.n_components)\n        model.fit(X0_train, X2_train)\n        Z0_train, Z2_train = model.test(X0_train, X2_train)\n        Z0_valid, Z2_valid = model.test(X0_valid, X2_valid)\n        Z0_test, Z2_test = model.test(X0_test, X2_test)\n        Z_trains.extend([Z0_train, Z2_train])\n        Z_valids.extend([Z0_valid, Z2_valid])\n        Z_tests.extend([Z0_test, Z2_test])\n\n        model = CCA(n_components=args.n_components)\n        model.fit(X1_train, X2_train)\n        Z1_train, Z2_train = model.test(X1_train, X2_train)\n        Z1_valid, Z2_valid = model.test(X1_valid, X2_valid)\n        Z1_test, Z2_test = model.test(X1_test, X2_test)\n        Z_trains.extend([Z1_train, Z2_train])\n        Z_valids.extend([Z1_valid, Z2_valid])\n        Z_tests.extend([Z1_test, Z2_test])\n\n        Z_train = np.concatenate(Z_trains, axis=1) # [samplenum, dim]\n        Z_valid = np.concatenate(Z_valids, axis=1) # [samplenum, dim]\n        Z_test  = np.concatenate(Z_tests, axis=1)  # [samplenum, dim]\n\n        # SVM classify\n        print (f' >>>>> training classifier >>>>> ')\n        clf = svm.LinearSVC(C=0.01, dual=False)\n        clf.fit(Z_train, y_train)\n        total_pred = clf.predict(Z_test)\n        total_label = y_test\n        f1 = f1_score(total_label, total_pred, average='weighted')\n        acc = accuracy_score(total_label, total_pred)\n\n        folder_acc.append(acc)\n        folder_f1.append(f1)\n        folder_save.append({'test_labels': total_label, 'test_preds': total_pred, 'test_hiddens': Z_test, 'test_names': name_test})\n        print (f'>>>>> Finish: training on the {index+1} folder >>>>>')\n\n    \n    ## save results\n    suffix_name = f'{args.dataset}_cca_mask:{args.missing_rate:.1f}'\n\n    mean_f1 = np.mean(np.array(folder_f1))\n    mean_acc = np.mean(np.array(folder_acc))\n    res_name = f'f1:{mean_f1:2.2%}_acc:{mean_acc:2.2%}'\n\n    save_root = config.MODEL_DIR\n    if not os.path.exists(save_root): os.makedirs(save_root)\n    save_path = f'{save_root}/{suffix_name}_{res_name}_{time.time()}.npz'\n    np.savez_compressed(save_path,\n                        args=np.array(args, dtype=object),\n                        folder_save=np.array(folder_save, dtype=object) # save non-structure type\n                        )\n    print (f' =========== finish =========== ')\n\n"}
{"type": "source_file", "path": "baseline-mmin/data/base_dataset.py", "content": "\"\"\"This module implements an abstract base class (ABC) 'BaseDataset' for datasets.\n\nIt also includes common transformation functions (e.g., get_transform, __scale_width), which can be later used in subclasses.\n\"\"\"\nimport random\nimport numpy as np\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom abc import ABC, abstractmethod\n\n\nclass BaseDataset(data.Dataset, ABC):\n    \"\"\"This class is an abstract base class (ABC) for datasets.\n\n    To create a subclass, you need to implement the following four functions:\n    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).\n    -- <__len__>:                       return the size of dataset.\n    -- <__getitem__>:                   get a data point.\n    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.\n    \"\"\"\n\n    def __init__(self, opt):\n        \"\"\"Initialize the class; save the options in the class\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        self.opt = opt\n        self.manual_collate_fn = False\n        # self.root = opt.dataroot\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"\n        return parser\n\n    @abstractmethod\n    def __len__(self):\n        \"\"\"Return the total number of images in the dataset.\"\"\"\n        return 0\n\n    @abstractmethod\n    def __getitem__(self, index):\n        \"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns:\n            a dictionary of data with their names. It ususally contains the data itself and its metadata information.\n        \"\"\"\n        pass\n\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.preprocess == 'resize_and_crop':\n        new_h = new_w = opt.load_size\n    elif opt.preprocess == 'scale_width_and_crop':\n        new_w = opt.load_size\n        new_h = opt.load_size * h // w\n\n    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n\n    flip = random.random() > 0.5\n\n    return {'crop_pos': (x, y), 'flip': flip}\n\n\ndef get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):\n    transform_list = []\n    if grayscale:\n        transform_list.append(transforms.Grayscale(1))\n    if 'resize' in opt.preprocess:\n        osize = [opt.load_size, opt.load_size]\n        transform_list.append(transforms.Resize(osize, method))\n    elif 'scale_width' in opt.preprocess:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n\n    if 'crop' in opt.preprocess:\n        if params is None:\n            transform_list.append(transforms.RandomCrop(opt.crop_size))\n        else:\n            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n\n    if opt.preprocess == 'none':\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n\n    if not opt.no_flip:\n        if params is None:\n            transform_list.append(transforms.RandomHorizontalFlip())\n        elif params['flip']:\n            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n\n    if convert:\n        transform_list += [transforms.ToTensor()]\n        if grayscale:\n            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n        else:\n            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size\n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n\n    __print_size_warning(ow, oh, w, h)\n    return img.resize((w, h), method)\n\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), method)\n\n\ndef __crop(img, pos, size):\n    ow, oh = img.size\n    x1, y1 = pos\n    tw = th = size\n    if (ow > tw or oh > th):\n        return img.crop((x1, y1, x1 + tw, y1 + th))\n    return img\n\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n\n\ndef __print_size_warning(ow, oh, w, h):\n    \"\"\"Print warning information about image size(only print once)\"\"\"\n    if not hasattr(__print_size_warning, 'has_printed'):\n        print(\"The image size needs to be a multiple of 4. \"\n              \"The loaded image size was (%d, %d), so it was adjusted to \"\n              \"(%d, %d). This adjustment will be done to all images \"\n              \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n"}
{"type": "source_file", "path": "baseline-mmin/models/networks/fc.py", "content": "import torch\nimport torch.nn as nn\n\nclass FcEncoder(nn.Module):\n    def __init__(self, input_dim, layers, dropout=0.5, use_bn=False):\n        ''' Fully Connect classifier\n            fc+relu+bn+dropout， 最后分类128-4层是直接fc的\n            Parameters:\n            --------------------------\n            input_dim: input feature dim\n            layers: [x1, x2, x3] will create 3 layers with x1, x2, x3 hidden nodes respectively.\n            dropout: dropout rate\n            use_bn: use batchnorm or not\n        '''\n        super().__init__()\n        self.all_layers = []\n        for i in range(0, len(layers)):\n            self.all_layers.append(nn.Linear(input_dim, layers[i]))\n            self.all_layers.append(nn.ReLU())\n            if use_bn:\n                self.all_layers.append(nn.BatchNorm1d(layers[i]))\n            if dropout > 0:\n                self.all_layers.append(nn.Dropout(dropout))\n            input_dim = layers[i]\n        \n        self.module = nn.Sequential(*self.all_layers)\n    \n    def forward(self, x):\n        ## make layers to a whole module\n        feat = self.module(x)\n        return feat"}
{"type": "source_file", "path": "baseline-mmin/data/multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass MultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--A_type', type=str, help='which audio feat to use')\n        parser.add_argument('--V_type', type=str, help='which visual feat to use')\n        parser.add_argument('--L_type', type=str, help='which lexical feat to use')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__) # gain file folder\n        pwd = os.path.dirname(pwd) # '/data5/lianzheng/MMIN-master/data'\n        '''\n        config = {'data_root': '/data5/lianzheng/MMIN-master/IEMOCAP_full_release/', \n        'target_root': '/data5/lianzheng/MMIN-master/IEMOCAP_features_2021/target', \n        'total_cv': 10, \n        'feature_root': '/data5/lianzheng/MMIN-master/IEMOCAP_features_2021'}\n        '''\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAP_config.json'))) # load json file\n        self.norm_method = opt.norm_method # 'trn'\n\n        # load feature\n        self.A_type = opt.A_type\n        # self.all_A: maybe map 5531 sample feats, each sample is frame-level features [17, 130]\n        self.all_A = h5py.File(os.path.join(config['feature_root'], 'A', f'{self.A_type}.h5'), 'r')\n        if self.A_type == 'comparE':\n            self.mean_std = h5py.File(os.path.join(config['feature_root'], 'A', 'comparE_mean_std.h5'), 'r')\n            self.mean = torch.from_numpy(self.mean_std[str(cvNo)]['mean'][()]).unsqueeze(0).float() # [1, 130]\n            self.std = torch.from_numpy(self.mean_std[str(cvNo)]['std'][()]).unsqueeze(0).float() # [1, 130]\n        self.V_type = opt.V_type\n        self.all_V = h5py.File(os.path.join(config['feature_root'], 'V', f'{self.V_type}.h5'), 'r')\n        self.L_type = opt.L_type\n        self.all_L = h5py.File(os.path.join(config['feature_root'], 'L', f'{self.L_type}.h5'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path) # [4500, 4]\n        self.label = np.argmax(self.label, axis=1) # [4500, ]\n        self.int2name = np.load(int2name_path) # [4500, 1] names\n        # in aligned dataset, you should move out Ses03M_impro03_M001\n        # if 'Ses03M_impro03_M001' in self.int2name:\n        #     idx = self.int2name.index('Ses03M_impro03_M001')\n        #     self.int2name.pop(idx)\n        #     self.label = np.delete(self.label, idx, axis=0)\n        self.manual_collate_fn = True\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index][0].decode() # str\n        label = torch.tensor(self.label[index]) # int\n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[int2name][()]).float()\n        if self.A_type == 'comparE':\n            A_feat = self.normalize_on_utt(A_feat) if self.norm_method == 'utt' else self.normalize_on_trn(A_feat)\n        # process V_feat \n        V_feat = torch.from_numpy(self.all_V[int2name][()]).float()\n        # process L_feat\n        L_feat = torch.from_numpy(self.all_L[int2name][()]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    ## utterance-level normalization\n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    ## train dataset-level normalization\n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch] # [batch * [?, 130]]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long() # [?1, ?2, ..., ?N] => frame lens\n        A = pad_sequence(A, batch_first=True, padding_value=0) # [batch, ?max, 130]\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch]) # [batch, 1]\n        int2name = [sample['int2name'] for sample in batch]         # [batch, 1]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert_large\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MultimodalDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/data/msp_multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass MSPMultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'MSP_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        # self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = True\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        label = torch.tensor(self.label[index])\n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[index]).float()\n        # process V_feat \n        V_feat = torch.from_numpy(self.all_V[index]).float()\n        # proveee L_feat\n        L_feat = torch.from_numpy(self.all_L[index]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert_large\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = MSPMultimodalDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-mmin/data/word_aligned_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass WordAlignedDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--A_type', type=str, help='which audio feat to use')\n        parser.add_argument('--V_type', type=str, help='which visual feat to use')\n        parser.add_argument('--L_type', type=str, help='which lexical feat to use')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAP_config.json')))\n        self.norm_method = opt.norm_method\n        # load feature\n        self.A_type = opt.A_type\n        self.all_A = h5py.File(os.path.join(config['feature_root'], 'aligned', 'A', f'aligned_{self.A_type}.h5'), 'r')\n        if self.A_type == 'comparE':\n            self.mean_std = h5py.File(os.path.join(config['feature_root'], 'aligned', 'A', 'aligned_comparE_mean_std.h5'), 'r')\n            self.mean = torch.from_numpy(self.mean_std[str(cvNo)]['mean'][()]).unsqueeze(0).float()\n            self.std = torch.from_numpy(self.mean_std[str(cvNo)]['std'][()]).unsqueeze(0).float()\n        self.V_type = opt.V_type\n        self.all_V = h5py.File(os.path.join(config['feature_root'], 'aligned', 'V', f'aligned_{self.V_type}.h5'), 'r')\n        self.L_type = opt.L_type\n        self.all_L = h5py.File(os.path.join(config['feature_root'], 'aligned', 'L', f'aligned_{self.L_type}.h5'), 'r')\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.label = np.argmax(self.label, axis=1)\n        self.int2name = np.load(int2name_path)\n        self.int2name = [x[0].decode() for x in self.int2name]\n        if 'Ses03M_impro03_M001' in self.int2name:\n            idx = self.int2name.index('Ses03M_impro03_M001')\n            self.int2name.pop(idx)\n            self.label = np.delete(self.label, idx, axis=0)\n        self.manual_collate_fn = True\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        label = torch.tensor(self.label[index])\n        # process A_feat\n        A_feat = torch.from_numpy(self.all_A[int2name][()]).float()\n        if self.A_type == 'comparE':\n            A_feat = self.normalize_on_utt(A_feat) if self.norm_method == 'utt' else self.normalize_on_trn(A_feat)\n        # process V_feat \n        V_feat = torch.from_numpy(self.all_V[int2name][()]).float()\n        # proveee L_feat\n        L_feat = torch.from_numpy(self.all_L[int2name][()]).float()\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n\nif __name__ == '__main__':\n    class test:\n        cvNo = 1\n        A_type = \"comparE\"\n        V_type = \"denseface\"\n        L_type = \"bert\"\n        norm_method = 'trn'\n\n    \n    opt = test()\n    print('Reading from dataset:')\n    a = WordAlignedDataset(opt, set_name='trn')\n    data = next(iter(a))\n    for k, v in data.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    print('Reading from dataloader:')\n    x = [a[100], a[34], a[890]]\n    print('each one:')\n    for i, _x in enumerate(x):\n        print(i, ':')\n        for k, v in _x.items():\n            if k not in ['int2name', 'label']:\n                print(k, v.shape)\n            else:\n                print(k, v)\n    print('packed output')\n    x = a.collate_fn(x)\n    for k, v in x.items():\n        if k not in ['int2name', 'label']:\n            print(k, v.shape)\n        else:\n            print(k, v)\n    "}
{"type": "source_file", "path": "baseline-cpmnet/util/util.py", "content": "import scipy.io as sio\nimport numpy as np\nimport math\nimport os\nfrom numpy.random import shuffle\nimport tensorflow as tf\n\nclass DataSet(object):\n\n    def __init__(self, data, view_number, labels, names):\n        \"\"\"\n        Construct a DataSet.\n        data: [2, samplenum, dim]\n        labels: [samplenum, 1]\n        differnt view: self.data[0]; self.data[1];...\n        \"\"\"\n        self.data = dict()\n        self._num_examples = data[0].shape[0] # samplenum\n        self._labels = labels # [samplenum, 1]\n        self._names = names   # [samplenum, 1]\n        for v_num in range(view_number):\n            self.data[str(v_num)] = data[v_num]\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def num_examples(self):\n        return self._num_examples\n\n    @property\n    def names(self):\n        return self._names\n\ndef Normalize(data):\n    \"\"\"\n    :param data:Input data\n    :return:normalized data\n    \"\"\"\n    m = np.mean(data)\n    mx = np.max(data)\n    mn = np.min(data)\n    return (data - m) / (mx - mn)\n\n\n# # './data/animal.mat', 0.8, 1\n# def read_data(str_name, ratio, Normal=1):\n#     \"\"\"read data and spilt it train set and test set evenly\n#     :param str_name:path and dataname\n#     :param ratio:training set ratio\n#     :param Normal:do you want normalize\n#     :return:dataset and view number\n#     \"\"\"\n\n#     ## X: (viewnum, 1, 1)\n#     data = sio.loadmat(str_name)\n#     view_number = data['X'].shape[1]\n#     X = np.split(data['X'], view_number, axis=1)\n\n#     ## labels: shape: (samplenum, 1), values: [1, 2, ..., classnum]\n#     if min(data['gt']) == 0: # 'gt' start from 1 [gt is sorted in decent order]\n#         labels = data['gt'] + 1\n#     else:\n#         labels = data['gt']\n#     classes = max(labels)[0]\n\n#     ## gain X_train~labels_test\n#     X_train = []\n#     X_test = []\n#     labels_train = []\n#     labels_test = []\n#     all_length = 0\n#     for c_num in range(1, classes + 1):\n#         c_length = np.sum(labels == c_num) # sample number\n#         indexes = np.arange(c_length)\n#         shuffle(indexes) # gain index list\n#         labels_train.extend(labels[all_length + indexes][0:math.floor(c_length * ratio)])\n#         labels_test.extend(labels[all_length + indexes][math.floor(c_length * ratio):])\n\n#         # X: (viewnum, 1, 1)\n#         X_train_temp = []\n#         X_test_temp = []\n#         for v_num in range(view_number):\n#             X_train_temp.append(X[v_num][0][0].transpose()[all_length + indexes][0:math.floor(c_length * ratio)]) # [2, samplenum, dim]\n#             X_test_temp.append(X[v_num][0][0].transpose()[all_length + indexes][math.floor(c_length * ratio):])   # [2, samplenum, dim]\n#         if c_num == 1: \n#             X_train = X_train_temp\n#             X_test = X_test_temp\n#         else: ## append behind\n#             for v_num in range(view_number):\n#                 X_train[v_num] = np.r_[X_train[v_num], X_train_temp[v_num]]\n#                 X_test[v_num] = np.r_[X_test[v_num], X_test_temp[v_num]]\n#         all_length = all_length + c_length\n\n#     ## normalize each view\n#     if (Normal == 1):\n#         for v_num in range(view_number):\n#             X_train[v_num] = Normalize(X_train[v_num]) # [2, samplenum, dim] \n#             X_test[v_num] = Normalize(X_test[v_num])   # [2, samplenum, dim]\n\n#     # X_train: [2, 8103, 4096]\n#     # labels_train: [8103, 1]\n#     traindata = DataSet(X_train, view_number, np.array(labels_train))\n#     testdata = DataSet(X_test, view_number, np.array(labels_test))\n#     return traindata, testdata, view_number\n\n\n\n\n\ndef read_cmumosi_data(data_root, normalize=True):\n    \"\"\"read data and spilt it train set and test set evenly\n    :param data_root: data root\n    :return:dataset and view number\n    \"\"\"\n    view_number = 3\n    trn_path = os.path.join(data_root, 'trn.npz')\n    val_path = os.path.join(data_root, 'val.npz')\n    tst_path = os.path.join(data_root, 'tst.npz')\n    \n    ### read train data\n    X_train = []\n    labels_train = []\n    names_train = []\n    name = np.load(trn_path)['name']\n    label = np.load(trn_path)['label']\n    audio = np.load(trn_path)['audio']\n    video = np.load(trn_path)['video']\n    text = np.load(trn_path)['text']\n\n    if min(label) == 0:\n        label = label + 1\n    classes = max(label)\n    # assert classes == 2, f'label needs modified'\n\n    labels_train = label\n    names_train = name\n    X_train.append(audio)\n    X_train.append(video)\n    X_train.append(text)\n\n    ### read test data\n    X_test = []\n    labels_test = []\n    names_test = []\n    name = np.load(tst_path)['name']\n    label = np.load(tst_path)['label']\n    audio = np.load(tst_path)['audio']\n    video = np.load(tst_path)['video']\n    text = np.load(tst_path)['text']\n\n    if min(label) == 0:\n        label = label + 1\n    classes = max(label)\n    # assert classes == 2, f'label needs modified'\n\n    labels_test= label\n    names_test = name\n    X_test.append(audio)\n    X_test.append(video)\n    X_test.append(text)\n\n    ## normalize each view\n    if normalize:\n        for v_num in range(view_number):\n            X_train[v_num] = Normalize(X_train[v_num]) # [2, samplenum, dim] \n            X_test[v_num] = Normalize(X_test[v_num])   # [2, samplenum, dim]\n\n    # X_train: [2, 8103, 4096]\n    # labels_train: [8103, 1]\n    traindata = DataSet(X_train, view_number, np.array(labels_train), names_train)\n    testdata = DataSet(X_test, view_number, np.array(labels_test), names_test)\n    return traindata, testdata, view_number\n\n\n\n\n\ndef xavier_init(fan_in, fan_out, constant=1):\n    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out),\n                             minval=low, maxval=high,\n                             dtype=tf.float32)\n"}
{"type": "source_file", "path": "baseline-mmin/data/iemocapsix_multimodal_dataset.py", "content": "import os\nimport json\nfrom typing import List\nimport torch\nimport numpy as np\nimport h5py\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom data.base_dataset import BaseDataset\n\n\nclass IEMOCAPSIXMultimodalDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, isTrain=None):\n        parser.add_argument('--cvNo', type=int, help='which cross validation set')\n        parser.add_argument('--output_dim', type=int, help='how many label types in this dataset')\n        parser.add_argument('--norm_method', type=str, choices=['utt', 'trn'], help='how to normalize input comparE feature')\n        return parser\n    \n    def __init__(self, opt, set_name):\n        ''' IEMOCAP dataset reader\n            set_name in ['trn', 'val', 'tst']\n        '''\n        super().__init__(opt)\n\n        # record & load basic settings \n        cvNo = opt.cvNo\n        self.set_name = set_name\n        self.dataset = opt.dataset_mode.split('_')[0]\n        pwd = os.path.abspath(__file__)\n        pwd = os.path.dirname(pwd)\n        config = json.load(open(os.path.join(pwd, 'config', 'IEMOCAPSIX_config.json')))\n        self.norm_method = opt.norm_method\n\n        # load feature\n        self.all_A = np.load(os.path.join(config['feature_root'], 'A', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_V = np.load(os.path.join(config['feature_root'], 'V', str(opt.cvNo), f'{set_name}.npy'), 'r')\n        self.all_L = np.load(os.path.join(config['feature_root'], 'L', str(opt.cvNo), f'{set_name}.npy'), 'r')\n\n        # load target\n        label_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_label.npy\")\n        int2name_path = os.path.join(config['target_root'], f'{cvNo}', f\"{set_name}_int2name.npy\")\n        self.label = np.load(label_path)\n        self.int2name = np.load(int2name_path)\n        self.manual_collate_fn = False ## for utterance level features\n\n    def __getitem__(self, index):\n        int2name = self.int2name[index]\n        if self.dataset in ['cmumosi']:\n            label = torch.tensor(self.label[index]).float()\n        elif self.dataset in ['iemocapfour', 'iemocapsix']:\n            label = torch.tensor(self.label[index]).long()\n        # process A_feat\n        A_feat = torch.FloatTensor(self.all_A[index])\n        # process V_feat \n        V_feat = torch.FloatTensor(self.all_V[index])\n        # proveee L_feat\n        L_feat = torch.FloatTensor(self.all_L[index])\n        return {\n            'A_feat': A_feat, \n            'V_feat': V_feat,\n            'L_feat': L_feat,\n            'label': label,\n            'int2name': int2name\n        }\n    \n    def __len__(self):\n        return len(self.label)\n    \n    def normalize_on_utt(self, features):\n        mean_f = torch.mean(features, dim=0).unsqueeze(0).float()\n        std_f = torch.std(features, dim=0).unsqueeze(0).float()\n        std_f[std_f == 0.0] = 1.0\n        features = (features - mean_f) / std_f\n        return features\n    \n    def normalize_on_trn(self, features):\n        features = (features - self.mean) / self.std\n        return features\n\n    def collate_fn(self, batch):\n        A = [sample['A_feat'] for sample in batch]\n        V = [sample['V_feat'] for sample in batch]\n        L = [sample['L_feat'] for sample in batch]\n        lengths = torch.tensor([len(sample) for sample in A]).long()\n        A = pad_sequence(A, batch_first=True, padding_value=0)\n        V = pad_sequence(V, batch_first=True, padding_value=0)\n        L = pad_sequence(L, batch_first=True, padding_value=0)\n        label = torch.tensor([sample['label'] for sample in batch])\n        int2name = [sample['int2name'] for sample in batch]\n        return {\n            'A_feat': A, \n            'V_feat': V,\n            'L_feat': L,\n            'label': label,\n            'lengths': lengths,\n            'int2name': int2name\n        }\n"}
{"type": "source_file", "path": "baseline-mmin/models/utils/config.py", "content": "import sys\n\nclass OptConfig(object):\n    def __init__(self):\n        pass\n\n    def load(self, config_dict):\n        if sys.version > '3':\n            for key, value in config_dict.items():\n                if not isinstance(value, dict):\n                    setattr(self, key, value)\n                else:\n                    self.load(value)\n        else:\n            for key, value in config_dict.iteritems():\n                if not isinstance(value, dict):\n                    setattr(self, key, value)\n                else:\n                    self.load(value)"}
