{"repo_info": {"repo_name": "DoubleSparse", "repo_owner": "andy-yang-1", "repo_url": "https://github.com/andy-yang-1/DoubleSparse"}}
{"type": "test_file", "path": "AIME/test_aime.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\n# from fastchat.model import get_conversation_template\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom modify_qwen2 import convert_kvcache_qwen2_heavy_recent, convert_qwen2_channel_config\n\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, default=\"meta-llama/Llama-3.1-8B\", help='Selected model')\n    parser.add_argument('--mode', type=str, default=\"ds\", choices=[\"ds\", \"ds-offload\", \"dense\"], help='Whether to use offloading')\n    parser.add_argument('--architecture', type=str, default=\"llama\", choices=[\"llama\", \"mistral\", \"mixtral\", \"qwen2\"])\n    parser.add_argument('--channel', type=str, default=\"q\", choices=[\"q\", \"k\", \"qk\"])\n    parser.add_argument('--heavy_const', type=int, default=128, help='Heavy constant')\n    parser.add_argument('--group_factor', type=int, default=2, help='Group factor')\n    parser.add_argument('--q_bits', type=int, default=2, help='Quantization bits')\n    parser.add_argument('--prompt', type=str, default=\"Hello, my name is\", help='Prompt for generation')\n\n    args = parser.parse_args()\n\n\n    model_path = args.model_path\n    channel_path = \"../config/\" + model_path + \".json\"\n\n\n    kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}\n\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n\n    channel_config = None\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n\n    if args.mode == \"ds\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_llama_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_mistral_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"qwen2\":\n            model = convert_kvcache_qwen2_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_qwen2_channel_config(model, channel_config, args.channel)\n    elif args.mode == \"ds-offload\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_llama_offloading_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_mistral_offloading_channel_config(model, channel_config, args.channel)\n\n\n    prompt = args.prompt\n    prompt = \"<｜begin▁of▁sentence｜><｜User｜>\" + prompt + \"<｜Assistant｜><think>\\n\"\n    # prompt = \"<｜begin▁of▁sentence｜><｜User｜>Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<｜Assistant｜><think>\\n\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n    max_new_tokens = 8000-input_ids.shape[-1]\n\n    output = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)[0][len(input_ids[0]):]\n    output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n    print(f\"Output: {output}\")\n\n\n    # prompt = \"<｜begin▁of▁sentence｜><｜User｜>Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<｜Assistant｜><think>\\n\"\n\n"}
{"type": "source_file", "path": "AIME/modify_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if q_len > 1 or self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # # free gpu memory\n        # if self.config.num_hidden_layers != 32:\n        #     h2_mask = None\n        #     grouped_attn_weights = None\n        #     indices = None\n        #     discard_indices = None\n        #     grouped_query = None\n        #     grouped_key = None\n        #     sorted_query_states = None\n        #     sorted_key_states = None\n        #     query_states = None\n        #     key_states = None\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_llama_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_llama_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef convert_llama_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_llama_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "AIME/modify_qwen2.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.qwen2.configuration_qwen2 import Qwen2Config\nfrom transformers.models.qwen2.modeling_qwen2 import Qwen2RotaryEmbedding, Qwen2Attention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass Qwen2Attention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.attention_dropout = config.attention_dropout\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = Qwen2RotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if q_len > 1 or self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # # free gpu memory\n        # if self.config.num_hidden_layers != 32:\n        #     h2_mask = None\n        #     grouped_attn_weights = None\n        #     indices = None\n        #     discard_indices = None\n        #     grouped_query = None\n        #     grouped_key = None\n        #     sorted_query_states = None\n        #     sorted_key_states = None\n        #     query_states = None\n        #     key_states = None\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_qwen2_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_qwen2_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, Qwen2Attention):\n            device = next(module.parameters()).device\n            new_module = Qwen2Attention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef convert_qwen2_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, Qwen2Attention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_qwen2_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, Qwen2Attention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "LongBench/h2o_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\n__all__ = ['convert_kvcache_llama_heavy_recent', 'LlamaAttention_heavy_hitter']\n\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n        self.heavy_budget_ratio = 0.8\n        self.recent_budget_ratio = 0.2\n        self.attention_masks_next = None \n        self.heavy_budget = None\n        self.recent_budget = None\n        self.cache_budget = config.cache_budget\n        self.previous_scores = None\n        self.input_length = []\n        self.cache_budget_records = []\n\n    def _reset_masks(self):\n        self.attention_masks_next = None \n        self.heavy_budget = None\n        self.recent_budget = None\n        # self.cache_budget = None\n        self.previous_scores = None\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        \n        if q_len > 1:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n        \n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n           \n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n        \n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n\n        kv_seq_len = key_states.shape[-2]\n\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n\n\n        if self.attention_masks_next is not None:\n            attn_weights = attn_weights * self.attention_masks_next + (1 - self.attention_masks_next) * torch.finfo(attn_weights.dtype).min\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n\n        # attn_weights (BS, heads, q-tokens, k-tokens) 16, 15, 15 // 16, 1, 16\n        current_scores_sum = attn_weights.sum(0).sum(1) # (heads, k-tokens)\n        # offset = attn_weights.gt(0).sum(0).sum(1)\n\n        # Accumulate attention scores\n        if not self.previous_scores == None:\n            current_scores_sum[:, :-1] += self.previous_scores #(Enlarged Sequence)\n        else:\n            # self.heavy_budget = int(self.heavy_budget_ratio * current_scores_sum.shape[-1])\n            # self.recent_budget = int(self.recent_budget_ratio * current_scores_sum.shape[-1])\n            self.heavy_budget = int(self.cache_budget * self.heavy_budget_ratio)\n            self.recent_budget = int(self.cache_budget * self.recent_budget_ratio)\n            # self.cache_budget = self.heavy_budget + self.recent_budget\n            self.cache_budget_records.append(self.cache_budget)\n            self.input_length.append(attn_weights.shape[-1])\n\n            # current_scores_sum = current_scores_sum / offset\n        dtype_attn_weights = attn_weights.dtype\n        attn_weights_devices = attn_weights.device\n        assert attn_weights.shape[0] == 1\n        self.previous_scores = current_scores_sum #(heads, k-tokens)\n        attn_mask = torch.ones(current_scores_sum.shape[0], current_scores_sum.shape[1]+1, device=attn_weights_devices, dtype=dtype_attn_weights)\n\n        attn_tokens_all = self.previous_scores.shape[-1]\n    \n        if attn_tokens_all > self.cache_budget:\n            # activate most recent k-cache\n            if not self.recent_budget == 0:\n                attn_mask[:, :-self.recent_budget] = 0\n                selected_set = self.previous_scores[:, :-self.recent_budget]\n            else:\n                # activate historical best self.cache_budget - self.recent_budget tokens.\n                # self.previous_scores # (k-Cache - 1)\n                selected_set = self.previous_scores\n\n            if not self.heavy_budget == 0:\n                _, keep_topk = selected_set.topk(k=self.heavy_budget, dim=-1, largest=True)\n                attn_mask = attn_mask.scatter(-1, keep_topk, 1)\n\n        self.attention_masks_next = attn_mask.unsqueeze(0).unsqueeze(2)\n\n        score_mask = attn_mask[:,:-1]\n        score_mask[:, -self.recent_budget:] = 1\n        self.previous_scores = self.previous_scores * score_mask\n\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_h2o(model, config):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_h2o(module, config)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\ndef reset_h2o(model):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = reset_h2o(module)\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            module._reset_masks()\n\n    return model\n\n"}
{"type": "source_file", "path": "LongBench/metrics.py", "content": "import re\nimport string\n\nimport jieba\nfrom fuzzywuzzy import fuzz\nimport difflib\n\nfrom typing import List\nfrom collections import Counter\nfrom rouge import Rouge\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef normalize_zh_answer(s):\n    \"\"\"Lower text and remove punctuation, extra whitespace.\"\"\"\n\n    def white_space_fix(text):\n        return \"\".join(text.split())\n\n    def remove_punc(text):\n        cn_punctuation = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n        all_punctuation = set(string.punctuation + cn_punctuation)\n        return \"\".join(ch for ch in text if ch not in all_punctuation)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\ndef count_score(prediction, ground_truth, **kwargs):\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\ndef retrieval_score(prediction, ground_truth, **kwargs):\n    pattern = r'Paragraph (\\d+)'\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\ndef retrieval_zh_score(prediction, ground_truth, **kwargs):\n    pattern = r'段落(\\d+)'\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\ndef code_sim_score(prediction, ground_truth, **kwargs):\n    all_lines = prediction.lstrip('\\n').split('\\n')\n    prediction = \"\"\n    for line in all_lines:\n        if ('`' not in line) and ('#' not in line) and ('//' not in line):\n            prediction = line\n            break\n    return (fuzz.ratio(prediction, ground_truth) / 100)\n\ndef classification_score(prediction, ground_truth, **kwargs):\n    em_match_list = []\n    all_classes = kwargs[\"all_classes\"]\n    for class_name in all_classes:\n        if class_name in prediction:\n            em_match_list.append(class_name)\n    for match_term in em_match_list:\n        if match_term in ground_truth and match_term != ground_truth:\n            em_match_list.remove(match_term)\n    if ground_truth in em_match_list:\n        score = (1.0 / len(em_match_list))\n    else:\n        score = 0.0\n    return score\n    \ndef rouge_score(prediction, ground_truth, **kwargs):\n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores([prediction], [ground_truth], avg=True)\n    except:\n        return 0.0\n    return scores[\"rouge-l\"][\"f\"]\n\ndef rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False))) \n    score = rouge_score(prediction, ground_truth)\n    return score\n\ndef f1_score(prediction, ground_truth, **kwargs):\n    common = Counter(prediction) & Counter(ground_truth)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction)\n    recall = 1.0 * num_same / len(ground_truth)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\ndef qa_f1_score(prediction, ground_truth, **kwargs):\n    normalized_prediction = normalize_answer(prediction)\n    normalized_ground_truth = normalize_answer(ground_truth)\n\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    return f1_score(prediction_tokens, ground_truth_tokens)\n\n\ndef qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens = [normalize_zh_answer(token) for token in prediction_tokens]\n    ground_truth_tokens = [normalize_zh_answer(token) for token in ground_truth_tokens]\n    prediction_tokens = [token for token in prediction_tokens if len(token) > 0]\n    ground_truth_tokens = [token for token in ground_truth_tokens if len(token) > 0]\n    return f1_score(prediction_tokens, ground_truth_tokens)\n"}
{"type": "source_file", "path": "LongBench/modify_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if q_len > 1 or self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # # free gpu memory\n        # if self.config.num_hidden_layers != 32:\n        #     h2_mask = None\n        #     grouped_attn_weights = None\n        #     indices = None\n        #     discard_indices = None\n        #     grouped_query = None\n        #     grouped_key = None\n        #     sorted_query_states = None\n        #     sorted_key_states = None\n        #     query_states = None\n        #     key_states = None\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_llama_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_llama_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef convert_llama_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_llama_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "LongBench/pred.py", "content": "import os\nfrom datasets import load_dataset\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    AutoModelForCausalLM,\n)\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport argparse\n# from evaluation.flash_attn_monkey_patch import (\n#     replace_llama_attn_with_flash_attn,\n#     replace_mistral_attn_with_flash_attn,\n# )\nfrom quest_attention import enable_quest_attention_eval\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config, change_llama_heavy_const\nfrom h2o_llama import convert_h2o, reset_h2o\nfrom streaming_llama import convert_streaming\nfrom sparq_llama import convert_sparq\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=None,\n        choices=[\n            \"llama2-7b-chat-4k\",\n            \"llama3.1-8b-instruct-128k\",\n            \"longchat-v1.5-7b-32k\",\n            \"xgen-7b-8k\",\n            \"internlm-7b-8k\",\n            \"chatglm2-6b\",\n            \"chatglm2-6b-32k\",\n            \"chatglm3-6b-32k\",\n            \"vicuna-v1.5-7b-16k\",\n            \"Mistral-7B-v0.2-hf\",\n        ],\n    )\n    parser.add_argument(\"--e\", action=\"store_true\", help=\"Evaluate on LongBench-E\")\n\n    parser.add_argument(\"--task\", type=str, help=\"task name\", required=True)\n\n    parser.add_argument(\"--token_budget\", type=int, default=None)\n    parser.add_argument(\"--chunk_size\", type=int, default=None)\n    \n    parser.add_argument(\"--group_factor\", type=int, default=2)\n    parser.add_argument(\"--heavy_const\", type=int, default=256)\n    parser.add_argument(\"--q_bits\", type=int, default=2)\n    parser.add_argument(\"--channel\", type=str, default=\"q\")\n    \n    parser.add_argument(\"--quest\", action=\"store_true\", help=\"Enable Quest Attention\")\n    parser.add_argument(\"--ds\", action=\"store_true\", help=\"Enable Double Sparsity Attention\")\n    parser.add_argument(\"--h2o\", action=\"store_true\", help=\"Enable H2O Attention\")\n    parser.add_argument(\"--streaming\", action=\"store_true\", help=\"Enable StreamingLLM Attention\")\n    parser.add_argument(\"--sparq\", action=\"store_true\", help=\"Enable Sparq Attention\")\n\n    return parser.parse_args(args)\n\n\n# This is the customized building prompt for chat models\ndef build_chat(tokenizer, prompt, model_name):\n    if \"chatglm3\" in model_name:\n        prompt = tokenizer.build_chat_input(prompt)\n    elif \"chatglm\" in model_name:\n        prompt = tokenizer.build_prompt(prompt)\n    elif \"longchat\" in model_name or \"vicuna\" in model_name:\n        from fastchat.model import get_conversation_template\n\n        conv = get_conversation_template(\"vicuna\")\n        conv.append_message(conv.roles[0], prompt)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n    elif \"llama2\" in model_name:\n        prompt = f\"[INST]{prompt}[/INST]\"\n    elif \"xgen\" in model_name:\n        header = (\n            \"A chat between a curious human and an artificial intelligence assistant. \"\n            \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\"\n        )\n        prompt = header + f\" ### Human: {prompt}\\n###\"\n    elif \"internlm\" in model_name:\n        prompt = f\"<|User|>:{prompt}<eoh>\\n<|Bot|>:\"\n    elif \"llama3.1\" in model_name:\n        prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|> {prompt} <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\"\n    return prompt\n\n\ndef post_process(response, model_name):\n    if \"xgen\" in model_name:\n        response = response.strip().replace(\"Assistant:\", \"\")\n    elif \"internlm\" in model_name:\n        response = response.split(\"<eoa>\")[0]\n    return response\n\n\ndef get_pred(\n    model,\n    tokenizer,\n    data,\n    max_length,\n    max_gen,\n    prompt_format,\n    dataset,\n    device,\n    model_name,\n):\n    preds = []\n    for json_obj in tqdm(data):\n        # clean up memory\n        prompt = prompt_format.format(**json_obj)\n        # truncate to fit max_length (we suggest truncate in the middle, since the left and right side may contain crucial instructions)\n        tokenized_prompt = tokenizer(\n            prompt, truncation=False, return_tensors=\"pt\"\n        ).input_ids[0]\n        if \"chatglm3\" in model_name:\n            tokenized_prompt = tokenizer(\n                prompt, truncation=False, return_tensors=\"pt\", add_special_tokens=False\n            ).input_ids[0]\n        if len(tokenized_prompt) > max_length:\n            half = int(max_length / 2)\n            prompt = tokenizer.decode(\n                tokenized_prompt[:half], skip_special_tokens=True\n            ) + tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)\n        if dataset not in [\n            \"trec\",\n            \"triviaqa\",\n            \"samsum\",\n            \"lsht\",\n            \"lcc\",\n            \"repobench-p\",\n        ]:  # chat models are better off without build prompts on these tasks\n            prompt = build_chat(tokenizer, prompt, model_name)\n\n        # split the prompt and question (simulate decoding in the question stage)\n        if dataset in [\"qasper\", \"hotpotqa\"]:\n            q_pos = prompt.rfind(\"Question:\")\n        elif dataset in [\"multifieldqa_en\", \"gov_report\"]:\n            q_pos = prompt.rfind(\"Now,\")\n        elif dataset in [\"triviaqa\"]:\n            q_pos = prompt.rfind(\"Answer the question\")\n        elif dataset in [\"narrativeqa\"]:\n            q_pos = prompt.rfind(\"Do not provide\")\n        else:\n            q_pos = -1\n\n        # max simulation length is 100\n        q_pos = max(len(prompt) - 100, q_pos)\n\n        if q_pos != None:\n            question = prompt[q_pos:]\n            prompt = prompt[:q_pos]\n\n        if \"chatglm3\" in model_name:\n            # input = prompt.to(device)\n            input = prompt.to(\"cuda\")\n        else:\n            # input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(device)\n            input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(\"cuda\")\n            q_input = tokenizer(question, truncation=False, return_tensors=\"pt\").to(\n                \"cuda\"\n            )\n            q_input.input_ids = q_input.input_ids[:, 1:]\n\n        context_length = input.input_ids.shape[-1] + q_input.input_ids.shape[-1]\n\n        if (\n            dataset == \"samsum\"\n        ):  # prevent illegal output on samsum (model endlessly repeat \"\\nDialogue\"), might be a prompting issue\n            assert False\n            output = model.generate(\n                **input,\n                max_new_tokens=max_gen,\n                num_beams=1,\n                do_sample=False,\n                temperature=1.0,\n                min_length=context_length + 1,\n                eos_token_id=[\n                    tokenizer.eos_token_id,\n                    tokenizer.encode(\"\\n\", add_special_tokens=False)[-1],\n                ],\n            )[0]\n        else:\n            with torch.no_grad():\n                # NOTE(narrativeqa needs to emptify cache with 2 GPUs)\n                torch.cuda.empty_cache()\n                # NOTE for h2o\n                if args.h2o:\n                    reset_h2o(model)\n                output = model(\n                    input_ids=input.input_ids,\n                    past_key_values=None,\n                    use_cache=True,\n                )\n                past_key_values = output.past_key_values\n                for input_id in q_input.input_ids[0]:\n                    output = model(\n                        input_ids=input_id.unsqueeze(0).unsqueeze(0),\n                        past_key_values=past_key_values,\n                        use_cache=True,\n                    )\n                    past_key_values = output.past_key_values\n\n                pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_content = [pred_token_idx.item()]\n                for _ in range(max_gen - 1):\n                    outputs = model(\n                        input_ids=pred_token_idx,\n                        past_key_values=past_key_values,\n                        use_cache=True,\n                    )\n\n                    past_key_values = outputs.past_key_values\n                    pred_token_idx = (\n                        outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                    )\n                    generated_content += [pred_token_idx.item()]\n                    if pred_token_idx.item() == tokenizer.eos_token_id:\n                        break\n\n            # output = model.generate(\n            #     **input,\n            #     max_new_tokens=max_gen,\n            #     num_beams=1,\n            #     do_sample=False,\n            #     temperature=1.0,\n            # )[0]\n\n        pred = tokenizer.decode(generated_content, skip_special_tokens=True)\n        # pred = tokenizer.decode(output[context_length:], skip_special_tokens=True)\n        pred = post_process(pred, model_name)\n        preds.append(\n            {\n                \"pred\": pred,\n                \"answers\": json_obj[\"answers\"],\n                \"all_classes\": json_obj[\"all_classes\"],\n                \"length\": json_obj[\"length\"],\n            }\n        )\n    return preds\n\n\ndef seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\n\n\ndef load_model_and_tokenizer(path, model_name, device):\n    if \"chatglm\" in model_name or \"internlm\" in model_name or \"xgen\" in model_name:\n        tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            path, trust_remote_code=True, torch_dtype=torch.bfloat16\n        ).to(device)\n    elif \"llama\" in model_name:\n        # replace_llama_attn_with_flash_attn()\n        print(f\"Path: {path}\")\n        tokenizer = AutoTokenizer.from_pretrained(path)\n        # kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\", \"attn_implementation\": \"flash_attention_2\"}\n        kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}\n        model = AutoModelForCausalLM.from_pretrained(path, **kwargs)\n        # model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\n        #     device\n        # )\n    elif \"longchat\" in model_name or \"vicuna\" in model_name:\n        # from fastchat.model import load_model\n        # replace_llama_attn_with_flash_attn()\n        model = AutoModelForCausalLM.from_pretrained(\n            path, trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            path, trust_remote_code=True, use_fast=False\n        )\n    elif \"Mistral\" in model_name:\n        replace_mistral_attn_with_flash_attn()\n        tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\"\n        )\n    model = model.eval()\n\n    if args.quest:\n        enable_quest_attention_eval(model, args)\n        \n    if args.ds:\n        # TODO: remove hard-coded path\n        channel_path = \"/root/DoubleSparse/config/\" + path + \".json\"\n        config = AutoConfig.from_pretrained(path)\n        channel_config = None\n        with open(channel_path, \"r\") as f:\n            channel_config = json.load(f)\n        model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n        model = convert_llama_channel_config(model, channel_config, args.channel)\n\n    if args.h2o:\n        config = AutoConfig.from_pretrained(path)\n        config.cache_budget = args.token_budget\n        model = convert_h2o(model, config)\n        \n    if args.streaming:\n        config = AutoConfig.from_pretrained(path)\n        model = convert_streaming(model, config, args.token_budget, 8)\n\n    if args.sparq:\n        config = AutoConfig.from_pretrained(path)\n        model = convert_sparq(model, config, args.token_budget, args.group_factor, args.q_bits)\n\n    return model, tokenizer\n\n\nif __name__ == \"__main__\":\n    seed_everything(42)\n    args = parse_args()\n    model2path = json.load(open(\"config/model2path.json\", \"r\"))\n    model2maxlen = json.load(open(\"config/model2maxlen.json\", \"r\"))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_name = args.model\n    # define your model\n    model, tokenizer = load_model_and_tokenizer(\n        model2path[model_name], model_name, device\n    )\n    max_length = model2maxlen[model_name]\n    if args.e:\n        datasets = [\n            \"qasper\",\n            \"multifieldqa_en\",\n            \"hotpotqa\",\n            \"2wikimqa\",\n            \"gov_report\",\n            \"multi_news\",\n            \"trec\",\n            \"triviaqa\",\n            \"samsum\",\n            \"passage_count\",\n            \"passage_retrieval_en\",\n            \"lcc\",\n            \"repobench-p\",\n        ]\n    else:\n        datasets = [args.task]\n    # we design specific prompt format and max generation length for each task, feel free to modify them to optimize model output\n    dataset2prompt = json.load(open(\"config/dataset2prompt.json\", \"r\"))\n    dataset2maxlen = json.load(open(\"config/dataset2maxlen.json\", \"r\"))\n    # predict on each dataset\n    if not os.path.exists(\"pred\"):\n        os.makedirs(\"pred\")\n    if not os.path.exists(\"pred_e\"):\n        os.makedirs(\"pred_e\")\n    for dataset in datasets:\n        if args.e:\n            data = load_dataset(\"THUDM/LongBench\", f\"{dataset}_e\", split=\"test\")\n            if not os.path.exists(f\"pred_e/{model_name}\"):\n                os.makedirs(f\"pred_e/{model_name}\")\n            out_path = f\"pred_e/{model_name}/{dataset}.jsonl\"\n            if args.quest:\n                out_path = f\"pred_e/{model_name}/{dataset}-{args.token_budget}.jsonl\"\n            elif args.ds:\n                out_path = f\"pred_e/{model_name}/{dataset}-{args.heavy_const}-{args.group_factor}-{args.q_bits}-{args.channel}.jsonl\"\n            elif args.h2o:\n                out_path = f\"pred_e/{model_name}/{dataset}-h2o-{args.token_budget}.jsonl\"\n            elif args.streaming:\n                out_path = f\"pred_e/{model_name}/{dataset}-streaming-{args.token_budget}.jsonl\"\n            else:\n                out_path = f\"pred_e/{model_name}/{dataset}.jsonl\"\n        else:\n            data = load_dataset(\"THUDM/LongBench\", dataset, split=\"test\")\n            if not os.path.exists(f\"pred/{model_name}\"):\n                os.makedirs(f\"pred/{model_name}\")\n            if args.quest:\n                out_path = f\"pred/{model_name}/{dataset}-{args.token_budget}.jsonl\"\n            elif args.ds:\n                out_path = f\"pred/{model_name}/{dataset}-{args.heavy_const}-{args.group_factor}-{args.q_bits}-{args.channel}.jsonl\"\n            elif args.h2o:\n                out_path = f\"pred/{model_name}/{dataset}-h2o-{args.token_budget}.jsonl\"\n            elif args.streaming:\n                out_path = f\"pred/{model_name}/{dataset}-streaming-{args.token_budget}.jsonl\"\n            elif args.sparq:\n                out_path = f\"pred/{model_name}/{dataset}-sparq-{args.token_budget}-{args.group_factor}-{args.q_bits}.jsonl\"\n            else:\n                out_path = f\"pred/{model_name}/{dataset}.jsonl\"\n        prompt_format = dataset2prompt[dataset]\n        max_gen = dataset2maxlen[dataset]\n        preds = get_pred(\n            model,\n            tokenizer,\n            data,\n            max_length,\n            max_gen,\n            prompt_format,\n            dataset,\n            device,\n            model_name,\n        )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            for pred in preds:\n                json.dump(pred, f, ensure_ascii=False)\n                f.write(\"\\n\")\n"}
{"type": "source_file", "path": "LongBench/quest_attention.py", "content": "import math\nimport numpy as np\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\n\nimport types\n\nfrom transformers.models.llama.modeling_llama import (\n    LlamaAttention,\n    apply_rotary_pos_emb,\n    repeat_kv,\n)\n\nfrom transformers.cache_utils import Cache\n\n\ndef local_heavy_hitter_mask(attn_weights, token_budget, chunk_size):\n    # attn_weights (BS, head, query, keys)\n\n    # expend attn_weights to be divisible by chunk_size\n    seq_length = attn_weights.shape[-1]\n    padding_length = chunk_size - ((seq_length - 1) % chunk_size + 1)\n    attn_weights = torch.cat(\n        [\n            attn_weights,\n            torch.ones(\n                (\n                    attn_weights.shape[0],\n                    attn_weights.shape[1],\n                    attn_weights.shape[2],\n                    padding_length,\n                ),\n                device=attn_weights.device,\n            )\n            * torch.tensor(torch.finfo(attn_weights.dtype).min),\n        ],\n        dim=-1,\n    )\n\n    # chunk attn_weights into chunk_size tokens\n    chunk_attn_weights = attn_weights.reshape(\n        attn_weights.shape[0],\n        attn_weights.shape[1],\n        attn_weights.shape[2],\n        attn_weights.shape[3] // chunk_size,\n        chunk_size,\n    ).amax(dim=-1)\n\n    _, topk = chunk_attn_weights.topk(\n        k=min(max(3, token_budget // chunk_size), chunk_attn_weights.size(-1)), dim=-1\n    )\n    # repeat topk chunk_size times and recover the original indexes (* chunk_size + arange(chunk_size))\n    topk = topk.unsqueeze(-1).repeat(\n        1, 1, 1, 1, chunk_size\n    ) * chunk_size + torch.arange(chunk_size, device=topk.device)\n    topk = topk.reshape(topk.shape[0], topk.shape[1], topk.shape[2], -1)\n    mask_bottom = torch.zeros_like(attn_weights, dtype=torch.bool)\n    mask_bottom.scatter_(-1, topk, True)\n\n    # remove the padding\n    mask_bottom = mask_bottom[:, :, :, :seq_length]\n\n    return mask_bottom\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Cache] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    cache_position: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n    **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    if q_len > 1 or self.layer_id < 2:\n        return self.flash_forward(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            cache_position,\n            position_embeddings,\n            **kwargs,\n        )\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    # kv_seq_len = key_states.shape[-2]\n    # if past_key_value is not None:\n    #     kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, position_ids)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin\n    )\n    # [bsz, nh, t, hd]\n        \n    if past_key_value is not None:\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n        kv_seq_len = key_states.shape[-2]\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n        self.head_dim\n    )\n\n    sign = (query_states > 0) + (~(query_states > 0)) * -1\n    max_key = key_states * sign\n    postive_query = query_states * sign\n\n    # expend max_key to be divisible by chunk_size\n    seq_length = max_key.shape[-2]\n    padding_length = self.chunk_size - ((seq_length - 1) % self.chunk_size + 1)\n    max_key = torch.cat(\n        [\n            max_key,\n            torch.ones(\n                (max_key.shape[0], max_key.shape[1], padding_length, max_key.shape[3]),\n                device=max_key.device,\n            )\n            * torch.tensor(torch.finfo(max_key.dtype).min),\n        ],\n        dim=-2,\n    )\n\n    # chunk max_key into chunk_size tokens\n    chunk_max_key = max_key.reshape(\n        max_key.shape[0],\n        max_key.shape[1],\n        max_key.shape[2] // self.chunk_size,\n        self.chunk_size,\n        max_key.shape[3],\n    ).amax(dim=-2)\n\n    # duplicate chunk_max_key chunk_size times\n    chunk_max_key = chunk_max_key.unsqueeze(-2).repeat(1, 1, 1, self.chunk_size, 1)\n    # reshape chunk_max_key to the original shape\n    chunk_max_key = chunk_max_key.reshape(\n        chunk_max_key.shape[0], chunk_max_key.shape[1], -1, chunk_max_key.shape[-1]\n    )[:, :, :seq_length, :]\n\n    quantized_weight = torch.matmul(\n        postive_query.float(),\n        chunk_max_key.transpose(2, 3),\n    )\n\n    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    if attention_mask is not None:\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        attn_weights = attn_weights + attention_mask\n        attn_weights = torch.max(\n            attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n        )\n        quantized_weight = quantized_weight + attention_mask\n        quantized_weight = torch.max(\n            quantized_weight, torch.tensor(torch.finfo(quantized_weight.dtype).min)\n        )\n\n    token_budget = min(kv_seq_len, self.token_budget)\n\n    attn_weights_for_selection = quantized_weight\n\n    if token_budget > 0:\n        mask_bottom = local_heavy_hitter_mask(\n            attn_weights_for_selection, token_budget, self.chunk_size\n        )  # Default: No padding applied to input\n    else:\n        mask_bottom = torch.zeros_like(attn_weights_for_selection, dtype=torch.bool)\n\n    mask_bottom = torch.tril(mask_bottom, diagonal=position_ids[0][0].item())\n    attn_weights[~mask_bottom] = torch.tensor(torch.finfo(attn_weights.dtype).min)\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        query_states.dtype\n    )\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef forward_yarn(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    is_padded_inputs: Optional[bool] = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, h_size = hidden_states.size()\n\n    # Prefill stage utilizes flash attention\n    if q_len > 1 or self.layer_id < 2:\n        return self.flash_forward(\n            hidden_states,\n            attention_mask,\n            position_ids,\n            past_key_value,\n            output_attentions,\n            use_cache,\n            is_padded_inputs,\n        )\n\n    has_layer_past = past_key_value is not None\n\n    if has_layer_past:\n        past_kv = past_key_value[0]\n        past_len = past_key_value[1]\n    else:\n        past_len = 0\n\n    if self.config.pretraining_tp > 1:\n        key_value_slicing = (\n            self.num_key_value_heads * self.head_dim\n        ) // self.config.pretraining_tp\n        query_slices = self.q_proj.weight.split(\n            (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n        )\n        key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n        value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n        q = [\n            F.linear(hidden_states, query_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        q = torch.cat(q, dim=-1)\n\n        k = [\n            F.linear(hidden_states, key_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        k = torch.cat(k, dim=-1)\n\n        v = [\n            F.linear(hidden_states, value_slices[i])\n            for i in range(self.config.pretraining_tp)\n        ]\n        v = torch.cat(v, dim=-1)\n\n    else:\n        q = self.q_proj(hidden_states)\n        k = self.k_proj(hidden_states)\n        v = self.v_proj(hidden_states)\n\n    q = q.view(bsz, q_len, self.num_heads, self.head_dim)\n    k = k.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n    v = v.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n\n    q, k = self.rotary_emb(q, k, past_len)\n\n    @torch.jit.script\n    def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n        \"\"\"\n        This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n        num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n        \"\"\"\n        batch, slen, _, num_key_value_heads, head_dim = hidden_states.shape\n        if n_rep == 1:\n            return hidden_states\n        hidden_states = hidden_states[:, :, :, :, None, :].expand(\n            batch, slen, 2, num_key_value_heads, n_rep, head_dim\n        )\n        return hidden_states.reshape(\n            batch, slen, 2, num_key_value_heads * n_rep, head_dim\n        )\n\n    kv = torch.stack([k, v], 2)\n    kv = repeat_kv(kv, self.num_key_value_groups)\n\n    # Cache QKV values\n    if has_layer_past:\n        new_len = past_len + q.size(1)\n        if new_len > past_kv.size(1):\n            past_kv = torch.cat(\n                [\n                    past_kv,\n                    torch.empty(\n                        bsz,\n                        256,\n                        2,\n                        kv.size(3),\n                        kv.size(4),\n                        dtype=kv.dtype,\n                        device=kv.device,\n                    ),\n                ],\n                1,\n            )\n        past_kv[:, past_len:new_len] = kv\n        kv = past_kv[:, :new_len]\n    else:\n        past_kv = kv\n\n    k, v = kv.split(1, dim=2)\n    k = k.squeeze(2)\n    v = v.squeeze(2)\n\n    past_key_value = (past_kv, past_len + q.size(1)) if use_cache else None\n\n    q = q.transpose(1, 2)\n    k = k.transpose(1, 2)\n    v = v.transpose(1, 2)\n\n    kv_seq_len = k.shape[-2]\n\n    attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n    sign = (q > 0) + (~(q > 0)) * -1\n    max_key = k * sign\n    postive_query = q * sign\n\n    # expend max_key to be divisible by chunk_size\n    seq_length = max_key.shape[-2]\n    padding_length = self.chunk_size - ((seq_length - 1) % self.chunk_size + 1)\n    max_key = torch.cat(\n        [\n            max_key,\n            torch.ones(\n                (max_key.shape[0], max_key.shape[1], padding_length, max_key.shape[3]),\n                device=max_key.device,\n            )\n            * torch.tensor(torch.finfo(max_key.dtype).min),\n        ],\n        dim=-2,\n    )\n\n    # chunk max_key into chunk_size tokens\n    chunk_max_key = max_key.reshape(\n        max_key.shape[0],\n        max_key.shape[1],\n        max_key.shape[2] // self.chunk_size,\n        self.chunk_size,\n        max_key.shape[3],\n    ).amax(dim=-2)\n\n    # duplicate chunk_max_key chunk_size times\n    chunk_max_key = chunk_max_key.unsqueeze(-2).repeat(1, 1, 1, self.chunk_size, 1)\n    # reshape chunk_max_key to the original shape\n    chunk_max_key = chunk_max_key.reshape(\n        chunk_max_key.shape[0], chunk_max_key.shape[1], -1, chunk_max_key.shape[-1]\n    )[:, :, :seq_length, :]\n\n    quantized_weight = torch.matmul(\n        postive_query.float(),\n        chunk_max_key.transpose(2, 3),\n    )\n\n    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    assert q_len == 1, \"Prefill stage utilizes flash attention.\"\n\n    token_budget = min(kv_seq_len, self.token_budget)\n\n    attn_weights_for_selection = quantized_weight\n    # attn_weights_for_selection = attn_weights\n\n    if token_budget > 0:\n        mask_bottom = local_heavy_hitter_mask(\n            attn_weights_for_selection, token_budget, self.chunk_size\n        )  # Default: No padding applied to input\n    else:\n        mask_bottom = torch.zeros_like(attn_weights_for_selection, dtype=torch.bool)\n\n    # Attention mask for multi-stage Q&A, todo\n    mask_bottom = torch.tril(mask_bottom, diagonal=k.shape[-2] - q.shape[-2])\n    attn_weights[~mask_bottom] = torch.tensor(torch.finfo(attn_weights.dtype).min)\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        q.dtype\n    )\n    attn_output = torch.matmul(attn_weights, v)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    if self.config.pretraining_tp > 1:\n        attn_output = attn_output.split(\n            self.hidden_size // self.config.pretraining_tp, dim=2\n        )\n        o_proj_slices = self.o_proj.weight.split(\n            self.hidden_size // self.config.pretraining_tp, dim=1\n        )\n        attn_output = sum(\n            [\n                F.linear(attn_output[i], o_proj_slices[i])\n                for i in range(self.config.pretraining_tp)\n            ]\n        )\n    else:\n        attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\nglobal layer_id\nlayer_id = 32\n\n\ndef enable_quest_attention_eval(model, args):\n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            enable_quest_attention_eval(\n                module,\n                args,\n            )\n\n        global layer_id\n        if isinstance(module, LlamaAttention):\n            # For longchat model\n            layer_id -= 1\n            model._modules[name].layer_id = layer_id\n            model._modules[name].flash_forward = model._modules[name].forward\n            model._modules[name].forward = types.MethodType(\n                forward, model._modules[name]\n            )\n\n            model._modules[name].token_budget = args.token_budget\n            model._modules[name].chunk_size = args.chunk_size\n        elif module.__class__.__name__ == \"LlamaAttention\":\n            # For yarn model\n            layer_id -= 1\n            model._modules[name].layer_id = layer_id\n            model._modules[name].flash_forward = model._modules[name].forward\n            model._modules[name].forward = types.MethodType(\n                forward_yarn, model._modules[name]\n            )\n\n            model._modules[name].token_budget = args.token_budget\n            model._modules[name].chunk_size = args.chunk_size\n"}
{"type": "source_file", "path": "LongBench/sparq_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if q_len > 1 or self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n\n        outlier_num = self.head_dim // self.group_factor\n\n        sorted_indices = torch.topk(torch.abs(query_states), outlier_num, dim=-1).indices # [bsz, num_heads, 1, outlier_num]\n        sorted_query_states = torch.gather(query_states, -1, sorted_indices)\n        sorted_key_states = torch.gather(key_states, -1, sorted_indices.expand(-1, -1, kv_seq_len, -1))\n\n        \n        # quantization\n        if self.label_bits < 16:\n            sorted_query_states = pseudo_quantize(sorted_query_states, self.label_bits)\n            sorted_key_states = pseudo_quantize(sorted_key_states, self.label_bits)\n\n        grouped_attn_weights = torch.matmul(sorted_query_states, sorted_key_states.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_sparq(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_sparq(module, config, heavy_const, group_factor, label_bits)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\n\ndef change_sparq_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "benchmark/attention/benchmark_gpt_fast.py", "content": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\ndef test_flash(B, N_CTX, H, D):\n    import time\n\n    # B, N_CTX, H, D = 1, 16384, 32, 128\n\n    dtype = torch.float16\n\n    q = torch.empty((B, 1, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    k = torch.empty((B, N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    v = torch.empty((B, N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=10)\n    att_out = torch.empty((H, B * N_CTX), dtype=dtype, device=\"cuda\")\n    out = torch.empty((B, H, D), dtype=dtype, device=\"cuda\")\n\n    # Warm up\n    for _ in range(10):\n        y = F.scaled_dot_product_attention(q, k, v)\n    run_iter = 1000\n    torch.cuda.synchronize()\n    t1 = time.time()\n    for _ in range(run_iter):\n        y = F.scaled_dot_product_attention(q, k, v)\n\n    torch.cuda.synchronize()\n    t2 = time.time()\n    print(\"Time cost {}\".format((t2 - t1) / run_iter))\n\n    # torch_out = torch_att(q, k, v, B, N_CTX, H, D).squeeze()\n    o = out\n\n    # print(\"max \", torch.max(torch.abs(torch_out - o)))\n    # print(\"mean \", torch.mean(torch.abs(torch_out - o)))\n    # assert torch.allclose(torch_out, o, atol=1e-2, rtol=0)\n    return (t2 - t1) / run_iter\n\nif __name__ == \"__main__\":\n\n    bszs = [1, 4, 8, 16, 32]\n    ctxs = [2048, 4096, 8192, 16384]\n\n    h = 32\n    d = 128\n\n    times = []\n\n    for b in bszs:\n        for n_ctx in ctxs:\n            times.append([b, n_ctx, test_flash(b, n_ctx, h, d)])\n\n    print(times)"}
{"type": "source_file", "path": "benchmark/attention/benchmark_light.py", "content": "import torch\n\nimport triton\nimport triton.language as tl\nimport math\n\n\n@triton.jit\ndef attention_fwd(\n    Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    Att_Out,\n    stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd,\n    att_stride_h, att_stride_bs,\n\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s * offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] < cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index + offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new < cur_batch_end_index)\n    return\n\n\n@triton.jit\ndef softmax_reducev_fwd_kernel(\n    Logics, V, Out,\n    B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs,\n    stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od,\n    stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, # 避免读取到nan的数据\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    off_v = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len) * stride_b_loc_s\n\n    v_ptrs = V + off_v\n\n    e_max = float(\"-inf\")\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) * stride_b_loc_s, mask=(start_n + offs_n) < cur_batch_seq_len, other=other_kv_index)\n\n        qk = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, \n                     mask=start_n + offs_n < cur_batch_seq_len, other=float(\"-inf\"))\n    \n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_att_fwd(q, k, att_out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len):\n    BLOCK = 32\n    # shape constraints\n    Lq, Lk = q.shape[-1], k.shape[-1]\n    assert Lq == Lk\n    assert Lk in {16, 32, 64, 128}\n    sm_scale = 1.0 / (Lk ** 0.5)\n\n    batch, head_num = B_Loc.shape[0], q.shape[1]\n\n    grid = (batch, head_num, triton.cdiv(max_input_len, BLOCK))\n\n    num_warps = 4\n    \n    attention_fwd[grid](\n        q, k, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n        att_out,\n        B_Loc.stride(0), B_Loc.stride(1),\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        att_out.stride(0), att_out.stride(1),\n        BLOCK_DMODEL=Lk,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n@torch.no_grad()\ndef token_softmax_reducev_fwd(logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len, other_kv_index):\n    BLOCK = 64\n    batch, head = b_seq_len.shape[0], logics.shape[0]\n    grid = (batch, head)\n    num_warps = 1\n    softmax_reducev_fwd_kernel[grid](\n        logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len,\n        logics.stride(0), logics.stride(1),\n        v.stride(0), v.stride(1), v.stride(2),\n        o.stride(0), o.stride(1), o.stride(2),\n        b_loc.stride(0), b_loc.stride(1),\n        other_kv_index,\n        BLOCK_DMODEL=v.shape[-1],\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=3\n    )\n    return\n\n\ndef torch_att(xq, xk, xv, bs, seqlen, num_head, head_dim):\n    xq = xq.view(bs, 1, num_head, head_dim)\n    xk = xk.view(bs, seqlen, num_head, head_dim)\n    xv = xv.view(bs, seqlen, num_head, head_dim)\n    keys = xk\n    xq = xq.transpose(1, 2)\n    keys = keys.transpose(1, 2)\n    scores = (torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(head_dim)).squeeze().reshape(bs, num_head, 1, seqlen)\n\n    # return scores\n\n    attn_weights = torch.softmax(scores, dim=-1)\n    values = xv.transpose(1, 2)\n\n    out = torch.matmul(attn_weights, values).transpose(1,2)\n\n    return out\n\n\n\n\ndef test_light(B, N_CTX, H, D):\n    import time\n\n    # B, N_CTX, H, D = 32, 2048, 32, 128\n\n    dtype = torch.float16\n\n    q = torch.empty((B, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    k = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    v = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=10)\n    att_out = torch.empty((H, B * N_CTX), dtype=dtype, device=\"cuda\")\n    out = torch.empty((B, H, D), dtype=dtype, device=\"cuda\")\n\n    # print(att_out)\n\n    b_loc = torch.zeros((B, N_CTX), dtype=torch.int32, device=\"cuda\")\n    b_start_loc = torch.zeros((B,), dtype=torch.int32, device=\"cuda\")\n    b_seq_len = torch.zeros((B,), dtype=torch.int32, device=\"cuda\")\n\n    for i in range(B):\n        b_start_loc[i] = i * N_CTX\n        b_seq_len[i] = N_CTX\n        b_loc[i] = i * N_CTX + torch.arange(0, N_CTX, dtype=torch.int32, device=\"cuda\")\n\n    # Warm up\n    for _ in range(10):\n        token_att_fwd(q, k, att_out, b_loc, b_start_loc, b_seq_len, N_CTX)\n        token_softmax_reducev_fwd(att_out, v, out, b_loc, b_start_loc, b_seq_len, N_CTX, b_loc[0, N_CTX - 1].item())\n    run_iter = 1000\n    torch.cuda.synchronize()\n    t1 = time.time()\n    for _ in range(run_iter):\n        token_att_fwd(q, k, att_out, b_loc, b_start_loc, b_seq_len, N_CTX)\n        token_softmax_reducev_fwd(att_out, v, out, b_loc, b_start_loc, b_seq_len, N_CTX, b_loc[0, N_CTX - 1].item())\n\n    torch.cuda.synchronize()\n    t2 = time.time()\n    print(\"Time cost {}\".format((t2 - t1) / run_iter))\n\n    torch_out = torch_att(q, k, v, B, N_CTX, H, D).squeeze()\n    o = out\n\n    print(\"max \", torch.max(torch.abs(torch_out - o)))\n    print(\"mean \", torch.mean(torch.abs(torch_out - o)))\n    assert torch.allclose(torch_out, o, atol=1e-2, rtol=0)\n    return (t2 - t1) / run_iter\n\n\n\nif __name__ == '__main__':\n\n    bszs = [1, 4, 8, 16, 32]\n    ctxs = [2048, 4096, 8192, 16384]\n\n    h = 32\n    d = 128\n\n    times = []\n\n    for b in bszs:\n        for n_ctx in ctxs:\n            times.append([b, n_ctx, test_light(b, n_ctx, h, d)])\n\n    print(times)\n"}
{"type": "source_file", "path": "benchmark/benchmark.py", "content": ""}
{"type": "source_file", "path": "AIME/aime.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\n# from fastchat.model import get_conversation_template\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom modify_qwen2 import convert_kvcache_qwen2_heavy_recent, convert_qwen2_channel_config\n\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, default=\"meta-llama/Llama-3.1-8B\", help='Selected model')\n    parser.add_argument('--mode', type=str, default=\"ds\", choices=[\"ds\", \"ds-offload\", \"dense\"], help='Whether to use offloading')\n    parser.add_argument('--architecture', type=str, default=\"llama\", choices=[\"llama\", \"mistral\", \"mixtral\", \"qwen2\"])\n    parser.add_argument('--channel', type=str, default=\"q\", choices=[\"q\", \"k\", \"qk\"])\n    parser.add_argument('--heavy_const', type=int, default=128, help='Heavy constant')\n    parser.add_argument('--group_factor', type=int, default=2, help='Group factor')\n    parser.add_argument('--q_bits', type=int, default=2, help='Quantization bits')\n    parser.add_argument('--output_path', type=str, required=True, help='Prompt for generation')\n\n    args = parser.parse_args()\n\n\n    model_path = args.model_path\n    channel_path = \"../config/\" + model_path + \".json\"\n\n\n    kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}\n\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n\n    channel_config = None\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n\n    if args.mode == \"ds\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_llama_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_mistral_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"qwen2\":\n            model = convert_kvcache_qwen2_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_qwen2_channel_config(model, channel_config, args.channel)\n    elif args.mode == \"ds-offload\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_llama_offloading_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_mistral_offloading_channel_config(model, channel_config, args.channel)\n\n    \n    data_path = \"./data/aime24.jsonl\"\n    print(\"Reading data from: \", data_path)\n\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data = [json.loads(l) for l in f]\n\n    print(\"Save output to: \", args.output_path)\n    with open(args.output_path, 'w', encoding='utf-8') as g:\n        for item in tqdm(data):\n            prompt = item['prompt']\n            answer = item['answer']\n            question = \"<｜begin▁of▁sentence｜><｜User｜>\" + prompt + \"<｜Assistant｜><think>\\n\"\n\n            input_ids = tokenizer(question, return_tensors=\"pt\").input_ids.cuda()\n            max_new_tokens = 32000-input_ids.shape[-1]\n\n            output = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)[0][len(input_ids[0]):]\n            output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n            item['gen'] = output\n            print(output)\n            g.write(json.dumps(item, ensure_ascii=False) + '\\n')\n            g.flush()\n\n    # prompt = args.prompt\n    # prompt = \"<｜begin▁of▁sentence｜><｜User｜>\" + prompt + \"<｜Assistant｜><think>\\n\"\n    # input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n    # max_new_tokens = 8000-input_ids.shape[-1]\n\n    # output = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)[0]\n    # output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n    # print(output)\n\n\n    # prompt = \"<｜begin▁of▁sentence｜><｜User｜>Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<｜Assistant｜><think>\\n\"\n\n"}
{"type": "source_file", "path": "models/triton_kernels/argsort.py", "content": "import numpy as np\nimport torch\nimport triton\n\n\n# modified from triton.language.standard\n\n\n@triton.jit\ndef custom_compare_and_swap(x, indexes, desc_mask, n_dims: triton.language.constexpr, idx: triton.language.constexpr):\n    x_int = triton.language.standard._cast_to_int(x)\n    l_int = triton.language.standard._take_slice(x_int, n_dims, idx, 0)\n    r_int = triton.language.standard._take_slice(x_int, n_dims, idx, 1)\n    l = l_int.to(x.dtype, bitcast=True)\n    r = r_int.to(x.dtype, bitcast=True)\n\n    l_idx = triton.language.standard._take_slice(indexes, n_dims, idx, 0)\n    r_idx = triton.language.standard._take_slice(indexes, n_dims, idx, 1)\n\n    desc_mask = desc_mask.to(x_int.dtype)\n    zero = triton.language.zeros_like(x_int)\n    swap = (l > r) ^ desc_mask\n\n    y = x_int ^ triton.language.where(swap, l_int ^ r_int, zero)\n    indexes_y = indexes ^ triton.language.where(swap, l_idx ^ r_idx, triton.language.zeros_like(indexes))\n\n    y = y.to(x.dtype, bitcast=True)\n    return y, indexes_y\n\n\n@triton.jit\ndef custom_bitonic_merge(x, indexes, n_dims: triton.language.constexpr, active_dims: triton.language.constexpr, order_type: triton.language.constexpr):\n    '''\n    order_type 0 == ascending\n    order_type 1 == descending\n    order_type 2 == alternating\n    '''\n    triton.language.static_assert(active_dims <= n_dims)\n\n    if order_type == 2:\n        desc_mask = triton.language.standard._indicator(n_dims, active_dims, 1)\n    else:\n        desc_mask = order_type\n\n    for i in triton.language.static_range(active_dims):\n        x, indexes = custom_compare_and_swap(x, indexes, desc_mask, n_dims, active_dims - 1 - i)\n\n    return x, indexes\n\n\n@triton.jit\ndef argsort(x, indexes, dim=None, descending: triton.language.constexpr = 0):\n    triton.language.static_assert(triton.language.standard._is_power_of_two(x.shape[triton.language.standard._get_sort_dim(dim, x.shape)]))\n    triton.language.static_assert(triton.language.standard._is_power_of_two(x.numel))\n    # reshape the tensor to have all dimensions be 2.\n    # TODO: We shouldn't have to change the dimensions not sorted.\n    y = triton.language.reshape(x, [2] * triton.language.standard._log2(x.numel))\n    y_indexes = triton.language.reshape(indexes, [2] * triton.language.standard._log2(x.numel))\n    for i in triton.language.static_range(1, triton.language.standard._log2(x.shape[triton.language.standard._get_sort_dim(dim, x.shape)]) + 1):\n        y, y_indexes = custom_bitonic_merge(y, y_indexes, triton.language.standard._log2(x.numel), i, (descending if\n                                                  (i == triton.language.standard._log2(x.shape[triton.language.standard._get_sort_dim(dim, x.shape)])) else 2))\n\n    x = triton.language.reshape(y, x.shape)\n    indexes = triton.language.reshape(y_indexes, indexes.shape)\n    return x, indexes\n\n\n@triton.jit\ndef sort_kernel(X, Z, I, N: triton.language.constexpr, M: triton.language.constexpr, descending: triton.language.constexpr):\n    offx = triton.language.arange(0, M)\n    offy = triton.language.arange(0, N) * M\n    off2d = offx[None, :] + offy[:, None]\n    x = triton.language.load(X + off2d)\n    indexes = triton.language.arange(0,M)[None,:]\n    indexes = triton.language.broadcast_to(indexes, [N, M])\n    x, indexes = argsort(x, indexes, descending=descending)\n    # x = triton.language.sort(x, descending=descending)\n    triton.language.store(Z + off2d, x)\n    triton.language.store(I + off2d, indexes)\n\n\ndef test_argsort():\n\n    M = 256\n    N = 8\n    x = np.random.rand(N, M).astype(np.float32)\n    x = torch.from_numpy(x).to(\"cuda\")\n    y, i0 = torch.sort(x, descending=True)\n    z = torch.empty_like(x)\n    i = torch.empty_like(i0)\n    pgm = sort_kernel[(1, )](x, z, i, N, M, True, num_warps=8)\n    assert (y == z).all(), (y, z)\n    assert (i0 == i).all(), (i0, i)\n\n\nif __name__ == \"__main__\":\n    test_argsort()\n\n"}
{"type": "source_file", "path": "models/tp.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nfrom typing import Optional, List\n\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom torch.distributed import _functional_collectives as funcol\nfrom model import Transformer, Attention, FeedForward\nfrom quantize import WeightOnlyInt4Linear, WeightOnlyInt8Linear\n\n\ndef _get_rank() -> int:\n    return int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n\ndef is_local():\n    return _get_rank() == 0\n\ndef local_break():\n    if is_local():\n        breakpoint()\n    dist.barrier()\n\ndef _get_world_size() -> int:\n    return int(os.environ.get(\"LOCAL_WORLD_SIZE\", \"1\"))\n\ndef maybe_init_dist() -> Optional[int]:\n    try:\n        # provided by torchrun\n        rank = _get_rank()\n        world_size = _get_world_size()\n\n        if world_size < 2:\n            # too few gpus to parallelize, tp is no-op\n            return None\n    except KeyError:\n        # not run via torchrun, no-op\n        return None\n\n    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    return rank\n\n\ndef _apply_tp_linear(linear: nn.Linear, style: str, weight_splits: List[int] = []) -> None:\n    rank = _get_rank()\n    world_size = _get_world_size()\n\n    # Linear's weight matrix is transposed, and is of shape\n    # (linear.out_features, linear.in_features)\n    dim_lookup = {\n        \"colwise\": (0, \"out_features\"),\n        \"rowwise\": (1, \"in_features\")\n    }\n    assert style in dim_lookup\n    shard_dim, size_attr = dim_lookup[style]\n\n    # ensure we can shard evenly\n    assert getattr(linear, size_attr) % world_size == 0\n    def shard(x, dim):\n        assert x.size(dim=dim) % world_size == 0\n        return torch.tensor_split(x, world_size, dim=dim)[rank]\n\n    def shard_qkv(qkv, dim, weight_splits):\n        q, k, v = qkv.split(weight_splits, dim=dim)\n        q = shard(q, dim)\n        k = shard(k, dim)\n        v = shard(v, dim)\n        return torch.cat((q,k,v), dim=dim)\n\n    # shard\n    if weight_splits:\n        # attention\n        assert len(weight_splits) == 3\n\n        if isinstance(linear, WeightOnlyInt4Linear):\n            sharded_weight = shard_qkv(linear.weight, shard_dim, [i//8 for i in weight_splits])\n            linear.scales_and_zeros = shard_qkv(linear.scales_and_zeros, 1 - shard_dim, weight_splits)\n        else:\n            sharded_weight = shard_qkv(linear.weight, shard_dim, weight_splits)\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard_qkv(linear.scales, 0, weight_splits)\n    else:\n        sharded_weight = shard(linear.weight, shard_dim)\n        if isinstance(linear, WeightOnlyInt4Linear):\n            linear.scales_and_zeros = shard(linear.scales_and_zeros, 1 - shard_dim)\n            if style == \"rowwise\":\n                assert linear.scales_and_zeros.shape[0] * 32 == sharded_weight.shape[1] * sharded_weight.shape[2] * sharded_weight.shape[3]\n                assert linear.scales_and_zeros.shape[1] == sharded_weight.shape[0] * 8\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard(linear.scales, 0)\n\n    # local_break()\n    linear.weight = nn.Parameter(sharded_weight, requires_grad=False)\n    setattr(linear, size_attr, getattr(linear, size_attr) // world_size)\n\n    # shape info should still be synced\n    # assert linear.weight.shape == (linear.out_features, linear.in_features)\n\n\ndef _apply_tp_ffn(mlp: FeedForward) -> None:\n    assert hasattr(mlp, \"w1\")\n    assert hasattr(mlp, \"w3\")\n    assert hasattr(mlp, \"w2\")\n\n    _apply_tp_linear(mlp.w1, \"colwise\")\n    _apply_tp_linear(mlp.w3, \"colwise\")\n    _apply_tp_linear(mlp.w2, \"rowwise\")\n\n    world_size = _get_world_size()\n    mlp.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output, \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_attn(attn: Attention) -> None:\n    assert hasattr(attn, \"wqkv\")\n    assert hasattr(attn, \"wo\")\n\n    kv_size = attn.n_local_heads * attn.head_dim\n    _apply_tp_linear(attn.wqkv, \"colwise\", [attn.dim, kv_size, kv_size])\n    _apply_tp_linear(attn.wo, \"rowwise\")\n\n    # overwrite\n    world_size = _get_world_size()\n    attn.n_head = attn.n_head // world_size\n    attn.dim = attn.dim // world_size\n    attn.head_dim = attn.dim // attn.n_head\n    attn.n_local_heads = attn.n_local_heads // world_size\n\n    attn.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output[0], \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_Transformer(Transformer: Transformer) -> None:\n    # overwrite config before Transformer.setup_cache is called\n    world_size = _get_world_size()\n    Transformer.config.n_head = Transformer.config.n_head // world_size\n    Transformer.config.dim = Transformer.config.dim // world_size\n    Transformer.config.n_local_heads = Transformer.config.n_local_heads // world_size\n\n\ndef apply_tp(model: Transformer) -> None:\n    _apply_tp_Transformer(model)\n    for block in model.layers:\n        # Apply to MLP\n        _apply_tp_ffn(block.feed_forward)\n        _apply_tp_attn(block.attention)\n"}
{"type": "source_file", "path": "evaluation/streaming_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n        t = t / self.scaling_factor\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # streaming config\n        self.local_const = 2044\n        self.sink_const = 4\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        kv_seq_len = key_states.shape[-2]\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        \n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n\n        # local\n        # streaming_mask = torch.tril(torch.ones(kv_seq_len, kv_seq_len), diagonal=-self.local_const).bool().to(attn_weights.device)\n        # input_pos = torch.arange(kv_seq_len-q_len, kv_seq_len, device=attn_weights.device)\n        # streaming_mask = streaming_mask[input_pos]\n        # # sink\n        # streaming_mask[:, :self.sink_const] = False\n        # attn_weights.masked_fill_(streaming_mask[None,None,:,:], float('-inf'))\n        \n        streaming_mask = torch.ones((1, kv_seq_len), device=attn_weights.device).bool()\n        # local\n        streaming_mask[:, -self.local_const:] = False\n        # sink\n        streaming_mask[:, :self.sink_const] = False\n        attn_weights.masked_fill_(streaming_mask[None,None,:,:], float('-inf'))\n\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_streaming(model, config, local_const, sink_const):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_streaming(module, config, local_const, sink_const)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.local_const = local_const\n            new_module.sink_const = sink_const\n            model._modules[name] = new_module\n\n    return model\n\n\ndef change_streaming_para(model, local_const, sink_const):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.local_const = local_const\n            module.sink_const = sink_const\n\n    return model"}
{"type": "source_file", "path": "models/model.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import Tensor\n\nfrom triton_kernels.channel import get_label_tensor\nfrom triton_kernels.sparse import fwd_sparse, torch_fwd_sparse, fwd_sparse_no_mask\n# from triton_kernels.heavy import get_heavy\nfrom triton_kernels.bgemv import bgemv\nfrom triton_kernels.bgemv_int8 import bgemv_int8\n\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n@dataclass\nclass ModelArgs:\n    block_size: int = 16384\n    vocab_size: int = 32000\n    n_layer: int = 32\n    n_head: int = 32\n    dim: int = 4096\n    intermediate_size: int = None\n    n_local_heads: int = -1\n    head_dim: int = 64\n    rope_base: float = 40000 # TODO: add config for vicuna-16k\n    norm_eps: float = 1e-5\n    heavy_const: int = 256\n    heavy_channel_num: int = 32\n\n\n    def __post_init__(self):\n        if self.n_local_heads == -1:\n            self.n_local_heads = self.n_head\n        if self.intermediate_size is None:\n            hidden_dim = 4 * self.dim\n            n_hidden = int(2 * hidden_dim / 3)\n            self.intermediate_size = find_multiple(n_hidden, 256)\n        self.head_dim = self.dim // self.n_head\n\n    @classmethod\n    def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        # fuzzy search\n        config = [config for config in transformer_configs if config in str(name).upper() or config in str(name)]\n        assert len(config) == 1, name\n        return cls(**transformer_configs[config[0]])\n\n\ntransformer_configs = {\n    \"CodeLlama-7b-Python-hf\": dict(block_size=16384, vocab_size=32000, n_layer=32, dim = 4096, rope_base=1000000),\n    \"7B\": dict(n_layer=32, n_head=32, dim=4096),\n    \"13B\": dict(n_layer=40, n_head=40, dim=5120),\n    \"30B\": dict(n_layer=60, n_head=52, dim=6656),\n    \"34B\": dict(n_layer=48, n_head=64, dim=8192, vocab_size=32000, n_local_heads=8, intermediate_size=22016, rope_base=1000000), # CodeLlama-34B-Python-hf\n    \"70B\": dict(n_layer=80, n_head=64, dim=8192, n_local_heads=8, intermediate_size=28672),\n}\n\nclass KVCache(nn.Module):\n    def __init__(self, max_batch_size, max_seq_length, n_heads, head_dim, heavy_channel_num, dtype=torch.bfloat16):\n        super().__init__()\n        self.max_batch_size = max_batch_size\n        self.max_seq_length = max_seq_length\n        cache_shape = (max_batch_size, max_seq_length, n_heads, head_dim)\n        self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))\n        self.register_buffer('v_cache', torch.zeros(cache_shape, dtype=dtype))\n        self.register_buffer('k_label', torch.zeros((max_batch_size, max_seq_length, n_heads, heavy_channel_num), dtype=dtype))\n\n        # store qk tmp label while decoding\n        # self.register_buffer('tmp_label', torch.zeros((max_batch_size * 2, n_heads, heavy_channel_num), dtype=dtype))\n        # store tmp label scores while decoding\n        # self.register_buffer('label_scores', torch.zeros((max_batch_size, n_heads, max_seq_length), dtype=dtype))\n        # store tmp attn output while decoding\n        self.register_buffer('attn_out', torch.zeros((max_batch_size, n_heads, head_dim), dtype=dtype))\n\n    def update(self, input_pos, k_val, v_val):\n        # input_pos: [S], k_val: [B, S, H, D]\n        assert input_pos.shape[0] == k_val.shape[1]\n\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, input_pos] = k_val\n        v_out[:, input_pos] = v_val\n        return k_out, v_out\n\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.config = config\n\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n        self.layers = nn.ModuleList(TransformerBlock(config) for _ in range(config.n_layer))\n        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n        self.freqs_cis: Optional[Tensor] = None\n        self.mask_cache: Optional[Tensor] = None\n        self.max_batch_size = -1\n        self.max_seq_length = -1\n\n    def setup_caches(self, max_batch_size, max_seq_length):\n        if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads, head_dim, self.config.heavy_channel_num)\n\n        self.freqs_cis = precompute_freqs_cis(self.config.block_size, self.config.dim // self.config.n_head, self.config.rope_base)\n        self.prefill_mask = torch.tril(torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool))\n\n        # TODO: change 16 to 32\n        self.label_mask = torch.zeros(self.max_seq_length, self.max_seq_length, dtype=torch.float16)\n        self.label_mask = self.label_mask.masked_fill(self.prefill_mask == False, float('-inf'))\n\n        # TODO: change 16 to 32\n        self.attn_mask = torch.zeros(self.max_seq_length, self.config.heavy_const, dtype=torch.float16)\n        self.attn_mask = self.attn_mask.masked_fill(torch.tril(torch.ones(self.max_seq_length, self.config.heavy_const, dtype=torch.bool)) == False, float('-inf'))\n\n\n\n    def forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n\n        # is_prefill = input_pos.shape[-1] > 1\n\n        # if is_prefill:\n        #     mask1 = self.prefill_mask[None, None, input_pos] # [B, H, S, S]\n        #     mask2 = None\n        # else:\n        #     # TODO: this is a shortcut, the mask broadcast should be rewritten\n        #     mask1 = self.label_mask[None, input_pos] # [1, 1, S]\n        #     mask2 = self.attn_mask[input_pos] # [1, HEAVY_CONST] \n\n        mask1 = self.label_mask[None, None, input_pos] # [B, H, S, S]\n        mask2 = torch.zeros(1, self.config.heavy_const, dtype=torch.float16).cuda() # [1, HEAVY_CONST]\n\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask1, mask2)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n    \n    def sparse_forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n\n        mask1 = self.label_mask[None, None, input_pos]\n        mask2 = torch.zeros(1, self.config.heavy_const, dtype=torch.float16).cuda()\n\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n\n        for i, layer in enumerate(self.layers):\n            x = layer.sparse_forward(x, input_pos, freqs_cis, mask1, mask2)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.attention = Attention(config)\n        self.feed_forward = FeedForward(config)\n        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n\n    def forward(self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask1: Tensor, mask2: Tensor) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), freqs_cis, mask1, mask2, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n    def sparse_forward(self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask1: Tensor, mask2: Tensor) -> Tensor:\n        h = x + self.attention.sparse_forward(self.attention_norm(x), freqs_cis, mask1, mask2, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        assert config.dim % config.n_head == 0\n\n        total_head_dim = (config.n_head + 2 * config.n_local_heads) * config.head_dim\n        # key, query, value projections for all heads, but in a batch\n        self.wqkv = nn.Linear(config.dim, total_head_dim, bias=False)\n        self.wo = nn.Linear(config.dim, config.dim, bias=False)\n        self.kv_cache = None\n\n        self.n_head = config.n_head\n        self.head_dim = config.head_dim\n        self.n_local_heads = config.n_local_heads\n        self.dim = config.dim\n        self._register_load_state_dict_pre_hook(self.load_hook)\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = config.heavy_const\n        self.heavy_channel_num = config.heavy_channel_num\n\n    def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])\n\n    def forward(self, x: Tensor, freqs_cis: Tensor, mask1: Tensor, mask2: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        \n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q = apply_rotary_emb(q, freqs_cis)\n        k = apply_rotary_emb(k, freqs_cis)\n\n        tmp_labels = torch.empty((bsz * seqlen, self.n_head, self.heavy_channel_num), dtype=self.kv_cache.k_label.dtype, device='cuda')\n        get_label_tensor(k.view(bsz * seqlen, self.n_local_heads, self.head_dim), self.sorted_channel, tmp_labels, self.heavy_channel_num)\n\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n            self.kv_cache.k_label[:, input_pos] = tmp_labels.view(bsz, seqlen, self.n_head, self.heavy_channel_num)\n\n        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n        k = k.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        v = v.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n        attn_weights += mask1\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n        y = torch.matmul(attn_weights, v)\n        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)\n        y = self.wo(y)\n\n        return y\n\n    def sparse_forward(self, x: Tensor, freqs_cis: Tensor, mask1: Tensor, mask2: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q = apply_rotary_emb(q, freqs_cis)\n        k = apply_rotary_emb(k, freqs_cis)\n\n        tmp_labels = torch.empty((bsz * seqlen, self.n_head, self.heavy_channel_num), dtype=self.kv_cache.k_label.dtype, device='cuda')\n        get_label_tensor(k.view(bsz * seqlen, self.n_local_heads, self.head_dim), self.sorted_channel, tmp_labels, self.heavy_channel_num)\n\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n            self.kv_cache.k_label[:, input_pos] = tmp_labels.view(bsz, seqlen, self.n_head, self.heavy_channel_num)\n\n        get_label_tensor(q.view(bsz, self.n_head, self.head_dim), self.sorted_channel, tmp_labels, self.heavy_channel_num)\n        label_scores = torch.matmul(tmp_labels.view(bsz, 1, self.n_head, self.heavy_channel_num).transpose(1,2), self.kv_cache.k_label.view(bsz, -1, self.n_head, self.heavy_channel_num).transpose(1,2).transpose(2, 3)).view(bsz, self.n_head, 1, -1)\n        label_scores += mask1\n        _, label_index = torch.topk(label_scores, self.heavy_const, dim=-1)\n        # fwd_sparse(q.view(-1, self.n_head, self.head_dim), k.view(-1, self.n_local_heads, self.head_dim), v.view(-1, self.n_local_heads, self.head_dim), self.kv_cache.attn_out, label_index.view(bsz, self.n_head, -1), mask2)\n        fwd_sparse_no_mask(q.view(-1, self.n_head, self.head_dim), k.view(-1, self.n_local_heads, self.head_dim), v.view(-1, self.n_local_heads, self.head_dim), self.kv_cache.attn_out, label_index.view(bsz, self.n_head, -1))\n        y = self.wo(self.kv_cache.attn_out.view(bsz, seqlen, self.dim))\n\n        return y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w3 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w2 = nn.Linear(config.intermediate_size, config.dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef precompute_freqs_cis(\n    seq_len: int, n_elem: int, base: int = 10000\n) -> Tensor:\n    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=torch.bfloat16)\n\n\ndef apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)\n\n\ndef init_model_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, Attention):\n            \n            layer_idx = int(name.split(\".\")[1])\n            key = \"model.layers.\" + str(layer_idx) + \".self_attn\" + selected_channel\n            \n            module.sorted_channel = permute_channel_config(torch.tensor(channel_config[key]))[:,:module.heavy_channel_num].contiguous().cuda()\n\n    return model\n\ndef permute_channel_config(sorted_channel):\n    head_num = sorted_channel.shape[0]\n    head_dim = sorted_channel.shape[1]\n    return (sorted_channel * 2) % head_dim + (sorted_channel * 2) // head_dim\n\n"}
{"type": "source_file", "path": "evaluation/sparq_ppl.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\nfrom fastchat.model import get_conversation_template\n# from modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom sparq_llama import convert_sparq\n\n\nif __name__ == \"__main__\":\n\n    # llama-2-7b: 5.47\n    # model_path = \"meta-llama/Llama-2-7b-hf\"\n    # channel_path = \"llama2-7b-channel-config.json\"\n    # channel_path = \"llama2-7b-qk-channel-config.json\"\n\n    # llama-2-7b-chat: 6.94\n    model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n\n    # llama-7b: 5.68\n    # model_path = \"/home/ec2-user/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16\"\n\n    # opt-6.7b: 10.86\n    # model_path = \"/home/ec2-user/.cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0\"\n\n    # model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n    model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = AutoTokenizer.from_pretrained(model_path)\n    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n\n    config = AutoConfig.from_pretrained(model_path)\n\n    # sparq\n    k = 32\n    r = 8\n    model = convert_sparq(model, config, k, r)\n\n    prompt = \"Hello, my name is\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n    max_new_tokens = 2048-input_ids.shape[-1]\n\n    output = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)[0]\n    output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n    print(output)\n"}
{"type": "source_file", "path": "models/quantize.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport importlib\nimport time\nfrom math import ceil\nfrom pathlib import Path\n\nimport torch\nimport importlib\nimport time\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nfrom sentencepiece import SentencePieceProcessor\n\ntry:\n    from GPTQ import GenericGPTQRunner, InputRecorder, lm_eval\nexcept:\n    pass\n\nfrom model import Transformer\n\n##### Quantization Primitives ######\n\ndef dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    # assumes symmetric quantization\n    # assumes axis == 0\n    # assumes dense memory format\n    # TODO(future): relax ^ as needed\n\n    # default setup for affine quantization of activations\n    eps = torch.finfo(torch.float32).eps\n\n    # get min and max\n    min_val, max_val = torch.aminmax(x, dim=1)\n\n    # calculate scales and zero_points based on min and max\n    # reference: https://fburl.com/code/srbiybme\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n\n    # reference: https://fburl.com/code/4wll53rk\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    # ensure scales is the same dtype as the original tensor\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n\n    # quantize based on qmin/qmax/scales/zp\n    # reference: https://www.internalfb.com/code/fbsource/[8edc275012b1]/fbcode/caffe2/torch/ao/quantization/fx/_decomposed.py?lines=63\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n\n    return quant, scales, zero_points\n\ndef get_group_qparams(w, n_bit=4, groupsize=128):\n    # needed for GPTQ with padding\n    if groupsize > w.shape[-1]:\n        groupsize = w.shape[-1]\n    assert groupsize > 1\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    max_val = to_quant.amax(dim=1, keepdim=True)\n    min_val = to_quant.amin(dim=1, keepdim=True)\n    max_int = 2**n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-6) / max_int\n    zeros = min_val + scales * (2 ** (n_bit - 1))\n    return scales.to(torch.bfloat16).reshape(w.shape[0], -1), zeros.to(\n        torch.bfloat16\n    ).reshape(w.shape[0], -1)\n\n\ndef pack_scales_and_zeros(scales, zeros):\n    assert scales.shape == zeros.shape\n    assert scales.dtype == torch.bfloat16\n    assert zeros.dtype == torch.bfloat16\n    return (\n        torch.cat(\n            [\n                scales.reshape(scales.size(0), scales.size(1), 1),\n                zeros.reshape(zeros.size(0), zeros.size(1), 1),\n            ],\n            2,\n        )\n        .transpose(0, 1)\n        .contiguous()\n    )\n\n\ndef unpack_scales_and_zeros(scales_and_zeros):\n    assert len(scales_and_zeros.shape) == 3 and scales_and_zeros.shape[2] == 2\n    assert scales_and_zeros.dtype == torch.float\n    return torch.split(scales_and_zeros.transpose(0, 1), 1, 2)\n\n\ndef group_quantize_tensor_from_qparams(w, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column quantize\n    if groupsize > w.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w.shape[-1]\n\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n    min_val = zeros - scales * (2 ** (n_bit - 1))\n    max_int = 2**n_bit - 1\n    min_int = 0\n    w_int32 = (\n        to_quant.sub(min_val)\n        .div(scales)\n        .round()\n        .clamp_(min_int, max_int)\n        .to(torch.int32)\n        .reshape_as(w)\n    )\n\n    return w_int32\n\n\ndef group_quantize_tensor(w, n_bit=4, groupsize=128):\n    scales, zeros = get_group_qparams(w, n_bit, groupsize)\n    w_int32 = group_quantize_tensor_from_qparams(w, scales, zeros, n_bit, groupsize)\n    scales_and_zeros = pack_scales_and_zeros(scales, zeros)\n    return w_int32, scales_and_zeros\n\n\ndef group_dequantize_tensor_from_qparams(\n    w_int32, scales, zeros, n_bit=4, groupsize=128\n):\n    assert groupsize > 1\n    # needed for GPTQ single column dequantize\n    if groupsize > w_int32.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w_int32.shape[-1]\n    assert w_int32.shape[-1] % groupsize == 0\n    assert w_int32.dim() == 2\n\n    w_int32_grouped = w_int32.reshape(-1, groupsize)\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n\n    w_dq = (\n        w_int32_grouped.sub(2 ** (n_bit - 1)).mul(scales).add(zeros).reshape_as(w_int32)\n    )\n    return w_dq\n\n\ndef group_dequantize_tensor(w_int32, scales_and_zeros, n_bit=4, groupsize=128):\n    scales, zeros = unpack_scales_and_zeros(scales_and_zeros)\n    return group_dequantize_tensor_from_qparams(\n        w_int32, scales, zeros, n_bit, groupsize\n    )\n\nclass QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    def create_quantized_state_dict(self) -> \"StateDict\":\n        pass\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\nclass GPTQQuantHandler(QuantHandler):\n    \"\"\"\n    This class implements a GPTQ QuantHandler that can be used to apply GPTQ to a model in concert with the GenericGPTQRunner class.\n    Unlike the base QuantHandler class, the user does not need to implement the create_quantized_state_dict, instead they have to reimplement\n    __init__ such that it defines the functions for the quantization mode. User is expected to reimplement convert_for_runtime.\n\n    The following functions (which must be defined in __init__) are used to define the quantization mode for both GPTQ and\n    create_quantized_state_dict. Here is a description of each function.\n\n    get_qparams_func:\n        A function that calculates the quantization qparams for an input tensor.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            qparams: it can have any format but will need to be handled by the other defined functions below.\n\n    quantize_func:\n        A function that applies quantization to an input tensor. It should be noted\n        that this function needs to be able to handle quantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n            qparams: the output from get_qparams_func\n        Returns:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n\n\n    dequantize_func:\n        A function that dequantizes an input quantized weight tensor. It should be noted\n        that this function needs to be able to handle dequantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            weight: A 2d weight tensor with non-integer dtype.\n\n    combine_qparams_list_func:\n        A function that combines several qparams into one qparam.\n        Args:\n            qparams_list: a list of qparams objects, each obtained by calling get_qparams_func\n            on a single group from a weight tensor\n        Returns:\n            qparams: an object of the same format as the qparams above.\n\n    skip_layer_func:\n        A function that determines which linear layers should be skipped during GPTQ\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            skip: boolean indicating whether layer should be skipped\n\n    make_names_and_values_dict_func:\n        A function that prepares the qparams and quantized_weight and creates a dictionary indicating how they\n        should be inserted into the state_dict. Generally any packing of the weight and qparams should be done here.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            names_and_values_dict: a dictionary mapping the name of the parameters of the quantized module to the\n            corresponding quantized weights and qparams.\n    \"\"\"\n    def __init__(self):\n        assert self.mod is not None\n        assert self.get_qparams_func is not None\n        assert self.quantize_func is not None\n        assert self.dequantize_func is not None\n        assert self.combine_qparams_list_func is not None\n        assert self.make_names_and_values_dict_func is not None\n\n    @staticmethod\n    def get_inputs(model, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs) -> \"MultiInput\":\n        input_recorder = InputRecorder(\n            model,\n            tokenizer,\n            calibration_seq_length,\n            pad_calibration_inputs,\n        )\n        task_dict = lm_eval.tasks.get_task_dict(calibration_tasks)\n        print(\"Obtaining GPTQ calibration inputs on: \", calibration_tasks)\n        lm_eval.evaluator.evaluate(\n            input_recorder,\n            task_dict,\n            limit=calibration_limit,\n        )\n        inputs = input_recorder.get_recorded_inputs()\n        print(f\"Obtained {len(inputs[0].values)} calibration samples\")\n        return inputs\n\n    @torch.no_grad()\n    def create_quantized_state_dict(\n        self,\n        tokenizer,\n        blocksize,\n        percdamp,\n        groupsize,\n        calibration_tasks,\n        calibration_limit,\n        calibration_seq_length,\n        pad_calibration_inputs,\n    ) -> \"StateDict\":\n        inputs = GPTQQuantHandler.get_inputs(self.mod, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs)\n        print(\"Tracing model for GPTQ\")\n        GPTQ_runner = GenericGPTQRunner(\n            self.mod,\n            inputs,\n            blocksize,\n            percdamp,\n            groupsize,\n        ).configure_quantization_mode(\n            self.get_qparams_func,\n            self.quantize_func,\n            self.dequantize_func,\n            self.combine_qparams_list_func,\n            self.make_names_and_values_dict_func,\n            self.skip_layer_func\n        )\n\n        print(\"Applying GPTQ to weights\")\n        GPTQ_runner.run()\n        return GPTQ_runner.get_quantized_state_dict()\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\n##### Weight-only int8 per-channel quantized code ######\n\ndef replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(module, name, WeightOnlyInt8Linear(child.in_features, child.out_features))\n        else:\n            replace_linear_weight_only_int8_per_channel(child)\n\nclass WeightOnlyInt8QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                int8_weight, scales, _ = dynamically_quantize_per_channel(mod.weight.float(), -128, 127, torch.int8)\n                cur_state_dict[f\"{fqn}.weight\"] = int8_weight\n                cur_state_dict[f\"{fqn}.scales\"] = scales.to(mod.weight.dtype)\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_weight_only_int8_per_channel(self.mod)\n        return self.mod\n\n\nclass WeightOnlyInt8Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.register_buffer(\"weight\", torch.empty((out_features, in_features), dtype=torch.int8))\n        self.register_buffer(\"scales\", torch.ones(out_features, dtype=torch.bfloat16))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.to(dtype=input.dtype)) * self.scales\n\n##### weight only int4 per channel groupwise quantized code ######\n\ndef prepare_int4_weight_and_scales_and_zeros(weight_bf16, groupsize, inner_k_tiles):\n    weight_int32, scales_and_zeros = group_quantize_tensor(\n        weight_bf16, n_bit=4, groupsize=groupsize\n    )\n    weight_int4pack = torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)\n    return weight_int4pack, scales_and_zeros\n\n\ndef linear_forward_int4(x, weight_int4pack, scales_and_zeros, out_features, groupsize):\n    origin_x_size = x.size()\n    x = x.reshape(-1, origin_x_size[-1])\n    c = torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)\n    new_shape = origin_x_size[:-1] + (out_features,)\n    c = c.reshape(new_shape)\n    return c\n\n\ndef _check_linear_int4_k(k, groupsize = 1, inner_k_tiles = 1):\n    return k % groupsize == 0 and k % (inner_k_tiles * 16) == 0\n\ndef replace_linear_int4(module, groupsize, inner_k_tiles, padding):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            if _check_linear_int4_k(child.in_features, groupsize, inner_k_tiles):\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=False,\n                ))\n            elif padding:\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=True,\n                ))\n        else:\n            replace_linear_int4(child, groupsize, inner_k_tiles, padding)\n\n\nclass WeightOnlyInt4QuantHandler:\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        assert groupsize in [32, 64, 128, 256]\n        assert inner_k_tiles in [2, 4, 8]\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                assert not mod.bias\n                out_features = mod.out_features\n                in_features = mod.in_features\n                assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n                print(f\"linear: {fqn}, in={in_features}, out={out_features}\")\n\n                weight = mod.weight.data\n                if not _check_linear_int4_k(in_features, self.groupsize, self.inner_k_tiles):\n                    if self.padding:\n                        from model import find_multiple\n                        import torch.nn.functional as F\n                        print(f\"warning: {fqn} is padded to satisfy in_features % 1024 == 0\")\n                        padded_in_features = find_multiple(in_features, 1024)\n                        weight = F.pad(weight, pad=(0, padded_in_features - in_features))\n                    else:\n                        print(f\"warning: {fqn} is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, \" +\n                            \"and that groupsize and inner_k_tiles*16 evenly divide into it\")\n                        continue\n                weight_int4pack, scales_and_zeros = prepare_int4_weight_and_scales_and_zeros(\n                    weight.to(torch.bfloat16).to('cuda'), self.groupsize, self.inner_k_tiles\n                )\n                cur_state_dict[f\"{fqn}.weight\"] = weight_int4pack.to('cpu')\n                cur_state_dict[f\"{fqn}.scales_and_zeros\"] = scales_and_zeros.to('cpu')\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4GPTQQuantHandler(GPTQQuantHandler):\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        from model import find_multiple\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        self.get_qparams_func = lambda w: get_group_qparams(w, 4, groupsize)\n        self.quantize_func = lambda w, qparams: \\\n            group_quantize_tensor_from_qparams(w, qparams[0], qparams[1], 4, groupsize)\n        self.dequantize_func = lambda q, qparams: \\\n            group_dequantize_tensor_from_qparams(q, qparams[0], qparams[1], 4, groupsize).float()\n        self.combine_qparams_list_func = lambda qparams_list: \\\n            [torch.cat(x, dim=1) for x in zip(*qparams_list)]\n        # skip unless padding=True or its correctly sized\n        self.skip_layer_func = lambda linear_weight: not (\n            _check_linear_int4_k(linear_weight.shape[-1], groupsize, inner_k_tiles) or padding\n        )\n        # we need to do the padding here, both for q and the qparams if necessary\n        def make_names_and_values_dict_func(q, qparams):\n            k = q.shape[1]\n            new_k = find_multiple(k, 1024)\n            # how much we need to pad the weight\n            delta_k = new_k - q.shape[1]\n            final_q = torch.ops.aten._convert_weight_to_int4pack(F.pad(q, pad=(0, delta_k)), inner_k_tiles)\n            scales_and_zeros = pack_scales_and_zeros(*qparams)\n            # how many new groups we need for padded weight\n            delta_groups = new_k // groupsize - scales_and_zeros.shape[0]\n            final_s_and_z = F.pad(scales_and_zeros, pad=(0,0,0,0,0, delta_groups), value=1)\n            return {\"weight\": final_q, \"scales_and_zeros\": final_s_and_z}\n        self.make_names_and_values_dict_func = make_names_and_values_dict_func\n        super().__init__()\n\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(\n            self, in_features: int, out_features: int,\n            bias=True, device=None, dtype=None, groupsize: int = 128, inner_k_tiles: int = 8, padding: bool = True,\n    ) -> None:\n        super().__init__()\n        self.padding = padding\n        if padding:\n            from model import find_multiple\n            self.origin_in_features = in_features\n            in_features = find_multiple(in_features, 1024)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias, \"require bias=False\"\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n\n        assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n        assert in_features % (inner_k_tiles * 16) == 0, \"require in_features % (innerKTiles * 16) == 0\"\n        self.register_buffer(\n            \"weight\",\n            torch.empty((out_features // 8, in_features // (inner_k_tiles * 16), 32, inner_k_tiles // 2), dtype=torch.int32)\n        )\n        self.register_buffer(\n            \"scales_and_zeros\",\n            torch.empty((in_features // groupsize, out_features, 2), dtype=torch.bfloat16)\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        input = input.to(torch.bfloat16)\n        if self.padding:\n            import torch.nn.functional as F\n            input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))\n        return linear_forward_int4(\n            input,\n            self.weight, self.scales_and_zeros, self.out_features, self.groupsize\n        )\n\n\ndef quantize(\n    checkpoint_path: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"),\n    mode: str = 'int8',\n    # following arguments only available when setting int4 quantization.\n    groupsize: int = 128,\n    # following arguments only used for GPTQ\n    calibration_tasks: list = [\"hellaswag\"],\n    calibration_limit: int = 1000,\n    calibration_seq_length: int = 100,\n    pad_calibration_inputs: bool = False,\n    percdamp: float = .01,\n    blocksize: int = 128,\n    label: str = '',\n) -> None:\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    device = 'cpu'\n    precision = torch.bfloat16\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n    model = model.to(dtype=precision, device=device)\n\n    if mode == 'int8':\n        print(\"Quantizing model weights for int8 weight-only symmetric per-channel quantization\")\n        quant_handler = WeightOnlyInt8QuantHandler(model)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f'{label}int8.pth')\n\n    elif mode == 'int4':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization\")\n        quant_handler = WeightOnlyInt4QuantHandler(model, groupsize)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4.g{groupsize}.pth\")\n\n    elif mode == 'int4-gptq':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization using GPTQ...\")\n        quant_handler = WeightOnlyInt4GPTQQuantHandler(model, groupsize)\n\n        tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n        assert tokenizer_path.is_file(), tokenizer_path\n        tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n\n        quantized_state_dict = quant_handler.create_quantized_state_dict(\n            tokenizer,\n            blocksize,\n            percdamp,\n            groupsize,\n            calibration_tasks,\n            calibration_limit,\n            calibration_seq_length,\n            pad_calibration_inputs\n        )\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4-gptq.g{groupsize}.pth\")\n    else:\n        raise ValueError(f\"Invalid quantization mode {mode} needs to be one of [int8, int4, int4-gpptq]\")\n\n    quantize_path = dir_name / new_base_name\n    print(f\"Writing quantized weights to {quantize_path}\")\n    quantize_path.unlink(missing_ok=True) # remove existing file if one already there\n    torch.save(quantized_state_dict, quantize_path)\n    print(f\"Quantization complete took {time.time() - t0:.02f} seconds\")\n    return\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Quantize a model.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"), help='Path to the model checkpoint to be quantized.')\n    parser.add_argument('--mode', '-q', type=str, default='int8', choices=['int8', 'int4', 'int4-gptq'], help='type of quantization to perform')\n    parser.add_argument('--groupsize', type=int, default=32, help='Group size for int4 quantization.')\n    parser.add_argument('--calibration_tasks', type=str, nargs='+', default=['hellaswag'], help='tasks to do gptq calibration on, if doing gptq')\n    parser.add_argument('--calibration_limit', type=int, default=1000, help='number of samples to use for gptq calibration')\n    parser.add_argument('--calibration_seq_length', type=int, default=100, help='length of sequences to use for gptq calibration')\n    parser.add_argument('--pad_calibration_inputs', type=bool, default=False, help='pads sequences shorter than calibration_seq_length to that length, yielding more calibration inputs but running much slower')\n    parser.add_argument('--percdamp', type=float, default=.01, help='gptq percentage dampening')\n    parser.add_argument('--blocksize', type=int, default=128, help='blocksize for gptq')\n    parser.add_argument('--label', type=str, default='_', help='label to add to output filename')\n\n    args = parser.parse_args()\n    quantize(args.checkpoint_path, args.mode, args.groupsize, args.calibration_tasks, args.calibration_limit, args.calibration_seq_length, args.pad_calibration_inputs, args.percdamp, args.blocksize, args.label)\n"}
{"type": "source_file", "path": "models/generate.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple\nimport itertools\nimport torch\nimport json\n\nimport torch._inductor.config\nimport torch._dynamo.config\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# torch._inductor.config.fx_graph_cache = True # Experimental feature to reduce compilation times, will be on by default in future\n\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer, init_model_channel_config\nfrom tp import maybe_init_dist\nfrom sentencepiece import SentencePieceProcessor\n\ndef multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\ndef logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    logits = logits / max(temperature, 1e-5)\n\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        pivot = v.select(-1, -1).unsqueeze(-1)\n        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\ndef sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n    # assert torch.allclose(logits[0], logits[1], atol=1e-6, rtol=1e-6)\n    # idx_next = multinomial_sample_one_no_sync(probs)\n    idx_next = torch.argmax(probs, dim=-1, keepdim=True).to(dtype=torch.int) # TODO: change the sampling method\n    return idx_next, probs\n\ndef prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n    # input_pos: [B, S]\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\ndef decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\ndef sparse_decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    logits = model.sparse_forward(x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\ndef decode_n_tokens(model: Transformer, cur_token: torch.Tensor, input_pos: torch.Tensor, num_new_tokens: int, callback=lambda _: _, **sampling_kwargs):\n    new_tokens, new_probs = [], []\n    heavy_const = model.config.heavy_const\n    normal_token_num = heavy_const - input_pos[0] if heavy_const > input_pos[0] else 0\n    sparse_token_num = num_new_tokens - normal_token_num\n    for i in range(normal_token_num):\n        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True): # Actually better for Inductor to codegen attention here\n            next_token, next_prob = decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n        input_pos += 1\n        new_tokens.append(next_token.clone())\n        callback(new_tokens[-1])\n        new_probs.append(next_prob.clone())\n        # cur_token = next_token.view(1, -1)\n        cur_token = next_token.clone()\n\n    for i in range(sparse_token_num):\n        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):\n            next_token, next_prob = sparse_decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n        input_pos += 1\n        new_tokens.append(next_token.clone())\n        callback(new_tokens[-1])\n        new_probs.append(next_prob.clone())\n        # cur_token = next_token.view(1, -1)\n        cur_token = next_token.clone()\n    \n    return new_tokens, new_probs\n\n\ndef model_forward(model, x, input_pos):\n    return model(x, input_pos)\n\ndef speculative_decode(\n    model: Transformer,\n    draft_model: Transformer,\n    cur_token: torch.Tensor,\n    input_pos: int,\n    speculate_k: int,\n    **sampling_kwargs\n) -> torch.Tensor:\n    # draft model inference sequentially\n    device = cur_token.device\n    orig_input_pos = torch.tensor([input_pos], dtype=torch.int64, device=cur_token.device)\n    draft_tokens, draft_probs = decode_n_tokens(draft_model, cur_token.view(1, -1), orig_input_pos.clone(), speculate_k, **sampling_kwargs)\n\n    draft_tokens = torch.cat(draft_tokens)\n    # parallel inference on target model using draft tokens\n    target_logits = model_forward(\n        model,\n        torch.cat([cur_token.view(1), draft_tokens]).view(1, -1),\n        torch.arange(input_pos, input_pos + speculate_k + 1, device=cur_token.device)\n    )\n    target_probs = logits_to_probs(target_logits[0], **sampling_kwargs)\n    draft_probs = torch.stack(draft_probs)\n    # q: target prob, p: draft prob\n    # q >= p: always accept draft token\n    # q < p: q/p prob to accept draft token\n    p = draft_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    q = target_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    accept_draft_prob = torch.minimum(torch.ones(()), q[:speculate_k]/ p)\n    rejected_locations = (torch.rand_like(accept_draft_prob) > accept_draft_prob).nonzero()\n\n    if rejected_locations.shape[0] == 0: # All draft tokens have been accepted\n        accept_length = speculate_k + 1\n        last_token = multinomial_sample_one_no_sync(target_probs[-1])\n        # fill last token into draft model\n        model_forward(\n            draft_model,\n            draft_tokens[-1].view(1, -1),\n            orig_input_pos + speculate_k,\n        )\n        return torch.cat([draft_tokens, last_token])\n    else:\n        accept_length = rejected_locations[0].item()\n        p = draft_probs[accept_length]\n        q = target_probs[accept_length]\n        new = q - p\n        new = torch.where(new > 0, new, 0.0)\n        new = new / new.sum()\n        next_token = multinomial_sample_one_no_sync(new)\n        return torch.cat([draft_tokens[:accept_length], next_token])\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    interactive: bool,\n    draft_model: Transformer,\n    speculate_k: Optional[int] = 8,\n    callback = lambda x: x,\n    **sampling_kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    is_speculative = draft_model is not None\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(1)\n    batch_size = prompt.size(0)\n    T_new = T + max_new_tokens\n    if interactive:\n        max_seq_length = 350\n    else:\n        max_seq_length = min(T_new, model.config.block_size)\n        print(f\"Max seq length: {max_seq_length}\")\n        print(f\"Model block size: {model.config.block_size}\")\n\n    device, dtype = prompt.device, prompt.dtype\n    max_seq_length = max_seq_length + speculate_k + 1 if is_speculative else max_seq_length\n    with torch.device(device):\n        model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n        if is_speculative and draft_model is not model:\n            draft_model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty((batch_size, T_new), dtype=dtype, device=device)\n    empty[:,:T] = prompt\n    seq = empty\n\n    # TODO: All sequences share the same position for now\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt, input_pos, **sampling_kwargs)\n    if is_speculative:\n        prefill(draft_model, prompt, input_pos, **sampling_kwargs)\n    print(next_token)\n    seq[:,T] = next_token[:,0]\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    accept_counts = [0] * (speculate_k + 1)\n\n    if is_speculative:\n        input_pos = input_pos.item()  # for speculative decoding easier to keep on host\n        while input_pos < T_new - 1:\n            cur_token = next_token.view(())\n\n            next_tokens = speculative_decode(\n                model, draft_model, cur_token, input_pos, speculate_k, **sampling_kwargs\n            )\n\n            accept_counts[len(next_tokens) - 1] += 1\n            num_added = min(T_new - input_pos - 1, len(next_tokens))\n            seq[input_pos + 1 : input_pos + num_added + 1] = next_tokens[: num_added]\n            for i in next_tokens[: num_added,]:\n                callback(i)\n            input_pos = input_pos + num_added\n            next_token = next_tokens[-1]\n    else:\n        generated_tokens, _ = decode_n_tokens(model, next_token, input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)\n        seq[:,T + 1:] = torch.cat(generated_tokens, dim=1)\n\n    generate_stats = {\n        'accept_counts': accept_counts\n    }\n    return seq, generate_stats\n\ndef encode_tokens(tokenizer, string, batch_size, bos=True, device='cuda'):\n    tokens = tokenizer.encode(string)\n    if bos:\n        tokens = [tokenizer.bos_id()] + tokens\n    tokens_tensor = torch.tensor(tokens, dtype=torch.int, device=device)\n    batch_tokens_tensor = tokens_tensor.unsqueeze(0).repeat(batch_size, 1)\n    return batch_tokens_tensor\n\ndef _load_model(checkpoint_path, device, precision, use_tp):\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    if \"int8\" in str(checkpoint_path):\n        print(\"Using int8 weight-only quantization!\")\n        from quantize import WeightOnlyInt8QuantHandler\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        model = simple_quantizer.convert_for_runtime()\n\n    if \"int4\" in str(checkpoint_path):\n        print(\"Using int4 quantization!\")\n        path_comps = checkpoint_path.name.split(\".\")\n        assert path_comps[-2].startswith(\"g\")\n        groupsize = int(path_comps[-2][1:])\n        from quantize import WeightOnlyInt4QuantHandler\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n        model = simple_quantizer.convert_for_runtime()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n\n    # TODO: remove hard-coded channel config\n    # channel_path = \"/home/ubuntu/DoubleSparse/llama2-7b-chat-qk-channel-config.json\"\n    # channel_path = \"/home/ubuntu/DoubleSparse/vicuna-7b-v1.5-16k-qk-channel-config.json\"\n    # channel_path = \"/home/ubuntu/DoubleSparse/vicuna-7b-v1.5-16k-channel-config.json\"\n\n    path_parts = str(checkpoint_path).split(os.sep)\n    model_path = os.path.join(path_parts[-3], path_parts[-2])\n    channel_path = os.path.join(\"..\", \"config\", model_path + \".json\")\n\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n    init_model_channel_config(model, channel_config, \"qk\")\n\n    if use_tp:\n        from tp import apply_tp\n        print(\"Applying tensor parallel to model ...\")\n        apply_tp(model)\n\n    model = model.to(device=device, dtype=precision)\n    return model.eval()\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    interactive: bool = False,\n    num_samples: int = 5,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"),\n    compile: bool = True,\n    compile_prefill: bool = False,\n    profile: Optional[Path] = None,\n    draft_checkpoint_path: Optional[Path] = None,\n    speculate_k: int = 5,\n    batch_size: int = 1,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    global print\n    rank = maybe_init_dist()\n    use_tp = rank is not None\n    if use_tp:\n        torch.cuda.set_device(rank)\n        if rank != 0:\n            # only print on rank 0\n            print = lambda *args, **kwargs: None\n\n    device = 'cuda'\n    precision = torch.bfloat16\n    is_speculative = draft_checkpoint_path is not None\n    is_chat = \"chat\" in str(checkpoint_path)\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, use_tp)\n\n    if is_speculative:\n        draft_model = _load_model(draft_checkpoint_path, device, precision, use_tp)\n    else:\n        draft_model = None\n\n    torch.cuda.synchronize()\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n    encoded = encode_tokens(tokenizer, prompt, batch_size=batch_size, bos=True, device=device)\n    prompt_length = encoded.size(1)\n\n    torch.manual_seed(1234)\n    model_size = sum([p.numel() * p.dtype.itemsize for p in itertools.chain(model.parameters(), model.buffers())])\n    if compile:\n        if is_speculative and use_tp:\n            torch._inductor.config.triton.cudagraph_trees = False # Bug with cudagraph trees in this case\n\n        if is_speculative:\n            global model_forward, logits_to_prob\n            model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        global decode_one_token, prefill, sparse_decode_one_token\n        decode_one_token = torch.compile(decode_one_token, fullgraph=True)\n        sparse_decode_one_token = torch.compile(sparse_decode_one_token, fullgraph=True) # mode limited\n\n        # Uncomment to squeeze more perf out of prefill\n        if args.compile_prefill:\n            prefill = torch.compile(prefill, fullgraph=True, dynamic=True)\n\n\n    aggregate_metrics = {\n        'tokens_per_sec': [],\n        'accept_counts': [],\n    }\n    start = -1 if compile else 0\n\n    for i in range(start, num_samples):\n        torch.cuda.synchronize()\n        if i >= 0 and interactive:\n            prompt = input(\"What is your prompt? \")\n            if is_chat:\n                prompt = f\"{B_INST} {prompt.strip()} {E_INST}\"\n            encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n\n        if interactive and i >= 0:\n            buffer = []\n            period_id = tokenizer.encode('.')[0]\n            done_generating = False\n            def callback(x):\n                nonlocal done_generating\n                if done_generating:\n                    return\n                buffer.append(tokenizer.decode([period_id] + x.tolist())[1:])\n                if x.item() == tokenizer.eos_id():\n                    done_generating = True\n                if len(buffer) == 4 or done_generating:\n                    print(''.join(buffer), end='', flush=True)\n                    buffer.clear()\n                # print(, end='', flush=True)\n        else:\n            callback = lambda x : x\n        t0 = time.perf_counter()\n        import contextlib\n        if (i != num_samples - 1 or not profile) or (use_tp and rank != 0):\n            prof = contextlib.nullcontext()\n        else:\n            torch.profiler._utils._init_for_cuda_graphs()\n            prof = torch.profiler.profile()\n        with prof:\n            y, metrics = generate(\n                model,\n                encoded,\n                max_new_tokens,\n                draft_model=draft_model,\n                speculate_k=speculate_k,\n                interactive=interactive,\n                callback=callback,\n                temperature=temperature,\n                top_k=top_k,\n            )\n            aggregate_metrics['accept_counts'].append(metrics['accept_counts'])\n        if i == -1:\n            print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n            continue\n        if hasattr(prof, \"export_chrome_trace\"):\n            if use_tp:\n                prof.export_chrome_trace(f\"{profile}_rank_{rank}.json\")\n            else:\n                prof.export_chrome_trace(f\"{profile}.json\")\n        torch.cuda.synchronize()\n        t = time.perf_counter() - t0\n\n        if not interactive:\n            print(tokenizer.decode(y.tolist()))\n        else:\n            print()\n        tokens_generated = batch_size * (y.size(1) - prompt_length)\n        tokens_sec = tokens_generated / t\n        aggregate_metrics['tokens_per_sec'].append(tokens_sec)\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_sec:.02f} tokens/sec\")\n        print(f\"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s\")\n    print(\"==========\")\n    if is_speculative:\n        counts_aggregated = [sum(i) for i in zip(*aggregate_metrics['accept_counts'])]\n        acceptance_probs = [i/sum(counts_aggregated) for i in counts_aggregated]\n        print(f\"Acceptance probs: {acceptance_probs}\")\n        print(f\"Mean Accepted: {sum([idx * i for idx, i in enumerate(counts_aggregated)])/sum(counts_aggregated)}\")\n\n    print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--prompt', type=str, default=\"Hello, my name is\", help='Input prompt.')\n    parser.add_argument('--interactive', action='store_true', help='Whether to launch in interactive mode')\n    parser.add_argument('--num_samples', type=int, default=5, help='Number of samples.')\n    parser.add_argument('--max_new_tokens', type=int, default=2040, help='Maximum number of new tokens.')\n    parser.add_argument('--top_k', type=int, default=200, help='Top-k for sampling.')\n    parser.add_argument('--temperature', type=float, default=0.8, help='Temperature for sampling.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"/home/ec2-user/benchmark/gpt-fast/checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--compile_prefill', action='store_true', help='Whether to compile the prefill (improves prefill perf, but higher compile times)')\n    parser.add_argument('--profile', type=Path, default=None, help='Profile path.')\n    parser.add_argument('--speculate_k', type=int, default=5, help='Speculative execution depth.')\n    parser.add_argument('--draft_checkpoint_path', type=Path, default=None, help='Draft checkpoint path.')\n    parser.add_argument('--batch_size', type=int, default=1, help='Batch Size')\n\n    args = parser.parse_args()\n    main(\n        args.prompt, args.interactive, args.num_samples, args.max_new_tokens, args.top_k,\n        args.temperature, args.checkpoint_path, args.compile, args.compile_prefill, args.profile, args.draft_checkpoint_path, args.speculate_k, args.batch_size\n    )\n"}
{"type": "source_file", "path": "models/triton_kernels/attention.py", "content": "import torch\nimport sys\n\nimport triton\nimport triton.language as tl\nimport math\n\nfrom channel import get_label_tensor\nfrom sparse import fwd_sparse, torch_fwd_sparse\nfrom heavy import get_heavy\nfrom bgemv import bgemv\nfrom bgemv_int8 import bgemv_int8\n\n\ndef att(Q, K, V, Out, q_label, k_label, label_scores, channel, heavy_const, heavy_channel_num, label_mask, attn_mask):\n\n    get_label_tensor(Q, channel, q_label, heavy_channel_num)\n\n    # get_label_tensor(K, channel, k_label, heavy_channel_num)\n\n    # bgemv(q_label, k_label, label_scores)\n\n    tmp_scores = torch.matmul(q_label.view(Q.shape[0], 1, Q.shape[1], heavy_channel_num).transpose(1,2), k_label.view(Q.shape[0], K.shape[0] // Q.shape[0], K.shape[1], heavy_channel_num).transpose(1,2).transpose(2, 3)).view(Q.shape[0], K.shape[1], K.shape[0] // Q.shape[0])\n\n    # assert torch.allclose(tmp_scores, label_scores, atol=1e-4, rtol=0)\n\n    _, label_index = torch.topk(tmp_scores, heavy_const, dim=-1)\n\n    fwd_sparse(Q, K, V, Out, label_index, attn_mask)\n\n    return Out\n\ndef att_int8(Q, K, V, Out, q_label, k_label, k_scales, label_scores, channel, heavy_const, heavy_channel_num, label_mask, attn_mask):\n\n    get_label_tensor(Q, channel, q_label, heavy_channel_num)\n\n    bgemv_int8(q_label, k_label, k_scales, label_scores)\n\n    _, label_index = torch.topk(label_scores, heavy_const, dim=-1)\n\n    fwd_sparse(Q, K, V, Out, label_index, attn_mask)\n\n    return Out\n\n\ndef torch_att(xq, xk, xv, bs, seqlen, num_head, head_dim, channel, heavy_const, label_mask, attn_mask):\n    q = xq.view(bs, 1, num_head, head_dim)\n    k = xk.view(bs, seqlen, num_head, head_dim)\n    v = xv.view(bs, seqlen, num_head, head_dim)\n\n\n    sorted_query_states = torch.gather(q, -1, channel.unsqueeze(0).unsqueeze(0).expand(bs, 1, -1, -1)).transpose(1,2)\n    sorted_key_states = torch.gather(k, -1, channel.unsqueeze(0).unsqueeze(0).expand(bs, seqlen, -1, -1)).transpose(1,2)\n\n    label_scores = torch.matmul(sorted_query_states, sorted_key_states.transpose(2, 3))\n\n    _, indices = torch.sort(label_scores, dim=-1, descending=True)\n\n    discarded_indices = indices[:, :, :, heavy_const:]\n\n    attn_weights = torch.matmul(q.transpose(1,2), k.transpose(1,2).transpose(2, 3)) / math.sqrt(head_dim)\n\n    h2_mask = torch.zeros_like(attn_weights).bool()\n    h2_mask.scatter_(dim=-1, index=discarded_indices, value=True)\n    attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n    # bsz, num_head, 1, seqlen\n    attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n    # attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n\n    # tmp_output = torch.matmul(attn_weights, v.transpose(1,2)).transpose(1,2).contiguous().reshape(bs, 1, num_head * head_dim)\n    attn_output = torch.matmul(attn_weights, v.transpose(1,2)).transpose(1,2).contiguous().reshape(bs, 1, num_head * head_dim)\n    \n    # assert torch.allclose(tmp_output.view(bs, num_head, head_dim), attn_output, atol=1e-2, rtol=0)\n\n    return attn_output\n\n\n\n\ndef test_att(B, N_CTX, H, D, HEAVY_CHANNEL_NUM, HEAVY_CONST):\n    import time\n\n    # B, N_CTX, H, D = 1, 16384, 32, 128\n\n    # HEAVY_CHANNEL_NUM = 8\n    # HEAVY_CONST = 1024\n\n    print(f\"B: {B}, N_CTX: {N_CTX}, H: {H}, D: {D}, HEAVY_CHANNEL_NUM: {HEAVY_CHANNEL_NUM}, HEAVY_CONST: {HEAVY_CONST}\")\n\n    dtype = torch.float16\n\n    q = torch.empty((B, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    k = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    v = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=10)\n\n    channel = torch.zeros(H, HEAVY_CHANNEL_NUM, dtype=torch.int64, device='cuda')\n    for h in range(H):\n        channel[h] = torch.randperm(D, device='cuda')[:HEAVY_CHANNEL_NUM]\n\n    out = torch.empty((B, H, D), dtype=dtype, device=\"cuda\")\n\n    q_label = torch.empty((B, H, HEAVY_CHANNEL_NUM), dtype=dtype, device=\"cuda\")\n    k_label = torch.empty((B * N_CTX, H, HEAVY_CHANNEL_NUM), dtype=dtype, device=\"cuda\")\n\n    label_scores = torch.empty((B, H, N_CTX), dtype=dtype, device=\"cuda\")\n\n    heavy_list = torch.empty((B, H, N_CTX), dtype=torch.int64, device='cuda')\n\n    get_label_tensor(k, channel, k_label, HEAVY_CHANNEL_NUM)\n\n    label_mask = torch.zeros((B, N_CTX), dtype=dtype, device=\"cuda\")\n    attn_mask = torch.zeros((B, HEAVY_CONST), dtype=dtype, device=\"cuda\")\n\n    # k_label = k_label.view(B, N_CTX, H, HEAVY_CHANNEL_NUM).transpose(1, 2).transpose(2,3)\n\n    # global att\n    # att = torch.compile(att, fullgraph=True) # mode limited\n\n    # Warm up\n    for _ in range(10):\n        att(q, k, v, out, q_label, k_label, label_scores, channel, HEAVY_CONST, HEAVY_CHANNEL_NUM, label_mask, attn_mask)\n\n    \n    run_iter = 1000\n    torch.cuda.synchronize()\n    t1 = time.time()\n    for _ in range(run_iter):\n        att(q, k, v, out, q_label, k_label, label_scores, channel, HEAVY_CONST, HEAVY_CHANNEL_NUM, label_mask, attn_mask)\n    torch.cuda.synchronize()\n    t2 = time.time()\n    print(\"Time cost {}\".format((t2 - t1) / run_iter))\n\n    torch_out = torch_att(q, k, v, B, N_CTX, H, D, channel, HEAVY_CONST, label_mask, attn_mask).squeeze().view(B, H, D)\n    # att(q, k, v, out, q_label, k_label, label_scores, channel, HEAVY_CONST, HEAVY_CHANNEL_NUM, mask)\n    o = out\n\n\n    print(\"max \", torch.max(torch.abs(torch_out - o)))\n    print(\"mean \", torch.mean(torch.abs(torch_out - o)))\n    assert torch.allclose(torch_out, o, atol=1e-2, rtol=0)\n\n    return (t2 - t1) / run_iter\n\n\ndef test_att_int8(B, N_CTX, H, D, HEAVY_CHANNEL_NUM, HEAVY_CONST):\n    import time\n\n    # B, N_CTX, H, D = 1, 16384, 32, 128\n\n    # HEAVY_CHANNEL_NUM = 8\n    # HEAVY_CONST = 1024\n\n    print(f\"B: {B}, N_CTX: {N_CTX}, H: {H}, D: {D}, HEAVY_CHANNEL_NUM: {HEAVY_CHANNEL_NUM}, HEAVY_CONST: {HEAVY_CONST}\")\n\n    dtype = torch.float16\n\n    q = torch.empty((B, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    k = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    v = torch.empty((B * N_CTX, H, D), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=10)\n\n    channel = torch.zeros(H, HEAVY_CHANNEL_NUM, dtype=torch.int64, device='cuda')\n    for h in range(H):\n        channel[h] = torch.randperm(D, device='cuda')[:HEAVY_CHANNEL_NUM]\n\n    out = torch.empty((B, H, D), dtype=dtype, device=\"cuda\")\n\n    q_label = torch.empty((B, H, HEAVY_CHANNEL_NUM), dtype=dtype, device=\"cuda\")\n    k_label = torch.empty((B * N_CTX, H, HEAVY_CHANNEL_NUM), dtype=dtype, device=\"cuda\")\n\n    label_scores = torch.empty((B, H, N_CTX), dtype=dtype, device=\"cuda\")\n\n    heavy_list = torch.empty((B, H, N_CTX), dtype=torch.int64, device='cuda')\n\n    get_label_tensor(k, channel, k_label, HEAVY_CHANNEL_NUM)\n\n    label_mask = torch.zeros((B, N_CTX), dtype=dtype, device=\"cuda\")\n    attn_mask = torch.zeros((B, HEAVY_CONST), dtype=dtype, device=\"cuda\")\n\n    k_scales = (k_label.abs().max(-1)[0] / 127.0)\n    k_label = (k_label / k_scales[:, :, None]).to(torch.int8)\n\n    # warm up\n    for _ in range(10):\n        att_int8(q, k, v, out, q_label, k_label, k_scales, label_scores, channel, HEAVY_CONST, HEAVY_CHANNEL_NUM, label_mask, attn_mask)\n    \n    run_iter = 1000\n    torch.cuda.synchronize()\n    t1 = time.time()\n    for _ in range(run_iter):\n        att_int8(q, k, v, out, q_label, k_label, k_scales, label_scores, channel, HEAVY_CONST, HEAVY_CHANNEL_NUM, label_mask, attn_mask)\n    torch.cuda.synchronize()\n\n    t2 = time.time()\n    print(\"Time cost {}\".format((t2 - t1) / run_iter))\n\n    return (t2 - t1) / run_iter\n\n\nif __name__ == '__main__':\n\n    # bszs = [1, 4, 8, 16, 32]\n    # ctxs = [2048, 4096, 8192, 16384]\n\n    bsz = int(sys.argv[1])\n    ctx = int(sys.argv[2])\n\n    sparsity_level = 16\n    h = 32\n    d = 128\n\n\n    # times = []\n\n    att = torch.compile(att, fullgraph=True) # mode limited\n    print(f\"bsz: {bsz}, ctx: {ctx}, time: {test_att(bsz, ctx, h, d, d // sparsity_level, ctx // sparsity_level)}\")\n\n\n    # for b in bszs:\n    #     for n_ctx in ctxs:\n    #         heavy_channel_num = d // sparsity_level\n    #         heavy_const = n_ctx // sparsity_level\n    #         # test_att(b, n_ctx, h, d, heavy_channel_num, heavy_const)\n    #         times.append([b, n_ctx, test_att(b, n_ctx, h, d, heavy_channel_num, heavy_const)])\n\n    # print(times)\n"}
{"type": "source_file", "path": "AIME/data/process_data.py", "content": "import json\nimport zlib\nimport pickle\nimport base64\nimport hashlib\nfrom enum import Enum\nfrom datetime import datetime\nfrom dataclasses import dataclass\n\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n\nclass Platform(Enum):\n    LEETCODE = \"leetcode\"\n    CODEFORCES = \"codeforces\"\n    ATCODER = \"atcoder\"\n\n\nclass Difficulty(Enum):\n    EASY = \"easy\"\n    MEDIUM = \"medium\"\n    HARD = \"hard\"\n\n\nclass TestType(Enum):\n    STDIN = \"stdin\"\n    FUNCTIONAL = \"functional\"\n\n\n@dataclass\nclass Test:\n    input: str\n    output: str\n    testtype: TestType\n\n    def __post_init__(self):\n        self.testtype = TestType(self.testtype)\n        # if self.testtype == TestType.FUNCTIONAL:\n        #     self.input = json.loads(self.input)\n        #     self.output = json.loads(self.output)\n\n\n@dataclass\nclass CodeGenerationProblem:\n    question_title: str\n    question_content: str\n    platform: Platform\n    question_id: str\n    contest_id: str\n    contest_date: datetime\n    starter_code: str\n    difficulty: Difficulty\n    public_test_cases: list[Test]\n    private_test_cases: list[Test]\n    metadata: dict\n\n    def __post_init__(self):\n        self.platform = Platform(self.platform)\n        self.difficulty = Difficulty(self.difficulty)\n        self.contest_date = datetime.fromisoformat(self.contest_date)\n\n        self.public_test_cases = json.loads(self.public_test_cases)  # type: ignore\n        self.public_test_cases = [Test(**t) for t in self.public_test_cases]\n\n        try:\n            self.private_test_cases = json.loads(self.private_test_cases)  # type: ignore\n        except:\n            self.private_test_cases = json.loads(pickle.loads(zlib.decompress(base64.b64decode(self.private_test_cases.encode(\"utf-8\"))  # type: ignore\n                                                                             )))  # type: ignore\n        self.private_test_cases = [Test(**t) for t in self.private_test_cases]\n\n        self.metadata = json.loads(self.metadata)  # type: ignore\n\n    def insert_output(self, output_list: list[str], code_list: list[str]) -> dict:\n        return {\n            \"question_title\": self.question_title,\n            \"question_content\": self.question_content,\n            \"platform\": self.platform.value,\n            \"question_id\": self.question_id,\n            \"contest_id\": self.contest_id,\n            \"contest_date\": self.contest_date.isoformat(),\n            \"starter_code\": self.starter_code,\n            \"difficulty\": self.difficulty.value,\n            \"output_list\": output_list,\n            \"code_list\": code_list,\n        }\n\n    def insert_output_evaluation(\n        self,\n        output_list: list[str],\n        code_list: list[str],\n        graded_list: list[bool],\n        **kwargs,\n    ) -> dict:\n        output = self.insert_output(output_list, code_list)\n        output[\"graded_list\"] = graded_list\n        output[\"pass@1\"] = graded_list.count(True) / len(graded_list)\n        for k, v in kwargs.items():\n            output[k] = v\n        return output\n\n    def get_evaluation_sample(self):\n        return {\n            \"input_output\": json.dumps({\n                \"inputs\": [t.input for t in self.public_test_cases + self.private_test_cases],\n                \"outputs\": [t.output for t in self.public_test_cases + self.private_test_cases],\n                \"fn_name\": self.metadata.get(\"func_name\", None),\n            }),\n        }\n\n\nclass PromptConstants:\n    SYSTEM_MESSAGE_GENERIC = f\"You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\"\n\n    SYSTEM_MESSAGE_GEMINI = f\"You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. Do NOT use system calls like `exit` in the generated program. Ensure that the first code block contains the solution.\"\n\n    SYSTEM_MESSAGE_GEMINITHINK = f\"You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\"\n\n    SYSTEM_MESSAGE_DEEPSEEK = f\"You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you answer questions related to computer science.\"\n\n    SYSTEM_MESSAGE_CODEQWEN = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\"\n\n    FORMATTING_MESSAGE_WITH_STARTER_CODE = \"You will use the following starter code to write the solution to the problem and enclose your code within delimiters.\"\n\n    FORMATTING_WITHOUT_STARTER_CODE = \"Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT.\"\n\n\ndef load_code_generation_dataset(release_version=\"release_v5\") -> list[CodeGenerationProblem]:\n    dataset = load_dataset(\"livecodebench/code_generation_lite\", split=\"test\", version_tag=release_version)\n    dataset = [CodeGenerationProblem(**p) for p in dataset]  # type: ignore\n    print(f\"Loaded {len(dataset)} problems\")\n    return dataset\n\n\ndef get_qwen_question_template_answer(question: CodeGenerationProblem):\n    prompt = \"You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. You will NOT return anything except for the program.\\n\\n\"\n    prompt += f\"Question: {question.question_content}\\n\\n\"\n    if question.starter_code:\n        prompt += f\"{PromptConstants.FORMATTING_MESSAGE_WITH_STARTER_CODE}\\n\"\n        prompt += f\"```python\\n{question.starter_code}\\n```\\n\\n\"\n    else:\n        prompt += f\"{PromptConstants.FORMATTING_WITHOUT_STARTER_CODE}\\n\"\n        prompt += f\"```python\\n# YOUR CODE HERE\\n```\\n\\n\"\n    return prompt\n\n\ndef get_qwen_reasoning_question_template_answer(question: CodeGenerationProblem):\n    prompt = \"You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\\n\\n\"\n    prompt += f\"Question: {question.question_content}\\n\\n\"\n    if question.starter_code:\n        prompt += f\"{PromptConstants.FORMATTING_MESSAGE_WITH_STARTER_CODE}\\n\"\n        prompt += f\"```python\\n{question.starter_code}\\n```\\n\\n\"\n    else:\n        prompt += f\"{PromptConstants.FORMATTING_WITHOUT_STARTER_CODE}\\n\"\n        prompt += f\"```python\\n# YOUR CODE HERE\\n```\\n\\n\"\n    return prompt\n\n\ndef calculate_string_md5(input_string: str):\n    md5 = hashlib.md5()\n    md5.update(input_string.encode('utf-8'))\n    return md5.hexdigest()\n\n\nif __name__ == \"__main__\":\n\n    output_livecodebench_v5_tests_dir = \"./data/livecodebench_v5_tests\"\n    Path(output_livecodebench_v5_tests_dir).mkdir(parents=True, exist_ok=True)\n\n    dataset = load_code_generation_dataset(release_version=\"release_v5\")\n    num_samples = 8\n\n    livecodebench_v5_inputs_outputs = []\n    livecodebench_v5_dataset = []\n\n    # template for general language model\n    # prompt_template = get_qwen_question_template_answer\n    # template for reasoning model\n    prompt_template = get_qwen_reasoning_question_template_answer\n\n    for global_id, sample in enumerate(tqdm(dataset)):\n        inputs_outputs = sample.get_evaluation_sample()\n        livecodebench_v5_dataset.append({\n            \"global_id\": global_id,\n            \"question_id\": sample.question_id,\n            \"contest_id\": sample.contest_id,\n            \"contest_date\": sample.contest_date.isoformat(),\n            \"prompt\": prompt_template(sample),\n            \"tests\": {\n                \"fname\": f\"{global_id}.json\",\n                \"md5\": calculate_string_md5(json.dumps(inputs_outputs)),\n            },\n        })\n        livecodebench_v5_inputs_outputs.append(inputs_outputs)\n\n        # save test cases\n        with open(Path(output_livecodebench_v5_tests_dir) / f\"{global_id}.json\", \"w\") as f:\n            json.dump(inputs_outputs, f)\n"}
{"type": "source_file", "path": "LongBench/streaming_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n        t = t / self.scaling_factor\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # streaming config\n        self.local_const = 2044\n        self.sink_const = 4\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        \n        if q_len > 1:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        kv_seq_len = key_states.shape[-2]\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        \n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n\n        streaming_mask = torch.ones((1, kv_seq_len), device=attn_weights.device).bool()\n        # local\n        streaming_mask[:, -self.local_const:] = False\n        # sink\n        streaming_mask[:, :self.sink_const] = False\n        attn_weights.masked_fill_(streaming_mask[None,None,:,:], float('-inf'))\n\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_streaming(model, config, local_const, sink_const):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_streaming(module, config, local_const, sink_const)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.local_const = local_const\n            new_module.sink_const = sink_const\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef change_streaming_para(model, local_const, sink_const):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.local_const = local_const\n            module.sink_const = sink_const\n\n    return model"}
{"type": "source_file", "path": "LongBench/eval.py", "content": "import os\nimport json\nimport argparse\nimport numpy as np\n\nfrom metrics import (\n    qa_f1_score,\n    rouge_zh_score,\n    qa_f1_zh_score,\n    rouge_score,\n    classification_score,\n    retrieval_score,\n    retrieval_zh_score,\n    count_score,\n    code_sim_score,\n)\n\ndataset2metric = {\n    \"narrativeqa\": qa_f1_score,\n    \"qasper\": qa_f1_score,\n    \"multifieldqa_en\": qa_f1_score,\n    \"multifieldqa_zh\": qa_f1_zh_score,\n    \"hotpotqa\": qa_f1_score,\n    \"2wikimqa\": qa_f1_score,\n    \"musique\": qa_f1_score,\n    \"dureader\": rouge_zh_score,\n    \"gov_report\": rouge_score,\n    \"qmsum\": rouge_score,\n    \"multi_news\": rouge_score,\n    \"vcsum\": rouge_zh_score,\n    \"trec\": classification_score,\n    \"triviaqa\": qa_f1_score,\n    \"samsum\": rouge_score,\n    \"lsht\": classification_score,\n    \"passage_retrieval_en\": retrieval_score,\n    \"passage_count\": count_score,\n    \"passage_retrieval_zh\": retrieval_zh_score,\n    \"lcc\": code_sim_score,\n    \"repobench-p\": code_sim_score,\n}\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default=None)\n    parser.add_argument('--e', action='store_true', help=\"Evaluate on LongBench-E\")\n    return parser.parse_args(args)\n\ndef scorer_e(dataset, predictions, answers, lengths, all_classes):\n    scores = {\"0-4k\": [], \"4-8k\": [], \"8k+\": []}\n    for (prediction, ground_truths, length) in zip(predictions, answers, lengths):\n        score = 0.\n        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n            prediction = prediction.lstrip('\\n').split('\\n')[0]\n        for ground_truth in ground_truths:\n            score = max(score, dataset2metric[dataset](prediction, ground_truth, all_classes=all_classes))\n        if length < 4000:\n            scores[\"0-4k\"].append(score)\n        elif length < 8000:\n            scores[\"4-8k\"].append(score)\n        else:\n            scores[\"8k+\"].append(score)\n    for key in scores.keys():\n        scores[key] = round(100 * np.mean(scores[key]), 2)\n    return scores\n\ndef scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.\n    for (prediction, ground_truths) in zip(predictions, answers):\n        score = 0.\n        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n            prediction = prediction.lstrip('\\n').split('\\n')[0]\n        for ground_truth in ground_truths:\n            score = max(score, dataset2metric[dataset](prediction, ground_truth, all_classes=all_classes))\n        total_score += score\n    return round(100 * total_score / len(predictions), 2)\n\nif __name__ == '__main__':\n    args = parse_args()\n    scores = dict()\n    if args.e:\n        path = f\"pred_e/{args.model}/\"\n    else:\n        path = f\"pred/{args.model}/\"\n    all_files = os.listdir(path)\n    print(\"Evaluating on:\", all_files)\n    for filename in all_files:\n        if not filename.endswith(\"jsonl\"):\n            continue\n        predictions, answers, lengths = [], [], []\n        dataset = filename.split('.jsonl')[0]\n        dataset = dataset.split('-')[0]\n        with open(f\"{path}{filename}\", \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                data = json.loads(line)\n                predictions.append(data[\"pred\"])\n                answers.append(data[\"answers\"])\n                all_classes = data[\"all_classes\"]\n                if \"length\" in data:\n                    lengths.append(data[\"length\"])\n        if args.e:\n            score = scorer_e(dataset, predictions, answers, lengths, all_classes)\n        else:\n            score = scorer(dataset, predictions, answers, all_classes)\n        scores[filename] = score\n    if args.e:\n        out_path = f\"pred_e/{args.model}/result.json\"\n    else:\n        out_path = f\"pred/{args.model}/result.json\"\n    with open(out_path, \"w\") as f:\n        json.dump(scores, f, ensure_ascii=False, indent=4)\n"}
{"type": "source_file", "path": "benchmark/e2e/flexgen/model.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import Tensor\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n@dataclass\nclass ModelArgs:\n    block_size: int = 16384\n    vocab_size: int = 32000\n    n_layer: int = 32\n    n_head: int = 32\n    dim: int = 4096\n    intermediate_size: int = None\n    n_local_heads: int = -1\n    head_dim: int = 64\n    rope_base: float = 40000\n    norm_eps: float = 1e-5\n\n    def __post_init__(self):\n        if self.n_local_heads == -1:\n            self.n_local_heads = self.n_head\n        if self.intermediate_size is None:\n            hidden_dim = 4 * self.dim\n            n_hidden = int(2 * hidden_dim / 3)\n            self.intermediate_size = find_multiple(n_hidden, 256)\n        self.head_dim = self.dim // self.n_head\n\n    @classmethod\n    def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        # fuzzy search\n        config = [config for config in transformer_configs if config in str(name).upper() or config in str(name)]\n        assert len(config) == 1, name\n        return cls(**transformer_configs[config[0]])\n\n\ntransformer_configs = {\n    \"CodeLlama-7b-Python-hf\": dict(block_size=16384, vocab_size=32000, n_layer=32, dim = 4096, rope_base=1000000),\n    \"7B\": dict(n_layer=32, n_head=32, dim=4096),\n    \"13B\": dict(n_layer=40, n_head=40, dim=5120),\n    \"30B\": dict(n_layer=60, n_head=52, dim=6656),\n    \"34B\": dict(n_layer=48, n_head=64, dim=8192, vocab_size=32000, n_local_heads=8, intermediate_size=22016, rope_base=1000000), # CodeLlama-34B-Python-hf\n    \"70B\": dict(n_layer=80, n_head=64, dim=8192, n_local_heads=8, intermediate_size=28672),\n}\n\n\nkv_cache_gpu_buffers = []\nkv_cache_cpus = []\nstream_device = torch.device(\"cuda:1\")\nloading_stream = torch.cuda.Stream(stream_device)\n\n\nclass KVCache(nn.Module):\n    def __init__(self, max_batch_size, max_seq_length, n_heads, head_dim, layer_id, dtype=torch.bfloat16):\n        super().__init__()\n        cache_shape = (max_batch_size, n_heads, max_seq_length, head_dim)\n        self.layer_id = layer_id\n        self.register_buffer('k_cache_cpu', torch.zeros(cache_shape, dtype=dtype, device='cpu', pin_memory=True))\n        self.register_buffer('v_cache_cpu', torch.zeros(cache_shape, dtype=dtype, device='cpu', pin_memory=True))\n        kv_cache_cpus.append((self.k_cache_cpu, self.v_cache_cpu))\n\n\n    def update(self, input_pos, k_val, v_val):\n        # input_pos: [S], k_val: [B, H, S, D]\n        assert input_pos.shape[0] == k_val.shape[2]\n\n        loading_stream.synchronize()\n        k_out, v_out = kv_cache_gpu_buffers[self.layer_id % 2]\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n        \n        with torch.cuda.stream(loading_stream):\n            next_layer_id = (self.layer_id + 1) % 32\n            next_k_out, next_v_out = kv_cache_gpu_buffers[next_layer_id % 2]\n            next_k_cache_cpu, next_v_cache_cpu = kv_cache_cpus[next_layer_id]\n            next_k_out.copy_(next_k_cache_cpu, non_blocking=True)\n            next_v_out.copy_(next_v_cache_cpu, non_blocking=True)\n\n        cpu_input_pos = input_pos.cpu()\n        self.k_cache_cpu[:, :, cpu_input_pos] = k_val.cpu()\n        self.v_cache_cpu[:, :, cpu_input_pos] = v_val.cpu()\n\n\n        return k_out, v_out\n\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.config = config\n\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n        self.layers = nn.ModuleList(TransformerBlock(config) for _ in range(config.n_layer))\n        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n        self.freqs_cis: Optional[Tensor] = None\n        self.mask_cache: Optional[Tensor] = None\n        self.max_batch_size = -1\n        self.max_seq_length = -1\n\n    def setup_caches(self, max_batch_size, max_seq_length):\n        if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for i in range(len(self.layers)):\n            self.layers[i].attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads, head_dim, i)\n\n        k_cache_gpu_buffer1 = torch.zeros(max_batch_size, self.config.n_local_heads, max_seq_length, head_dim, dtype=torch.bfloat16, device=stream_device)\n        v_cache_gpu_buffer1 = torch.zeros(max_batch_size, self.config.n_local_heads, max_seq_length, head_dim, dtype=torch.bfloat16, device=stream_device)\n        k_cache_gpu_buffer2 = torch.zeros(max_batch_size, self.config.n_local_heads, max_seq_length, head_dim, dtype=torch.bfloat16, device=stream_device)\n        v_cache_gpu_buffer2 = torch.zeros(max_batch_size, self.config.n_local_heads, max_seq_length, head_dim, dtype=torch.bfloat16, device=stream_device)\n\n        kv_cache_gpu_buffers.append((k_cache_gpu_buffer1, v_cache_gpu_buffer1))\n        kv_cache_gpu_buffers.append((k_cache_gpu_buffer2, v_cache_gpu_buffer2))\n\n        self.freqs_cis = precompute_freqs_cis(self.config.block_size, self.config.dim // self.config.n_head, self.config.rope_base)\n        self.causal_mask = torch.tril(torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool))\n\n    def forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n        mask = self.causal_mask[None, None, input_pos]\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.attention = Attention(config)\n        self.feed_forward = FeedForward(config)\n        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n\n    def forward(self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: Tensor) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        assert config.dim % config.n_head == 0\n\n        total_head_dim = (config.n_head + 2 * config.n_local_heads) * config.head_dim\n        # key, query, value projections for all heads, but in a batch\n        self.wqkv = nn.Linear(config.dim, total_head_dim, bias=False)\n        self.wo = nn.Linear(config.dim, config.dim, bias=False)\n        self.kv_cache = None\n\n        self.n_head = config.n_head\n        self.head_dim = config.head_dim\n        self.n_local_heads = config.n_local_heads\n        self.dim = config.dim\n        self._register_load_state_dict_pre_hook(self.load_hook)\n\n    def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])\n\n    def forward(self, x: Tensor, freqs_cis: Tensor, mask: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q = apply_rotary_emb(q, freqs_cis)\n        k = apply_rotary_emb(k, freqs_cis)\n\n        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n\n        k = k.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        v = v.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n\n        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)\n\n        y = self.wo(y)\n        return y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w3 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w2 = nn.Linear(config.intermediate_size, config.dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef precompute_freqs_cis(\n    seq_len: int, n_elem: int, base: int = 10000\n) -> Tensor:\n    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=torch.bfloat16)\n\n\ndef apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)\n"}
{"type": "source_file", "path": "evaluation/generate.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\nfrom fastchat.model import get_conversation_template\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom modify_mistral import convert_kvcache_mistral_heavy_recent, convert_mistral_channel_config\n# from modify_mixtral import convert_kvcache_mixtral_heavy_recent, convert_mixtral_channel_config\nfrom streaming_llama import convert_streaming\nfrom rtn_llama import convert_rtn\nfrom offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config\nfrom offload_mistral import convert_kvcache_mistral_offloading, convert_mistral_offloading_channel_config\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, default=\"meta-llama/Llama-2-7b-chat-hf\", help='Selected model')\n    parser.add_argument('--offloading', action='store_true', help='Whether to use offloading')\n    parser.add_argument('--architecture', type=str, default=\"llama\", choices=[\"llama\", \"mistral\", \"mixtral\"])\n    parser.add_argument('--channel', type=str, default=\"qk\", choices=[\"q\", \"k\", \"qk\"])\n    parser.add_argument('--heavy_const', type=int, default=128, help='Heavy constant')\n    parser.add_argument('--group_factor', type=int, default=4, help='Group factor')\n    parser.add_argument('--q_bits', type=int, default=4, help='Quantization bits')\n    parser.add_argument('--prompt', type=str, default=\"Hello, my name is\", help='Prompt for generation')\n\n    args = parser.parse_args()\n\n\n    model_path = args.model_path\n    channel_path = \"config/\" + model_path + \".json\"\n\n\n    if \"70b\" in model_path:\n        # TODO: support more than 8 x a10g\n        device_map = {\"model.embed_tokens\": 0, \"model.norm\": 7, \"lm_head\": 7}\n        for i in range(80):\n            device_map[f\"model.layers.{i}\"] = i // 10\n    else:\n        device_map = \"auto\"\n\n    kwargs = {\"torch_dtype\": torch.float16}\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs).cuda()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n\n    channel_config = None\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n    if args.offloading:\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_llama_offloading_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_mistral_offloading_channel_config(model, channel_config, args.channel)\n    else:\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_llama_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_mistral_channel_config(model, channel_config, args.channel)\n\n    prompt = args.prompt\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n    max_new_tokens = 2048-input_ids.shape[-1]\n\n    output = model.generate(input_ids, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)[0]\n    output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n    print(output)\n"}
{"type": "source_file", "path": "benchmark/e2e/gpt-fast/generate.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple\nimport itertools\nimport torch\n\nimport torch._inductor.config\nimport torch._dynamo.config\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# torch._inductor.config.fx_graph_cache = True # Experimental feature to reduce compilation times, will be on by default in future\n\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer\nfrom tp import maybe_init_dist\nfrom sentencepiece import SentencePieceProcessor\n\ndef multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\ndef logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    logits = logits / max(temperature, 1e-5)\n\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        pivot = v.select(-1, -1).unsqueeze(-1)\n        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\ndef sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n    # assert torch.allclose(logits[0], logits[1], atol=1e-6, rtol=1e-6)\n    # idx_next = multinomial_sample_one_no_sync(probs)\n    idx_next = torch.argmax(probs, dim=-1, keepdim=True).to(dtype=torch.int) # TODO: change the sampling method\n    return idx_next, probs\n\ndef prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n    # input_pos: [B, S]\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\ndef decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\ndef decode_n_tokens(model: Transformer, cur_token: torch.Tensor, input_pos: torch.Tensor, num_new_tokens: int, callback=lambda _: _, **sampling_kwargs):\n    new_tokens, new_probs = [], []\n    for i in range(num_new_tokens):\n        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True): # Actually better for Inductor to codegen attention here\n            next_token, next_prob = decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n        input_pos += 1\n        new_tokens.append(next_token.clone())\n        callback(new_tokens[-1])\n        new_probs.append(next_prob.clone())\n        # cur_token = next_token.view(1, -1)\n        cur_token = next_token.clone()\n    return new_tokens, new_probs\n\n\ndef model_forward(model, x, input_pos):\n    return model(x, input_pos)\n\ndef speculative_decode(\n    model: Transformer,\n    draft_model: Transformer,\n    cur_token: torch.Tensor,\n    input_pos: int,\n    speculate_k: int,\n    **sampling_kwargs\n) -> torch.Tensor:\n    # draft model inference sequentially\n    device = cur_token.device\n    orig_input_pos = torch.tensor([input_pos], dtype=torch.int64, device=cur_token.device)\n    draft_tokens, draft_probs = decode_n_tokens(draft_model, cur_token.view(1, -1), orig_input_pos.clone(), speculate_k, **sampling_kwargs)\n\n    draft_tokens = torch.cat(draft_tokens)\n    # parallel inference on target model using draft tokens\n    target_logits = model_forward(\n        model,\n        torch.cat([cur_token.view(1), draft_tokens]).view(1, -1),\n        torch.arange(input_pos, input_pos + speculate_k + 1, device=cur_token.device)\n    )\n    target_probs = logits_to_probs(target_logits[0], **sampling_kwargs)\n    draft_probs = torch.stack(draft_probs)\n    # q: target prob, p: draft prob\n    # q >= p: always accept draft token\n    # q < p: q/p prob to accept draft token\n    p = draft_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    q = target_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    accept_draft_prob = torch.minimum(torch.ones(()), q[:speculate_k]/ p)\n    rejected_locations = (torch.rand_like(accept_draft_prob) > accept_draft_prob).nonzero()\n\n    if rejected_locations.shape[0] == 0: # All draft tokens have been accepted\n        accept_length = speculate_k + 1\n        last_token = multinomial_sample_one_no_sync(target_probs[-1])\n        # fill last token into draft model\n        model_forward(\n            draft_model,\n            draft_tokens[-1].view(1, -1),\n            orig_input_pos + speculate_k,\n        )\n        return torch.cat([draft_tokens, last_token])\n    else:\n        accept_length = rejected_locations[0].item()\n        p = draft_probs[accept_length]\n        q = target_probs[accept_length]\n        new = q - p\n        new = torch.where(new > 0, new, 0.0)\n        new = new / new.sum()\n        next_token = multinomial_sample_one_no_sync(new)\n        return torch.cat([draft_tokens[:accept_length], next_token])\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    interactive: bool,\n    draft_model: Transformer,\n    speculate_k: Optional[int] = 8,\n    callback = lambda x: x,\n    **sampling_kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    is_speculative = draft_model is not None\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(1)\n    batch_size = prompt.size(0)\n    T_new = T + max_new_tokens\n    if interactive:\n        max_seq_length = 350\n    else:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = prompt.device, prompt.dtype\n    max_seq_length = max_seq_length + speculate_k + 1 if is_speculative else max_seq_length\n    with torch.device(device):\n        model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n        if is_speculative and draft_model is not model:\n            draft_model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty((batch_size, T_new), dtype=dtype, device=device)\n    empty[:,:T] = prompt\n    seq = empty\n\n    # TODO: All sequences share the same position for now\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt, input_pos, **sampling_kwargs)\n    if is_speculative:\n        prefill(draft_model, prompt, input_pos, **sampling_kwargs)\n    print(next_token)\n    seq[:,T] = next_token[:,0]\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    accept_counts = [0] * (speculate_k + 1)\n\n    if is_speculative:\n        input_pos = input_pos.item()  # for speculative decoding easier to keep on host\n        while input_pos < T_new - 1:\n            cur_token = next_token.view(())\n\n            next_tokens = speculative_decode(\n                model, draft_model, cur_token, input_pos, speculate_k, **sampling_kwargs\n            )\n\n            accept_counts[len(next_tokens) - 1] += 1\n            num_added = min(T_new - input_pos - 1, len(next_tokens))\n            seq[input_pos + 1 : input_pos + num_added + 1] = next_tokens[: num_added]\n            for i in next_tokens[: num_added,]:\n                callback(i)\n            input_pos = input_pos + num_added\n            next_token = next_tokens[-1]\n    else:\n        generated_tokens, _ = decode_n_tokens(model, next_token, input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)\n        seq[:,T + 1:] = torch.cat(generated_tokens, dim=1)\n\n    generate_stats = {\n        'accept_counts': accept_counts\n    }\n    return seq, generate_stats\n\ndef encode_tokens(tokenizer, string, batch_size, bos=True, device='cuda'):\n    tokens = tokenizer.encode(string)\n    if bos:\n        tokens = [tokenizer.bos_id()] + tokens\n    tokens_tensor = torch.tensor(tokens, dtype=torch.int, device=device)\n    batch_tokens_tensor = tokens_tensor.unsqueeze(0).repeat(batch_size, 1)\n    return batch_tokens_tensor\n\ndef _load_model(checkpoint_path, device, precision, use_tp):\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    if \"int8\" in str(checkpoint_path):\n        print(\"Using int8 weight-only quantization!\")\n        from quantize import WeightOnlyInt8QuantHandler\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        model = simple_quantizer.convert_for_runtime()\n\n    if \"int4\" in str(checkpoint_path):\n        print(\"Using int4 quantization!\")\n        path_comps = checkpoint_path.name.split(\".\")\n        assert path_comps[-2].startswith(\"g\")\n        groupsize = int(path_comps[-2][1:])\n        from quantize import WeightOnlyInt4QuantHandler\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n        model = simple_quantizer.convert_for_runtime()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n\n    if use_tp:\n        from tp import apply_tp\n        print(\"Applying tensor parallel to model ...\")\n        apply_tp(model)\n\n    model = model.to(device=device, dtype=precision)\n    return model.eval()\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    interactive: bool = False,\n    num_samples: int = 5,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"),\n    compile: bool = True,\n    compile_prefill: bool = False,\n    profile: Optional[Path] = None,\n    draft_checkpoint_path: Optional[Path] = None,\n    speculate_k: int = 5,\n    batch_size: int = 1,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    global print\n    rank = maybe_init_dist()\n    use_tp = rank is not None\n    if use_tp:\n        torch.cuda.set_device(rank)\n        if rank != 0:\n            # only print on rank 0\n            print = lambda *args, **kwargs: None\n\n    device = 'cuda:0'\n    precision = torch.bfloat16\n    is_speculative = draft_checkpoint_path is not None\n    is_chat = \"chat\" in str(checkpoint_path)\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, use_tp)\n\n    if is_speculative:\n        draft_model = _load_model(draft_checkpoint_path, device, precision, use_tp)\n    else:\n        draft_model = None\n\n    torch.cuda.synchronize()\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n    encoded = encode_tokens(tokenizer, prompt, batch_size=batch_size, bos=True, device=device)\n    prompt_length = encoded.size(1)\n\n    torch.manual_seed(1234)\n    model_size = sum([p.numel() * p.dtype.itemsize for p in itertools.chain(model.parameters(), model.buffers())])\n    if compile:\n        if is_speculative and use_tp:\n            torch._inductor.config.triton.cudagraph_trees = False # Bug with cudagraph trees in this case\n\n        if is_speculative:\n            global model_forward, logits_to_prob\n            model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        global decode_one_token, prefill\n        decode_one_token = torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True)\n        # decode_one_token = torch.compile(decode_one_token, fullgraph=True)\n\n        # Uncomment to squeeze more perf out of prefill\n        if args.compile_prefill:\n            prefill = torch.compile(prefill, fullgraph=True, dynamic=True)\n\n\n    aggregate_metrics = {\n        'tokens_per_sec': [],\n        'accept_counts': [],\n    }\n    start = -1 if compile else 0\n\n    for i in range(start, num_samples):\n        torch.cuda.synchronize()\n        if i >= 0 and interactive:\n            prompt = input(\"What is your prompt? \")\n            if is_chat:\n                prompt = f\"{B_INST} {prompt.strip()} {E_INST}\"\n            encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n\n        if interactive and i >= 0:\n            buffer = []\n            period_id = tokenizer.encode('.')[0]\n            done_generating = False\n            def callback(x):\n                nonlocal done_generating\n                if done_generating:\n                    return\n                buffer.append(tokenizer.decode([period_id] + x.tolist())[1:])\n                if x.item() == tokenizer.eos_id():\n                    done_generating = True\n                if len(buffer) == 4 or done_generating:\n                    print(''.join(buffer), end='', flush=True)\n                    buffer.clear()\n                # print(, end='', flush=True)\n        else:\n            callback = lambda x : x\n        t0 = time.perf_counter()\n        import contextlib\n        if (i != num_samples - 1 or not profile) or (use_tp and rank != 0):\n            prof = contextlib.nullcontext()\n        else:\n            torch.profiler._utils._init_for_cuda_graphs()\n            prof = torch.profiler.profile()\n        with prof:\n            y, metrics = generate(\n                model,\n                encoded,\n                max_new_tokens,\n                draft_model=draft_model,\n                speculate_k=speculate_k,\n                interactive=interactive,\n                callback=callback,\n                temperature=temperature,\n                top_k=top_k,\n            )\n            aggregate_metrics['accept_counts'].append(metrics['accept_counts'])\n        if i == -1:\n            print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n            continue\n        if hasattr(prof, \"export_chrome_trace\"):\n            if use_tp:\n                prof.export_chrome_trace(f\"{profile}_rank_{rank}.json\")\n            else:\n                prof.export_chrome_trace(f\"{profile}.json\")\n        torch.cuda.synchronize()\n        t = time.perf_counter() - t0\n\n        if not interactive:\n            print(tokenizer.decode(y.tolist()))\n        else:\n            print()\n        tokens_generated = batch_size * (y.size(1) - prompt_length)\n        tokens_sec = tokens_generated / t\n        aggregate_metrics['tokens_per_sec'].append(tokens_sec)\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_sec:.02f} tokens/sec\")\n        print(f\"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s\")\n    print(\"==========\")\n    if is_speculative:\n        counts_aggregated = [sum(i) for i in zip(*aggregate_metrics['accept_counts'])]\n        acceptance_probs = [i/sum(counts_aggregated) for i in counts_aggregated]\n        print(f\"Acceptance probs: {acceptance_probs}\")\n        print(f\"Mean Accepted: {sum([idx * i for idx, i in enumerate(counts_aggregated)])/sum(counts_aggregated)}\")\n\n    print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--prompt', type=str, default=\"Hello, my name is\", help='Input prompt.')\n    parser.add_argument('--interactive', action='store_true', help='Whether to launch in interactive mode')\n    parser.add_argument('--num_samples', type=int, default=1, help='Number of samples.')\n    parser.add_argument('--max_new_tokens', type=int, default=200, help='Maximum number of new tokens.')\n    parser.add_argument('--top_k', type=int, default=200, help='Top-k for sampling.')\n    parser.add_argument('--temperature', type=float, default=0.8, help='Temperature for sampling.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--compile_prefill', action='store_true', help='Whether to compile the prefill (improves prefill perf, but higher compile times)')\n    parser.add_argument('--profile', type=Path, default=None, help='Profile path.')\n    parser.add_argument('--speculate_k', type=int, default=5, help='Speculative execution depth.')\n    parser.add_argument('--draft_checkpoint_path', type=Path, default=None, help='Draft checkpoint path.')\n    parser.add_argument('--batch_size', type=int, default=1, help='Batch Size')\n\n    args = parser.parse_args()\n    main(\n        args.prompt, args.interactive, args.num_samples, args.max_new_tokens, args.top_k,\n        args.temperature, args.checkpoint_path, args.compile, args.compile_prefill, args.profile, args.draft_checkpoint_path, args.speculate_k, args.batch_size\n    )\n"}
{"type": "source_file", "path": "evaluation/modify_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        if self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # # free gpu memory\n        # if self.config.num_hidden_layers != 32:\n        #     h2_mask = None\n        #     grouped_attn_weights = None\n        #     indices = None\n        #     discard_indices = None\n        #     grouped_query = None\n        #     grouped_key = None\n        #     sorted_query_states = None\n        #     sorted_key_states = None\n        #     query_states = None\n        #     key_states = None\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_llama_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_llama_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef convert_llama_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_llama_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "benchmark/e2e/gpt-fast/tp.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nfrom typing import Optional, List\n\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom torch.distributed import _functional_collectives as funcol\nfrom model import Transformer, Attention, FeedForward\nfrom quantize import WeightOnlyInt4Linear, WeightOnlyInt8Linear\n\n\ndef _get_rank() -> int:\n    return int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n\ndef is_local():\n    return _get_rank() == 0\n\ndef local_break():\n    if is_local():\n        breakpoint()\n    dist.barrier()\n\ndef _get_world_size() -> int:\n    return int(os.environ.get(\"LOCAL_WORLD_SIZE\", \"1\"))\n\ndef maybe_init_dist() -> Optional[int]:\n    try:\n        # provided by torchrun\n        rank = _get_rank()\n        world_size = _get_world_size()\n\n        if world_size < 2:\n            # too few gpus to parallelize, tp is no-op\n            return None\n    except KeyError:\n        # not run via torchrun, no-op\n        return None\n\n    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    return rank\n\n\ndef _apply_tp_linear(linear: nn.Linear, style: str, weight_splits: List[int] = []) -> None:\n    rank = _get_rank()\n    world_size = _get_world_size()\n\n    # Linear's weight matrix is transposed, and is of shape\n    # (linear.out_features, linear.in_features)\n    dim_lookup = {\n        \"colwise\": (0, \"out_features\"),\n        \"rowwise\": (1, \"in_features\")\n    }\n    assert style in dim_lookup\n    shard_dim, size_attr = dim_lookup[style]\n\n    # ensure we can shard evenly\n    assert getattr(linear, size_attr) % world_size == 0\n    def shard(x, dim):\n        assert x.size(dim=dim) % world_size == 0\n        return torch.tensor_split(x, world_size, dim=dim)[rank]\n\n    def shard_qkv(qkv, dim, weight_splits):\n        q, k, v = qkv.split(weight_splits, dim=dim)\n        q = shard(q, dim)\n        k = shard(k, dim)\n        v = shard(v, dim)\n        return torch.cat((q,k,v), dim=dim)\n\n    # shard\n    if weight_splits:\n        # attention\n        assert len(weight_splits) == 3\n\n        if isinstance(linear, WeightOnlyInt4Linear):\n            sharded_weight = shard_qkv(linear.weight, shard_dim, [i//8 for i in weight_splits])\n            linear.scales_and_zeros = shard_qkv(linear.scales_and_zeros, 1 - shard_dim, weight_splits)\n        else:\n            sharded_weight = shard_qkv(linear.weight, shard_dim, weight_splits)\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard_qkv(linear.scales, 0, weight_splits)\n    else:\n        sharded_weight = shard(linear.weight, shard_dim)\n        if isinstance(linear, WeightOnlyInt4Linear):\n            linear.scales_and_zeros = shard(linear.scales_and_zeros, 1 - shard_dim)\n            if style == \"rowwise\":\n                assert linear.scales_and_zeros.shape[0] * 32 == sharded_weight.shape[1] * sharded_weight.shape[2] * sharded_weight.shape[3]\n                assert linear.scales_and_zeros.shape[1] == sharded_weight.shape[0] * 8\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard(linear.scales, 0)\n\n    # local_break()\n    linear.weight = nn.Parameter(sharded_weight, requires_grad=False)\n    setattr(linear, size_attr, getattr(linear, size_attr) // world_size)\n\n    # shape info should still be synced\n    # assert linear.weight.shape == (linear.out_features, linear.in_features)\n\n\ndef _apply_tp_ffn(mlp: FeedForward) -> None:\n    assert hasattr(mlp, \"w1\")\n    assert hasattr(mlp, \"w3\")\n    assert hasattr(mlp, \"w2\")\n\n    _apply_tp_linear(mlp.w1, \"colwise\")\n    _apply_tp_linear(mlp.w3, \"colwise\")\n    _apply_tp_linear(mlp.w2, \"rowwise\")\n\n    world_size = _get_world_size()\n    mlp.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output, \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_attn(attn: Attention) -> None:\n    assert hasattr(attn, \"wqkv\")\n    assert hasattr(attn, \"wo\")\n\n    kv_size = attn.n_local_heads * attn.head_dim\n    _apply_tp_linear(attn.wqkv, \"colwise\", [attn.dim, kv_size, kv_size])\n    _apply_tp_linear(attn.wo, \"rowwise\")\n\n    # overwrite\n    world_size = _get_world_size()\n    attn.n_head = attn.n_head // world_size\n    attn.dim = attn.dim // world_size\n    attn.head_dim = attn.dim // attn.n_head\n    attn.n_local_heads = attn.n_local_heads // world_size\n\n    attn.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output[0], \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_Transformer(Transformer: Transformer) -> None:\n    # overwrite config before Transformer.setup_cache is called\n    world_size = _get_world_size()\n    Transformer.config.n_head = Transformer.config.n_head // world_size\n    Transformer.config.dim = Transformer.config.dim // world_size\n    Transformer.config.n_local_heads = Transformer.config.n_local_heads // world_size\n\n\ndef apply_tp(model: Transformer) -> None:\n    _apply_tp_Transformer(model)\n    for block in model.layers:\n        # Apply to MLP\n        _apply_tp_ffn(block.feed_forward)\n        _apply_tp_attn(block.attention)\n"}
{"type": "source_file", "path": "evaluation/perplexity_eval.py", "content": "import json\nimport tqdm\nimport torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nfrom datasets import load_dataset\nfrom functools import partial\nimport gc\n\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom modify_mistral import convert_kvcache_mistral_heavy_recent, convert_mistral_channel_config\nfrom modify_qwen2 import convert_kvcache_qwen2_heavy_recent, convert_qwen2_channel_config\n# from modify_mixtral import convert_kvcache_mixtral_heavy_recent, convert_mixtral_channel_config\nfrom streaming_llama import convert_streaming\nfrom rtn_llama import convert_rtn\n# from offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config\n# from offload_mistral import convert_kvcache_mistral_offloading, convert_mistral_offloading_channel_config\n\n\ndef evaluate(model, tokenizer):\n    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    max_seq_len = 4096\n\n    testenc = testenc.input_ids.to(model.device)\n    print(testenc.shape)\n    nsamples = testenc.shape[1] // max_seq_len\n    model = model.eval()\n\n    nlls = []\n\n    # 57 -> nan\n\n    gc.collect()\n    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n        batch = testenc[:, (i * max_seq_len):((i + 1) * max_seq_len)].to(model.device)\n        with torch.no_grad():\n            lm_logits = model(batch).logits\n        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n        shift_labels = testenc[:, (i * max_seq_len):((i + 1) * max_seq_len)][:, 1:]\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * max_seq_len\n        nlls.append(neg_log_likelihood)\n        batch = None\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return torch.exp(torch.stack(nlls).sum() / (nsamples * max_seq_len))\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, default=\"meta-llama/Llama-3.1-8B\", help='Selected model')\n    parser.add_argument('--mode', type=str, default=\"ds\", choices=[\"ds\", \"ds-offload\", \"dense\"], help='Whether to use offloading')\n    parser.add_argument('--architecture', type=str, default=\"llama\", choices=[\"llama\", \"mistral\", \"mixtral\", \"qwen2\"])\n    parser.add_argument('--channel', type=str, default=\"q\", choices=[\"q\", \"k\", \"qk\"])\n    parser.add_argument('--heavy_const', type=int, default=128, help='Heavy constant')\n    parser.add_argument('--group_factor', type=int, default=2, help='Group factor')\n    parser.add_argument('--q_bits', type=int, default=2, help='Quantization bits')\n\n    args = parser.parse_args()\n\n\n    model_path = args.model_path\n    channel_path = \"config/\" + model_path + \".json\"\n\n\n    kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n\n    channel_config = None\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n    if args.mode == \"ds\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_llama_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_mistral_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"qwen2\":\n            model = convert_kvcache_qwen2_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_qwen2_channel_config(model, channel_config, args.channel)\n    elif args.mode == \"ds-offload\":\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_llama_offloading_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_mistral_offloading_channel_config(model, channel_config, args.channel)\n\n\n    # model = convert_kvcache_mixtral_heavy_recent(model, config, 128, 4, 4)\n    # model = convert_mixtral_channel_config(model, channel_config, \"q\") #TODO: no k outlier for gqa\n\n    # model = convert_streaming(model, config, 128, 4)\n    # model = convert_rtn(model, config, 2)\n\n    model.eval()\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(evaluate(model, tokenizer))\n\n\n"}
{"type": "source_file", "path": "benchmark/e2e/flexgen/tp.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nfrom typing import Optional, List\n\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom torch.distributed import _functional_collectives as funcol\nfrom model import Transformer, Attention, FeedForward\nfrom quantize import WeightOnlyInt4Linear, WeightOnlyInt8Linear\n\n\ndef _get_rank() -> int:\n    return int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n\ndef is_local():\n    return _get_rank() == 0\n\ndef local_break():\n    if is_local():\n        breakpoint()\n    dist.barrier()\n\ndef _get_world_size() -> int:\n    return int(os.environ.get(\"LOCAL_WORLD_SIZE\", \"1\"))\n\ndef maybe_init_dist() -> Optional[int]:\n    try:\n        # provided by torchrun\n        rank = _get_rank()\n        world_size = _get_world_size()\n\n        if world_size < 2:\n            # too few gpus to parallelize, tp is no-op\n            return None\n    except KeyError:\n        # not run via torchrun, no-op\n        return None\n\n    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    return rank\n\n\ndef _apply_tp_linear(linear: nn.Linear, style: str, weight_splits: List[int] = []) -> None:\n    rank = _get_rank()\n    world_size = _get_world_size()\n\n    # Linear's weight matrix is transposed, and is of shape\n    # (linear.out_features, linear.in_features)\n    dim_lookup = {\n        \"colwise\": (0, \"out_features\"),\n        \"rowwise\": (1, \"in_features\")\n    }\n    assert style in dim_lookup\n    shard_dim, size_attr = dim_lookup[style]\n\n    # ensure we can shard evenly\n    assert getattr(linear, size_attr) % world_size == 0\n    def shard(x, dim):\n        assert x.size(dim=dim) % world_size == 0\n        return torch.tensor_split(x, world_size, dim=dim)[rank]\n\n    def shard_qkv(qkv, dim, weight_splits):\n        q, k, v = qkv.split(weight_splits, dim=dim)\n        q = shard(q, dim)\n        k = shard(k, dim)\n        v = shard(v, dim)\n        return torch.cat((q,k,v), dim=dim)\n\n    # shard\n    if weight_splits:\n        # attention\n        assert len(weight_splits) == 3\n\n        if isinstance(linear, WeightOnlyInt4Linear):\n            sharded_weight = shard_qkv(linear.weight, shard_dim, [i//8 for i in weight_splits])\n            linear.scales_and_zeros = shard_qkv(linear.scales_and_zeros, 1 - shard_dim, weight_splits)\n        else:\n            sharded_weight = shard_qkv(linear.weight, shard_dim, weight_splits)\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard_qkv(linear.scales, 0, weight_splits)\n    else:\n        sharded_weight = shard(linear.weight, shard_dim)\n        if isinstance(linear, WeightOnlyInt4Linear):\n            linear.scales_and_zeros = shard(linear.scales_and_zeros, 1 - shard_dim)\n            if style == \"rowwise\":\n                assert linear.scales_and_zeros.shape[0] * 32 == sharded_weight.shape[1] * sharded_weight.shape[2] * sharded_weight.shape[3]\n                assert linear.scales_and_zeros.shape[1] == sharded_weight.shape[0] * 8\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard(linear.scales, 0)\n\n    # local_break()\n    linear.weight = nn.Parameter(sharded_weight, requires_grad=False)\n    setattr(linear, size_attr, getattr(linear, size_attr) // world_size)\n\n    # shape info should still be synced\n    # assert linear.weight.shape == (linear.out_features, linear.in_features)\n\n\ndef _apply_tp_ffn(mlp: FeedForward) -> None:\n    assert hasattr(mlp, \"w1\")\n    assert hasattr(mlp, \"w3\")\n    assert hasattr(mlp, \"w2\")\n\n    _apply_tp_linear(mlp.w1, \"colwise\")\n    _apply_tp_linear(mlp.w3, \"colwise\")\n    _apply_tp_linear(mlp.w2, \"rowwise\")\n\n    world_size = _get_world_size()\n    mlp.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output, \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_attn(attn: Attention) -> None:\n    assert hasattr(attn, \"wqkv\")\n    assert hasattr(attn, \"wo\")\n\n    kv_size = attn.n_local_heads * attn.head_dim\n    _apply_tp_linear(attn.wqkv, \"colwise\", [attn.dim, kv_size, kv_size])\n    _apply_tp_linear(attn.wo, \"rowwise\")\n\n    # overwrite\n    world_size = _get_world_size()\n    attn.n_head = attn.n_head // world_size\n    attn.dim = attn.dim // world_size\n    attn.head_dim = attn.dim // attn.n_head\n    attn.n_local_heads = attn.n_local_heads // world_size\n\n    attn.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output[0], \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_Transformer(Transformer: Transformer) -> None:\n    # overwrite config before Transformer.setup_cache is called\n    world_size = _get_world_size()\n    Transformer.config.n_head = Transformer.config.n_head // world_size\n    Transformer.config.dim = Transformer.config.dim // world_size\n    Transformer.config.n_local_heads = Transformer.config.n_local_heads // world_size\n\n\ndef apply_tp(model: Transformer) -> None:\n    _apply_tp_Transformer(model)\n    for block in model.layers:\n        # Apply to MLP\n        _apply_tp_ffn(block.feed_forward)\n        _apply_tp_attn(block.attention)\n"}
{"type": "source_file", "path": "benchmark/e2e/flexgen/generate.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple\nimport itertools\nimport torch\n\nimport torch._inductor.config\nimport torch._dynamo.config\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# torch._inductor.config.fx_graph_cache = True # Experimental feature to reduce compilation times, will be on by default in future\n\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer\nfrom tp import maybe_init_dist\nfrom sentencepiece import SentencePieceProcessor\n\ndef multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\ndef logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    logits = logits / max(temperature, 1e-5)\n\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        pivot = v.select(-1, -1).unsqueeze(-1)\n        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\ndef sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n    # assert torch.allclose(logits[0], logits[1], atol=1e-6, rtol=1e-6)\n    # idx_next = multinomial_sample_one_no_sync(probs)\n    idx_next = torch.argmax(probs, dim=-1, keepdim=True).to(dtype=torch.int) # TODO: change the sampling method\n    return idx_next, probs\n\ndef prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n    # input_pos: [B, S]\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\ndef decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    logits = model(x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\ndef decode_n_tokens(model: Transformer, cur_token: torch.Tensor, input_pos: torch.Tensor, num_new_tokens: int, callback=lambda _: _, **sampling_kwargs):\n    new_tokens, new_probs = [], []\n    for i in range(num_new_tokens):\n        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True): # Actually better for Inductor to codegen attention here\n            next_token, next_prob = decode_one_token(\n                model, cur_token, input_pos, **sampling_kwargs\n            )\n        input_pos += 1\n        new_tokens.append(next_token.clone())\n        callback(new_tokens[-1])\n        new_probs.append(next_prob.clone())\n        # cur_token = next_token.view(1, -1)\n        cur_token = next_token.clone()\n    return new_tokens, new_probs\n\n\ndef model_forward(model, x, input_pos):\n    return model(x, input_pos)\n\ndef speculative_decode(\n    model: Transformer,\n    draft_model: Transformer,\n    cur_token: torch.Tensor,\n    input_pos: int,\n    speculate_k: int,\n    **sampling_kwargs\n) -> torch.Tensor:\n    # draft model inference sequentially\n    device = cur_token.device\n    orig_input_pos = torch.tensor([input_pos], dtype=torch.int64, device=cur_token.device)\n    draft_tokens, draft_probs = decode_n_tokens(draft_model, cur_token.view(1, -1), orig_input_pos.clone(), speculate_k, **sampling_kwargs)\n\n    draft_tokens = torch.cat(draft_tokens)\n    # parallel inference on target model using draft tokens\n    target_logits = model_forward(\n        model,\n        torch.cat([cur_token.view(1), draft_tokens]).view(1, -1),\n        torch.arange(input_pos, input_pos + speculate_k + 1, device=cur_token.device)\n    )\n    target_probs = logits_to_probs(target_logits[0], **sampling_kwargs)\n    draft_probs = torch.stack(draft_probs)\n    # q: target prob, p: draft prob\n    # q >= p: always accept draft token\n    # q < p: q/p prob to accept draft token\n    p = draft_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    q = target_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    accept_draft_prob = torch.minimum(torch.ones(()), q[:speculate_k]/ p)\n    rejected_locations = (torch.rand_like(accept_draft_prob) > accept_draft_prob).nonzero()\n\n    if rejected_locations.shape[0] == 0: # All draft tokens have been accepted\n        accept_length = speculate_k + 1\n        last_token = multinomial_sample_one_no_sync(target_probs[-1])\n        # fill last token into draft model\n        model_forward(\n            draft_model,\n            draft_tokens[-1].view(1, -1),\n            orig_input_pos + speculate_k,\n        )\n        return torch.cat([draft_tokens, last_token])\n    else:\n        accept_length = rejected_locations[0].item()\n        p = draft_probs[accept_length]\n        q = target_probs[accept_length]\n        new = q - p\n        new = torch.where(new > 0, new, 0.0)\n        new = new / new.sum()\n        next_token = multinomial_sample_one_no_sync(new)\n        return torch.cat([draft_tokens[:accept_length], next_token])\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    interactive: bool,\n    draft_model: Transformer,\n    speculate_k: Optional[int] = 8,\n    callback = lambda x: x,\n    **sampling_kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    is_speculative = draft_model is not None\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(1)\n    batch_size = prompt.size(0)\n    T_new = T + max_new_tokens\n    if interactive:\n        max_seq_length = 350\n    else:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = prompt.device, prompt.dtype\n    max_seq_length = max_seq_length + speculate_k + 1 if is_speculative else max_seq_length\n    with torch.device(device):\n        model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n        if is_speculative and draft_model is not model:\n            draft_model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty((batch_size, T_new), dtype=dtype, device=device)\n    empty[:,:T] = prompt\n    seq = empty\n\n    # TODO: All sequences share the same position for now\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt, input_pos, **sampling_kwargs)\n    if is_speculative:\n        prefill(draft_model, prompt, input_pos, **sampling_kwargs)\n    print(next_token)\n    seq[:,T] = next_token[:,0]\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    accept_counts = [0] * (speculate_k + 1)\n\n    if is_speculative:\n        input_pos = input_pos.item()  # for speculative decoding easier to keep on host\n        while input_pos < T_new - 1:\n            cur_token = next_token.view(())\n\n            next_tokens = speculative_decode(\n                model, draft_model, cur_token, input_pos, speculate_k, **sampling_kwargs\n            )\n\n            accept_counts[len(next_tokens) - 1] += 1\n            num_added = min(T_new - input_pos - 1, len(next_tokens))\n            seq[input_pos + 1 : input_pos + num_added + 1] = next_tokens[: num_added]\n            for i in next_tokens[: num_added,]:\n                callback(i)\n            input_pos = input_pos + num_added\n            next_token = next_tokens[-1]\n    else:\n        generated_tokens, _ = decode_n_tokens(model, next_token, input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)\n        seq[:,T + 1:] = torch.cat(generated_tokens, dim=1)\n\n    generate_stats = {\n        'accept_counts': accept_counts\n    }\n    return seq, generate_stats\n\ndef encode_tokens(tokenizer, string, batch_size, bos=True, device='cuda'):\n    tokens = tokenizer.encode(string)\n    if bos:\n        tokens = [tokenizer.bos_id()] + tokens\n    tokens_tensor = torch.tensor(tokens, dtype=torch.int, device=device)\n    batch_tokens_tensor = tokens_tensor.unsqueeze(0).repeat(batch_size, 1)\n    return batch_tokens_tensor\n\ndef _load_model(checkpoint_path, device, precision, use_tp):\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    if \"int8\" in str(checkpoint_path):\n        print(\"Using int8 weight-only quantization!\")\n        from quantize import WeightOnlyInt8QuantHandler\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        model = simple_quantizer.convert_for_runtime()\n\n    if \"int4\" in str(checkpoint_path):\n        print(\"Using int4 quantization!\")\n        path_comps = checkpoint_path.name.split(\".\")\n        assert path_comps[-2].startswith(\"g\")\n        groupsize = int(path_comps[-2][1:])\n        from quantize import WeightOnlyInt4QuantHandler\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n        model = simple_quantizer.convert_for_runtime()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n\n    if use_tp:\n        from tp import apply_tp\n        print(\"Applying tensor parallel to model ...\")\n        apply_tp(model)\n\n    model = model.to(device=device, dtype=precision)\n    return model.eval()\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    interactive: bool = False,\n    num_samples: int = 5,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"),\n    compile: bool = True,\n    compile_prefill: bool = False,\n    profile: Optional[Path] = None,\n    draft_checkpoint_path: Optional[Path] = None,\n    speculate_k: int = 5,\n    batch_size: int = 1,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    global print\n    rank = maybe_init_dist()\n    use_tp = rank is not None\n    if use_tp:\n        torch.cuda.set_device(rank)\n        if rank != 0:\n            # only print on rank 0\n            print = lambda *args, **kwargs: None\n\n    device = 'cuda:1'\n    precision = torch.bfloat16\n    is_speculative = draft_checkpoint_path is not None\n    is_chat = \"chat\" in str(checkpoint_path)\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, use_tp)\n\n    if is_speculative:\n        draft_model = _load_model(draft_checkpoint_path, device, precision, use_tp)\n    else:\n        draft_model = None\n\n    torch.cuda.synchronize()\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n    encoded = encode_tokens(tokenizer, prompt, batch_size=batch_size, bos=True, device=device)\n    prompt_length = encoded.size(1)\n\n    torch.manual_seed(1234)\n    model_size = sum([p.numel() * p.dtype.itemsize for p in itertools.chain(model.parameters(), model.buffers())])\n    if compile:\n        if is_speculative and use_tp:\n            torch._inductor.config.triton.cudagraph_trees = False # Bug with cudagraph trees in this case\n\n        if is_speculative:\n            global model_forward, logits_to_prob\n            model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        global decode_one_token, prefill\n        decode_one_token = torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True)\n        # decode_one_token = torch.compile(decode_one_token, fullgraph=True)\n\n        # Uncomment to squeeze more perf out of prefill\n        if args.compile_prefill:\n            prefill = torch.compile(prefill, fullgraph=True, dynamic=True)\n\n\n    aggregate_metrics = {\n        'tokens_per_sec': [],\n        'accept_counts': [],\n    }\n    start = -1 if compile else 0\n\n    for i in range(start, num_samples):\n        torch.cuda.synchronize()\n        if i >= 0 and interactive:\n            prompt = input(\"What is your prompt? \")\n            if is_chat:\n                prompt = f\"{B_INST} {prompt.strip()} {E_INST}\"\n            encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n\n        if interactive and i >= 0:\n            buffer = []\n            period_id = tokenizer.encode('.')[0]\n            done_generating = False\n            def callback(x):\n                nonlocal done_generating\n                if done_generating:\n                    return\n                buffer.append(tokenizer.decode([period_id] + x.tolist())[1:])\n                if x.item() == tokenizer.eos_id():\n                    done_generating = True\n                if len(buffer) == 4 or done_generating:\n                    print(''.join(buffer), end='', flush=True)\n                    buffer.clear()\n                # print(, end='', flush=True)\n        else:\n            callback = lambda x : x\n        t0 = time.perf_counter()\n        import contextlib\n        if (i != num_samples - 1 or not profile) or (use_tp and rank != 0):\n            prof = contextlib.nullcontext()\n        else:\n            torch.profiler._utils._init_for_cuda_graphs()\n            prof = torch.profiler.profile()\n        with prof:\n            y, metrics = generate(\n                model,\n                encoded,\n                max_new_tokens,\n                draft_model=draft_model,\n                speculate_k=speculate_k,\n                interactive=interactive,\n                callback=callback,\n                temperature=temperature,\n                top_k=top_k,\n            )\n            aggregate_metrics['accept_counts'].append(metrics['accept_counts'])\n        if i == -1:\n            print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n            continue\n        if hasattr(prof, \"export_chrome_trace\"):\n            if use_tp:\n                prof.export_chrome_trace(f\"{profile}_rank_{rank}.json\")\n            else:\n                prof.export_chrome_trace(f\"{profile}.json\")\n        torch.cuda.synchronize()\n        t = time.perf_counter() - t0\n\n        if not interactive:\n            print(tokenizer.decode(y.tolist()))\n        else:\n            print()\n        tokens_generated = batch_size * (y.size(1) - prompt_length)\n        tokens_sec = tokens_generated / t\n        aggregate_metrics['tokens_per_sec'].append(tokens_sec)\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_sec:.02f} tokens/sec\")\n        print(f\"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s\")\n    print(\"==========\")\n    if is_speculative:\n        counts_aggregated = [sum(i) for i in zip(*aggregate_metrics['accept_counts'])]\n        acceptance_probs = [i/sum(counts_aggregated) for i in counts_aggregated]\n        print(f\"Acceptance probs: {acceptance_probs}\")\n        print(f\"Mean Accepted: {sum([idx * i for idx, i in enumerate(counts_aggregated)])/sum(counts_aggregated)}\")\n\n    print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--prompt', type=str, default=\"Hello, my name is\", help='Input prompt.')\n    parser.add_argument('--interactive', action='store_true', help='Whether to launch in interactive mode')\n    parser.add_argument('--num_samples', type=int, default=1, help='Number of samples.')\n    parser.add_argument('--max_new_tokens', type=int, default=200, help='Maximum number of new tokens.')\n    parser.add_argument('--top_k', type=int, default=200, help='Top-k for sampling.')\n    parser.add_argument('--temperature', type=float, default=0.8, help='Temperature for sampling.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--compile_prefill', action='store_true', help='Whether to compile the prefill (improves prefill perf, but higher compile times)')\n    parser.add_argument('--profile', type=Path, default=None, help='Profile path.')\n    parser.add_argument('--speculate_k', type=int, default=5, help='Speculative execution depth.')\n    parser.add_argument('--draft_checkpoint_path', type=Path, default=None, help='Draft checkpoint path.')\n    parser.add_argument('--batch_size', type=int, default=1, help='Batch Size')\n\n    args = parser.parse_args()\n    main(\n        args.prompt, args.interactive, args.num_samples, args.max_new_tokens, args.top_k,\n        args.temperature, args.checkpoint_path, args.compile, args.compile_prefill, args.profile, args.draft_checkpoint_path, args.speculate_k, args.batch_size\n    )\n"}
{"type": "source_file", "path": "evaluation/outlier_channel.py", "content": "import torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nfrom datasets import load_dataset\nfrom functools import partial\nimport tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n    dataset = dataset.shuffle(seed=42)\n    samples = []\n    n_run = 0\n    for data in dataset:\n        line = data[\"text\"]\n        line = line.strip()\n        line_encoded = tokenizer.encode(line)\n        if len(line_encoded) > block_size:\n            continue\n        sample = torch.tensor([line_encoded])\n        if sample.numel() == 0:\n            continue\n        samples.append(sample)\n        n_run += 1\n        if n_run == n_samples:\n            break\n\n    # now concatenate all samples and split according to block size\n    cat_samples = torch.cat(samples, dim=1)\n    n_split = cat_samples.shape[1] // block_size\n    print(f\" * Split into {n_split} blocks\")\n    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n\n@torch.no_grad()\ndef get_calib_feat(model, tokenizer):\n    input_dict = dict()\n    output_dict = dict()\n    def stat_input_max_hook(m, x, y, name):\n        if isinstance(x, tuple):\n            x = x[0]\n        # x_max [4096]\n        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach().tolist()\n        if name not in input_dict:\n            input_dict[name] = [x_max]\n        else:\n            input_dict[name] += [x_max]\n        # feat [times, 4096]\n\n    def stat_output_max_hook(m, x, y, name):\n        if isinstance(x, tuple):\n            y = y[0]\n        # x_max [4096]\n        y_max = y.view(-1, y.shape[-1]).abs().mean(dim=0).cpu().detach().tolist()\n        if name not in output_dict:\n            output_dict[name] = [y_max]\n        else:\n            output_dict[name] += [y_max]\n        # feat [times, 4096]\n\n    hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            hooks.append(\n                m.register_forward_hook(\n                    partial(stat_output_max_hook, name=name)))\n\n    print(\"Collecting activation scales...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    samples = get_calib_dataset(tokenizer)\n    pbar = tqdm.tqdm(samples)\n    for input_ids in pbar:\n        input_ids = input_ids.to(device)\n        model(input_ids)\n\n    for hook in hooks:\n        hook.remove()\n    return output_dict\n\n\n\nmodel_path = \"meta-llama/Llama-2-7b-hf\"\n# model_path = \"/home/ec2-user/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16\"\n# model_path = \"/home/ec2-user/.cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n# model = LlamaForCausalLM.from_pretrained(model_path).half().cuda()\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# tokenizer = LlamaTokenizer.from_pretrained(model_path)\n\n\ninput_feat = get_calib_feat(model, tokenizer)\n\n# print(input_feat)\n\nchannels = [5, 1644, 2047, 3150, 4090]\n\nfig, axes = plt.subplots(5, 2, figsize=(15, 20))  # 5x2 grid of plots\n\n# name -> [times, 4096]\nrandom_q_feat = np.array(list(input_feat.values())[0])\nrandom_k_feat = np.array(list(input_feat.values())[1])\nprint(random_q_feat.shape)\n\nfor i, channel in enumerate(channels):\n    ax = axes[i, 0]\n    ax.hist(random_q_feat[:, channel], bins=50, alpha=0.75)\n    ax.set_title(f\"Q Channel {channel} Distribution\")\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    ax = axes[i, 1]\n    ax.hist(random_k_feat[:, channel], bins=50, alpha=0.75)\n    ax.set_title(f\"K Channel {channel} Distribution\")\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n\n# for i, (name, feat) in enumerate(input_feat.items()):\n#     feat = np.array(feat)\n#     ax = axes[i//2, i%2]\n#     ax.plot(sum(feat))\n#     ax.set_title(name)\n#     if i == 9:\n#         break\n\nplt.savefig(\"llama_2_channel_distribution.png\")"}
{"type": "source_file", "path": "evaluation/sparq_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n\n        outlier_num = self.head_dim // self.group_factor\n\n        sorted_indices = torch.topk(torch.abs(query_states), outlier_num, dim=-1).indices\n        sorted_query_states = torch.gather(query_states, -1, sorted_indices)\n        sorted_key_states = torch.gather(key_states, -1, sorted_indices)\n\n        \n        # quantization\n        if self.label_bits < 16:\n            sorted_query_states = pseudo_quantize(sorted_query_states, self.label_bits)\n            sorted_key_states = pseudo_quantize(sorted_key_states, self.label_bits)\n\n        grouped_attn_weights = torch.matmul(sorted_query_states, sorted_key_states.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        import IPython; IPython.embed(); exit()\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_sparq(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_sparq(module, config, heavy_const, group_factor, label_bits)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\n\ndef change_sparq_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "evaluation/retrieval_eval.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\nimport gc\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\nfrom fastchat.model import load_model, get_conversation_template\n# from utils import maybe_monkey_patch, get_output_dir, longeval_load_model, load_testcases, test_topics_one_sample, test_lines_one_sample \n\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom streaming_llama import convert_streaming, change_streaming_para\nfrom rtn_llama import convert_rtn, change_rtn_para\n# from h2o_llama import convert_h2o, reset_h2o\nfrom advanced_h2o_llama import convert_h2o, reset_h2o\nfrom sparq_llama import convert_sparq\nfrom get_config import get_best_config\nfrom offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config\n\n\ndef load_testcases(test_file):\n    with open(test_file, 'r') as json_file:\n        json_list = list(json_file)\n\n    test_cases = []\n    for test_case in json_list:\n        test_case = json.loads(test_case)\n        test_cases.append(test_case)\n    return test_cases\n\ndef test_lines_one_sample(model, tokenizer, test_case, output_file, idx, args):\n    prompt = test_case[\"prompt\"]\n    correct_line = test_case[\"correct_line\"]\n    expected_number = test_case[\"expected_number\"]\n\n    if \"longchat\" in args.model_name_or_path:\n        conv = get_conversation_template(\"vicuna\")\n    else:\n        conv = get_conversation_template(args.model_name_or_path)\n    print(f\"Using conversation template: {conv.name}\")\n    if \"mosaicml/mpt-30b-chat\" in args.model_name_or_path:\n        prompt += f'Answer in the format <{test_case[\"random_idx\"][0]}> <REGISTER_CONTENT>.'\n    \n    conv.append_message(conv.roles[0], prompt)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n    input = tokenizer(prompt, return_tensors=\"pt\")\n    prompt_length = input.input_ids.shape[-1]\n    \n    # Disable use_cache if using longchat models with flash attention\n    use_cache = not (\"longchat\" in args.model_name_or_path and args.longchat_flash_attn)\n    device = getattr(model, \"device\", \"cpu\")\n    \n    output = model.generate(input.input_ids.to(device), max_new_tokens=100, use_cache=use_cache)[0]\n    output = output[prompt_length:]\n    output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n\n    # Matching the last digit of the model output\n    response_number = re.findall(\"\\d+\", output)\n    if response_number is not None and len(response_number) > 0:\n        response_number = int(response_number[-1])\n    else:\n        print(f\"Got unparsable result\")\n        response_number = -1\n\n    summary = f\"Label: {expected_number}, Predict: {output}, Parsed: {response_number}, prompt length: {prompt_length}\".replace('\\n', ' ')\n    print(summary)\n    # if idx ==0:\n    #     with open(output_file, \"w\") as f:\n    #         f.write(summary)\n    #         f.write(\"\\n\")\n    # else:\n    #     with open(output_file, \"a+\") as f:\n    #         f.write(summary)\n    #         f.write(\"\\n\")\n    \n    return expected_number == response_number, prompt_length, summary\n\ndef longeval_test(model, tokenizer, output_dir, args):\n            \n    if args.task == \"lines\":\n        # for num_lines in [50, 100, 200]:\n        for num_lines in [50]:\n            print(f\"************ Start testing {num_lines} lines per LRT prompt ************\")\n            test_file = os.path.join(args.test_dir, f\"lines/testcases/{num_lines}_lines.jsonl\")\n            \n            # output_file = os.path.join(output_dir, f\"{num_lines}_response.txt\")\n            num_correct = 0\n            avg_length = 0\n\n            test_cases = load_testcases(test_file)\n            for idx, test_case in tqdm(enumerate(test_cases)):\n                correct, prompt_length, summary = test_lines_one_sample(model=model, tokenizer=tokenizer, test_case=test_case, output_file=None, idx=idx, args=args)\n                avg_length += prompt_length / len(test_cases)\n                num_correct += correct\n                reset_h2o(model)\n            accuracy = num_correct / len(test_cases)\n\n            # with open(output_file, \"a+\") as f:\n            #     f.write(f\"Accuracy: {accuracy}\")\n\n            print(f\"************ Finish testing {num_lines} lines per prompt with average prompt length {avg_length}, accuracy: {accuracy} ************\")\n            if args.eval_shortest_only:\n                break\n    else:\n        print(f\"Unsupported task: {args.task}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name-or-path\", type=str, default=\"meta-llama/Llama-2-7b-chat-hf\", help=\"model path\")\n    parser.add_argument(\"--task\", type=str, default=\"lines\", help=\"Which evaluation task to use. currently support [topics, lines]\")\n    parser.add_argument(\"--num_gpus\", type=int, default=1, help=\"number of gpus to use\")\n    parser.add_argument(\"--max_gpu_memory\", type=int, default=40, help=\"max per gpu memory in GiB. A100 is 40 or 80.\")\n    parser.add_argument(\"--longchat_flash_attn\", action='store_true', help=\"Only apply to longchat models. Whether to enable flash attention to save memory, but slower.\")\n    parser.add_argument(\"--longchat_ratio\", type=int, default=8, help=\"Only apply to longchat models. Use ratio=8 for 16K context length model. Only ratio=8 is supported now.\")\n    parser.add_argument(\"--eval_shortest_only\", action='store_true', default=0, help=\"Only eval the shortest case for illustration purpose\")\n    parser.add_argument(\"--test_dir\", type=str, default=\"./\", help=\"Directory of the testcases\")\n    parser.add_argument(\"--framework\", type=str, default=None, help=\"Framework for serving\")\n    args = parser.parse_args()\n\n    # maybe_monkey_patch(args)\n    output_dir = None\n\n    # h2o\n    # model_path = args.model_name_or_path\n    # model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    # config = AutoConfig.from_pretrained(model_path)\n    # config.heavy_ratio = 0.04\n    # config.recent_ratio = 0.0225\n    # model = convert_h2o(model, config)\n    # longeval_test(model, tokenizer, output_dir, args)\n\n    # streaming llm\n    # model_path = args.model_name_or_path\n    # model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    # config = AutoConfig.from_pretrained(model_path)\n    # model = convert_streaming(model, config, 128, 4)\n    # local_consts = [1280 // factor for factor in [1, 2, 4, 8, 16, 32]]\n    # for local_const in local_consts:\n    #     model = change_streaming_para(model, local_const-4, 4)\n    #     # model = change_streaming_para(model, local_const, 0)\n    #     longeval_test(model, tokenizer, output_dir, args)\n\n    \n    # rtn\n    # model_path = args.model_name_or_path\n    # model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    # config = AutoConfig.from_pretrained(model_path)\n    # model = convert_rtn(model, config, 16)\n    # qbits = [3]\n    # for qbit in qbits:\n    #     model = change_rtn_para(model, qbit)\n    #     # model = change_streaming_para(model, local_const, 0)\n    #     longeval_test(model, tokenizer, output_dir, args)\n\n\n    # sparq\n    # model_path = args.model_name_or_path\n    # model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    # config = AutoConfig.from_pretrained(model_path)\n    # sparsity_factor = 24\n    # model = convert_sparq(model, config, 1280 // sparsity_factor, 128 // sparsity_factor)\n    # longeval_test(model, tokenizer, output_dir, args)\n\n\n    # double sparsity\n\n    # model_path = args.model_name_or_path\n    # model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    # config = AutoConfig.from_pretrained(model_path)\n    # sparsity_factor = 16\n    # model = convert_kvcache_llama_heavy_recent(model, config, 1280 // sparsity_factor, 2, 4)\n    # channel_path = \"llama2-7b-chat-qk-channel-config.json\"\n    # with open(channel_path, \"r\") as f:\n    #     channel_config = json.load(f)\n    # model = convert_llama_channel_config(model, channel_config, \"qk\")\n    # longeval_test(model, tokenizer, output_dir, args)\n\n\n    # double sparsity offloading\n\n    model_path = args.model_name_or_path\n    device = \"cuda\"\n    model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n    sparsity_factor = 32\n    model = convert_kvcache_llama_offloading(model, config, 1280 // sparsity_factor, 2, 1, [0,1,31])\n    channel_path = \"llama2-7b-chat-qk-channel-config.json\"\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n    model = convert_llama_offloading_channel_config(model, channel_config, \"qk\")\n    longeval_test(model, tokenizer, output_dir, args)\n\n\n    # best_configs = get_best_config()\n\n    # for model_path, heavy_config in best_configs.items():\n    #     if \"chat\" not in model_path and \"vicuna\" not in model_path:\n    #         continue\n    #     args.model_name_or_path = model_path\n    #     model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n    #     tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    #     config = AutoConfig.from_pretrained(model_path)\n\n    #     channel_path = heavy_config[\"channel_path\"]\n\n    #     sparsity_factor = heavy_config[\"sparsity_factor\"]\n    #     qbit = heavy_config[\"qbit\"]\n    #     # sparsity_factor = 24\n    #     # qbit = 3\n    #     group_factor = sparsity_factor * qbit // 16\n    #     channel_type = heavy_config[\"channel_type\"]\n\n    #     channel_config = None\n    #     with open(channel_path, \"r\") as f:\n    #         channel_config = json.load(f)\n\n    #     model = convert_kvcache_llama_heavy_recent(model, config, 1280 // sparsity_factor, group_factor, qbit)\n    #     model = convert_llama_channel_config(model, channel_config, channel_type)\n\n    #     longeval_test(model, tokenizer, output_dir, args)\n    #     model = None\n    #     gc.collect()\n    #     torch.cuda.empty_cache()\n"}
{"type": "source_file", "path": "evaluation/h2o_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\n__all__ = ['convert_kvcache_llama_heavy_recent', 'LlamaAttention_heavy_hitter']\n\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n        self.heavy_budget_ratio = 0.8\n        self.recent_budget_ratio = 0.2\n        self.attention_masks_next = None \n        self.heavy_budget = None\n        self.recent_budget = None\n        self.cache_budget = config.cache_budget\n        self.previous_scores = None\n        self.input_length = []\n        self.cache_budget_records = []\n\n    def _reset_masks(self):\n        self.attention_masks_next = None \n        self.heavy_budget = None\n        self.recent_budget = None\n        # self.cache_budget = None\n        self.previous_scores = None\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n        \n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n           \n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n        \n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n\n        kv_seq_len = key_states.shape[-2]\n\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n\n\n        if self.attention_masks_next is not None:\n            attn_weights = attn_weights * self.attention_masks_next + (1 - self.attention_masks_next) * torch.finfo(attn_weights.dtype).min\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n\n        # attn_weights (BS, heads, q-tokens, k-tokens) 16, 15, 15 // 16, 1, 16\n        current_scores_sum = attn_weights.sum(0).sum(1) # (heads, k-tokens)\n        # offset = attn_weights.gt(0).sum(0).sum(1)\n\n        # Accumulate attention scores\n        if not self.previous_scores == None:\n            current_scores_sum[:, :-1] += self.previous_scores #(Enlarged Sequence)\n        else:\n            # self.heavy_budget = int(self.heavy_budget_ratio * current_scores_sum.shape[-1])\n            # self.recent_budget = int(self.recent_budget_ratio * current_scores_sum.shape[-1])\n            self.heavy_budget = int(self.cache_budget * self.heavy_budget_ratio)\n            self.recent_budget = int(self.cache_budget * self.recent_budget_ratio)\n            # self.cache_budget = self.heavy_budget + self.recent_budget\n            self.cache_budget_records.append(self.cache_budget)\n            self.input_length.append(attn_weights.shape[-1])\n\n            # current_scores_sum = current_scores_sum / offset\n        dtype_attn_weights = attn_weights.dtype\n        attn_weights_devices = attn_weights.device\n        assert attn_weights.shape[0] == 1\n        self.previous_scores = current_scores_sum #(heads, k-tokens)\n        attn_mask = torch.ones(current_scores_sum.shape[0], current_scores_sum.shape[1]+1).to(dtype_attn_weights).to(attn_weights_devices)\n\n        attn_tokens_all = self.previous_scores.shape[-1]\n    \n        if attn_tokens_all > self.cache_budget:\n            # activate most recent k-cache\n            if not self.recent_budget == 0:\n                attn_mask[:, :-self.recent_budget] = 0\n                selected_set = self.previous_scores[:, :-self.recent_budget]\n            else:\n                # activate historical best self.cache_budget - self.recent_budget tokens.\n                # self.previous_scores # (k-Cache - 1)\n                selected_set = self.previous_scores\n\n            if not self.heavy_budget == 0:\n                _, keep_topk = selected_set.topk(k=self.heavy_budget, dim=-1, largest=True)\n                attn_mask = attn_mask.scatter(-1, keep_topk, 1)\n\n        self.attention_masks_next = attn_mask.unsqueeze(0).unsqueeze(2)\n\n        score_mask = attn_mask[:,:-1]\n        score_mask[:, -self.recent_budget:] = 1\n        self.previous_scores = self.previous_scores * score_mask\n\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_h2o(model, config):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_h2o(module, config)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            model._modules[name] = new_module\n\n    return model\n\ndef reset_h2o(model):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = reset_h2o(module)\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            module._reset_masks()\n\n    return model\n\n"}
{"type": "source_file", "path": "evaluation/chat.py", "content": "import argparse\nimport os\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nimport numpy as np\n\nfrom fastchat.model import get_conversation_template\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config\nfrom modify_mistral import convert_kvcache_mistral_heavy_recent, convert_mistral_channel_config\n# from modify_mixtral import convert_kvcache_mixtral_heavy_recent, convert_mixtral_channel_config\nfrom streaming_llama import convert_streaming\nfrom rtn_llama import convert_rtn\nfrom offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config\nfrom offload_mistral import convert_kvcache_mistral_offloading, convert_mistral_offloading_channel_config\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, default=\"meta-llama/Llama-2-7b-chat-hf\", help='Selected model')\n    parser.add_argument('--offloading', action='store_true', help='Whether to use offloading')\n    parser.add_argument('--architecture', type=str, default=\"llama\", choices=[\"llama\", \"mistral\", \"mixtral\"])\n    parser.add_argument('--channel', type=str, default=\"qk\", choices=[\"q\", \"k\", \"qk\"])\n    parser.add_argument('--heavy_const', type=int, default=128, help='Heavy constant')\n    parser.add_argument('--group_factor', type=int, default=4, help='Group factor')\n    parser.add_argument('--q_bits', type=int, default=4, help='Quantization bits')\n\n    args = parser.parse_args()\n\n\n    model_path = args.model_path\n    channel_path = \"config/\" + model_path + \".json\"\n\n\n    if \"70b\" in model_path:\n        # TODO: support more than 8 x a10g\n        device_map = {\"model.embed_tokens\": 0, \"model.norm\": 7, \"lm_head\": 7}\n        for i in range(80):\n            device_map[f\"model.layers.{i}\"] = i // 10\n    else:\n        device_map = \"auto\"\n\n    kwargs = {\"torch_dtype\": torch.float16}\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs).cuda()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n\n    channel_config = None\n    with open(channel_path, \"r\") as f:\n        channel_config = json.load(f)\n\n    if args.offloading:\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_llama_offloading_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_offloading(model, config, args.heavy_const, args.group_factor, args.q_bits, [0,1,31])\n            model = convert_mistral_offloading_channel_config(model, channel_config, args.channel)\n    else:\n        if args.architecture == \"llama\":\n            model = convert_kvcache_llama_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_llama_channel_config(model, channel_config, args.channel)\n        elif args.architecture == \"mistral\":\n            model = convert_kvcache_mistral_heavy_recent(model, config, args.heavy_const, args.group_factor, args.q_bits)\n            model = convert_mistral_channel_config(model, channel_config, args.channel)\n\n    conv = get_conversation_template(model_path)\n\n    while True:\n        print(f\"{conv.roles[0]}:\", end=\"\")\n        inp = input()\n        if inp == \"quit\":\n            break\n        \n        conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n\n        prompt = conv.get_prompt()\n\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\n        prompt_length = input_ids.shape[-1]\n        output = model.generate(input_ids, do_sample=True, max_new_tokens=2048-prompt_length, use_cache=True)[0]\n        \n        output = output[prompt_length:]\n        output = tokenizer.batch_decode([output], skip_special_tokens=True)[0]\n\n        print(f\"{conv.roles[1]}:{output}\")\n        conv.update_last_message(output)\n"}
{"type": "source_file", "path": "benchmark/e2e/gpt-fast/quantize.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport importlib\nimport time\nfrom math import ceil\nfrom pathlib import Path\n\nimport torch\nimport importlib\nimport time\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nfrom sentencepiece import SentencePieceProcessor\n\ntry:\n    from GPTQ import GenericGPTQRunner, InputRecorder, lm_eval\nexcept:\n    pass\n\nfrom model import Transformer\n\n##### Quantization Primitives ######\n\ndef dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    # assumes symmetric quantization\n    # assumes axis == 0\n    # assumes dense memory format\n    # TODO(future): relax ^ as needed\n\n    # default setup for affine quantization of activations\n    eps = torch.finfo(torch.float32).eps\n\n    # get min and max\n    min_val, max_val = torch.aminmax(x, dim=1)\n\n    # calculate scales and zero_points based on min and max\n    # reference: https://fburl.com/code/srbiybme\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n\n    # reference: https://fburl.com/code/4wll53rk\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    # ensure scales is the same dtype as the original tensor\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n\n    # quantize based on qmin/qmax/scales/zp\n    # reference: https://www.internalfb.com/code/fbsource/[8edc275012b1]/fbcode/caffe2/torch/ao/quantization/fx/_decomposed.py?lines=63\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n\n    return quant, scales, zero_points\n\ndef get_group_qparams(w, n_bit=4, groupsize=128):\n    # needed for GPTQ with padding\n    if groupsize > w.shape[-1]:\n        groupsize = w.shape[-1]\n    assert groupsize > 1\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    max_val = to_quant.amax(dim=1, keepdim=True)\n    min_val = to_quant.amin(dim=1, keepdim=True)\n    max_int = 2**n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-6) / max_int\n    zeros = min_val + scales * (2 ** (n_bit - 1))\n    return scales.to(torch.bfloat16).reshape(w.shape[0], -1), zeros.to(\n        torch.bfloat16\n    ).reshape(w.shape[0], -1)\n\n\ndef pack_scales_and_zeros(scales, zeros):\n    assert scales.shape == zeros.shape\n    assert scales.dtype == torch.bfloat16\n    assert zeros.dtype == torch.bfloat16\n    return (\n        torch.cat(\n            [\n                scales.reshape(scales.size(0), scales.size(1), 1),\n                zeros.reshape(zeros.size(0), zeros.size(1), 1),\n            ],\n            2,\n        )\n        .transpose(0, 1)\n        .contiguous()\n    )\n\n\ndef unpack_scales_and_zeros(scales_and_zeros):\n    assert len(scales_and_zeros.shape) == 3 and scales_and_zeros.shape[2] == 2\n    assert scales_and_zeros.dtype == torch.float\n    return torch.split(scales_and_zeros.transpose(0, 1), 1, 2)\n\n\ndef group_quantize_tensor_from_qparams(w, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column quantize\n    if groupsize > w.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w.shape[-1]\n\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n    min_val = zeros - scales * (2 ** (n_bit - 1))\n    max_int = 2**n_bit - 1\n    min_int = 0\n    w_int32 = (\n        to_quant.sub(min_val)\n        .div(scales)\n        .round()\n        .clamp_(min_int, max_int)\n        .to(torch.int32)\n        .reshape_as(w)\n    )\n\n    return w_int32\n\n\ndef group_quantize_tensor(w, n_bit=4, groupsize=128):\n    scales, zeros = get_group_qparams(w, n_bit, groupsize)\n    w_int32 = group_quantize_tensor_from_qparams(w, scales, zeros, n_bit, groupsize)\n    scales_and_zeros = pack_scales_and_zeros(scales, zeros)\n    return w_int32, scales_and_zeros\n\n\ndef group_dequantize_tensor_from_qparams(\n    w_int32, scales, zeros, n_bit=4, groupsize=128\n):\n    assert groupsize > 1\n    # needed for GPTQ single column dequantize\n    if groupsize > w_int32.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w_int32.shape[-1]\n    assert w_int32.shape[-1] % groupsize == 0\n    assert w_int32.dim() == 2\n\n    w_int32_grouped = w_int32.reshape(-1, groupsize)\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n\n    w_dq = (\n        w_int32_grouped.sub(2 ** (n_bit - 1)).mul(scales).add(zeros).reshape_as(w_int32)\n    )\n    return w_dq\n\n\ndef group_dequantize_tensor(w_int32, scales_and_zeros, n_bit=4, groupsize=128):\n    scales, zeros = unpack_scales_and_zeros(scales_and_zeros)\n    return group_dequantize_tensor_from_qparams(\n        w_int32, scales, zeros, n_bit, groupsize\n    )\n\nclass QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    def create_quantized_state_dict(self) -> \"StateDict\":\n        pass\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\nclass GPTQQuantHandler(QuantHandler):\n    \"\"\"\n    This class implements a GPTQ QuantHandler that can be used to apply GPTQ to a model in concert with the GenericGPTQRunner class.\n    Unlike the base QuantHandler class, the user does not need to implement the create_quantized_state_dict, instead they have to reimplement\n    __init__ such that it defines the functions for the quantization mode. User is expected to reimplement convert_for_runtime.\n\n    The following functions (which must be defined in __init__) are used to define the quantization mode for both GPTQ and\n    create_quantized_state_dict. Here is a description of each function.\n\n    get_qparams_func:\n        A function that calculates the quantization qparams for an input tensor.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            qparams: it can have any format but will need to be handled by the other defined functions below.\n\n    quantize_func:\n        A function that applies quantization to an input tensor. It should be noted\n        that this function needs to be able to handle quantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n            qparams: the output from get_qparams_func\n        Returns:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n\n\n    dequantize_func:\n        A function that dequantizes an input quantized weight tensor. It should be noted\n        that this function needs to be able to handle dequantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            weight: A 2d weight tensor with non-integer dtype.\n\n    combine_qparams_list_func:\n        A function that combines several qparams into one qparam.\n        Args:\n            qparams_list: a list of qparams objects, each obtained by calling get_qparams_func\n            on a single group from a weight tensor\n        Returns:\n            qparams: an object of the same format as the qparams above.\n\n    skip_layer_func:\n        A function that determines which linear layers should be skipped during GPTQ\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            skip: boolean indicating whether layer should be skipped\n\n    make_names_and_values_dict_func:\n        A function that prepares the qparams and quantized_weight and creates a dictionary indicating how they\n        should be inserted into the state_dict. Generally any packing of the weight and qparams should be done here.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            names_and_values_dict: a dictionary mapping the name of the parameters of the quantized module to the\n            corresponding quantized weights and qparams.\n    \"\"\"\n    def __init__(self):\n        assert self.mod is not None\n        assert self.get_qparams_func is not None\n        assert self.quantize_func is not None\n        assert self.dequantize_func is not None\n        assert self.combine_qparams_list_func is not None\n        assert self.make_names_and_values_dict_func is not None\n\n    @staticmethod\n    def get_inputs(model, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs) -> \"MultiInput\":\n        input_recorder = InputRecorder(\n            model,\n            tokenizer,\n            calibration_seq_length,\n            pad_calibration_inputs,\n        )\n        task_dict = lm_eval.tasks.get_task_dict(calibration_tasks)\n        print(\"Obtaining GPTQ calibration inputs on: \", calibration_tasks)\n        lm_eval.evaluator.evaluate(\n            input_recorder,\n            task_dict,\n            limit=calibration_limit,\n        )\n        inputs = input_recorder.get_recorded_inputs()\n        print(f\"Obtained {len(inputs[0].values)} calibration samples\")\n        return inputs\n\n    @torch.no_grad()\n    def create_quantized_state_dict(\n        self,\n        tokenizer,\n        blocksize,\n        percdamp,\n        groupsize,\n        calibration_tasks,\n        calibration_limit,\n        calibration_seq_length,\n        pad_calibration_inputs,\n    ) -> \"StateDict\":\n        inputs = GPTQQuantHandler.get_inputs(self.mod, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs)\n        print(\"Tracing model for GPTQ\")\n        GPTQ_runner = GenericGPTQRunner(\n            self.mod,\n            inputs,\n            blocksize,\n            percdamp,\n            groupsize,\n        ).configure_quantization_mode(\n            self.get_qparams_func,\n            self.quantize_func,\n            self.dequantize_func,\n            self.combine_qparams_list_func,\n            self.make_names_and_values_dict_func,\n            self.skip_layer_func\n        )\n\n        print(\"Applying GPTQ to weights\")\n        GPTQ_runner.run()\n        return GPTQ_runner.get_quantized_state_dict()\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\n##### Weight-only int8 per-channel quantized code ######\n\ndef replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(module, name, WeightOnlyInt8Linear(child.in_features, child.out_features))\n        else:\n            replace_linear_weight_only_int8_per_channel(child)\n\nclass WeightOnlyInt8QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                int8_weight, scales, _ = dynamically_quantize_per_channel(mod.weight.float(), -128, 127, torch.int8)\n                cur_state_dict[f\"{fqn}.weight\"] = int8_weight\n                cur_state_dict[f\"{fqn}.scales\"] = scales.to(mod.weight.dtype)\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_weight_only_int8_per_channel(self.mod)\n        return self.mod\n\n\nclass WeightOnlyInt8Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.register_buffer(\"weight\", torch.empty((out_features, in_features), dtype=torch.int8))\n        self.register_buffer(\"scales\", torch.ones(out_features, dtype=torch.bfloat16))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.to(dtype=input.dtype)) * self.scales\n\n##### weight only int4 per channel groupwise quantized code ######\n\ndef prepare_int4_weight_and_scales_and_zeros(weight_bf16, groupsize, inner_k_tiles):\n    weight_int32, scales_and_zeros = group_quantize_tensor(\n        weight_bf16, n_bit=4, groupsize=groupsize\n    )\n    weight_int4pack = torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)\n    return weight_int4pack, scales_and_zeros\n\n\ndef linear_forward_int4(x, weight_int4pack, scales_and_zeros, out_features, groupsize):\n    origin_x_size = x.size()\n    x = x.reshape(-1, origin_x_size[-1])\n    c = torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)\n    new_shape = origin_x_size[:-1] + (out_features,)\n    c = c.reshape(new_shape)\n    return c\n\n\ndef _check_linear_int4_k(k, groupsize = 1, inner_k_tiles = 1):\n    return k % groupsize == 0 and k % (inner_k_tiles * 16) == 0\n\ndef replace_linear_int4(module, groupsize, inner_k_tiles, padding):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            if _check_linear_int4_k(child.in_features, groupsize, inner_k_tiles):\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=False,\n                ))\n            elif padding:\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=True,\n                ))\n        else:\n            replace_linear_int4(child, groupsize, inner_k_tiles, padding)\n\n\nclass WeightOnlyInt4QuantHandler:\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        assert groupsize in [32, 64, 128, 256]\n        assert inner_k_tiles in [2, 4, 8]\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                assert not mod.bias\n                out_features = mod.out_features\n                in_features = mod.in_features\n                assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n                print(f\"linear: {fqn}, in={in_features}, out={out_features}\")\n\n                weight = mod.weight.data\n                if not _check_linear_int4_k(in_features, self.groupsize, self.inner_k_tiles):\n                    if self.padding:\n                        from model import find_multiple\n                        import torch.nn.functional as F\n                        print(f\"warning: {fqn} is padded to satisfy in_features % 1024 == 0\")\n                        padded_in_features = find_multiple(in_features, 1024)\n                        weight = F.pad(weight, pad=(0, padded_in_features - in_features))\n                    else:\n                        print(f\"warning: {fqn} is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, \" +\n                            \"and that groupsize and inner_k_tiles*16 evenly divide into it\")\n                        continue\n                weight_int4pack, scales_and_zeros = prepare_int4_weight_and_scales_and_zeros(\n                    weight.to(torch.bfloat16).to('cuda'), self.groupsize, self.inner_k_tiles\n                )\n                cur_state_dict[f\"{fqn}.weight\"] = weight_int4pack.to('cpu')\n                cur_state_dict[f\"{fqn}.scales_and_zeros\"] = scales_and_zeros.to('cpu')\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4GPTQQuantHandler(GPTQQuantHandler):\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        from model import find_multiple\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        self.get_qparams_func = lambda w: get_group_qparams(w, 4, groupsize)\n        self.quantize_func = lambda w, qparams: \\\n            group_quantize_tensor_from_qparams(w, qparams[0], qparams[1], 4, groupsize)\n        self.dequantize_func = lambda q, qparams: \\\n            group_dequantize_tensor_from_qparams(q, qparams[0], qparams[1], 4, groupsize).float()\n        self.combine_qparams_list_func = lambda qparams_list: \\\n            [torch.cat(x, dim=1) for x in zip(*qparams_list)]\n        # skip unless padding=True or its correctly sized\n        self.skip_layer_func = lambda linear_weight: not (\n            _check_linear_int4_k(linear_weight.shape[-1], groupsize, inner_k_tiles) or padding\n        )\n        # we need to do the padding here, both for q and the qparams if necessary\n        def make_names_and_values_dict_func(q, qparams):\n            k = q.shape[1]\n            new_k = find_multiple(k, 1024)\n            # how much we need to pad the weight\n            delta_k = new_k - q.shape[1]\n            final_q = torch.ops.aten._convert_weight_to_int4pack(F.pad(q, pad=(0, delta_k)), inner_k_tiles)\n            scales_and_zeros = pack_scales_and_zeros(*qparams)\n            # how many new groups we need for padded weight\n            delta_groups = new_k // groupsize - scales_and_zeros.shape[0]\n            final_s_and_z = F.pad(scales_and_zeros, pad=(0,0,0,0,0, delta_groups), value=1)\n            return {\"weight\": final_q, \"scales_and_zeros\": final_s_and_z}\n        self.make_names_and_values_dict_func = make_names_and_values_dict_func\n        super().__init__()\n\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(\n            self, in_features: int, out_features: int,\n            bias=True, device=None, dtype=None, groupsize: int = 128, inner_k_tiles: int = 8, padding: bool = True,\n    ) -> None:\n        super().__init__()\n        self.padding = padding\n        if padding:\n            from model import find_multiple\n            self.origin_in_features = in_features\n            in_features = find_multiple(in_features, 1024)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias, \"require bias=False\"\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n\n        assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n        assert in_features % (inner_k_tiles * 16) == 0, \"require in_features % (innerKTiles * 16) == 0\"\n        self.register_buffer(\n            \"weight\",\n            torch.empty((out_features // 8, in_features // (inner_k_tiles * 16), 32, inner_k_tiles // 2), dtype=torch.int32)\n        )\n        self.register_buffer(\n            \"scales_and_zeros\",\n            torch.empty((in_features // groupsize, out_features, 2), dtype=torch.bfloat16)\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        input = input.to(torch.bfloat16)\n        if self.padding:\n            import torch.nn.functional as F\n            input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))\n        return linear_forward_int4(\n            input,\n            self.weight, self.scales_and_zeros, self.out_features, self.groupsize\n        )\n\n\ndef quantize(\n    checkpoint_path: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"),\n    mode: str = 'int8',\n    # following arguments only available when setting int4 quantization.\n    groupsize: int = 128,\n    # following arguments only used for GPTQ\n    calibration_tasks: list = [\"hellaswag\"],\n    calibration_limit: int = 1000,\n    calibration_seq_length: int = 100,\n    pad_calibration_inputs: bool = False,\n    percdamp: float = .01,\n    blocksize: int = 128,\n    label: str = '',\n) -> None:\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    device = 'cpu'\n    precision = torch.bfloat16\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n    model = model.to(dtype=precision, device=device)\n\n    if mode == 'int8':\n        print(\"Quantizing model weights for int8 weight-only symmetric per-channel quantization\")\n        quant_handler = WeightOnlyInt8QuantHandler(model)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f'{label}int8.pth')\n\n    elif mode == 'int4':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization\")\n        quant_handler = WeightOnlyInt4QuantHandler(model, groupsize)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4.g{groupsize}.pth\")\n\n    elif mode == 'int4-gptq':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization using GPTQ...\")\n        quant_handler = WeightOnlyInt4GPTQQuantHandler(model, groupsize)\n\n        tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n        assert tokenizer_path.is_file(), tokenizer_path\n        tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n\n        quantized_state_dict = quant_handler.create_quantized_state_dict(\n            tokenizer,\n            blocksize,\n            percdamp,\n            groupsize,\n            calibration_tasks,\n            calibration_limit,\n            calibration_seq_length,\n            pad_calibration_inputs\n        )\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4-gptq.g{groupsize}.pth\")\n    else:\n        raise ValueError(f\"Invalid quantization mode {mode} needs to be one of [int8, int4, int4-gpptq]\")\n\n    quantize_path = dir_name / new_base_name\n    print(f\"Writing quantized weights to {quantize_path}\")\n    quantize_path.unlink(missing_ok=True) # remove existing file if one already there\n    torch.save(quantized_state_dict, quantize_path)\n    print(f\"Quantization complete took {time.time() - t0:.02f} seconds\")\n    return\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Quantize a model.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"), help='Path to the model checkpoint to be quantized.')\n    parser.add_argument('--mode', '-q', type=str, default='int8', choices=['int8', 'int4', 'int4-gptq'], help='type of quantization to perform')\n    parser.add_argument('--groupsize', type=int, default=32, help='Group size for int4 quantization.')\n    parser.add_argument('--calibration_tasks', type=str, nargs='+', default=['hellaswag'], help='tasks to do gptq calibration on, if doing gptq')\n    parser.add_argument('--calibration_limit', type=int, default=1000, help='number of samples to use for gptq calibration')\n    parser.add_argument('--calibration_seq_length', type=int, default=100, help='length of sequences to use for gptq calibration')\n    parser.add_argument('--pad_calibration_inputs', type=bool, default=False, help='pads sequences shorter than calibration_seq_length to that length, yielding more calibration inputs but running much slower')\n    parser.add_argument('--percdamp', type=float, default=.01, help='gptq percentage dampening')\n    parser.add_argument('--blocksize', type=int, default=128, help='blocksize for gptq')\n    parser.add_argument('--label', type=str, default='_', help='label to add to output filename')\n\n    args = parser.parse_args()\n    quantize(args.checkpoint_path, args.mode, args.groupsize, args.calibration_tasks, args.calibration_limit, args.calibration_seq_length, args.pad_calibration_inputs, args.percdamp, args.blocksize, args.label)\n"}
{"type": "source_file", "path": "evaluation/offload_mistral.py", "content": "# algo 1: use the upper layer's approximate attention\n# algo 2: use the upper layer's embedding\n# oracle: use the upper layer's full attention\n\nimport gc\nimport math\nfrom typing import List, Optional, Tuple, Union\nimport types\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPast\n\nfrom transformers.models.mistral.configuration_mistral import MistralConfig\nfrom transformers.models.mistral.modeling_mistral import MistralRotaryEmbedding, MistralAttention, apply_rotary_pos_emb, repeat_kv, MistralDecoderLayer, MistralModel\n\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\n\ndef model_forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[List[torch.FloatTensor]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -> Union[Tuple, BaseModelOutputWithPast]:\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    # retrieve input_ids and inputs_embeds\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    elif input_ids is not None:\n        batch_size, seq_length = input_ids.shape\n    elif inputs_embeds is not None:\n        batch_size, seq_length, _ = inputs_embeds.shape\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n    seq_length_with_past = seq_length\n    past_key_values_length = 0\n    if past_key_values is not None:\n        past_key_values_length = past_key_values[0][0].shape[2]\n        seq_length_with_past = seq_length_with_past + past_key_values_length\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = torch.arange(\n            past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n        )\n        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n    else:\n        position_ids = position_ids.view(-1, seq_length).long()\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids)\n    # embed positions\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n        )\n        padding_mask = None\n    else:\n        if 0 in attention_mask:\n            padding_mask = attention_mask\n        else:\n            padding_mask = None\n    attention_mask = self._prepare_decoder_attention_mask(\n        attention_mask,\n        (batch_size, seq_length),\n        inputs_embeds,\n        past_key_values_length,\n        sliding_window=self.config.sliding_window,\n    )\n    hidden_states = inputs_embeds\n\n    # if self.gradient_checkpointing and self.training:\n    #     if use_cache:\n    #         logger.warning_once(\n    #             \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n    #         )\n    #         use_cache = False\n    # decoder layers\n\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    next_decoder_cache = () if use_cache else None\n\n    upper_layer_embedding = None\n\n    for idx, decoder_layer in enumerate(self.layers):\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        if idx in self.unstable_list:\n            # change a lot here, can not inherant the upper layer's embedding\n            upper_layer_embedding = None \n        past_key_value = past_key_values[idx] if past_key_values is not None else None\n        if self.gradient_checkpointing and self.training:\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\n                return custom_forward\n            layer_outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n            )\n        else:\n            layer_outputs = decoder_layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                padding_mask=padding_mask,\n                upper_layer_embedding=upper_layer_embedding,\n            )\n        # algo 1: use the upper layer's approximate attention\n        upper_layer_embedding = hidden_states\n        hidden_states = layer_outputs[0]\n        if use_cache:\n            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n    hidden_states = self.norm(hidden_states)\n    # add hidden states from the last decoder layer\n    if output_hidden_states:\n        all_hidden_states += (hidden_states,)\n    next_cache = next_decoder_cache if use_cache else None\n    if not return_dict:\n        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n    return BaseModelOutputWithPast(\n        last_hidden_state=hidden_states,\n        past_key_values=next_cache,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attns,\n    )\n\n\ndef layer_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n    upper_layer_embedding: Optional[torch.Tensor] = None,\n) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n    \"\"\"\n    Args:\n        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n            (see `past_key_values`).\n        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    \"\"\"\n    residual = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n    # Self Attention\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n        hidden_states=hidden_states,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_value=past_key_value,\n        output_attentions=output_attentions,\n        use_cache=use_cache,\n        padding_mask=padding_mask,\n        upper_layer_embedding=upper_layer_embedding,\n    )\n    hidden_states = residual + hidden_states\n    # Fully Connected\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states)\n    hidden_states = residual + hidden_states\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (self_attn_weights,)\n    if use_cache:\n        outputs += (present_key_value,)\n    return outputs\n\n\n\n\nclass MistralAttention_offloading(nn.Module):\n    \"\"\"\n    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n    and \"Generating Long Sequences with Sparse Transformers\".\n    \"\"\"\n\n    def __init__(self, config: MistralConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = MistralRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        upper_layer_embedding: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        if self.config.num_hidden_layers != 32:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n\n        # use previous embedding to compute approximate attention\n        if upper_layer_embedding is not None:\n            # print(\"upper_layer_embedding device:\", upper_layer_embedding.device)\n            # print(\"weight device:\", self.q_proj.weight.device)\n            previous_query_states = self.q_proj(upper_layer_embedding.to(hidden_states.device))\n        else:\n            previous_query_states = query_states\n        # previous_query_states = query_states\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        previous_query_states = previous_query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        k = key_states\n        query_states, key_states = apply_rotary_pos_emb(query_states, k, cos, sin, position_ids)\n        \n        # apply rotary to previous query states\n        previous_query_states, _ = apply_rotary_pos_emb(previous_query_states, k, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        if self.sorted_channel is not None:\n            sorted_query_states = previous_query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # free gpu memory\n        if self.config.num_hidden_layers != 32:\n            h2_mask = None\n            grouped_attn_weights = None\n            indices = None\n            discard_indices = None\n            grouped_query = None\n            grouped_key = None\n            sorted_query_states = None\n            sorted_key_states = None\n            query_states = None\n            key_states = None\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_mistral_offloading(model, config, heavy_const=256, group_factor=8, label_bits=4, unstable_list=[0,1,31]):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_mistral_offloading(module, config, heavy_const, group_factor)\n\n        if isinstance(module, MistralAttention):\n            device = next(module.parameters()).device\n            new_module = MistralAttention_offloading(config).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n\n        if isinstance(module, MistralDecoderLayer):\n            module.forward = layer_forward.__get__(module)\n        \n        if isinstance(module, MistralModel):\n            module.forward = model_forward.__get__(module)\n            module.unstable_list = unstable_list\n\n    return model\n\n\ndef convert_mistral_offloading_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MistralAttention_offloading):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_mistral_offloading_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MistralAttention_offloading):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "evaluation/perplexity_3d.py", "content": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport json\n\n# with open(\"3d-bar-ppl.json\", \"r\") as f:\nwith open(\"3d-bar-ppl-mistral.json\", \"r\") as f:\n    wiki_2_perplexity_results = json.load(f)\n\nplt.rc('font', size=12)  # 默认字体大小12，加粗\nfig = plt.figure(figsize=(7, 7))\n# fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Set the positions for the ticks to be at 0, 1, 2, 3, etc.\nsparsity_levels = [1, 2, 4, 8, 16, 32, 64, 128]\npositions = list(range(len(sparsity_levels)))\n\n# Set the labels to be 1, 2, 4, 8, etc.\nlabels = [f\"1/{i}\" if i !=1 else 1 for i in sparsity_levels]\n\nax.set_xticks(positions)\nax.set_xticklabels(labels)\nax.set_yticks(positions)\nax.set_yticklabels(labels)\n\nax.set_xlabel('Token Sparsity')\nax.set_ylabel('Channel Sparsity')\nax.set_zlabel('Wiki-2 Score')\n\n# for model_path, model_results in wiki_2_perplexity_results.items():\n# for config_name, perplexity in wiki_2_perplexity_results[\"meta-llama/Llama-2-7b-hf\"].items():\nfor config_name, perplexity in wiki_2_perplexity_results[\"mistralai/Mistral-7B-v0.1\"].items():\n    parts = config_name.split('-')\n    if \"heavy\" in parts and \"group\" in parts:\n        heavy_index = parts.index(\"heavy\") + 1\n        group_index = parts.index(\"group\") + 1\n        heavy_const = int(parts[heavy_index])\n        group_factor = int(parts[group_index])\n        qbit = int(parts[1])\n        x = np.log2(2048 // heavy_const)\n        y = np.log2(group_factor * 16 // qbit)\n        if x == 4 and y == 4:\n            color = 'r'\n        else:\n            color = 'g'\n\n        if np.isnan(perplexity) or perplexity > 10:\n            z = 0\n        else:\n            z = 10 - perplexity\n        if np.isnan(perplexity):\n            print(f\"perplexity is nan for {config_name}, z = {z}\")\n        if perplexity > 10:\n            print(f\"perplexity is {perplexity} for {config_name}, z = {z}\")\n\n        # z = -perplexity if perplexity <= 10 else -10\n        ax.bar3d(x, y, 0, 1, 1, z, shade=True,color=color)\n\nax.set_title('Mistral-7B-v0.1')\n# ax.set_title('Llama-2-7b-hf')\n# ax.set_zlim(0, 6)\nax.view_init(elev=30, azim=30)\n\nplt.tight_layout()\n\nplt.savefig(\"data/3d/Mistral-7B-v0.1.png\")\n# plt.savefig(\"data/3d/Llama-2-7b-hf.png\")\n"}
{"type": "source_file", "path": "evaluation/rtn_llama.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n        t = t / self.scaling_factor\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        # streaming config\n        self.qbit = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if self.qbit != 16:\n            key_states = pseudo_quantize(key_states, self.qbit)\n            value_states = pseudo_quantize(value_states, self.qbit)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_rtn(model, config, qbit):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_rtn(module, config, qbit)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_heavy_hitter(config).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.qbit = qbit\n            model._modules[name] = new_module\n\n    return model\n\ndef change_rtn_para(model, qbit):\n\n    for name, module in model._modules.items():\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = change_rtn_para(module, qbit)\n\n        if isinstance(module, LlamaAttention_heavy_hitter):\n            module.qbit = qbit\n\n    return model"}
{"type": "source_file", "path": "evaluation/plot.py", "content": "import json\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\nimport os\n\n# Function to save the plots to the local filesystem\ndef save_perplexity_plots(data_json):\n    # Load JSON data\n    data = json.loads(data_json)\n\n    # Create a directory for the plots\n    output_dir = \"./data/ppl/\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Iterate through each model in the data\n    for model, values in data.items():\n        # Separate data for qbit-8 and qbit-4\n        qbit_8_data = {k: v for k, v in values.items() if 'qbit-8' in k}\n        qbit_4_data = {k: v for k, v in values.items() if 'qbit-4' in k}\n\n        # Prepare plot data for q, k, qk for both qbit-8 and qbit-4\n        for qbit_data, bit_num in [(qbit_8_data, 8), (qbit_4_data, 4)]:\n            if bit_num == 8:\n                sparsity_levels = [1, 2, 4, 8, 16, 32]\n            else:\n                sparsity_levels = [1, 4, 8, 16, 32]\n            # sparsity_levels = [1, 2, 4, 8, 16, 32]\n            q_values = [qbit_data.get(f'qbit-{bit_num}-sparsity-{s}-q', None) for s in sparsity_levels]\n            k_values = [qbit_data.get(f'qbit-{bit_num}-sparsity-{s}-k', None) for s in sparsity_levels]\n            qk_values = [qbit_data.get(f'qbit-{bit_num}-sparsity-{s}-qk', None) for s in sparsity_levels]\n            q_values[0] = values.get('original', None)\n            k_values[0] = values.get('original', None)\n            qk_values[0] = values.get('original', None)\n            print(f\"qbit_data: {qbit_data}\")\n            print(f\"q_values: {q_values}\")\n            # Create the plot\n            plt.figure(figsize=(10, 6))\n            plt.plot(sparsity_levels, q_values, label='q', marker='o')\n            plt.plot(sparsity_levels, k_values, label='k', marker='s')\n            plt.plot(sparsity_levels, qk_values, label='qk', marker='^')\n\n            # Set plot attributes\n            title = f\"{model}-{bit_num}-bit\"\n            plt.title(f\"{title}\")\n            plt.xlabel('Sparsity Level')\n            plt.ylabel('Perplexity')\n            plt.ylim(5, 50)\n\n            plt.xscale('log')\n            plt.gca().xaxis.set_major_locator(ticker.LogLocator(base=2))\n\n            plt.xticks(sparsity_levels, sparsity_levels)  # Set x-axis ticks to sparsity levels\n            plt.grid(True)\n            plt.legend()\n\n\n            # Save the plot\n            filename = f\"{output_dir}/{title.replace('/', '_').replace(' ', '_')}.png\"\n            plt.savefig(filename)\n            plt.close()\n\n    return output_dir\n\n# Assuming data_json is a valid JSON string\n# data_json = \"your JSON data here\"\n\n# Call the function with the provided JSON data\n# output_dir = save_perplexity_plots(data_json)\n\n# The function will save the plots in the specified directory and return the directory path.\n# Note: The above function call is commented out because 'data_json' is not defined.\n# Replace 'your JSON data here' with your actual JSON data string to use this function.\n\nwith open(\"wiki-2-ppl.json\", \"r\") as f:\n    data_json = f.read()\n\noutput_dir = save_perplexity_plots(data_json)"}
{"type": "source_file", "path": "evaluation/modify_mixtral.py", "content": "import warnings\nimport os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers.cache_utils import Cache\n\n\nfrom transformers.models.mixtral.configuration_mixtral import MixtralConfig\nfrom transformers.models.mixtral.modeling_mixtral import MixtralRotaryEmbedding, MixtralAttention, apply_rotary_pos_emb\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass MixtralAttention_heavy_hitter(nn.Module):\n    \"\"\"\n    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n    and \"Generating Long Sequences with Sparse Transformers\".\n    \"\"\"\n\n    def __init__(self, config: MixtralConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            warnings.warn(\n                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n                    \"with a layer index.\"\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.attention_dropout = config.attention_dropout\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = MixtralRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            if self.layer_idx is None:\n                raise ValueError(\n                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n                    \"with a layer index.\"\n                )\n            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # repeat k/v heads if n_kv_heads < n_heads\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        assert self.head_dim % self.group_factor == 0\n\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n            grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))        \n\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_mixtral_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_mixtral_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, MixtralAttention):\n            device = next(module.parameters()).device\n            new_module = MixtralAttention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n\n    return model\n\n\ndef convert_mixtral_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MixtralAttention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_mixtral_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MixtralAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "evaluation/offload_llama.py", "content": "# algo 1: use the upper layer's approximate attention\n# algo 2: use the upper layer's embedding\n# oracle: use the upper layer's full attention\n\nimport gc\nimport math\nfrom typing import List, Optional, Tuple, Union\nimport types\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPast\n\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding, LlamaAttention, apply_rotary_pos_emb, LlamaLinearScalingRotaryEmbedding, LlamaDynamicNTKScalingRotaryEmbedding, repeat_kv, LlamaDecoderLayer, LlamaModel\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\n\ndef model_forward(\n    self,\n    input_ids: torch.LongTensor = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[List[torch.FloatTensor]] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -> Union[Tuple, BaseModelOutputWithPast]:\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    # retrieve input_ids and inputs_embeds\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    elif input_ids is not None:\n        batch_size, seq_length = input_ids.shape\n    elif inputs_embeds is not None:\n        batch_size, seq_length, _ = inputs_embeds.shape\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n    seq_length_with_past = seq_length\n    past_key_values_length = 0\n    if past_key_values is not None:\n        past_key_values_length = past_key_values[0][0].shape[2]\n        seq_length_with_past = seq_length_with_past + past_key_values_length\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = torch.arange(\n            past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n        )\n        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n    else:\n        position_ids = position_ids.view(-1, seq_length).long()\n    if inputs_embeds is None:\n        inputs_embeds = self.embed_tokens(input_ids)\n    # embed positions\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n        )\n        padding_mask = None\n    else:\n        if 0 in attention_mask:\n            padding_mask = attention_mask\n        else:\n            padding_mask = None\n    attention_mask = self._prepare_decoder_attention_mask(\n        attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n    )\n    hidden_states = inputs_embeds\n\n    # if self.gradient_checkpointing and self.training:\n    #     if use_cache:\n    #         logger.warning_once(\n    #             \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n    #         )\n    #         use_cache = False\n    # decoder layers\n\n    all_hidden_states = () if output_hidden_states else None\n    all_self_attns = () if output_attentions else None\n    next_decoder_cache = () if use_cache else None\n\n    upper_layer_embedding = None\n\n    for idx, decoder_layer in enumerate(self.layers):\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        if idx in self.unstable_list:\n            # change a lot here, can not inherant the upper layer's embedding\n            upper_layer_embedding = None \n        past_key_value = past_key_values[idx] if past_key_values is not None else None\n        if self.gradient_checkpointing and self.training:\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\n                return custom_forward\n            layer_outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n            )\n        else:\n            layer_outputs = decoder_layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                padding_mask=padding_mask,\n                upper_layer_embedding=upper_layer_embedding,\n            )\n        # algo 1: use the upper layer's approximate attention\n        upper_layer_embedding = hidden_states\n        hidden_states = layer_outputs[0]\n        if use_cache:\n            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n        if output_attentions:\n            all_self_attns += (layer_outputs[1],)\n    hidden_states = self.norm(hidden_states)\n    # add hidden states from the last decoder layer\n    if output_hidden_states:\n        all_hidden_states += (hidden_states,)\n    next_cache = next_decoder_cache if use_cache else None\n    if not return_dict:\n        return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n    return BaseModelOutputWithPast(\n        last_hidden_state=hidden_states,\n        past_key_values=next_cache,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attns,\n    )\n\n\ndef layer_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n    upper_layer_embedding: Optional[torch.Tensor] = None,\n) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n    \"\"\"\n    Args:\n        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n            (see `past_key_values`).\n        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    \"\"\"\n    residual = hidden_states\n    hidden_states = self.input_layernorm(hidden_states)\n    # Self Attention\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n        hidden_states=hidden_states,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_value=past_key_value,\n        output_attentions=output_attentions,\n        use_cache=use_cache,\n        padding_mask=padding_mask,\n        upper_layer_embedding=upper_layer_embedding,\n    )\n    hidden_states = residual + hidden_states\n    # Fully Connected\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states)\n    hidden_states = residual + hidden_states\n    outputs = (hidden_states,)\n    if output_attentions:\n        outputs += (self_attn_weights,)\n    if use_cache:\n        outputs += (present_key_value,)\n    return outputs\n\n\n\n\nclass LlamaAttention_offloading(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        upper_layer_embedding: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        if self.config.num_hidden_layers != 32:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n\n            # use previous embedding to compute approximate attention\n            if upper_layer_embedding is not None:\n                # print(\"upper_layer_embedding device:\", upper_layer_embedding.device)\n                # print(\"weight device:\", self.q_proj.weight.device)\n                previous_query_states = self.q_proj(upper_layer_embedding.to(hidden_states.device))\n            else:\n                previous_query_states = query_states\n            # previous_query_states = query_states\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        previous_query_states = previous_query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        k = key_states\n        query_states, key_states = apply_rotary_pos_emb(query_states, k, cos, sin, position_ids)\n        \n        # apply rotary to previous query states\n        previous_query_states, _ = apply_rotary_pos_emb(previous_query_states, k, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        if self.sorted_channel is not None:\n            sorted_query_states = previous_query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # free gpu memory\n        if self.config.num_hidden_layers != 32:\n            h2_mask = None\n            grouped_attn_weights = None\n            indices = None\n            discard_indices = None\n            grouped_query = None\n            grouped_key = None\n            sorted_query_states = None\n            sorted_key_states = None\n            query_states = None\n            key_states = None\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_llama_offloading(model, config, heavy_const=256, group_factor=8, label_bits=4, unstable_list=[0,1,31]):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_llama_offloading(module, config, heavy_const, group_factor)\n\n        if isinstance(module, LlamaAttention):\n            device = next(module.parameters()).device\n            new_module = LlamaAttention_offloading(config).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n\n        if isinstance(module, LlamaDecoderLayer):\n            module.forward = layer_forward.__get__(module)\n        \n        if isinstance(module, LlamaModel):\n            module.forward = model_forward.__get__(module)\n            module.unstable_list = unstable_list\n\n    return model\n\n\ndef convert_llama_offloading_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_offloading):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_llama_offloading_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, LlamaAttention_offloading):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "evaluation/llama_ppl.py", "content": "import json\nimport tqdm\nimport torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nfrom datasets import load_dataset\nfrom functools import partial\nimport gc\n\nfrom modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config, change_llama_heavy_const\nfrom offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config, change_llama_offloading_heavy_const\n\ndef evaluate(model, tokenizer):\n    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    max_seq_len = 2048\n\n    testenc = testenc.input_ids.to(model.device)\n    print(testenc.shape)\n    nsamples = testenc.shape[1] // max_seq_len\n    model = model.eval()\n\n    nlls = []\n\n    # 57 -> nan\n\n    gc.collect()\n    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n        batch = testenc[:, (i * max_seq_len):((i + 1) * max_seq_len)].to(model.device)\n        with torch.no_grad():\n            lm_logits = model(batch).logits\n        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n        shift_labels = testenc[:, (i * max_seq_len):((i + 1) * max_seq_len)][:, 1:]\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * max_seq_len\n        nlls.append(neg_log_likelihood)\n        batch = None\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return torch.exp(torch.stack(nlls).sum() / (nsamples * max_seq_len))\n\n\nmodels_configs = [\n    {\"model_path\": \"meta-llama/Llama-2-7b-chat-hf\", \"channel_configs\": [\"llama2-7b-chat-channel-config.json\", \"llama2-7b-chat-qk-channel-config.json\"]},\n    {\"model_path\": \"meta-llama/Llama-2-7b-hf\", \"channel_configs\": [\"llama2-7b-channel-config.json\", \"llama2-7b-qk-channel-config.json\"]},\n    # {\"model_path\": \"huggyllama/llama-7b\", \"channel_configs\": [\"llama-7b-channel-config.json\", \"llama-7b-qk-channel-config.json\"]},\n    # {\"model_path\": \"lmsys/vicuna-7b-v1.5-16k\", \"channel_configs\": [\"vicuna-7b-v1.5-16k-channel-config.json\", \"vicuna-7b-v1.5-16k-qk-channel-config.json\"]},\n]\n\n\n\nheavy_consts = [2048 // factor for factor in [4, 8, 16, 32]]\ngroup_factors = [4, 8, 16, 32]\n\n# heavy_consts = [2048 // factor for factor in [8, 16]]\n# group_factors = [8, 16]\n\nresults = {}\n\nfor model_config in models_configs:\n    model_path = model_config[\"model_path\"]\n    qk_config = model_config[\"channel_configs\"][1]\n\n    model = LlamaForCausalLM.from_pretrained(model_path).half().cuda()\n    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path)\n\n    original_score = evaluate(model, tokenizer).item()\n\n    results[model_path] = {\"original\": original_score}\n\n    # qk outlier\n    with open(qk_config, \"r\") as f:\n        channel_config = json.load(f)\n\n    model = convert_kvcache_llama_offloading(model, config, 128, 4, 4, [0,1,31])\n    model = convert_llama_offloading_channel_config(model, channel_config, \"qk\")\n\n    for heavy_const in heavy_consts:\n        for group_factor in group_factors:\n            qbit = 4\n            if group_factor < qbit:\n                qbit = 16 // group_factor\n                group_factor = 1\n            else:\n                group_factor = group_factor // qbit\n            model = change_llama_offloading_heavy_const(model, heavy_const, group_factor, qbit)\n            score = evaluate(model, tokenizer).item()\n            print(f\"qbit: {qbit}, heavy_const: {heavy_const}, group_factor: {group_factor}, score: {score}\")\n            results[model_path][f\"qbit-{qbit}-heavy-{heavy_const}-group-{group_factor}-qk\"] = score\n\n    model = None\n    gc.collect()\n    torch.cuda.empty_cache()\n\nwith open(\"ppl-offloading.json\", \"w\") as f:\n    json.dump(results, f)\n\n\n# sparsity_factors = [2, 4, 8, 16, 32, 64]\n# qbits = [8, 4]\n\n# results = {}\n\n# for model_config in models_configs:\n#     model_path = model_config[\"model_path\"]\n#     normal_config = model_config[\"channel_configs\"][0]\n#     qk_config = model_config[\"channel_configs\"][1]\n\n#     model = LlamaForCausalLM.from_pretrained(model_path).half().cuda()\n#     tokenizer = LlamaTokenizer.from_pretrained(model_path)\n#     config = AutoConfig.from_pretrained(model_path)\n\n#     original_score = evaluate(model, tokenizer).item()\n\n#     results[model_path] = {\"original\": original_score}\n\n#     channel_config = None\n#     with open(normal_config, \"r\") as f:\n#         channel_config = json.load(f)\n\n#     model = convert_kvcache_llama_heavy_recent(model, config, 128, 4, 4)\n\n#     # q outlier\n#     model = convert_llama_channel_config(model, channel_config, \"q\")\n\n#     for qbit in qbits:\n#         for sparsity_factor in sparsity_factors:\n#             heavy_const = 2048 // sparsity_factor\n#             group_factor = sparsity_factor * qbit // 16\n#             if group_factor == 0 or group_factor >= 128:\n#                 continue\n#             model = change_llama_heavy_const(model, heavy_const, group_factor, qbit)\n#             score = evaluate(model, tokenizer).item()\n#             print(f\"qbit: {qbit}, sparsity_factor: {sparsity_factor}, score: {score}\")\n#             results[model_path][f\"qbit-{qbit}-sparsity-{sparsity_factor}-q\"] = score\n\n#     # k outlier\n#     model = convert_llama_channel_config(model, channel_config, \"k\")\n\n#     for qbit in qbits:\n#         for sparsity_factor in sparsity_factors:\n#             heavy_const = 2048 // sparsity_factor\n#             group_factor = sparsity_factor * qbit // 16\n#             if group_factor == 0 or group_factor >= 128:\n#                 continue\n#             model = change_llama_heavy_const(model, heavy_const, group_factor, qbit)\n#             score = evaluate(model, tokenizer).item()\n#             print(f\"qbit: {qbit}, sparsity_factor: {sparsity_factor}, score: {score}\")\n#             results[model_path][f\"qbit-{qbit}-sparsity-{sparsity_factor}-k\"] = score\n\n#     # qk outlier\n#     with open(qk_config, \"r\") as f:\n#         channel_config = json.load(f)\n#     model = convert_llama_channel_config(model, channel_config, \"qk\")\n\n#     for qbit in qbits:\n#         for sparsity_factor in sparsity_factors:\n#             heavy_const = 2048 // sparsity_factor\n#             group_factor = sparsity_factor * qbit // 16\n#             if group_factor == 0 or group_factor >= 128:\n#                 continue\n#             model = change_llama_heavy_const(model, heavy_const, group_factor, qbit)\n#             score = evaluate(model, tokenizer).item()\n#             print(f\"qbit: {qbit}, sparsity_factor: {sparsity_factor}, score: {score}\")\n#             results[model_path][f\"qbit-{qbit}-sparsity-{sparsity_factor}-qk\"] = score\n\n#     model = None\n#     gc.collect()\n#     torch.cuda.empty_cache()\n\n\n# with open(\"wiki-2-ppl.json\", \"w\") as f:\n#     json.dump(results, f)\n"}
{"type": "source_file", "path": "evaluation/mmlu.py", "content": "\"\"\"\nAdapted from https://github.com/declare-lab/instruct-eval/blob/main/mmlu.py\nAdapted from https://github.com/hendrycks/test/blob/master/evaluate_flan.py\n\"\"\"\n\nimport argparse\nimport os\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, LlamaForCausalLM, LlamaTokenizer\nfrom tqdm import tqdm\n\n# from fastchat.model.model_adapter import load_model, add_model_args\n# from fastchat.utils import get_context_length\n# from modify_llama import convert_kvcache_llama_heavy_recent, convert_llama_channel_config, change_llama_heavy_const\nfrom offload_llama import convert_kvcache_llama_offloading, convert_llama_offloading_channel_config, change_llama_offloading_heavy_const\n\n\ndef get_choices():\n    return [\"A\", \"B\", \"C\", \"D\"]\n\n\ndef get_subcategories():\n    return {\n        \"abstract_algebra\": [\"math\"],\n        \"anatomy\": [\"health\"],\n        \"astronomy\": [\"physics\"],\n        \"business_ethics\": [\"business\"],\n        \"clinical_knowledge\": [\"health\"],\n        \"college_biology\": [\"biology\"],\n        \"college_chemistry\": [\"chemistry\"],\n        \"college_computer_science\": [\"computer science\"],\n        \"college_mathematics\": [\"math\"],\n        \"college_medicine\": [\"health\"],\n        \"college_physics\": [\"physics\"],\n        \"computer_security\": [\"computer science\"],\n        \"conceptual_physics\": [\"physics\"],\n        \"econometrics\": [\"economics\"],\n        \"electrical_engineering\": [\"engineering\"],\n        \"elementary_mathematics\": [\"math\"],\n        \"formal_logic\": [\"philosophy\"],\n        \"global_facts\": [\"other\"],\n        \"high_school_biology\": [\"biology\"],\n        \"high_school_chemistry\": [\"chemistry\"],\n        \"high_school_computer_science\": [\"computer science\"],\n        \"high_school_european_history\": [\"history\"],\n        \"high_school_geography\": [\"geography\"],\n        \"high_school_government_and_politics\": [\"politics\"],\n        \"high_school_macroeconomics\": [\"economics\"],\n        \"high_school_mathematics\": [\"math\"],\n        \"high_school_microeconomics\": [\"economics\"],\n        \"high_school_physics\": [\"physics\"],\n        \"high_school_psychology\": [\"psychology\"],\n        \"high_school_statistics\": [\"math\"],\n        \"high_school_us_history\": [\"history\"],\n        \"high_school_world_history\": [\"history\"],\n        \"human_aging\": [\"health\"],\n        \"human_sexuality\": [\"culture\"],\n        \"international_law\": [\"law\"],\n        \"jurisprudence\": [\"law\"],\n        \"logical_fallacies\": [\"philosophy\"],\n        \"machine_learning\": [\"computer science\"],\n        \"management\": [\"business\"],\n        \"marketing\": [\"business\"],\n        \"medical_genetics\": [\"health\"],\n        \"miscellaneous\": [\"other\"],\n        \"moral_disputes\": [\"philosophy\"],\n        \"moral_scenarios\": [\"philosophy\"],\n        \"nutrition\": [\"health\"],\n        \"philosophy\": [\"philosophy\"],\n        \"prehistory\": [\"history\"],\n        \"professional_accounting\": [\"other\"],\n        \"professional_law\": [\"law\"],\n        \"professional_medicine\": [\"health\"],\n        \"professional_psychology\": [\"psychology\"],\n        \"public_relations\": [\"politics\"],\n        \"security_studies\": [\"politics\"],\n        \"sociology\": [\"culture\"],\n        \"us_foreign_policy\": [\"politics\"],\n        \"virology\": [\"health\"],\n        \"world_religions\": [\"philosophy\"],\n    }\n\n\ndef get_categories():\n    return {\n        \"STEM\": [\n            \"physics\",\n            \"chemistry\",\n            \"biology\",\n            \"computer science\",\n            \"math\",\n            \"engineering\",\n        ],\n        \"humanities\": [\"history\", \"philosophy\", \"law\"],\n        \"social sciences\": [\n            \"politics\",\n            \"culture\",\n            \"economics\",\n            \"geography\",\n            \"psychology\",\n        ],\n        \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n    }\n\n\ndef format_subject(subject):\n    line = subject.split(\"_\")\n    s = \"\"\n    for entry in line:\n        s += \" \" + entry\n    return s\n\n\ndef format_example(df, idx, include_answer=True):\n    prompt = df.iloc[idx, 0]\n    k = df.shape[1] - 2\n    for j in range(k):\n        prompt += \"\\n{}. {}\".format(get_choices()[j], df.iloc[idx, j + 1])\n    prompt += \"\\nAnswer:\"\n    if include_answer:\n        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n    return prompt\n\n\ndef gen_prompt(train_df, subject, k=-1):\n    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n        format_subject(subject)\n    )\n    k = min(k, train_df.shape[0])\n    # TODO: change the prompt length to 2048\n\n    if k == -1:\n        k = train_df.shape[0]\n    for i in range(k):\n        prompt += format_example(train_df, i)\n    return prompt\n\n# def gen_max_seq_len_prompt(train_df, subject):\n    \n#     train_prompt = gen_prompt(train_df, subject, train_df.shape[0])\n#     single_len = len(tokenizer.encode(train_prompt))\n#     turn = 2048 // single_len\n\n#     train_prompt *= turn\n\n#     return train_prompt\n\n\ndef check_valid_length(model, tokenizer, prompt):\n    context_length = 2048\n    prompt_len = len(tokenizer.encode(prompt))\n\n    return prompt_len < context_length - 4\n\n\ndef evaluate(ntrain, subject, model, tokenizer, dev_df, test_df, sparsity_level=16, device=\"cuda\"):\n    cors = []\n\n    seq_len_sum = 0\n\n    for i in range(test_df.shape[0]):\n        # get prompt and make sure it fits\n        k = ntrain\n        prompt_end = format_example(test_df, i, include_answer=False)\n        # train_prompt = gen_max_seq_len_prompt(dev_df, subject)\n        # prompt = train_prompt + prompt_end\n        train_prompt = gen_prompt(dev_df, subject, k)\n        prompt = train_prompt + prompt_end\n\n        while not check_valid_length(model, tokenizer, prompt) and k > 0:\n            k -= 1\n            train_prompt = gen_prompt(dev_df, subject, k)\n            prompt = train_prompt + prompt_end\n\n        label = test_df.iloc[i, test_df.shape[1] - 1]\n\n        inputs = tokenizer([prompt])\n        inputs = {k: torch.tensor(v).to(device) for k, v in inputs.items()}\n        seq_len_sum += inputs[\"input_ids\"].shape[-1]\n        model = change_llama_offloading_heavy_const(model, inputs[\"input_ids\"].shape[-1] // sparsity_level, (sparsity_level - 1) // 4 + 1, 4)\n        # model = change_llama_offloading_heavy_const(model, inputs[\"input_ids\"].shape[-1] // sparsity_level, 2, 2)\n        output_ids = model.generate(\n            **inputs,\n            do_sample=False,\n            max_new_tokens=4,\n        )\n        output_ids = output_ids[0][len(inputs[\"input_ids\"][0]) :]\n        output = tokenizer.decode(\n            output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n        )\n\n        cor = output.strip().startswith(label)\n        cors.append(cor)\n\n    acc = np.mean(cors)\n    cors = np.array(cors)\n\n    print(\"Average accuracy {:.3f} - {}\".format(acc, subject))\n    print(\"Average sequence length: {:.3f}\".format(seq_len_sum / test_df.shape[0]))\n\n    return cors, acc\n\n\ndef main(model, tokenizer, device, sparsity_level=16):\n    data_dir = \"/home/ubuntu/data/mmlu\"\n    ntrain = 15\n\n    # model, tokenizer = load_model(\n    #     args.model_path,\n    #     device=args.device,\n    #     num_gpus=args.num_gpus,\n    #     max_gpu_memory=args.max_gpu_memory,\n    #     load_8bit=args.load_8bit,\n    #     cpu_offloading=args.cpu_offloading,\n    #     revision=args.revision,\n    # )\n\n    # model = AutoModelForCausalLM.from_pretrained(args.model_path).half().cuda()\n    # tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n\n    subjects = sorted(\n        [\n            f.split(\"_test.csv\")[0]\n            for f in os.listdir(os.path.join(data_dir, \"test\"))\n            if \"_test.csv\" in f\n        ]\n    )\n\n    all_cors = []\n    subcat_cors = {\n        subcat: []\n        for subcat_lists in get_subcategories().values()\n        for subcat in subcat_lists\n    }\n    cat_cors = {cat: [] for cat in get_categories()}\n\n    for subject in tqdm(subjects):\n        dev_df = pd.read_csv(\n            os.path.join(data_dir, \"dev\", subject + \"_dev.csv\"), header=None\n        )\n        # val_df = pd.read_csv(\n        #     os.path.join(data_dir, \"val\", subject + \"_val.csv\"), header=None\n        # )\n        # dev_df = pd.concat([dev_df, val_df])\n        dev_df = dev_df[:ntrain]\n        test_df = pd.read_csv(\n            os.path.join(data_dir, \"test\", subject + \"_test.csv\"), header=None\n        )\n\n        cors, acc = evaluate(ntrain, subject, model, tokenizer, dev_df, test_df, sparsity_level, device)\n        subcats = get_subcategories()[subject]\n        for subcat in subcats:\n            subcat_cors[subcat].append(cors)\n            for key in get_categories().keys():\n                if subcat in get_categories()[key]:\n                    cat_cors[key].append(cors)\n        all_cors.append(cors)\n\n    for subcat in subcat_cors:\n        subcat_acc = np.mean(np.concatenate(subcat_cors[subcat]))\n        print(\"Average accuracy {:.3f} - {}\".format(subcat_acc, subcat))\n\n    for cat in cat_cors:\n        cat_acc = np.mean(np.concatenate(cat_cors[cat]))\n        print(\"Average accuracy {:.3f} - {}\".format(cat_acc, cat))\n\n    weighted_acc = np.mean(np.concatenate(all_cors))\n    # print(f\"Model path: {args.model_path}\")\n    print(f\"Average accuracy: {weighted_acc:.4f}\")\n    return weighted_acc\n\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument(\n#         \"--model_path\",\n#         type=str,\n#         default=\"meta-llama/Llama-2-7b-hf\",\n#         help=\"Path to model\",\n#     )\n#     parser.add_argument(\n#         \"--device\", type=str, default=\"cuda\", help=\"Device to run model on\"\n#     )\n#     # add_model_args(parser)\n#     args = parser.parse_args()\n\n#     main(args)\n\n\nmodel_path = \"meta-llama/Llama-2-7b-hf\"\n\n# llama-7b: 5.68\n# model_path = \"/home/ec2-user/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16\"\n\n# opt-6.7b: 10.86\n# model_path = \"/home/ec2-user/.cache/huggingface/hub/models--facebook--opt-6.7b/snapshots/a45aa65bbeb77c1558bc99bedc6779195462dab0\"\n\n# model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n\ndevice = \"cuda\"\n\nmodel = LlamaForCausalLM.from_pretrained(model_path).half().to(device)\n# tokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\n\nconfig = AutoConfig.from_pretrained(model_path)\n\n\n\nmodel = convert_kvcache_llama_offloading(model, config, 128, 4, 4)\n\nchannel_config = None\nwith open(\"llama2-7b-qk-channel-config.json\", \"r\") as f:\n    channel_config = json.load(f)\n\n# with open(\"llama-7b-channel-config.json\", \"r\") as f:\n#     channel_config = json.load(f)\n\nmodel = convert_llama_offloading_channel_config(model, channel_config, \"qk\")\n\nmain(model, tokenizer, device, 16)\n\n# sparsity_levels = [1, 2, 4, 8, 16, 32, 64]\n\n# scores = {}\n\n# for sparsity_level in sparsity_levels:\n#     score = main(model, tokenizer, device, sparsity_level)\n#     scores[sparsity_level] = score\n\n# print(scores)\n\n# with open(\"offloading-mmlu-scores.json\", \"w\") as f:\n#     json.dump(scores, f)\n\n"}
{"type": "source_file", "path": "evaluation/modify_mistral.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.mistral.configuration_mistral import MistralConfig\nfrom transformers.models.mistral.modeling_mistral import MistralRotaryEmbedding, MistralAttention, apply_rotary_pos_emb\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass MistralAttention_heavy_hitter(nn.Module):\n    \"\"\"\n    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n    and \"Generating Long Sequences with Sparse Transformers\".\n    \"\"\"\n\n    def __init__(self, config: MistralConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        # channel config\n        self.sorted_channel = None\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = MistralRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n            grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_mistral_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_mistral_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, MistralAttention):\n            device = next(module.parameters()).device\n            new_module = MistralAttention_heavy_hitter(config).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n\n    return model\n\n\ndef convert_mistral_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MistralAttention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_mistral_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, MistralAttention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "evaluation/modify_qwen2.py", "content": "import os\nimport pdb\nimport copy\nimport math\nimport numpy as np \nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\n\nfrom transformers.models.qwen2.configuration_qwen2 import Qwen2Config\nfrom transformers.models.qwen2.modeling_qwen2 import Qwen2RotaryEmbedding, Qwen2Attention, apply_rotary_pos_emb, repeat_kv\nfrom transformers.cache_utils import Cache\n\n\ndef pseudo_quantize(tensor, q_bit):\n    max_quant = 2 ** q_bit - 1\n\n    min_val = tensor.min(dim=-1, keepdim=True)[0]\n    max_val = tensor.max(dim=-1, keepdim=True)[0]\n    \n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n\n    scale = max_quant / range_val\n    quantized = torch.round((tensor - min_val) * scale).clamp(0, max_quant)\n\n    dequantized = quantized / scale + min_val\n\n    return dequantized\n\nclass Qwen2Attention_heavy_hitter(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            print(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.attention_dropout = config.attention_dropout\n\n        # channel config\n        self.sorted_channel = None\n\n        # heavy const\n        self.heavy_const = 2048\n        self.group_factor = 1\n        self.label_bits = 16\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = Qwen2RotaryEmbedding(config=self.config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        # if self.config.num_hidden_layers != 32:\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n        \n        if q_len > 1 or self.layer_idx < 2:\n            return self.flash_forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            \n        if position_embeddings is None:\n            print(\n                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n                \"removed and `position_embeddings` will be mandatory.\"\n            )\n            cos, sin = self.rotary_emb(value_states, position_ids)\n        else:\n            cos, sin = position_embeddings\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n\n        # group_factor = 8\n        assert self.head_dim % self.group_factor == 0\n\n        kv_seq_len = key_states.shape[-2]\n        if self.sorted_channel is not None:\n            sorted_query_states = query_states.transpose(1,2)\n            sorted_key_states = key_states.transpose(1,2)\n            sorted_query_states = torch.gather(sorted_query_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, q_len, -1, -1)).transpose(1,2)\n            sorted_key_states = torch.gather(sorted_key_states, -1, self.sorted_channel.unsqueeze(0).unsqueeze(0).expand(bsz, kv_seq_len, -1, -1)).transpose(1,2)\n\n            # grouped by mean\n            # grouped_query = sorted_query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_key = sorted_key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // group_factor, group_factor).sum(dim=-1) / group_factor\n            # grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // group_factor)\n\n            # outlier channel only\n            outlier_num = self.head_dim // self.group_factor\n            grouped_query = sorted_query_states[:,:,:,:outlier_num]\n            grouped_key = sorted_key_states[:,:,:,:outlier_num]\n\n\n            # quantization\n            if self.label_bits < 16:\n                grouped_query = pseudo_quantize(grouped_query, self.label_bits)\n                grouped_key = pseudo_quantize(grouped_key, self.label_bits)\n\n\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n            # precision problem??\n        else:\n            grouped_query = query_states.reshape(bsz, self.num_heads, q_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_key = key_states.reshape(bsz, self.num_heads, kv_seq_len, self.head_dim // self.group_factor, self.group_factor).sum(dim=-1) / self.group_factor\n            grouped_attn_weights = torch.matmul(grouped_query, grouped_key.transpose(2, 3)) / math.sqrt(self.head_dim // self.group_factor)\n\n        # assert torch.allclose(attn_weights, grouped_attn_weights, atol=0.001), f\"{torch.nonzero(torch.abs(attn_weights - grouped_attn_weights))}\"\n\n\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        # NOTE: transformers 4.44 doesn't provide attention_mask for LlamaAttention??\n        # print(f\"attention mask is {attention_mask}.\")\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n        elif q_len == kv_seq_len:\n            boolean_mask = torch.tril(torch.ones(q_len, kv_seq_len, dtype=torch.bool, device=attn_weights.device))\n            attention_mask = torch.zeros(q_len, kv_seq_len, dtype=torch.float16, device=attn_weights.device)\n            attention_mask = attention_mask.masked_fill(boolean_mask == False, float('-inf')).view(1, 1, q_len, kv_seq_len)\n            attn_weights = attn_weights + attention_mask\n            grouped_attn_weights = grouped_attn_weights + attention_mask\n\n\n        h2_mask = torch.zeros_like(attn_weights).bool()\n        # heavy_const = 256\n        # [bs, num_heads, q_len, kv_len] -> [bs, num_heads, q_len, heavy_const]\n        # sorted_weights, indices = attn_weights.sort(dim=-1, descending=True)\n        _, indices = grouped_attn_weights.sort(dim=-1, descending=True)\n        discard_indices = indices[:, :, :, self.heavy_const:]\n        h2_mask.scatter_(3, discard_indices, 1)\n        attn_weights.masked_fill_(h2_mask, float('-inf'))\n\n        # # free gpu memory\n        # if self.config.num_hidden_layers != 32:\n        #     h2_mask = None\n        #     grouped_attn_weights = None\n        #     indices = None\n        #     discard_indices = None\n        #     grouped_query = None\n        #     grouped_key = None\n        #     sorted_query_states = None\n        #     sorted_key_states = None\n        #     query_states = None\n        #     key_states = None\n        #     gc.collect()\n        #     torch.cuda.empty_cache()\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n\ndef convert_kvcache_qwen2_heavy_recent(model, config, heavy_const=256, group_factor=8, label_bits=4):\n\n    for name, module in reversed(model._modules.items()):\n\n        if len(list(module.children())) > 0:\n            model._modules[name] = convert_kvcache_qwen2_heavy_recent(module, config, heavy_const, group_factor)\n\n        if isinstance(module, Qwen2Attention):\n            device = next(module.parameters()).device\n            new_module = Qwen2Attention_heavy_hitter(config, module.layer_idx).half().to(device)\n            new_module.load_state_dict(module.state_dict())\n            new_module.heavy_const = heavy_const\n            new_module.group_factor = group_factor\n            new_module.label_bits = label_bits\n            model._modules[name] = new_module\n            model._modules[name].flash_forward = module.forward\n\n    return model\n\n\ndef convert_qwen2_channel_config(model, channel_config, selected_channel=\"k\"):\n\n    selected_channel = \".\" + selected_channel + \"_proj\"\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, Qwen2Attention_heavy_hitter):\n            device = next(module.parameters()).device\n            module.sorted_channel = torch.tensor(channel_config[name + selected_channel]).to(device)\n\n    return model\n\n\ndef change_qwen2_heavy_const(model, heavy_const=128, group_factor=4, label_bits=4):\n\n    for name, module in model.named_modules():\n\n        if isinstance(module, Qwen2Attention_heavy_hitter):\n            \n            module.heavy_const = heavy_const\n            module.group_factor = group_factor\n            module.label_bits = label_bits\n\n    return model"}
{"type": "source_file", "path": "config/offline_calibration.py", "content": "from argparse import ArgumentParser\nimport json\nimport os\n\nimport torch\nfrom torch import nn\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, AutoConfig\nfrom transformers.models.llama.modeling_llama import LlamaAttention, repeat_kv\nfrom transformers.models.mistral.modeling_mistral import MistralAttention\nfrom transformers.models.mixtral.modeling_mixtral import MixtralAttention\nfrom transformers.models.mllama.modeling_mllama import MllamaTextSelfAttention\nfrom transformers.models.qwen2.modeling_qwen2 import Qwen2Attention\nfrom datasets import load_dataset\nfrom functools import partial\nimport tqdm\n\ndef get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n    dataset = dataset.shuffle(seed=42)\n    samples = []\n    n_run = 0\n    for data in dataset:\n        line = data[\"text\"]\n        line = line.strip()\n        line_encoded = tokenizer.encode(line)\n        if len(line_encoded) > block_size:\n            continue\n        sample = torch.tensor([line_encoded])\n        if sample.numel() == 0:\n            continue\n        samples.append(sample)\n        n_run += 1\n        if n_run == n_samples:\n            break\n\n    # now concatenate all samples and split according to block size\n    cat_samples = torch.cat(samples, dim=1)\n    n_split = cat_samples.shape[1] // block_size\n    print(f\" * Split into {n_split} blocks\")\n    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n\n\ndef get_q_hook(m, x, y, name, output_dict, model):\n    if isinstance(y, tuple):\n        y = y[0]\n    head_dim = model.config.hidden_size // model.config.num_attention_heads\n    assert y.shape[-1] == model.config.hidden_size\n    y_max = y.view(-1, model.config.num_attention_heads, head_dim).abs().mean(dim=0).cpu().detach()\n    if name not in output_dict:\n        output_dict[name] = y_max\n    else:\n        output_dict[name] += y_max\n        \ndef get_k_hook(m, x, y, name, output_dict, model):\n    if isinstance(y, tuple):\n        y = y[0]\n    head_dim = model.config.hidden_size // model.config.num_attention_heads\n    assert y.shape[-1] == model.config.num_key_value_heads * head_dim\n    y_max = y.view(-1, model.config.num_key_value_heads, head_dim).abs().mean(dim=0).cpu().detach()\n    if name not in output_dict:\n        output_dict[name] = y_max\n    else:\n        output_dict[name] += y_max\n        \ndef get_qk_hook(m, args, kwargs, result, name, output_dict):\n    assert isinstance(kwargs, dict)\n    \n    hidden_states = kwargs[\"hidden_states\"]\n    position_ids = kwargs[\"position_ids\"]\n\n    bsz, q_len, _ = hidden_states.size()\n\n    q = m.q_proj(hidden_states).view(bsz, q_len, m.num_heads, m.head_dim).transpose(1, 2)\n    k = m.k_proj(hidden_states).view(bsz, q_len, m.num_key_value_heads, m.head_dim).transpose(1, 2)\n    v = m.v_proj(hidden_states).view(bsz, q_len, m.num_key_value_heads, m.head_dim).transpose(1, 2)\n    kv_seq_len = k.shape[-2]\n    if isinstance(m, LlamaAttention):\n        cos, sin = m.rotary_emb(v, position_ids)\n        q, k = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(q, k, cos, sin)\n    elif isinstance(m, MixtralAttention):\n        cos, sin = m.rotary_emb(v, kv_seq_len)\n        q, k = transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n    elif isinstance(m, MllamaTextSelfAttention):\n        cos, sin = kwargs[\"position_embeddings\"]\n        q, k = transformers.models.mllama.modeling_mllama.apply_rotary_pos_emb(q, k, cos, sin)\n    \n    k = repeat_kv(k, m.num_key_value_groups)\n    v = repeat_kv(v, m.num_key_value_groups)\n    \n    # Method 1: every token only attend to itself\n    out = q * k\n    out = out.reshape(-1, m.num_heads, m.head_dim).abs().mean(dim=0).cpu().detach()\n    \n    # Method 2: every token attend to all tokens and get the abs mean\n    # out = torch.einsum('bhik,bhjk->bhijk', q, k) # [bsz, num_heads, kv_seq_len, kv_seq_len, head_dim]\n    # out = out.abs().mean(dim=3).mean(dim=2).mean(dim=0).cpu().detach()\n    \n    # Method 3: every token attend to all tokens with mask and use softmax\n    # out = torch.einsum('bhik,bhjk->bhijk', q, k) # [bsz, num_heads, kv_seq_len, kv_seq_len, head_dim]\n    # boolean_mask = torch.tril(torch.ones(kv_seq_len, kv_seq_len, dtype=torch.bool, device=out.device))\n    # mask = torch.zeros(kv_seq_len, kv_seq_len, dtype=torch.float16, device=out.device)\n    # mask = mask.masked_fill(boolean_mask == False, float('-inf'))\n    # out += mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n    # out = out.reshape(bsz, m.num_heads, kv_seq_len, kv_seq_len * m.head_dim)\n    # attn_score = torch.softmax(out, dim=-1) # Compute the contribution of each channel\n    # attn_score = attn_score.reshape(bsz, m.num_heads, kv_seq_len, kv_seq_len, m.head_dim)\n    # out = attn_score.mean(dim=3).mean(dim=2).mean(dim=0).cpu().detach()\n    \n    # Method 4: per channel contribution to attention scores\n    # out = torch.einsum('bhik,bhjk->bhijk', q, k) # [bsz, num_heads, kv_seq_len, kv_seq_len, head_dim]\n    # out = out.sum(dim=3).mean(dim=2).mean(dim=0).abs().cpu().detach()\n    \n    # Method 5: chanel ranking based on contribution to attention score variance\n    # out = torch.einsum('bhik,bhjk->bhijk', q, k) # [bsz, num_heads, kv_seq_len, kv_seq_len, head_dim]\n    # variance = out.var(dim=3).mean(dim=2).mean(dim=0)\n    # out = variance.cpu().detach()\n    \n    \n    if name not in output_dict:\n        output_dict[name+\".qk_proj\"] = out\n    else:\n        output_dict[name+\".qk_proj\"] += out\n\n\n\n@torch.no_grad()\ndef get_calib_feat(model: nn.Module, tokenizer):\n    output_dict = dict()\n\n    hooks = []\n    for name, m in model.named_modules():\n        # get_qkv hook\n        if isinstance(m, nn.Linear) and \"q_proj\" in name:\n            hooks.append(\n                m.register_forward_hook(\n                    partial(get_q_hook, name=name, output_dict=output_dict, model=model)))\n        if isinstance(m, nn.Linear) and \"k_proj\" in name:\n            hooks.append(\n                m.register_forward_hook(\n                    partial(get_k_hook, name=name, output_dict=output_dict, model=model)))\n        # attention hook\n        if isinstance(m, LlamaAttention) or isinstance(m, MistralAttention) or isinstance(m, MixtralAttention) or isinstance(m, MllamaTextSelfAttention) or isinstance(m, Qwen2Attention):\n            hooks.append(\n                m.register_forward_hook(\n                    partial(get_qk_hook, name=name, output_dict=output_dict), with_kwargs=True))\n\n    print(\"Collecting activation scales...\")\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = model.device\n\n    samples = get_calib_dataset(tokenizer, n_samples=256, block_size=512)\n    pbar = tqdm.tqdm(samples)\n    for input_ids in pbar:\n        input_ids = input_ids.to(device)\n        model(input_ids)\n\n    for hook in hooks:\n        hook.remove()\n    return output_dict\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--model_path', type=str, required=True, help='Selected model')\n    parser.add_argument('--output_dir', type=str, default=\"config/\", help='Output directory')\n    \n    args = parser.parse_args()\n    \n    \n    model_path = args.model_path\n    kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    output_dict = get_calib_feat(model, tokenizer)\n\n    channel_config = dict()\n\n    for k, v in output_dict.items():\n        vals, inds = torch.sort(output_dict[k], dim=-1, descending=True)\n        channel_config[k] = inds.tolist()\n        \n    #model_owner, model_name = model_path.split(\"/\")\n    path_parts = [p for p in model_path.split(\"/\") if p.strip()]  \n    model_owner = path_parts[-2]\n    model_name = path_parts[-1]\n    \n    output_dir = os.path.join(args.output_dir, model_owner)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    output_path = os.path.join(output_dir, model_name + \".json\")\n\n    with open(output_path, \"w\") as f:\n        json.dump(channel_config, f)\n"}
{"type": "source_file", "path": "benchmark/e2e/gpt-fast/model.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import Tensor\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n@dataclass\nclass ModelArgs:\n    block_size: int = 16384\n    vocab_size: int = 32000\n    n_layer: int = 32\n    n_head: int = 32\n    dim: int = 4096\n    intermediate_size: int = None\n    n_local_heads: int = -1\n    head_dim: int = 64\n    rope_base: float = 40000\n    norm_eps: float = 1e-5\n\n    def __post_init__(self):\n        if self.n_local_heads == -1:\n            self.n_local_heads = self.n_head\n        if self.intermediate_size is None:\n            hidden_dim = 4 * self.dim\n            n_hidden = int(2 * hidden_dim / 3)\n            self.intermediate_size = find_multiple(n_hidden, 256)\n        self.head_dim = self.dim // self.n_head\n\n    @classmethod\n    def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        # fuzzy search\n        config = [config for config in transformer_configs if config in str(name).upper() or config in str(name)]\n        assert len(config) == 1, name\n        return cls(**transformer_configs[config[0]])\n\n\ntransformer_configs = {\n    \"CodeLlama-7b-Python-hf\": dict(block_size=16384, vocab_size=32000, n_layer=32, dim = 4096, rope_base=1000000),\n    \"7B\": dict(n_layer=32, n_head=32, dim=4096),\n    \"13B\": dict(n_layer=40, n_head=40, dim=5120),\n    \"30B\": dict(n_layer=60, n_head=52, dim=6656),\n    \"34B\": dict(n_layer=48, n_head=64, dim=8192, vocab_size=32000, n_local_heads=8, intermediate_size=22016, rope_base=1000000), # CodeLlama-34B-Python-hf\n    \"70B\": dict(n_layer=80, n_head=64, dim=8192, n_local_heads=8, intermediate_size=28672),\n}\n\nclass KVCache(nn.Module):\n    def __init__(self, max_batch_size, max_seq_length, n_heads, head_dim, dtype=torch.bfloat16):\n        super().__init__()\n        cache_shape = (max_batch_size, n_heads, max_seq_length, head_dim)\n        self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))\n        self.register_buffer('v_cache', torch.zeros(cache_shape, dtype=dtype))\n\n    def update(self, input_pos, k_val, v_val):\n        # input_pos: [S], k_val: [B, H, S, D]\n        assert input_pos.shape[0] == k_val.shape[2]\n\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n\n        return k_out, v_out\n\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.config = config\n\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n        self.layers = nn.ModuleList(TransformerBlock(config) for _ in range(config.n_layer))\n        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n        self.freqs_cis: Optional[Tensor] = None\n        self.mask_cache: Optional[Tensor] = None\n        self.max_batch_size = -1\n        self.max_seq_length = -1\n\n    def setup_caches(self, max_batch_size, max_seq_length):\n        if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads, head_dim)\n\n        self.freqs_cis = precompute_freqs_cis(self.config.block_size, self.config.dim // self.config.n_head, self.config.rope_base)\n        self.causal_mask = torch.tril(torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool))\n\n    def forward(self, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n        mask = self.causal_mask[None, None, input_pos]\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.attention = Attention(config)\n        self.feed_forward = FeedForward(config)\n        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n\n    def forward(self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: Tensor) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        assert config.dim % config.n_head == 0\n\n        total_head_dim = (config.n_head + 2 * config.n_local_heads) * config.head_dim\n        # key, query, value projections for all heads, but in a batch\n        self.wqkv = nn.Linear(config.dim, total_head_dim, bias=False)\n        self.wo = nn.Linear(config.dim, config.dim, bias=False)\n        self.kv_cache = None\n\n        self.n_head = config.n_head\n        self.head_dim = config.head_dim\n        self.n_local_heads = config.n_local_heads\n        self.dim = config.dim\n        self._register_load_state_dict_pre_hook(self.load_hook)\n\n    def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])\n\n    def forward(self, x: Tensor, freqs_cis: Tensor, mask: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q = apply_rotary_emb(q, freqs_cis)\n        k = apply_rotary_emb(k, freqs_cis)\n\n        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n\n        k = k.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        v = v.repeat_interleave(self.n_head // self.n_local_heads, dim=1)\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n\n        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)\n\n        y = self.wo(y)\n        return y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w3 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w2 = nn.Linear(config.intermediate_size, config.dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef precompute_freqs_cis(\n    seq_len: int, n_elem: int, base: int = 10000\n) -> Tensor:\n    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=torch.bfloat16)\n\n\ndef apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)\n"}
{"type": "source_file", "path": "benchmark/e2e/flexgen/quantize.py", "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport importlib\nimport time\nfrom math import ceil\nfrom pathlib import Path\n\nimport torch\nimport importlib\nimport time\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nfrom sentencepiece import SentencePieceProcessor\n\ntry:\n    from GPTQ import GenericGPTQRunner, InputRecorder, lm_eval\nexcept:\n    pass\n\nfrom model import Transformer\n\n##### Quantization Primitives ######\n\ndef dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    # assumes symmetric quantization\n    # assumes axis == 0\n    # assumes dense memory format\n    # TODO(future): relax ^ as needed\n\n    # default setup for affine quantization of activations\n    eps = torch.finfo(torch.float32).eps\n\n    # get min and max\n    min_val, max_val = torch.aminmax(x, dim=1)\n\n    # calculate scales and zero_points based on min and max\n    # reference: https://fburl.com/code/srbiybme\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n\n    # reference: https://fburl.com/code/4wll53rk\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    # ensure scales is the same dtype as the original tensor\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n\n    # quantize based on qmin/qmax/scales/zp\n    # reference: https://www.internalfb.com/code/fbsource/[8edc275012b1]/fbcode/caffe2/torch/ao/quantization/fx/_decomposed.py?lines=63\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n\n    return quant, scales, zero_points\n\ndef get_group_qparams(w, n_bit=4, groupsize=128):\n    # needed for GPTQ with padding\n    if groupsize > w.shape[-1]:\n        groupsize = w.shape[-1]\n    assert groupsize > 1\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    max_val = to_quant.amax(dim=1, keepdim=True)\n    min_val = to_quant.amin(dim=1, keepdim=True)\n    max_int = 2**n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-6) / max_int\n    zeros = min_val + scales * (2 ** (n_bit - 1))\n    return scales.to(torch.bfloat16).reshape(w.shape[0], -1), zeros.to(\n        torch.bfloat16\n    ).reshape(w.shape[0], -1)\n\n\ndef pack_scales_and_zeros(scales, zeros):\n    assert scales.shape == zeros.shape\n    assert scales.dtype == torch.bfloat16\n    assert zeros.dtype == torch.bfloat16\n    return (\n        torch.cat(\n            [\n                scales.reshape(scales.size(0), scales.size(1), 1),\n                zeros.reshape(zeros.size(0), zeros.size(1), 1),\n            ],\n            2,\n        )\n        .transpose(0, 1)\n        .contiguous()\n    )\n\n\ndef unpack_scales_and_zeros(scales_and_zeros):\n    assert len(scales_and_zeros.shape) == 3 and scales_and_zeros.shape[2] == 2\n    assert scales_and_zeros.dtype == torch.float\n    return torch.split(scales_and_zeros.transpose(0, 1), 1, 2)\n\n\ndef group_quantize_tensor_from_qparams(w, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column quantize\n    if groupsize > w.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w.shape[-1]\n\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n    min_val = zeros - scales * (2 ** (n_bit - 1))\n    max_int = 2**n_bit - 1\n    min_int = 0\n    w_int32 = (\n        to_quant.sub(min_val)\n        .div(scales)\n        .round()\n        .clamp_(min_int, max_int)\n        .to(torch.int32)\n        .reshape_as(w)\n    )\n\n    return w_int32\n\n\ndef group_quantize_tensor(w, n_bit=4, groupsize=128):\n    scales, zeros = get_group_qparams(w, n_bit, groupsize)\n    w_int32 = group_quantize_tensor_from_qparams(w, scales, zeros, n_bit, groupsize)\n    scales_and_zeros = pack_scales_and_zeros(scales, zeros)\n    return w_int32, scales_and_zeros\n\n\ndef group_dequantize_tensor_from_qparams(\n    w_int32, scales, zeros, n_bit=4, groupsize=128\n):\n    assert groupsize > 1\n    # needed for GPTQ single column dequantize\n    if groupsize > w_int32.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w_int32.shape[-1]\n    assert w_int32.shape[-1] % groupsize == 0\n    assert w_int32.dim() == 2\n\n    w_int32_grouped = w_int32.reshape(-1, groupsize)\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n\n    w_dq = (\n        w_int32_grouped.sub(2 ** (n_bit - 1)).mul(scales).add(zeros).reshape_as(w_int32)\n    )\n    return w_dq\n\n\ndef group_dequantize_tensor(w_int32, scales_and_zeros, n_bit=4, groupsize=128):\n    scales, zeros = unpack_scales_and_zeros(scales_and_zeros)\n    return group_dequantize_tensor_from_qparams(\n        w_int32, scales, zeros, n_bit, groupsize\n    )\n\nclass QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    def create_quantized_state_dict(self) -> \"StateDict\":\n        pass\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\nclass GPTQQuantHandler(QuantHandler):\n    \"\"\"\n    This class implements a GPTQ QuantHandler that can be used to apply GPTQ to a model in concert with the GenericGPTQRunner class.\n    Unlike the base QuantHandler class, the user does not need to implement the create_quantized_state_dict, instead they have to reimplement\n    __init__ such that it defines the functions for the quantization mode. User is expected to reimplement convert_for_runtime.\n\n    The following functions (which must be defined in __init__) are used to define the quantization mode for both GPTQ and\n    create_quantized_state_dict. Here is a description of each function.\n\n    get_qparams_func:\n        A function that calculates the quantization qparams for an input tensor.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            qparams: it can have any format but will need to be handled by the other defined functions below.\n\n    quantize_func:\n        A function that applies quantization to an input tensor. It should be noted\n        that this function needs to be able to handle quantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n            qparams: the output from get_qparams_func\n        Returns:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n\n\n    dequantize_func:\n        A function that dequantizes an input quantized weight tensor. It should be noted\n        that this function needs to be able to handle dequantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            weight: A 2d weight tensor with non-integer dtype.\n\n    combine_qparams_list_func:\n        A function that combines several qparams into one qparam.\n        Args:\n            qparams_list: a list of qparams objects, each obtained by calling get_qparams_func\n            on a single group from a weight tensor\n        Returns:\n            qparams: an object of the same format as the qparams above.\n\n    skip_layer_func:\n        A function that determines which linear layers should be skipped during GPTQ\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            skip: boolean indicating whether layer should be skipped\n\n    make_names_and_values_dict_func:\n        A function that prepares the qparams and quantized_weight and creates a dictionary indicating how they\n        should be inserted into the state_dict. Generally any packing of the weight and qparams should be done here.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            names_and_values_dict: a dictionary mapping the name of the parameters of the quantized module to the\n            corresponding quantized weights and qparams.\n    \"\"\"\n    def __init__(self):\n        assert self.mod is not None\n        assert self.get_qparams_func is not None\n        assert self.quantize_func is not None\n        assert self.dequantize_func is not None\n        assert self.combine_qparams_list_func is not None\n        assert self.make_names_and_values_dict_func is not None\n\n    @staticmethod\n    def get_inputs(model, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs) -> \"MultiInput\":\n        input_recorder = InputRecorder(\n            model,\n            tokenizer,\n            calibration_seq_length,\n            pad_calibration_inputs,\n        )\n        task_dict = lm_eval.tasks.get_task_dict(calibration_tasks)\n        print(\"Obtaining GPTQ calibration inputs on: \", calibration_tasks)\n        lm_eval.evaluator.evaluate(\n            input_recorder,\n            task_dict,\n            limit=calibration_limit,\n        )\n        inputs = input_recorder.get_recorded_inputs()\n        print(f\"Obtained {len(inputs[0].values)} calibration samples\")\n        return inputs\n\n    @torch.no_grad()\n    def create_quantized_state_dict(\n        self,\n        tokenizer,\n        blocksize,\n        percdamp,\n        groupsize,\n        calibration_tasks,\n        calibration_limit,\n        calibration_seq_length,\n        pad_calibration_inputs,\n    ) -> \"StateDict\":\n        inputs = GPTQQuantHandler.get_inputs(self.mod, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs)\n        print(\"Tracing model for GPTQ\")\n        GPTQ_runner = GenericGPTQRunner(\n            self.mod,\n            inputs,\n            blocksize,\n            percdamp,\n            groupsize,\n        ).configure_quantization_mode(\n            self.get_qparams_func,\n            self.quantize_func,\n            self.dequantize_func,\n            self.combine_qparams_list_func,\n            self.make_names_and_values_dict_func,\n            self.skip_layer_func\n        )\n\n        print(\"Applying GPTQ to weights\")\n        GPTQ_runner.run()\n        return GPTQ_runner.get_quantized_state_dict()\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\n##### Weight-only int8 per-channel quantized code ######\n\ndef replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(module, name, WeightOnlyInt8Linear(child.in_features, child.out_features))\n        else:\n            replace_linear_weight_only_int8_per_channel(child)\n\nclass WeightOnlyInt8QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                int8_weight, scales, _ = dynamically_quantize_per_channel(mod.weight.float(), -128, 127, torch.int8)\n                cur_state_dict[f\"{fqn}.weight\"] = int8_weight\n                cur_state_dict[f\"{fqn}.scales\"] = scales.to(mod.weight.dtype)\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_weight_only_int8_per_channel(self.mod)\n        return self.mod\n\n\nclass WeightOnlyInt8Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.register_buffer(\"weight\", torch.empty((out_features, in_features), dtype=torch.int8))\n        self.register_buffer(\"scales\", torch.ones(out_features, dtype=torch.bfloat16))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.to(dtype=input.dtype)) * self.scales\n\n##### weight only int4 per channel groupwise quantized code ######\n\ndef prepare_int4_weight_and_scales_and_zeros(weight_bf16, groupsize, inner_k_tiles):\n    weight_int32, scales_and_zeros = group_quantize_tensor(\n        weight_bf16, n_bit=4, groupsize=groupsize\n    )\n    weight_int4pack = torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)\n    return weight_int4pack, scales_and_zeros\n\n\ndef linear_forward_int4(x, weight_int4pack, scales_and_zeros, out_features, groupsize):\n    origin_x_size = x.size()\n    x = x.reshape(-1, origin_x_size[-1])\n    c = torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)\n    new_shape = origin_x_size[:-1] + (out_features,)\n    c = c.reshape(new_shape)\n    return c\n\n\ndef _check_linear_int4_k(k, groupsize = 1, inner_k_tiles = 1):\n    return k % groupsize == 0 and k % (inner_k_tiles * 16) == 0\n\ndef replace_linear_int4(module, groupsize, inner_k_tiles, padding):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            if _check_linear_int4_k(child.in_features, groupsize, inner_k_tiles):\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=False,\n                ))\n            elif padding:\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=True,\n                ))\n        else:\n            replace_linear_int4(child, groupsize, inner_k_tiles, padding)\n\n\nclass WeightOnlyInt4QuantHandler:\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        assert groupsize in [32, 64, 128, 256]\n        assert inner_k_tiles in [2, 4, 8]\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                assert not mod.bias\n                out_features = mod.out_features\n                in_features = mod.in_features\n                assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n                print(f\"linear: {fqn}, in={in_features}, out={out_features}\")\n\n                weight = mod.weight.data\n                if not _check_linear_int4_k(in_features, self.groupsize, self.inner_k_tiles):\n                    if self.padding:\n                        from model import find_multiple\n                        import torch.nn.functional as F\n                        print(f\"warning: {fqn} is padded to satisfy in_features % 1024 == 0\")\n                        padded_in_features = find_multiple(in_features, 1024)\n                        weight = F.pad(weight, pad=(0, padded_in_features - in_features))\n                    else:\n                        print(f\"warning: {fqn} is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, \" +\n                            \"and that groupsize and inner_k_tiles*16 evenly divide into it\")\n                        continue\n                weight_int4pack, scales_and_zeros = prepare_int4_weight_and_scales_and_zeros(\n                    weight.to(torch.bfloat16).to('cuda'), self.groupsize, self.inner_k_tiles\n                )\n                cur_state_dict[f\"{fqn}.weight\"] = weight_int4pack.to('cpu')\n                cur_state_dict[f\"{fqn}.scales_and_zeros\"] = scales_and_zeros.to('cpu')\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4GPTQQuantHandler(GPTQQuantHandler):\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        from model import find_multiple\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        self.get_qparams_func = lambda w: get_group_qparams(w, 4, groupsize)\n        self.quantize_func = lambda w, qparams: \\\n            group_quantize_tensor_from_qparams(w, qparams[0], qparams[1], 4, groupsize)\n        self.dequantize_func = lambda q, qparams: \\\n            group_dequantize_tensor_from_qparams(q, qparams[0], qparams[1], 4, groupsize).float()\n        self.combine_qparams_list_func = lambda qparams_list: \\\n            [torch.cat(x, dim=1) for x in zip(*qparams_list)]\n        # skip unless padding=True or its correctly sized\n        self.skip_layer_func = lambda linear_weight: not (\n            _check_linear_int4_k(linear_weight.shape[-1], groupsize, inner_k_tiles) or padding\n        )\n        # we need to do the padding here, both for q and the qparams if necessary\n        def make_names_and_values_dict_func(q, qparams):\n            k = q.shape[1]\n            new_k = find_multiple(k, 1024)\n            # how much we need to pad the weight\n            delta_k = new_k - q.shape[1]\n            final_q = torch.ops.aten._convert_weight_to_int4pack(F.pad(q, pad=(0, delta_k)), inner_k_tiles)\n            scales_and_zeros = pack_scales_and_zeros(*qparams)\n            # how many new groups we need for padded weight\n            delta_groups = new_k // groupsize - scales_and_zeros.shape[0]\n            final_s_and_z = F.pad(scales_and_zeros, pad=(0,0,0,0,0, delta_groups), value=1)\n            return {\"weight\": final_q, \"scales_and_zeros\": final_s_and_z}\n        self.make_names_and_values_dict_func = make_names_and_values_dict_func\n        super().__init__()\n\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(\n            self, in_features: int, out_features: int,\n            bias=True, device=None, dtype=None, groupsize: int = 128, inner_k_tiles: int = 8, padding: bool = True,\n    ) -> None:\n        super().__init__()\n        self.padding = padding\n        if padding:\n            from model import find_multiple\n            self.origin_in_features = in_features\n            in_features = find_multiple(in_features, 1024)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias, \"require bias=False\"\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n\n        assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n        assert in_features % (inner_k_tiles * 16) == 0, \"require in_features % (innerKTiles * 16) == 0\"\n        self.register_buffer(\n            \"weight\",\n            torch.empty((out_features // 8, in_features // (inner_k_tiles * 16), 32, inner_k_tiles // 2), dtype=torch.int32)\n        )\n        self.register_buffer(\n            \"scales_and_zeros\",\n            torch.empty((in_features // groupsize, out_features, 2), dtype=torch.bfloat16)\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        input = input.to(torch.bfloat16)\n        if self.padding:\n            import torch.nn.functional as F\n            input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))\n        return linear_forward_int4(\n            input,\n            self.weight, self.scales_and_zeros, self.out_features, self.groupsize\n        )\n\n\ndef quantize(\n    checkpoint_path: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"),\n    mode: str = 'int8',\n    # following arguments only available when setting int4 quantization.\n    groupsize: int = 128,\n    # following arguments only used for GPTQ\n    calibration_tasks: list = [\"hellaswag\"],\n    calibration_limit: int = 1000,\n    calibration_seq_length: int = 100,\n    pad_calibration_inputs: bool = False,\n    percdamp: float = .01,\n    blocksize: int = 128,\n    label: str = '',\n) -> None:\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    device = 'cpu'\n    precision = torch.bfloat16\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n    model = model.to(dtype=precision, device=device)\n\n    if mode == 'int8':\n        print(\"Quantizing model weights for int8 weight-only symmetric per-channel quantization\")\n        quant_handler = WeightOnlyInt8QuantHandler(model)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f'{label}int8.pth')\n\n    elif mode == 'int4':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization\")\n        quant_handler = WeightOnlyInt4QuantHandler(model, groupsize)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4.g{groupsize}.pth\")\n\n    elif mode == 'int4-gptq':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization using GPTQ...\")\n        quant_handler = WeightOnlyInt4GPTQQuantHandler(model, groupsize)\n\n        tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n        assert tokenizer_path.is_file(), tokenizer_path\n        tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n\n        quantized_state_dict = quant_handler.create_quantized_state_dict(\n            tokenizer,\n            blocksize,\n            percdamp,\n            groupsize,\n            calibration_tasks,\n            calibration_limit,\n            calibration_seq_length,\n            pad_calibration_inputs\n        )\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4-gptq.g{groupsize}.pth\")\n    else:\n        raise ValueError(f\"Invalid quantization mode {mode} needs to be one of [int8, int4, int4-gpptq]\")\n\n    quantize_path = dir_name / new_base_name\n    print(f\"Writing quantized weights to {quantize_path}\")\n    quantize_path.unlink(missing_ok=True) # remove existing file if one already there\n    torch.save(quantized_state_dict, quantize_path)\n    print(f\"Quantization complete took {time.time() - t0:.02f} seconds\")\n    return\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Quantize a model.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"), help='Path to the model checkpoint to be quantized.')\n    parser.add_argument('--mode', '-q', type=str, default='int8', choices=['int8', 'int4', 'int4-gptq'], help='type of quantization to perform')\n    parser.add_argument('--groupsize', type=int, default=32, help='Group size for int4 quantization.')\n    parser.add_argument('--calibration_tasks', type=str, nargs='+', default=['hellaswag'], help='tasks to do gptq calibration on, if doing gptq')\n    parser.add_argument('--calibration_limit', type=int, default=1000, help='number of samples to use for gptq calibration')\n    parser.add_argument('--calibration_seq_length', type=int, default=100, help='length of sequences to use for gptq calibration')\n    parser.add_argument('--pad_calibration_inputs', type=bool, default=False, help='pads sequences shorter than calibration_seq_length to that length, yielding more calibration inputs but running much slower')\n    parser.add_argument('--percdamp', type=float, default=.01, help='gptq percentage dampening')\n    parser.add_argument('--blocksize', type=int, default=128, help='blocksize for gptq')\n    parser.add_argument('--label', type=str, default='_', help='label to add to output filename')\n\n    args = parser.parse_args()\n    quantize(args.checkpoint_path, args.mode, args.groupsize, args.calibration_tasks, args.calibration_limit, args.calibration_seq_length, args.pad_calibration_inputs, args.percdamp, args.blocksize, args.label)\n"}
