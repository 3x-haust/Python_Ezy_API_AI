{"repo_info": {"repo_name": "credentialLeakDB", "repo_owner": "EC-DIGIT-CSIRC", "repo_url": "https://github.com/EC-DIGIT-CSIRC/credentialLeakDB"}}
{"type": "test_file", "path": "tests/lib/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/lib/basecollector/test_collector.py", "content": "import unittest\n\nimport pandas as pd\n\nfrom lib.basecollector.collector import *\n\n\nclass TestBaseCollector(unittest.TestCase):\n    def test_collect(self):\n        valid_csv_file = 'tests/fixtures/data.csv'\n        invalid_csv_file = 'tests/fixtures/dataDOESNTEXIST.csv'\n\n        tc = BaseCollector()\n        df: pd.DataFrame\n        status, df = tc.collect(valid_csv_file)\n        assert status == \"OK\"\n        assert not df.empty\n        assert df.shape[0] > 1\n\n        status, df = tc.collect(invalid_csv_file)\n        assert status != \"OK\"\n        assert df.empty\n"}
{"type": "test_file", "path": "tests/lib/baseenricher/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/lib/basecollector/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_parser_spycloud.py", "content": "import unittest\nfrom pathlib import Path\nfrom modules.parsers.spycloud import SpyCloudParser\nfrom modules.collectors.spycloud.collector import SpyCloudCollector\n\n\nclass SpyCloudParserTest(unittest.TestCase):\n    def test_parse(self):\n        path = 'tests/fixtures/data_anonymized_spycloud.csv'\n        tc = SpyCloudCollector()\n        statuscode, df = tc.collect(Path(path))\n        assert statuscode == \"OK\"\n        tp = SpyCloudParser()\n        idf = tp.parse(df)\n        assert idf\n        # print([ i for i in idf ])\n        for i in idf:\n            if \"error_msg\" in i.dict() and i.error_msg:\n                print(\"error_msg: %s\" % i.error_msg)\n                print(\"orig_line: %s\" % i.original_line)\n"}
{"type": "test_file", "path": "tests/modules/enrichers/test_external_email.py", "content": "import unittest\n\nfrom modules.enrichers.external_email import ExternalEmailEnricher\n\nclass TestExternalEmailEnricher(unittest.TestCase):\n    def test_is_external(self):\n        external_email = \"foobar@example.com\"\n        tee = ExternalEmailEnricher()\n        assert tee.is_external_email(external_email)\n\n        internal_email = \"foobar.example@ec.europa.eu\"\n        assert tee.is_internal_email(internal_email)\n"}
{"type": "test_file", "path": "tests/lib/baseparser/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/modules/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_deduper.py", "content": "from models.idf import InternalDataFormat\n\nfrom modules.filters.deduper import Deduper\n\n\ndef test_load_bf():\n    dd = Deduper()\n    assert not dd.bloomf_loaded\n    dd.load_bf()\n    assert dd.bloomf_loaded\n\n\ndef test_dedup():\n    dd = Deduper()\n    idf = InternalDataFormat(email=\"aaron@example.com\", password=\"12345\",\n                             notify=False, needs_human_intervention=False)\n    idf2 = dd.dedup(idf)\n    assert not idf2\n    idf = InternalDataFormat(email=\"aaron999735@example.com\", password=\"12345XXX\",\n                             notify=False, needs_human_intervention=False)\n    idf2 = dd.dedup(idf)\n    assert idf2\n"}
{"type": "test_file", "path": "tests/lib/baseoutput/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/lib/baseoutput/test_output.py", "content": "import unittest\n\nfrom lib.baseoutput.output import BaseOutput\n\nclass TestBaseOutput(unittest.TestCase):\n    def test_process(self):\n        to = BaseOutput()\n        assert to.process(\"test_outputfile.txt\")\n"}
{"type": "test_file", "path": "tests/test_collector_spycloud.py", "content": "import unittest\n\nfrom pathlib import Path\n\nfrom modules.collectors.spycloud.collector import SpyCloudCollector\n\n\nclass SpyCloudCollectorTest(unittest.TestCase):\n    def test_collect(self):\n        path = Path('tests/fixtures/data_anonymized_spycloud.csv')\n        tc = SpyCloudCollector()\n        statuscode, data = tc.collect(path)\n        assert statuscode == \"OK\"\n        assert data.iloc[0]['breach_title'] == 'Freedom Fox Combo List'\n        assert data.iloc[0]['email'] == 'peter@example.com'\n"}
{"type": "test_file", "path": "tests/test_enrichment.py", "content": "import unittest\nfrom pathlib import Path\n\n# from modules.enrichers.ldap import LDAPEnricher\nfrom modules.enrichers.external_email import ExternalEmailEnricher\nfrom modules.enrichers.abuse_contact import AbuseContactLookup\nfrom modules.enrichers.vip import VIPEnricher\n\n\nclass TestVIPenrichment(unittest.TestCase):\n\n    def test_load_vips(self):\n        path = 'tests/fixtures/vips.txt'\n        te = VIPEnricher(Path(path))\n\n        assert te.is_vip('AARON@example.com')\n        assert te.is_vip('aaron@example.com')\n        assert not te.is_vip('foobar-doesnotexist')\n\n    def test_load_vips_invalid_path(self):\n        path = 'tests/fixtures/vips.txt-doesnotexist'\n        te = VIPEnricher(Path(path))  # will pass because there we catch the exception\n        self.assertRaises(Exception, te.load_vips, path)\n\n\nclass TestIsExternalEmail(unittest.TestCase):\n    def test_is_internal(self):\n        email = \"foobar.example@ext.ec.europa.eu\"\n        te = ExternalEmailEnricher()\n        assert te.is_internal_email(email)\n        domain = \"ec.europa.eu\"\n        assert te.is_internal_email(domain)\n\n    def test_is_external(self):\n        email = \"aaron@example.com\"\n        te = ExternalEmailEnricher()\n        assert te.is_external_email(email)\n\n\nclass TestAbuseContactLookup(unittest.TestCase):\n    def test_lookup(self):\n        email = \"aaron@example.com\"\n        te = AbuseContactLookup()\n        assert email == te.lookup(email)[0]\n        email = \"aaron@example.ec.europa.eu\"\n        assert \"ec-digit-csirc@ec.europa.eu\" == te.lookup(email)[0]\n"}
{"type": "test_file", "path": "tests/lib/test_logger.py", "content": "from lib.helpers import getlogger\n\n\nlogger = getlogger(__name__)\n\n\nclass Foo:\n    def __init__(self):\n        pass\n\n    def do_smthg(self):\n        logger.info(\"bar\")\n        print(\"baz\")\n\n\ndef test_logger():\n    logger.info(\"starting up the class\")\n\n    f = Foo()\n    f.do_smthg()\n    logger.info(\"DONE\")\n    assert True\n\n\nif __name__ == \"__main__\":\n    test_logger()\n"}
{"type": "test_file", "path": "tests/modules/enrichers/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/lib/baseparser/test_parser.py", "content": "import unittest\n\nimport pandas as pd\n\nfrom lib.baseparser.parser import BaseParser\n\nclass TestBaseParser(unittest.TestCase):\n    def test_parse(self):\n        tp = BaseParser()\n        df = pd.DataFrame()\n        tp.parse(df)\n        assert True     # not very useful right now but the structure for the test case is here\n"}
{"type": "test_file", "path": "tests/test_filter.py", "content": "from models.idf import InternalDataFormat\n\nfrom modules.filters.filter import Filter\n\n\ndef test_filter():\n    fi = Filter()\n    idf = InternalDataFormat(email = \"aaron@example.com\", password = \"12345\", notify = False,\n                             needs_human_intervention = False)\n    idf2 = fi.filter(idf)\n    assert idf2 == idf\n"}
{"type": "test_file", "path": "tests/test_main.py", "content": "from lib.helpers import getlogger\n\nimport urllib.parse\nimport uuid\nimport unittest\n\nfrom fastapi.testclient import TestClient\n\nfrom lib.db.db import _connect_db as connect_db\n\nfrom api.main import *\n\nVALID_AUTH = {'x-api-key': 'random-test-api-key'}\nINVALID_AUTH = {'x-api-key': 'random-test-api-XXX'}\n\nlogger = getlogger(__name__)\nclient = TestClient(app)  # ,  base_url='http://localhost:8080/')\n\n\ndef test_ping():\n    response = client.get(\"/ping\")\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"pong\"}\n\n\nclass DBTestCases(unittest.TestCase):\n    def test_get_db(self):\n        assert get_db() is not None\n\n    def test_close_db(self):\n        get_db()  # initialize connection\n        self.assertIsNone(close_db())\n        get_db()  # re-initialize connection\n\n    def test_connect_invalid_db(self):\n        self.assertRaises(Exception, connect_db, 'SOME INVALID DSN')\n\n\ndef test_fetch_valid_api_keys():\n    assert True\n\n\nclass APIKeyTests(unittest.TestCase):\n    \"\"\"Test API key functions\"\"\"\n\n    def test_validate_api_key_header(self):\n        self.assertRaises(Exception, validate_api_key_header, \"\")\n\n    def test_is_valid_api_key(self):\n        assert is_valid_api_key(VALID_AUTH['x-api-key'])\n\n    def test_is_INVALID_api_key(self):\n        assert not is_valid_api_key(INVALID_AUTH['x-api-key'])\n\n    def test_validate_api_key(self):\n        assert True\n\n\ndef test_root_auth():\n    response = client.get(\"/\", headers = VALID_AUTH)\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"Hello World\"}\n\n\n# noinspection PyPep8Naming\ndef test_root_INVALID_auth():\n    response = client.get(\"/\", headers = INVALID_AUTH)\n    assert response.status_code == 403\n\n\ndef test_get_user_by_email():\n    email = urllib.parse.quote(\"aaron@example.com\")\n    response = client.get(\"/user/%s\" % email, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] >= 1\n\n\n# noinspection PyPep8Naming\ndef test_get_nonexistent_user_by_INVALID_email():\n    email = urllib.parse.quote(\"aaron@doesnotexist.com\")\n    response = client.get(\"/user/%s\" % email, headers = VALID_AUTH)\n    assert response.status_code != 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] == 0\n\n\ndef test_get_user_by_email_and_password():\n    email = urllib.parse.quote(\"aaron@example.com\")\n    passwd = \"12345\"\n    response = client.get(\"/user_and_password/%s/%s\" % (email, passwd), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] >= 1\n\n\n# noinspection PyPep8Naming\ndef test_get_nonexistent_user_by_email_and_INVALID_password():\n    email = urllib.parse.quote(\"aaron@example.com\")\n    passwd = \"12345XXXXXXXXXX\"\n    response = client.get(\"/user_and_password/%s/%s\" % (email, passwd), headers = VALID_AUTH)\n    assert response.status_code == 404\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] == 0\n\n\ndef test_check_user_by_email():\n    email = urllib.parse.quote(\"aaron@example.com\")\n    response = client.get(\"/exists/by_email/%s\" % email, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] >= 1\n\n\n# noinspection PyPep8Naming\ndef test_check_nonexistent_user_by_INVALID_email():\n    email = urllib.parse.quote(\"aaron@doesnotexist.com\")\n    response = client.get(\"/exists/by_email/%s\" % email, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    print(data)\n    assert \"meta\" in response.text and \"data\" in response.text and data['data'][0]['count'] == 0\n\n\ndef test_check_user_by_password():\n    password = \"12345\"\n    response = client.get(\"/exists/by_password/%s\" % password, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] >= 1\n\n\n# noinspection PyPep8Naming\ndef test_check_nonexistent_user_by_INVALID_password():\n    password = 'DOESNOTEXIST@59w47YTISJGw496UASGJSATARSASJKGJSAKGASRG'\n    response = client.get(\"/exists/by_password/%s\" % password, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['data'][0]['count'] == 0\n\n\ndef test_check_user_by_domain():\n    domain = \"example.com\"\n    response = client.get(\"/exists/by_domain/%s\" % domain, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['meta']['count'] >= 1\n\n\n# noinspection PyPep8Naming\ndef test_check_nonexistent_user_by_INVALID_domain():\n    domain = \"example.com-foobar-2esugksti2uwasgjskhsjhsa.net\"\n    response = client.get(\"/exists/by_domain/%s\" % domain, headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \"data\" in response.text and data['data'][0]['count'] == 0\n\n\ndef test_get_reporters():\n    response = client.get(\"/reporter/\", headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert \"meta\" in response.text and \\\n           \"data\" in response.text and \\\n           data['meta']['count'] >= 1 and \\\n           data['data'][0]['reporter_name'] == 'aaron'\n\n\ndef test_get_sources():\n    response = client.get(\"/source_name/\", headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    answerset = set(i['source_name'] for i in data['data'])\n    print(answerset)\n    assert \"meta\" in response.text and \\\n           \"data\" in response.text and \\\n           data['meta']['count'] >= 1 and \\\n           \"HaveIBeenPwned\" in answerset\n\n\ndef test_new_leak():\n    test_data = {\n        \"ticket_id\": \"CSIRC-202\",\n        \"summary\": \"a test leak, please ignore\",\n        \"reporter_name\": \"aaron\",\n        \"source_name\": \"spycloud\",\n        \"breach_ts\": \"2021-03-24T16:08:33.405Z\",\n        \"source_publish_ts\": \"2021-03-24T16:08:33.405Z\"\n    }\n    response = client.post(\"/leak/\", json = test_data, headers = VALID_AUTH)\n    assert response.status_code == 201\n    data = response.json()\n    assert \"meta\" in response.text and \\\n           \"data\" in response.text and \\\n           data['meta']['count'] >= 1 and \\\n           data['data'][0]['id'] >= 1\n    return int(data['data'][0]['id'])\n\n\ndef test_update_leak():\n    test_data = {\n        \"ticket_id\": \"CSIRC-202\",\n        \"summary\": \"an UPDATE-able test leak, please ignore\",\n        \"reporter_name\": \"aaron\",\n        \"source_name\": \"spycloud\",\n        \"breach_ts\": \"2021-01-01T00:00:00.000Z\",\n        \"source_publish_ts\": \"2021-01-02T00:00:00.000Z\",\n    }\n    response = client.post(\"/leak/\", json = test_data, headers = VALID_AUTH)\n    assert response.status_code == 201\n    data = response.json()\n    assert \"meta\" in response.text and \\\n           \"data\" in response.text and \\\n           data['meta']['count'] >= 1 and \\\n           data['data'][0]['id'] >= 1\n    _id = data['data'][0]['id']\n\n    # now UPDATE it\n    test_data['summary'] = \"We UPDATED the test leak now!\"\n    test_data['id'] = _id\n    response = client.put('/leak/', json = test_data, headers = VALID_AUTH)\n    assert response.status_code == 200\n\n    # fetch the results and see if it's really updated\n    response = client.get('/leak/%s' % (_id,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    assert response.json()['data'][0]['summary'] == \"We UPDATED the test leak now!\"\n\n    # now try to fetch an invalid ID\n    response = client.get('/leak/%s' % (_id + 10000,), headers = VALID_AUTH)\n    assert response.status_code == 404\n\n\n# noinspection PyPep8Naming\ndef test_update_INVALID_leak():\n    test_data = {\n        \"id\": -1,\n        \"ticket_id\": \"CSIRC-202\",\n        \"summary\": \"trying to update a leak which does NOT EXIST\",\n        \"reporter_name\": \"aaron\",\n        \"source_name\": \"spycloud\",\n        \"breach_ts\": \"2021-01-01T00:00:00.000Z\",\n        \"source_publish_ts\": \"2021-01-02T00:00:00.000Z\",\n    }\n    response = client.put('/leak/', json = test_data, headers = VALID_AUTH)\n    assert response.status_code == 400\n    assert response.json()['data'] == []\n\n\n# By summary\ndef test_get_leak_by_summary():\n    summary = \"COMB\"\n    response = client.get('/leak/by_summary/%s' % (summary,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert data['data'][0]['summary'] == summary\n    assert data['data'][0]['reporter_name'] == 'aaron'\n\n\n# noinspection PyPep8Naming\ndef test_get_leak_by_INVALID_summary():\n    summary = \"COMB-XXX-DOESNETEXIST\"\n    response = client.get('/leak/by_summary/%s' % (summary,), headers = VALID_AUTH)\n    assert response.status_code == 404\n    data = response.json()\n    assert data['meta']['count'] == 0\n\n\n# By ticket_id\ndef test_get_leak_by_ticket_id():\n    ticket_id = \"CSIRC-102\"  # we know that exists based on the db.sql import\n    response = client.get('/leak/by_ticket_id/%s' % (ticket_id,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert data['data'][0]['summary'] == \"COMB\"\n\n\n# noinspection PyPep8Naming\ndef test_get_leak_by_INVALID_ticket_id():\n    ticket_id = \"COMB-XXX-DOESNETEXIST\"\n    response = client.get('/leak/by_ticket_id/%s' % (ticket_id,), headers = VALID_AUTH)\n    assert response.status_code == 404\n    data = response.json()\n    assert data['meta']['count'] == 0\n\n\ndef test_get_all_leaks():\n    response = client.get('/leak/all', headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] > 0\n\n\ndef test_get_leak_by_reporter():\n    response = client.get('leak/by_reporter/%s' % (\"aaron\",), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] > 0\n\n\ndef test_get_leak_by_source():\n    response = client.get('leak/by_source/%s' % (\"spycloud\",), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] > 0\n\n\n# #################################################################################\n# leak_data\n\ndef test_get_leak_data_by_leak():\n    leak_id = 1  # we know this exists by the db.sql INSERT\n    response = client.get('/leak_data/%s' % (leak_id,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert data['data'][0]['email'] == 'aaron@example.com'\n\n\n# noinspection PyPep8Naming\ndef test_get_leak_data_by_INVALID_leak():\n    leak_id = -1  # we know this does not exist\n    response = client.get('/leak_data/%s' % (leak_id,), headers = VALID_AUTH)\n    assert response.status_code == 404\n    data = response.json()\n    assert data['meta']['count'] == 0\n    assert data['data'] == []\n\n\ndef test_get_leak_data_by_ticket_id():\n    ticket_id = 'CISRC-199'  # we know this exists by the db.sql INSERT\n    response = client.get('/leak_data/by_ticket_id/%s' % (ticket_id,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert data['data'][0]['email'] == 'aaron@example.com'\n    assert data['data'][1]['email'] == 'sarah@example.com'\n\n\ndef insert_leak_data(d: dict) -> int:\n    \"\"\" generic test function for INSERTing a leak_data row given by d.\n\n    @:param d: a row as dict\n    @:returns ID: ID of the newly inserted row\n    @:rtype: int\n    \"\"\"\n    response = client.post(\"/leak_data/\", json = d, headers = VALID_AUTH)\n    print(response)\n    print(response.text)\n    assert response.status_code == 201\n    data = response.json()\n    print(data)\n    assert \"meta\" in data and \\\n           \"data\" in data and \\\n           data['meta']['count'] >= 1 and \\\n           data['data'][0]['id'] >= 1\n    return data['data'][0]['id']\n\n\ndef test_new_leak_data():\n    \"\"\" INSERT a new leak_data row.\"\"\"\n    test_data = {\n        \"leak_id\": 1,\n        \"email\": \"aaron2@example.com\",\n        \"password\": \"000000\",\n        \"password_plain\": \"000000\",\n        \"password_hashed\": \"d232105eb59a344df4b54db1c24009b1\",\n        \"hash_algo\": \"md5\",\n        \"ticket_id\": \"CSIRC-102\",\n        \"email_verified\": False,\n        \"password_verified_ok\": False,\n        \"ip\": \"5.6.7.8\",\n        \"domain\": \"example.com\",\n        \"browser\": \"Chrome\",\n        \"malware_name\": \"n/a\",\n        \"infected_machine\": \"n/a\",\n        \"dg\": \"DIGIT\",\n        \"needs_human_intervention\": False,\n        \"notify\": False\n    }\n    _id = insert_leak_data(test_data)\n    assert _id >= 0\n    return _id\n\n\ndef test_update_leak_data():\n    random_str = uuid.uuid4()\n    test_data = {\n        \"leak_id\": 1,\n        \"email\": \"aaron%s@example.com\" % (random_str,),\n        \"password\": \"000000\",\n        \"password_plain\": \"000000\",\n        \"password_hashed\": \"d232105eb59a344df4b54db1c24009b1\",\n        \"hash_algo\": \"md5\",\n        \"ticket_id\": \"CSIRC-102\",\n        \"email_verified\": False,\n        \"password_verified_ok\": False,\n        \"ip\": \"5.6.7.8\",\n        \"domain\": \"example.com\",\n        \"browser\": \"Chrome\",\n        \"malware_name\": \"n/a\",\n        \"infected_machine\": \"n/a\",\n        \"dg\": \"DIGIT\",\n        \"needs_human_intervention\": False,\n        \"notify\": False\n    }\n    # create my own leak_data row\n    _id = insert_leak_data(test_data)\n\n    # now UPDATE it\n    random_str2 = uuid.uuid4()\n    email2 = \"aaron-%s@example.com\" % random_str2\n\n    test_data['id'] = _id\n    test_data.update({\"email\": email2})\n    response = client.put('/leak_data/', json = test_data, headers = VALID_AUTH)\n    assert response.status_code == 200\n    print(\"after UPDATE: response = %r\" % response.json())\n\n    # fetch the results and see if it's really updated\n    response = client.get('/leak_data/%s' % (_id,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    print(\"data: %r\" % response.json()['data'])\n    assert response.json()['data'][0]['email'] == email2\n\n\ndef test_import_csv_with_leak_id():\n    _id = test_new_leak()\n    fixtures_file = \"./tests/fixtures/data.csv\"\n    f = open(fixtures_file, \"rb\")\n    response = client.post('/import/csv/by_leak/%s' % (_id,), files = {\"_file\": f}, headers = VALID_AUTH)\n    logger.info(\"response = %r\" % response.text)\n    assert 200 <= response.status_code < 300\n    assert response.json()['meta']['count'] >= 0\n\n\ndef test_check_file():\n    assert True  # trivial check, not implemented yet actually in main.py\n\n\ndef test_enrich_email_to_vip():\n    email_vip = \"aaron@example.com\"\n    response = client.get('/enrich/email_to_vip/%s' % (email_vip,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert data['data'][0]['is_vip']\n\n\n# noinspection PyPep8Naming\ndef test_enrich_email_to_vip_INVALID():\n    email_vip = \"aaron-invalid-does-not-exist@example.com\"\n    response = client.get('/enrich/email_to_vip/%s' % (email_vip,), headers = VALID_AUTH)\n    assert response.status_code == 200\n    data = response.json()\n    assert data['meta']['count'] >= 1\n    assert not data['data'][0]['is_vip']\n\n\nclass TestImportCSVSpycloud(unittest.TestCase):\n    def test_import_csv_spycloud_invalid_ticket_id(self):\n        fixtures_file = \"./tests/fixtures/data_anonymized_spycloud.csv\"\n        f = open(fixtures_file, \"rb\")\n        response = client.post('/import/csv/spycloud/?summary=test2', files = {\"_file\": f}, headers = VALID_AUTH)\n        assert response.status_code >= 400\n\n    def test_import_csv_spycloud(self):\n        fixtures_file = \"./tests/fixtures/data_anonymized_spycloud.csv\"\n        f = open(fixtures_file, \"rb\")\n        response = client.post('/import/csv/spycloud/%s?summary=test2' % (\"ticket99\",), files = {\"_file\": f},\n                               headers = VALID_AUTH)\n        assert 200 <= response.status_code < 300\n        assert response.json()['meta']['count'] >= 0\n\n\nclass TestEnricherEmailToDG(unittest.TestCase):\n    response = None\n\n    def test_enrich_dg_by_email(self):\n        email = \"aaron@example.com\"\n        if not os.getenv('CED_SERVER'):\n            with self.assertRaises(Exception):\n                client.get('/enrich/email_to_dg/%s' % (email,), headers = VALID_AUTH)\n        else:\n            response = client.get('/enrich/email_to_dg/%s' % (email,), headers = VALID_AUTH)\n            assert response.status_code == 200\n            data = response.json()\n            assert data['meta']['count'] >= 1\n            assert data['data'][0]['dg']\n"}
{"type": "test_file", "path": "tests/lib/baseenricher/test_enricher.py", "content": "import unittest\n\nfrom lib.baseenricher.enricher import BaseEnricher\nfrom models.idf import InternalDataFormat\n\n\nclass TestBaseEnricher(unittest.TestCase):\n    def test_enrich(self):\n        idf = InternalDataFormat(email=\"foo@example.com\", password = \"12345\", notify = True)\n        te = BaseEnricher()\n        result = te.enrich(idf)\n        assert result == idf"}
{"type": "test_file", "path": "tests/lib/test_helpers.py", "content": "from lib.helpers import anonymize_password\n\ndef test_anonymize_password():\n    pass1 = \"12345678\"\n    expected = \"1*****78\"\n    assert anonymize_password(pass1) == expected\n\n    pass2 = \"123\"\n    expected = \"123\"\n    assert anonymize_password(pass2) == expected\n\n    pass3 = \"12\"\n    expected = \"12\"\n    assert anonymize_password(pass3) == expected\n\n    pass4 = \"\"\n    expected = \"\"\n    assert anonymize_password(pass4) == expected\n\n    pass5 = None\n    expected = None\n    assert anonymize_password(pass5) == expected\n"}
{"type": "source_file", "path": "lib/baseparser/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/enrichers/vip.py", "content": "\"\"\"VIP Enricher. Can determine if an email addr. is a VIP and needs to be treated specially.\"\"\"\n\nimport os\nimport logging\nfrom pathlib import Path\n\nfrom typing import List\n\n\nclass VIPEnricher:\n    \"\"\"Can determine if an Email Address is a VIP. Super trivial code.\"\"\"\n\n    vips = []\n\n    def __init__(self, vipfile: Path = Path('VIPs.txt')):\n        try:\n            self.load_vips(os.getenv('VIPLIST', default = vipfile))\n        except Exception as ex:\n            logging.error(\"Could not load VIP list. Using an empty list and continuing. Exception: %s\" % str(ex))\n\n    def load_vips(self, path: Path) -> List[str]:\n        \"\"\"Load the external reference data set of the known VIPs.\"\"\"\n        with open(path, 'r') as f:\n            self.vips = [x.strip().upper() for x in f.readlines()]\n            return self.vips\n\n    def is_vip(self, email: str) -> bool:\n        \"\"\"Check if an email address is a VIP.\"\"\"\n        return email.upper() in self.vips\n\n    def __str__(self):\n        return \",\".join(self.vips)\n\n    def __repr__(self):\n        return \",\".join(self.vips)\n"}
{"type": "source_file", "path": "config.SAMPLE.py", "content": "\"\"\"Configuration stored here.\nTo make this work, please copy it over to api/config.py (make sure you don't overwrite\nan existing file!!!\nEdit that file there and add a random string to the list.\nCommunicate that random string to the API key user.\n\nThen reload the server (or it gets reloaded automatically).\n\"\"\"\n\n\nconfig = {\n    \"api_keys\": [\"random-test-api-key\", \"another-example-api-key\"]\n}\n"}
{"type": "source_file", "path": "modules/collectors/spycloud/__init__.py", "content": ""}
{"type": "source_file", "path": "models/__init__.py", "content": ""}
{"type": "source_file", "path": "lib/baseoutput/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/collectors/parser.py", "content": "#!/usr/bin/env python3\n\"\"\"importer.parser \"\"\"\n\n\nfrom lib.helpers import getlogger\nfrom pathlib import Path\nimport csv\nimport time\n\nimport pandas as pd\n\ndebug = True\n\nlogger = getlogger(__name__)\n\n\n# noinspection PyTypeChecker\ndef peek_into_file(fname: Path) -> csv.Dialect:\n    \"\"\"\n    Peek into a file in order to determine the dialect for pandas.read_csv() / csv functions.\n\n    :param fname: a Path object for the filename\n    :return: a csv.Dialect\n    \"\"\"\n\n    with fname.open(mode='r') as f:\n        sniffer = csv.Sniffer()\n        logger.debug(\"has apikeyheader: %s\", sniffer.has_header(f.readline()))\n        f.seek(0)\n        dialect = sniffer.sniff(f.readline(50))\n        logger.debug(\"delim: '%s'\", dialect.delimiter)\n        logger.debug(\"quotechar: '%s'\", dialect.quotechar)\n        logger.debug(\"doublequote: %s\", dialect.doublequote)\n        logger.debug(\"escapechar: '%s'\", dialect.escapechar)\n        logger.debug(\"lineterminator: %r\", dialect.lineterminator)\n        logger.debug(\"quoting: %s\", dialect.quoting)\n        logger.debug(\"skipinitialspace: %s\", dialect.skipinitialspace)\n        return dialect\n\n\nclass BaseParser:\n    \"\"\"The abstract Parser class.\"\"\"\n    def __init__(self):\n        pass\n\n    def parse_file(self, fname: Path, leak_id: int = None, csv_dialect=None) -> pd.DataFrame:\n        \"\"\"Parse file (non-recursive) and returns a DataFrame with the contents.\n        Overwrite this method in YOUR Parser subclass.\n\n        # Parameters\n          * fname: a Path object with the filename of the CSV file which should be parsed.\n          * leak_id: the leak_id in the DB which is associated with that CSV dump file.\n        # Returns\n            a DataFrame\n            number of errors while parsing\n        \"\"\"\n        logger.info(\"Parsing file %s...\" % fname)\n        try:\n            if csv_dialect:\n                dialect = csv_dialect\n            else:\n                dialect = peek_into_file(fname)     # try to guess\n            df = pd.read_csv(fname, dialect=dialect, error_bad_lines=False, warn_bad_lines=True)  # , usecols=range(2))\n            logger.debug(df.head())\n            logger.debug(df.info())\n            logger.debug(\"Parsing file 2...\")\n            df.insert(0, 'leak_id', leak_id)\n            logger.debug(df.head())\n            logger.debug(\"parsed %s\", fname)\n            return df\n\n        except Exception as ex:\n            logger.error(\"could not pandas.read_csv(%s). Reason: %s. Skipping file.\" % (fname, str(ex)))\n            raise ex        # pass it on\n\n    def normalize_data(self, df: pd.DataFrame, leak_id: int = None) -> pd.DataFrame:\n        \"\"\"\n        Normalize the given data / data frame\n\n        :param df: a pandas df with the leak_data\n        :param leak_id: foreign key to the leak table\n        :return: a pandas df\n        \"\"\"\n        # replace NaN with None\n        return df.where(pd.notnull(df), None)\n\n\nif __name__ == \"__main__\":\n\n\n    p = BaseParser()\n    t0 = time.time()\n    # p.parse_recursively('test_leaks', '*.txt')\n    t1 = time.time()\n    logger.info(\"processed everything in %f [sec]\", (t1 - t0))\n"}
{"type": "source_file", "path": "lib/baseoutput/output.py", "content": "\"\"\"Base, abstract Output class\"\"\"\n\nfrom models.outdf import Answer\n\n\nclass BaseOutput:\n    def __init__(self):\n        pass\n\n    def process(self, output_data: Answer) -> bool:\n        \"\"\"\n        Process the output_data and do something with it.\n\n        :returns bool... True on success.\n        \"\"\"\n        return True\n"}
{"type": "source_file", "path": "modules/collectors/__init__.py", "content": ""}
{"type": "source_file", "path": "models/idf.py", "content": "from typing import List, Optional\nfrom pydantic import BaseModel, IPvAnyAddress\n\n\nclass InternalDataFormat(BaseModel):\n    \"\"\"The Internal Data Format (IDF).\"\"\"\n    leak_id: Optional[str]      # the leak(id) reference\n    email: str\n    password: Optional[str]     # not mandatory yet\n    password_plain: Optional[str]\n    password_hashed: Optional[str]\n    hash_algo: Optional[str]\n    ticket_id: Optional[str]\n    email_verified: Optional[bool] = False\n    password_verified_ok: Optional[bool] = False\n    ip: Optional[IPvAnyAddress]\n    domain: Optional[str]\n    target_domain: Optional[str]\n    browser: Optional[str]\n    malware_name: Optional[str]\n    infected_machine: Optional[str]\n    #\n    # flags set by the enrichers\n    dg: Optional[str]\n    external_user: Optional[bool]\n    is_vip: Optional[bool]\n    is_active_account: Optional[bool]\n    credential_type: Optional[List[str]]    # External, EU Login, etc.\n    report_to: Optional[List[str]]          # whom to report this to?\n    #\n    # meta stuff and things for error reporting\n    count_seen: Optional[int] = 1\n    original_line: Optional[str]\n    error_msg: Optional[str]\n    notify: Optional[bool]\n    needs_human_intervention: Optional[bool]\n"}
{"type": "source_file", "path": "modules/parsers/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/collectors/spycloud/collector.py", "content": "\"\"\"\nSpycloud collector\n\nThis code implements a SpyCloud collector (inherits from BaseCollector)\n\nUpon running a SpyCloud parser on a CSV, the result will be a\n\"\"\"\nfrom pathlib import Path\nimport logging\nimport pandas as pd\n\nfrom lib.basecollector.collector import BaseCollector\nfrom lib.helpers import peek_into_file\n\nNaN_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', '<NA>', 'N/A',\n              'NA', 'NULL', 'NaN', 'n/a', 'null', '-']\n\n\nclass SpyCloudCollector(BaseCollector):\n    def __init__(self):\n        super().__init__()\n\n    def collect(self, input_file: Path, **kwargs) -> (str, pd.DataFrame):\n        try:\n            dialect = peek_into_file(input_file)\n            df = pd.read_csv(input_file, dialect=dialect, na_values=NaN_values,\n                             keep_default_na=False, error_bad_lines=False, warn_bad_lines=True)\n            # XXX FIXME: need to collect the list of (pandas-) unparseable rows and present to user.\n            # For now we simply fail on the whole file. Good enough for the moment.\n        except pd.errors.ParserError as ex:\n            logging.error(\"could not parse CSV file. Reason: %r\" % (str(ex),))\n            return str(ex), pd.DataFrame()\n        return \"OK\", df\n"}
{"type": "source_file", "path": "lib/helpers.py", "content": "import csv\nimport logging\nfrom pathlib import Path\n\nLOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nLOG_FORMAT = '%(asctime)s - [%(name)s:%(module)s:%(funcName)s] - %(levelname)s - %(message)s'\n\ndef getlogger(name: str, log_level=logging.INFO) -> logging.Logger:\n    \"\"\"This is how we do logging. How to use it:\n\n    Add the following code snippet to every module\n    ```\n    logger = getlogger(__name__)\n    logger.info(\"foobar\")\n    ```\n\n    :param name - name of the logger\n    :param log_level - default log level\n\n    :returns logging.Logger object\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(log_level)\n\n    # create console handler\n    ch = logging.StreamHandler()\n\n    formatter = logging.Formatter(LOG_FORMAT)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    logger.info('Logger ready')\n\n    return logger\n\n\ndef peek_into_file(fname: Path) -> csv.Dialect:\n    \"\"\"\n    Peek into a file in order to determine the dialect for pandas.read_csv() / csv functions.\n\n    :param fname: a Path object for the filename\n    :return: a csv.Dialect\n    \"\"\"\n\n    with fname.open(mode = 'r') as f:\n        sniffer = csv.Sniffer()\n        logging.debug(\"has apikeyheader: %s\", sniffer.has_header(f.readline()))\n        f.seek(0)\n        dialect = sniffer.sniff(f.readline(50))\n        logging.debug(\"delim: '%s'\", dialect.delimiter)\n        logging.debug(\"quotechar: '%s'\", dialect.quotechar)\n        logging.debug(\"doublequote: %s\", dialect.doublequote)\n        logging.debug(\"escapechar: '%s'\", dialect.escapechar)\n        logging.debug(\"lineterminator: %r\", dialect.lineterminator)\n        logging.debug(\"quoting: %s\", dialect.quoting)\n        logging.debug(\"skipinitialspace: %s\", dialect.skipinitialspace)\n        # noinspection PyTypeChecker\n        return dialect\n\n\ndef anonymize_password(password: str) -> str:\n    \"\"\"\n    \"*\"-out the characters of a password. Must be 4 chars in length at least.\n\n    :param password: str\n    :returns anonymized password (str):\n\n    \"\"\"\n    anon_password = password\n    if password and len(password) >= 4:\n        prefix = password[:1]\n        suffix = password[-2:]\n        anon_password = prefix + \"*\" * (len(password) - 3) + suffix\n    return anon_password\n"}
{"type": "source_file", "path": "api/__init__.py", "content": ""}
{"type": "source_file", "path": "lib/basecollector/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/enrichers/external_email.py", "content": "\"\"\"ExternalEmailEnricher\"\"\"\n\n\nclass ExternalEmailEnricher:\n    \"\"\"Can determine if an Email Adress is an (organisation-) external email address. Also super trivial code.\"\"\"\n\n    @staticmethod\n    def is_internal_email(email: str) -> bool:\n        email = email.lower()\n        if email and email.endswith('europa.eu') or email.endswith('jrc.it'):\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def is_external_email(email: str) -> bool:\n        return not ExternalEmailEnricher.is_internal_email(email)\n"}
{"type": "source_file", "path": "lib/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/output/db.py", "content": "\"\"\"Database output module. Stores an IDF item to the DB.\"\"\"\nfrom lib.helpers import getlogger\n\nimport psycopg2\nimport psycopg2.extras\n\nfrom lib.baseoutput.output import BaseOutput\nfrom lib.db.db import _get_db\nfrom models.outdf import LeakData\n\n\nlogger = getlogger(__name__)\n\n\nclass PostgresqlOutput(BaseOutput):\n    dbconn = None\n\n    def __init__(self):\n        super().__init__()\n        self.dbconn = _get_db()\n\n    def process(self, data: LeakData) -> bool:\n        \"\"\"Store the output format data into Postgresql.\n\n        :returns True on success\n        :raises psycopg2.Error exception\n        \"\"\"\n\n        sql = \"\"\"\n                INSERT into leak_data(\n                  leak_id, email, password, password_plain, password_hashed, hash_algo, ticket_id, email_verified,\n                  password_verified_ok, ip, domain, browser , malware_name, infected_machine, dg\n                  )\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s )\n                ON CONFLICT ON CONSTRAINT constr_unique_leak_data_leak_id_email_password_domain\n                DO UPDATE SET  count_seen = leak_data.count_seen + 1\n                RETURNING id\n                \"\"\"\n        if data:\n            try:\n                with self.dbconn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n                    print(cur.mogrify(sql, (\n                        data.leak_id, data.email, data.password, data.password_plain, data.password, data.hash_algo,\n                        data.ticket_id, data.email_verified, data.password_verified_ok, data.ip, data.domain,\n                        data.browser, data.malware_name, data.infected_machine, data.dg)))\n                    cur.execute(sql, (\n                        data.leak_id, data.email, data.password, data.password_plain, data.password, data.hash_algo,\n                        data.ticket_id, data.email_verified, data.password_verified_ok, data.ip, data.domain,\n                        data.browser, data.malware_name, data.infected_machine, data.dg))\n                    leak_data_id = int(cur.fetchone()['id'])\n                    print(\"leak_data_id: %s\" % leak_data_id)\n            except psycopg2.Error as ex:\n                logger.error(\"%s(): error: %s\" % (self.process.__name__, ex.pgerror))\n                raise ex\n            return True\n"}
{"type": "source_file", "path": "models/indf.py", "content": "from datetime import datetime\nfrom typing import Optional, Union\nfrom pydantic import BaseModel, IPvAnyAddress\n\n\nclass SpyCloudInputEntry(BaseModel):\n    \"\"\"The SpyCloud intput format - one entry.\"\"\"\n    breach_title: str\n    spycloud_publish_date: Optional[Union[str, datetime]]\n    breach_date: Optional[Union[str, datetime]]\n    email: str  # mandatory\n    domain: str  # mandatory\n    username: Optional[str]\n    password: str\n    salt: Optional[str]\n    target_domain: Optional[str]\n    target_url: Optional[str]\n    password_plaintext: str = None\n    sighting: Optional[int]\n    severity: Optional[str]\n    status: Optional[str]\n    password_type: Optional[str]\n    cc_number: Optional[str]\n    infected_path: Optional[str]\n    infected_machine_id: Optional[str]\n    email_domain: str\n    cc_expiration: Optional[str]\n    cc_last_four: Optional[str]\n    email_username: str\n    user_browser: Optional[str]\n    infected_time: Optional[Union[str, datetime]]\n    ip_addresses: Optional[Union[str, IPvAnyAddress]]\n"}
{"type": "source_file", "path": "modules/enrichers/abuse_contact.py", "content": "\"\"\"AbuseContactLookup: look up the right abuse contact based on a user's email address.\"\"\"\n\nimport collections\nimport re\nfrom typing import List\n\n\nclass AbuseContactLookup:\n    \"\"\"A simple abuse contact lookup class.\"\"\"\n\n    def lookup(self, email: str) -> List[str]:\n        \"\"\"Look up the right abuse contact for credential leaks based on the email address.\n        Example:\n            lookup(\"example@jrc.it\")   --> \"reports@jrc.it\"\n\n        :argument email: the email address\n        :rtype string: string\n        :returns email: the email address for the abuse contact\n        \"\"\"\n\n        \"\"\"The following mapping table is of the form:\n           regular expression   --> email address or \"DIRECT\".   If DIRECT is returned, send directly to the email addr.\n           The matching should proceed top down\n        \"\"\"\n\n        mapping_table = collections.OrderedDict({\n            re.compile(r\"example\\.ec\\.europa\\.eu\", re.X): [\"ec-digit-csirc@ec.europa.eu\"],       # example\n            re.compile(r\".*\\.ec\\.europa\\.eu\", re.X): \"DIRECT\",\n            re.compile(r\".*\", re.X): \"DIRECT\"          # the default catch-all rule. Don't delete!\n        })\n\n        domain = email.split('@')[-1]\n        for k, v in mapping_table.items():\n            if re.match(k, domain):\n                if v == \"DIRECT\":\n                    return [email]\n                else:\n                    return v\n        return [\"\"]\n"}
{"type": "source_file", "path": "modules/enrichers/ldap.py", "content": "import logging\nimport os\nfrom typing import Union\n\nfrom modules.enrichers.ldap_lib import CEDQuery\n\n\nclass LDAPEnricher:\n    \"\"\"LDAP Enricher can query LDAP and offers multiple functions such as email-> dg\"\"\"\n\n    simulate_ldap: bool = False\n\n    def __init__(self):\n        self.simulate_ldap = bool(os.getenv('SIMULATE_LDAP', default = False))\n        self.ced = CEDQuery()\n\n    def email_to_dg(self, email: str) -> str:\n        \"\"\"Return the DG of an email. Note that there might be multiple DGs, we just return the first one here.\"\"\"\n\n        if self.simulate_ldap:\n            return \"Not connected to LDAP\"\n        try:\n            results = self.ced.search_by_mail(email)\n            if results and results[0]['attributes'] and results[0]['attributes']['dg'] and \\\n                    results[0]['attributes']['dg'][0]:\n                return results[0]['attributes']['dg'][0]\n            else:\n                return \"Unknown\"\n        except Exception as ex:\n            logging.error(\"could not query LDAP/CED. Reason: %s\" % str(ex))\n            raise ex\n\n    def email_to_user_id(self, email: str) -> Union[str, None]:\n        \"\"\"Return the userID of an email. \"\"\"\n\n        if self.simulate_ldap:\n            return \"Not connected to LDAP\"\n        try:\n            results = self.ced.search_by_mail(email)\n            if results and results[0]['attributes'] and results[0]['attributes']['ecMoniker'] and \\\n                    results[0]['attributes']['ecMoniker'][0]:\n                return results[0]['attributes']['ecMoniker'][0]\n            else:\n                return None\n        except Exception as ex:\n            logging.error(\"could not query LDAP/CED. Reason: %s\" % str(ex))\n            raise ex\n\n    def email_to_status(self, email: str) -> str:\n        \"\"\"Return the active status.\"\"\"\n\n        if self.simulate_ldap:\n            return \"Not connected to LDAP\"\n\n        try:\n            results = self.ced.search_by_mail(email)\n            if results and results[0]['attributes'] and results[0]['attributes']['recordStatus'] and \\\n                    results[0]['attributes']['recordStatus'][0]:\n                return results[0]['attributes']['recordStatus'][0]\n        except Exception as ex:\n            logging.error(\"could not query LDAP/CED. Reason: %s\" % str(ex))\n            raise ex\n\n    def exists(self, email: str) -> bool:\n        \"\"\"Check if a user exists.\"\"\"\n\n        if self.simulate_ldap:\n            return False\n\n        status = self.email_to_status(email)\n        if status and status.upper() == \"A\":\n            return True\n        else:\n            return False\n"}
{"type": "source_file", "path": "modules/collectors/spycloud.py", "content": "#!/usr/bin/env python3\n\"\"\"Spycloud parser\"\"\"\nimport collections\nimport logging\nfrom pathlib import Path\n\n\nimport pandas as pd\n# from parser import BaseParser\nfrom .parser import BaseParser\n\n\nclass SpycloudParser(BaseParser):\n    \"\"\"Parse Spycloud CSVs\"\"\"\n    def parse_file(self, fname: Path, csv_dialect='excel', leak_id=None) -> pd.DataFrame:\n        \"\"\"Parse the Spycloud CSV files, which are in the form:\n\n            breach_title,spycloud_publish_date,breach_date,email,domain,username,password,salt,target_domain,target_url,password_plaintext,sighting,severity,status,password_type,cc_number,infected_path,infected_machine_id,email_domain,cc_expiration,cc_last_four,email_username,user_browser,infected_time,ip_addresses\n\n        Returns:\n            a DataFrame\n            number of errors while parsing\n        \"\"\"\n        logging.debug(\"Parsing SPYCLOUD file %s...\", fname)\n        try:\n            # df = pd.read_csv(fname, dialect=csv_dialect, header=1, error_bad_lines=False, warn_bad_lines=True)\n            df = pd.read_csv(fname, error_bad_lines=False, warn_bad_lines=True)\n            logging.debug(df)\n            return df\n\n        except Exception as ex:\n            logging.error(\"could not pandas.read_csv(%s). Reason: %s. Skipping file.\" % (fname, str(ex)))\n            return pd.DataFrame()\n\n    def normalize_data(self, df: pd.DataFrame, leak_id=None) -> pd.DataFrame:\n        \"\"\"Bring the pandas DataFrame into an internal data format.\"\"\"\n\n        \"\"\" Spycloud headers:\n          breach_title, spycloud_publish_date, breach_date, email,     domain, username, password, salt, target_domain, target_url, password_plaintext, sighting, severity, status, password_type, cc_number, infected_path, infected_machine_id, email_domain, cc_expiration, cc_last_four, email_username, user_browser, infected_time, ip_addresses\n        map to:\n          _,            leak.source_publish_ts, leak.breach_ts, email, domain, _,        password, _,    target_domain, _,          password_plain, _,            _,          _,    hash_algo, _, _,                          infected_machine, _ , _, _, _, browser, _, ip\n        \"\"\"\n        mapping_tbl = collections.OrderedDict({\n            \"breach_title\": None,\n            \"spycloud_publish_date\": None,\n            \"breach_date\": None,\n            \"email\": \"email\",\n            \"domain\": None,\n            \"username\": None,\n            \"password\": \"password\",\n            \"salt\": None,\n            \"target_domain\": \"target_domain\",\n            \"target_url\": None,\n            \"password_plaintext\": \"password_plain\",\n            \"sighting\": None,\n            \"severity\": None,\n            \"status\": None,\n            \"password_type\": \"hash_algo\",\n            \"cc_number\": None,\n            \"infected_path\": None,\n            \"infected_machine_id\": \"infected_machine\",\n            \"email_domain\": \"domain\",\n            \"cc_expiration\": None,\n            \"cc_last_four\": None,\n            \"email_username\": None,\n            \"user_browser\": \"browser\",\n            \"infected_time\": None,\n            \"ip_addresses\": \"ip\"\n        })\n\n        # This complexity sucks! need to get rid of it. No, itertools won't make it more understandable.\n        retdf = pd.DataFrame()\n        for i, r in df.iterrows():       # go over all df rows. Returns index, row\n            # print(f\"{i}:{r}\")\n            retrow = dict()             # build up what we want to return\n            for k, v in r.items():       # go over all key-val items in the row\n                # print(f\"{k}:{v}\", file=sys.stderr)\n                if k in mapping_tbl.keys():\n                    map_to = mapping_tbl[k]\n                    if k == 'ip_addresses' and v == '-':\n                        v = None\n                    if map_to:\n                        # print(f\"mapping {k} to {map_to}!\")\n                        retrow[map_to] = v\n                    else:\n                        # don't map it\n                        pass\n            logging.debug(\"retrow = %r\" % retrow)\n            retdf = retdf.append(pd.Series(retrow), ignore_index=True)\n        # retdf[:,'leak_id'] = leak_id\n        logging.debug(\"retdf: %s\" % retdf)\n        return retdf\n"}
{"type": "source_file", "path": "modules/output/__init__.py", "content": ""}
{"type": "source_file", "path": "lib/db/__init__.py", "content": ""}
{"type": "source_file", "path": "api/main.py", "content": "\"\"\"\nFastAPI based API on the credentialLeakDB\n\nAuthor: Aaron Kaplan\nLicense: see LICENSE\n\n\"\"\"\n\n# system / base packages\nfrom lib.helpers import getlogger, anonymize_password\nimport os\nimport shutil\nimport time\nfrom pathlib import Path\nfrom tempfile import SpooledTemporaryFile\nfrom typing import List\n\n# database, ASGI, etc.\nimport pandas as pd\nimport psycopg2\nimport psycopg2.extras\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, File, UploadFile, Depends, Security, Response\nfrom fastapi.security.api_key import APIKeyHeader, APIKey, Request\nfrom pydantic import EmailStr\n\n# packages from this code repo\nfrom api.config import config\nfrom lib.db.db import _get_db, _close_db, _connect_db, DSN\nfrom models.idf import InternalDataFormat\nfrom models.outdf import Leak, LeakData, Answer, AnswerMeta\nfrom modules.collectors.parser import BaseParser  # XXX FIXME: this should be in lib, no? Or called \"genericparser\"\nfrom modules.collectors.spycloud.collector import SpyCloudCollector\nfrom modules.enrichers.abuse_contact import AbuseContactLookup\nfrom modules.enrichers.external_email import ExternalEmailEnricher\nfrom modules.enrichers.ldap import LDAPEnricher\nfrom modules.enrichers.vip import VIPEnricher\nfrom modules.filters.deduper import Deduper\nfrom modules.filters.filter import Filter\nfrom modules.output.db import PostgresqlOutput\nfrom modules.parsers.spycloud import SpyCloudParser\n\n###############################################################################\n# API key stuff\nAPI_KEYLEN = 32\nAPI_KEY_NAME = \"x-api-key\"\napi_key_header = APIKeyHeader(name = API_KEY_NAME, auto_error = True)\n\nVER = \"0.6\"\n\nlogger = getlogger(__name__)\n\napp = FastAPI(title = \"CredentialLeakDB\", version = VER, )  # root_path='/api/v1')\n\n\n# ##############################################################################\n# DB specific functions\n@app.on_event('startup')\ndef get_db():\n    return _get_db()\n\n\n@app.on_event('shutdown')\ndef close_db():\n    return _close_db()\n\n\n# ##############################################################################\n# security / authentication\ndef fetch_valid_api_keys() -> List[str]:\n    \"\"\"Fetch the list of valid API keys from a DB or a config file.\n\n    :returns: List of strings - the API keys\n    \"\"\"\n    return config['api_keys']\n\n\ndef is_valid_api_key(key: str) -> bool:\n    \"\"\"\n    Validate a given key if it is in the list of allowed API keys *or* if the source IP where the\n    request is coming from in in a list of valid IP addresses.\n\n    :param key: the API key\n    :returns: boolean: YES/NO\n    \"\"\"\n\n    valid_api_keys = fetch_valid_api_keys()\n\n    # allowed_ips = ['127.0.0.1',\n    #                '192.168.1.1',     # my own IP, in this example an RFC1918\n    #               ]\n    # if key in valid_api_keys or (request.client.host in allowed_ips):\n    if key in valid_api_keys:\n        return True\n    return False\n\n\ndef validate_api_key_header(apikeyheader: str = Security(api_key_header)):\n    \"\"\"\n    Validate if a given API key is present in the HTTP apikeyheader.\n\n    :param apikeyheader: the required HTTP Header\n    :returns: the apikey apikeyheader again, if it is valid. Otherwise, raise an HTTPException and return 403.\n    \"\"\"\n    if not apikeyheader:\n        raise HTTPException(status_code = 403,\n                            detail = \"\"\"need API key. Please get in contact with the admins of this\n                            site in order get your API key.\"\"\")\n    if is_valid_api_key(apikeyheader):\n        return apikeyheader\n    else:\n        raise HTTPException(\n            status_code = 403,  # HTTP FORBIDDEN\n            detail = \"\"\"Could not validate the provided credentials. Please get in contact with the admins of this\n            site in order get your API key.\"\"\"\n        )\n\n\n# ##############################################################################\n# File uploading\nasync def store_file(orig_filename: str, _file: SpooledTemporaryFile,\n                     upload_path=os.getenv('UPLOAD_PATH', default = '/tmp')) -> str:\n    \"\"\"\n    Stores a SpooledTemporaryFile to a permanent location and returns the path to it\n\n    :param orig_filename:  the filename according to multipart\n    :param _file: the SpooledTemporary File\n    :param upload_path: where the uploaded file should be stored permanently\n    :returns: full path to the stored file\n    \"\"\"\n    # Unfortunately we need to really shutil.copyfileobj() the file object to disk, even though we already have a\n    # SpooledTemporaryFile object... this is needed for SpooledTemporaryFiles . Sucks. See here:\n    #   https://stackoverflow.com/questions/94153/how-do-i-persist-to-disk-a-temporary-file-using-python\n    #\n    # filepath syntax:  <UPLOAD_PATH>/<original filename>\n    #   example: /tmp/Spycloud.csv\n    path = \"{}/{}\".format(upload_path, orig_filename)  # prefix, orig_filename, sha256, pid, suffix)\n    logger.info(\"storing %s ... to %s\" % (orig_filename, path))\n    _file.seek(0)\n    with open(path, \"w+b\") as outfile:\n        shutil.copyfileobj(_file._file, outfile)\n    return path\n\n\nasync def check_file(filename: str) -> bool:\n    return True  # XXX FIXME Implement\n\n\n# ====================================================\n# API endpoints\n\n@app.get(\"/ping\",\n         name = \"Ping test\",\n         summary = \"Run a ping test, to check if the service is running\",\n         tags = [\"Tests\"])\nasync def ping():\n    \"\"\"A simple ping / liveliness test endpoint. No API Key required.\"\"\"\n    return {\"message\": \"pong\"}\n\n\n@app.get(\"/timeout_test\",\n         name = \"A simple timeout test\",\n         summary = \"Call this and the GET request will sleep for 5 seconds\",\n         tags = [\"Tests\"])\nasync def timeout_test():\n    \"\"\"A simple timeout/ liveliness test endpoint. No API Key required.\"\"\"\n    time.sleep(5)\n    return {\"message\": \"OK\"}\n\n\n@app.get(\"/\", tags = [\"Tests\"])\nasync def root(api_key: APIKey = Depends(validate_api_key_header)):\n    \"\"\"A simple hello world endpoint. This one requires an API key.\"\"\"\n    return {\"message\": \"Hello World\"}  # , \"root_path\": request.scope.get(\"root_path\")}\n\n\n# ##############################################################################\n# General API endpoints\n\n\n@app.get('/user/{email}',\n         tags = [\"General queries\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_user_by_email(email: EmailStr,\n                            response: Response,\n                            api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Get the all credential leaks in the DB of a given user specified by his email address.\n\n    # Parameters\n      * email: string. The email address of the user (case insensitive).\n\n    # Returns\n      * A JSON Answer object with rows being an array of answers, or [] in case there was no data in the DB\n    \"\"\"\n    sql = \"\"\"SELECT * from leak_data where upper(email)=upper(%s)\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (email,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get('/user_and_password/{email}/{password}',\n         tags = [\"General queries\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_user_by_email_and_password(email: EmailStr,\n                                         password: str,\n                                         response: Response,\n                                         api_key: APIKey = Depends(validate_api_key_header)\n                                         ) -> Answer:\n    \"\"\"\n    Get the all credential leaks in the DB of a given user given by the combination email + password.\n    Note that both email and password must match (where email is case insensitive, the password *is case sensitive*).\n\n    # Parameters\n      * email: string. The email address of the user (**case insensitive**, since email is usually case insensitive).\n      * password: string. The (hashed or plaintext) password (**note: this is case sensitive**)\n\n    # Returns\n      * A JSON Answer object with rows being an array of answers, or [] in case there was no data in the DB\n\n    # Example\n    ``foo@example.com`` and ``12345`` -->\n\n    ``{ \"meta\": { ... }, \"data\": [ { \"id\": 14, \"leak_id\": 1, \"email\": \"aaron@example.com\", \"password\": \"12345\", ...,  ],\n        \"errormsg\": null }``\n\n    \"\"\"\n    sql = \"\"\"SELECT * from leak_data where upper(email)=upper(%s) and password=%s\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (email, password))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get('/exists/by_email/{email}',\n         tags = [\"General queries\"],\n         status_code = 200,\n         response_model = Answer)\nasync def check_user_by_email(email: EmailStr,\n                              response: Response,\n                              api_key: APIKey = Depends(validate_api_key_header)\n                              ) -> Answer:\n    \"\"\"\n    Check if a certain email address was present in any leak.\n\n    # Parameters\n    * email: string. The email address of the user (**case insensitive**, since email is usually case insensitive).\n\n    # Returns\n    * A JSON Answer object with rows being an array of answers, or [] in case there was no data in the DB\n\n    # Example\n    ``foo@example.com`` -->\n    ``{ \"meta\": { \"version\": \"0.5\", \"duration\": 0.002, \"count\": 1 }, \"data\": [ { \"count\": 1 } ], \"success\": true,\n        \"errormsg\": null }``\n    \"\"\"\n    sql = \"\"\"SELECT count(*) from leak_data where upper(email)=upper(%s)\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (email,))\n        rows = cur.fetchall()\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get('/exists/by_password/{password}',\n         tags = [\"General queries\"],\n         status_code = 200,\n         response_model = Answer)\nasync def check_user_by_password(password: str,\n                                 response: Response,\n                                 api_key: APIKey = Depends(validate_api_key_header)\n                                 ) -> Answer:\n    \"\"\"\n    Check if a user exists with the given password (either plaintext or hashed) in the DB. If so, return the user.\n\n    # Parameters\n    * password: string. The password to be searched.\n\n    # Returns\n    * A JSON Answer object with rows being an array of answers, or [] in case there was no data in the DB\n\n    # Example\n    ``12345`` -->\n    ``{ \"meta\": { ... }, \"data\": [ { \"id\": 14, \"leak_id\": 1, \"email\": \"aaron@example.com\", \"password\": \"12345\",\n        ...,  ], \"errormsg\": null }``\n    \"\"\"\n    # can do better... use the hashid library?\n\n    sql = \"\"\"SELECT count(*) from leak_data where password=%s or password_plain=%s or password_hashed=%s\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (password, password, password))\n        rows = cur.fetchall()\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get('/exists/by_domain/{domain}',\n         tags = [\"General queries\"],\n         status_code = 200,\n         response_model = Answer)\nasync def check_by_domain(domain: str,\n                          response: Response,\n                          api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Check if a given domain appears in some leak.\n\n    # Parameters\n      * domain : string. The domain to search for (case insensitive).\n\n    # Returns:\n    A JSON Answer object with the count of occurrences in the data: field.\n    \"\"\"\n\n    sql = \"\"\"SELECT count(*) from leak_data where upper(domain)=upper(%s)\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (domain,))\n        rows = cur.fetchall()\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n# ##############################################################################\n# Reference data (reporter, source, etc) starts here\n@app.get('/reporter',\n         tags = [\"Reference data\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_reporters(response: Response,\n                        api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Get the all reporter_name entries (sorted, unique).\n\n    # Parameters\n\n    # Returns\n      * A JSON Answer object with data containing an array of answers, or [] in case there was no data in the DB\n    \"\"\"\n    sql = \"\"\"SELECT distinct(reporter_name) from leak ORDER by reporter_name asc\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql)\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get('/source_name',\n         tags = [\"Reference data\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_sources(response: Response,\n                      api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Get the all names of sources of leaks (sorted, unique) - i.e. \"SpyCloud\", \"HaveIBeenPwned\", etc..\n\n    # Parameters\n\n    # Returns\n      * A JSON Answer object with data containing an array of answers, or [] in case there was no data in the DB\n    \"\"\"\n    sql = \"\"\"SELECT distinct(source_name) from leak ORDER by source_name asc\"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql)\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n# ##############################################################################\n# Leak table starts here\n\n@app.get(\"/leak/all\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_all_leaks(response: Response,\n                        api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"Fetch all leaks.\n\n    # Parameters\n\n    # Returns\n     * A JSON Answer object with all leak (i.e. meta-data of leaks) data from the `leak` table.\n    \"\"\"\n\n    t0 = time.time()\n    sql = \"SELECT * from leak\"\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql)\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak/by_ticket_id/{ticket_id}\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_by_ticket_id(ticket_id: str,\n                                response: Response,\n                                api_key: APIKey = Depends(validate_api_key_header)\n                                ) -> Answer:\n    \"\"\"Fetch a leak by its ticket system id\"\"\"\n    t0 = time.time()\n    sql = \"SELECT * from leak WHERE ticket_id = %s\"\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (ticket_id,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak/by_summary/{summary}\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_by_summary(summary: str,\n                              response: Response,\n                              api_key: APIKey = Depends(validate_api_key_header)\n                              ) -> Answer:\n    \"\"\"Fetch a leak by summary\"\"\"\n    sql = \"SELECT * from leak WHERE summary = %s\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (summary,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak/by_reporter/{reporter}\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_by_reporter(reporter: str,\n                               response: Response,\n                               api_key: APIKey = Depends(validate_api_key_header)\n                               ) -> Answer:\n    \"\"\"Fetch a leak by its reporter. \"\"\"\n    sql = \"SELECT * from leak WHERE reporter_name = %s\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (reporter,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak/by_source/{source_name}\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_by_source(source_name: str,\n                             response: Response,\n                             api_key: APIKey = Depends(validate_api_key_header)\n                             ) -> Answer:\n    \"\"\"Fetch all leaks by their source (i.e. *who* collected the leak data (spycloud, HaveIBeenPwned, etc.).\n\n    # Parameters\n      * source_name: string. The name of the source (case insensitive).\n\n    # Returns\n      * a JSON Answer object with all leaks for that given source_name.\n    \"\"\"\n\n    sql = \"SELECT * from leak WHERE upper(source_name) = upper(%s)\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (source_name,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak/{_id}\", tags = [\"Leak\"],\n         description = 'Get the leak info by its ID.',\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_by_id(_id: int,\n                         response: Response,\n                         api_key: APIKey = Depends(validate_api_key_header)\n                         ) -> Answer:\n    \"\"\"Fetch a leak by its ID\"\"\"\n    t0 = time.time()\n    sql = \"SELECT * from leak WHERE id = %s\"\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (_id,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.post(\"/leak/\",\n          tags = [\"Leak\"],\n          description = \"INSERT a new leak into the DB\",\n          status_code = 201,\n          response_model = Answer)\nasync def new_leak(leak: Leak,\n                   response: Response,\n                   api_key: APIKey = Depends(validate_api_key_header)\n                   ) -> Answer:\n    \"\"\"\n    INSERT a new leak into the leak table in the database.\n\n    # Parameters\n      * leak:  a Leak object. Note that all fields must be set, except for leak.id\n    # Returns\n      * a JSON Answer object with the leak_id in the data: field\n\n    \"\"\"\n    sql = \"\"\"INSERT into leak\n             (summary, ticket_id, reporter_name, source_name, breach_ts, source_publish_ts, ingestion_ts)\n             VALUES (%s, %s, %s, %s, %s, %s, now())\n             RETURNING id\n        \"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (leak.summary, leak.ticket_id, leak.reporter_name, leak.source_name, leak.breach_ts,\n                          leak.source_publish_ts,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 400 in case the INSERT failed.\n            response.status_code = 400\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.put(\"/leak/\",\n         tags = [\"Leak\"],\n         status_code = 200,\n         response_model = Answer)\nasync def update_leak(leak: Leak,\n                      response: Response,\n                      api_key: APIKey = Depends(validate_api_key_header)\n                      ) -> Answer:\n    \"\"\"\n    UPDATE an existing leak.\n\n    # Parameters\n      * leak: a Leak object. Note that all fields must be set in the Leak object.\n    # Returns\n      * a JSON Answer object with the ID of the updated leak.\n    \"\"\"\n    sql = \"\"\"UPDATE leak SET\n                summary = %s, ticket_id = %s, reporter_name = %s, source_name = %s,\n                breach_ts = %s, source_publish_ts = %s\n             WHERE id = %s\n             RETURNING id\n        \"\"\"\n    t0 = time.time()\n    db = get_db()\n    if not leak.id:\n        return Answer(success = False, errormsg = \"id %s not given. Please specify a leak.id you want to UPDATE\",\n                      data = [])\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (leak.summary, leak.ticket_id, leak.reporter_name,\n                          leak.source_name, leak.breach_ts, leak.source_publish_ts, leak.id))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 400 in case the INSERT failed.\n            response.status_code = 400\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n# ############################################################################################################\n# Leak Data starts here\n\n@app.get(\"/leak_data/{leak_data_id}\",\n         tags = [\"Leak Data\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_data_by_id(leak_data_id: int,\n                              response: Response,\n                              api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Fetch all leak data entries of a given id.\n\n    # Parameters\n        * leak_data_id: integer, the DB internal leak_data_id.\n\n    # Returns\n     * A JSON Answer object with the corresponding leak data (i.e. actual usernames, passwords) from the `leak_data`\n       table which are contained within the specified leak (leak_data_id).\n    \"\"\"\n    t0 = time.time()\n    sql = \"SELECT * from leak_data where id=%s\"\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (leak_data_id,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.get(\"/leak_data/by_ticket_id/{ticket_id}\",\n         tags = [\"Leak Data\"],\n         status_code = 200,\n         response_model = Answer)\nasync def get_leak_data_by_ticket_id(ticket_id: str,\n                                     response: Response,\n                                     api_key: APIKey = Depends(validate_api_key_header)\n                                     ) -> Answer:\n    \"\"\"Fetch a leak row (leak_data table) by its ticket system id\n\n    # Parameters\n      * ticket_id: string. The ticket system ID which references the leak_data row\n    # Returns\n      * a JSON Answer object with the leak data row or in data.\n    \"\"\"\n    sql = \"SELECT * from leak_data WHERE ticket_id = %s\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (ticket_id,))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 404 in case no data was found\n            response.status_code = 404\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.post(\"/leak_data/\",\n          tags = [\"Leak Data\"],\n          status_code = 201,\n          response_model = Answer)\nasync def new_leak_data(row: LeakData,\n                        response: Response,\n                        api_key: APIKey = Depends(validate_api_key_header)\n                        ) -> Answer:\n    \"\"\"\n    INSERT a new leak_data row into the leak_data table.\n\n    # Parameters\n      * row: a leakData object. If that data already exists, it will not be inserted again.\n    # Returns\n      * a JSON Answer object containing the ID of the inserted leak_data row.\n    \"\"\"\n    sql = \"\"\"INSERT into leak_data\n             (leak_id, email, password, password_plain, password_hashed, hash_algo, ticket_id,\n             email_verified, password_verified_ok, ip, domain, browser, malware_name, infected_machine, dg)\n             VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n             ON CONFLICT ON CONSTRAINT constr_unique_leak_data_leak_id_email_password_domain DO UPDATE SET email=%s\n             RETURNING id\n        \"\"\"\n    t0 = time.time()\n    db = get_db()\n    logger.debug(row)\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (row.leak_id, row.email, row.password, row.password_plain, row.password_hashed, row.hash_algo,\n                          row.ticket_id, row.email_verified, row.password_verified_ok, row.ip, row.domain, row.browser,\n                          row.malware_name, row.infected_machine, row.dg, row.email))\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 400 in case the INSERT failed.\n            response.status_code = 400\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n@app.put(\"/leak_data/\",\n         tags = [\"Leak Data\"],\n         status_code = 200,\n         response_model = Answer)\nasync def update_leak_data(row: LeakData,\n                           request: Request,\n                           response: Response,\n                           api_key: APIKey = Depends(validate_api_key_header)\n                           ) -> Answer:\n    \"\"\"\n    UPDATE leak_data row in the leak_data table.\n\n    # Parameters\n      * row : a leakData object with all the relevant information. Please note that you **have to** supply all fields,\n        even if you do not plan to update them. In other words: you might have to GET / the leak_data object first.\n    # Returns\n      * a JSON Answer object containing the ID of the inserted leak_data row.\n    \"\"\"\n    sql = \"\"\"UPDATE leak_data SET\n                leak_id = %s,\n                email = %s,\n                password = %s,\n                password_plain = %s,\n                password_hashed = %s,\n                hash_algo = %s,\n                ticket_id = %s,\n                email_verified = %s,\n                password_verified_ok = %s,\n                ip = %s,\n                domain = %s,\n                browser = %s,\n                malware_name = %s,\n                infected_machine = %s,\n                dg = %s\n             WHERE id = %s\n             RETURNING id\n        \"\"\"\n    t0 = time.time()\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        logger.debug(\"HTTP request: '%r'\" % request)\n        logger.debug(\"SQL command: '%s'\" % cur.mogrify(sql, (row.leak_id, row.email, row.password, row.password_plain,\n                                                              row.password_hashed, row.hash_algo,\n                                                              row.ticket_id, row.email_verified,\n                                                              row.password_verified_ok, row.ip, row.domain, row.browser,\n                                                              row.malware_name, row.infected_machine, row.dg, row.id)))\n        cur.execute(sql, (row.leak_id, row.email, row.password, row.password_plain, row.password_hashed, row.hash_algo,\n                          row.ticket_id, row.email_verified, row.password_verified_ok, row.ip, row.domain, row.browser,\n                          row.malware_name, row.infected_machine, row.dg, row.id))\n        db.commit()\n        rows = cur.fetchall()\n        if len(rows) == 0:  # return 400 in case the INSERT failed.\n            response.status_code = 400\n        t1 = time.time()\n        d = round(t1 - t0, 3)\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(rows)), data = rows)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n# ############################################################################################################\n# CSV file importing\n\ndef enrich(item: InternalDataFormat, leak_id: str) -> InternalDataFormat:\n    \"\"\"Initial enricher chain. This SHOULD be configurable and a pipeline via a MQ.\"\"\"\n    # set leak_id\n    item.leak_id = leak_id\n\n    # VIP status\n    if not item.is_vip:\n        vip_enricher = VIPEnricher()\n        item.is_vip = vip_enricher.is_vip(item.email)\n\n    # DG\n    ldap_enricher = LDAPEnricher()\n    if not item.dg:\n        dg = ldap_enricher.email_to_dg(item.email)\n        if not dg:\n            dg = \"Unknown\"\n        item.dg = dg\n\n    # Active account or outdated?\n    if not item.is_active_account:\n        item.is_active_account = ldap_enricher.exists(item.email)\n\n    # External Address or internal?\n    if not item.external_user:\n        ext_email_enricher = ExternalEmailEnricher()\n        item.external_user = ext_email_enricher.is_external_email(item.email)\n\n    # credential Type\n    if not item.credential_type:\n        item.credential_type = [\"EU Login\"]  # XXX FIXME! This is mock-up data!\n\n    # Abuse contact / report to\n    if not item.report_to:\n        abuse_enricher = AbuseContactLookup()\n        item.report_to = abuse_enricher.lookup(item.email)\n\n    # all is good, we went through the pipeline\n    item.notify = True\n    item.needs_human_intervention = False\n    item.error_msg = None\n    return item\n\n\ndef store(idf: InternalDataFormat) -> InternalDataFormat:\n    \"\"\"Store the item in the DB.\n\n    :returns the idf item.\n    \"\"\"\n    # XXX FIXME!! need to implement / refactor existing code.\n    # convert the idf to the DB row\n\n    return idf\n\n\ndef convert_to_output(idf: InternalDataFormat) -> LeakData:\n    \"\"\"Convert the internal data format to the output data format.\n\n    \":returns LeakData\n    \"\"\"\n    output_data_entry = LeakData(**idf.dict())  # here the validation pydantic magic happens\n    return output_data_entry\n\n\n@app.post(\"/import/csv/spycloud/{parent_ticket_id}\",\n          tags = [\"CSV import\"],\n          status_code = 200,\n          response_model = Answer)\nasync def import_csv_spycloud(parent_ticket_id: str,\n                              response: Response,\n                              summary: str = None,\n                              _file: UploadFile = File(...),\n                              api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Import a spycloud CSV file into the DB. Note that you do not need to specify a leak_id parameter here.\n    The API will automatically create a leak object in the DB for you and link it.\n\n    # Parameters\n     * parent_ticket_id: a ticket ID which allows us to link the leak object to the ticket\n     * summary: a summary string for the new leak object (if it's created)\n     * _file: a file which must be uploaded via HTML forms/multipart.\n\n    # Returns\n     * a JSON Answer object where the data: field is the **deduplicated** CSV file (i.e. lines which were already\n       imported as part of that leak (same username, same password, same domain) will not be returned.\n       In other words, data: [] contains the rows from the CSV file which did not yet exist in the DB.\n    \"\"\"\n\n    t0 = time.time()\n\n    if not parent_ticket_id:\n        response.status_code = 400\n        return Answer(success = False,\n                      errormsg = \"Please specify a parent_ticket_id as a GET-style parameter in the URL. \"\n                                 \"This is the parameter, needed to link the sub-issues against\", data = [])\n    if not summary:\n        response.status_code = 400\n        return Answer(success = False,\n                      errormsg = \"Please specify a summary for the Leak object which needs to be created. \", data = [])\n\n    # first check if the leak_id for that summary already exists and if it's already linked to the parent_ticket_id.\n    sql = \"\"\"SELECT id from leak where summary = %s and ticket_id=%s\"\"\"\n    db = get_db()\n    try:\n        with db.cursor(cursor_factory = psycopg2.extras.RealDictCursor) as cur:\n            logger.debug(cur.mogrify(sql, (summary, parent_ticket_id)))\n            cur.execute(sql, (summary, parent_ticket_id))\n            rows = cur.fetchall()\n            nr_results = len(rows)\n            if nr_results >= 1:\n                # take the first one\n                leak_id = rows[0]['id']\n                logger.info(\"Found existing leak object: %s\" % leak_id)\n            else:\n                # nothing found, create one\n                source_name = \"SpyCloud\"\n                leak = Leak(ticket_id = parent_ticket_id, summary = summary, source_name = source_name)\n                answer = await new_leak(leak, response = response, api_key = api_key)\n                logger.info(\"Did not find existing leak object, creating one\")\n                if answer.success:\n                    leak_id = int(answer.data[0]['id'])\n                    logger.info(\"Created with id %s\" % leak_id)\n                else:\n                    logger.error(\"Could not create leak object for spycloud CSV file\")\n                    return Answer(success = False, errormsg = \"could not create leak object\", data = [])\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n    # okay, we found the leak, let's insert the CSV\n    # noinspection PyTypeChecker\n    file_on_disk = await store_file(_file.filename, _file.file)\n    await check_file(file_on_disk)  # XXX FIXME. Additional checks on the dumped file still missing\n\n    collector = SpyCloudCollector()\n    status, df = collector.collect(Path(file_on_disk))\n    if status != \"OK\":\n        return Answer(success = False, errormsg = \"Could not read input CSV file\", data = [])\n\n    p = SpyCloudParser()\n    try:\n        items = p.parse(df)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n    deduper = Deduper()\n    db_output = PostgresqlOutput()\n    filter = Filter()\n\n    data = []\n    for item in items:  # FIXME: this pipeline could be done nicer with functools and reduce\n        # send it through the complete pipeline\n        item = filter.filter(item)\n        email = item.email\n        password = anonymize_password(item.password)\n        if not item:\n            logger.info(\"skipping item (%s, %s), It got filtered out by the filter.\" % (email, password))\n            continue\n        try:\n            item = deduper.dedup(item)\n            if not item:\n                logger.info(\"skipping item (%s, %s), since it already existed in the DB.\" % (email, password))\n                continue  # next item\n        except Exception as ex:\n            logger.error(\"Could not deduplicate item (%s, %s). Skipping this row. Reason: %s\" % (email, password, str(ex)))\n            continue\n        try:\n            item = enrich(item, leak_id = leak_id)\n            item.leak_id = leak_id\n        except Exception as ex:\n            errmsg = \"Could not enrich item (%s, %s). Skipping this row. Reason: %s\" % (email, password, str(ex),)\n            logger.error(errmsg)\n            item.error_msg = errmsg\n            item.needs_human_intervention = True\n            item.notify = False\n        if item.external_user:\n            item.notify = False\n        # after all is finished, convert to output format and return the (deduped) row\n        # convert to output format:\n        out_item = convert_to_output(item)\n        logger.info(out_item)\n\n        # and finally, store it in the DB\n        if not item.needs_human_intervention:\n            try:\n                db_output.process(out_item)\n            except Exception as ex:\n                errmsg = \"Could not store row. Skipping this row. Reason: %s\" % str(ex)\n                logger.error(errmsg)\n                out_item.error_msg = errmsg\n                out_item.needs_human_intervention = True\n                out_item.notify = False\n\n        data.append(out_item)\n    # done! Emit all the output items with the header\n    t1 = time.time()\n    d = round(t1 - t0, 3)\n    return Answer(success = True, errormsg = None,\n                  meta = AnswerMeta(version = VER, duration = d, count = len(data)),\n                  data = data)\n\n\n# noinspection PyTypeChecker\n@app.post(\"/import/csv/by_leak/{leak_id}\",\n          tags = [\"CSV import\"],\n          status_code = 200,\n          response_model = Answer)\nasync def import_csv_with_leak_id(leak_id: int,\n                                  response: Response,\n                                  _file: UploadFile = File(...),\n                                  api_key: APIKey = Depends(validate_api_key_header)\n                                  ) -> Answer:\n    \"\"\"\n    Import a CSV file into the DB. You **need** to specify a ?leak_id=<int> parameter so that the CSV file may be\n    linked to a leak_id. Failure to provide a leak_id will result in the file not being imported into the DB.\n\n    # Parameters\n      * leak_id : int. As a GET parameter. This allows the DB to link the leak data (CSV file) to the leak_id entry in\n        in the leak table.\n      * _file: a file which must be uploaded via HTML forms/multipart.\n\n    # Returns\n      * a JSON Answer object where the data: field is the **deduplicated** CSV file (i.e. lines which were already\n        imported as part of that leak (same username, same password, same domain) will not be returned.\n        In other words, data: [] contains the rows from the CSV file which did not yet exist in the DB.\n    \"\"\"\n\n    t0 = time.time()\n\n    if not leak_id:\n        return Answer(success = False, errormsg = \"Please specify a leak_id GET-style parameter in the URL\", data = [])\n\n    # first check if the leak_id exists\n    sql = \"\"\"SELECT count(*) from leak where id = %s\"\"\"\n    db = get_db()\n    try:\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (leak_id,))\n        rows = cur.fetchone()\n        nr_results = int(rows['count'])\n        if nr_results != 1:\n            response.status_code = 404\n            return Answer(success = False, errormsg = \"Leak ID %s not found\" % leak_id, data = [])\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n    # okay, we found the leak, let's insert the CSV\n    file_on_disk = await store_file(_file.filename, _file.file)\n    await check_file(file_on_disk)  # XXX FIXME. Additional checks on the dumped file still missing\n\n    p = BaseParser()\n    df = pd.DataFrame()\n    try:\n        df = p.parse_file(Path(file_on_disk), leak_id = leak_id)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n    df = p.normalize_data(df, leak_id = leak_id)\n    \"\"\"\n    Now, after normalization, the df is in the format:\n      leak_id, email, password, password_plain, password_hashed, hash_algo, ticket_id, email_verified,\n         password_verified_ok, ip, domain, browser , malware_name, infected_machine, dg\n\n    Example\n    -------\n    [5 rows x 15 columns]\n       leak_id                email  ... infected_machine     dg\n    0        1    aaron@example.com  ...     local_laptop  DIGIT\n    1        1    sarah@example.com  ...    sarahs_laptop  DIGIT\n    2        1  rousben@example.com  ...      WORKSTATION  DIGIT\n    3        1    david@example.com  ...      Macbook Pro  DIGIT\n    4        1    lauri@example.com  ...  Raspberry PI 3+  DIGIT\n    5        1  natasha@example.com  ...  Raspberry PI 3+  DIGIT\n\n    \"\"\"\n\n    inserted_ids = []\n    for r in df.reset_index().to_dict(orient = 'records'):\n        sql = \"\"\"\n        INSERT into leak_data(\n          leak_id, email, password, password_plain, password_hashed, hash_algo, ticket_id, email_verified,\n          password_verified_ok, ip, domain, browser , malware_name, infected_machine, dg\n          )\n        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s )\n        ON CONFLICT ON CONSTRAINT constr_unique_leak_data_leak_id_email_password_domain\n        DO UPDATE SET  count_seen = leak_data.count_seen + 1\n        RETURNING id\n        \"\"\"\n        try:\n            cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n            cur.execute(sql, (r['leak_id'], r['email'], r['password'], r['password_plain'], r['password_hashed'],\n                              r['hash_algo'], r['ticket_id'], r['email_verified'], r['password_verified_ok'], r['ip'],\n                              r['domain'], r['browser'], r['malware_name'], r['infected_machine'], r['dg']))\n            leak_data_id = int(cur.fetchone()['id'])\n            inserted_ids.append(leak_data_id)\n        except Exception as ex:\n            return Answer(success = False, errormsg = str(ex), data = [])\n    t1 = time.time()\n    d = round(t1 - t0, 3)\n\n    # now get the data of all the IDs / dedup\n    try:\n        sql = \"\"\"SELECT * from leak_data where id in %s\"\"\"\n        cur = db.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n        cur.execute(sql, (tuple(inserted_ids),))\n        data = cur.fetchall()\n        return Answer(success = True, errormsg = None,\n                      meta = AnswerMeta(version = VER, duration = d, count = len(inserted_ids)), data = data)\n    except Exception as ex:\n        return Answer(success = False, errormsg = str(ex), data = [])\n\n\n# ############################################################################################################\n# enrichers\n\n@app.get('/enrich/email_to_dg/{email}',\n         tags = [\"Enricher\"],\n         status_code = 200,\n         response_model = Answer)\nasync def enrich_dg_by_email(email: EmailStr,\n                             response: Response,\n                             api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    \"\"\"\n    Enricher function: insert an email, returns the DG.\n\n    :param email:\n    :return: The DG or \"Unknown\"\n    \"\"\"\n    t0 = time.time()\n    le = LDAPEnricher()\n    retval = le.email_to_dg(email)\n    t1 = time.time()\n    d = round(t1 - t0, 3)\n    if not retval:\n        response.status_code = 404\n        return Answer(success = False, errormsg = \"not found\",\n                      meta = AnswerMeta(version = VER, duration = d, count = 0), data = [])\n    else:\n        response.status_code = 200\n        return Answer(success = True, errormsg = None, meta = AnswerMeta(version = VER, duration = d, count = 1),\n                      data = [{\"dg\": retval}])\n\n\n@app.get('/enrich/email_to_userid/{email}',\n         tags = [\"Enricher\"],\n         status_code = 200,\n         response_model = Answer)\nasync def enrich_userid_by_email(email: EmailStr, response: Response,\n                                 api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    t0 = time.time()\n    le = LDAPEnricher()\n    retval = le.email_to_user_id(email)\n    t1 = time.time()\n    d = round(t1 - t0, 3)\n    if not retval:\n        response.status_code = 404\n        return Answer(success = False, errormsg = \"not found\",\n                      meta = AnswerMeta(version = VER, duration = d, count = 0), data = [])\n    else:\n        response.status_code = 200\n        return Answer(success = True, errormsg = None, meta = AnswerMeta(version = VER, duration = d, count = 1),\n                      data = [{\"ecMoniker\": retval}])\n\n\n@app.get('/enrich/email_to_vip/{email}',\n         tags = [\"Enricher\"],\n         status_code = 200,\n         response_model = Answer)\nasync def enrich_vip_via_email(email: EmailStr, response: Response,\n                               api_key: APIKey = Depends(validate_api_key_header)) -> Answer:\n    t0 = time.time()\n    enr = VIPEnricher()\n    retval = enr.is_vip(email)\n    t1 = time.time()\n    d = round(t1 - t0, 3)\n    response.status_code = 200\n    return Answer(success = True, errormsg = None, meta = AnswerMeta(version = VER, duration = d, count = 1),\n                  data = [{\"is_vip\": retval}])\n\n\nif __name__ == \"__main__\":\n    db_conn = _connect_db(DSN)\n    uvicorn.run(app, debug = True, port = os.getenv('PORT', default = 8080))\n"}
{"type": "source_file", "path": "modules/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/filters/deduper.py", "content": "\"\"\"Deduper - this package offers different deduplicaton functions.\"\"\"\n\nimport logging\nfrom typing import Union\n\nimport psycopg2\nimport psycopg2.extras\n\nfrom lib.db.db import _get_db\n\nfrom models.idf import InternalDataFormat\n\n\nclass Deduper:\n    \"\"\"The DB based deduper.\"\"\"\n\n    bloomf_loaded = False\n\n    def __init__(self):\n        pass\n\n    def load_bf(self):\n        # XXX IMPROVEMENT: we might want to use bloomfilters here\n        self.bloomf_loaded = True\n\n    def dedup(self, idf: InternalDataFormat) -> Union[None, InternalDataFormat]:\n        \"\"\"Deduplicate an IDF element based on the existence in the DB.\n        FIXME: this is O(n^2) with n entries in the DB unless indexed properly. Think about indices or a bloom filter\n\n        :param idf - internal data format element\n        :returns: None if it already exists, otherwise the idf\n        :raises Exception on DB problem\n\n        \"\"\"\n        if not self.bloomf_loaded:\n            self.load_bf()\n            self.bloomf_loaded = True\n        # at the moment, we'll use postgresql\n\n        conn = _get_db()\n        sql = \"SELECT count(*) from leak_data WHERE email=%s and password=%s\"\n\n        try:\n            cur = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n            cur.execute(sql, (idf.email, idf.password))\n            rows = cur.fetchall()\n            count = int(rows[0]['count'])\n            if count >= 1:\n                # row already exists, return None\n                return None\n            else:\n                return idf\n        except Exception as ex:\n            logging.error(\"Deduper: could not select data from the DB. Reason: %s\" % (str(ex)))\n            raise ex\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "modules/filters/__init__.py", "content": ""}
{"type": "source_file", "path": "api/models.py", "content": "\"\"\"Pydantic models definitions\n\nAuthor: Aaron Kaplan\nLicense: see LICENSE.\n\"\"\"\n\nimport datetime\nfrom enum import Enum\nfrom typing import Optional, Dict, List  # Union\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass Leak(BaseModel):\n    id: Optional[int]\n    ticket_id: Optional[str]\n    summary: str\n    reporter_name: Optional[str]\n    source_name: Optional[str]\n    breach_ts: Optional[datetime.datetime]\n    source_publish_ts: Optional[datetime.datetime]\n\n\nclass CredentialType(Enum):\n    is_external = \"External\"\n    is_proxy_login = \"Proxy\"\n    is_EU_login = \"EU Login\"\n    is_domain_login = \"Domain\"\n    is_secem_login = \"SECEM\"\n\n\nclass LeakData(BaseModel):\n    id: Optional[int]\n    leak_id: int\n    email: EmailStr\n    password: str\n    password_plain: Optional[str]\n    password_hashed: Optional[str]\n    hash_algo: Optional[str]\n    ticket_id: Optional[str]\n    email_verified: Optional[bool]\n    password_verified_ok: Optional[bool]\n    ip: Optional[str]\n    domain: Optional[str]\n    target_domain: Optional[str]  # new\n    browser: Optional[str]\n    malware_name: Optional[str]\n    infected_machine: Optional[str]\n    dg: Optional[str]\n    is_vip: Optional[bool]\n    credential_type: Optional[List[CredentialType]]\n    report_to: Optional[List[str]]  # the security contact to report this to, in case it's not the the user directly.\n    #\n    # meta stuff and things for error reporting\n    count_seen: Optional[int] = 1\n    original_line: Optional[str]        # the original CSV file in case of errors\n    error_msg: Optional[str]\n    notify: bool\n    needs_human_intervention: bool\n\n\nclass AnswerMeta(BaseModel):\n    version: str\n    duration: float\n    count: int\n\n\nclass Answer(BaseModel):\n    meta: Optional[AnswerMeta]\n    data: List[Dict]  # Union[Dict,List]\n    success: bool\n    errormsg: Optional[str] = \"\"\n\n\n\"\"\" Example:\nMultiple answers:\n{ \"meta\": { \"version\": \"rel-1.0\", \"duration\": 0.78, \"count\": 3 }, \"data\": [ <dict>, <dict>, <dict> ], \"success\": true,\n  \"errormsg\": \"all OK\" }\n\nNo data:\n{ \"meta\": { \"version\": \"rel-1.0\", \"duration\": 0.78 , \"count\": 0 }, \"data\": [], \"success\": true, \"errormsg\": \"all OK\" }\n\nSingle result:\n{ \"meta\": { \"version\": \"rel-1.0\", \"duration\": 0.78 , \"count\": 1 }, \"data\": [ { \"foo\": \"bar\", \"baz\": 77 } ],\n  \"success\": true, \"errormsg\": \"all OK\" }\n\"\"\"\n"}
{"type": "source_file", "path": "models/outdf.py", "content": "import datetime\nfrom enum import Enum\nfrom typing import Optional, Dict, List  # Union\nfrom pydantic import BaseModel, EmailStr\n\n\nclass Leak(BaseModel):\n    id: Optional[int]\n    ticket_id: Optional[str]\n    summary: str\n    reporter_name: Optional[str]\n    source_name: Optional[str]\n    breach_ts: Optional[datetime.datetime]\n    source_publish_ts: Optional[datetime.datetime]\n\n\nclass CredentialType(Enum):\n    is_external = \"External\"\n    is_proxy_login = \"Proxy\"\n    is_EU_login = \"EU Login\"\n    is_domain_login = \"Domain\"\n    is_secem_login = \"SECEM\"\n\n\nclass LeakData(BaseModel):\n    id: Optional[int]\n    leak_id: int\n    email: EmailStr\n    password: str\n    password_plain: Optional[str]\n    password_hashed: Optional[str]\n    hash_algo: Optional[str]\n    ticket_id: Optional[str]\n    email_verified: Optional[bool]\n    password_verified_ok: Optional[bool]\n    ip: Optional[str]\n    domain: Optional[str]\n    target_domain: Optional[str]  # new\n    browser: Optional[str]\n    malware_name: Optional[str]\n    infected_machine: Optional[str]\n    dg: Optional[str]\n    is_vip: Optional[bool]\n    credential_type: Optional[List[CredentialType]]\n    report_to: Optional[List[str]]  # the security contact to report this to, in case it's not the the user directly.\n    #\n    # meta stuff and things for error reporting\n    count_seen: Optional[int] = 1\n    original_line: Optional[str]  # the original CSV file in case of errors\n    error_msg: Optional[str]\n    notify: bool\n    needs_human_intervention: bool\n\n\nclass AnswerMeta(BaseModel):\n    version: str\n    duration: float\n    count: int\n\n\nclass Answer(BaseModel):\n    meta: Optional[AnswerMeta]\n    data: List[Dict]  # Union[Dict,List]\n    success: bool\n    errormsg: Optional[str] = \"\"\n"}
{"type": "source_file", "path": "lib/baseparser/parser.py", "content": "\"\"\"Base Parser definitions. Purely abstract.\"\"\"\n\nimport pandas as pd\n\nfrom models.idf import InternalDataFormat\n\n\nclass BaseParser:\n    def __init__(self):\n        pass\n\n    def parse(self, df: pd.DataFrame) -> InternalDataFormat:\n        pass\n"}
{"type": "source_file", "path": "api/enrichment.py", "content": "\"\"\"\nEnrichment code\n\nAuthor: Aaron Kaplan\nLicense: see LICENSE\n\nThis basically just pulls in the enricher classes.\n\n\"\"\"\nfrom modules.enrichers.ldap_lib import CEDQuery\nfrom modules.enrichers.ldap import LDAPEnricher\nfrom modules.enrichers.vip import VIPEnricher\nfrom modules.enrichers.external_email import ExternalEmailEnricher\n"}
{"type": "source_file", "path": "modules/enrichers/__init__.py", "content": ""}
{"type": "source_file", "path": "lib/baseenricher/__init__.py", "content": ""}
{"type": "source_file", "path": "lib/baseenricher/enricher.py", "content": "\"\"\"Purely abstract base enricher class.\"\"\"\n\nfrom models.idf import InternalDataFormat\n\n\nclass BaseEnricher:\n    def __init__(self):\n        pass\n\n    def enrich(self, idf: InternalDataFormat) -> InternalDataFormat:\n        return idf\n"}
{"type": "source_file", "path": "lib/basecollector/collector.py", "content": "\"\"\"\nBaseCollector\n\nThis implements the abstract collector interface\n\"\"\"\nimport pandas as pd\nimport logging\n\n\nclass BaseCollector:\n    \"\"\"\n    BaseCollector: purely abstract class which defines the interface:\n      collect(input_source)\n\n    Please note that this does *not* yet return a data frame in the internal data format (IDF).\n    So all that a BaseCollector shall return is a tuple (\"OK\"/some_error string and a pandas DF (which may be empty\n    in case of error).\n\n    Example:\n        (\"OK\", pd.DataFrame(... my data...) )           --> all ok, the data is in the DF.\n    or\n        (\"Could not parse CSV file: file does not exist\", pd.DataFrame())   --> error message and empty DF.\n\n    The role of the Collector is to\n     1. fetch the data\n     2. check if the data is complete\n     3. put it into an internal format (in our case a pandas DF) which may be processed by a parser\n     4. return it as pandas DF to the next processing step in the chain\n     5. return errors in case it encountered errors in validation.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def collect(self, input_file: str, **kwargs) -> (str, pd.DataFrame):\n        \"\"\"\n        Collect the data from input_file and return a pandas DF.\n\n        :rtype: tuple return code (\"OK\" in case of success) and pandas DataFrame with the data\n        \"\"\"\n        try:\n            with open(input_file, \"r\") as f:\n                df = pd.read_csv(f, **kwargs)\n                return \"OK\", df\n        except Exception as ex:\n            logging.exception(\"could not parse CSV file. Reason: %r\" % (str(ex),))\n            return str(ex), pd.DataFrame()\n"}
{"type": "source_file", "path": "lib/db/db.py", "content": "\"\"\"Very very lightweight DB abstraction\"\"\"\n\nimport os\nimport psycopg2\nimport psycopg2.extras\n\nfrom fastapi import HTTPException\nimport logging\n\n\n#################################\n# DB functions\n\ndb_conn = None\nDSN = \"host=%s dbname=%s user=%s password=%s\" % (os.getenv('DBHOST', 'localhost'),\n                                                 os.getenv('DBNAME', 'credentialleakdb'),\n                                                 os.getenv('DBUSER', 'credentialleakdb'),\n                                                 os.getenv('DBPASSWORD'))\n\n\ndef _get_db():\n    \"\"\"\n    Open a new database connection if there is none yet for the\n    current application context.\n\n    :returns: the DB handle.\"\"\"\n    global db_conn\n\n    if not db_conn:\n        db_conn = _connect_db(DSN)\n    return db_conn\n\n\n# noinspection PyUnresolvedReferences\ndef _close_db():\n    \"\"\"Closes the database again at the end of the request.\"\"\"\n    global db_conn\n\n    logging.info('shutting down....')\n    if db_conn:\n        db_conn.close()\n        db_conn = None\n    return db_conn\n\n\ndef _connect_db(dsn: str):\n    \"\"\"Connects to the specific database.\n\n    :param dsn: the database connection string.\n    :returns: the DB connection handle\n    \"\"\"\n    try:\n        conn = psycopg2.connect(dsn)\n        conn.set_session(autocommit=True)\n    except Exception as ex:\n        raise HTTPException(status_code=500, detail=\"could not connect to the DB. Reason: %s\" % (str(ex)))\n    logging.info(\"connection to DB established\")\n    return conn\n"}
{"type": "source_file", "path": "modules/filters/filter.py", "content": "from typing import Union\n\nfrom models.idf import InternalDataFormat\n\n\nclass Filter:\n    def __init__(self):\n        pass\n\n    def filter(self, idf: InternalDataFormat) -> Union[None, InternalDataFormat]:\n        \"\"\"Here we could implement all kinds of filters on data elements or whole rows.\n        At the moment, this is a NOP.\n        \"\"\"\n        return idf\n"}
{"type": "source_file", "path": "modules/enrichers/ldap_lib.py", "content": "import sys\nimport os\nimport logging\nfrom ldap3 import Server, Connection, ALL\n\nimport json\n\nfrom typing import List\n\n\nclass CEDQuery:\n    \"\"\" CEDQuery class. Encapsulates the LDAP connect and queries to CED.\n    Author: L. Aaron Kaplan <leon-aaron.kaplan@ext.ec.europa.eu>\n    \"\"\"\n\n    is_connected = False\n    conn = None\n\n    def __init__(self):\n        \"\"\" init() function. Automatically connects to LDAP (calls the connect_ldap() function). \"\"\"\n        if not self.is_connected:\n            self.server = os.getenv('CED_SERVER', default = 'localhost')\n            self.port = int(os.getenv('CED_PORT', default = 389))\n            self.user = os.getenv('CED_USER')\n            self.password = os.getenv('CED_PASSWORD')\n            self.base_dn = os.getenv('CED_BASEDN')\n            try:\n                self.connect_ldap(self.server, self.port, self.user, self.password)\n            except Exception as ex:\n                logging.error(\"could ot connect to LDAP. Reason: %s\" % str(ex))\n                self.is_connected = False\n\n    def connect_ldap(self, server=\"ldap.example.com\", port=389, user=None, password=None):\n        \"\"\" Connects to the CED LDAP server. Returns None on failure. \"\"\"\n        try:\n            ldap_server = Server(server, port = port, get_info = ALL)\n            self.conn = Connection(ldap_server, user = user, password = password)\n            self.is_connected = self.conn.bind()\n            print(\"Connection = %s\" % self.conn)\n            logging.info(\"connect_ldap(): self.conn = %s\" % (self.conn,))\n            logging.info(\"connect_ldap(): conn.bind() = %s\" % (self.conn.bind(),))\n        except Exception as ex:\n            logging.error(\"error connecting to CED. Reason: %s\" % (str(ex)))\n            self.is_connected = False\n            return None\n\n    def search_by_mail(self, email: str) -> List[dict]:\n        attributes = ['cn', 'dg', 'uid', 'ecMoniker', 'employeeType', 'recordStatus', 'sn', 'givenName', 'mail']\n        if not self.is_connected:\n            logging.error(\"Could not search via email. Not connected to LDAP.\")\n            raise Exception(\"Could not search via email. Not connected to LDAP.\")\n        try:\n            self.conn.search(self.base_dn, \"(mail=%s)\" % (email,), attributes = attributes)\n        except Exception as ex:\n            logging.error(\"could not search LDAP. error: %s\" % str(ex))\n            raise ex\n        logging.info(\"search_by_mail(): %s\" % (self.conn.entries,))\n        results = []\n        for entry in self.conn.entries:\n            results.append(json.loads(entry.entry_to_json()))\n        return results  # yeah, a list comprehension would be more pythonic\n\n\nif __name__ == \"__main__\":\n    ced = CEDQuery()\n    email = sys.argv[1]\n    print(ced.search_by_mail(email))\n"}
{"type": "source_file", "path": "modules/parsers/spycloud.py", "content": "\"\"\"\nSpyCloud Parser\n\nAccepts a pandas DF, parses and validates it against the *IN*put format and returns it in the *internal* IDF format\n\n\"\"\"\n\nimport logging\n# from typing import List\n\nfrom pydantic import parse_obj_as, ValidationError\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\nfrom lib.baseparser.parser import BaseParser\nfrom models.indf import SpyCloudInputEntry\nfrom models.idf import InternalDataFormat\n\n\nclass SpyCloudParser(BaseParser):\n    def __init__(self):\n        \"\"\"init\"\"\"\n        super().__init__()\n\n    def parse(self, df: pd.DataFrame) -> List[InternalDataFormat]:\n        \"\"\"parse a pandas DF and return the data in the Internal Data Format.\"\"\"\n\n        # First, map empty columns to None so that it fits nicely into the IDF\n        df.replace({\"-\": None}, inplace = True)\n        df.replace({\"nan\": None}, inplace = True)\n        df.replace({np.nan: None}, inplace = True)\n        df.replace({'breach_date': {'Unknown': None}}, inplace = True)\n\n        # some initial checks on the df\n\n        # validate via pydantic\n        items = []\n        for row in df.reset_index().to_dict(orient = 'records'):\n            logging.debug(\"row=%s\" % row)\n            idf_dict = dict(email = None, password = None, notify = False, domain = None, error_msg = \"incomplete data\",\n                            needs_human_intervention = True)\n            idf_dict['original_line'] = str(row)\n            try:\n                input_data_item = parse_obj_as(SpyCloudInputEntry, row)  # here the validation magic happens\n                idf_dict = input_data_item.dict()  # conversion magic happens between input format and internal df\n                idf_dict['domain'] = input_data_item.email_domain        # map specific fields\n            except Exception as ex:\n                idf_dict['needs_human_intervention'] = True\n                idf_dict['notify'] = False\n                idf_dict['error_msg'] = str(ex)\n                logging.error(\"could not parse CSV row. Original line: %r.\\nReason: %s\" % (repr(row), str(ex)))\n                logging.debug(\"idf_dict = %s\" % idf_dict)\n            else:\n                logging.error(\"everything successfully converted\")\n                idf_dict['needs_human_intervention'] = False\n                idf_dict['notify'] = True\n                idf_dict['error_msg'] = None\n            finally:\n                try:\n                    idf = InternalDataFormat(**idf_dict)  # another step of validation happens here\n                    logging.debug(\"idf = %r\" % idf)\n                except Exception as ex2:\n                    logging.error(\"Exception in finally. idf_dict = %r\" % idf_dict)\n                    raise ex2\n                else:\n                    items.append(idf)\n\n        return items\n"}
