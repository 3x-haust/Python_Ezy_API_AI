{"repo_info": {"repo_name": "x-attention", "repo_owner": "mit-han-lab", "repo_url": "https://github.com/mit-han-lab/x-attention"}}
{"type": "source_file", "path": "demo/demo.py", "content": "import time\nimport torch\nfrom transformers import StaticCache\nfrom xattn.src.load_llama import load_model,FastPrefillConfig\nfrom eval.efficiency.generate_prompt import generate_prompt\nimport argparse\nfrom tqdm import tqdm\nif __name__ == \"__main__\":\n    # load model and tokenizer\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--len\", type=int, default=450560)\n    parser.add_argument(\"--chunk_size\", type=int, default=32768)\n    parser.add_argument(\"--stride\", type=int, default=16)\n    parser.add_argument(\"--threshold\", type=float, default=0.1)\n    parser.add_argument(\"--metric\", type=str, default='xattn')\n    args = parser.parse_args()\n    config = FastPrefillConfig(metric = args.metric,stride = args.stride, threshold = args.threshold)\n    \n    model, tokenizer = load_model(name_or_path=\"/NFS/raid0/model/llama3-1048k\", fastprefillconfig=config)\n    input_ids = generate_prompt(tokenizer,args.len)\n    # -------------------\n    # 1. Prefill\n    # -------------------\n    past_key_values = StaticCache(config=model.config, batch_size=1, max_cache_len=450710, device=model.device, dtype=model.dtype)\n    start_prefill = time.time()\n    with torch.no_grad():\n        for i in tqdm(range(0, input_ids.size(1), args.chunk_size), desc=\"Prefilling\", unit=\"chunk\"):\n            chunk = input_ids[:, i: i + args.chunk_size]\n            output = model(\n                input_ids=chunk,\n                past_key_values=past_key_values,\n                use_cache=True,\n                num_logits_to_keep=1,\n            )\n            past_key_values = output.past_key_values\n    torch.cuda.synchronize()\n    end_prefill = time.time()\n    prefill_time = end_prefill - start_prefill\n    print(f\"Prefill Time: {prefill_time:.4f} s\")\n\n     # -------------------\n    # 2. Decode\n    # -------------------\n    start_decode = time.time()\n    eos_token_id = tokenizer.eos_token_id  # 获取eos token ID\n    pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n    generated_content = [pred_token_idx.item()]\n    torch.cuda.empty_cache()\n    with torch.no_grad():\n        for _ in tqdm(range(50), desc=\"Decoding\", unit=\"token\"):\n            outputs = model(\n                input_ids=pred_token_idx,\n                past_key_values=past_key_values,\n                use_cache=True,\n                num_logits_to_keep=1,\n            )\n            past_key_values = outputs.past_key_values\n            pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_token = pred_token_idx.item()\n            \n            if generated_token == eos_token_id:\n                break  # 如果生成EOS token，提前结束\n            \n            generated_content += [generated_token]\n    torch.cuda.synchronize()\n    end_decode = time.time()\n    decode_time = end_decode - start_decode\n    print(f\"Prefill Time: {prefill_time:.4f} s\")\n    print(f\"Decode Time: {decode_time:.4f} s\")\n    output_text = tokenizer.decode(generated_content, skip_special_tokens=True)\n    print(\"Generated Text:\", output_text)\n"}
{"type": "source_file", "path": "demo/example.py", "content": "import torch\nfrom xattn.src.Xattention import Xattention_prefill\n\nbsz = 1\nheads = 32\nseq_len = 1024\ndim = 128\nq = torch.randn((bsz,heads,seq_len,dim),dtype=torch.bfloat16).to(\"cuda\")\nk = torch.randn((bsz,heads,seq_len,dim),dtype=torch.bfloat16).to(\"cuda\")\nv = torch.randn((bsz,heads,seq_len,dim),dtype=torch.bfloat16).to(\"cuda\")\n\nattention_output = Xattention_prefill(query_states=q,key_states=k,value_states=v,stride=16,block_size=128,use_triton=True,chunk_size=2048)\nprint(attention_output)"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/qwen_vl_api.py", "content": "from __future__ import annotations\n\nimport os\nimport warnings\n\nfrom vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.vlm.qwen2_vl.prompt import Qwen2VLPromptMixin\n\n\ndef ensure_image_url(image: str) -> str:\n    prefixes = ['http://', 'https://', 'file://', 'data:image;']\n    if any(image.startswith(prefix) for prefix in prefixes):\n        return image\n    if os.path.exists(image):\n        return 'file://' + image\n    raise ValueError(f'Invalid image: {image}')\n\n\nclass Qwen2VLAPI(Qwen2VLPromptMixin, BaseAPI):\n    is_api: bool = True\n\n    def __init__(\n        self,\n        model: str = 'qwen-vl-max-0809',\n        key: str | None = None,\n        min_pixels: int | None = None,\n        max_pixels: int | None = None,\n        max_length=1024,\n        top_p=0.001,\n        top_k=1,\n        temperature=0.01,\n        repetition_penalty=1.0,\n        presence_penalty=0.0,\n        seed=3407,\n        use_custom_prompt: bool = True,\n        **kwargs,\n    ):\n        import dashscope\n\n        self.model = model\n        self.min_pixels = min_pixels\n        self.max_pixels = max_pixels\n        self.generate_kwargs = dict(\n            max_length=max_length,\n            top_p=top_p,\n            top_k=top_k,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            presence_penalty=presence_penalty,\n            seed=seed,\n        )\n\n        key = os.environ.get('DASHSCOPE_API_KEY', None) if key is None else key\n        assert key is not None, (\n            'Please set the API Key (obtain it here: '\n            'https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)'\n        )\n        dashscope.api_key = key\n        super().__init__(use_custom_prompt=use_custom_prompt, **kwargs)\n\n    def _prepare_content(self, inputs: list[dict[str, str]], dataset: str | None = None) -> list[dict[str, str]]:\n        \"\"\"\n        inputs list[dict[str, str]], each dict has keys: ['type', 'value']\n        \"\"\"\n        content = []\n        for s in inputs:\n            if s['type'] == 'image':\n                item = {'type': 'image', 'image': ensure_image_url(s['value'])}\n                if dataset == 'OCRBench':\n                    item['min_pixels'] = 10 * 10 * 28 * 28\n                    warnings.warn(f\"OCRBench dataset uses custom min_pixels={item['min_pixels']}\")\n                    if self.max_pixels is not None:\n                        item['max_pixels'] = self.max_pixels\n                else:\n                    if self.min_pixels is not None:\n                        item['min_pixels'] = self.min_pixels\n                    if self.max_pixels is not None:\n                        item['max_pixels'] = self.max_pixels\n            elif s['type'] == 'text':\n                item = {'type': 'text', 'text': s['value']}\n            else:\n                raise ValueError(f\"Invalid message type: {s['type']}, {s}\")\n            content.append(item)\n        return content\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        import dashscope\n\n        messages = []\n        if self.system_prompt is not None:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n        messages.append(\n            {'role': 'user', 'content': self._prepare_content(inputs, dataset=kwargs.get('dataset', None))}\n        )\n        if self.verbose:\n            print(f'\\033[31m{messages}\\033[0m')\n\n        # generate\n        generation_kwargs = self.generate_kwargs.copy()\n        kwargs.pop('dataset', None)\n        generation_kwargs.update(kwargs)\n        try:\n            response = dashscope.MultiModalConversation.call(\n                model=self.model,\n                messages=messages,\n                **generation_kwargs,\n            )\n            if self.verbose:\n                print(response)\n            answer = response.output.choices[0]['message']['content'][0]['text']\n            return 0, answer, 'Succeeded! '\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(f'The input messages are {inputs}.')\n            return -1, '', ''\n\n\nclass QwenVLWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'qwen-vl-plus',\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = True,\n                 temperature: float = 0.0,\n                 system_prompt: str = None,\n                 max_tokens: int = 2048,\n                 proxy: str = None,\n                 **kwargs):\n\n        assert model in ['qwen-vl-plus', 'qwen-vl-max']\n        self.model = model\n        import dashscope\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        if key is None:\n            key = os.environ.get('DASHSCOPE_API_KEY', None)\n        assert key is not None, (\n            'Please set the API Key (obtain it here: '\n            'https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)'\n        )\n        dashscope.api_key = key\n        if proxy is not None:\n            proxy_set(proxy)\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(text=msg['value']))\n                elif msg['type'] == 'image':\n                    content_list.append(dict(image='file://' + msg['value']))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(role='system', content=self.system_prompt))\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(role=item['role'], content=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(role='user', content=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        from dashscope import MultiModalConversation\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n\n        if 'type' in inputs[0]:\n            pure_text = np.all([x['type'] == 'text' for x in inputs])\n        else:\n            pure_text = True\n            for inp in inputs:\n                if not np.all([x['type'] == 'text' for x in inp['content']]):\n                    pure_text = False\n                    break\n\n        assert not pure_text\n        messages = self.prepare_inputs(inputs)\n        gen_config = dict(max_output_tokens=self.max_tokens, temperature=self.temperature)\n        gen_config.update(kwargs)\n        try:\n            response = MultiModalConversation.call(model=self.model, messages=messages)\n            if self.verbose:\n                print(response)\n            answer = response.output.choices[0]['message']['content'][0]['text']\n            return 0, answer, 'Succeeded! '\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(f'The input messages are {inputs}.')\n\n            return -1, '', ''\n\n\nclass QwenVLAPI(QwenVLWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(QwenVLAPI, self).generate(message)\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/common_words_extraction.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nCreate a dataset jsonl file for common words extraction.\n\npython common_words_extraction.py   \\\n    --save_dir=./ \\\n    --save_name=vt \\\n    --tokenizer_path=tokenizer.model \\\n    --tokenizer_type nemo \\\n    --max_seq_length 4096 \\\n    --tokens_to_generate 30 \\\n    --num_samples 10 \\\n    --random_seed 42  \\\n    -freq_cw 30 --freq_ucw 3 --num_cw 10 \\\n    --template \"[INST] Below is a numbered list of words. In these words, some appear more often than others. Memorize the ones that appear most often.\\n{context}\\nQuestion: What are the 10 most common words in the above list? [/INST] Answer: The top 10 words that appear most often in the list are:\"\n\"\"\"\n\nimport os\nimport argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport wonderwords\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\nimport sys\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\")) \nfrom tokenizer import select_tokenizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--save_name\", type=str, required=True, help='name of the save dataset jsonl file')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--tokens_to_generate\", type=int, required=True, help='expected generated token amount.')\nparser.add_argument(\"--num_samples\", type=int, required=True, help='number of samples to generate')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--template\", type=str, default='', help='prompt template')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\n\nparser.add_argument(\"--freq_cw\", type=int, default=30)\nparser.add_argument(\"--freq_ucw\", type=int, default=3)\nparser.add_argument(\"--num_cw\", type=int, default=10)\n\nargs = parser.parse_args()\nrandom.seed(args.random_seed)\n\n# Load Tokenizer\nTOKENIZER = select_tokenizer(args.tokenizer_type, args.tokenizer_path)\n\nnouns = wonderwords.random_word._get_words_from_text_file(\"nounlist.txt\")\nadjs = wonderwords.random_word._get_words_from_text_file(\"adjectivelist.txt\")\nverbs = wonderwords.random_word._get_words_from_text_file(\"verblist.txt\")\nwords = nouns + adjs + verbs\nwords = sorted(list(set(words)))\nrandom.Random(args.random_seed).shuffle(words)\n\ndef get_example(num_words, common_repeats=30, uncommon_repeats=3, common_nums=10):\n    word_list_full = random.sample(words, num_words)\n    common, uncommon = word_list_full[:common_nums], word_list_full[common_nums:]  \n    word_list = common * int(common_repeats) + uncommon * int(uncommon_repeats) \n    random.Random(args.random_seed).shuffle(word_list)\n\n    # Formatting the word list as \"1. word1 2. word2 3. word3 ...\"\n    context = ' '.join([f\"{i + 1}. {word}\" for i, word in enumerate(word_list)])\n\n    return context, common\n\ndef generate_input_output(num_words):\n    if args.max_seq_length < 4096:\n        context_example, answer_example = get_example(20, 3, 1, args.num_cw)\n        context, answer = get_example(num_words, 6, 1, args.num_cw)\n    else:\n        context_example, answer_example = get_example(40, 10, 3, args.num_cw)\n        context, answer = get_example(num_words, args.freq_cw, args.freq_ucw, args.num_cw)\n\n    template = args.template\n\n    input_example = template.format(\n        context=context_example,\n        query='',\n    ) + ' '.join([f\"{i + 1}. {word}\" for i, word in enumerate(answer_example)])\n\n    input_text = template.format(\n        context=context,\n        query='',\n    )\n\n    return input_example + \"\\n\" + input_text, answer\n\ndef sys_word_pair_random(num_samples: int, max_seq_length: int, save_dir: str, incremental: int = 10):\n    write_jsons = []\n    tokens_to_generate = args.tokens_to_generate\n    \n    # Find the perfect num_words\n    num_words = incremental\n    \n    total_tokens = 0  \n    while total_tokens + tokens_to_generate < max_seq_length:\n        \n        input_text, answer = generate_input_output(num_words)\n        # Calculate the number of tokens in the example\n        total_tokens = len(TOKENIZER.text_to_tokens(input_text + ' ' + ' '.join([f\"{i + 1}. {word}\" for i, word in enumerate(answer)])))\n        print(f'Max length {max_seq_length} | Current length {total_tokens + tokens_to_generate} | Words: {num_words}')\n        if total_tokens + tokens_to_generate > max_seq_length:\n            num_words -= incremental\n            break\n            \n        num_words += incremental\n        if num_words > len(words):\n            num_words = len(words)\n            break\n\n    print('num_words:', num_words)\n    \n    # Generate samples\n    for index in tqdm(range(num_samples)):\n        used_words = num_words\n        while(True):\n            try:\n                input_text, answer = generate_input_output(used_words)\n                length = len(TOKENIZER.text_to_tokens(input_text)) + tokens_to_generate\n                assert length <= max_seq_length, f\"{length} exceeds max_seq_length.\"\n                break\n            except:\n                if used_words > incremental:\n                    used_words -= incremental\n\n        if args.remove_newline_tab:\n            input_text = ' '.join(input_text.replace('\\n', ' ').replace('\\t', ' ').strip().split())\n        \n        formatted_output = {\n            'index': index,\n            \"input\": input_text,\n            \"outputs\": answer,\n            \"length\": length,\n        }\n        write_jsons.append(formatted_output)\n\n    return write_jsons\n\n\ndef main():\n    save_file = args.save_dir / f'{args.save_name}' / f'{args.subset}.jsonl'\n    save_file.parent.mkdir(parents=True, exist_ok=True)\n\n    write_jsons = sys_word_pair_random(num_samples=args.num_samples, max_seq_length=args.max_seq_length, save_dir=args.save_dir)\n\n    write_manifest(save_file, write_jsons)\n\nif __name__==\"__main__\":\n    main()\n"}
{"type": "source_file", "path": "eval/RULER/scripts/pred/call_api.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nPrepare prediction jsonl with field `pred` .\ndataset jsonl:\n{\n    \"index\" int,\n    \"input\": str,\n    \"outputs\": [str],\n}\n\nprediction jsonl: \n{\n    \"index\" int,\n    \"input\": str,\n    \"outputs\": [str],\n    \"pred\": str,\n}\n\"\"\"\n\nimport argparse\nimport json\nimport yaml\nimport os\nimport sys\nimport threading\nimport importlib\nimport math\nimport time\nimport torch\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport traceback\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest\nfrom xattn.src.load_llama import FastPrefillConfig\n\nSERVER_TYPES = (\n    'trtllm',\n    'vllm',\n    'sglang',\n    'openai',\n    'gemini',\n    'hf',\n    'mamba',\n)\n\n\nclass ServerAction(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        namespace.server_type = values\n\n\nparser = argparse.ArgumentParser()\n# Data\nparser.add_argument(\"--data_dir\", type=Path, required=True, help='path to load the dataset jsonl files')\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='path to save the prediction jsonl files')\nparser.add_argument(\"--benchmark\", type=str, default='synthetic', help='Options: [synthetic]')\nparser.add_argument(\"--task\", type=str, required=True, help='Options: tasks in benchmark')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--chunk_idx\", type=int, default=0, help='index of current split chunk')\nparser.add_argument(\"--chunk_amount\", type=int, default=1, help='size of split chunk')\n\n# Server\nparser.add_argument(\"--server_type\", default='nemo', action=ServerAction, choices=SERVER_TYPES)\nparser.add_argument(\"--server_host\", type=str, default='127.0.0.1')\nparser.add_argument(\"--server_port\", type=str, default='5000')\nparser.add_argument(\"--ssh_server\", type=str)\nparser.add_argument(\"--ssh_key_path\", type=str)\nparser.add_argument(\"--model_name_or_path\", type=str, default='gpt-3.5-turbo', \n                    help='supported models from OpenAI or HF (provide a key or a local path to the checkpoint)')\n\n# Inference\nparser.add_argument(\"--temperature\", type=float, default=1.0)\nparser.add_argument(\"--top_k\", type=int, default=32)\nparser.add_argument(\"--top_p\", type=float, default=1.0)\nparser.add_argument(\"--random_seed\", type=int, default=0)\nparser.add_argument(\"--stop_words\", type=str, default='')\nparser.add_argument(\"--sliding_window_size\", type=int)\nparser.add_argument(\"--threads\", type=int, default=4)\nparser.add_argument(\"--batch_size\", type=int, default=1)\n\n# Xattention\nparser.add_argument(\"--threshold\", type=float, default=None, help=\"Threshold for grouping.\")\nparser.add_argument(\"--print_detail\", action='store_true', default=False, help=\"Print detailed information. Default is False.\")\nparser.add_argument(\"--stride\", type=int, default=16, help=\"Small block size\") \nparser.add_argument(\"--metric\", type=str, default=\"xattn\", help=\"\")\n\n\n\nargs = parser.parse_args()\nargs.stop_words = list(filter(None, args.stop_words.split(',')))\nif args.server_type == 'hf' or args.server_type == 'gemini':\n    args.threads = 1\n\nfastprefillconfig = FastPrefillConfig(\n    threshold=args.threshold,\n    print_detail=args.print_detail,\n    stride = args.stride,\n    metric=args.metric,\n)\n\ndef get_llm(tokens_to_generate):\n    if args.server_type == 'trtllm':\n        from client_wrappers import TRTLLMClient\n        llm = TRTLLMClient(\n            server_host=args.server_host,\n            server_port=args.server_port,\n            ssh_server=args.ssh_server,\n            ssh_key_path=args.ssh_key_path,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            random_seed=args.random_seed,\n            stop=args.stop_words,\n            tokens_to_generate=tokens_to_generate,\n            max_attention_window_size=args.sliding_window_size,\n        )\n\n    elif args.server_type == 'vllm':\n        from client_wrappers import VLLMClient\n        llm = VLLMClient(\n            server_host=args.server_host,\n            server_port=args.server_port,\n            ssh_server=args.ssh_server,\n            ssh_key_path=args.ssh_key_path,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            random_seed=args.random_seed,\n            stop=args.stop_words,\n            tokens_to_generate=tokens_to_generate,\n        )\n\n    elif args.server_type == 'sglang':\n        from client_wrappers import SGLClient\n        llm = SGLClient(\n            server_host=args.server_host,\n            server_port=args.server_port,\n            ssh_server=args.ssh_server,\n            ssh_key_path=args.ssh_key_path,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            random_seed=args.random_seed,\n            stop=args.stop_words,\n            tokens_to_generate=tokens_to_generate,\n        )\n        \n    elif args.server_type == 'openai':\n        from client_wrappers import OpenAIClient\n        llm = OpenAIClient(\n            model_name=args.model_name_or_path,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            random_seed=args.random_seed,\n            stop=args.stop_words,\n            tokens_to_generate=tokens_to_generate,\n        )\n\n    elif args.server_type == 'gemini':\n        from client_wrappers import GeminiClient\n        llm = GeminiClient(\n            model_name=args.model_name_or_path,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            random_seed=args.random_seed,\n            stop=args.stop_words,\n            tokens_to_generate=tokens_to_generate,\n        )\n        \n    elif args.server_type == 'hf':\n        from model_wrappers import HuggingFaceModel\n        llm = HuggingFaceModel(\n            name_or_path=args.model_name_or_path,\n            fastprefillconfig=fastprefillconfig,\n            do_sample=args.temperature > 0,\n            repetition_penalty=1,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            stop=args.stop_words,\n            max_new_tokens=tokens_to_generate,\n        )\n    \n    elif args.server_type == 'mamba':\n        from model_wrappers import MambaModel\n        # mamba uses its own generation function, do not pass in do_sample\n        # https://github.com/state-spaces/mamba/blob/009bec5ee37f586844a3fc89c040a9c1a9d8badf/mamba_ssm/utils/generation.py#L121\n        llm = MambaModel(\n            name_or_path=args.model_name_or_path,\n            repetition_penalty=1,\n            temperature=args.temperature,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            stop=args.stop_words,\n            max_new_tokens=tokens_to_generate,\n        )\n        \n    else:\n        raise RuntimeError(f'Unsupported server type {args.server_type}')\n\n    return llm\n\n\ndef main():\n    start_time = time.time()\n    \n    curr_folder = os.path.dirname(os.path.abspath(__file__))\n    \n    try:\n        sys.path.append(os.path.dirname(curr_folder))\n        module = importlib.import_module(f\"data.{args.benchmark}.constants\")\n    except ImportError:\n        print(f\"Module data.{args.benchmark}.constants not found.\")\n\n    tasks_base = module.TASKS\n    with open(os.path.join(curr_folder, f\"../{args.benchmark}.yaml\"), \"r\") as f:\n        tasks_customized = yaml.safe_load(f)\n\n    if args.task not in tasks_customized:\n        raise ValueError(f'{args.task} is not found in config_tasks.yaml')\n        \n    config = tasks_customized.get(args.task)\n    config.update(tasks_base[config['task']])\n\n    task_file = args.data_dir / args.task / f'{args.subset}.jsonl'\n    \n    if args.chunk_amount > 1:\n        pred_file = args.save_dir / f'{args.task}-{args.chunk_idx}.jsonl'\n    else:\n        pred_file = args.save_dir / f'{args.task}.jsonl'\n        \n    print(f'Predict {args.task} \\nfrom {task_file}\\nto {pred_file}')\n    pred_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load data\n    if os.path.exists(pred_file):\n        pred_index = [sample['index'] for sample in read_manifest(pred_file)]\n        data = [sample for sample in read_manifest(task_file) if sample['index'] not in pred_index]\n    else:\n        data = read_manifest(task_file)\n\n    # Load api\n    llm = get_llm(config['tokens_to_generate'])\n\n    def get_output(idx_list, index_list, input_list, outputs_list, others_list, truncation_list, length_list):\n        nonlocal llm\n\n        while True:\n            try:\n                with torch.no_grad():\n                    pred_list = llm.process_batch(prompts=input_list)\n                    break\n            except Exception as e:\n                traceback.print_exc()\n\n        zipped_iter = zip(pred_list, idx_list, index_list, input_list,\n                          outputs_list, others_list, truncation_list, length_list)\n\n        for pred, idx, index, input, outputs, others, truncation, length in zipped_iter:\n            if isinstance(pred['text'], str):\n                pred_text = pred['text']\n            elif len(pred['text']) > 0:\n                pred_text = pred['text'][0]\n            else:\n                pred_text = ''\n\n            outputs_parallel[idx] = {\n                'index': index,\n                'pred': pred_text,\n                'input': input,\n                'outputs': outputs,\n                'others': others,\n                'truncation': truncation,\n                'length': length,\n            }\n\n    threads = []\n    outputs_parallel = [{} for _ in range(len(data))]\n\n    batched_data = []\n    batch = []\n    for idx, data_point in enumerate(data):\n        data_point['idx'] = idx\n\n        if len(batch) >= args.batch_size:\n            batched_data.append(batch)\n            batch = []\n\n        batch.append(data_point)\n\n    if len(batch):\n        batched_data.append(batch)\n\n    # setting buffering=1 to force to dump the output after every line, so that we can see intermediate generations\n    with open(pred_file, 'at', encoding=\"utf-8\", buffering=1) as fout:\n        # the data is processed sequentially, so we can store the start and end of current processing window\n        start_idx = 0  # window: [start_idx, end_idx]\n\n        for batch_idx, batch in tqdm(enumerate(batched_data), total=len(batched_data)):\n            idx_list = [data_point['idx'] for data_point in batch]\n            end_idx = idx_list[-1]  # the data in a batch is ordered\n\n            thread = threading.Thread(\n                target=get_output,\n                kwargs=dict(\n                    idx_list=idx_list,\n                    index_list=[data_point['index'] for data_point in batch],\n                    input_list=[data_point['input'] for data_point in batch],\n                    outputs_list=[data_point['outputs'] for data_point in batch],\n                    others_list=[data_point.get('others', {}) for data_point in batch],\n                    truncation_list=[data_point.get('truncation', -1) for data_point in batch],\n                    length_list=[data_point.get('length', -1) for data_point in batch],\n                ),\n            )\n            thread.start()\n            threads.append(thread)\n\n            is_last_batch = (batch_idx == len(batched_data) - 1)\n\n            if (len(threads) == args.threads) or is_last_batch:\n                for thread in threads:\n                    thread.join()\n                threads = []\n\n                # dump the results in current processing window on disk\n                for idx in range(start_idx, end_idx + 1):\n                    if len(outputs_parallel[idx]) > 0:\n                        fout.write(json.dumps(outputs_parallel[idx]) + '\\n')\n\n                start_idx = end_idx + 1\n\n    print(f\"Used time: {round((time.time() - start_time) / 60, 1)} minutes\")\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "eval/LongBench/eval.py", "content": "import os\nimport json\nimport argparse\nimport numpy as np\n\nfrom metrics import (\n    qa_f1_score,\n    rouge_zh_score,\n    qa_f1_zh_score,\n    rouge_score,\n    classification_score,\n    retrieval_score,\n    retrieval_zh_score,\n    count_score,\n    code_sim_score,\n)\n\ndataset2metric = {\n    \"narrativeqa\": qa_f1_score,\n    \"qasper\": qa_f1_score,\n    \"multifieldqa_en\": qa_f1_score,\n    \"multifieldqa_zh\": qa_f1_zh_score,\n    \"hotpotqa\": qa_f1_score,\n    \"2wikimqa\": qa_f1_score,\n    \"musique\": qa_f1_score,\n    \"dureader\": rouge_zh_score,\n    \"gov_report\": rouge_score,\n    \"qmsum\": rouge_score,\n    \"multi_news\": rouge_score,\n    \"vcsum\": rouge_zh_score,\n    \"trec\": classification_score,\n    \"triviaqa\": qa_f1_score,\n    \"samsum\": rouge_score,\n    \"lsht\": classification_score,\n    \"passage_retrieval_en\": retrieval_score,\n    \"passage_count\": count_score,\n    \"passage_retrieval_zh\": retrieval_zh_score,\n    \"lcc\": code_sim_score,\n    \"repobench-p\": code_sim_score,\n}\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=None)\n    parser.add_argument(\"--e\", action=\"store_true\", help=\"Evaluate on LongBench-E\")\n    parser.add_argument(\"--results_path\", type=str, default=None)\n    return parser.parse_args(args)\n\n\ndef scorer_e(dataset, predictions, answers, lengths, all_classes):\n    scores = {\"0-4k\": [], \"4-8k\": [], \"8k+\": []}\n    for prediction, ground_truths, length in zip(predictions, answers, lengths):\n        score = 0.0\n        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n            prediction = prediction.lstrip(\"\\n\").split(\"\\n\")[0]\n        for ground_truth in ground_truths:\n            score = max(\n                score,\n                dataset2metric[dataset](\n                    prediction, ground_truth, all_classes=all_classes\n                ),\n            )\n        if length < 4000:\n            scores[\"0-4k\"].append(score)\n        elif length < 8000:\n            scores[\"4-8k\"].append(score)\n        else:\n            scores[\"8k+\"].append(score)\n    for key in scores.keys():\n        scores[key] = round(100 * np.mean(scores[key]), 2)\n    return scores\n\n\ndef scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.0\n    for prediction, ground_truths in zip(predictions, answers):\n        score = 0.0\n        prediction = (\n            prediction.split(\".assistant\")[0]\n            .split(\"\\n\\nQuestion\")[0]\n            .split(\"</s>\")[0]\n            .split(\"(Document\")[0]\n            .split(\"\\n\\nQuestion\")[0]\n            .split(\"\\n\\nAnswer\")[0]\n            .split(\"(Passage\")[0]\n            .strip()\n        )\n        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n            prediction = prediction.lstrip(\"\\n\").split(\"\\n\")[0]\n        if dataset in [\"multifieldqa_zh\", \"dureader\"]:\n            prediction = prediction.split(\"问题：\")[0].strip()\n        if dataset in [\"lsht\"]:\n            prediction = prediction.split(\"新闻内容：\")[0].strip()\n        if dataset in [\"passage_retrieval_zh\"]:\n            prediction = prediction.split(\"请问\")[0].split(\"提示\")[0].strip()\n        for ground_truth in ground_truths:\n            score = max(\n                score,\n                dataset2metric[dataset](\n                    prediction, ground_truth, all_classes=all_classes\n                ),\n            )\n        total_score += score\n    return round(100 * total_score / len(predictions), 2)\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    scores = dict()\n    if args.results_path:\n        path = args.results_path\n    else:\n        if args.e:\n            path = f\"pred_e/{args.model}/\"\n        else:\n            path = f\"pred/{args.model}/\"\n    all_files = os.listdir(path)\n    all_files.sort()\n    print(\"Evaluating on:\", all_files)\n    for filename in all_files:\n        if not filename.endswith(\"jsonl\"):\n            continue\n        predictions, answers, lengths = [], [], []\n        dataset = filename.split(\"-\")[0]\n        if dataset == \"repobench\":\n            dataset = \"repobench-p\"\n        with open(f\"{path}{filename}\", \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                data = json.loads(line)\n                predictions.append(data[\"pred\"])\n                answers.append(data[\"answers\"])\n                all_classes = data[\"all_classes\"]\n                if \"length\" in data:\n                    lengths.append(data[\"length\"])\n        if len(predictions) == 0:\n            continue\n        if args.e:\n            score = scorer_e(dataset, predictions, answers, lengths, all_classes)\n        else:\n            score = scorer(dataset, predictions, answers, all_classes)\n        scores[filename] = score\n\n        print(f\"{filename}: {score}\")\n\n    if args.results_path:\n        out_path = os.path.join(args.results_path, \"result.json\")\n    else:\n        if args.e:\n            out_path = f\"pred_e/{args.model}/result.json\"\n        else:\n            out_path = f\"pred/{args.model}/result.json\"\n\n    with open(out_path, \"w\") as f:\n        json.dump(scores, f, ensure_ascii=False, indent=4)\n"}
{"type": "source_file", "path": "eval/RULER/scripts/pred/client_wrappers.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport abc\nimport json\nimport multiprocessing\nimport os\nimport re\nimport sys\nimport time\nimport requests\nimport traceback\nfrom pathlib import Path\nfrom typing import List, Tuple, Union\nfrom concurrent.futures import ThreadPoolExecutor\nfrom collections import defaultdict\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n) \n\n\nclass Client(abc.ABC):\n    def __init__(\n        self,\n        server_host,\n        server_port='5000',\n        ssh_server=None,\n        ssh_key_path=None,\n        **generation_kwargs\n    ):\n        self.server_host = server_host\n        self.server_port = server_port\n        self.ssh_server = os.getenv(\"SSH_SERVER\", ssh_server)\n        self.ssh_key_path = os.getenv(\"SSH_KEY_PATH\", ssh_key_path)\n        self.generation_kwargs = generation_kwargs\n        \n    @abc.abstractmethod\n    def _single_call(\n        self,\n        prompts,\n    ):\n        pass\n\n    def __call__(\n        self,\n        prompt: str,\n        **kwargs\n    ):\n        request = self.generation_kwargs\n        # prompts are added later\n        request['prompts'] = [f'{prompt}']\n        if 'others' in kwargs:\n            request['others'] = kwargs['others']\n\n        outputs = self._single_call(**request)\n        response = {'text': outputs}\n        return response\n        \n    @retry(wait=wait_random_exponential(min=15, max=60), stop=stop_after_attempt(3))\n    def _send_request(self, request, route=\"generate\"):\n        if self.ssh_server and self.ssh_key_path:\n            import sshtunnel_requests\n\n            sshtunnel_request = sshtunnel_requests.from_url(f\"ssh://{self.ssh_server}:22\", self.ssh_key_path)\n            outputs = sshtunnel_request.put(\n                url=\"http://{}:{}/{}\".format(self.server_host, self.server_port, route),\n                data=json.dumps(request),\n                headers={\"Content-Type\": \"application/json\"},\n            ).json()\n        else:\n            outputs = requests.put(\n                url=\"http://{}:{}/{}\".format(self.server_host, self.server_port, route),\n                data=json.dumps(request),\n                headers={\"Content-Type\": \"application/json\"},\n            ).json()\n        return outputs\n\n    def process_batch(self, prompts: List[str], **kwargs) -> List[dict]:\n        num_threads = max(96, multiprocessing.cpu_count() * 16)\n        with ThreadPoolExecutor(num_threads) as executor:\n            futures = []\n            for prompt in prompts:\n                futures.append(\n                    executor.submit(\n                        self.__call__,\n                        prompt,\n                        **kwargs,\n                    )\n                )\n            rets = [f.result() for f in futures]\n        return rets\n\n\nclass TRTLLMClient(Client):\n    def _single_call(\n        self,\n        prompts,\n        tokens_to_generate,\n        temperature,\n        top_p,\n        top_k,\n        random_seed,\n        stop: List[str],\n        max_attention_window_size=None,\n    ):\n        request = {\n            \"prompts\": prompts,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"random_seed\": random_seed,\n            'stop_words_list': \",\".join(stop),\n        }\n        if max_attention_window_size:\n            request[\"max_attention_window_size\"] = max_attention_window_size\n            \n        outputs = self._send_request(request)\n        return outputs\n\n\nclass VLLMClient(Client):\n    def _single_call(\n        self,\n        prompts,\n        tokens_to_generate,\n        temperature,\n        top_p,\n        top_k,\n        random_seed,\n        stop: List[str],\n    ):\n        request = {\n            \"prompt\": prompts[0],\n            \"max_tokens\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"stop\": stop,\n        }\n        # TODO: random seed is not supported?\n        outputs = self._send_request(request)\n        outputs = outputs['text']\n        return outputs\n\n\nclass SGLClient(Client):\n    def _single_call(\n        self,\n        prompts,\n        tokens_to_generate,\n        temperature,\n        top_p,\n        top_k,\n        random_seed,\n        stop: List[str],\n    ):\n        request = {\n            \"text\": prompts[0],\n            \"sampling_params\": {\n                \"max_new_tokens\": tokens_to_generate,\n                \"temperature\": temperature,\n                \"top_k\": top_k,\n                \"top_p\": top_p,\n                \"stop\": stop,\n            }\n        }\n        # TODO: random seed is not supported?\n        outputs = self._send_request(request)\n        outputs = outputs['text']\n        return outputs\n\n\nclass OpenAIClient:\n    def __init__(\n        self,\n        model_name,\n        **generation_kwargs\n    ):  \n        model2length = {\n            # OpenAI\n            'gpt-4': 8192,\n            'gpt-4-0613': 8192,\n            'gpt-4-1106-preview': 128000,\n            'gpt-4-0125-preview': 128000,\n            'gpt-4-turbo-preview': 128000,\n            'gpt-3.5-turbo-0125': 16385,\n            'gpt-3.5-turbo-1106': 16385,\n            'gpt-3.5-turbo-0613': 4096,\n            'gpt-3.5-turbo': 16385,\n            'gpt-3.5-turbo-16k': 16385,\n            'gpt-3.5-turbo-16k-0613': 16385,\n\n            # Azure\n            'gpt-4-32k': 32768,\n            'gpt-4': 128000,\n            'gpt-35-turbo-16k': 16384,\n        }\n        self.openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n        self.azure_api_id = os.environ[\"AZURE_API_ID\"]\n        self.azure_api_secret = os.environ[\"AZURE_API_SECRET\"]\n        self.azure_api_endpoint = os.environ[\"AZURE_API_ENDPOINT\"]\n        self.model_name = model_name    \n            \n        # Azure\n        if self.azure_api_id and self.azure_api_secret:\n            if 'gpt-3.5' in model_name: self.model_name = 'gpt-35-turbo-16k'\n            if 'gpt-4' in model_name: self.model_name = 'gpt-4'\n        \n        import tiktoken\n        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n        self.max_length = model2length[self.model_name]\n        self.generation_kwargs = generation_kwargs\n        self._create_client()\n        \n    def _create_client(self,):\n        from openai import OpenAI, AzureOpenAI\n        \n        # OpenAI\n        if self.openai_api_key:\n            self.client = OpenAI(\n                api_key=self.openai_api_key\n            )\n\n        # Azure\n        elif self.azure_api_id and self.azure_api_secret:\n            self.client = AzureOpenAI(\n                api_key=self.get_azure_api_key(\n                    self.azure_api_id, \n                    self.azure_api_secret,\n                    self.azure_api_endpoint,\n                ),\n                api_version=\"2024-02-15-preview\",\n                azure_endpoint=os.path.join(self.azure_api_endpoint, \"llm/v1/azure\"),\n            )\n        \n    def _count_tokens(self, messages):\n        tokens_per_message = 3\n        tokens_per_name = 1\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                num_tokens += len(self.encoding.encode(value))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n        return num_tokens\n        \n    @retry(wait=wait_random_exponential(min=15, max=60), stop=stop_after_attempt(3))\n    def _send_request(self, request):\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=request['msgs'],\n                max_tokens=request['tokens_to_generate'],\n                temperature=request['temperature'],\n                seed=request['random_seed'],\n                top_p=request['top_p'],\n                stop=request['stop'],\n            )\n        except Exception as e:\n            print(f\"Error occurred while calling OpenAI: {e}\")\n            if self.azure_api_id and self.azure_api_secret and e.status_code == 401:\n                # token expired\n                self._create_client()\n            \n        return response\n        \n    def __call__(\n        self,\n        prompt: str,\n    ):\n        # system_msg = [{\"role\": \"system\", \"content\": \"\"}]\n        system_msg = []\n        user_assistant_msgs = [{\"role\": \"user\", \"content\": prompt}]\n        msgs = system_msg + user_assistant_msgs\n        openai_length = self._count_tokens(msgs)\n        request = self.generation_kwargs\n        \n        tokens_to_generate_new = self.max_length - openai_length\n        if tokens_to_generate_new < request['tokens_to_generate']:\n            print(f\"Reduce generate tokens from {request['tokens_to_generate']} to {tokens_to_generate_new}\")\n            request['tokens_to_generate'] = tokens_to_generate_new\n    \n        request[\"msgs\"] = msgs\n        outputs = self._send_request(request)\n        response = {'text': [outputs.choices[0].message.content]}\n        return response\n\n    \n    def get_azure_api_key(\n        self,\n        p_client_id, \n        p_client_secret, \n        p_token_url, \n        p_scope=\"azureopenai-readwrite\",\n        cache_file=\"azure_openai_key.json\"\n    ):\n        base_path = Path(__file__).parent\n        file_path = Path.joinpath(base_path, cache_file)\n     \n        # Check if the token is cached\n        renew = True\n        if os.path.exists(file_path):\n            with open(file_path, \"r\") as f:\n                token = json.load(f)\n                renew = True if time.time() > token[\"expires_in\"] else False\n\n        if renew:\n            # Get a new token from the OAuth server\n            response = requests.post(\n                os.path.join(p_token_url, \"oauth/api/v1/ssa/default/token\"),\n                data={\"grant_type\": \"client_credentials\", \"client_id\": p_client_id,\n                        \"client_secret\": p_client_secret, \"scope\": p_scope}\n            )\n            response.raise_for_status()\n            token = response.json()\n            token[\"expires_in\"] += time.time()\n            with open(file_path, \"w\") as f:\n                json.dump(token, f)\n     \n     \n        authToken = token[\"access_token\"]\n        return authToken\n\n\nclass GeminiClient:\n    def __init__(\n        self,\n        model_name,\n        **generation_kwargs\n    ):\n        model2length = {\n            'gemini-1.0-pro-latest': (30720, 2048),\n            'gemini-1.5-pro-latest': (1048576, 8192)\n        }\n        \n        self.model_name = model_name\n        self.model = self._initialize_model()\n        self.max_input_length = model2length[model_name][0]\n        self.max_output_length = model2length[model_name][1]\n        assert generation_kwargs['tokens_to_generate'] < self.max_output_length, \\\n            print(f'tokens_to_generate exceeds {self.max_output_length}')\n        \n        import google.generativeai as genai        \n        self.config = genai.GenerationConfig(\n            candidate_count=1,\n            stop_sequences=generation_kwargs['stop'],\n            max_output_tokens=generation_kwargs['tokens_to_generate'],\n            temperature=generation_kwargs['temperature'],\n            top_p=generation_kwargs['top_p'],\n            top_k=generation_kwargs['top_k'],\n        )\n\n        from google.generativeai.types import HarmCategory, HarmBlockThreshold\n        self.safety_settings = {\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n        }\n\n    @retry(wait=wait_random_exponential(min=60, max=60), stop=stop_after_attempt(3))\n    def _send_request(self, request):\n        try:\n            response = self.model.generate_content(request['prompt'], \n                                                   generation_config=request['config'],\n                                                   safety_settings=self.safety_settings)\n        except Exception as e:\n            traceback.print_exc()\n            return None\n        return response\n        \n    def __call__(\n        self,\n        prompt: str,\n    ):\n        assert self.model.count_tokens(prompt).total_tokens < self.max_input_length, \\\n            print(f'input length exceeds {self.max_input_length}')\n        \n        request = {\n            'prompt': prompt,\n            'config': self.config,\n        }\n        \n        outputs = self._send_request(request)\n\n        try:\n            response = {'text': [outputs.candidates[0].content.parts[0].text]}\n        except Exception as e:\n            response = {'text': []}\n            print(outputs)\n            traceback.print_exc()\n            \n        return response\n\n    def _initialize_model(self):\n        import google.generativeai as genai\n        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n        return genai.GenerativeModel(self.model_name)\n\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/template.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nTemplates = {\n    'base': \"{task_template}\",\n\n    'meta-chat': \"[INST] {task_template} [/INST]\",\n\n    'vicuna-chat': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {task_template} ASSISTANT:\",\n\n    'lwm-chat': \"You are a helpful assistant. USER: {task_template} ASSISTANT: \",\n\n    'command-r-chat': \"<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{task_template}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n\n    'chatglm-chat': \"[gMASK]sop<|user|> \\n {task_template}<|assistant|> \\n \",\n    \n    'RWKV': \"User: hi\\n\\nAssistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it\\n\\nUser: {task_template}\\n\\nAssistant:\",\n\n    'Phi3': \"<|user|>\\n{task_template}<|end|>\\n<|assistant|>\\n\",\n\n    'meta-llama3': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{task_template}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n    \n    'jamba': \"<|startoftext|><|bom|><|system|> <|eom|><|bom|><|user|> {task_template}<|eom|><|bom|><|assistant|>\",\n}"}
{"type": "source_file", "path": "eval/LongBench/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/RULER/scripts/data/prepare.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nPrepare jsonl with field `input` and `outputs`.\n{\n    \"index\" int,\n    \"input\": str,\n    \"outputs\": [str],\n}\n\npython prepare.py \\\n    --save_dir ./ \\\n    --benchmark synthetic \\\n    --task niah_single_1 \\\n    --tokenizer_path tokenizer.model \\\n    --tokenizer_type nemo \\\n    --max_seq_length 4096 \\\n    --model_template_type base \\\n    --num_samples 10 \\\n\"\"\"\nimport os\nimport argparse\nimport importlib\nimport subprocess\nimport time\nimport yaml\nfrom pathlib import Path\nfrom template import Templates\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--benchmark\", type=str, default='synthetic', help='Options: [synthetic]')\nparser.add_argument(\"--task\", type=str, required=True, help='tasks in benchmark')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--num_samples\", type=int, default=500, help='maximum number of samples we want to test')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--model_template_type\", type=str, default='base', help='Options in `template.py`')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\nparser.add_argument(\"--chunk_idx\", type=int, default=0, help='index of current split chunk')\nparser.add_argument(\"--chunk_amount\", type=int, default=1, help='size of split chunk')\n\nargs = parser.parse_args()\n\ndef main():\n    start_time = time.time()\n    curr_folder = os.path.dirname(os.path.abspath(__file__))\n    \n    try:\n        module = importlib.import_module(f\"{args.benchmark}.constants\")\n    except ImportError:\n        print(f\"Module data.{args.benchmark}.constants not found.\")\n\n    tasks_base = module.TASKS\n    with open(os.path.join(curr_folder, f\"../{args.benchmark}.yaml\"), \"r\") as f:\n        tasks_customized = yaml.safe_load(f)\n\n    if args.task not in tasks_customized:\n        raise ValueError(f'{args.task} is not found in config_tasks.yaml')\n        \n    config = tasks_customized.get(args.task)\n    config.update(tasks_base[config['task']])\n\n    # Add templates\n    assert args.model_template_type in Templates, print(f'{args.model_template_type} is not found in {Templates.keys()}')\n    model_template = Templates[args.model_template_type]    \n    task_template = config['template']\n\n    # Add answer prefix for all models\n    answer_prefix = config['answer_prefix'] if 'answer_prefix' in config else ''\n    config['template'] = model_template.format(task_template=task_template) + answer_prefix\n\n    # Split task into multiple chunks \n    chunks = [(args.num_samples // args.chunk_amount) + (1 if i < args.num_samples % args.chunk_amount else 0) for i in range(args.chunk_amount)]\n    num_samples = chunks[args.chunk_idx]\n    pre_samples = sum(chunks[:args.chunk_idx])\n    \n    random_seed = 42 + args.chunk_idx\n\n    \n    save_file = args.save_dir / args.task / f\"{args.subset}.jsonl\"\n    file_exists = False\n    if os.path.exists(save_file):\n        with open(save_file, \"r\") as f:\n            data = f.readlines()\n        if len(data) == args.num_samples: file_exists = True\n\n    if not file_exists:\n        try:\n            script = os.path.join(curr_folder, args.benchmark, f\"{config['task']}.py\")\n            additional_args = \" \".join([f\"--{k} {v}\" for k, v in config['args'].items()])\n            command = f\"\"\"python {script} \\\n            --save_dir  {args.save_dir} \\\n            --save_name {args.task} \\\n            --subset {args.subset} \\\n            --tokenizer_path {args.tokenizer_path} \\\n            --tokenizer_type {args.tokenizer_type} \\\n            --max_seq_length {args.max_seq_length} \\\n            --tokens_to_generate {config['tokens_to_generate']} \\\n            --num_samples {num_samples} \\\n            --random_seed {random_seed} \\\n            {additional_args} \\\n            {f\"--remove_newline_tab\" if args.remove_newline_tab else \"\"} \\\n            {f\"--pre_samples {pre_samples}\" if config['task'] == 'qa' else \"\"} \\\n            --template \"{config['template']}\"\n            \"\"\"\n            print(command)\n            result = subprocess.run(command, \n                                    shell=True, \n                                    check=True, \n                                    stdout=subprocess.PIPE, \n                                    stderr=subprocess.PIPE, \n                                    text=True)\n            \n            if result.returncode == 0:\n                print(\"Output:\")\n                print(result.stdout)\n            else:\n                print(\"Error:\")\n                print(result.stderr)\n        except subprocess.CalledProcessError as e:\n            print(\"Error output:\", e.stderr)\n\n        print(f\"Prepare {args.task} with lines: {args.num_samples} to {save_file}\")\n        print(f\"Used time: {round((time.time() - start_time) / 60, 1)} minutes\")\n    else:\n        print(f\"Skip preparing {args.task} with lines: {args.num_samples} to {save_file} (file exists)\")\n    \nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/freq_words_extraction.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nCreate a dataset jsonl file for frequent words extraction.\n\npython freq_words_extraction.py   \\\n    --save_dir=./ \\\n    --save_name=vt \\\n    --tokenizer_path=tokenizer.model \\\n    --tokenizer_type nemo \\\n    --max_seq_length 4096 \\\n    --tokens_to_generate 30 \\\n    --num_samples 10 \\\n    --random_seed 42  \\\n    --alpha 2.0 \\\n    --template \"[INST] Read the following coded text and track the frequency of each coded word. Find the three most frequently appeared coded words. {context}\\nQuestion: Do not provide any explanation. Please ignore the dots '....'. What are the three most frequently appeared words in the above coded text? [/INST] Answer: According to the coded text above, the three most frequently appeared words are:\"\n\"\"\"\n\nimport os\nimport argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport string\nimport numpy as np\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\nimport sys\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\")) \nfrom tokenizer import select_tokenizer\nfrom scipy.special import zeta \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--save_name\", type=str, required=True, help='name of the save dataset jsonl file')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--tokens_to_generate\", type=int, default=50, help='number of tokens to generate')\nparser.add_argument(\"--num_samples\", type=int, required=True, help='number of samples to generate')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--template\", type=str, default='', help='prompt template')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\nparser.add_argument(\"--coded_wordlen\", type=int, default=6, help=\"length of synthetic word\")\nparser.add_argument(\"--vocab_size\", type=int, default=-1, help='synthetic vocab size to sample from')\nparser.add_argument(\"--alpha\", type=float, default=2.0, help='zeta distribution alpha')\nparser.add_argument(\"--add_fewshot\", action=\"store_true\", default=False)\n\nargs = parser.parse_args()\nrandom.seed(args.random_seed)\nnp.random.seed(args.random_seed)\n\n# Load Tokenizer\nTOKENIZER = select_tokenizer(args.tokenizer_type, args.tokenizer_path)\n\ndef generate_input_output(max_len, num_words=-1, coded_wordlen=6, vocab_size=2000, incremental=10, alpha=2.0):\n    # generate vocab\n    vocab = [''.join(random.choices(string.ascii_lowercase, k=coded_wordlen)) for _ in range(vocab_size)]\n    while len(set(vocab)) < vocab_size:\n        vocab.append(''.join(random.choices(string.ascii_lowercase, k=coded_wordlen)))\n    vocab = sorted(list(set(vocab)))\n    random.Random(args.random_seed).shuffle(vocab)\n    vocab[0] = '...' # treat the top ranked as noise\n\n    # sample words\n    template = args.template\n    def gen_text(num_words):\n        k = np.arange(1, len(vocab)+1)\n        sampled_cnt = num_words*(k**-alpha)/zeta(alpha)\n        sampled_words = [[w] * zi for w, zi in zip(vocab, sampled_cnt.astype(int))]\n        sampled_words = [x for wlst in sampled_words for x in wlst]\n        random.Random(args.random_seed).shuffle(sampled_words)\n        return template.format(context=' '.join(sampled_words), query=''), vocab[1:4]\n    \n    if num_words > 0:\n        num_words = num_words\n        text, answer = gen_text(num_words)\n        while len(TOKENIZER.text_to_tokens(text)) > max_len:\n            num_words -= incremental\n            text, answer = gen_text(num_words)\n    else:\n        num_words = max_len // coded_wordlen # init\n        text, answer = gen_text(num_words)\n        while len(TOKENIZER.text_to_tokens(text)) < max_len:\n            num_words += incremental\n            text, answer = gen_text(num_words)\n        num_words -= incremental\n    text, answer = gen_text(num_words)\n    return text, answer, num_words\n\ndef sys_kwext(num_samples: int, max_seq_length: int, incremental: int = 10):\n    write_jsons = []\n    tokens_to_generate = args.tokens_to_generate\n\n    vocab_size = max_seq_length // 50 if args.vocab_size == -1 else args.vocab_size\n\n    # get number of words\n    input_max_len = max_seq_length \n    _, _, num_example_words = generate_input_output(input_max_len, \n                                                    coded_wordlen=args.coded_wordlen, \n                                                    vocab_size=vocab_size, \n                                                    incremental=input_max_len//32, \n                                                    alpha=args.alpha) \n    print('num_example_words:', num_example_words)\n    # Generate samples\n    for index in tqdm(range(num_samples)):\n        \n        # construct input\n        input_max_len = max_seq_length \n        input_text, answer, _ = generate_input_output(input_max_len,\n                                                   num_words=num_example_words,\n                                                   coded_wordlen=args.coded_wordlen, \n                                                   vocab_size=vocab_size,\n                                                   incremental=input_max_len//32,\n                                                   alpha=args.alpha)\n        \n\n        length = len(TOKENIZER.text_to_tokens(input_text)) + tokens_to_generate\n\n        if args.remove_newline_tab:\n            input_text = ' '.join(input_text.replace('\\n', ' ').replace('\\t', ' ').strip().split())\n        \n        formatted_output = {\n            'index': index,\n            \"input\": input_text,\n            \"outputs\": answer,\n            \"length\": length,\n        }\n        write_jsons.append(formatted_output)\n\n    return write_jsons\n\n\ndef main():   \n    save_file = args.save_dir / f'{args.save_name}' / f'{args.subset}.jsonl'\n    save_file.parent.mkdir(parents=True, exist_ok=True)\n    write_jsons = sys_kwext(num_samples=args.num_samples, max_seq_length=args.max_seq_length, \n                            incremental=10)\n    \n    write_manifest(save_file, write_jsons)\n\nif __name__==\"__main__\":\n    main()"}
{"type": "source_file", "path": "eval/VLMEvalKit/docs/en/conf.py", "content": "# flake8: noqa\n# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport ast\nimport subprocess\nimport sys\n\nimport pytorch_sphinx_theme\nfrom sphinx.builders.html import StandaloneHTMLBuilder\n\nsys.path.insert(0, os.path.abspath('../../'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'VLMEvalKit'\ncopyright = '2023, VLMEvalKit'\nauthor = 'VLMEvalKit Authors'\n\n# The full version, including alpha/beta/rc tags\nversion_file = '../../vlmeval/__init__.py'\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        file_content = f.read()\n    # Parse the file content into an abstract syntax tree (AST)\n    tree = ast.parse(file_content, filename=version_file)\n\n    # Iterate through the body of the AST, looking for an assignment to __version__\n    for node in tree.body:\n        if isinstance(node, ast.Assign):\n            for target in node.targets:\n                if isinstance(target, ast.Name) and target.id == '__version__':\n                    return node.value.s\n    raise ValueError('__version__ not found')\n\n\nrelease = get_version()\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'myst_parser',\n    'sphinx_copybutton',\n    'sphinx_tabs.tabs',\n    'notfound.extension',\n    'sphinxcontrib.jquery',\n    'sphinx_design',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = {\n    '.rst': 'restructuredtext',\n    '.md': 'markdown',\n}\n\nlanguage = 'en'\n\n# The master toctree document.\nroot_doc = 'index'\nhtml_context = {\n    'github_version': 'latest',\n}\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'pytorch_sphinx_theme'\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# yapf: disable\nhtml_theme_options = {\n    'menu': [\n        {\n            'name': 'GitHub',\n            'url': 'https://github.com/open-compass/VLMEvalKit'\n        },\n    ],\n    # Specify the language of shared menu\n    'menu_lang': 'en',\n    # Disable the default edit on GitHub\n    'default_edit_on_github': False,\n}\n# yapf: enable\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\nhtml_css_files = [\n    'https://cdn.datatables.net/v/bs4/dt-1.12.1/datatables.min.css',\n    'css/readthedocs.css'\n]\nhtml_js_files = [\n    'https://cdn.datatables.net/v/bs4/dt-1.12.1/datatables.min.js',\n    'js/custom.js'\n]\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'vlmevalkitdoc'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (root_doc, 'vlmevalkit.tex', 'VLMEvalKit Documentation', author,\n     'manual'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(root_doc, 'vlmevalkit', 'VLMEvalKit Documentation', [author],\n              1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (root_doc, 'vlmevalkit', 'VLMEvalKit Documentation', author,\n     'VLMEvalKit Authors', 'AGI evaluation toolbox and benchmark.',\n     'Miscellaneous'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = ''\n\n# A unique identification for the text.\n#\n# epub_uid = ''\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = ['search.html']\n\n# set priority when building html\nStandaloneHTMLBuilder.supported_image_types = [\n    'image/svg+xml', 'image/gif', 'image/png', 'image/jpeg'\n]\n\n# -- Extension configuration -------------------------------------------------\n# Ignore >>> when copying code\ncopybutton_prompt_text = r'>>> |\\.\\.\\. '\ncopybutton_prompt_is_regexp = True\n\n# Auto-generated header anchors\nmyst_heading_anchors = 3\n# Enable \"colon_fence\" extension of myst.\nmyst_enable_extensions = ['colon_fence', 'dollarmath']\n\n# Configuration for intersphinx\nintersphinx_mapping = {\n    'python': ('https://docs.python.org/3', None),\n    'numpy': ('https://numpy.org/doc/stable', None),\n    'torch': ('https://pytorch.org/docs/stable/', None),\n    'mmengine': ('https://mmengine.readthedocs.io/en/latest/', None),\n    'transformers':\n    ('https://huggingface.co/docs/transformers/main/en/', None),\n}\nnapoleon_custom_sections = [\n    # Custom sections for data elements.\n    ('Meta fields', 'params_style'),\n    ('Data fields', 'params_style'),\n]\n\n# Disable docstring inheritance\nautodoc_inherit_docstrings = False\n# Mock some imports during generate API docs.\nautodoc_mock_imports = ['rich', 'attr', 'einops']\n# Disable displaying type annotations, these can be very verbose\nautodoc_typehints = 'none'\n\n# The not found page\nnotfound_template = '404.html'\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/json/download_paulgraham_essay.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\nimport os\nimport shutil\nimport glob\nimport json\nimport urllib.request\nimport html2text\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n\ntemp_folder_repo = 'essay_repo'\ntemp_folder_html = 'essay_html'\nos.makedirs(temp_folder_repo, exist_ok=True)\nos.makedirs(temp_folder_html, exist_ok=True)\n\nh = html2text.HTML2Text()\nh.ignore_images = True\nh.ignore_tables = True\nh.escape_all = True\nh.reference_links = False\nh.mark_code = False\n\nwith open('PaulGrahamEssays_URLs.txt') as f:\n    urls = [line.strip() for line in f]\n\nfor url in tqdm(urls):\n    if '.html' in url:\n        filename = url.split('/')[-1].replace('.html', '.txt')        \n        try:\n            with urllib.request.urlopen(url) as website:\n                content = website.read().decode(\"unicode_escape\", \"utf-8\")\n                soup = BeautifulSoup(content, 'html.parser')\n                specific_tag = soup.find('font')\n                parsed = h.handle(str(specific_tag))\n                \n                with open(os.path.join(temp_folder_html, filename), 'w') as file:\n                    file.write(parsed)\n        \n        except Exception as e:\n            print(f\"Fail download {filename}, ({e})\")\n\n    else:\n        filename = url.split('/')[-1]\n        try:\n            with urllib.request.urlopen(url) as website:\n                content = website.read().decode('utf-8')\n            \n            with open(os.path.join(temp_folder_repo, filename), 'w') as file:\n                file.write(content)\n                    \n        except Exception as e:\n            print(f\"Fail download {filename}, ({e})\")\n\nfiles_repo = sorted(glob.glob(os.path.join(temp_folder_repo,'*.txt')))\nfiles_html = sorted(glob.glob(os.path.join(temp_folder_html,'*.txt')))\nprint(f'Download {len(files_repo)} essays from `https://github.com/gkamradt/LLMTest_NeedleInAHaystack/`') \nprint(f'Download {len(files_html)} essays from `http://www.paulgraham.com/`') \n\ntext = \"\"\nfor file in files_repo + files_html:\n    with open(file, 'r') as f:\n        text += f.read()\n        \nwith open('PaulGrahamEssays.json', 'w') as f:\n    json.dump({\"text\": text}, f)\n\n\nshutil.rmtree(temp_folder_repo)\nshutil.rmtree(temp_folder_html)\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/constants.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nAdd a new task (required arguments):\n\nTASK_NAME: {\n    'tokens_to_generate': how many tokens we want to generate.\n    'template': the template with at least {context} and {query}.\n}\n\"\"\"\n\nTASKS = {\n    'niah': {\n        'tokens_to_generate': 128,\n        'template': \"\"\"Some special magic {type_needle_v} are hidden within the following text. Make sure to memorize it. I will quiz you about the {type_needle_v} afterwards.\\n{context}\\nWhat are all the special magic {type_needle_v} for {query} mentioned in the provided text?\"\"\",\n        'answer_prefix': \"\"\" The special magic {type_needle_v} for {query} mentioned in the provided text are\"\"\"\n    },\n    \n    'variable_tracking': {\n        'tokens_to_generate': 30,\n        'template': \"\"\"Memorize and track the chain(s) of variable assignment hidden in the following text.\\n\\n{context}\\nQuestion: Find all variables that are assigned the value {query} in the text above.\"\"\",\n        'answer_prefix': \"\"\" Answer: According to the chain(s) of variable assignment in the text above, {num_v} variables are assgined the value {query}, they are: \"\"\"\n    },\n    \n    'common_words_extraction': {\n        'tokens_to_generate': 120,\n        'template': \"\"\"Below is a numbered list of words. In these words, some appear more often than others (not including very common words like \"the\", \"and\", etc.). Memorize the ones that appear most often.\\n{context}\\nQuestion: What are the 10 most common words (not including very common words like \"the\", \"and\", etc.) in the above list?\"\"\",\n        'answer_prefix': \"\"\" Answer: The top 10 words that appear most often in the list, excluding common words (like \"the\", \"and\", etc.), are:\"\"\"\n    },\n    \n    'freq_words_extraction' : {\n        'tokens_to_generate': 50,\n        'template': \"\"\"Read the following coded text and track the frequency of each coded word. Find the three most frequently appeared coded words. {context}\\nQuestion: Do not provide any explanation. Please ignore the dots '....'. What are the three most frequently appeared words in the above coded text?\"\"\",\n        'answer_prefix': \"\"\" Answer: According to the coded text above, the three most frequently appeared words are:\"\"\"\n    },\n\n    'qa': {\n        'tokens_to_generate': 32, \n        'template': \"\"\"Answer the question based on the given documents. Only give me the answer and do not output any other words.\\n\\nThe following are given documents.\\n\\n{context}\\n\\nAnswer the question based on the given documents. Only give me the answer and do not output any other words.\\n\\nQuestion: {query}\"\"\",\n        'answer_prefix': \"\"\" Answer:\"\"\",\n    },\n}"}
{"type": "source_file", "path": "eval/RULER/scripts/eval/synthetic/constants.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nAdd a new task:\n\nTASK_NAME: {\n    'metric_fn': the metric function with input (predictions: [str], references: [[str]]) to compute score.\n}\n\"\"\"\n\n\ndef string_match_part(preds, refs):\n    score = sum([max([1.0 if r.lower() in pred.lower() else 0.0 for r in ref]) for pred, ref in zip(preds, refs)]) / len(preds) * 100\n    return round(score, 2)\n\ndef string_match_all(preds, refs):\n    score = sum([sum([1.0 if r.lower() in pred.lower() else 0.0 for r in ref]) / len(ref) for pred, ref in zip(preds, refs)]) / len(preds) * 100\n    return round(score, 2)\n    \n\nTASKS = {\n    'niah': {\n        'metric_fn': string_match_all,\n    },\n    'variable_tracking': {\n        'metric_fn': string_match_all,\n    },\n    'common_words_extraction': {\n        'metric_fn': string_match_all,\n    },\n    'freq_words_extraction': {\n        'metric_fn': string_match_all\n    },\n    'qa': {\n        'metric_fn': string_match_part,\n    },\n}\n"}
{"type": "source_file", "path": "eval/LongBench/metrics.py", "content": "import re\nimport string\n\nimport jieba\nfrom fuzzywuzzy import fuzz\nimport difflib\n\nfrom typing import List\nfrom collections import Counter\nfrom rouge import Rouge\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef normalize_zh_answer(s):\n    \"\"\"Lower text and remove punctuation, extra whitespace.\"\"\"\n\n    def white_space_fix(text):\n        return \"\".join(text.split())\n\n    def remove_punc(text):\n        cn_punctuation = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n        all_punctuation = set(string.punctuation + cn_punctuation)\n        return \"\".join(ch for ch in text if ch not in all_punctuation)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_punc(lower(s)))\n\n\ndef count_score(prediction, ground_truth, **kwargs):\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef retrieval_score(prediction, ground_truth, **kwargs):\n    pattern = r\"Paragraph (\\d+)\"\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef retrieval_zh_score(prediction, ground_truth, **kwargs):\n    pattern = r\"段落(\\d+)\"\n    matches = re.findall(pattern, ground_truth)\n    ground_truth_id = matches[0]\n    numbers = re.findall(r\"\\d+\", prediction)\n    right_num = 0\n    for number in numbers:\n        if str(number) == str(ground_truth_id):\n            right_num += 1\n    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)\n    return float(final_score)\n\n\ndef code_sim_score(prediction, ground_truth, **kwargs):\n    all_lines = prediction.lstrip(\"\\n\").split(\"\\n\")\n    prediction = \"\"\n    for line in all_lines:\n        if (\"`\" not in line) and (\"#\" not in line) and (\"//\" not in line):\n            prediction = line\n            break\n    return fuzz.ratio(prediction, ground_truth) / 100\n\n\ndef classification_score(prediction, ground_truth, **kwargs):\n    em_match_list = []\n    all_classes = kwargs[\"all_classes\"]\n    for class_name in all_classes:\n        if class_name in prediction:\n            em_match_list.append(class_name)\n    for match_term in em_match_list:\n        if match_term in ground_truth and match_term != ground_truth:\n            em_match_list.remove(match_term)\n    if ground_truth in em_match_list:\n        score = 1.0 / len(em_match_list)\n    else:\n        score = 0.0\n    return score\n\n\ndef rouge_score(prediction, ground_truth, **kwargs):\n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores([prediction], [ground_truth], avg=True)\n    except:\n        return 0.0\n    return scores[\"rouge-l\"][\"f\"]\n\n\ndef rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False)))\n    score = rouge_score(prediction, ground_truth)\n    return score\n\n\ndef f1_score(prediction, ground_truth, **kwargs):\n    common = Counter(prediction) & Counter(ground_truth)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction)\n    recall = 1.0 * num_same / len(ground_truth)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef qa_f1_score(prediction, ground_truth, **kwargs):\n    normalized_prediction = normalize_answer(prediction)\n    normalized_ground_truth = normalize_answer(ground_truth)\n\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    return f1_score(prediction_tokens, ground_truth_tokens)\n\n\ndef qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens = [normalize_zh_answer(token) for token in prediction_tokens]\n    ground_truth_tokens = [normalize_zh_answer(token) for token in ground_truth_tokens]\n    prediction_tokens = [token for token in prediction_tokens if len(token) > 0]\n    ground_truth_tokens = [token for token in ground_truth_tokens if len(token) > 0]\n    return f1_score(prediction_tokens, ground_truth_tokens)\n"}
{"type": "source_file", "path": "eval/RULER/scripts/pred/serve_vllm.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# adapted from https://github.com/vllm-project/vllm/blob/v0.4.0/vllm/entrypoints/api_server.py\n\nimport argparse\nimport json\nfrom typing import AsyncGenerator\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse, Response, StreamingResponse\nimport uvicorn\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import random_uuid\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds.\napp = FastAPI()\nengine = None\n\n\n@app.get(\"/health\")\nasync def health() -> Response:\n    \"\"\"Health check.\"\"\"\n    return Response(status_code=200)\n\n\n@app.put(\"/generate\")\nasync def generate(request: Request) -> Response:\n    \"\"\"Generate completion for the request.\n\n    The request should be a JSON object with the following fields:\n    - prompt: the prompt to use for the generation.\n    - stream: whether to stream the results or not.\n    - other fields: the sampling parameters (See `SamplingParams` for details).\n    \"\"\"\n    request_dict = await request.json()\n    prompt = request_dict.pop(\"prompt\")\n    stream = request_dict.pop(\"stream\", False)\n    sampling_params = SamplingParams(**request_dict)\n    request_id = random_uuid()\n\n    results_generator = engine.generate(prompt,\n                                        sampling_params,\n                                        request_id)\n\n    # Streaming case\n    async def stream_results() -> AsyncGenerator[bytes, None]:\n        async for request_output in results_generator:\n            prompt = request_output.prompt\n            text_outputs = [\n                prompt + output.text for output in request_output.outputs\n            ]\n            ret = {\"text\": text_outputs}\n            yield (json.dumps(ret) + \"\\0\").encode(\"utf-8\")\n\n    if stream:\n        return StreamingResponse(stream_results())\n\n    # Non-streaming case\n    final_output = None\n    async for request_output in results_generator:\n        if await request.is_disconnected():\n            # Abort the request if the client disconnects.\n            await engine.abort(request_id)\n            return Response(status_code=499)\n        final_output = request_output\n    assert final_output is not None\n    text_outputs = [output.text for output in final_output.outputs]\n    ret = {\"text\": text_outputs}\n    return JSONResponse(ret)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int, default=5000)\n    parser.add_argument(\"--ssl-keyfile\", type=str, default=None)\n    parser.add_argument(\"--ssl-certfile\", type=str, default=None)\n    parser.add_argument(\n        \"--root-path\",\n        type=str,\n        default=None,\n        help=\"FastAPI root_path when app is behind a path based routing proxy\")\n    parser = AsyncEngineArgs.add_cli_args(parser)\n    args = parser.parse_args()\n\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n    app.root_path = args.root_path\n    uvicorn.run(app,\n                host=args.host,\n                port=args.port,\n                log_level=\"debug\",\n                timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n                ssl_keyfile=args.ssl_keyfile,\n                ssl_certfile=args.ssl_certfile)"}
{"type": "source_file", "path": "eval/LongBench/pred.py", "content": "import os\nfrom datasets import load_dataset\nimport torch\nimport json\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    GenerationConfig,\n)\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport argparse\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nfrom transformers.cache_utils import Cache\nfrom transformers.models.llama.modeling_llama import (\n    repeat_kv,\n    apply_rotary_pos_emb,\n    nn,\n)\nimport math\nfrom xattn.src.Xattention import Xattention_prefill\nfrom xattn.src.Flexprefill import Flexprefill_prefill\nfrom xattn.src.Minference import Minference_prefill\nfrom flash_attn import flash_attn_func\nimport types\nfrom ratio import max_ratio, max\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\"--e\", action=\"store_true\", help=\"Evaluate on LongBench-E\")\n\n    parser.add_argument(\"--task\", type=str, help=\"task name\", required=True)\n\n    parser.add_argument(\n        \"--method\",\n        type=str,\n        default=\"full\",\n    )\n\n    return parser.parse_args(args)\n\n\n# This is the customized building prompt for chat models\ndef build_chat(tokenizer, prompt, model_name):\n    if \"llama-2\" in model_name:\n        prompt = f\"[INST]{prompt}[/INST]\"\n    return prompt\n\n\ndef post_process(response, model_name):\n    if \"xgen\" in model_name:\n        response = response.strip().replace(\"Assistant:\", \"\")\n    elif \"internlm\" in model_name:\n        response = response.split(\"<eoa>\")[0]\n    elif \"llama-3\" in model_name.lower():\n        response = (\n            response.split(\".assistant\")[0]\n            .split(\"\\n\\nQuestion\")[0]\n            .split(\"</s>\")[0]\n            .strip()\n        )\n    elif \"Llama-2-7B-32K-Instruct\" in model_name:\n        response = (\n            response.split(\"(Document\")[0]\n            .split(\"\\n\\nQuestion\")[0]\n            .split(\"\\n\\nAnswer\")[0]\n            .split(\"(Passage\")[0]\n            .strip()\n        )\n    return response\n\n\n@torch.no_grad()\ndef new_attention_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Cache] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    cache_position: Optional[torch.LongTensor] = None,\n    position_embeddings: Optional[\n        Tuple[torch.Tensor, torch.Tensor]\n    ] = None,  # will become mandatory in v4.46\n    **kwargs,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states)\n    key_states = self.k_proj(hidden_states)\n    value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(\n        bsz, q_len, self.num_heads, self.head_dim\n    ).transpose(1, 2)\n    key_states = key_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n    value_states = value_states.view(\n        bsz, q_len, self.num_key_value_heads, self.head_dim\n    ).transpose(1, 2)\n\n    if position_embeddings is None:\n        cos, sin = self.rotary_emb(value_states, position_ids)\n    else:\n        cos, sin = position_embeddings\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n    if past_key_value is not None:\n        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n        key_states, value_states = past_key_value.update(\n            key_states, value_states, self.layer_idx, cache_kwargs\n        )\n\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    if key_states.shape[2] == query_states.shape[2]:\n        if self.method == \"xattn\":\n            self.threshold = self.threshold.to(key_states.device)\n            threshold = self.threshold\n            attn_output = Xattention_prefill(\n                key_states,\n                query_states,\n                value_states,\n                norm=1,\n                stride=8,\n                threshold=threshold,\n                use_triton=True,\n                keep_sink=True,\n                keep_recent=True,\n            )\n        elif self.method == \"flex\":\n            attn_output = Flexprefill_prefill(\n                query_states.transpose(1, 2),\n                key_states.transpose(1, 2),\n                value_states.transpose(1, 2),\n                gamma=0.9,\n                tau=0.1,\n            ).transpose(1, 2)\n        elif self.method == \"minference\":\n            attn_output = Minference_prefill(\n                query_states, key_states, value_states\n            )\n        elif self.method == \"full\":\n            attn_output = flash_attn_func(\n                query_states.transpose(1, 2),\n                key_states.transpose(1, 2),\n                value_states.transpose(1, 2),\n                causal=True,\n            ).transpose(1, 2)\n    else:\n        ########################################################################################################################\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.attention_dropout, training=self.training\n        )\n        attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    attn_output = attn_output.reshape(bsz, q_len, -1)\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef get_pred(\n    model,\n    tokenizer,\n    eos_token_ids,\n    data,\n    max_length,\n    max_gen,\n    prompt_format,\n    dataset,\n    model_name,\n):\n    preds = []\n    pbar = tqdm(data)\n    for idx, json_obj in enumerate(pbar):\n        prompt = prompt_format.format(**json_obj)\n        # truncate to fit max_length (we suggest truncate in the middle, since the left and right side may contain crucial instructions)\n        tokenized_prompt = tokenizer(\n            prompt, truncation=False, return_tensors=\"pt\"\n        ).input_ids[0]\n        if len(tokenized_prompt) > max_length:\n            half = int(max_length / 2)\n            prompt = tokenizer.decode(\n                tokenized_prompt[:half], skip_special_tokens=True\n            ) + tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)\n        if dataset not in [\n            \"trec\",\n            \"triviaqa\",\n            \"samsum\",\n            \"lsht\",\n            \"lcc\",\n            \"repobench-p\",\n        ]:  # chat models are better off without build prompts on these tasks\n            prompt = build_chat(tokenizer, prompt, model_name)\n\n        input = tokenizer(prompt, truncation=False, return_tensors=\"pt\").to(\"cuda\")\n        pbar.set_description(f\"Generating for {idx}, len = {input.input_ids.shape[-1]}\")\n        with torch.no_grad():\n            output = model(\n                input_ids=input.input_ids,\n                past_key_values=None,\n                use_cache=True,\n                num_logits_to_keep=1,\n            )\n            past_key_values = output.past_key_values\n            pred_token_idx = output.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n            generated_content = [pred_token_idx.item()]\n            for _ in range(max_gen - 1):\n                outputs = model(\n                    input_ids=pred_token_idx,\n                    past_key_values=past_key_values,\n                    use_cache=True,\n                    num_logits_to_keep=1,\n                )\n\n                past_key_values = outputs.past_key_values\n                pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n                generated_content += [pred_token_idx.item()]\n                if pred_token_idx.item() in eos_token_ids:\n                    break\n\n        pred = tokenizer.decode(generated_content, skip_special_tokens=True)\n        pred = post_process(pred, model_name)\n        print(f\"Prediction: {pred}\")\n        preds.append(\n            {\n                \"pred\": pred,\n                \"answers\": json_obj[\"answers\"],\n                \"all_classes\": json_obj[\"all_classes\"],\n                \"length\": json_obj[\"length\"],\n            }\n        )\n    return preds\n\n\ndef seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\n\n\ndef load_model_and_tokenizer(path, model_name):\n    tokenizer = AutoTokenizer.from_pretrained(\n        path, trust_remote_code=True, use_fast=False\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        path,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,\n        device_map=\"auto\",\n        attn_implementation=\"eager\",\n    )\n\n    generation_config = GenerationConfig.from_pretrained(path)\n    eos_token_ids = generation_config.eos_token_id\n    if not isinstance(eos_token_ids, list):\n        eos_token_ids = [eos_token_ids]\n\n    model = model.eval()\n\n    return model, tokenizer, eos_token_ids\n\n\nif __name__ == \"__main__\":\n    seed_everything(42)\n    args = parse_args()\n    model2path = json.load(open(\"eval/LongBench/config/model2path.json\", \"r\"))\n    model2maxlen = json.load(open(\"eval/LongBench/config/model2maxlen.json\", \"r\"))\n    device_list = [i for i in range(torch.cuda.device_count())]\n    model_name = args.model\n    # define your model\n    # breakpoint()\n    model, tokenizer, eos_token_ids = load_model_and_tokenizer(\n        model2path[model_name], model_name\n    )\n\n    for name, module in model.named_modules():\n        if name.split(\".\")[-1] == \"self_attn\":\n            layer_idx = int(name.split(\".\")[2])\n            module.method = args.method\n            if args.method == \"xattn\":\n                module.threshold = torch.tensor(max[layer_idx])\n            module.forward = types.MethodType(new_attention_forward, module)\n\n    max_length = model2maxlen[model_name]\n    if args.e:\n        datasets = [\n            \"qasper\",\n            \"multifieldqa_en\",\n            \"hotpotqa\",\n            \"2wikimqa\",\n            \"gov_report\",\n            \"multi_news\",\n            \"trec\",\n            \"triviaqa\",\n            \"samsum\",\n            \"passage_count\",\n            \"passage_retrieval_en\",\n            \"lcc\",\n            \"repobench-p\",\n        ]\n    else:\n        datasets = [args.task]\n    # we design specific prompt format and max generation length for each task, feel free to modify them to optimize model output\n    dataset2prompt = json.load(open(\"eval/LongBench/config/dataset2prompt.json\", \"r\"))\n    dataset2maxlen = json.load(open(\"eval/LongBench/config/dataset2maxlen.json\", \"r\"))\n    # predict on each dataset\n    if not os.path.exists(\"eval/LongBench/pred\"):\n        os.makedirs(\"eval/LongBench/pred\")\n    if not os.path.exists(\"eval/LongBench/pred_e\"):\n        os.makedirs(\"eval/LongBench/pred_e\")\n    for dataset in datasets:\n        data = load_dataset(\"THUDM/LongBench\", dataset, split=\"test\")\n        if not os.path.exists(f\"eval/LongBench/pred/{model_name}\"):\n            os.makedirs(f\"eval/LongBench/pred/{model_name}\")\n        if args.method == \"full\":\n            out_path = f\"eval/LongBench/pred/{model_name}/{dataset}-full.jsonl\"\n        elif args.method == \"xattn\":\n            out_path = f\"eval/LongBench/pred/{model_name}/{dataset}-xattn-stride=8.jsonl\"\n        elif args.method == \"flex\":\n            out_path = f\"eval/LongBench/pred/{model_name}/{dataset}-flex.jsonl\"\n        elif args.method == \"minference\":\n            out_path = f\"eval/LongBench/pred/{model_name}/{dataset}-minference.jsonl\"\n        prompt_format = dataset2prompt[dataset]\n        max_gen = dataset2maxlen[dataset]\n        preds = get_pred(\n            model,\n            tokenizer,\n            eos_token_ids,\n            data,\n            max_length,\n            max_gen,\n            prompt_format,\n            dataset,\n            model_name,\n        )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            for pred in preds:\n                json.dump(pred, f, ensure_ascii=False)\n                f.write(\"\\n\")\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/docs/zh-CN/conf.py", "content": "# flake8: noqa\n# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport ast\nimport subprocess\nimport sys\n\nimport pytorch_sphinx_theme\nfrom sphinx.builders.html import StandaloneHTMLBuilder\n\nsys.path.insert(0, os.path.abspath('../../'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'VLMEvalKit'\ncopyright = '2023, VLMEvalKit'\nauthor = 'VLMEvalKit Authors'\n\n# The full version, including alpha/beta/rc tags\nversion_file = '../../vlmeval/__init__.py'\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        file_content = f.read()\n    # Parse the file content into an abstract syntax tree (AST)\n    tree = ast.parse(file_content, filename=version_file)\n\n    # Iterate through the body of the AST, looking for an assignment to __version__\n    for node in tree.body:\n        if isinstance(node, ast.Assign):\n            for target in node.targets:\n                if isinstance(target, ast.Name) and target.id == '__version__':\n                    return node.value.s\n    raise ValueError('__version__ not found')\n\n\nrelease = get_version()\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'myst_parser',\n    'sphinx_copybutton',\n    'sphinx_tabs.tabs',\n    'notfound.extension',\n    'sphinxcontrib.jquery',\n    'sphinx_design',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = {\n    '.rst': 'restructuredtext',\n    '.md': 'markdown',\n}\n\nlanguage = 'cn'\n\n# The master toctree document.\nroot_doc = 'index'\nhtml_context = {\n    'github_version': 'latest',\n}\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'pytorch_sphinx_theme'\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# yapf: disable\nhtml_theme_options = {\n    'menu': [\n        {\n            'name': 'GitHub',\n            'url': 'https://github.com/open-compass/VLMEvalKit'\n        },\n    ],\n    # Specify the language of shared menu\n    'menu_lang': 'cn',\n    # Disable the default edit on GitHub\n    'default_edit_on_github': False,\n}\n# yapf: enable\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\nhtml_css_files = [\n    'https://cdn.datatables.net/v/bs4/dt-1.12.1/datatables.min.css',\n    'css/readthedocs.css'\n]\nhtml_js_files = [\n    'https://cdn.datatables.net/v/bs4/dt-1.12.1/datatables.min.js',\n    'js/custom.js'\n]\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'vlmevalkitdoc'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (root_doc, 'vlmevalkit.tex', 'VLMEvalKit Documentation', author,\n     'manual'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(root_doc, 'vlmevalkit', 'VLMEvalKit Documentation', [author],\n              1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (root_doc, 'vlmevalkit', 'VLMEvalKit Documentation', author,\n     'VLMEvalKit Authors', 'AGI evaluation toolbox and benchmark.',\n     'Miscellaneous'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = ''\n\n# A unique identification for the text.\n#\n# epub_uid = ''\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = ['search.html']\n\n# set priority when building html\nStandaloneHTMLBuilder.supported_image_types = [\n    'image/svg+xml', 'image/gif', 'image/png', 'image/jpeg'\n]\n\n# -- Extension configuration -------------------------------------------------\n# Ignore >>> when copying code\ncopybutton_prompt_text = r'>>> |\\.\\.\\. '\ncopybutton_prompt_is_regexp = True\n\n# Auto-generated header anchors\nmyst_heading_anchors = 3\n# Enable \"colon_fence\" extension of myst.\nmyst_enable_extensions = ['colon_fence', 'dollarmath']\n\n# Configuration for intersphinx\nintersphinx_mapping = {\n    'python': ('https://docs.python.org/3', None),\n    'numpy': ('https://numpy.org/doc/stable', None),\n    'torch': ('https://pytorch.org/docs/stable/', None),\n    'mmengine': ('https://mmengine.readthedocs.io/en/latest/', None),\n    'transformers':\n    ('https://huggingface.co/docs/transformers/main/en/', None),\n}\nnapoleon_custom_sections = [\n    # Custom sections for data elements.\n    ('Meta fields', 'params_style'),\n    ('Data fields', 'params_style'),\n]\n\n# Disable docstring inheritance\nautodoc_inherit_docstrings = False\n# Mock some imports during generate API docs.\nautodoc_mock_imports = ['rich', 'attr', 'einops']\n# Disable displaying type annotations, these can be very verbose\nautodoc_typehints = 'none'\n\n# The not found page\nnotfound_template = '404.html'\n\n\ndef builder_inited_handler(app):\n    subprocess.run(['./cp_origin_docs.sh'])\n\n\ndef setup(app):\n    app.connect('builder-inited', builder_inited_handler)\n"}
{"type": "source_file", "path": "eval/RULER/scripts/pred/model_wrappers.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport requests\nimport torch\nfrom typing import Dict, List, Optional\nfrom xattn.src.load_llama import load_model, FastPrefillConfig\n\nclass HuggingFaceModel:\n    def __init__(self, name_or_path: str,fastprefillconfig:FastPrefillConfig, **generation_kwargs) -> None:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n        self.tokenizer = AutoTokenizer.from_pretrained(name_or_path, trust_remote_code=True)\n\n        if 'Yarn-Llama' in name_or_path:\n            model_kwargs = None\n        else:\n            model_kwargs = {\"attn_implementation\": \"flash_attention_2\"}\n        if \"Llama-3.1-8B-Instruct\" in name_or_path:\n            self.pipeline = None\n            self.model,_ = load_model(fastprefillconfig,name_or_path = name_or_path)\n        else:\n            try:\n                self.pipeline = pipeline(\n                    \"text-generation\",\n                    model=name_or_path,\n                    tokenizer=self.tokenizer,\n                    trust_remote_code=True,\n                    device_map=\"auto\",\n                    torch_dtype=torch.bfloat16,\n                    model_kwargs=model_kwargs,\n                )\n            except:\n                self.pipeline = None\n                self.model = AutoModelForCausalLM.from_pretrained(name_or_path, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16,)\n            \n        self.generation_kwargs = generation_kwargs\n        self.stop = self.generation_kwargs.pop('stop')\n\n        if self.tokenizer.pad_token is None:\n            # add pad token to allow batching (known issue for llama2)\n            self.tokenizer.padding_side = 'left'\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n\n    def __call__(self, prompt: str, **kwargs) -> dict:\n        return self.process_batch([prompt], **kwargs)[0]\n\n    def process_batch(self, prompts: List[str], **kwargs) -> List[dict]:\n        if self.pipeline is None:\n            inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(self.model.device)\n\n            generated_ids = self.model.generate(\n                **inputs,\n                **self.generation_kwargs\n            )\n            generated_texts = self.tokenizer.batch_decode(generated_ids[:,inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n        else:\n            output = self.pipeline(text_inputs=prompts, **self.generation_kwargs, )\n            assert len(output) == len(prompts)\n            # output in the form of a list of list of dictionaries\n            # outer list len = batch size\n            # inner list len = 1\n            generated_texts = [llm_result[0][\"generated_text\"] for llm_result in output]\n\n        results = []\n\n        for text, prompt in zip(generated_texts, prompts):\n            # remove the input form the generated text\n            if text.startswith(prompt):\n                text = text[len(prompt):]\n\n            if self.stop is not None:\n                for s in self.stop:\n                    text = text.split(s)[0]\n\n            results.append({'text': [text]})\n\n        return results\n\n\nclass MambaModel:\n    def __init__(self, name_or_path: str, **generation_kwargs) -> None:\n        from transformers import AutoTokenizer\n        from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n        self.device = \"cuda\"\n        self.model = MambaLMHeadModel.from_pretrained(name_or_path, device=self.device, dtype=torch.bfloat16)\n        self.generation_kwargs = generation_kwargs\n        self.stop = self.generation_kwargs.pop('stop')\n        self.max_genlen = self.generation_kwargs.pop('max_new_tokens')\n        self.minp = 0.0\n\n    def __call__(self, prompt: str, **kwargs) -> Dict[str, List[str]]:\n        # tokenize\n        tokens = self.tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = tokens.input_ids.to(self.device)\n        max_length = input_ids.shape[1] + self.max_genlen\n\n        # generate\n        out = self.model.generate(\n            input_ids=input_ids,\n            max_length=max_length,\n            cg=True,\n            return_dict_in_generate=True,\n            output_scores=True,\n            enable_timing=False,\n            **self.generation_kwargs,\n        )\n        assert len(out.sequences) == 1\n        # detok\n        return {'text': [self.tokenizer.decode(out.sequences[0][input_ids.shape[1]:])]}\n\n    def process_batch(self, prompts: List[str], **kwargs) -> List[dict]:\n        # FIXME: naive implementation\n        return [self.__call__(prompt, **kwargs) for prompt in prompts]\n"}
{"type": "source_file", "path": "eval/LongBench/ratio.py", "content": "max_ratio = [\n    [\n        0.5993353724479675,\n        0.602241575717926,\n        0.5930868983268738,\n        0.6021507978439331,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.9650202989578247,\n        0.9649718999862671,\n        0.9637318849563599,\n        0.9648846983909607,\n        0.6806816458702087,\n        0.6808730363845825,\n        0.6808798909187317,\n        0.6813652515411377,\n        0.024773599579930305,\n        0.02480248548090458,\n        0.024748198688030243,\n        0.024731513112783432,\n        0.9649331569671631,\n        0.9643228054046631,\n        0.9653981328010559,\n        0.9649524688720703,\n        0.03959713876247406,\n        0.0396265871822834,\n        0.0395875908434391,\n        0.03962937369942665,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n    ],\n    [\n        0.9066047072410583,\n        0.8639519810676575,\n        0.9496957063674927,\n        0.9289669394493103,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.7096781134605408,\n        0.8022949695587158,\n        0.76171875,\n        0.73836749792099,\n        0.98580002784729,\n        0.8159999847412109,\n        0.8901200294494629,\n        0.8124300241470337,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.5671455264091492,\n        0.5991567373275757,\n        0.5987281203269958,\n        0.6125350594520569,\n        0.344654381275177,\n        0.24141812324523926,\n        0.3685743808746338,\n        0.3692203164100647,\n        0.9758399724960327,\n        0.9478899836540222,\n        0.836359977722168,\n        0.9734600186347961,\n    ],\n    [\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.9241433143615723,\n        0.9389864206314087,\n        0.9765208959579468,\n        0.933340847492218,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.7349218726158142,\n        0.8973633050918579,\n        0.9679394364356995,\n        0.964062511920929,\n        0.923259973526001,\n        0.8877500295639038,\n        0.912850022315979,\n        0.9871500134468079,\n        0.9817596077919006,\n        0.956230640411377,\n        0.9879607558250427,\n        0.8370887637138367,\n        0.4004043936729431,\n        0.37780749797821045,\n        0.37746623158454895,\n        0.41707751154899597,\n        0.4882138669490814,\n        0.46682071685791016,\n        0.47732752561569214,\n        0.4926545023918152,\n    ],\n    [\n        0.82591313123703,\n        0.8110293745994568,\n        0.7938928008079529,\n        0.7050543427467346,\n        0.06683176010847092,\n        0.06734757870435715,\n        0.06408195197582245,\n        0.06429866701364517,\n        0.2142985612154007,\n        0.19336950778961182,\n        0.21132007241249084,\n        0.21186617016792297,\n        0.1901717483997345,\n        0.17386434972286224,\n        0.15815365314483643,\n        0.17568421363830566,\n        0.7542555928230286,\n        0.8005370497703552,\n        0.7751709818840027,\n        0.715983510017395,\n        0.9853299856185913,\n        0.924019992351532,\n        0.9881899952888489,\n        0.9759799838066101,\n        0.2780669033527374,\n        0.26591747999191284,\n        0.2948375940322876,\n        0.3202289044857025,\n        0.8427152037620544,\n        0.858826756477356,\n        0.8317914009094238,\n        0.8444072604179382,\n    ],\n    [\n        0.8597891330718994,\n        0.8766722083091736,\n        0.7316316366195679,\n        0.7838770747184753,\n        0.34049034118652344,\n        0.34266191720962524,\n        0.35350897908210754,\n        0.33540645241737366,\n        0.9231906533241272,\n        0.8688599467277527,\n        0.9182140827178955,\n        0.8155341744422913,\n        0.7813898324966431,\n        0.8487477898597717,\n        0.7928344011306763,\n        0.8626237511634827,\n        0.9873654842376709,\n        0.9456440210342407,\n        0.9471322894096375,\n        0.9875142574310303,\n        0.8733286261558533,\n        0.8856229782104492,\n        0.9021873474121094,\n        0.8830842971801758,\n        0.9219810962677002,\n        0.9133134484291077,\n        0.9216927886009216,\n        0.8776146173477173,\n        0.2548667788505554,\n        0.2771099805831909,\n        0.22655166685581207,\n        0.2585674524307251,\n    ],\n    [\n        0.4931042194366455,\n        0.5354232788085938,\n        0.5021478533744812,\n        0.5269941091537476,\n        0.8793501257896423,\n        0.8818151950836182,\n        0.8618728518486023,\n        0.8814959526062012,\n        0.9640091061592102,\n        0.9350919127464294,\n        0.9365150332450867,\n        0.9589391350746155,\n        0.6750763654708862,\n        0.7435914874076843,\n        0.7549640536308289,\n        0.7155293822288513,\n        0.7537218928337097,\n        0.8367806077003479,\n        0.8292543292045593,\n        0.8033512830734253,\n        0.8016625046730042,\n        0.9040459394454956,\n        0.8976112604141235,\n        0.9058712720870972,\n        0.9409739375114441,\n        0.875739336013794,\n        0.9152817130088806,\n        0.925548791885376,\n        0.8266302943229675,\n        0.8206713199615479,\n        0.8003489971160889,\n        0.8281222581863403,\n    ],\n    [\n        0.8155418038368225,\n        0.8244001865386963,\n        0.9138103127479553,\n        0.9121396541595459,\n        0.8337589502334595,\n        0.88116055727005,\n        0.8867039084434509,\n        0.8395898342132568,\n        0.0,\n        0.0,\n        0.0,\n        0.0,\n        0.5157405734062195,\n        0.5187257528305054,\n        0.5365656018257141,\n        0.523423969745636,\n        0.7471420168876648,\n        0.730463445186615,\n        0.7700362205505371,\n        0.7213152647018433,\n        0.9849900007247925,\n        0.9950299859046936,\n        0.9926599860191345,\n        0.9396700263023376,\n        0.9950100183486938,\n        0.9959499835968018,\n        0.9958800077438354,\n        0.9641299843788147,\n        0.9124976396560669,\n        0.9119377136230469,\n        0.818185567855835,\n        0.9124792814254761,\n    ],\n    [\n        0.9230819344520569,\n        0.9455934762954712,\n        0.9445737600326538,\n        0.9765370488166809,\n        0.8927888870239258,\n        0.815413236618042,\n        0.9120386838912964,\n        0.9114695191383362,\n        0.849533200263977,\n        0.8473095893859863,\n        0.859930694103241,\n        0.8266991972923279,\n        0.9465500116348267,\n        0.9799500107765198,\n        0.9240599870681763,\n        0.9531400203704834,\n        0.9590187072753906,\n        0.9320014119148254,\n        0.9591377377510071,\n        0.9248576760292053,\n        0.9609900116920471,\n        0.9906799793243408,\n        0.992579996585846,\n        0.99413001537323,\n        0.9348437190055847,\n        0.9651947021484375,\n        0.9599634408950806,\n        0.9420803189277649,\n        0.9586700201034546,\n        0.9438300132751465,\n        0.9530100226402283,\n        0.957539975643158,\n    ],\n    [\n        0.9623100161552429,\n        0.894569993019104,\n        0.9834100008010864,\n        0.9929999709129333,\n        0.9101200103759766,\n        0.9764699935913086,\n        0.8794599771499634,\n        0.9640899896621704,\n        0.9950399994850159,\n        0.9961199760437012,\n        0.9918699860572815,\n        0.9736899733543396,\n        0.5551574230194092,\n        0.5595130920410156,\n        0.5571058392524719,\n        0.5458909869194031,\n        0.9195073843002319,\n        0.9140129685401917,\n        0.9159188270568848,\n        0.9196561574935913,\n        0.8942371606826782,\n        0.8861799836158752,\n        0.8712732791900635,\n        0.9067838788032532,\n        0.9033271074295044,\n        0.9026937484741211,\n        0.9099916219711304,\n        0.9067144393920898,\n        0.9761456251144409,\n        0.9776123762130737,\n        0.9792365431785583,\n        0.9800437092781067,\n    ],\n    [\n        0.9666063189506531,\n        0.9584360122680664,\n        0.9669370055198669,\n        0.9659448862075806,\n        0.978951096534729,\n        0.9737240672111511,\n        0.9544991850852966,\n        0.9627777934074402,\n        0.9115157723426819,\n        0.9021185636520386,\n        0.9268646836280823,\n        0.9153031706809998,\n        0.9952300190925598,\n        0.9949100017547607,\n        0.9924299716949463,\n        0.9948999881744385,\n        0.7813998460769653,\n        0.793059766292572,\n        0.7943150997161865,\n        0.7961176037788391,\n        0.9889816641807556,\n        0.9836924076080322,\n        0.9831345677375793,\n        0.9828058481216431,\n        0.9383500218391418,\n        0.9957699775695801,\n        0.9529100060462952,\n        0.9896699786186218,\n        0.9904857277870178,\n        0.9855750203132629,\n        0.975932776927948,\n        0.9907447099685669,\n    ],\n    [\n        0.9811699986457825,\n        0.9968699812889099,\n        0.9556699991226196,\n        0.9221000075340271,\n        0.8688892722129822,\n        0.9447264671325684,\n        0.9552872180938721,\n        0.9551334381103516,\n        0.7961675524711609,\n        0.7705650329589844,\n        0.8065032362937927,\n        0.7681076526641846,\n        0.9606536626815796,\n        0.9668592214584351,\n        0.9664895534515381,\n        0.9344308376312256,\n        0.8156399130821228,\n        0.8619211316108704,\n        0.8674787282943726,\n        0.8662069439888,\n        0.8100339770317078,\n        0.8070398569107056,\n        0.8124210834503174,\n        0.7992550730705261,\n        0.9906600117683411,\n        0.98785001039505,\n        0.9963700175285339,\n        0.9716600179672241,\n        0.9891899824142456,\n        0.9913700222969055,\n        0.9861500263214111,\n        0.8874099850654602,\n    ],\n    [\n        0.9176991581916809,\n        0.9179584383964539,\n        0.9211986660957336,\n        0.920967161655426,\n        0.9043499827384949,\n        0.9959800243377686,\n        0.9963099956512451,\n        0.9961100220680237,\n        0.9544427394866943,\n        0.9585626125335693,\n        0.9498114585876465,\n        0.9601932168006897,\n        0.9801300168037415,\n        0.9937999844551086,\n        0.992609977722168,\n        0.9911699891090393,\n        0.963670015335083,\n        0.9811199903488159,\n        0.9955199956893921,\n        0.9897800087928772,\n        0.8856731653213501,\n        0.8835623860359192,\n        0.8852812647819519,\n        0.8848092555999756,\n        0.808803141117096,\n        0.8075844049453735,\n        0.8083562254905701,\n        0.8089006543159485,\n        0.9457643032073975,\n        0.9539610743522644,\n        0.9570745825767517,\n        0.8853117227554321,\n    ],\n    [\n        0.95321124792099,\n        0.9643712639808655,\n        0.9468852877616882,\n        0.9535697102546692,\n        0.6656399965286255,\n        0.6644507646560669,\n        0.6648069024085999,\n        0.6628718376159668,\n        0.9240722060203552,\n        0.9196003675460815,\n        0.924713671207428,\n        0.9239420294761658,\n        0.9393201470375061,\n        0.9476578235626221,\n        0.9766181707382202,\n        0.9668334126472473,\n        0.7595847845077515,\n        0.7470477223396301,\n        0.7623770236968994,\n        0.7801123261451721,\n        0.9212459325790405,\n        0.908128023147583,\n        0.8656227588653564,\n        0.9207253456115723,\n        0.5977407097816467,\n        0.6010094881057739,\n        0.6006660461425781,\n        0.606559693813324,\n        0.8976642489433289,\n        0.896923840045929,\n        0.9052234888076782,\n        0.9003058671951294,\n    ],\n    [\n        0.9899299740791321,\n        0.9871900081634521,\n        0.9914399981498718,\n        0.9829300045967102,\n        0.986810028553009,\n        0.9629200100898743,\n        0.9282699823379517,\n        0.9718800187110901,\n        0.9935100078582764,\n        0.9896799921989441,\n        0.992929995059967,\n        0.986519992351532,\n        0.953036904335022,\n        0.9532596468925476,\n        0.9559528231620789,\n        0.9564952850341797,\n        0.9866213202476501,\n        0.9869189858436584,\n        0.9702304005622864,\n        0.9857779741287231,\n        0.9548000693321228,\n        0.9356514811515808,\n        0.9179736375808716,\n        0.9649087190628052,\n        0.9630699753761292,\n        0.9875699877738953,\n        0.9930400252342224,\n        0.9935200214385986,\n        0.9758109450340271,\n        0.9226251244544983,\n        0.9729464054107666,\n        0.9812250137329102,\n    ],\n    [\n        0.9061406254768372,\n        0.9130687713623047,\n        0.9092999696731567,\n        0.9066937565803528,\n        0.9188056588172913,\n        0.9250047206878662,\n        0.9192724227905273,\n        0.9252848029136658,\n        0.781306266784668,\n        0.7861109375953674,\n        0.7890257835388184,\n        0.7832761406898499,\n        0.9757992625236511,\n        0.971589207649231,\n        0.9812842011451721,\n        0.9621807336807251,\n        0.970166027545929,\n        0.9575976133346558,\n        0.9682421684265137,\n        0.9693359732627869,\n        0.9950900077819824,\n        0.9876000285148621,\n        0.9915500283241272,\n        0.9898599982261658,\n        0.8516160845756531,\n        0.8405793309211731,\n        0.8460758924484253,\n        0.8602051138877869,\n        0.9920600056648254,\n        0.9920300245285034,\n        0.9866999983787537,\n        0.9871100187301636,\n    ],\n    [\n        0.9464499950408936,\n        0.9104400277137756,\n        0.9680299758911133,\n        0.9804700016975403,\n        0.8940896987915039,\n        0.9190909266471863,\n        0.9171550273895264,\n        0.8865764141082764,\n        0.9929599761962891,\n        0.9910200238227844,\n        0.9936299920082092,\n        0.995140016078949,\n        0.7456770539283752,\n        0.7654633522033691,\n        0.7739212512969971,\n        0.7675899267196655,\n        0.9901400208473206,\n        0.9965900182723999,\n        0.994629979133606,\n        0.9515900015830994,\n        0.8152559399604797,\n        0.8220879435539246,\n        0.7996209263801575,\n        0.8118771910667419,\n        0.9922599792480469,\n        0.9887099862098694,\n        0.9830600023269653,\n        0.9836099743843079,\n        0.9922699928283691,\n        0.9935700297355652,\n        0.9954000115394592,\n        0.9636399745941162,\n    ],\n    [\n        0.9920399785041809,\n        0.9151300191879272,\n        0.9884499907493591,\n        0.978380024433136,\n        0.9071823358535767,\n        0.911200225353241,\n        0.8933696746826172,\n        0.9136442542076111,\n        0.9691100120544434,\n        0.9517899751663208,\n        0.9425399899482727,\n        0.8750200271606445,\n        0.8828250169754028,\n        0.9105937480926514,\n        0.8494406342506409,\n        0.9195656180381775,\n        0.9154126644134521,\n        0.914168119430542,\n        0.9079177975654602,\n        0.9155786037445068,\n        0.9898200035095215,\n        0.8692100048065186,\n        0.9693999886512756,\n        0.9878799915313721,\n        0.9727799892425537,\n        0.992389976978302,\n        0.9907699823379517,\n        0.9767900109291077,\n        0.8825470209121704,\n        0.866391122341156,\n        0.8691520094871521,\n        0.8840076327323914,\n    ],\n    [\n        0.9184234738349915,\n        0.9126303195953369,\n        0.9476603269577026,\n        0.9487065672874451,\n        0.9700199961662292,\n        0.9702699780464172,\n        0.9889000058174133,\n        0.9665300250053406,\n        0.9487310647964478,\n        0.9345390200614929,\n        0.9468629956245422,\n        0.9199085831642151,\n        0.9155698418617249,\n        0.915838360786438,\n        0.9042753577232361,\n        0.9189304709434509,\n        0.8237425088882446,\n        0.7008574604988098,\n        0.8570537567138672,\n        0.7978337407112122,\n        0.8758944869041443,\n        0.9512993097305298,\n        0.9454663991928101,\n        0.9428238272666931,\n        0.9901700019836426,\n        0.9932500123977661,\n        0.9400399923324585,\n        0.9892600178718567,\n        0.8885460495948792,\n        0.9329332113265991,\n        0.8595958948135376,\n        0.9125101566314697,\n    ],\n    [\n        0.6181601881980896,\n        0.49986428022384644,\n        0.6728955507278442,\n        0.5341191291809082,\n        0.9729899764060974,\n        0.7979300022125244,\n        0.7174299955368042,\n        0.9328200221061707,\n        0.8668688535690308,\n        0.85232013463974,\n        0.8371005058288574,\n        0.8075615763664246,\n        0.971530020236969,\n        0.7348300218582153,\n        0.9894400238990784,\n        0.8841599822044373,\n        0.8887702822685242,\n        0.7527109384536743,\n        0.8622574210166931,\n        0.874413251876831,\n        0.9742820262908936,\n        0.8818140029907227,\n        0.9751741886138916,\n        0.9733701348304749,\n        0.738156259059906,\n        0.6941484212875366,\n        0.7472109794616699,\n        0.7088202834129333,\n        0.9380748867988586,\n        0.926485002040863,\n        0.8769357204437256,\n        0.7918002605438232,\n    ],\n    [\n        0.9800043702125549,\n        0.9570782780647278,\n        0.9796991944313049,\n        0.9747379422187805,\n        0.7245123386383057,\n        0.6522718667984009,\n        0.7196875214576721,\n        0.6617085933685303,\n        0.8719210624694824,\n        0.9341951012611389,\n        0.8309416174888611,\n        0.9235760569572449,\n        0.9507148265838623,\n        0.9532701373100281,\n        0.9514996409416199,\n        0.9490783214569092,\n        0.18344593048095703,\n        0.17473094165325165,\n        0.1739324927330017,\n        0.20848843455314636,\n        0.9454280138015747,\n        0.7637627124786377,\n        0.9008885025978088,\n        0.9520295858383179,\n        0.6062999963760376,\n        0.7391024827957153,\n        0.6608175039291382,\n        0.6847424507141113,\n        0.7837268114089966,\n        0.692814826965332,\n        0.919684648513794,\n        0.7394616603851318,\n    ],\n    [\n        0.9885433912277222,\n        0.9883042573928833,\n        0.9765304327011108,\n        0.9868400692939758,\n        0.7789472937583923,\n        0.8008142113685608,\n        0.7858707904815674,\n        0.7357487082481384,\n        0.9225199818611145,\n        0.9765499830245972,\n        0.8905500173568726,\n        0.9876700043678284,\n        0.9783309698104858,\n        0.9763227701187134,\n        0.9775828123092651,\n        0.9637523293495178,\n        0.7481836676597595,\n        0.7207987904548645,\n        0.7591567039489746,\n        0.6446049809455872,\n        0.8760684132575989,\n        0.8509193658828735,\n        0.9465739727020264,\n        0.9602416753768921,\n        0.9749730229377747,\n        0.981799304485321,\n        0.9847163558006287,\n        0.9781083464622498,\n        0.29843851923942566,\n        0.3088151514530182,\n        0.32476019859313965,\n        0.29320797324180603,\n    ],\n    [\n        0.8592319488525391,\n        0.9540278315544128,\n        0.9596529006958008,\n        0.9541243314743042,\n        0.6704715490341187,\n        0.6786149740219116,\n        0.6654834747314453,\n        0.663823127746582,\n        0.9336867332458496,\n        0.8866729140281677,\n        0.9072896838188171,\n        0.8982521891593933,\n        0.9035500288009644,\n        0.9037299752235413,\n        0.9822700023651123,\n        0.9133800268173218,\n        0.7061752676963806,\n        0.7056728005409241,\n        0.8069406151771545,\n        0.774598240852356,\n        0.8376469016075134,\n        0.902984082698822,\n        0.8772257566452026,\n        0.8695567846298218,\n        0.7288711071014404,\n        0.7922971844673157,\n        0.8437994122505188,\n        0.8307309746742249,\n        0.6700401306152344,\n        0.7225427627563477,\n        0.7543671131134033,\n        0.7437530755996704,\n    ],\n    [\n        0.8490002751350403,\n        0.8233713507652283,\n        0.9201821684837341,\n        0.8174102306365967,\n        0.9465907216072083,\n        0.811069667339325,\n        0.9489930272102356,\n        0.8737708926200867,\n        0.8322802782058716,\n        0.837815523147583,\n        0.81851726770401,\n        0.8637349009513855,\n        0.9398244023323059,\n        0.9449045062065125,\n        0.986262321472168,\n        0.983921468257904,\n        0.7473253011703491,\n        0.7565371990203857,\n        0.7876949906349182,\n        0.7298657894134521,\n        0.7602499723434448,\n        0.7345100045204163,\n        0.7663499712944031,\n        0.7821000218391418,\n        0.7539673447608948,\n        0.6876343488693237,\n        0.6596519351005554,\n        0.8271356225013733,\n        0.9504468441009521,\n        0.9606240391731262,\n        0.8807746767997742,\n        0.881500244140625,\n    ],\n    [\n        0.9781100153923035,\n        0.7062199711799622,\n        0.9416900277137756,\n        0.7704100012779236,\n        0.9377400279045105,\n        0.875029981136322,\n        0.9606800079345703,\n        0.9678599834442139,\n        0.3203396797180176,\n        0.3235992193222046,\n        0.3091810941696167,\n        0.2984393835067749,\n        0.8623570203781128,\n        0.9087916612625122,\n        0.9489136934280396,\n        0.8506364822387695,\n        0.6460171937942505,\n        0.5953429937362671,\n        0.5743547081947327,\n        0.589605450630188,\n        0.9134288430213928,\n        0.9004032611846924,\n        0.9389857649803162,\n        0.8480737805366516,\n        0.952668309211731,\n        0.9378337264060974,\n        0.9182151556015015,\n        0.9202232956886292,\n        0.5282987952232361,\n        0.5255273580551147,\n        0.5388808846473694,\n        0.5489824414253235,\n    ],\n    [\n        0.6918653249740601,\n        0.8146348595619202,\n        0.7112185955047607,\n        0.8053349852561951,\n        0.7237914800643921,\n        0.7061617374420166,\n        0.6915600299835205,\n        0.8446396589279175,\n        0.5181649327278137,\n        0.5149579644203186,\n        0.5138114094734192,\n        0.4882602095603943,\n        0.7889586687088013,\n        0.7840683460235596,\n        0.8096545338630676,\n        0.7403008341789246,\n        0.8328443765640259,\n        0.8646571636199951,\n        0.8515130281448364,\n        0.9164035320281982,\n        0.9373592138290405,\n        0.8030269742012024,\n        0.8444210290908813,\n        0.9003307819366455,\n        0.9821799993515015,\n        0.907729983329773,\n        0.9880200028419495,\n        0.9945799708366394,\n        0.00040074859862215817,\n        0.0003984768991358578,\n        0.0004005115188192576,\n        0.0003678543434944004,\n    ],\n    [\n        0.7543484568595886,\n        0.8894971013069153,\n        0.9232540726661682,\n        0.9199815988540649,\n        0.8633420467376709,\n        0.8647906184196472,\n        0.8732738494873047,\n        0.8390929698944092,\n        0.43321749567985535,\n        0.4496016800403595,\n        0.5122689604759216,\n        0.5177749991416931,\n        0.8454399704933167,\n        0.8708199858665466,\n        0.8175399899482727,\n        0.8254899978637695,\n        0.7836523652076721,\n        0.93907231092453,\n        0.9671190977096558,\n        0.7260058522224426,\n        0.7907624840736389,\n        0.6764343976974487,\n        0.7996134757995605,\n        0.8151637315750122,\n        0.8042817115783691,\n        0.7384772300720215,\n        0.7404398322105408,\n        0.7350610494613647,\n        0.8247370719909668,\n        0.8281571865081787,\n        0.8095906376838684,\n        0.8112684488296509,\n    ],\n    [\n        0.9870380163192749,\n        0.974219024181366,\n        0.982970118522644,\n        0.9860755801200867,\n        0.8389593362808228,\n        0.8089077472686768,\n        0.8265776038169861,\n        0.8308610320091248,\n        0.702171802520752,\n        0.7081586718559265,\n        0.7165940999984741,\n        0.6709877252578735,\n        0.9153326749801636,\n        0.918616771697998,\n        0.9007971286773682,\n        0.9801622033119202,\n        0.8165445327758789,\n        0.8543828129768372,\n        0.8913192749023438,\n        0.9242674112319946,\n        0.7306129932403564,\n        0.7495315670967102,\n        0.747403085231781,\n        0.7542401552200317,\n        0.6658870577812195,\n        0.677182674407959,\n        0.69656902551651,\n        0.8206967115402222,\n        0.9712926745414734,\n        0.9021009206771851,\n        0.9734877943992615,\n        0.9600609540939331,\n    ],\n    [\n        0.5536054968833923,\n        0.5817726254463196,\n        0.5801554918289185,\n        0.5395429730415344,\n        0.9791004061698914,\n        0.9850471019744873,\n        0.9824174046516418,\n        0.9869396090507507,\n        0.7255606651306152,\n        0.6709057092666626,\n        0.6347447037696838,\n        0.6924487352371216,\n        0.8722781538963318,\n        0.9145553112030029,\n        0.9011697173118591,\n        0.9097983837127686,\n        0.9362850189208984,\n        0.6866334676742554,\n        0.7966650128364563,\n        0.7942173480987549,\n        0.9493600130081177,\n        0.993149995803833,\n        0.9104999899864197,\n        0.9844800233840942,\n        0.8892599940299988,\n        0.6459100246429443,\n        0.7403600215911865,\n        0.7409800291061401,\n        0.9575838446617126,\n        0.8905872702598572,\n        0.9527311325073242,\n        0.938413143157959,\n    ],\n    [\n        0.9680166840553284,\n        0.9398486018180847,\n        0.8974505066871643,\n        0.9653710722923279,\n        0.6357484459877014,\n        0.6827557682991028,\n        0.6096487641334534,\n        0.6602692008018494,\n        0.8722063302993774,\n        0.8689567446708679,\n        0.9203839302062988,\n        0.9132739305496216,\n        0.8386439681053162,\n        0.9616436958312988,\n        0.9531430602073669,\n        0.9738898277282715,\n        0.9270351529121399,\n        0.9699538946151733,\n        0.9795811176300049,\n        0.8415421843528748,\n        0.9515091776847839,\n        0.9494132399559021,\n        0.787301778793335,\n        0.9508296847343445,\n        0.9117500185966492,\n        0.9756600260734558,\n        0.9392099976539612,\n        0.978950023651123,\n        0.8594648241996765,\n        0.873043954372406,\n        0.8736679553985596,\n        0.8287822008132935,\n    ],\n    [\n        0.601993978023529,\n        0.5699103474617004,\n        0.6102738380432129,\n        0.5862492322921753,\n        0.5748213529586792,\n        0.5700160264968872,\n        0.5924245119094849,\n        0.6095852255821228,\n        0.9863100051879883,\n        0.9929800033569336,\n        0.9879000186920166,\n        0.9949100017547607,\n        0.06524296849966049,\n        0.06282958388328552,\n        0.06628869473934174,\n        0.06392592936754227,\n        0.981112539768219,\n        0.7746521234512329,\n        0.9231398701667786,\n        0.8567701578140259,\n        0.9879310131072998,\n        0.9867900013923645,\n        0.9840813279151917,\n        0.9861450791358948,\n        0.8517299890518188,\n        0.7500200271606445,\n        0.926829993724823,\n        0.8319900035858154,\n        0.9765999913215637,\n        0.9753000140190125,\n        0.9896199703216553,\n        0.9943100214004517,\n    ],\n    [\n        0.9715978503227234,\n        0.9788526892662048,\n        0.9803587794303894,\n        0.9622364044189453,\n        0.7173399925231934,\n        0.8274099826812744,\n        0.9957000017166138,\n        0.6710399985313416,\n        0.9807339906692505,\n        0.9895394444465637,\n        0.9561703205108643,\n        0.9867205023765564,\n        0.9816200137138367,\n        0.9838799834251404,\n        0.9815599918365479,\n        0.9925100207328796,\n        0.9940900206565857,\n        0.9922299981117249,\n        0.997439980506897,\n        0.9725300073623657,\n        0.9783334136009216,\n        0.9659021496772766,\n        0.955821692943573,\n        0.8859855532646179,\n        0.9952999949455261,\n        0.985040009021759,\n        0.9955300092697144,\n        0.9936699867248535,\n        0.9003900289535522,\n        0.9389500021934509,\n        0.966480016708374,\n        0.9753199815750122,\n    ],\n    [\n        0.8636746406555176,\n        0.830899715423584,\n        0.7854511141777039,\n        0.8012062907218933,\n        0.8258437514305115,\n        0.8666812181472778,\n        0.921375036239624,\n        0.9306562542915344,\n        0.6517374515533447,\n        0.7417076826095581,\n        0.7384781241416931,\n        0.766984760761261,\n        0.5165804624557495,\n        0.5368331670761108,\n        0.5379191637039185,\n        0.5117046236991882,\n        0.8910373449325562,\n        0.8815357089042664,\n        0.7964265942573547,\n        0.8969477415084839,\n        0.9715331792831421,\n        0.9693359732627869,\n        0.9540722966194153,\n        0.9667285084724426,\n        0.7468146085739136,\n        0.7853376865386963,\n        0.8104400038719177,\n        0.8106147646903992,\n        0.9146358370780945,\n        0.9214504957199097,\n        0.9213853478431702,\n        0.900922954082489,\n    ],\n]\nmax = [\n    [\n        0.989870011806488,\n        0.9946699738502502,\n        0.9795500040054321,\n        0.9945200085639954,\n        0.9944499731063843,\n        0.9942200183868408,\n        0.9937899708747864,\n        0.9945499897003174,\n        0.996150016784668,\n        0.9961000084877014,\n        0.9948199987411499,\n        0.9960100054740906,\n        0.9957399964332581,\n        0.9960200190544128,\n        0.9960299730300903,\n        0.9967399835586548,\n        0.9948300123214722,\n        0.995989978313446,\n        0.9938099980354309,\n        0.9931399822235107,\n        0.9960600137710571,\n        0.9954299926757812,\n        0.9965400099754333,\n        0.9960799813270569,\n        0.9950299859046936,\n        0.9957699775695801,\n        0.9947900176048279,\n        0.9958400130271912,\n        0.9932399988174438,\n        0.8057600259780884,\n        0.9557499885559082,\n        0.9831799864768982,\n    ],\n    [\n        0.9101600050926208,\n        0.8673400282859802,\n        0.9534199833869934,\n        0.9326099753379822,\n        0.8504800200462341,\n        0.8827499747276306,\n        0.8563799858093262,\n        0.8286399841308594,\n        0.873449981212616,\n        0.9874399900436401,\n        0.9375,\n        0.9087600111961365,\n        0.98580002784729,\n        0.8159999847412109,\n        0.8901200294494629,\n        0.8124300241470337,\n        0.889989972114563,\n        0.9480000138282776,\n        0.9048600196838379,\n        0.9239699840545654,\n        0.9131399989128113,\n        0.964680016040802,\n        0.9639899730682373,\n        0.9862200021743774,\n        0.8483800292015076,\n        0.5942599773406982,\n        0.907260000705719,\n        0.9088500142097473,\n        0.9758399724960327,\n        0.9478899836540222,\n        0.836359977722168,\n        0.9734600186347961,\n    ],\n    [\n        0.8934599757194519,\n        0.6090999841690063,\n        0.8747699856758118,\n        0.6320499777793884,\n        0.9314200282096863,\n        0.9463800191879272,\n        0.9842100143432617,\n        0.9406899809837341,\n        0.847000002861023,\n        0.8705599904060364,\n        0.7318099737167358,\n        0.8279299736022949,\n        0.752560019493103,\n        0.9189000129699707,\n        0.9911699891090393,\n        0.9872000217437744,\n        0.923259973526001,\n        0.8877500295639038,\n        0.912850022315979,\n        0.9871500134468079,\n        0.9894899725914001,\n        0.9637600183486938,\n        0.9957399964332581,\n        0.8436800241470337,\n        0.9152100086212158,\n        0.8635600209236145,\n        0.862779974937439,\n        0.9533200263977051,\n        0.8619499802589417,\n        0.824180006980896,\n        0.8427299857139587,\n        0.8697900176048279,\n    ],\n    [\n        0.9788600206375122,\n        0.9612200260162354,\n        0.9409099817276001,\n        0.8356199860572815,\n        0.9004700183868408,\n        0.9074199795722961,\n        0.8634200096130371,\n        0.8663399815559387,\n        0.9418100118637085,\n        0.8498299717903137,\n        0.9287199974060059,\n        0.931119978427887,\n        0.9592900276184082,\n        0.8770300149917603,\n        0.7977799773216248,\n        0.886210024356842,\n        0.8287100195884705,\n        0.8795599937438965,\n        0.8516899943351746,\n        0.7866600155830383,\n        0.9853299856185913,\n        0.924019992351532,\n        0.9881899952888489,\n        0.9759799838066101,\n        0.86285001039505,\n        0.8251500129699707,\n        0.9148899912834167,\n        0.9936800003051758,\n        0.9064499735832214,\n        0.9237800240516663,\n        0.8946999907493591,\n        0.908270001411438,\n    ],\n    [\n        0.9696300029754639,\n        0.9886699914932251,\n        0.8251000046730042,\n        0.8840199708938599,\n        0.942330002784729,\n        0.9483399987220764,\n        0.9783599972724915,\n        0.9282600283622742,\n        0.9646400213241577,\n        0.9078699946403503,\n        0.9594399929046631,\n        0.852150022983551,\n        0.8773499727249146,\n        0.9529799818992615,\n        0.8902000188827515,\n        0.968559980392456,\n        0.995140016078949,\n        0.9530900120735168,\n        0.9545900225639343,\n        0.9952899813652039,\n        0.8978800177574158,\n        0.9105200171470642,\n        0.9275500178337097,\n        0.9079099893569946,\n        0.9594600200653076,\n        0.950439989566803,\n        0.9591599702835083,\n        0.9132900238037109,\n        0.9125300049781799,\n        0.992169976234436,\n        0.8111500144004822,\n        0.9257799983024597,\n    ],\n    [\n        0.8827599883079529,\n        0.9585199952125549,\n        0.8989499807357788,\n        0.9434300065040588,\n        0.9916899800300598,\n        0.9944700002670288,\n        0.9719799757003784,\n        0.9941099882125854,\n        0.9754400253295898,\n        0.946179986000061,\n        0.9476199746131897,\n        0.9703099727630615,\n        0.8684399724006653,\n        0.9565799832344055,\n        0.9712100028991699,\n        0.9204800128936768,\n        0.8932999968528748,\n        0.9917399883270264,\n        0.9828199744224548,\n        0.9521200060844421,\n        0.8695999979972839,\n        0.9806600213050842,\n        0.9736800193786621,\n        0.9826400279998779,\n        0.9559100270271301,\n        0.8896399736404419,\n        0.9298099875450134,\n        0.9402400255203247,\n        0.9363600015640259,\n        0.929610013961792,\n        0.9065899848937988,\n        0.9380499720573425,\n    ],\n    [\n        0.8884199857711792,\n        0.8980699777603149,\n        0.9954699873924255,\n        0.9936500191688538,\n        0.9280099868774414,\n        0.9807699918746948,\n        0.9869400262832642,\n        0.934499979019165,\n        0.9196500182151794,\n        0.9578400254249573,\n        0.8849499821662903,\n        0.9026700258255005,\n        0.9363800287246704,\n        0.9417999982833862,\n        0.9741899967193604,\n        0.9503300189971924,\n        0.9375900030136108,\n        0.9166600108146667,\n        0.9663199782371521,\n        0.9051799774169922,\n        0.9849900007247925,\n        0.9950299859046936,\n        0.9926599860191345,\n        0.9396700263023376,\n        0.9950100183486938,\n        0.9959499835968018,\n        0.9958800077438354,\n        0.9641299843788147,\n        0.9940400123596191,\n        0.9934300184249878,\n        0.8913000226020813,\n        0.9940199851989746,\n    ],\n    [\n        0.9414700269699097,\n        0.9644299745559692,\n        0.9633899927139282,\n        0.995989978313446,\n        0.97257000207901,\n        0.8882799744606018,\n        0.9935399889945984,\n        0.9929199814796448,\n        0.9665799736976624,\n        0.9640499949455261,\n        0.978410005569458,\n        0.9405999779701233,\n        0.9465500116348267,\n        0.9799500107765198,\n        0.9240599870681763,\n        0.9531400203704834,\n        0.9665700197219849,\n        0.9393399953842163,\n        0.9666900038719177,\n        0.9321399927139282,\n        0.9609900116920471,\n        0.9906799793243408,\n        0.992579996585846,\n        0.99413001537323,\n        0.9649999737739563,\n        0.9963300228118896,\n        0.9909300208091736,\n        0.9724699854850769,\n        0.9586700201034546,\n        0.9438300132751465,\n        0.9530100226402283,\n        0.957539975643158,\n    ],\n    [\n        0.9623100161552429,\n        0.894569993019104,\n        0.9834100008010864,\n        0.9929999709129333,\n        0.9101200103759766,\n        0.9764699935913086,\n        0.8794599771499634,\n        0.9640899896621704,\n        0.9950399994850159,\n        0.9961199760437012,\n        0.9918699860572815,\n        0.9736899733543396,\n        0.980139970779419,\n        0.9878299832344055,\n        0.9835799932479858,\n        0.9637799859046936,\n        0.9890499711036682,\n        0.9831399917602539,\n        0.9851899743080139,\n        0.9892100095748901,\n        0.9700199961662292,\n        0.9612799882888794,\n        0.945110023021698,\n        0.9836300015449524,\n        0.9840499758720398,\n        0.9833599925041199,\n        0.9913100004196167,\n        0.9877399802207947,\n        0.9916399717330933,\n        0.9931300282478333,\n        0.9947800040245056,\n        0.9955999851226807,\n    ],\n    [\n        0.9937800168991089,\n        0.9853799939155579,\n        0.9941200017929077,\n        0.9930999875068665,\n        0.9944900274276733,\n        0.9891800284385681,\n        0.9696499705314636,\n        0.9780600070953369,\n        0.9602800011634827,\n        0.9503800272941589,\n        0.9764500260353088,\n        0.9642699956893921,\n        0.9952300190925598,\n        0.9949100017547607,\n        0.9924299716949463,\n        0.9948999881744385,\n        0.9710599780082703,\n        0.9855499863624573,\n        0.9871100187301636,\n        0.9893500208854675,\n        0.9928600192070007,\n        0.9875500202178955,\n        0.9869899749755859,\n        0.9866600036621094,\n        0.9383500218391418,\n        0.9957699775695801,\n        0.9529100060462952,\n        0.9896699786186218,\n        0.9943699836730957,\n        0.9894400238990784,\n        0.9797599911689758,\n        0.994629979133606,\n    ],\n    [\n        0.9811699986457825,\n        0.9968699812889099,\n        0.9556699991226196,\n        0.9221000075340271,\n        0.9042099714279175,\n        0.9831299781799316,\n        0.9941200017929077,\n        0.9939600229263306,\n        0.9752100110054016,\n        0.9438499808311462,\n        0.9878699779510498,\n        0.9408400058746338,\n        0.9876599907875061,\n        0.9940400123596191,\n        0.9936599731445312,\n        0.9606999754905701,\n        0.9363399744033813,\n        0.9894700050354004,\n        0.9958500266075134,\n        0.9943900108337402,\n        0.9874699711799622,\n        0.9838200211524963,\n        0.9903799891471863,\n        0.9743300080299377,\n        0.9906600117683411,\n        0.98785001039505,\n        0.9963700175285339,\n        0.9716600179672241,\n        0.9891899824142456,\n        0.9913700222969055,\n        0.9861500263214111,\n        0.8874099850654602,\n    ],\n    [\n        0.9912700057029724,\n        0.9915500283241272,\n        0.9950500130653381,\n        0.9947999715805054,\n        0.9043499827384949,\n        0.9959800243377686,\n        0.9963099956512451,\n        0.9961100220680237,\n        0.9892200231552124,\n        0.9934899806976318,\n        0.9844200015068054,\n        0.9951800107955933,\n        0.9801300168037415,\n        0.9937999844551086,\n        0.992609977722168,\n        0.9911699891090393,\n        0.963670015335083,\n        0.9811199903488159,\n        0.9955199956893921,\n        0.9897800087928772,\n        0.9944400191307068,\n        0.9920700192451477,\n        0.9940000176429749,\n        0.9934700131416321,\n        0.9954500198364258,\n        0.9939500093460083,\n        0.9948999881744385,\n        0.9955700039863586,\n        0.9842100143432617,\n        0.9927399754524231,\n        0.9959800243377686,\n        0.9212999939918518,\n    ],\n    [\n        0.983959972858429,\n        0.9954800009727478,\n        0.977429986000061,\n        0.9843299984931946,\n        0.9907199740409851,\n        0.9889500141143799,\n        0.9894800186157227,\n        0.9865999817848206,\n        0.9939600229263306,\n        0.9891499876976013,\n        0.9946500062942505,\n        0.9938200116157532,\n        0.954230010509491,\n        0.9627000093460083,\n        0.9921200275421143,\n        0.9821799993515015,\n        0.9439499974250793,\n        0.9283699989318848,\n        0.9474200010299683,\n        0.9694600105285645,\n        0.9909200072288513,\n        0.9768099784851074,\n        0.9310899972915649,\n        0.9903600215911865,\n        0.9746599793434143,\n        0.9799900054931641,\n        0.9794300198554993,\n        0.9890400171279907,\n        0.9820600152015686,\n        0.981249988079071,\n        0.9903299808502197,\n        0.9849500060081482,\n    ],\n    [\n        0.9899299740791321,\n        0.9871900081634521,\n        0.9914399981498718,\n        0.9829300045967102,\n        0.986810028553009,\n        0.9629200100898743,\n        0.9282699823379517,\n        0.9718800187110901,\n        0.9935100078582764,\n        0.9896799921989441,\n        0.992929995059967,\n        0.986519992351532,\n        0.983780026435852,\n        0.9840099811553955,\n        0.9867900013923645,\n        0.9873499870300293,\n        0.9943900108337402,\n        0.9946900010108948,\n        0.977869987487793,\n        0.9935399889945984,\n        0.9738199710845947,\n        0.954289972782135,\n        0.9362599849700928,\n        0.9841300249099731,\n        0.9630699753761292,\n        0.9875699877738953,\n        0.9930400252342224,\n        0.9935200214385986,\n        0.9912999868392944,\n        0.9372699856758118,\n        0.9883900284767151,\n        0.9968000054359436,\n    ],\n    [\n        0.9665499925613403,\n        0.9739400148391724,\n        0.9699199795722961,\n        0.9671400189399719,\n        0.9841600060462952,\n        0.9908000230789185,\n        0.9846600294113159,\n        0.991100013256073,\n        0.9756799936294556,\n        0.9816799759864807,\n        0.985319972038269,\n        0.9781399965286255,\n        0.9873700141906738,\n        0.9831100106239319,\n        0.9929199814796448,\n        0.9735900163650513,\n        0.9934499859809875,\n        0.9805799722671509,\n        0.9914799928665161,\n        0.9926000237464905,\n        0.9950900077819824,\n        0.9876000285148621,\n        0.9915500283241272,\n        0.9898599982261658,\n        0.9776399731636047,\n        0.9649699926376343,\n        0.9712799787521362,\n        0.987500011920929,\n        0.9920600056648254,\n        0.9920300245285034,\n        0.9866999983787537,\n        0.9871100187301636,\n    ],\n    [\n        0.9464499950408936,\n        0.9104400277137756,\n        0.9680299758911133,\n        0.9804700016975403,\n        0.9698600172996521,\n        0.9969800114631653,\n        0.9948800206184387,\n        0.9617099761962891,\n        0.9929599761962891,\n        0.9910200238227844,\n        0.9936299920082092,\n        0.995140016078949,\n        0.9221900105476379,\n        0.9466599822044373,\n        0.9571200013160706,\n        0.9492899775505066,\n        0.9901400208473206,\n        0.9965900182723999,\n        0.994629979133606,\n        0.9515900015830994,\n        0.9844599962234497,\n        0.9927099943161011,\n        0.9655799865722656,\n        0.9803799986839294,\n        0.9922599792480469,\n        0.9887099862098694,\n        0.9830600023269653,\n        0.9836099743843079,\n        0.9922699928283691,\n        0.9935700297355652,\n        0.9954000115394592,\n        0.9636399745941162,\n    ],\n    [\n        0.9920399785041809,\n        0.9151300191879272,\n        0.9884499907493591,\n        0.978380024433136,\n        0.9799100160598755,\n        0.984250009059906,\n        0.9649900197982788,\n        0.9868900179862976,\n        0.9691100120544434,\n        0.9517899751663208,\n        0.9425399899482727,\n        0.8750200271606445,\n        0.9416800141334534,\n        0.9713000059127808,\n        0.9060699939727783,\n        0.9808700084686279,\n        0.9929900169372559,\n        0.9916399717330933,\n        0.9848600029945374,\n        0.9931700229644775,\n        0.9898200035095215,\n        0.8692100048065186,\n        0.9693999886512756,\n        0.9878799915313721,\n        0.9727799892425537,\n        0.992389976978302,\n        0.9907699823379517,\n        0.9767900109291077,\n        0.9909300208091736,\n        0.972790002822876,\n        0.9758899807929993,\n        0.9925699830055237,\n    ],\n    [\n        0.9480500221252441,\n        0.9420700073242188,\n        0.9782299995422363,\n        0.9793099761009216,\n        0.9700199961662292,\n        0.9702699780464172,\n        0.9889000058174133,\n        0.9665300250053406,\n        0.995389997959137,\n        0.9804999828338623,\n        0.9934300184249878,\n        0.965149998664856,\n        0.9889699816703796,\n        0.9892600178718567,\n        0.9767699837684631,\n        0.9926000237464905,\n        0.9414200186729431,\n        0.8009799718856812,\n        0.9794899821281433,\n        0.9118099808692932,\n        0.9114999771118164,\n        0.9899700284004211,\n        0.9839000105857849,\n        0.9811499714851379,\n        0.9901700019836426,\n        0.9932500123977661,\n        0.9400399923324585,\n        0.9892600178718567,\n        0.9284399747848511,\n        0.9748200178146362,\n        0.8981900215148926,\n        0.9534800052642822,\n    ],\n    [\n        0.9042800068855286,\n        0.7312300205230713,\n        0.9843500256538391,\n        0.7813400030136108,\n        0.9729899764060974,\n        0.7979300022125244,\n        0.7174299955368042,\n        0.9328200221061707,\n        0.981939971446991,\n        0.9654600024223328,\n        0.9482200145721436,\n        0.9147599935531616,\n        0.971530020236969,\n        0.7348300218582153,\n        0.9894400238990784,\n        0.8841599822044373,\n        0.9892399907112122,\n        0.8378000259399414,\n        0.9597300291061401,\n        0.9732599854469299,\n        0.993690013885498,\n        0.8993800282478333,\n        0.9945999979972839,\n        0.9927600026130676,\n        0.9448400139808655,\n        0.88850998878479,\n        0.9564300179481506,\n        0.907289981842041,\n        0.9882599711418152,\n        0.9760500192642212,\n        0.9238499999046326,\n        0.8341599702835083,\n    ],\n    [\n        0.9955599904060364,\n        0.9722700119018555,\n        0.9952499866485596,\n        0.9902099967002869,\n        0.9865700006484985,\n        0.8881999850273132,\n        0.9800000190734863,\n        0.9010499715805054,\n        0.9261900186538696,\n        0.9923400282859802,\n        0.8826599717140198,\n        0.9810600280761719,\n        0.993399977684021,\n        0.9960700273513794,\n        0.9942200183868408,\n        0.9916899800300598,\n        0.8386099934577942,\n        0.798770010471344,\n        0.7951200008392334,\n        0.9530900120735168,\n        0.9838600158691406,\n        0.7948099970817566,\n        0.9375100135803223,\n        0.9907299876213074,\n        0.8083999752998352,\n        0.9854699969291687,\n        0.881089985370636,\n        0.9129899740219116,\n        0.7930200099945068,\n        0.7010300159454346,\n        0.9305899739265442,\n        0.74822998046875,\n    ],\n    [\n        0.9924200177192688,\n        0.9921799898147583,\n        0.9803599715232849,\n        0.9907100200653076,\n        0.9023100137710571,\n        0.9276400208473206,\n        0.9103299975395203,\n        0.8522700071334839,\n        0.9225199818611145,\n        0.9765499830245972,\n        0.8905500173568726,\n        0.9876700043678284,\n        0.9938600063323975,\n        0.9918199777603149,\n        0.9930999875068665,\n        0.9790499806404114,\n        0.7882099747657776,\n        0.7593600153923035,\n        0.7997699975967407,\n        0.6790900230407715,\n        0.8935199975967407,\n        0.8678699731826782,\n        0.9654300212860107,\n        0.9793699979782104,\n        0.9826499819755554,\n        0.9895300269126892,\n        0.9924700260162354,\n        0.9858099818229675,\n        0.8832399845123291,\n        0.9139500260353088,\n        0.961139976978302,\n        0.8677600026130676,\n    ],\n    [\n        0.8905400037765503,\n        0.988789975643158,\n        0.9946200251579285,\n        0.9888899922370911,\n        0.9328299760818481,\n        0.944159984588623,\n        0.9258900284767151,\n        0.9235799908638,\n        0.9918000102043152,\n        0.941860020160675,\n        0.9637600183486938,\n        0.9541599750518799,\n        0.9035500288009644,\n        0.9037299752235413,\n        0.9822700023651123,\n        0.9133800268173218,\n        0.8292700052261353,\n        0.8286799788475037,\n        0.9476000070571899,\n        0.9096199870109558,\n        0.9164000153541565,\n        0.9878799915313721,\n        0.9596999883651733,\n        0.9513099789619446,\n        0.840499997138977,\n        0.913640022277832,\n        0.9730299711227417,\n        0.9579600095748901,\n        0.7556399703025818,\n        0.8148499727249146,\n        0.8507400155067444,\n        0.8387699723243713,\n    ],\n    [\n        0.8944200277328491,\n        0.8674200177192688,\n        0.9694100022315979,\n        0.8611400127410889,\n        0.985069990158081,\n        0.8440399765968323,\n        0.9875699877738953,\n        0.9092900156974792,\n        0.9427599906921387,\n        0.9490299820899963,\n        0.9271699786186218,\n        0.9783899784088135,\n        0.9435099959373474,\n        0.9486100077629089,\n        0.9901300072669983,\n        0.987779974937439,\n        0.9378200173377991,\n        0.9493799805641174,\n        0.9884799718856812,\n        0.915910005569458,\n        0.7602499723434448,\n        0.7345100045204163,\n        0.7663499712944031,\n        0.7821000218391418,\n        0.8813499808311462,\n        0.8038100004196167,\n        0.7710999846458435,\n        0.9668800234794617,\n        0.9693800210952759,\n        0.9797599911689758,\n        0.8983200192451477,\n        0.8990600109100342,\n    ],\n    [\n        0.9781100153923035,\n        0.7062199711799622,\n        0.9416900277137756,\n        0.7704100012779236,\n        0.9377400279045105,\n        0.875029981136322,\n        0.9606800079345703,\n        0.9678599834442139,\n        0.8913800120353699,\n        0.9004499912261963,\n        0.8603299856185913,\n        0.8304399847984314,\n        0.8866000175476074,\n        0.9343400001525879,\n        0.9755899906158447,\n        0.8745499849319458,\n        0.9187800288200378,\n        0.8467100262641907,\n        0.816860020160675,\n        0.8385499715805054,\n        0.9242600202560425,\n        0.911080002784729,\n        0.9501199722290039,\n        0.8581299781799316,\n        0.9677900075912476,\n        0.9527199864387512,\n        0.9327899813652039,\n        0.9348300099372864,\n        0.9016299843788147,\n        0.8968999981880188,\n        0.9196900129318237,\n        0.9369300007820129,\n    ],\n    [\n        0.8354600071907043,\n        0.983709990978241,\n        0.8588299751281738,\n        0.9724799990653992,\n        0.8198699951171875,\n        0.7998999953269958,\n        0.7833600044250488,\n        0.9567599892616272,\n        0.8902699947357178,\n        0.8847600221633911,\n        0.8827900290489197,\n        0.838890016078949,\n        0.9663800001144409,\n        0.9603899717330933,\n        0.9917299747467041,\n        0.9067800045013428,\n        0.8427199721336365,\n        0.8749099969863892,\n        0.8616099953651428,\n        0.9272699952125549,\n        0.9447399973869324,\n        0.8093500137329102,\n        0.851069986820221,\n        0.9074199795722961,\n        0.9821799993515015,\n        0.907729983329773,\n        0.9880200028419495,\n        0.9945799708366394,\n        0.9296799898147583,\n        0.9244099855422974,\n        0.9291300177574158,\n        0.8533700108528137,\n    ],\n    [\n        0.8113999962806702,\n        0.9567700028419495,\n        0.9930800199508667,\n        0.9895600080490112,\n        0.870140016078949,\n        0.8715999722480774,\n        0.8801500201225281,\n        0.8457000255584717,\n        0.7443199753761292,\n        0.7724699974060059,\n        0.8801400065422058,\n        0.8895999789237976,\n        0.8454399704933167,\n        0.8708199858665466,\n        0.8175399899482727,\n        0.8254899978637695,\n        0.8024600148200989,\n        0.9616100192070007,\n        0.9903299808502197,\n        0.7434300184249878,\n        0.9372000098228455,\n        0.8016999959945679,\n        0.9476900100708008,\n        0.9661200046539307,\n        0.8687599897384644,\n        0.7976800203323364,\n        0.7997999787330627,\n        0.7939900159835815,\n        0.8946300148963928,\n        0.8983399868011475,\n        0.8781999945640564,\n        0.880020022392273,\n    ],\n    [\n        0.9948099851608276,\n        0.9818900227546692,\n        0.9907100200653076,\n        0.9938399791717529,\n        0.9851999878883362,\n        0.9499099850654602,\n        0.9706599712371826,\n        0.9756900072097778,\n        0.9664300084114075,\n        0.9746699929237366,\n        0.9862800240516663,\n        0.9235100150108337,\n        0.922540009021759,\n        0.925849974155426,\n        0.9078900218009949,\n        0.9878799915313721,\n        0.8783000111579895,\n        0.9190000295639038,\n        0.9587299823760986,\n        0.9941700100898743,\n        0.9542700052261353,\n        0.9789800047874451,\n        0.9761999845504761,\n        0.9851300120353699,\n        0.8040900230407715,\n        0.8177300095558167,\n        0.8411399722099304,\n        0.9910299777984619,\n        0.9867100119590759,\n        0.9164199829101562,\n        0.9889400005340576,\n        0.9753000140190125,\n    ],\n    [\n        0.7873499989509583,\n        0.8274099826812744,\n        0.8251100182533264,\n        0.7673500180244446,\n        0.9829400181770325,\n        0.9889100193977356,\n        0.986270010471344,\n        0.990809977054596,\n        0.9827700257301331,\n        0.9087399840354919,\n        0.8597599864006042,\n        0.9379199743270874,\n        0.9462000131607056,\n        0.9920600056648254,\n        0.9775400161743164,\n        0.9868999719619751,\n        0.9945600032806396,\n        0.7293699979782104,\n        0.8462499976158142,\n        0.8436499834060669,\n        0.9493600130081177,\n        0.993149995803833,\n        0.9104999899864197,\n        0.9844800233840942,\n        0.8892599940299988,\n        0.6459100246429443,\n        0.7403600215911865,\n        0.7409800291061401,\n        0.9965100288391113,\n        0.9267899990081787,\n        0.9914600253105164,\n        0.9765599966049194,\n    ],\n    [\n        0.9952300190925598,\n        0.9662700295448303,\n        0.9226800203323364,\n        0.9925100207328796,\n        0.8657000064849854,\n        0.9297099709510803,\n        0.8301600217819214,\n        0.8990899920463562,\n        0.9421300292015076,\n        0.9386199712753296,\n        0.9941700100898743,\n        0.98649001121521,\n        0.8553500175476074,\n        0.9807999730110168,\n        0.9721300005912781,\n        0.9932900071144104,\n        0.9417499899864197,\n        0.9853500127792358,\n        0.9951300024986267,\n        0.8549000024795532,\n        0.9942299723625183,\n        0.9920399785041809,\n        0.8226500153541565,\n        0.9935200214385986,\n        0.9117500185966492,\n        0.9756600260734558,\n        0.9392099976539612,\n        0.978950023651123,\n        0.9778800010681152,\n        0.9933300018310547,\n        0.9940400123596191,\n        0.9429699778556824,\n    ],\n    [\n        0.7903100252151489,\n        0.7481899857521057,\n        0.8011800050735474,\n        0.7696400284767151,\n        0.8313800096511841,\n        0.824429988861084,\n        0.8568400144577026,\n        0.881659984588623,\n        0.9863100051879883,\n        0.9929800033569336,\n        0.9879000186920166,\n        0.9949100017547607,\n        0.8248000144958496,\n        0.7942900061607361,\n        0.8380200266838074,\n        0.8081499934196472,\n        0.9849600195884705,\n        0.7776899933815002,\n        0.9267600178718567,\n        0.8601300120353699,\n        0.995710015296936,\n        0.9945600032806396,\n        0.9918299913406372,\n        0.993910014629364,\n        0.8517299890518188,\n        0.7500200271606445,\n        0.926829993724823,\n        0.8319900035858154,\n        0.9765999913215637,\n        0.9753000140190125,\n        0.9896199703216553,\n        0.9943100214004517,\n    ],\n    [\n        0.9870200157165527,\n        0.9943900108337402,\n        0.9959200024604797,\n        0.9775099754333496,\n        0.7173399925231934,\n        0.8274099826812744,\n        0.9957000017166138,\n        0.6710399985313416,\n        0.9845799803733826,\n        0.9934200048446655,\n        0.9599199891090393,\n        0.99058997631073,\n        0.9816200137138367,\n        0.9838799834251404,\n        0.9815599918365479,\n        0.9925100207328796,\n        0.9940900206565857,\n        0.9922299981117249,\n        0.997439980506897,\n        0.9725300073623657,\n        0.9821699857711792,\n        0.9696900248527527,\n        0.9595699906349182,\n        0.889460027217865,\n        0.9952999949455261,\n        0.985040009021759,\n        0.9955300092697144,\n        0.9936699867248535,\n        0.9003900289535522,\n        0.9389500021934509,\n        0.966480016708374,\n        0.9753199815750122,\n    ],\n    [\n        0.9697399735450745,\n        0.9329400062561035,\n        0.8819100260734558,\n        0.8996000289916992,\n        0.8809000253677368,\n        0.9244599938392639,\n        0.9828000068664551,\n        0.9926999807357788,\n        0.7688699960708618,\n        0.8750100135803223,\n        0.8712000250816345,\n        0.9048299789428711,\n        0.9513999819755554,\n        0.9886999726295471,\n        0.9907000064849854,\n        0.9424200057983398,\n        0.9874699711799622,\n        0.9769399762153625,\n        0.8826199769973755,\n        0.9940199851989746,\n        0.9948499798774719,\n        0.9926000237464905,\n        0.9769700169563293,\n        0.9899299740791321,\n        0.8975800275802612,\n        0.943880021572113,\n        0.974049985408783,\n        0.9742599725723267,\n        0.9838100075721741,\n        0.9911400079727173,\n        0.9910699725151062,\n        0.9690600037574768,\n    ],\n]\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/niah.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nCreate a dataset jsonl file for needle in a haystack.\n\npython niah.py \\\n    --save_dir=./ \\\n    --save_name=niah_single \\\n    --tokenizer_path=tokenizer.model \\\n    --tokenizer_type=nemo \\\n    --max_seq_length=4096 \\\n    --tokens_to_generate=128 \\\n    --num_samples=10 \\\n    --template=\"Some special magic {type_needle_v} are hidden within the following text. Make sure to memorize it. I will quiz you about the {type_needle_v} afterwards.\\n{context}\\nWhat are all the special magic {type_needle_v} for {query} mentioned in the provided text? The special magic {type_needle_v} for {query} mentioned in the provided text are\"\n\"\"\"\nimport os\nimport re\nimport json\nimport uuid\nimport argparse\nimport importlib\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport wonderwords\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\nimport sys\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\")) \nfrom tokenizer import select_tokenizer\nfrom nltk.tokenize import sent_tokenize\n\n\nparser = argparse.ArgumentParser()\n# Basic Configurations\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--save_name\", type=str, required=True, help='name of the save dataset jsonl file')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--tokens_to_generate\", type=int, required=True, help='expected generated token amount.')\nparser.add_argument(\"--num_samples\", type=int, required=True, help='number of samples to generate')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--template\", type=str, default='', help='prompt template')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\n\n# Complexity Configurations\nparser.add_argument(\"--num_needle_k\", type=int, default=1)\nparser.add_argument(\"--num_needle_v\", type=int, default=1)\nparser.add_argument(\"--num_needle_q\", type=int, default=1)\nparser.add_argument(\"--type_haystack\", type=str, default='essay', help='[Options] repeat, essay, needle.')\nparser.add_argument(\"--type_needle_k\", type=str, default='words', help='[Options] numbers, words, uuids.')\nparser.add_argument(\"--type_needle_v\", type=str, default='numbers', help='[Options] numbers, words, uuids.')\n\nargs = parser.parse_args()\nrandom.seed(args.random_seed)\nnp.random.seed(args.random_seed)\nargs.num_needle_k = max(args.num_needle_k, args.num_needle_q)\n\n# Load Tokenizer\nTOKENIZER = select_tokenizer(args.tokenizer_type, args.tokenizer_path)\n\n# Define Needle/Haystack Format \nneedle = \"One of the special magic {type_needle_v} for {key} is: {value}.\"\nif args.type_haystack == 'essay':\n    essay = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"json/PaulGrahamEssays.json\")\n    essay = json.load(open(essay))['text']\n    haystack = re.sub(r'\\s+', \" \", essay).split(\" \")\nelif args.type_haystack == 'repeat':\n    haystack = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\nelif args.type_haystack == 'needle':\n    haystack = needle\nelse:\n    raise NotImplementedError(f'{args.type_haystack} is not implemented.')\n\n\n# Words\nnouns = wonderwords.random_word._get_words_from_text_file(\"nounlist.txt\")\nadjs = wonderwords.random_word._get_words_from_text_file(\"adjectivelist.txt\")\n# verbs = wonderwords.random_word._get_words_from_text_file(\"verblist.txt\")\nwords = [f\"{adj}-{noun}\" for adj in adjs for noun in nouns]\nwords = sorted(list(set(words)))\n\n\n# Positions\nDEPTHS = list(np.round(np.linspace(0, 100, num=40, endpoint=True)).astype(int))\n\n\ndef generate_random_number(num_digits=7):\n    lower_bound = 10**(num_digits - 1)\n    upper_bound = 10**num_digits - 1\n    return str(random.randint(lower_bound, upper_bound))\n\ndef generate_random_word():\n    word = random.choice(words)\n    return word\n\ndef generate_random_uuid():\n    return str(uuid.UUID(int=random.getrandbits(128), version=4))\n\ndef generate_random(type_needle: str):\n    if type_needle == 'numbers':\n        return generate_random_number()\n    elif type_needle == 'words':\n        return generate_random_word()\n    elif type_needle == 'uuids':\n        return generate_random_uuid()\n    else:\n        raise NotImplementedError(f'{args.type_needle} is not implemented.')\n\ndef generate_input_output(num_haystack):\n    keys, values, needles = [], [], []\n    for _ in range(args.num_needle_k):\n        keys.append(generate_random(args.type_needle_k))\n        value = []\n        for _ in range(args.num_needle_v):\n            value.append(generate_random(args.type_needle_v))\n            needles.append(needle.format(\n                type_needle_v=args.type_needle_v,\n                key=keys[-1], \n                value=value[-1],\n            ))\n        values.append(value)\n    \n    random.Random(args.random_seed).shuffle(needles)\n    \n    # Context\n    if args.type_haystack == 'essay':\n        text = \" \".join(haystack[:num_haystack])\n        document_sents = sent_tokenize(text.strip())\n        insertion_positions = [0] + \\\n                              sorted([int(len(document_sents) * (depth / 100)) for depth in random.sample(DEPTHS, len(needles))]) + \\\n                              [len(document_sents)]\n        document_sents_list = []\n        for i in range(1,len(insertion_positions)):\n            last_pos = insertion_positions[i-1]\n            next_pos = insertion_positions[i]\n            document_sents_list.append(\" \".join(document_sents[last_pos:next_pos]))\n            if i-1 < len(needles):\n                document_sents_list.append(needles[i-1])\n        context = \" \".join(document_sents_list)\n\n    else:\n        if args.type_haystack == 'repeat':\n            sentences = [haystack] * num_haystack\n        elif args.type_haystack == 'needle':\n            sentences = [haystack.format(\n                type_needle_v=args.type_needle_v,\n                key=generate_random(args.type_needle_k),\n                value=generate_random(args.type_needle_v),\n            ) for _ in range(num_haystack)]\n\n            \n        indexes = sorted(random.sample(range(num_haystack), len(needles)), reverse=True)\n        for index, element in zip(indexes, needles):\n            sentences.insert(index, element)\n        context = \"\\n\".join(sentences)\n\n\n    ## Query and Answer\n    indices = random.sample(range(args.num_needle_k), args.num_needle_q)\n    queries = [keys[i] for i in indices]\n    answers = [a for i in indices for a in values[i]]\n    query = ', '.join(queries[:-1]) + ', and ' + queries[-1] if len(queries) > 1 else queries[0]\n    \n    template = args.template\n    type_needle_v = args.type_needle_v\n    if args.num_needle_q * args.num_needle_v == 1:\n        template = template.replace('Some', 'A')\n        template = template.replace('are all', 'is')\n        template = template.replace('are', 'is')\n        template = template.replace('answers', 'answer')\n        type_needle_v = type_needle_v[:-1] # remove \"s\"\n\n    input_text = template.format(\n        type_needle_v=type_needle_v,\n        context=context,\n        query=query,\n    )\n\n    return input_text, answers\n\n\ndef generate_samples(num_samples: int, max_seq_length: int, save_dir: str, incremental: int = 500):\n    write_jsons = []\n    tokens_to_generate = args.tokens_to_generate\n\n    if args.type_haystack == 'essay':\n        incremental = 500\n    elif args.type_haystack == 'repeat':\n        incremental = 25\n    elif args.type_haystack == 'needle':\n        incremental = 25\n        \n    if args.type_haystack != 'essay' and args.max_seq_length < 4096:\n        incremental = 5\n\n    num_haystack = incremental\n        \n    total_tokens = 0  # Track the total tokens generated for the first example\n    while total_tokens + tokens_to_generate < max_seq_length :  \n        input_text, answer = generate_input_output(num_haystack)\n        # Calculate the number of tokens in the example\n        total_tokens = len(TOKENIZER.text_to_tokens(input_text + ' '.join(answer)))\n        print(f'Max length {max_seq_length} | Current length {total_tokens + tokens_to_generate} | Haystack: {num_haystack}')\n        if total_tokens + tokens_to_generate > max_seq_length:\n            num_haystack -= incremental\n            break\n    \n        if args.type_haystack == 'essay' and num_haystack > len(haystack):\n            num_haystack = len(haystack)\n            break\n        \n        num_haystack += incremental\n\n    print('Num haystack:', num_haystack)\n    \n    # Generate samples\n    for index in tqdm(range(num_samples)):\n        used_haystack = num_haystack\n        while(True):\n            try:\n                input_text, answer  = generate_input_output(used_haystack)\n                length = len(TOKENIZER.text_to_tokens(input_text)) + tokens_to_generate\n                assert length <= max_seq_length, f\"{length} exceeds max_seq_length.\"\n                break\n            except:\n                if used_haystack > incremental:\n                    used_haystack -= incremental\n        \n        if args.remove_newline_tab:\n            input_text = ' '.join(input_text.replace('\\n', ' ').replace('\\t', ' ').strip().split())\n\n        formatted_output = {\n            'index': index,\n            \"input\": input_text,\n            \"outputs\": answer,\n            \"length\": length,\n        }\n        write_jsons.append(formatted_output)\n\n    return write_jsons\n\n\ndef main():\n    save_file = args.save_dir / f'{args.save_name}' / f'{args.subset}.jsonl'\n    save_file.parent.mkdir(parents=True, exist_ok=True)\n\n    write_jsons = generate_samples(\n        num_samples=args.num_samples, \n        max_seq_length=args.max_seq_length,\n        save_dir=args.save_dir\n    )\n\n    write_manifest(save_file, write_jsons)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "eval/RULER/scripts/pred/serve_trt.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# adapted from https://github.com/Kipok/NeMo-Skills/blob/v0.1/nemo_skills/inference/server/serve_trt.py\n\nimport json\nimport logging\nimport sys\nimport json\nfrom pathlib import Path\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport torch\nimport tensorrt_llm\nfrom flask import Flask, jsonify, request\nfrom flask_restful import Api, Resource\nfrom tensorrt_llm.runtime import ModelRunnerCpp\nfrom mpi4py import MPI\nfrom transformers import AutoTokenizer\n\n\nclass TritonServerGenerate(Resource):\n    def __init__(self, model):\n        self.model = model\n        self.comm = MPI.COMM_WORLD\n\n    def generate(\n        self,\n        prompts,\n        max_new_tokens,\n        temperature,\n        top_k,\n        top_p,\n        repetition_penalty,\n        random_seed,\n        stop_words_list,\n        max_attention_window_size=None\n    ):\n        output = self.model.forward(\n            prompts,\n            max_output_token=max_new_tokens,\n            top_k=top_k,\n            top_p=top_p,\n            temperature=temperature,\n            repetition_penalty=repetition_penalty,\n            random_seed=random_seed,\n            stop_words_list=stop_words_list,\n            max_attention_window_size=max_attention_window_size,\n        )\n        return output\n\n    def put(self):\n        logging.info(\"request IP: \" + str(request.remote_addr))\n        logging.info(json.dumps(request.get_json()))\n\n        input_request = request.get_json()\n\n        tokens_to_generate = input_request.get(\"tokens_to_generate\", 64)\n        temperature = input_request.get(\"temperature\", 1.0)\n        top_k = input_request.get(\"top_k\", 0)\n        top_p = input_request.get(\"top_p\", 1.0)\n        repetition_penalty = input_request.get(\"repetition_penalty\", 1.2)\n        stop_words_list = input_request.get(\"stop_words_list\")\n        max_attention_window_size = input_request.get(\"max_attention_window_size\")\n        random_seed = input_request.get(\"random_seed\", 0)\n        prompts = input_request[\"prompts\"]\n\n        data = dict(\n            prompts=prompts,\n            max_new_tokens=tokens_to_generate,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            repetition_penalty=repetition_penalty,\n            random_seed=random_seed,\n            stop_words_list=stop_words_list,\n            max_attention_window_size=max_attention_window_size,\n        )\n        self.comm.Barrier()\n        data = self.comm.bcast(data, root=0)\n\n        out = self.generate(**data)\n        return jsonify(out)\n\n\ndef parse_input(input_texts: str, tokenizer):\n    batch_input_ids = [\n        tokenizer.encode(\n            input_text,\n            add_special_tokens=False,  # TODO: does this need to be true?\n        )\n        for input_text in input_texts\n    ]\n    batch_input_ids = [torch.tensor(x, dtype=torch.int32, device=\"cuda\") for x in batch_input_ids]\n    input_lengths = [x.size(0) for x in batch_input_ids]\n\n    return batch_input_ids, input_lengths\n\n\ndef get_output(output_ids, input_lengths, max_output_len, tokenizer, eos_token):\n    num_beams = output_ids.size(1)\n    assert num_beams == 1\n    output_texts = []\n    for idx, input_len in enumerate(input_lengths):\n        output_begin = input_len\n        output_end = input_len + max_output_len\n        outputs = output_ids[idx][0][output_begin:output_end]\n        eos_ids = (outputs == eos_token).nonzero(as_tuple=True)[-1]\n        if len(eos_ids) > 0:\n            outputs = outputs[: eos_ids[0]]\n        outputs = outputs.tolist()\n        output_texts.append(tokenizer.decode(outputs))\n    return output_texts\n\n\ndef prepare_stop_words(stop_words_list, tokenizer):\n    # adapted from https://github.com/NVIDIA/TensorRT-LLM/blob/b310ec675145c9ee7668592549f733df4abf1e94/tensorrt_llm/runtime/generation.py#L46\n    flat_ids = []\n    offsets = []\n    for batch_stop_words in stop_words_list:\n        item_flat_ids = []\n        item_offsets = []\n\n        for word in batch_stop_words:\n            # there is a known issue in TensorRT-LLM that word ids are not unique and might change depending on\n            # where in the text it appears. In our case we mainly need to stop on ids as they appear in the middle\n            # of the text. The following is a workaround to get such ids that works for both <TOKEN> kind of stop\n            # words as well as newlines that we commonly use. But note that it's not a universal fix, so this might\n            # require refactoring if different stop words are used in the future.\n            # Eventually, this needs to be fixed inside TensorRT-LLM itself.\n            ids = tokenizer.encode('magic' + word)\n            ids = ids[2:]  # skipping \"magic\"\n\n            if len(ids) == 0:\n                continue\n\n            item_flat_ids += ids\n            item_offsets.append(len(ids))\n\n        flat_ids.append(np.array(item_flat_ids))\n        offsets.append(np.cumsum(np.array(item_offsets)))\n\n    pad_to = max(1, max(len(ids) for ids in flat_ids))\n\n    for i, (ids, offs) in enumerate(zip(flat_ids, offsets)):\n        flat_ids[i] = np.pad(ids, (0, pad_to - len(ids)), constant_values=0)\n        offsets[i] = np.pad(offs, (0, pad_to - len(offs)), constant_values=-1)\n\n    stop_words = np.array([flat_ids, offsets], dtype=\"int32\").transpose((1, 0, 2))\n    return torch.Tensor(stop_words).to(torch.int32).to(\"cuda\").contiguous()\n\n\ndef load_tokenizer(tokenizer_dir: str):\n    tokenizer = AutoTokenizer.from_pretrained(\n        tokenizer_dir,\n        legacy=False,\n        trust_remote_code=True,\n    )\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    pad_id = tokenizer.pad_token_id\n    end_id = tokenizer.eos_token_id\n\n    return tokenizer, pad_id, end_id\n\n\n\nclass TensorRTLLM:\n    def __init__(self, model_path: str):\n        self.tokenizer, self.pad_id, self.end_id = load_tokenizer(tokenizer_dir=model_path)\n        self.runner = ModelRunnerCpp.from_dir(engine_dir=model_path, rank=tensorrt_llm.mpi_rank())\n\n    @torch.no_grad()\n    def forward(\n        self,\n        input_texts,\n        max_output_token,\n        top_k,\n        top_p,\n        temperature,\n        repetition_penalty,\n        random_seed,\n        stop_words_list,\n        max_attention_window_size,\n    ):\n        batch_input_ids, input_lengths = parse_input(input_texts, self.tokenizer)\n\n        stop_words_list = [stop_words_list for _ in range(len(input_texts))]\n        stop_words_list = prepare_stop_words(stop_words_list, self.tokenizer)\n\n        # TODO: return dictionary with a proper error reporting\n        try:\n            output_ids = self.runner.generate(\n                batch_input_ids,\n                max_new_tokens=max_output_token,\n                end_id=self.end_id,\n                pad_id=self.pad_id,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                repetition_penalty=repetition_penalty,\n                random_seed=random_seed,\n                stop_words_list=stop_words_list,\n                max_attention_window_size=max_attention_window_size,\n                return_dict=False,\n            )\n            torch.cuda.synchronize()\n\n            output = get_output(output_ids, input_lengths, max_output_token, self.tokenizer, self.end_id)\n        except RuntimeError as e:\n            logging.error(\"RuntimeError: %s\", e)\n            output = [f\"RuntimeError: {e}\"] * len(input_texts)\n\n        return output\n\n\nclass WrapperServer:\n    def __init__(self, model_path: str):\n        self.comm = MPI.COMM_WORLD\n        self.rank = self.comm.Get_rank()\n\n        self.model = TensorRTLLM(model_path=model_path)\n\n        if self.rank == 0:\n            self.app = Flask(__file__, static_url_path=\"\")\n            api = Api(self.app)\n            api.add_resource(TritonServerGenerate, \"/generate\", resource_class_args=[self.model])\n\n    def run(self, url, port=5000):\n        if self.rank == 0:\n            self.app.run(url, threaded=True, port=port, debug=False)\n        else:\n            self.worker_loop()\n\n    def worker_loop(self):\n        triton = TritonServerGenerate(self.model)\n        while True:\n            self.comm.Barrier()\n            data = None\n            data = self.comm.bcast(data, root=0)\n            triton.generate(**data)\n\n\nif __name__ == \"__main__\":\n    # TODO: can we reuse normal logger here?\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model_path\", required=True)\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int, default=5000)\n    args = parser.parse_args()\n\n    server = WrapperServer(model_path=args.model_path)\n    server.run(args.host, args.port)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/run.py", "content": "import json\n\nimport torch\nimport torch.distributed as dist\n\nfrom vlmeval.config import supported_VLM\nfrom vlmeval.dataset.video_dataset_config import supported_video_datasets\nfrom vlmeval.dataset import build_dataset\nfrom vlmeval.inference import infer_data_job\nfrom vlmeval.inference_video import infer_data_job_video\nfrom vlmeval.inference_mt import infer_data_job_mt\nfrom vlmeval.smp import *\nfrom vlmeval.utils.result_transfer import MMMU_result_transfer, MMTBench_result_transfer\n\n\ndef build_model_from_config(cfg, model_name):\n    import vlmeval.api\n    import vlmeval.vlm\n    config = cp.deepcopy(cfg[model_name])\n    if config == {}:\n        return supported_VLM[model_name]()\n    assert 'class' in config\n    cls_name = config.pop('class')\n    if hasattr(vlmeval.api, cls_name):\n        return getattr(vlmeval.api, cls_name)(**config)\n    elif hasattr(vlmeval.vlm, cls_name):\n        return getattr(vlmeval.vlm, cls_name)(**config)\n    else:\n        raise ValueError(f'Class {cls_name} is not supported in `vlmeval.api` or `vlmeval.vlm`')\n\n\ndef build_dataset_from_config(cfg, dataset_name):\n    import vlmeval.dataset\n    import inspect\n    config = cp.deepcopy(cfg[dataset_name])\n    if config == {}:\n        return supported_video_datasets[dataset_name]()\n    assert 'class' in config\n    cls_name = config.pop('class')\n    if hasattr(vlmeval.dataset, cls_name):\n        cls = getattr(vlmeval.dataset, cls_name)\n        sig = inspect.signature(cls.__init__)\n        valid_params = {k: v for k, v in config.items() if k in sig.parameters}\n        if cls.MODALITY == 'VIDEO':\n            if valid_params.get('fps', 0) > 0 and valid_params.get('nframe', 0) > 0:\n                raise ValueError('fps and nframe should not be set at the same time')\n            if valid_params.get('fps', 0) <= 0 and valid_params.get('nframe', 0) <= 0:\n                raise ValueError('fps and nframe should be set at least one valid value')\n        return cls(**valid_params)\n    else:\n        raise ValueError(f'Class {cls_name} is not supported in `vlmeval.dataset`')\n\n\ndef parse_args():\n    help_msg = \"\"\"\\\nYou can launch the evaluation by setting either --data and --model or --config.\n\n--data and --model:\n    Each Arg should be a list of strings, specifying the names of datasets and models.\n    To find all supported model names, please refer to the `vlmeval/config.py` of check the output of the command \\\n        `vlmutil mlist all` in the terminal (you should first have vlmeval installed).\n    To find all supported dataset names, please refer to the `vlmeval/dataset/__init__.py` file. The python script \\\n        to print all supported dataset names is as follows:\n        ```python\n        from vlmeval.dataset import SUPPORTED_DATASETS\n        print(SUPPORTED_DATASETS)\n        ```\n        or you can check the output of the command `vlmutil dlist all` in the terminal.\n    To find all supported video dataset default settings, please refer to the \\\n        `vlmeval/dataset/video_dataset_config.py` file.\n\n--config:\n    Launch the evaluation by specifying the path to the config json file. Sample Json Content:\n    ```json\n    {\n        \"model\": {\n            \"GPT4o_20240806_T00_HIGH\": {\n                \"class\": \"GPT4V\",\n                \"model\": \"gpt-4o-2024-08-06\",\n                \"temperature\": 0,\n                \"img_detail\": \"high\"\n            },\n            \"GPT4o_20240806_T10_Low\": {\n                \"class\": \"GPT4V\",\n                \"model\": \"gpt-4o-2024-08-06\",\n                \"temperature\": 1.0,\n                \"img_detail\": \"low\"\n            },\n            \"GPT4o_20241120\": {}\n        },\n        \"data\": {\n            \"MME-RealWorld-Lite\": {\n                \"class\": \"MMERealWorld\",\n                \"dataset\": \"MME-RealWorld-Lite\"\n            },\n            \"MMBench_DEV_EN_V11\": {\n                \"class\": \"ImageMCQDataset\",\n                \"dataset\": \"MMBench_DEV_EN_V11\"\n            },\n            \"MMBench_Video_8frame_nopack\": {},\n            \"Video-MME_16frame_subs\": {\n                \"class\": \"VideoMME\",\n                \"dataset\": \"Video-MME\",\n                \"nframe\": 16,\n                \"use_subtitle\": true,\n            }\n        }\n    }\n    ```\n    Currently, only `model` and `data` are supported fields. The content of each field is a dictionary.\n    For `model`, the key is the name of the model, and the value is a dictionary containing the following keys:\n    - `class`: The class name of the model, which should be a class in `vlmeval.vlm` or `vlmeval.api`.\n    - Other keys are specific to the model, please refer to the corresponding class.\n    - Tip: The defined model in the `supported_VLM` of `vlmeval/config.py` can be used as a shortcut.\n    For `data`, the key is the name of the dataset (should be the same as the `dataset` field in most cases, \\\n        except for video datasets), and the value is a dictionary containing the following keys:\n    - `class`: The class name of the dataset, which should be a class in `vlmeval.dataset`.\n    - `dataset`: The name of the dataset, which should be a string that is accepted by the `dataset` argument of the \\\n        corresponding class.\n    - Other keys are specific to the dataset, please refer to the corresponding class.\n    - Tip: The defined dataset in the `supported_video_datasets` of `vlmeval/dataset/video_dataset_config.py` \\\n        can be used as a shortcut.\n\n    The keys in the `model` and `data` fields will be used for naming the prediction files and evaluation results.\n    When launching with `--config`, args for API VLMs, such as `--retry`, `--verbose`, will be ignored.\n\"\"\"\n    parser = argparse.ArgumentParser(description=help_msg, formatter_class=argparse.RawTextHelpFormatter)\n    # Essential Args, Setting the Names of Datasets and Models\n    parser.add_argument('--data', type=str, nargs='+', help='Names of Datasets')\n    parser.add_argument('--model', type=str, nargs='+', help='Names of Models')\n    parser.add_argument('--config', type=str, help='Path to the Config Json File')\n    # Work Dir\n    parser.add_argument('--work-dir', type=str, default='./outputs', help='select the output directory')\n    # Infer + Eval or Infer Only\n    parser.add_argument('--mode', type=str, default='all', choices=['all', 'infer'])\n    # API Kwargs, Apply to API VLMs and Judge API LLMs\n    parser.add_argument('--api-nproc', type=int, default=4, help='Parallel API calling')\n    parser.add_argument('--retry', type=int, default=None, help='retry numbers for API VLMs')\n    parser.add_argument('--judge-args', type=str, default=None, help='Judge arguments in JSON format')\n    # Explicitly Set the Judge Model\n    parser.add_argument('--judge', type=str, default=None)\n    # Logging Utils\n    parser.add_argument('--verbose', action='store_true')\n    # Configuration for Resume\n    # Ignore: will not rerun failed VLM inference\n    parser.add_argument('--ignore', action='store_true', help='Ignore failed indices. ')\n    # Reuse: will reuse the existing prediction files\n    parser.add_argument('--reuse', action='store_true')\n    # Reuse-aux: if set, when reuse is True, will also reuse the auxiliary evaluation files\n    parser.add_argument('--reuse-aux', type=bool, default=True, help='reuse auxiliary evaluation files')\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    logger = get_logger('RUN')\n    rank, world_size = get_rank_and_world_size()\n    args = parse_args()\n    use_config, cfg = False, None\n    if args.config is not None:\n        assert args.data is None and args.model is None, '--data and --model should not be set when using --config'\n        use_config, cfg = True, load(args.config)\n        args.model = list(cfg['model'].keys())\n        args.data = list(cfg['data'].keys())\n    else:\n        assert len(args.data), '--data should be a list of data files'\n\n    if rank == 0:\n        if not args.reuse:\n            logger.warning('--reuse is not set, will not reuse previous (before one day) temporary files')\n        else:\n            logger.warning('--reuse is set, will reuse the latest prediction & temporary pickle files')\n\n    if 'MMEVAL_ROOT' in os.environ:\n        args.work_dir = os.environ['MMEVAL_ROOT']\n\n    if not use_config:\n        for k, v in supported_VLM.items():\n            if hasattr(v, 'keywords') and 'retry' in v.keywords and args.retry is not None:\n                v.keywords['retry'] = args.retry\n                supported_VLM[k] = v\n            if hasattr(v, 'keywords') and 'verbose' in v.keywords and args.verbose is not None:\n                v.keywords['verbose'] = args.verbose\n                supported_VLM[k] = v\n\n    if world_size > 1:\n        local_rank = os.environ.get('LOCAL_RANK', 0)\n        torch.cuda.set_device(int(local_rank))\n        dist.init_process_group(\n            backend='nccl',\n            timeout=datetime.timedelta(seconds=int(os.environ.get('DIST_TIMEOUT', 3600)))\n        )\n\n    for _, model_name in enumerate(args.model):\n        model = None\n        date, commit_id = timestr('day'), githash(digits=8)\n        eval_id = f\"T{date}_G{commit_id}\"\n\n        pred_root = osp.join(args.work_dir, model_name, eval_id)\n        pred_root_meta = osp.join(args.work_dir, model_name)\n        os.makedirs(pred_root_meta, exist_ok=True)\n\n        prev_pred_roots = ls(osp.join(args.work_dir, model_name), mode='dir')\n        if len(prev_pred_roots) and args.reuse:\n            prev_pred_roots.sort()\n\n        if not osp.exists(pred_root):\n            os.makedirs(pred_root, exist_ok=True)\n\n        if use_config:\n            model = build_model_from_config(cfg['model'], model_name)\n\n        for _, dataset_name in enumerate(args.data):\n            if world_size > 1:\n                dist.barrier()\n\n            try:\n                result_file_base = f'{model_name}_{dataset_name}.xlsx'\n\n                if use_config:\n                    if world_size > 1:\n                        if rank == 0:\n                            dataset = build_dataset_from_config(cfg['data'], dataset_name)\n                        dist.barrier()\n                    dataset = build_dataset_from_config(cfg['data'], dataset_name)\n                    if dataset is None:\n                        logger.error(f'Dataset {dataset_name} is not valid, will be skipped. ')\n                        continue\n                else:\n                    dataset_kwargs = {}\n                    if dataset_name in ['MMLongBench_DOC', 'DUDE', 'DUDE_MINI', 'SLIDEVQA', 'SLIDEVQA_MINI']:\n                        dataset_kwargs['model'] = model_name\n\n                    # If distributed, first build the dataset on the main process for doing preparation works\n                    if world_size > 1:\n                        if rank == 0:\n                            dataset = build_dataset(dataset_name, **dataset_kwargs)\n                        dist.barrier()\n\n                    dataset = build_dataset(dataset_name, **dataset_kwargs)\n                    if dataset is None:\n                        logger.error(f'Dataset {dataset_name} is not valid, will be skipped. ')\n                        continue\n\n                # Handling Multi-Turn Dataset\n                if dataset.TYPE == 'MT':\n                    result_file_base = result_file_base.replace('.xlsx', '.tsv')\n\n                result_file = osp.join(pred_root, result_file_base)\n\n                # Reuse the previous prediction file if exists\n                if rank == 0 and len(prev_pred_roots):\n                    prev_result_files = []\n                    prev_pkl_file_list = []\n                    for root in prev_pred_roots[::-1]:\n                        if osp.exists(osp.join(root, result_file_base)):\n                            if args.reuse_aux:\n                                prev_result_files = fetch_aux_files(osp.join(root, result_file_base))\n                            else:\n                                prev_result_files = [osp.join(root, result_file_base)]\n                            break\n                        elif commit_id in root and len(ls(root)) and root != pred_root:\n                            temp_files = ls(root, match=[dataset_name, '.pkl'])\n                            if len(temp_files):\n                                prev_pkl_file_list.extend(temp_files)\n                                break\n                    if not args.reuse:\n                        prev_result_files = []\n                        prev_pkl_file_list = []\n                    if len(prev_result_files):\n                        for prev_result_file in prev_result_files:\n                            src = prev_result_file\n                            tgt = osp.join(pred_root, osp.basename(src))\n                            if not osp.exists(tgt):\n                                shutil.copy(src, tgt)\n                                logger.info(f'--reuse is set, will reuse the prediction file {src}.')\n                            else:\n                                logger.warning(f'File already exists: {tgt}')\n                        \n                    elif len(prev_pkl_file_list):\n                        for fname in prev_pkl_file_list:\n                            target_path = osp.join(pred_root, osp.basename(fname))\n                            if not osp.exists(target_path):\n                                shutil.copy(fname, target_path)\n                                logger.info(f'--reuse is set, will reuse the prediction pickle file {fname}.')\n                            else:\n                                logger.warning(f'File already exists: {target_path}')\n\n                if world_size > 1:\n                    dist.barrier()\n\n                if model is None:\n                    model = model_name  # which is only a name\n\n                # Perform the Inference\n                if dataset.MODALITY == 'VIDEO':\n                    model = infer_data_job_video(\n                        model,\n                        work_dir=pred_root,\n                        model_name=model_name,\n                        dataset=dataset,\n                        result_file_name=result_file_base,\n                        verbose=args.verbose,\n                        api_nproc=args.api_nproc)\n                elif dataset.TYPE == 'MT':\n                    model = infer_data_job_mt(\n                        model,\n                        work_dir=pred_root,\n                        model_name=model_name,\n                        dataset=dataset,\n                        verbose=args.verbose,\n                        api_nproc=args.api_nproc,\n                        ignore_failed=args.ignore)\n                else:\n                    model = infer_data_job(\n                        model,\n                        work_dir=pred_root,\n                        model_name=model_name,\n                        dataset=dataset,\n                        verbose=args.verbose,\n                        api_nproc=args.api_nproc,\n                        ignore_failed=args.ignore)\n\n                # Set the judge kwargs first before evaluation or dumping\n\n                judge_kwargs = {\n                    'nproc': args.api_nproc,\n                    'verbose': args.verbose,\n                    'retry': args.retry if args.retry is not None else 3,\n                    **(json.loads(args.judge_args) if args.judge_args else {}),\n                }\n\n                if args.retry is not None:\n                    judge_kwargs['retry'] = args.retry\n                if args.judge is not None:\n                    judge_kwargs['model'] = args.judge\n                else:\n                    if dataset.TYPE in ['MCQ', 'Y/N', 'MCQ_MMMU_Pro']:\n                        if listinstr(['WeMath'], dataset_name):\n                            judge_kwargs['model'] = 'gpt-4o-mini'\n                        else:\n                            judge_kwargs['model'] = 'chatgpt-0125'\n                    elif listinstr(['MMVet', 'LLaVABench', 'MMBench-Video'], dataset_name):\n                        judge_kwargs['model'] = 'gpt-4-turbo'\n                    elif listinstr(['MathVista', 'MathVerse', 'MathVision', 'DynaMath', 'VL-RewardBench', 'LogicVista'], dataset_name):  # noqa: E501\n                        judge_kwargs['model'] = 'gpt-4o-mini'\n                    elif listinstr(['MMLongBench', 'MMDU', 'DUDE', 'SLIDEVQA', 'MIA-Bench', 'WildVision'], dataset_name):  # noqa: E501\n                        judge_kwargs['model'] = 'gpt-4o'\n\n                if rank == 0:\n                    logger.info(judge_kwargs)\n\n                if world_size > 1:\n                    dist.barrier()\n\n                # Only Rank 0 handles the evaluation part\n                if rank == 0:\n                    # Prepare Submission Files for MMMU_TEST AND MMT-Bench_ALL\n                    if dataset_name in ['MMMU_TEST']:\n                        result_json = MMMU_result_transfer(result_file)\n                        logger.info(f'Transfer MMMU_TEST result to json for official evaluation, '\n                                    f'json file saved in {result_json}')\n                        continue\n                    elif 'MMT-Bench_ALL' in dataset_name:\n                        submission_file = MMTBench_result_transfer(result_file, **judge_kwargs)\n                        logger.info(f'Extract options from prediction of MMT-Bench FULL split for official evaluation '\n                                    f'(https://eval.ai/web/challenges/challenge-page/2328/overview), '\n                                    f'submission file saved in {submission_file}')\n                        continue\n\n                    # Skip the evaluation part if only infer\n                    if args.mode == 'infer':\n                        continue\n\n                    # Skip the evaluation part if the dataset evaluation is not supported or annotations are missing\n                    if 'MLLMGuard_DS' in dataset_name:\n                        logger.info('The evaluation of MLLMGuard_DS is not supported yet. ')\n                        continue\n                    elif 'AesBench_TEST' == dataset_name:\n                        logger.info(f'The results are saved in {result_file}. '\n                                    f'Please send it to the AesBench Team via huangyipo@hotmail.com.')\n                        continue\n                    elif dataset_name in ['DocVQA_TEST', 'InfoVQA_TEST', 'Q-Bench1_TEST', 'A-Bench_TEST']:\n                        logger.info(f'{dataset_name} is a test split without ground-truth. '\n                                    'Thus only the inference part is supported for those datasets. ')\n                        continue\n                    elif dataset_name in [\n                        'MMBench_TEST_CN', 'MMBench_TEST_EN', 'MMBench', 'MMBench_CN',\n                        'MMBench_TEST_CN_V11', 'MMBench_TEST_EN_V11', 'MMBench_V11', 'MMBench_CN_V11'\n                    ] and not MMBenchOfficialServer(dataset_name):\n                        logger.error(\n                            f'Can not evaluate {dataset_name} on non-official servers, will skip the evaluation.')\n                        continue\n\n                    # Setup the proxy for the evaluation\n                    eval_proxy = os.environ.get('EVAL_PROXY', None)\n                    old_proxy = os.environ.get('HTTP_PROXY', '')\n                    if eval_proxy is not None:\n                        proxy_set(eval_proxy)\n\n                    # Perform the Evaluation\n                    eval_results = dataset.evaluate(result_file, **judge_kwargs)\n                    # Display Evaluation Results in Terminal\n                    if eval_results is not None:\n                        assert isinstance(eval_results, dict) or isinstance(eval_results, pd.DataFrame)\n                        logger.info(f'The evaluation of model {model_name} x dataset {dataset_name} has finished! ')\n                        logger.info('Evaluation Results:')\n                        if isinstance(eval_results, dict):\n                            logger.info('\\n' + json.dumps(eval_results, indent=4))\n                        elif isinstance(eval_results, pd.DataFrame):\n                            if len(eval_results) < len(eval_results.columns):\n                                eval_results = eval_results.T\n                            logger.info('\\n' + tabulate(eval_results))\n\n                    # Restore the proxy\n                    if eval_proxy is not None:\n                        proxy_set(old_proxy)\n\n                    # Create the symbolic links for the prediction files\n                    files = os.listdir(pred_root)\n                    files = [x for x in files if (f'{model_name}_{dataset_name}' in x or \"status.json\" in x)]\n                    for f in files:\n                        cwd = os.getcwd()\n                        file_addr = osp.join(cwd, pred_root, f)\n                        link_addr = osp.join(cwd, pred_root_meta, f)\n                        if osp.exists(link_addr) or osp.islink(link_addr):\n                            os.remove(link_addr)\n                        os.symlink(file_addr, link_addr)\n\n            except Exception as e:\n                logger.exception(f'Model {model_name} x Dataset {dataset_name} combination failed: {e}, '\n                                 'skipping this combination.')\n                continue\n\n    if world_size > 1:\n        dist.destroy_process_group()\n\n\nif __name__ == '__main__':\n    load_env()\n    main()\n"}
{"type": "source_file", "path": "eval/RULER/scripts/eval/evaluate.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nGet summary.csv with score and null predictions amount.\n\nRunning\n```\npython evaluate.py \\\n    --data_dir /path/to/your/prediction_jsonl_folder \\\n    --benchmark synthetic\n```\n\"\"\"\n\nimport re\nimport os\nimport argparse\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n    \nimport pandas as pd\nimport importlib\nimport yaml\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--data_dir\", type=str, required=True, help='path to the prediction jsonl files')\nparser.add_argument(\"--benchmark\", type=str, default='synthetic', help='Options: [synthetic]')\nparser.add_argument(\"--verbose\", type=int, default=0, help='how many lines you want to display.')\nargs = parser.parse_args()\n\n\ndef postprocess_pred(predict_str: str, task_config: dict):\n\n    predict_str = predict_str.strip()\n\n    # Remove all non-printable characters\n    np_pattern = re.compile(r'[\\x00-\\x1f]')\n    predict_str = np_pattern.sub('\\n', predict_str).strip()\n\n    return predict_str\n\n\ndef get_pred_and_ref(\n    predictions_file: str,\n    task_config: dict,\n    input_field: str = 'input',\n    references_field: str = 'outputs',\n    prediction_field: str = 'pred',\n    metadata_field: str = 'others',\n):\n    lines = read_manifest(predictions_file)\n\n    inputs = []\n    predicts = []\n    references = []\n    indices = []\n\n    for line in tqdm(lines):\n        input = line[input_field]\n        predict = line[prediction_field]\n        predict = postprocess_pred(predict, task_config)\n        reference = line.get(references_field, [line.get('output', '')])\n        index = line[metadata_field].get('id', line['index'])\n        \n        inputs.append(input)\n        predicts.append(predict)\n        references.append(reference)\n        indices.append(index)\n        \n    return inputs, predicts, references, indices\n\ndef run_evaluation_per_task(task_config: dict, predictions_file: str, verbose: int = 0):\n    inputs, predicts, references, indices = get_pred_and_ref(\n        predictions_file=predictions_file,\n        task_config=task_config,\n    )\n\n    task_nulls = f'{sum([len(x)==0 for x in predicts])}/{len(predicts)}'\n\n    if len(references) > 0 and references[0][0] is not None:\n        task_score = task_config['metric_fn'](predicts, references)\n    else:\n        task_score = 0.0\n\n    if verbose != 0:\n        print('=' * 40)\n        for i, (input, reference, predict) in enumerate(zip(inputs, references, predicts)):\n            print(f'Input     : {input}')\n            print(f'Reference : {reference}')\n            print(f'Prediction: {predict}')\n            print('=' * 40)\n            if i > verbose:\n                break\n\n    return task_score, task_nulls, predicts, indices\n\n\ndef write_evaluation(results: dict):\n    tasks = list(results.keys())\n    score = [results[task]['score'] for task in tasks]\n    nulls = [results[task]['nulls'] for task in tasks]\n    dfs = [\n        ['Tasks'] + tasks,\n        ['Score'] + score,\n        ['Nulls'] + nulls,\n    ]\n\n    output_file = os.path.join(args.data_dir, 'summary.csv' if len(tasks) > 1 else f'summary-{tasks[0]}.csv')\n    df = pd.DataFrame(dfs)\n    df.to_csv(output_file, index=False)\n    print('\\n=============================================\\n')\n    print(df)\n    print(f'\\nSaved eval results to {output_file}')\n\n\ndef write_submission(results: dict):\n    COLUMNS = [\"Task\", \"ID\", \"Prediction\"]\n    dfs = pd.DataFrame(columns=COLUMNS, data=[])\n    \n    for task, result in results.items():\n        df = pd.DataFrame({\n            'Task': task,\n            'ID': result['indices'], \n            'Prediction': result['predicts']\n        })\n        dfs = pd.concat((dfs, df[COLUMNS]))\n        \n    output_file = os.path.join(args.data_dir, 'submission.csv')\n    dfs = dfs.reset_index(drop=True)\n    dfs.to_csv(output_file, index=False)\n    print(f'\\nSaved submission results to {output_file}')\n\n\ndef aggregate_chunk(folder):\n    jsonl_files = [file for file in os.listdir(folder) if Path(file).suffix == '.jsonl' ]\n    chunk_files = sorted([file for file in jsonl_files if re.match(r'.*[^_]+-\\d+\\.jsonl', file)])\n    chunk_files_dict = defaultdict(list)\n    for file in chunk_files:\n        task = '-'.join(file.split('-')[:-1])\n        chunk_files_dict[task].append(file)\n\n    for task, files in chunk_files_dict.items():\n        lines = []\n        for file in sorted(files):\n            file = os.path.join(folder, file)\n            lines += read_manifest(file)\n            os.remove(file) # Remove chunk files\n        write_manifest(os.path.join(folder, f'{task}.jsonl'), lines)\n\n\ndef main():\n    curr_folder = os.path.dirname(os.path.abspath(__file__))\n    \n    try:\n        module = importlib.import_module(f\"{args.benchmark}.constants\")\n    except ImportError:\n        print(f\"Module eval.{args.benchmark}.constants not found.\")\n\n    tasks_base = module.TASKS\n    with open(os.path.join(curr_folder, f\"../{args.benchmark}.yaml\"), \"r\") as f:\n        tasks_customized = yaml.safe_load(f)\n\n        \n    TASKS = tasks_customized\n    for _, config in TASKS.items():\n        config.update(tasks_base[config['task']])\n\n    print(f\"Total tasks: {list(TASKS.keys())}\")\n\n    # Aggregate all prediction files\n    aggregate_chunk(args.data_dir)\n\n    # Get scores and nulls\n    jsonl_files = [file for file in os.listdir(args.data_dir) if Path(file).suffix == '.jsonl']\n    eval_results = {}\n    subm_results = {}\n\n\n    for task, config in TASKS.items():\n        if f'{task}.jsonl' not in jsonl_files:\n            print(f'Prediction file {task}.jsonl is not found.')\n            continue\n\n        print(f'Evaluate task {task}...')\n        task_score, task_nulls, predicts, indices = run_evaluation_per_task(\n            predictions_file=os.path.join(args.data_dir, f'{task}.jsonl'),\n            task_config=config,\n        )\n        eval_results[task] = {\n            'score': task_score,\n            'nulls': task_nulls,\n        }\n        subm_results[task] = {\n            'predicts': predicts,\n            'indices':indices,\n        }\n        \n    # Write to csv\n    write_evaluation(eval_results)\n    write_submission(subm_results)\n\nif __name__ == '__main__':\n    main()"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/variable_tracking.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nCreate a dataset jsonl file for variable tracking.\n\npython variable_tracking.py   \\\n    --save_dir=./ \\\n    --save_name=vt \\\n    --tokenizer_path=tokenizer.model \\\n    --tokenizer_type nemo \\\n    --max_seq_length 4096 \\\n    --tokens_to_generate 30 \\\n    --num_samples 10 \\\n    --random_seed 42  \\\n    --num_chains 1  --num_hops 4 \\\n    --template \"[INST] Memorize and track the chain(s) of variable assignment hidden in the following text.\\n\\n{context}\\nQuestion: Find all variables that are assigned the value {query} in the text above. [/INST] Answer: According to the chain(s) of variable assignment in the text above, {num_v} variables are assgined the value {query}, they are: \"\n\"\"\"\nimport os\nimport argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport string\nfrom constants import TASKS\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\")) \nfrom tokenizer import select_tokenizer\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--save_name\", type=str, required=True, help='name of the save dataset jsonl file')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--tokens_to_generate\", type=int, default=120, help='number of tokens to generate')\nparser.add_argument(\"--num_samples\", type=int, required=True, help='number of samples to generate')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--template\", type=str, default='', help='prompt template')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\n\nparser.add_argument(\"--num_chains\", type=int, default=1, help='number of inserted variable chains')\nparser.add_argument(\"--num_hops\", type=int, default=4, help='number of hops in each chain')\nparser.add_argument(\"--add_fewshot\", action=\"store_true\", default=False)\n\nargs = parser.parse_args()\nrandom.seed(args.random_seed)\nnp.random.seed(args.random_seed)\n\n# Load Tokenizer\nTOKENIZER = select_tokenizer(args.tokenizer_type, args.tokenizer_path)\n\ndef generate_chains(num_chains, num_hops, is_icl=False):\n    \n    vars_all = []\n    k = 5 if not is_icl else 3\n    num_hops = num_hops if not is_icl else min(10, num_hops)\n    vars_all = [''.join(random.choices(string.ascii_uppercase, k=k)).upper() for _ in range((num_hops+1) * num_chains)]\n    while len(set(vars_all)) < num_chains * (num_hops+1):\n        vars_all.append(''.join(random.choices(string.ascii_uppercase, k=k)).upper())\n\n    vars_ret = []\n    chains_ret = []\n    for i in range(0, len(vars_all), num_hops+1):\n        this_vars = vars_all[i:i+num_hops+1]\n        vars_ret.append(this_vars)\n        this_chain = [f\"VAR {this_vars[0]} = {np.random.randint(10000, 99999)}\"]\n        for j in range(num_hops):\n            this_chain.append(f\"VAR {this_vars[j+1]} = VAR {this_vars[j]} \")\n        chains_ret.append(this_chain)\n    return vars_ret, chains_ret\n    \ndef generate_input_output(num_noises, num_chains, num_hops, is_icl=False):\n\n    vars, chains = generate_chains(num_chains, num_hops, is_icl=is_icl)\n\n    noise = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\\n\"\n\n    # Create a list of the repeated noise\n    sentences = [noise] * num_noises\n    if len(sentences) <= len(chains[0]):\n        sentences = [n + '.' if len(n.strip()) > 0 else n for n in [x for noise in sentences for x in noise.split('.')] ]\n        try:\n            assert len(sentences) > len(chains[0]), \"Noises too short, unable to generate data\"\n        except:\n            print(\"reduces chain length for not enough noises\")\n            chains = [chain[:len(sentences)-1] for chain in chains]\n    # sample random positions to insert variable assignment\n    for chain_i in chains:\n        # sample random positions (sorted) to insert variable assignment\n        positions = list(sorted(random.sample(range(len(sentences)), len(chain_i))))\n        for insert_pi, j in zip(positions, range(len(chain_i))):\n            sentences.insert(insert_pi+j, chain_i[j])\n\n    # Insert the passkey sentence at the random position\n    context = \" \".join(sentences)\n    context = context.replace(\". \\n\", \".\\n\")\n\n    template = args.template\n    if is_icl:\n        # remove model template\n        cutoff = template.index(TASKS['variable_tracking']['template'][:20])\n        cutoff_ans = template.index(TASKS['variable_tracking']['answer_prefix'][:10])\n        template = ' '.join(template[cutoff:cutoff_ans].split()[:-1]) + template[cutoff_ans:]\n\n    value = chains[0][0].split(\"=\")[-1].strip()\n    input_text = template.format(\n        context=context,\n        query=value,\n        num_v=num_hops+1\n    )\n\n    return input_text, vars[0]\n\n\ndef sys_vartrack_w_noise_random(num_samples: int, max_seq_length: int, incremental: int = 10, \n                                num_chains: int = 1, num_hops: int = 4,\n                                add_fewshot: bool = True,\n                                icl_example: str = None):\n    write_jsons = []\n    tokens_to_generate = args.tokens_to_generate\n    \n    # Find the perfect num_noises\n    num_noises = incremental\n        \n    total_tokens = 0  # Track the total tokens generated for this example\n    example_tokens = 0\n    if add_fewshot and (icl_example is not None):\n        icl_example_out = ' '.join(icl_example['outputs'])\n        icl_example = icl_example['input'] + \" \" + icl_example_out + '\\n\\n'\n        example_tokens = len(TOKENIZER.text_to_tokens(icl_example)) \n        \n    while total_tokens + tokens_to_generate + example_tokens < max_seq_length :\n        input_text, answer = generate_input_output(num_noises, num_chains, num_hops, is_icl=add_fewshot & (icl_example is None))\n        # Calculate the number of tokens in the example\n        total_tokens = len(TOKENIZER.text_to_tokens(input_text + f' {answer}'))\n        print(f'Max length {max_seq_length} | Current length {total_tokens + tokens_to_generate + example_tokens} | Noises: {num_noises}')\n        if total_tokens + tokens_to_generate + example_tokens > max_seq_length:\n            num_noises -= incremental\n            break\n        num_noises += incremental\n    print('Num noises:', num_noises)\n    \n    # Generate samples\n    for index in tqdm(range(num_samples)):\n        used_noises = num_noises\n        while(True):\n            try:\n                input_text, answer = generate_input_output(used_noises, num_chains, num_hops, is_icl=add_fewshot & (icl_example is None))\n                length = len(TOKENIZER.text_to_tokens(input_text)) + tokens_to_generate + example_tokens\n                assert length <= max_seq_length, f\"{length} exceeds max_seq_length.\"\n                break\n            except:\n                if used_noises > incremental:\n                    used_noises -= incremental\n\n        if add_fewshot and (icl_example is not None):\n            # insert icl_example between model template and input\n            cutoff = input_text.index(TASKS['variable_tracking']['template'][:20])\n            input_text = input_text[:cutoff] + ' ' + icl_example + '\\n\\n' + input_text[cutoff:]\n        if args.remove_newline_tab:\n            input_text = ' '.join(input_text.replace('\\n', ' ').replace('\\t', ' ').strip().split())\n        \n        formatted_output = {\n            'index': index,\n            \"input\": input_text,\n            \"outputs\": answer,\n            \"length\": length,\n        }\n        write_jsons.append(formatted_output)\n\n    return write_jsons\n\n\ndef main():   \n    save_file = args.save_dir / f'{args.save_name}' / f'{args.subset}.jsonl'\n    save_file.parent.mkdir(parents=True, exist_ok=True)\n\n    icl_example = sys_vartrack_w_noise_random(num_samples=1, \n                                              max_seq_length=500, \n                                              incremental=5,\n                                              num_chains=args.num_chains, \n                                              num_hops=args.num_hops)[0]\n    write_jsons = sys_vartrack_w_noise_random(num_samples=args.num_samples,\n                                              max_seq_length=args.max_seq_length, \n                                              num_chains=args.num_chains,\n                                              num_hops=args.num_hops,\n                                              icl_example=icl_example)\n    \n    write_manifest(save_file, write_jsons)\n\nif __name__==\"__main__\":\n    main()\n"}
{"type": "source_file", "path": "eval/RULER/scripts/data/tokenizer.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom typing import List\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_fixed,\n    wait_random,\n) \n\n\ndef select_tokenizer(tokenizer_type, tokenizer_path):\n    if tokenizer_type == 'nemo':\n        return NeMoSentencePieceTokenizer(model_path=tokenizer_path)\n    elif tokenizer_type == 'hf':\n        return HFTokenizer(model_path=tokenizer_path)\n    elif tokenizer_type == 'openai':\n        return OpenAITokenizer(model_path=tokenizer_path)\n    elif tokenizer_type == 'gemini':\n        return GeminiTokenizer(model_path=tokenizer_path)\n    else:\n        raise ValueError(f\"Unknown tokenizer_type {tokenizer_type}\")\n\n\nclass NeMoSentencePieceTokenizer:\n    \"\"\"\n    Tokenizer from NeMo SentencePieceTokenizer\n    \"\"\"\n    def __init__(self, model_path) -> None:\n        from nemo.collections.common.tokenizers.sentencepiece_tokenizer import SentencePieceTokenizer\n        self.tokenizer = SentencePieceTokenizer(model_path=model_path)\n    \n    def text_to_tokens(self, text: str) -> List[str]:\n        tokens = self.tokenizer.text_to_tokens(text)\n        return tokens\n\n    def tokens_to_text(self, tokens: List[int]) -> str:\n        text = self.tokenizer.tokens_to_text(tokens)\n        return text\n\n\nclass HFTokenizer:\n    \"\"\"\n    Tokenizer from HF models\n    \"\"\"\n    def __init__(self, model_path) -> None:\n        from transformers import AutoTokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    \n    def text_to_tokens(self, text: str) -> List[str]:\n        tokens = self.tokenizer.tokenize(text)\n        return tokens\n\n    def tokens_to_text(self, tokens: List[int]) -> str:\n        text = self.tokenizer.convert_tokens_to_string(tokens)\n        return text\n\n\nclass OpenAITokenizer:\n    \"\"\"\n    Tokenizer from tiktoken\n    \"\"\"\n    def __init__(self, model_path=\"cl100k_base\") -> None:\n        import tiktoken\n        self.tokenizer = tiktoken.get_encoding(model_path)\n\n    def text_to_tokens(self, text: str) -> List[int]:\n        tokens = self.tokenizer.encode(text)\n        return tokens\n\n    def tokens_to_text(self, tokens: List[int]) -> str:\n        text = self.tokenizer.decode(tokens)\n        return text\n\n\nclass GeminiTokenizer:\n    \"\"\"\n    Tokenizer from gemini\n    \"\"\"\n    def __init__(self, model_path=\"gemini-1.5-pro-latest\") -> None:\n        import google.generativeai as genai\n        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n        self.model = genai.GenerativeModel(model_path)\n        \n    @retry(wait=wait_fixed(60) + wait_random(0, 10), stop=stop_after_attempt(3))\n    def text_to_tokens(self, text: str) -> List[int]:\n        tokens = list(range(self.model.count_tokens(text).total_tokens))\n        return tokens\n\n    def tokens_to_text(self, tokens: List[int]) -> str:\n        pass"}
{"type": "source_file", "path": "eval/RULER/scripts/data/synthetic/qa.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\n\"\"\"\nCreate a dataset jsonl file for QA task.\n\npython qa.py \\\n    --save_dir=./ \\\n    --save_name=niah_single \\\n    --tokenizer_path=tokenizer.model \\\n    --tokenizer_type=nemo \\\n    --max_seq_length=4096 \\\n    --tokens_to_generate=128 \\\n    --num_samples=10 \\\n    --template=\"Answer the question based on the given documents. Only give me the answer and do not output any other words.\\n\\nThe following are given documents.\\n\\n{context}\\n\\nAnswer the question based on the given documents. Only give me the answer and do not output any other words.\\n\\nQuestion: {query} Answer:\"\n\"\"\"\nimport os\nimport re\nimport json\nimport argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport numpy as np\nfrom nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\nimport sys\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\")) \nfrom tokenizer import select_tokenizer\n\n\nparser = argparse.ArgumentParser()\n# Basic Configurations\nparser.add_argument(\"--save_dir\", type=Path, required=True, help='dataset folder to save dataset')\nparser.add_argument(\"--save_name\", type=str, required=True, help='name of the save dataset jsonl file')\nparser.add_argument(\"--subset\", type=str, default='validation', help='Options: validation or test')\nparser.add_argument(\"--tokenizer_path\", type=str, required=True, help='path to the tokenizer model')\nparser.add_argument(\"--tokenizer_type\",  type=str, default='nemo', help='[Options] nemo, hf, openai.')\nparser.add_argument(\"--max_seq_length\", type=int, required=True, help='max sequence length including all input tokens and generated tokens.')\nparser.add_argument(\"--tokens_to_generate\", type=int, required=True, help='expected generated token amount.')\nparser.add_argument(\"--num_samples\", type=int, required=True, help='number of samples to generate')\nparser.add_argument(\"--pre_samples\", type=int, default=0, help='number of samples are already generated')\nparser.add_argument(\"--random_seed\", type=int, default=42)\nparser.add_argument(\"--template\", type=str, required=True, help='prompt template')\nparser.add_argument(\"--remove_newline_tab\", action='store_true', help='remove `\\n` and `\\t` in all strings.')\n\n# Complexity Configurations\nparser.add_argument(\"--dataset\", type=str, required=True, help='dataset file')\n\nargs = parser.parse_args()\nrandom.seed(args.random_seed)\nnp.random.seed(args.random_seed)\n\n# Load Tokenizer\nTOKENIZER = select_tokenizer(args.tokenizer_type, args.tokenizer_path)\n\n# Read SQuAD QA dataset\ndef read_squad(file):\n    with open(file) as f:\n        data = json.load(f)\n        \n    total_docs = [p['context'] for d in data['data'] for p in d['paragraphs']]\n    total_docs = sorted(list(set(total_docs)))\n    total_docs_dict = {c: idx for idx, c in enumerate(total_docs)}\n\n    total_qas = []\n    for d in data['data']:\n        more_docs = [total_docs_dict[p['context']] for p in d['paragraphs']]\n        for p in d['paragraphs']:\n            for qas in p['qas']:\n                if not qas['is_impossible']:\n                    total_qas.append({\n                        'query': qas['question'],\n                        'outputs': [a['text'] for a in qas['answers']],\n                        'context': [total_docs_dict[p['context']]],\n                        'more_context': [idx for idx in more_docs if idx != total_docs_dict[p['context']]]\n                    })\n                        \n    return total_qas, total_docs\n\n# Read Hotpot QA dataset\ndef read_hotpotqa(file):\n    with open(file) as f:\n        data = json.load(f)\n\n    total_docs = [f\"{t}\\n{''.join(p)}\" for d in data for t, p in d['context']]\n    total_docs = sorted(list(set(total_docs)))\n    total_docs_dict = {c: idx for idx, c in enumerate(total_docs)}\n    \n    total_qas = []\n    for d in data:\n        total_qas.append({\n            'query': d['question'],\n            'outputs': [d['answer']],\n            'context': [total_docs_dict[f\"{t}\\n{''.join(p)}\"] for t, p in d['context']],\n        })\n        \n    return total_qas, total_docs\n\n\nDOCUMENT_PROMPT = \"Document {i}:\\n{document}\"\nif args.dataset == 'squad':\n    QAS, DOCS = read_squad(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"json/squad.json\"))\nelif args.dataset == 'hotpotqa':\n    QAS, DOCS = read_hotpotqa(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"json/hotpotqa.json\"))\nelse:\n    raise NotImplementedError(f'{args.dataset} is not implemented.')\n\n\ndef generate_input_output(index, num_docs):\n    curr_q = QAS[index]['query']\n    curr_a = QAS[index]['outputs']\n    curr_docs = QAS[index]['context']\n    curr_more = QAS[index].get('more_context', [])\n    if num_docs < len(DOCS):\n        if (num_docs - len(curr_docs)) > len(curr_more):\n            addition_docs = [i for i, d in enumerate(DOCS) if i not in curr_docs + curr_more]\n            all_docs = curr_docs + curr_more + random.sample(addition_docs, max(0, num_docs - len(curr_docs) - len(curr_more)))\n        else:\n            all_docs = curr_docs + random.sample(curr_more, num_docs - len(curr_docs))\n    \n        all_docs = [DOCS[idx] for idx in all_docs]\n    else:\n        all_docs = DOCS\n        \n    random.Random(args.random_seed).shuffle(all_docs)\n    \n    context = '\\n\\n'.join([DOCUMENT_PROMPT.format(i=i+1, document=d) for i, d in enumerate(all_docs)])\n    input_text = args.template.format(\n        context=context, \n        query=curr_q\n    )\n    return input_text, curr_a\n\n\ndef generate_samples(num_samples: int, max_seq_length: int, save_dir: str, incremental: int = 10): \n    \n    write_jsons = []\n    tokens_to_generate = args.tokens_to_generate\n    \n    # Find the perfect num_docs\n    num_docs = incremental\n    \n    total_tokens = 0  # Track the total tokens generated for this example\n    while total_tokens + tokens_to_generate < max_seq_length :  \n        input_text, answer = generate_input_output(0, num_docs)\n        # Calculate the number of tokens in the example\n        total_tokens = len(TOKENIZER.text_to_tokens(input_text + f' {answer}'))\n        print(f'Max length {max_seq_length} | Current length {total_tokens + tokens_to_generate} | Docs: {num_docs}')\n        if total_tokens + tokens_to_generate > max_seq_length:\n            num_docs -= incremental\n            break\n            \n        num_docs += incremental\n        if num_docs > len(DOCS):\n            num_docs = len(DOCS)\n            break\n    print('Number of documents:', num_docs)\n    \n    # Generate samples\n    for index in tqdm(range(num_samples)):\n        used_docs = num_docs\n        while(True):\n            try:\n                input_text, answer = generate_input_output(index + args.pre_samples, used_docs)\n                length = len(TOKENIZER.text_to_tokens(input_text)) + tokens_to_generate\n                assert length <= max_seq_length, f\"{length} exceeds max_seq_length.\"\n                break\n            except:\n                if used_docs > incremental:\n                    used_docs -= incremental\n        \n        if args.remove_newline_tab:\n            input_text = ' '.join(input_text.replace('\\n', ' ').replace('\\t', ' ').strip().split())\n        \n        formatted_output = {\n            \"index\": index,\n            \"input\": input_text,\n            \"outputs\": answer,\n            \"length\": length\n        }\n        write_jsons.append(formatted_output)\n\n    return write_jsons\n\n\ndef main():\n    save_file = args.save_dir / f'{args.save_name}' / f'{args.subset}.jsonl'\n    save_file.parent.mkdir(parents=True, exist_ok=True)\n\n    write_jsons = generate_samples(\n        num_samples=args.num_samples, \n        max_seq_length=args.max_seq_length, \n        save_dir=args.save_dir\n    )\n    \n    write_manifest(save_file, write_jsons)\n\nif __name__==\"__main__\":\n    main()\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/scripts/apires_scan.py", "content": "import sys\nfrom vlmeval import *\nfrom vlmeval.dataset import SUPPORTED_DATASETS\nFAIL_MSG = 'Failed to obtain answer via API.'\n\nroot = sys.argv[1]\nif root[-1] in '/\\\\':\n    root = root[:-1]\n\nmodel_name = root.split('/')[-1]\n\nfor d in SUPPORTED_DATASETS:\n    fname = f'{model_name}_{d}.xlsx'\n    pth = osp.join(root, fname)\n    if osp.exists(pth):\n        data = load(pth)\n        # Detect Failure\n        assert 'prediction' in data\n        data['prediction'] = [str(x) for x in data['prediction']]\n        fail = [FAIL_MSG in x for x in data['prediction']]\n        if sum(fail):\n            nfail = sum(fail)\n            ntot = len(fail)\n            print(f'Model {model_name} x Dataset {d}: {nfail} out of {ntot} failed. {nfail / ntot * 100: .2f}%. ')\n\n        eval_files = ls(root, match=f'{model_name}_{d}_')\n        eval_files = [x for x in eval_files if listinstr([f'{d}_openai', f'{d}_gpt'], x) and x.endswith('.xlsx')]\n\n        if len(eval_files) == 0:\n            print(f'Model {model_name} x Dataset {d} openai missing')\n            continue\n        \n        assert len(eval_files) == 1\n        eval_file = eval_files[0]\n        data = load(eval_file)\n        \n        if 'MMVet' in d:\n            bad = [x for x in data['log'] if 'All 5 retries failed.' in str(x)]\n            if len(bad):\n                print(f'Model {model_name} x Dataset {d} Evaluation: {len(bad)} out of {len(data)} failed.')\n        elif 'MathVista' in d:\n            bad = [x for x in data['res'] if FAIL_MSG in str(x)]\n            if len(bad):\n                print(f'Model {model_name} x Dataset {d} Evaluation: {len(bad)} out of {len(data)} failed.')\n            \n        elif d == 'LLaVABench':\n            sub = data[data['gpt4_score'] == -1]\n            sub = sub[sub['gpt4_score'] == -1]\n            if len(sub):\n                print(f'Model {model_name} x Dataset {d} Evaluation: {len(sub)} out of {len(data)} failed.')\n        else:\n            bad = [x for x in data['log'] if FAIL_MSG in str(x)]\n            if len(bad):\n                print(f'Model {model_name} x Dataset {d} Evaluation: {len(bad)} out of {len(data)} failed.')\n                "}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/glm_vision.py", "content": "import requests\nrequests.packages.urllib3.disable_warnings()\n\nfrom vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.dataset import DATASET_TYPE\nfrom vlmeval.smp.vlm import encode_image_file_to_base64\n\n\nclass GLMVisionWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str,\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 max_tokens: int = 4096,\n                 proxy: str = None,\n                 **kwargs):\n                 \n        from zhipuai import ZhipuAI\n        self.model = model\n        self.fail_msg = 'Failed to obtain answer via API. '\n        if key is None:\n            key = os.environ.get('GLMV_API_KEY', None)\n        assert key is not None, (\n            'Please set the API Key (obtain it here: '\n            'https://bigmodel.cn)'\n        )\n        self.client = ZhipuAI(api_key=key)\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    def build_msgs(self, msgs_raw, system_prompt=None, dataset=None):\n        msgs = cp.deepcopy(msgs_raw)\n        content = []\n        for i, msg in enumerate(msgs):\n            if msg['type'] == 'text':\n                content.append(dict(type='text', text=msg['value']))\n            elif msg['type'] == 'image':\n                content.append(dict(type='image_url', image_url=dict(url=encode_image_file_to_base64(msg['value']))))\n        if dataset in {'HallusionBench', 'POPE'}:\n            content.append(dict(type=\"text\", text=\"Please answer yes or no.\"))\n        ret = [dict(role='user', content=content)]\n        return ret\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n        inputs = [inputs] if isinstance(inputs, str) else inputs\n\n        messages = self.build_msgs(msgs_raw=inputs, dataset=kwargs.get('dataset', None))\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            do_sample=False,\n            max_tokens=2048\n        )\n        try:\n            answer = response.choices[0].message.content.strip()\n            if self.verbose:\n                self.logger.info(f'inputs: {inputs}\\nanswer: {answer}')\n            return 0, answer, 'Succeeded!'\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(f'The input messages are {inputs}.')\n            return -1, self.fail_msg, ''\n\n\nclass GLMVisionAPI(GLMVisionWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(GLMVisionAPI, self).generate(message, dataset=dataset)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/__init__.py", "content": "try:\n    import torch\nexcept ImportError:\n    pass\n\nfrom .smp import *\nfrom .api import *\nfrom .dataset import *\nfrom .utils import *\nfrom .vlm import *\nfrom .config import *\nfrom .tools import cli\n\nload_env()\n\n__version__ = '0.2rc1'\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/doubao_vl_api.py", "content": "from vlmeval.smp import *\nimport os\nimport sys\nfrom vlmeval.api.base import BaseAPI\nimport math\nfrom vlmeval.dataset import DATASET_TYPE\nfrom vlmeval.dataset import img_root_map\nfrom io import BytesIO\nimport pandas as pd\nimport requests\nimport json\nimport base64\nimport time\nfrom openai import OpenAI\n\n\nclass DoubaoVLWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = '',\n                 retry: int = 5,\n                 wait: int = 5,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 temperature: float = 0,\n                 timeout: int = 60,\n                 max_tokens: int = 4096,\n                 api_base: str = 'https://ark.cn-beijing.volces.com/api/v3',#使用系统推荐的服务区域地址\n                 **kwargs):\n\n        self.model = model# This variable is unused\n        self.cur_idx = 0\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\n        warnings.warn('You may need to set the env variable  DOUBAO_VL_KEY& DOUBAO_VL_ENDPOINT to use DOUBAO_VL.')\n\n        key = os.environ.get('DOUBAO_VL_KEY', None)\n        assert key is not None, 'Please set the environment variable DOUBAO_VL_KEY. '\n        self.key = key\n\n        endpoint = os.getenv('DOUBAO_VL_ENDPOINT', None)\n        assert endpoint is not None, 'Please set the environment variable DOUBAO_VL_ENDPOINT. '\n        self.endpoint = endpoint\n\n        assert api_base is not None, 'Please set the variable API_BASE. '\n        self.api_base = api_base\n        self.timeout = timeout\n\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n\n        self.client = OpenAI(\n            api_key = self.key,\n            base_url = self.api_base,\n        )\n\n        self.logger.info(f'Using API Base: {self.api_base}; End Point: {self.endpoint}; API Key: {self.key}')\n\n    def dump_image(self, line, dataset):\n        \"\"\"Dump the image(s) of the input line to the corresponding dataset folder.\n\n        Args:\n            line (line of pd.DataFrame): The raw input line.\n            dataset (str): The name of the dataset.\n\n        Returns:\n            str | list[str]: The paths of the dumped images.\n        \"\"\"\n        ROOT = LMUDataRoot()\n        assert isinstance(dataset, str)\n\n        img_root = os.path.join(ROOT, 'images', img_root_map(dataset) if dataset in img_root_map(dataset) else dataset)\n        os.makedirs(img_root, exist_ok=True)\n        if 'image' in line:\n            if isinstance(line['image'], list):\n                tgt_path = []\n                assert 'image_path' in line\n                for img, im_name in zip(line['image'], line['image_path']):\n                    path = osp.join(img_root, im_name)\n                    if not read_ok(path):\n                        decode_base64_to_image_file(img, path)\n                    tgt_path.append(path)\n            else:\n                tgt_path = osp.join(img_root, f\"{line['index']}.jpg\")\n                if not read_ok(tgt_path):\n                    decode_base64_to_image_file(line['image'], tgt_path)\n                tgt_path = [tgt_path]\n        else:\n            assert 'image_path' in line\n            tgt_path = toliststr(line['image_path'])\n\n        return tgt_path\n\n    def use_custom_prompt(self, dataset_name):\n        if dataset_name == 'MathVerse_MINI_Vision_Only':\n            return True\n        else:\n            return False\n\n\n    def build_prompt(self, line, dataset: str) -> list[dict[str, str]]:\n        \n        if dataset in {'MathVerse_MINI_Vision_Only'}:\n            return self. _build_mathVerse_mini_vision_only_prompt(line, dataset)\n        raise ValueError(f'Unsupported dataset: {dataset}')\n\n    def _build_mathVerse_mini_vision_only_prompt(self, line, dataset=None):\n        assert self.use_custom_prompt(dataset)\n        assert dataset is None or isinstance(dataset, str)\n\n        tgt_path = self.dump_image(line, dataset)\n\n        question = line['question']\n        \n        ###remove 'directly' from the prompt, so the model will answer the question in Chain-of-Thought (CoT) manner\n        prompt = question.replace('directly','',1)\n\n        msgs = []\n        if isinstance(tgt_path, list):\n            msgs.extend([dict(type='image', value=p) for p in tgt_path])\n        else:\n            msgs = [dict(type='image', value=tgt_path)]\n        msgs.append(dict(type='text', value=prompt))\n        return msgs\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(type='text', text=msg['value']))\n                elif msg['type'] == 'image':\n                    from PIL import Image\n                    img = Image.open(msg['value'])\n                    b64 = encode_image_to_base64(img)\n                    img_struct = dict(url=f'data:image/jpeg;base64,{b64}')\n                    content_list.append(dict(type='image_url', image_url=img_struct))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(type='text', text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(role='system', content=self.system_prompt))\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(role=item['role'], content=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(role='user', content=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n\n\n        input_msgs = self.prepare_inputs(inputs)\n        temperature = kwargs.pop('temperature', self.temperature)\n        max_tokens = kwargs.pop('max_tokens', self.max_tokens)\n\n        ret_code = -1 \n        answer = self.fail_msg\n        response = None\n        try:\n            response = self.client.chat.completions.create(\n                model=self.endpoint,\n                messages=input_msgs,\n                max_tokens=max_tokens,\n                temperature=temperature\n            )\n            answer = response.choices[0].message.content.strip()\n            ret_code = 0\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(response.text if hasattr(response, 'text') else response)\n\n        return ret_code, answer, response\n\nclass DoubaoVL(DoubaoVLWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(DoubaoVL, self).generate(message)\n\nif __name__ == '__main__':\n    #export DOUBAO_VL_KEY=''\n    #export DOUBAO_VL_ENDPOINT=''\n    model = DoubaoVLWrapper( verbose=True)\n    inputs = [\n        {'type': 'image', 'value': './assets/apple.jpg'},\n        {'type': 'text', 'value': '请详细描述一下这张图片。'},\n    ]\n    code, answer, resp = model.generate_inner(inputs)\n    print(code, answer, resp)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/setup.py", "content": "import re\nimport sys\nfrom os.path import exists\nfrom setuptools import find_packages, setup\n\n\ndef parse_requirements(fname='requirements.txt', with_version=True):\n    \"\"\"Parse the package dependencies listed in a requirements file but strips\n    specific versioning information.\n\n    Args:\n        fname (str): path to requirements file\n        with_version (bool, default=False): if True include version specs\n\n    Returns:\n        List[str]: list of requirements items\n\n    CommandLine:\n        python -c \"import setup; print(setup.parse_requirements())\"\n    \"\"\"\n\n    require_fpath = fname\n\n    def parse_line(line):\n        \"\"\"Parse information from a line in a requirements text file.\"\"\"\n        if line.startswith('-r '):\n            # Allow specifying requirements in other files\n            target = line.split(' ')[1]\n            for info in parse_require_file(target):\n                yield info\n        else:\n            info = {'line': line}\n            if line.startswith('-e '):\n                info['package'] = line.split('#egg=')[1]\n            elif '@git+' in line:\n                info['package'] = line\n            else:\n                # Remove versioning from the package\n                pat = '(' + '|'.join(['>=', '==', '>']) + ')'\n                parts = re.split(pat, line, maxsplit=1)\n                parts = [p.strip() for p in parts]\n\n                info['package'] = parts[0]\n                if len(parts) > 1:\n                    op, rest = parts[1:]\n                    if ';' in rest:\n                        # Handle platform specific dependencies\n                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n                        version, platform_deps = map(str.strip,\n                                                     rest.split(';'))\n                        info['platform_deps'] = platform_deps\n                    else:\n                        version = rest  # NOQA\n                    info['version'] = (op, version)\n            yield info\n\n    def parse_require_file(fpath):\n        with open(fpath, 'r') as f:\n            for line in f.readlines():\n                line = line.strip()\n                if line and not line.startswith('#'):\n                    for info in parse_line(line):\n                        yield info\n\n    def gen_packages_items():\n        if exists(require_fpath):\n            for info in parse_require_file(require_fpath):\n                parts = [info['package']]\n                if with_version and 'version' in info:\n                    parts.extend(info['version'])\n                if not sys.version.startswith('3.4'):\n                    # apparently package_deps are broken in 3.4\n                    platform_deps = info.get('platform_deps')\n                    if platform_deps is not None:\n                        parts.append(';' + platform_deps)\n                item = ''.join(parts)\n                yield item\n\n    packages = list(gen_packages_items())\n    return packages\n\n\nwith open('README.md') as f:\n    readme = f.read()\n\n\ndef do_setup():\n    setup(\n        name='vlmeval',\n        version='0.1.0',\n        description='OpenCompass VLM Evaluation Kit',\n        author='Haodong Duan',\n        author_email='dhd.efz@gmail.com',\n        maintainer='Haodong Duan',\n        maintainer_email='dhd.efz@gmail.com',\n        long_description=readme,\n        long_description_content_type='text/markdown',\n        cmdclass={},\n        install_requires=parse_requirements('requirements.txt'),\n        setup_requires=[],\n        python_requires='>=3.7.0',\n        packages=find_packages(exclude=[\n            'test*',\n            'paper_test*',\n        ]),\n        keywords=['AI', 'NLP', 'in-context learning'],\n        entry_points={\n            'console_scripts': ['vlmutil = vlmeval:cli']\n        },\n        classifiers=[\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n            'Programming Language :: Python :: 3.9',\n            'Programming Language :: Python :: 3.10',\n            'Intended Audience :: Developers',\n            'Intended Audience :: Education',\n            'Intended Audience :: Science/Research',\n        ])\n\n\nif __name__ == '__main__':\n    do_setup()\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/qwen_api.py", "content": "from http import HTTPStatus\nimport os\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.smp import *\n\n\n# Note: This is a pure language model API.\nclass QwenAPI(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'qwen-max-1201',\n                 retry: int = 5,\n                 wait: int = 5,\n                 verbose: bool = True,\n                 seed: int = 2680,\n                 temperature: float = 0.0,\n                 system_prompt: str = None,\n                 key: str = None,\n                 max_tokens: int = 2048,\n                 proxy: str = None,\n                 **kwargs):\n\n        assert model in ['qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-1201', 'qwen-max-longcontext']\n        self.model = model\n        import dashscope\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.seed = seed\n        if key is None:\n            key = os.environ.get('DASHSCOPE_API_KEY', None)\n        assert key is not None, (\n            'Please set the API Key (obtain it here: '\n            'https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)'\n        )\n        dashscope.api_key = key\n        if proxy is not None:\n            proxy_set(proxy)\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    @staticmethod\n    def build_msgs(msgs_raw, system_prompt=None):\n        msgs = cp.deepcopy(msgs_raw)\n        ret = []\n        if system_prompt is not None:\n            ret.append(dict(role='system', content=system_prompt))\n        for i, msg in enumerate(msgs):\n            role = 'user' if i % 2 == 0 else 'assistant'\n            ret.append(dict(role=role, content=msg))\n        return ret\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        from dashscope import MultiModalConversation\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n        inputs = [inputs] if isinstance(inputs, str) else inputs\n        messages = self.build_msgs(msgs_raw=inputs, system_prompt=self.system_prompt)\n\n        import dashscope\n        response = dashscope.Generation.call(\n            model=self.model,\n            messages=messages,\n            seed=self.seed,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            result_format='message',  # set the result to be \"message\" format.\n        )\n        if response.status_code != HTTPStatus.OK:\n            return -1, 'Error: Bad Response Statuse Code. ', f'The response status code is {response.status_code}. '\n\n        try:\n            return 0, response['output']['choices'][0]['message']['content'].strip(), 'Succeeded! '\n        except Exception as err:\n            return -1, f'Error: Failed to parse the response. {err}', response\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/bailingmm.py", "content": "import base64\nfrom vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.dataset import DATASET_TYPE\nfrom vlmeval.smp.vlm import encode_image_file_to_base64\nimport time\n\n\nclass bailingMMWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str,\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 max_tokens: int = 1024,\n                 proxy: str = None,\n                 **kwargs):\n\n        self.model = model\n        self.fail_msg = 'Failed to obtain answer via bailingMM API.'\n        if key is None:\n            key = os.environ.get('BAILINGMM_API_KEY', None)\n        assert key is not None, ('Please set the API Key for bailingMM.')\n        self.key = key\n        self.headers = {\"Content-Type\": \"application/json\"}\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    def image_to_base64(self, image_path):\n        with open(image_path, 'rb') as image_file:\n            encoded_string = str(base64.b64encode(image_file.read()), 'utf-8')\n            return encoded_string\n\n    def prepare_inputs(self, inputs):\n        msgs = cp.deepcopy(inputs)\n        content = []\n        for i, msg in enumerate(msgs):\n            if msg['type'] == 'text':\n                pass\n            else:\n                try:\n                    image_data = self.image_to_base64(msg['value'])\n                except Exception as e:\n                    if self.verbose:\n                        self.logger.error(e)\n                    image_data = ''\n                msg['value'] = image_data\n            content.append(msg)\n        return content\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n        start = time.time()\n        inputs = [inputs] if isinstance(inputs, str) else inputs\n\n        messages = self.prepare_inputs(inputs)\n\n        service_url = \"https://bailingchat.alipay.com/api/proxy/eval/antgmm/completions\"\n\n        payload = {\n            \"structInput\": messages,\n            \"sk\": self.key,\n            \"model\": self.model,\n            \"timeout\": 180000\n        }\n        response = requests.post(service_url, headers=self.headers, json=payload)\n        if self.verbose:\n            self.logger.info('Time for requesting is:')\n            self.logger.info(time.time() - start)\n        try:\n            assert response.status_code == 200\n            output = json.loads(response.text)\n            answer = output['preds']['pred']\n            if self.verbose:\n                self.logger.info(f'inputs: {inputs}\\nanswer: {answer}')\n            return 0, answer, 'Succeeded! '\n        except Exception as e:\n            if self.verbose:\n                self.logger.error(e)\n                self.logger.error(f'The input messages are {inputs}.')\n            return -1, self.fail_msg, ''\n\n\nclass bailingMMAPI(bailingMMWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(bailingMMAPI, self).generate(message, dataset=dataset)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/__init__.py", "content": "from .gpt import OpenAIWrapper, GPT4V\nfrom .hf_chat_model import HFChatModel\nfrom .gemini import GeminiWrapper, GeminiProVision\nfrom .qwen_vl_api import QwenVLWrapper, QwenVLAPI, Qwen2VLAPI\nfrom .qwen_api import QwenAPI\nfrom .claude import Claude_Wrapper, Claude3V\nfrom .reka import Reka\nfrom .glm_vision import GLMVisionAPI\nfrom .cloudwalk import CWWrapper\nfrom .sensechat_vision import SenseChatVisionAPI\nfrom .siliconflow import SiliconFlowAPI, TeleMMAPI\nfrom .hunyuan import HunyuanVision\nfrom .bailingmm import bailingMMAPI\nfrom .bluelm_v_api import BlueLMWrapper, BlueLM_V_API\nfrom .jt_vl_chat import JTVLChatAPI\nfrom .taiyi import TaiyiAPI\nfrom .lmdeploy import LMDeployAPI\nfrom .taichu import TaichuVLAPI\nfrom .doubao_vl_api import DoubaoVL\n\n\n__all__ = [\n    'OpenAIWrapper', 'HFChatModel', 'GeminiWrapper', 'GPT4V',\n    'GeminiProVision', 'QwenVLWrapper', 'QwenVLAPI', 'QwenAPI',\n    'Claude3V', 'Claude_Wrapper', 'Reka', 'GLMVisionAPI',\n    'CWWrapper', 'SenseChatVisionAPI', 'HunyuanVision', 'Qwen2VLAPI',\n    'BlueLMWrapper', 'BlueLM_V_API', 'JTVLChatAPI', 'bailingMMAPI',\n    'TaiyiAPI', 'TeleMMAPI', 'SiliconFlowAPI', 'LMDeployAPI',\n    'TaichuVLAPI', 'DoubaoVL'\n]\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/scripts/mmb_eval_gradio.py", "content": "from vlmeval.smp import *\nfrom vlmeval.tools import EVAL\nfrom vlmeval.dataset import build_dataset\nimport gradio as gr\n\nHEADER = \"\"\"\n# Welcome to MMBench👏👏\nWe are delighted that you are willing to submit the evaluation results to the MMBench official website! The evaluation service currently can handle submissions of MMBench, MMBench-CN, and CCBench. We use `gpt-3.5-turbo-0125` to help answer matching. Evaluation Codes in VLMEvalKit: https://github.com/open-compass/VLMEvalKit. Please adopt / follow the implementation of VLMEvalKit to generate the submission files. \n\nThe evaluation script is available at https://github.com/open-compass/VLMEvalKit/tree/main/scripts/mmb_eval_gradio.py\nPlease contact `opencompass@pjlab.org.cn` for any inquirys about this script. \n\"\"\"\n\ndef upload_file(file):\n    file_path = file.name\n    return file_path\n\ndef prepare_file(file_name):\n    file_md5 = md5(file_name)\n    root = LMUDataRoot()\n    root = osp.join(root, 'eval_server')\n    os.makedirs(root, exist_ok=True)\n    suffix = file_name.split('.')[-1]\n    if suffix not in ['xlsx', 'tsv', 'csv']:\n        return False, \"Please submit a file that ends with `.xlsx`, `.tsv`, or `.csv`\"\n    new_file_name = osp.join(root, f'{file_md5}.{suffix}')\n    shutil.move(file_name, new_file_name)\n    eval_file = new_file_name\n    try:\n        data = load(eval_file)\n    except:\n        return False, \"Your excel file can not be successfully loaded by `pd.read_excel`, please double check and submit again. \"\n    for k in data.keys():\n        data[k.lower() if k not in 'ABCD' else k] = data.pop(k)\n    if \"index\" not in data:\n        return False, \"Your excel file should have a column named `index`, please double check and submit again\" , {}\n    if \"prediction\" not in data:\n        return False, \"Your excel file should have a column named `prediction`, please double check and submit again\" , {}\n    for ch in 'ABCD':\n        if ch not in data:\n            return False, f\"Your excel file should have a column named `{ch}`, please double check and submit again\" , {}\n    dump(data, eval_file)\n    return True, eval_file\n\ndef determine_dataset(eval_file):\n    data = load(eval_file)\n    def cn_ratio(data):\n        iscn = [cn_string(x) for x in data['question']]\n        return np.mean(iscn)\n    max_ind = np.max([int(x) for x in data['index'] if int(x) < 1e5])\n    if max_ind < 1000 and 'l2-category' not in data:\n        return 'CCBench' if cn_ratio(data) > 0.5 else \"Unknown\" \n    elif max_ind < 3000 :\n        return 'MMBench_CN' if cn_ratio(data) > 0.5 else \"MMBench\"\n    else:\n        return 'MMBench_CN_V11' if cn_ratio(data) > 0.5 else \"MMBench_V11\"\n\n    \ndef reformat_acc(acc):\n    splits = set(acc['split'])\n    keys = list(acc.keys())\n    keys.remove('split')\n    nacc = {'Category': []}\n    for sp in splits:\n        nacc[sp.upper()] = []\n    for k in keys:\n        nacc['Category'].append(k)\n        for sp in splits:\n            nacc[sp.upper()].append(acc[acc['split'] == sp].iloc[0][k] * 100)\n    return pd.DataFrame(nacc)\n\ndef evaluate(file):\n    file_name = file.name\n    flag, eval_file = prepare_file(file_name)\n    if not flag:\n        return \"Error: \" + eval_file\n    dataset = determine_dataset(eval_file)\n    if dataset == 'Unknown':\n        return \"Error: Cannot determine the dataset given your submitted file. \" \n\n    eval_id = eval_file.split('/')[-1].split('.')[0]\n    ret = f\"Evaluation ID: {eval_id}\\n\"\n    timestamp = datetime.datetime.now().strftime('%Y.%m.%d  %H:%M:%S')\n    ret += f'Evaluation Timestamp: {timestamp}\\n'\n    eval_data = load(eval_file)\n    eval_data['index'] = [int(x) for x in eval_data['index']]\n    base_data = build_dataset(dataset).data\n    base_index_set = set([int(x) for x in base_data['index']])\n    inds_more = {k for k in eval_data['index'] if k not in base_index_set}\n    if len(inds_more) > 0:\n        inds_more = set([x % 1e6 for x in inds_more])\n        ret += f\"Warning: The matched dataset is {dataset}. The following indices are not in the base dataset: {inds_more}\\n\"\n        ret += f\"We automatically remove those indices, and still recommend you to check the indices in your prediction file.\\n\"\n        eval_data = eval_data[eval_data['index'].isin(base_index_set)]\n        dump(eval_data, eval_file)\n\n    acc = EVAL(dataset, eval_file)\n    nacc = reformat_acc(acc).round(1)\n    return ret, nacc\n\nwith gr.Blocks() as demo:\n    gr.Markdown(HEADER)\n    file_output = gr.File()\n    upload_button = gr.UploadButton(\"Click to upload you prediction files for a supported benchmark\")\n    upload_button.upload(upload_file, upload_button, file_output)\n    \n    btn = gr.Button(\"🚀 Evaluate\")\n    eval_log = gr.Textbox(label=\"Evaluation Log\", placeholder=\"Your evaluation log will be displayed here\")\n    df_empty = pd.DataFrame([], columns=['Evaluation Result'])\n    eval_result = gr.components.DataFrame(value=df_empty)\n    btn.click(evaluate, inputs=[file_output], outputs=[eval_log, eval_result])\n\nif __name__ == '__main__':\n    demo.launch(server_name='0.0.0.0', debug=True, show_error=True)"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/gemini.py", "content": "from vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\n\nheaders = 'Content-Type: application/json'\n\n\nclass GeminiWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'gemini-1.0-pro',\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = True,\n                 temperature: float = 0.0,\n                 system_prompt: str = None,\n                 max_tokens: int = 2048,\n                 proxy: str = None,\n                 backend='genai',\n                 project_id='vlmeval',\n                 **kwargs):\n\n        self.model = model\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        if key is None:\n            key = os.environ.get('GOOGLE_API_KEY', None)\n        # Try to load backend from environment variable\n        be = os.environ.get('GOOGLE_API_BACKEND', None)\n        if be is not None and be in ['genai', 'vertex']:\n            backend = be\n\n        assert backend in ['genai', 'vertex']\n        if backend == 'genai':\n            # We have not evaluated Gemini-1.5 w. GenAI backend\n            assert key is not None  # Vertex does not require API Key\n\n        self.backend = backend\n        self.project_id = project_id\n        self.api_key = key\n\n        if proxy is not None:\n            proxy_set(proxy)\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    def build_msgs_genai(self, inputs):\n        messages = [] if self.system_prompt is None else [self.system_prompt]\n        for inp in inputs:\n            if inp['type'] == 'text':\n                messages.append(inp['value'])\n            elif inp['type'] == 'image':\n                messages.append(Image.open(inp['value']))\n        return messages\n\n    def build_msgs_vertex(self, inputs):\n        from vertexai.generative_models import Part, Image\n        messages = [] if self.system_prompt is None else [self.system_prompt]\n        for inp in inputs:\n            if inp['type'] == 'text':\n                messages.append(inp['value'])\n            elif inp['type'] == 'image':\n                messages.append(Part.from_image(Image.load_from_file(inp['value'])))\n        return messages\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        if self.backend == 'genai':\n            import google.generativeai as genai\n            assert isinstance(inputs, list)\n            pure_text = np.all([x['type'] == 'text' for x in inputs])\n            genai.configure(api_key=self.api_key)\n\n            if pure_text and self.model == 'gemini-1.0-pro':\n                model = genai.GenerativeModel('gemini-1.0-pro')\n            else:\n                model = genai.GenerativeModel(self.model)\n\n            messages = self.build_msgs_genai(inputs)\n            gen_config = dict(max_output_tokens=self.max_tokens, temperature=self.temperature)\n            gen_config.update(kwargs)\n            try:\n                answer = model.generate_content(\n                    messages,\n                    generation_config=genai.types.GenerationConfig(**gen_config)).text\n                return 0, answer, 'Succeeded! '\n            except Exception as err:\n                if self.verbose:\n                    self.logger.error(f'{type(err)}: {err}')\n                    self.logger.error(f'The input messages are {inputs}.')\n\n                return -1, '', ''\n        elif self.backend == 'vertex':\n            import vertexai\n            from vertexai.generative_models import GenerativeModel\n            vertexai.init(project=self.project_id, location='us-central1')\n            model_name = 'gemini-1.0-pro-vision' if self.model == 'gemini-1.0-pro' else self.model\n            model = GenerativeModel(model_name=model_name)\n            messages = self.build_msgs_vertex(inputs)\n            try:\n                resp = model.generate_content(messages)\n                answer = resp.text\n                return 0, answer, 'Succeeded! '\n            except Exception as err:\n                if self.verbose:\n                    self.logger.error(f'{type(err)}: {err}')\n                    self.logger.error(f'The input messages are {inputs}.')\n\n                return -1, '', ''\n\n\nclass GeminiProVision(GeminiWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(GeminiProVision, self).generate(message)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/gpt.py", "content": "from ..smp import *\nimport os\nimport sys\nfrom .base import BaseAPI\n\nAPIBASES = {\n    'OFFICIAL': 'https://api.openai.com/v1/chat/completions',\n}\n\n\ndef GPT_context_window(model):\n    length_map = {\n        'gpt-4': 8192,\n        'gpt-4-0613': 8192,\n        'gpt-4-turbo-preview': 128000,\n        'gpt-4-1106-preview': 128000,\n        'gpt-4-0125-preview': 128000,\n        'gpt-4-vision-preview': 128000,\n        'gpt-4-turbo': 128000,\n        'gpt-4-turbo-2024-04-09': 128000,\n        'gpt-3.5-turbo': 16385,\n        'gpt-3.5-turbo-0125': 16385,\n        'gpt-3.5-turbo-1106': 16385,\n        'gpt-3.5-turbo-instruct': 4096,\n    }\n    if model in length_map:\n        return length_map[model]\n    else:\n        return 128000\n\n\nclass OpenAIWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'gpt-3.5-turbo-0613',\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = False,\n                 system_prompt: str = None,\n                 temperature: float = 0,\n                 timeout: int = 60,\n                 api_base: str = None,\n                 max_tokens: int = 2048,\n                 img_size: int = 512,\n                 img_detail: str = 'low',\n                 use_azure: bool = False,\n                 **kwargs):\n\n        self.model = model\n        self.cur_idx = 0\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.use_azure = use_azure\n\n        if 'step' in model:\n            env_key = os.environ.get('STEPAI_API_KEY', '')\n            if key is None:\n                key = env_key\n        elif 'yi-vision' in model:\n            env_key = os.environ.get('YI_API_KEY', '')\n            if key is None:\n                key = env_key\n        elif 'internvl2-pro' in model:\n            env_key = os.environ.get('InternVL2_PRO_KEY', '')\n            if key is None:\n                key = env_key\n        elif 'abab' in model:\n            env_key = os.environ.get('MiniMax_API_KEY', '')\n            if key is None:\n                key = env_key\n        else:\n            if use_azure:\n                env_key = os.environ.get('AZURE_OPENAI_API_KEY', None)\n                assert env_key is not None, 'Please set the environment variable AZURE_OPENAI_API_KEY. '\n\n                if key is None:\n                    key = env_key\n                assert isinstance(key, str), (\n                    'Please set the environment variable AZURE_OPENAI_API_KEY to your openai key. '\n                )\n            else:\n                env_key = os.environ.get('OPENAI_API_KEY', '')\n                if key is None:\n                    key = env_key\n                assert isinstance(key, str) and key.startswith('sk-'), (\n                    f'Illegal openai_key {key}. '\n                    'Please set the environment variable OPENAI_API_KEY to your openai key. '\n                )\n\n        self.key = key\n        assert img_size > 0 or img_size == -1\n        self.img_size = img_size\n        assert img_detail in ['high', 'low']\n        self.img_detail = img_detail\n        self.timeout = timeout\n\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n        if use_azure:\n            api_base_template = (\n                '{endpoint}openai/deployments/{deployment_name}/chat/completions?api-version={api_version}'\n            )\n            endpoint = os.getenv('AZURE_OPENAI_ENDPOINT', None)\n            assert endpoint is not None, 'Please set the environment variable AZURE_OPENAI_ENDPOINT. '\n            deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', None)\n            assert deployment_name is not None, 'Please set the environment variable AZURE_OPENAI_DEPLOYMENT_NAME. '\n            api_version = os.getenv('OPENAI_API_VERSION', None)\n            assert api_version is not None, 'Please set the environment variable OPENAI_API_VERSION. '\n\n            self.api_base = api_base_template.format(\n                endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n                deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'),\n                api_version=os.getenv('OPENAI_API_VERSION')\n            )\n        else:\n            if api_base is None:\n                if 'OPENAI_API_BASE' in os.environ and os.environ['OPENAI_API_BASE'] != '':\n                    self.logger.info('Environment variable OPENAI_API_BASE is set. Will use it as api_base. ')\n                    api_base = os.environ['OPENAI_API_BASE']\n                else:\n                    api_base = 'OFFICIAL'\n\n            assert api_base is not None\n\n            if api_base in APIBASES:\n                self.api_base = APIBASES[api_base]\n            elif api_base.startswith('http'):\n                self.api_base = api_base\n            else:\n                self.logger.error('Unknown API Base. ')\n                raise NotImplementedError\n\n        self.logger.info(f'Using API Base: {self.api_base}; API Key: {self.key}')\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(type='text', text=msg['value']))\n                elif msg['type'] == 'image':\n                    from PIL import Image\n                    img = Image.open(msg['value'])\n                    b64 = encode_image_to_base64(img, target_size=self.img_size)\n                    img_struct = dict(url=f'data:image/jpeg;base64,{b64}', detail=self.img_detail)\n                    content_list.append(dict(type='image_url', image_url=img_struct))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(type='text', text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(role='system', content=self.system_prompt))\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(role=item['role'], content=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(role='user', content=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        input_msgs = self.prepare_inputs(inputs)\n        temperature = kwargs.pop('temperature', self.temperature)\n        max_tokens = kwargs.pop('max_tokens', self.max_tokens)\n\n        # context_window = GPT_context_window(self.model)\n        # new_max_tokens = min(max_tokens, context_window - self.get_token_len(inputs))\n        # if 0 < new_max_tokens <= 100 and new_max_tokens < max_tokens:\n        #     self.logger.warning(\n        #         'Less than 100 tokens left, '\n        #         'may exceed the context window with some additional meta symbols. '\n        #     )\n        # if new_max_tokens <= 0:\n        #     return 0, self.fail_msg + 'Input string longer than context window. ', 'Length Exceeded. '\n        # max_tokens = new_max_tokens\n\n        # Will send request if use Azure, dk how to use openai client for it\n        if self.use_azure:\n            headers = {'Content-Type': 'application/json', 'api-key': self.key}\n        elif 'internvl2-pro' in self.model:\n            headers = {'Content-Type': 'application/json', 'Authorization': self.key}\n        else:\n            headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.key}'}\n        payload = dict(\n            model=self.model,\n            messages=input_msgs,\n            max_tokens=max_tokens,\n            n=1,\n            temperature=temperature,\n            **kwargs)\n        response = requests.post(\n            self.api_base,\n            headers=headers, data=json.dumps(payload), timeout=self.timeout * 1.1)\n        ret_code = response.status_code\n        ret_code = 0 if (200 <= int(ret_code) < 300) else ret_code\n        answer = self.fail_msg\n        try:\n            resp_struct = json.loads(response.text)\n            answer = resp_struct['choices'][0]['message']['content'].strip()\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(response.text if hasattr(response, 'text') else response)\n\n        return ret_code, answer, response\n\n    def get_image_token_len(self, img_path, detail='low'):\n        import math\n        if detail == 'low':\n            return 85\n\n        im = Image.open(img_path)\n        height, width = im.size\n        if width > 1024 or height > 1024:\n            if width > height:\n                height = int(height * 1024 / width)\n                width = 1024\n            else:\n                width = int(width * 1024 / height)\n                height = 1024\n\n        h = math.ceil(height / 512)\n        w = math.ceil(width / 512)\n        total = 85 + 170 * h * w\n        return total\n\n    def get_token_len(self, inputs) -> int:\n        import tiktoken\n        try:\n            enc = tiktoken.encoding_for_model(self.model)\n        except Exception as err:\n            if 'gpt' in self.model.lower():\n                if self.verbose:\n                    self.logger.warning(f'{type(err)}: {err}')\n                enc = tiktoken.encoding_for_model('gpt-4')\n            else:\n                return 0\n        assert isinstance(inputs, list)\n        tot = 0\n        for item in inputs:\n            if 'role' in item:\n                tot += self.get_token_len(item['content'])\n            elif item['type'] == 'text':\n                tot += len(enc.encode(item['value']))\n            elif item['type'] == 'image':\n                tot += self.get_image_token_len(item['value'], detail=self.img_detail)\n        return tot\n\n\nclass GPT4V(OpenAIWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(GPT4V, self).generate(message)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/jt_vl_chat.py", "content": "import pandas as pd\nimport requests\nimport json\nimport os\nimport base64\nfrom vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.dataset import DATASET_TYPE\nfrom vlmeval.dataset import img_root_map\n\n\nAPI_ENDPOINT = 'https://jiutian.10086.cn/kunlun/ingress/api/h3t-eeceff/92390745235a40a484d850be19e1f8b4/ai-5d7ae47ec93f4280953273c4001aafee/service-7544ea5ee3e841ad9d01e7af44acef7c/v1/chat/completions'  # noqa: E501\nAPP_CODE = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiI5ZGQwNmQ2ZjU4YTU0ZGY0OGEzNjRhMjQyNGMwODEyNSIsImlzcyI6ImFwaS1hdXRoLWtleSIsImV4cCI6NDg4MjkwNDA3OX0.k5t_T-955xWMndzBbx4WQQNAgm5DpMos9mHm7vkFipQ3yebCFMfyufpSxORSfEVpBaDS3Nly0dd8ygQYGnDgIQcC72vQ1xtkjCP49LNcqlceoET4rGc1zwRi76XLPSGFES4GcwvEmr7Ilth7XtqZNxcDF_Z7HyHyf1-zF0JIQETYSoxenqLU-gNteNfqRUnlyCgaKh03DscAbYvtoMUxEaFa2ZqyRSwekdHI_SPKCq9aC9G19yDPHTjeiwl1ubtyC5uMy5pERn_ClRsZS3Wyb-GmD5QQsFofrWvCiU_fVJuUiez39pYZvEP8awH0R9B7SkpQ4XOzj3fdytTPYy3g6g'  # noqa: E501\n\n\nclass JTVLChatWrapper(BaseAPI):\n    is_api: bool = True\n    INTERLEAVE = False\n\n    def __init__(self,\n                 model: str = 'jt-vl-chat',\n                 retry: int = 5,\n                 wait: int = 5,\n                 api_base: str = API_ENDPOINT,\n                 key: str = APP_CODE,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 temperature: float = 0.7,\n                 max_tokens: int = 2048,\n                 proxy: str = None,\n                 **kwargs):\n        self.model = model\n\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.api_base = api_base\n\n        if key is None:\n            key = os.environ.get('JTVLChat_API_KEY', None)\n        assert key is not None, (\n            'Please set the API Key (also called app_code, obtain it here: https://github.com/jiutiancv/JT-VL-Chat)'\n        )\n\n        self.key = key\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    def dump_image(self, line, dataset):\n        \"\"\"Dump the image(s) of the input line to the corresponding dataset folder.\n\n        Args:\n            line (line of pd.DataFrame): The raw input line.\n            dataset (str): The name of the dataset.\n\n        Returns:\n            str | list[str]: The paths of the dumped images.\n        \"\"\"\n        ROOT = LMUDataRoot()\n        assert isinstance(dataset, str)\n\n        img_root = os.path.join(ROOT, 'images', img_root_map(dataset) if dataset in img_root_map(dataset) else dataset)\n        os.makedirs(img_root, exist_ok=True)\n        if 'image' in line:\n            if isinstance(line['image'], list):\n                tgt_path = []\n                assert 'image_path' in line\n                for img, im_name in zip(line['image'], line['image_path']):\n                    path = osp.join(img_root, im_name)\n                    if not read_ok(path):\n                        decode_base64_to_image_file(img, path)\n                    tgt_path.append(path)\n            else:\n                tgt_path = osp.join(img_root, f\"{line['index']}.jpg\")\n                if not read_ok(tgt_path):\n                    decode_base64_to_image_file(line['image'], tgt_path)\n                tgt_path = [tgt_path]\n        else:\n            assert 'image_path' in line\n            tgt_path = toliststr(line['image_path'])\n\n        return tgt_path\n\n    def use_custom_prompt(self, dataset):\n        assert dataset is not None\n        if listinstr(['MMMU_DEV_VAL','MMMU_TEST'], dataset):\n            return False\n        else:\n            return True\n\n    def build_multi_choice_prompt(self, line, dataset=None):\n        question = line['question']\n        hint = line['hint'] if ('hint' in line and not pd.isna(line['hint'])) else None\n        if hint is not None:\n            question = hint + '\\n' + question\n\n        options = {\n            cand: line[cand]\n            for cand in string.ascii_uppercase\n            if cand in line and not pd.isna(line[cand])\n        }\n        for key, item in options.items():\n            question += f'\\n{key}. {item}'\n        prompt = question\n\n        if len(options):\n            prompt += '\\n请直接回答选项字母。' if cn_string(\n                prompt) else \"\\nAnswer with the option's letter from the given choices directly.\"\n        else:\n            prompt += '\\n请直接回答问题。' if cn_string(prompt) else '\\nAnswer the question directly.'\n\n        return prompt\n\n    def build_prompt(self, line, dataset=None):\n        assert self.use_custom_prompt(dataset)\n        assert dataset is None or isinstance(dataset, str)\n\n        tgt_path = self.dump_image(line, dataset)\n\n        if dataset is not None and listinstr(['MME'], dataset):\n            question = line['question']\n            prompt = question + ' Answer the question using a single word or phrase.'\n        elif dataset is not None and listinstr(['HallusionBench'], dataset):\n            question = line['question']\n            prompt = question + ' Please answer yes or no. Answer the question using a single word or phrase.'\n        elif dataset is not None and DATASET_TYPE(dataset) == 'MCQ':\n            prompt = self.build_multi_choice_prompt(line, dataset)\n        elif dataset is not None and DATASET_TYPE(dataset) == 'VQA':\n            if listinstr(['MathVista', 'MathVision'], dataset):\n                prompt = line['question']\n            elif listinstr(['LLaVABench'], dataset):\n                question = line['question']\n                prompt = question + '\\nAnswer this question in detail.'\n            elif listinstr(['MMVet'], dataset):\n                prompt = line['question']\n            else:\n                question = line['question']\n                prompt = question + '\\nAnswer the question using a single word or phrase.'\n        else:\n            prompt = line['question']\n        message = [dict(type='text', value=prompt)]\n        message.extend([dict(type='image', value=s) for s in tgt_path])\n        return message\n\n    def message_to_promptimg(self, message, dataset=None):\n        assert not self.INTERLEAVE\n        model_name = self.__class__.__name__\n        import warnings\n        warnings.warn(\n            f'Model {model_name} does not support interleaved input. '\n            'Will use the first image and aggregated texts as prompt. ')\n        num_images = len([x for x in message if x['type'] == 'image'])\n        if num_images == 0:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            image = None\n        else:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            if dataset == 'BLINK':\n                image = concat_images_vlmeval(\n                    [x['value'] for x in message if x['type'] == 'image'],\n                    target_size=512)\n            else:\n                image = [x['value'] for x in message if x['type'] == 'image'][0]\n        return prompt, image\n\n    def get_send_data(self,prompt, image_path, temperature, max_tokens):\n        image = ''\n        with open(image_path, 'rb') as f:\n            image = str(base64.b64encode(f.read()), 'utf-8')\n        send_data = {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            \"image_base64\": image,\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature\n        }\n        return send_data\n\n    def get_send_data_no_image(self,prompt, temperature, max_tokens):\n        send_data = {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            \"max_tokens\": max_tokens,\n            \"temperature\": temperature\n        }\n        return send_data\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n        inputs = [inputs] if isinstance(inputs, str) else inputs\n        dataset = kwargs.get('dataset', None)\n        prompt, image_path = self.message_to_promptimg(message=inputs, dataset=dataset)\n        # print(\"prompt:\",prompt)\n        if image_path:\n            send_data = self.get_send_data(\n                prompt=prompt,\n                image_path=image_path,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens)\n        else:\n            send_data = self.get_send_data_no_image(\n                prompt=prompt,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens)\n\n        json_data = json.dumps(send_data)\n\n        header_dict = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + self.key}\n\n        r = requests.post(self.api_base, headers=header_dict, data=json_data, timeout=3000)\n        try:\n            assert r.status_code == 200\n            r_json = r.json()\n            output = r_json['choices'][0]['message']['content']\n            if self.verbose:\n                self.logger.info(f'inputs: {inputs}\\nanswer: {output}')\n\n            return 0,output,'Succeeded! '\n\n        except:\n            error_msg = f'Error! code {r.status_code} content: {r.content}'\n            error_con = r.content.decode('utf-8')\n            if self.verbose:\n                self.logger.error(error_msg)\n                self.logger.error(error_con)\n                self.logger.error(f'The input messages are {inputs}.')\n            return -1,error_msg,''\n\n\nclass JTVLChatAPI(JTVLChatWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(JTVLChatAPI, self).generate(message, dataset=dataset)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/hf_chat_model.py", "content": "import os\nimport sys\nimport os.path as osp\nimport torch\nfrom ..smp import *\n\n\ndef get_gpu_num(model_name):\n    model_name = model_name.lower()\n    kws = {\n        8: ['65b', '70b'],\n        4: ['30b', '33b', '35b', '40b'],\n        2: ['13b', '14b', '20b'],\n        1: ['6b', '7b', 'moss'],\n    }\n    for k in [8, 4, 2, 1]:\n        for keyword in kws[k]:\n            if keyword in model_name:\n                return k\n    return 8\n\n\nvalidated_llms = [\n    'internlm/internlm-chat-7b', 'internlm/internlm-chat-7b-8k', 'internlm/internlm-chat-20b',\n    'Qwen/Qwen-7B-Chat', 'Qwen/Qwen-14B-Chat',\n    'THUDM/chatglm2-6b', 'THUDM/chatglm2-6b-32k', 'THUDM/chatglm3-6b', 'THUDM/chatglm3-6b-32k',\n    'baichuan-inc/Baichuan2-7B-Chat', 'baichuan-inc/Baichuan2-13B-Chat',\n    'lmsys/vicuna-7b-v1.5', 'lmsys/vicuna-13b-v1.5',\n    'meta-llama/Llama-2-7b-chat-hf'\n]\nAuto_model = ['chatglm']\n\n\nclass HFChatModel:\n\n    def _get_context_length(self, model, model_path):\n        # By default, we use model.config.seq_length\n        model_path = model_path.lower()\n        if 'baichuan' in model_path:\n            context_window = model.config.model_max_length\n        elif 'internlm' in model_path or 'llama' in model_path:\n            context_window = model.config.max_position_embeddings\n        elif 'vicuna' in model_path:\n            context_window = model.generation_config.max_length\n        else:\n            # chatglm & qwen\n            context_window = model.config.seq_length\n        return context_window\n\n    def _get_context_length_robust(self, model, model_path):\n        try:\n            context_window = self._get_context_length(model, model_path)\n            return context_window\n        except Exception as err:\n            self.logger.critical(f'{type(err)}: {err}')\n            self.logger.critical(\n                'Failed to extract context_window information from config / generation_config. '\n                'Please read the above code and check if the logic works for you model path'\n            )\n            raise NotImplementedError\n\n    def __init__(self,\n                 model_path,\n                 system_prompt: str = None,\n                 **kwargs):\n\n        self.logger = get_logger('HFChatModel')\n        if 'vicuna' in model_path.lower():\n            try:\n                from fastchat.model import get_conversation_template\n            except Exception as err:\n                self.logger.critical('Please install fastchat first to use vicuna. ')\n                raise err\n\n        self.explicit_device = kwargs.pop('device', None)\n\n        if self.explicit_device is None:\n            # If CUDA_VISIBLE_DEVICES is not properly set\n            if 'CUDA_VISIBLE_DEVICES' not in os.environ or os.environ['CUDA_VISIBLE_DEVICES'] == '0,1,2,3,4,5,6,7':\n                num_gpu = get_gpu_num(model_path)\n                gpu_offset = kwargs.pop('gpu_offset', 0)\n                cuda_visible_devices = ','.join([str(i) for i in range(gpu_offset, gpu_offset + num_gpu)])\n                os.environ['CUDA_VISIBLE_DEVICES'] = cuda_visible_devices\n\n        from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n        from transformers.generation import GenerationConfig\n\n        if model_path not in validated_llms:\n            self.logger.warning(f'{model_path} not in validated LLMs, may have inference troubles. ')\n\n        self.model_path = model_path\n        if listinstr(Auto_model, model_path):\n            LoadModel = AutoModel\n        else:\n            LoadModel = AutoModelForCausalLM\n\n        assert osp.exists(model_path) or len(model_path.split('/')) == 2\n\n        device = self.explicit_device if self.explicit_device else 'auto'\n\n        precision = {}\n        if 'internlm-chat-7b' in model_path:\n            precision = {'torch_dtype': torch.float16}\n        elif 'internlm-chat-20b' in model_path:\n            precision = {'torch_dtype': torch.bfloat16}\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        model = LoadModel.from_pretrained(model_path, trust_remote_code=True, device_map='cpu', **precision)\n        model = model.eval()\n\n        if device != 'cpu':\n            model = model.to(f'cuda:{device}' if isinstance(device, int) else 'cuda')\n        try:\n            model.generation_config = GenerationConfig.from_pretrained(\n                model_path, trust_remote_code=True, device_map=device)\n        except Exception as err:\n            self.logger.warning(f'{type(err)}: {err}')\n\n        torch.cuda.empty_cache()\n        self.model = model\n        self.context_length = self._get_context_length_robust(model=model, model_path=model_path)\n        self.answer_buffer = 192\n        self.system_prompt = system_prompt\n        for k, v in kwargs.items():\n            self.logger.info(f'Following args will be used for generation (If not set specifically), {k}: {v}. ')\n        self.kwargs = kwargs\n\n    def generate_str(self, input, **kwargs):\n        if 'baichuan' in self.model_path.lower():\n            messages = []\n            messages.append({'role': 'user', 'content': input})\n            resp = self.model.chat(self.tokenizer, messages, **kwargs)\n        elif 'vicuna' in self.model_path.lower():\n            from fastchat.model import get_conversation_template\n            conv = get_conversation_template('vicuna')\n            conv.append_message(conv.roles[0], input)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n            inputs = self.tokenizer([prompt], return_tensors='pt')\n            if torch.cuda.is_available():\n                for k in inputs:\n                    inputs[k] = inputs[k].cuda()\n\n            params = dict(do_sample=True, temperature=0.7, repetition_penalty=1.0, max_new_tokens=512)\n            params.update(self.kwargs)\n            params.update(kwargs)\n            outputs = self.model.generate(**inputs, **params)\n            resp = self.tokenizer.decode(\n                outputs[0][len(inputs['input_ids'][0]):],\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False)\n\n        else:\n            params = self.kwargs\n            params.update(kwargs)\n            resp, _ = self.model.chat(self.tokenizer, input, history=[], **params)\n\n        return resp\n\n    def length_ok(self, inputs):\n        tot = len(self.tokenizer.encode(self.system_prompt)) if self.system_prompt is not None else 0\n        for s in inputs:\n            tot += len(self.tokenizer.encode(s))\n        return tot + self.answer_buffer < self.context_length\n\n    def generate_list(self, full_inputs, offset=0, **kwargs):\n        assert isinstance(full_inputs, list)\n\n        inputs = full_inputs[offset:]\n        if not self.length_ok(inputs):\n            return self.chat(full_inputs, offset + 1)\n\n        model_path = self.model_path.lower()\n\n        if sum([x in model_path for x in ['baichuan']]):\n            input_msgs = []\n            if self.system_prompt is not None:\n                input_msgs.append(dict(role='user', content=self.system_prompt))\n            if len(inputs):\n                assert isinstance(inputs, list) and isinstance(inputs[0], str)\n                roles = ['user', 'assistant'] if len(inputs) % 2 == 1 else ['assistant', 'user']\n                roles = roles * len(inputs)\n                for role, msg in zip(roles, inputs):\n                    input_msgs.append(dict(role=role, content=msg))\n            response = self.model.chat(self.tokenizer, input_msgs)\n        elif sum([x in model_path for x in ['vicuna']]):\n            from fastchat.model import get_conversation_template\n            conv = get_conversation_template('vicuna')\n            assert isinstance(inputs, list) and isinstance(inputs[0], str)\n            if len(inputs) % 2 == 1:\n                if self.system_prompt is not None:\n                    conv.append_message(conv.roles[0], self.system_prompt)\n                for i in range(len(inputs) // 2):\n                    conv.append_message(conv.roles[0], inputs[2 * i])\n                    conv.append_message(conv.roles[1], inputs[2 * i + 1])\n            else:\n                assert self.system_prompt is not None\n                conv.append_message(conv.roles[0], self.system_prompt)\n                conv.append_message(conv.roles[1], inputs[0])\n                for i in range(len(inputs) // 2 - 1):\n                    conv.append_message(conv.roles[0], inputs[2 * i + 1])\n                    conv.append_message(conv.roles[1], inputs[2 * i + 2])\n            conv.append_message(conv.roles[0], inputs[-1])\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n            inputs = self.tokenizer([prompt], return_tensors='pt')\n            if torch.cuda.is_available():\n                for k in inputs:\n                    inputs[k] = inputs[k].cuda()\n\n            params = dict(do_sample=True, temperature=0.7, repetition_penalty=1.0, max_new_tokens=512)\n            params.update(self.kwargs)\n            params.update(kwargs)\n\n            outputs = self.model.generate(**inputs, **params)\n            response = self.tokenizer.decode(\n                outputs[0][len(inputs['input_ids'][0]):],\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False)\n            response = response.lstrip('\\n')\n        else:\n            # The default option, support internlm, chatglm, qwen\n            history, msg = [], None\n            if len(inputs) % 2 == 1:\n                if self.system_prompt is not None:\n                    history = [(self.system_prompt, '')]\n                for i in range(len(inputs) // 2):\n                    history.append((inputs[2 * i], inputs[2 * i + 1]))\n            else:\n                assert self.system_prompt is not None\n                history = [(self.system_prompt, inputs[0])]\n                for i in range(len(inputs) // 2 - 1):\n                    history.append((inputs[2 * i + 1], inputs[2 * i + 2]))\n            msg = inputs[-1]\n\n            params = self.kwargs\n            params.update(kwargs)\n            response, _ = self.model.chat(self.tokenizer, msg, history=history, **params)\n\n        return response, offset\n\n    def generate(self, inputs, **kwargs):\n        if isinstance(inputs, str):\n            return self.generate_str(inputs, **kwargs)\n        elif isinstance(inputs, list):\n            return self.generate_list(inputs, **kwargs)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/claude.py", "content": "from vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nfrom time import sleep\nimport base64\nimport mimetypes\nfrom PIL import Image\n\nalles_url = 'https://openxlab.org.cn/gw/alles-apin-hub/v1/claude/v1/text/chat'\nalles_headers = {\n    'alles-apin-token': '',\n    'Content-Type': 'application/json'\n}\nofficial_url = 'https://api.anthropic.com/v1/messages'\nofficial_headers = {\n    'x-api-key': '',\n    'anthropic-version': '2023-06-01',\n    'content-type': 'application/json'\n}\n\n\nclass Claude_Wrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 backend: str = 'alles',\n                 model: str = 'claude-3-opus-20240229',\n                 key: str = None,\n                 retry: int = 10,\n                 timeout: int = 60,\n                 wait: int = 3,\n                 system_prompt: str = None,\n                 verbose: bool = True,\n                 temperature: float = 0,\n                 max_tokens: int = 2048,\n                 **kwargs):\n\n        if os.environ.get('ANTHROPIC_BACKEND', '') == 'official':\n            backend = 'official'\n\n        assert backend in ['alles', 'official'], f'Invalid backend: {backend}'\n        self.backend = backend\n        self.url = alles_url if backend == 'alles' else official_url\n        self.model = model\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.headers = alles_headers if backend == 'alles' else official_headers\n        self.timeout = timeout\n\n        if key is not None:\n            self.key = key\n        else:\n            self.key = os.environ.get('ALLES', '') if self.backend == 'alles' else os.environ.get('ANTHROPIC_API_KEY', '')  # noqa: E501\n\n        if self.backend == 'alles':\n            self.headers['alles-apin-token'] = self.key\n        else:\n            self.headers['x-api-key'] = self.key\n\n        super().__init__(retry=retry, wait=wait, verbose=verbose, system_prompt=system_prompt, **kwargs)\n\n    def encode_image_file_to_base64(self, image_path, target_size=-1, fmt='.jpg'):\n        image = Image.open(image_path)\n        if fmt in ('.jpg', '.jpeg'):\n            format = 'JPEG'\n        elif fmt == '.png':\n            format = 'PNG'\n        else:\n            print(f'Unsupported image format: {fmt}, will cause media type match error.')\n\n        return encode_image_to_base64(image, target_size=target_size, fmt=format)\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text' and msg['value'] != '':\n                    content_list.append(dict(type='text', text=msg['value']))\n                elif msg['type'] == 'image':\n                    pth = msg['value']\n                    suffix = osp.splitext(pth)[-1].lower()\n                    media_type = mimetypes.types_map.get(suffix, None)\n                    assert media_type is not None\n\n                    content_list.append(dict(\n                        type='image',\n                        source={\n                            'type': 'base64',\n                            'media_type': media_type,\n                            'data': self.encode_image_file_to_base64(pth, target_size=4096, fmt=suffix)\n                        }))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(type='text', text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(role=item['role'], content=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(role='user', content=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        payload = {\n            'model': self.model,\n            'max_tokens': self.max_tokens,\n            'messages': self.prepare_inputs(inputs),\n            **kwargs\n        }\n        if self.system_prompt is not None:\n            payload['system'] = self.system_prompt\n\n        response = requests.request('POST', self.url, headers=self.headers, data=json.dumps(payload), timeout=self.timeout * 1.1)\n        ret_code = response.status_code\n        ret_code = 0 if (200 <= int(ret_code) < 300) else ret_code\n        answer = self.fail_msg\n\n        try:\n            resp_struct = json.loads(response.text)\n            if self.backend == 'alles':\n                answer = resp_struct['data']['content'][0]['text'].strip()\n            elif self.backend == 'official':\n                answer = resp_struct['content'][0]['text'].strip()\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(response.text if hasattr(response, 'text') else response)\n\n        return ret_code, answer, response\n\n\nclass Claude3V(Claude_Wrapper):\n\n    def generate(self, message, dataset=None):\n        return super(Claude_Wrapper, self).generate(message)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/cloudwalk.py", "content": "from ..smp import *\nimport os\nfrom .base import BaseAPI\n\n\nclass CWWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'cw-congrong-v1.5',\n                 retry: int = 10,\n                 wait: int = 5,\n                 key: str = None,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 temperature: float = 0,\n                 timeout: int = 600,\n                 api_base: str = 'http://cwapi-vlm01.cw_rb.azurebot.tk/v1/chat/completions',\n                 max_tokens: int = 2048,\n                 img_size: int = 512,\n                 img_detail: str = 'low',\n                 **kwargs):\n\n        self.model = model\n        self.cur_idx = 0\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n\n        base = os.environ.get('CW_API_BASE', None)\n        self.api_base = base if base is not None else api_base\n\n        env_key = os.environ.get('CW_API_KEY', None)\n        self.key = env_key if env_key is not None else key\n        assert self.key is not None, 'API key not provided. Please set CW_API_KEY environment variable or \\\n            pass it to the constructor.'\n\n        assert img_size > 0 or img_size == -1\n        self.img_size = -1  # allways send full size image\n        assert img_detail in ['high', 'low']\n        self.img_detail = img_detail\n\n        self.vision = True\n        self.timeout = timeout\n\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(role='system', content=self.system_prompt))\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(type='text', text=msg['value']))\n                elif msg['type'] == 'image':\n                    from PIL import Image\n                    img = Image.open(msg['value'])\n                    b64 = encode_image_to_base64(img, target_size=self.img_size)\n                    img_struct = dict(url=f\"data:image/jpeg;base64,{b64}\", detail=self.img_detail)\n                    content_list.append(dict(type='image_url', image_url=img_struct))\n            input_msgs.append(dict(role='user', content=content_list))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            input_msgs.append(dict(role='user', content=text))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        input_msgs = self.prepare_inputs(inputs)\n        temperature = kwargs.pop('temperature', self.temperature)\n        max_tokens = kwargs.pop('max_tokens', self.max_tokens)\n\n        if 0 < max_tokens <= 100:\n            self.logger.warning(\n                'Less than 100 tokens left, '\n                'may exceed the context window with some additional meta symbols. '\n            )\n        if max_tokens <= 0:\n            return 0, self.fail_msg + 'Input string longer than context window. ', 'Length Exceeded. '\n\n        headers = {'Content-Type': 'application/json', 'Authorization': f'{self.key}'}\n        payload = dict(\n            model=self.model,\n            messages=input_msgs,\n            max_tokens=max_tokens,\n            n=1,\n            temperature=temperature,\n            **kwargs)\n        response = requests.post(self.api_base, headers=headers, data=json.dumps(payload), timeout=self.timeout * 1.1)\n        ret_code = response.status_code\n        ret_code = 0 if (200 <= int(ret_code) < 300) else ret_code\n        answer = self.fail_msg\n        try:\n            resp_struct = json.loads(response.text)\n            answer = resp_struct['choices'][0]['message']['content'].strip()\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(response.text if hasattr(response, 'text') else response)\n\n        return ret_code, answer, response\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/lmdeploy.py", "content": "# from http import HTTPStatus\nimport os\nimport requests\nfrom ..dataset import DATASET_TYPE, DATASET_MODALITY\nfrom vlmeval.api.base import BaseAPI\nfrom vlmeval.smp import *\n\n\nclass InternVL2_PromptUtil:\n\n    def __init__(self, use_mpo_prompt=False):\n        self.use_mpo_prompt = use_mpo_prompt\n\n    def dump_image(self, line, dataset):\n        return self.dump_image_func(line)\n\n    def use_custom_prompt(self, dataset):\n        assert dataset is not None\n        assert DATASET_MODALITY(dataset) != 'VIDEO', 'not supported'\n        if listinstr(['MMDU', 'MME-RealWorld', 'MME-RealWorld-CN'], dataset):\n            # For Multi-Turn we don't have custom prompt\n            return False\n        if DATASET_MODALITY(dataset) == 'VIDEO':\n            # For Video benchmarks we don't have custom prompt at here\n            return False\n        else:\n            return True\n\n    def build_prompt(self, line, dataset=None):\n        assert self.use_custom_prompt(dataset)\n        assert dataset is None or isinstance(dataset, str)\n        from ..vlm.internvl.utils import (build_multi_choice_prompt,\n                                          build_mcq_cot_prompt,\n                                          build_qa_cot_prompt,\n                                          build_mpo_prompt,\n                                          reorganize_prompt)\n\n        tgt_path = self.dump_image(line, dataset)\n        max_num = self.get_max_num(dataset)\n        if dataset is not None and DATASET_TYPE(dataset) == 'Y/N':\n            question = line['question']\n            if listinstr(['MME'], dataset):\n                prompt = question + ' Answer the question using a single word or phrase.'\n            elif listinstr(['HallusionBench', 'AMBER'], dataset):\n                prompt = question + ' Please answer yes or no. Answer the question using a single word or phrase.'\n            else:\n                prompt = question\n        elif dataset is not None and DATASET_TYPE(dataset) == 'MCQ':\n            prompt = build_multi_choice_prompt(line, dataset)\n            if os.getenv('USE_COT') == '1':\n                prompt = build_mcq_cot_prompt(line, prompt)\n        elif dataset is not None and DATASET_TYPE(dataset) == 'VQA':\n            question = line['question']\n            if listinstr(['LLaVABench', 'WildVision'], dataset):\n                prompt = question + '\\nAnswer this question in detail.'\n            elif listinstr(['OCRVQA', 'TextVQA', 'ChartQA', 'DocVQA', 'InfoVQA', 'OCRBench',\n                            'DUDE', 'SLIDEVQA', 'GQA', 'MMLongBench_DOC'], dataset):\n                prompt = question + '\\nAnswer the question using a single word or phrase.'\n            elif listinstr(['MathVista', 'MathVision', 'VCR', 'MTVQA', 'MMVet', 'MathVerse',\n                            'MMDU', 'CRPE', 'MIA-Bench', 'MM-Math', 'DynaMath', 'QSpatial', 'WeMath', 'LogicVista'], dataset):\n                prompt = question\n                if os.getenv('USE_COT') == '1':\n                    prompt = build_qa_cot_prompt(line, prompt)\n            else:\n                prompt = question + '\\nAnswer the question using a single word or phrase.'\n        else:\n            # VQA_ex_prompt: OlympiadBench, VizWiz\n            prompt = line['question']\n            if os.getenv('USE_COT') == '1':\n                prompt = build_qa_cot_prompt(line, prompt)\n\n        message = [dict(type='text', value=prompt)]\n        image_num = len(tgt_path)\n        max_num = max(1, min(max_num, 64 // image_num))\n        # TODO：support upscale_flag\n        message.extend([dict(type='image', value=s, max_dynamic_patch=max_num) for s in tgt_path])\n\n        if self.use_mpo_prompt:\n            message = build_mpo_prompt(message, line, dataset)\n\n        # reorganize_prompt\n        prompt = reorganize_prompt(message, image_num, dataset=dataset)\n        prompt.replace('<image>', '<IMAGE_TOKEN>')\n        message[0] = dict(type='text', value=prompt)\n        return message\n\n    def get_max_num(self, dataset):\n        assert dataset is not None\n        res_1_datasets = ['MMBench-Video', 'Video-MME', 'MVBench', 'Video', 'WorldSense']\n        res_12_datasets = ['ChartQA_TEST', 'MMMU_DEV_VAL', 'MMMU_TEST', 'MME-RealWorld',\n                           'VCR_EN', 'VCR_ZH', 'OCRVQA']\n        res_18_datasets = ['DocVQA_VAL', 'DocVQA_TEST', 'DUDE', 'MMLongBench_DOC', 'SLIDEVQA']\n        res_24_datasets = ['InfoVQA_VAL', 'InfoVQA_TEST', 'OCRBench', 'HRBench4K', 'HRBench8K']\n        if listinstr(res_1_datasets, dataset):\n            return 1\n        elif listinstr(res_12_datasets, dataset):\n            return 12\n        elif listinstr(res_18_datasets, dataset):\n            return 18\n        elif listinstr(res_24_datasets, dataset):\n            return 24\n        else:\n            return 6\n\n\nclass CogVLM2_PromptUtil:\n\n    def dump_image(self, line, dataset):\n        return self.dump_image_func(line)\n\n    def use_custom_prompt(self, dataset):\n        assert dataset is not None\n        if DATASET_TYPE(dataset) in 'MCQ':\n            return True\n        return False\n\n    def build_prompt(self, line, dataset=None):\n        assert dataset is None or isinstance(dataset, str)\n        assert self.use_custom_prompt(dataset)\n        tgt_path = self.dump_image(line, dataset)\n\n        if dataset is not None and DATASET_TYPE(dataset) == 'MCQ':\n            question = line['question']\n            hint = line['hint'] if ('hint' in line and not pd.isna(line['hint'])) else None\n            if hint is not None:\n                question = hint + '\\n' + question\n\n            option_candidate = string.ascii_uppercase\n            options = {\n                cand: line[cand]\n                for cand in option_candidate\n                if cand in line and not pd.isna(line[cand])\n            }\n            for key, item in options.items():\n                question += f'\\n{key}. {item}'\n            prompt = question\n\n            if not cn_string(prompt):\n                prompt = prompt + '\\n' + \"Answer with the option's letter from the given choices directly.\"\n            else:\n                prompt = prompt + '\\n' + '请直接回答选项字母。'\n        else:\n            prompt = line['question']\n        message = [dict(type='text', value=prompt)]\n        message.extend([dict(type='image', value=p) for p in tgt_path])\n        return message\n\n\nclass LMDeployWrapper(BaseAPI):\n\n    is_api: bool = True\n\n    custom_prompt: str = None\n    prompt_map = {\n        'cogvlm2': CogVLM2_PromptUtil(),\n        'internvl2': InternVL2_PromptUtil(),\n        'internvl2-mpo-cot': InternVL2_PromptUtil(use_mpo_prompt=True),\n    }\n\n    def __init__(self,\n                 retry: int = 5,\n                 wait: int = 5,\n                 key: str = 'sk-123456',\n                 verbose: bool = True,\n                 temperature: float = 0.0,\n                 timeout: int = 60,\n                 api_base: str = None,\n                 system_prompt: str = None,\n                 max_tokens: int = 1024,\n                 **kwargs):\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.max_tokens = max_tokens\n        self.timeout = timeout\n\n        key = os.environ.get('LMDEPLOY_API_KEY', key)\n        api_base = os.environ.get('LMDEPLOY_API_BASE', api_base)\n        assert key is not None, 'Please set the environment variable LMDEPLOY_API_KEY.'\n        assert api_base is not None, 'Please set the environment variable LMDEPLOY_API_BASE.'\n        self.key = key\n        self.api_base = api_base\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n        model_url = ''.join([api_base.split('v1')[0], 'v1/models'])\n        resp = requests.get(model_url)\n        self.model = resp.json()['data'][0]['id']\n        self.logger.info(f'lmdeploy evaluate model: {self.model}')\n        self.set_prompt_pattern(self.model)\n        if hasattr(self, 'custom_prompt'):\n            self.logger.info(f'using custom prompt {self.custom_prompt}')\n        self.temperature = temperature\n        self.logger.info(f'Init temperature: {self.temperature}')\n\n    def set_dump_image(self, dump_image_func):\n        if self.custom_prompt in self.prompt_map:\n            self.prompt_map[self.custom_prompt].dump_image_func = dump_image_func\n        self.dump_image_func = dump_image_func\n\n    def use_custom_prompt(self, dataset):\n        if self.custom_prompt in self.prompt_map:\n            return self.prompt_map[self.custom_prompt].use_custom_prompt(dataset)\n        return False\n\n    def build_prompt(self, line, dataset=None):\n        if self.custom_prompt in self.prompt_map:\n            return self.prompt_map[self.custom_prompt].build_prompt(line, dataset)\n        raise NotImplementedError\n\n    def set_prompt_pattern(self, model_name):\n        if 'Phi-3.5-Vision'.lower() in model_name.lower():\n            self.max_tokens = 1000\n            self.temperature = 0.0\n        if 'cogvlm2-llama3-chat-19B'.lower() in model_name.lower():\n            self.max_tokens = 2048\n            self.temperature = 0.0\n            self.custom_prompt = 'cogvlm2'\n        if 'InternVL2'.lower() in model_name.lower():\n            self.max_tokens = 1024\n            self.temperature = 0.0\n            if 'mpo' in model_name.lower():\n                self.max_tokens = 4096\n                self.logger.info('Use custom prompt internvl2-mpo-cot')\n                self.custom_prompt = 'internvl2-mpo-cot'\n            else:\n                self.logger.info('Use custom prompt internvl2')\n                self.custom_prompt = 'internvl2'\n        if 'internvl2-8b-mpo-cot'.lower() in model_name.lower():\n            self.use_mpo_prompt = True\n            self.max_tokens = 1024\n            self.temperature = 0.0\n            self.logger.info('Use custom prompt internvl2-mpo-cot')\n            self.custom_prompt = 'internvl2-mpo-cot'\n        if 'qvq'.lower() in model_name.lower():\n            self.max_tokens = 4096\n            self.temperature = 0.0\n            self.logger.info('QVQ model detected, do not use custom prompt')\n\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(type='text', text=msg['value']))\n                elif msg['type'] == 'image':\n                    from PIL import Image\n                    img = Image.open(msg['value'])\n                    b64 = encode_image_to_base64(img)\n                    extra_args = msg.copy()\n                    extra_args.pop('type')\n                    extra_args.pop('value')\n                    img_struct = dict(url=f'data:image/jpeg;base64,{b64}', **extra_args)\n                    content_list.append(dict(type='image_url', image_url=img_struct))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(type='text', text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(role='system', content=self.system_prompt))\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(role=item['role'], content=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(role='user', content=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        input_msgs = self.prepare_inputs(inputs)\n\n        temperature = kwargs.pop('temperature', self.temperature)\n        self.logger.info(f'Generate temperature: {temperature}')\n        max_tokens = kwargs.pop('max_tokens', self.max_tokens)\n\n        headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.key}'}\n        payload = dict(\n            model=self.model,\n            messages=input_msgs,\n            max_tokens=max_tokens,\n            n=1,\n            temperature=temperature,\n            **kwargs)\n        response = requests.post(\n            self.api_base,\n            headers=headers, data=json.dumps(payload), timeout=self.timeout * 1.1)\n        ret_code = response.status_code\n        ret_code = 0 if (200 <= int(ret_code) < 300) else ret_code\n        answer = self.fail_msg\n        try:\n            resp_struct = json.loads(response.text)\n            answer = resp_struct['choices'][0]['message']['content'].strip()\n\n            # for internvl2-8b-mpo-cot\n            if getattr(self, 'use_mpo_prompt', False):\n                from ..vlm.internvl.utils import mpo_post_processing\n                answer = mpo_post_processing(answer, kwargs.get('dataset'))\n        except:\n            pass\n        return ret_code, answer, response\n\n\nclass LMDeployAPI(LMDeployWrapper):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def generate(self, message, dataset=None):\n        return super(LMDeployAPI, self).generate(message, dataset=dataset)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/scripts/summarize.py", "content": "from vlmeval.smp import *\nfrom vlmeval.dataset import SUPPORTED_DATASETS\n\ndef get_score(model, dataset):\n\n    file_name = f'{model}/{model}_{dataset}'\n    if listinstr([\n        'CCBench', 'MMBench', 'SEEDBench_IMG', 'MMMU', 'ScienceQA', \n        'AI2D_TEST', 'MMStar', 'RealWorldQA', 'BLINK', 'VisOnlyQA-VLMEvalKit'\n    ], dataset):\n        file_name += '_acc.csv'\n    elif listinstr(['MME', 'Hallusion', 'LLaVABench'], dataset):\n        file_name += '_score.csv'\n    elif listinstr(['MMVet', 'MathVista'], dataset):\n        file_name += '_gpt-4-turbo_score.csv'\n    elif listinstr(['COCO', 'OCRBench'], dataset):\n        file_name += '_score.json'\n    else:\n        raise NotImplementedError\n    \n    if not osp.exists(file_name):\n        return {}\n    \n    data = load(file_name)\n    ret = {}\n    if dataset == 'CCBench':\n        ret[dataset] = data['Overall'][0] * 100\n    elif dataset == 'MMBench':\n        for n, a in zip(data['split'], data['Overall']):\n            if n == 'dev':\n                ret['MMBench_DEV_EN'] = a * 100\n            elif n == 'test':\n                ret['MMBench_TEST_EN'] = a * 100\n    elif dataset == 'MMBench_CN':\n        for n, a in zip(data['split'], data['Overall']):\n            if n == 'dev':\n                ret['MMBench_DEV_CN'] = a * 100\n            elif n == 'test':\n                ret['MMBench_TEST_CN'] = a * 100\n    elif listinstr(['SEEDBench', 'ScienceQA', 'MMBench', 'AI2D_TEST', 'MMStar', 'RealWorldQA', 'BLINK'], dataset):\n        ret[dataset] = data['Overall'][0] * 100\n    elif 'MME' == dataset:\n        ret[dataset] = data['perception'][0] + data['reasoning'][0]\n    elif 'MMVet' == dataset:\n        data = data[data['Category'] == 'Overall']\n        ret[dataset] = float(data.iloc[0]['acc'])\n    elif 'HallusionBench' == dataset:\n        data = data[data['split'] == 'Overall']\n        for met in ['aAcc', 'qAcc', 'fAcc']:\n            ret[dataset + f' ({met})'] = float(data.iloc[0][met])\n    elif 'MMMU' in dataset:\n        data = data[data['split'] == 'validation']\n        ret['MMMU (val)'] = float(data.iloc[0]['Overall']) * 100\n    elif 'MathVista' in dataset:\n        data = data[data['Task&Skill'] == 'Overall']\n        ret[dataset] = float(data.iloc[0]['acc'])\n    elif 'LLaVABench' in dataset:\n        data = data[data['split'] == 'overall'].iloc[0]\n        ret[dataset] = float(data['Relative Score (main)'])\n    elif 'OCRBench' in dataset:\n        ret[dataset] = data['Final Score']\n    elif dataset == 'VisOnlyQA-VLMEvalKit':\n        for n, a in zip(data['split'], data['Overall']):\n            ret[f'VisOnlyQA-VLMEvalKit_{n}'] = a * 100\n     \n    return ret\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, nargs='+', default=[])\n    parser.add_argument(\"--model\", type=str, nargs='+', required=True)\n    args = parser.parse_args()\n    return args\n\ndef gen_table(models, datasets):\n    res = defaultdict(dict)\n    for m in models:\n        for d in datasets:\n            try:\n                res[m].update(get_score(m, d))\n            except Exception as e:\n                logging.warning(f'{type(e)}: {e}')\n                logging.warning(f'Missing Results for Model {m} x Dataset {d}')\n    keys = []\n    for m in models:\n        for d in res[m]:\n            keys.append(d)\n    keys = list(set(keys))\n    keys.sort()\n    final = defaultdict(list)\n    for m in models:\n        final['Model'].append(m)\n        for k in keys:\n            if k in res[m]:\n                final[k].append(res[m][k])\n            else:\n                final[k].append(None)\n    final = pd.DataFrame(final)\n    dump(final, 'summ.csv')\n    if len(final) >= len(final.iloc[0].keys()):\n        print(tabulate(final))\n    else:\n        print(tabulate(final.T))\n    \nif __name__ == '__main__':\n    args = parse_args()\n    if args.data == []:\n        args.data = list(SUPPORTED_DATASETS)\n    gen_table(args.model, args.data)"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/base.py", "content": "import time\nimport random as rd\nfrom abc import abstractmethod\nimport os.path as osp\nimport copy as cp\nfrom ..smp import get_logger, parse_file, concat_images_vlmeval, LMUDataRoot, md5, decode_base64_to_image_file\n\n\nclass BaseAPI:\n\n    allowed_types = ['text', 'image']\n    INTERLEAVE = True\n    INSTALL_REQ = False\n\n    def __init__(self,\n                 retry=10,\n                 wait=3,\n                 system_prompt=None,\n                 verbose=True,\n                 fail_msg='Failed to obtain answer via API.',\n                 **kwargs):\n        \"\"\"Base Class for all APIs.\n\n        Args:\n            retry (int, optional): The retry times for `generate_inner`. Defaults to 10.\n            wait (int, optional): The wait time after each failed retry of `generate_inner`. Defaults to 3.\n            system_prompt (str, optional): Defaults to None.\n            verbose (bool, optional): Defaults to True.\n            fail_msg (str, optional): The message to return when failed to obtain answer.\n                Defaults to 'Failed to obtain answer via API.'.\n            **kwargs: Other kwargs for `generate_inner`.\n        \"\"\"\n\n        self.wait = wait\n        self.retry = retry\n        self.system_prompt = system_prompt\n        self.verbose = verbose\n        self.fail_msg = fail_msg\n        self.logger = get_logger('ChatAPI')\n\n        if len(kwargs):\n            self.logger.info(f'BaseAPI received the following kwargs: {kwargs}')\n            self.logger.info('Will try to use them as kwargs for `generate`. ')\n        self.default_kwargs = kwargs\n\n    @abstractmethod\n    def generate_inner(self, inputs, **kwargs):\n        \"\"\"The inner function to generate the answer.\n\n        Returns:\n            tuple(int, str, str): ret_code, response, log\n        \"\"\"\n        self.logger.warning('For APIBase, generate_inner is an abstract method. ')\n        assert 0, 'generate_inner not defined'\n        ret_code, answer, log = None, None, None\n        # if ret_code is 0, means succeed\n        return ret_code, answer, log\n\n    def working(self):\n        \"\"\"If the API model is working, return True, else return False.\n\n        Returns:\n            bool: If the API model is working, return True, else return False.\n        \"\"\"\n        self.old_timeout = None\n        if hasattr(self, 'timeout'):\n            self.old_timeout = self.timeout\n            self.timeout = 120\n\n        retry = 5\n        while retry > 0:\n            ret = self.generate('hello')\n            if ret is not None and ret != '' and self.fail_msg not in ret:\n                if self.old_timeout is not None:\n                    self.timeout = self.old_timeout\n                return True\n            retry -= 1\n\n        if self.old_timeout is not None:\n            self.timeout = self.old_timeout\n        return False\n\n    def check_content(self, msgs):\n        \"\"\"Check the content type of the input. Four types are allowed: str, dict, liststr, listdict.\n\n        Args:\n            msgs: Raw input messages.\n\n        Returns:\n            str: The message type.\n        \"\"\"\n        if isinstance(msgs, str):\n            return 'str'\n        if isinstance(msgs, dict):\n            return 'dict'\n        if isinstance(msgs, list):\n            types = [self.check_content(m) for m in msgs]\n            if all(t == 'str' for t in types):\n                return 'liststr'\n            if all(t == 'dict' for t in types):\n                return 'listdict'\n        return 'unknown'\n\n    def preproc_content(self, inputs):\n        \"\"\"Convert the raw input messages to a list of dicts.\n\n        Args:\n            inputs: raw input messages.\n\n        Returns:\n            list(dict): The preprocessed input messages. Will return None if failed to preprocess the input.\n        \"\"\"\n        if self.check_content(inputs) == 'str':\n            return [dict(type='text', value=inputs)]\n        elif self.check_content(inputs) == 'dict':\n            assert 'type' in inputs and 'value' in inputs\n            return [inputs]\n        elif self.check_content(inputs) == 'liststr':\n            res = []\n            for s in inputs:\n                mime, pth = parse_file(s)\n                if mime is None or mime == 'unknown':\n                    res.append(dict(type='text', value=s))\n                else:\n                    res.append(dict(type=mime.split('/')[0], value=pth))\n            return res\n        elif self.check_content(inputs) == 'listdict':\n            for item in inputs:\n                assert 'type' in item and 'value' in item\n                mime, s = parse_file(item['value'])\n                if mime is None:\n                    assert item['type'] == 'text', item['value']\n                else:\n                    assert mime.split('/')[0] == item['type']\n                    item['value'] = s\n            return inputs\n        else:\n            return None\n\n    # May exceed the context windows size, so try with different turn numbers.\n    def chat_inner(self, inputs, **kwargs):\n        _ = kwargs.pop('dataset', None)\n        while len(inputs):\n            try:\n                return self.generate_inner(inputs, **kwargs)\n            except Exception as e:\n                if self.verbose:\n                    self.logger.info(f'{type(e)}: {e}')\n                inputs = inputs[1:]\n                while len(inputs) and inputs[0]['role'] != 'user':\n                    inputs = inputs[1:]\n                continue\n        return -1, self.fail_msg + ': ' + 'Failed with all possible conversation turns.', None\n\n    def chat(self, messages, **kwargs1):\n        \"\"\"The main function for multi-turn chatting. Will call `chat_inner` with the preprocessed input messages.\"\"\"\n        assert hasattr(self, 'chat_inner'), 'The API model should has the `chat_inner` method. '\n        for msg in messages:\n            assert isinstance(msg, dict) and 'role' in msg and 'content' in msg, msg\n            assert self.check_content(msg['content']) in ['str', 'dict', 'liststr', 'listdict'], msg\n            msg['content'] = self.preproc_content(msg['content'])\n        # merge kwargs\n        kwargs = cp.deepcopy(self.default_kwargs)\n        kwargs.update(kwargs1)\n\n        answer = None\n        # a very small random delay [0s - 0.5s]\n        T = rd.random() * 0.5\n        time.sleep(T)\n\n        assert messages[-1]['role'] == 'user'\n\n        for i in range(self.retry):\n            try:\n                ret_code, answer, log = self.chat_inner(messages, **kwargs)\n                if ret_code == 0 and self.fail_msg not in answer and answer != '':\n                    if self.verbose:\n                        print(answer)\n                    return answer\n                elif self.verbose:\n                    if not isinstance(log, str):\n                        try:\n                            log = log.text\n                        except Exception as e:\n                            self.logger.warning(f'Failed to parse {log} as an http response: {str(e)}. ')\n                    self.logger.info(f'RetCode: {ret_code}\\nAnswer: {answer}\\nLog: {log}')\n            except Exception as err:\n                if self.verbose:\n                    self.logger.error(f'An error occured during try {i}: ')\n                    self.logger.error(f'{type(err)}: {err}')\n            # delay before each retry\n            T = rd.random() * self.wait * 2\n            time.sleep(T)\n\n        return self.fail_msg if answer in ['', None] else answer\n\n    def preprocess_message_with_role(self, message):\n        system_prompt = ''\n        new_message = []\n\n        for data in message:\n            assert isinstance(data, dict)\n            role = data.pop('role', 'user')\n            if role == 'system':\n                system_prompt += data['value'] + '\\n'\n            else:\n                new_message.append(data)\n\n        if system_prompt != '':\n            if self.system_prompt is None:\n                self.system_prompt = system_prompt\n            else:\n                self.system_prompt += '\\n' + system_prompt\n        return new_message\n\n    def generate(self, message, **kwargs1):\n        \"\"\"The main function to generate the answer. Will call `generate_inner` with the preprocessed input messages.\n\n        Args:\n            message: raw input messages.\n\n        Returns:\n            str: The generated answer of the Failed Message if failed to obtain answer.\n        \"\"\"\n        if self.check_content(message) == 'listdict':\n            message = self.preprocess_message_with_role(message)\n\n        assert self.check_content(message) in ['str', 'dict', 'liststr', 'listdict'], f'Invalid input type: {message}'\n        message = self.preproc_content(message)\n        assert message is not None and self.check_content(message) == 'listdict'\n        for item in message:\n            assert item['type'] in self.allowed_types, f'Invalid input type: {item[\"type\"]}'\n\n        # merge kwargs\n        kwargs = cp.deepcopy(self.default_kwargs)\n        kwargs.update(kwargs1)\n\n        answer = None\n        # a very small random delay [0s - 0.5s]\n        T = rd.random() * 0.5\n        time.sleep(T)\n\n        for i in range(self.retry):\n            try:\n                ret_code, answer, log = self.generate_inner(message, **kwargs)\n                if ret_code == 0 and self.fail_msg not in answer and answer != '':\n                    if self.verbose:\n                        print(answer)\n                    return answer\n                elif self.verbose:\n                    if not isinstance(log, str):\n                        try:\n                            log = log.text\n                        except Exception as e:\n                            self.logger.warning(f'Failed to parse {log} as an http response: {str(e)}. ')\n                    self.logger.info(f'RetCode: {ret_code}\\nAnswer: {answer}\\nLog: {log}')\n            except Exception as err:\n                if self.verbose:\n                    self.logger.error(f'An error occured during try {i}: ')\n                    self.logger.error(f'{type(err)}: {err}')\n            # delay before each retry\n            T = rd.random() * self.wait * 2\n            time.sleep(T)\n\n        return self.fail_msg if answer in ['', None] else answer\n\n    def message_to_promptimg(self, message, dataset=None):\n        assert not self.INTERLEAVE\n        model_name = self.__class__.__name__\n        import warnings\n        warnings.warn(\n            f'Model {model_name} does not support interleaved input. '\n            'Will use the first image and aggregated texts as prompt. ')\n        num_images = len([x for x in message if x['type'] == 'image'])\n        if num_images == 0:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            image = None\n        elif num_images == 1:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            image = [x['value'] for x in message if x['type'] == 'image'][0]\n        else:\n            prompt = '\\n'.join([x['value'] if x['type'] == 'text' else '<image>' for x in message])\n            if dataset == 'BLINK':\n                image = concat_images_vlmeval(\n                    [x['value'] for x in message if x['type'] == 'image'],\n                    target_size=512)\n            else:\n                image = [x['value'] for x in message if x['type'] == 'image'][0]\n        return prompt, image\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/bluelm_v_api.py", "content": "from vlmeval.smp import *\nfrom vlmeval.api.base import BaseAPI\nimport os\nimport json\n\n\ndef multimodal(images, text, url, key, temperature=0, max_tokens=1024, history=[]):\n    if images:\n        pics = []\n        for image in images:\n            with open(image, 'rb') as f:\n                pic = base64.b64encode(f.read()).decode('utf-8')\n            pics.append(pic)\n        data = {'images': pics, 'text': text, 'key': key, 'temperature': temperature, 'max_new_tokens': max_tokens}\n    else:\n        data = {'text': text, 'key': key, 'temperature': temperature, 'max_new_tokens': max_tokens}\n    response = requests.post(url, json=data, headers={'Content-Type': 'application/json'})\n    response = json.loads(response.text)\n    return response\n\n\nclass BlueLMWrapper(BaseAPI):\n    is_api: bool = True\n\n    def __init__(self,\n                 model: str = 'BlueLM-V-v3.0',\n                 retry: int = 5,\n                 wait: int = 5,\n                 verbose: bool = True,\n                 temperature: float = 0.0,\n                 system_prompt: str = None,\n                 max_tokens: int = 1024,\n                 key: str = None,\n                 url: str = 'http://api-ai.vivo.com.cn/multimodal',\n                 **kwargs):\n\n        self.model = model\n        self.fail_msg = 'Failed to obtain answer BlueLM-V API. '\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.url = url\n        self.key = key\n\n        if self.key is None:\n            self.key = os.environ.get('BLUELM_V_API_KEY', None)\n        assert self.key is not None, (\n            'Please set the API Key (obtain it here: '\n            'contact by email : shuai.ren@vivo.com'\n        )\n\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n    def message_to_promptimg(self, message, dataset=None):\n\n        num_images = len([x for x in message if x['type'] == 'image'])\n        if num_images == 0:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            image = None\n        elif num_images == 1:\n            prompt = '\\n'.join([x['value'] for x in message if x['type'] == 'text'])\n            image = [x['value'] for x in message if x['type'] == 'image']\n        else:\n            prompt = '\\n'.join([x['value'] if x['type'] == 'text' else '<image>' for x in message])\n            if dataset == 'BLINK':\n                image = concat_images_vlmeval(\n                    [x['value'] for x in message if x['type'] == 'image'],\n                    target_size=512)\n            else:\n                image = [x['value'] for x in message if x['type'] == 'image']\n\n        if dataset in ['MMBench_DEV_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_EN_V11', 'MMBench_TEST_CN_V11',\n                       'AI2D_TEST', 'AI2D_TEST_TO_MASK', 'MMMU_DEV_VAL']:\n            prompt = prompt.replace('Please select the correct answer from the options above.',\n                                    'Answer with the option’s letter from the given choices directly.')\n        elif dataset in ['ChartQA_TEST']:\n            prompt = prompt.replace('Answer the question using a single word or phrase.',\n                                    'Answer the question using a single number or phrase.')\n        elif dataset in ['DocVQA_VAL', 'DocVQA_TEST', ]:\n            prompt = prompt.replace('Answer the question using a single word or phrase.',\n                                    'Give the short answer directly.')\n        elif dataset in ['TextVQA_VAL']:\n            prompt = prompt.replace('Answer the question using a single word or phrase.',\n                                    'When the provided information is insufficient, respond with ’Unanswerable’.'\n                                    'Answer the question using a single word or phrase.')\n        elif dataset in ['MTVQA_TEST']:\n            prompt = prompt.replace('\\nAnswer the question using a word or phrase in the language of the question.', '')\n        elif dataset in ['MathVista_MINI']:\n            if 'Choices:' in prompt:\n                prompt = prompt.replace('Choices:', 'Options:').replace('Hint:', 'Context:')\n                for i in range(1, 7):  # replace A ~ F\n                    prompt = prompt.replace(f'({chr(64 + i)})', f'{chr(64 + i)}.')\n                prompt += '\\nAnswer with the option’s letter from the given choices directly.'\n            else:\n                prompt += '\\nAnswer the question using a single word or phrase.'\n\n        return prompt, image\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n\n        assert isinstance(inputs, str) or isinstance(inputs, list)\n        pure_text = np.all([x['type'] == 'text' for x in inputs])\n        assert not pure_text\n\n        prompt, image_path = self.message_to_promptimg(inputs, kwargs['dataset'])\n\n        try:\n            response = multimodal(image_path, prompt, self.url, self.key, self.temperature, self.max_tokens)\n            answer = response['result']\n            return 0, answer, 'Succeeded! '\n        except Exception as err:\n            if self.verbose:\n                self.logger.error(f'{type(err)}: {err}')\n                self.logger.error(f'The input messages are {inputs}.')\n            return -1, '', ''\n\n\nclass BlueLM_V_API(BlueLMWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(BlueLM_V_API, self).generate(message, dataset=dataset)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/scripts/auto_run.py", "content": "import argparse\nfrom vlmeval.smp import *\nfrom vlmeval.config import supported_VLM\n\ndef is_api(x):\n    return getattr(supported_VLM[x].func, 'is_api', False)\n\nmodels = list(supported_VLM)\nmodels = [x for x in models if 'fs' not in x]\nmodels = [x for x in models if not is_api(x)]\nexclude_list = ['cogvlm-grounding-generalist', 'emu2']\nmodels = [x for x in models if x not in exclude_list]\n\ndef is_large(x):\n    return '80b' in x or 'emu2' in x or '34B' in x\n\nsmall_models = [x for x in models if not is_large(x)]\nlarge_models = [x for x in models if is_large(x)]\nmodels = small_models + large_models\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data', type=str, nargs='+', required=True)\nargs = parser.parse_args()\n\n# Skip some models\nmodels = [x for x in models if not listinstr(['MiniGPT', 'grounding-generalist'], x)]\n\nfor m in models:\n    unknown_datasets = [x for x in args.data if not osp.exists(f'{m}/{m}_{x}.xlsx')]\n    if len(unknown_datasets) == 0:\n        continue\n    dataset_str = ' '.join(unknown_datasets)\n    if '80b' in m:\n        cmd = f'python run.py --data {dataset_str} --model {m}'\n    else:\n        cmd = f'bash run.sh --data {dataset_str} --model {m}'\n    print(cmd)\n    os.system(cmd)"}
{"type": "source_file", "path": "eval/VLMEvalKit/vlmeval/api/hunyuan.py", "content": "from vlmeval.smp import *\nimport os\nimport sys\nfrom vlmeval.api.base import BaseAPI\nimport math\nfrom vlmeval.dataset import DATASET_TYPE\nfrom vlmeval.dataset import img_root_map\nfrom io import BytesIO\nimport pandas as pd\nimport requests\nimport json\nimport base64\nimport time\n\n\nclass HunyuanWrapper(BaseAPI):\n\n    is_api: bool = True\n    _apiVersion = '2024-12-31'\n    _service = 'hunyuan'\n\n    def __init__(self,\n                 model: str = 'hunyuan-standard-vision',\n                 retry: int = 5,\n                 wait: int = 5,\n                 secret_key: str = None,\n                 secret_id: str = None,\n                 verbose: bool = True,\n                 system_prompt: str = None,\n                 temperature: float = 0,\n                 timeout: int = 60,\n                 api_base: str = 'hunyuan.tencentcloudapi.com',\n                 **kwargs):\n\n        self.model = model\n        self.cur_idx = 0\n        self.fail_msg = 'Failed to obtain answer via API. '\n        self.temperature = temperature\n\n        warnings.warn('You may need to set the env variable HUNYUAN_SECRET_ID & HUNYUAN_SECRET_KEY to use Hunyuan. ')\n\n        secret_key = os.environ.get('HUNYUAN_SECRET_KEY', secret_key)\n        assert secret_key is not None, 'Please set the environment variable HUNYUAN_SECRET_KEY. '\n        secret_id = os.environ.get('HUNYUAN_SECRET_ID', secret_id)\n        assert secret_id is not None, 'Please set the environment variable HUNYUAN_SECRET_ID. '\n\n        self.model = model\n        self.endpoint = api_base\n        self.secret_id = secret_id\n        self.secret_key = secret_key\n        self.timeout = timeout\n\n        try:\n            from tencentcloud.common import credential\n            from tencentcloud.common.profile.client_profile import ClientProfile\n            from tencentcloud.common.profile.http_profile import HttpProfile\n            from tencentcloud.hunyuan.v20230901 import hunyuan_client\n        except ImportError as err:\n            self.logger.critical('Please install tencentcloud-sdk-python to use Hunyuan API. ')\n            raise err\n\n        super().__init__(wait=wait, retry=retry, system_prompt=system_prompt, verbose=verbose, **kwargs)\n\n        cred = credential.Credential(self.secret_id, self.secret_key)\n        httpProfile = HttpProfile(reqTimeout=300)\n        httpProfile.endpoint = self.endpoint\n        clientProfile = ClientProfile()\n        clientProfile.httpProfile = httpProfile\n        self.client = hunyuan_client.HunyuanClient(cred, '', clientProfile)\n        self.logger.info(\n            f'Using Endpoint: {self.endpoint}; API Secret ID: {self.secret_id}; API Secret Key: {self.secret_key}'\n        )\n\n    def dump_image(self, line, dataset):\n        \"\"\"Dump the image(s) of the input line to the corresponding dataset folder.\n\n        Args:\n            line (line of pd.DataFrame): The raw input line.\n            dataset (str): The name of the dataset.\n\n        Returns:\n            str | list[str]: The paths of the dumped images.\n        \"\"\"\n        ROOT = LMUDataRoot()\n        assert isinstance(dataset, str)\n\n        img_root = os.path.join(ROOT, 'images', img_root_map(dataset) if dataset in img_root_map(dataset) else dataset)\n        os.makedirs(img_root, exist_ok=True)\n        if 'image' in line:\n            if isinstance(line['image'], list):\n                tgt_path = []\n                assert 'image_path' in line\n                for img, im_name in zip(line['image'], line['image_path']):\n                    path = osp.join(img_root, im_name)\n                    if not read_ok(path):\n                        decode_base64_to_image_file(img, path)\n                    tgt_path.append(path)\n            else:\n                tgt_path = osp.join(img_root, f\"{line['index']}.jpg\")\n                if not read_ok(tgt_path):\n                    decode_base64_to_image_file(line['image'], tgt_path)\n                tgt_path = [tgt_path]\n        else:\n            assert 'image_path' in line\n            tgt_path = toliststr(line['image_path'])\n\n        return tgt_path\n\n    def use_custom_prompt(self, dataset_name):\n        if DATASET_TYPE(dataset_name) == 'MCQ':\n            return True\n        else:\n            return False\n\n    def build_prompt(self, line, dataset=None):\n        assert self.use_custom_prompt(dataset)\n        assert dataset is None or isinstance(dataset, str)\n\n        tgt_path = self.dump_image(line, dataset)\n\n        question = line['question']\n        options = {\n            cand: line[cand]\n            for cand in string.ascii_uppercase\n            if cand in line and not pd.isna(line[cand])\n        }\n        options_prompt = 'Options:\\n'\n        for key, item in options.items():\n            options_prompt += f'{key}. {item}\\n'\n        hint = line['hint'] if ('hint' in line and not pd.isna(line['hint'])) else None\n        prompt = ''\n        if hint is not None:\n            prompt += f'Hint: {hint}\\n'\n        prompt += f'Question: {question}\\n'\n        if len(options):\n            prompt += options_prompt\n            prompt += 'Answer with the option letter from the given choices directly.'\n\n        msgs = []\n        if isinstance(tgt_path, list):\n            msgs.extend([dict(type='image', value=p) for p in tgt_path])\n        else:\n            msgs = [dict(type='image', value=tgt_path)]\n        msgs.append(dict(type='text', value=prompt))\n        return msgs\n\n    # inputs can be a lvl-2 nested list: [content1, content2, content3, ...]\n    # content can be a string or a list of image & text\n    def prepare_itlist(self, inputs):\n        assert np.all([isinstance(x, dict) for x in inputs])\n        has_images = np.sum([x['type'] == 'image' for x in inputs])\n        if has_images:\n            content_list = []\n            for msg in inputs:\n                if msg['type'] == 'text':\n                    content_list.append(dict(Type='text', Text=msg['value']))\n                elif msg['type'] == 'image':\n                    from PIL import Image\n                    img = Image.open(msg['value'])\n                    b64 = encode_image_to_base64(img)\n                    img_struct = dict(Url=f'data:image/jpeg;base64,{b64}')\n                    content_list.append(dict(Type='image_url', ImageUrl=img_struct))\n        else:\n            assert all([x['type'] == 'text' for x in inputs])\n            text = '\\n'.join([x['value'] for x in inputs])\n            content_list = [dict(Type='text', Text=text)]\n        return content_list\n\n    def prepare_inputs(self, inputs):\n        input_msgs = []\n        if self.system_prompt is not None:\n            input_msgs.append(dict(Role='system', Content=self.system_prompt))\n        assert isinstance(inputs, list) and isinstance(inputs[0], dict)\n        assert np.all(['type' in x for x in inputs]) or np.all(['role' in x for x in inputs]), inputs\n        if 'role' in inputs[0]:\n            assert inputs[-1]['role'] == 'user', inputs[-1]\n            for item in inputs:\n                input_msgs.append(dict(Role=item['role'], Contents=self.prepare_itlist(item['content'])))\n        else:\n            input_msgs.append(dict(Role='user', Contents=self.prepare_itlist(inputs)))\n        return input_msgs\n\n    def generate_inner(self, inputs, **kwargs) -> str:\n        from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException\n        from tencentcloud.hunyuan.v20230901 import models\n\n        input_msgs = self.prepare_inputs(inputs)\n        temperature = kwargs.pop('temperature', self.temperature)\n\n        payload = dict(\n            Model=self.model,\n            Messages=input_msgs,\n            Temperature=temperature,\n            TopK=1,\n            **kwargs)\n\n        try:\n            req = models.ChatCompletionsRequest()\n            req.from_json_string(json.dumps(payload))\n            resp = self.client.ChatCompletions(req)\n            resp = json.loads(resp.to_json_string())\n            answer = resp['Choices'][0]['Message']['Content']\n            return 0, answer, resp\n        except TencentCloudSDKException as e:\n            self.logger.error(f'Got error code: {e.get_code()}')\n            if e.get_code() == 'ClientNetworkError':\n                return -1, self.fail_msg + e.get_code(), None\n            elif e.get_code() in ['InternalError', 'ServerNetworkError']:\n                return -1, self.fail_msg + e.get_code(), None\n            elif e.get_code() in ['LimitExceeded']:\n                return -1, self.fail_msg + e.get_code(), None\n            else:\n                return -1, self.fail_msg + str(e), None\n\n\nclass HunyuanVision(HunyuanWrapper):\n\n    def generate(self, message, dataset=None):\n        return super(HunyuanVision, self).generate(message)\n"}
{"type": "source_file", "path": "eval/VLMEvalKit/scripts/data_browser.py", "content": "\"\"\"\npip install gradio    # proxy_on first\npython vis_geochat_data.py\n# browse data in http://127.0.0.1:10064\n\"\"\"\n\nimport os\nimport io\nimport json\nimport copy\nimport time\nimport gradio as gr\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nfrom argparse import Namespace\n# from llava import conversation as conversation_lib\nfrom typing import Sequence\nfrom vlmeval import *\nfrom vlmeval.dataset import SUPPORTED_DATASETS, build_dataset\n\nSYS = \"You are a helpful assistant. Your job is to faithfully translate all provided text into Chinese faithfully. \"\n\n# Translator = SiliconFlowAPI(model='Qwen/Qwen2.5-7B-Instruct', system_prompt=SYS)\nTranslator = OpenAIWrapper(model='gpt-4o-mini', system_prompt=SYS)\n\n\ndef image_to_mdstring(image):\n    return f\"![image](data:image/jpeg;base64,{image})\"\n\n\ndef images_to_md(images):\n    return '\\n\\n'.join([image_to_mdstring(image) for image in images])\n\n\ndef mmqa_display(question, target_size=2048):\n    question = {k.lower() if len(k) > 1 else k: v for k, v in question.items()}\n    keys = list(question.keys())\n    keys = [k for k in keys if k not in ['index', 'image']]\n\n    idx = question.pop('index', 'XXX')\n    text = f'\\n- INDEX: {idx}\\n'\n\n    if 'image' in question:\n        images = question.pop('image')\n        if images[0] == '[' and images[-1] == ']':\n            images = eval(images)\n        else:\n            images = [images]\n    else:\n        images = question.pop('image_path')\n        if images[0] == '[' and images[-1] == ']':\n            images = eval(images)\n        else:\n            images = [images]\n        images = [encode_image_file_to_base64(x) for x in images]\n    \n    qtext = question.pop('question', None)\n    if qtext is not None:\n        text += f'- QUESTION: {qtext}\\n'\n\n    if 'A' in question:\n        text += f'- Choices: \\n'\n        for k in string.ascii_uppercase:\n            if k in question:\n                text += f'\\t-{k}: {question.pop(k)}\\n'\n    answer = question.pop('answer', None)\n    \n    for k in question:\n        if not pd.isna(question[k]):\n            text += f'- {k.upper()}. {question[k]}\\n'\n    \n    if answer is not None:\n        text += f'- ANSWER: {answer}\\n'\n\n    image_md = images_to_md(images)\n\n    return text, image_md\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    # Essential Args, Setting the Names of Datasets and Models\n    parser.add_argument('--port', type=int, default=7860)\n    args = parser.parse_args()\n    return args\n\n\ndef gradio_app_vis_dataset(port=7860):\n    data, loaded_obj = None, {}\n\n    def btn_submit_click(filename, ann_id):\n        if filename not in loaded_obj:\n            return filename_change(filename, ann_id)\n        nonlocal data\n        data_desc = gr.Markdown(f'Visualizing {filename}, {len(data)} samples in total. ')\n        if ann_id < 0 or ann_id >= len(data):\n            return filename, ann_id, data_desc, gr.Markdown('Invalid Index'), gr.Markdown(f'Index out of range [0, {len(data) - 1}]')\n        item = data.iloc[ann_id]\n        text, image_md = mmqa_display(item)\n        return filename, ann_id, data_desc, image_md, text\n\n    def btn_next_click(filename, ann_id):\n        return btn_submit_click(filename, ann_id + 1)\n\n    # def translate_click(anno_en):\n    #     return gr.Markdown(Translator.generate(anno_en))\n\n    def filename_change(filename, ann_id):\n        nonlocal data, loaded_obj\n\n        def legal_filename(filename):\n            LMURoot = LMUDataRoot()\n            if filename in SUPPORTED_DATASETS:\n                return build_dataset(filename).data\n            elif osp.exists(filename):\n                data = load(filename)\n                assert 'index' in data and 'image' in data\n                image_map = {i: image for i, image in zip(data['index'], data['image'])}\n                for k, v in image_map.items():\n                    if (not isinstance(v, str) or len(v) < 64) and v in image_map:\n                        image_map[k] = image_map[v]\n                data['image'] = [image_map[k] for k in data['index']]\n                return data\n            elif osp.exists(osp.join(LMURoot, filename)):\n                filename = osp.join(LMURoot, filename)\n                return legal_filename(filename)\n            else:\n                return None\n\n        data = legal_filename(filename)\n        if data is None:\n            return filename, 0, gr.Markdown(''), gr.Markdown(\"File not found\"), gr.Markdown(\"File not found\")\n        \n        loaded_obj[filename] = data\n        return btn_submit_click(filename, 0)\n\n    with gr.Blocks() as app:\n        \n        filename = gr.Textbox(\n            value='Dataset Name (supported by VLMEvalKit) or TSV FileName (Relative under `LMURoot` or Real Path)', \n            label='Dataset', \n            interactive=True,\n            visible=True)\n            \n        with gr.Row():\n            ann_id = gr.Number(0, label='Sample Index (Press Enter)', interactive=True, visible=True)\n            btn_next = gr.Button(\"Next\")\n            # btn_translate = gr.Button('CN Translate')\n\n        with gr.Row():\n            data_desc = gr.Markdown('Dataset Description', label='Dataset Description')\n        \n        with gr.Row():\n            image_output = gr.Markdown('Image PlaceHolder', label='Image Visualization')\n            anno_en = gr.Markdown('Image Annotation', label='Image Annotation')\n            # anno_cn = gr.Markdown('Image Annotation (Chinese)', label='Image Annotation (Chinese)')\n\n        input_components = [filename, ann_id]\n        all_components = [filename, ann_id, data_desc, image_output, anno_en]\n\n        filename.submit(filename_change, input_components, all_components)\n        ann_id.submit(btn_submit_click, input_components, all_components)\n        btn_next.click(btn_next_click, input_components, all_components)\n        # btn_translate.click(translate_click, anno_en, anno_cn)\n\n    # app.launch()\n    app.launch(server_name='0.0.0.0', debug=True, show_error=True, server_port=port)\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    gradio_app_vis_dataset(port=args.port)\n\n"}
